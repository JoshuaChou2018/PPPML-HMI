Loading anaconda...
...Anaconda env loaded
directing: X rim_enhanced: False test_id 0
removed wrong scan: weights_X_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_218_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_46_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_49_xwqg-A00121_2019-05-15.npy
# all image files: 12135 # all weight files in weight_dir: 9414 # image files with weight 9372
removed wrong scan: weights_X_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_218_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_46_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_49_xwqg-A00121_2019-05-15.npy
# all image files: 12135 # all weight files in weight_dir: 2468 # image files with weight 2460
train: /ibex/scratch/projects/c2052/air_tube_seg/training_samples_enhanced/X 9372
Using 4 GPUs
best_f1 is: 0.34115071098092575
Going to train epochs [8-37]
Training epoch 8
	step [1/196], loss=20.6089
	step [2/196], loss=20.6950
	step [3/196], loss=21.2341
	step [4/196], loss=20.6599
	step [5/196], loss=20.5525
	step [6/196], loss=21.0288
	step [7/196], loss=21.4969
	step [8/196], loss=19.7419
	step [9/196], loss=21.0158
	step [10/196], loss=20.6117
	step [11/196], loss=21.2535
	step [12/196], loss=20.8361
	step [13/196], loss=21.2369
	step [14/196], loss=20.4461
	step [15/196], loss=21.5605
	step [16/196], loss=21.1213
	step [17/196], loss=20.7094
	step [18/196], loss=20.4603
	step [19/196], loss=20.4259
	step [20/196], loss=19.7706
	step [21/196], loss=20.8191
	step [22/196], loss=20.7487
	step [23/196], loss=20.8234
	step [24/196], loss=19.3682
	step [25/196], loss=19.5554
	step [26/196], loss=21.4073
	step [27/196], loss=19.6562
	step [28/196], loss=19.7578
	step [29/196], loss=23.5512
	step [30/196], loss=20.7736
	step [31/196], loss=23.0568
	step [32/196], loss=19.9331
	step [33/196], loss=20.6139
	step [34/196], loss=20.5547
	step [35/196], loss=21.2351
	step [36/196], loss=19.7764
	step [37/196], loss=19.9084
	step [38/196], loss=20.9503
	step [39/196], loss=19.5074
	step [40/196], loss=18.8481
	step [41/196], loss=19.7771
	step [42/196], loss=19.9307
	step [43/196], loss=21.5997
	step [44/196], loss=19.1534
	step [45/196], loss=20.3212
	step [46/196], loss=21.7619
	step [47/196], loss=20.3151
	step [48/196], loss=19.3053
	step [49/196], loss=19.6550
	step [50/196], loss=19.6681
	step [51/196], loss=20.2943
	step [52/196], loss=20.0559
	step [53/196], loss=19.0796
	step [54/196], loss=20.9717
	step [55/196], loss=18.3191
	step [56/196], loss=20.0321
	step [57/196], loss=19.7455
	step [58/196], loss=20.8433
	step [59/196], loss=18.1708
	step [60/196], loss=20.4707
	step [61/196], loss=19.2840
	step [62/196], loss=19.9141
	step [63/196], loss=19.8647
	step [64/196], loss=18.0265
	step [65/196], loss=22.5308
	step [66/196], loss=19.3493
	step [67/196], loss=20.2945
	step [68/196], loss=20.1678
	step [69/196], loss=19.3077
	step [70/196], loss=19.7444
	step [71/196], loss=19.9786
	step [72/196], loss=19.5116
	step [73/196], loss=20.0713
	step [74/196], loss=21.4171
	step [75/196], loss=20.0037
	step [76/196], loss=21.4956
	step [77/196], loss=18.3406
	step [78/196], loss=20.8463
	step [79/196], loss=21.3589
	step [80/196], loss=19.8064
	step [81/196], loss=21.9814
	step [82/196], loss=20.5338
	step [83/196], loss=19.8297
	step [84/196], loss=20.0477
	step [85/196], loss=19.2167
	step [86/196], loss=18.8903
	step [87/196], loss=20.6392
	step [88/196], loss=21.4794
	step [89/196], loss=18.6204
	step [90/196], loss=20.0532
	step [91/196], loss=20.7250
	step [92/196], loss=20.7530
	step [93/196], loss=18.6018
	step [94/196], loss=18.6905
	step [95/196], loss=19.8111
	step [96/196], loss=20.5206
	step [97/196], loss=19.8871
	step [98/196], loss=19.3659
	step [99/196], loss=18.6430
	step [100/196], loss=18.9254
	step [101/196], loss=20.1974
	step [102/196], loss=17.8107
	step [103/196], loss=18.8330
	step [104/196], loss=18.2990
	step [105/196], loss=19.4563
	step [106/196], loss=18.6890
	step [107/196], loss=20.0818
	step [108/196], loss=17.9329
	step [109/196], loss=19.6326
	step [110/196], loss=20.4115
	step [111/196], loss=19.4124
	step [112/196], loss=20.8105
	step [113/196], loss=20.8729
	step [114/196], loss=19.2015
	step [115/196], loss=20.8328
	step [116/196], loss=19.7800
	step [117/196], loss=20.1817
	step [118/196], loss=19.4158
	step [119/196], loss=19.8559
	step [120/196], loss=18.1144
	step [121/196], loss=19.5848
	step [122/196], loss=18.4553
	step [123/196], loss=19.2247
	step [124/196], loss=20.1438
	step [125/196], loss=19.6982
	step [126/196], loss=19.7016
	step [127/196], loss=18.7454
	step [128/196], loss=19.6293
	step [129/196], loss=18.9073
	step [130/196], loss=20.6321
	step [131/196], loss=19.1496
	step [132/196], loss=19.4536
	step [133/196], loss=21.9167
	step [134/196], loss=17.5137
	step [135/196], loss=19.3905
	step [136/196], loss=18.5537
	step [137/196], loss=19.5543
	step [138/196], loss=18.8590
	step [139/196], loss=18.2945
	step [140/196], loss=18.9999
	step [141/196], loss=19.4892
	step [142/196], loss=17.9029
	step [143/196], loss=18.9092
	step [144/196], loss=18.7149
	step [145/196], loss=19.2413
	step [146/196], loss=19.9590
	step [147/196], loss=18.3652
	step [148/196], loss=19.5105
	step [149/196], loss=17.3252
	step [150/196], loss=17.9949
	step [151/196], loss=19.5595
	step [152/196], loss=19.3126
	step [153/196], loss=19.0939
	step [154/196], loss=19.2997
	step [155/196], loss=17.5388
	step [156/196], loss=18.7883
	step [157/196], loss=18.8409
	step [158/196], loss=18.9932
	step [159/196], loss=19.8667
	step [160/196], loss=19.0577
	step [161/196], loss=19.1417
	step [162/196], loss=19.4996
	step [163/196], loss=18.1001
	step [164/196], loss=18.3685
	step [165/196], loss=19.0678
	step [166/196], loss=20.2745
	step [167/196], loss=18.8354
	step [168/196], loss=18.1023
	step [169/196], loss=16.9476
	step [170/196], loss=18.1499
	step [171/196], loss=18.2751
	step [172/196], loss=18.8462
	step [173/196], loss=19.9654
	step [174/196], loss=19.2475
	step [175/196], loss=16.8493
	step [176/196], loss=19.6313
	step [177/196], loss=18.3124
	step [178/196], loss=17.8587
	step [179/196], loss=18.0681
	step [180/196], loss=18.8172
	step [181/196], loss=19.7975
	step [182/196], loss=18.0743
	step [183/196], loss=19.3951
	step [184/196], loss=18.9345
	step [185/196], loss=18.1945
	step [186/196], loss=19.2218
	step [187/196], loss=19.4606
	step [188/196], loss=18.4528
	step [189/196], loss=17.7882
	step [190/196], loss=18.7530
	step [191/196], loss=17.7480
	step [192/196], loss=17.6505
	step [193/196], loss=18.0835
	step [194/196], loss=19.3641
	step [195/196], loss=18.3247
	step [196/196], loss=4.5308
	Evaluating
	loss=0.0774, precision=0.1843, recall=0.9957, f1=0.3110
Training epoch 9
	step [1/196], loss=19.6327
	step [2/196], loss=17.3924
	step [3/196], loss=17.8763
	step [4/196], loss=19.3867
	step [5/196], loss=18.6115
	step [6/196], loss=18.4738
	step [7/196], loss=18.0705
	step [8/196], loss=17.4424
	step [9/196], loss=18.2866
	step [10/196], loss=17.8576
	step [11/196], loss=16.6670
	step [12/196], loss=17.6867
	step [13/196], loss=18.0119
	step [14/196], loss=17.6990
	step [15/196], loss=19.8731
	step [16/196], loss=17.8433
	step [17/196], loss=18.1034
	step [18/196], loss=18.5954
	step [19/196], loss=17.7951
	step [20/196], loss=17.9710
	step [21/196], loss=17.4361
	step [22/196], loss=18.0895
	step [23/196], loss=16.7114
	step [24/196], loss=17.8755
	step [25/196], loss=16.5308
	step [26/196], loss=18.3143
	step [27/196], loss=17.6862
	step [28/196], loss=17.1205
	step [29/196], loss=19.0454
	step [30/196], loss=16.9807
	step [31/196], loss=19.3495
	step [32/196], loss=17.0746
	step [33/196], loss=18.2965
	step [34/196], loss=19.6357
	step [35/196], loss=19.0550
	step [36/196], loss=18.0921
	step [37/196], loss=17.1265
	step [38/196], loss=18.0413
	step [39/196], loss=18.0961
	step [40/196], loss=18.0467
	step [41/196], loss=17.3695
	step [42/196], loss=16.6866
	step [43/196], loss=17.9979
	step [44/196], loss=20.2314
	step [45/196], loss=17.1298
	step [46/196], loss=17.8098
	step [47/196], loss=17.8419
	step [48/196], loss=18.9678
	step [49/196], loss=17.5794
	step [50/196], loss=16.7820
	step [51/196], loss=17.2846
	step [52/196], loss=18.2245
	step [53/196], loss=17.2981
	step [54/196], loss=16.9907
	step [55/196], loss=18.5111
	step [56/196], loss=18.1455
	step [57/196], loss=18.5164
	step [58/196], loss=17.2460
	step [59/196], loss=18.3243
	step [60/196], loss=18.0825
	step [61/196], loss=18.1739
	step [62/196], loss=17.7971
	step [63/196], loss=15.5966
	step [64/196], loss=16.4921
	step [65/196], loss=17.6067
	step [66/196], loss=19.1384
	step [67/196], loss=17.3783
	step [68/196], loss=18.6171
	step [69/196], loss=17.3160
	step [70/196], loss=16.9121
	step [71/196], loss=16.7116
	step [72/196], loss=19.1853
	step [73/196], loss=18.2325
	step [74/196], loss=18.0585
	step [75/196], loss=15.6815
	step [76/196], loss=18.1201
	step [77/196], loss=16.1313
	step [78/196], loss=18.1115
	step [79/196], loss=16.6216
	step [80/196], loss=17.1074
	step [81/196], loss=16.8749
	step [82/196], loss=17.5024
	step [83/196], loss=18.9304
	step [84/196], loss=17.3229
	step [85/196], loss=17.7108
	step [86/196], loss=17.4174
	step [87/196], loss=16.2945
	step [88/196], loss=16.7078
	step [89/196], loss=18.0009
	step [90/196], loss=16.9859
	step [91/196], loss=16.6519
	step [92/196], loss=17.9201
	step [93/196], loss=17.4764
	step [94/196], loss=16.9395
	step [95/196], loss=16.4577
	step [96/196], loss=18.2812
	step [97/196], loss=16.7005
	step [98/196], loss=16.1536
	step [99/196], loss=17.5744
	step [100/196], loss=17.6285
	step [101/196], loss=15.9697
	step [102/196], loss=17.6822
	step [103/196], loss=16.7363
	step [104/196], loss=16.8133
	step [105/196], loss=15.7327
	step [106/196], loss=17.5370
	step [107/196], loss=16.0670
	step [108/196], loss=17.7474
	step [109/196], loss=17.6394
	step [110/196], loss=16.3467
	step [111/196], loss=17.5829
	step [112/196], loss=20.0370
	step [113/196], loss=19.1570
	step [114/196], loss=17.2053
	step [115/196], loss=17.0085
	step [116/196], loss=17.0832
	step [117/196], loss=16.3517
	step [118/196], loss=17.1784
	step [119/196], loss=17.6423
	step [120/196], loss=17.7225
	step [121/196], loss=16.4893
	step [122/196], loss=17.8603
	step [123/196], loss=16.2379
	step [124/196], loss=17.3453
	step [125/196], loss=17.8017
	step [126/196], loss=19.2167
	step [127/196], loss=17.3408
	step [128/196], loss=18.9795
	step [129/196], loss=16.8255
	step [130/196], loss=16.6142
	step [131/196], loss=16.6188
	step [132/196], loss=17.5641
	step [133/196], loss=18.7383
	step [134/196], loss=16.8644
	step [135/196], loss=17.8475
	step [136/196], loss=16.0072
	step [137/196], loss=18.7000
	step [138/196], loss=18.7403
	step [139/196], loss=17.8582
	step [140/196], loss=16.8841
	step [141/196], loss=16.3404
	step [142/196], loss=17.1614
	step [143/196], loss=18.5701
	step [144/196], loss=17.2588
	step [145/196], loss=17.8757
	step [146/196], loss=17.2739
	step [147/196], loss=16.8462
	step [148/196], loss=16.5167
	step [149/196], loss=18.2257
	step [150/196], loss=16.0442
	step [151/196], loss=16.8591
	step [152/196], loss=18.3328
	step [153/196], loss=18.7308
	step [154/196], loss=16.9099
	step [155/196], loss=17.1835
	step [156/196], loss=18.3547
	step [157/196], loss=16.5350
	step [158/196], loss=15.2175
	step [159/196], loss=16.1864
	step [160/196], loss=16.1630
	step [161/196], loss=15.4273
	step [162/196], loss=19.1439
	step [163/196], loss=18.6264
	step [164/196], loss=16.4835
	step [165/196], loss=15.8003
	step [166/196], loss=16.4183
	step [167/196], loss=15.4301
	step [168/196], loss=17.1230
	step [169/196], loss=19.2718
	step [170/196], loss=18.3022
	step [171/196], loss=16.5833
	step [172/196], loss=17.1799
	step [173/196], loss=18.1432
	step [174/196], loss=16.0970
	step [175/196], loss=18.4875
	step [176/196], loss=16.3156
	step [177/196], loss=17.5413
	step [178/196], loss=15.9250
	step [179/196], loss=15.3606
	step [180/196], loss=15.4628
	step [181/196], loss=16.3203
	step [182/196], loss=16.8325
	step [183/196], loss=16.7625
	step [184/196], loss=16.0807
	step [185/196], loss=15.7871
	step [186/196], loss=17.6314
	step [187/196], loss=15.7383
	step [188/196], loss=16.8306
	step [189/196], loss=15.3417
	step [190/196], loss=16.7201
	step [191/196], loss=15.4579
	step [192/196], loss=17.3444
	step [193/196], loss=15.6033
	step [194/196], loss=16.7130
	step [195/196], loss=17.6965
	step [196/196], loss=4.6070
	Evaluating
	loss=0.0733, precision=0.1609, recall=0.9963, f1=0.2770
Training epoch 10
	step [1/196], loss=16.4548
	step [2/196], loss=16.9350
	step [3/196], loss=15.7317
	step [4/196], loss=15.5050
	step [5/196], loss=16.9462
	step [6/196], loss=17.3374
	step [7/196], loss=14.8856
	step [8/196], loss=17.1049
	step [9/196], loss=16.7044
	step [10/196], loss=16.7530
	step [11/196], loss=17.4856
	step [12/196], loss=16.0979
	step [13/196], loss=15.0195
	step [14/196], loss=16.1313
	step [15/196], loss=16.4806
	step [16/196], loss=18.4435
	step [17/196], loss=15.6557
	step [18/196], loss=14.6887
	step [19/196], loss=16.5793
	step [20/196], loss=16.2397
	step [21/196], loss=16.4836
	step [22/196], loss=16.3158
	step [23/196], loss=15.2300
	step [24/196], loss=16.0892
	step [25/196], loss=15.4063
	step [26/196], loss=17.8467
	step [27/196], loss=15.9300
	step [28/196], loss=16.1025
	step [29/196], loss=15.1666
	step [30/196], loss=15.5607
	step [31/196], loss=15.8585
	step [32/196], loss=16.2736
	step [33/196], loss=17.0962
	step [34/196], loss=16.5570
	step [35/196], loss=15.2340
	step [36/196], loss=16.4800
	step [37/196], loss=15.3275
	step [38/196], loss=16.7540
	step [39/196], loss=16.1440
	step [40/196], loss=15.9298
	step [41/196], loss=15.5784
	step [42/196], loss=14.9768
	step [43/196], loss=15.6502
	step [44/196], loss=18.4295
	step [45/196], loss=15.9916
	step [46/196], loss=16.9480
	step [47/196], loss=15.1421
	step [48/196], loss=17.6539
	step [49/196], loss=15.4864
	step [50/196], loss=15.0709
	step [51/196], loss=15.7483
	step [52/196], loss=15.0984
	step [53/196], loss=17.7081
	step [54/196], loss=18.0872
	step [55/196], loss=18.4455
	step [56/196], loss=15.4152
	step [57/196], loss=16.1393
	step [58/196], loss=14.9944
	step [59/196], loss=15.3308
	step [60/196], loss=14.9422
	step [61/196], loss=16.0487
	step [62/196], loss=16.8053
	step [63/196], loss=15.2594
	step [64/196], loss=16.1687
	step [65/196], loss=17.0195
	step [66/196], loss=14.5951
	step [67/196], loss=15.0207
	step [68/196], loss=15.2880
	step [69/196], loss=17.4547
	step [70/196], loss=16.4221
	step [71/196], loss=16.0163
	step [72/196], loss=15.3919
	step [73/196], loss=14.7142
	step [74/196], loss=16.1011
	step [75/196], loss=16.0481
	step [76/196], loss=14.5345
	step [77/196], loss=13.9260
	step [78/196], loss=15.4547
	step [79/196], loss=15.7631
	step [80/196], loss=15.7322
	step [81/196], loss=17.8906
	step [82/196], loss=15.7489
	step [83/196], loss=16.5338
	step [84/196], loss=16.0402
	step [85/196], loss=15.4614
	step [86/196], loss=16.7966
	step [87/196], loss=15.2166
	step [88/196], loss=15.9062
	step [89/196], loss=15.7472
	step [90/196], loss=15.1994
	step [91/196], loss=15.5537
	step [92/196], loss=15.4163
	step [93/196], loss=15.9334
	step [94/196], loss=14.2448
	step [95/196], loss=15.7658
	step [96/196], loss=16.4688
	step [97/196], loss=15.4496
	step [98/196], loss=15.2315
	step [99/196], loss=15.4634
	step [100/196], loss=17.4177
	step [101/196], loss=15.9740
	step [102/196], loss=16.7804
	step [103/196], loss=15.8680
	step [104/196], loss=16.4988
	step [105/196], loss=15.9773
	step [106/196], loss=15.5811
	step [107/196], loss=17.1703
	step [108/196], loss=15.5880
	step [109/196], loss=16.6065
	step [110/196], loss=15.8033
	step [111/196], loss=15.9782
	step [112/196], loss=14.1432
	step [113/196], loss=18.0032
	step [114/196], loss=16.2989
	step [115/196], loss=14.0997
	step [116/196], loss=14.8371
	step [117/196], loss=14.8228
	step [118/196], loss=14.9952
	step [119/196], loss=15.2473
	step [120/196], loss=15.7833
	step [121/196], loss=15.8293
	step [122/196], loss=14.5557
	step [123/196], loss=14.3563
	step [124/196], loss=14.4173
	step [125/196], loss=15.9201
	step [126/196], loss=14.3559
	step [127/196], loss=15.8489
	step [128/196], loss=14.6864
	step [129/196], loss=14.0005
	step [130/196], loss=18.8343
	step [131/196], loss=14.6364
	step [132/196], loss=16.1604
	step [133/196], loss=14.9836
	step [134/196], loss=15.5894
	step [135/196], loss=16.5702
	step [136/196], loss=14.1608
	step [137/196], loss=15.4228
	step [138/196], loss=14.7639
	step [139/196], loss=15.3611
	step [140/196], loss=15.0047
	step [141/196], loss=16.9060
	step [142/196], loss=16.0546
	step [143/196], loss=15.1981
	step [144/196], loss=15.9699
	step [145/196], loss=15.2979
	step [146/196], loss=15.6055
	step [147/196], loss=13.9833
	step [148/196], loss=14.8162
	step [149/196], loss=14.3457
	step [150/196], loss=13.8831
	step [151/196], loss=16.5117
	step [152/196], loss=14.3217
	step [153/196], loss=14.0694
	step [154/196], loss=14.7521
	step [155/196], loss=14.1005
	step [156/196], loss=14.4214
	step [157/196], loss=14.9888
	step [158/196], loss=13.5929
	step [159/196], loss=16.0990
	step [160/196], loss=15.0813
	step [161/196], loss=16.6417
	step [162/196], loss=16.2456
	step [163/196], loss=14.5477
	step [164/196], loss=14.0058
	step [165/196], loss=16.3164
	step [166/196], loss=18.3710
	step [167/196], loss=14.6199
	step [168/196], loss=16.4503
	step [169/196], loss=13.1117
	step [170/196], loss=14.2087
	step [171/196], loss=14.5062
	step [172/196], loss=16.4594
	step [173/196], loss=15.2075
	step [174/196], loss=14.2965
	step [175/196], loss=15.4742
	step [176/196], loss=17.2134
	step [177/196], loss=14.7761
	step [178/196], loss=14.0863
	step [179/196], loss=14.1400
	step [180/196], loss=16.4421
	step [181/196], loss=14.4085
	step [182/196], loss=14.4319
	step [183/196], loss=15.6952
	step [184/196], loss=15.8757
	step [185/196], loss=15.5206
	step [186/196], loss=14.6185
	step [187/196], loss=15.2293
	step [188/196], loss=14.6505
	step [189/196], loss=13.8972
	step [190/196], loss=14.3847
	step [191/196], loss=13.3725
	step [192/196], loss=14.6653
	step [193/196], loss=15.2910
	step [194/196], loss=14.0937
	step [195/196], loss=15.6698
	step [196/196], loss=3.5818
	Evaluating
	loss=0.0620, precision=0.1711, recall=0.9955, f1=0.2921
Training epoch 11
	step [1/196], loss=14.7333
	step [2/196], loss=14.0751
	step [3/196], loss=13.2381
	step [4/196], loss=15.7921
	step [5/196], loss=14.9530
	step [6/196], loss=15.9484
	step [7/196], loss=15.0081
	step [8/196], loss=16.7810
	step [9/196], loss=14.7976
	step [10/196], loss=13.9184
	step [11/196], loss=14.0080
	step [12/196], loss=14.6197
	step [13/196], loss=14.7988
	step [14/196], loss=14.4009
	step [15/196], loss=15.7860
	step [16/196], loss=14.3573
	step [17/196], loss=15.1734
	step [18/196], loss=13.9781
	step [19/196], loss=14.9822
	step [20/196], loss=14.1699
	step [21/196], loss=14.4162
	step [22/196], loss=17.5548
	step [23/196], loss=13.9746
	step [24/196], loss=14.7822
	step [25/196], loss=15.8803
	step [26/196], loss=15.8514
	step [27/196], loss=14.9542
	step [28/196], loss=14.0411
	step [29/196], loss=15.4404
	step [30/196], loss=15.7784
	step [31/196], loss=15.7050
	step [32/196], loss=16.9942
	step [33/196], loss=14.9953
	step [34/196], loss=14.3374
	step [35/196], loss=14.6886
	step [36/196], loss=14.6751
	step [37/196], loss=14.2548
	step [38/196], loss=15.0534
	step [39/196], loss=16.1055
	step [40/196], loss=13.0509
	step [41/196], loss=13.5862
	step [42/196], loss=15.2625
	step [43/196], loss=13.4249
	step [44/196], loss=15.0153
	step [45/196], loss=15.1805
	step [46/196], loss=15.2398
	step [47/196], loss=15.6374
	step [48/196], loss=17.2977
	step [49/196], loss=14.2726
	step [50/196], loss=12.6924
	step [51/196], loss=14.2740
	step [52/196], loss=14.3368
	step [53/196], loss=14.3499
	step [54/196], loss=14.9655
	step [55/196], loss=14.6028
	step [56/196], loss=14.7114
	step [57/196], loss=15.0803
	step [58/196], loss=14.2244
	step [59/196], loss=12.7201
	step [60/196], loss=15.1921
	step [61/196], loss=14.8188
	step [62/196], loss=15.2556
	step [63/196], loss=14.6064
	step [64/196], loss=13.2560
	step [65/196], loss=13.1988
	step [66/196], loss=13.1135
	step [67/196], loss=15.2060
	step [68/196], loss=14.6628
	step [69/196], loss=14.1504
	step [70/196], loss=14.2939
	step [71/196], loss=14.5683
	step [72/196], loss=13.9111
	step [73/196], loss=13.8480
	step [74/196], loss=14.7271
	step [75/196], loss=13.9942
	step [76/196], loss=15.6398
	step [77/196], loss=14.6312
	step [78/196], loss=13.5118
	step [79/196], loss=15.6324
	step [80/196], loss=13.4740
	step [81/196], loss=13.8204
	step [82/196], loss=13.7890
	step [83/196], loss=15.0288
	step [84/196], loss=13.5733
	step [85/196], loss=14.3071
	step [86/196], loss=14.5972
	step [87/196], loss=14.0613
	step [88/196], loss=13.7418
	step [89/196], loss=14.4586
	step [90/196], loss=14.1047
	step [91/196], loss=14.9999
	step [92/196], loss=13.8429
	step [93/196], loss=14.4390
	step [94/196], loss=14.7163
	step [95/196], loss=15.5074
	step [96/196], loss=13.3530
	step [97/196], loss=14.0637
	step [98/196], loss=14.0225
	step [99/196], loss=14.7913
	step [100/196], loss=14.4329
	step [101/196], loss=13.3892
	step [102/196], loss=14.1428
	step [103/196], loss=15.0997
	step [104/196], loss=12.5592
	step [105/196], loss=13.6813
	step [106/196], loss=14.7264
	step [107/196], loss=14.4193
	step [108/196], loss=13.5010
	step [109/196], loss=13.6808
	step [110/196], loss=14.5400
	step [111/196], loss=14.2684
	step [112/196], loss=15.7723
	step [113/196], loss=14.2904
	step [114/196], loss=14.4017
	step [115/196], loss=13.0403
	step [116/196], loss=14.9834
	step [117/196], loss=15.0819
	step [118/196], loss=12.6150
	step [119/196], loss=14.8026
	step [120/196], loss=13.3747
	step [121/196], loss=14.5131
	step [122/196], loss=13.6474
	step [123/196], loss=13.9543
	step [124/196], loss=13.4166
	step [125/196], loss=14.7741
	step [126/196], loss=13.0815
	step [127/196], loss=14.7250
	step [128/196], loss=15.0072
	step [129/196], loss=14.9449
	step [130/196], loss=12.4971
	step [131/196], loss=13.9463
	step [132/196], loss=14.8228
	step [133/196], loss=13.3417
	step [134/196], loss=13.0906
	step [135/196], loss=13.7135
	step [136/196], loss=14.1364
	step [137/196], loss=14.1239
	step [138/196], loss=14.2524
	step [139/196], loss=12.9677
	step [140/196], loss=13.7559
	step [141/196], loss=13.0967
	step [142/196], loss=13.6815
	step [143/196], loss=14.4382
	step [144/196], loss=15.9515
	step [145/196], loss=13.0020
	step [146/196], loss=14.5181
	step [147/196], loss=12.8979
	step [148/196], loss=15.2180
	step [149/196], loss=15.9817
	step [150/196], loss=14.2505
	step [151/196], loss=14.6310
	step [152/196], loss=13.8616
	step [153/196], loss=15.8583
	step [154/196], loss=13.6811
	step [155/196], loss=13.5304
	step [156/196], loss=13.6727
	step [157/196], loss=13.3477
	step [158/196], loss=15.5799
	step [159/196], loss=13.5814
	step [160/196], loss=12.2864
	step [161/196], loss=12.2547
	step [162/196], loss=13.5622
	step [163/196], loss=14.1457
	step [164/196], loss=14.8638
	step [165/196], loss=13.9469
	step [166/196], loss=13.7456
	step [167/196], loss=13.9055
	step [168/196], loss=12.9278
	step [169/196], loss=13.0424
	step [170/196], loss=13.0306
	step [171/196], loss=14.1144
	step [172/196], loss=15.8725
	step [173/196], loss=13.5501
	step [174/196], loss=13.9385
	step [175/196], loss=13.5077
	step [176/196], loss=12.3557
	step [177/196], loss=16.1082
	step [178/196], loss=14.2371
	step [179/196], loss=12.1284
	step [180/196], loss=13.1266
	step [181/196], loss=13.4003
	step [182/196], loss=13.2362
	step [183/196], loss=14.2475
	step [184/196], loss=14.9674
	step [185/196], loss=14.8105
	step [186/196], loss=13.2053
	step [187/196], loss=13.5673
	step [188/196], loss=13.3284
	step [189/196], loss=13.6114
	step [190/196], loss=13.1475
	step [191/196], loss=13.8756
	step [192/196], loss=12.7717
	step [193/196], loss=15.1301
	step [194/196], loss=13.4941
	step [195/196], loss=13.0902
	step [196/196], loss=2.9017
	Evaluating
	loss=0.0511, precision=0.2012, recall=0.9925, f1=0.3346
Training epoch 12
	step [1/196], loss=13.4959
	step [2/196], loss=14.1680
	step [3/196], loss=14.1053
	step [4/196], loss=13.4346
	step [5/196], loss=13.5315
	step [6/196], loss=13.9641
	step [7/196], loss=15.6935
	step [8/196], loss=12.2672
	step [9/196], loss=14.0917
	step [10/196], loss=13.7763
	step [11/196], loss=14.6593
	step [12/196], loss=12.4519
	step [13/196], loss=12.2040
	step [14/196], loss=13.3073
	step [15/196], loss=13.1317
	step [16/196], loss=12.9852
	step [17/196], loss=14.6727
	step [18/196], loss=13.4621
	step [19/196], loss=12.2771
	step [20/196], loss=13.6353
	step [21/196], loss=13.4272
	step [22/196], loss=14.5668
	step [23/196], loss=12.8391
	step [24/196], loss=13.0548
	step [25/196], loss=12.7672
	step [26/196], loss=13.7519
	step [27/196], loss=12.4130
	step [28/196], loss=13.2597
	step [29/196], loss=14.4243
	step [30/196], loss=13.0042
	step [31/196], loss=13.7658
	step [32/196], loss=13.1094
	step [33/196], loss=11.2104
	step [34/196], loss=12.3463
	step [35/196], loss=12.9004
	step [36/196], loss=13.8060
	step [37/196], loss=14.0508
	step [38/196], loss=12.4029
	step [39/196], loss=15.4474
	step [40/196], loss=13.7138
	step [41/196], loss=13.3416
	step [42/196], loss=12.9849
	step [43/196], loss=13.0278
	step [44/196], loss=13.0965
	step [45/196], loss=12.5816
	step [46/196], loss=12.9005
	step [47/196], loss=13.5146
	step [48/196], loss=12.2170
	step [49/196], loss=12.7420
	step [50/196], loss=13.8527
	step [51/196], loss=12.1536
	step [52/196], loss=12.7186
	step [53/196], loss=12.8131
	step [54/196], loss=13.5712
	step [55/196], loss=12.9951
	step [56/196], loss=13.1431
	step [57/196], loss=11.9989
	step [58/196], loss=14.1111
	step [59/196], loss=13.6724
	step [60/196], loss=13.5629
	step [61/196], loss=14.4521
	step [62/196], loss=12.0930
	step [63/196], loss=13.0502
	step [64/196], loss=13.4880
	step [65/196], loss=12.9133
	step [66/196], loss=12.5431
	step [67/196], loss=13.6383
	step [68/196], loss=12.4758
	step [69/196], loss=12.4521
	step [70/196], loss=12.9407
	step [71/196], loss=14.8298
	step [72/196], loss=13.0094
	step [73/196], loss=13.6977
	step [74/196], loss=12.1134
	step [75/196], loss=12.3607
	step [76/196], loss=13.9097
	step [77/196], loss=13.5737
	step [78/196], loss=13.6551
	step [79/196], loss=15.0648
	step [80/196], loss=12.8886
	step [81/196], loss=13.0447
	step [82/196], loss=13.5221
	step [83/196], loss=13.3253
	step [84/196], loss=14.0177
	step [85/196], loss=13.0487
	step [86/196], loss=12.0293
	step [87/196], loss=12.7298
	step [88/196], loss=12.5357
	step [89/196], loss=13.1622
	step [90/196], loss=14.3502
	step [91/196], loss=14.3117
	step [92/196], loss=14.2764
	step [93/196], loss=12.8831
	step [94/196], loss=13.3773
	step [95/196], loss=13.0654
	step [96/196], loss=12.6437
	step [97/196], loss=12.4993
	step [98/196], loss=12.0221
	step [99/196], loss=13.4138
	step [100/196], loss=13.2811
	step [101/196], loss=12.1279
	step [102/196], loss=15.8115
	step [103/196], loss=13.9970
	step [104/196], loss=13.1703
	step [105/196], loss=13.5313
	step [106/196], loss=13.7779
	step [107/196], loss=12.8515
	step [108/196], loss=12.6653
	step [109/196], loss=12.6038
	step [110/196], loss=13.1477
	step [111/196], loss=13.5092
	step [112/196], loss=15.3237
	step [113/196], loss=12.8681
	step [114/196], loss=13.6450
	step [115/196], loss=14.5602
	step [116/196], loss=13.4170
	step [117/196], loss=11.8307
	step [118/196], loss=11.8328
	step [119/196], loss=15.0250
	step [120/196], loss=13.2963
	step [121/196], loss=14.1236
	step [122/196], loss=14.3651
	step [123/196], loss=13.6642
	step [124/196], loss=13.1079
	step [125/196], loss=13.3933
	step [126/196], loss=13.4589
	step [127/196], loss=12.6223
	step [128/196], loss=13.8141
	step [129/196], loss=13.8755
	step [130/196], loss=12.8975
	step [131/196], loss=11.9365
	step [132/196], loss=12.5033
	step [133/196], loss=12.2428
	step [134/196], loss=13.3986
	step [135/196], loss=13.9662
	step [136/196], loss=11.9245
	step [137/196], loss=11.9435
	step [138/196], loss=13.5043
	step [139/196], loss=13.0566
	step [140/196], loss=12.6221
	step [141/196], loss=12.6469
	step [142/196], loss=12.6177
	step [143/196], loss=12.4679
	step [144/196], loss=12.6407
	step [145/196], loss=13.0386
	step [146/196], loss=11.3142
	step [147/196], loss=11.9321
	step [148/196], loss=13.1704
	step [149/196], loss=14.9461
	step [150/196], loss=12.9833
	step [151/196], loss=12.4102
	step [152/196], loss=12.9103
	step [153/196], loss=12.6149
	step [154/196], loss=13.1159
	step [155/196], loss=12.3837
	step [156/196], loss=12.9044
	step [157/196], loss=14.2357
	step [158/196], loss=12.1579
	step [159/196], loss=12.7759
	step [160/196], loss=14.2920
	step [161/196], loss=11.5992
	step [162/196], loss=12.3270
	step [163/196], loss=13.0175
	step [164/196], loss=11.7553
	step [165/196], loss=11.4454
	step [166/196], loss=13.9899
	step [167/196], loss=13.1133
	step [168/196], loss=11.0079
	step [169/196], loss=13.4148
	step [170/196], loss=13.0252
	step [171/196], loss=12.7341
	step [172/196], loss=13.5523
	step [173/196], loss=13.1930
	step [174/196], loss=12.8760
	step [175/196], loss=10.7778
	step [176/196], loss=12.9196
	step [177/196], loss=13.4353
	step [178/196], loss=13.8148
	step [179/196], loss=12.4605
	step [180/196], loss=11.7876
	step [181/196], loss=13.9124
	step [182/196], loss=11.9451
	step [183/196], loss=13.0030
	step [184/196], loss=13.3499
	step [185/196], loss=12.2374
	step [186/196], loss=13.1578
	step [187/196], loss=12.7874
	step [188/196], loss=12.3583
	step [189/196], loss=10.9884
	step [190/196], loss=12.6946
	step [191/196], loss=12.6635
	step [192/196], loss=11.6307
	step [193/196], loss=14.9945
	step [194/196], loss=12.4791
	step [195/196], loss=12.2833
	step [196/196], loss=3.6886
	Evaluating
	loss=0.0596, precision=0.1531, recall=0.9966, f1=0.2654
Training epoch 13
	step [1/196], loss=11.6387
	step [2/196], loss=11.3673
	step [3/196], loss=13.4012
	step [4/196], loss=13.6366
	step [5/196], loss=12.2215
	step [6/196], loss=12.4095
	step [7/196], loss=12.5696
	step [8/196], loss=13.5186
	step [9/196], loss=13.1864
	step [10/196], loss=14.5875
	step [11/196], loss=13.9194
	step [12/196], loss=12.4402
	step [13/196], loss=11.8037
	step [14/196], loss=12.6184
	step [15/196], loss=12.8718
	step [16/196], loss=12.0644
	step [17/196], loss=12.8367
	step [18/196], loss=12.2241
	step [19/196], loss=10.9225
	step [20/196], loss=13.1389
	step [21/196], loss=12.5087
	step [22/196], loss=13.7775
	step [23/196], loss=12.3149
	step [24/196], loss=12.2553
	step [25/196], loss=12.8003
	step [26/196], loss=13.0401
	step [27/196], loss=13.9582
	step [28/196], loss=12.2559
	step [29/196], loss=10.8998
	step [30/196], loss=11.9498
	step [31/196], loss=13.6887
	step [32/196], loss=12.9644
	step [33/196], loss=12.9588
	step [34/196], loss=11.7886
	step [35/196], loss=11.1167
	step [36/196], loss=12.1674
	step [37/196], loss=13.7494
	step [38/196], loss=13.1190
	step [39/196], loss=12.0344
	step [40/196], loss=11.9218
	step [41/196], loss=15.3533
	step [42/196], loss=12.2221
	step [43/196], loss=12.5776
	step [44/196], loss=11.9233
	step [45/196], loss=13.2520
	step [46/196], loss=12.2208
	step [47/196], loss=12.2268
	step [48/196], loss=12.3891
	step [49/196], loss=11.2839
	step [50/196], loss=13.3845
	step [51/196], loss=10.7168
	step [52/196], loss=12.2553
	step [53/196], loss=12.2710
	step [54/196], loss=15.2070
	step [55/196], loss=12.9105
	step [56/196], loss=12.1048
	step [57/196], loss=11.7387
	step [58/196], loss=11.7237
	step [59/196], loss=12.1961
	step [60/196], loss=12.5192
	step [61/196], loss=10.7742
	step [62/196], loss=10.8442
	step [63/196], loss=12.1419
	step [64/196], loss=12.2448
	step [65/196], loss=13.0407
	step [66/196], loss=10.8869
	step [67/196], loss=13.3722
	step [68/196], loss=10.7697
	step [69/196], loss=12.7248
	step [70/196], loss=12.2561
	step [71/196], loss=10.9250
	step [72/196], loss=12.0050
	step [73/196], loss=12.4490
	step [74/196], loss=11.1926
	step [75/196], loss=12.2942
	step [76/196], loss=13.7590
	step [77/196], loss=11.1393
	step [78/196], loss=13.4726
	step [79/196], loss=10.9501
	step [80/196], loss=12.6242
	step [81/196], loss=12.0744
	step [82/196], loss=12.3283
	step [83/196], loss=10.1583
	step [84/196], loss=10.9994
	step [85/196], loss=11.7373
	step [86/196], loss=12.0688
	step [87/196], loss=13.3683
	step [88/196], loss=13.8073
	step [89/196], loss=11.2351
	step [90/196], loss=10.5357
	step [91/196], loss=12.5413
	step [92/196], loss=10.9020
	step [93/196], loss=11.5759
	step [94/196], loss=11.7153
	step [95/196], loss=14.0708
	step [96/196], loss=12.5000
	step [97/196], loss=13.1579
	step [98/196], loss=10.6492
	step [99/196], loss=11.7318
	step [100/196], loss=11.6516
	step [101/196], loss=12.3689
	step [102/196], loss=13.6452
	step [103/196], loss=12.5830
	step [104/196], loss=11.6657
	step [105/196], loss=12.6777
	step [106/196], loss=11.9108
	step [107/196], loss=11.5880
	step [108/196], loss=11.2730
	step [109/196], loss=12.5913
	step [110/196], loss=12.0249
	step [111/196], loss=12.0970
	step [112/196], loss=11.6494
	step [113/196], loss=11.8329
	step [114/196], loss=12.2408
	step [115/196], loss=11.9541
	step [116/196], loss=13.4107
	step [117/196], loss=12.3296
	step [118/196], loss=11.1516
	step [119/196], loss=12.7261
	step [120/196], loss=11.4945
	step [121/196], loss=12.6366
	step [122/196], loss=13.5151
	step [123/196], loss=12.7040
	step [124/196], loss=10.9057
	step [125/196], loss=10.8266
	step [126/196], loss=11.2998
	step [127/196], loss=11.7158
	step [128/196], loss=10.7376
	step [129/196], loss=13.7368
	step [130/196], loss=12.4791
	step [131/196], loss=12.6529
	step [132/196], loss=12.4075
	step [133/196], loss=13.1048
	step [134/196], loss=12.5904
	step [135/196], loss=11.7412
	step [136/196], loss=11.1880
	step [137/196], loss=11.2830
	step [138/196], loss=12.5201
	step [139/196], loss=12.8641
	step [140/196], loss=11.5514
	step [141/196], loss=11.0099
	step [142/196], loss=12.2469
	step [143/196], loss=10.7617
	step [144/196], loss=13.7782
	step [145/196], loss=12.7343
	step [146/196], loss=12.6790
	step [147/196], loss=11.7968
	step [148/196], loss=11.9770
	step [149/196], loss=10.6942
	step [150/196], loss=10.7077
	step [151/196], loss=10.6062
	step [152/196], loss=10.0944
	step [153/196], loss=12.6713
	step [154/196], loss=12.0215
	step [155/196], loss=11.9270
	step [156/196], loss=13.4632
	step [157/196], loss=11.0553
	step [158/196], loss=11.2230
	step [159/196], loss=13.5131
	step [160/196], loss=10.8612
	step [161/196], loss=11.8463
	step [162/196], loss=12.9411
	step [163/196], loss=11.6148
	step [164/196], loss=12.6593
	step [165/196], loss=10.9550
	step [166/196], loss=10.1705
	step [167/196], loss=11.8553
	step [168/196], loss=11.5516
	step [169/196], loss=12.8568
	step [170/196], loss=12.1438
	step [171/196], loss=11.5061
	step [172/196], loss=12.9305
	step [173/196], loss=13.2557
	step [174/196], loss=12.8216
	step [175/196], loss=13.0718
	step [176/196], loss=11.6183
	step [177/196], loss=11.0352
	step [178/196], loss=12.3402
	step [179/196], loss=12.2682
	step [180/196], loss=10.8176
	step [181/196], loss=12.6374
	step [182/196], loss=12.2411
	step [183/196], loss=11.6408
	step [184/196], loss=11.2431
	step [185/196], loss=11.9622
	step [186/196], loss=10.2898
	step [187/196], loss=12.7612
	step [188/196], loss=12.0567
	step [189/196], loss=11.5043
	step [190/196], loss=11.4050
	step [191/196], loss=13.8032
	step [192/196], loss=11.6100
	step [193/196], loss=12.4568
	step [194/196], loss=11.4622
	step [195/196], loss=11.7888
	step [196/196], loss=3.0181
	Evaluating
	loss=0.0510, precision=0.1556, recall=0.9960, f1=0.2691
Training epoch 14
	step [1/196], loss=11.0720
	step [2/196], loss=12.7117
	step [3/196], loss=13.8144
	step [4/196], loss=11.4459
	step [5/196], loss=12.2990
	step [6/196], loss=13.3415
	step [7/196], loss=11.2103
	step [8/196], loss=11.1214
	step [9/196], loss=11.2404
	step [10/196], loss=10.6152
	step [11/196], loss=10.6483
	step [12/196], loss=15.4116
	step [13/196], loss=10.9299
	step [14/196], loss=14.1953
	step [15/196], loss=11.5666
	step [16/196], loss=11.1557
	step [17/196], loss=10.4827
	step [18/196], loss=10.4884
	step [19/196], loss=12.0065
	step [20/196], loss=12.7624
	step [21/196], loss=12.4931
	step [22/196], loss=11.4899
	step [23/196], loss=10.9054
	step [24/196], loss=11.1773
	step [25/196], loss=11.1059
	step [26/196], loss=10.6562
	step [27/196], loss=12.5755
	step [28/196], loss=10.2007
	step [29/196], loss=11.9604
	step [30/196], loss=10.2428
	step [31/196], loss=9.9902
	step [32/196], loss=9.4566
	step [33/196], loss=12.1239
	step [34/196], loss=12.2236
	step [35/196], loss=10.8712
	step [36/196], loss=10.7507
	step [37/196], loss=9.9166
	step [38/196], loss=11.7825
	step [39/196], loss=11.2938
	step [40/196], loss=13.2805
	step [41/196], loss=10.7581
	step [42/196], loss=11.0184
	step [43/196], loss=12.4023
	step [44/196], loss=10.9087
	step [45/196], loss=11.9881
	step [46/196], loss=10.6958
	step [47/196], loss=13.6042
	step [48/196], loss=13.0761
	step [49/196], loss=11.5360
	step [50/196], loss=11.8490
	step [51/196], loss=10.6538
	step [52/196], loss=11.6900
	step [53/196], loss=13.5400
	step [54/196], loss=12.2206
	step [55/196], loss=10.6419
	step [56/196], loss=12.0863
	step [57/196], loss=11.6719
	step [58/196], loss=11.7890
	step [59/196], loss=11.5416
	step [60/196], loss=12.6307
	step [61/196], loss=11.2703
	step [62/196], loss=13.0360
	step [63/196], loss=12.9304
	step [64/196], loss=11.4968
	step [65/196], loss=10.8240
	step [66/196], loss=10.7426
	step [67/196], loss=10.9210
	step [68/196], loss=11.5835
	step [69/196], loss=11.6403
	step [70/196], loss=11.4888
	step [71/196], loss=11.5801
	step [72/196], loss=12.7687
	step [73/196], loss=11.9641
	step [74/196], loss=15.0792
	step [75/196], loss=11.2483
	step [76/196], loss=12.2918
	step [77/196], loss=11.4675
	step [78/196], loss=12.1370
	step [79/196], loss=11.4795
	step [80/196], loss=12.1694
	step [81/196], loss=10.9640
	step [82/196], loss=10.8228
	step [83/196], loss=11.1204
	step [84/196], loss=10.9757
	step [85/196], loss=10.9855
	step [86/196], loss=11.8497
	step [87/196], loss=10.7409
	step [88/196], loss=11.0700
	step [89/196], loss=11.1312
	step [90/196], loss=10.9137
	step [91/196], loss=11.4533
	step [92/196], loss=11.0352
	step [93/196], loss=10.7657
	step [94/196], loss=10.9184
	step [95/196], loss=12.0536
	step [96/196], loss=11.7501
	step [97/196], loss=10.1420
	step [98/196], loss=11.0579
	step [99/196], loss=11.5465
	step [100/196], loss=9.6632
	step [101/196], loss=12.0201
	step [102/196], loss=12.2556
	step [103/196], loss=10.7573
	step [104/196], loss=11.3785
	step [105/196], loss=11.0032
	step [106/196], loss=11.1513
	step [107/196], loss=11.3508
	step [108/196], loss=12.2010
	step [109/196], loss=12.0937
	step [110/196], loss=11.5023
	step [111/196], loss=11.8360
	step [112/196], loss=12.3561
	step [113/196], loss=10.1624
	step [114/196], loss=12.4987
	step [115/196], loss=10.9773
	step [116/196], loss=11.0004
	step [117/196], loss=12.1452
	step [118/196], loss=10.9614
	step [119/196], loss=11.2517
	step [120/196], loss=11.7008
	step [121/196], loss=11.2559
	step [122/196], loss=11.6436
	step [123/196], loss=12.4092
	step [124/196], loss=10.7299
	step [125/196], loss=13.9408
	step [126/196], loss=10.7545
	step [127/196], loss=11.7076
	step [128/196], loss=11.2974
	step [129/196], loss=10.1202
	step [130/196], loss=10.0800
	step [131/196], loss=11.3131
	step [132/196], loss=10.9093
	step [133/196], loss=11.6972
	step [134/196], loss=11.0072
	step [135/196], loss=11.4399
	step [136/196], loss=13.5499
	step [137/196], loss=11.4245
	step [138/196], loss=11.9637
	step [139/196], loss=10.9030
	step [140/196], loss=12.1602
	step [141/196], loss=11.6796
	step [142/196], loss=10.8716
	step [143/196], loss=9.9095
	step [144/196], loss=11.3926
	step [145/196], loss=12.8650
	step [146/196], loss=10.9688
	step [147/196], loss=10.4574
	step [148/196], loss=10.1464
	step [149/196], loss=11.1407
	step [150/196], loss=10.8631
	step [151/196], loss=9.9328
	step [152/196], loss=11.2605
	step [153/196], loss=10.7995
	step [154/196], loss=10.6451
	step [155/196], loss=10.2218
	step [156/196], loss=11.2665
	step [157/196], loss=9.0568
	step [158/196], loss=11.2822
	step [159/196], loss=9.8417
	step [160/196], loss=11.6570
	step [161/196], loss=12.1950
	step [162/196], loss=11.4048
	step [163/196], loss=10.6442
	step [164/196], loss=9.5602
	step [165/196], loss=12.6223
	step [166/196], loss=11.5719
	step [167/196], loss=11.1044
	step [168/196], loss=11.0916
	step [169/196], loss=10.8465
	step [170/196], loss=11.6828
	step [171/196], loss=11.2482
	step [172/196], loss=11.3416
	step [173/196], loss=11.3410
	step [174/196], loss=10.8949
	step [175/196], loss=11.3617
	step [176/196], loss=11.6159
	step [177/196], loss=11.1088
	step [178/196], loss=10.3629
	step [179/196], loss=10.2775
	step [180/196], loss=11.1530
	step [181/196], loss=11.4190
	step [182/196], loss=10.3671
	step [183/196], loss=11.4538
	step [184/196], loss=12.1570
	step [185/196], loss=12.1149
	step [186/196], loss=11.4047
	step [187/196], loss=11.1222
	step [188/196], loss=10.5719
	step [189/196], loss=10.6522
	step [190/196], loss=11.6671
	step [191/196], loss=12.4756
	step [192/196], loss=10.6473
	step [193/196], loss=10.8103
	step [194/196], loss=10.8725
	step [195/196], loss=9.8204
	step [196/196], loss=3.5562
	Evaluating
	loss=0.0514, precision=0.1473, recall=0.9970, f1=0.2566
Training epoch 15
	step [1/196], loss=10.8963
	step [2/196], loss=12.0476
	step [3/196], loss=11.3212
	step [4/196], loss=11.4451
	step [5/196], loss=10.3784
	step [6/196], loss=11.0768
	step [7/196], loss=12.4029
	step [8/196], loss=10.3931
	step [9/196], loss=11.4364
	step [10/196], loss=9.3205
	step [11/196], loss=10.5687
	step [12/196], loss=12.4036
	step [13/196], loss=11.7357
	step [14/196], loss=10.8299
	step [15/196], loss=11.3187
	step [16/196], loss=10.4170
	step [17/196], loss=10.0387
	step [18/196], loss=10.3941
	step [19/196], loss=12.1374
	step [20/196], loss=11.7976
	step [21/196], loss=10.0382
	step [22/196], loss=11.5238
	step [23/196], loss=10.0009
	step [24/196], loss=11.2714
	step [25/196], loss=10.2303
	step [26/196], loss=11.9830
	step [27/196], loss=10.4213
	step [28/196], loss=10.4801
	step [29/196], loss=12.4660
	step [30/196], loss=10.1355
	step [31/196], loss=12.5104
	step [32/196], loss=9.7314
	step [33/196], loss=9.2866
	step [34/196], loss=10.4638
	step [35/196], loss=10.6354
	step [36/196], loss=10.8123
	step [37/196], loss=10.3072
	step [38/196], loss=12.0858
	step [39/196], loss=10.2457
	step [40/196], loss=11.2988
	step [41/196], loss=11.3781
	step [42/196], loss=11.2726
	step [43/196], loss=10.3461
	step [44/196], loss=10.2844
	step [45/196], loss=11.1688
	step [46/196], loss=12.4425
	step [47/196], loss=12.4022
	step [48/196], loss=11.4782
	step [49/196], loss=8.5997
	step [50/196], loss=10.7237
	step [51/196], loss=11.5019
	step [52/196], loss=10.9471
	step [53/196], loss=10.5166
	step [54/196], loss=12.4676
	step [55/196], loss=11.0644
	step [56/196], loss=10.2857
	step [57/196], loss=11.1375
	step [58/196], loss=10.3402
	step [59/196], loss=10.2341
	step [60/196], loss=11.4350
	step [61/196], loss=10.3926
	step [62/196], loss=9.7304
	step [63/196], loss=10.5553
	step [64/196], loss=10.8933
	step [65/196], loss=9.5344
	step [66/196], loss=11.7434
	step [67/196], loss=11.5680
	step [68/196], loss=9.8147
	step [69/196], loss=10.7482
	step [70/196], loss=11.5039
	step [71/196], loss=11.1490
	step [72/196], loss=10.4637
	step [73/196], loss=11.5598
	step [74/196], loss=10.3988
	step [75/196], loss=11.4389
	step [76/196], loss=11.4025
	step [77/196], loss=10.9743
	step [78/196], loss=11.8028
	step [79/196], loss=10.4678
	step [80/196], loss=9.9804
	step [81/196], loss=10.6064
	step [82/196], loss=11.5154
	step [83/196], loss=10.2828
	step [84/196], loss=10.2842
	step [85/196], loss=10.0693
	step [86/196], loss=11.0955
	step [87/196], loss=11.4561
	step [88/196], loss=10.0151
	step [89/196], loss=9.5187
	step [90/196], loss=10.6699
	step [91/196], loss=10.3396
	step [92/196], loss=10.3664
	step [93/196], loss=12.4512
	step [94/196], loss=10.5075
	step [95/196], loss=9.1003
	step [96/196], loss=9.8506
	step [97/196], loss=11.1377
	step [98/196], loss=10.9711
	step [99/196], loss=10.3034
	step [100/196], loss=10.9446
	step [101/196], loss=10.2478
	step [102/196], loss=11.4265
	step [103/196], loss=10.5679
	step [104/196], loss=10.2000
	step [105/196], loss=10.9941
	step [106/196], loss=11.4876
	step [107/196], loss=11.3081
	step [108/196], loss=10.1338
	step [109/196], loss=10.4183
	step [110/196], loss=9.9303
	step [111/196], loss=10.3222
	step [112/196], loss=10.0644
	step [113/196], loss=10.9487
	step [114/196], loss=9.9782
	step [115/196], loss=11.2375
	step [116/196], loss=10.7459
	step [117/196], loss=11.0732
	step [118/196], loss=11.0087
	step [119/196], loss=12.0853
	step [120/196], loss=11.6445
	step [121/196], loss=9.4925
	step [122/196], loss=10.8451
	step [123/196], loss=10.6476
	step [124/196], loss=10.2887
	step [125/196], loss=12.2117
	step [126/196], loss=10.6902
	step [127/196], loss=10.3894
	step [128/196], loss=12.1887
	step [129/196], loss=9.8159
	step [130/196], loss=11.5767
	step [131/196], loss=9.3045
	step [132/196], loss=8.7106
	step [133/196], loss=10.2911
	step [134/196], loss=11.9713
	step [135/196], loss=11.7271
	step [136/196], loss=9.9451
	step [137/196], loss=10.3490
	step [138/196], loss=9.2924
	step [139/196], loss=10.0591
	step [140/196], loss=9.7321
	step [141/196], loss=9.4960
	step [142/196], loss=9.7204
	step [143/196], loss=10.6165
	step [144/196], loss=9.5033
	step [145/196], loss=10.3118
	step [146/196], loss=9.1534
	step [147/196], loss=9.9845
	step [148/196], loss=8.8733
	step [149/196], loss=11.2706
	step [150/196], loss=10.8344
	step [151/196], loss=10.1002
	step [152/196], loss=11.7473
	step [153/196], loss=9.6811
	step [154/196], loss=10.9835
	step [155/196], loss=11.5053
	step [156/196], loss=10.9995
	step [157/196], loss=10.2975
	step [158/196], loss=11.6425
	step [159/196], loss=9.5822
	step [160/196], loss=9.8336
	step [161/196], loss=10.7668
	step [162/196], loss=11.2180
	step [163/196], loss=10.4653
	step [164/196], loss=10.5245
	step [165/196], loss=10.9724
	step [166/196], loss=10.6408
	step [167/196], loss=9.1196
	step [168/196], loss=9.9782
	step [169/196], loss=9.7366
	step [170/196], loss=11.0385
	step [171/196], loss=10.8227
	step [172/196], loss=9.6434
	step [173/196], loss=10.6700
	step [174/196], loss=10.3584
	step [175/196], loss=12.0144
	step [176/196], loss=9.1769
	step [177/196], loss=9.5260
	step [178/196], loss=10.0252
	step [179/196], loss=11.2990
	step [180/196], loss=9.0664
	step [181/196], loss=9.9082
	step [182/196], loss=10.8409
	step [183/196], loss=10.7022
	step [184/196], loss=9.1757
	step [185/196], loss=9.8927
	step [186/196], loss=9.5820
	step [187/196], loss=9.1260
	step [188/196], loss=9.8212
	step [189/196], loss=9.8141
	step [190/196], loss=10.0772
	step [191/196], loss=10.9616
	step [192/196], loss=10.5963
	step [193/196], loss=10.8134
	step [194/196], loss=10.0869
	step [195/196], loss=10.4900
	step [196/196], loss=3.1671
	Evaluating
	loss=0.0448, precision=0.1588, recall=0.9965, f1=0.2739
Training epoch 16
	step [1/196], loss=9.8518
	step [2/196], loss=12.0107
	step [3/196], loss=9.0191
	step [4/196], loss=11.9462
	step [5/196], loss=9.9798
	step [6/196], loss=9.8251
	step [7/196], loss=9.8464
	step [8/196], loss=9.3396
	step [9/196], loss=9.5725
	step [10/196], loss=12.5641
	step [11/196], loss=11.0884
	step [12/196], loss=10.2774
	step [13/196], loss=11.2143
	step [14/196], loss=10.7868
	step [15/196], loss=9.6729
	step [16/196], loss=8.9725
	step [17/196], loss=10.3665
	step [18/196], loss=9.1871
	step [19/196], loss=10.8216
	step [20/196], loss=10.2808
	step [21/196], loss=12.4512
	step [22/196], loss=10.5436
	step [23/196], loss=9.2852
	step [24/196], loss=10.6943
	step [25/196], loss=10.6877
	step [26/196], loss=9.0930
	step [27/196], loss=10.9944
	step [28/196], loss=9.5893
	step [29/196], loss=10.0714
	step [30/196], loss=9.1596
	step [31/196], loss=14.0842
	step [32/196], loss=9.3627
	step [33/196], loss=9.4996
	step [34/196], loss=9.9174
	step [35/196], loss=10.5639
	step [36/196], loss=11.5196
	step [37/196], loss=10.6497
	step [38/196], loss=10.7063
	step [39/196], loss=10.3489
	step [40/196], loss=12.8987
	step [41/196], loss=10.7069
	step [42/196], loss=9.5970
	step [43/196], loss=9.2277
	step [44/196], loss=10.5788
	step [45/196], loss=9.8084
	step [46/196], loss=10.0441
	step [47/196], loss=10.6560
	step [48/196], loss=11.1893
	step [49/196], loss=11.0025
	step [50/196], loss=12.2354
	step [51/196], loss=9.7686
	step [52/196], loss=11.9157
	step [53/196], loss=12.1844
	step [54/196], loss=10.4596
	step [55/196], loss=10.8814
	step [56/196], loss=10.3272
	step [57/196], loss=10.2911
	step [58/196], loss=10.6754
	step [59/196], loss=10.8304
	step [60/196], loss=10.2683
	step [61/196], loss=9.7933
	step [62/196], loss=10.2531
	step [63/196], loss=9.9963
	step [64/196], loss=12.8929
	step [65/196], loss=11.2981
	step [66/196], loss=8.7332
	step [67/196], loss=9.8671
	step [68/196], loss=10.4857
	step [69/196], loss=11.6156
	step [70/196], loss=9.4231
	step [71/196], loss=9.7004
	step [72/196], loss=11.5056
	step [73/196], loss=10.5805
	step [74/196], loss=10.0386
	step [75/196], loss=10.3142
	step [76/196], loss=11.6375
	step [77/196], loss=9.6428
	step [78/196], loss=12.3545
	step [79/196], loss=10.8588
	step [80/196], loss=11.1389
	step [81/196], loss=10.0598
	step [82/196], loss=10.0945
	step [83/196], loss=9.2927
	step [84/196], loss=10.0063
	step [85/196], loss=10.0279
	step [86/196], loss=9.2442
	step [87/196], loss=9.5998
	step [88/196], loss=9.1495
	step [89/196], loss=10.3330
	step [90/196], loss=11.2298
	step [91/196], loss=7.8996
	step [92/196], loss=9.7596
	step [93/196], loss=8.8996
	step [94/196], loss=9.6039
	step [95/196], loss=10.2568
	step [96/196], loss=9.8804
	step [97/196], loss=10.8460
	step [98/196], loss=10.0528
	step [99/196], loss=9.5392
	step [100/196], loss=11.4541
	step [101/196], loss=9.0931
	step [102/196], loss=10.1888
	step [103/196], loss=11.0669
	step [104/196], loss=9.5345
	step [105/196], loss=9.7560
	step [106/196], loss=9.6010
	step [107/196], loss=10.1152
	step [108/196], loss=9.5601
	step [109/196], loss=9.6349
	step [110/196], loss=10.2507
	step [111/196], loss=8.8136
	step [112/196], loss=10.6309
	step [113/196], loss=9.7424
	step [114/196], loss=9.4931
	step [115/196], loss=9.6379
	step [116/196], loss=8.6807
	step [117/196], loss=9.9794
	step [118/196], loss=10.6092
	step [119/196], loss=10.8283
	step [120/196], loss=10.8907
	step [121/196], loss=8.6782
	step [122/196], loss=9.4986
	step [123/196], loss=10.3959
	step [124/196], loss=11.6551
	step [125/196], loss=11.2284
	step [126/196], loss=9.5828
	step [127/196], loss=9.3549
	step [128/196], loss=8.9552
	step [129/196], loss=8.1484
	step [130/196], loss=9.9356
	step [131/196], loss=10.6442
	step [132/196], loss=9.7204
	step [133/196], loss=12.4540
	step [134/196], loss=9.2804
	step [135/196], loss=8.3307
	step [136/196], loss=10.4229
	step [137/196], loss=9.2279
	step [138/196], loss=11.2078
	step [139/196], loss=9.4228
	step [140/196], loss=10.0126
	step [141/196], loss=9.9677
	step [142/196], loss=9.4472
	step [143/196], loss=9.3834
	step [144/196], loss=9.5234
	step [145/196], loss=8.8477
	step [146/196], loss=9.3997
	step [147/196], loss=9.9843
	step [148/196], loss=9.0736
	step [149/196], loss=10.7146
	step [150/196], loss=10.3239
	step [151/196], loss=9.0469
	step [152/196], loss=11.2913
	step [153/196], loss=11.2579
	step [154/196], loss=10.2673
	step [155/196], loss=10.0514
	step [156/196], loss=9.2551
	step [157/196], loss=9.2152
	step [158/196], loss=9.5723
	step [159/196], loss=9.7137
	step [160/196], loss=9.3358
	step [161/196], loss=10.7274
	step [162/196], loss=9.2803
	step [163/196], loss=9.3827
	step [164/196], loss=8.4487
	step [165/196], loss=9.4785
	step [166/196], loss=10.0987
	step [167/196], loss=10.4139
	step [168/196], loss=10.8752
	step [169/196], loss=10.0102
	step [170/196], loss=10.4262
	step [171/196], loss=9.5529
	step [172/196], loss=10.9954
	step [173/196], loss=10.2330
	step [174/196], loss=9.2976
	step [175/196], loss=10.9346
	step [176/196], loss=10.1768
	step [177/196], loss=8.9468
	step [178/196], loss=10.2927
	step [179/196], loss=9.0031
	step [180/196], loss=10.1332
	step [181/196], loss=10.5089
	step [182/196], loss=11.0895
	step [183/196], loss=10.7472
	step [184/196], loss=10.1983
	step [185/196], loss=9.2574
	step [186/196], loss=9.6347
	step [187/196], loss=8.5676
	step [188/196], loss=8.3570
	step [189/196], loss=9.9025
	step [190/196], loss=12.3375
	step [191/196], loss=10.9658
	step [192/196], loss=8.7015
	step [193/196], loss=11.4204
	step [194/196], loss=9.1638
	step [195/196], loss=10.5005
	step [196/196], loss=2.4347
	Evaluating
	loss=0.0412, precision=0.1864, recall=0.9956, f1=0.3140
Training epoch 17
	step [1/196], loss=8.8698
	step [2/196], loss=10.3513
	step [3/196], loss=10.8539
	step [4/196], loss=9.9938
	step [5/196], loss=9.8112
	step [6/196], loss=8.8161
	step [7/196], loss=9.5312
	step [8/196], loss=9.4845
	step [9/196], loss=8.4184
	step [10/196], loss=10.3769
	step [11/196], loss=8.8918
	step [12/196], loss=8.3235
	step [13/196], loss=9.7467
	step [14/196], loss=9.4872
	step [15/196], loss=9.7097
	step [16/196], loss=8.3931
	step [17/196], loss=9.1852
	step [18/196], loss=9.4694
	step [19/196], loss=9.6018
	step [20/196], loss=8.4945
	step [21/196], loss=8.2858
	step [22/196], loss=9.4178
	step [23/196], loss=10.9463
	step [24/196], loss=9.9436
	step [25/196], loss=10.0830
	step [26/196], loss=9.6912
	step [27/196], loss=10.5148
	step [28/196], loss=8.9334
	step [29/196], loss=9.2240
	step [30/196], loss=9.0183
	step [31/196], loss=10.0276
	step [32/196], loss=9.0535
	step [33/196], loss=9.2128
	step [34/196], loss=10.1565
	step [35/196], loss=8.9235
	step [36/196], loss=11.7190
	step [37/196], loss=11.2209
	step [38/196], loss=10.4385
	step [39/196], loss=9.1288
	step [40/196], loss=9.1028
	step [41/196], loss=9.6593
	step [42/196], loss=9.9132
	step [43/196], loss=8.9004
	step [44/196], loss=9.2628
	step [45/196], loss=10.2506
	step [46/196], loss=11.0425
	step [47/196], loss=9.5031
	step [48/196], loss=8.6969
	step [49/196], loss=9.1595
	step [50/196], loss=8.9334
	step [51/196], loss=9.6954
	step [52/196], loss=10.1197
	step [53/196], loss=10.7421
	step [54/196], loss=9.3690
	step [55/196], loss=9.1028
	step [56/196], loss=10.5674
	step [57/196], loss=8.4826
	step [58/196], loss=9.8584
	step [59/196], loss=9.7331
	step [60/196], loss=8.7961
	step [61/196], loss=9.9075
	step [62/196], loss=10.3198
	step [63/196], loss=8.8341
	step [64/196], loss=9.3761
	step [65/196], loss=9.1352
	step [66/196], loss=9.5533
	step [67/196], loss=9.4663
	step [68/196], loss=9.3568
	step [69/196], loss=8.6703
	step [70/196], loss=9.7110
	step [71/196], loss=8.3222
	step [72/196], loss=8.0300
	step [73/196], loss=11.0671
	step [74/196], loss=10.9990
	step [75/196], loss=9.2439
	step [76/196], loss=9.7005
	step [77/196], loss=9.7547
	step [78/196], loss=11.8179
	step [79/196], loss=10.3448
	step [80/196], loss=9.3466
	step [81/196], loss=8.7159
	step [82/196], loss=10.4533
	step [83/196], loss=9.5875
	step [84/196], loss=10.4622
	step [85/196], loss=9.4295
	step [86/196], loss=9.9995
	step [87/196], loss=9.1319
	step [88/196], loss=10.1760
	step [89/196], loss=8.0808
	step [90/196], loss=10.8971
	step [91/196], loss=9.3617
	step [92/196], loss=9.3185
	step [93/196], loss=8.9310
	step [94/196], loss=8.3258
	step [95/196], loss=10.6506
	step [96/196], loss=8.5642
	step [97/196], loss=9.3727
	step [98/196], loss=8.8721
	step [99/196], loss=11.3386
	step [100/196], loss=9.2704
	step [101/196], loss=11.0680
	step [102/196], loss=9.0129
	step [103/196], loss=9.2898
	step [104/196], loss=9.5518
	step [105/196], loss=10.2019
	step [106/196], loss=8.8259
	step [107/196], loss=10.9857
	step [108/196], loss=9.6719
	step [109/196], loss=10.0693
	step [110/196], loss=9.0811
	step [111/196], loss=10.0766
	step [112/196], loss=11.3097
	step [113/196], loss=9.5825
	step [114/196], loss=10.1598
	step [115/196], loss=9.5514
	step [116/196], loss=8.2257
	step [117/196], loss=8.1342
	step [118/196], loss=9.4074
	step [119/196], loss=10.6953
	step [120/196], loss=11.7609
	step [121/196], loss=10.1934
	step [122/196], loss=10.5476
	step [123/196], loss=8.5678
	step [124/196], loss=9.6092
	step [125/196], loss=9.2823
	step [126/196], loss=9.7335
	step [127/196], loss=8.3909
	step [128/196], loss=10.8156
	step [129/196], loss=8.2526
	step [130/196], loss=9.2512
	step [131/196], loss=10.1439
	step [132/196], loss=8.3900
	step [133/196], loss=9.7114
	step [134/196], loss=8.6552
	step [135/196], loss=11.7134
	step [136/196], loss=10.2199
	step [137/196], loss=9.9163
	step [138/196], loss=10.8617
	step [139/196], loss=8.6466
	step [140/196], loss=12.0936
	step [141/196], loss=8.5079
	step [142/196], loss=8.0305
	step [143/196], loss=8.8414
	step [144/196], loss=8.5996
	step [145/196], loss=9.6755
	step [146/196], loss=9.3479
	step [147/196], loss=8.4150
	step [148/196], loss=9.3702
	step [149/196], loss=9.1153
	step [150/196], loss=7.7419
	step [151/196], loss=10.4138
	step [152/196], loss=9.2280
	step [153/196], loss=9.6766
	step [154/196], loss=10.3392
	step [155/196], loss=9.2859
	step [156/196], loss=10.2027
	step [157/196], loss=9.2897
	step [158/196], loss=9.3039
	step [159/196], loss=8.6597
	step [160/196], loss=8.8347
	step [161/196], loss=8.2365
	step [162/196], loss=11.2984
	step [163/196], loss=10.0818
	step [164/196], loss=9.8493
	step [165/196], loss=9.9847
	step [166/196], loss=9.5960
	step [167/196], loss=8.2110
	step [168/196], loss=8.9543
	step [169/196], loss=8.8905
	step [170/196], loss=10.5766
	step [171/196], loss=10.1071
	step [172/196], loss=8.6172
	step [173/196], loss=8.8244
	step [174/196], loss=8.6838
	step [175/196], loss=10.2538
	step [176/196], loss=8.8252
	step [177/196], loss=10.7725
	step [178/196], loss=8.7831
	step [179/196], loss=9.2457
	step [180/196], loss=9.1120
	step [181/196], loss=9.2732
	step [182/196], loss=10.8989
	step [183/196], loss=10.0673
	step [184/196], loss=9.5684
	step [185/196], loss=8.3477
	step [186/196], loss=7.8188
	step [187/196], loss=9.0575
	step [188/196], loss=8.9797
	step [189/196], loss=8.7690
	step [190/196], loss=8.6987
	step [191/196], loss=8.6164
	step [192/196], loss=7.9475
	step [193/196], loss=9.8715
	step [194/196], loss=9.9512
	step [195/196], loss=9.4254
	step [196/196], loss=3.0890
	Evaluating
	loss=0.0391, precision=0.1918, recall=0.9957, f1=0.3217
Training epoch 18
	step [1/196], loss=9.8876
	step [2/196], loss=8.1583
	step [3/196], loss=9.7142
	step [4/196], loss=9.6980
	step [5/196], loss=8.7836
	step [6/196], loss=8.9486
	step [7/196], loss=8.6476
	step [8/196], loss=9.3108
	step [9/196], loss=9.1733
	step [10/196], loss=9.2005
	step [11/196], loss=10.7076
	step [12/196], loss=9.7712
	step [13/196], loss=8.1047
	step [14/196], loss=9.3425
	step [15/196], loss=8.5830
	step [16/196], loss=9.0150
	step [17/196], loss=8.5335
	step [18/196], loss=10.2500
	step [19/196], loss=9.0935
	step [20/196], loss=8.9455
	step [21/196], loss=8.5817
	step [22/196], loss=10.3953
	step [23/196], loss=9.4899
	step [24/196], loss=9.4208
	step [25/196], loss=9.5180
	step [26/196], loss=9.1637
	step [27/196], loss=9.4977
	step [28/196], loss=8.4284
	step [29/196], loss=9.6062
	step [30/196], loss=9.3620
	step [31/196], loss=10.2068
	step [32/196], loss=8.5044
	step [33/196], loss=7.8820
	step [34/196], loss=8.6057
	step [35/196], loss=9.1547
	step [36/196], loss=8.1817
	step [37/196], loss=8.1242
	step [38/196], loss=9.3921
	step [39/196], loss=9.2590
	step [40/196], loss=8.3841
	step [41/196], loss=8.4063
	step [42/196], loss=10.4580
	step [43/196], loss=8.7189
	step [44/196], loss=9.1234
	step [45/196], loss=9.7044
	step [46/196], loss=8.0537
	step [47/196], loss=8.4050
	step [48/196], loss=8.9769
	step [49/196], loss=7.9700
	step [50/196], loss=8.8181
	step [51/196], loss=9.3720
	step [52/196], loss=9.6600
	step [53/196], loss=7.9266
	step [54/196], loss=9.2126
	step [55/196], loss=9.9965
	step [56/196], loss=8.8155
	step [57/196], loss=8.8948
	step [58/196], loss=9.6554
	step [59/196], loss=10.0193
	step [60/196], loss=9.2417
	step [61/196], loss=8.1838
	step [62/196], loss=8.5101
	step [63/196], loss=9.7326
	step [64/196], loss=7.4125
	step [65/196], loss=8.7929
	step [66/196], loss=9.2524
	step [67/196], loss=9.9640
	step [68/196], loss=8.3424
	step [69/196], loss=8.7933
	step [70/196], loss=8.2666
	step [71/196], loss=8.9006
	step [72/196], loss=9.9793
	step [73/196], loss=9.1857
	step [74/196], loss=8.8363
	step [75/196], loss=9.0117
	step [76/196], loss=8.3541
	step [77/196], loss=10.4257
	step [78/196], loss=8.2478
	step [79/196], loss=8.6428
	step [80/196], loss=9.2278
	step [81/196], loss=11.6645
	step [82/196], loss=8.8196
	step [83/196], loss=9.8513
	step [84/196], loss=8.9301
	step [85/196], loss=9.3804
	step [86/196], loss=8.6738
	step [87/196], loss=9.3726
	step [88/196], loss=8.3950
	step [89/196], loss=8.8897
	step [90/196], loss=10.4488
	step [91/196], loss=8.6105
	step [92/196], loss=9.5442
	step [93/196], loss=7.7735
	step [94/196], loss=9.1980
	step [95/196], loss=11.0947
	step [96/196], loss=8.7520
	step [97/196], loss=8.2891
	step [98/196], loss=9.7720
	step [99/196], loss=8.6603
	step [100/196], loss=9.2292
	step [101/196], loss=9.5287
	step [102/196], loss=8.7348
	step [103/196], loss=10.4901
	step [104/196], loss=8.3907
	step [105/196], loss=10.6008
	step [106/196], loss=9.1905
	step [107/196], loss=9.6974
	step [108/196], loss=8.1134
	step [109/196], loss=8.8192
	step [110/196], loss=7.4346
	step [111/196], loss=8.9813
	step [112/196], loss=8.4312
	step [113/196], loss=9.1317
	step [114/196], loss=8.1535
	step [115/196], loss=7.8290
	step [116/196], loss=9.9282
	step [117/196], loss=8.2731
	step [118/196], loss=10.7546
	step [119/196], loss=9.5555
	step [120/196], loss=8.2376
	step [121/196], loss=8.8742
	step [122/196], loss=8.8862
	step [123/196], loss=9.8555
	step [124/196], loss=9.3018
	step [125/196], loss=10.3895
	step [126/196], loss=8.7741
	step [127/196], loss=9.6373
	step [128/196], loss=8.6976
	step [129/196], loss=9.0538
	step [130/196], loss=8.5612
	step [131/196], loss=7.6548
	step [132/196], loss=7.6804
	step [133/196], loss=10.0610
	step [134/196], loss=9.2870
	step [135/196], loss=7.9424
	step [136/196], loss=10.7501
	step [137/196], loss=9.6806
	step [138/196], loss=8.7620
	step [139/196], loss=9.9573
	step [140/196], loss=8.6143
	step [141/196], loss=7.8910
	step [142/196], loss=7.1253
	step [143/196], loss=6.7467
	step [144/196], loss=8.7320
	step [145/196], loss=9.3249
	step [146/196], loss=8.7609
	step [147/196], loss=11.3803
	step [148/196], loss=8.1606
	step [149/196], loss=9.2104
	step [150/196], loss=8.8716
	step [151/196], loss=8.1156
	step [152/196], loss=9.6088
	step [153/196], loss=9.6914
	step [154/196], loss=9.5048
	step [155/196], loss=8.0715
	step [156/196], loss=10.0723
	step [157/196], loss=10.2779
	step [158/196], loss=10.2783
	step [159/196], loss=8.9594
	step [160/196], loss=8.8168
	step [161/196], loss=8.7219
	step [162/196], loss=8.8388
	step [163/196], loss=8.5079
	step [164/196], loss=11.5680
	step [165/196], loss=8.5338
	step [166/196], loss=10.0768
	step [167/196], loss=7.1808
	step [168/196], loss=8.7509
	step [169/196], loss=8.6141
	step [170/196], loss=9.5020
	step [171/196], loss=9.6129
	step [172/196], loss=7.9527
	step [173/196], loss=8.6321
	step [174/196], loss=8.2163
	step [175/196], loss=9.3115
	step [176/196], loss=8.2877
	step [177/196], loss=8.8913
	step [178/196], loss=8.7659
	step [179/196], loss=8.9761
	step [180/196], loss=8.8835
	step [181/196], loss=8.2468
	step [182/196], loss=9.5906
	step [183/196], loss=7.5758
	step [184/196], loss=8.3567
	step [185/196], loss=8.8814
	step [186/196], loss=9.7116
	step [187/196], loss=9.3142
	step [188/196], loss=8.5059
	step [189/196], loss=10.3289
	step [190/196], loss=8.8266
	step [191/196], loss=8.8303
	step [192/196], loss=7.7796
	step [193/196], loss=9.6832
	step [194/196], loss=7.3703
	step [195/196], loss=8.3296
	step [196/196], loss=2.6192
	Evaluating
	loss=0.0317, precision=0.2158, recall=0.9944, f1=0.3546
saving model as: 0_saved_model.pth
Training epoch 19
	step [1/196], loss=8.3166
	step [2/196], loss=7.3237
	step [3/196], loss=9.2545
	step [4/196], loss=7.8185
	step [5/196], loss=9.5896
	step [6/196], loss=10.2286
	step [7/196], loss=9.4533
	step [8/196], loss=9.2356
	step [9/196], loss=8.8219
	step [10/196], loss=7.8839
	step [11/196], loss=8.8965
	step [12/196], loss=9.9256
	step [13/196], loss=9.0471
	step [14/196], loss=8.1498
	step [15/196], loss=8.6020
	step [16/196], loss=9.0968
	step [17/196], loss=8.3485
	step [18/196], loss=7.5176
	step [19/196], loss=8.2471
	step [20/196], loss=8.6332
	step [21/196], loss=9.1956
	step [22/196], loss=7.6443
	step [23/196], loss=11.2761
	step [24/196], loss=8.1589
	step [25/196], loss=7.8610
	step [26/196], loss=10.0206
	step [27/196], loss=8.6436
	step [28/196], loss=9.0977
	step [29/196], loss=9.2361
	step [30/196], loss=8.8026
	step [31/196], loss=7.9947
	step [32/196], loss=8.5260
	step [33/196], loss=9.5638
	step [34/196], loss=8.7798
	step [35/196], loss=8.6503
	step [36/196], loss=8.0007
	step [37/196], loss=8.6615
	step [38/196], loss=9.0606
	step [39/196], loss=9.4541
	step [40/196], loss=7.5117
	step [41/196], loss=8.4501
	step [42/196], loss=10.2871
	step [43/196], loss=8.5664
	step [44/196], loss=8.2419
	step [45/196], loss=8.3293
	step [46/196], loss=7.6830
	step [47/196], loss=9.1979
	step [48/196], loss=8.6845
	step [49/196], loss=7.6719
	step [50/196], loss=9.1875
	step [51/196], loss=8.1202
	step [52/196], loss=7.5562
	step [53/196], loss=9.7353
	step [54/196], loss=8.4216
	step [55/196], loss=9.1039
	step [56/196], loss=8.2308
	step [57/196], loss=10.5076
	step [58/196], loss=8.1086
	step [59/196], loss=8.1656
	step [60/196], loss=9.8027
	step [61/196], loss=8.4529
	step [62/196], loss=8.2213
	step [63/196], loss=8.4421
	step [64/196], loss=8.4760
	step [65/196], loss=9.9232
	step [66/196], loss=8.3390
	step [67/196], loss=9.1699
	step [68/196], loss=9.3337
	step [69/196], loss=8.1497
	step [70/196], loss=8.4174
	step [71/196], loss=9.2785
	step [72/196], loss=8.8299
	step [73/196], loss=8.8310
	step [74/196], loss=9.5704
	step [75/196], loss=7.9853
	step [76/196], loss=8.5915
	step [77/196], loss=9.4392
	step [78/196], loss=8.6693
	step [79/196], loss=8.5104
	step [80/196], loss=8.7629
	step [81/196], loss=8.4398
	step [82/196], loss=8.9335
	step [83/196], loss=8.4768
	step [84/196], loss=9.2415
	step [85/196], loss=9.2567
	step [86/196], loss=8.8336
	step [87/196], loss=8.1352
	step [88/196], loss=8.5258
	step [89/196], loss=9.1427
	step [90/196], loss=7.0873
	step [91/196], loss=8.2110
	step [92/196], loss=9.5429
	step [93/196], loss=8.5467
	step [94/196], loss=8.9658
	step [95/196], loss=8.6985
	step [96/196], loss=8.3355
	step [97/196], loss=9.1756
	step [98/196], loss=9.6553
	step [99/196], loss=8.0087
	step [100/196], loss=8.3431
	step [101/196], loss=8.1122
	step [102/196], loss=7.7159
	step [103/196], loss=7.8777
	step [104/196], loss=7.7261
	step [105/196], loss=8.0050
	step [106/196], loss=7.8339
	step [107/196], loss=8.4786
	step [108/196], loss=8.7834
	step [109/196], loss=8.1320
	step [110/196], loss=8.0251
	step [111/196], loss=9.9528
	step [112/196], loss=7.2389
	step [113/196], loss=7.5939
	step [114/196], loss=8.5908
	step [115/196], loss=8.3344
	step [116/196], loss=8.8539
	step [117/196], loss=9.2238
	step [118/196], loss=8.7116
	step [119/196], loss=8.6265
	step [120/196], loss=8.0129
	step [121/196], loss=8.0536
	step [122/196], loss=9.0626
	step [123/196], loss=10.1362
	step [124/196], loss=8.3585
	step [125/196], loss=9.3540
	step [126/196], loss=8.2582
	step [127/196], loss=8.5790
	step [128/196], loss=8.7083
	step [129/196], loss=8.3563
	step [130/196], loss=8.6353
	step [131/196], loss=8.0914
	step [132/196], loss=9.6189
	step [133/196], loss=7.4075
	step [134/196], loss=8.9414
	step [135/196], loss=9.9747
	step [136/196], loss=8.7747
	step [137/196], loss=8.9717
	step [138/196], loss=7.8000
	step [139/196], loss=7.8255
	step [140/196], loss=7.6919
	step [141/196], loss=8.0047
	step [142/196], loss=9.1062
	step [143/196], loss=8.7208
	step [144/196], loss=8.0723
	step [145/196], loss=8.2228
	step [146/196], loss=8.8930
	step [147/196], loss=9.9968
	step [148/196], loss=10.2745
	step [149/196], loss=7.6958
	step [150/196], loss=8.9198
	step [151/196], loss=10.0371
	step [152/196], loss=8.8331
	step [153/196], loss=7.8027
	step [154/196], loss=9.3191
	step [155/196], loss=7.6432
	step [156/196], loss=7.8725
	step [157/196], loss=7.5692
	step [158/196], loss=7.7405
	step [159/196], loss=10.5158
	step [160/196], loss=9.5712
	step [161/196], loss=7.8140
	step [162/196], loss=8.6182
	step [163/196], loss=8.6187
	step [164/196], loss=8.3350
	step [165/196], loss=8.8344
	step [166/196], loss=7.7231
	step [167/196], loss=8.0631
	step [168/196], loss=8.7213
	step [169/196], loss=8.2863
	step [170/196], loss=8.5383
	step [171/196], loss=8.2384
	step [172/196], loss=8.6113
	step [173/196], loss=8.8625
	step [174/196], loss=9.1082
	step [175/196], loss=8.8744
	step [176/196], loss=8.2939
	step [177/196], loss=7.8857
	step [178/196], loss=8.5628
	step [179/196], loss=8.8908
	step [180/196], loss=8.2247
	step [181/196], loss=7.7620
	step [182/196], loss=7.3717
	step [183/196], loss=9.8079
	step [184/196], loss=8.9248
	step [185/196], loss=7.4484
	step [186/196], loss=9.1445
	step [187/196], loss=9.5501
	step [188/196], loss=8.4489
	step [189/196], loss=7.7922
	step [190/196], loss=8.3726
	step [191/196], loss=8.8177
	step [192/196], loss=8.6669
	step [193/196], loss=8.6143
	step [194/196], loss=8.1542
	step [195/196], loss=7.2215
	step [196/196], loss=2.5253
	Evaluating
	loss=0.0327, precision=0.2065, recall=0.9929, f1=0.3419
Training epoch 20
	step [1/196], loss=9.2631
	step [2/196], loss=9.1235
	step [3/196], loss=9.6580
	step [4/196], loss=7.9189
	step [5/196], loss=7.6801
	step [6/196], loss=8.7529
	step [7/196], loss=8.3495
	step [8/196], loss=9.2163
	step [9/196], loss=8.0294
	step [10/196], loss=8.0465
	step [11/196], loss=8.2588
	step [12/196], loss=8.1118
	step [13/196], loss=8.4414
	step [14/196], loss=8.3481
	step [15/196], loss=8.6113
	step [16/196], loss=7.7159
	step [17/196], loss=7.9484
	step [18/196], loss=7.8034
	step [19/196], loss=9.3006
	step [20/196], loss=10.1477
	step [21/196], loss=7.5588
	step [22/196], loss=7.9779
	step [23/196], loss=8.0656
	step [24/196], loss=10.1270
	step [25/196], loss=8.1508
	step [26/196], loss=8.9073
	step [27/196], loss=7.5631
	step [28/196], loss=7.0286
	step [29/196], loss=8.6695
	step [30/196], loss=8.9437
	step [31/196], loss=8.1581
	step [32/196], loss=8.1779
	step [33/196], loss=8.6975
	step [34/196], loss=8.7146
	step [35/196], loss=7.8670
	step [36/196], loss=7.4487
	step [37/196], loss=7.3463
	step [38/196], loss=8.5720
	step [39/196], loss=8.1664
	step [40/196], loss=7.3349
	step [41/196], loss=7.6233
	step [42/196], loss=7.4205
	step [43/196], loss=8.2866
	step [44/196], loss=8.6567
	step [45/196], loss=8.9973
	step [46/196], loss=9.0486
	step [47/196], loss=7.4517
	step [48/196], loss=7.8422
	step [49/196], loss=7.8749
	step [50/196], loss=9.4520
	step [51/196], loss=7.8626
	step [52/196], loss=8.0620
	step [53/196], loss=7.0651
	step [54/196], loss=8.2561
	step [55/196], loss=9.3280
	step [56/196], loss=8.1259
	step [57/196], loss=9.2590
	step [58/196], loss=7.4224
	step [59/196], loss=8.1582
	step [60/196], loss=9.4174
	step [61/196], loss=8.6834
	step [62/196], loss=8.3241
	step [63/196], loss=7.8544
	step [64/196], loss=8.6903
	step [65/196], loss=8.0161
	step [66/196], loss=7.6632
	step [67/196], loss=7.6314
	step [68/196], loss=8.5721
	step [69/196], loss=8.9740
	step [70/196], loss=7.7695
	step [71/196], loss=7.8569
	step [72/196], loss=8.0075
	step [73/196], loss=8.0249
	step [74/196], loss=9.2113
	step [75/196], loss=8.5659
	step [76/196], loss=8.8121
	step [77/196], loss=8.7755
	step [78/196], loss=8.8089
	step [79/196], loss=7.8843
	step [80/196], loss=9.4304
	step [81/196], loss=7.4644
	step [82/196], loss=7.6342
	step [83/196], loss=8.0186
	step [84/196], loss=8.2289
	step [85/196], loss=9.1817
	step [86/196], loss=8.7088
	step [87/196], loss=11.6533
	step [88/196], loss=8.7605
	step [89/196], loss=6.5653
	step [90/196], loss=7.7852
	step [91/196], loss=7.5516
	step [92/196], loss=8.9430
	step [93/196], loss=9.6565
	step [94/196], loss=7.0182
	step [95/196], loss=6.9153
	step [96/196], loss=7.9434
	step [97/196], loss=9.6872
	step [98/196], loss=8.3057
	step [99/196], loss=9.4402
	step [100/196], loss=8.2367
	step [101/196], loss=8.7156
	step [102/196], loss=8.5590
	step [103/196], loss=7.4484
	step [104/196], loss=8.1973
	step [105/196], loss=8.4830
	step [106/196], loss=8.0120
	step [107/196], loss=7.7093
	step [108/196], loss=9.4908
	step [109/196], loss=8.2827
	step [110/196], loss=10.1520
	step [111/196], loss=7.6381
	step [112/196], loss=8.4768
	step [113/196], loss=8.2536
	step [114/196], loss=7.2983
	step [115/196], loss=8.4494
	step [116/196], loss=8.2776
	step [117/196], loss=6.9324
	step [118/196], loss=7.7926
	step [119/196], loss=7.4699
	step [120/196], loss=8.3041
	step [121/196], loss=9.0449
	step [122/196], loss=8.0848
	step [123/196], loss=8.4538
	step [124/196], loss=8.7756
	step [125/196], loss=7.7068
	step [126/196], loss=8.7524
	step [127/196], loss=7.9466
	step [128/196], loss=8.5526
	step [129/196], loss=7.7210
	step [130/196], loss=8.6103
	step [131/196], loss=7.3288
	step [132/196], loss=7.0039
	step [133/196], loss=9.3620
	step [134/196], loss=7.1029
	step [135/196], loss=7.0296
	step [136/196], loss=8.2871
	step [137/196], loss=7.9349
	step [138/196], loss=7.4943
	step [139/196], loss=7.8375
	step [140/196], loss=8.4965
	step [141/196], loss=7.6775
	step [142/196], loss=7.7640
	step [143/196], loss=7.8257
	step [144/196], loss=8.8344
	step [145/196], loss=9.3957
	step [146/196], loss=8.2579
	step [147/196], loss=7.8774
	step [148/196], loss=7.4271
	step [149/196], loss=8.1444
	step [150/196], loss=6.7997
	step [151/196], loss=8.8135
	step [152/196], loss=8.2369
	step [153/196], loss=6.7572
	step [154/196], loss=7.6662
	step [155/196], loss=7.3790
	step [156/196], loss=8.3119
	step [157/196], loss=8.5265
	step [158/196], loss=9.9677
	step [159/196], loss=7.3497
	step [160/196], loss=8.4241
	step [161/196], loss=9.5702
	step [162/196], loss=7.1296
	step [163/196], loss=6.9821
	step [164/196], loss=8.2072
	step [165/196], loss=8.4006
	step [166/196], loss=7.1614
	step [167/196], loss=8.4278
	step [168/196], loss=6.9696
	step [169/196], loss=7.8758
	step [170/196], loss=7.6069
	step [171/196], loss=7.7289
	step [172/196], loss=8.3911
	step [173/196], loss=8.8962
	step [174/196], loss=7.2874
	step [175/196], loss=8.5797
	step [176/196], loss=8.1838
	step [177/196], loss=7.3695
	step [178/196], loss=8.6368
	step [179/196], loss=8.7141
	step [180/196], loss=9.2832
	step [181/196], loss=6.9435
	step [182/196], loss=9.9911
	step [183/196], loss=8.3258
	step [184/196], loss=9.3696
	step [185/196], loss=8.5512
	step [186/196], loss=7.2730
	step [187/196], loss=7.3602
	step [188/196], loss=7.8174
	step [189/196], loss=7.8327
	step [190/196], loss=8.7438
	step [191/196], loss=8.0220
	step [192/196], loss=8.3161
	step [193/196], loss=7.9848
	step [194/196], loss=8.5145
	step [195/196], loss=7.6331
	step [196/196], loss=1.7032
	Evaluating
	loss=0.0381, precision=0.1663, recall=0.9961, f1=0.2851
Training epoch 21
	step [1/196], loss=7.1301
	step [2/196], loss=8.9761
	step [3/196], loss=7.8058
	step [4/196], loss=7.4801
	step [5/196], loss=7.9876
	step [6/196], loss=6.9321
	step [7/196], loss=6.8203
	step [8/196], loss=8.0299
	step [9/196], loss=8.1178
	step [10/196], loss=7.0639
	step [11/196], loss=8.2272
	step [12/196], loss=9.8229
	step [13/196], loss=7.1455
	step [14/196], loss=7.3242
	step [15/196], loss=7.7163
	step [16/196], loss=8.1619
	step [17/196], loss=8.8131
	step [18/196], loss=7.2002
	step [19/196], loss=7.4391
	step [20/196], loss=7.3266
	step [21/196], loss=8.3762
	step [22/196], loss=7.6195
	step [23/196], loss=6.7765
	step [24/196], loss=7.9657
	step [25/196], loss=7.6842
	step [26/196], loss=7.6371
	step [27/196], loss=7.3020
	step [28/196], loss=7.3622
	step [29/196], loss=6.9840
	step [30/196], loss=9.1202
	step [31/196], loss=9.0821
	step [32/196], loss=7.5331
	step [33/196], loss=8.1322
	step [34/196], loss=9.0581
	step [35/196], loss=8.9577
	step [36/196], loss=8.1077
	step [37/196], loss=7.2865
	step [38/196], loss=8.8138
	step [39/196], loss=6.1722
	step [40/196], loss=7.2223
	step [41/196], loss=8.1187
	step [42/196], loss=7.4855
	step [43/196], loss=6.8817
	step [44/196], loss=6.6073
	step [45/196], loss=7.8077
	step [46/196], loss=6.6240
	step [47/196], loss=8.0058
	step [48/196], loss=8.6983
	step [49/196], loss=7.5691
	step [50/196], loss=7.8482
	step [51/196], loss=8.3525
	step [52/196], loss=8.6688
	step [53/196], loss=7.7953
	step [54/196], loss=7.0505
	step [55/196], loss=6.7190
	step [56/196], loss=6.9030
	step [57/196], loss=8.7060
	step [58/196], loss=10.0152
	step [59/196], loss=8.6898
	step [60/196], loss=8.1032
	step [61/196], loss=7.4968
	step [62/196], loss=7.3155
	step [63/196], loss=7.7235
	step [64/196], loss=8.5348
	step [65/196], loss=7.8769
	step [66/196], loss=7.7039
	step [67/196], loss=7.5110
	step [68/196], loss=7.3885
	step [69/196], loss=8.3625
	step [70/196], loss=8.4175
	step [71/196], loss=6.6782
	step [72/196], loss=6.8934
	step [73/196], loss=7.2103
	step [74/196], loss=7.7974
	step [75/196], loss=7.2995
	step [76/196], loss=8.9215
	step [77/196], loss=8.4995
	step [78/196], loss=7.5257
	step [79/196], loss=7.2120
	step [80/196], loss=7.5604
	step [81/196], loss=8.8564
	step [82/196], loss=7.4827
	step [83/196], loss=7.7909
	step [84/196], loss=8.0049
	step [85/196], loss=7.9081
	step [86/196], loss=6.7775
	step [87/196], loss=8.7132
	step [88/196], loss=8.9936
	step [89/196], loss=7.6494
	step [90/196], loss=8.7669
	step [91/196], loss=8.3120
	step [92/196], loss=8.2361
	step [93/196], loss=8.5362
	step [94/196], loss=7.1006
	step [95/196], loss=7.9022
	step [96/196], loss=6.2066
	step [97/196], loss=7.2220
	step [98/196], loss=8.3124
	step [99/196], loss=8.1079
	step [100/196], loss=8.7975
	step [101/196], loss=8.0108
	step [102/196], loss=7.7423
	step [103/196], loss=6.8611
	step [104/196], loss=6.9204
	step [105/196], loss=7.6712
	step [106/196], loss=8.1253
	step [107/196], loss=8.3446
	step [108/196], loss=6.5430
	step [109/196], loss=6.9139
	step [110/196], loss=7.7384
	step [111/196], loss=8.0111
	step [112/196], loss=6.7181
	step [113/196], loss=8.5198
	step [114/196], loss=11.2693
	step [115/196], loss=7.3100
	step [116/196], loss=8.2686
	step [117/196], loss=7.8060
	step [118/196], loss=7.4991
	step [119/196], loss=6.4563
	step [120/196], loss=8.1139
	step [121/196], loss=7.7861
	step [122/196], loss=7.2168
	step [123/196], loss=6.6864
	step [124/196], loss=7.3306
	step [125/196], loss=7.9111
	step [126/196], loss=8.3823
	step [127/196], loss=7.2150
	step [128/196], loss=9.0346
	step [129/196], loss=9.4307
	step [130/196], loss=7.5087
	step [131/196], loss=7.6636
	step [132/196], loss=8.0331
	step [133/196], loss=9.0783
	step [134/196], loss=7.9940
	step [135/196], loss=7.4992
	step [136/196], loss=8.5475
	step [137/196], loss=8.6934
	step [138/196], loss=7.5533
	step [139/196], loss=8.4861
	step [140/196], loss=7.2482
	step [141/196], loss=7.8793
	step [142/196], loss=8.3401
	step [143/196], loss=5.9562
	step [144/196], loss=7.2340
	step [145/196], loss=6.8676
	step [146/196], loss=7.5327
	step [147/196], loss=7.6831
	step [148/196], loss=8.1366
	step [149/196], loss=7.9361
	step [150/196], loss=8.7006
	step [151/196], loss=7.4449
	step [152/196], loss=6.9998
	step [153/196], loss=7.4311
	step [154/196], loss=7.6709
	step [155/196], loss=8.3284
	step [156/196], loss=7.3403
	step [157/196], loss=7.3652
	step [158/196], loss=7.4892
	step [159/196], loss=9.3125
	step [160/196], loss=9.0089
	step [161/196], loss=7.3627
	step [162/196], loss=7.2239
	step [163/196], loss=8.2363
	step [164/196], loss=8.3679
	step [165/196], loss=8.6095
	step [166/196], loss=8.1899
	step [167/196], loss=8.2293
	step [168/196], loss=7.5901
	step [169/196], loss=7.8257
	step [170/196], loss=8.8033
	step [171/196], loss=8.6820
	step [172/196], loss=8.5132
	step [173/196], loss=8.1184
	step [174/196], loss=7.9634
	step [175/196], loss=7.0083
	step [176/196], loss=7.6567
	step [177/196], loss=7.9436
	step [178/196], loss=8.1544
	step [179/196], loss=7.3593
	step [180/196], loss=8.5724
	step [181/196], loss=8.2806
	step [182/196], loss=8.3954
	step [183/196], loss=8.2357
	step [184/196], loss=7.4320
	step [185/196], loss=6.6911
	step [186/196], loss=8.3287
	step [187/196], loss=6.6846
	step [188/196], loss=7.2755
	step [189/196], loss=7.0571
	step [190/196], loss=7.6503
	step [191/196], loss=6.8072
	step [192/196], loss=8.2245
	step [193/196], loss=7.0745
	step [194/196], loss=7.4363
	step [195/196], loss=8.8051
	step [196/196], loss=2.4340
	Evaluating
	loss=0.0335, precision=0.1870, recall=0.9956, f1=0.3148
Training epoch 22
	step [1/196], loss=7.3618
	step [2/196], loss=8.3964
	step [3/196], loss=7.7867
	step [4/196], loss=7.5895
	step [5/196], loss=7.7061
	step [6/196], loss=6.6810
	step [7/196], loss=6.2753
	step [8/196], loss=6.4034
	step [9/196], loss=8.5792
	step [10/196], loss=7.9797
	step [11/196], loss=8.0451
	step [12/196], loss=7.4296
	step [13/196], loss=8.8363
	step [14/196], loss=7.1472
	step [15/196], loss=8.1031
	step [16/196], loss=8.8696
	step [17/196], loss=7.7714
	step [18/196], loss=9.1492
	step [19/196], loss=7.5660
	step [20/196], loss=8.2408
	step [21/196], loss=8.7490
	step [22/196], loss=8.2645
	step [23/196], loss=7.9075
	step [24/196], loss=8.7879
	step [25/196], loss=7.5866
	step [26/196], loss=9.1352
	step [27/196], loss=7.1047
	step [28/196], loss=8.4854
	step [29/196], loss=7.8324
	step [30/196], loss=7.8008
	step [31/196], loss=7.6643
	step [32/196], loss=8.6509
	step [33/196], loss=8.3924
	step [34/196], loss=9.3514
	step [35/196], loss=7.0719
	step [36/196], loss=7.4724
	step [37/196], loss=6.2523
	step [38/196], loss=9.1008
	step [39/196], loss=8.1442
	step [40/196], loss=6.8500
	step [41/196], loss=7.9130
	step [42/196], loss=6.2296
	step [43/196], loss=6.4841
	step [44/196], loss=6.6165
	step [45/196], loss=8.7553
	step [46/196], loss=8.4266
	step [47/196], loss=6.8707
	step [48/196], loss=7.9617
	step [49/196], loss=7.2105
	step [50/196], loss=7.0703
	step [51/196], loss=6.1441
	step [52/196], loss=7.5529
	step [53/196], loss=7.1073
	step [54/196], loss=7.2203
	step [55/196], loss=6.7677
	step [56/196], loss=7.2240
	step [57/196], loss=5.9887
	step [58/196], loss=8.3317
	step [59/196], loss=7.6644
	step [60/196], loss=8.1817
	step [61/196], loss=6.8520
	step [62/196], loss=7.4563
	step [63/196], loss=7.9867
	step [64/196], loss=7.9475
	step [65/196], loss=7.8774
	step [66/196], loss=7.3999
	step [67/196], loss=7.2508
	step [68/196], loss=8.2535
	step [69/196], loss=6.7616
	step [70/196], loss=9.2617
	step [71/196], loss=8.1986
	step [72/196], loss=7.4102
	step [73/196], loss=7.7157
	step [74/196], loss=7.1229
	step [75/196], loss=6.4487
	step [76/196], loss=7.2926
	step [77/196], loss=7.6512
	step [78/196], loss=6.7217
	step [79/196], loss=6.9944
	step [80/196], loss=7.4095
	step [81/196], loss=6.0868
	step [82/196], loss=8.7519
	step [83/196], loss=6.4758
	step [84/196], loss=7.1002
	step [85/196], loss=6.8157
	step [86/196], loss=7.4955
	step [87/196], loss=6.7565
	step [88/196], loss=7.9258
	step [89/196], loss=7.0487
	step [90/196], loss=8.7094
	step [91/196], loss=8.8332
	step [92/196], loss=8.0903
	step [93/196], loss=6.9074
	step [94/196], loss=7.5695
	step [95/196], loss=7.0947
	step [96/196], loss=7.6343
	step [97/196], loss=8.5224
	step [98/196], loss=7.7542
	step [99/196], loss=7.6714
	step [100/196], loss=8.5087
	step [101/196], loss=7.5654
	step [102/196], loss=8.1598
	step [103/196], loss=7.4422
	step [104/196], loss=7.8818
	step [105/196], loss=6.9263
	step [106/196], loss=6.6534
	step [107/196], loss=8.0738
	step [108/196], loss=8.0449
	step [109/196], loss=6.6436
	step [110/196], loss=7.3033
	step [111/196], loss=7.3408
	step [112/196], loss=8.5031
	step [113/196], loss=7.6749
	step [114/196], loss=7.9980
	step [115/196], loss=7.2584
	step [116/196], loss=8.4877
	step [117/196], loss=7.9188
	step [118/196], loss=6.5686
	step [119/196], loss=7.1732
	step [120/196], loss=7.2084
	step [121/196], loss=8.8690
	step [122/196], loss=8.8647
	step [123/196], loss=7.6842
	step [124/196], loss=7.2691
	step [125/196], loss=6.6511
	step [126/196], loss=8.1829
	step [127/196], loss=7.3046
	step [128/196], loss=7.8821
	step [129/196], loss=7.3082
	step [130/196], loss=6.2422
	step [131/196], loss=8.0976
	step [132/196], loss=8.1007
	step [133/196], loss=7.8623
	step [134/196], loss=7.3305
	step [135/196], loss=6.6998
	step [136/196], loss=7.2220
	step [137/196], loss=6.3535
	step [138/196], loss=8.7811
	step [139/196], loss=6.9014
	step [140/196], loss=7.1794
	step [141/196], loss=8.1270
	step [142/196], loss=7.8849
	step [143/196], loss=6.9293
	step [144/196], loss=7.4844
	step [145/196], loss=8.6022
	step [146/196], loss=7.9543
	step [147/196], loss=8.1938
	step [148/196], loss=8.3006
	step [149/196], loss=7.7830
	step [150/196], loss=6.4805
	step [151/196], loss=7.2214
	step [152/196], loss=8.6704
	step [153/196], loss=7.1679
	step [154/196], loss=6.2279
	step [155/196], loss=6.7464
	step [156/196], loss=7.7778
	step [157/196], loss=7.9477
	step [158/196], loss=8.0633
	step [159/196], loss=6.5296
	step [160/196], loss=6.6370
	step [161/196], loss=7.2211
	step [162/196], loss=7.9518
	step [163/196], loss=7.6486
	step [164/196], loss=8.1444
	step [165/196], loss=6.2752
	step [166/196], loss=6.9362
	step [167/196], loss=7.3666
	step [168/196], loss=6.7895
	step [169/196], loss=6.8618
	step [170/196], loss=6.9165
	step [171/196], loss=6.9853
	step [172/196], loss=6.9926
	step [173/196], loss=7.7856
	step [174/196], loss=6.3451
	step [175/196], loss=8.2044
	step [176/196], loss=8.2289
	step [177/196], loss=6.8004
	step [178/196], loss=7.5980
	step [179/196], loss=6.4262
	step [180/196], loss=8.9452
	step [181/196], loss=7.5345
	step [182/196], loss=7.2352
	step [183/196], loss=6.7182
	step [184/196], loss=7.2824
	step [185/196], loss=7.7355
	step [186/196], loss=7.1444
	step [187/196], loss=6.3232
	step [188/196], loss=8.3965
	step [189/196], loss=8.1940
	step [190/196], loss=7.0613
	step [191/196], loss=7.4971
	step [192/196], loss=7.2356
	step [193/196], loss=7.9602
	step [194/196], loss=8.0848
	step [195/196], loss=8.9480
	step [196/196], loss=1.7815
	Evaluating
	loss=0.0393, precision=0.1699, recall=0.9960, f1=0.2903
Training epoch 23
	step [1/196], loss=7.4179
	step [2/196], loss=8.5263
	step [3/196], loss=7.9756
	step [4/196], loss=8.5607
	step [5/196], loss=7.1079
	step [6/196], loss=8.2556
	step [7/196], loss=7.6855
	step [8/196], loss=7.5550
	step [9/196], loss=7.7676
	step [10/196], loss=6.8146
	step [11/196], loss=6.8452
	step [12/196], loss=6.4135
	step [13/196], loss=8.3432
	step [14/196], loss=7.3178
	step [15/196], loss=6.5242
	step [16/196], loss=7.2073
	step [17/196], loss=7.1469
	step [18/196], loss=8.0634
	step [19/196], loss=6.1485
	step [20/196], loss=7.3813
	step [21/196], loss=7.7029
	step [22/196], loss=6.9239
	step [23/196], loss=6.4783
	step [24/196], loss=7.5511
	step [25/196], loss=8.5790
	step [26/196], loss=6.9209
	step [27/196], loss=7.6112
	step [28/196], loss=7.3400
	step [29/196], loss=8.1288
	step [30/196], loss=7.7743
	step [31/196], loss=6.9651
	step [32/196], loss=7.0022
	step [33/196], loss=6.4022
	step [34/196], loss=7.8256
	step [35/196], loss=5.6589
	step [36/196], loss=7.2685
	step [37/196], loss=6.7914
	step [38/196], loss=7.6908
	step [39/196], loss=7.5723
	step [40/196], loss=6.9597
	step [41/196], loss=6.2951
	step [42/196], loss=7.6898
	step [43/196], loss=9.1039
	step [44/196], loss=7.6027
	step [45/196], loss=8.1609
	step [46/196], loss=7.4429
	step [47/196], loss=7.9705
	step [48/196], loss=7.5440
	step [49/196], loss=7.4729
	step [50/196], loss=7.8458
	step [51/196], loss=7.3004
	step [52/196], loss=7.0654
	step [53/196], loss=6.9504
	step [54/196], loss=7.2339
	step [55/196], loss=7.8047
	step [56/196], loss=8.3372
	step [57/196], loss=6.7413
	step [58/196], loss=7.2535
	step [59/196], loss=6.9685
	step [60/196], loss=7.2867
	step [61/196], loss=7.7730
	step [62/196], loss=6.5010
	step [63/196], loss=5.8898
	step [64/196], loss=6.3758
	step [65/196], loss=6.4193
	step [66/196], loss=7.0057
	step [67/196], loss=6.9824
	step [68/196], loss=7.8651
	step [69/196], loss=6.8704
	step [70/196], loss=6.7495
	step [71/196], loss=7.1748
	step [72/196], loss=6.8422
	step [73/196], loss=8.5203
	step [74/196], loss=6.9173
	step [75/196], loss=7.5367
	step [76/196], loss=7.2199
	step [77/196], loss=7.1578
	step [78/196], loss=6.6108
	step [79/196], loss=7.8637
	step [80/196], loss=6.5092
	step [81/196], loss=7.0758
	step [82/196], loss=6.9053
	step [83/196], loss=5.9093
	step [84/196], loss=7.9088
	step [85/196], loss=7.1683
	step [86/196], loss=7.0713
	step [87/196], loss=7.3580
	step [88/196], loss=6.7315
	step [89/196], loss=7.7990
	step [90/196], loss=7.0107
	step [91/196], loss=7.6557
	step [92/196], loss=7.8037
	step [93/196], loss=7.2776
	step [94/196], loss=5.1235
	step [95/196], loss=7.5023
	step [96/196], loss=7.2881
	step [97/196], loss=7.0359
	step [98/196], loss=6.2221
	step [99/196], loss=8.0207
	step [100/196], loss=6.7001
	step [101/196], loss=7.2969
	step [102/196], loss=8.5653
	step [103/196], loss=7.1529
	step [104/196], loss=6.3537
	step [105/196], loss=8.2912
	step [106/196], loss=6.2452
	step [107/196], loss=6.8844
	step [108/196], loss=6.4001
	step [109/196], loss=7.2412
	step [110/196], loss=5.9951
	step [111/196], loss=7.2792
	step [112/196], loss=7.3674
	step [113/196], loss=7.1557
	step [114/196], loss=6.7399
	step [115/196], loss=7.6238
	step [116/196], loss=6.7737
	step [117/196], loss=7.5539
	step [118/196], loss=7.7053
	step [119/196], loss=7.0960
	step [120/196], loss=6.7576
	step [121/196], loss=8.8202
	step [122/196], loss=7.9524
	step [123/196], loss=7.1514
	step [124/196], loss=7.3447
	step [125/196], loss=7.8647
	step [126/196], loss=6.5186
	step [127/196], loss=7.0139
	step [128/196], loss=7.4674
	step [129/196], loss=6.5577
	step [130/196], loss=6.5861
	step [131/196], loss=7.9487
	step [132/196], loss=7.5431
	step [133/196], loss=9.0791
	step [134/196], loss=5.6211
	step [135/196], loss=7.3036
	step [136/196], loss=7.6267
	step [137/196], loss=6.2071
	step [138/196], loss=6.3183
	step [139/196], loss=7.7128
	step [140/196], loss=7.4777
	step [141/196], loss=6.3047
	step [142/196], loss=8.0808
	step [143/196], loss=6.8394
	step [144/196], loss=7.4208
	step [145/196], loss=6.2204
	step [146/196], loss=7.0756
	step [147/196], loss=6.9295
	step [148/196], loss=8.5208
	step [149/196], loss=7.7122
	step [150/196], loss=8.1214
	step [151/196], loss=7.4548
	step [152/196], loss=9.2232
	step [153/196], loss=6.8460
	step [154/196], loss=8.5654
	step [155/196], loss=7.6026
	step [156/196], loss=6.6939
	step [157/196], loss=6.9382
	step [158/196], loss=8.3673
	step [159/196], loss=6.6485
	step [160/196], loss=8.2293
	step [161/196], loss=7.1776
	step [162/196], loss=6.9571
	step [163/196], loss=8.4274
	step [164/196], loss=8.1561
	step [165/196], loss=7.5831
	step [166/196], loss=7.0797
	step [167/196], loss=6.9811
	step [168/196], loss=6.9893
	step [169/196], loss=6.5402
	step [170/196], loss=7.0116
	step [171/196], loss=6.9405
	step [172/196], loss=6.2090
	step [173/196], loss=7.2868
	step [174/196], loss=7.6121
	step [175/196], loss=7.2064
	step [176/196], loss=7.5058
	step [177/196], loss=7.8304
	step [178/196], loss=8.4825
	step [179/196], loss=6.8004
	step [180/196], loss=6.7726
	step [181/196], loss=7.5264
	step [182/196], loss=6.5761
	step [183/196], loss=8.5281
	step [184/196], loss=7.1618
	step [185/196], loss=7.3843
	step [186/196], loss=7.0446
	step [187/196], loss=6.8006
	step [188/196], loss=6.8956
	step [189/196], loss=6.9836
	step [190/196], loss=7.5867
	step [191/196], loss=7.2397
	step [192/196], loss=8.2387
	step [193/196], loss=7.5627
	step [194/196], loss=7.1947
	step [195/196], loss=7.5828
	step [196/196], loss=2.1401
	Evaluating
	loss=0.0269, precision=0.2180, recall=0.9917, f1=0.3574
saving model as: 0_saved_model.pth
Training epoch 24
	step [1/196], loss=7.2640
	step [2/196], loss=6.9410
	step [3/196], loss=6.2949
	step [4/196], loss=7.6576
	step [5/196], loss=6.8352
	step [6/196], loss=8.1521
	step [7/196], loss=6.0129
	step [8/196], loss=8.0204
	step [9/196], loss=8.2741
	step [10/196], loss=6.7000
	step [11/196], loss=5.7013
	step [12/196], loss=7.4137
	step [13/196], loss=6.6881
	step [14/196], loss=7.4116
	step [15/196], loss=6.5617
	step [16/196], loss=6.5571
	step [17/196], loss=7.5386
	step [18/196], loss=6.2493
	step [19/196], loss=8.6798
	step [20/196], loss=6.0972
	step [21/196], loss=6.8513
	step [22/196], loss=7.3875
	step [23/196], loss=7.1131
	step [24/196], loss=7.6904
	step [25/196], loss=7.3168
	step [26/196], loss=7.8322
	step [27/196], loss=8.1784
	step [28/196], loss=6.4364
	step [29/196], loss=7.0068
	step [30/196], loss=5.9550
	step [31/196], loss=6.1113
	step [32/196], loss=9.4641
	step [33/196], loss=6.7864
	step [34/196], loss=6.3512
	step [35/196], loss=7.0408
	step [36/196], loss=5.9806
	step [37/196], loss=8.2614
	step [38/196], loss=7.0208
	step [39/196], loss=7.7670
	step [40/196], loss=8.4722
	step [41/196], loss=6.3797
	step [42/196], loss=6.6193
	step [43/196], loss=6.2063
	step [44/196], loss=7.9901
	step [45/196], loss=6.5848
	step [46/196], loss=7.6612
	step [47/196], loss=7.5779
	step [48/196], loss=7.6381
	step [49/196], loss=6.3290
	step [50/196], loss=6.7939
	step [51/196], loss=6.2211
	step [52/196], loss=7.3712
	step [53/196], loss=8.1107
	step [54/196], loss=6.5150
	step [55/196], loss=5.7984
	step [56/196], loss=6.9763
	step [57/196], loss=7.2906
	step [58/196], loss=6.6467
	step [59/196], loss=6.7380
	step [60/196], loss=6.9765
	step [61/196], loss=7.1321
	step [62/196], loss=5.5422
	step [63/196], loss=7.9332
	step [64/196], loss=6.9859
	step [65/196], loss=7.3912
	step [66/196], loss=7.0137
	step [67/196], loss=6.9950
	step [68/196], loss=6.6740
	step [69/196], loss=6.7163
	step [70/196], loss=7.4736
	step [71/196], loss=6.5708
	step [72/196], loss=6.6965
	step [73/196], loss=5.9957
	step [74/196], loss=6.1082
	step [75/196], loss=6.8856
	step [76/196], loss=7.3554
	step [77/196], loss=6.5968
	step [78/196], loss=7.8708
	step [79/196], loss=6.2820
	step [80/196], loss=6.0015
	step [81/196], loss=6.5634
	step [82/196], loss=5.8119
	step [83/196], loss=7.3936
	step [84/196], loss=7.1301
	step [85/196], loss=7.1869
	step [86/196], loss=7.3500
	step [87/196], loss=7.1175
	step [88/196], loss=5.9301
	step [89/196], loss=7.1907
	step [90/196], loss=6.6642
	step [91/196], loss=6.7878
	step [92/196], loss=6.6188
	step [93/196], loss=6.9332
	step [94/196], loss=6.6993
	step [95/196], loss=6.6190
	step [96/196], loss=7.3148
	step [97/196], loss=7.0148
	step [98/196], loss=8.5409
	step [99/196], loss=6.7622
	step [100/196], loss=8.2682
	step [101/196], loss=8.1616
	step [102/196], loss=6.9891
	step [103/196], loss=6.7283
	step [104/196], loss=6.6771
	step [105/196], loss=7.4037
	step [106/196], loss=7.5033
	step [107/196], loss=6.2674
	step [108/196], loss=7.7841
	step [109/196], loss=6.8384
	step [110/196], loss=7.3053
	step [111/196], loss=6.8708
	step [112/196], loss=7.1311
	step [113/196], loss=7.5541
	step [114/196], loss=7.3977
	step [115/196], loss=6.5519
	step [116/196], loss=6.2467
	step [117/196], loss=7.3652
	step [118/196], loss=6.1803
	step [119/196], loss=7.8276
	step [120/196], loss=6.3038
	step [121/196], loss=7.4395
	step [122/196], loss=6.7731
	step [123/196], loss=7.6709
	step [124/196], loss=7.3671
	step [125/196], loss=7.5859
	step [126/196], loss=6.5062
	step [127/196], loss=8.0813
	step [128/196], loss=6.0212
	step [129/196], loss=8.0894
	step [130/196], loss=5.9800
	step [131/196], loss=7.1870
	step [132/196], loss=6.4345
	step [133/196], loss=5.8114
	step [134/196], loss=6.9616
	step [135/196], loss=6.4502
	step [136/196], loss=6.5201
	step [137/196], loss=6.0456
	step [138/196], loss=6.4719
	step [139/196], loss=6.8719
	step [140/196], loss=6.3208
	step [141/196], loss=7.0063
	step [142/196], loss=6.7037
	step [143/196], loss=8.8921
	step [144/196], loss=7.4304
	step [145/196], loss=6.5093
	step [146/196], loss=7.0716
	step [147/196], loss=6.5658
	step [148/196], loss=7.1632
	step [149/196], loss=6.7989
	step [150/196], loss=7.5117
	step [151/196], loss=6.8891
	step [152/196], loss=7.1048
	step [153/196], loss=6.7249
	step [154/196], loss=6.6524
	step [155/196], loss=7.6601
	step [156/196], loss=6.5185
	step [157/196], loss=7.5808
	step [158/196], loss=9.1773
	step [159/196], loss=6.7708
	step [160/196], loss=7.4958
	step [161/196], loss=6.4885
	step [162/196], loss=7.3436
	step [163/196], loss=7.4715
	step [164/196], loss=7.8415
	step [165/196], loss=7.7623
	step [166/196], loss=7.2573
	step [167/196], loss=7.5460
	step [168/196], loss=7.2646
	step [169/196], loss=7.6362
	step [170/196], loss=5.9440
	step [171/196], loss=6.4096
	step [172/196], loss=7.4159
	step [173/196], loss=6.0905
	step [174/196], loss=6.3781
	step [175/196], loss=6.3860
	step [176/196], loss=6.3209
	step [177/196], loss=7.5625
	step [178/196], loss=7.0691
	step [179/196], loss=7.2692
	step [180/196], loss=6.4882
	step [181/196], loss=6.1528
	step [182/196], loss=7.5034
	step [183/196], loss=7.8287
	step [184/196], loss=7.8128
	step [185/196], loss=6.8107
	step [186/196], loss=7.1408
	step [187/196], loss=6.5062
	step [188/196], loss=6.2512
	step [189/196], loss=6.5412
	step [190/196], loss=8.0091
	step [191/196], loss=7.2000
	step [192/196], loss=7.9252
	step [193/196], loss=6.7550
	step [194/196], loss=7.7342
	step [195/196], loss=8.7153
	step [196/196], loss=1.6142
	Evaluating
	loss=0.0320, precision=0.2002, recall=0.9937, f1=0.3333
Training epoch 25
	step [1/196], loss=7.5125
	step [2/196], loss=7.1877
	step [3/196], loss=5.8270
	step [4/196], loss=6.8828
	step [5/196], loss=7.5545
	step [6/196], loss=7.2507
	step [7/196], loss=5.9113
	step [8/196], loss=6.5548
	step [9/196], loss=6.6817
	step [10/196], loss=7.4250
	step [11/196], loss=7.8924
	step [12/196], loss=8.0165
	step [13/196], loss=6.3220
	step [14/196], loss=7.4128
	step [15/196], loss=7.4256
	step [16/196], loss=7.1766
	step [17/196], loss=7.8471
	step [18/196], loss=6.5062
	step [19/196], loss=7.4799
	step [20/196], loss=5.2306
	step [21/196], loss=8.3946
	step [22/196], loss=6.9050
	step [23/196], loss=7.2330
	step [24/196], loss=7.7843
	step [25/196], loss=5.9387
	step [26/196], loss=6.2483
	step [27/196], loss=6.4806
	step [28/196], loss=6.2748
	step [29/196], loss=7.2627
	step [30/196], loss=7.5861
	step [31/196], loss=7.3693
	step [32/196], loss=6.6996
	step [33/196], loss=6.8289
	step [34/196], loss=7.0332
	step [35/196], loss=7.7155
	step [36/196], loss=5.5164
	step [37/196], loss=6.3778
	step [38/196], loss=7.1720
	step [39/196], loss=6.2178
	step [40/196], loss=6.9881
	step [41/196], loss=6.6174
	step [42/196], loss=6.6779
	step [43/196], loss=6.4869
	step [44/196], loss=7.5177
	step [45/196], loss=6.2156
	step [46/196], loss=5.8453
	step [47/196], loss=7.8739
	step [48/196], loss=6.8431
	step [49/196], loss=6.7080
	step [50/196], loss=7.2177
	step [51/196], loss=5.9779
	step [52/196], loss=7.2014
	step [53/196], loss=8.6107
	step [54/196], loss=6.6986
	step [55/196], loss=7.8115
	step [56/196], loss=6.9794
	step [57/196], loss=5.2002
	step [58/196], loss=6.8667
	step [59/196], loss=6.3564
	step [60/196], loss=8.5587
	step [61/196], loss=5.2591
	step [62/196], loss=7.5728
	step [63/196], loss=7.4085
	step [64/196], loss=7.0327
	step [65/196], loss=7.0858
	step [66/196], loss=6.6088
	step [67/196], loss=7.3648
	step [68/196], loss=6.4822
	step [69/196], loss=5.8797
	step [70/196], loss=8.1214
	step [71/196], loss=6.0717
	step [72/196], loss=7.3258
	step [73/196], loss=7.4071
	step [74/196], loss=6.4089
	step [75/196], loss=7.4442
	step [76/196], loss=7.5917
	step [77/196], loss=6.4050
	step [78/196], loss=6.4513
	step [79/196], loss=7.4267
	step [80/196], loss=7.6142
	step [81/196], loss=6.7668
	step [82/196], loss=6.7997
	step [83/196], loss=7.4165
	step [84/196], loss=6.4023
	step [85/196], loss=5.8091
	step [86/196], loss=7.0743
	step [87/196], loss=6.7457
	step [88/196], loss=6.7682
	step [89/196], loss=6.7225
	step [90/196], loss=8.2419
	step [91/196], loss=5.9655
	step [92/196], loss=8.1100
	step [93/196], loss=7.7850
	step [94/196], loss=7.0679
	step [95/196], loss=5.4082
	step [96/196], loss=6.2704
	step [97/196], loss=6.1549
	step [98/196], loss=6.8199
	step [99/196], loss=5.9926
	step [100/196], loss=6.1135
	step [101/196], loss=9.5497
	step [102/196], loss=7.6029
	step [103/196], loss=6.3114
	step [104/196], loss=7.3542
	step [105/196], loss=7.2057
	step [106/196], loss=7.1399
	step [107/196], loss=7.8339
	step [108/196], loss=7.0767
	step [109/196], loss=6.4437
	step [110/196], loss=6.5212
	step [111/196], loss=6.5968
	step [112/196], loss=7.0724
	step [113/196], loss=6.3724
	step [114/196], loss=5.7564
	step [115/196], loss=6.0784
	step [116/196], loss=7.4701
	step [117/196], loss=7.0147
	step [118/196], loss=8.1216
	step [119/196], loss=6.8334
	step [120/196], loss=6.5303
	step [121/196], loss=6.6384
	step [122/196], loss=6.4551
	step [123/196], loss=6.5508
	step [124/196], loss=6.1727
	step [125/196], loss=6.0369
	step [126/196], loss=7.9132
	step [127/196], loss=6.9812
	step [128/196], loss=7.2929
	step [129/196], loss=5.6945
	step [130/196], loss=7.0889
	step [131/196], loss=6.6588
	step [132/196], loss=8.1966
	step [133/196], loss=7.6598
	step [134/196], loss=6.1897
	step [135/196], loss=6.4784
	step [136/196], loss=6.6306
	step [137/196], loss=7.3945
	step [138/196], loss=6.4925
	step [139/196], loss=7.4151
	step [140/196], loss=6.1996
	step [141/196], loss=6.8502
	step [142/196], loss=6.7150
	step [143/196], loss=5.8866
	step [144/196], loss=7.2571
	step [145/196], loss=6.1279
	step [146/196], loss=6.6095
	step [147/196], loss=6.5830
	step [148/196], loss=6.0995
	step [149/196], loss=6.1145
	step [150/196], loss=8.4910
	step [151/196], loss=7.0899
	step [152/196], loss=5.8777
	step [153/196], loss=7.2332
	step [154/196], loss=7.3195
	step [155/196], loss=5.9621
	step [156/196], loss=5.9034
	step [157/196], loss=6.3112
	step [158/196], loss=7.0988
	step [159/196], loss=6.8752
	step [160/196], loss=6.6979
	step [161/196], loss=6.9535
	step [162/196], loss=8.2135
	step [163/196], loss=6.3093
	step [164/196], loss=7.2126
	step [165/196], loss=6.8389
	step [166/196], loss=7.0989
	step [167/196], loss=6.4517
	step [168/196], loss=6.7404
	step [169/196], loss=6.2706
	step [170/196], loss=6.3085
	step [171/196], loss=8.2948
	step [172/196], loss=7.1564
	step [173/196], loss=6.2772
	step [174/196], loss=6.3325
	step [175/196], loss=7.3287
	step [176/196], loss=7.9274
	step [177/196], loss=6.2876
	step [178/196], loss=6.5037
	step [179/196], loss=5.3966
	step [180/196], loss=6.0457
	step [181/196], loss=7.0982
	step [182/196], loss=5.9557
	step [183/196], loss=6.6512
	step [184/196], loss=6.4304
	step [185/196], loss=6.9208
	step [186/196], loss=5.7003
	step [187/196], loss=5.6211
	step [188/196], loss=6.1285
	step [189/196], loss=5.9658
	step [190/196], loss=7.5924
	step [191/196], loss=6.9078
	step [192/196], loss=8.8816
	step [193/196], loss=6.5181
	step [194/196], loss=6.7585
	step [195/196], loss=7.0308
	step [196/196], loss=1.9472
	Evaluating
	loss=0.0276, precision=0.2224, recall=0.9940, f1=0.3635
saving model as: 0_saved_model.pth
Training epoch 26
	step [1/196], loss=7.1214
	step [2/196], loss=6.3403
	step [3/196], loss=7.0913
	step [4/196], loss=6.1605
	step [5/196], loss=6.1237
	step [6/196], loss=6.4346
	step [7/196], loss=6.9039
	step [8/196], loss=6.8561
	step [9/196], loss=6.6227
	step [10/196], loss=7.5748
	step [11/196], loss=6.5962
	step [12/196], loss=7.5275
	step [13/196], loss=6.8033
	step [14/196], loss=5.6682
	step [15/196], loss=6.7303
	step [16/196], loss=6.5848
	step [17/196], loss=7.7719
	step [18/196], loss=6.8555
	step [19/196], loss=6.5696
	step [20/196], loss=6.0091
	step [21/196], loss=7.5779
	step [22/196], loss=7.4028
	step [23/196], loss=6.1354
	step [24/196], loss=6.5975
	step [25/196], loss=6.0888
	step [26/196], loss=6.7690
	step [27/196], loss=5.5760
	step [28/196], loss=6.2884
	step [29/196], loss=7.0443
	step [30/196], loss=5.9858
	step [31/196], loss=6.6234
	step [32/196], loss=8.3938
	step [33/196], loss=6.2435
	step [34/196], loss=6.2807
	step [35/196], loss=7.7557
	step [36/196], loss=7.0718
	step [37/196], loss=7.2754
	step [38/196], loss=5.8081
	step [39/196], loss=5.6222
	step [40/196], loss=7.5712
	step [41/196], loss=6.9390
	step [42/196], loss=7.2713
	step [43/196], loss=6.8282
	step [44/196], loss=6.5459
	step [45/196], loss=6.4877
	step [46/196], loss=6.6786
	step [47/196], loss=7.6588
	step [48/196], loss=7.6572
	step [49/196], loss=7.6213
	step [50/196], loss=6.8675
	step [51/196], loss=5.8215
	step [52/196], loss=7.3152
	step [53/196], loss=6.8475
	step [54/196], loss=7.5533
	step [55/196], loss=7.4032
	step [56/196], loss=6.4786
	step [57/196], loss=7.2459
	step [58/196], loss=6.0928
	step [59/196], loss=5.9580
	step [60/196], loss=6.8650
	step [61/196], loss=6.1822
	step [62/196], loss=6.7855
	step [63/196], loss=6.6303
	step [64/196], loss=7.0631
	step [65/196], loss=6.3422
	step [66/196], loss=5.8768
	step [67/196], loss=6.0851
	step [68/196], loss=5.5013
	step [69/196], loss=5.9950
	step [70/196], loss=6.9481
	step [71/196], loss=6.6566
	step [72/196], loss=7.0481
	step [73/196], loss=7.3342
	step [74/196], loss=6.2311
	step [75/196], loss=6.9365
	step [76/196], loss=6.2347
	step [77/196], loss=6.0209
	step [78/196], loss=7.9879
	step [79/196], loss=6.2510
	step [80/196], loss=5.6589
	step [81/196], loss=7.0534
	step [82/196], loss=6.6772
	step [83/196], loss=6.4477
	step [84/196], loss=6.7940
	step [85/196], loss=7.6872
	step [86/196], loss=5.5022
	step [87/196], loss=6.3118
	step [88/196], loss=7.1912
	step [89/196], loss=6.2105
	step [90/196], loss=6.4420
	step [91/196], loss=5.7999
	step [92/196], loss=6.4031
	step [93/196], loss=6.6405
	step [94/196], loss=6.8203
	step [95/196], loss=7.5760
	step [96/196], loss=6.3140
	step [97/196], loss=5.6689
	step [98/196], loss=6.4739
	step [99/196], loss=6.5232
	step [100/196], loss=6.4044
	step [101/196], loss=7.5872
	step [102/196], loss=6.2620
	step [103/196], loss=6.4854
	step [104/196], loss=7.3196
	step [105/196], loss=5.6193
	step [106/196], loss=6.2935
	step [107/196], loss=5.9780
	step [108/196], loss=5.9528
	step [109/196], loss=5.6341
	step [110/196], loss=6.6912
	step [111/196], loss=7.0553
	step [112/196], loss=6.7204
	step [113/196], loss=6.2450
	step [114/196], loss=6.5283
	step [115/196], loss=7.2475
	step [116/196], loss=7.1132
	step [117/196], loss=6.8527
	step [118/196], loss=5.9210
	step [119/196], loss=7.3103
	step [120/196], loss=6.6237
	step [121/196], loss=7.4670
	step [122/196], loss=7.2386
	step [123/196], loss=7.6648
	step [124/196], loss=7.3153
	step [125/196], loss=6.3509
	step [126/196], loss=6.7945
	step [127/196], loss=6.1351
	step [128/196], loss=6.7174
	step [129/196], loss=5.5479
	step [130/196], loss=5.5029
	step [131/196], loss=6.7476
	step [132/196], loss=5.8437
	step [133/196], loss=6.2660
	step [134/196], loss=6.3901
	step [135/196], loss=6.6722
	step [136/196], loss=7.0455
	step [137/196], loss=6.3034
	step [138/196], loss=6.1781
	step [139/196], loss=6.8785
	step [140/196], loss=6.4655
	step [141/196], loss=6.0165
	step [142/196], loss=5.9051
	step [143/196], loss=5.9732
	step [144/196], loss=6.2185
	step [145/196], loss=6.9760
	step [146/196], loss=6.0445
	step [147/196], loss=6.1059
	step [148/196], loss=6.3688
	step [149/196], loss=7.1710
	step [150/196], loss=6.2020
	step [151/196], loss=5.9689
	step [152/196], loss=7.7680
	step [153/196], loss=5.4794
	step [154/196], loss=6.0571
	step [155/196], loss=5.4748
	step [156/196], loss=6.5987
	step [157/196], loss=5.3245
	step [158/196], loss=6.8396
	step [159/196], loss=6.6516
	step [160/196], loss=5.9614
	step [161/196], loss=6.3852
	step [162/196], loss=6.5795
	step [163/196], loss=6.5238
	step [164/196], loss=7.2211
	step [165/196], loss=6.0669
	step [166/196], loss=7.0013
	step [167/196], loss=6.1208
	step [168/196], loss=6.2327
	step [169/196], loss=7.7934
	step [170/196], loss=6.6462
	step [171/196], loss=6.3687
	step [172/196], loss=6.8638
	step [173/196], loss=6.2868
	step [174/196], loss=6.3006
	step [175/196], loss=5.9911
	step [176/196], loss=6.2977
	step [177/196], loss=6.2352
	step [178/196], loss=6.3683
	step [179/196], loss=5.9303
	step [180/196], loss=6.8003
	step [181/196], loss=6.7559
	step [182/196], loss=5.8531
	step [183/196], loss=6.3773
	step [184/196], loss=5.5948
	step [185/196], loss=6.5650
	step [186/196], loss=6.5406
	step [187/196], loss=6.1025
	step [188/196], loss=7.1437
	step [189/196], loss=5.9447
	step [190/196], loss=6.8236
	step [191/196], loss=7.1092
	step [192/196], loss=5.9628
	step [193/196], loss=6.8874
	step [194/196], loss=7.3304
	step [195/196], loss=7.2747
	step [196/196], loss=2.3172
	Evaluating
	loss=0.0259, precision=0.2257, recall=0.9923, f1=0.3677
saving model as: 0_saved_model.pth
Training epoch 27
	step [1/196], loss=6.3355
	step [2/196], loss=5.7724
	step [3/196], loss=5.8182
	step [4/196], loss=6.4378
	step [5/196], loss=5.7003
	step [6/196], loss=6.4346
	step [7/196], loss=6.2702
	step [8/196], loss=6.0069
	step [9/196], loss=6.6676
	step [10/196], loss=5.8121
	step [11/196], loss=6.9711
	step [12/196], loss=6.5052
	step [13/196], loss=5.8186
	step [14/196], loss=6.5633
	step [15/196], loss=7.8698
	step [16/196], loss=5.6546
	step [17/196], loss=6.3219
	step [18/196], loss=5.9618
	step [19/196], loss=6.3457
	step [20/196], loss=6.3557
	step [21/196], loss=5.5177
	step [22/196], loss=6.0391
	step [23/196], loss=6.2556
	step [24/196], loss=5.6808
	step [25/196], loss=6.0084
	step [26/196], loss=6.4833
	step [27/196], loss=6.4472
	step [28/196], loss=5.8617
	step [29/196], loss=6.4831
	step [30/196], loss=7.3763
	step [31/196], loss=7.5715
	step [32/196], loss=6.0278
	step [33/196], loss=6.9436
	step [34/196], loss=6.6297
	step [35/196], loss=5.7103
	step [36/196], loss=5.9037
	step [37/196], loss=6.0649
	step [38/196], loss=6.4230
	step [39/196], loss=5.7612
	step [40/196], loss=6.4232
	step [41/196], loss=5.6726
	step [42/196], loss=7.1570
	step [43/196], loss=6.2210
	step [44/196], loss=6.0228
	step [45/196], loss=6.4734
	step [46/196], loss=5.9652
	step [47/196], loss=6.8072
	step [48/196], loss=5.7020
	step [49/196], loss=6.2606
	step [50/196], loss=6.0179
	step [51/196], loss=6.4278
	step [52/196], loss=5.4409
	step [53/196], loss=6.9751
	step [54/196], loss=5.9488
	step [55/196], loss=7.5578
	step [56/196], loss=5.6092
	step [57/196], loss=5.8586
	step [58/196], loss=6.4203
	step [59/196], loss=7.5510
	step [60/196], loss=6.0688
	step [61/196], loss=5.6968
	step [62/196], loss=6.1612
	step [63/196], loss=6.1883
	step [64/196], loss=6.7025
	step [65/196], loss=6.3982
	step [66/196], loss=6.5127
	step [67/196], loss=5.4340
	step [68/196], loss=6.9113
	step [69/196], loss=6.1773
	step [70/196], loss=5.6215
	step [71/196], loss=6.0340
	step [72/196], loss=5.9251
	step [73/196], loss=6.3965
	step [74/196], loss=5.4122
	step [75/196], loss=7.9721
	step [76/196], loss=8.2727
	step [77/196], loss=6.6928
	step [78/196], loss=6.2088
	step [79/196], loss=6.5901
	step [80/196], loss=7.3879
	step [81/196], loss=6.3833
	step [82/196], loss=7.1331
	step [83/196], loss=6.5809
	step [84/196], loss=7.1215
	step [85/196], loss=6.3996
	step [86/196], loss=6.7824
	step [87/196], loss=7.4194
	step [88/196], loss=5.1312
	step [89/196], loss=5.9943
	step [90/196], loss=6.5532
	step [91/196], loss=5.8389
	step [92/196], loss=6.3983
	step [93/196], loss=5.9500
	step [94/196], loss=5.1516
	step [95/196], loss=6.1635
	step [96/196], loss=5.7749
	step [97/196], loss=7.3963
	step [98/196], loss=5.7277
	step [99/196], loss=5.8963
	step [100/196], loss=7.0108
	step [101/196], loss=6.3117
	step [102/196], loss=6.8709
	step [103/196], loss=6.0501
	step [104/196], loss=6.0740
	step [105/196], loss=5.6429
	step [106/196], loss=6.5764
	step [107/196], loss=6.4424
	step [108/196], loss=6.4404
	step [109/196], loss=7.6882
	step [110/196], loss=6.3131
	step [111/196], loss=6.2320
	step [112/196], loss=7.3340
	step [113/196], loss=5.7574
	step [114/196], loss=6.7950
	step [115/196], loss=6.7713
	step [116/196], loss=6.9039
	step [117/196], loss=6.4962
	step [118/196], loss=6.3294
	step [119/196], loss=7.0148
	step [120/196], loss=5.9563
	step [121/196], loss=6.0956
	step [122/196], loss=5.5567
	step [123/196], loss=6.2742
	step [124/196], loss=4.7529
	step [125/196], loss=6.8398
	step [126/196], loss=6.9289
	step [127/196], loss=7.0150
	step [128/196], loss=5.9498
	step [129/196], loss=6.3877
	step [130/196], loss=6.0644
	step [131/196], loss=5.9853
	step [132/196], loss=6.2540
	step [133/196], loss=5.9014
	step [134/196], loss=6.8115
	step [135/196], loss=5.5842
	step [136/196], loss=5.8411
	step [137/196], loss=4.5816
	step [138/196], loss=5.8007
	step [139/196], loss=6.3151
	step [140/196], loss=5.8395
	step [141/196], loss=6.7267
	step [142/196], loss=6.1423
	step [143/196], loss=5.6947
	step [144/196], loss=6.0971
	step [145/196], loss=6.5824
	step [146/196], loss=6.7950
	step [147/196], loss=5.6318
	step [148/196], loss=6.5460
	step [149/196], loss=6.6233
	step [150/196], loss=6.4721
	step [151/196], loss=6.8822
	step [152/196], loss=5.6235
	step [153/196], loss=6.3973
	step [154/196], loss=6.1569
	step [155/196], loss=6.5189
	step [156/196], loss=4.9624
	step [157/196], loss=6.1581
	step [158/196], loss=5.4690
	step [159/196], loss=6.6831
	step [160/196], loss=6.5601
	step [161/196], loss=6.6401
	step [162/196], loss=6.9196
	step [163/196], loss=5.8984
	step [164/196], loss=5.9555
	step [165/196], loss=6.2688
	step [166/196], loss=6.3299
	step [167/196], loss=7.8452
	step [168/196], loss=5.9758
	step [169/196], loss=6.4849
	step [170/196], loss=6.5242
	step [171/196], loss=6.2799
	step [172/196], loss=6.6462
	step [173/196], loss=6.0419
	step [174/196], loss=7.0324
	step [175/196], loss=6.2743
	step [176/196], loss=5.4920
	step [177/196], loss=6.6719
	step [178/196], loss=5.2122
	step [179/196], loss=4.5039
	step [180/196], loss=7.1309
	step [181/196], loss=5.1738
	step [182/196], loss=7.4479
	step [183/196], loss=5.9205
	step [184/196], loss=6.8372
	step [185/196], loss=5.5435
	step [186/196], loss=7.3898
	step [187/196], loss=7.1374
	step [188/196], loss=7.2070
	step [189/196], loss=6.4451
	step [190/196], loss=6.6673
	step [191/196], loss=4.9308
	step [192/196], loss=6.1557
	step [193/196], loss=5.8024
	step [194/196], loss=7.6914
	step [195/196], loss=5.7469
	step [196/196], loss=1.8977
	Evaluating
	loss=0.0244, precision=0.2478, recall=0.9930, f1=0.3966
saving model as: 0_saved_model.pth
Training epoch 28
	step [1/196], loss=7.4326
	step [2/196], loss=6.0490
	step [3/196], loss=6.0389
	step [4/196], loss=5.2566
	step [5/196], loss=6.5120
	step [6/196], loss=5.9783
	step [7/196], loss=7.1896
	step [8/196], loss=6.4946
	step [9/196], loss=6.4231
	step [10/196], loss=6.3420
	step [11/196], loss=5.6792
	step [12/196], loss=6.8039
	step [13/196], loss=6.3667
	step [14/196], loss=6.9000
	step [15/196], loss=6.0220
	step [16/196], loss=5.1640
	step [17/196], loss=5.9448
	step [18/196], loss=6.0653
	step [19/196], loss=5.0647
	step [20/196], loss=7.0851
	step [21/196], loss=6.3916
	step [22/196], loss=5.8299
	step [23/196], loss=6.5105
	step [24/196], loss=6.1201
	step [25/196], loss=6.5848
	step [26/196], loss=7.1229
	step [27/196], loss=6.3007
	step [28/196], loss=6.8395
	step [29/196], loss=5.9458
	step [30/196], loss=6.3812
	step [31/196], loss=5.7915
	step [32/196], loss=6.1283
	step [33/196], loss=6.6681
	step [34/196], loss=6.3417
	step [35/196], loss=6.3632
	step [36/196], loss=6.9409
	step [37/196], loss=6.5711
	step [38/196], loss=6.3409
	step [39/196], loss=6.2002
	step [40/196], loss=6.4407
	step [41/196], loss=6.1936
	step [42/196], loss=6.6699
	step [43/196], loss=5.4709
	step [44/196], loss=6.7048
	step [45/196], loss=6.3823
	step [46/196], loss=6.8604
	step [47/196], loss=5.5845
	step [48/196], loss=7.0490
	step [49/196], loss=7.0966
	step [50/196], loss=6.2095
	step [51/196], loss=7.5662
	step [52/196], loss=6.9526
	step [53/196], loss=6.9364
	step [54/196], loss=6.4028
	step [55/196], loss=5.3238
	step [56/196], loss=6.3860
	step [57/196], loss=5.6874
	step [58/196], loss=5.6059
	step [59/196], loss=5.3518
	step [60/196], loss=5.5374
	step [61/196], loss=5.8912
	step [62/196], loss=6.3177
	step [63/196], loss=7.1385
	step [64/196], loss=5.4831
	step [65/196], loss=7.2326
	step [66/196], loss=6.3206
	step [67/196], loss=5.8878
	step [68/196], loss=5.9615
	step [69/196], loss=5.6464
	step [70/196], loss=6.5408
	step [71/196], loss=4.6935
	step [72/196], loss=8.0463
	step [73/196], loss=5.8556
	step [74/196], loss=6.5584
	step [75/196], loss=6.3784
	step [76/196], loss=5.4165
	step [77/196], loss=6.2911
	step [78/196], loss=5.4387
	step [79/196], loss=6.7190
	step [80/196], loss=6.4683
	step [81/196], loss=6.0802
	step [82/196], loss=5.6107
	step [83/196], loss=6.3632
	step [84/196], loss=6.2192
	step [85/196], loss=5.2063
	step [86/196], loss=6.0457
	step [87/196], loss=6.4425
	step [88/196], loss=5.8620
	step [89/196], loss=6.2157
	step [90/196], loss=5.5546
	step [91/196], loss=6.8300
	step [92/196], loss=6.1756
	step [93/196], loss=6.0826
	step [94/196], loss=5.5377
	step [95/196], loss=6.2480
	step [96/196], loss=6.4854
	step [97/196], loss=5.7878
	step [98/196], loss=5.4406
	step [99/196], loss=5.9313
	step [100/196], loss=6.5796
	step [101/196], loss=8.0102
	step [102/196], loss=7.2699
	step [103/196], loss=6.9585
	step [104/196], loss=5.8024
	step [105/196], loss=5.9368
	step [106/196], loss=6.0463
	step [107/196], loss=6.1685
	step [108/196], loss=6.2429
	step [109/196], loss=6.9824
	step [110/196], loss=5.5232
	step [111/196], loss=5.2122
	step [112/196], loss=6.0134
	step [113/196], loss=6.2934
	step [114/196], loss=4.9706
	step [115/196], loss=6.5716
	step [116/196], loss=4.8361
	step [117/196], loss=6.3365
	step [118/196], loss=5.5729
	step [119/196], loss=6.5823
	step [120/196], loss=5.4576
	step [121/196], loss=6.3564
	step [122/196], loss=6.5650
	step [123/196], loss=5.8801
	step [124/196], loss=6.7093
	step [125/196], loss=6.8162
	step [126/196], loss=6.1768
	step [127/196], loss=6.2984
	step [128/196], loss=6.5310
	step [129/196], loss=5.4418
	step [130/196], loss=6.7194
	step [131/196], loss=5.8894
	step [132/196], loss=5.7107
	step [133/196], loss=5.1892
	step [134/196], loss=5.8892
	step [135/196], loss=5.8803
	step [136/196], loss=6.4621
	step [137/196], loss=6.3505
	step [138/196], loss=6.3642
	step [139/196], loss=5.1562
	step [140/196], loss=6.6227
	step [141/196], loss=5.5397
	step [142/196], loss=5.8492
	step [143/196], loss=5.7448
	step [144/196], loss=7.1275
	step [145/196], loss=6.6169
	step [146/196], loss=6.9055
	step [147/196], loss=5.5255
	step [148/196], loss=5.8593
	step [149/196], loss=5.7871
	step [150/196], loss=5.6986
	step [151/196], loss=6.0222
	step [152/196], loss=6.0709
	step [153/196], loss=6.8580
	step [154/196], loss=6.9164
	step [155/196], loss=7.0619
	step [156/196], loss=6.0073
	step [157/196], loss=5.3244
	step [158/196], loss=7.3176
	step [159/196], loss=6.5909
	step [160/196], loss=5.1211
	step [161/196], loss=6.9776
	step [162/196], loss=7.2811
	step [163/196], loss=6.3209
	step [164/196], loss=5.4971
	step [165/196], loss=5.2085
	step [166/196], loss=7.1995
	step [167/196], loss=5.5203
	step [168/196], loss=6.1767
	step [169/196], loss=6.2440
	step [170/196], loss=5.7725
	step [171/196], loss=5.7098
	step [172/196], loss=6.5358
	step [173/196], loss=6.7262
	step [174/196], loss=6.1955
	step [175/196], loss=7.5452
	step [176/196], loss=5.4713
	step [177/196], loss=5.7205
	step [178/196], loss=7.0107
	step [179/196], loss=7.5658
	step [180/196], loss=5.8993
	step [181/196], loss=6.1728
	step [182/196], loss=6.1414
	step [183/196], loss=6.6521
	step [184/196], loss=6.1200
	step [185/196], loss=6.4483
	step [186/196], loss=6.7686
	step [187/196], loss=5.5528
	step [188/196], loss=6.1071
	step [189/196], loss=6.6394
	step [190/196], loss=5.8522
	step [191/196], loss=5.6792
	step [192/196], loss=7.2864
	step [193/196], loss=5.3621
	step [194/196], loss=6.4143
	step [195/196], loss=6.8177
	step [196/196], loss=1.6717
	Evaluating
	loss=0.0253, precision=0.2245, recall=0.9933, f1=0.3662
Training epoch 29
	step [1/196], loss=5.9578
	step [2/196], loss=5.8439
	step [3/196], loss=5.8604
	step [4/196], loss=6.2680
	step [5/196], loss=5.2327
	step [6/196], loss=5.5753
	step [7/196], loss=5.3983
	step [8/196], loss=6.5849
	step [9/196], loss=6.5571
	step [10/196], loss=5.2913
	step [11/196], loss=6.3170
	step [12/196], loss=5.9153
	step [13/196], loss=6.6133
	step [14/196], loss=5.7597
	step [15/196], loss=5.8853
	step [16/196], loss=7.4131
	step [17/196], loss=5.9073
	step [18/196], loss=5.3564
	step [19/196], loss=6.4025
	step [20/196], loss=5.6213
	step [21/196], loss=6.0244
	step [22/196], loss=6.1818
	step [23/196], loss=6.2231
	step [24/196], loss=6.3024
	step [25/196], loss=5.7895
	step [26/196], loss=6.0188
	step [27/196], loss=6.2659
	step [28/196], loss=6.8442
	step [29/196], loss=6.2466
	step [30/196], loss=5.7745
	step [31/196], loss=6.1126
	step [32/196], loss=5.6158
	step [33/196], loss=6.0074
	step [34/196], loss=5.7266
	step [35/196], loss=7.1316
	step [36/196], loss=5.9804
	step [37/196], loss=5.6306
	step [38/196], loss=6.4870
	step [39/196], loss=7.0729
	step [40/196], loss=5.7710
	step [41/196], loss=5.7392
	step [42/196], loss=6.6637
	step [43/196], loss=5.7151
	step [44/196], loss=5.4372
	step [45/196], loss=5.2770
	step [46/196], loss=6.2679
	step [47/196], loss=4.6884
	step [48/196], loss=4.4858
	step [49/196], loss=5.7946
	step [50/196], loss=5.5785
	step [51/196], loss=5.9298
	step [52/196], loss=4.7070
	step [53/196], loss=5.5417
	step [54/196], loss=6.7552
	step [55/196], loss=6.9076
	step [56/196], loss=6.3162
	step [57/196], loss=4.9158
	step [58/196], loss=6.2060
	step [59/196], loss=5.6225
	step [60/196], loss=5.8605
	step [61/196], loss=5.2863
	step [62/196], loss=5.1689
	step [63/196], loss=7.4901
	step [64/196], loss=5.6314
	step [65/196], loss=6.0423
	step [66/196], loss=6.1420
	step [67/196], loss=6.0357
	step [68/196], loss=5.9060
	step [69/196], loss=5.5207
	step [70/196], loss=6.6703
	step [71/196], loss=6.0481
	step [72/196], loss=6.6872
	step [73/196], loss=5.2516
	step [74/196], loss=5.6243
	step [75/196], loss=7.6154
	step [76/196], loss=6.1426
	step [77/196], loss=6.1894
	step [78/196], loss=6.0893
	step [79/196], loss=6.0144
	step [80/196], loss=6.2639
	step [81/196], loss=7.1149
	step [82/196], loss=4.3943
	step [83/196], loss=5.9892
	step [84/196], loss=6.0759
	step [85/196], loss=6.5884
	step [86/196], loss=5.6455
	step [87/196], loss=6.1162
	step [88/196], loss=5.8581
	step [89/196], loss=6.8434
	step [90/196], loss=6.2296
	step [91/196], loss=6.4618
	step [92/196], loss=5.4947
	step [93/196], loss=6.0054
	step [94/196], loss=6.1443
	step [95/196], loss=7.2064
	step [96/196], loss=6.1250
	step [97/196], loss=5.9132
	step [98/196], loss=5.9838
	step [99/196], loss=6.3579
	step [100/196], loss=5.7003
	step [101/196], loss=5.7229
	step [102/196], loss=6.6499
	step [103/196], loss=6.3371
	step [104/196], loss=5.1578
	step [105/196], loss=6.2760
	step [106/196], loss=5.7018
	step [107/196], loss=5.2891
	step [108/196], loss=6.2112
	step [109/196], loss=7.0633
	step [110/196], loss=6.1781
	step [111/196], loss=5.7392
	step [112/196], loss=5.3105
	step [113/196], loss=5.4399
	step [114/196], loss=5.8445
	step [115/196], loss=4.9977
	step [116/196], loss=6.3586
	step [117/196], loss=5.7356
	step [118/196], loss=7.9100
	step [119/196], loss=5.6651
	step [120/196], loss=6.0322
	step [121/196], loss=5.8747
	step [122/196], loss=6.1605
	step [123/196], loss=6.0748
	step [124/196], loss=6.9709
	step [125/196], loss=6.1903
	step [126/196], loss=6.0581
	step [127/196], loss=6.5291
	step [128/196], loss=6.7463
	step [129/196], loss=5.7206
	step [130/196], loss=6.7536
	step [131/196], loss=5.7536
	step [132/196], loss=5.3277
	step [133/196], loss=6.0869
	step [134/196], loss=5.7414
	step [135/196], loss=5.2539
	step [136/196], loss=5.7464
	step [137/196], loss=5.6314
	step [138/196], loss=5.4673
	step [139/196], loss=6.5355
	step [140/196], loss=7.0247
	step [141/196], loss=5.9286
	step [142/196], loss=5.9979
	step [143/196], loss=5.5921
	step [144/196], loss=6.1198
	step [145/196], loss=6.0929
	step [146/196], loss=5.8103
	step [147/196], loss=5.7005
	step [148/196], loss=5.2941
	step [149/196], loss=6.9462
	step [150/196], loss=5.2565
	step [151/196], loss=5.4521
	step [152/196], loss=5.4980
	step [153/196], loss=7.1591
	step [154/196], loss=5.8483
	step [155/196], loss=6.1308
	step [156/196], loss=5.9821
	step [157/196], loss=5.9398
	step [158/196], loss=5.6883
	step [159/196], loss=6.2934
	step [160/196], loss=5.9016
	step [161/196], loss=5.1770
	step [162/196], loss=6.2823
	step [163/196], loss=6.1402
	step [164/196], loss=5.0472
	step [165/196], loss=5.7049
	step [166/196], loss=5.4268
	step [167/196], loss=5.3451
	step [168/196], loss=5.1248
	step [169/196], loss=7.1484
	step [170/196], loss=5.9075
	step [171/196], loss=6.2461
	step [172/196], loss=5.5345
	step [173/196], loss=6.3800
	step [174/196], loss=4.8492
	step [175/196], loss=7.8264
	step [176/196], loss=5.6771
	step [177/196], loss=6.7099
	step [178/196], loss=6.4677
	step [179/196], loss=5.4319
	step [180/196], loss=5.9397
	step [181/196], loss=5.9457
	step [182/196], loss=5.5854
	step [183/196], loss=4.8248
	step [184/196], loss=5.4850
	step [185/196], loss=5.5936
	step [186/196], loss=6.3056
	step [187/196], loss=5.7528
	step [188/196], loss=5.6683
	step [189/196], loss=5.9581
	step [190/196], loss=5.8544
	step [191/196], loss=5.3189
	step [192/196], loss=6.6347
	step [193/196], loss=5.2183
	step [194/196], loss=6.6266
	step [195/196], loss=5.8606
	step [196/196], loss=1.7299
	Evaluating
	loss=0.0242, precision=0.2332, recall=0.9917, f1=0.3776
Training epoch 30
	step [1/196], loss=6.4234
	step [2/196], loss=5.7570
	step [3/196], loss=6.5168
	step [4/196], loss=6.1209
	step [5/196], loss=6.2601
	step [6/196], loss=5.2556
	step [7/196], loss=7.1765
	step [8/196], loss=5.7896
	step [9/196], loss=6.5836
	step [10/196], loss=6.3165
	step [11/196], loss=6.0309
	step [12/196], loss=6.1914
	step [13/196], loss=4.6345
	step [14/196], loss=5.7534
	step [15/196], loss=6.6097
	step [16/196], loss=5.2267
	step [17/196], loss=5.8159
	step [18/196], loss=6.1178
	step [19/196], loss=6.0095
	step [20/196], loss=5.7759
	step [21/196], loss=5.6497
	step [22/196], loss=6.1159
	step [23/196], loss=6.1794
	step [24/196], loss=5.9951
	step [25/196], loss=6.7209
	step [26/196], loss=6.1477
	step [27/196], loss=5.7069
	step [28/196], loss=6.5472
	step [29/196], loss=6.3320
	step [30/196], loss=5.7508
	step [31/196], loss=5.2056
	step [32/196], loss=7.1069
	step [33/196], loss=5.2749
	step [34/196], loss=6.4129
	step [35/196], loss=5.6372
	step [36/196], loss=5.0495
	step [37/196], loss=6.5436
	step [38/196], loss=5.5670
	step [39/196], loss=6.1868
	step [40/196], loss=5.9708
	step [41/196], loss=4.9078
	step [42/196], loss=5.7412
	step [43/196], loss=6.2359
	step [44/196], loss=5.9587
	step [45/196], loss=5.5403
	step [46/196], loss=6.4929
	step [47/196], loss=6.3603
	step [48/196], loss=6.2797
	step [49/196], loss=5.2104
	step [50/196], loss=6.6152
	step [51/196], loss=6.1555
	step [52/196], loss=6.2466
	step [53/196], loss=6.3799
	step [54/196], loss=6.2849
	step [55/196], loss=4.6604
	step [56/196], loss=6.1509
	step [57/196], loss=4.9180
	step [58/196], loss=6.3440
	step [59/196], loss=6.4043
	step [60/196], loss=6.0714
	step [61/196], loss=5.8933
	step [62/196], loss=6.1850
	step [63/196], loss=5.3092
	step [64/196], loss=5.2388
	step [65/196], loss=5.5422
	step [66/196], loss=5.2603
	step [67/196], loss=5.6246
	step [68/196], loss=6.2689
	step [69/196], loss=5.4579
	step [70/196], loss=5.1782
	step [71/196], loss=5.7376
	step [72/196], loss=5.7447
	step [73/196], loss=5.8787
	step [74/196], loss=5.6781
	step [75/196], loss=5.2379
	step [76/196], loss=6.4534
	step [77/196], loss=6.7601
	step [78/196], loss=6.2075
	step [79/196], loss=7.0745
	step [80/196], loss=5.9963
	step [81/196], loss=5.6511
	step [82/196], loss=5.6875
	step [83/196], loss=5.7314
	step [84/196], loss=6.3287
	step [85/196], loss=6.3655
	step [86/196], loss=4.4471
	step [87/196], loss=6.0190
	step [88/196], loss=6.1179
	step [89/196], loss=5.8856
	step [90/196], loss=6.2766
	step [91/196], loss=4.9832
	step [92/196], loss=5.1475
	step [93/196], loss=5.9491
	step [94/196], loss=7.2885
	step [95/196], loss=6.2058
	step [96/196], loss=5.8685
	step [97/196], loss=6.4966
	step [98/196], loss=6.0250
	step [99/196], loss=5.3818
	step [100/196], loss=5.5957
	step [101/196], loss=5.2941
	step [102/196], loss=7.3928
	step [103/196], loss=5.1942
	step [104/196], loss=6.0178
	step [105/196], loss=5.4428
	step [106/196], loss=6.9023
	step [107/196], loss=5.6385
	step [108/196], loss=6.5059
	step [109/196], loss=5.1601
	step [110/196], loss=5.6525
	step [111/196], loss=5.8262
	step [112/196], loss=6.9234
	step [113/196], loss=6.8653
	step [114/196], loss=5.8222
	step [115/196], loss=6.5137
	step [116/196], loss=5.4308
	step [117/196], loss=6.0379
	step [118/196], loss=5.9395
	step [119/196], loss=5.2776
	step [120/196], loss=6.2120
	step [121/196], loss=5.3942
	step [122/196], loss=5.0943
	step [123/196], loss=6.3029
	step [124/196], loss=6.0267
	step [125/196], loss=5.8513
	step [126/196], loss=5.4340
	step [127/196], loss=5.9621
	step [128/196], loss=4.8420
	step [129/196], loss=5.3141
	step [130/196], loss=6.2126
	step [131/196], loss=5.9732
	step [132/196], loss=5.4569
	step [133/196], loss=6.0215
	step [134/196], loss=6.6163
	step [135/196], loss=5.9987
	step [136/196], loss=5.0406
	step [137/196], loss=6.3452
	step [138/196], loss=5.4235
	step [139/196], loss=5.0596
	step [140/196], loss=6.9164
	step [141/196], loss=5.1839
	step [142/196], loss=6.4724
	step [143/196], loss=6.1356
	step [144/196], loss=4.9701
	step [145/196], loss=6.2145
	step [146/196], loss=5.5683
	step [147/196], loss=5.4508
	step [148/196], loss=5.2340
	step [149/196], loss=5.9289
	step [150/196], loss=6.3582
	step [151/196], loss=6.2474
	step [152/196], loss=5.5331
	step [153/196], loss=4.8887
	step [154/196], loss=4.8804
	step [155/196], loss=6.0927
	step [156/196], loss=5.4337
	step [157/196], loss=4.9429
	step [158/196], loss=5.5192
	step [159/196], loss=5.7776
	step [160/196], loss=5.5101
	step [161/196], loss=5.2182
	step [162/196], loss=6.0745
	step [163/196], loss=5.3139
	step [164/196], loss=6.7629
	step [165/196], loss=5.6488
	step [166/196], loss=6.5406
	step [167/196], loss=4.6049
	step [168/196], loss=6.1427
	step [169/196], loss=5.3212
	step [170/196], loss=5.5422
	step [171/196], loss=6.6594
	step [172/196], loss=8.1899
	step [173/196], loss=6.5664
	step [174/196], loss=6.3274
	step [175/196], loss=5.8661
	step [176/196], loss=5.9111
	step [177/196], loss=5.2824
	step [178/196], loss=7.0860
	step [179/196], loss=5.0889
	step [180/196], loss=6.1231
	step [181/196], loss=6.6052
	step [182/196], loss=5.8539
	step [183/196], loss=5.5920
	step [184/196], loss=6.9119
	step [185/196], loss=5.4010
	step [186/196], loss=6.0049
	step [187/196], loss=6.0982
	step [188/196], loss=6.1697
	step [189/196], loss=4.6244
	step [190/196], loss=5.3974
	step [191/196], loss=5.9880
	step [192/196], loss=6.2861
	step [193/196], loss=6.5501
	step [194/196], loss=5.5912
	step [195/196], loss=5.6072
	step [196/196], loss=1.3345
	Evaluating
	loss=0.0253, precision=0.2333, recall=0.9938, f1=0.3779
Training epoch 31
	step [1/196], loss=5.7600
	step [2/196], loss=5.5887
	step [3/196], loss=4.9860
	step [4/196], loss=4.8723
	step [5/196], loss=6.5687
	step [6/196], loss=6.8559
	step [7/196], loss=5.6988
	step [8/196], loss=4.4058
	step [9/196], loss=6.0025
	step [10/196], loss=5.2810
	step [11/196], loss=5.3600
	step [12/196], loss=6.3105
	step [13/196], loss=5.7792
	step [14/196], loss=6.2856
	step [15/196], loss=5.4774
	step [16/196], loss=4.4920
	step [17/196], loss=4.2174
	step [18/196], loss=4.9257
	step [19/196], loss=5.8576
	step [20/196], loss=6.0288
	step [21/196], loss=4.7999
	step [22/196], loss=5.3353
	step [23/196], loss=6.8897
	step [24/196], loss=6.6996
	step [25/196], loss=5.7170
	step [26/196], loss=5.4673
	step [27/196], loss=7.5061
	step [28/196], loss=5.6397
	step [29/196], loss=6.4004
	step [30/196], loss=5.3102
	step [31/196], loss=4.8750
	step [32/196], loss=6.3759
	step [33/196], loss=4.7331
	step [34/196], loss=5.4040
	step [35/196], loss=6.0268
	step [36/196], loss=6.3580
	step [37/196], loss=6.8690
	step [38/196], loss=6.2337
	step [39/196], loss=6.9768
	step [40/196], loss=5.3266
	step [41/196], loss=6.2709
	step [42/196], loss=6.7802
	step [43/196], loss=5.4267
	step [44/196], loss=5.9223
	step [45/196], loss=5.6616
	step [46/196], loss=5.1087
	step [47/196], loss=5.8681
	step [48/196], loss=5.4002
	step [49/196], loss=6.2245
	step [50/196], loss=6.3745
	step [51/196], loss=5.1447
	step [52/196], loss=4.7707
	step [53/196], loss=5.5562
	step [54/196], loss=5.5149
	step [55/196], loss=5.5117
	step [56/196], loss=7.0758
	step [57/196], loss=5.1882
	step [58/196], loss=5.3596
	step [59/196], loss=6.0568
	step [60/196], loss=5.3786
	step [61/196], loss=6.3951
	step [62/196], loss=6.4313
	step [63/196], loss=5.1819
	step [64/196], loss=5.4872
	step [65/196], loss=6.4702
	step [66/196], loss=6.1976
	step [67/196], loss=5.2998
	step [68/196], loss=6.0062
	step [69/196], loss=5.6693
	step [70/196], loss=5.5707
	step [71/196], loss=4.3681
	step [72/196], loss=5.3849
	step [73/196], loss=6.2443
	step [74/196], loss=5.1833
	step [75/196], loss=6.1824
	step [76/196], loss=5.4903
	step [77/196], loss=5.8752
	step [78/196], loss=5.1725
	step [79/196], loss=5.5937
	step [80/196], loss=6.6017
	step [81/196], loss=6.6048
	step [82/196], loss=6.5910
	step [83/196], loss=5.8878
	step [84/196], loss=5.1204
	step [85/196], loss=5.6973
	step [86/196], loss=6.1779
	step [87/196], loss=5.3427
	step [88/196], loss=5.7665
	step [89/196], loss=6.1515
	step [90/196], loss=5.1737
	step [91/196], loss=5.5191
	step [92/196], loss=6.0778
	step [93/196], loss=6.3494
	step [94/196], loss=5.5795
	step [95/196], loss=6.6399
	step [96/196], loss=4.8769
	step [97/196], loss=5.0665
	step [98/196], loss=5.8192
	step [99/196], loss=6.8219
	step [100/196], loss=5.4067
	step [101/196], loss=5.6127
	step [102/196], loss=5.7394
	step [103/196], loss=4.6905
	step [104/196], loss=5.1153
	step [105/196], loss=4.4590
	step [106/196], loss=6.3261
	step [107/196], loss=6.1194
	step [108/196], loss=5.9311
	step [109/196], loss=4.8109
	step [110/196], loss=5.5580
	step [111/196], loss=5.6586
	step [112/196], loss=6.7041
	step [113/196], loss=6.2822
	step [114/196], loss=6.1575
	step [115/196], loss=5.5129
	step [116/196], loss=5.0935
	step [117/196], loss=6.0261
	step [118/196], loss=6.4446
	step [119/196], loss=4.3044
	step [120/196], loss=5.9488
	step [121/196], loss=5.5244
	step [122/196], loss=5.3681
	step [123/196], loss=6.8169
	step [124/196], loss=5.9698
	step [125/196], loss=5.5725
	step [126/196], loss=7.0147
	step [127/196], loss=6.0022
	step [128/196], loss=6.0144
	step [129/196], loss=6.7515
	step [130/196], loss=5.9797
	step [131/196], loss=6.2182
	step [132/196], loss=5.9527
	step [133/196], loss=5.2818
	step [134/196], loss=7.2446
	step [135/196], loss=5.2779
	step [136/196], loss=6.1689
	step [137/196], loss=6.1328
	step [138/196], loss=5.0994
	step [139/196], loss=4.4054
	step [140/196], loss=6.4456
	step [141/196], loss=4.8728
	step [142/196], loss=5.2134
	step [143/196], loss=5.8453
	step [144/196], loss=4.5625
	step [145/196], loss=6.0960
	step [146/196], loss=6.7832
	step [147/196], loss=6.7376
	step [148/196], loss=6.7573
	step [149/196], loss=6.3436
	step [150/196], loss=6.0229
	step [151/196], loss=4.8032
	step [152/196], loss=6.0164
	step [153/196], loss=5.0840
	step [154/196], loss=5.9184
	step [155/196], loss=6.5186
	step [156/196], loss=5.6115
	step [157/196], loss=5.7824
	step [158/196], loss=5.3660
	step [159/196], loss=5.6843
	step [160/196], loss=5.7966
	step [161/196], loss=5.9370
	step [162/196], loss=6.0677
	step [163/196], loss=5.9489
	step [164/196], loss=5.7643
	step [165/196], loss=5.4381
	step [166/196], loss=5.5134
	step [167/196], loss=5.7902
	step [168/196], loss=5.2481
	step [169/196], loss=5.3063
	step [170/196], loss=4.9583
	step [171/196], loss=4.7600
	step [172/196], loss=5.9453
	step [173/196], loss=5.1887
	step [174/196], loss=4.9546
	step [175/196], loss=5.6102
	step [176/196], loss=5.2060
	step [177/196], loss=5.0500
	step [178/196], loss=5.0057
	step [179/196], loss=5.7860
	step [180/196], loss=6.3438
	step [181/196], loss=5.3229
	step [182/196], loss=4.5644
	step [183/196], loss=5.5782
	step [184/196], loss=6.1837
	step [185/196], loss=4.2840
	step [186/196], loss=5.5063
	step [187/196], loss=6.4433
	step [188/196], loss=5.3187
	step [189/196], loss=5.2169
	step [190/196], loss=5.4963
	step [191/196], loss=5.1604
	step [192/196], loss=5.0543
	step [193/196], loss=5.1519
	step [194/196], loss=6.2575
	step [195/196], loss=6.7741
	step [196/196], loss=1.2675
	Evaluating
	loss=0.0215, precision=0.2560, recall=0.9903, f1=0.4069
saving model as: 0_saved_model.pth
Training epoch 32
	step [1/196], loss=5.0372
	step [2/196], loss=5.3792
	step [3/196], loss=6.2681
	step [4/196], loss=6.5440
	step [5/196], loss=5.6296
	step [6/196], loss=6.0989
	step [7/196], loss=5.8059
	step [8/196], loss=6.8560
	step [9/196], loss=6.1105
	step [10/196], loss=5.1328
	step [11/196], loss=5.7883
	step [12/196], loss=5.1382
	step [13/196], loss=6.3802
	step [14/196], loss=5.3273
	step [15/196], loss=6.5260
	step [16/196], loss=5.8660
	step [17/196], loss=5.4985
	step [18/196], loss=5.5503
	step [19/196], loss=5.6694
	step [20/196], loss=6.3961
	step [21/196], loss=6.1062
	step [22/196], loss=5.5726
	step [23/196], loss=5.2512
	step [24/196], loss=6.4449
	step [25/196], loss=7.3514
	step [26/196], loss=5.2148
	step [27/196], loss=5.6735
	step [28/196], loss=6.4983
	step [29/196], loss=5.5033
	step [30/196], loss=5.5976
	step [31/196], loss=5.6137
	step [32/196], loss=5.5414
	step [33/196], loss=5.3804
	step [34/196], loss=4.4138
	step [35/196], loss=5.1445
	step [36/196], loss=5.7203
	step [37/196], loss=5.5044
	step [38/196], loss=5.1964
	step [39/196], loss=5.1635
	step [40/196], loss=5.4989
	step [41/196], loss=5.3094
	step [42/196], loss=5.6995
	step [43/196], loss=5.1801
	step [44/196], loss=5.5411
	step [45/196], loss=6.5103
	step [46/196], loss=6.3691
	step [47/196], loss=4.9505
	step [48/196], loss=6.1594
	step [49/196], loss=5.1312
	step [50/196], loss=4.8293
	step [51/196], loss=4.8290
	step [52/196], loss=4.7859
	step [53/196], loss=5.6104
	step [54/196], loss=5.5678
	step [55/196], loss=5.0131
	step [56/196], loss=4.7900
	step [57/196], loss=5.5569
	step [58/196], loss=6.1644
	step [59/196], loss=6.2443
	step [60/196], loss=6.2160
	step [61/196], loss=5.4499
	step [62/196], loss=5.3057
	step [63/196], loss=5.9471
	step [64/196], loss=7.1414
	step [65/196], loss=5.6125
	step [66/196], loss=6.0245
	step [67/196], loss=5.3719
	step [68/196], loss=5.6498
	step [69/196], loss=4.8237
	step [70/196], loss=6.4896
	step [71/196], loss=5.1491
	step [72/196], loss=4.6893
	step [73/196], loss=6.4555
	step [74/196], loss=4.9136
	step [75/196], loss=4.5536
	step [76/196], loss=5.4712
	step [77/196], loss=5.3205
	step [78/196], loss=5.5387
	step [79/196], loss=6.2012
	step [80/196], loss=5.5750
	step [81/196], loss=4.7181
	step [82/196], loss=5.3664
	step [83/196], loss=5.1903
	step [84/196], loss=5.8675
	step [85/196], loss=4.7288
	step [86/196], loss=5.1745
	step [87/196], loss=5.2514
	step [88/196], loss=5.4931
	step [89/196], loss=5.1937
	step [90/196], loss=6.3808
	step [91/196], loss=6.7672
	step [92/196], loss=6.1284
	step [93/196], loss=4.6567
	step [94/196], loss=5.3702
	step [95/196], loss=5.2348
	step [96/196], loss=5.2752
	step [97/196], loss=5.1208
	step [98/196], loss=5.5830
	step [99/196], loss=6.1453
	step [100/196], loss=5.3376
	step [101/196], loss=6.3243
	step [102/196], loss=4.7588
	step [103/196], loss=5.3497
	step [104/196], loss=5.4112
	step [105/196], loss=5.1378
	step [106/196], loss=5.2040
	step [107/196], loss=5.9304
	step [108/196], loss=5.2898
	step [109/196], loss=5.9414
	step [110/196], loss=5.1519
	step [111/196], loss=5.2708
	step [112/196], loss=5.4134
	step [113/196], loss=5.9117
	step [114/196], loss=4.7389
	step [115/196], loss=5.6168
	step [116/196], loss=6.1016
	step [117/196], loss=6.6126
	step [118/196], loss=4.9751
	step [119/196], loss=5.2937
	step [120/196], loss=6.2451
	step [121/196], loss=5.8404
	step [122/196], loss=4.8736
	step [123/196], loss=6.0185
	step [124/196], loss=5.2169
	step [125/196], loss=6.2409
	step [126/196], loss=6.7864
	step [127/196], loss=5.6699
	step [128/196], loss=4.7724
	step [129/196], loss=6.2324
	step [130/196], loss=5.8730
	step [131/196], loss=5.3336
	step [132/196], loss=5.8362
	step [133/196], loss=6.3131
	step [134/196], loss=5.8509
	step [135/196], loss=5.6384
	step [136/196], loss=5.8216
	step [137/196], loss=5.8344
	step [138/196], loss=6.5740
	step [139/196], loss=4.8608
	step [140/196], loss=6.2407
	step [141/196], loss=5.8421
	step [142/196], loss=5.2140
	step [143/196], loss=5.8241
	step [144/196], loss=4.9201
	step [145/196], loss=6.7010
	step [146/196], loss=6.2586
	step [147/196], loss=5.7117
	step [148/196], loss=6.3266
	step [149/196], loss=5.8053
	step [150/196], loss=5.5237
	step [151/196], loss=5.4295
	step [152/196], loss=5.1180
	step [153/196], loss=5.7079
	step [154/196], loss=5.3885
	step [155/196], loss=5.8580
	step [156/196], loss=5.8598
	step [157/196], loss=5.2702
	step [158/196], loss=4.7117
	step [159/196], loss=5.4371
	step [160/196], loss=5.4205
	step [161/196], loss=6.2392
	step [162/196], loss=5.6886
	step [163/196], loss=5.5936
	step [164/196], loss=5.5142
	step [165/196], loss=5.8916
	step [166/196], loss=5.7443
	step [167/196], loss=5.1967
	step [168/196], loss=5.5034
	step [169/196], loss=5.2750
	step [170/196], loss=4.9235
	step [171/196], loss=4.5321
	step [172/196], loss=6.5305
	step [173/196], loss=4.6889
	step [174/196], loss=6.2863
	step [175/196], loss=5.1859
	step [176/196], loss=5.1930
	step [177/196], loss=5.7532
	step [178/196], loss=5.7837
	step [179/196], loss=6.6011
	step [180/196], loss=6.1288
	step [181/196], loss=5.3172
	step [182/196], loss=5.6763
	step [183/196], loss=4.9723
	step [184/196], loss=5.7390
	step [185/196], loss=5.5691
	step [186/196], loss=6.1109
	step [187/196], loss=5.4587
	step [188/196], loss=6.2004
	step [189/196], loss=5.5189
	step [190/196], loss=5.1389
	step [191/196], loss=5.0410
	step [192/196], loss=6.9902
	step [193/196], loss=4.9260
	step [194/196], loss=5.1929
	step [195/196], loss=4.8433
	step [196/196], loss=1.3361
	Evaluating
	loss=0.0233, precision=0.2405, recall=0.9930, f1=0.3872
Training epoch 33
	step [1/196], loss=5.6658
	step [2/196], loss=5.1914
	step [3/196], loss=4.3240
	step [4/196], loss=5.4188
	step [5/196], loss=6.3594
	step [6/196], loss=5.7195
	step [7/196], loss=5.4081
	step [8/196], loss=5.8826
	step [9/196], loss=5.4815
	step [10/196], loss=5.9183
	step [11/196], loss=5.4954
	step [12/196], loss=6.2097
	step [13/196], loss=5.5253
	step [14/196], loss=6.4364
	step [15/196], loss=5.2572
	step [16/196], loss=6.2175
	step [17/196], loss=5.6312
	step [18/196], loss=5.4826
	step [19/196], loss=5.6673
	step [20/196], loss=5.2453
	step [21/196], loss=6.0844
	step [22/196], loss=6.0194
	step [23/196], loss=4.3219
	step [24/196], loss=4.5591
	step [25/196], loss=5.2548
	step [26/196], loss=5.6489
	step [27/196], loss=6.0819
	step [28/196], loss=5.2696
	step [29/196], loss=6.1113
	step [30/196], loss=4.8359
	step [31/196], loss=4.9608
	step [32/196], loss=5.8486
	step [33/196], loss=5.6502
	step [34/196], loss=5.2988
	step [35/196], loss=6.0614
	step [36/196], loss=4.8637
	step [37/196], loss=5.6072
	step [38/196], loss=5.0843
	step [39/196], loss=5.7318
	step [40/196], loss=4.9480
	step [41/196], loss=5.5608
	step [42/196], loss=5.5204
	step [43/196], loss=5.4557
	step [44/196], loss=5.6983
	step [45/196], loss=5.4938
	step [46/196], loss=4.9088
	step [47/196], loss=5.4820
	step [48/196], loss=5.9902
	step [49/196], loss=5.8159
	step [50/196], loss=4.8090
	step [51/196], loss=6.8095
	step [52/196], loss=6.5008
	step [53/196], loss=5.6741
	step [54/196], loss=5.7603
	step [55/196], loss=4.4797
	step [56/196], loss=6.0705
	step [57/196], loss=4.6176
	step [58/196], loss=5.4848
	step [59/196], loss=4.7090
	step [60/196], loss=5.7836
	step [61/196], loss=5.6393
	step [62/196], loss=5.7797
	step [63/196], loss=5.4086
	step [64/196], loss=5.8005
	step [65/196], loss=5.2549
	step [66/196], loss=5.1727
	step [67/196], loss=4.8410
	step [68/196], loss=5.2834
	step [69/196], loss=5.5938
	step [70/196], loss=5.7488
	step [71/196], loss=5.0570
	step [72/196], loss=6.1874
	step [73/196], loss=5.4196
	step [74/196], loss=4.8289
	step [75/196], loss=5.7207
	step [76/196], loss=4.6973
	step [77/196], loss=5.8677
	step [78/196], loss=5.1472
	step [79/196], loss=5.1001
	step [80/196], loss=5.4451
	step [81/196], loss=4.8404
	step [82/196], loss=4.5610
	step [83/196], loss=4.5628
	step [84/196], loss=6.1049
	step [85/196], loss=6.1252
	step [86/196], loss=5.4677
	step [87/196], loss=5.3142
	step [88/196], loss=5.6779
	step [89/196], loss=5.8256
	step [90/196], loss=4.9960
	step [91/196], loss=4.5701
	step [92/196], loss=6.2315
	step [93/196], loss=4.4640
	step [94/196], loss=4.8376
	step [95/196], loss=4.9390
	step [96/196], loss=5.2765
	step [97/196], loss=4.7905
	step [98/196], loss=4.6388
	step [99/196], loss=4.3941
	step [100/196], loss=5.3207
	step [101/196], loss=5.3744
	step [102/196], loss=5.6263
	step [103/196], loss=5.9798
	step [104/196], loss=5.6732
	step [105/196], loss=5.6772
	step [106/196], loss=5.2141
	step [107/196], loss=5.9286
	step [108/196], loss=5.1761
	step [109/196], loss=6.2922
	step [110/196], loss=5.1001
	step [111/196], loss=6.1024
	step [112/196], loss=5.4376
	step [113/196], loss=5.7539
	step [114/196], loss=4.7190
	step [115/196], loss=5.5540
	step [116/196], loss=5.4561
	step [117/196], loss=5.0970
	step [118/196], loss=5.1295
	step [119/196], loss=4.9644
	step [120/196], loss=5.4548
	step [121/196], loss=6.3050
	step [122/196], loss=4.6427
	step [123/196], loss=5.6150
	step [124/196], loss=6.0274
	step [125/196], loss=4.4833
	step [126/196], loss=5.7385
	step [127/196], loss=5.6004
	step [128/196], loss=7.3484
	step [129/196], loss=4.9255
	step [130/196], loss=6.0395
	step [131/196], loss=5.7256
	step [132/196], loss=5.4773
	step [133/196], loss=5.8928
	step [134/196], loss=4.8954
	step [135/196], loss=5.0579
	step [136/196], loss=6.1688
	step [137/196], loss=4.9779
	step [138/196], loss=7.1995
	step [139/196], loss=5.7402
	step [140/196], loss=6.3142
	step [141/196], loss=5.8215
	step [142/196], loss=5.7152
	step [143/196], loss=5.6346
	step [144/196], loss=5.7988
	step [145/196], loss=4.2023
	step [146/196], loss=5.6724
	step [147/196], loss=5.3226
	step [148/196], loss=4.3961
	step [149/196], loss=4.9312
	step [150/196], loss=5.0441
	step [151/196], loss=4.6241
	step [152/196], loss=5.1283
	step [153/196], loss=5.4911
	step [154/196], loss=5.3353
	step [155/196], loss=5.7669
	step [156/196], loss=5.5833
	step [157/196], loss=5.5798
	step [158/196], loss=5.1978
	step [159/196], loss=5.2612
	step [160/196], loss=5.3465
	step [161/196], loss=5.4430
	step [162/196], loss=4.5522
	step [163/196], loss=5.3944
	step [164/196], loss=6.1260
	step [165/196], loss=5.6821
	step [166/196], loss=4.2797
	step [167/196], loss=5.0181
	step [168/196], loss=5.2592
	step [169/196], loss=5.5184
	step [170/196], loss=5.6934
	step [171/196], loss=7.1022
	step [172/196], loss=5.0538
	step [173/196], loss=6.4817
	step [174/196], loss=6.1175
	step [175/196], loss=5.1228
	step [176/196], loss=5.1734
	step [177/196], loss=5.9048
	step [178/196], loss=4.8846
	step [179/196], loss=5.5403
	step [180/196], loss=5.9701
	step [181/196], loss=5.2317
	step [182/196], loss=5.9335
	step [183/196], loss=6.0917
	step [184/196], loss=6.0824
	step [185/196], loss=5.5408
	step [186/196], loss=5.6442
	step [187/196], loss=5.5681
	step [188/196], loss=5.5283
	step [189/196], loss=3.8319
	step [190/196], loss=5.8309
	step [191/196], loss=6.0319
	step [192/196], loss=5.8954
	step [193/196], loss=4.9469
	step [194/196], loss=5.3053
	step [195/196], loss=5.6564
	step [196/196], loss=1.3462
	Evaluating
	loss=0.0244, precision=0.2326, recall=0.9935, f1=0.3770
Training epoch 34
	step [1/196], loss=4.8429
	step [2/196], loss=5.3393
	step [3/196], loss=5.3439
	step [4/196], loss=5.4921
	step [5/196], loss=4.6598
	step [6/196], loss=5.6140
	step [7/196], loss=5.4142
	step [8/196], loss=5.7190
	step [9/196], loss=4.8409
	step [10/196], loss=5.4496
	step [11/196], loss=5.7521
	step [12/196], loss=5.7595
	step [13/196], loss=5.6321
	step [14/196], loss=4.6408
	step [15/196], loss=5.5039
	step [16/196], loss=6.0923
	step [17/196], loss=5.8184
	step [18/196], loss=5.7274
	step [19/196], loss=5.5552
	step [20/196], loss=6.2376
	step [21/196], loss=4.9431
	step [22/196], loss=5.7162
	step [23/196], loss=4.9477
	step [24/196], loss=5.5376
	step [25/196], loss=4.6538
	step [26/196], loss=6.3726
	step [27/196], loss=4.3641
	step [28/196], loss=4.9929
	step [29/196], loss=4.6831
	step [30/196], loss=5.4153
	step [31/196], loss=5.5685
	step [32/196], loss=5.4242
	step [33/196], loss=4.6192
	step [34/196], loss=6.1617
	step [35/196], loss=4.9367
	step [36/196], loss=5.9008
	step [37/196], loss=4.9902
	step [38/196], loss=5.4739
	step [39/196], loss=6.3302
	step [40/196], loss=5.2436
	step [41/196], loss=5.9198
	step [42/196], loss=6.2755
	step [43/196], loss=5.4580
	step [44/196], loss=5.1453
	step [45/196], loss=4.9829
	step [46/196], loss=5.7674
	step [47/196], loss=4.8578
	step [48/196], loss=5.4351
	step [49/196], loss=5.0831
	step [50/196], loss=5.0901
	step [51/196], loss=5.2026
	step [52/196], loss=5.4802
	step [53/196], loss=4.6736
	step [54/196], loss=4.7988
	step [55/196], loss=5.1724
	step [56/196], loss=5.5152
	step [57/196], loss=5.8697
	step [58/196], loss=5.0457
	step [59/196], loss=5.5248
	step [60/196], loss=5.4655
	step [61/196], loss=6.1216
	step [62/196], loss=5.4263
	step [63/196], loss=5.6302
	step [64/196], loss=4.8194
	step [65/196], loss=5.3020
	step [66/196], loss=6.1358
	step [67/196], loss=4.9106
	step [68/196], loss=5.6061
	step [69/196], loss=5.3635
	step [70/196], loss=5.4603
	step [71/196], loss=4.6943
	step [72/196], loss=5.2100
	step [73/196], loss=4.8769
	step [74/196], loss=5.7552
	step [75/196], loss=4.7073
	step [76/196], loss=5.5260
	step [77/196], loss=5.1016
	step [78/196], loss=4.2312
	step [79/196], loss=5.3720
	step [80/196], loss=5.8529
	step [81/196], loss=5.4985
	step [82/196], loss=5.6339
	step [83/196], loss=5.2455
	step [84/196], loss=5.5804
	step [85/196], loss=4.9256
	step [86/196], loss=4.5598
	step [87/196], loss=4.6482
	step [88/196], loss=6.1167
	step [89/196], loss=5.2115
	step [90/196], loss=5.8937
	step [91/196], loss=5.1277
	step [92/196], loss=5.2375
	step [93/196], loss=5.0192
	step [94/196], loss=5.7142
	step [95/196], loss=5.2354
	step [96/196], loss=7.2810
	step [97/196], loss=5.3262
	step [98/196], loss=5.4143
	step [99/196], loss=5.5283
	step [100/196], loss=5.4073
	step [101/196], loss=6.0339
	step [102/196], loss=5.2598
	step [103/196], loss=5.1156
	step [104/196], loss=6.2707
	step [105/196], loss=4.9864
	step [106/196], loss=5.5212
	step [107/196], loss=6.0132
	step [108/196], loss=6.4658
	step [109/196], loss=5.8372
	step [110/196], loss=5.8525
	step [111/196], loss=6.4263
	step [112/196], loss=4.4748
	step [113/196], loss=5.3800
	step [114/196], loss=4.5974
	step [115/196], loss=5.8779
	step [116/196], loss=5.3022
	step [117/196], loss=4.6591
	step [118/196], loss=5.2312
	step [119/196], loss=4.2812
	step [120/196], loss=6.2587
	step [121/196], loss=6.1460
	step [122/196], loss=4.7158
	step [123/196], loss=5.5725
	step [124/196], loss=5.4591
	step [125/196], loss=4.9016
	step [126/196], loss=5.3010
	step [127/196], loss=5.2855
	step [128/196], loss=4.9158
	step [129/196], loss=4.3767
	step [130/196], loss=6.4266
	step [131/196], loss=5.5668
	step [132/196], loss=5.1099
	step [133/196], loss=5.1976
	step [134/196], loss=4.6895
	step [135/196], loss=6.0122
	step [136/196], loss=4.9776
	step [137/196], loss=5.0209
	step [138/196], loss=5.4911
	step [139/196], loss=6.0048
	step [140/196], loss=6.0478
	step [141/196], loss=5.2856
	step [142/196], loss=6.0107
	step [143/196], loss=5.2846
	step [144/196], loss=5.6811
	step [145/196], loss=6.2129
	step [146/196], loss=4.9117
	step [147/196], loss=5.5349
	step [148/196], loss=5.0439
	step [149/196], loss=5.2907
	step [150/196], loss=5.3304
	step [151/196], loss=6.0202
	step [152/196], loss=5.5460
	step [153/196], loss=5.7107
	step [154/196], loss=5.5190
	step [155/196], loss=6.0288
	step [156/196], loss=6.0193
	step [157/196], loss=4.8100
	step [158/196], loss=5.8267
	step [159/196], loss=4.8062
	step [160/196], loss=4.9204
	step [161/196], loss=6.2785
	step [162/196], loss=5.0175
	step [163/196], loss=5.4025
	step [164/196], loss=5.3924
	step [165/196], loss=4.8651
	step [166/196], loss=5.6008
	step [167/196], loss=5.1858
	step [168/196], loss=5.1453
	step [169/196], loss=6.0134
	step [170/196], loss=5.3517
	step [171/196], loss=5.4620
	step [172/196], loss=5.6443
	step [173/196], loss=5.3388
	step [174/196], loss=6.6442
	step [175/196], loss=4.5454
	step [176/196], loss=5.7490
	step [177/196], loss=5.6532
	step [178/196], loss=5.4781
	step [179/196], loss=5.5382
	step [180/196], loss=5.0355
	step [181/196], loss=4.9837
	step [182/196], loss=5.2318
	step [183/196], loss=5.1632
	step [184/196], loss=5.8282
	step [185/196], loss=5.4940
	step [186/196], loss=4.2834
	step [187/196], loss=4.9356
	step [188/196], loss=5.3151
	step [189/196], loss=5.1448
	step [190/196], loss=4.6040
	step [191/196], loss=4.5724
	step [192/196], loss=5.6688
	step [193/196], loss=5.9645
	step [194/196], loss=5.4243
	step [195/196], loss=4.5019
	step [196/196], loss=1.3541
	Evaluating
	loss=0.0243, precision=0.2389, recall=0.9914, f1=0.3850
Training epoch 35
	step [1/196], loss=6.6131
	step [2/196], loss=5.4237
	step [3/196], loss=5.2423
	step [4/196], loss=6.6614
	step [5/196], loss=4.8963
	step [6/196], loss=5.0238
	step [7/196], loss=4.0955
	step [8/196], loss=4.9547
	step [9/196], loss=5.6474
	step [10/196], loss=4.0733
	step [11/196], loss=5.2079
	step [12/196], loss=5.0550
	step [13/196], loss=5.8550
	step [14/196], loss=5.4741
	step [15/196], loss=5.1449
	step [16/196], loss=4.8564
	step [17/196], loss=5.4402
	step [18/196], loss=5.8342
	step [19/196], loss=5.8005
	step [20/196], loss=4.4972
	step [21/196], loss=5.0263
	step [22/196], loss=5.8358
	step [23/196], loss=6.2431
	step [24/196], loss=5.9226
	step [25/196], loss=4.3785
	step [26/196], loss=5.4943
	step [27/196], loss=5.8649
	step [28/196], loss=4.5369
	step [29/196], loss=4.9685
	step [30/196], loss=5.7500
	step [31/196], loss=4.3355
	step [32/196], loss=5.6006
	step [33/196], loss=5.9459
	step [34/196], loss=5.8642
	step [35/196], loss=4.7789
	step [36/196], loss=5.6920
	step [37/196], loss=6.0157
	step [38/196], loss=5.3546
	step [39/196], loss=5.9297
	step [40/196], loss=4.9667
	step [41/196], loss=5.6604
	step [42/196], loss=6.4803
	step [43/196], loss=4.6768
	step [44/196], loss=4.5444
	step [45/196], loss=6.2924
	step [46/196], loss=5.3525
	step [47/196], loss=4.3078
	step [48/196], loss=5.1919
	step [49/196], loss=5.5337
	step [50/196], loss=5.5932
	step [51/196], loss=4.7982
	step [52/196], loss=4.9277
	step [53/196], loss=5.7831
	step [54/196], loss=5.4068
	step [55/196], loss=5.1184
	step [56/196], loss=4.9123
	step [57/196], loss=6.4828
	step [58/196], loss=5.3784
	step [59/196], loss=5.3225
	step [60/196], loss=4.8788
	step [61/196], loss=6.0588
	step [62/196], loss=5.3323
	step [63/196], loss=5.4875
	step [64/196], loss=5.4608
	step [65/196], loss=5.9183
	step [66/196], loss=4.5043
	step [67/196], loss=4.5777
	step [68/196], loss=4.6014
	step [69/196], loss=4.5025
	step [70/196], loss=5.0505
	step [71/196], loss=5.0617
	step [72/196], loss=5.0858
	step [73/196], loss=6.4732
	step [74/196], loss=5.6122
	step [75/196], loss=6.1103
	step [76/196], loss=4.7602
	step [77/196], loss=4.9151
	step [78/196], loss=5.0024
	step [79/196], loss=5.3278
	step [80/196], loss=4.9169
	step [81/196], loss=5.9892
	step [82/196], loss=4.1802
	step [83/196], loss=4.8734
	step [84/196], loss=5.5275
	step [85/196], loss=5.4359
	step [86/196], loss=5.3784
	step [87/196], loss=5.4668
	step [88/196], loss=5.9069
	step [89/196], loss=4.7534
	step [90/196], loss=4.6774
	step [91/196], loss=5.8311
	step [92/196], loss=4.5299
	step [93/196], loss=4.6564
	step [94/196], loss=4.1764
	step [95/196], loss=5.1763
	step [96/196], loss=5.2868
	step [97/196], loss=5.3860
	step [98/196], loss=5.1553
	step [99/196], loss=5.3094
	step [100/196], loss=6.7635
	step [101/196], loss=5.3891
	step [102/196], loss=5.5441
	step [103/196], loss=4.8657
	step [104/196], loss=5.1923
	step [105/196], loss=5.0165
	step [106/196], loss=5.5546
	step [107/196], loss=6.8760
	step [108/196], loss=5.3479
	step [109/196], loss=5.5173
	step [110/196], loss=5.4313
	step [111/196], loss=5.6885
	step [112/196], loss=5.8331
	step [113/196], loss=4.9294
	step [114/196], loss=6.1063
	step [115/196], loss=5.0111
	step [116/196], loss=4.7246
	step [117/196], loss=6.1166
	step [118/196], loss=5.1626
	step [119/196], loss=4.8164
	step [120/196], loss=4.5161
	step [121/196], loss=5.5592
	step [122/196], loss=5.0106
	step [123/196], loss=6.0039
	step [124/196], loss=5.0467
	step [125/196], loss=5.4932
	step [126/196], loss=5.1441
	step [127/196], loss=4.5654
	step [128/196], loss=5.0301
	step [129/196], loss=6.4071
	step [130/196], loss=5.3426
	step [131/196], loss=5.2120
	step [132/196], loss=4.4686
	step [133/196], loss=5.0882
	step [134/196], loss=4.9499
	step [135/196], loss=5.8702
	step [136/196], loss=5.0063
	step [137/196], loss=5.0604
	step [138/196], loss=5.0398
	step [139/196], loss=5.8760
	step [140/196], loss=4.7612
	step [141/196], loss=5.1559
	step [142/196], loss=5.7211
	step [143/196], loss=5.5088
	step [144/196], loss=4.6101
	step [145/196], loss=4.2533
	step [146/196], loss=5.6423
	step [147/196], loss=4.4805
	step [148/196], loss=5.7856
	step [149/196], loss=4.8265
	step [150/196], loss=6.0329
	step [151/196], loss=4.9202
	step [152/196], loss=4.7268
	step [153/196], loss=5.2098
	step [154/196], loss=4.5996
	step [155/196], loss=4.9684
	step [156/196], loss=5.5667
	step [157/196], loss=5.6546
	step [158/196], loss=5.8977
	step [159/196], loss=5.0617
	step [160/196], loss=4.4634
	step [161/196], loss=4.4472
	step [162/196], loss=4.9689
	step [163/196], loss=5.1850
	step [164/196], loss=5.8994
	step [165/196], loss=4.9395
	step [166/196], loss=5.6132
	step [167/196], loss=6.1410
	step [168/196], loss=5.2158
	step [169/196], loss=5.1607
	step [170/196], loss=4.4362
	step [171/196], loss=6.1849
	step [172/196], loss=4.6869
	step [173/196], loss=4.6936
	step [174/196], loss=4.7169
	step [175/196], loss=5.6289
	step [176/196], loss=4.7530
	step [177/196], loss=5.0582
	step [178/196], loss=5.6297
	step [179/196], loss=5.2210
	step [180/196], loss=5.5982
	step [181/196], loss=4.7170
	step [182/196], loss=4.4721
	step [183/196], loss=5.3573
	step [184/196], loss=5.1085
	step [185/196], loss=5.2578
	step [186/196], loss=4.8635
	step [187/196], loss=4.5902
	step [188/196], loss=5.5344
	step [189/196], loss=4.5568
	step [190/196], loss=4.8753
	step [191/196], loss=5.2748
	step [192/196], loss=4.4194
	step [193/196], loss=5.5761
	step [194/196], loss=5.1358
	step [195/196], loss=4.8088
	step [196/196], loss=1.3347
	Evaluating
	loss=0.0213, precision=0.2591, recall=0.9905, f1=0.4107
saving model as: 0_saved_model.pth
Training epoch 36
	step [1/196], loss=4.8006
	step [2/196], loss=4.6343
	step [3/196], loss=4.7811
	step [4/196], loss=4.9094
	step [5/196], loss=5.5688
	step [6/196], loss=4.4807
	step [7/196], loss=5.2177
	step [8/196], loss=6.1921
	step [9/196], loss=4.5904
	step [10/196], loss=5.3821
	step [11/196], loss=4.2591
	step [12/196], loss=6.1046
	step [13/196], loss=4.2613
	step [14/196], loss=4.8967
	step [15/196], loss=4.7563
	step [16/196], loss=5.4791
	step [17/196], loss=5.0758
	step [18/196], loss=4.4053
	step [19/196], loss=6.1419
	step [20/196], loss=4.5485
	step [21/196], loss=4.7171
	step [22/196], loss=4.7017
	step [23/196], loss=5.1163
	step [24/196], loss=4.5566
	step [25/196], loss=5.2347
	step [26/196], loss=4.6157
	step [27/196], loss=4.4581
	step [28/196], loss=5.6226
	step [29/196], loss=4.9956
	step [30/196], loss=5.9334
	step [31/196], loss=5.2667
	step [32/196], loss=5.5126
	step [33/196], loss=4.9665
	step [34/196], loss=4.5854
	step [35/196], loss=4.9177
	step [36/196], loss=4.0783
	step [37/196], loss=5.1783
	step [38/196], loss=6.1800
	step [39/196], loss=5.3917
	step [40/196], loss=4.9487
	step [41/196], loss=5.6931
	step [42/196], loss=5.0124
	step [43/196], loss=4.4452
	step [44/196], loss=5.3166
	step [45/196], loss=5.4382
	step [46/196], loss=5.8609
	step [47/196], loss=4.2917
	step [48/196], loss=5.1437
	step [49/196], loss=5.6737
	step [50/196], loss=5.6000
	step [51/196], loss=4.8757
	step [52/196], loss=6.3192
	step [53/196], loss=4.6422
	step [54/196], loss=5.2910
	step [55/196], loss=4.8744
	step [56/196], loss=6.3546
	step [57/196], loss=5.3850
	step [58/196], loss=5.1378
	step [59/196], loss=4.8556
	step [60/196], loss=5.4240
	step [61/196], loss=6.1150
	step [62/196], loss=4.2715
	step [63/196], loss=5.1293
	step [64/196], loss=5.5287
	step [65/196], loss=5.1236
	step [66/196], loss=4.9170
	step [67/196], loss=4.9858
	step [68/196], loss=5.1927
	step [69/196], loss=6.4539
	step [70/196], loss=5.3652
	step [71/196], loss=5.1972
	step [72/196], loss=5.6041
	step [73/196], loss=4.4929
	step [74/196], loss=5.0663
	step [75/196], loss=4.7363
	step [76/196], loss=5.0576
	step [77/196], loss=4.5162
	step [78/196], loss=5.8568
	step [79/196], loss=5.6669
	step [80/196], loss=5.3687
	step [81/196], loss=5.3078
	step [82/196], loss=5.3447
	step [83/196], loss=4.9485
	step [84/196], loss=4.1226
	step [85/196], loss=5.2225
	step [86/196], loss=5.0681
	step [87/196], loss=5.6451
	step [88/196], loss=4.8283
	step [89/196], loss=5.4364
	step [90/196], loss=4.8786
	step [91/196], loss=5.0304
	step [92/196], loss=5.2895
	step [93/196], loss=5.7371
	step [94/196], loss=5.2018
	step [95/196], loss=5.5612
	step [96/196], loss=4.8735
	step [97/196], loss=5.0521
	step [98/196], loss=5.0934
	step [99/196], loss=5.0455
	step [100/196], loss=4.1340
	step [101/196], loss=4.8084
	step [102/196], loss=5.4535
	step [103/196], loss=5.3605
	step [104/196], loss=4.4263
	step [105/196], loss=5.4735
	step [106/196], loss=5.0402
	step [107/196], loss=4.9649
	step [108/196], loss=5.6020
	step [109/196], loss=4.9401
	step [110/196], loss=4.6362
	step [111/196], loss=5.4935
	step [112/196], loss=5.1156
	step [113/196], loss=5.8038
	step [114/196], loss=5.2519
	step [115/196], loss=5.8997
	step [116/196], loss=5.1977
	step [117/196], loss=5.4607
	step [118/196], loss=4.7032
	step [119/196], loss=5.7493
	step [120/196], loss=4.5945
	step [121/196], loss=5.1196
	step [122/196], loss=4.3664
	step [123/196], loss=4.6019
	step [124/196], loss=4.7938
	step [125/196], loss=6.5983
	step [126/196], loss=5.7845
	step [127/196], loss=4.9739
	step [128/196], loss=5.1122
	step [129/196], loss=5.3294
	step [130/196], loss=5.5433
	step [131/196], loss=5.1655
	step [132/196], loss=5.2441
	step [133/196], loss=5.1900
	step [134/196], loss=5.9900
	step [135/196], loss=6.2329
	step [136/196], loss=5.1475
	step [137/196], loss=5.2185
	step [138/196], loss=5.0822
	step [139/196], loss=4.8113
	step [140/196], loss=5.4979
	step [141/196], loss=5.0683
	step [142/196], loss=4.9616
	step [143/196], loss=5.5308
	step [144/196], loss=4.3621
	step [145/196], loss=5.0071
	step [146/196], loss=4.4691
	step [147/196], loss=4.8712
	step [148/196], loss=5.0244
	step [149/196], loss=5.1383
	step [150/196], loss=5.1207
	step [151/196], loss=5.1957
	step [152/196], loss=6.2544
	step [153/196], loss=5.1381
	step [154/196], loss=4.9134
	step [155/196], loss=4.5594
	step [156/196], loss=6.2744
	step [157/196], loss=4.0762
	step [158/196], loss=4.6595
	step [159/196], loss=4.7527
	step [160/196], loss=5.3798
	step [161/196], loss=5.4158
	step [162/196], loss=5.1500
	step [163/196], loss=4.6253
	step [164/196], loss=5.0418
	step [165/196], loss=3.8854
	step [166/196], loss=5.4769
	step [167/196], loss=5.4002
	step [168/196], loss=4.4563
	step [169/196], loss=4.5898
	step [170/196], loss=5.3950
	step [171/196], loss=6.8756
	step [172/196], loss=5.6003
	step [173/196], loss=6.2125
	step [174/196], loss=4.7104
	step [175/196], loss=5.4380
	step [176/196], loss=5.6619
	step [177/196], loss=4.3337
	step [178/196], loss=5.8993
	step [179/196], loss=5.6375
	step [180/196], loss=4.4015
	step [181/196], loss=4.8774
	step [182/196], loss=4.0302
	step [183/196], loss=5.7051
	step [184/196], loss=4.9914
	step [185/196], loss=5.8484
	step [186/196], loss=4.6787
	step [187/196], loss=4.7794
	step [188/196], loss=4.8460
	step [189/196], loss=4.9017
	step [190/196], loss=5.5610
	step [191/196], loss=4.9418
	step [192/196], loss=4.6936
	step [193/196], loss=5.4688
	step [194/196], loss=5.8669
	step [195/196], loss=5.4895
	step [196/196], loss=1.2238
	Evaluating
	loss=0.0210, precision=0.2746, recall=0.9906, f1=0.4300
saving model as: 0_saved_model.pth
Training epoch 37
	step [1/196], loss=5.0350
	step [2/196], loss=4.8400
	step [3/196], loss=5.6466
	step [4/196], loss=4.6514
	step [5/196], loss=4.9800
	step [6/196], loss=4.9818
	step [7/196], loss=5.3682
	step [8/196], loss=5.5757
	step [9/196], loss=4.4613
	step [10/196], loss=5.4601
	step [11/196], loss=6.3121
	step [12/196], loss=4.9346
	step [13/196], loss=5.1964
	step [14/196], loss=4.3995
	step [15/196], loss=5.8060
	step [16/196], loss=5.1156
	step [17/196], loss=4.6608
	step [18/196], loss=6.4366
	step [19/196], loss=4.2442
	step [20/196], loss=5.5720
	step [21/196], loss=4.5526
	step [22/196], loss=5.2124
	step [23/196], loss=4.7382
	step [24/196], loss=5.3454
	step [25/196], loss=5.6810
	step [26/196], loss=4.9958
	step [27/196], loss=5.0754
	step [28/196], loss=5.0250
	step [29/196], loss=4.7865
	step [30/196], loss=4.5984
	step [31/196], loss=5.9302
	step [32/196], loss=4.3787
	step [33/196], loss=6.6121
	step [34/196], loss=5.9325
	step [35/196], loss=5.8679
	step [36/196], loss=6.4323
	step [37/196], loss=6.2104
	step [38/196], loss=6.3114
	step [39/196], loss=5.2596
	step [40/196], loss=5.7766
	step [41/196], loss=6.1469
	step [42/196], loss=6.1512
	step [43/196], loss=5.5681
	step [44/196], loss=5.6876
	step [45/196], loss=5.6510
	step [46/196], loss=5.2298
	step [47/196], loss=5.7068
	step [48/196], loss=6.9317
	step [49/196], loss=6.0363
	step [50/196], loss=5.7655
	step [51/196], loss=5.1565
	step [52/196], loss=4.2728
	step [53/196], loss=4.9163
	step [54/196], loss=5.8034
	step [55/196], loss=5.1773
	step [56/196], loss=6.2332
	step [57/196], loss=6.1129
	step [58/196], loss=5.8033
	step [59/196], loss=5.6724
	step [60/196], loss=6.0974
	step [61/196], loss=5.8256
	step [62/196], loss=6.4467
	step [63/196], loss=5.7668
	step [64/196], loss=5.5051
	step [65/196], loss=4.9227
	step [66/196], loss=4.9909
	step [67/196], loss=6.0855
	step [68/196], loss=5.8199
	step [69/196], loss=5.8943
	step [70/196], loss=5.7120
	step [71/196], loss=5.2933
	step [72/196], loss=5.4060
	step [73/196], loss=5.0374
	step [74/196], loss=4.8500
	step [75/196], loss=5.4371
	step [76/196], loss=5.7522
	step [77/196], loss=5.2255
	step [78/196], loss=4.8640
	step [79/196], loss=5.1171
	step [80/196], loss=5.4344
	step [81/196], loss=4.8631
	step [82/196], loss=4.7031
	step [83/196], loss=5.8395
	step [84/196], loss=5.1561
	step [85/196], loss=5.1405
	step [86/196], loss=5.9986
	step [87/196], loss=6.2730
	step [88/196], loss=4.4135
	step [89/196], loss=5.7143
	step [90/196], loss=4.7256
	step [91/196], loss=5.6978
	step [92/196], loss=5.5843
	step [93/196], loss=6.4040
	step [94/196], loss=5.0922
	step [95/196], loss=4.5057
	step [96/196], loss=4.4853
	step [97/196], loss=6.0744
	step [98/196], loss=4.8695
	step [99/196], loss=5.6229
	step [100/196], loss=5.1722
	step [101/196], loss=4.5469
	step [102/196], loss=5.4033
	step [103/196], loss=4.8532
	step [104/196], loss=4.8259
	step [105/196], loss=6.2721
	step [106/196], loss=5.1541
	step [107/196], loss=4.9367
	step [108/196], loss=4.7108
	step [109/196], loss=5.3284
	step [110/196], loss=4.8638
	step [111/196], loss=4.9905
	step [112/196], loss=4.7129
	step [113/196], loss=5.4565
	step [114/196], loss=5.5351
	step [115/196], loss=5.2154
	step [116/196], loss=4.3777
	step [117/196], loss=4.2905
	step [118/196], loss=5.1268
	step [119/196], loss=5.4058
	step [120/196], loss=4.3691
	step [121/196], loss=5.0398
	step [122/196], loss=5.0066
	step [123/196], loss=4.5580
	step [124/196], loss=5.4719
	step [125/196], loss=5.3219
	step [126/196], loss=5.0261
	step [127/196], loss=4.6013
	step [128/196], loss=4.5391
	step [129/196], loss=5.5530
	step [130/196], loss=4.7130
	step [131/196], loss=4.5392
	step [132/196], loss=4.3197
	step [133/196], loss=5.9440
	step [134/196], loss=4.3857
	step [135/196], loss=5.0002
	step [136/196], loss=5.6768
	step [137/196], loss=4.8542
	step [138/196], loss=5.6534
	step [139/196], loss=5.5860
	step [140/196], loss=5.1957
	step [141/196], loss=6.2944
	step [142/196], loss=5.8995
	step [143/196], loss=4.6631
	step [144/196], loss=5.4236
	step [145/196], loss=6.5026
	step [146/196], loss=5.2189
	step [147/196], loss=3.9965
	step [148/196], loss=5.0867
	step [149/196], loss=5.2686
	step [150/196], loss=4.4242
	step [151/196], loss=5.7275
	step [152/196], loss=5.3594
	step [153/196], loss=4.7434
	step [154/196], loss=5.7023
	step [155/196], loss=4.9573
	step [156/196], loss=5.5152
	step [157/196], loss=4.4734
	step [158/196], loss=5.1608
	step [159/196], loss=5.5745
	step [160/196], loss=5.8507
	step [161/196], loss=5.5783
	step [162/196], loss=4.2136
	step [163/196], loss=4.7793
	step [164/196], loss=5.8769
	step [165/196], loss=4.3619
	step [166/196], loss=5.4969
	step [167/196], loss=5.4333
	step [168/196], loss=5.1524
	step [169/196], loss=4.7610
	step [170/196], loss=5.6322
	step [171/196], loss=5.4494
	step [172/196], loss=4.4620
	step [173/196], loss=5.4115
	step [174/196], loss=5.0263
	step [175/196], loss=5.6050
	step [176/196], loss=5.8513
	step [177/196], loss=6.4342
	step [178/196], loss=5.0214
	step [179/196], loss=5.6145
	step [180/196], loss=5.4593
	step [181/196], loss=4.1262
	step [182/196], loss=5.4356
	step [183/196], loss=6.2127
	step [184/196], loss=5.5821
	step [185/196], loss=4.6631
	step [186/196], loss=5.0361
	step [187/196], loss=4.7176
	step [188/196], loss=4.6283
	step [189/196], loss=4.5985
	step [190/196], loss=4.9893
	step [191/196], loss=4.3145
	step [192/196], loss=6.1455
	step [193/196], loss=4.8388
	step [194/196], loss=4.1205
	step [195/196], loss=5.7052
	step [196/196], loss=1.6325
	Evaluating
	loss=0.0194, precision=0.2751, recall=0.9909, f1=0.4307
saving model as: 0_saved_model.pth
Training finished
best_f1: 0.43069960416651015
directing: Y rim_enhanced: False test_id 0
removed wrong scan: weights_Y_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_171_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_299_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_87_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_241_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_224_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_336_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_350_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_244_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_373_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_308_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_285_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_261_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_243_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_274_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_188_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_218_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_387_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_360_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_372_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_316_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_320_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_160_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_253_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_254_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_204_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_349_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_149_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_283_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_300_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_333_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_221_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_159_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_215_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_363_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_329_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_368_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_254_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_282_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_170_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_211_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_260_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_258_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_231_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_222_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_313_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_282_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_259_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_263_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_177_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_190_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_281_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_187_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_261_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_245_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_315_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_286_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_184_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_312_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_302_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_276_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_157_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_236_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_331_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_214_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_212_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_356_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_167_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_383_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_255_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_224_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_264_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_343_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_375_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_168_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_318_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_367_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_220_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_163_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_255_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_351_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_305_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_330_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_181_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_229_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_309_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_225_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_164_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_242_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_233_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_346_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_326_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_230_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_371_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_275_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_377_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_273_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_197_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_151_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_199_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_332_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_239_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_307_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_380_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_321_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_265_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_150_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_201_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_291_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_353_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_246_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_322_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_310_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_352_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_262_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_297_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_338_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_319_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_179_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_236_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_239_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_215_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_218_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_253_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_293_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_303_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_158_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_203_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_381_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_317_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_226_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_189_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_270_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_294_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_354_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_359_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_266_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_292_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_314_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_218_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_238_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_200_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_262_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_229_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_169_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_271_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_147_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_288_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_244_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_347_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_248_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_323_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_335_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_278_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_233_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_306_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_209_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_365_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_259_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_378_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_298_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_256_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_276_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_193_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_384_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_208_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_237_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_221_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_258_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_339_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_232_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_325_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_238_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_290_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_231_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_369_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_280_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_337_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_216_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_304_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_250_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_194_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_301_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_324_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_340_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_162_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_295_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_366_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_370_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_362_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_379_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_269_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_180_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_289_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_296_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_183_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_271_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_226_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_237_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_341_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_161_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_358_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_254_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_235_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_334_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_376_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_287_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_252_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_205_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_348_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_249_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_251_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_186_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_257_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_80_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_243_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_279_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_268_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_230_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_206_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_165_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_207_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_227_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_260_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_155_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_225_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_311_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_327_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_344_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_267_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_256_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_328_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_153_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_257_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_152_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_176_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_342_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_222_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_263_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_385_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_156_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_386_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_196_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_243_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_195_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_277_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_252_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_232_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_220_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_388_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_210_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_241_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_265_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_146_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_266_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_284_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_355_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_174_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_225_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_345_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_272_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_205_xwqg-B00034_2020-04-03.npy
# all image files: 20333 # all weight files in weight_dir: 15956 # image files with weight 15917
removed wrong scan: weights_Y_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_171_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_299_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_87_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_241_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_224_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_336_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_350_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_244_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_373_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_308_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_285_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_261_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_243_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_274_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_188_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_218_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_387_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_360_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_372_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_316_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_320_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_160_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_253_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_254_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_204_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_349_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_149_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_283_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_300_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_333_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_221_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_159_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_215_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_363_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_329_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_368_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_254_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_282_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_170_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_211_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_260_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_258_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_231_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_222_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_313_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_282_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_259_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_263_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_177_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_190_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_281_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_187_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_261_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_245_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_315_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_286_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_184_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_312_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_302_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_276_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_157_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_236_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_331_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_214_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_212_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_356_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_167_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_383_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_255_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_224_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_264_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_343_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_375_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_168_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_318_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_367_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_220_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_163_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_255_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_351_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_305_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_330_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_181_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_229_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_309_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_225_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_164_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_242_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_233_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_346_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_326_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_230_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_371_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_275_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_377_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_273_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_197_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_151_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_199_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_332_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_239_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_307_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_380_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_321_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_265_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_150_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_201_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_291_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_353_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_246_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_322_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_310_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_352_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_262_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_297_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_338_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_319_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_179_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_236_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_239_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_215_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_218_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_253_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_293_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_303_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_158_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_203_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_381_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_317_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_226_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_189_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_270_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_294_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_354_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_359_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_266_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_292_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_314_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_218_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_238_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_200_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_262_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_229_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_169_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_271_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_147_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_288_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_244_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_347_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_248_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_323_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_335_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_278_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_233_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_306_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_209_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_365_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_259_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_378_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_298_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_256_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_276_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_193_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_384_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_208_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_237_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_221_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_258_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_339_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_232_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_325_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_238_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_290_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_231_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_369_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_280_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_337_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_216_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_304_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_250_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_194_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_301_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_324_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_340_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_162_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_295_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_366_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_370_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_362_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_379_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_269_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_180_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_289_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_296_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_183_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_271_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_226_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_237_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_341_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_161_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_358_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_254_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_235_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_334_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_376_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_287_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_252_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_205_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_348_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_249_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_251_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_186_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_257_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_80_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_243_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_279_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_268_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_230_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_206_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_165_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_207_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_227_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_260_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_155_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_225_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_311_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_327_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_344_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_267_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_256_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_328_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_153_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_257_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_152_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_176_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_342_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_222_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_263_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_385_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_156_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_386_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_196_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_243_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_195_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_277_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_252_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_232_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_220_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_388_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_210_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_241_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_265_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_146_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_266_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_284_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_355_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_174_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_225_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_345_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_272_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_205_xwqg-B00034_2020-04-03.npy
# all image files: 20333 # all weight files in weight_dir: 4127 # image files with weight 4113
train: /ibex/scratch/projects/c2052/air_tube_seg/training_samples_enhanced/Y 15917
Using 4 GPUs
Going to train epochs [1-30]
Training epoch 1
	step [1/332], loss=252.7594
	step [2/332], loss=179.0173
	step [3/332], loss=158.8697
	step [4/332], loss=141.8526
	step [5/332], loss=144.8311
	step [6/332], loss=136.5594
	step [7/332], loss=131.5282
	step [8/332], loss=128.8495
	step [9/332], loss=124.9319
	step [10/332], loss=121.2889
	step [11/332], loss=120.1182
	step [12/332], loss=119.3556
	step [13/332], loss=116.8686
	step [14/332], loss=115.1525
	step [15/332], loss=115.3557
	step [16/332], loss=113.2120
	step [17/332], loss=110.7745
	step [18/332], loss=110.6099
	step [19/332], loss=111.5575
	step [20/332], loss=108.5274
	step [21/332], loss=108.0768
	step [22/332], loss=107.9298
	step [23/332], loss=105.1854
	step [24/332], loss=106.8550
	step [25/332], loss=102.7793
	step [26/332], loss=102.8046
	step [27/332], loss=103.2134
	step [28/332], loss=100.5318
	step [29/332], loss=100.3212
	step [30/332], loss=100.4699
	step [31/332], loss=100.9093
	step [32/332], loss=96.6396
	step [33/332], loss=98.2513
	step [34/332], loss=97.2799
	step [35/332], loss=94.8178
	step [36/332], loss=94.8546
	step [37/332], loss=96.4490
	step [38/332], loss=94.0712
	step [39/332], loss=94.3644
	step [40/332], loss=91.1340
	step [41/332], loss=91.8736
	step [42/332], loss=92.6253
	step [43/332], loss=91.1818
	step [44/332], loss=92.2922
	step [45/332], loss=93.4464
	step [46/332], loss=91.1658
	step [47/332], loss=90.8929
	step [48/332], loss=89.6918
	step [49/332], loss=88.7063
	step [50/332], loss=89.2752
	step [51/332], loss=88.1935
	step [52/332], loss=88.5390
	step [53/332], loss=87.0257
	step [54/332], loss=85.7562
	step [55/332], loss=86.5608
	step [56/332], loss=87.5314
	step [57/332], loss=85.2770
	step [58/332], loss=84.9907
	step [59/332], loss=84.6314
	step [60/332], loss=83.7855
	step [61/332], loss=84.9620
	step [62/332], loss=85.3416
	step [63/332], loss=84.7379
	step [64/332], loss=84.0926
	step [65/332], loss=83.0284
	step [66/332], loss=81.9598
	step [67/332], loss=84.5165
	step [68/332], loss=82.8066
	step [69/332], loss=81.8708
	step [70/332], loss=82.3107
	step [71/332], loss=82.0192
	step [72/332], loss=81.6791
	step [73/332], loss=81.1799
	step [74/332], loss=80.1707
	step [75/332], loss=80.8261
	step [76/332], loss=79.6572
	step [77/332], loss=81.2118
	step [78/332], loss=80.3126
	step [79/332], loss=80.6957
	step [80/332], loss=80.2713
	step [81/332], loss=81.1859
	step [82/332], loss=76.7497
	step [83/332], loss=79.0940
	step [84/332], loss=78.8872
	step [85/332], loss=77.3866
	step [86/332], loss=77.6137
	step [87/332], loss=77.3469
	step [88/332], loss=78.2886
	step [89/332], loss=79.3296
	step [90/332], loss=77.9915
	step [91/332], loss=79.2930
	step [92/332], loss=77.6460
	step [93/332], loss=77.3197
	step [94/332], loss=77.2172
	step [95/332], loss=74.8045
	step [96/332], loss=77.4806
	step [97/332], loss=75.4675
	step [98/332], loss=76.1524
	step [99/332], loss=75.4282
	step [100/332], loss=75.8912
	step [101/332], loss=73.7542
	step [102/332], loss=76.3505
	step [103/332], loss=74.1872
	step [104/332], loss=73.3616
	step [105/332], loss=74.8877
	step [106/332], loss=73.4947
	step [107/332], loss=72.7616
	step [108/332], loss=72.5231
	step [109/332], loss=74.4795
	step [110/332], loss=74.5195
	step [111/332], loss=71.9835
	step [112/332], loss=74.7617
	step [113/332], loss=73.8556
	step [114/332], loss=72.5460
	step [115/332], loss=71.4764
	step [116/332], loss=73.0741
	step [117/332], loss=73.4313
	step [118/332], loss=71.9026
	step [119/332], loss=73.2578
	step [120/332], loss=71.2570
	step [121/332], loss=75.7082
	step [122/332], loss=71.7553
	step [123/332], loss=72.4294
	step [124/332], loss=69.5692
	step [125/332], loss=71.7730
	step [126/332], loss=71.7146
	step [127/332], loss=70.0060
	step [128/332], loss=70.4340
	step [129/332], loss=69.2396
	step [130/332], loss=70.9949
	step [131/332], loss=70.3707
	step [132/332], loss=70.3796
	step [133/332], loss=69.5966
	step [134/332], loss=70.3631
	step [135/332], loss=68.3974
	step [136/332], loss=70.0654
	step [137/332], loss=70.3566
	step [138/332], loss=69.4887
	step [139/332], loss=68.8662
	step [140/332], loss=69.7534
	step [141/332], loss=67.9704
	step [142/332], loss=67.6442
	step [143/332], loss=68.1739
	step [144/332], loss=68.9039
	step [145/332], loss=69.1279
	step [146/332], loss=69.5569
	step [147/332], loss=68.7127
	step [148/332], loss=68.4996
	step [149/332], loss=68.9633
	step [150/332], loss=67.9048
	step [151/332], loss=67.9237
	step [152/332], loss=68.0645
	step [153/332], loss=66.5604
	step [154/332], loss=66.4347
	step [155/332], loss=66.2812
	step [156/332], loss=65.1855
	step [157/332], loss=66.7964
	step [158/332], loss=67.9556
	step [159/332], loss=68.4648
	step [160/332], loss=64.1103
	step [161/332], loss=63.9958
	step [162/332], loss=65.9181
	step [163/332], loss=65.1287
	step [164/332], loss=66.1813
	step [165/332], loss=65.4381
	step [166/332], loss=64.7242
	step [167/332], loss=64.9020
	step [168/332], loss=64.2353
	step [169/332], loss=66.3867
	step [170/332], loss=65.5010
	step [171/332], loss=64.3357
	step [172/332], loss=65.8710
	step [173/332], loss=64.5326
	step [174/332], loss=63.6073
	step [175/332], loss=64.3754
	step [176/332], loss=64.7840
	step [177/332], loss=63.2845
	step [178/332], loss=64.3554
	step [179/332], loss=63.9926
	step [180/332], loss=62.5166
	step [181/332], loss=62.5493
	step [182/332], loss=63.8088
	step [183/332], loss=63.7891
	step [184/332], loss=62.6907
	step [185/332], loss=62.3435
	step [186/332], loss=61.1399
	step [187/332], loss=62.5790
	step [188/332], loss=62.7838
	step [189/332], loss=61.4181
	step [190/332], loss=62.0056
	step [191/332], loss=62.4032
	step [192/332], loss=61.3505
	step [193/332], loss=62.2059
	step [194/332], loss=61.2678
	step [195/332], loss=63.4516
	step [196/332], loss=61.7486
	step [197/332], loss=61.3215
	step [198/332], loss=62.3211
	step [199/332], loss=60.7028
	step [200/332], loss=61.1559
	step [201/332], loss=61.3248
	step [202/332], loss=61.2541
	step [203/332], loss=61.3839
	step [204/332], loss=59.6280
	step [205/332], loss=60.9182
	step [206/332], loss=62.1170
	step [207/332], loss=60.3279
	step [208/332], loss=60.9185
	step [209/332], loss=61.2531
	step [210/332], loss=60.9218
	step [211/332], loss=61.4559
	step [212/332], loss=61.6813
	step [213/332], loss=61.9832
	step [214/332], loss=60.1014
	step [215/332], loss=61.0052
	step [216/332], loss=58.7925
	step [217/332], loss=61.0930
	step [218/332], loss=60.9762
	step [219/332], loss=59.4237
	step [220/332], loss=58.7932
	step [221/332], loss=58.2820
	step [222/332], loss=59.2017
	step [223/332], loss=58.6890
	step [224/332], loss=58.3931
	step [225/332], loss=59.9473
	step [226/332], loss=59.3353
	step [227/332], loss=57.9261
	step [228/332], loss=57.7857
	step [229/332], loss=57.4715
	step [230/332], loss=59.7067
	step [231/332], loss=57.2866
	step [232/332], loss=55.8881
	step [233/332], loss=57.6661
	step [234/332], loss=56.9800
	step [235/332], loss=56.6947
	step [236/332], loss=57.5367
	step [237/332], loss=56.7852
	step [238/332], loss=57.0248
	step [239/332], loss=57.2186
	step [240/332], loss=56.6605
	step [241/332], loss=56.5396
	step [242/332], loss=57.6681
	step [243/332], loss=57.5729
	step [244/332], loss=56.6930
	step [245/332], loss=56.9633
	step [246/332], loss=55.6061
	step [247/332], loss=55.8784
	step [248/332], loss=56.3248
	step [249/332], loss=59.2165
	step [250/332], loss=55.2548
	step [251/332], loss=57.0760
	step [252/332], loss=54.1482
	step [253/332], loss=55.6223
	step [254/332], loss=55.6209
	step [255/332], loss=55.8436
	step [256/332], loss=55.0993
	step [257/332], loss=56.5062
	step [258/332], loss=56.0877
	step [259/332], loss=54.8492
	step [260/332], loss=55.8922
	step [261/332], loss=54.4231
	step [262/332], loss=54.4649
	step [263/332], loss=55.4186
	step [264/332], loss=54.1046
	step [265/332], loss=52.6043
	step [266/332], loss=53.8240
	step [267/332], loss=53.3094
	step [268/332], loss=55.3322
	step [269/332], loss=53.3570
	step [270/332], loss=53.8643
	step [271/332], loss=55.6390
	step [272/332], loss=53.4603
	step [273/332], loss=54.4739
	step [274/332], loss=56.7773
	step [275/332], loss=53.8515
	step [276/332], loss=54.9400
	step [277/332], loss=52.3261
	step [278/332], loss=53.6302
	step [279/332], loss=52.1880
	step [280/332], loss=55.4972
	step [281/332], loss=52.5077
	step [282/332], loss=53.6497
	step [283/332], loss=53.9491
	step [284/332], loss=54.1222
	step [285/332], loss=52.9472
	step [286/332], loss=51.8436
	step [287/332], loss=52.5567
	step [288/332], loss=53.2111
	step [289/332], loss=51.6197
	step [290/332], loss=53.9376
	step [291/332], loss=51.1057
	step [292/332], loss=52.2243
	step [293/332], loss=52.3242
	step [294/332], loss=51.4635
	step [295/332], loss=51.8587
	step [296/332], loss=51.3309
	step [297/332], loss=52.2351
	step [298/332], loss=51.3056
	step [299/332], loss=51.3396
	step [300/332], loss=51.4601
	step [301/332], loss=51.4925
	step [302/332], loss=51.5079
	step [303/332], loss=53.7350
	step [304/332], loss=52.0572
	step [305/332], loss=51.0212
	step [306/332], loss=52.4940
	step [307/332], loss=49.7813
	step [308/332], loss=50.0806
	step [309/332], loss=49.6608
	step [310/332], loss=50.0225
	step [311/332], loss=51.1434
	step [312/332], loss=51.1521
	step [313/332], loss=50.0599
	step [314/332], loss=51.9944
	step [315/332], loss=49.9108
	step [316/332], loss=51.2428
	step [317/332], loss=48.6173
	step [318/332], loss=49.2825
	step [319/332], loss=49.0114
	step [320/332], loss=48.6835
	step [321/332], loss=49.7965
	step [322/332], loss=50.9585
	step [323/332], loss=48.9917
	step [324/332], loss=49.5006
	step [325/332], loss=48.5182
	step [326/332], loss=48.3768
	step [327/332], loss=48.6017
	step [328/332], loss=46.5777
	step [329/332], loss=48.2580
	step [330/332], loss=49.7155
	step [331/332], loss=48.1898
	step [332/332], loss=29.7515
	Evaluating
	loss=0.2409, precision=0.1332, recall=0.9962, f1=0.2350
saving model as: 0_saved_model.pth
Training epoch 2
	step [1/332], loss=48.5161
	step [2/332], loss=51.2671
	step [3/332], loss=49.4411
	step [4/332], loss=47.9436
	step [5/332], loss=49.2064
	step [6/332], loss=47.8156
	step [7/332], loss=48.3535
	step [8/332], loss=46.6307
	step [9/332], loss=46.1569
	step [10/332], loss=47.3337
	step [11/332], loss=47.5466
	step [12/332], loss=46.8355
	step [13/332], loss=48.8107
	step [14/332], loss=48.8566
	step [15/332], loss=47.1577
	step [16/332], loss=47.0578
	step [17/332], loss=46.7862
	step [18/332], loss=47.8923
	step [19/332], loss=46.6458
	step [20/332], loss=47.7105
	step [21/332], loss=46.4364
	step [22/332], loss=47.0829
	step [23/332], loss=47.0240
	step [24/332], loss=46.9773
	step [25/332], loss=46.6398
	step [26/332], loss=45.7259
	step [27/332], loss=45.4467
	step [28/332], loss=45.1771
	step [29/332], loss=47.1427
	step [30/332], loss=46.6339
	step [31/332], loss=45.5664
	step [32/332], loss=46.8480
	step [33/332], loss=46.5307
	step [34/332], loss=45.8177
	step [35/332], loss=46.0721
	step [36/332], loss=45.0186
	step [37/332], loss=45.2384
	step [38/332], loss=45.1194
	step [39/332], loss=45.6819
	step [40/332], loss=45.4709
	step [41/332], loss=46.7381
	step [42/332], loss=44.1391
	step [43/332], loss=44.4035
	step [44/332], loss=43.6945
	step [45/332], loss=46.2762
	step [46/332], loss=44.2842
	step [47/332], loss=44.4538
	step [48/332], loss=45.9329
	step [49/332], loss=44.3207
	step [50/332], loss=47.1103
	step [51/332], loss=44.7850
	step [52/332], loss=44.1406
	step [53/332], loss=45.4291
	step [54/332], loss=43.9021
	step [55/332], loss=44.8559
	step [56/332], loss=43.5787
	step [57/332], loss=44.1250
	step [58/332], loss=44.0680
	step [59/332], loss=44.3814
	step [60/332], loss=44.8481
	step [61/332], loss=43.5649
	step [62/332], loss=44.2006
	step [63/332], loss=43.9645
	step [64/332], loss=43.4323
	step [65/332], loss=43.5795
	step [66/332], loss=43.0789
	step [67/332], loss=42.6547
	step [68/332], loss=43.3241
	step [69/332], loss=43.8319
	step [70/332], loss=43.9381
	step [71/332], loss=42.5073
	step [72/332], loss=43.9465
	step [73/332], loss=42.1931
	step [74/332], loss=42.1206
	step [75/332], loss=42.2565
	step [76/332], loss=41.0822
	step [77/332], loss=44.2097
	step [78/332], loss=42.7529
	step [79/332], loss=41.8808
	step [80/332], loss=42.1920
	step [81/332], loss=43.2542
	step [82/332], loss=43.8311
	step [83/332], loss=41.6722
	step [84/332], loss=41.4448
	step [85/332], loss=42.0607
	step [86/332], loss=41.8019
	step [87/332], loss=41.3632
	step [88/332], loss=42.5652
	step [89/332], loss=42.5067
	step [90/332], loss=42.5163
	step [91/332], loss=40.7193
	step [92/332], loss=42.1777
	step [93/332], loss=41.5899
	step [94/332], loss=45.1382
	step [95/332], loss=42.7694
	step [96/332], loss=40.1995
	step [97/332], loss=41.4667
	step [98/332], loss=40.8590
	step [99/332], loss=41.7953
	step [100/332], loss=40.6493
	step [101/332], loss=41.0897
	step [102/332], loss=41.3188
	step [103/332], loss=40.1300
	step [104/332], loss=39.7378
	step [105/332], loss=40.6786
	step [106/332], loss=40.1712
	step [107/332], loss=39.8529
	step [108/332], loss=40.8656
	step [109/332], loss=40.1569
	step [110/332], loss=38.8807
	step [111/332], loss=40.3756
	step [112/332], loss=40.3195
	step [113/332], loss=40.3152
	step [114/332], loss=41.2784
	step [115/332], loss=39.6948
	step [116/332], loss=39.5397
	step [117/332], loss=41.6748
	step [118/332], loss=40.2744
	step [119/332], loss=39.7223
	step [120/332], loss=39.8217
	step [121/332], loss=38.8431
	step [122/332], loss=39.9529
	step [123/332], loss=39.7632
	step [124/332], loss=39.1665
	step [125/332], loss=40.1452
	step [126/332], loss=39.6543
	step [127/332], loss=40.4357
	step [128/332], loss=38.6576
	step [129/332], loss=38.3924
	step [130/332], loss=39.1522
	step [131/332], loss=39.6704
	step [132/332], loss=38.5827
	step [133/332], loss=38.9390
	step [134/332], loss=40.4486
	step [135/332], loss=38.5860
	step [136/332], loss=38.8081
	step [137/332], loss=38.8627
	step [138/332], loss=40.6277
	step [139/332], loss=39.5823
	step [140/332], loss=38.7385
	step [141/332], loss=38.5886
	step [142/332], loss=39.6594
	step [143/332], loss=37.9951
	step [144/332], loss=39.5027
	step [145/332], loss=37.4262
	step [146/332], loss=38.3729
	step [147/332], loss=37.8029
	step [148/332], loss=37.9588
	step [149/332], loss=38.0489
	step [150/332], loss=38.4092
	step [151/332], loss=36.3542
	step [152/332], loss=39.6218
	step [153/332], loss=38.0598
	step [154/332], loss=37.3095
	step [155/332], loss=37.3488
	step [156/332], loss=37.0682
	step [157/332], loss=37.5382
	step [158/332], loss=35.6380
	step [159/332], loss=37.8151
	step [160/332], loss=38.4470
	step [161/332], loss=36.9790
	step [162/332], loss=37.0373
	step [163/332], loss=37.8479
	step [164/332], loss=36.8316
	step [165/332], loss=36.2272
	step [166/332], loss=35.7860
	step [167/332], loss=39.3305
	step [168/332], loss=36.4702
	step [169/332], loss=36.4715
	step [170/332], loss=36.1088
	step [171/332], loss=36.7030
	step [172/332], loss=37.7271
	step [173/332], loss=36.8245
	step [174/332], loss=36.8267
	step [175/332], loss=35.8940
	step [176/332], loss=36.6685
	step [177/332], loss=35.6584
	step [178/332], loss=36.6659
	step [179/332], loss=35.3255
	step [180/332], loss=36.5110
	step [181/332], loss=37.1120
	step [182/332], loss=39.6548
	step [183/332], loss=35.4987
	step [184/332], loss=36.3862
	step [185/332], loss=37.1971
	step [186/332], loss=36.5936
	step [187/332], loss=37.0148
	step [188/332], loss=34.4028
	step [189/332], loss=35.5500
	step [190/332], loss=34.9809
	step [191/332], loss=37.5416
	step [192/332], loss=35.5122
	step [193/332], loss=34.2021
	step [194/332], loss=35.5420
	step [195/332], loss=35.9409
	step [196/332], loss=36.5043
	step [197/332], loss=35.6727
	step [198/332], loss=36.3676
	step [199/332], loss=35.5440
	step [200/332], loss=36.0776
	step [201/332], loss=35.3469
	step [202/332], loss=35.3772
	step [203/332], loss=34.6030
	step [204/332], loss=34.8445
	step [205/332], loss=35.3614
	step [206/332], loss=34.2634
	step [207/332], loss=36.5945
	step [208/332], loss=34.8429
	step [209/332], loss=33.7710
	step [210/332], loss=36.1825
	step [211/332], loss=33.1880
	step [212/332], loss=34.5946
	step [213/332], loss=34.5486
	step [214/332], loss=35.1161
	step [215/332], loss=33.3231
	step [216/332], loss=33.5268
	step [217/332], loss=36.1788
	step [218/332], loss=34.8073
	step [219/332], loss=34.5052
	step [220/332], loss=35.0498
	step [221/332], loss=34.3063
	step [222/332], loss=35.2226
	step [223/332], loss=34.4978
	step [224/332], loss=34.3590
	step [225/332], loss=33.4517
	step [226/332], loss=33.6110
	step [227/332], loss=33.7244
	step [228/332], loss=32.0916
	step [229/332], loss=33.4450
	step [230/332], loss=33.7221
	step [231/332], loss=35.1034
	step [232/332], loss=35.8601
	step [233/332], loss=33.7910
	step [234/332], loss=33.5780
	step [235/332], loss=33.9107
	step [236/332], loss=33.3119
	step [237/332], loss=34.0877
	step [238/332], loss=33.1935
	step [239/332], loss=33.8761
	step [240/332], loss=33.2024
	step [241/332], loss=33.3005
	step [242/332], loss=32.7811
	step [243/332], loss=33.6034
	step [244/332], loss=33.6978
	step [245/332], loss=33.9702
	step [246/332], loss=33.9488
	step [247/332], loss=33.1756
	step [248/332], loss=33.3159
	step [249/332], loss=33.8783
	step [250/332], loss=32.5885
	step [251/332], loss=33.1054
	step [252/332], loss=34.1655
	step [253/332], loss=31.9700
	step [254/332], loss=33.3058
	step [255/332], loss=31.7468
	step [256/332], loss=31.3796
	step [257/332], loss=31.2516
	step [258/332], loss=32.8669
	step [259/332], loss=32.6586
	step [260/332], loss=32.1166
	step [261/332], loss=32.3400
	step [262/332], loss=31.7282
	step [263/332], loss=33.2560
	step [264/332], loss=30.6840
	step [265/332], loss=33.0161
	step [266/332], loss=32.0982
	step [267/332], loss=32.2408
	step [268/332], loss=33.4244
	step [269/332], loss=33.8432
	step [270/332], loss=31.5681
	step [271/332], loss=30.9783
	step [272/332], loss=31.3626
	step [273/332], loss=31.8160
	step [274/332], loss=31.3309
	step [275/332], loss=33.0911
	step [276/332], loss=31.2443
	step [277/332], loss=31.1195
	step [278/332], loss=31.6810
	step [279/332], loss=31.6249
	step [280/332], loss=31.1278
	step [281/332], loss=32.7365
	step [282/332], loss=32.6408
	step [283/332], loss=31.2928
	step [284/332], loss=30.9169
	step [285/332], loss=32.2256
	step [286/332], loss=31.5754
	step [287/332], loss=33.4093
	step [288/332], loss=31.8374
	step [289/332], loss=31.3305
	step [290/332], loss=31.0284
	step [291/332], loss=30.1629
	step [292/332], loss=31.0127
	step [293/332], loss=30.1622
	step [294/332], loss=32.3405
	step [295/332], loss=31.4192
	step [296/332], loss=31.5949
	step [297/332], loss=31.6270
	step [298/332], loss=29.8705
	step [299/332], loss=31.0026
	step [300/332], loss=29.2383
	step [301/332], loss=30.1344
	step [302/332], loss=30.5610
	step [303/332], loss=30.9331
	step [304/332], loss=29.9257
	step [305/332], loss=29.0369
	step [306/332], loss=29.6481
	step [307/332], loss=29.0010
	step [308/332], loss=30.5554
	step [309/332], loss=30.4921
	step [310/332], loss=29.9481
	step [311/332], loss=31.5782
	step [312/332], loss=28.8173
	step [313/332], loss=30.8148
	step [314/332], loss=29.3230
	step [315/332], loss=29.4835
	step [316/332], loss=28.8341
	step [317/332], loss=31.6015
	step [318/332], loss=31.1345
	step [319/332], loss=28.0895
	step [320/332], loss=29.6790
	step [321/332], loss=29.3364
	step [322/332], loss=29.8390
	step [323/332], loss=31.5984
	step [324/332], loss=29.5762
	step [325/332], loss=29.4583
	step [326/332], loss=29.8762
	step [327/332], loss=30.5380
	step [328/332], loss=29.5058
	step [329/332], loss=27.6290
	step [330/332], loss=29.0468
	step [331/332], loss=28.8627
	step [332/332], loss=18.3677
	Evaluating
	loss=0.1424, precision=0.1344, recall=0.9965, f1=0.2368
saving model as: 0_saved_model.pth
Training epoch 3
	step [1/332], loss=29.1097
	step [2/332], loss=29.7965
	step [3/332], loss=30.6505
	step [4/332], loss=27.8078
	step [5/332], loss=27.9809
	step [6/332], loss=28.8368
	step [7/332], loss=30.1303
	step [8/332], loss=28.9243
	step [9/332], loss=29.1236
	step [10/332], loss=27.9087
	step [11/332], loss=28.0597
	step [12/332], loss=28.7834
	step [13/332], loss=27.8837
	step [14/332], loss=28.3440
	step [15/332], loss=29.0033
	step [16/332], loss=29.5841
	step [17/332], loss=27.2059
	step [18/332], loss=27.6260
	step [19/332], loss=26.7979
	step [20/332], loss=28.9405
	step [21/332], loss=32.1373
	step [22/332], loss=26.6182
	step [23/332], loss=27.2560
	step [24/332], loss=27.6423
	step [25/332], loss=27.2109
	step [26/332], loss=28.2946
	step [27/332], loss=28.6135
	step [28/332], loss=26.8914
	step [29/332], loss=27.6883
	step [30/332], loss=27.8679
	step [31/332], loss=27.3011
	step [32/332], loss=26.6967
	step [33/332], loss=27.7354
	step [34/332], loss=29.0117
	step [35/332], loss=27.2681
	step [36/332], loss=27.7688
	step [37/332], loss=27.7751
	step [38/332], loss=27.8583
	step [39/332], loss=27.1824
	step [40/332], loss=27.7036
	step [41/332], loss=26.7442
	step [42/332], loss=27.0083
	step [43/332], loss=26.9002
	step [44/332], loss=28.0788
	step [45/332], loss=27.5825
	step [46/332], loss=28.4486
	step [47/332], loss=28.9698
	step [48/332], loss=27.6222
	step [49/332], loss=26.2382
	step [50/332], loss=26.7902
	step [51/332], loss=27.4737
	step [52/332], loss=26.4336
	step [53/332], loss=28.6499
	step [54/332], loss=25.8707
	step [55/332], loss=28.2438
	step [56/332], loss=26.9868
	step [57/332], loss=27.4710
	step [58/332], loss=27.6544
	step [59/332], loss=28.7605
	step [60/332], loss=27.6203
	step [61/332], loss=28.5574
	step [62/332], loss=26.2785
	step [63/332], loss=26.2753
	step [64/332], loss=26.8504
	step [65/332], loss=28.5033
	step [66/332], loss=26.7228
	step [67/332], loss=26.8346
	step [68/332], loss=27.4236
	step [69/332], loss=27.1511
	step [70/332], loss=25.8139
	step [71/332], loss=24.9500
	step [72/332], loss=28.4520
	step [73/332], loss=26.9964
	step [74/332], loss=26.7149
	step [75/332], loss=25.3381
	step [76/332], loss=26.8410
	step [77/332], loss=27.3566
	step [78/332], loss=26.8062
	step [79/332], loss=26.4622
	step [80/332], loss=25.8656
	step [81/332], loss=27.4056
	step [82/332], loss=24.7867
	step [83/332], loss=26.0773
	step [84/332], loss=27.2150
	step [85/332], loss=25.1909
	step [86/332], loss=26.6778
	step [87/332], loss=26.4927
	step [88/332], loss=25.1423
	step [89/332], loss=25.5579
	step [90/332], loss=28.2592
	step [91/332], loss=26.8996
	step [92/332], loss=27.2669
	step [93/332], loss=27.4459
	step [94/332], loss=28.0815
	step [95/332], loss=25.5884
	step [96/332], loss=24.5104
	step [97/332], loss=24.8448
	step [98/332], loss=26.6457
	step [99/332], loss=26.9179
	step [100/332], loss=26.1721
	step [101/332], loss=27.0579
	step [102/332], loss=25.4918
	step [103/332], loss=26.8493
	step [104/332], loss=26.2269
	step [105/332], loss=24.6133
	step [106/332], loss=26.8550
	step [107/332], loss=25.9016
	step [108/332], loss=25.9005
	step [109/332], loss=27.1069
	step [110/332], loss=26.5849
	step [111/332], loss=25.0503
	step [112/332], loss=24.7073
	step [113/332], loss=24.4443
	step [114/332], loss=27.3976
	step [115/332], loss=26.9091
	step [116/332], loss=26.3509
	step [117/332], loss=25.6160
	step [118/332], loss=24.6723
	step [119/332], loss=26.0789
	step [120/332], loss=25.7036
	step [121/332], loss=24.2781
	step [122/332], loss=25.0236
	step [123/332], loss=24.9854
	step [124/332], loss=24.9954
	step [125/332], loss=24.2434
	step [126/332], loss=25.0073
	step [127/332], loss=24.6835
	step [128/332], loss=24.1412
	step [129/332], loss=25.2242
	step [130/332], loss=24.8552
	step [131/332], loss=23.3270
	step [132/332], loss=24.8588
	step [133/332], loss=24.2690
	step [134/332], loss=24.1625
	step [135/332], loss=24.5080
	step [136/332], loss=24.3804
	step [137/332], loss=23.8486
	step [138/332], loss=24.1520
	step [139/332], loss=24.0275
	step [140/332], loss=25.9254
	step [141/332], loss=24.8297
	step [142/332], loss=23.8087
	step [143/332], loss=24.3037
	step [144/332], loss=23.1142
	step [145/332], loss=25.5397
	step [146/332], loss=24.3745
	step [147/332], loss=24.8396
	step [148/332], loss=24.1683
	step [149/332], loss=24.4050
	step [150/332], loss=24.6184
	step [151/332], loss=24.6342
	step [152/332], loss=23.9804
	step [153/332], loss=22.8017
	step [154/332], loss=24.3443
	step [155/332], loss=23.7906
	step [156/332], loss=25.3677
	step [157/332], loss=25.0119
	step [158/332], loss=22.7396
	step [159/332], loss=23.7292
	step [160/332], loss=22.6038
	step [161/332], loss=23.9872
	step [162/332], loss=23.7263
	step [163/332], loss=22.9005
	step [164/332], loss=24.8511
	step [165/332], loss=24.3927
	step [166/332], loss=24.5673
	step [167/332], loss=23.0570
	step [168/332], loss=24.5542
	step [169/332], loss=24.7426
	step [170/332], loss=22.9316
	step [171/332], loss=23.7594
	step [172/332], loss=21.6611
	step [173/332], loss=23.9916
	step [174/332], loss=23.7276
	step [175/332], loss=24.8175
	step [176/332], loss=23.0773
	step [177/332], loss=23.6234
	step [178/332], loss=23.7154
	step [179/332], loss=23.3086
	step [180/332], loss=23.4738
	step [181/332], loss=23.9448
	step [182/332], loss=23.1184
	step [183/332], loss=23.4429
	step [184/332], loss=25.2389
	step [185/332], loss=24.3360
	step [186/332], loss=21.6818
	step [187/332], loss=22.7640
	step [188/332], loss=22.6358
	step [189/332], loss=22.3093
	step [190/332], loss=25.1359
	step [191/332], loss=23.2258
	step [192/332], loss=24.7423
	step [193/332], loss=23.7268
	step [194/332], loss=23.3590
	step [195/332], loss=23.3651
	step [196/332], loss=23.9366
	step [197/332], loss=24.0881
	step [198/332], loss=23.2434
	step [199/332], loss=22.6869
	step [200/332], loss=23.4814
	step [201/332], loss=24.2110
	step [202/332], loss=24.1611
	step [203/332], loss=22.3003
	step [204/332], loss=22.3985
	step [205/332], loss=22.6435
	step [206/332], loss=22.7048
	step [207/332], loss=22.8571
	step [208/332], loss=23.5646
	step [209/332], loss=22.5448
	step [210/332], loss=23.4524
	step [211/332], loss=24.0083
	step [212/332], loss=20.6290
	step [213/332], loss=23.1270
	step [214/332], loss=21.2003
	step [215/332], loss=22.0890
	step [216/332], loss=20.5298
	step [217/332], loss=22.4385
	step [218/332], loss=20.6763
	step [219/332], loss=22.6854
	step [220/332], loss=20.8441
	step [221/332], loss=22.7098
	step [222/332], loss=21.3897
	step [223/332], loss=23.2677
	step [224/332], loss=22.1773
	step [225/332], loss=25.2547
	step [226/332], loss=21.7521
	step [227/332], loss=21.0394
	step [228/332], loss=21.6267
	step [229/332], loss=23.0654
	step [230/332], loss=24.1643
	step [231/332], loss=22.3471
	step [232/332], loss=21.0108
	step [233/332], loss=22.2564
	step [234/332], loss=20.9413
	step [235/332], loss=23.4018
	step [236/332], loss=21.3909
	step [237/332], loss=23.4984
	step [238/332], loss=21.9562
	step [239/332], loss=22.7929
	step [240/332], loss=21.8676
	step [241/332], loss=20.9226
	step [242/332], loss=21.4905
	step [243/332], loss=20.5419
	step [244/332], loss=20.3391
	step [245/332], loss=20.2767
	step [246/332], loss=22.4281
	step [247/332], loss=21.3873
	step [248/332], loss=21.0924
	step [249/332], loss=21.6230
	step [250/332], loss=21.5461
	step [251/332], loss=20.1882
	step [252/332], loss=20.6992
	step [253/332], loss=21.5732
	step [254/332], loss=22.2200
	step [255/332], loss=21.3882
	step [256/332], loss=22.1267
	step [257/332], loss=24.9189
	step [258/332], loss=22.9657
	step [259/332], loss=21.1277
	step [260/332], loss=20.9043
	step [261/332], loss=20.9953
	step [262/332], loss=20.8584
	step [263/332], loss=21.4438
	step [264/332], loss=21.4705
	step [265/332], loss=22.0683
	step [266/332], loss=20.6580
	step [267/332], loss=20.0195
	step [268/332], loss=22.2323
	step [269/332], loss=21.0612
	step [270/332], loss=21.7380
	step [271/332], loss=21.7093
	step [272/332], loss=20.1709
	step [273/332], loss=20.8222
	step [274/332], loss=21.9775
	step [275/332], loss=20.9567
	step [276/332], loss=20.7828
	step [277/332], loss=20.9264
	step [278/332], loss=21.3484
	step [279/332], loss=20.1220
	step [280/332], loss=20.9022
	step [281/332], loss=20.1285
	step [282/332], loss=22.2633
	step [283/332], loss=22.5633
	step [284/332], loss=20.8557
	step [285/332], loss=20.8776
	step [286/332], loss=20.4317
	step [287/332], loss=21.5409
	step [288/332], loss=21.1952
	step [289/332], loss=21.7493
	step [290/332], loss=23.2658
	step [291/332], loss=20.1801
	step [292/332], loss=20.7221
	step [293/332], loss=21.6490
	step [294/332], loss=19.8843
	step [295/332], loss=20.1485
	step [296/332], loss=20.2947
	step [297/332], loss=20.4176
	step [298/332], loss=20.4875
	step [299/332], loss=22.5487
	step [300/332], loss=20.1781
	step [301/332], loss=20.1666
	step [302/332], loss=19.3615
	step [303/332], loss=21.6388
	step [304/332], loss=19.7203
	step [305/332], loss=21.7329
	step [306/332], loss=21.3812
	step [307/332], loss=21.8278
	step [308/332], loss=20.2463
	step [309/332], loss=20.9206
	step [310/332], loss=19.1276
	step [311/332], loss=20.6041
	step [312/332], loss=21.1617
	step [313/332], loss=21.1760
	step [314/332], loss=19.4672
	step [315/332], loss=20.1426
	step [316/332], loss=20.3798
	step [317/332], loss=19.1695
	step [318/332], loss=21.1035
	step [319/332], loss=20.0041
	step [320/332], loss=19.0938
	step [321/332], loss=19.9534
	step [322/332], loss=20.1780
	step [323/332], loss=19.2529
	step [324/332], loss=19.6719
	step [325/332], loss=19.7906
	step [326/332], loss=18.3273
	step [327/332], loss=20.5410
	step [328/332], loss=18.7115
	step [329/332], loss=19.4209
	step [330/332], loss=20.6929
	step [331/332], loss=21.6888
	step [332/332], loss=10.6267
	Evaluating
	loss=0.0925, precision=0.1435, recall=0.9965, f1=0.2509
saving model as: 0_saved_model.pth
Training epoch 4
	step [1/332], loss=20.7888
	step [2/332], loss=21.1319
	step [3/332], loss=20.0465
	step [4/332], loss=19.2913
	step [5/332], loss=19.6996
	step [6/332], loss=20.9917
	step [7/332], loss=18.0262
	step [8/332], loss=21.3863
	step [9/332], loss=20.4782
	step [10/332], loss=20.8674
	step [11/332], loss=19.9993
	step [12/332], loss=19.8248
	step [13/332], loss=19.0428
	step [14/332], loss=21.2802
	step [15/332], loss=21.1735
	step [16/332], loss=19.2394
	step [17/332], loss=21.0036
	step [18/332], loss=19.9798
	step [19/332], loss=18.7119
	step [20/332], loss=19.6561
	step [21/332], loss=20.1645
	step [22/332], loss=19.9467
	step [23/332], loss=19.6943
	step [24/332], loss=20.2531
	step [25/332], loss=18.9491
	step [26/332], loss=20.3033
	step [27/332], loss=19.1495
	step [28/332], loss=18.5504
	step [29/332], loss=19.3109
	step [30/332], loss=20.2639
	step [31/332], loss=18.8414
	step [32/332], loss=18.9147
	step [33/332], loss=21.1929
	step [34/332], loss=19.4217
	step [35/332], loss=19.6377
	step [36/332], loss=18.4583
	step [37/332], loss=19.0503
	step [38/332], loss=20.1906
	step [39/332], loss=19.5382
	step [40/332], loss=17.8938
	step [41/332], loss=19.6310
	step [42/332], loss=19.7664
	step [43/332], loss=20.6733
	step [44/332], loss=20.3458
	step [45/332], loss=18.8403
	step [46/332], loss=20.0148
	step [47/332], loss=17.4506
	step [48/332], loss=20.9848
	step [49/332], loss=18.0986
	step [50/332], loss=17.1996
	step [51/332], loss=17.8327
	step [52/332], loss=20.0202
	step [53/332], loss=17.5563
	step [54/332], loss=21.1114
	step [55/332], loss=18.3669
	step [56/332], loss=18.6898
	step [57/332], loss=17.5196
	step [58/332], loss=20.4285
	step [59/332], loss=17.7252
	step [60/332], loss=17.0720
	step [61/332], loss=17.0377
	step [62/332], loss=18.7786
	step [63/332], loss=18.0668
	step [64/332], loss=19.5493
	step [65/332], loss=17.1706
	step [66/332], loss=18.6386
	step [67/332], loss=17.3947
	step [68/332], loss=19.5133
	step [69/332], loss=18.4603
	step [70/332], loss=18.7795
	step [71/332], loss=19.4292
	step [72/332], loss=18.7375
	step [73/332], loss=18.6101
	step [74/332], loss=18.8040
	step [75/332], loss=18.7772
	step [76/332], loss=17.7765
	step [77/332], loss=17.8489
	step [78/332], loss=18.9481
	step [79/332], loss=19.2048
	step [80/332], loss=17.8233
	step [81/332], loss=18.9483
	step [82/332], loss=20.0068
	step [83/332], loss=18.2755
	step [84/332], loss=16.7306
	step [85/332], loss=17.5268
	step [86/332], loss=18.2648
	step [87/332], loss=18.4415
	step [88/332], loss=21.0917
	step [89/332], loss=17.3225
	step [90/332], loss=18.2025
	step [91/332], loss=18.0270
	step [92/332], loss=18.2360
	step [93/332], loss=19.2365
	step [94/332], loss=17.1283
	step [95/332], loss=19.8937
	step [96/332], loss=17.1025
	step [97/332], loss=17.3934
	step [98/332], loss=18.6068
	step [99/332], loss=18.0702
	step [100/332], loss=17.2096
	step [101/332], loss=17.3469
	step [102/332], loss=18.3293
	step [103/332], loss=20.3720
	step [104/332], loss=17.5267
	step [105/332], loss=17.2633
	step [106/332], loss=17.5670
	step [107/332], loss=17.0859
	step [108/332], loss=19.0253
	step [109/332], loss=17.2027
	step [110/332], loss=16.8859
	step [111/332], loss=17.1543
	step [112/332], loss=17.3684
	step [113/332], loss=17.6445
	step [114/332], loss=17.2112
	step [115/332], loss=18.1084
	step [116/332], loss=17.2196
	step [117/332], loss=18.2740
	step [118/332], loss=19.9171
	step [119/332], loss=17.1794
	step [120/332], loss=16.4594
	step [121/332], loss=16.7712
	step [122/332], loss=19.1030
	step [123/332], loss=20.4415
	step [124/332], loss=16.9304
	step [125/332], loss=16.0591
	step [126/332], loss=18.4624
	step [127/332], loss=19.0056
	step [128/332], loss=19.2218
	step [129/332], loss=19.2767
	step [130/332], loss=17.5500
	step [131/332], loss=17.3828
	step [132/332], loss=16.8620
	step [133/332], loss=17.2296
	step [134/332], loss=18.1712
	step [135/332], loss=17.0245
	step [136/332], loss=17.4302
	step [137/332], loss=17.8119
	step [138/332], loss=15.5070
	step [139/332], loss=18.3088
	step [140/332], loss=17.7202
	step [141/332], loss=16.9308
	step [142/332], loss=16.5570
	step [143/332], loss=17.3246
	step [144/332], loss=18.8293
	step [145/332], loss=16.0848
	step [146/332], loss=16.5377
	step [147/332], loss=19.0241
	step [148/332], loss=17.2974
	step [149/332], loss=16.2827
	step [150/332], loss=17.3031
	step [151/332], loss=17.1983
	step [152/332], loss=16.2084
	step [153/332], loss=16.7307
	step [154/332], loss=19.8479
	step [155/332], loss=17.7396
	step [156/332], loss=16.7460
	step [157/332], loss=16.0443
	step [158/332], loss=16.2263
	step [159/332], loss=16.2231
	step [160/332], loss=16.5406
	step [161/332], loss=18.7402
	step [162/332], loss=17.2158
	step [163/332], loss=17.3088
	step [164/332], loss=15.1998
	step [165/332], loss=17.1852
	step [166/332], loss=18.0709
	step [167/332], loss=17.1730
	step [168/332], loss=17.6893
	step [169/332], loss=15.4769
	step [170/332], loss=16.7536
	step [171/332], loss=15.4870
	step [172/332], loss=15.6546
	step [173/332], loss=15.6093
	step [174/332], loss=16.4746
	step [175/332], loss=15.7824
	step [176/332], loss=18.0846
	step [177/332], loss=15.8480
	step [178/332], loss=15.1176
	step [179/332], loss=16.3060
	step [180/332], loss=18.4894
	step [181/332], loss=15.8488
	step [182/332], loss=15.6813
	step [183/332], loss=17.4027
	step [184/332], loss=15.4916
	step [185/332], loss=16.6085
	step [186/332], loss=17.0566
	step [187/332], loss=18.7497
	step [188/332], loss=14.7152
	step [189/332], loss=16.4026
	step [190/332], loss=18.1055
	step [191/332], loss=16.3014
	step [192/332], loss=15.2080
	step [193/332], loss=16.8827
	step [194/332], loss=15.4916
	step [195/332], loss=16.4887
	step [196/332], loss=18.1149
	step [197/332], loss=17.3735
	step [198/332], loss=16.4299
	step [199/332], loss=17.8117
	step [200/332], loss=16.0197
	step [201/332], loss=17.4584
	step [202/332], loss=16.7762
	step [203/332], loss=17.2633
	step [204/332], loss=16.3366
	step [205/332], loss=14.8931
	step [206/332], loss=16.6903
	step [207/332], loss=15.4479
	step [208/332], loss=15.5086
	step [209/332], loss=16.2952
	step [210/332], loss=16.6323
	step [211/332], loss=17.9745
	step [212/332], loss=16.4995
	step [213/332], loss=15.3028
	step [214/332], loss=15.8341
	step [215/332], loss=15.4872
	step [216/332], loss=14.1976
	step [217/332], loss=14.9871
	step [218/332], loss=16.1692
	step [219/332], loss=14.9645
	step [220/332], loss=15.5147
	step [221/332], loss=14.9015
	step [222/332], loss=15.8075
	step [223/332], loss=15.5441
	step [224/332], loss=16.4770
	step [225/332], loss=16.7868
	step [226/332], loss=15.4996
	step [227/332], loss=17.2409
	step [228/332], loss=15.2961
	step [229/332], loss=16.6418
	step [230/332], loss=18.3530
	step [231/332], loss=16.2339
	step [232/332], loss=16.1136
	step [233/332], loss=16.9882
	step [234/332], loss=16.4310
	step [235/332], loss=15.6798
	step [236/332], loss=15.9024
	step [237/332], loss=15.7163
	step [238/332], loss=17.6009
	step [239/332], loss=15.3094
	step [240/332], loss=17.3983
	step [241/332], loss=16.2331
	step [242/332], loss=18.1179
	step [243/332], loss=15.1958
	step [244/332], loss=15.3444
	step [245/332], loss=16.5876
	step [246/332], loss=15.7666
	step [247/332], loss=15.4033
	step [248/332], loss=14.3863
	step [249/332], loss=15.1183
	step [250/332], loss=15.3539
	step [251/332], loss=15.0798
	step [252/332], loss=14.9376
	step [253/332], loss=14.9615
	step [254/332], loss=15.2757
	step [255/332], loss=18.1265
	step [256/332], loss=16.0150
	step [257/332], loss=16.2354
	step [258/332], loss=15.7917
	step [259/332], loss=16.1424
	step [260/332], loss=15.5099
	step [261/332], loss=16.9868
	step [262/332], loss=15.2733
	step [263/332], loss=17.6888
	step [264/332], loss=15.0377
	step [265/332], loss=15.2259
	step [266/332], loss=15.1044
	step [267/332], loss=15.2616
	step [268/332], loss=15.7797
	step [269/332], loss=16.8768
	step [270/332], loss=18.4924
	step [271/332], loss=14.7097
	step [272/332], loss=15.6121
	step [273/332], loss=14.7784
	step [274/332], loss=16.0458
	step [275/332], loss=15.1102
	step [276/332], loss=16.1614
	step [277/332], loss=14.5447
	step [278/332], loss=14.1849
	step [279/332], loss=15.4194
	step [280/332], loss=14.8583
	step [281/332], loss=15.0987
	step [282/332], loss=14.9209
	step [283/332], loss=18.8087
	step [284/332], loss=15.7053
	step [285/332], loss=13.8385
	step [286/332], loss=15.0690
	step [287/332], loss=16.2489
	step [288/332], loss=16.0039
	step [289/332], loss=15.6832
	step [290/332], loss=13.3104
	step [291/332], loss=15.2883
	step [292/332], loss=16.8508
	step [293/332], loss=15.8445
	step [294/332], loss=14.4143
	step [295/332], loss=17.4099
	step [296/332], loss=15.4923
	step [297/332], loss=16.3001
	step [298/332], loss=15.5102
	step [299/332], loss=13.7305
	step [300/332], loss=15.6020
	step [301/332], loss=15.2550
	step [302/332], loss=14.0159
	step [303/332], loss=14.8601
	step [304/332], loss=15.0538
	step [305/332], loss=13.5475
	step [306/332], loss=14.5566
	step [307/332], loss=15.1505
	step [308/332], loss=18.0668
	step [309/332], loss=15.4991
	step [310/332], loss=15.3222
	step [311/332], loss=15.2414
	step [312/332], loss=16.0308
	step [313/332], loss=17.0369
	step [314/332], loss=14.1831
	step [315/332], loss=14.7150
	step [316/332], loss=14.0286
	step [317/332], loss=15.2645
	step [318/332], loss=15.0984
	step [319/332], loss=14.0970
	step [320/332], loss=14.2224
	step [321/332], loss=14.8821
	step [322/332], loss=15.6185
	step [323/332], loss=14.7379
	step [324/332], loss=15.8412
	step [325/332], loss=16.2529
	step [326/332], loss=14.0822
	step [327/332], loss=15.7906
	step [328/332], loss=15.4713
	step [329/332], loss=13.8029
	step [330/332], loss=15.7260
	step [331/332], loss=14.9316
	step [332/332], loss=9.8603
	Evaluating
	loss=0.0667, precision=0.1373, recall=0.9967, f1=0.2414
Training epoch 5
	step [1/332], loss=15.0381
	step [2/332], loss=16.8330
	step [3/332], loss=13.7372
	step [4/332], loss=14.8484
	step [5/332], loss=16.4396
	step [6/332], loss=14.6981
	step [7/332], loss=13.8528
	step [8/332], loss=15.4232
	step [9/332], loss=14.2663
	step [10/332], loss=15.1799
	step [11/332], loss=14.3817
	step [12/332], loss=15.1709
	step [13/332], loss=14.9290
	step [14/332], loss=15.6960
	step [15/332], loss=14.7532
	step [16/332], loss=13.9754
	step [17/332], loss=15.5012
	step [18/332], loss=13.1959
	step [19/332], loss=15.3337
	step [20/332], loss=16.3340
	step [21/332], loss=14.6054
	step [22/332], loss=14.0607
	step [23/332], loss=15.2590
	step [24/332], loss=14.4104
	step [25/332], loss=14.5710
	step [26/332], loss=16.5093
	step [27/332], loss=15.6506
	step [28/332], loss=15.5950
	step [29/332], loss=15.0331
	step [30/332], loss=13.9374
	step [31/332], loss=14.5621
	step [32/332], loss=13.7910
	step [33/332], loss=12.9542
	step [34/332], loss=15.6184
	step [35/332], loss=14.5694
	step [36/332], loss=13.9533
	step [37/332], loss=14.8353
	step [38/332], loss=14.1946
	step [39/332], loss=14.7336
	step [40/332], loss=13.4696
	step [41/332], loss=13.5959
	step [42/332], loss=14.1999
	step [43/332], loss=14.5075
	step [44/332], loss=12.7175
	step [45/332], loss=13.2501
	step [46/332], loss=14.1736
	step [47/332], loss=15.0989
	step [48/332], loss=14.8747
	step [49/332], loss=12.0774
	step [50/332], loss=13.1448
	step [51/332], loss=13.9961
	step [52/332], loss=17.5594
	step [53/332], loss=14.1427
	step [54/332], loss=13.6305
	step [55/332], loss=17.7885
	step [56/332], loss=13.4102
	step [57/332], loss=13.7038
	step [58/332], loss=16.8532
	step [59/332], loss=14.3373
	step [60/332], loss=15.1028
	step [61/332], loss=15.7501
	step [62/332], loss=16.4080
	step [63/332], loss=15.1301
	step [64/332], loss=15.7767
	step [65/332], loss=14.5444
	step [66/332], loss=12.8641
	step [67/332], loss=14.2052
	step [68/332], loss=13.8398
	step [69/332], loss=13.4801
	step [70/332], loss=13.9492
	step [71/332], loss=13.8620
	step [72/332], loss=13.2507
	step [73/332], loss=13.1266
	step [74/332], loss=13.4889
	step [75/332], loss=13.7308
	step [76/332], loss=14.6795
	step [77/332], loss=14.7803
	step [78/332], loss=13.8585
	step [79/332], loss=15.1143
	step [80/332], loss=12.2579
	step [81/332], loss=14.8750
	step [82/332], loss=16.1707
	step [83/332], loss=13.9861
	step [84/332], loss=14.9696
	step [85/332], loss=14.1492
	step [86/332], loss=13.5198
	step [87/332], loss=16.7570
	step [88/332], loss=14.1707
	step [89/332], loss=12.9060
	step [90/332], loss=14.5883
	step [91/332], loss=12.8586
	step [92/332], loss=13.8714
	step [93/332], loss=13.5794
	step [94/332], loss=14.6561
	step [95/332], loss=14.7577
	step [96/332], loss=13.9442
	step [97/332], loss=16.8187
	step [98/332], loss=16.0087
	step [99/332], loss=13.0095
	step [100/332], loss=14.9990
	step [101/332], loss=13.5699
	step [102/332], loss=14.7227
	step [103/332], loss=13.1579
	step [104/332], loss=13.8481
	step [105/332], loss=14.0989
	step [106/332], loss=12.4568
	step [107/332], loss=13.8079
	step [108/332], loss=14.3873
	step [109/332], loss=14.1106
	step [110/332], loss=14.1304
	step [111/332], loss=13.7369
	step [112/332], loss=14.7871
	step [113/332], loss=14.4144
	step [114/332], loss=12.8460
	step [115/332], loss=12.5750
	step [116/332], loss=12.7342
	step [117/332], loss=14.5317
	step [118/332], loss=11.8726
	step [119/332], loss=13.0556
	step [120/332], loss=16.4038
	step [121/332], loss=13.7764
	step [122/332], loss=14.2026
	step [123/332], loss=13.2891
	step [124/332], loss=14.8358
	step [125/332], loss=14.1106
	step [126/332], loss=13.3585
	step [127/332], loss=13.9535
	step [128/332], loss=13.1905
	step [129/332], loss=12.9257
	step [130/332], loss=13.1477
	step [131/332], loss=15.0699
	step [132/332], loss=15.3647
	step [133/332], loss=12.7525
	step [134/332], loss=12.3549
	step [135/332], loss=14.6083
	step [136/332], loss=14.1959
	step [137/332], loss=12.8995
	step [138/332], loss=12.0303
	step [139/332], loss=12.7789
	step [140/332], loss=13.8181
	step [141/332], loss=12.7941
	step [142/332], loss=13.4226
	step [143/332], loss=13.7150
	step [144/332], loss=13.6673
	step [145/332], loss=13.8168
	step [146/332], loss=12.3509
	step [147/332], loss=10.9504
	step [148/332], loss=15.9239
	step [149/332], loss=13.8346
	step [150/332], loss=12.2317
	step [151/332], loss=12.3090
	step [152/332], loss=13.4749
	step [153/332], loss=13.1906
	step [154/332], loss=12.6642
	step [155/332], loss=13.5109
	step [156/332], loss=13.6722
	step [157/332], loss=14.1178
	step [158/332], loss=13.4040
	step [159/332], loss=12.4928
	step [160/332], loss=13.0668
	step [161/332], loss=14.3937
	step [162/332], loss=12.2817
	step [163/332], loss=13.6254
	step [164/332], loss=11.6242
	step [165/332], loss=12.7092
	step [166/332], loss=13.6378
	step [167/332], loss=13.0823
	step [168/332], loss=12.9281
	step [169/332], loss=16.6140
	step [170/332], loss=12.2092
	step [171/332], loss=13.4274
	step [172/332], loss=12.9047
	step [173/332], loss=13.8880
	step [174/332], loss=11.9674
	step [175/332], loss=12.1505
	step [176/332], loss=12.8813
	step [177/332], loss=14.8617
	step [178/332], loss=12.7848
	step [179/332], loss=12.9316
	step [180/332], loss=14.0610
	step [181/332], loss=13.4112
	step [182/332], loss=15.0673
	step [183/332], loss=11.9586
	step [184/332], loss=12.8828
	step [185/332], loss=13.9168
	step [186/332], loss=13.0961
	step [187/332], loss=13.6654
	step [188/332], loss=13.3317
	step [189/332], loss=13.0099
	step [190/332], loss=12.1051
	step [191/332], loss=12.5612
	step [192/332], loss=15.1649
	step [193/332], loss=12.7293
	step [194/332], loss=15.9071
	step [195/332], loss=13.8758
	step [196/332], loss=12.9876
	step [197/332], loss=13.6337
	step [198/332], loss=12.2693
	step [199/332], loss=12.5757
	step [200/332], loss=12.0861
	step [201/332], loss=13.1380
	step [202/332], loss=11.8169
	step [203/332], loss=13.0511
	step [204/332], loss=12.7252
	step [205/332], loss=11.3850
	step [206/332], loss=13.6781
	step [207/332], loss=14.2338
	step [208/332], loss=15.7715
	step [209/332], loss=15.2350
	step [210/332], loss=12.1990
	step [211/332], loss=13.8451
	step [212/332], loss=12.7448
	step [213/332], loss=12.6739
	step [214/332], loss=12.2509
	step [215/332], loss=14.3916
	step [216/332], loss=12.5783
	step [217/332], loss=12.5559
	step [218/332], loss=12.6305
	step [219/332], loss=11.9825
	step [220/332], loss=11.2495
	step [221/332], loss=11.5386
	step [222/332], loss=13.1245
	step [223/332], loss=12.8155
	step [224/332], loss=13.4981
	step [225/332], loss=13.8918
	step [226/332], loss=12.2406
	step [227/332], loss=12.5346
	step [228/332], loss=14.9072
	step [229/332], loss=12.3221
	step [230/332], loss=13.3653
	step [231/332], loss=11.9781
	step [232/332], loss=13.1027
	step [233/332], loss=12.8269
	step [234/332], loss=14.5276
	step [235/332], loss=13.5014
	step [236/332], loss=12.7868
	step [237/332], loss=11.2706
	step [238/332], loss=14.1718
	step [239/332], loss=12.0300
	step [240/332], loss=12.2008
	step [241/332], loss=12.5408
	step [242/332], loss=11.5230
	step [243/332], loss=12.3611
	step [244/332], loss=12.2144
	step [245/332], loss=11.2149
	step [246/332], loss=12.2607
	step [247/332], loss=11.8715
	step [248/332], loss=13.6699
	step [249/332], loss=13.1073
	step [250/332], loss=11.2152
	step [251/332], loss=13.2723
	step [252/332], loss=12.1879
	step [253/332], loss=13.3024
	step [254/332], loss=12.0251
	step [255/332], loss=12.3710
	step [256/332], loss=12.8122
	step [257/332], loss=13.7506
	step [258/332], loss=11.2897
	step [259/332], loss=11.7519
	step [260/332], loss=14.4204
	step [261/332], loss=12.1228
	step [262/332], loss=12.6474
	step [263/332], loss=11.8882
	step [264/332], loss=12.1562
	step [265/332], loss=13.0010
	step [266/332], loss=13.9345
	step [267/332], loss=11.7322
	step [268/332], loss=14.9794
	step [269/332], loss=11.4597
	step [270/332], loss=11.4614
	step [271/332], loss=10.3596
	step [272/332], loss=12.2409
	step [273/332], loss=12.4499
	step [274/332], loss=11.7447
	step [275/332], loss=12.0905
	step [276/332], loss=10.6584
	step [277/332], loss=12.5000
	step [278/332], loss=11.5348
	step [279/332], loss=12.1757
	step [280/332], loss=13.4000
	step [281/332], loss=12.8358
	step [282/332], loss=12.1046
	step [283/332], loss=11.7664
	step [284/332], loss=13.5431
	step [285/332], loss=11.7955
	step [286/332], loss=11.8137
	step [287/332], loss=11.4019
	step [288/332], loss=13.5773
	step [289/332], loss=12.6528
	step [290/332], loss=12.8719
	step [291/332], loss=11.8471
	step [292/332], loss=13.2155
	step [293/332], loss=13.4833
	step [294/332], loss=11.9488
	step [295/332], loss=11.2475
	step [296/332], loss=11.6400
	step [297/332], loss=11.5118
	step [298/332], loss=12.3373
	step [299/332], loss=11.5247
	step [300/332], loss=13.1622
	step [301/332], loss=13.8101
	step [302/332], loss=13.6867
	step [303/332], loss=10.5668
	step [304/332], loss=11.3043
	step [305/332], loss=11.7489
	step [306/332], loss=11.9796
	step [307/332], loss=12.9056
	step [308/332], loss=11.7626
	step [309/332], loss=11.5321
	step [310/332], loss=11.6472
	step [311/332], loss=12.8217
	step [312/332], loss=12.6320
	step [313/332], loss=11.8553
	step [314/332], loss=12.7042
	step [315/332], loss=13.1499
	step [316/332], loss=12.1550
	step [317/332], loss=12.3146
	step [318/332], loss=13.2677
	step [319/332], loss=11.8916
	step [320/332], loss=12.2010
	step [321/332], loss=11.7899
	step [322/332], loss=10.4392
	step [323/332], loss=10.1193
	step [324/332], loss=10.5718
	step [325/332], loss=10.1409
	step [326/332], loss=14.6767
	step [327/332], loss=12.7990
	step [328/332], loss=12.4625
	step [329/332], loss=12.6187
	step [330/332], loss=12.0943
	step [331/332], loss=12.9067
	step [332/332], loss=7.0566
	Evaluating
	loss=0.0510, precision=0.1450, recall=0.9967, f1=0.2532
saving model as: 0_saved_model.pth
Training epoch 6
	step [1/332], loss=13.5079
	step [2/332], loss=13.0221
	step [3/332], loss=12.1091
	step [4/332], loss=12.0868
	step [5/332], loss=11.9900
	step [6/332], loss=11.3554
	step [7/332], loss=12.8035
	step [8/332], loss=11.4959
	step [9/332], loss=12.6898
	step [10/332], loss=13.2605
	step [11/332], loss=14.2557
	step [12/332], loss=13.1511
	step [13/332], loss=11.5060
	step [14/332], loss=11.5725
	step [15/332], loss=11.9549
	step [16/332], loss=12.2932
	step [17/332], loss=11.8419
	step [18/332], loss=11.4970
	step [19/332], loss=14.1196
	step [20/332], loss=12.0960
	step [21/332], loss=10.7879
	step [22/332], loss=13.3519
	step [23/332], loss=10.7462
	step [24/332], loss=12.5359
	step [25/332], loss=11.6955
	step [26/332], loss=10.8244
	step [27/332], loss=11.4455
	step [28/332], loss=12.0039
	step [29/332], loss=16.7864
	step [30/332], loss=10.8786
	step [31/332], loss=11.6279
	step [32/332], loss=11.2967
	step [33/332], loss=12.0985
	step [34/332], loss=13.0651
	step [35/332], loss=11.4576
	step [36/332], loss=11.1823
	step [37/332], loss=10.6202
	step [38/332], loss=12.3632
	step [39/332], loss=10.5458
	step [40/332], loss=13.4596
	step [41/332], loss=11.1105
	step [42/332], loss=11.9119
	step [43/332], loss=11.8429
	step [44/332], loss=12.1498
	step [45/332], loss=11.7682
	step [46/332], loss=12.8117
	step [47/332], loss=10.7843
	step [48/332], loss=11.1724
	step [49/332], loss=10.6391
	step [50/332], loss=10.7042
	step [51/332], loss=10.8292
	step [52/332], loss=11.5121
	step [53/332], loss=12.3470
	step [54/332], loss=9.6730
	step [55/332], loss=11.6755
	step [56/332], loss=12.7914
	step [57/332], loss=10.1794
	step [58/332], loss=10.3809
	step [59/332], loss=11.2992
	step [60/332], loss=10.0436
	step [61/332], loss=12.1365
	step [62/332], loss=11.4008
	step [63/332], loss=12.0917
	step [64/332], loss=10.9132
	step [65/332], loss=11.1869
	step [66/332], loss=12.2022
	step [67/332], loss=10.7638
	step [68/332], loss=11.5341
	step [69/332], loss=10.7766
	step [70/332], loss=11.4130
	step [71/332], loss=12.2911
	step [72/332], loss=10.5588
	step [73/332], loss=11.3042
	step [74/332], loss=11.1569
	step [75/332], loss=10.3254
	step [76/332], loss=11.2565
	step [77/332], loss=10.5455
	step [78/332], loss=11.7938
	step [79/332], loss=11.9227
	step [80/332], loss=10.4686
	step [81/332], loss=10.1630
	step [82/332], loss=12.2229
	step [83/332], loss=11.6950
	step [84/332], loss=10.7707
	step [85/332], loss=12.1837
	step [86/332], loss=11.5675
	step [87/332], loss=12.0654
	step [88/332], loss=10.7161
	step [89/332], loss=11.2130
	step [90/332], loss=10.3182
	step [91/332], loss=11.6435
	step [92/332], loss=10.0901
	step [93/332], loss=11.6199
	step [94/332], loss=12.7514
	step [95/332], loss=11.8102
	step [96/332], loss=11.8594
	step [97/332], loss=10.6812
	step [98/332], loss=11.6206
	step [99/332], loss=11.4865
	step [100/332], loss=9.9377
	step [101/332], loss=10.6738
	step [102/332], loss=11.7645
	step [103/332], loss=11.0081
	step [104/332], loss=10.9319
	step [105/332], loss=12.3778
	step [106/332], loss=11.4314
	step [107/332], loss=10.8310
	step [108/332], loss=11.5607
	step [109/332], loss=10.8364
	step [110/332], loss=12.7992
	step [111/332], loss=11.1150
	step [112/332], loss=11.5980
	step [113/332], loss=10.8648
	step [114/332], loss=11.6478
	step [115/332], loss=12.0918
	step [116/332], loss=11.4852
	step [117/332], loss=11.4161
	step [118/332], loss=11.3104
	step [119/332], loss=10.6092
	step [120/332], loss=10.4190
	step [121/332], loss=9.3629
	step [122/332], loss=12.5128
	step [123/332], loss=11.6287
	step [124/332], loss=11.0659
	step [125/332], loss=11.7874
	step [126/332], loss=13.9383
	step [127/332], loss=10.8094
	step [128/332], loss=11.1557
	step [129/332], loss=11.7305
	step [130/332], loss=12.2345
	step [131/332], loss=9.6763
	step [132/332], loss=11.0602
	step [133/332], loss=11.9916
	step [134/332], loss=9.8737
	step [135/332], loss=13.5476
	step [136/332], loss=11.3885
	step [137/332], loss=12.6665
	step [138/332], loss=9.9597
	step [139/332], loss=11.7086
	step [140/332], loss=10.9700
	step [141/332], loss=11.1075
	step [142/332], loss=9.4653
	step [143/332], loss=10.2754
	step [144/332], loss=12.7075
	step [145/332], loss=11.6279
	step [146/332], loss=11.9658
	step [147/332], loss=12.2861
	step [148/332], loss=10.8214
	step [149/332], loss=10.2436
	step [150/332], loss=10.9393
	step [151/332], loss=12.6615
	step [152/332], loss=9.1653
	step [153/332], loss=12.7476
	step [154/332], loss=9.9835
	step [155/332], loss=9.6340
	step [156/332], loss=11.7513
	step [157/332], loss=10.3226
	step [158/332], loss=14.2575
	step [159/332], loss=10.0321
	step [160/332], loss=10.1148
	step [161/332], loss=14.2549
	step [162/332], loss=10.7872
	step [163/332], loss=11.4479
	step [164/332], loss=11.2381
	step [165/332], loss=11.0348
	step [166/332], loss=11.5526
	step [167/332], loss=9.4293
	step [168/332], loss=11.9444
	step [169/332], loss=10.5152
	step [170/332], loss=10.2737
	step [171/332], loss=11.2041
	step [172/332], loss=12.7697
	step [173/332], loss=11.2301
	step [174/332], loss=10.7670
	step [175/332], loss=12.1938
	step [176/332], loss=11.6331
	step [177/332], loss=9.5797
	step [178/332], loss=12.3818
	step [179/332], loss=11.0223
	step [180/332], loss=11.3419
	step [181/332], loss=11.0385
	step [182/332], loss=10.1154
	step [183/332], loss=11.7016
	step [184/332], loss=10.8697
	step [185/332], loss=9.7380
	step [186/332], loss=11.5237
	step [187/332], loss=10.8649
	step [188/332], loss=11.2197
	step [189/332], loss=10.6240
	step [190/332], loss=10.6029
	step [191/332], loss=10.5121
	step [192/332], loss=10.9460
	step [193/332], loss=10.6673
	step [194/332], loss=13.3439
	step [195/332], loss=12.1679
	step [196/332], loss=11.1592
	step [197/332], loss=11.5600
	step [198/332], loss=11.6192
	step [199/332], loss=14.4655
	step [200/332], loss=11.9131
	step [201/332], loss=9.8987
	step [202/332], loss=11.1315
	step [203/332], loss=13.6597
	step [204/332], loss=10.8487
	step [205/332], loss=10.1491
	step [206/332], loss=9.8930
	step [207/332], loss=9.3027
	step [208/332], loss=11.5545
	step [209/332], loss=10.4087
	step [210/332], loss=10.3385
	step [211/332], loss=11.3049
	step [212/332], loss=11.2691
	step [213/332], loss=12.4726
	step [214/332], loss=11.4452
	step [215/332], loss=11.9135
	step [216/332], loss=9.4049
	step [217/332], loss=10.4282
	step [218/332], loss=10.5337
	step [219/332], loss=12.9003
	step [220/332], loss=10.1348
	step [221/332], loss=11.3998
	step [222/332], loss=10.3307
	step [223/332], loss=9.5682
	step [224/332], loss=10.3163
	step [225/332], loss=9.8971
	step [226/332], loss=10.9710
	step [227/332], loss=8.9897
	step [228/332], loss=10.8423
	step [229/332], loss=9.0415
	step [230/332], loss=10.4899
	step [231/332], loss=11.6421
	step [232/332], loss=12.4661
	step [233/332], loss=10.3470
	step [234/332], loss=13.0118
	step [235/332], loss=12.8451
	step [236/332], loss=9.8214
	step [237/332], loss=11.2839
	step [238/332], loss=9.8075
	step [239/332], loss=10.5210
	step [240/332], loss=11.4767
	step [241/332], loss=11.7420
	step [242/332], loss=10.9961
	step [243/332], loss=9.1623
	step [244/332], loss=10.0295
	step [245/332], loss=9.8748
	step [246/332], loss=12.5040
	step [247/332], loss=10.7148
	step [248/332], loss=10.4824
	step [249/332], loss=13.5029
	step [250/332], loss=10.2951
	step [251/332], loss=9.0711
	step [252/332], loss=10.2540
	step [253/332], loss=11.5143
	step [254/332], loss=13.1843
	step [255/332], loss=10.4230
	step [256/332], loss=9.3522
	step [257/332], loss=10.5274
	step [258/332], loss=9.4809
	step [259/332], loss=8.8170
	step [260/332], loss=10.1833
	step [261/332], loss=10.0199
	step [262/332], loss=10.9423
	step [263/332], loss=12.8431
	step [264/332], loss=9.8502
	step [265/332], loss=11.3422
	step [266/332], loss=9.4306
	step [267/332], loss=10.0119
	step [268/332], loss=8.9559
	step [269/332], loss=10.0729
	step [270/332], loss=10.4563
	step [271/332], loss=11.8424
	step [272/332], loss=10.8993
	step [273/332], loss=11.4687
	step [274/332], loss=10.0720
	step [275/332], loss=10.2467
	step [276/332], loss=11.7018
	step [277/332], loss=10.8600
	step [278/332], loss=11.0805
	step [279/332], loss=9.6982
	step [280/332], loss=9.7509
	step [281/332], loss=10.1411
	step [282/332], loss=11.2158
	step [283/332], loss=10.5500
	step [284/332], loss=11.1075
	step [285/332], loss=10.9026
	step [286/332], loss=13.2371
	step [287/332], loss=9.6478
	step [288/332], loss=10.0419
	step [289/332], loss=10.7388
	step [290/332], loss=9.2906
	step [291/332], loss=9.4758
	step [292/332], loss=9.4977
	step [293/332], loss=10.5591
	step [294/332], loss=11.9636
	step [295/332], loss=10.1805
	step [296/332], loss=12.5197
	step [297/332], loss=9.5530
	step [298/332], loss=9.9744
	step [299/332], loss=9.5270
	step [300/332], loss=10.6281
	step [301/332], loss=10.6890
	step [302/332], loss=9.5212
	step [303/332], loss=9.3909
	step [304/332], loss=10.6937
	step [305/332], loss=8.9493
	step [306/332], loss=9.6951
	step [307/332], loss=11.1060
	step [308/332], loss=11.3582
	step [309/332], loss=10.9593
	step [310/332], loss=8.7843
	step [311/332], loss=11.3701
	step [312/332], loss=10.6735
	step [313/332], loss=11.4651
	step [314/332], loss=10.3352
	step [315/332], loss=10.1037
	step [316/332], loss=9.7241
	step [317/332], loss=11.1141
	step [318/332], loss=10.5576
	step [319/332], loss=10.3483
	step [320/332], loss=10.4845
	step [321/332], loss=10.0149
	step [322/332], loss=11.3186
	step [323/332], loss=9.6313
	step [324/332], loss=9.3509
	step [325/332], loss=11.0573
	step [326/332], loss=13.2191
	step [327/332], loss=9.7872
	step [328/332], loss=10.2530
	step [329/332], loss=11.3207
	step [330/332], loss=11.0015
	step [331/332], loss=9.3037
	step [332/332], loss=5.8206
	Evaluating
	loss=0.0461, precision=0.1240, recall=0.9973, f1=0.2206
Training epoch 7
	step [1/332], loss=10.0345
	step [2/332], loss=9.1734
	step [3/332], loss=10.0497
	step [4/332], loss=9.4760
	step [5/332], loss=9.9625
	step [6/332], loss=9.5657
	step [7/332], loss=11.4954
	step [8/332], loss=9.6977
	step [9/332], loss=11.0700
	step [10/332], loss=9.1191
	step [11/332], loss=9.3268
	step [12/332], loss=9.5591
	step [13/332], loss=10.2447
	step [14/332], loss=9.7225
	step [15/332], loss=8.8369
	step [16/332], loss=11.0688
	step [17/332], loss=9.1506
	step [18/332], loss=9.5053
	step [19/332], loss=10.8404
	step [20/332], loss=8.9009
	step [21/332], loss=9.7413
	step [22/332], loss=10.3809
	step [23/332], loss=10.3777
	step [24/332], loss=10.3663
	step [25/332], loss=8.8234
	step [26/332], loss=9.8799
	step [27/332], loss=11.2076
	step [28/332], loss=8.9623
	step [29/332], loss=11.5463
	step [30/332], loss=10.2110
	step [31/332], loss=10.5104
	step [32/332], loss=9.8912
	step [33/332], loss=10.9890
	step [34/332], loss=8.1691
	step [35/332], loss=12.8273
	step [36/332], loss=9.7865
	step [37/332], loss=10.9046
	step [38/332], loss=11.7536
	step [39/332], loss=10.7033
	step [40/332], loss=10.5492
	step [41/332], loss=10.4528
	step [42/332], loss=11.0420
	step [43/332], loss=11.1640
	step [44/332], loss=10.9299
	step [45/332], loss=8.9597
	step [46/332], loss=10.6026
	step [47/332], loss=9.8986
	step [48/332], loss=11.8496
	step [49/332], loss=9.5889
	step [50/332], loss=10.2793
	step [51/332], loss=9.6969
	step [52/332], loss=9.1759
	step [53/332], loss=10.2290
	step [54/332], loss=12.2787
	step [55/332], loss=10.4692
	step [56/332], loss=11.1479
	step [57/332], loss=10.4807
	step [58/332], loss=9.6939
	step [59/332], loss=9.8117
	step [60/332], loss=9.7596
	step [61/332], loss=8.9956
	step [62/332], loss=9.4320
	step [63/332], loss=12.0712
	step [64/332], loss=8.7675
	step [65/332], loss=10.4673
	step [66/332], loss=8.3720
	step [67/332], loss=9.4740
	step [68/332], loss=9.3320
	step [69/332], loss=10.4969
	step [70/332], loss=11.1740
	step [71/332], loss=9.3572
	step [72/332], loss=10.6795
	step [73/332], loss=9.8285
	step [74/332], loss=10.4004
	step [75/332], loss=9.9418
	step [76/332], loss=8.7178
	step [77/332], loss=9.4601
	step [78/332], loss=8.4439
	step [79/332], loss=10.9758
	step [80/332], loss=8.6765
	step [81/332], loss=9.8695
	step [82/332], loss=9.2925
	step [83/332], loss=10.3747
	step [84/332], loss=9.0182
	step [85/332], loss=11.5048
	step [86/332], loss=12.3478
	step [87/332], loss=8.7061
	step [88/332], loss=10.3043
	step [89/332], loss=9.7795
	step [90/332], loss=8.3550
	step [91/332], loss=9.7137
	step [92/332], loss=9.6465
	step [93/332], loss=12.5137
	step [94/332], loss=11.2510
	step [95/332], loss=8.6791
	step [96/332], loss=11.0728
	step [97/332], loss=8.9335
	step [98/332], loss=9.0604
	step [99/332], loss=11.2265
	step [100/332], loss=9.0764
	step [101/332], loss=10.6358
	step [102/332], loss=10.2872
	step [103/332], loss=9.4350
	step [104/332], loss=9.7966
	step [105/332], loss=9.8999
	step [106/332], loss=9.6433
	step [107/332], loss=11.0237
	step [108/332], loss=9.9455
	step [109/332], loss=11.8346
	step [110/332], loss=9.1347
	step [111/332], loss=8.6233
	step [112/332], loss=9.8035
	step [113/332], loss=10.4117
	step [114/332], loss=9.2559
	step [115/332], loss=10.5820
	step [116/332], loss=9.9161
	step [117/332], loss=9.2166
	step [118/332], loss=10.9302
	step [119/332], loss=10.2805
	step [120/332], loss=8.3430
	step [121/332], loss=8.8231
	step [122/332], loss=10.2662
	step [123/332], loss=9.7230
	step [124/332], loss=9.9780
	step [125/332], loss=12.6682
	step [126/332], loss=8.7273
	step [127/332], loss=9.5844
	step [128/332], loss=10.3327
	step [129/332], loss=11.4365
	step [130/332], loss=10.7659
	step [131/332], loss=10.1245
	step [132/332], loss=9.1026
	step [133/332], loss=11.0205
	step [134/332], loss=8.5664
	step [135/332], loss=8.4805
	step [136/332], loss=8.8317
	step [137/332], loss=7.9686
	step [138/332], loss=8.7284
	step [139/332], loss=11.2627
	step [140/332], loss=8.9411
	step [141/332], loss=8.7710
	step [142/332], loss=11.0454
	step [143/332], loss=9.5826
	step [144/332], loss=9.0009
	step [145/332], loss=9.1094
	step [146/332], loss=8.8633
	step [147/332], loss=10.4000
	step [148/332], loss=8.0043
	step [149/332], loss=8.4701
	step [150/332], loss=9.6935
	step [151/332], loss=9.3907
	step [152/332], loss=9.8908
	step [153/332], loss=9.0280
	step [154/332], loss=8.4294
	step [155/332], loss=9.1951
	step [156/332], loss=9.9417
	step [157/332], loss=8.9516
	step [158/332], loss=15.4028
	step [159/332], loss=10.1938
	step [160/332], loss=10.5260
	step [161/332], loss=9.5803
	step [162/332], loss=9.4253
	step [163/332], loss=10.5267
	step [164/332], loss=8.0041
	step [165/332], loss=10.3302
	step [166/332], loss=9.5001
	step [167/332], loss=10.2671
	step [168/332], loss=9.4666
	step [169/332], loss=10.1187
	step [170/332], loss=8.5433
	step [171/332], loss=8.6484
	step [172/332], loss=10.8420
	step [173/332], loss=10.5901
	step [174/332], loss=11.4075
	step [175/332], loss=8.5239
	step [176/332], loss=10.0737
	step [177/332], loss=10.0120
	step [178/332], loss=9.3985
	step [179/332], loss=8.7856
	step [180/332], loss=9.6696
	step [181/332], loss=10.0168
	step [182/332], loss=10.0978
	step [183/332], loss=9.8089
	step [184/332], loss=9.0294
	step [185/332], loss=10.6085
	step [186/332], loss=10.6508
	step [187/332], loss=10.4374
	step [188/332], loss=10.2123
	step [189/332], loss=9.9005
	step [190/332], loss=10.7394
	step [191/332], loss=9.7858
	step [192/332], loss=8.9119
	step [193/332], loss=9.6553
	step [194/332], loss=9.7084
	step [195/332], loss=10.0066
	step [196/332], loss=8.8529
	step [197/332], loss=8.8408
	step [198/332], loss=8.6535
	step [199/332], loss=7.7906
	step [200/332], loss=10.0359
	step [201/332], loss=10.5565
	step [202/332], loss=9.2598
	step [203/332], loss=7.4174
	step [204/332], loss=11.0838
	step [205/332], loss=8.8102
	step [206/332], loss=8.3222
	step [207/332], loss=8.6115
	step [208/332], loss=11.1021
	step [209/332], loss=9.0897
	step [210/332], loss=11.1500
	step [211/332], loss=8.9257
	step [212/332], loss=9.3558
	step [213/332], loss=10.1620
	step [214/332], loss=11.6329
	step [215/332], loss=8.8015
	step [216/332], loss=9.7436
	step [217/332], loss=8.6308
	step [218/332], loss=8.9375
	step [219/332], loss=8.0586
	step [220/332], loss=11.4594
	step [221/332], loss=8.9143
	step [222/332], loss=9.2609
	step [223/332], loss=8.8250
	step [224/332], loss=9.0076
	step [225/332], loss=8.4486
	step [226/332], loss=9.7915
	step [227/332], loss=8.5942
	step [228/332], loss=11.6654
	step [229/332], loss=9.4254
	step [230/332], loss=8.8483
	step [231/332], loss=8.6097
	step [232/332], loss=10.3236
	step [233/332], loss=10.1162
	step [234/332], loss=10.0877
	step [235/332], loss=9.3652
	step [236/332], loss=9.0682
	step [237/332], loss=7.8419
	step [238/332], loss=9.4386
	step [239/332], loss=10.0622
	step [240/332], loss=9.0353
	step [241/332], loss=8.1464
	step [242/332], loss=8.5401
	step [243/332], loss=9.1980
	step [244/332], loss=10.5718
	step [245/332], loss=9.4079
	step [246/332], loss=10.0363
	step [247/332], loss=12.4090
	step [248/332], loss=10.0273
	step [249/332], loss=8.6723
	step [250/332], loss=8.2068
	step [251/332], loss=8.4844
	step [252/332], loss=8.8033
	step [253/332], loss=7.8317
	step [254/332], loss=8.1691
	step [255/332], loss=10.3418
	step [256/332], loss=10.4873
	step [257/332], loss=8.9461
	step [258/332], loss=8.6072
	step [259/332], loss=9.2231
	step [260/332], loss=8.0354
	step [261/332], loss=9.4287
	step [262/332], loss=10.4394
	step [263/332], loss=9.2728
	step [264/332], loss=10.8163
	step [265/332], loss=10.5793
	step [266/332], loss=10.1555
	step [267/332], loss=9.9117
	step [268/332], loss=10.6741
	step [269/332], loss=11.4468
	step [270/332], loss=8.4313
	step [271/332], loss=9.8478
	step [272/332], loss=8.0557
	step [273/332], loss=10.4784
	step [274/332], loss=8.2681
	step [275/332], loss=10.3176
	step [276/332], loss=7.6747
	step [277/332], loss=9.7366
	step [278/332], loss=9.9409
	step [279/332], loss=9.2349
	step [280/332], loss=11.4049
	step [281/332], loss=10.5623
	step [282/332], loss=7.8586
	step [283/332], loss=10.2605
	step [284/332], loss=9.1297
	step [285/332], loss=10.6231
	step [286/332], loss=8.0026
	step [287/332], loss=11.0723
	step [288/332], loss=7.9126
	step [289/332], loss=8.5731
	step [290/332], loss=8.4584
	step [291/332], loss=8.7430
	step [292/332], loss=7.2332
	step [293/332], loss=10.0018
	step [294/332], loss=9.8051
	step [295/332], loss=8.9475
	step [296/332], loss=8.5269
	step [297/332], loss=8.8019
	step [298/332], loss=8.2481
	step [299/332], loss=9.9109
	step [300/332], loss=8.2416
	step [301/332], loss=9.3824
	step [302/332], loss=7.8277
	step [303/332], loss=10.1470
	step [304/332], loss=8.8152
	step [305/332], loss=9.9522
	step [306/332], loss=9.3144
	step [307/332], loss=8.8363
	step [308/332], loss=9.3554
	step [309/332], loss=7.7533
	step [310/332], loss=10.0919
	step [311/332], loss=7.2406
	step [312/332], loss=9.0445
	step [313/332], loss=11.2005
	step [314/332], loss=9.9131
	step [315/332], loss=8.7987
	step [316/332], loss=9.5405
	step [317/332], loss=11.3042
	step [318/332], loss=8.2642
	step [319/332], loss=9.8131
	step [320/332], loss=11.2785
	step [321/332], loss=10.2709
	step [322/332], loss=8.8951
	step [323/332], loss=11.1471
	step [324/332], loss=9.9901
	step [325/332], loss=9.5905
	step [326/332], loss=9.8863
	step [327/332], loss=7.8579
	step [328/332], loss=8.3070
	step [329/332], loss=8.3211
	step [330/332], loss=9.1632
	step [331/332], loss=8.8410
	step [332/332], loss=5.6508
	Evaluating
	loss=0.0369, precision=0.1650, recall=0.9962, f1=0.2832
saving model as: 0_saved_model.pth
Training epoch 8
	step [1/332], loss=6.5207
	step [2/332], loss=8.5279
	step [3/332], loss=8.5778
	step [4/332], loss=9.6392
	step [5/332], loss=8.9287
	step [6/332], loss=10.2771
	step [7/332], loss=8.1068
	step [8/332], loss=8.8219
	step [9/332], loss=9.6153
	step [10/332], loss=10.2093
	step [11/332], loss=9.6407
	step [12/332], loss=11.5464
	step [13/332], loss=9.2461
	step [14/332], loss=9.9917
	step [15/332], loss=9.5664
	step [16/332], loss=8.5894
	step [17/332], loss=8.8605
	step [18/332], loss=9.7496
	step [19/332], loss=9.8466
	step [20/332], loss=8.3676
	step [21/332], loss=8.2067
	step [22/332], loss=7.2320
	step [23/332], loss=9.7584
	step [24/332], loss=8.3780
	step [25/332], loss=8.5912
	step [26/332], loss=7.0164
	step [27/332], loss=9.9041
	step [28/332], loss=9.1749
	step [29/332], loss=8.2635
	step [30/332], loss=9.3594
	step [31/332], loss=7.2560
	step [32/332], loss=10.9431
	step [33/332], loss=10.2222
	step [34/332], loss=9.8685
	step [35/332], loss=9.6669
	step [36/332], loss=9.2629
	step [37/332], loss=7.4875
	step [38/332], loss=8.3899
	step [39/332], loss=7.2334
	step [40/332], loss=10.2961
	step [41/332], loss=9.1165
	step [42/332], loss=9.1951
	step [43/332], loss=7.4030
	step [44/332], loss=10.5988
	step [45/332], loss=7.7811
	step [46/332], loss=8.6086
	step [47/332], loss=7.8455
	step [48/332], loss=7.9937
	step [49/332], loss=7.8617
	step [50/332], loss=8.8698
	step [51/332], loss=8.7496
	step [52/332], loss=8.9912
	step [53/332], loss=9.4586
	step [54/332], loss=8.7483
	step [55/332], loss=9.3795
	step [56/332], loss=8.1814
	step [57/332], loss=9.3264
	step [58/332], loss=9.8429
	step [59/332], loss=9.8199
	step [60/332], loss=8.5512
	step [61/332], loss=9.0909
	step [62/332], loss=10.8902
	step [63/332], loss=10.3495
	step [64/332], loss=8.9461
	step [65/332], loss=8.9009
	step [66/332], loss=10.7100
	step [67/332], loss=11.2283
	step [68/332], loss=9.8578
	step [69/332], loss=9.7213
	step [70/332], loss=9.9955
	step [71/332], loss=8.8917
	step [72/332], loss=10.7044
	step [73/332], loss=8.5529
	step [74/332], loss=8.5966
	step [75/332], loss=9.3395
	step [76/332], loss=10.8661
	step [77/332], loss=7.8117
	step [78/332], loss=7.7630
	step [79/332], loss=8.6607
	step [80/332], loss=9.8250
	step [81/332], loss=8.5481
	step [82/332], loss=10.2750
	step [83/332], loss=9.2157
	step [84/332], loss=7.1190
	step [85/332], loss=8.7131
	step [86/332], loss=8.3357
	step [87/332], loss=7.8061
	step [88/332], loss=8.6924
	step [89/332], loss=9.3663
	step [90/332], loss=7.3761
	step [91/332], loss=9.6713
	step [92/332], loss=10.6666
	step [93/332], loss=9.3329
	step [94/332], loss=9.8803
	step [95/332], loss=6.8750
	step [96/332], loss=9.9077
	step [97/332], loss=10.7808
	step [98/332], loss=9.5379
	step [99/332], loss=9.0279
	step [100/332], loss=10.0763
	step [101/332], loss=9.0518
	step [102/332], loss=8.2326
	step [103/332], loss=8.3771
	step [104/332], loss=11.8688
	step [105/332], loss=8.0057
	step [106/332], loss=8.4899
	step [107/332], loss=7.4971
	step [108/332], loss=9.3238
	step [109/332], loss=9.2471
	step [110/332], loss=9.8024
	step [111/332], loss=7.8156
	step [112/332], loss=7.2156
	step [113/332], loss=9.5419
	step [114/332], loss=7.8449
	step [115/332], loss=8.1813
	step [116/332], loss=7.5650
	step [117/332], loss=8.4988
	step [118/332], loss=7.4136
	step [119/332], loss=7.9147
	step [120/332], loss=8.7831
	step [121/332], loss=8.8514
	step [122/332], loss=7.8766
	step [123/332], loss=10.3385
	step [124/332], loss=9.6389
	step [125/332], loss=9.2180
	step [126/332], loss=8.1741
	step [127/332], loss=8.3715
	step [128/332], loss=8.1957
	step [129/332], loss=7.8660
	step [130/332], loss=8.3026
	step [131/332], loss=8.5306
	step [132/332], loss=8.2643
	step [133/332], loss=8.9895
	step [134/332], loss=8.5521
	step [135/332], loss=9.7035
	step [136/332], loss=8.9428
	step [137/332], loss=8.6983
	step [138/332], loss=8.2537
	step [139/332], loss=9.3250
	step [140/332], loss=8.4767
	step [141/332], loss=9.7239
	step [142/332], loss=6.7693
	step [143/332], loss=9.2901
	step [144/332], loss=10.1058
	step [145/332], loss=8.5402
	step [146/332], loss=9.7499
	step [147/332], loss=8.8463
	step [148/332], loss=7.6738
	step [149/332], loss=8.5139
	step [150/332], loss=9.3158
	step [151/332], loss=9.9621
	step [152/332], loss=9.2898
	step [153/332], loss=8.0364
	step [154/332], loss=9.2598
	step [155/332], loss=7.9530
	step [156/332], loss=8.5718
	step [157/332], loss=8.6847
	step [158/332], loss=7.4810
	step [159/332], loss=7.0756
	step [160/332], loss=8.0341
	step [161/332], loss=8.3030
	step [162/332], loss=9.0863
	step [163/332], loss=7.9454
	step [164/332], loss=7.7543
	step [165/332], loss=11.7343
	step [166/332], loss=7.8771
	step [167/332], loss=9.4499
	step [168/332], loss=9.5990
	step [169/332], loss=8.6779
	step [170/332], loss=7.6279
	step [171/332], loss=8.3513
	step [172/332], loss=7.3063
	step [173/332], loss=8.6531
	step [174/332], loss=8.8553
	step [175/332], loss=8.4813
	step [176/332], loss=8.0102
	step [177/332], loss=9.2201
	step [178/332], loss=7.3898
	step [179/332], loss=8.2647
	step [180/332], loss=9.2858
	step [181/332], loss=7.6261
	step [182/332], loss=9.1395
	step [183/332], loss=9.1931
	step [184/332], loss=7.1170
	step [185/332], loss=7.9805
	step [186/332], loss=10.6013
	step [187/332], loss=8.8121
	step [188/332], loss=7.6736
	step [189/332], loss=9.4795
	step [190/332], loss=7.3240
	step [191/332], loss=8.6227
	step [192/332], loss=7.9602
	step [193/332], loss=9.1975
	step [194/332], loss=6.9504
	step [195/332], loss=8.7571
	step [196/332], loss=8.7964
	step [197/332], loss=8.8447
	step [198/332], loss=8.1075
	step [199/332], loss=7.7619
	step [200/332], loss=7.1420
	step [201/332], loss=7.9972
	step [202/332], loss=7.6700
	step [203/332], loss=8.5451
	step [204/332], loss=10.0018
	step [205/332], loss=8.0993
	step [206/332], loss=9.5156
	step [207/332], loss=9.2109
	step [208/332], loss=9.9407
	step [209/332], loss=8.8597
	step [210/332], loss=11.1719
	step [211/332], loss=8.2856
	step [212/332], loss=10.0548
	step [213/332], loss=10.2113
	step [214/332], loss=7.9508
	step [215/332], loss=8.6471
	step [216/332], loss=10.2376
	step [217/332], loss=6.9408
	step [218/332], loss=8.1433
	step [219/332], loss=8.9745
	step [220/332], loss=8.6937
	step [221/332], loss=8.3485
	step [222/332], loss=9.1576
	step [223/332], loss=7.1409
	step [224/332], loss=8.6419
	step [225/332], loss=7.4485
	step [226/332], loss=9.4102
	step [227/332], loss=8.9690
	step [228/332], loss=8.6062
	step [229/332], loss=7.1831
	step [230/332], loss=6.4295
	step [231/332], loss=8.1066
	step [232/332], loss=9.1375
	step [233/332], loss=7.2958
	step [234/332], loss=8.2688
	step [235/332], loss=10.0225
	step [236/332], loss=8.2514
	step [237/332], loss=9.2291
	step [238/332], loss=10.1096
	step [239/332], loss=8.7527
	step [240/332], loss=8.6214
	step [241/332], loss=8.5987
	step [242/332], loss=7.4376
	step [243/332], loss=8.7056
	step [244/332], loss=7.3526
	step [245/332], loss=8.8668
	step [246/332], loss=8.2321
	step [247/332], loss=9.7881
	step [248/332], loss=9.4483
	step [249/332], loss=9.8865
	step [250/332], loss=9.1902
	step [251/332], loss=9.2911
	step [252/332], loss=7.1913
	step [253/332], loss=7.7737
	step [254/332], loss=7.4801
	step [255/332], loss=7.8372
	step [256/332], loss=9.5152
	step [257/332], loss=8.4421
	step [258/332], loss=8.2174
	step [259/332], loss=9.4390
	step [260/332], loss=9.7304
	step [261/332], loss=10.9105
	step [262/332], loss=6.8454
	step [263/332], loss=9.1784
	step [264/332], loss=7.9265
	step [265/332], loss=8.2793
	step [266/332], loss=8.7111
	step [267/332], loss=9.8368
	step [268/332], loss=7.7080
	step [269/332], loss=9.2858
	step [270/332], loss=10.8341
	step [271/332], loss=7.8196
	step [272/332], loss=8.2295
	step [273/332], loss=8.6613
	step [274/332], loss=8.8532
	step [275/332], loss=7.6526
	step [276/332], loss=8.3572
	step [277/332], loss=7.9752
	step [278/332], loss=7.6293
	step [279/332], loss=8.6717
	step [280/332], loss=9.4683
	step [281/332], loss=9.1147
	step [282/332], loss=7.4652
	step [283/332], loss=8.7050
	step [284/332], loss=8.4099
	step [285/332], loss=9.0147
	step [286/332], loss=7.7301
	step [287/332], loss=6.7882
	step [288/332], loss=9.3098
	step [289/332], loss=10.1598
	step [290/332], loss=8.8357
	step [291/332], loss=7.7300
	step [292/332], loss=8.6129
	step [293/332], loss=8.0026
	step [294/332], loss=9.1311
	step [295/332], loss=6.9413
	step [296/332], loss=8.4712
	step [297/332], loss=6.4411
	step [298/332], loss=6.9238
	step [299/332], loss=9.4869
	step [300/332], loss=8.9406
	step [301/332], loss=9.1610
	step [302/332], loss=8.7657
	step [303/332], loss=8.6300
	step [304/332], loss=9.6779
	step [305/332], loss=7.2113
	step [306/332], loss=6.3259
	step [307/332], loss=8.3527
	step [308/332], loss=7.1817
	step [309/332], loss=8.9393
	step [310/332], loss=7.4345
	step [311/332], loss=6.6110
	step [312/332], loss=8.8894
	step [313/332], loss=8.8529
	step [314/332], loss=6.9849
	step [315/332], loss=7.9190
	step [316/332], loss=7.5973
	step [317/332], loss=6.8354
	step [318/332], loss=6.7914
	step [319/332], loss=8.2335
	step [320/332], loss=7.0691
	step [321/332], loss=7.8863
	step [322/332], loss=8.5935
	step [323/332], loss=9.1734
	step [324/332], loss=7.9982
	step [325/332], loss=6.7619
	step [326/332], loss=8.0310
	step [327/332], loss=9.3531
	step [328/332], loss=8.8443
	step [329/332], loss=6.7500
	step [330/332], loss=9.7214
	step [331/332], loss=8.1540
	step [332/332], loss=5.4339
	Evaluating
	loss=0.0352, precision=0.1278, recall=0.9972, f1=0.2265
Training epoch 9
	step [1/332], loss=6.9246
	step [2/332], loss=7.7904
	step [3/332], loss=8.5016
	step [4/332], loss=7.7920
	step [5/332], loss=8.9672
	step [6/332], loss=8.9525
	step [7/332], loss=8.4974
	step [8/332], loss=8.0923
	step [9/332], loss=8.1050
	step [10/332], loss=8.6032
	step [11/332], loss=9.0320
	step [12/332], loss=6.3075
	step [13/332], loss=8.2054
	step [14/332], loss=6.5339
	step [15/332], loss=8.0958
	step [16/332], loss=7.9159
	step [17/332], loss=9.2079
	step [18/332], loss=7.2971
	step [19/332], loss=7.4685
	step [20/332], loss=8.4064
	step [21/332], loss=6.9838
	step [22/332], loss=7.3155
	step [23/332], loss=7.7807
	step [24/332], loss=8.9148
	step [25/332], loss=7.1689
	step [26/332], loss=7.9794
	step [27/332], loss=8.6466
	step [28/332], loss=6.6098
	step [29/332], loss=7.8919
	step [30/332], loss=8.0013
	step [31/332], loss=7.6427
	step [32/332], loss=7.7639
	step [33/332], loss=7.7234
	step [34/332], loss=8.3005
	step [35/332], loss=7.1522
	step [36/332], loss=10.0336
	step [37/332], loss=8.3060
	step [38/332], loss=7.3206
	step [39/332], loss=8.4753
	step [40/332], loss=8.1873
	step [41/332], loss=8.4120
	step [42/332], loss=9.1339
	step [43/332], loss=8.8683
	step [44/332], loss=6.5814
	step [45/332], loss=7.1343
	step [46/332], loss=8.9098
	step [47/332], loss=7.2891
	step [48/332], loss=7.7431
	step [49/332], loss=7.7448
	step [50/332], loss=8.2023
	step [51/332], loss=6.6369
	step [52/332], loss=9.1017
	step [53/332], loss=8.3309
	step [54/332], loss=8.3570
	step [55/332], loss=7.7785
	step [56/332], loss=8.4975
	step [57/332], loss=8.1112
	step [58/332], loss=9.6305
	step [59/332], loss=7.5351
	step [60/332], loss=8.8782
	step [61/332], loss=7.6661
	step [62/332], loss=7.5069
	step [63/332], loss=7.9798
	step [64/332], loss=8.3662
	step [65/332], loss=10.9859
	step [66/332], loss=7.3132
	step [67/332], loss=7.5526
	step [68/332], loss=7.4847
	step [69/332], loss=7.5485
	step [70/332], loss=9.7498
	step [71/332], loss=8.1821
	step [72/332], loss=6.8218
	step [73/332], loss=9.5409
	step [74/332], loss=8.9718
	step [75/332], loss=8.7444
	step [76/332], loss=7.1620
	step [77/332], loss=9.8755
	step [78/332], loss=8.4530
	step [79/332], loss=9.3788
	step [80/332], loss=7.3452
	step [81/332], loss=7.6680
	step [82/332], loss=8.5329
	step [83/332], loss=8.0968
	step [84/332], loss=7.7662
	step [85/332], loss=9.5020
	step [86/332], loss=7.4332
	step [87/332], loss=7.7506
	step [88/332], loss=8.1530
	step [89/332], loss=8.7949
	step [90/332], loss=7.4695
	step [91/332], loss=9.3004
	step [92/332], loss=7.2647
	step [93/332], loss=6.7776
	step [94/332], loss=8.2033
	step [95/332], loss=8.3540
	step [96/332], loss=8.7103
	step [97/332], loss=8.6016
	step [98/332], loss=8.6472
	step [99/332], loss=8.0507
	step [100/332], loss=10.3179
	step [101/332], loss=6.5533
	step [102/332], loss=7.7405
	step [103/332], loss=8.4793
	step [104/332], loss=6.4195
	step [105/332], loss=10.3361
	step [106/332], loss=8.7305
	step [107/332], loss=7.1667
	step [108/332], loss=8.6380
	step [109/332], loss=8.3739
	step [110/332], loss=8.8982
	step [111/332], loss=8.6440
	step [112/332], loss=8.8438
	step [113/332], loss=6.9846
	step [114/332], loss=7.9248
	step [115/332], loss=9.1144
	step [116/332], loss=7.6767
	step [117/332], loss=7.5779
	step [118/332], loss=8.2510
	step [119/332], loss=7.8996
	step [120/332], loss=9.3791
	step [121/332], loss=7.3996
	step [122/332], loss=8.9159
	step [123/332], loss=7.3511
	step [124/332], loss=10.1983
	step [125/332], loss=8.2458
	step [126/332], loss=7.9951
	step [127/332], loss=9.1462
	step [128/332], loss=8.2882
	step [129/332], loss=8.9456
	step [130/332], loss=7.2005
	step [131/332], loss=7.2527
	step [132/332], loss=9.6590
	step [133/332], loss=8.6575
	step [134/332], loss=6.6610
	step [135/332], loss=7.4706
	step [136/332], loss=6.8779
	step [137/332], loss=8.2698
	step [138/332], loss=7.8446
	step [139/332], loss=8.4151
	step [140/332], loss=6.5712
	step [141/332], loss=7.5221
	step [142/332], loss=8.1633
	step [143/332], loss=6.6917
	step [144/332], loss=8.4588
	step [145/332], loss=7.7103
	step [146/332], loss=7.8559
	step [147/332], loss=9.8695
	step [148/332], loss=7.4952
	step [149/332], loss=6.7982
	step [150/332], loss=8.2745
	step [151/332], loss=8.1010
	step [152/332], loss=8.1239
	step [153/332], loss=8.2485
	step [154/332], loss=8.0170
	step [155/332], loss=8.9726
	step [156/332], loss=9.7905
	step [157/332], loss=7.9809
	step [158/332], loss=7.9650
	step [159/332], loss=6.5271
	step [160/332], loss=7.6791
	step [161/332], loss=7.8477
	step [162/332], loss=8.2164
	step [163/332], loss=8.1605
	step [164/332], loss=8.4749
	step [165/332], loss=7.4866
	step [166/332], loss=9.4545
	step [167/332], loss=7.8449
	step [168/332], loss=7.7480
	step [169/332], loss=8.2005
	step [170/332], loss=7.6085
	step [171/332], loss=8.4707
	step [172/332], loss=6.8975
	step [173/332], loss=6.7544
	step [174/332], loss=7.2353
	step [175/332], loss=8.3658
	step [176/332], loss=5.9828
	step [177/332], loss=8.4414
	step [178/332], loss=8.5017
	step [179/332], loss=9.2097
	step [180/332], loss=6.8894
	step [181/332], loss=6.9835
	step [182/332], loss=7.3408
	step [183/332], loss=9.1640
	step [184/332], loss=7.1990
	step [185/332], loss=9.0892
	step [186/332], loss=7.9956
	step [187/332], loss=7.4608
	step [188/332], loss=7.0555
	step [189/332], loss=7.0035
	step [190/332], loss=9.3657
	step [191/332], loss=7.7471
	step [192/332], loss=7.6890
	step [193/332], loss=8.3413
	step [194/332], loss=7.5169
	step [195/332], loss=8.5426
	step [196/332], loss=10.5528
	step [197/332], loss=7.6315
	step [198/332], loss=7.5951
	step [199/332], loss=8.1307
	step [200/332], loss=8.2228
	step [201/332], loss=7.9361
	step [202/332], loss=7.0440
	step [203/332], loss=7.2288
	step [204/332], loss=7.1748
	step [205/332], loss=6.9675
	step [206/332], loss=7.8478
	step [207/332], loss=8.6218
	step [208/332], loss=8.7294
	step [209/332], loss=9.6347
	step [210/332], loss=8.6313
	step [211/332], loss=9.3033
	step [212/332], loss=7.7743
	step [213/332], loss=8.3656
	step [214/332], loss=7.4164
	step [215/332], loss=9.7514
	step [216/332], loss=6.4407
	step [217/332], loss=6.7351
	step [218/332], loss=8.2965
	step [219/332], loss=8.2729
	step [220/332], loss=9.1001
	step [221/332], loss=7.2188
	step [222/332], loss=7.4518
	step [223/332], loss=7.4913
	step [224/332], loss=6.7051
	step [225/332], loss=8.3522
	step [226/332], loss=6.8606
	step [227/332], loss=8.6663
	step [228/332], loss=10.1913
	step [229/332], loss=7.6407
	step [230/332], loss=6.8037
	step [231/332], loss=6.3882
	step [232/332], loss=7.2093
	step [233/332], loss=5.8068
	step [234/332], loss=7.7139
	step [235/332], loss=7.7152
	step [236/332], loss=8.2608
	step [237/332], loss=7.0475
	step [238/332], loss=9.0728
	step [239/332], loss=8.1215
	step [240/332], loss=8.0147
	step [241/332], loss=9.0423
	step [242/332], loss=7.3843
	step [243/332], loss=6.8926
	step [244/332], loss=6.4699
	step [245/332], loss=8.2514
	step [246/332], loss=9.3620
	step [247/332], loss=5.5590
	step [248/332], loss=8.2197
	step [249/332], loss=7.9591
	step [250/332], loss=6.6898
	step [251/332], loss=6.9926
	step [252/332], loss=9.2410
	step [253/332], loss=7.6633
	step [254/332], loss=7.8416
	step [255/332], loss=8.4541
	step [256/332], loss=7.7964
	step [257/332], loss=9.0526
	step [258/332], loss=9.8804
	step [259/332], loss=7.3003
	step [260/332], loss=7.6297
	step [261/332], loss=6.9780
	step [262/332], loss=7.0662
	step [263/332], loss=8.1695
	step [264/332], loss=10.3376
	step [265/332], loss=8.7874
	step [266/332], loss=9.2703
	step [267/332], loss=8.0096
	step [268/332], loss=8.0454
	step [269/332], loss=7.9639
	step [270/332], loss=8.0210
	step [271/332], loss=9.3189
	step [272/332], loss=8.2464
	step [273/332], loss=8.2593
	step [274/332], loss=6.2978
	step [275/332], loss=8.6549
	step [276/332], loss=8.3608
	step [277/332], loss=8.4078
	step [278/332], loss=7.0725
	step [279/332], loss=7.1525
	step [280/332], loss=7.9700
	step [281/332], loss=7.0450
	step [282/332], loss=9.3953
	step [283/332], loss=8.8388
	step [284/332], loss=7.8629
	step [285/332], loss=8.4119
	step [286/332], loss=7.0014
	step [287/332], loss=6.4435
	step [288/332], loss=9.9685
	step [289/332], loss=6.8440
	step [290/332], loss=7.9334
	step [291/332], loss=9.2714
	step [292/332], loss=6.5485
	step [293/332], loss=7.7082
	step [294/332], loss=6.2310
	step [295/332], loss=7.6924
	step [296/332], loss=6.6649
	step [297/332], loss=7.2183
	step [298/332], loss=7.1345
	step [299/332], loss=8.8943
	step [300/332], loss=7.2936
	step [301/332], loss=8.3371
	step [302/332], loss=8.1728
	step [303/332], loss=9.3688
	step [304/332], loss=6.8101
	step [305/332], loss=8.4745
	step [306/332], loss=7.0819
	step [307/332], loss=8.1903
	step [308/332], loss=8.3306
	step [309/332], loss=8.1466
	step [310/332], loss=6.9084
	step [311/332], loss=7.8044
	step [312/332], loss=7.6834
	step [313/332], loss=7.5194
	step [314/332], loss=8.5674
	step [315/332], loss=6.6296
	step [316/332], loss=7.5667
	step [317/332], loss=7.3842
	step [318/332], loss=8.2014
	step [319/332], loss=7.8604
	step [320/332], loss=7.0759
	step [321/332], loss=7.7921
	step [322/332], loss=8.0347
	step [323/332], loss=7.5698
	step [324/332], loss=7.3196
	step [325/332], loss=6.9074
	step [326/332], loss=8.0923
	step [327/332], loss=7.8118
	step [328/332], loss=9.6882
	step [329/332], loss=8.0887
	step [330/332], loss=7.8592
	step [331/332], loss=6.3835
	step [332/332], loss=4.1480
	Evaluating
	loss=0.0312, precision=0.1482, recall=0.9970, f1=0.2581
Training epoch 10
	step [1/332], loss=6.4039
	step [2/332], loss=6.1334
	step [3/332], loss=7.1624
	step [4/332], loss=8.0115
	step [5/332], loss=7.4321
	step [6/332], loss=8.1666
	step [7/332], loss=10.4372
	step [8/332], loss=7.9411
	step [9/332], loss=7.3778
	step [10/332], loss=7.9279
	step [11/332], loss=7.8415
	step [12/332], loss=6.9721
	step [13/332], loss=8.4452
	step [14/332], loss=7.9649
	step [15/332], loss=7.0649
	step [16/332], loss=6.9719
	step [17/332], loss=7.4821
	step [18/332], loss=8.4486
	step [19/332], loss=6.0032
	step [20/332], loss=8.9799
	step [21/332], loss=7.8777
	step [22/332], loss=9.3408
	step [23/332], loss=9.2812
	step [24/332], loss=8.7147
	step [25/332], loss=7.1713
	step [26/332], loss=6.0945
	step [27/332], loss=7.4254
	step [28/332], loss=8.3000
	step [29/332], loss=6.8463
	step [30/332], loss=6.6187
	step [31/332], loss=9.0156
	step [32/332], loss=7.9454
	step [33/332], loss=7.6126
	step [34/332], loss=6.8924
	step [35/332], loss=7.5895
	step [36/332], loss=6.7020
	step [37/332], loss=7.9318
	step [38/332], loss=7.4050
	step [39/332], loss=7.1213
	step [40/332], loss=7.8454
	step [41/332], loss=6.9564
	step [42/332], loss=7.3941
	step [43/332], loss=8.9457
	step [44/332], loss=7.9739
	step [45/332], loss=6.8096
	step [46/332], loss=7.9541
	step [47/332], loss=6.7235
	step [48/332], loss=7.6187
	step [49/332], loss=6.8967
	step [50/332], loss=7.9393
	step [51/332], loss=9.1963
	step [52/332], loss=7.2939
	step [53/332], loss=7.7300
	step [54/332], loss=7.6715
	step [55/332], loss=6.2093
	step [56/332], loss=9.7327
	step [57/332], loss=6.8173
	step [58/332], loss=7.7363
	step [59/332], loss=6.2745
	step [60/332], loss=8.6189
	step [61/332], loss=6.0193
	step [62/332], loss=6.6409
	step [63/332], loss=7.1679
	step [64/332], loss=7.1915
	step [65/332], loss=7.3172
	step [66/332], loss=7.8690
	step [67/332], loss=7.5839
	step [68/332], loss=8.0844
	step [69/332], loss=6.8751
	step [70/332], loss=7.5901
	step [71/332], loss=6.0418
	step [72/332], loss=7.1319
	step [73/332], loss=6.9613
	step [74/332], loss=7.8480
	step [75/332], loss=8.1118
	step [76/332], loss=7.0032
	step [77/332], loss=7.2916
	step [78/332], loss=6.6447
	step [79/332], loss=9.4627
	step [80/332], loss=7.7144
	step [81/332], loss=6.0318
	step [82/332], loss=7.7765
	step [83/332], loss=6.8546
	step [84/332], loss=6.6847
	step [85/332], loss=7.2064
	step [86/332], loss=8.7370
	step [87/332], loss=7.4018
	step [88/332], loss=7.1355
	step [89/332], loss=7.9307
	step [90/332], loss=7.4478
	step [91/332], loss=9.4783
	step [92/332], loss=7.5322
	step [93/332], loss=7.1211
	step [94/332], loss=6.3402
	step [95/332], loss=6.6386
	step [96/332], loss=9.0671
	step [97/332], loss=6.1989
	step [98/332], loss=6.3260
	step [99/332], loss=6.9301
	step [100/332], loss=6.2143
	step [101/332], loss=7.8186
	step [102/332], loss=6.5046
	step [103/332], loss=6.8759
	step [104/332], loss=5.0748
	step [105/332], loss=8.3308
	step [106/332], loss=8.9594
	step [107/332], loss=6.9939
	step [108/332], loss=7.5268
	step [109/332], loss=6.2912
	step [110/332], loss=7.6230
	step [111/332], loss=8.0019
	step [112/332], loss=5.7965
	step [113/332], loss=7.1224
	step [114/332], loss=7.5656
	step [115/332], loss=9.1973
	step [116/332], loss=7.2909
	step [117/332], loss=7.2388
	step [118/332], loss=7.1798
	step [119/332], loss=6.7602
	step [120/332], loss=8.2357
	step [121/332], loss=8.3793
	step [122/332], loss=8.2148
	step [123/332], loss=7.5621
	step [124/332], loss=8.1261
	step [125/332], loss=8.0382
	step [126/332], loss=5.9162
	step [127/332], loss=7.0232
	step [128/332], loss=6.6888
	step [129/332], loss=7.6853
	step [130/332], loss=7.4747
	step [131/332], loss=9.0368
	step [132/332], loss=6.1076
	step [133/332], loss=7.9911
	step [134/332], loss=8.0379
	step [135/332], loss=9.6054
	step [136/332], loss=8.4223
	step [137/332], loss=7.8410
	step [138/332], loss=7.0634
	step [139/332], loss=7.5370
	step [140/332], loss=6.6455
	step [141/332], loss=9.5380
	step [142/332], loss=6.5347
	step [143/332], loss=7.4420
	step [144/332], loss=7.4329
	step [145/332], loss=7.0342
	step [146/332], loss=7.4425
	step [147/332], loss=8.7150
	step [148/332], loss=6.7413
	step [149/332], loss=6.4890
	step [150/332], loss=7.3350
	step [151/332], loss=6.3601
	step [152/332], loss=6.9103
	step [153/332], loss=8.3715
	step [154/332], loss=8.2795
	step [155/332], loss=7.9120
	step [156/332], loss=7.6203
	step [157/332], loss=8.6979
	step [158/332], loss=8.8144
	step [159/332], loss=6.9409
	step [160/332], loss=7.3029
	step [161/332], loss=6.7681
	step [162/332], loss=8.5181
	step [163/332], loss=7.7749
	step [164/332], loss=7.0691
	step [165/332], loss=7.2300
	step [166/332], loss=7.1601
	step [167/332], loss=8.9529
	step [168/332], loss=7.0353
	step [169/332], loss=8.2973
	step [170/332], loss=9.3246
	step [171/332], loss=7.7075
	step [172/332], loss=6.2137
	step [173/332], loss=7.4796
	step [174/332], loss=7.5387
	step [175/332], loss=9.2672
	step [176/332], loss=6.4264
	step [177/332], loss=8.5158
	step [178/332], loss=6.6785
	step [179/332], loss=7.5828
	step [180/332], loss=8.7113
	step [181/332], loss=8.7321
	step [182/332], loss=7.8231
	step [183/332], loss=8.5140
	step [184/332], loss=8.4272
	step [185/332], loss=8.6840
	step [186/332], loss=7.3604
	step [187/332], loss=6.2374
	step [188/332], loss=7.0798
	step [189/332], loss=7.4097
	step [190/332], loss=8.3253
	step [191/332], loss=7.7772
	step [192/332], loss=6.0509
	step [193/332], loss=7.8979
	step [194/332], loss=8.7571
	step [195/332], loss=6.6719
	step [196/332], loss=9.4559
	step [197/332], loss=7.7128
	step [198/332], loss=8.1845
	step [199/332], loss=7.8315
	step [200/332], loss=7.2254
	step [201/332], loss=7.9561
	step [202/332], loss=9.2139
	step [203/332], loss=7.1965
	step [204/332], loss=6.7675
	step [205/332], loss=8.8274
	step [206/332], loss=6.4438
	step [207/332], loss=6.7454
	step [208/332], loss=6.5678
	step [209/332], loss=6.7403
	step [210/332], loss=6.7671
	step [211/332], loss=8.0786
	step [212/332], loss=7.2491
	step [213/332], loss=8.7807
	step [214/332], loss=9.0750
	step [215/332], loss=8.8819
	step [216/332], loss=5.9206
	step [217/332], loss=8.0526
	step [218/332], loss=7.3926
	step [219/332], loss=6.2567
	step [220/332], loss=7.3083
	step [221/332], loss=6.5287
	step [222/332], loss=6.8858
	step [223/332], loss=6.0767
	step [224/332], loss=6.2770
	step [225/332], loss=8.3390
	step [226/332], loss=7.9233
	step [227/332], loss=7.6459
	step [228/332], loss=6.5723
	step [229/332], loss=6.7134
	step [230/332], loss=5.8469
	step [231/332], loss=6.4526
	step [232/332], loss=7.0644
	step [233/332], loss=8.0342
	step [234/332], loss=6.8424
	step [235/332], loss=8.2925
	step [236/332], loss=9.6985
	step [237/332], loss=8.1142
	step [238/332], loss=8.0462
	step [239/332], loss=7.9795
	step [240/332], loss=6.3835
	step [241/332], loss=8.6833
	step [242/332], loss=8.7571
	step [243/332], loss=7.5051
	step [244/332], loss=6.6529
	step [245/332], loss=7.3359
	step [246/332], loss=8.0729
	step [247/332], loss=7.9287
	step [248/332], loss=6.8208
	step [249/332], loss=7.7831
	step [250/332], loss=6.9434
	step [251/332], loss=7.6545
	step [252/332], loss=7.2153
	step [253/332], loss=5.7103
	step [254/332], loss=8.3306
	step [255/332], loss=6.8110
	step [256/332], loss=7.2981
	step [257/332], loss=7.4239
	step [258/332], loss=8.8692
	step [259/332], loss=6.7235
	step [260/332], loss=8.9471
	step [261/332], loss=8.8290
	step [262/332], loss=7.0474
	step [263/332], loss=6.7997
	step [264/332], loss=7.2049
	step [265/332], loss=6.0323
	step [266/332], loss=8.0850
	step [267/332], loss=9.9995
	step [268/332], loss=6.3754
	step [269/332], loss=5.7321
	step [270/332], loss=7.0820
	step [271/332], loss=7.3927
	step [272/332], loss=7.0897
	step [273/332], loss=8.6230
	step [274/332], loss=7.9173
	step [275/332], loss=6.1480
	step [276/332], loss=6.8923
	step [277/332], loss=6.4596
	step [278/332], loss=9.1571
	step [279/332], loss=8.3104
	step [280/332], loss=7.6424
	step [281/332], loss=7.6812
	step [282/332], loss=7.3034
	step [283/332], loss=8.6362
	step [284/332], loss=7.1780
	step [285/332], loss=6.3126
	step [286/332], loss=7.3425
	step [287/332], loss=7.9774
	step [288/332], loss=7.6978
	step [289/332], loss=8.9856
	step [290/332], loss=7.1200
	step [291/332], loss=7.2950
	step [292/332], loss=7.2076
	step [293/332], loss=6.4109
	step [294/332], loss=6.1090
	step [295/332], loss=8.9427
	step [296/332], loss=6.3277
	step [297/332], loss=8.7103
	step [298/332], loss=7.6095
	step [299/332], loss=7.8425
	step [300/332], loss=7.3426
	step [301/332], loss=7.0348
	step [302/332], loss=8.0349
	step [303/332], loss=6.0412
	step [304/332], loss=8.7715
	step [305/332], loss=7.4171
	step [306/332], loss=8.4940
	step [307/332], loss=6.8511
	step [308/332], loss=6.7065
	step [309/332], loss=5.6191
	step [310/332], loss=7.2496
	step [311/332], loss=5.8568
	step [312/332], loss=6.8409
	step [313/332], loss=7.2437
	step [314/332], loss=7.5097
	step [315/332], loss=6.6620
	step [316/332], loss=7.3401
	step [317/332], loss=7.3597
	step [318/332], loss=5.8602
	step [319/332], loss=5.9841
	step [320/332], loss=6.8863
	step [321/332], loss=10.1982
	step [322/332], loss=9.0157
	step [323/332], loss=7.2298
	step [324/332], loss=7.8085
	step [325/332], loss=7.7870
	step [326/332], loss=7.1124
	step [327/332], loss=7.9897
	step [328/332], loss=7.3930
	step [329/332], loss=7.9042
	step [330/332], loss=6.8822
	step [331/332], loss=6.7899
	step [332/332], loss=4.5065
	Evaluating
	loss=0.0320, precision=0.1280, recall=0.9970, f1=0.2269
Training epoch 11
	step [1/332], loss=8.0736
	step [2/332], loss=8.9052
	step [3/332], loss=7.6170
	step [4/332], loss=7.4107
	step [5/332], loss=6.9087
	step [6/332], loss=6.0392
	step [7/332], loss=6.7507
	step [8/332], loss=6.8660
	step [9/332], loss=7.7298
	step [10/332], loss=8.4138
	step [11/332], loss=6.1977
	step [12/332], loss=7.6456
	step [13/332], loss=7.5556
	step [14/332], loss=7.8012
	step [15/332], loss=7.4297
	step [16/332], loss=6.4157
	step [17/332], loss=8.1084
	step [18/332], loss=9.5715
	step [19/332], loss=7.7145
	step [20/332], loss=6.9284
	step [21/332], loss=7.0889
	step [22/332], loss=6.5584
	step [23/332], loss=8.4809
	step [24/332], loss=8.7827
	step [25/332], loss=6.9822
	step [26/332], loss=8.4419
	step [27/332], loss=7.4682
	step [28/332], loss=7.2238
	step [29/332], loss=6.3253
	step [30/332], loss=7.3400
	step [31/332], loss=8.8942
	step [32/332], loss=7.4116
	step [33/332], loss=7.2321
	step [34/332], loss=7.0843
	step [35/332], loss=6.7062
	step [36/332], loss=8.3611
	step [37/332], loss=6.5424
	step [38/332], loss=7.5502
	step [39/332], loss=9.5247
	step [40/332], loss=7.8759
	step [41/332], loss=7.6800
	step [42/332], loss=7.7029
	step [43/332], loss=6.9790
	step [44/332], loss=6.8473
	step [45/332], loss=8.3692
	step [46/332], loss=6.5189
	step [47/332], loss=8.3758
	step [48/332], loss=5.8970
	step [49/332], loss=7.0793
	step [50/332], loss=7.4275
	step [51/332], loss=6.7734
	step [52/332], loss=8.5261
	step [53/332], loss=8.2297
	step [54/332], loss=7.7520
	step [55/332], loss=7.3393
	step [56/332], loss=6.9349
	step [57/332], loss=7.5186
	step [58/332], loss=8.9548
	step [59/332], loss=6.9756
	step [60/332], loss=5.9846
	step [61/332], loss=8.3128
	step [62/332], loss=6.0718
	step [63/332], loss=7.6482
	step [64/332], loss=6.7242
	step [65/332], loss=7.1135
	step [66/332], loss=8.8337
	step [67/332], loss=6.8407
	step [68/332], loss=5.5444
	step [69/332], loss=8.3643
	step [70/332], loss=6.9422
	step [71/332], loss=7.2904
	step [72/332], loss=5.8650
	step [73/332], loss=8.1336
	step [74/332], loss=7.4047
	step [75/332], loss=5.3766
	step [76/332], loss=6.4523
	step [77/332], loss=7.0662
	step [78/332], loss=6.5790
	step [79/332], loss=7.0626
	step [80/332], loss=6.9000
	step [81/332], loss=8.6480
	step [82/332], loss=6.7661
	step [83/332], loss=7.7243
	step [84/332], loss=6.8380
	step [85/332], loss=5.6996
	step [86/332], loss=7.2594
	step [87/332], loss=6.2368
	step [88/332], loss=5.5909
	step [89/332], loss=5.8650
	step [90/332], loss=7.8809
	step [91/332], loss=7.1593
	step [92/332], loss=7.2832
	step [93/332], loss=7.2601
	step [94/332], loss=6.5427
	step [95/332], loss=8.5757
	step [96/332], loss=8.3027
	step [97/332], loss=7.3250
	step [98/332], loss=7.7147
	step [99/332], loss=7.4842
	step [100/332], loss=7.1723
	step [101/332], loss=6.3354
	step [102/332], loss=7.8574
	step [103/332], loss=6.6906
	step [104/332], loss=7.2065
	step [105/332], loss=6.0789
	step [106/332], loss=6.7992
	step [107/332], loss=8.7711
	step [108/332], loss=7.5553
	step [109/332], loss=6.3707
	step [110/332], loss=8.2452
	step [111/332], loss=6.6500
	step [112/332], loss=5.7812
	step [113/332], loss=6.5826
	step [114/332], loss=6.0047
	step [115/332], loss=6.8409
	step [116/332], loss=7.3705
	step [117/332], loss=6.5689
	step [118/332], loss=5.6153
	step [119/332], loss=6.1273
	step [120/332], loss=8.1297
	step [121/332], loss=7.9842
	step [122/332], loss=7.0283
	step [123/332], loss=6.4533
	step [124/332], loss=7.4763
	step [125/332], loss=8.1738
	step [126/332], loss=8.3018
	step [127/332], loss=6.7985
	step [128/332], loss=5.7590
	step [129/332], loss=7.8231
	step [130/332], loss=5.9862
	step [131/332], loss=6.2074
	step [132/332], loss=7.6339
	step [133/332], loss=7.2036
	step [134/332], loss=6.6024
	step [135/332], loss=6.1815
	step [136/332], loss=7.4385
	step [137/332], loss=6.4258
	step [138/332], loss=5.0709
	step [139/332], loss=7.0531
	step [140/332], loss=8.6369
	step [141/332], loss=6.4517
	step [142/332], loss=7.2856
	step [143/332], loss=5.7064
	step [144/332], loss=7.8910
	step [145/332], loss=7.1498
	step [146/332], loss=7.4525
	step [147/332], loss=6.7826
	step [148/332], loss=6.2868
	step [149/332], loss=8.0889
	step [150/332], loss=8.6334
	step [151/332], loss=6.3876
	step [152/332], loss=7.6507
	step [153/332], loss=7.1141
	step [154/332], loss=5.8415
	step [155/332], loss=6.7262
	step [156/332], loss=7.6775
	step [157/332], loss=5.7975
	step [158/332], loss=8.4142
	step [159/332], loss=7.9885
	step [160/332], loss=7.4989
	step [161/332], loss=7.7460
	step [162/332], loss=6.6586
	step [163/332], loss=8.0653
	step [164/332], loss=7.6047
	step [165/332], loss=8.6423
	step [166/332], loss=6.8221
	step [167/332], loss=7.1522
	step [168/332], loss=6.3024
	step [169/332], loss=6.3068
	step [170/332], loss=6.2542
	step [171/332], loss=6.3419
	step [172/332], loss=6.8494
	step [173/332], loss=6.9434
	step [174/332], loss=7.1731
	step [175/332], loss=6.7546
	step [176/332], loss=6.2545
	step [177/332], loss=7.1869
	step [178/332], loss=6.6823
	step [179/332], loss=6.2607
	step [180/332], loss=5.0886
	step [181/332], loss=7.1689
	step [182/332], loss=7.2595
	step [183/332], loss=6.4677
	step [184/332], loss=6.1872
	step [185/332], loss=8.1964
	step [186/332], loss=4.7652
	step [187/332], loss=5.6752
	step [188/332], loss=5.8492
	step [189/332], loss=8.2849
	step [190/332], loss=6.8806
	step [191/332], loss=7.0115
	step [192/332], loss=5.7622
	step [193/332], loss=6.1387
	step [194/332], loss=8.4087
	step [195/332], loss=6.6373
	step [196/332], loss=7.5311
	step [197/332], loss=6.6816
	step [198/332], loss=7.3877
	step [199/332], loss=5.7321
	step [200/332], loss=6.0611
	step [201/332], loss=7.6652
	step [202/332], loss=6.9248
	step [203/332], loss=9.4362
	step [204/332], loss=6.5865
	step [205/332], loss=6.7736
	step [206/332], loss=7.7234
	step [207/332], loss=7.1347
	step [208/332], loss=6.1461
	step [209/332], loss=6.9211
	step [210/332], loss=7.1493
	step [211/332], loss=7.0607
	step [212/332], loss=7.0032
	step [213/332], loss=6.3261
	step [214/332], loss=5.8580
	step [215/332], loss=6.7729
	step [216/332], loss=8.0726
	step [217/332], loss=7.2382
	step [218/332], loss=7.2730
	step [219/332], loss=5.6778
	step [220/332], loss=7.8123
	step [221/332], loss=6.4628
	step [222/332], loss=5.6345
	step [223/332], loss=6.4784
	step [224/332], loss=6.4907
	step [225/332], loss=7.2871
	step [226/332], loss=6.7266
	step [227/332], loss=8.5550
	step [228/332], loss=5.3850
	step [229/332], loss=8.0300
	step [230/332], loss=7.1886
	step [231/332], loss=7.1027
	step [232/332], loss=7.0123
	step [233/332], loss=8.1745
	step [234/332], loss=7.4672
	step [235/332], loss=6.3570
	step [236/332], loss=6.6014
	step [237/332], loss=6.4583
	step [238/332], loss=7.0757
	step [239/332], loss=7.4413
	step [240/332], loss=9.5444
	step [241/332], loss=5.9814
	step [242/332], loss=5.9719
	step [243/332], loss=6.1036
	step [244/332], loss=7.2030
	step [245/332], loss=6.6399
	step [246/332], loss=5.0246
	step [247/332], loss=5.7047
	step [248/332], loss=6.6346
	step [249/332], loss=7.2717
	step [250/332], loss=7.1454
	step [251/332], loss=6.6385
	step [252/332], loss=8.3213
	step [253/332], loss=7.6063
	step [254/332], loss=9.8544
	step [255/332], loss=7.6597
	step [256/332], loss=6.4527
	step [257/332], loss=7.6760
	step [258/332], loss=7.2358
	step [259/332], loss=6.5969
	step [260/332], loss=6.0155
	step [261/332], loss=7.3960
	step [262/332], loss=9.9780
	step [263/332], loss=6.8898
	step [264/332], loss=5.5231
	step [265/332], loss=7.6526
	step [266/332], loss=6.5954
	step [267/332], loss=7.6828
	step [268/332], loss=7.0032
	step [269/332], loss=7.3010
	step [270/332], loss=7.5375
	step [271/332], loss=8.0613
	step [272/332], loss=6.9450
	step [273/332], loss=6.8950
	step [274/332], loss=7.0469
	step [275/332], loss=7.1675
	step [276/332], loss=5.7749
	step [277/332], loss=6.9965
	step [278/332], loss=7.1121
	step [279/332], loss=7.2187
	step [280/332], loss=7.5039
	step [281/332], loss=8.5517
	step [282/332], loss=5.7595
	step [283/332], loss=7.3405
	step [284/332], loss=6.6706
	step [285/332], loss=5.1948
	step [286/332], loss=6.7263
	step [287/332], loss=7.8707
	step [288/332], loss=6.2978
	step [289/332], loss=5.7965
	step [290/332], loss=7.8203
	step [291/332], loss=6.5032
	step [292/332], loss=5.4567
	step [293/332], loss=6.8141
	step [294/332], loss=6.4496
	step [295/332], loss=9.6560
	step [296/332], loss=4.7547
	step [297/332], loss=7.4997
	step [298/332], loss=5.1832
	step [299/332], loss=6.1309
	step [300/332], loss=8.6556
	step [301/332], loss=7.9423
	step [302/332], loss=6.9863
	step [303/332], loss=6.2947
	step [304/332], loss=7.9999
	step [305/332], loss=8.2674
	step [306/332], loss=6.4536
	step [307/332], loss=4.8750
	step [308/332], loss=6.4968
	step [309/332], loss=7.4234
	step [310/332], loss=9.1396
	step [311/332], loss=7.4784
	step [312/332], loss=6.2302
	step [313/332], loss=7.4826
	step [314/332], loss=7.4658
	step [315/332], loss=7.0477
	step [316/332], loss=6.0238
	step [317/332], loss=6.0604
	step [318/332], loss=6.6655
	step [319/332], loss=7.5189
	step [320/332], loss=7.4479
	step [321/332], loss=7.4015
	step [322/332], loss=4.2740
	step [323/332], loss=4.6879
	step [324/332], loss=5.5434
	step [325/332], loss=8.2677
	step [326/332], loss=7.8710
	step [327/332], loss=6.5525
	step [328/332], loss=8.2489
	step [329/332], loss=8.2292
	step [330/332], loss=6.5735
	step [331/332], loss=6.5260
	step [332/332], loss=3.7781
	Evaluating
	loss=0.0278, precision=0.1362, recall=0.9973, f1=0.2396
Training epoch 12
	step [1/332], loss=5.5023
	step [2/332], loss=7.3464
	step [3/332], loss=6.5960
	step [4/332], loss=6.0387
	step [5/332], loss=5.0864
	step [6/332], loss=7.1069
	step [7/332], loss=5.9661
	step [8/332], loss=6.2649
	step [9/332], loss=5.4245
	step [10/332], loss=6.4281
	step [11/332], loss=7.8849
	step [12/332], loss=6.9759
	step [13/332], loss=6.0326
	step [14/332], loss=5.0661
	step [15/332], loss=5.8657
	step [16/332], loss=5.0767
	step [17/332], loss=6.4906
	step [18/332], loss=7.7473
	step [19/332], loss=8.0134
	step [20/332], loss=6.8324
	step [21/332], loss=7.6318
	step [22/332], loss=6.7926
	step [23/332], loss=6.6649
	step [24/332], loss=7.8361
	step [25/332], loss=7.3526
	step [26/332], loss=6.3508
	step [27/332], loss=7.3010
	step [28/332], loss=6.6674
	step [29/332], loss=6.8137
	step [30/332], loss=7.0600
	step [31/332], loss=5.4245
	step [32/332], loss=6.4300
	step [33/332], loss=6.4876
	step [34/332], loss=6.2753
	step [35/332], loss=6.9211
	step [36/332], loss=6.8919
	step [37/332], loss=6.0202
	step [38/332], loss=6.0956
	step [39/332], loss=7.5286
	step [40/332], loss=7.5476
	step [41/332], loss=5.9352
	step [42/332], loss=7.2042
	step [43/332], loss=5.8717
	step [44/332], loss=7.6548
	step [45/332], loss=7.2758
	step [46/332], loss=7.2303
	step [47/332], loss=6.7800
	step [48/332], loss=7.6425
	step [49/332], loss=6.9206
	step [50/332], loss=7.2725
	step [51/332], loss=6.5793
	step [52/332], loss=8.1971
	step [53/332], loss=5.9369
	step [54/332], loss=6.5736
	step [55/332], loss=7.6986
	step [56/332], loss=5.6499
	step [57/332], loss=5.2578
	step [58/332], loss=7.4420
	step [59/332], loss=5.5200
	step [60/332], loss=6.7992
	step [61/332], loss=8.0939
	step [62/332], loss=6.5071
	step [63/332], loss=5.9177
	step [64/332], loss=6.6513
	step [65/332], loss=6.9075
	step [66/332], loss=6.5186
	step [67/332], loss=5.8082
	step [68/332], loss=5.4329
	step [69/332], loss=6.3910
	step [70/332], loss=7.1310
	step [71/332], loss=6.4711
	step [72/332], loss=7.7804
	step [73/332], loss=6.2752
	step [74/332], loss=7.1842
	step [75/332], loss=5.7072
	step [76/332], loss=5.7025
	step [77/332], loss=10.5216
	step [78/332], loss=8.0609
	step [79/332], loss=5.8236
	step [80/332], loss=7.9392
	step [81/332], loss=7.1266
	step [82/332], loss=5.4988
	step [83/332], loss=5.6327
	step [84/332], loss=7.4127
	step [85/332], loss=7.8319
	step [86/332], loss=7.8767
	step [87/332], loss=6.4261
	step [88/332], loss=7.7666
	step [89/332], loss=5.2410
	step [90/332], loss=6.3628
	step [91/332], loss=5.1437
	step [92/332], loss=7.0895
	step [93/332], loss=6.9026
	step [94/332], loss=6.6791
	step [95/332], loss=7.4232
	step [96/332], loss=6.4641
	step [97/332], loss=5.4599
	step [98/332], loss=6.4029
	step [99/332], loss=6.6178
	step [100/332], loss=5.6254
	step [101/332], loss=6.9496
	step [102/332], loss=7.2991
	step [103/332], loss=7.7039
	step [104/332], loss=6.7744
	step [105/332], loss=6.8557
	step [106/332], loss=8.0288
	step [107/332], loss=7.7633
	step [108/332], loss=7.7236
	step [109/332], loss=6.7645
	step [110/332], loss=5.7295
	step [111/332], loss=6.9801
	step [112/332], loss=7.3101
	step [113/332], loss=7.3092
	step [114/332], loss=5.8191
	step [115/332], loss=6.9976
	step [116/332], loss=6.8916
	step [117/332], loss=5.9656
	step [118/332], loss=6.8838
	step [119/332], loss=6.0650
	step [120/332], loss=6.3014
	step [121/332], loss=5.6729
	step [122/332], loss=5.8833
	step [123/332], loss=7.4168
	step [124/332], loss=5.0449
	step [125/332], loss=6.6558
	step [126/332], loss=6.4329
	step [127/332], loss=6.4111
	step [128/332], loss=7.9608
	step [129/332], loss=8.0064
	step [130/332], loss=7.4908
	step [131/332], loss=5.7754
	step [132/332], loss=5.7558
	step [133/332], loss=7.8028
	step [134/332], loss=6.0569
	step [135/332], loss=6.5811
	step [136/332], loss=6.8287
	step [137/332], loss=7.1072
	step [138/332], loss=6.5428
	step [139/332], loss=7.4715
	step [140/332], loss=7.5134
	step [141/332], loss=6.2180
	step [142/332], loss=7.4749
	step [143/332], loss=9.0205
	step [144/332], loss=6.3088
	step [145/332], loss=6.8249
	step [146/332], loss=7.7525
	step [147/332], loss=6.3746
	step [148/332], loss=7.5562
	step [149/332], loss=6.0005
	step [150/332], loss=6.0669
	step [151/332], loss=5.4052
	step [152/332], loss=5.6322
	step [153/332], loss=6.1062
	step [154/332], loss=6.2260
	step [155/332], loss=8.6145
	step [156/332], loss=6.7047
	step [157/332], loss=7.4917
	step [158/332], loss=6.1990
	step [159/332], loss=6.2163
	step [160/332], loss=6.0051
	step [161/332], loss=7.3614
	step [162/332], loss=7.1756
	step [163/332], loss=4.7484
	step [164/332], loss=6.5220
	step [165/332], loss=6.3742
	step [166/332], loss=6.0455
	step [167/332], loss=6.8599
	step [168/332], loss=5.6281
	step [169/332], loss=7.5100
	step [170/332], loss=7.3907
	step [171/332], loss=5.9566
	step [172/332], loss=6.9125
	step [173/332], loss=7.3576
	step [174/332], loss=6.5338
	step [175/332], loss=5.5342
	step [176/332], loss=7.8044
	step [177/332], loss=6.3144
	step [178/332], loss=6.6520
	step [179/332], loss=5.5690
	step [180/332], loss=7.1900
	step [181/332], loss=5.7220
	step [182/332], loss=9.0511
	step [183/332], loss=6.8984
	step [184/332], loss=6.5480
	step [185/332], loss=7.3345
	step [186/332], loss=6.9290
	step [187/332], loss=9.2260
	step [188/332], loss=6.9279
	step [189/332], loss=7.6577
	step [190/332], loss=7.9251
	step [191/332], loss=8.1101
	step [192/332], loss=5.5107
	step [193/332], loss=8.2613
	step [194/332], loss=6.4513
	step [195/332], loss=7.0701
	step [196/332], loss=6.7306
	step [197/332], loss=5.8219
	step [198/332], loss=6.4252
	step [199/332], loss=9.9007
	step [200/332], loss=6.7830
	step [201/332], loss=5.2613
	step [202/332], loss=7.0958
	step [203/332], loss=6.7584
	step [204/332], loss=7.3248
	step [205/332], loss=6.1399
	step [206/332], loss=7.0987
	step [207/332], loss=6.1352
	step [208/332], loss=6.2658
	step [209/332], loss=7.0947
	step [210/332], loss=6.4202
	step [211/332], loss=5.7015
	step [212/332], loss=6.8853
	step [213/332], loss=8.8116
	step [214/332], loss=5.7414
	step [215/332], loss=7.9977
	step [216/332], loss=7.1107
	step [217/332], loss=9.1390
	step [218/332], loss=6.0613
	step [219/332], loss=6.5327
	step [220/332], loss=6.7220
	step [221/332], loss=7.4591
	step [222/332], loss=7.2521
	step [223/332], loss=4.9578
	step [224/332], loss=5.1027
	step [225/332], loss=6.3082
	step [226/332], loss=6.7486
	step [227/332], loss=5.1401
	step [228/332], loss=7.0332
	step [229/332], loss=7.7063
	step [230/332], loss=6.6453
	step [231/332], loss=7.4918
	step [232/332], loss=7.4952
	step [233/332], loss=6.7329
	step [234/332], loss=7.1246
	step [235/332], loss=6.3632
	step [236/332], loss=8.1859
	step [237/332], loss=6.1913
	step [238/332], loss=6.3686
	step [239/332], loss=7.2941
	step [240/332], loss=7.0286
	step [241/332], loss=5.3910
	step [242/332], loss=5.7378
	step [243/332], loss=6.2901
	step [244/332], loss=7.5204
	step [245/332], loss=8.8691
	step [246/332], loss=6.2526
	step [247/332], loss=6.6591
	step [248/332], loss=7.3024
	step [249/332], loss=6.4344
	step [250/332], loss=6.9695
	step [251/332], loss=7.1046
	step [252/332], loss=6.9935
	step [253/332], loss=6.6192
	step [254/332], loss=6.8287
	step [255/332], loss=6.9897
	step [256/332], loss=7.6292
	step [257/332], loss=7.6159
	step [258/332], loss=5.6544
	step [259/332], loss=7.4158
	step [260/332], loss=7.7747
	step [261/332], loss=6.2076
	step [262/332], loss=6.5527
	step [263/332], loss=6.5527
	step [264/332], loss=6.3042
	step [265/332], loss=6.2341
	step [266/332], loss=5.7873
	step [267/332], loss=6.9274
	step [268/332], loss=5.7498
	step [269/332], loss=6.8134
	step [270/332], loss=6.2037
	step [271/332], loss=7.4189
	step [272/332], loss=5.3908
	step [273/332], loss=7.4689
	step [274/332], loss=6.6309
	step [275/332], loss=6.3797
	step [276/332], loss=6.1249
	step [277/332], loss=5.7310
	step [278/332], loss=8.1098
	step [279/332], loss=6.8097
	step [280/332], loss=6.5589
	step [281/332], loss=5.8926
	step [282/332], loss=7.8420
	step [283/332], loss=5.4492
	step [284/332], loss=7.4214
	step [285/332], loss=5.8664
	step [286/332], loss=6.8787
	step [287/332], loss=6.3383
	step [288/332], loss=6.2267
	step [289/332], loss=8.7946
	step [290/332], loss=6.2690
	step [291/332], loss=6.8386
	step [292/332], loss=6.2720
	step [293/332], loss=6.9840
	step [294/332], loss=5.7574
	step [295/332], loss=5.6203
	step [296/332], loss=6.3612
	step [297/332], loss=7.5288
	step [298/332], loss=6.0589
	step [299/332], loss=7.4335
	step [300/332], loss=8.3346
	step [301/332], loss=8.4345
	step [302/332], loss=6.7041
	step [303/332], loss=5.5306
	step [304/332], loss=6.1085
	step [305/332], loss=5.8359
	step [306/332], loss=6.0873
	step [307/332], loss=6.3787
	step [308/332], loss=7.6308
	step [309/332], loss=6.1111
	step [310/332], loss=7.1226
	step [311/332], loss=7.5715
	step [312/332], loss=5.1133
	step [313/332], loss=5.8040
	step [314/332], loss=7.6749
	step [315/332], loss=7.4975
	step [316/332], loss=7.3654
	step [317/332], loss=7.3212
	step [318/332], loss=10.4950
	step [319/332], loss=5.2298
	step [320/332], loss=6.9843
	step [321/332], loss=6.9197
	step [322/332], loss=6.2421
	step [323/332], loss=6.4758
	step [324/332], loss=5.4485
	step [325/332], loss=7.2360
	step [326/332], loss=7.0085
	step [327/332], loss=6.1627
	step [328/332], loss=7.0931
	step [329/332], loss=6.1376
	step [330/332], loss=7.7049
	step [331/332], loss=6.1907
	step [332/332], loss=2.9927
	Evaluating
	loss=0.0238, precision=0.1587, recall=0.9959, f1=0.2738
Training epoch 13
	step [1/332], loss=8.1123
	step [2/332], loss=7.0284
	step [3/332], loss=6.0826
	step [4/332], loss=5.0234
	step [5/332], loss=5.5388
	step [6/332], loss=8.9316
	step [7/332], loss=6.9080
	step [8/332], loss=6.8680
	step [9/332], loss=6.7353
	step [10/332], loss=6.6243
	step [11/332], loss=6.9686
	step [12/332], loss=6.5009
	step [13/332], loss=7.5950
	step [14/332], loss=6.8189
	step [15/332], loss=6.3549
	step [16/332], loss=6.2267
	step [17/332], loss=6.1779
	step [18/332], loss=6.7189
	step [19/332], loss=6.6936
	step [20/332], loss=4.7517
	step [21/332], loss=7.6087
	step [22/332], loss=6.6058
	step [23/332], loss=5.7515
	step [24/332], loss=6.3601
	step [25/332], loss=4.8318
	step [26/332], loss=7.0929
	step [27/332], loss=6.3787
	step [28/332], loss=6.2783
	step [29/332], loss=5.9791
	step [30/332], loss=6.2971
	step [31/332], loss=7.6734
	step [32/332], loss=5.3063
	step [33/332], loss=5.5943
	step [34/332], loss=6.2918
	step [35/332], loss=6.7320
	step [36/332], loss=5.4463
	step [37/332], loss=7.4690
	step [38/332], loss=6.8763
	step [39/332], loss=6.4737
	step [40/332], loss=5.9963
	step [41/332], loss=6.9551
	step [42/332], loss=6.9663
	step [43/332], loss=6.6725
	step [44/332], loss=5.6959
	step [45/332], loss=7.4747
	step [46/332], loss=6.1552
	step [47/332], loss=6.9116
	step [48/332], loss=7.1978
	step [49/332], loss=7.0546
	step [50/332], loss=5.3875
	step [51/332], loss=5.8669
	step [52/332], loss=5.8879
	step [53/332], loss=7.4292
	step [54/332], loss=5.7956
	step [55/332], loss=6.5982
	step [56/332], loss=5.2390
	step [57/332], loss=6.0663
	step [58/332], loss=6.7431
	step [59/332], loss=6.3649
	step [60/332], loss=6.6157
	step [61/332], loss=7.7060
	step [62/332], loss=5.1188
	step [63/332], loss=5.3074
	step [64/332], loss=5.6764
	step [65/332], loss=6.1940
	step [66/332], loss=6.6651
	step [67/332], loss=5.0925
	step [68/332], loss=6.2483
	step [69/332], loss=6.2150
	step [70/332], loss=7.0549
	step [71/332], loss=6.1131
	step [72/332], loss=5.2333
	step [73/332], loss=6.1988
	step [74/332], loss=6.8514
	step [75/332], loss=7.2775
	step [76/332], loss=6.5681
	step [77/332], loss=6.6243
	step [78/332], loss=6.2752
	step [79/332], loss=5.9497
	step [80/332], loss=5.3809
	step [81/332], loss=4.9957
	step [82/332], loss=5.9786
	step [83/332], loss=7.2202
	step [84/332], loss=7.3495
	step [85/332], loss=7.7272
	step [86/332], loss=5.4581
	step [87/332], loss=6.6758
	step [88/332], loss=6.7554
	step [89/332], loss=7.5165
	step [90/332], loss=7.1145
	step [91/332], loss=5.8871
	step [92/332], loss=6.2967
	step [93/332], loss=5.2573
	step [94/332], loss=7.1724
	step [95/332], loss=6.3844
	step [96/332], loss=5.9222
	step [97/332], loss=6.7397
	step [98/332], loss=8.0866
	step [99/332], loss=5.7009
	step [100/332], loss=5.8664
	step [101/332], loss=5.8425
	step [102/332], loss=7.6406
	step [103/332], loss=6.6957
	step [104/332], loss=8.6166
	step [105/332], loss=6.8353
	step [106/332], loss=5.4593
	step [107/332], loss=6.2247
	step [108/332], loss=6.3321
	step [109/332], loss=5.0423
	step [110/332], loss=7.6640
	step [111/332], loss=6.0228
	step [112/332], loss=7.5118
	step [113/332], loss=5.5981
	step [114/332], loss=8.7724
	step [115/332], loss=5.4974
	step [116/332], loss=5.9830
	step [117/332], loss=7.2834
	step [118/332], loss=7.8278
	step [119/332], loss=6.3167
	step [120/332], loss=5.9543
	step [121/332], loss=6.0994
	step [122/332], loss=6.7330
	step [123/332], loss=7.8945
	step [124/332], loss=6.5290
	step [125/332], loss=5.5516
	step [126/332], loss=6.3704
	step [127/332], loss=6.8779
	step [128/332], loss=6.7295
	step [129/332], loss=6.7602
	step [130/332], loss=7.3040
	step [131/332], loss=6.5652
	step [132/332], loss=4.9537
	step [133/332], loss=5.7747
	step [134/332], loss=6.5610
	step [135/332], loss=7.5539
	step [136/332], loss=6.7629
	step [137/332], loss=6.0134
	step [138/332], loss=5.8660
	step [139/332], loss=6.2123
	step [140/332], loss=5.9061
	step [141/332], loss=7.4308
	step [142/332], loss=5.2180
	step [143/332], loss=5.8991
	step [144/332], loss=6.7073
	step [145/332], loss=8.5427
	step [146/332], loss=6.0725
	step [147/332], loss=6.0011
	step [148/332], loss=6.6931
	step [149/332], loss=6.6805
	step [150/332], loss=6.2590
	step [151/332], loss=6.8197
	step [152/332], loss=6.2918
	step [153/332], loss=7.1665
	step [154/332], loss=6.0941
	step [155/332], loss=6.3079
	step [156/332], loss=6.8867
	step [157/332], loss=6.0413
	step [158/332], loss=5.7509
	step [159/332], loss=9.3227
	step [160/332], loss=6.2502
	step [161/332], loss=7.1421
	step [162/332], loss=5.2811
	step [163/332], loss=7.0115
	step [164/332], loss=7.6714
	step [165/332], loss=6.0132
	step [166/332], loss=5.5677
	step [167/332], loss=5.5233
	step [168/332], loss=7.2862
	step [169/332], loss=6.9082
	step [170/332], loss=6.8922
	step [171/332], loss=5.7370
	step [172/332], loss=6.3897
	step [173/332], loss=4.7295
	step [174/332], loss=6.3222
	step [175/332], loss=5.6192
	step [176/332], loss=5.6007
	step [177/332], loss=6.7202
	step [178/332], loss=5.7362
	step [179/332], loss=4.8960
	step [180/332], loss=6.2342
	step [181/332], loss=8.2198
	step [182/332], loss=5.3841
	step [183/332], loss=6.7746
	step [184/332], loss=6.4938
	step [185/332], loss=7.2699
	step [186/332], loss=7.8924
	step [187/332], loss=6.8841
	step [188/332], loss=6.5194
	step [189/332], loss=7.4618
	step [190/332], loss=7.2614
	step [191/332], loss=5.7168
	step [192/332], loss=6.7285
	step [193/332], loss=6.9488
	step [194/332], loss=6.3574
	step [195/332], loss=6.5086
	step [196/332], loss=8.2527
	step [197/332], loss=6.8029
	step [198/332], loss=5.7149
	step [199/332], loss=6.4907
	step [200/332], loss=7.6086
	step [201/332], loss=7.4063
	step [202/332], loss=6.4952
	step [203/332], loss=5.3028
	step [204/332], loss=6.7533
	step [205/332], loss=5.8484
	step [206/332], loss=7.0221
	step [207/332], loss=5.3473
	step [208/332], loss=5.8251
	step [209/332], loss=5.8950
	step [210/332], loss=5.4271
	step [211/332], loss=6.2661
	step [212/332], loss=5.7687
	step [213/332], loss=6.9614
	step [214/332], loss=6.6193
	step [215/332], loss=6.8006
	step [216/332], loss=5.2362
	step [217/332], loss=6.5060
	step [218/332], loss=7.2055
	step [219/332], loss=5.4588
	step [220/332], loss=6.3865
	step [221/332], loss=6.5369
	step [222/332], loss=7.0379
	step [223/332], loss=9.2732
	step [224/332], loss=5.8824
	step [225/332], loss=5.0268
	step [226/332], loss=6.7216
	step [227/332], loss=5.8270
	step [228/332], loss=6.0663
	step [229/332], loss=6.7196
	step [230/332], loss=5.4647
	step [231/332], loss=6.8714
	step [232/332], loss=6.6504
	step [233/332], loss=6.4653
	step [234/332], loss=5.9012
	step [235/332], loss=7.5011
	step [236/332], loss=6.7463
	step [237/332], loss=5.9827
	step [238/332], loss=6.7529
	step [239/332], loss=7.5800
	step [240/332], loss=5.7151
	step [241/332], loss=8.2593
	step [242/332], loss=7.1983
	step [243/332], loss=7.2368
	step [244/332], loss=6.9629
	step [245/332], loss=6.3050
	step [246/332], loss=5.8275
	step [247/332], loss=6.3169
	step [248/332], loss=6.3526
	step [249/332], loss=5.6732
	step [250/332], loss=5.5536
	step [251/332], loss=9.5486
	step [252/332], loss=5.8399
	step [253/332], loss=7.4204
	step [254/332], loss=5.9829
	step [255/332], loss=6.3047
	step [256/332], loss=6.0627
	step [257/332], loss=8.5244
	step [258/332], loss=7.7340
	step [259/332], loss=5.3142
	step [260/332], loss=6.0025
	step [261/332], loss=7.3979
	step [262/332], loss=6.2909
	step [263/332], loss=5.7940
	step [264/332], loss=6.9258
	step [265/332], loss=5.6871
	step [266/332], loss=6.1110
	step [267/332], loss=5.6740
	step [268/332], loss=5.2319
	step [269/332], loss=8.0120
	step [270/332], loss=5.8928
	step [271/332], loss=7.8944
	step [272/332], loss=5.9399
	step [273/332], loss=6.6005
	step [274/332], loss=7.5873
	step [275/332], loss=6.4780
	step [276/332], loss=6.3221
	step [277/332], loss=5.7873
	step [278/332], loss=7.7996
	step [279/332], loss=5.4481
	step [280/332], loss=5.4883
	step [281/332], loss=6.6583
	step [282/332], loss=5.1996
	step [283/332], loss=6.6981
	step [284/332], loss=5.2068
	step [285/332], loss=6.1118
	step [286/332], loss=5.8817
	step [287/332], loss=4.7578
	step [288/332], loss=6.9201
	step [289/332], loss=5.9010
	step [290/332], loss=6.5392
	step [291/332], loss=6.9568
	step [292/332], loss=5.4667
	step [293/332], loss=5.1588
	step [294/332], loss=8.1111
	step [295/332], loss=5.2353
	step [296/332], loss=6.1522
	step [297/332], loss=7.4531
	step [298/332], loss=7.4285
	step [299/332], loss=5.9986
	step [300/332], loss=5.8458
	step [301/332], loss=6.9113
	step [302/332], loss=6.9685
	step [303/332], loss=8.0281
	step [304/332], loss=6.6501
	step [305/332], loss=5.9798
	step [306/332], loss=5.3655
	step [307/332], loss=6.7195
	step [308/332], loss=5.2675
	step [309/332], loss=5.6836
	step [310/332], loss=6.0219
	step [311/332], loss=5.1811
	step [312/332], loss=6.0453
	step [313/332], loss=5.4781
	step [314/332], loss=7.2295
	step [315/332], loss=7.6033
	step [316/332], loss=6.0106
	step [317/332], loss=5.6968
	step [318/332], loss=5.6615
	step [319/332], loss=6.1268
	step [320/332], loss=7.2490
	step [321/332], loss=6.1265
	step [322/332], loss=6.6236
	step [323/332], loss=6.4712
	step [324/332], loss=5.6763
	step [325/332], loss=5.8372
	step [326/332], loss=6.4754
	step [327/332], loss=6.1018
	step [328/332], loss=4.6471
	step [329/332], loss=6.7139
	step [330/332], loss=5.3642
	step [331/332], loss=4.9329
	step [332/332], loss=3.4608
	Evaluating
	loss=0.0238, precision=0.1623, recall=0.9964, f1=0.2791
Training epoch 14
	step [1/332], loss=5.1352
	step [2/332], loss=7.0376
	step [3/332], loss=6.5220
	step [4/332], loss=5.5865
	step [5/332], loss=4.9840
	step [6/332], loss=6.0692
	step [7/332], loss=5.1815
	step [8/332], loss=6.0642
	step [9/332], loss=7.6463
	step [10/332], loss=7.3454
	step [11/332], loss=6.0834
	step [12/332], loss=6.2962
	step [13/332], loss=5.9679
	step [14/332], loss=5.5002
	step [15/332], loss=7.4773
	step [16/332], loss=6.2500
	step [17/332], loss=5.9876
	step [18/332], loss=4.6966
	step [19/332], loss=5.4714
	step [20/332], loss=5.2135
	step [21/332], loss=6.0176
	step [22/332], loss=6.8551
	step [23/332], loss=7.1296
	step [24/332], loss=8.5835
	step [25/332], loss=6.6068
	step [26/332], loss=5.9730
	step [27/332], loss=7.0260
	step [28/332], loss=5.6973
	step [29/332], loss=6.2964
	step [30/332], loss=5.7962
	step [31/332], loss=5.4804
	step [32/332], loss=6.1466
	step [33/332], loss=5.5269
	step [34/332], loss=6.7935
	step [35/332], loss=4.9544
	step [36/332], loss=5.4831
	step [37/332], loss=5.4196
	step [38/332], loss=6.4722
	step [39/332], loss=5.4431
	step [40/332], loss=5.1694
	step [41/332], loss=5.8807
	step [42/332], loss=5.3625
	step [43/332], loss=5.7025
	step [44/332], loss=4.8124
	step [45/332], loss=5.8269
	step [46/332], loss=4.9041
	step [47/332], loss=6.0680
	step [48/332], loss=6.6314
	step [49/332], loss=6.4879
	step [50/332], loss=6.2548
	step [51/332], loss=5.7633
	step [52/332], loss=4.8837
	step [53/332], loss=4.7997
	step [54/332], loss=6.2100
	step [55/332], loss=6.6250
	step [56/332], loss=5.8415
	step [57/332], loss=6.5664
	step [58/332], loss=7.1979
	step [59/332], loss=4.6841
	step [60/332], loss=5.2115
	step [61/332], loss=5.2502
	step [62/332], loss=7.4158
	step [63/332], loss=6.7551
	step [64/332], loss=5.4608
	step [65/332], loss=5.5558
	step [66/332], loss=6.6925
	step [67/332], loss=7.2057
	step [68/332], loss=5.4299
	step [69/332], loss=5.7030
	step [70/332], loss=6.2786
	step [71/332], loss=5.2601
	step [72/332], loss=5.6717
	step [73/332], loss=6.6675
	step [74/332], loss=6.2133
	step [75/332], loss=7.3495
	step [76/332], loss=5.9740
	step [77/332], loss=5.2255
	step [78/332], loss=5.6110
	step [79/332], loss=6.3625
	step [80/332], loss=6.6025
	step [81/332], loss=5.4702
	step [82/332], loss=8.7136
	step [83/332], loss=6.6204
	step [84/332], loss=5.4079
	step [85/332], loss=7.6344
	step [86/332], loss=6.5940
	step [87/332], loss=6.3031
	step [88/332], loss=5.5091
	step [89/332], loss=5.5932
	step [90/332], loss=6.0377
	step [91/332], loss=6.4108
	step [92/332], loss=5.7605
	step [93/332], loss=8.7766
	step [94/332], loss=6.0832
	step [95/332], loss=7.3623
	step [96/332], loss=7.1553
	step [97/332], loss=8.2726
	step [98/332], loss=5.7320
	step [99/332], loss=5.5306
	step [100/332], loss=6.3355
	step [101/332], loss=5.2890
	step [102/332], loss=6.7266
	step [103/332], loss=6.5969
	step [104/332], loss=6.5225
	step [105/332], loss=4.8789
	step [106/332], loss=6.3247
	step [107/332], loss=6.9265
	step [108/332], loss=6.1280
	step [109/332], loss=6.9275
	step [110/332], loss=5.9522
	step [111/332], loss=6.1322
	step [112/332], loss=7.2764
	step [113/332], loss=8.3021
	step [114/332], loss=7.3611
	step [115/332], loss=6.1765
	step [116/332], loss=7.2414
	step [117/332], loss=5.7522
	step [118/332], loss=6.8535
	step [119/332], loss=5.5541
	step [120/332], loss=5.1130
	step [121/332], loss=5.5754
	step [122/332], loss=6.4694
	step [123/332], loss=5.6543
	step [124/332], loss=5.8799
	step [125/332], loss=5.5802
	step [126/332], loss=8.1986
	step [127/332], loss=5.1408
	step [128/332], loss=5.3594
	step [129/332], loss=5.6705
	step [130/332], loss=7.4459
	step [131/332], loss=5.9034
	step [132/332], loss=7.0049
	step [133/332], loss=5.2209
	step [134/332], loss=7.7759
	step [135/332], loss=5.1397
	step [136/332], loss=7.3128
	step [137/332], loss=5.8489
	step [138/332], loss=6.3774
	step [139/332], loss=6.2856
	step [140/332], loss=6.6620
	step [141/332], loss=6.4758
	step [142/332], loss=5.8547
	step [143/332], loss=5.2334
	step [144/332], loss=6.2662
	step [145/332], loss=5.8438
	step [146/332], loss=7.0053
	step [147/332], loss=4.5862
	step [148/332], loss=6.2831
	step [149/332], loss=6.7867
	step [150/332], loss=7.1746
	step [151/332], loss=6.5245
	step [152/332], loss=6.4190
	step [153/332], loss=6.3742
	step [154/332], loss=4.9742
	step [155/332], loss=5.6509
	step [156/332], loss=6.0547
	step [157/332], loss=6.6982
	step [158/332], loss=6.4233
	step [159/332], loss=6.2377
	step [160/332], loss=6.6844
	step [161/332], loss=4.8809
	step [162/332], loss=6.0684
	step [163/332], loss=6.7151
	step [164/332], loss=6.2869
	step [165/332], loss=5.3493
	step [166/332], loss=5.0065
	step [167/332], loss=4.8402
	step [168/332], loss=4.7609
	step [169/332], loss=6.7279
	step [170/332], loss=5.5977
	step [171/332], loss=5.1672
	step [172/332], loss=6.2463
	step [173/332], loss=5.2933
	step [174/332], loss=5.2457
	step [175/332], loss=5.8559
	step [176/332], loss=6.8867
	step [177/332], loss=7.2526
	step [178/332], loss=6.7124
	step [179/332], loss=5.0778
	step [180/332], loss=5.3588
	step [181/332], loss=6.1987
	step [182/332], loss=8.3419
	step [183/332], loss=5.7482
	step [184/332], loss=5.7634
	step [185/332], loss=6.9338
	step [186/332], loss=5.4906
	step [187/332], loss=6.2244
	step [188/332], loss=6.0872
	step [189/332], loss=5.6121
	step [190/332], loss=5.9113
	step [191/332], loss=5.6311
	step [192/332], loss=6.0185
	step [193/332], loss=5.8680
	step [194/332], loss=6.5739
	step [195/332], loss=6.4545
	step [196/332], loss=6.4925
	step [197/332], loss=7.2369
	step [198/332], loss=5.9738
	step [199/332], loss=6.5361
	step [200/332], loss=5.4236
	step [201/332], loss=7.1218
	step [202/332], loss=5.5177
	step [203/332], loss=5.8922
	step [204/332], loss=5.3231
	step [205/332], loss=6.1954
	step [206/332], loss=5.4116
	step [207/332], loss=5.0866
	step [208/332], loss=4.6065
	step [209/332], loss=6.4511
	step [210/332], loss=8.2873
	step [211/332], loss=6.6354
	step [212/332], loss=6.3848
	step [213/332], loss=6.3651
	step [214/332], loss=6.2921
	step [215/332], loss=4.3848
	step [216/332], loss=5.6312
	step [217/332], loss=7.1391
	step [218/332], loss=4.7963
	step [219/332], loss=6.3627
	step [220/332], loss=6.4386
	step [221/332], loss=7.0279
	step [222/332], loss=6.4867
	step [223/332], loss=8.1927
	step [224/332], loss=6.8452
	step [225/332], loss=4.8730
	step [226/332], loss=5.1967
	step [227/332], loss=6.0907
	step [228/332], loss=5.5020
	step [229/332], loss=4.3136
	step [230/332], loss=6.0059
	step [231/332], loss=5.6584
	step [232/332], loss=8.5950
	step [233/332], loss=7.3711
	step [234/332], loss=5.0557
	step [235/332], loss=5.6059
	step [236/332], loss=5.9801
	step [237/332], loss=7.8613
	step [238/332], loss=6.4702
	step [239/332], loss=6.5106
	step [240/332], loss=5.8899
	step [241/332], loss=5.9714
	step [242/332], loss=5.8803
	step [243/332], loss=5.1280
	step [244/332], loss=6.9676
	step [245/332], loss=5.4852
	step [246/332], loss=7.9157
	step [247/332], loss=6.1620
	step [248/332], loss=5.9918
	step [249/332], loss=6.2270
	step [250/332], loss=6.3585
	step [251/332], loss=5.8788
	step [252/332], loss=5.9343
	step [253/332], loss=9.0992
	step [254/332], loss=6.8678
	step [255/332], loss=6.3717
	step [256/332], loss=7.7221
	step [257/332], loss=7.0212
	step [258/332], loss=5.6554
	step [259/332], loss=6.1985
	step [260/332], loss=5.4417
	step [261/332], loss=7.3026
	step [262/332], loss=6.2496
	step [263/332], loss=5.8615
	step [264/332], loss=6.6797
	step [265/332], loss=7.3482
	step [266/332], loss=4.9295
	step [267/332], loss=6.5101
	step [268/332], loss=7.2196
	step [269/332], loss=6.8500
	step [270/332], loss=5.0696
	step [271/332], loss=7.2158
	step [272/332], loss=7.7127
	step [273/332], loss=4.7364
	step [274/332], loss=6.8328
	step [275/332], loss=7.7697
	step [276/332], loss=6.2268
	step [277/332], loss=7.2707
	step [278/332], loss=8.0065
	step [279/332], loss=6.5178
	step [280/332], loss=5.9938
	step [281/332], loss=6.6556
	step [282/332], loss=6.4957
	step [283/332], loss=6.5619
	step [284/332], loss=5.4546
	step [285/332], loss=6.9331
	step [286/332], loss=6.1838
	step [287/332], loss=7.1635
	step [288/332], loss=5.7245
	step [289/332], loss=7.1547
	step [290/332], loss=5.9904
	step [291/332], loss=5.9188
	step [292/332], loss=5.6252
	step [293/332], loss=6.4407
	step [294/332], loss=8.5387
	step [295/332], loss=5.3174
	step [296/332], loss=5.7494
	step [297/332], loss=5.9727
	step [298/332], loss=6.8969
	step [299/332], loss=5.2537
	step [300/332], loss=6.6143
	step [301/332], loss=7.5815
	step [302/332], loss=6.1692
	step [303/332], loss=5.1268
	step [304/332], loss=6.0831
	step [305/332], loss=7.7731
	step [306/332], loss=4.9216
	step [307/332], loss=6.1878
	step [308/332], loss=6.1129
	step [309/332], loss=6.9074
	step [310/332], loss=6.0620
	step [311/332], loss=5.7900
	step [312/332], loss=5.8102
	step [313/332], loss=6.2688
	step [314/332], loss=6.3183
	step [315/332], loss=7.2299
	step [316/332], loss=6.1608
	step [317/332], loss=6.5004
	step [318/332], loss=7.5043
	step [319/332], loss=6.2386
	step [320/332], loss=6.5918
	step [321/332], loss=6.4786
	step [322/332], loss=6.0859
	step [323/332], loss=6.9172
	step [324/332], loss=7.2588
	step [325/332], loss=6.8748
	step [326/332], loss=6.7320
	step [327/332], loss=5.3557
	step [328/332], loss=6.1819
	step [329/332], loss=6.2681
	step [330/332], loss=6.8624
	step [331/332], loss=6.4204
	step [332/332], loss=3.5253
	Evaluating
	loss=0.0202, precision=0.1801, recall=0.9963, f1=0.3051
saving model as: 0_saved_model.pth
Training epoch 15
	step [1/332], loss=6.7368
	step [2/332], loss=6.5403
	step [3/332], loss=5.7898
	step [4/332], loss=5.6910
	step [5/332], loss=6.3316
	step [6/332], loss=4.9714
	step [7/332], loss=6.8050
	step [8/332], loss=5.4036
	step [9/332], loss=6.7535
	step [10/332], loss=6.6931
	step [11/332], loss=4.6778
	step [12/332], loss=6.6526
	step [13/332], loss=5.7171
	step [14/332], loss=5.4397
	step [15/332], loss=5.4412
	step [16/332], loss=7.7990
	step [17/332], loss=5.7161
	step [18/332], loss=7.1458
	step [19/332], loss=6.1349
	step [20/332], loss=7.1789
	step [21/332], loss=5.9248
	step [22/332], loss=7.4872
	step [23/332], loss=6.3534
	step [24/332], loss=5.4299
	step [25/332], loss=6.2864
	step [26/332], loss=6.7001
	step [27/332], loss=5.5002
	step [28/332], loss=5.3213
	step [29/332], loss=5.1336
	step [30/332], loss=6.2327
	step [31/332], loss=5.4504
	step [32/332], loss=6.3621
	step [33/332], loss=6.6522
	step [34/332], loss=6.8139
	step [35/332], loss=7.4979
	step [36/332], loss=6.3311
	step [37/332], loss=7.1548
	step [38/332], loss=7.7116
	step [39/332], loss=6.5200
	step [40/332], loss=5.7896
	step [41/332], loss=7.6548
	step [42/332], loss=8.8216
	step [43/332], loss=5.5778
	step [44/332], loss=7.3989
	step [45/332], loss=5.8076
	step [46/332], loss=5.6638
	step [47/332], loss=6.0980
	step [48/332], loss=5.8952
	step [49/332], loss=5.4934
	step [50/332], loss=5.1714
	step [51/332], loss=7.1522
	step [52/332], loss=6.6787
	step [53/332], loss=5.9342
	step [54/332], loss=6.9308
	step [55/332], loss=5.5982
	step [56/332], loss=7.6148
	step [57/332], loss=6.4317
	step [58/332], loss=6.2099
	step [59/332], loss=6.3998
	step [60/332], loss=5.7452
	step [61/332], loss=5.6487
	step [62/332], loss=6.1824
	step [63/332], loss=4.8968
	step [64/332], loss=6.5699
	step [65/332], loss=8.4036
	step [66/332], loss=7.8006
	step [67/332], loss=6.9432
	step [68/332], loss=5.9790
	step [69/332], loss=7.5438
	step [70/332], loss=6.6864
	step [71/332], loss=6.3541
	step [72/332], loss=6.5448
	step [73/332], loss=6.8581
	step [74/332], loss=6.2215
	step [75/332], loss=7.0487
	step [76/332], loss=5.6814
	step [77/332], loss=5.9076
	step [78/332], loss=5.5020
	step [79/332], loss=7.1424
	step [80/332], loss=5.9369
	step [81/332], loss=8.1504
	step [82/332], loss=5.4195
	step [83/332], loss=6.0664
	step [84/332], loss=6.8944
	step [85/332], loss=7.1256
	step [86/332], loss=7.5540
	step [87/332], loss=5.3993
	step [88/332], loss=6.8583
	step [89/332], loss=5.7711
	step [90/332], loss=5.1862
	step [91/332], loss=5.4468
	step [92/332], loss=7.4966
	step [93/332], loss=6.1406
	step [94/332], loss=5.9190
	step [95/332], loss=8.6933
	step [96/332], loss=6.6613
	step [97/332], loss=5.2328
	step [98/332], loss=7.2546
	step [99/332], loss=5.3653
	step [100/332], loss=6.0593
	step [101/332], loss=5.8665
	step [102/332], loss=6.5213
	step [103/332], loss=6.4609
	step [104/332], loss=5.9288
	step [105/332], loss=6.1178
	step [106/332], loss=5.9900
	step [107/332], loss=5.8682
	step [108/332], loss=5.1161
	step [109/332], loss=5.1306
	step [110/332], loss=6.1821
	step [111/332], loss=6.8019
	step [112/332], loss=5.1137
	step [113/332], loss=5.6679
	step [114/332], loss=5.1210
	step [115/332], loss=6.4472
	step [116/332], loss=5.9577
	step [117/332], loss=6.5598
	step [118/332], loss=5.5240
	step [119/332], loss=5.0432
	step [120/332], loss=6.1470
	step [121/332], loss=6.4510
	step [122/332], loss=4.4920
	step [123/332], loss=6.7071
	step [124/332], loss=6.2620
	step [125/332], loss=5.3043
	step [126/332], loss=6.8622
	step [127/332], loss=6.3917
	step [128/332], loss=6.8211
	step [129/332], loss=5.3489
	step [130/332], loss=6.2779
	step [131/332], loss=6.0944
	step [132/332], loss=6.3763
	step [133/332], loss=5.8166
	step [134/332], loss=4.6061
	step [135/332], loss=5.8894
	step [136/332], loss=7.3787
	step [137/332], loss=6.6775
	step [138/332], loss=5.9954
	step [139/332], loss=7.4299
	step [140/332], loss=6.4949
	step [141/332], loss=6.7680
	step [142/332], loss=7.3910
	step [143/332], loss=5.9681
	step [144/332], loss=6.5131
	step [145/332], loss=5.6562
	step [146/332], loss=5.9768
	step [147/332], loss=6.7765
	step [148/332], loss=6.1234
	step [149/332], loss=6.7564
	step [150/332], loss=6.7896
	step [151/332], loss=6.2671
	step [152/332], loss=4.5330
	step [153/332], loss=5.2788
	step [154/332], loss=6.5076
	step [155/332], loss=5.6992
	step [156/332], loss=4.8185
	step [157/332], loss=6.6571
	step [158/332], loss=7.0482
	step [159/332], loss=5.4278
	step [160/332], loss=8.2802
	step [161/332], loss=5.9076
	step [162/332], loss=5.3872
	step [163/332], loss=5.9107
	step [164/332], loss=6.8697
	step [165/332], loss=5.6757
	step [166/332], loss=6.3860
	step [167/332], loss=5.7145
	step [168/332], loss=6.4263
	step [169/332], loss=7.1271
	step [170/332], loss=6.1403
	step [171/332], loss=6.2286
	step [172/332], loss=4.4370
	step [173/332], loss=5.3538
	step [174/332], loss=7.4643
	step [175/332], loss=6.7228
	step [176/332], loss=6.6537
	step [177/332], loss=5.6257
	step [178/332], loss=4.9059
	step [179/332], loss=6.5672
	step [180/332], loss=4.5739
	step [181/332], loss=6.4801
	step [182/332], loss=8.0268
	step [183/332], loss=5.4074
	step [184/332], loss=5.4041
	step [185/332], loss=5.4528
	step [186/332], loss=5.6337
	step [187/332], loss=5.0109
	step [188/332], loss=5.2021
	step [189/332], loss=5.5397
	step [190/332], loss=5.2802
	step [191/332], loss=7.0345
	step [192/332], loss=8.4372
	step [193/332], loss=5.7887
	step [194/332], loss=6.1625
	step [195/332], loss=5.2342
	step [196/332], loss=5.7084
	step [197/332], loss=5.3908
	step [198/332], loss=5.5438
	step [199/332], loss=5.7898
	step [200/332], loss=5.9001
	step [201/332], loss=5.8978
	step [202/332], loss=4.7844
	step [203/332], loss=5.6678
	step [204/332], loss=6.0971
	step [205/332], loss=6.2372
	step [206/332], loss=5.2394
	step [207/332], loss=5.4132
	step [208/332], loss=4.4068
	step [209/332], loss=6.1261
	step [210/332], loss=5.1730
	step [211/332], loss=7.8142
	step [212/332], loss=6.4660
	step [213/332], loss=5.9474
	step [214/332], loss=5.6306
	step [215/332], loss=4.9529
	step [216/332], loss=5.4433
	step [217/332], loss=4.6901
	step [218/332], loss=6.5709
	step [219/332], loss=5.7393
	step [220/332], loss=5.7566
	step [221/332], loss=5.5681
	step [222/332], loss=5.7684
	step [223/332], loss=7.6453
	step [224/332], loss=6.2988
	step [225/332], loss=4.4592
	step [226/332], loss=7.2877
	step [227/332], loss=6.7863
	step [228/332], loss=6.4930
	step [229/332], loss=5.7462
	step [230/332], loss=5.7341
	step [231/332], loss=5.9235
	step [232/332], loss=6.0837
	step [233/332], loss=6.3777
	step [234/332], loss=4.6831
	step [235/332], loss=6.3275
	step [236/332], loss=6.4861
	step [237/332], loss=5.0070
	step [238/332], loss=4.7579
	step [239/332], loss=4.5160
	step [240/332], loss=6.8822
	step [241/332], loss=5.7602
	step [242/332], loss=5.0473
	step [243/332], loss=6.2069
	step [244/332], loss=5.3130
	step [245/332], loss=5.5510
	step [246/332], loss=5.9810
	step [247/332], loss=5.2865
	step [248/332], loss=5.8893
	step [249/332], loss=5.1166
	step [250/332], loss=4.7826
	step [251/332], loss=5.5876
	step [252/332], loss=6.1364
	step [253/332], loss=5.3063
	step [254/332], loss=6.0747
	step [255/332], loss=4.8640
	step [256/332], loss=4.6734
	step [257/332], loss=5.8417
	step [258/332], loss=5.7307
	step [259/332], loss=7.3607
	step [260/332], loss=6.5936
	step [261/332], loss=5.8966
	step [262/332], loss=5.3858
	step [263/332], loss=5.1807
	step [264/332], loss=6.0562
	step [265/332], loss=4.6571
	step [266/332], loss=7.5949
	step [267/332], loss=5.0774
	step [268/332], loss=6.7688
	step [269/332], loss=4.7949
	step [270/332], loss=5.4824
	step [271/332], loss=5.9565
	step [272/332], loss=5.8137
	step [273/332], loss=5.2648
	step [274/332], loss=5.4146
	step [275/332], loss=5.8660
	step [276/332], loss=6.0497
	step [277/332], loss=5.9253
	step [278/332], loss=5.6289
	step [279/332], loss=5.2953
	step [280/332], loss=6.0894
	step [281/332], loss=5.3754
	step [282/332], loss=5.6219
	step [283/332], loss=5.0540
	step [284/332], loss=6.0820
	step [285/332], loss=5.9052
	step [286/332], loss=4.3755
	step [287/332], loss=7.3447
	step [288/332], loss=5.8916
	step [289/332], loss=5.4996
	step [290/332], loss=5.7368
	step [291/332], loss=5.4779
	step [292/332], loss=6.0961
	step [293/332], loss=4.1421
	step [294/332], loss=6.1019
	step [295/332], loss=5.3622
	step [296/332], loss=5.6421
	step [297/332], loss=6.0773
	step [298/332], loss=6.6411
	step [299/332], loss=5.6450
	step [300/332], loss=4.9830
	step [301/332], loss=5.5310
	step [302/332], loss=4.5779
	step [303/332], loss=5.5393
	step [304/332], loss=6.2196
	step [305/332], loss=4.3942
	step [306/332], loss=5.4658
	step [307/332], loss=5.7604
	step [308/332], loss=5.8502
	step [309/332], loss=6.0516
	step [310/332], loss=6.8652
	step [311/332], loss=5.9870
	step [312/332], loss=7.1956
	step [313/332], loss=5.7803
	step [314/332], loss=6.0293
	step [315/332], loss=5.5007
	step [316/332], loss=7.0323
	step [317/332], loss=6.5486
	step [318/332], loss=5.4798
	step [319/332], loss=6.3890
	step [320/332], loss=5.5178
	step [321/332], loss=6.1410
	step [322/332], loss=5.7251
	step [323/332], loss=5.5886
	step [324/332], loss=5.2082
	step [325/332], loss=5.0823
	step [326/332], loss=5.3963
	step [327/332], loss=5.4348
	step [328/332], loss=5.6826
	step [329/332], loss=5.8341
	step [330/332], loss=5.8762
	step [331/332], loss=5.7155
	step [332/332], loss=3.3081
	Evaluating
	loss=0.0196, precision=0.1760, recall=0.9960, f1=0.2991
Training epoch 16
	step [1/332], loss=7.0002
	step [2/332], loss=5.0682
	step [3/332], loss=5.0771
	step [4/332], loss=5.4347
	step [5/332], loss=5.5112
	step [6/332], loss=7.4401
	step [7/332], loss=6.2448
	step [8/332], loss=5.2265
	step [9/332], loss=6.4314
	step [10/332], loss=5.7663
	step [11/332], loss=6.1137
	step [12/332], loss=5.9785
	step [13/332], loss=5.2051
	step [14/332], loss=6.0147
	step [15/332], loss=4.7713
	step [16/332], loss=5.9893
	step [17/332], loss=5.1116
	step [18/332], loss=4.6825
	step [19/332], loss=6.8719
	step [20/332], loss=5.4067
	step [21/332], loss=8.1637
	step [22/332], loss=5.4611
	step [23/332], loss=5.4532
	step [24/332], loss=6.1503
	step [25/332], loss=4.8716
	step [26/332], loss=6.6525
	step [27/332], loss=5.7236
	step [28/332], loss=6.7160
	step [29/332], loss=5.1890
	step [30/332], loss=5.7763
	step [31/332], loss=4.7578
	step [32/332], loss=5.5446
	step [33/332], loss=5.9730
	step [34/332], loss=7.4235
	step [35/332], loss=5.7292
	step [36/332], loss=7.0451
	step [37/332], loss=5.8945
	step [38/332], loss=5.1784
	step [39/332], loss=6.5731
	step [40/332], loss=5.8813
	step [41/332], loss=5.4943
	step [42/332], loss=5.3145
	step [43/332], loss=6.0519
	step [44/332], loss=6.3459
	step [45/332], loss=5.1006
	step [46/332], loss=5.8448
	step [47/332], loss=4.9390
	step [48/332], loss=5.5121
	step [49/332], loss=5.9453
	step [50/332], loss=5.8031
	step [51/332], loss=6.0554
	step [52/332], loss=5.5656
	step [53/332], loss=5.6194
	step [54/332], loss=5.4506
	step [55/332], loss=5.7394
	step [56/332], loss=6.5844
	step [57/332], loss=5.5458
	step [58/332], loss=5.5937
	step [59/332], loss=6.5635
	step [60/332], loss=5.7114
	step [61/332], loss=5.8256
	step [62/332], loss=4.6971
	step [63/332], loss=4.9807
	step [64/332], loss=6.1669
	step [65/332], loss=7.0417
	step [66/332], loss=6.0935
	step [67/332], loss=6.1893
	step [68/332], loss=5.7084
	step [69/332], loss=5.9919
	step [70/332], loss=6.1656
	step [71/332], loss=5.1210
	step [72/332], loss=4.8765
	step [73/332], loss=7.6782
	step [74/332], loss=4.6312
	step [75/332], loss=4.7756
	step [76/332], loss=5.7569
	step [77/332], loss=5.5024
	step [78/332], loss=6.1725
	step [79/332], loss=5.8242
	step [80/332], loss=7.0061
	step [81/332], loss=4.7682
	step [82/332], loss=6.4991
	step [83/332], loss=6.2734
	step [84/332], loss=6.0771
	step [85/332], loss=5.5021
	step [86/332], loss=6.0472
	step [87/332], loss=6.9313
	step [88/332], loss=6.4692
	step [89/332], loss=7.1697
	step [90/332], loss=6.4954
	step [91/332], loss=4.9665
	step [92/332], loss=6.1650
	step [93/332], loss=6.6063
	step [94/332], loss=6.2174
	step [95/332], loss=6.7297
	step [96/332], loss=5.7502
	step [97/332], loss=6.9290
	step [98/332], loss=4.8195
	step [99/332], loss=6.4008
	step [100/332], loss=4.9972
	step [101/332], loss=5.9400
	step [102/332], loss=4.5201
	step [103/332], loss=5.1528
	step [104/332], loss=5.1367
	step [105/332], loss=8.4375
	step [106/332], loss=4.4907
	step [107/332], loss=7.6309
	step [108/332], loss=4.4284
	step [109/332], loss=5.1065
	step [110/332], loss=6.4334
	step [111/332], loss=5.8985
	step [112/332], loss=5.6890
	step [113/332], loss=5.6344
	step [114/332], loss=4.8770
	step [115/332], loss=4.8653
	step [116/332], loss=7.6707
	step [117/332], loss=6.0532
	step [118/332], loss=5.7402
	step [119/332], loss=6.5480
	step [120/332], loss=6.6970
	step [121/332], loss=4.9489
	step [122/332], loss=5.1286
	step [123/332], loss=5.2937
	step [124/332], loss=4.3715
	step [125/332], loss=5.4860
	step [126/332], loss=6.2230
	step [127/332], loss=5.7585
	step [128/332], loss=6.6249
	step [129/332], loss=5.4254
	step [130/332], loss=4.1225
	step [131/332], loss=5.9787
	step [132/332], loss=5.9202
	step [133/332], loss=4.9506
	step [134/332], loss=7.0733
	step [135/332], loss=6.0282
	step [136/332], loss=5.8840
	step [137/332], loss=5.7854
	step [138/332], loss=5.4893
	step [139/332], loss=5.5633
	step [140/332], loss=5.3441
	step [141/332], loss=6.0664
	step [142/332], loss=6.1218
	step [143/332], loss=6.2644
	step [144/332], loss=5.2320
	step [145/332], loss=5.8347
	step [146/332], loss=4.8323
	step [147/332], loss=5.6409
	step [148/332], loss=6.2678
	step [149/332], loss=5.6830
	step [150/332], loss=5.9603
	step [151/332], loss=5.4040
	step [152/332], loss=6.1635
	step [153/332], loss=6.3652
	step [154/332], loss=6.6870
	step [155/332], loss=5.3071
	step [156/332], loss=5.3577
	step [157/332], loss=4.8749
	step [158/332], loss=4.5035
	step [159/332], loss=5.8493
	step [160/332], loss=5.0536
	step [161/332], loss=5.0689
	step [162/332], loss=6.7929
	step [163/332], loss=5.2471
	step [164/332], loss=4.7301
	step [165/332], loss=5.9431
	step [166/332], loss=5.4501
	step [167/332], loss=6.0784
	step [168/332], loss=6.6116
	step [169/332], loss=6.2762
	step [170/332], loss=6.2293
	step [171/332], loss=6.9507
	step [172/332], loss=5.6269
	step [173/332], loss=7.7475
	step [174/332], loss=5.8023
	step [175/332], loss=5.4044
	step [176/332], loss=5.3190
	step [177/332], loss=5.8009
	step [178/332], loss=5.7265
	step [179/332], loss=5.4813
	step [180/332], loss=5.4814
	step [181/332], loss=7.0113
	step [182/332], loss=5.6472
	step [183/332], loss=5.1840
	step [184/332], loss=5.5295
	step [185/332], loss=4.8822
	step [186/332], loss=5.7638
	step [187/332], loss=5.3348
	step [188/332], loss=5.6340
	step [189/332], loss=4.6042
	step [190/332], loss=5.6959
	step [191/332], loss=6.9691
	step [192/332], loss=6.2632
	step [193/332], loss=5.4166
	step [194/332], loss=5.4213
	step [195/332], loss=6.9945
	step [196/332], loss=6.8373
	step [197/332], loss=5.4465
	step [198/332], loss=7.4125
	step [199/332], loss=5.6068
	step [200/332], loss=7.4420
	step [201/332], loss=5.2640
	step [202/332], loss=4.6530
	step [203/332], loss=5.7058
	step [204/332], loss=4.7613
	step [205/332], loss=5.8580
	step [206/332], loss=7.0180
	step [207/332], loss=5.3924
	step [208/332], loss=6.2212
	step [209/332], loss=5.0462
	step [210/332], loss=5.9824
	step [211/332], loss=5.8770
	step [212/332], loss=5.7710
	step [213/332], loss=5.8567
	step [214/332], loss=5.3288
	step [215/332], loss=6.9213
	step [216/332], loss=5.2559
	step [217/332], loss=5.1149
	step [218/332], loss=5.1520
	step [219/332], loss=5.2475
	step [220/332], loss=7.4455
	step [221/332], loss=5.7410
	step [222/332], loss=6.3267
	step [223/332], loss=5.8518
	step [224/332], loss=5.5965
	step [225/332], loss=5.4554
	step [226/332], loss=5.6482
	step [227/332], loss=6.7943
	step [228/332], loss=7.1422
	step [229/332], loss=5.8726
	step [230/332], loss=6.6116
	step [231/332], loss=5.2064
	step [232/332], loss=5.4959
	step [233/332], loss=5.7676
	step [234/332], loss=4.1700
	step [235/332], loss=5.8163
	step [236/332], loss=5.6899
	step [237/332], loss=6.2529
	step [238/332], loss=6.0265
	step [239/332], loss=3.9785
	step [240/332], loss=7.7590
	step [241/332], loss=5.7809
	step [242/332], loss=5.6783
	step [243/332], loss=6.8199
	step [244/332], loss=6.3691
	step [245/332], loss=5.6028
	step [246/332], loss=6.0480
	step [247/332], loss=4.9523
	step [248/332], loss=5.9140
	step [249/332], loss=4.9346
	step [250/332], loss=4.1686
	step [251/332], loss=6.7306
	step [252/332], loss=5.7448
	step [253/332], loss=6.3246
	step [254/332], loss=7.7947
	step [255/332], loss=6.9057
	step [256/332], loss=4.5373
	step [257/332], loss=5.4513
	step [258/332], loss=5.9114
	step [259/332], loss=5.6332
	step [260/332], loss=6.4673
	step [261/332], loss=4.3021
	step [262/332], loss=5.3434
	step [263/332], loss=4.2816
	step [264/332], loss=5.6726
	step [265/332], loss=5.4140
	step [266/332], loss=5.2363
	step [267/332], loss=6.2961
	step [268/332], loss=4.2396
	step [269/332], loss=7.8796
	step [270/332], loss=6.0071
	step [271/332], loss=5.8786
	step [272/332], loss=6.6782
	step [273/332], loss=5.9066
	step [274/332], loss=6.5756
	step [275/332], loss=6.1019
	step [276/332], loss=5.0603
	step [277/332], loss=5.6510
	step [278/332], loss=5.7208
	step [279/332], loss=5.3902
	step [280/332], loss=5.9429
	step [281/332], loss=5.5284
	step [282/332], loss=6.3337
	step [283/332], loss=4.7236
	step [284/332], loss=6.1088
	step [285/332], loss=6.2214
	step [286/332], loss=6.0358
	step [287/332], loss=4.6347
	step [288/332], loss=6.1153
	step [289/332], loss=5.9979
	step [290/332], loss=5.2857
	step [291/332], loss=6.6386
	step [292/332], loss=5.0146
	step [293/332], loss=6.8479
	step [294/332], loss=4.4027
	step [295/332], loss=5.2566
	step [296/332], loss=5.6926
	step [297/332], loss=6.0165
	step [298/332], loss=6.6051
	step [299/332], loss=6.5915
	step [300/332], loss=5.4828
	step [301/332], loss=5.8283
	step [302/332], loss=6.1870
	step [303/332], loss=6.8675
	step [304/332], loss=7.2103
	step [305/332], loss=6.0892
	step [306/332], loss=6.8930
	step [307/332], loss=6.2115
	step [308/332], loss=4.4239
	step [309/332], loss=5.7387
	step [310/332], loss=4.7740
	step [311/332], loss=5.2085
	step [312/332], loss=5.5302
	step [313/332], loss=7.1795
	step [314/332], loss=3.5750
	step [315/332], loss=4.6118
	step [316/332], loss=6.5379
	step [317/332], loss=5.1427
	step [318/332], loss=6.1884
	step [319/332], loss=5.8661
	step [320/332], loss=8.0093
	step [321/332], loss=4.5222
	step [322/332], loss=5.4975
	step [323/332], loss=6.1275
	step [324/332], loss=5.8837
	step [325/332], loss=5.7737
	step [326/332], loss=4.6148
	step [327/332], loss=5.7668
	step [328/332], loss=5.6630
	step [329/332], loss=6.1491
	step [330/332], loss=6.6089
	step [331/332], loss=5.6434
	step [332/332], loss=5.6787
	Evaluating
	loss=0.0238, precision=0.1578, recall=0.9965, f1=0.2724
Training epoch 17
	step [1/332], loss=5.6946
	step [2/332], loss=4.7473
	step [3/332], loss=5.2647
	step [4/332], loss=6.6896
	step [5/332], loss=5.7719
	step [6/332], loss=5.2480
	step [7/332], loss=5.1277
	step [8/332], loss=6.7912
	step [9/332], loss=7.0276
	step [10/332], loss=4.8431
	step [11/332], loss=6.0783
	step [12/332], loss=5.8954
	step [13/332], loss=5.9314
	step [14/332], loss=5.0264
	step [15/332], loss=6.1492
	step [16/332], loss=5.0260
	step [17/332], loss=6.2018
	step [18/332], loss=4.5742
	step [19/332], loss=5.5223
	step [20/332], loss=4.7700
	step [21/332], loss=5.7698
	step [22/332], loss=6.5718
	step [23/332], loss=5.5828
	step [24/332], loss=5.6855
	step [25/332], loss=6.8271
	step [26/332], loss=5.8499
	step [27/332], loss=6.2219
	step [28/332], loss=4.6001
	step [29/332], loss=4.2356
	step [30/332], loss=4.3103
	step [31/332], loss=6.7870
	step [32/332], loss=5.6941
	step [33/332], loss=4.7853
	step [34/332], loss=5.7537
	step [35/332], loss=4.7372
	step [36/332], loss=6.4894
	step [37/332], loss=6.0461
	step [38/332], loss=5.8788
	step [39/332], loss=7.3531
	step [40/332], loss=5.2144
	step [41/332], loss=4.7307
	step [42/332], loss=4.8014
	step [43/332], loss=6.0705
	step [44/332], loss=5.0560
	step [45/332], loss=5.5971
	step [46/332], loss=4.9454
	step [47/332], loss=5.5248
	step [48/332], loss=6.2821
	step [49/332], loss=4.5006
	step [50/332], loss=5.5544
	step [51/332], loss=4.9325
	step [52/332], loss=7.1385
	step [53/332], loss=4.7675
	step [54/332], loss=4.8420
	step [55/332], loss=5.3132
	step [56/332], loss=6.3738
	step [57/332], loss=4.9400
	step [58/332], loss=5.6810
	step [59/332], loss=4.6899
	step [60/332], loss=6.2751
	step [61/332], loss=6.3977
	step [62/332], loss=5.6094
	step [63/332], loss=6.7514
	step [64/332], loss=5.4688
	step [65/332], loss=4.7113
	step [66/332], loss=6.0001
	step [67/332], loss=5.5820
	step [68/332], loss=5.6052
	step [69/332], loss=5.6752
	step [70/332], loss=6.3009
	step [71/332], loss=5.5415
	step [72/332], loss=5.1943
	step [73/332], loss=4.9242
	step [74/332], loss=5.1947
	step [75/332], loss=5.0880
	step [76/332], loss=5.0730
	step [77/332], loss=4.4032
	step [78/332], loss=4.8009
	step [79/332], loss=5.4237
	step [80/332], loss=5.0180
	step [81/332], loss=6.1092
	step [82/332], loss=5.9085
	step [83/332], loss=5.6609
	step [84/332], loss=5.4682
	step [85/332], loss=5.4439
	step [86/332], loss=7.8928
	step [87/332], loss=5.2997
	step [88/332], loss=4.9454
	step [89/332], loss=5.8169
	step [90/332], loss=5.2682
	step [91/332], loss=6.9772
	step [92/332], loss=6.9100
	step [93/332], loss=5.7626
	step [94/332], loss=4.6641
	step [95/332], loss=5.0319
	step [96/332], loss=5.2957
	step [97/332], loss=4.6100
	step [98/332], loss=5.3104
	step [99/332], loss=4.5660
	step [100/332], loss=6.1078
	step [101/332], loss=4.6933
	step [102/332], loss=4.7021
	step [103/332], loss=4.8382
	step [104/332], loss=5.8242
	step [105/332], loss=4.6446
	step [106/332], loss=6.7676
	step [107/332], loss=4.6798
	step [108/332], loss=7.2530
	step [109/332], loss=5.0941
	step [110/332], loss=5.2880
	step [111/332], loss=4.7322
	step [112/332], loss=6.7474
	step [113/332], loss=4.6289
	step [114/332], loss=5.0173
	step [115/332], loss=5.9736
	step [116/332], loss=5.8649
	step [117/332], loss=7.0100
	step [118/332], loss=5.5441
	step [119/332], loss=6.4997
	step [120/332], loss=5.7381
	step [121/332], loss=4.8875
	step [122/332], loss=6.2039
	step [123/332], loss=5.5669
	step [124/332], loss=5.4642
	step [125/332], loss=6.2815
	step [126/332], loss=4.8640
	step [127/332], loss=5.6589
	step [128/332], loss=5.8957
	step [129/332], loss=6.0193
	step [130/332], loss=6.0930
	step [131/332], loss=7.1479
	step [132/332], loss=5.1263
	step [133/332], loss=5.0217
	step [134/332], loss=7.0930
	step [135/332], loss=5.7480
	step [136/332], loss=4.5133
	step [137/332], loss=5.1069
	step [138/332], loss=7.5557
	step [139/332], loss=5.1494
	step [140/332], loss=5.3732
	step [141/332], loss=5.3638
	step [142/332], loss=5.2967
	step [143/332], loss=5.5137
	step [144/332], loss=5.0070
	step [145/332], loss=7.1245
	step [146/332], loss=5.1305
	step [147/332], loss=6.5875
	step [148/332], loss=6.3489
	step [149/332], loss=5.5782
	step [150/332], loss=5.4939
	step [151/332], loss=5.1172
	step [152/332], loss=4.8072
	step [153/332], loss=5.7739
	step [154/332], loss=5.2371
	step [155/332], loss=5.8614
	step [156/332], loss=5.5973
	step [157/332], loss=5.3760
	step [158/332], loss=5.3441
	step [159/332], loss=5.6198
	step [160/332], loss=5.5759
	step [161/332], loss=5.3122
	step [162/332], loss=3.6223
	step [163/332], loss=5.3356
	step [164/332], loss=6.0278
	step [165/332], loss=5.9781
	step [166/332], loss=6.8610
	step [167/332], loss=5.9036
	step [168/332], loss=6.0176
	step [169/332], loss=5.5668
	step [170/332], loss=5.5068
	step [171/332], loss=6.0308
	step [172/332], loss=5.3850
	step [173/332], loss=4.5486
	step [174/332], loss=5.0418
	step [175/332], loss=5.5919
	step [176/332], loss=5.8149
	step [177/332], loss=3.6619
	step [178/332], loss=4.5671
	step [179/332], loss=6.2001
	step [180/332], loss=5.9645
	step [181/332], loss=3.3753
	step [182/332], loss=5.0630
	step [183/332], loss=6.7813
	step [184/332], loss=6.5440
	step [185/332], loss=6.2794
	step [186/332], loss=4.8870
	step [187/332], loss=4.9432
	step [188/332], loss=5.4062
	step [189/332], loss=4.4836
	step [190/332], loss=5.7277
	step [191/332], loss=5.4964
	step [192/332], loss=5.2798
	step [193/332], loss=4.8904
	step [194/332], loss=5.0679
	step [195/332], loss=6.9430
	step [196/332], loss=5.5947
	step [197/332], loss=6.1805
	step [198/332], loss=7.1241
	step [199/332], loss=5.8910
	step [200/332], loss=5.0152
	step [201/332], loss=4.5798
	step [202/332], loss=5.2373
	step [203/332], loss=5.9198
	step [204/332], loss=5.1111
	step [205/332], loss=4.9019
	step [206/332], loss=5.2920
	step [207/332], loss=7.0433
	step [208/332], loss=6.2289
	step [209/332], loss=6.2901
	step [210/332], loss=5.7966
	step [211/332], loss=4.7525
	step [212/332], loss=5.7236
	step [213/332], loss=6.4215
	step [214/332], loss=5.5138
	step [215/332], loss=5.2146
	step [216/332], loss=4.7094
	step [217/332], loss=6.1404
	step [218/332], loss=5.3393
	step [219/332], loss=4.1836
	step [220/332], loss=5.5802
	step [221/332], loss=4.2877
	step [222/332], loss=5.3169
	step [223/332], loss=8.2124
	step [224/332], loss=4.8020
	step [225/332], loss=6.9396
	step [226/332], loss=6.2131
	step [227/332], loss=6.3999
	step [228/332], loss=6.4537
	step [229/332], loss=4.7897
	step [230/332], loss=6.0872
	step [231/332], loss=6.1569
	step [232/332], loss=5.6034
	step [233/332], loss=6.0889
	step [234/332], loss=5.1335
	step [235/332], loss=6.6995
	step [236/332], loss=5.1231
	step [237/332], loss=5.6216
	step [238/332], loss=5.0378
	step [239/332], loss=5.0457
	step [240/332], loss=6.9269
	step [241/332], loss=5.0451
	step [242/332], loss=6.6058
	step [243/332], loss=6.4121
	step [244/332], loss=4.4497
	step [245/332], loss=5.0270
	step [246/332], loss=6.8836
	step [247/332], loss=6.2284
	step [248/332], loss=5.2240
	step [249/332], loss=7.7017
	step [250/332], loss=6.0292
	step [251/332], loss=4.8807
	step [252/332], loss=5.6171
	step [253/332], loss=4.4708
	step [254/332], loss=5.9658
	step [255/332], loss=7.1540
	step [256/332], loss=5.3265
	step [257/332], loss=5.8040
	step [258/332], loss=6.0518
	step [259/332], loss=7.1074
	step [260/332], loss=5.6199
	step [261/332], loss=4.7518
	step [262/332], loss=5.3793
	step [263/332], loss=4.9636
	step [264/332], loss=4.5053
	step [265/332], loss=5.8099
	step [266/332], loss=4.7352
	step [267/332], loss=4.8406
	step [268/332], loss=5.1836
	step [269/332], loss=5.2839
	step [270/332], loss=4.8375
	step [271/332], loss=6.0040
	step [272/332], loss=7.9105
	step [273/332], loss=5.4580
	step [274/332], loss=5.3291
	step [275/332], loss=5.3988
	step [276/332], loss=4.7919
	step [277/332], loss=8.0108
	step [278/332], loss=5.4968
	step [279/332], loss=5.3566
	step [280/332], loss=4.0248
	step [281/332], loss=4.4759
	step [282/332], loss=6.6322
	step [283/332], loss=6.0335
	step [284/332], loss=6.2997
	step [285/332], loss=6.4828
	step [286/332], loss=6.1420
	step [287/332], loss=5.5882
	step [288/332], loss=5.5756
	step [289/332], loss=5.0549
	step [290/332], loss=5.5238
	step [291/332], loss=6.6868
	step [292/332], loss=4.9063
	step [293/332], loss=5.9442
	step [294/332], loss=6.4614
	step [295/332], loss=5.4673
	step [296/332], loss=5.2929
	step [297/332], loss=6.1830
	step [298/332], loss=5.3904
	step [299/332], loss=4.3429
	step [300/332], loss=6.2197
	step [301/332], loss=6.6569
	step [302/332], loss=5.5700
	step [303/332], loss=6.0072
	step [304/332], loss=6.3506
	step [305/332], loss=4.7752
	step [306/332], loss=5.7280
	step [307/332], loss=4.4478
	step [308/332], loss=6.0034
	step [309/332], loss=6.2649
	step [310/332], loss=6.8568
	step [311/332], loss=6.1295
	step [312/332], loss=7.3344
	step [313/332], loss=6.5439
	step [314/332], loss=5.5655
	step [315/332], loss=4.8590
	step [316/332], loss=5.6803
	step [317/332], loss=5.4098
	step [318/332], loss=4.6278
	step [319/332], loss=8.3909
	step [320/332], loss=7.0979
	step [321/332], loss=7.2454
	step [322/332], loss=4.9080
	step [323/332], loss=5.5700
	step [324/332], loss=5.8963
	step [325/332], loss=5.3419
	step [326/332], loss=5.6782
	step [327/332], loss=5.1067
	step [328/332], loss=5.9752
	step [329/332], loss=5.6742
	step [330/332], loss=5.4959
	step [331/332], loss=4.5476
	step [332/332], loss=4.6026
	Evaluating
	loss=0.0177, precision=0.2095, recall=0.9948, f1=0.3462
saving model as: 0_saved_model.pth
Training epoch 18
	step [1/332], loss=5.8966
	step [2/332], loss=4.8232
	step [3/332], loss=5.8585
	step [4/332], loss=4.7929
	step [5/332], loss=5.4424
	step [6/332], loss=4.7202
	step [7/332], loss=5.9162
	step [8/332], loss=4.4838
	step [9/332], loss=6.4205
	step [10/332], loss=5.7492
	step [11/332], loss=3.7170
	step [12/332], loss=4.9467
	step [13/332], loss=4.8738
	step [14/332], loss=6.2486
	step [15/332], loss=5.7783
	step [16/332], loss=5.4718
	step [17/332], loss=5.6593
	step [18/332], loss=5.1260
	step [19/332], loss=5.4461
	step [20/332], loss=4.1593
	step [21/332], loss=4.9657
	step [22/332], loss=4.4280
	step [23/332], loss=6.0017
	step [24/332], loss=6.0133
	step [25/332], loss=6.6875
	step [26/332], loss=4.9993
	step [27/332], loss=6.5769
	step [28/332], loss=5.8448
	step [29/332], loss=5.9195
	step [30/332], loss=4.8421
	step [31/332], loss=7.3274
	step [32/332], loss=6.2410
	step [33/332], loss=6.7193
	step [34/332], loss=6.1373
	step [35/332], loss=5.7091
	step [36/332], loss=4.9780
	step [37/332], loss=5.8437
	step [38/332], loss=6.3524
	step [39/332], loss=4.3747
	step [40/332], loss=5.0342
	step [41/332], loss=4.7127
	step [42/332], loss=6.7655
	step [43/332], loss=5.4163
	step [44/332], loss=5.9049
	step [45/332], loss=4.6325
	step [46/332], loss=5.7292
	step [47/332], loss=6.4688
	step [48/332], loss=6.1719
	step [49/332], loss=5.6360
	step [50/332], loss=5.9123
	step [51/332], loss=5.4589
	step [52/332], loss=5.4842
	step [53/332], loss=6.2148
	step [54/332], loss=6.1839
	step [55/332], loss=5.6936
	step [56/332], loss=4.7757
	step [57/332], loss=5.3201
	step [58/332], loss=6.3889
	step [59/332], loss=5.4436
	step [60/332], loss=4.4724
	step [61/332], loss=5.0957
	step [62/332], loss=4.1797
	step [63/332], loss=5.2645
	step [64/332], loss=3.9502
	step [65/332], loss=6.1715
	step [66/332], loss=5.1098
	step [67/332], loss=4.9649
	step [68/332], loss=5.4684
	step [69/332], loss=4.9473
	step [70/332], loss=5.9311
	step [71/332], loss=4.7730
	step [72/332], loss=4.8903
	step [73/332], loss=5.2339
	step [74/332], loss=4.1809
	step [75/332], loss=4.9349
	step [76/332], loss=4.7652
	step [77/332], loss=6.5381
	step [78/332], loss=4.6634
	step [79/332], loss=4.9926
	step [80/332], loss=6.3289
	step [81/332], loss=5.6852
	step [82/332], loss=4.8064
	step [83/332], loss=5.2385
	step [84/332], loss=4.9536
	step [85/332], loss=7.1272
	step [86/332], loss=5.8166
	step [87/332], loss=6.8478
	step [88/332], loss=4.8155
	step [89/332], loss=4.9468
	step [90/332], loss=6.5416
	step [91/332], loss=6.7099
	step [92/332], loss=5.5277
	step [93/332], loss=5.3513
	step [94/332], loss=5.9179
	step [95/332], loss=5.1373
	step [96/332], loss=5.6148
	step [97/332], loss=5.0388
	step [98/332], loss=4.9777
	step [99/332], loss=5.4568
	step [100/332], loss=5.0559
	step [101/332], loss=6.3163
	step [102/332], loss=5.1500
	step [103/332], loss=4.8930
	step [104/332], loss=5.3197
	step [105/332], loss=7.3374
	step [106/332], loss=5.0746
	step [107/332], loss=5.3684
	step [108/332], loss=4.4066
	step [109/332], loss=4.7469
	step [110/332], loss=5.0397
	step [111/332], loss=5.2725
	step [112/332], loss=6.9629
	step [113/332], loss=4.8441
	step [114/332], loss=4.9396
	step [115/332], loss=5.5651
	step [116/332], loss=6.9098
	step [117/332], loss=5.5750
	step [118/332], loss=4.5157
	step [119/332], loss=5.5629
	step [120/332], loss=6.1490
	step [121/332], loss=6.6806
	step [122/332], loss=5.2114
	step [123/332], loss=4.5768
	step [124/332], loss=5.8030
	step [125/332], loss=4.5577
	step [126/332], loss=5.7697
	step [127/332], loss=5.4870
	step [128/332], loss=5.0161
	step [129/332], loss=5.3031
	step [130/332], loss=6.6631
	step [131/332], loss=5.5495
	step [132/332], loss=4.7899
	step [133/332], loss=4.7902
	step [134/332], loss=5.6565
	step [135/332], loss=4.0593
	step [136/332], loss=5.3469
	step [137/332], loss=5.4092
	step [138/332], loss=6.0427
	step [139/332], loss=5.1364
	step [140/332], loss=5.2552
	step [141/332], loss=4.6942
	step [142/332], loss=5.1715
	step [143/332], loss=4.7536
	step [144/332], loss=4.4164
	step [145/332], loss=4.6193
	step [146/332], loss=6.7332
	step [147/332], loss=4.5616
	step [148/332], loss=6.0450
	step [149/332], loss=4.7918
	step [150/332], loss=5.5535
	step [151/332], loss=5.1601
	step [152/332], loss=4.4814
	step [153/332], loss=4.6573
	step [154/332], loss=4.9108
	step [155/332], loss=5.4737
	step [156/332], loss=5.3836
	step [157/332], loss=5.4331
	step [158/332], loss=5.3228
	step [159/332], loss=5.5026
	step [160/332], loss=4.5666
	step [161/332], loss=4.7875
	step [162/332], loss=4.8166
	step [163/332], loss=4.2984
	step [164/332], loss=4.9875
	step [165/332], loss=7.2761
	step [166/332], loss=5.6789
	step [167/332], loss=4.6767
	step [168/332], loss=4.6517
	step [169/332], loss=6.0278
	step [170/332], loss=4.3053
	step [171/332], loss=4.7527
	step [172/332], loss=5.0972
	step [173/332], loss=6.0919
	step [174/332], loss=8.6410
	step [175/332], loss=4.8449
	step [176/332], loss=3.6239
	step [177/332], loss=6.3690
	step [178/332], loss=4.7720
	step [179/332], loss=5.4743
	step [180/332], loss=5.3164
	step [181/332], loss=5.9153
	step [182/332], loss=5.5742
	step [183/332], loss=7.0821
	step [184/332], loss=5.6405
	step [185/332], loss=5.4478
	step [186/332], loss=4.8080
	step [187/332], loss=4.7482
	step [188/332], loss=4.9168
	step [189/332], loss=4.0037
	step [190/332], loss=5.5946
	step [191/332], loss=4.9185
	step [192/332], loss=5.0552
	step [193/332], loss=5.0102
	step [194/332], loss=6.2298
	step [195/332], loss=8.6324
	step [196/332], loss=5.3019
	step [197/332], loss=5.4140
	step [198/332], loss=6.0498
	step [199/332], loss=5.8337
	step [200/332], loss=5.2915
	step [201/332], loss=4.9318
	step [202/332], loss=6.9387
	step [203/332], loss=6.0051
	step [204/332], loss=5.7526
	step [205/332], loss=5.6804
	step [206/332], loss=4.7400
	step [207/332], loss=4.8568
	step [208/332], loss=5.6014
	step [209/332], loss=5.4828
	step [210/332], loss=6.3112
	step [211/332], loss=5.4484
	step [212/332], loss=5.3289
	step [213/332], loss=5.3450
	step [214/332], loss=6.1515
	step [215/332], loss=4.7240
	step [216/332], loss=3.6191
	step [217/332], loss=4.1823
	step [218/332], loss=5.5642
	step [219/332], loss=5.7396
	step [220/332], loss=5.5367
	step [221/332], loss=5.9073
	step [222/332], loss=5.4193
	step [223/332], loss=5.9272
	step [224/332], loss=6.1387
	step [225/332], loss=4.9429
	step [226/332], loss=4.4949
	step [227/332], loss=4.2533
	step [228/332], loss=5.9302
	step [229/332], loss=5.9762
	step [230/332], loss=5.2914
	step [231/332], loss=5.4932
	step [232/332], loss=5.6334
	step [233/332], loss=6.4000
	step [234/332], loss=4.9298
	step [235/332], loss=5.2201
	step [236/332], loss=6.4442
	step [237/332], loss=5.9959
	step [238/332], loss=4.3155
	step [239/332], loss=3.8075
	step [240/332], loss=5.0028
	step [241/332], loss=6.9584
	step [242/332], loss=7.0601
	step [243/332], loss=5.2023
	step [244/332], loss=6.9540
	step [245/332], loss=6.0674
	step [246/332], loss=5.3236
	step [247/332], loss=6.5820
	step [248/332], loss=5.9442
	step [249/332], loss=4.1905
	step [250/332], loss=4.6632
	step [251/332], loss=5.0337
	step [252/332], loss=5.7679
	step [253/332], loss=5.7313
	step [254/332], loss=5.1365
	step [255/332], loss=4.2935
	step [256/332], loss=4.2064
	step [257/332], loss=5.5147
	step [258/332], loss=4.6050
	step [259/332], loss=4.2714
	step [260/332], loss=5.0405
	step [261/332], loss=5.7610
	step [262/332], loss=7.1290
	step [263/332], loss=5.3218
	step [264/332], loss=5.7674
	step [265/332], loss=5.3196
	step [266/332], loss=5.0962
	step [267/332], loss=5.6902
	step [268/332], loss=5.6053
	step [269/332], loss=6.2781
	step [270/332], loss=5.6673
	step [271/332], loss=4.6396
	step [272/332], loss=4.5318
	step [273/332], loss=5.7710
	step [274/332], loss=5.3098
	step [275/332], loss=5.3948
	step [276/332], loss=6.2778
	step [277/332], loss=5.2220
	step [278/332], loss=4.3761
	step [279/332], loss=4.4604
	step [280/332], loss=4.4262
	step [281/332], loss=5.8923
	step [282/332], loss=5.4422
	step [283/332], loss=5.1998
	step [284/332], loss=4.5263
	step [285/332], loss=6.3312
	step [286/332], loss=4.8880
	step [287/332], loss=6.7003
	step [288/332], loss=5.9122
	step [289/332], loss=5.6369
	step [290/332], loss=3.8914
	step [291/332], loss=5.5942
	step [292/332], loss=5.5701
	step [293/332], loss=4.9583
	step [294/332], loss=5.8614
	step [295/332], loss=6.3966
	step [296/332], loss=5.1158
	step [297/332], loss=4.5377
	step [298/332], loss=6.0445
	step [299/332], loss=5.0604
	step [300/332], loss=5.7912
	step [301/332], loss=5.4920
	step [302/332], loss=5.0503
	step [303/332], loss=5.9939
	step [304/332], loss=6.1143
	step [305/332], loss=5.2935
	step [306/332], loss=4.9887
	step [307/332], loss=5.8091
	step [308/332], loss=5.6033
	step [309/332], loss=5.3565
	step [310/332], loss=5.0914
	step [311/332], loss=4.0700
	step [312/332], loss=5.0497
	step [313/332], loss=4.8147
	step [314/332], loss=4.9893
	step [315/332], loss=5.2610
	step [316/332], loss=5.3311
	step [317/332], loss=4.7986
	step [318/332], loss=5.7506
	step [319/332], loss=6.0933
	step [320/332], loss=5.5540
	step [321/332], loss=6.1564
	step [322/332], loss=5.8647
	step [323/332], loss=6.3274
	step [324/332], loss=6.8989
	step [325/332], loss=4.7228
	step [326/332], loss=5.6644
	step [327/332], loss=5.1613
	step [328/332], loss=5.2781
	step [329/332], loss=6.0248
	step [330/332], loss=4.7623
	step [331/332], loss=5.9067
	step [332/332], loss=2.8417
	Evaluating
	loss=0.0179, precision=0.1945, recall=0.9951, f1=0.3254
Training epoch 19
	step [1/332], loss=5.3390
	step [2/332], loss=5.3782
	step [3/332], loss=5.3750
	step [4/332], loss=4.2845
	step [5/332], loss=4.0831
	step [6/332], loss=4.0501
	step [7/332], loss=4.2323
	step [8/332], loss=4.5789
	step [9/332], loss=5.2808
	step [10/332], loss=5.5128
	step [11/332], loss=5.4857
	step [12/332], loss=5.6678
	step [13/332], loss=4.9997
	step [14/332], loss=4.3390
	step [15/332], loss=5.2381
	step [16/332], loss=4.6989
	step [17/332], loss=5.3964
	step [18/332], loss=5.4105
	step [19/332], loss=3.7476
	step [20/332], loss=5.3630
	step [21/332], loss=5.7509
	step [22/332], loss=4.8140
	step [23/332], loss=5.2395
	step [24/332], loss=4.4226
	step [25/332], loss=4.8265
	step [26/332], loss=5.3824
	step [27/332], loss=5.5303
	step [28/332], loss=4.7607
	step [29/332], loss=4.4940
	step [30/332], loss=5.7626
	step [31/332], loss=4.5157
	step [32/332], loss=5.2306
	step [33/332], loss=4.4953
	step [34/332], loss=4.9775
	step [35/332], loss=5.7025
	step [36/332], loss=5.1380
	step [37/332], loss=5.7680
	step [38/332], loss=4.7522
	step [39/332], loss=5.2906
	step [40/332], loss=4.6981
	step [41/332], loss=5.2956
	step [42/332], loss=4.2983
	step [43/332], loss=5.1979
	step [44/332], loss=5.5575
	step [45/332], loss=5.9689
	step [46/332], loss=5.6483
	step [47/332], loss=5.2452
	step [48/332], loss=5.0880
	step [49/332], loss=4.4651
	step [50/332], loss=6.0095
	step [51/332], loss=6.1129
	step [52/332], loss=5.0878
	step [53/332], loss=6.6941
	step [54/332], loss=5.2502
	step [55/332], loss=5.3291
	step [56/332], loss=4.6320
	step [57/332], loss=5.6960
	step [58/332], loss=5.4437
	step [59/332], loss=5.0916
	step [60/332], loss=6.2114
	step [61/332], loss=5.8396
	step [62/332], loss=5.0990
	step [63/332], loss=7.0954
	step [64/332], loss=5.9942
	step [65/332], loss=4.7295
	step [66/332], loss=5.4008
	step [67/332], loss=4.0924
	step [68/332], loss=5.0686
	step [69/332], loss=5.1451
	step [70/332], loss=5.7163
	step [71/332], loss=6.7065
	step [72/332], loss=5.7510
	step [73/332], loss=4.8061
	step [74/332], loss=4.6034
	step [75/332], loss=6.6923
	step [76/332], loss=5.1106
	step [77/332], loss=5.6361
	step [78/332], loss=5.0254
	step [79/332], loss=4.4059
	step [80/332], loss=5.1721
	step [81/332], loss=3.7145
	step [82/332], loss=5.8280
	step [83/332], loss=4.8732
	step [84/332], loss=6.1918
	step [85/332], loss=4.8722
	step [86/332], loss=5.1917
	step [87/332], loss=4.8911
	step [88/332], loss=3.8578
	step [89/332], loss=5.2153
	step [90/332], loss=6.1990
	step [91/332], loss=5.9526
	step [92/332], loss=5.5593
	step [93/332], loss=5.2050
	step [94/332], loss=4.1948
	step [95/332], loss=6.1927
	step [96/332], loss=4.5577
	step [97/332], loss=5.5511
	step [98/332], loss=5.4056
	step [99/332], loss=4.7329
	step [100/332], loss=5.6919
	step [101/332], loss=4.7775
	step [102/332], loss=5.4582
	step [103/332], loss=4.6794
	step [104/332], loss=6.3307
	step [105/332], loss=5.2768
	step [106/332], loss=4.9831
	step [107/332], loss=7.4550
	step [108/332], loss=6.3263
	step [109/332], loss=5.4813
	step [110/332], loss=5.3649
	step [111/332], loss=6.1537
	step [112/332], loss=4.8623
	step [113/332], loss=5.4297
	step [114/332], loss=5.9374
	step [115/332], loss=4.6098
	step [116/332], loss=5.1210
	step [117/332], loss=5.8132
	step [118/332], loss=4.4765
	step [119/332], loss=4.9830
	step [120/332], loss=4.6367
	step [121/332], loss=5.2754
	step [122/332], loss=6.0413
	step [123/332], loss=4.6865
	step [124/332], loss=4.6999
	step [125/332], loss=5.0840
	step [126/332], loss=5.4101
	step [127/332], loss=4.1990
	step [128/332], loss=4.7212
	step [129/332], loss=4.5605
	step [130/332], loss=4.4234
	step [131/332], loss=5.9644
	step [132/332], loss=6.0134
	step [133/332], loss=6.0544
	step [134/332], loss=6.0059
	step [135/332], loss=5.9779
	step [136/332], loss=5.7588
	step [137/332], loss=5.0208
	step [138/332], loss=4.4488
	step [139/332], loss=6.2496
	step [140/332], loss=5.3662
	step [141/332], loss=5.0738
	step [142/332], loss=4.3692
	step [143/332], loss=5.6569
	step [144/332], loss=5.0160
	step [145/332], loss=5.5305
	step [146/332], loss=5.2776
	step [147/332], loss=5.1353
	step [148/332], loss=4.7324
	step [149/332], loss=4.1569
	step [150/332], loss=5.2487
	step [151/332], loss=6.2957
	step [152/332], loss=4.9741
	step [153/332], loss=4.2602
	step [154/332], loss=5.3494
	step [155/332], loss=5.5736
	step [156/332], loss=6.6180
	step [157/332], loss=5.3441
	step [158/332], loss=5.7127
	step [159/332], loss=4.6100
	step [160/332], loss=6.8055
	step [161/332], loss=4.4070
	step [162/332], loss=5.7975
	step [163/332], loss=4.7768
	step [164/332], loss=5.3479
	step [165/332], loss=6.6969
	step [166/332], loss=4.8846
	step [167/332], loss=7.6069
	step [168/332], loss=4.0945
	step [169/332], loss=6.6069
	step [170/332], loss=5.1613
	step [171/332], loss=4.5215
	step [172/332], loss=5.0499
	step [173/332], loss=3.9683
	step [174/332], loss=5.9389
	step [175/332], loss=5.3722
	step [176/332], loss=5.8753
	step [177/332], loss=5.1267
	step [178/332], loss=4.5379
	step [179/332], loss=5.7862
	step [180/332], loss=4.8727
	step [181/332], loss=6.3842
	step [182/332], loss=4.7441
	step [183/332], loss=3.8496
	step [184/332], loss=5.0436
	step [185/332], loss=6.3780
	step [186/332], loss=5.1258
	step [187/332], loss=5.0253
	step [188/332], loss=4.8147
	step [189/332], loss=4.1170
	step [190/332], loss=4.5686
	step [191/332], loss=5.3401
	step [192/332], loss=4.3999
	step [193/332], loss=5.1188
	step [194/332], loss=4.5242
	step [195/332], loss=4.7700
	step [196/332], loss=4.9504
	step [197/332], loss=3.5126
	step [198/332], loss=4.6217
	step [199/332], loss=5.3528
	step [200/332], loss=5.2365
	step [201/332], loss=4.8969
	step [202/332], loss=5.1488
	step [203/332], loss=4.7186
	step [204/332], loss=4.8064
	step [205/332], loss=4.2730
	step [206/332], loss=6.4325
	step [207/332], loss=4.6050
	step [208/332], loss=5.1089
	step [209/332], loss=5.4095
	step [210/332], loss=4.7020
	step [211/332], loss=5.8066
	step [212/332], loss=6.5699
	step [213/332], loss=4.5786
	step [214/332], loss=4.9426
	step [215/332], loss=4.5198
	step [216/332], loss=5.6613
	step [217/332], loss=5.1808
	step [218/332], loss=4.5408
	step [219/332], loss=4.6600
	step [220/332], loss=5.2644
	step [221/332], loss=5.6433
	step [222/332], loss=5.6495
	step [223/332], loss=5.7409
	step [224/332], loss=4.9291
	step [225/332], loss=4.8469
	step [226/332], loss=5.3412
	step [227/332], loss=5.6345
	step [228/332], loss=4.6432
	step [229/332], loss=4.9195
	step [230/332], loss=4.4375
	step [231/332], loss=5.2081
	step [232/332], loss=5.1700
	step [233/332], loss=5.8491
	step [234/332], loss=4.3140
	step [235/332], loss=4.6908
	step [236/332], loss=6.0421
	step [237/332], loss=5.6038
	step [238/332], loss=4.8704
	step [239/332], loss=6.2556
	step [240/332], loss=4.5851
	step [241/332], loss=5.1441
	step [242/332], loss=6.5553
	step [243/332], loss=5.9839
	step [244/332], loss=5.1215
	step [245/332], loss=4.3920
	step [246/332], loss=5.8250
	step [247/332], loss=4.2324
	step [248/332], loss=5.2831
	step [249/332], loss=6.1880
	step [250/332], loss=4.8380
	step [251/332], loss=4.0004
	step [252/332], loss=5.7163
	step [253/332], loss=6.5537
	step [254/332], loss=6.1983
	step [255/332], loss=4.6710
	step [256/332], loss=5.8581
	step [257/332], loss=5.2586
	step [258/332], loss=6.0775
	step [259/332], loss=5.1983
	step [260/332], loss=6.6717
	step [261/332], loss=4.4782
	step [262/332], loss=4.6149
	step [263/332], loss=5.4868
	step [264/332], loss=5.7931
	step [265/332], loss=5.4409
	step [266/332], loss=6.7272
	step [267/332], loss=3.9218
	step [268/332], loss=4.6119
	step [269/332], loss=4.6984
	step [270/332], loss=5.4389
	step [271/332], loss=4.5799
	step [272/332], loss=5.6141
	step [273/332], loss=5.2283
	step [274/332], loss=4.7746
	step [275/332], loss=5.4961
	step [276/332], loss=4.7461
	step [277/332], loss=4.3768
	step [278/332], loss=4.5895
	step [279/332], loss=5.7320
	step [280/332], loss=5.4017
	step [281/332], loss=4.9884
	step [282/332], loss=6.4355
	step [283/332], loss=4.6022
	step [284/332], loss=5.1840
	step [285/332], loss=5.2738
	step [286/332], loss=5.7919
	step [287/332], loss=5.8155
	step [288/332], loss=6.4812
	step [289/332], loss=5.9270
	step [290/332], loss=5.5552
	step [291/332], loss=4.6553
	step [292/332], loss=4.7683
	step [293/332], loss=5.0281
	step [294/332], loss=5.5184
	step [295/332], loss=4.9306
	step [296/332], loss=4.6528
	step [297/332], loss=4.4345
	step [298/332], loss=5.6479
	step [299/332], loss=5.6643
	step [300/332], loss=4.5035
	step [301/332], loss=5.9658
	step [302/332], loss=5.5421
	step [303/332], loss=5.8732
	step [304/332], loss=4.4772
	step [305/332], loss=5.3967
	step [306/332], loss=5.1629
	step [307/332], loss=4.6401
	step [308/332], loss=6.8598
	step [309/332], loss=6.2209
	step [310/332], loss=5.9188
	step [311/332], loss=5.1340
	step [312/332], loss=5.5638
	step [313/332], loss=4.9897
	step [314/332], loss=4.4748
	step [315/332], loss=4.3793
	step [316/332], loss=5.0106
	step [317/332], loss=4.2336
	step [318/332], loss=4.9490
	step [319/332], loss=7.1148
	step [320/332], loss=5.0020
	step [321/332], loss=5.8427
	step [322/332], loss=6.2730
	step [323/332], loss=4.9419
	step [324/332], loss=4.2638
	step [325/332], loss=4.8344
	step [326/332], loss=4.5391
	step [327/332], loss=5.7027
	step [328/332], loss=4.1225
	step [329/332], loss=4.8883
	step [330/332], loss=6.0307
	step [331/332], loss=4.6584
	step [332/332], loss=3.7673
	Evaluating
	loss=0.0225, precision=0.1580, recall=0.9963, f1=0.2727
Training epoch 20
	step [1/332], loss=5.2622
	step [2/332], loss=5.2034
	step [3/332], loss=6.4399
	step [4/332], loss=4.2207
	step [5/332], loss=5.5535
	step [6/332], loss=6.2022
	step [7/332], loss=5.4025
	step [8/332], loss=5.7576
	step [9/332], loss=4.7642
	step [10/332], loss=6.6468
	step [11/332], loss=3.7117
	step [12/332], loss=4.4604
	step [13/332], loss=5.6635
	step [14/332], loss=4.2009
	step [15/332], loss=4.8588
	step [16/332], loss=3.9834
	step [17/332], loss=5.5278
	step [18/332], loss=4.1000
	step [19/332], loss=4.8203
	step [20/332], loss=5.4614
	step [21/332], loss=4.6681
	step [22/332], loss=4.2593
	step [23/332], loss=4.5831
	step [24/332], loss=6.5960
	step [25/332], loss=5.5720
	step [26/332], loss=5.0268
	step [27/332], loss=5.2212
	step [28/332], loss=5.7718
	step [29/332], loss=4.7765
	step [30/332], loss=5.6593
	step [31/332], loss=4.8192
	step [32/332], loss=5.1332
	step [33/332], loss=5.8742
	step [34/332], loss=6.8574
	step [35/332], loss=6.1180
	step [36/332], loss=7.2684
	step [37/332], loss=6.0526
	step [38/332], loss=4.2669
	step [39/332], loss=6.0412
	step [40/332], loss=5.0001
	step [41/332], loss=4.5200
	step [42/332], loss=5.3291
	step [43/332], loss=3.9370
	step [44/332], loss=5.0637
	step [45/332], loss=5.5890
	step [46/332], loss=4.5895
	step [47/332], loss=5.7078
	step [48/332], loss=4.8610
	step [49/332], loss=4.2667
	step [50/332], loss=5.1189
	step [51/332], loss=4.2301
	step [52/332], loss=5.4922
	step [53/332], loss=4.0719
	step [54/332], loss=5.5137
	step [55/332], loss=5.5514
	step [56/332], loss=6.0466
	step [57/332], loss=5.7544
	step [58/332], loss=4.7185
	step [59/332], loss=6.4861
	step [60/332], loss=4.4675
	step [61/332], loss=4.5050
	step [62/332], loss=5.5686
	step [63/332], loss=4.8470
	step [64/332], loss=4.8846
	step [65/332], loss=5.2128
	step [66/332], loss=4.9094
	step [67/332], loss=4.4490
	step [68/332], loss=4.4921
	step [69/332], loss=4.8480
	step [70/332], loss=4.6306
	step [71/332], loss=4.5144
	step [72/332], loss=6.0458
	step [73/332], loss=5.3922
	step [74/332], loss=4.9620
	step [75/332], loss=4.2657
	step [76/332], loss=4.9551
	step [77/332], loss=5.0687
	step [78/332], loss=4.1317
	step [79/332], loss=5.0303
	step [80/332], loss=4.3771
	step [81/332], loss=5.9055
	step [82/332], loss=5.2686
	step [83/332], loss=4.5227
	step [84/332], loss=4.4548
	step [85/332], loss=4.7190
	step [86/332], loss=4.3256
	step [87/332], loss=5.2989
	step [88/332], loss=4.7404
	step [89/332], loss=6.3440
	step [90/332], loss=4.6501
	step [91/332], loss=6.0165
	step [92/332], loss=6.0704
	step [93/332], loss=3.9847
	step [94/332], loss=6.6522
	step [95/332], loss=4.7942
	step [96/332], loss=6.1713
	step [97/332], loss=4.6897
	step [98/332], loss=4.0954
	step [99/332], loss=4.3574
	step [100/332], loss=4.8539
	step [101/332], loss=4.9078
	step [102/332], loss=4.6557
	step [103/332], loss=5.2516
	step [104/332], loss=4.4754
	step [105/332], loss=5.0512
	step [106/332], loss=5.6819
	step [107/332], loss=4.3002
	step [108/332], loss=5.2522
	step [109/332], loss=6.1965
	step [110/332], loss=5.9486
	step [111/332], loss=5.2199
	step [112/332], loss=5.7734
	step [113/332], loss=4.5128
	step [114/332], loss=5.1828
	step [115/332], loss=4.5540
	step [116/332], loss=4.4394
	step [117/332], loss=5.5272
	step [118/332], loss=4.7701
	step [119/332], loss=4.1424
	step [120/332], loss=4.9104
	step [121/332], loss=4.4315
	step [122/332], loss=4.8368
	step [123/332], loss=4.4814
	step [124/332], loss=5.0878
	step [125/332], loss=4.3309
	step [126/332], loss=4.8051
	step [127/332], loss=5.0878
	step [128/332], loss=7.1419
	step [129/332], loss=4.9118
	step [130/332], loss=6.3434
	step [131/332], loss=5.9505
	step [132/332], loss=5.6162
	step [133/332], loss=4.9979
	step [134/332], loss=4.9391
	step [135/332], loss=6.0279
	step [136/332], loss=6.4312
	step [137/332], loss=4.5415
	step [138/332], loss=5.8237
	step [139/332], loss=5.2906
	step [140/332], loss=5.6256
	step [141/332], loss=5.6852
	step [142/332], loss=5.0377
	step [143/332], loss=4.5775
	step [144/332], loss=6.1766
	step [145/332], loss=4.8832
	step [146/332], loss=4.6200
	step [147/332], loss=4.9971
	step [148/332], loss=4.6636
	step [149/332], loss=3.8914
	step [150/332], loss=5.6757
	step [151/332], loss=6.1472
	step [152/332], loss=5.0345
	step [153/332], loss=5.3177
	step [154/332], loss=5.9024
	step [155/332], loss=5.8688
	step [156/332], loss=5.2063
	step [157/332], loss=4.6899
	step [158/332], loss=5.1491
	step [159/332], loss=5.7667
	step [160/332], loss=5.8654
	step [161/332], loss=4.9243
	step [162/332], loss=3.7346
	step [163/332], loss=5.9440
	step [164/332], loss=5.6648
	step [165/332], loss=4.9553
	step [166/332], loss=5.3783
	step [167/332], loss=5.2264
	step [168/332], loss=4.4260
	step [169/332], loss=4.9517
	step [170/332], loss=4.7554
	step [171/332], loss=6.1022
	step [172/332], loss=4.7553
	step [173/332], loss=5.0053
	step [174/332], loss=4.0910
	step [175/332], loss=5.9818
	step [176/332], loss=4.2795
	step [177/332], loss=5.0696
	step [178/332], loss=4.2034
	step [179/332], loss=5.0300
	step [180/332], loss=4.1412
	step [181/332], loss=3.9643
	step [182/332], loss=5.5975
	step [183/332], loss=5.6779
	step [184/332], loss=4.3609
	step [185/332], loss=5.2405
	step [186/332], loss=5.2409
	step [187/332], loss=5.5351
	step [188/332], loss=5.1945
	step [189/332], loss=5.0440
	step [190/332], loss=5.7521
	step [191/332], loss=3.3460
	step [192/332], loss=5.8304
	step [193/332], loss=6.4337
	step [194/332], loss=4.8620
	step [195/332], loss=4.6913
	step [196/332], loss=4.9769
	step [197/332], loss=5.2175
	step [198/332], loss=4.8332
	step [199/332], loss=5.1434
	step [200/332], loss=4.8625
	step [201/332], loss=4.7016
	step [202/332], loss=4.9886
	step [203/332], loss=4.8002
	step [204/332], loss=5.1315
	step [205/332], loss=5.1762
	step [206/332], loss=4.5898
	step [207/332], loss=5.0153
	step [208/332], loss=4.6044
	step [209/332], loss=5.1067
	step [210/332], loss=4.5348
	step [211/332], loss=6.7973
	step [212/332], loss=4.7656
	step [213/332], loss=4.4879
	step [214/332], loss=5.0128
	step [215/332], loss=4.0504
	step [216/332], loss=4.9806
	step [217/332], loss=5.4867
	step [218/332], loss=5.1338
	step [219/332], loss=3.9955
	step [220/332], loss=4.8254
	step [221/332], loss=4.4090
	step [222/332], loss=5.1802
	step [223/332], loss=5.8107
	step [224/332], loss=4.5066
	step [225/332], loss=6.3175
	step [226/332], loss=4.4516
	step [227/332], loss=4.1053
	step [228/332], loss=4.6400
	step [229/332], loss=6.0149
	step [230/332], loss=4.6241
	step [231/332], loss=5.4884
	step [232/332], loss=4.3451
	step [233/332], loss=5.7229
	step [234/332], loss=4.3926
	step [235/332], loss=4.0994
	step [236/332], loss=4.8347
	step [237/332], loss=5.2090
	step [238/332], loss=4.5430
	step [239/332], loss=4.0199
	step [240/332], loss=4.0240
	step [241/332], loss=5.1851
	step [242/332], loss=5.1694
	step [243/332], loss=4.9493
	step [244/332], loss=6.3564
	step [245/332], loss=5.5096
	step [246/332], loss=4.7643
	step [247/332], loss=5.8217
	step [248/332], loss=4.9690
	step [249/332], loss=6.6816
	step [250/332], loss=5.4278
	step [251/332], loss=3.8255
	step [252/332], loss=5.2543
	step [253/332], loss=6.0289
	step [254/332], loss=5.4763
	step [255/332], loss=5.2222
	step [256/332], loss=4.7980
	step [257/332], loss=5.5898
	step [258/332], loss=4.7282
	step [259/332], loss=4.2792
	step [260/332], loss=5.0986
	step [261/332], loss=6.1551
	step [262/332], loss=5.2004
	step [263/332], loss=5.8755
	step [264/332], loss=3.8675
	step [265/332], loss=4.9630
	step [266/332], loss=4.6679
	step [267/332], loss=5.2986
	step [268/332], loss=4.7565
	step [269/332], loss=4.3805
	step [270/332], loss=4.4678
	step [271/332], loss=4.7546
	step [272/332], loss=6.6501
	step [273/332], loss=4.0044
	step [274/332], loss=5.5095
	step [275/332], loss=3.8346
	step [276/332], loss=5.3348
	step [277/332], loss=4.9726
	step [278/332], loss=4.0415
	step [279/332], loss=5.5751
	step [280/332], loss=5.5172
	step [281/332], loss=5.2931
	step [282/332], loss=4.8193
	step [283/332], loss=5.9134
	step [284/332], loss=5.0230
	step [285/332], loss=5.8360
	step [286/332], loss=5.6649
	step [287/332], loss=4.7099
	step [288/332], loss=4.4348
	step [289/332], loss=5.3215
	step [290/332], loss=6.3348
	step [291/332], loss=7.4902
	step [292/332], loss=5.6395
	step [293/332], loss=5.1328
	step [294/332], loss=4.9416
	step [295/332], loss=5.1778
	step [296/332], loss=6.1013
	step [297/332], loss=5.1757
	step [298/332], loss=5.5481
	step [299/332], loss=5.9829
	step [300/332], loss=5.5117
	step [301/332], loss=6.5228
	step [302/332], loss=5.1187
	step [303/332], loss=4.0265
	step [304/332], loss=4.3478
	step [305/332], loss=4.1146
	step [306/332], loss=5.4931
	step [307/332], loss=5.1616
	step [308/332], loss=6.3043
	step [309/332], loss=6.2214
	step [310/332], loss=4.8112
	step [311/332], loss=5.3647
	step [312/332], loss=6.0817
	step [313/332], loss=4.2157
	step [314/332], loss=5.3512
	step [315/332], loss=4.0096
	step [316/332], loss=5.3551
	step [317/332], loss=5.9705
	step [318/332], loss=4.7492
	step [319/332], loss=4.7554
	step [320/332], loss=5.0678
	step [321/332], loss=5.4453
	step [322/332], loss=6.8649
	step [323/332], loss=4.7522
	step [324/332], loss=3.7076
	step [325/332], loss=6.2752
	step [326/332], loss=5.3924
	step [327/332], loss=4.7778
	step [328/332], loss=4.9314
	step [329/332], loss=4.7393
	step [330/332], loss=4.8129
	step [331/332], loss=4.4510
	step [332/332], loss=2.9584
	Evaluating
	loss=0.0177, precision=0.2018, recall=0.9954, f1=0.3355
Training epoch 21
	step [1/332], loss=4.9755
	step [2/332], loss=5.3423
	step [3/332], loss=6.6851
	step [4/332], loss=6.0567
	step [5/332], loss=5.0309
	step [6/332], loss=3.4585
	step [7/332], loss=5.3298
	step [8/332], loss=5.0834
	step [9/332], loss=4.3272
	step [10/332], loss=4.2813
	step [11/332], loss=4.9728
	step [12/332], loss=5.6805
	step [13/332], loss=4.8867
	step [14/332], loss=4.7681
	step [15/332], loss=5.0308
	step [16/332], loss=4.3123
	step [17/332], loss=3.8331
	step [18/332], loss=5.6444
	step [19/332], loss=4.1842
	step [20/332], loss=5.7725
	step [21/332], loss=3.3412
	step [22/332], loss=4.5155
	step [23/332], loss=5.0401
	step [24/332], loss=3.8666
	step [25/332], loss=5.4290
	step [26/332], loss=4.8512
	step [27/332], loss=3.3433
	step [28/332], loss=4.0603
	step [29/332], loss=3.8787
	step [30/332], loss=4.7881
	step [31/332], loss=5.7286
	step [32/332], loss=6.2770
	step [33/332], loss=5.0928
	step [34/332], loss=4.0209
	step [35/332], loss=6.4339
	step [36/332], loss=5.3136
	step [37/332], loss=5.4767
	step [38/332], loss=4.6285
	step [39/332], loss=4.8558
	step [40/332], loss=4.4539
	step [41/332], loss=6.0039
	step [42/332], loss=5.0639
	step [43/332], loss=3.9524
	step [44/332], loss=4.6793
	step [45/332], loss=5.3115
	step [46/332], loss=5.2028
	step [47/332], loss=5.7695
	step [48/332], loss=4.7428
	step [49/332], loss=4.1187
	step [50/332], loss=4.0650
	step [51/332], loss=4.4479
	step [52/332], loss=4.6408
	step [53/332], loss=4.7034
	step [54/332], loss=5.0289
	step [55/332], loss=4.7611
	step [56/332], loss=5.3014
	step [57/332], loss=5.6306
	step [58/332], loss=4.4678
	step [59/332], loss=5.2667
	step [60/332], loss=4.7447
	step [61/332], loss=5.3947
	step [62/332], loss=4.7188
	step [63/332], loss=3.9509
	step [64/332], loss=5.3384
	step [65/332], loss=4.1631
	step [66/332], loss=5.0742
	step [67/332], loss=5.6343
	step [68/332], loss=5.2938
	step [69/332], loss=3.9145
	step [70/332], loss=4.7967
	step [71/332], loss=4.7138
	step [72/332], loss=7.6866
	step [73/332], loss=3.6778
	step [74/332], loss=5.2426
	step [75/332], loss=4.1631
	step [76/332], loss=5.6483
	step [77/332], loss=5.2902
	step [78/332], loss=5.3025
	step [79/332], loss=5.0032
	step [80/332], loss=5.9060
	step [81/332], loss=5.8507
	step [82/332], loss=4.2240
	step [83/332], loss=4.5630
	step [84/332], loss=4.2588
	step [85/332], loss=6.3777
	step [86/332], loss=3.6986
	step [87/332], loss=5.5302
	step [88/332], loss=4.7240
	step [89/332], loss=6.4865
	step [90/332], loss=5.1978
	step [91/332], loss=5.4683
	step [92/332], loss=4.7699
	step [93/332], loss=5.1901
	step [94/332], loss=4.3777
	step [95/332], loss=5.1806
	step [96/332], loss=5.1678
	step [97/332], loss=4.3968
	step [98/332], loss=5.0234
	step [99/332], loss=4.8242
	step [100/332], loss=6.1297
	step [101/332], loss=6.0945
	step [102/332], loss=4.9280
	step [103/332], loss=4.3859
	step [104/332], loss=4.6348
	step [105/332], loss=5.5953
	step [106/332], loss=5.3958
	step [107/332], loss=4.6100
	step [108/332], loss=5.1902
	step [109/332], loss=6.8712
	step [110/332], loss=4.5294
	step [111/332], loss=5.1018
	step [112/332], loss=5.0744
	step [113/332], loss=4.6377
	step [114/332], loss=3.9185
	step [115/332], loss=5.5413
	step [116/332], loss=5.5214
	step [117/332], loss=5.1775
	step [118/332], loss=5.5540
	step [119/332], loss=5.7777
	step [120/332], loss=6.3281
	step [121/332], loss=3.8900
	step [122/332], loss=4.9531
	step [123/332], loss=4.3982
	step [124/332], loss=5.4928
	step [125/332], loss=4.8749
	step [126/332], loss=5.3594
	step [127/332], loss=5.4687
	step [128/332], loss=4.4968
	step [129/332], loss=4.3854
	step [130/332], loss=4.3402
	step [131/332], loss=5.4932
	step [132/332], loss=6.0590
	step [133/332], loss=3.6091
	step [134/332], loss=4.9519
	step [135/332], loss=4.5365
	step [136/332], loss=5.2035
	step [137/332], loss=5.0642
	step [138/332], loss=3.9967
	step [139/332], loss=4.8062
	step [140/332], loss=5.3873
	step [141/332], loss=5.6537
	step [142/332], loss=4.3690
	step [143/332], loss=5.4945
	step [144/332], loss=5.2408
	step [145/332], loss=4.7546
	step [146/332], loss=5.2990
	step [147/332], loss=3.6316
	step [148/332], loss=4.2582
	step [149/332], loss=4.8575
	step [150/332], loss=4.5293
	step [151/332], loss=6.2157
	step [152/332], loss=4.5914
	step [153/332], loss=4.8680
	step [154/332], loss=7.3932
	step [155/332], loss=4.2341
	step [156/332], loss=4.2951
	step [157/332], loss=5.2767
	step [158/332], loss=4.2096
	step [159/332], loss=4.8835
	step [160/332], loss=4.2081
	step [161/332], loss=3.8631
	step [162/332], loss=4.5641
	step [163/332], loss=5.2625
	step [164/332], loss=4.4583
	step [165/332], loss=4.1877
	step [166/332], loss=5.7182
	step [167/332], loss=5.4267
	step [168/332], loss=3.2723
	step [169/332], loss=5.1034
	step [170/332], loss=5.0019
	step [171/332], loss=5.5113
	step [172/332], loss=4.6317
	step [173/332], loss=4.0551
	step [174/332], loss=5.7718
	step [175/332], loss=5.6234
	step [176/332], loss=4.3500
	step [177/332], loss=4.5366
	step [178/332], loss=4.5213
	step [179/332], loss=5.1126
	step [180/332], loss=4.8823
	step [181/332], loss=4.9093
	step [182/332], loss=6.1168
	step [183/332], loss=4.9037
	step [184/332], loss=4.6757
	step [185/332], loss=4.9831
	step [186/332], loss=3.9683
	step [187/332], loss=5.2743
	step [188/332], loss=5.2456
	step [189/332], loss=4.3239
	step [190/332], loss=5.2975
	step [191/332], loss=4.8992
	step [192/332], loss=4.2341
	step [193/332], loss=5.0893
	step [194/332], loss=4.6700
	step [195/332], loss=5.3373
	step [196/332], loss=4.2623
	step [197/332], loss=5.9786
	step [198/332], loss=4.7538
	step [199/332], loss=4.5556
	step [200/332], loss=4.4487
	step [201/332], loss=6.1183
	step [202/332], loss=5.1212
	step [203/332], loss=5.1059
	step [204/332], loss=5.3948
	step [205/332], loss=4.4594
	step [206/332], loss=4.3563
	step [207/332], loss=4.8849
	step [208/332], loss=4.3813
	step [209/332], loss=4.9094
	step [210/332], loss=5.0799
	step [211/332], loss=4.8665
	step [212/332], loss=5.0580
	step [213/332], loss=5.5422
	step [214/332], loss=5.4817
	step [215/332], loss=4.8758
	step [216/332], loss=4.0675
	step [217/332], loss=4.1037
	step [218/332], loss=3.8902
	step [219/332], loss=5.2094
	step [220/332], loss=5.3102
	step [221/332], loss=4.5548
	step [222/332], loss=5.5348
	step [223/332], loss=5.4941
	step [224/332], loss=5.4903
	step [225/332], loss=5.2905
	step [226/332], loss=5.0642
	step [227/332], loss=6.7784
	step [228/332], loss=5.0909
	step [229/332], loss=4.5801
	step [230/332], loss=4.3014
	step [231/332], loss=5.8792
	step [232/332], loss=4.9549
	step [233/332], loss=6.3250
	step [234/332], loss=4.5594
	step [235/332], loss=5.1007
	step [236/332], loss=4.1919
	step [237/332], loss=5.5446
	step [238/332], loss=4.2846
	step [239/332], loss=3.7575
	step [240/332], loss=4.9790
	step [241/332], loss=5.9731
	step [242/332], loss=5.3411
	step [243/332], loss=4.9296
	step [244/332], loss=5.6620
	step [245/332], loss=4.5858
	step [246/332], loss=5.3759
	step [247/332], loss=4.9303
	step [248/332], loss=4.9437
	step [249/332], loss=6.3778
	step [250/332], loss=5.0376
	step [251/332], loss=5.3653
	step [252/332], loss=5.6992
	step [253/332], loss=4.6891
	step [254/332], loss=5.5199
	step [255/332], loss=5.4702
	step [256/332], loss=3.9950
	step [257/332], loss=4.8384
	step [258/332], loss=4.2039
	step [259/332], loss=5.9166
	step [260/332], loss=5.5295
	step [261/332], loss=4.6584
	step [262/332], loss=4.8990
	step [263/332], loss=3.6882
	step [264/332], loss=5.3026
	step [265/332], loss=5.8184
	step [266/332], loss=5.0969
	step [267/332], loss=6.0769
	step [268/332], loss=5.0114
	step [269/332], loss=5.3727
	step [270/332], loss=4.7357
	step [271/332], loss=4.9520
	step [272/332], loss=6.9688
	step [273/332], loss=5.5697
	step [274/332], loss=4.4522
	step [275/332], loss=4.8698
	step [276/332], loss=4.0524
	step [277/332], loss=5.9167
	step [278/332], loss=5.1241
	step [279/332], loss=4.1067
	step [280/332], loss=4.1372
	step [281/332], loss=4.5650
	step [282/332], loss=5.0636
	step [283/332], loss=4.9655
	step [284/332], loss=4.9259
	step [285/332], loss=5.7341
	step [286/332], loss=5.5822
	step [287/332], loss=4.2199
	step [288/332], loss=5.0770
	step [289/332], loss=4.9695
	step [290/332], loss=4.7980
	step [291/332], loss=4.3917
	step [292/332], loss=4.6702
	step [293/332], loss=4.2963
	step [294/332], loss=5.0645
	step [295/332], loss=4.3762
	step [296/332], loss=4.2704
	step [297/332], loss=4.1648
	step [298/332], loss=4.5375
	step [299/332], loss=5.1254
	step [300/332], loss=6.0176
	step [301/332], loss=4.5405
	step [302/332], loss=4.6436
	step [303/332], loss=5.9899
	step [304/332], loss=4.4465
	step [305/332], loss=5.2585
	step [306/332], loss=4.8793
	step [307/332], loss=4.8918
	step [308/332], loss=5.1435
	step [309/332], loss=5.5821
	step [310/332], loss=6.2905
	step [311/332], loss=4.1956
	step [312/332], loss=4.0705
	step [313/332], loss=3.8520
	step [314/332], loss=4.9740
	step [315/332], loss=6.3018
	step [316/332], loss=3.6113
	step [317/332], loss=3.7552
	step [318/332], loss=4.2927
	step [319/332], loss=5.1377
	step [320/332], loss=4.0166
	step [321/332], loss=5.3578
	step [322/332], loss=4.6700
	step [323/332], loss=4.8207
	step [324/332], loss=5.3512
	step [325/332], loss=4.6823
	step [326/332], loss=6.6631
	step [327/332], loss=3.9259
	step [328/332], loss=5.5635
	step [329/332], loss=6.0650
	step [330/332], loss=4.4401
	step [331/332], loss=5.3817
	step [332/332], loss=2.5815
	Evaluating
	loss=0.0215, precision=0.1579, recall=0.9959, f1=0.2726
Training epoch 22
	step [1/332], loss=6.5449
	step [2/332], loss=4.6333
	step [3/332], loss=4.8912
	step [4/332], loss=4.3989
	step [5/332], loss=4.1657
	step [6/332], loss=4.9609
	step [7/332], loss=5.2796
	step [8/332], loss=4.0852
	step [9/332], loss=5.4055
	step [10/332], loss=4.5795
	step [11/332], loss=4.8036
	step [12/332], loss=5.6350
	step [13/332], loss=4.6955
	step [14/332], loss=5.5359
	step [15/332], loss=5.1293
	step [16/332], loss=4.5266
	step [17/332], loss=4.0203
	step [18/332], loss=5.5198
	step [19/332], loss=6.1199
	step [20/332], loss=4.4198
	step [21/332], loss=5.9621
	step [22/332], loss=5.5633
	step [23/332], loss=4.1715
	step [24/332], loss=5.1308
	step [25/332], loss=4.3403
	step [26/332], loss=4.9807
	step [27/332], loss=4.9086
	step [28/332], loss=4.6980
	step [29/332], loss=5.1469
	step [30/332], loss=3.7491
	step [31/332], loss=4.1153
	step [32/332], loss=4.4730
	step [33/332], loss=5.6366
	step [34/332], loss=6.5414
	step [35/332], loss=4.2059
	step [36/332], loss=4.6209
	step [37/332], loss=5.0540
	step [38/332], loss=4.9932
	step [39/332], loss=4.6387
	step [40/332], loss=4.5252
	step [41/332], loss=4.9683
	step [42/332], loss=3.9232
	step [43/332], loss=4.1936
	step [44/332], loss=4.1608
	step [45/332], loss=4.9333
	step [46/332], loss=5.3807
	step [47/332], loss=4.6705
	step [48/332], loss=5.1355
	step [49/332], loss=5.2175
	step [50/332], loss=4.4666
	step [51/332], loss=4.5075
	step [52/332], loss=4.1099
	step [53/332], loss=4.4659
	step [54/332], loss=3.8974
	step [55/332], loss=4.6582
	step [56/332], loss=5.0972
	step [57/332], loss=4.5603
	step [58/332], loss=4.5990
	step [59/332], loss=5.1130
	step [60/332], loss=4.3820
	step [61/332], loss=5.8364
	step [62/332], loss=4.5514
	step [63/332], loss=4.2633
	step [64/332], loss=5.2286
	step [65/332], loss=5.9892
	step [66/332], loss=6.4977
	step [67/332], loss=5.2107
	step [68/332], loss=5.2050
	step [69/332], loss=4.9080
	step [70/332], loss=4.2972
	step [71/332], loss=4.4069
	step [72/332], loss=4.3517
	step [73/332], loss=5.7078
	step [74/332], loss=3.6670
	step [75/332], loss=4.3365
	step [76/332], loss=5.6730
	step [77/332], loss=5.0020
	step [78/332], loss=5.3149
	step [79/332], loss=5.4783
	step [80/332], loss=4.3183
	step [81/332], loss=4.5017
	step [82/332], loss=4.0119
	step [83/332], loss=4.4445
	step [84/332], loss=3.9112
	step [85/332], loss=4.5111
	step [86/332], loss=3.9224
	step [87/332], loss=6.1430
	step [88/332], loss=5.0246
	step [89/332], loss=5.7162
	step [90/332], loss=4.8438
	step [91/332], loss=3.8945
	step [92/332], loss=5.2080
	step [93/332], loss=4.7808
	step [94/332], loss=5.5018
	step [95/332], loss=5.6562
	step [96/332], loss=4.2121
	step [97/332], loss=4.7342
	step [98/332], loss=4.3091
	step [99/332], loss=4.9774
	step [100/332], loss=3.5438
	step [101/332], loss=5.2590
	step [102/332], loss=6.5621
	step [103/332], loss=4.1801
	step [104/332], loss=4.8435
	step [105/332], loss=3.4739
	step [106/332], loss=4.6577
	step [107/332], loss=4.8749
	step [108/332], loss=5.4225
	step [109/332], loss=6.0484
	step [110/332], loss=4.4942
	step [111/332], loss=5.9115
	step [112/332], loss=5.1523
	step [113/332], loss=4.0099
	step [114/332], loss=4.9923
	step [115/332], loss=4.8658
	step [116/332], loss=4.7695
	step [117/332], loss=4.4826
	step [118/332], loss=5.4037
	step [119/332], loss=4.6475
	step [120/332], loss=4.3103
	step [121/332], loss=4.7797
	step [122/332], loss=3.7233
	step [123/332], loss=4.3993
	step [124/332], loss=4.9550
	step [125/332], loss=4.9697
	step [126/332], loss=4.5369
	step [127/332], loss=4.1359
	step [128/332], loss=5.7269
	step [129/332], loss=4.6292
	step [130/332], loss=4.0012
	step [131/332], loss=3.6186
	step [132/332], loss=4.9484
	step [133/332], loss=5.0233
	step [134/332], loss=5.2452
	step [135/332], loss=6.7374
	step [136/332], loss=4.7089
	step [137/332], loss=3.9101
	step [138/332], loss=5.5734
	step [139/332], loss=5.0754
	step [140/332], loss=4.2782
	step [141/332], loss=7.0621
	step [142/332], loss=5.6176
	step [143/332], loss=4.5516
	step [144/332], loss=4.9585
	step [145/332], loss=5.8929
	step [146/332], loss=4.8993
	step [147/332], loss=4.6097
	step [148/332], loss=4.5961
	step [149/332], loss=4.3055
	step [150/332], loss=5.5443
	step [151/332], loss=5.3858
	step [152/332], loss=5.1871
	step [153/332], loss=4.5628
	step [154/332], loss=5.6141
	step [155/332], loss=4.7102
	step [156/332], loss=4.2254
	step [157/332], loss=4.6644
	step [158/332], loss=4.3527
	step [159/332], loss=3.8668
	step [160/332], loss=3.9914
	step [161/332], loss=4.7315
	step [162/332], loss=6.0665
	step [163/332], loss=5.6957
	step [164/332], loss=5.7918
	step [165/332], loss=4.4525
	step [166/332], loss=4.6177
	step [167/332], loss=5.5713
	step [168/332], loss=4.9054
	step [169/332], loss=5.0857
	step [170/332], loss=4.2199
	step [171/332], loss=5.6468
	step [172/332], loss=4.3068
	step [173/332], loss=3.4622
	step [174/332], loss=6.1448
	step [175/332], loss=6.2554
	step [176/332], loss=4.7479
	step [177/332], loss=5.5669
	step [178/332], loss=4.5960
	step [179/332], loss=4.8949
	step [180/332], loss=5.6467
	step [181/332], loss=4.6608
	step [182/332], loss=4.9440
	step [183/332], loss=5.1154
	step [184/332], loss=5.6205
	step [185/332], loss=5.3893
	step [186/332], loss=5.0220
	step [187/332], loss=5.3979
	step [188/332], loss=3.3451
	step [189/332], loss=5.1141
	step [190/332], loss=6.1848
	step [191/332], loss=3.8480
	step [192/332], loss=3.7830
	step [193/332], loss=5.4223
	step [194/332], loss=5.2037
	step [195/332], loss=5.2806
	step [196/332], loss=4.3339
	step [197/332], loss=5.0027
	step [198/332], loss=4.9736
	step [199/332], loss=4.5724
	step [200/332], loss=4.0111
	step [201/332], loss=5.3081
	step [202/332], loss=4.3289
	step [203/332], loss=3.8562
	step [204/332], loss=5.0469
	step [205/332], loss=4.8629
	step [206/332], loss=4.1884
	step [207/332], loss=6.0977
	step [208/332], loss=5.0134
	step [209/332], loss=4.9164
	step [210/332], loss=5.5474
	step [211/332], loss=4.1156
	step [212/332], loss=5.7032
	step [213/332], loss=3.6159
	step [214/332], loss=6.2055
	step [215/332], loss=4.8284
	step [216/332], loss=4.6298
	step [217/332], loss=6.1572
	step [218/332], loss=5.0270
	step [219/332], loss=4.8019
	step [220/332], loss=4.4134
	step [221/332], loss=6.0688
	step [222/332], loss=4.7466
	step [223/332], loss=4.3555
	step [224/332], loss=5.2025
	step [225/332], loss=6.0853
	step [226/332], loss=4.6307
	step [227/332], loss=4.0931
	step [228/332], loss=5.0758
	step [229/332], loss=4.6616
	step [230/332], loss=5.9628
	step [231/332], loss=5.7855
	step [232/332], loss=4.5408
	step [233/332], loss=5.0397
	step [234/332], loss=4.1049
	step [235/332], loss=4.2554
	step [236/332], loss=4.0722
	step [237/332], loss=5.0619
	step [238/332], loss=5.0305
	step [239/332], loss=3.4701
	step [240/332], loss=5.2775
	step [241/332], loss=4.9297
	step [242/332], loss=5.5718
	step [243/332], loss=3.6606
	step [244/332], loss=4.6432
	step [245/332], loss=3.9963
	step [246/332], loss=5.6485
	step [247/332], loss=4.5079
	step [248/332], loss=5.7487
	step [249/332], loss=5.9976
	step [250/332], loss=4.6308
	step [251/332], loss=3.9602
	step [252/332], loss=3.6202
	step [253/332], loss=4.9335
	step [254/332], loss=4.4570
	step [255/332], loss=3.8455
	step [256/332], loss=4.5432
	step [257/332], loss=4.1604
	step [258/332], loss=4.4202
	step [259/332], loss=4.6014
	step [260/332], loss=5.5515
	step [261/332], loss=4.6660
	step [262/332], loss=4.8761
	step [263/332], loss=5.8656
	step [264/332], loss=4.8218
	step [265/332], loss=4.6236
	step [266/332], loss=4.4695
	step [267/332], loss=5.4912
	step [268/332], loss=4.1655
	step [269/332], loss=5.0482
	step [270/332], loss=4.1605
	step [271/332], loss=5.3194
	step [272/332], loss=5.1763
	step [273/332], loss=5.8562
	step [274/332], loss=5.9023
	step [275/332], loss=3.6585
	step [276/332], loss=3.3234
	step [277/332], loss=4.9917
	step [278/332], loss=5.2562
	step [279/332], loss=4.3836
	step [280/332], loss=3.8201
	step [281/332], loss=5.1028
	step [282/332], loss=5.2121
	step [283/332], loss=5.5925
	step [284/332], loss=4.0903
	step [285/332], loss=4.5924
	step [286/332], loss=3.8601
	step [287/332], loss=5.7257
	step [288/332], loss=5.8932
	step [289/332], loss=4.6139
	step [290/332], loss=5.5344
	step [291/332], loss=4.1946
	step [292/332], loss=5.3756
	step [293/332], loss=5.8823
	step [294/332], loss=5.2991
	step [295/332], loss=4.3799
	step [296/332], loss=4.7321
	step [297/332], loss=4.9016
	step [298/332], loss=5.0143
	step [299/332], loss=5.0050
	step [300/332], loss=5.6168
	step [301/332], loss=5.2747
	step [302/332], loss=5.8720
	step [303/332], loss=3.5214
	step [304/332], loss=4.3287
	step [305/332], loss=3.5368
	step [306/332], loss=3.6477
	step [307/332], loss=3.3039
	step [308/332], loss=5.0571
	step [309/332], loss=4.2523
	step [310/332], loss=6.0545
	step [311/332], loss=4.8309
	step [312/332], loss=5.1608
	step [313/332], loss=3.0646
	step [314/332], loss=5.4011
	step [315/332], loss=4.4913
	step [316/332], loss=4.6053
	step [317/332], loss=4.8795
	step [318/332], loss=4.1171
	step [319/332], loss=5.8657
	step [320/332], loss=4.1910
	step [321/332], loss=6.4241
	step [322/332], loss=4.9260
	step [323/332], loss=5.1082
	step [324/332], loss=4.2135
	step [325/332], loss=5.1567
	step [326/332], loss=4.4145
	step [327/332], loss=5.3966
	step [328/332], loss=4.7312
	step [329/332], loss=5.4904
	step [330/332], loss=4.7784
	step [331/332], loss=6.4296
	step [332/332], loss=1.9609
	Evaluating
	loss=0.0164, precision=0.1919, recall=0.9952, f1=0.3217
Training epoch 23
	step [1/332], loss=4.9268
	step [2/332], loss=4.8837
	step [3/332], loss=3.3653
	step [4/332], loss=4.2074
	step [5/332], loss=5.3654
	step [6/332], loss=4.8111
	step [7/332], loss=4.4389
	step [8/332], loss=4.2633
	step [9/332], loss=4.5766
	step [10/332], loss=5.0160
	step [11/332], loss=3.6077
	step [12/332], loss=4.6971
	step [13/332], loss=4.5991
	step [14/332], loss=4.2368
	step [15/332], loss=5.3458
	step [16/332], loss=5.7745
	step [17/332], loss=5.1890
	step [18/332], loss=5.3455
	step [19/332], loss=3.3079
	step [20/332], loss=5.2117
	step [21/332], loss=4.0938
	step [22/332], loss=3.8671
	step [23/332], loss=5.4908
	step [24/332], loss=5.5942
	step [25/332], loss=5.3438
	step [26/332], loss=4.7765
	step [27/332], loss=4.5494
	step [28/332], loss=4.3178
	step [29/332], loss=4.8456
	step [30/332], loss=4.9700
	step [31/332], loss=5.1882
	step [32/332], loss=4.3769
	step [33/332], loss=3.8370
	step [34/332], loss=5.3387
	step [35/332], loss=5.2090
	step [36/332], loss=5.0207
	step [37/332], loss=5.2329
	step [38/332], loss=4.5938
	step [39/332], loss=4.3797
	step [40/332], loss=3.9540
	step [41/332], loss=5.1432
	step [42/332], loss=4.8550
	step [43/332], loss=4.8878
	step [44/332], loss=5.3566
	step [45/332], loss=4.1271
	step [46/332], loss=4.3778
	step [47/332], loss=3.8568
	step [48/332], loss=5.0512
	step [49/332], loss=4.3865
	step [50/332], loss=4.9590
	step [51/332], loss=5.3704
	step [52/332], loss=4.0722
	step [53/332], loss=4.7987
	step [54/332], loss=3.8912
	step [55/332], loss=4.8315
	step [56/332], loss=4.8590
	step [57/332], loss=4.0404
	step [58/332], loss=5.0454
	step [59/332], loss=4.7738
	step [60/332], loss=4.4607
	step [61/332], loss=4.0779
	step [62/332], loss=3.7540
	step [63/332], loss=4.4047
	step [64/332], loss=2.8830
	step [65/332], loss=5.1755
	step [66/332], loss=3.6687
	step [67/332], loss=6.6106
	step [68/332], loss=5.2612
	step [69/332], loss=6.0149
	step [70/332], loss=4.6891
	step [71/332], loss=4.4354
	step [72/332], loss=4.4397
	step [73/332], loss=5.7927
	step [74/332], loss=5.7175
	step [75/332], loss=4.8209
	step [76/332], loss=6.1118
	step [77/332], loss=4.6227
	step [78/332], loss=6.1253
	step [79/332], loss=4.9953
	step [80/332], loss=5.3790
	step [81/332], loss=5.1087
	step [82/332], loss=4.1768
	step [83/332], loss=4.7866
	step [84/332], loss=5.6397
	step [85/332], loss=4.8129
	step [86/332], loss=5.2932
	step [87/332], loss=4.6881
	step [88/332], loss=4.2287
	step [89/332], loss=4.9517
	step [90/332], loss=4.3869
	step [91/332], loss=4.0299
	step [92/332], loss=5.2203
	step [93/332], loss=4.1448
	step [94/332], loss=6.1672
	step [95/332], loss=5.4986
	step [96/332], loss=4.0836
	step [97/332], loss=4.8564
	step [98/332], loss=4.0164
	step [99/332], loss=5.1800
	step [100/332], loss=3.9636
	step [101/332], loss=5.6044
	step [102/332], loss=4.7117
	step [103/332], loss=4.5008
	step [104/332], loss=4.7081
	step [105/332], loss=4.7921
	step [106/332], loss=4.6208
	step [107/332], loss=4.1556
	step [108/332], loss=5.1781
	step [109/332], loss=4.4079
	step [110/332], loss=4.7312
	step [111/332], loss=5.4686
	step [112/332], loss=6.0576
	step [113/332], loss=4.7007
	step [114/332], loss=4.0984
	step [115/332], loss=4.7479
	step [116/332], loss=4.1599
	step [117/332], loss=5.3901
	step [118/332], loss=4.2775
	step [119/332], loss=4.3129
	step [120/332], loss=4.6126
	step [121/332], loss=4.1519
	step [122/332], loss=5.0897
	step [123/332], loss=5.4626
	step [124/332], loss=4.8203
	step [125/332], loss=4.5909
	step [126/332], loss=6.2379
	step [127/332], loss=5.5419
	step [128/332], loss=3.9729
	step [129/332], loss=4.7613
	step [130/332], loss=4.5207
	step [131/332], loss=5.2784
	step [132/332], loss=4.9126
	step [133/332], loss=4.9513
	step [134/332], loss=5.0784
	step [135/332], loss=3.5387
	step [136/332], loss=5.1854
	step [137/332], loss=4.4444
	step [138/332], loss=5.6978
	step [139/332], loss=5.4383
	step [140/332], loss=4.6867
	step [141/332], loss=5.5453
	step [142/332], loss=4.3177
	step [143/332], loss=4.4209
	step [144/332], loss=3.8816
	step [145/332], loss=5.6268
	step [146/332], loss=4.5262
	step [147/332], loss=3.5684
	step [148/332], loss=5.4512
	step [149/332], loss=4.2604
	step [150/332], loss=3.1853
	step [151/332], loss=4.8193
	step [152/332], loss=5.0722
	step [153/332], loss=5.4307
	step [154/332], loss=4.1844
	step [155/332], loss=4.4602
	step [156/332], loss=4.6854
	step [157/332], loss=4.0918
	step [158/332], loss=4.5913
	step [159/332], loss=4.3777
	step [160/332], loss=5.2556
	step [161/332], loss=4.5319
	step [162/332], loss=3.5001
	step [163/332], loss=3.7446
	step [164/332], loss=4.5385
	step [165/332], loss=3.6664
	step [166/332], loss=4.7807
	step [167/332], loss=4.0118
	step [168/332], loss=4.5958
	step [169/332], loss=3.4016
	step [170/332], loss=4.0844
	step [171/332], loss=3.9995
	step [172/332], loss=4.7610
	step [173/332], loss=4.9329
	step [174/332], loss=4.2351
	step [175/332], loss=5.9537
	step [176/332], loss=5.3692
	step [177/332], loss=5.4392
	step [178/332], loss=5.2805
	step [179/332], loss=5.0528
	step [180/332], loss=4.7368
	step [181/332], loss=3.4912
	step [182/332], loss=4.6074
	step [183/332], loss=4.7106
	step [184/332], loss=4.5181
	step [185/332], loss=4.5871
	step [186/332], loss=4.4462
	step [187/332], loss=7.4000
	step [188/332], loss=3.3360
	step [189/332], loss=3.6065
	step [190/332], loss=5.4474
	step [191/332], loss=5.0225
	step [192/332], loss=4.4521
	step [193/332], loss=5.4784
	step [194/332], loss=4.2074
	step [195/332], loss=5.6976
	step [196/332], loss=4.3683
	step [197/332], loss=3.9335
	step [198/332], loss=4.9184
	step [199/332], loss=4.9411
	step [200/332], loss=4.7075
	step [201/332], loss=4.5546
	step [202/332], loss=4.9595
	step [203/332], loss=4.6686
	step [204/332], loss=5.6040
	step [205/332], loss=3.9566
	step [206/332], loss=5.2854
	step [207/332], loss=3.6070
	step [208/332], loss=3.1828
	step [209/332], loss=4.1467
	step [210/332], loss=4.8014
	step [211/332], loss=4.2354
	step [212/332], loss=3.5908
	step [213/332], loss=5.3556
	step [214/332], loss=5.5609
	step [215/332], loss=4.0978
	step [216/332], loss=3.7000
	step [217/332], loss=4.1640
	step [218/332], loss=4.7807
	step [219/332], loss=5.0351
	step [220/332], loss=4.9418
	step [221/332], loss=5.3056
	step [222/332], loss=4.6281
	step [223/332], loss=4.8604
	step [224/332], loss=4.5427
	step [225/332], loss=4.3320
	step [226/332], loss=5.0500
	step [227/332], loss=4.1805
	step [228/332], loss=4.5578
	step [229/332], loss=3.3727
	step [230/332], loss=4.5931
	step [231/332], loss=3.8959
	step [232/332], loss=5.5426
	step [233/332], loss=4.6462
	step [234/332], loss=4.9321
	step [235/332], loss=5.2675
	step [236/332], loss=3.7678
	step [237/332], loss=4.3005
	step [238/332], loss=4.4932
	step [239/332], loss=5.3154
	step [240/332], loss=4.8958
	step [241/332], loss=4.5113
	step [242/332], loss=4.7323
	step [243/332], loss=3.6514
	step [244/332], loss=4.2378
	step [245/332], loss=3.8732
	step [246/332], loss=4.7160
	step [247/332], loss=5.2406
	step [248/332], loss=4.2490
	step [249/332], loss=3.6539
	step [250/332], loss=3.4521
	step [251/332], loss=4.4670
	step [252/332], loss=3.3356
	step [253/332], loss=5.0565
	step [254/332], loss=3.9913
	step [255/332], loss=5.5150
	step [256/332], loss=4.1041
	step [257/332], loss=5.3729
	step [258/332], loss=5.4175
	step [259/332], loss=3.8676
	step [260/332], loss=4.4304
	step [261/332], loss=4.7176
	step [262/332], loss=5.2251
	step [263/332], loss=5.1166
	step [264/332], loss=4.2870
	step [265/332], loss=4.3714
	step [266/332], loss=5.0428
	step [267/332], loss=4.8989
	step [268/332], loss=4.6379
	step [269/332], loss=3.6770
	step [270/332], loss=4.6247
	step [271/332], loss=5.4354
	step [272/332], loss=5.2278
	step [273/332], loss=4.1202
	step [274/332], loss=4.7770
	step [275/332], loss=4.3290
	step [276/332], loss=4.2133
	step [277/332], loss=4.5893
	step [278/332], loss=4.3216
	step [279/332], loss=4.7953
	step [280/332], loss=5.1652
	step [281/332], loss=4.7182
	step [282/332], loss=3.9477
	step [283/332], loss=4.3198
	step [284/332], loss=4.4670
	step [285/332], loss=4.7483
	step [286/332], loss=4.3211
	step [287/332], loss=4.1890
	step [288/332], loss=4.8655
	step [289/332], loss=5.3235
	step [290/332], loss=4.6509
	step [291/332], loss=4.3558
	step [292/332], loss=5.4791
	step [293/332], loss=4.5731
	step [294/332], loss=4.1301
	step [295/332], loss=4.1682
	step [296/332], loss=3.8942
	step [297/332], loss=3.9558
	step [298/332], loss=4.0261
	step [299/332], loss=4.1902
	step [300/332], loss=4.7563
	step [301/332], loss=4.9962
	step [302/332], loss=4.4613
	step [303/332], loss=5.0774
	step [304/332], loss=6.0856
	step [305/332], loss=5.7295
	step [306/332], loss=5.5841
	step [307/332], loss=4.3342
	step [308/332], loss=5.4517
	step [309/332], loss=4.8011
	step [310/332], loss=4.3051
	step [311/332], loss=5.4844
	step [312/332], loss=4.4808
	step [313/332], loss=4.1317
	step [314/332], loss=3.9402
	step [315/332], loss=4.2450
	step [316/332], loss=4.2412
	step [317/332], loss=3.9007
	step [318/332], loss=4.4102
	step [319/332], loss=3.7524
	step [320/332], loss=4.7868
	step [321/332], loss=5.0601
	step [322/332], loss=4.8840
	step [323/332], loss=4.2636
	step [324/332], loss=4.9963
	step [325/332], loss=3.8989
	step [326/332], loss=4.6664
	step [327/332], loss=4.7415
	step [328/332], loss=4.4993
	step [329/332], loss=5.2030
	step [330/332], loss=4.9574
	step [331/332], loss=4.0405
	step [332/332], loss=2.5118
	Evaluating
	loss=0.0155, precision=0.2040, recall=0.9948, f1=0.3385
Training epoch 24
	step [1/332], loss=5.7803
	step [2/332], loss=4.0867
	step [3/332], loss=4.1720
	step [4/332], loss=5.2765
	step [5/332], loss=4.2288
	step [6/332], loss=4.5845
	step [7/332], loss=4.5256
	step [8/332], loss=4.5617
	step [9/332], loss=4.4943
	step [10/332], loss=4.0112
	step [11/332], loss=4.0896
	step [12/332], loss=5.2242
	step [13/332], loss=6.2437
	step [14/332], loss=4.1388
	step [15/332], loss=3.5118
	step [16/332], loss=3.9082
	step [17/332], loss=4.8614
	step [18/332], loss=4.7922
	step [19/332], loss=5.0417
	step [20/332], loss=3.8717
	step [21/332], loss=4.3157
	step [22/332], loss=4.1813
	step [23/332], loss=4.5426
	step [24/332], loss=4.5252
	step [25/332], loss=4.9688
	step [26/332], loss=3.5830
	step [27/332], loss=4.3458
	step [28/332], loss=4.8030
	step [29/332], loss=4.4526
	step [30/332], loss=4.5302
	step [31/332], loss=4.2780
	step [32/332], loss=3.5539
	step [33/332], loss=3.5712
	step [34/332], loss=5.4436
	step [35/332], loss=4.3514
	step [36/332], loss=4.0964
	step [37/332], loss=5.4723
	step [38/332], loss=4.1137
	step [39/332], loss=3.8868
	step [40/332], loss=4.9340
	step [41/332], loss=5.8566
	step [42/332], loss=4.9919
	step [43/332], loss=5.8249
	step [44/332], loss=5.6782
	step [45/332], loss=4.6014
	step [46/332], loss=5.3024
	step [47/332], loss=4.9596
	step [48/332], loss=4.9607
	step [49/332], loss=4.5989
	step [50/332], loss=3.7060
	step [51/332], loss=3.7186
	step [52/332], loss=4.0294
	step [53/332], loss=4.2845
	step [54/332], loss=6.1441
	step [55/332], loss=5.0637
	step [56/332], loss=4.6132
	step [57/332], loss=4.3181
	step [58/332], loss=4.7034
	step [59/332], loss=3.4955
	step [60/332], loss=4.3880
	step [61/332], loss=4.2599
	step [62/332], loss=4.8420
	step [63/332], loss=5.0574
	step [64/332], loss=5.9891
	step [65/332], loss=3.8675
	step [66/332], loss=5.0587
	step [67/332], loss=4.3046
	step [68/332], loss=4.4357
	step [69/332], loss=5.2533
	step [70/332], loss=4.8038
	step [71/332], loss=4.4656
	step [72/332], loss=3.9608
	step [73/332], loss=6.0709
	step [74/332], loss=4.0770
	step [75/332], loss=4.2281
	step [76/332], loss=4.4051
	step [77/332], loss=4.5947
	step [78/332], loss=4.3792
	step [79/332], loss=4.4399
	step [80/332], loss=4.2988
	step [81/332], loss=4.0645
	step [82/332], loss=5.2099
	step [83/332], loss=4.2525
	step [84/332], loss=3.8068
	step [85/332], loss=4.1117
	step [86/332], loss=4.5382
	step [87/332], loss=4.1547
	step [88/332], loss=4.9828
	step [89/332], loss=3.7454
	step [90/332], loss=4.5772
	step [91/332], loss=4.5487
	step [92/332], loss=5.8629
	step [93/332], loss=3.2889
	step [94/332], loss=4.3276
	step [95/332], loss=4.3159
	step [96/332], loss=4.9810
	step [97/332], loss=5.3506
	step [98/332], loss=4.6465
	step [99/332], loss=4.2649
	step [100/332], loss=4.8346
	step [101/332], loss=5.1767
	step [102/332], loss=4.2929
	step [103/332], loss=5.0273
	step [104/332], loss=4.7020
	step [105/332], loss=3.9456
	step [106/332], loss=4.0985
	step [107/332], loss=4.9765
	step [108/332], loss=5.0510
	step [109/332], loss=4.9097
	step [110/332], loss=4.3932
	step [111/332], loss=4.8118
	step [112/332], loss=4.1172
	step [113/332], loss=3.3170
	step [114/332], loss=4.4869
	step [115/332], loss=3.7742
	step [116/332], loss=4.5112
	step [117/332], loss=4.4166
	step [118/332], loss=4.8634
	step [119/332], loss=4.2834
	step [120/332], loss=4.0361
	step [121/332], loss=4.7255
	step [122/332], loss=4.3086
	step [123/332], loss=3.8045
	step [124/332], loss=4.8955
	step [125/332], loss=3.8057
	step [126/332], loss=3.8857
	step [127/332], loss=4.2320
	step [128/332], loss=4.6062
	step [129/332], loss=5.3709
	step [130/332], loss=4.7748
	step [131/332], loss=4.1111
	step [132/332], loss=4.7060
	step [133/332], loss=3.9714
	step [134/332], loss=3.9204
	step [135/332], loss=4.6318
	step [136/332], loss=5.6245
	step [137/332], loss=4.8557
	step [138/332], loss=4.1834
	step [139/332], loss=3.8148
	step [140/332], loss=3.8080
	step [141/332], loss=4.1412
	step [142/332], loss=4.0501
	step [143/332], loss=5.1365
	step [144/332], loss=4.8270
	step [145/332], loss=4.2007
	step [146/332], loss=4.3420
	step [147/332], loss=3.9363
	step [148/332], loss=4.3089
	step [149/332], loss=4.1464
	step [150/332], loss=4.9082
	step [151/332], loss=3.8734
	step [152/332], loss=4.8377
	step [153/332], loss=4.0012
	step [154/332], loss=5.0847
	step [155/332], loss=5.0572
	step [156/332], loss=4.4637
	step [157/332], loss=3.6782
	step [158/332], loss=4.6497
	step [159/332], loss=5.0288
	step [160/332], loss=5.9570
	step [161/332], loss=4.2740
	step [162/332], loss=4.6617
	step [163/332], loss=4.0178
	step [164/332], loss=5.0570
	step [165/332], loss=4.6322
	step [166/332], loss=4.4709
	step [167/332], loss=4.1767
	step [168/332], loss=3.9421
	step [169/332], loss=4.3624
	step [170/332], loss=4.0086
	step [171/332], loss=3.6422
	step [172/332], loss=3.9799
	step [173/332], loss=4.3335
	step [174/332], loss=4.5007
	step [175/332], loss=5.5404
	step [176/332], loss=5.3527
	step [177/332], loss=5.0680
	step [178/332], loss=5.8144
	step [179/332], loss=4.9813
	step [180/332], loss=3.9764
	step [181/332], loss=4.2124
	step [182/332], loss=4.2706
	step [183/332], loss=4.6478
	step [184/332], loss=4.3593
	step [185/332], loss=4.9888
	step [186/332], loss=6.0654
	step [187/332], loss=4.3820
	step [188/332], loss=4.5490
	step [189/332], loss=4.0432
	step [190/332], loss=4.1640
	step [191/332], loss=4.2048
	step [192/332], loss=5.0072
	step [193/332], loss=5.2422
	step [194/332], loss=4.9164
	step [195/332], loss=3.8710
	step [196/332], loss=4.2415
	step [197/332], loss=3.9561
	step [198/332], loss=4.4585
	step [199/332], loss=4.5128
	step [200/332], loss=5.3106
	step [201/332], loss=4.8963
	step [202/332], loss=4.7358
	step [203/332], loss=3.7229
	step [204/332], loss=5.5408
	step [205/332], loss=4.5561
	step [206/332], loss=5.2315
	step [207/332], loss=4.7667
	step [208/332], loss=4.3008
	step [209/332], loss=5.6907
	step [210/332], loss=3.9844
	step [211/332], loss=7.1473
	step [212/332], loss=4.1839
	step [213/332], loss=5.0622
	step [214/332], loss=4.4841
	step [215/332], loss=5.5162
	step [216/332], loss=3.6753
	step [217/332], loss=4.3498
	step [218/332], loss=5.3520
	step [219/332], loss=3.7850
	step [220/332], loss=4.3923
	step [221/332], loss=3.8327
	step [222/332], loss=4.4872
	step [223/332], loss=4.2355
	step [224/332], loss=4.7996
	step [225/332], loss=4.6066
	step [226/332], loss=3.6377
	step [227/332], loss=4.0840
	step [228/332], loss=4.2868
	step [229/332], loss=5.2866
	step [230/332], loss=4.9190
	step [231/332], loss=4.1647
	step [232/332], loss=5.4454
	step [233/332], loss=4.2281
	step [234/332], loss=4.8215
	step [235/332], loss=3.8469
	step [236/332], loss=4.4208
	step [237/332], loss=5.4065
	step [238/332], loss=3.7690
	step [239/332], loss=4.6585
	step [240/332], loss=5.2883
	step [241/332], loss=4.2524
	step [242/332], loss=4.3763
	step [243/332], loss=3.8547
	step [244/332], loss=4.9942
	step [245/332], loss=4.7439
	step [246/332], loss=3.9978
	step [247/332], loss=4.7631
	step [248/332], loss=6.7956
	step [249/332], loss=4.7950
	step [250/332], loss=4.1469
	step [251/332], loss=5.3742
	step [252/332], loss=4.5531
	step [253/332], loss=3.7050
	step [254/332], loss=5.3080
	step [255/332], loss=5.7246
	step [256/332], loss=3.4053
	step [257/332], loss=4.5777
	step [258/332], loss=5.2863
	step [259/332], loss=3.3981
	step [260/332], loss=4.7682
	step [261/332], loss=4.5258
	step [262/332], loss=4.0665
	step [263/332], loss=4.9558
	step [264/332], loss=5.3802
	step [265/332], loss=4.4388
	step [266/332], loss=5.3188
	step [267/332], loss=4.1735
	step [268/332], loss=4.9741
	step [269/332], loss=4.3925
	step [270/332], loss=4.7468
	step [271/332], loss=5.2670
	step [272/332], loss=4.5150
	step [273/332], loss=3.8172
	step [274/332], loss=5.4656
	step [275/332], loss=5.1158
	step [276/332], loss=5.3920
	step [277/332], loss=3.8441
	step [278/332], loss=4.0460
	step [279/332], loss=5.3993
	step [280/332], loss=3.6803
	step [281/332], loss=3.8054
	step [282/332], loss=5.0354
	step [283/332], loss=5.3971
	step [284/332], loss=4.9801
	step [285/332], loss=4.7918
	step [286/332], loss=3.7918
	step [287/332], loss=3.7665
	step [288/332], loss=5.1682
	step [289/332], loss=5.1720
	step [290/332], loss=4.1276
	step [291/332], loss=4.6326
	step [292/332], loss=4.0677
	step [293/332], loss=6.4777
	step [294/332], loss=5.3044
	step [295/332], loss=4.3642
	step [296/332], loss=5.2160
	step [297/332], loss=4.3207
	step [298/332], loss=5.2634
	step [299/332], loss=4.2021
	step [300/332], loss=3.2033
	step [301/332], loss=3.5725
	step [302/332], loss=4.8499
	step [303/332], loss=4.3520
	step [304/332], loss=4.5816
	step [305/332], loss=4.3881
	step [306/332], loss=4.3314
	step [307/332], loss=3.3319
	step [308/332], loss=3.5808
	step [309/332], loss=5.0584
	step [310/332], loss=5.2444
	step [311/332], loss=3.3837
	step [312/332], loss=5.0307
	step [313/332], loss=3.5456
	step [314/332], loss=4.8637
	step [315/332], loss=5.5439
	step [316/332], loss=4.5144
	step [317/332], loss=4.4254
	step [318/332], loss=4.2428
	step [319/332], loss=3.6854
	step [320/332], loss=4.5017
	step [321/332], loss=3.8996
	step [322/332], loss=4.1423
	step [323/332], loss=3.8652
	step [324/332], loss=4.2643
	step [325/332], loss=4.0014
	step [326/332], loss=4.9047
	step [327/332], loss=5.0834
	step [328/332], loss=4.4442
	step [329/332], loss=4.1182
	step [330/332], loss=3.7667
	step [331/332], loss=4.1613
	step [332/332], loss=2.7604
	Evaluating
	loss=0.0163, precision=0.2058, recall=0.9945, f1=0.3411
Training epoch 25
	step [1/332], loss=5.3704
	step [2/332], loss=3.8528
	step [3/332], loss=4.0616
	step [4/332], loss=4.1871
	step [5/332], loss=5.4515
	step [6/332], loss=3.3799
	step [7/332], loss=4.9377
	step [8/332], loss=4.0853
	step [9/332], loss=5.8746
	step [10/332], loss=4.5243
	step [11/332], loss=3.9735
	step [12/332], loss=4.9642
	step [13/332], loss=4.5911
	step [14/332], loss=4.6959
	step [15/332], loss=4.8039
	step [16/332], loss=5.1293
	step [17/332], loss=4.9037
	step [18/332], loss=6.0849
	step [19/332], loss=4.4169
	step [20/332], loss=4.8407
	step [21/332], loss=4.7355
	step [22/332], loss=5.2623
	step [23/332], loss=4.9308
	step [24/332], loss=4.4596
	step [25/332], loss=5.3703
	step [26/332], loss=5.0196
	step [27/332], loss=4.9161
	step [28/332], loss=4.1526
	step [29/332], loss=3.2607
	step [30/332], loss=4.2430
	step [31/332], loss=4.2933
	step [32/332], loss=3.6764
	step [33/332], loss=4.3363
	step [34/332], loss=4.3871
	step [35/332], loss=4.7142
	step [36/332], loss=4.9787
	step [37/332], loss=3.9996
	step [38/332], loss=4.9231
	step [39/332], loss=4.6719
	step [40/332], loss=5.0194
	step [41/332], loss=5.5650
	step [42/332], loss=5.5060
	step [43/332], loss=5.1643
	step [44/332], loss=4.3891
	step [45/332], loss=4.7776
	step [46/332], loss=4.7202
	step [47/332], loss=3.5961
	step [48/332], loss=3.9374
	step [49/332], loss=4.8800
	step [50/332], loss=4.1245
	step [51/332], loss=4.2832
	step [52/332], loss=4.3610
	step [53/332], loss=4.8053
	step [54/332], loss=4.2626
	step [55/332], loss=4.0096
	step [56/332], loss=3.7467
	step [57/332], loss=5.0730
	step [58/332], loss=3.8077
	step [59/332], loss=3.9797
	step [60/332], loss=4.8291
	step [61/332], loss=4.9193
	step [62/332], loss=4.2756
	step [63/332], loss=5.5370
	step [64/332], loss=4.7180
	step [65/332], loss=4.7752
	step [66/332], loss=5.0231
	step [67/332], loss=3.7707
	step [68/332], loss=3.7173
	step [69/332], loss=4.8375
	step [70/332], loss=4.8642
	step [71/332], loss=5.1765
	step [72/332], loss=4.7357
	step [73/332], loss=4.5161
	step [74/332], loss=5.2556
	step [75/332], loss=4.5286
	step [76/332], loss=4.0059
	step [77/332], loss=3.7450
	step [78/332], loss=4.8680
	step [79/332], loss=2.9187
	step [80/332], loss=3.9326
	step [81/332], loss=5.2553
	step [82/332], loss=4.3617
	step [83/332], loss=5.0567
	step [84/332], loss=3.9879
	step [85/332], loss=5.6861
	step [86/332], loss=4.9943
	step [87/332], loss=4.7925
	step [88/332], loss=5.0814
	step [89/332], loss=5.3961
	step [90/332], loss=5.1805
	step [91/332], loss=4.0875
	step [92/332], loss=4.0994
	step [93/332], loss=4.4446
	step [94/332], loss=4.4316
	step [95/332], loss=3.7304
	step [96/332], loss=4.8227
	step [97/332], loss=4.1743
	step [98/332], loss=4.4659
	step [99/332], loss=3.6829
	step [100/332], loss=3.5127
	step [101/332], loss=4.5038
	step [102/332], loss=4.8678
	step [103/332], loss=4.4066
	step [104/332], loss=3.7104
	step [105/332], loss=5.5692
	step [106/332], loss=4.7418
	step [107/332], loss=2.9371
	step [108/332], loss=4.4268
	step [109/332], loss=4.0943
	step [110/332], loss=4.1165
	step [111/332], loss=5.1456
	step [112/332], loss=4.0497
	step [113/332], loss=5.3018
	step [114/332], loss=4.9660
	step [115/332], loss=4.3397
	step [116/332], loss=4.2053
	step [117/332], loss=3.6702
	step [118/332], loss=4.2406
	step [119/332], loss=3.8817
	step [120/332], loss=4.7929
	step [121/332], loss=3.4073
	step [122/332], loss=4.3797
	step [123/332], loss=4.3057
	step [124/332], loss=4.1969
	step [125/332], loss=5.6792
	step [126/332], loss=4.2583
	step [127/332], loss=4.7418
	step [128/332], loss=5.2229
	step [129/332], loss=3.8376
	step [130/332], loss=3.7949
	step [131/332], loss=5.4534
	step [132/332], loss=5.1291
	step [133/332], loss=4.5951
	step [134/332], loss=4.5193
	step [135/332], loss=4.1755
	step [136/332], loss=4.4831
	step [137/332], loss=4.1816
	step [138/332], loss=3.9980
	step [139/332], loss=4.5324
	step [140/332], loss=3.5279
	step [141/332], loss=5.1442
	step [142/332], loss=5.5194
	step [143/332], loss=4.3224
	step [144/332], loss=4.0356
	step [145/332], loss=3.7366
	step [146/332], loss=4.6506
	step [147/332], loss=4.0186
	step [148/332], loss=3.9531
	step [149/332], loss=3.8157
	step [150/332], loss=3.8977
	step [151/332], loss=4.6884
	step [152/332], loss=4.7478
	step [153/332], loss=4.7991
	step [154/332], loss=3.7997
	step [155/332], loss=3.8193
	step [156/332], loss=4.5183
	step [157/332], loss=4.7896
	step [158/332], loss=4.8972
	step [159/332], loss=4.8499
	step [160/332], loss=4.3039
	step [161/332], loss=4.8188
	step [162/332], loss=4.9167
	step [163/332], loss=4.2149
	step [164/332], loss=4.5628
	step [165/332], loss=5.0771
	step [166/332], loss=5.1919
	step [167/332], loss=4.5339
	step [168/332], loss=5.4517
	step [169/332], loss=4.8663
	step [170/332], loss=3.2776
	step [171/332], loss=3.9246
	step [172/332], loss=3.8919
	step [173/332], loss=4.7263
	step [174/332], loss=4.9174
	step [175/332], loss=3.4461
	step [176/332], loss=4.8399
	step [177/332], loss=4.7411
	step [178/332], loss=4.7187
	step [179/332], loss=3.4505
	step [180/332], loss=4.1573
	step [181/332], loss=3.9874
	step [182/332], loss=4.7424
	step [183/332], loss=3.8855
	step [184/332], loss=4.6193
	step [185/332], loss=4.7414
	step [186/332], loss=4.9573
	step [187/332], loss=4.2491
	step [188/332], loss=5.3378
	step [189/332], loss=5.5055
	step [190/332], loss=4.6063
	step [191/332], loss=4.9068
	step [192/332], loss=4.5749
	step [193/332], loss=3.7583
	step [194/332], loss=4.3640
	step [195/332], loss=3.6607
	step [196/332], loss=4.4697
	step [197/332], loss=4.7626
	step [198/332], loss=4.2656
	step [199/332], loss=3.7057
	step [200/332], loss=3.6722
	step [201/332], loss=4.0510
	step [202/332], loss=3.7021
	step [203/332], loss=5.4470
	step [204/332], loss=4.8752
	step [205/332], loss=3.6604
	step [206/332], loss=5.1474
	step [207/332], loss=3.8307
	step [208/332], loss=4.1035
	step [209/332], loss=4.0503
	step [210/332], loss=5.1773
	step [211/332], loss=3.0321
	step [212/332], loss=4.3638
	step [213/332], loss=4.1629
	step [214/332], loss=3.8627
	step [215/332], loss=5.1975
	step [216/332], loss=4.2235
	step [217/332], loss=5.1884
	step [218/332], loss=4.2921
	step [219/332], loss=3.9684
	step [220/332], loss=3.8787
	step [221/332], loss=5.1131
	step [222/332], loss=3.3601
	step [223/332], loss=3.2411
	step [224/332], loss=5.3155
	step [225/332], loss=4.7263
	step [226/332], loss=4.3264
	step [227/332], loss=3.3448
	step [228/332], loss=5.0317
	step [229/332], loss=3.2718
	step [230/332], loss=4.9120
	step [231/332], loss=4.1079
	step [232/332], loss=4.7109
	step [233/332], loss=4.3955
	step [234/332], loss=4.8037
	step [235/332], loss=4.8026
	step [236/332], loss=4.2572
	step [237/332], loss=4.9422
	step [238/332], loss=4.5608
	step [239/332], loss=3.2666
	step [240/332], loss=4.4128
	step [241/332], loss=3.9343
	step [242/332], loss=4.3075
	step [243/332], loss=4.8926
	step [244/332], loss=3.9514
	step [245/332], loss=4.5021
	step [246/332], loss=3.5358
	step [247/332], loss=4.8184
	step [248/332], loss=4.4936
	step [249/332], loss=5.8779
	step [250/332], loss=4.4441
	step [251/332], loss=3.8676
	step [252/332], loss=4.3938
	step [253/332], loss=3.9641
	step [254/332], loss=4.2975
	step [255/332], loss=3.9140
	step [256/332], loss=4.2590
	step [257/332], loss=4.8851
	step [258/332], loss=3.5624
	step [259/332], loss=7.2678
	step [260/332], loss=3.5351
	step [261/332], loss=4.9206
	step [262/332], loss=4.6314
	step [263/332], loss=4.3862
	step [264/332], loss=3.9866
	step [265/332], loss=4.9181
	step [266/332], loss=4.1329
	step [267/332], loss=4.1407
	step [268/332], loss=3.6429
	step [269/332], loss=3.7802
	step [270/332], loss=4.4529
	step [271/332], loss=4.4026
	step [272/332], loss=3.4925
	step [273/332], loss=3.6934
	step [274/332], loss=4.4581
	step [275/332], loss=3.8569
	step [276/332], loss=3.6986
	step [277/332], loss=4.8753
	step [278/332], loss=3.3356
	step [279/332], loss=4.0458
	step [280/332], loss=4.2553
	step [281/332], loss=4.7937
	step [282/332], loss=4.8783
	step [283/332], loss=5.3214
	step [284/332], loss=3.8021
	step [285/332], loss=3.3481
	step [286/332], loss=4.5063
	step [287/332], loss=3.8210
	step [288/332], loss=5.7257
	step [289/332], loss=4.5905
	step [290/332], loss=5.1385
	step [291/332], loss=3.8054
	step [292/332], loss=4.6721
	step [293/332], loss=3.9809
	step [294/332], loss=5.5300
	step [295/332], loss=4.9827
	step [296/332], loss=4.4142
	step [297/332], loss=4.1490
	step [298/332], loss=3.5823
	step [299/332], loss=4.3746
	step [300/332], loss=4.6416
	step [301/332], loss=4.3427
	step [302/332], loss=3.9509
	step [303/332], loss=5.3592
	step [304/332], loss=3.9056
	step [305/332], loss=5.8071
	step [306/332], loss=5.2220
	step [307/332], loss=4.0816
	step [308/332], loss=5.2082
	step [309/332], loss=5.0363
	step [310/332], loss=4.1531
	step [311/332], loss=4.5282
	step [312/332], loss=5.9036
	step [313/332], loss=3.6995
	step [314/332], loss=4.0956
	step [315/332], loss=3.8180
	step [316/332], loss=4.6216
	step [317/332], loss=3.9067
	step [318/332], loss=4.8226
	step [319/332], loss=4.3209
	step [320/332], loss=4.6087
	step [321/332], loss=4.9149
	step [322/332], loss=4.3386
	step [323/332], loss=4.0466
	step [324/332], loss=4.6781
	step [325/332], loss=3.8544
	step [326/332], loss=4.1220
	step [327/332], loss=4.3556
	step [328/332], loss=4.1707
	step [329/332], loss=3.7769
	step [330/332], loss=5.2101
	step [331/332], loss=4.3880
	step [332/332], loss=3.4770
	Evaluating
	loss=0.0164, precision=0.2047, recall=0.9946, f1=0.3395
Training epoch 26
	step [1/332], loss=4.5838
	step [2/332], loss=3.3043
	step [3/332], loss=5.5226
	step [4/332], loss=3.7110
	step [5/332], loss=6.6317
	step [6/332], loss=4.0655
	step [7/332], loss=5.1109
	step [8/332], loss=4.0272
	step [9/332], loss=4.2132
	step [10/332], loss=3.1762
	step [11/332], loss=3.9079
	step [12/332], loss=4.2894
	step [13/332], loss=3.6138
	step [14/332], loss=5.0309
	step [15/332], loss=3.8782
	step [16/332], loss=3.4891
	step [17/332], loss=3.8628
	step [18/332], loss=4.2376
	step [19/332], loss=4.7342
	step [20/332], loss=4.5611
	step [21/332], loss=5.0839
	step [22/332], loss=3.8732
	step [23/332], loss=4.4282
	step [24/332], loss=4.1359
	step [25/332], loss=3.9285
	step [26/332], loss=3.7013
	step [27/332], loss=3.7584
	step [28/332], loss=4.1505
	step [29/332], loss=4.3089
	step [30/332], loss=4.3469
	step [31/332], loss=4.9720
	step [32/332], loss=4.2466
	step [33/332], loss=3.8991
	step [34/332], loss=5.0000
	step [35/332], loss=4.0112
	step [36/332], loss=5.5406
	step [37/332], loss=4.0318
	step [38/332], loss=5.1312
	step [39/332], loss=4.1343
	step [40/332], loss=3.4239
	step [41/332], loss=4.6753
	step [42/332], loss=4.1132
	step [43/332], loss=4.0892
	step [44/332], loss=4.6395
	step [45/332], loss=3.9763
	step [46/332], loss=4.9037
	step [47/332], loss=5.0391
	step [48/332], loss=5.0907
	step [49/332], loss=3.4727
	step [50/332], loss=4.0918
	step [51/332], loss=3.5520
	step [52/332], loss=4.5875
	step [53/332], loss=5.7082
	step [54/332], loss=4.7795
	step [55/332], loss=4.6173
	step [56/332], loss=5.1764
	step [57/332], loss=4.1624
	step [58/332], loss=3.5101
	step [59/332], loss=3.7053
	step [60/332], loss=4.2051
	step [61/332], loss=4.2482
	step [62/332], loss=4.3834
	step [63/332], loss=4.4203
	step [64/332], loss=3.6709
	step [65/332], loss=3.9490
	step [66/332], loss=4.7916
	step [67/332], loss=4.0750
	step [68/332], loss=4.8211
	step [69/332], loss=4.7631
	step [70/332], loss=4.6822
	step [71/332], loss=5.6449
	step [72/332], loss=4.5029
	step [73/332], loss=3.9612
	step [74/332], loss=4.3758
	step [75/332], loss=4.7636
	step [76/332], loss=3.9321
	step [77/332], loss=4.6132
	step [78/332], loss=4.6445
	step [79/332], loss=4.0437
	step [80/332], loss=4.1268
	step [81/332], loss=4.5526
	step [82/332], loss=5.1609
	step [83/332], loss=3.7601
	step [84/332], loss=5.5333
	step [85/332], loss=4.4873
	step [86/332], loss=5.1295
	step [87/332], loss=4.2768
	step [88/332], loss=4.0472
	step [89/332], loss=3.8076
	step [90/332], loss=4.6265
	step [91/332], loss=4.2101
	step [92/332], loss=4.3126
	step [93/332], loss=3.8637
	step [94/332], loss=4.0052
	step [95/332], loss=4.1905
	step [96/332], loss=4.9563
	step [97/332], loss=4.7939
	step [98/332], loss=4.1285
	step [99/332], loss=3.9706
	step [100/332], loss=5.1392
	step [101/332], loss=4.0839
	step [102/332], loss=4.1246
	step [103/332], loss=3.7988
	step [104/332], loss=4.5032
	step [105/332], loss=3.7782
	step [106/332], loss=4.7682
	step [107/332], loss=4.7057
	step [108/332], loss=4.6198
	step [109/332], loss=4.6558
	step [110/332], loss=5.1879
	step [111/332], loss=5.1257
	step [112/332], loss=4.5588
	step [113/332], loss=4.8414
	step [114/332], loss=4.0126
	step [115/332], loss=3.7001
	step [116/332], loss=4.7298
	step [117/332], loss=4.5561
	step [118/332], loss=4.7925
	step [119/332], loss=3.3608
	step [120/332], loss=4.9562
	step [121/332], loss=5.2867
	step [122/332], loss=4.2226
	step [123/332], loss=3.2643
	step [124/332], loss=3.8016
	step [125/332], loss=3.9971
	step [126/332], loss=4.0202
	step [127/332], loss=6.0879
	step [128/332], loss=4.5124
	step [129/332], loss=3.8481
	step [130/332], loss=4.1600
	step [131/332], loss=3.9058
	step [132/332], loss=3.7527
	step [133/332], loss=5.4444
	step [134/332], loss=4.4878
	step [135/332], loss=5.3128
	step [136/332], loss=4.4717
	step [137/332], loss=3.7554
	step [138/332], loss=3.6894
	step [139/332], loss=4.4510
	step [140/332], loss=4.4784
	step [141/332], loss=3.7006
	step [142/332], loss=5.5540
	step [143/332], loss=4.0599
	step [144/332], loss=3.9331
	step [145/332], loss=4.5798
	step [146/332], loss=4.4428
	step [147/332], loss=4.8115
	step [148/332], loss=4.9414
	step [149/332], loss=4.9249
	step [150/332], loss=5.2437
	step [151/332], loss=4.3577
	step [152/332], loss=4.2544
	step [153/332], loss=3.8798
	step [154/332], loss=4.2438
	step [155/332], loss=4.3705
	step [156/332], loss=3.9255
	step [157/332], loss=5.0563
	step [158/332], loss=4.2047
	step [159/332], loss=4.1738
	step [160/332], loss=4.2154
	step [161/332], loss=3.6015
	step [162/332], loss=4.3334
	step [163/332], loss=3.7701
	step [164/332], loss=3.5014
	step [165/332], loss=5.0245
	step [166/332], loss=4.6105
	step [167/332], loss=4.2395
	step [168/332], loss=4.2045
	step [169/332], loss=4.8360
	step [170/332], loss=4.7288
	step [171/332], loss=4.7640
	step [172/332], loss=4.4604
	step [173/332], loss=5.4054
	step [174/332], loss=4.7280
	step [175/332], loss=3.8868
	step [176/332], loss=4.5891
	step [177/332], loss=5.0047
	step [178/332], loss=5.3091
	step [179/332], loss=4.2745
	step [180/332], loss=5.2008
	step [181/332], loss=4.6366
	step [182/332], loss=5.1630
	step [183/332], loss=3.6762
	step [184/332], loss=4.4064
	step [185/332], loss=4.8618
	step [186/332], loss=4.1535
	step [187/332], loss=5.1722
	step [188/332], loss=4.4306
	step [189/332], loss=4.5546
	step [190/332], loss=5.0204
	step [191/332], loss=4.1899
	step [192/332], loss=5.3442
	step [193/332], loss=4.2007
	step [194/332], loss=4.9970
	step [195/332], loss=3.7542
	step [196/332], loss=4.4298
	step [197/332], loss=2.9780
	step [198/332], loss=5.6370
	step [199/332], loss=3.9329
	step [200/332], loss=3.7197
	step [201/332], loss=4.0775
	step [202/332], loss=4.6116
	step [203/332], loss=4.4970
	step [204/332], loss=4.0094
	step [205/332], loss=4.2294
	step [206/332], loss=4.2358
	step [207/332], loss=4.7335
	step [208/332], loss=4.3976
	step [209/332], loss=4.5309
	step [210/332], loss=3.6257
	step [211/332], loss=4.3546
	step [212/332], loss=4.3329
	step [213/332], loss=3.5636
	step [214/332], loss=4.1815
	step [215/332], loss=3.6104
	step [216/332], loss=4.2762
	step [217/332], loss=4.9745
	step [218/332], loss=5.0907
	step [219/332], loss=4.2736
	step [220/332], loss=3.5341
	step [221/332], loss=4.8900
	step [222/332], loss=4.5829
	step [223/332], loss=4.7473
	step [224/332], loss=4.3511
	step [225/332], loss=4.7613
	step [226/332], loss=5.0120
	step [227/332], loss=3.8651
	step [228/332], loss=3.6151
	step [229/332], loss=4.3890
	step [230/332], loss=4.1673
	step [231/332], loss=3.2787
	step [232/332], loss=4.3674
	step [233/332], loss=5.3252
	step [234/332], loss=5.2259
	step [235/332], loss=4.2804
	step [236/332], loss=4.1754
	step [237/332], loss=4.1823
	step [238/332], loss=4.8980
	step [239/332], loss=3.1430
	step [240/332], loss=3.9198
	step [241/332], loss=4.9551
	step [242/332], loss=4.2357
	step [243/332], loss=3.6426
	step [244/332], loss=4.9909
	step [245/332], loss=3.8056
	step [246/332], loss=5.3894
	step [247/332], loss=3.9212
	step [248/332], loss=4.2943
	step [249/332], loss=4.4057
	step [250/332], loss=4.4851
	step [251/332], loss=3.6521
	step [252/332], loss=4.1026
	step [253/332], loss=4.2961
	step [254/332], loss=4.1889
	step [255/332], loss=4.1820
	step [256/332], loss=3.3091
	step [257/332], loss=3.7696
	step [258/332], loss=3.3614
	step [259/332], loss=4.1407
	step [260/332], loss=4.7078
	step [261/332], loss=4.4654
	step [262/332], loss=4.2382
	step [263/332], loss=3.8171
	step [264/332], loss=3.6609
	step [265/332], loss=4.9078
	step [266/332], loss=4.7949
	step [267/332], loss=4.3551
	step [268/332], loss=4.4476
	step [269/332], loss=5.2592
	step [270/332], loss=3.8726
	step [271/332], loss=4.4077
	step [272/332], loss=4.8857
	step [273/332], loss=3.6592
	step [274/332], loss=5.8423
	step [275/332], loss=4.6469
	step [276/332], loss=5.1279
	step [277/332], loss=4.7860
	step [278/332], loss=4.4446
	step [279/332], loss=4.0923
	step [280/332], loss=4.4717
	step [281/332], loss=4.5123
	step [282/332], loss=4.9432
	step [283/332], loss=3.3199
	step [284/332], loss=3.6453
	step [285/332], loss=4.6316
	step [286/332], loss=3.6701
	step [287/332], loss=4.6188
	step [288/332], loss=3.8671
	step [289/332], loss=3.8481
	step [290/332], loss=4.5275
	step [291/332], loss=4.6034
	step [292/332], loss=3.7104
	step [293/332], loss=4.2187
	step [294/332], loss=3.7074
	step [295/332], loss=3.7673
	step [296/332], loss=3.6388
	step [297/332], loss=4.9467
	step [298/332], loss=4.5394
	step [299/332], loss=4.7564
	step [300/332], loss=4.4202
	step [301/332], loss=4.2417
	step [302/332], loss=4.3771
	step [303/332], loss=3.9530
	step [304/332], loss=3.6384
	step [305/332], loss=4.7349
	step [306/332], loss=3.5074
	step [307/332], loss=4.5529
	step [308/332], loss=5.7382
	step [309/332], loss=4.6677
	step [310/332], loss=5.2473
	step [311/332], loss=4.4970
	step [312/332], loss=5.5934
	step [313/332], loss=4.2593
	step [314/332], loss=4.2122
	step [315/332], loss=4.0474
	step [316/332], loss=4.2466
	step [317/332], loss=4.8155
	step [318/332], loss=4.6431
	step [319/332], loss=4.1979
	step [320/332], loss=2.8693
	step [321/332], loss=3.5315
	step [322/332], loss=3.9817
	step [323/332], loss=5.1733
	step [324/332], loss=3.9937
	step [325/332], loss=4.5060
	step [326/332], loss=4.2542
	step [327/332], loss=4.6899
	step [328/332], loss=4.1202
	step [329/332], loss=4.8881
	step [330/332], loss=4.9490
	step [331/332], loss=4.7870
	step [332/332], loss=2.7434
	Evaluating
	loss=0.0186, precision=0.1816, recall=0.9948, f1=0.3072
Training epoch 27
	step [1/332], loss=4.6808
	step [2/332], loss=4.4784
	step [3/332], loss=3.7464
	step [4/332], loss=4.9390
	step [5/332], loss=4.6849
	step [6/332], loss=3.7854
	step [7/332], loss=4.5498
	step [8/332], loss=4.8636
	step [9/332], loss=3.7159
	step [10/332], loss=4.3329
	step [11/332], loss=4.5791
	step [12/332], loss=3.5588
	step [13/332], loss=5.0577
	step [14/332], loss=4.0417
	step [15/332], loss=4.0131
	step [16/332], loss=3.5286
	step [17/332], loss=3.6672
	step [18/332], loss=4.9042
	step [19/332], loss=5.2532
	step [20/332], loss=4.6504
	step [21/332], loss=6.0714
	step [22/332], loss=3.8946
	step [23/332], loss=4.6023
	step [24/332], loss=4.4325
	step [25/332], loss=4.4800
	step [26/332], loss=4.4925
	step [27/332], loss=4.8968
	step [28/332], loss=4.1154
	step [29/332], loss=4.4590
	step [30/332], loss=4.1361
	step [31/332], loss=3.7602
	step [32/332], loss=4.7112
	step [33/332], loss=4.5658
	step [34/332], loss=5.1666
	step [35/332], loss=3.7247
	step [36/332], loss=4.6253
	step [37/332], loss=4.4157
	step [38/332], loss=4.7885
	step [39/332], loss=4.9804
	step [40/332], loss=4.3368
	step [41/332], loss=4.8308
	step [42/332], loss=4.4437
	step [43/332], loss=4.5048
	step [44/332], loss=3.9624
	step [45/332], loss=3.5272
	step [46/332], loss=5.2766
	step [47/332], loss=3.9199
	step [48/332], loss=4.3753
	step [49/332], loss=4.0881
	step [50/332], loss=3.8479
	step [51/332], loss=5.4360
	step [52/332], loss=4.5980
	step [53/332], loss=3.6408
	step [54/332], loss=4.5422
	step [55/332], loss=2.9336
	step [56/332], loss=3.5866
	step [57/332], loss=4.6576
	step [58/332], loss=3.8496
	step [59/332], loss=4.2990
	step [60/332], loss=3.1849
	step [61/332], loss=4.0747
	step [62/332], loss=4.8610
	step [63/332], loss=4.2310
	step [64/332], loss=4.9841
	step [65/332], loss=4.0931
	step [66/332], loss=4.7302
	step [67/332], loss=3.3279
	step [68/332], loss=4.8843
	step [69/332], loss=3.6593
	step [70/332], loss=4.3980
	step [71/332], loss=4.8038
	step [72/332], loss=3.8372
	step [73/332], loss=4.5269
	step [74/332], loss=4.3696
	step [75/332], loss=4.4249
	step [76/332], loss=3.9082
	step [77/332], loss=4.5366
	step [78/332], loss=4.9970
	step [79/332], loss=4.4025
	step [80/332], loss=5.2898
	step [81/332], loss=4.8080
	step [82/332], loss=5.1249
	step [83/332], loss=3.3720
	step [84/332], loss=4.6165
	step [85/332], loss=3.6431
	step [86/332], loss=5.9358
	step [87/332], loss=3.2819
	step [88/332], loss=4.5903
	step [89/332], loss=3.4712
	step [90/332], loss=4.1584
	step [91/332], loss=4.6800
	step [92/332], loss=5.0466
	step [93/332], loss=4.0820
	step [94/332], loss=4.6404
	step [95/332], loss=3.7809
	step [96/332], loss=4.1450
	step [97/332], loss=4.3566
	step [98/332], loss=5.2529
	step [99/332], loss=5.2096
	step [100/332], loss=3.4488
	step [101/332], loss=4.5157
	step [102/332], loss=4.6237
	step [103/332], loss=4.1935
	step [104/332], loss=3.4042
	step [105/332], loss=4.9774
	step [106/332], loss=4.1731
	step [107/332], loss=4.9521
	step [108/332], loss=4.6986
	step [109/332], loss=3.9592
	step [110/332], loss=4.0802
	step [111/332], loss=3.8413
	step [112/332], loss=5.8418
	step [113/332], loss=3.4401
	step [114/332], loss=3.8060
	step [115/332], loss=4.1887
	step [116/332], loss=4.5551
	step [117/332], loss=4.7539
	step [118/332], loss=5.3624
	step [119/332], loss=4.6604
	step [120/332], loss=3.8059
	step [121/332], loss=3.8952
	step [122/332], loss=4.3460
	step [123/332], loss=4.7159
	step [124/332], loss=4.4869
	step [125/332], loss=4.0879
	step [126/332], loss=4.2767
	step [127/332], loss=3.3261
	step [128/332], loss=4.1033
	step [129/332], loss=3.5527
	step [130/332], loss=4.0099
	step [131/332], loss=5.5046
	step [132/332], loss=4.2335
	step [133/332], loss=4.0229
	step [134/332], loss=6.1785
	step [135/332], loss=4.2325
	step [136/332], loss=4.5426
	step [137/332], loss=4.7110
	step [138/332], loss=4.5051
	step [139/332], loss=4.4995
	step [140/332], loss=3.8222
	step [141/332], loss=4.3750
	step [142/332], loss=3.8126
	step [143/332], loss=3.8387
	step [144/332], loss=3.8338
	step [145/332], loss=4.3087
	step [146/332], loss=4.8076
	step [147/332], loss=4.0913
	step [148/332], loss=4.2610
	step [149/332], loss=3.6076
	step [150/332], loss=3.8751
	step [151/332], loss=3.9523
	step [152/332], loss=4.0088
	step [153/332], loss=4.3517
	step [154/332], loss=5.9260
	step [155/332], loss=3.5663
	step [156/332], loss=4.7672
	step [157/332], loss=4.9757
	step [158/332], loss=4.0392
	step [159/332], loss=3.9026
	step [160/332], loss=4.7253
	step [161/332], loss=3.7610
	step [162/332], loss=3.9022
	step [163/332], loss=5.0514
	step [164/332], loss=4.4177
	step [165/332], loss=3.6892
	step [166/332], loss=4.2539
	step [167/332], loss=3.5234
	step [168/332], loss=2.9745
	step [169/332], loss=3.4401
	step [170/332], loss=4.1413
	step [171/332], loss=5.1046
	step [172/332], loss=3.5393
	step [173/332], loss=3.7727
	step [174/332], loss=3.8695
	step [175/332], loss=4.7117
	step [176/332], loss=4.1586
	step [177/332], loss=4.0798
	step [178/332], loss=3.8625
	step [179/332], loss=4.2823
	step [180/332], loss=4.9543
	step [181/332], loss=4.2688
	step [182/332], loss=4.9266
	step [183/332], loss=3.9041
	step [184/332], loss=5.0239
	step [185/332], loss=4.2606
	step [186/332], loss=4.5870
	step [187/332], loss=3.3096
	step [188/332], loss=4.1262
	step [189/332], loss=4.7972
	step [190/332], loss=4.0788
	step [191/332], loss=4.5945
	step [192/332], loss=3.8718
	step [193/332], loss=4.2828
	step [194/332], loss=4.7598
	step [195/332], loss=3.0656
	step [196/332], loss=4.3859
	step [197/332], loss=4.2929
	step [198/332], loss=3.4793
	step [199/332], loss=3.6775
	step [200/332], loss=3.7796
	step [201/332], loss=4.6219
	step [202/332], loss=3.8622
	step [203/332], loss=4.4318
	step [204/332], loss=3.6073
	step [205/332], loss=5.0912
	step [206/332], loss=4.0392
	step [207/332], loss=3.7441
	step [208/332], loss=4.2365
	step [209/332], loss=3.6218
	step [210/332], loss=4.1053
	step [211/332], loss=4.0107
	step [212/332], loss=5.2156
	step [213/332], loss=4.2483
	step [214/332], loss=3.8618
	step [215/332], loss=3.5527
	step [216/332], loss=3.5758
	step [217/332], loss=4.3601
	step [218/332], loss=4.6938
	step [219/332], loss=3.9687
	step [220/332], loss=3.8866
	step [221/332], loss=4.6631
	step [222/332], loss=4.0158
	step [223/332], loss=4.8898
	step [224/332], loss=3.2705
	step [225/332], loss=4.0330
	step [226/332], loss=3.9923
	step [227/332], loss=3.7139
	step [228/332], loss=4.2412
	step [229/332], loss=4.2611
	step [230/332], loss=4.1223
	step [231/332], loss=3.9714
	step [232/332], loss=4.2358
	step [233/332], loss=4.6069
	step [234/332], loss=3.2511
	step [235/332], loss=4.1002
	step [236/332], loss=4.6284
	step [237/332], loss=3.8167
	step [238/332], loss=4.5092
	step [239/332], loss=5.2592
	step [240/332], loss=3.9033
	step [241/332], loss=4.1257
	step [242/332], loss=3.7199
	step [243/332], loss=4.6541
	step [244/332], loss=3.8521
	step [245/332], loss=4.1306
	step [246/332], loss=5.5959
	step [247/332], loss=4.6260
	step [248/332], loss=3.9557
	step [249/332], loss=4.4363
	step [250/332], loss=4.0079
	step [251/332], loss=3.9990
	step [252/332], loss=3.7853
	step [253/332], loss=3.6324
	step [254/332], loss=4.2880
	step [255/332], loss=4.1855
	step [256/332], loss=5.4394
	step [257/332], loss=4.8631
	step [258/332], loss=3.9660
	step [259/332], loss=3.7113
	step [260/332], loss=3.4640
	step [261/332], loss=3.9557
	step [262/332], loss=4.4195
	step [263/332], loss=4.3846
	step [264/332], loss=5.6844
	step [265/332], loss=4.1651
	step [266/332], loss=4.7703
	step [267/332], loss=3.9724
	step [268/332], loss=4.7155
	step [269/332], loss=4.3832
	step [270/332], loss=4.4522
	step [271/332], loss=4.0322
	step [272/332], loss=3.8938
	step [273/332], loss=4.7704
	step [274/332], loss=4.2619
	step [275/332], loss=2.9323
	step [276/332], loss=3.8061
	step [277/332], loss=4.0057
	step [278/332], loss=4.2985
	step [279/332], loss=4.8379
	step [280/332], loss=5.1223
	step [281/332], loss=3.6669
	step [282/332], loss=4.3482
	step [283/332], loss=4.0090
	step [284/332], loss=4.7678
	step [285/332], loss=4.1503
	step [286/332], loss=4.6336
	step [287/332], loss=5.3397
	step [288/332], loss=3.6855
	step [289/332], loss=4.8479
	step [290/332], loss=4.0860
	step [291/332], loss=3.8899
	step [292/332], loss=4.8543
	step [293/332], loss=4.7294
	step [294/332], loss=5.1260
	step [295/332], loss=3.6145
	step [296/332], loss=4.3936
	step [297/332], loss=3.7856
	step [298/332], loss=4.2726
	step [299/332], loss=4.4873
	step [300/332], loss=4.4200
	step [301/332], loss=4.3565
	step [302/332], loss=4.0963
	step [303/332], loss=4.6183
	step [304/332], loss=4.0626
	step [305/332], loss=4.7858
	step [306/332], loss=4.4646
	step [307/332], loss=3.1083
	step [308/332], loss=4.3208
	step [309/332], loss=3.5268
	step [310/332], loss=4.4068
	step [311/332], loss=6.3868
	step [312/332], loss=4.8718
	step [313/332], loss=3.9025
	step [314/332], loss=4.0748
	step [315/332], loss=4.6452
	step [316/332], loss=4.4422
	step [317/332], loss=4.4298
	step [318/332], loss=3.6049
	step [319/332], loss=3.9049
	step [320/332], loss=4.0204
	step [321/332], loss=3.9699
	step [322/332], loss=3.6551
	step [323/332], loss=4.8681
	step [324/332], loss=3.9138
	step [325/332], loss=4.3585
	step [326/332], loss=4.5334
	step [327/332], loss=3.3344
	step [328/332], loss=3.8792
	step [329/332], loss=4.5490
	step [330/332], loss=5.0932
	step [331/332], loss=4.6710
	step [332/332], loss=1.8726
	Evaluating
	loss=0.0154, precision=0.2021, recall=0.9952, f1=0.3360
Training epoch 28
	step [1/332], loss=4.4185
	step [2/332], loss=5.9669
	step [3/332], loss=4.6121
	step [4/332], loss=3.8762
	step [5/332], loss=4.1445
	step [6/332], loss=4.8227
	step [7/332], loss=4.1677
	step [8/332], loss=3.7979
	step [9/332], loss=3.8115
	step [10/332], loss=3.8639
	step [11/332], loss=4.5394
	step [12/332], loss=4.5829
	step [13/332], loss=3.7284
	step [14/332], loss=3.8037
	step [15/332], loss=4.9217
	step [16/332], loss=4.1314
	step [17/332], loss=4.6040
	step [18/332], loss=3.5857
	step [19/332], loss=4.7464
	step [20/332], loss=5.0377
	step [21/332], loss=4.0547
	step [22/332], loss=3.6812
	step [23/332], loss=4.9114
	step [24/332], loss=3.8315
	step [25/332], loss=3.4463
	step [26/332], loss=3.6453
	step [27/332], loss=4.8657
	step [28/332], loss=4.8239
	step [29/332], loss=3.7382
	step [30/332], loss=4.1844
	step [31/332], loss=4.4805
	step [32/332], loss=4.0553
	step [33/332], loss=4.0813
	step [34/332], loss=4.5641
	step [35/332], loss=3.7908
	step [36/332], loss=4.4935
	step [37/332], loss=3.7485
	step [38/332], loss=3.9103
	step [39/332], loss=4.2658
	step [40/332], loss=4.9402
	step [41/332], loss=4.4357
	step [42/332], loss=3.4602
	step [43/332], loss=3.8057
	step [44/332], loss=4.4616
	step [45/332], loss=4.1793
	step [46/332], loss=4.4311
	step [47/332], loss=3.7625
	step [48/332], loss=4.3910
	step [49/332], loss=4.0438
	step [50/332], loss=4.2291
	step [51/332], loss=4.0049
	step [52/332], loss=4.5400
	step [53/332], loss=4.2759
	step [54/332], loss=4.5769
	step [55/332], loss=4.1626
	step [56/332], loss=3.4812
	step [57/332], loss=4.5159
	step [58/332], loss=3.7046
	step [59/332], loss=3.5216
	step [60/332], loss=4.0253
	step [61/332], loss=4.0704
	step [62/332], loss=3.4248
	step [63/332], loss=3.4950
	step [64/332], loss=3.6745
	step [65/332], loss=3.5963
	step [66/332], loss=3.7619
	step [67/332], loss=4.0050
	step [68/332], loss=5.2291
	step [69/332], loss=4.0497
	step [70/332], loss=3.6608
	step [71/332], loss=4.9546
	step [72/332], loss=4.3014
	step [73/332], loss=3.4974
	step [74/332], loss=3.7229
	step [75/332], loss=3.6512
	step [76/332], loss=3.4264
	step [77/332], loss=3.8149
	step [78/332], loss=4.1197
	step [79/332], loss=4.2123
	step [80/332], loss=4.4797
	step [81/332], loss=5.2627
	step [82/332], loss=4.6723
	step [83/332], loss=3.8870
	step [84/332], loss=3.4369
	step [85/332], loss=3.8932
	step [86/332], loss=4.7456
	step [87/332], loss=4.0014
	step [88/332], loss=4.3925
	step [89/332], loss=4.1636
	step [90/332], loss=4.2364
	step [91/332], loss=4.8668
	step [92/332], loss=4.4104
	step [93/332], loss=3.1080
	step [94/332], loss=3.8640
	step [95/332], loss=4.0361
	step [96/332], loss=3.6609
	step [97/332], loss=4.3220
	step [98/332], loss=3.8068
	step [99/332], loss=4.3898
	step [100/332], loss=3.3498
	step [101/332], loss=3.7353
	step [102/332], loss=3.5000
	step [103/332], loss=4.3707
	step [104/332], loss=4.5204
	step [105/332], loss=3.7140
	step [106/332], loss=4.6957
	step [107/332], loss=4.5540
	step [108/332], loss=4.1845
	step [109/332], loss=3.6912
	step [110/332], loss=4.7642
	step [111/332], loss=4.6081
	step [112/332], loss=4.5390
	step [113/332], loss=3.6582
	step [114/332], loss=3.6852
	step [115/332], loss=5.0097
	step [116/332], loss=3.4490
	step [117/332], loss=5.0555
	step [118/332], loss=4.8147
	step [119/332], loss=3.4551
	step [120/332], loss=3.7207
	step [121/332], loss=4.2399
	step [122/332], loss=4.1468
	step [123/332], loss=3.9500
	step [124/332], loss=4.4393
	step [125/332], loss=4.2493
	step [126/332], loss=3.9304
	step [127/332], loss=4.0617
	step [128/332], loss=3.7346
	step [129/332], loss=3.9693
	step [130/332], loss=4.6920
	step [131/332], loss=4.2324
	step [132/332], loss=4.6325
	step [133/332], loss=4.2382
	step [134/332], loss=3.7061
	step [135/332], loss=4.5992
	step [136/332], loss=3.9449
	step [137/332], loss=4.3201
	step [138/332], loss=3.5225
	step [139/332], loss=2.9279
	step [140/332], loss=4.2646
	step [141/332], loss=4.2565
	step [142/332], loss=3.5478
	step [143/332], loss=3.3326
	step [144/332], loss=4.2391
	step [145/332], loss=4.5063
	step [146/332], loss=2.8890
	step [147/332], loss=4.6682
	step [148/332], loss=3.4231
	step [149/332], loss=4.4755
	step [150/332], loss=3.8334
	step [151/332], loss=5.6853
	step [152/332], loss=4.4668
	step [153/332], loss=3.3115
	step [154/332], loss=3.6866
	step [155/332], loss=5.0275
	step [156/332], loss=3.7413
	step [157/332], loss=4.4740
	step [158/332], loss=4.1601
	step [159/332], loss=3.8369
	step [160/332], loss=4.9757
	step [161/332], loss=3.9757
	step [162/332], loss=3.7867
	step [163/332], loss=5.0351
	step [164/332], loss=4.6904
	step [165/332], loss=3.8798
	step [166/332], loss=3.0944
	step [167/332], loss=4.5461
	step [168/332], loss=3.5755
	step [169/332], loss=4.2141
	step [170/332], loss=3.4536
	step [171/332], loss=4.0643
	step [172/332], loss=4.5509
	step [173/332], loss=3.9286
	step [174/332], loss=4.7576
	step [175/332], loss=4.8663
	step [176/332], loss=3.5721
	step [177/332], loss=4.6851
	step [178/332], loss=3.9238
	step [179/332], loss=3.8905
	step [180/332], loss=4.5639
	step [181/332], loss=4.8548
	step [182/332], loss=3.9437
	step [183/332], loss=3.5896
	step [184/332], loss=4.9609
	step [185/332], loss=4.1023
	step [186/332], loss=4.5528
	step [187/332], loss=3.5905
	step [188/332], loss=4.3845
	step [189/332], loss=4.5002
	step [190/332], loss=4.2632
	step [191/332], loss=4.3641
	step [192/332], loss=3.6916
	step [193/332], loss=3.8720
	step [194/332], loss=4.7452
	step [195/332], loss=3.9260
	step [196/332], loss=4.3228
	step [197/332], loss=3.9935
	step [198/332], loss=3.9474
	step [199/332], loss=3.6997
	step [200/332], loss=4.3336
	step [201/332], loss=4.3991
	step [202/332], loss=4.4496
	step [203/332], loss=3.7809
	step [204/332], loss=4.1857
	step [205/332], loss=4.2522
	step [206/332], loss=4.6028
	step [207/332], loss=4.5758
	step [208/332], loss=4.4897
	step [209/332], loss=3.8304
	step [210/332], loss=4.3517
	step [211/332], loss=4.0168
	step [212/332], loss=3.3524
	step [213/332], loss=3.7423
	step [214/332], loss=5.1262
	step [215/332], loss=4.3308
	step [216/332], loss=4.8218
	step [217/332], loss=4.1672
	step [218/332], loss=4.2168
	step [219/332], loss=4.3545
	step [220/332], loss=4.1238
	step [221/332], loss=4.6342
	step [222/332], loss=4.8900
	step [223/332], loss=3.4981
	step [224/332], loss=4.5163
	step [225/332], loss=4.1746
	step [226/332], loss=2.7474
	step [227/332], loss=4.0795
	step [228/332], loss=4.2613
	step [229/332], loss=4.5627
	step [230/332], loss=5.6382
	step [231/332], loss=4.3885
	step [232/332], loss=4.4243
	step [233/332], loss=4.1822
	step [234/332], loss=3.7731
	step [235/332], loss=5.0602
	step [236/332], loss=5.0691
	step [237/332], loss=3.3898
	step [238/332], loss=4.6414
	step [239/332], loss=4.2491
	step [240/332], loss=3.9881
	step [241/332], loss=3.0572
	step [242/332], loss=4.3676
	step [243/332], loss=3.9639
	step [244/332], loss=3.9955
	step [245/332], loss=4.0119
	step [246/332], loss=4.4960
	step [247/332], loss=3.8429
	step [248/332], loss=3.9895
	step [249/332], loss=4.2069
	step [250/332], loss=4.3417
	step [251/332], loss=4.6779
	step [252/332], loss=4.6651
	step [253/332], loss=3.7114
	step [254/332], loss=5.2295
	step [255/332], loss=3.9251
	step [256/332], loss=3.3188
	step [257/332], loss=4.4236
	step [258/332], loss=3.8333
	step [259/332], loss=3.8139
	step [260/332], loss=3.6286
	step [261/332], loss=4.0612
	step [262/332], loss=3.7496
	step [263/332], loss=4.2874
	step [264/332], loss=3.9002
	step [265/332], loss=4.5371
	step [266/332], loss=5.3258
	step [267/332], loss=3.0128
	step [268/332], loss=4.2323
	step [269/332], loss=3.6066
	step [270/332], loss=3.9656
	step [271/332], loss=4.8262
	step [272/332], loss=3.6524
	step [273/332], loss=4.3053
	step [274/332], loss=4.0870
	step [275/332], loss=4.3592
	step [276/332], loss=4.3040
	step [277/332], loss=3.4615
	step [278/332], loss=5.1601
	step [279/332], loss=3.9350
	step [280/332], loss=3.9052
	step [281/332], loss=4.3511
	step [282/332], loss=4.0288
	step [283/332], loss=4.2148
	step [284/332], loss=3.6529
	step [285/332], loss=4.7918
	step [286/332], loss=3.9316
	step [287/332], loss=4.7831
	step [288/332], loss=3.7010
	step [289/332], loss=3.6025
	step [290/332], loss=3.4466
	step [291/332], loss=4.7374
	step [292/332], loss=3.3668
	step [293/332], loss=4.0556
	step [294/332], loss=4.4924
	step [295/332], loss=4.2211
	step [296/332], loss=4.0937
	step [297/332], loss=4.5965
	step [298/332], loss=4.5623
	step [299/332], loss=4.1727
	step [300/332], loss=4.3661
	step [301/332], loss=4.1762
	step [302/332], loss=3.4175
	step [303/332], loss=4.5229
	step [304/332], loss=4.2635
	step [305/332], loss=3.4185
	step [306/332], loss=3.5175
	step [307/332], loss=4.2861
	step [308/332], loss=4.3967
	step [309/332], loss=4.7086
	step [310/332], loss=4.9345
	step [311/332], loss=4.1485
	step [312/332], loss=3.5818
	step [313/332], loss=3.7726
	step [314/332], loss=4.5450
	step [315/332], loss=4.7619
	step [316/332], loss=3.8501
	step [317/332], loss=4.7772
	step [318/332], loss=5.4531
	step [319/332], loss=4.0223
	step [320/332], loss=4.6573
	step [321/332], loss=5.1304
	step [322/332], loss=3.6548
	step [323/332], loss=3.9842
	step [324/332], loss=3.9019
	step [325/332], loss=4.8660
	step [326/332], loss=3.4169
	step [327/332], loss=4.2094
	step [328/332], loss=3.9801
	step [329/332], loss=3.9438
	step [330/332], loss=4.8686
	step [331/332], loss=4.3329
	step [332/332], loss=1.9148
	Evaluating
	loss=0.0156, precision=0.2169, recall=0.9939, f1=0.3561
saving model as: 0_saved_model.pth
Training epoch 29
	step [1/332], loss=4.3860
	step [2/332], loss=3.6759
	step [3/332], loss=4.4309
	step [4/332], loss=4.3627
	step [5/332], loss=3.5506
	step [6/332], loss=4.0846
	step [7/332], loss=3.7177
	step [8/332], loss=4.4349
	step [9/332], loss=5.0975
	step [10/332], loss=3.5459
	step [11/332], loss=3.4923
	step [12/332], loss=4.6568
	step [13/332], loss=4.7905
	step [14/332], loss=3.6495
	step [15/332], loss=4.2869
	step [16/332], loss=4.1104
	step [17/332], loss=4.1127
	step [18/332], loss=3.7371
	step [19/332], loss=3.8132
	step [20/332], loss=3.4971
	step [21/332], loss=4.0966
	step [22/332], loss=4.0575
	step [23/332], loss=5.2942
	step [24/332], loss=5.0993
	step [25/332], loss=4.8438
	step [26/332], loss=5.2102
	step [27/332], loss=4.1964
	step [28/332], loss=4.4249
	step [29/332], loss=4.7842
	step [30/332], loss=4.0019
	step [31/332], loss=3.4715
	step [32/332], loss=3.5997
	step [33/332], loss=4.2942
	step [34/332], loss=4.0591
	step [35/332], loss=3.9165
	step [36/332], loss=4.2462
	step [37/332], loss=4.3522
	step [38/332], loss=4.1732
	step [39/332], loss=3.7324
	step [40/332], loss=4.3033
	step [41/332], loss=5.4125
	step [42/332], loss=5.4122
	step [43/332], loss=4.6908
	step [44/332], loss=3.6929
	step [45/332], loss=3.8051
	step [46/332], loss=4.6281
	step [47/332], loss=3.9196
	step [48/332], loss=4.9980
	step [49/332], loss=4.1404
	step [50/332], loss=2.9691
	step [51/332], loss=3.1797
	step [52/332], loss=3.5747
	step [53/332], loss=3.4178
	step [54/332], loss=4.3191
	step [55/332], loss=3.4427
	step [56/332], loss=5.0461
	step [57/332], loss=3.4685
	step [58/332], loss=4.3660
	step [59/332], loss=4.2710
	step [60/332], loss=4.5136
	step [61/332], loss=4.8783
	step [62/332], loss=3.7974
	step [63/332], loss=4.0393
	step [64/332], loss=5.1859
	step [65/332], loss=4.9468
	step [66/332], loss=5.7026
	step [67/332], loss=4.0338
	step [68/332], loss=4.6561
	step [69/332], loss=4.2282
	step [70/332], loss=2.9307
	step [71/332], loss=4.4104
	step [72/332], loss=3.4033
	step [73/332], loss=3.7151
	step [74/332], loss=5.5921
	step [75/332], loss=4.5292
	step [76/332], loss=4.4496
	step [77/332], loss=4.7213
	step [78/332], loss=4.8621
	step [79/332], loss=4.6104
	step [80/332], loss=4.7998
	step [81/332], loss=4.2097
	step [82/332], loss=4.2540
	step [83/332], loss=4.1802
	step [84/332], loss=3.0418
	step [85/332], loss=4.7288
	step [86/332], loss=5.5312
	step [87/332], loss=4.1986
	step [88/332], loss=2.9872
	step [89/332], loss=4.6221
	step [90/332], loss=4.3596
	step [91/332], loss=3.5266
	step [92/332], loss=4.7289
	step [93/332], loss=3.8228
	step [94/332], loss=3.9096
	step [95/332], loss=3.4281
	step [96/332], loss=4.8109
	step [97/332], loss=3.8949
	step [98/332], loss=3.6357
	step [99/332], loss=3.5186
	step [100/332], loss=3.5205
	step [101/332], loss=4.4279
	step [102/332], loss=4.6879
	step [103/332], loss=4.8506
	step [104/332], loss=3.8943
	step [105/332], loss=3.1954
	step [106/332], loss=3.6358
	step [107/332], loss=4.3308
	step [108/332], loss=4.6016
	step [109/332], loss=4.0243
	step [110/332], loss=4.4336
	step [111/332], loss=4.6313
	step [112/332], loss=3.6979
	step [113/332], loss=4.4409
	step [114/332], loss=4.7929
	step [115/332], loss=4.0528
	step [116/332], loss=3.5598
	step [117/332], loss=4.1766
	step [118/332], loss=3.2586
	step [119/332], loss=4.7923
	step [120/332], loss=4.1705
	step [121/332], loss=5.0175
	step [122/332], loss=4.4234
	step [123/332], loss=4.9464
	step [124/332], loss=4.8063
	step [125/332], loss=4.0475
	step [126/332], loss=4.4605
	step [127/332], loss=3.6731
	step [128/332], loss=4.4342
	step [129/332], loss=4.9059
	step [130/332], loss=4.1696
	step [131/332], loss=5.1896
	step [132/332], loss=3.1729
	step [133/332], loss=4.1280
	step [134/332], loss=4.3944
	step [135/332], loss=3.4213
	step [136/332], loss=3.7383
	step [137/332], loss=3.7756
	step [138/332], loss=4.3922
	step [139/332], loss=4.9156
	step [140/332], loss=4.3169
	step [141/332], loss=3.8117
	step [142/332], loss=4.0549
	step [143/332], loss=4.4560
	step [144/332], loss=3.8446
	step [145/332], loss=3.4087
	step [146/332], loss=3.6720
	step [147/332], loss=4.9385
	step [148/332], loss=3.3860
	step [149/332], loss=3.8245
	step [150/332], loss=4.6196
	step [151/332], loss=3.6514
	step [152/332], loss=3.1004
	step [153/332], loss=4.1330
	step [154/332], loss=4.3025
	step [155/332], loss=3.3020
	step [156/332], loss=4.9291
	step [157/332], loss=4.2776
	step [158/332], loss=3.7107
	step [159/332], loss=3.4735
	step [160/332], loss=3.6735
	step [161/332], loss=4.2464
	step [162/332], loss=3.2580
	step [163/332], loss=4.0181
	step [164/332], loss=3.2888
	step [165/332], loss=4.1640
	step [166/332], loss=4.5291
	step [167/332], loss=4.3491
	step [168/332], loss=4.4460
	step [169/332], loss=3.4276
	step [170/332], loss=3.2792
	step [171/332], loss=4.9669
	step [172/332], loss=3.7196
	step [173/332], loss=4.3669
	step [174/332], loss=5.5657
	step [175/332], loss=3.6679
	step [176/332], loss=3.7359
	step [177/332], loss=4.9088
	step [178/332], loss=3.7142
	step [179/332], loss=3.7143
	step [180/332], loss=4.1914
	step [181/332], loss=3.7011
	step [182/332], loss=3.9516
	step [183/332], loss=4.4397
	step [184/332], loss=4.5843
	step [185/332], loss=3.8402
	step [186/332], loss=2.8800
	step [187/332], loss=3.7873
	step [188/332], loss=4.4552
	step [189/332], loss=3.7963
	step [190/332], loss=4.2048
	step [191/332], loss=3.9163
	step [192/332], loss=3.7492
	step [193/332], loss=4.1282
	step [194/332], loss=3.6650
	step [195/332], loss=3.6764
	step [196/332], loss=5.1835
	step [197/332], loss=5.8704
	step [198/332], loss=4.0240
	step [199/332], loss=3.8371
	step [200/332], loss=4.4143
	step [201/332], loss=4.0241
	step [202/332], loss=3.7363
	step [203/332], loss=4.3783
	step [204/332], loss=3.9250
	step [205/332], loss=3.4579
	step [206/332], loss=3.0181
	step [207/332], loss=3.7460
	step [208/332], loss=3.2281
	step [209/332], loss=4.4214
	step [210/332], loss=4.3983
	step [211/332], loss=4.5992
	step [212/332], loss=4.3375
	step [213/332], loss=4.2739
	step [214/332], loss=4.0809
	step [215/332], loss=3.1752
	step [216/332], loss=3.1158
	step [217/332], loss=3.9771
	step [218/332], loss=3.7226
	step [219/332], loss=3.6617
	step [220/332], loss=2.7263
	step [221/332], loss=4.4099
	step [222/332], loss=5.0195
	step [223/332], loss=4.9760
	step [224/332], loss=4.0168
	step [225/332], loss=4.0670
	step [226/332], loss=3.0934
	step [227/332], loss=4.3525
	step [228/332], loss=4.1621
	step [229/332], loss=3.8670
	step [230/332], loss=3.6029
	step [231/332], loss=4.7628
	step [232/332], loss=4.4635
	step [233/332], loss=4.8312
	step [234/332], loss=3.6726
	step [235/332], loss=4.1492
	step [236/332], loss=4.3489
	step [237/332], loss=3.3511
	step [238/332], loss=4.0434
	step [239/332], loss=4.3300
	step [240/332], loss=3.3045
	step [241/332], loss=4.3864
	step [242/332], loss=3.9942
	step [243/332], loss=3.4832
	step [244/332], loss=4.2578
	step [245/332], loss=4.8539
	step [246/332], loss=3.0438
	step [247/332], loss=4.4274
	step [248/332], loss=4.6061
	step [249/332], loss=3.3348
	step [250/332], loss=4.4623
	step [251/332], loss=4.3564
	step [252/332], loss=3.8552
	step [253/332], loss=4.3475
	step [254/332], loss=3.7586
	step [255/332], loss=3.7020
	step [256/332], loss=4.6507
	step [257/332], loss=3.9902
	step [258/332], loss=3.1857
	step [259/332], loss=3.7058
	step [260/332], loss=4.4432
	step [261/332], loss=4.0902
	step [262/332], loss=3.8507
	step [263/332], loss=4.2509
	step [264/332], loss=4.1323
	step [265/332], loss=4.1631
	step [266/332], loss=3.9785
	step [267/332], loss=4.1526
	step [268/332], loss=3.9216
	step [269/332], loss=3.6627
	step [270/332], loss=3.4988
	step [271/332], loss=4.2931
	step [272/332], loss=4.2097
	step [273/332], loss=4.2799
	step [274/332], loss=4.1926
	step [275/332], loss=3.2578
	step [276/332], loss=3.8714
	step [277/332], loss=3.3996
	step [278/332], loss=3.9426
	step [279/332], loss=3.6183
	step [280/332], loss=3.3717
	step [281/332], loss=3.9174
	step [282/332], loss=3.6750
	step [283/332], loss=3.9910
	step [284/332], loss=3.7236
	step [285/332], loss=4.4029
	step [286/332], loss=4.0258
	step [287/332], loss=3.5682
	step [288/332], loss=3.5977
	step [289/332], loss=4.5354
	step [290/332], loss=4.4185
	step [291/332], loss=4.2262
	step [292/332], loss=3.9802
	step [293/332], loss=3.6613
	step [294/332], loss=4.0317
	step [295/332], loss=4.8739
	step [296/332], loss=4.7824
	step [297/332], loss=4.8354
	step [298/332], loss=4.1712
	step [299/332], loss=4.4117
	step [300/332], loss=3.6622
	step [301/332], loss=5.2564
	step [302/332], loss=3.3564
	step [303/332], loss=3.5796
	step [304/332], loss=3.0583
	step [305/332], loss=4.3610
	step [306/332], loss=4.1766
	step [307/332], loss=3.5326
	step [308/332], loss=4.2895
	step [309/332], loss=3.5444
	step [310/332], loss=4.6543
	step [311/332], loss=4.0983
	step [312/332], loss=3.7689
	step [313/332], loss=4.5243
	step [314/332], loss=4.3559
	step [315/332], loss=3.9491
	step [316/332], loss=3.5368
	step [317/332], loss=3.8921
	step [318/332], loss=4.6198
	step [319/332], loss=3.9642
	step [320/332], loss=5.3654
	step [321/332], loss=4.4224
	step [322/332], loss=4.8304
	step [323/332], loss=3.6877
	step [324/332], loss=3.6584
	step [325/332], loss=4.2873
	step [326/332], loss=4.7415
	step [327/332], loss=4.4513
	step [328/332], loss=3.9668
	step [329/332], loss=3.5938
	step [330/332], loss=4.7433
	step [331/332], loss=3.4429
	step [332/332], loss=2.8163
	Evaluating
	loss=0.0170, precision=0.1967, recall=0.9952, f1=0.3285
Training epoch 30
	step [1/332], loss=3.9269
	step [2/332], loss=3.8418
	step [3/332], loss=3.9749
	step [4/332], loss=3.6608
	step [5/332], loss=3.9728
	step [6/332], loss=3.5277
	step [7/332], loss=4.3362
	step [8/332], loss=4.6329
	step [9/332], loss=3.6396
	step [10/332], loss=3.6829
	step [11/332], loss=4.7990
	step [12/332], loss=4.2855
	step [13/332], loss=3.3665
	step [14/332], loss=4.2587
	step [15/332], loss=3.6449
	step [16/332], loss=3.7491
	step [17/332], loss=3.9898
	step [18/332], loss=3.6783
	step [19/332], loss=2.9982
	step [20/332], loss=3.9044
	step [21/332], loss=5.7880
	step [22/332], loss=5.2421
	step [23/332], loss=4.8287
	step [24/332], loss=4.6544
	step [25/332], loss=5.0571
	step [26/332], loss=4.2746
	step [27/332], loss=3.7332
	step [28/332], loss=4.7630
	step [29/332], loss=4.2954
	step [30/332], loss=4.2418
	step [31/332], loss=4.1112
	step [32/332], loss=3.7727
	step [33/332], loss=4.0648
	step [34/332], loss=4.0968
	step [35/332], loss=3.7885
	step [36/332], loss=3.2742
	step [37/332], loss=3.8423
	step [38/332], loss=4.2826
	step [39/332], loss=4.2581
	step [40/332], loss=5.0316
	step [41/332], loss=3.8542
	step [42/332], loss=4.8913
	step [43/332], loss=4.1880
	step [44/332], loss=4.7094
	step [45/332], loss=4.2814
	step [46/332], loss=4.6655
	step [47/332], loss=4.1465
	step [48/332], loss=3.0784
	step [49/332], loss=4.1639
	step [50/332], loss=4.8722
	step [51/332], loss=3.9124
	step [52/332], loss=3.5766
	step [53/332], loss=5.2037
	step [54/332], loss=3.5917
	step [55/332], loss=3.6026
	step [56/332], loss=3.0392
	step [57/332], loss=4.1879
	step [58/332], loss=3.3474
	step [59/332], loss=3.2420
	step [60/332], loss=3.9661
	step [61/332], loss=5.5754
	step [62/332], loss=4.2542
	step [63/332], loss=4.4933
	step [64/332], loss=3.1748
	step [65/332], loss=4.6206
	step [66/332], loss=4.2292
	step [67/332], loss=4.3735
	step [68/332], loss=4.2781
	step [69/332], loss=4.5882
	step [70/332], loss=3.5234
	step [71/332], loss=3.5385
	step [72/332], loss=4.0804
	step [73/332], loss=3.3503
	step [74/332], loss=5.2432
	step [75/332], loss=4.7195
	step [76/332], loss=4.4610
	step [77/332], loss=3.9914
	step [78/332], loss=3.6826
	step [79/332], loss=2.7871
	step [80/332], loss=3.6407
	step [81/332], loss=4.0657
	step [82/332], loss=3.3280
	step [83/332], loss=4.4424
	step [84/332], loss=3.5585
	step [85/332], loss=4.7855
	step [86/332], loss=3.4213
	step [87/332], loss=2.9798
	step [88/332], loss=4.8851
	step [89/332], loss=4.1265
	step [90/332], loss=3.9059
	step [91/332], loss=3.7152
	step [92/332], loss=3.2652
	step [93/332], loss=3.5346
	step [94/332], loss=4.0688
	step [95/332], loss=4.0399
	step [96/332], loss=4.4497
	step [97/332], loss=4.2447
	step [98/332], loss=5.0327
	step [99/332], loss=3.7907
	step [100/332], loss=3.8114
	step [101/332], loss=6.2831
	step [102/332], loss=4.4641
	step [103/332], loss=4.9300
	step [104/332], loss=4.3368
	step [105/332], loss=3.3805
	step [106/332], loss=3.5267
	step [107/332], loss=4.4396
	step [108/332], loss=5.1987
	step [109/332], loss=3.9401
	step [110/332], loss=5.5367
	step [111/332], loss=4.1310
	step [112/332], loss=4.0833
	step [113/332], loss=3.5172
	step [114/332], loss=3.8798
	step [115/332], loss=3.9800
	step [116/332], loss=3.7064
	step [117/332], loss=5.1789
	step [118/332], loss=4.4582
	step [119/332], loss=4.6433
	step [120/332], loss=3.8788
	step [121/332], loss=2.9499
	step [122/332], loss=4.8341
	step [123/332], loss=4.1458
	step [124/332], loss=4.9064
	step [125/332], loss=3.3162
	step [126/332], loss=4.6038
	step [127/332], loss=3.0283
	step [128/332], loss=3.2539
	step [129/332], loss=4.8253
	step [130/332], loss=4.4194
	step [131/332], loss=4.1594
	step [132/332], loss=4.5675
	step [133/332], loss=4.5314
	step [134/332], loss=4.4198
	step [135/332], loss=4.6368
	step [136/332], loss=3.6579
	step [137/332], loss=3.5390
	step [138/332], loss=3.8455
	step [139/332], loss=4.0795
	step [140/332], loss=3.7800
	step [141/332], loss=4.5647
	step [142/332], loss=5.6063
	step [143/332], loss=4.0688
	step [144/332], loss=4.0510
	step [145/332], loss=3.7349
	step [146/332], loss=4.4118
	step [147/332], loss=5.3462
	step [148/332], loss=3.9297
	step [149/332], loss=3.3109
	step [150/332], loss=3.5857
	step [151/332], loss=4.7028
	step [152/332], loss=4.7601
	step [153/332], loss=3.6221
	step [154/332], loss=3.9832
	step [155/332], loss=4.7907
	step [156/332], loss=3.1278
	step [157/332], loss=3.5241
	step [158/332], loss=4.2892
	step [159/332], loss=4.3195
	step [160/332], loss=4.5921
	step [161/332], loss=3.9203
	step [162/332], loss=3.9814
	step [163/332], loss=4.2399
	step [164/332], loss=3.9700
	step [165/332], loss=2.8872
	step [166/332], loss=4.5175
	step [167/332], loss=4.1204
	step [168/332], loss=3.8240
	step [169/332], loss=2.8747
	step [170/332], loss=3.3097
	step [171/332], loss=3.7858
	step [172/332], loss=4.8439
	step [173/332], loss=5.3044
	step [174/332], loss=3.7006
	step [175/332], loss=5.1685
	step [176/332], loss=3.1461
	step [177/332], loss=3.8791
	step [178/332], loss=3.6571
	step [179/332], loss=3.9873
	step [180/332], loss=3.0594
	step [181/332], loss=4.7417
	step [182/332], loss=3.6843
	step [183/332], loss=3.3087
	step [184/332], loss=3.8804
	step [185/332], loss=3.6553
	step [186/332], loss=4.1081
	step [187/332], loss=3.4283
	step [188/332], loss=2.5376
	step [189/332], loss=4.2101
	step [190/332], loss=3.7108
	step [191/332], loss=3.1684
	step [192/332], loss=4.2380
	step [193/332], loss=4.5141
	step [194/332], loss=5.4683
	step [195/332], loss=4.9785
	step [196/332], loss=4.1552
	step [197/332], loss=3.9929
	step [198/332], loss=3.8217
	step [199/332], loss=3.7682
	step [200/332], loss=3.3518
	step [201/332], loss=4.0387
	step [202/332], loss=4.5213
	step [203/332], loss=3.4067
	step [204/332], loss=4.1403
	step [205/332], loss=4.4279
	step [206/332], loss=4.4237
	step [207/332], loss=3.8286
	step [208/332], loss=3.4423
	step [209/332], loss=4.1834
	step [210/332], loss=4.2445
	step [211/332], loss=3.3881
	step [212/332], loss=4.1289
	step [213/332], loss=4.2205
	step [214/332], loss=3.6976
	step [215/332], loss=3.2533
	step [216/332], loss=3.9553
	step [217/332], loss=3.3405
	step [218/332], loss=4.2033
	step [219/332], loss=4.5486
	step [220/332], loss=3.7115
	step [221/332], loss=2.9524
	step [222/332], loss=3.8890
	step [223/332], loss=3.6903
	step [224/332], loss=4.2614
	step [225/332], loss=4.2140
	step [226/332], loss=4.1540
	step [227/332], loss=4.6961
	step [228/332], loss=3.3681
	step [229/332], loss=3.6442
	step [230/332], loss=3.6880
	step [231/332], loss=3.9422
	step [232/332], loss=3.6629
	step [233/332], loss=4.0900
	step [234/332], loss=3.2776
	step [235/332], loss=4.5542
	step [236/332], loss=3.4241
	step [237/332], loss=3.8671
	step [238/332], loss=3.8884
	step [239/332], loss=4.3690
	step [240/332], loss=3.4217
	step [241/332], loss=3.5929
	step [242/332], loss=3.2047
	step [243/332], loss=3.6354
	step [244/332], loss=3.9171
	step [245/332], loss=3.9167
	step [246/332], loss=4.2778
	step [247/332], loss=3.2227
	step [248/332], loss=3.3546
	step [249/332], loss=3.9397
	step [250/332], loss=3.8630
	step [251/332], loss=4.3659
	step [252/332], loss=5.3798
	step [253/332], loss=3.3541
	step [254/332], loss=2.8150
	step [255/332], loss=3.4841
	step [256/332], loss=3.6528
	step [257/332], loss=4.2788
	step [258/332], loss=3.6390
	step [259/332], loss=3.4954
	step [260/332], loss=3.6251
	step [261/332], loss=3.4369
	step [262/332], loss=3.6063
	step [263/332], loss=4.1442
	step [264/332], loss=4.3777
	step [265/332], loss=4.5911
	step [266/332], loss=4.3700
	step [267/332], loss=4.2120
	step [268/332], loss=4.1507
	step [269/332], loss=3.5945
	step [270/332], loss=4.4863
	step [271/332], loss=3.1878
	step [272/332], loss=4.0165
	step [273/332], loss=3.6023
	step [274/332], loss=3.9701
	step [275/332], loss=3.5837
	step [276/332], loss=4.0885
	step [277/332], loss=3.7311
	step [278/332], loss=3.9765
	step [279/332], loss=3.8463
	step [280/332], loss=3.5326
	step [281/332], loss=4.0438
	step [282/332], loss=3.0893
	step [283/332], loss=5.4259
	step [284/332], loss=3.4993
	step [285/332], loss=4.0710
	step [286/332], loss=4.1917
	step [287/332], loss=4.0993
	step [288/332], loss=4.8302
	step [289/332], loss=4.1577
	step [290/332], loss=3.5086
	step [291/332], loss=4.2569
	step [292/332], loss=3.8961
	step [293/332], loss=4.3143
	step [294/332], loss=3.9288
	step [295/332], loss=4.1794
	step [296/332], loss=3.7610
	step [297/332], loss=3.8136
	step [298/332], loss=4.4094
	step [299/332], loss=4.5188
	step [300/332], loss=3.7121
	step [301/332], loss=4.5950
	step [302/332], loss=3.6890
	step [303/332], loss=3.0193
	step [304/332], loss=4.1995
	step [305/332], loss=3.7573
	step [306/332], loss=5.4415
	step [307/332], loss=3.1754
	step [308/332], loss=4.3234
	step [309/332], loss=4.4884
	step [310/332], loss=3.6990
	step [311/332], loss=3.3409
	step [312/332], loss=3.7796
	step [313/332], loss=3.3704
	step [314/332], loss=3.5328
	step [315/332], loss=3.8959
	step [316/332], loss=3.1449
	step [317/332], loss=3.2463
	step [318/332], loss=3.1472
	step [319/332], loss=3.7030
	step [320/332], loss=3.9928
	step [321/332], loss=3.8072
	step [322/332], loss=4.3370
	step [323/332], loss=2.8417
	step [324/332], loss=3.0702
	step [325/332], loss=3.7694
	step [326/332], loss=4.9774
	step [327/332], loss=3.8567
	step [328/332], loss=4.0686
	step [329/332], loss=3.4946
	step [330/332], loss=3.9579
	step [331/332], loss=3.6580
	step [332/332], loss=2.6412
	Evaluating
	loss=0.0147, precision=0.2255, recall=0.9938, f1=0.3676
saving model as: 0_saved_model.pth
Training finished
best_f1: 0.3676038888550433
directing: Z rim_enhanced: False test_id 0
removed wrong scan: weights_Z_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_292_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_299_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_58_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_265_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_271_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_300_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_206_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_252_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_294_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_46_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_291_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_298_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_276_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_262_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_293_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_295_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_278_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_87_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_214_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_297_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_154_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_282_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_218_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_254_xwqg-B00034_2020-04-03.npy
# all image files: 15579 # all weight files in weight_dir: 12263 # image files with weight 12232
removed wrong scan: weights_Z_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_292_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_299_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_58_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_265_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_271_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_300_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_206_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_252_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_294_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_46_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_291_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_298_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_276_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_262_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_293_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_295_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_278_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_87_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_214_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_297_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_154_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_282_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_218_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_254_xwqg-B00034_2020-04-03.npy
# all image files: 15579 # all weight files in weight_dir: 3256 # image files with weight 3252
train: /ibex/scratch/projects/c2052/air_tube_seg/training_samples_enhanced/Z 12232
Using 4 GPUs
Going to train epochs [1-30]
Training epoch 1
	step [1/255], loss=258.1904
	step [2/255], loss=167.5998
	step [3/255], loss=137.8649
	step [4/255], loss=135.0205
	step [5/255], loss=132.2360
	step [6/255], loss=129.1608
	step [7/255], loss=125.5876
	step [8/255], loss=124.3108
	step [9/255], loss=123.3539
	step [10/255], loss=121.6078
	step [11/255], loss=120.3615
	step [12/255], loss=118.4846
	step [13/255], loss=116.9319
	step [14/255], loss=116.4278
	step [15/255], loss=115.1308
	step [16/255], loss=114.3679
	step [17/255], loss=112.5894
	step [18/255], loss=111.0328
	step [19/255], loss=111.4975
	step [20/255], loss=110.0394
	step [21/255], loss=108.0639
	step [22/255], loss=108.3329
	step [23/255], loss=105.7441
	step [24/255], loss=105.3698
	step [25/255], loss=104.0986
	step [26/255], loss=103.4779
	step [27/255], loss=101.9646
	step [28/255], loss=101.1554
	step [29/255], loss=100.5403
	step [30/255], loss=100.4163
	step [31/255], loss=100.3404
	step [32/255], loss=98.4106
	step [33/255], loss=97.2463
	step [34/255], loss=97.4751
	step [35/255], loss=94.5714
	step [36/255], loss=94.0511
	step [37/255], loss=94.4982
	step [38/255], loss=92.8667
	step [39/255], loss=94.6624
	step [40/255], loss=94.3158
	step [41/255], loss=92.5136
	step [42/255], loss=91.8422
	step [43/255], loss=89.4297
	step [44/255], loss=89.3519
	step [45/255], loss=87.6982
	step [46/255], loss=89.4585
	step [47/255], loss=90.0805
	step [48/255], loss=87.6273
	step [49/255], loss=86.1764
	step [50/255], loss=88.0578
	step [51/255], loss=86.0676
	step [52/255], loss=85.0434
	step [53/255], loss=84.9287
	step [54/255], loss=85.6481
	step [55/255], loss=83.7365
	step [56/255], loss=83.0637
	step [57/255], loss=84.2160
	step [58/255], loss=81.5120
	step [59/255], loss=81.4491
	step [60/255], loss=81.9466
	step [61/255], loss=81.4570
	step [62/255], loss=80.2624
	step [63/255], loss=80.0556
	step [64/255], loss=80.2484
	step [65/255], loss=79.1433
	step [66/255], loss=78.4927
	step [67/255], loss=78.6783
	step [68/255], loss=78.7113
	step [69/255], loss=77.3642
	step [70/255], loss=78.1206
	step [71/255], loss=77.6863
	step [72/255], loss=75.9795
	step [73/255], loss=76.2047
	step [74/255], loss=75.2424
	step [75/255], loss=76.3553
	step [76/255], loss=76.2037
	step [77/255], loss=73.7073
	step [78/255], loss=76.1790
	step [79/255], loss=75.0490
	step [80/255], loss=74.1308
	step [81/255], loss=75.1506
	step [82/255], loss=72.8380
	step [83/255], loss=73.5612
	step [84/255], loss=73.4103
	step [85/255], loss=73.2484
	step [86/255], loss=71.2289
	step [87/255], loss=72.3125
	step [88/255], loss=72.7352
	step [89/255], loss=72.1071
	step [90/255], loss=71.3440
	step [91/255], loss=70.5321
	step [92/255], loss=70.6515
	step [93/255], loss=70.4000
	step [94/255], loss=70.9885
	step [95/255], loss=70.4084
	step [96/255], loss=72.0946
	step [97/255], loss=71.1756
	step [98/255], loss=71.1260
	step [99/255], loss=70.6969
	step [100/255], loss=68.7676
	step [101/255], loss=67.6385
	step [102/255], loss=69.7518
	step [103/255], loss=67.7762
	step [104/255], loss=69.2411
	step [105/255], loss=69.5239
	step [106/255], loss=68.2862
	step [107/255], loss=68.3491
	step [108/255], loss=68.1029
	step [109/255], loss=70.2875
	step [110/255], loss=67.6511
	step [111/255], loss=66.1388
	step [112/255], loss=68.4862
	step [113/255], loss=66.8498
	step [114/255], loss=68.3295
	step [115/255], loss=66.4419
	step [116/255], loss=67.5530
	step [117/255], loss=66.2871
	step [118/255], loss=67.4297
	step [119/255], loss=66.3976
	step [120/255], loss=64.5500
	step [121/255], loss=65.7863
	step [122/255], loss=66.8192
	step [123/255], loss=66.4764
	step [124/255], loss=65.4499
	step [125/255], loss=65.2041
	step [126/255], loss=65.5614
	step [127/255], loss=64.8478
	step [128/255], loss=64.5455
	step [129/255], loss=63.7584
	step [130/255], loss=64.9516
	step [131/255], loss=64.5422
	step [132/255], loss=65.2097
	step [133/255], loss=64.3994
	step [134/255], loss=65.2545
	step [135/255], loss=63.8531
	step [136/255], loss=65.3071
	step [137/255], loss=63.2715
	step [138/255], loss=62.8796
	step [139/255], loss=65.3816
	step [140/255], loss=65.7265
	step [141/255], loss=65.7501
	step [142/255], loss=63.2566
	step [143/255], loss=63.7714
	step [144/255], loss=62.4228
	step [145/255], loss=62.1105
	step [146/255], loss=63.0408
	step [147/255], loss=61.8667
	step [148/255], loss=61.8540
	step [149/255], loss=60.7631
	step [150/255], loss=62.9224
	step [151/255], loss=61.9962
	step [152/255], loss=63.7066
	step [153/255], loss=61.9913
	step [154/255], loss=63.1040
	step [155/255], loss=62.8878
	step [156/255], loss=61.2210
	step [157/255], loss=61.7545
	step [158/255], loss=64.2007
	step [159/255], loss=60.0549
	step [160/255], loss=60.4391
	step [161/255], loss=61.0029
	step [162/255], loss=61.6674
	step [163/255], loss=60.6643
	step [164/255], loss=60.4444
	step [165/255], loss=60.7302
	step [166/255], loss=61.4947
	step [167/255], loss=63.7363
	step [168/255], loss=59.9708
	step [169/255], loss=62.1038
	step [170/255], loss=59.5305
	step [171/255], loss=60.7111
	step [172/255], loss=59.4261
	step [173/255], loss=60.2575
	step [174/255], loss=58.0941
	step [175/255], loss=59.9824
	step [176/255], loss=59.1590
	step [177/255], loss=59.0526
	step [178/255], loss=59.3988
	step [179/255], loss=60.2025
	step [180/255], loss=58.3369
	step [181/255], loss=59.1585
	step [182/255], loss=59.8919
	step [183/255], loss=58.7215
	step [184/255], loss=59.7340
	step [185/255], loss=59.4993
	step [186/255], loss=59.7757
	step [187/255], loss=59.3572
	step [188/255], loss=57.6995
	step [189/255], loss=57.4329
	step [190/255], loss=56.7911
	step [191/255], loss=57.0720
	step [192/255], loss=57.7675
	step [193/255], loss=56.3624
	step [194/255], loss=59.5211
	step [195/255], loss=59.2651
	step [196/255], loss=58.0758
	step [197/255], loss=58.6967
	step [198/255], loss=57.0522
	step [199/255], loss=57.2046
	step [200/255], loss=58.2831
	step [201/255], loss=55.1505
	step [202/255], loss=57.8752
	step [203/255], loss=56.2693
	step [204/255], loss=55.5265
	step [205/255], loss=55.8510
	step [206/255], loss=55.6921
	step [207/255], loss=56.0906
	step [208/255], loss=56.0139
	step [209/255], loss=55.9830
	step [210/255], loss=57.7592
	step [211/255], loss=56.6314
	step [212/255], loss=56.6270
	step [213/255], loss=57.1112
	step [214/255], loss=56.5702
	step [215/255], loss=58.3414
	step [216/255], loss=57.0095
	step [217/255], loss=57.8401
	step [218/255], loss=57.6026
	step [219/255], loss=55.3848
	step [220/255], loss=54.6522
	step [221/255], loss=58.3833
	step [222/255], loss=54.9913
	step [223/255], loss=56.0202
	step [224/255], loss=54.6334
	step [225/255], loss=55.4063
	step [226/255], loss=55.7310
	step [227/255], loss=53.6677
	step [228/255], loss=56.8551
	step [229/255], loss=55.1823
	step [230/255], loss=52.9845
	step [231/255], loss=55.4062
	step [232/255], loss=53.2322
	step [233/255], loss=55.1376
	step [234/255], loss=52.9235
	step [235/255], loss=54.7481
	step [236/255], loss=53.1983
	step [237/255], loss=54.6252
	step [238/255], loss=54.2499
	step [239/255], loss=55.7502
	step [240/255], loss=52.4316
	step [241/255], loss=54.7986
	step [242/255], loss=53.8756
	step [243/255], loss=53.9232
	step [244/255], loss=53.5157
	step [245/255], loss=52.9005
	step [246/255], loss=53.8813
	step [247/255], loss=56.8815
	step [248/255], loss=54.5477
	step [249/255], loss=54.3840
	step [250/255], loss=54.3384
	step [251/255], loss=53.9100
	step [252/255], loss=54.2484
	step [253/255], loss=54.3783
	step [254/255], loss=53.7858
	step [255/255], loss=43.3072
	Evaluating
	loss=0.2679, precision=0.1435, recall=0.9960, f1=0.2509
saving model as: 0_saved_model.pth
Training epoch 2
	step [1/255], loss=52.2171
	step [2/255], loss=55.3881
	step [3/255], loss=52.3430
	step [4/255], loss=51.0051
	step [5/255], loss=52.3978
	step [6/255], loss=52.4639
	step [7/255], loss=53.0360
	step [8/255], loss=51.1316
	step [9/255], loss=51.3628
	step [10/255], loss=52.8114
	step [11/255], loss=52.7507
	step [12/255], loss=51.4466
	step [13/255], loss=53.7908
	step [14/255], loss=51.0438
	step [15/255], loss=50.8665
	step [16/255], loss=51.6738
	step [17/255], loss=51.6324
	step [18/255], loss=51.4747
	step [19/255], loss=51.2997
	step [20/255], loss=52.7901
	step [21/255], loss=51.6880
	step [22/255], loss=53.2421
	step [23/255], loss=50.5461
	step [24/255], loss=49.8660
	step [25/255], loss=50.9341
	step [26/255], loss=50.6572
	step [27/255], loss=52.5489
	step [28/255], loss=49.2817
	step [29/255], loss=51.2454
	step [30/255], loss=51.0036
	step [31/255], loss=50.0376
	step [32/255], loss=48.7024
	step [33/255], loss=52.2375
	step [34/255], loss=50.4088
	step [35/255], loss=51.7110
	step [36/255], loss=51.6041
	step [37/255], loss=51.5617
	step [38/255], loss=50.7535
	step [39/255], loss=49.5120
	step [40/255], loss=49.9968
	step [41/255], loss=51.6057
	step [42/255], loss=48.7207
	step [43/255], loss=49.0272
	step [44/255], loss=49.1035
	step [45/255], loss=52.3408
	step [46/255], loss=49.4118
	step [47/255], loss=50.5565
	step [48/255], loss=51.7917
	step [49/255], loss=48.8310
	step [50/255], loss=51.1320
	step [51/255], loss=50.8412
	step [52/255], loss=50.7571
	step [53/255], loss=51.2047
	step [54/255], loss=48.5774
	step [55/255], loss=48.3479
	step [56/255], loss=47.0268
	step [57/255], loss=48.2184
	step [58/255], loss=48.0008
	step [59/255], loss=48.2552
	step [60/255], loss=47.3889
	step [61/255], loss=49.1423
	step [62/255], loss=47.9599
	step [63/255], loss=48.6896
	step [64/255], loss=49.6739
	step [65/255], loss=48.2924
	step [66/255], loss=46.4729
	step [67/255], loss=48.4816
	step [68/255], loss=47.1747
	step [69/255], loss=48.0294
	step [70/255], loss=48.4303
	step [71/255], loss=49.6177
	step [72/255], loss=47.8964
	step [73/255], loss=48.4433
	step [74/255], loss=49.5329
	step [75/255], loss=48.6893
	step [76/255], loss=47.7701
	step [77/255], loss=47.9058
	step [78/255], loss=48.1597
	step [79/255], loss=46.5004
	step [80/255], loss=47.1321
	step [81/255], loss=46.1289
	step [82/255], loss=46.6947
	step [83/255], loss=47.5218
	step [84/255], loss=46.7960
	step [85/255], loss=48.4788
	step [86/255], loss=46.6244
	step [87/255], loss=47.1058
	step [88/255], loss=47.7380
	step [89/255], loss=46.0626
	step [90/255], loss=46.3913
	step [91/255], loss=46.8110
	step [92/255], loss=44.6745
	step [93/255], loss=46.2062
	step [94/255], loss=45.7709
	step [95/255], loss=45.7327
	step [96/255], loss=44.2141
	step [97/255], loss=46.2602
	step [98/255], loss=44.6721
	step [99/255], loss=45.5503
	step [100/255], loss=46.3781
	step [101/255], loss=46.6761
	step [102/255], loss=45.4622
	step [103/255], loss=46.7746
	step [104/255], loss=43.8965
	step [105/255], loss=44.5475
	step [106/255], loss=46.1880
	step [107/255], loss=44.4851
	step [108/255], loss=44.5567
	step [109/255], loss=45.5232
	step [110/255], loss=46.1308
	step [111/255], loss=44.1511
	step [112/255], loss=43.9618
	step [113/255], loss=44.9253
	step [114/255], loss=43.9231
	step [115/255], loss=43.9778
	step [116/255], loss=42.6162
	step [117/255], loss=45.8486
	step [118/255], loss=45.5696
	step [119/255], loss=43.5882
	step [120/255], loss=46.0239
	step [121/255], loss=44.1123
	step [122/255], loss=43.1917
	step [123/255], loss=44.8392
	step [124/255], loss=43.5188
	step [125/255], loss=44.2957
	step [126/255], loss=42.0642
	step [127/255], loss=43.6723
	step [128/255], loss=43.4378
	step [129/255], loss=43.6985
	step [130/255], loss=43.6496
	step [131/255], loss=43.1553
	step [132/255], loss=44.1214
	step [133/255], loss=42.8555
	step [134/255], loss=42.8744
	step [135/255], loss=41.2591
	step [136/255], loss=42.2529
	step [137/255], loss=41.7819
	step [138/255], loss=42.5462
	step [139/255], loss=40.2586
	step [140/255], loss=44.8748
	step [141/255], loss=43.1069
	step [142/255], loss=43.5860
	step [143/255], loss=44.2448
	step [144/255], loss=41.8224
	step [145/255], loss=41.1255
	step [146/255], loss=42.8757
	step [147/255], loss=41.8036
	step [148/255], loss=41.6426
	step [149/255], loss=41.1676
	step [150/255], loss=41.1857
	step [151/255], loss=41.7262
	step [152/255], loss=41.0190
	step [153/255], loss=41.6334
	step [154/255], loss=41.4688
	step [155/255], loss=43.1736
	step [156/255], loss=40.0126
	step [157/255], loss=40.2656
	step [158/255], loss=41.3665
	step [159/255], loss=41.9291
	step [160/255], loss=40.6601
	step [161/255], loss=41.8292
	step [162/255], loss=40.2593
	step [163/255], loss=40.9434
	step [164/255], loss=41.6395
	step [165/255], loss=42.1010
	step [166/255], loss=41.7615
	step [167/255], loss=41.2950
	step [168/255], loss=41.7813
	step [169/255], loss=42.0243
	step [170/255], loss=41.4464
	step [171/255], loss=40.7148
	step [172/255], loss=42.2523
	step [173/255], loss=39.4211
	step [174/255], loss=41.9651
	step [175/255], loss=40.8957
	step [176/255], loss=39.6659
	step [177/255], loss=38.0263
	step [178/255], loss=38.9665
	step [179/255], loss=41.1697
	step [180/255], loss=40.1333
	step [181/255], loss=40.1036
	step [182/255], loss=40.3030
	step [183/255], loss=41.3753
	step [184/255], loss=38.7714
	step [185/255], loss=40.7959
	step [186/255], loss=39.3319
	step [187/255], loss=38.9656
	step [188/255], loss=39.8085
	step [189/255], loss=40.8960
	step [190/255], loss=39.9844
	step [191/255], loss=38.0773
	step [192/255], loss=38.4792
	step [193/255], loss=38.8182
	step [194/255], loss=38.9611
	step [195/255], loss=39.7388
	step [196/255], loss=38.8146
	step [197/255], loss=39.3556
	step [198/255], loss=39.4823
	step [199/255], loss=38.9608
	step [200/255], loss=39.2746
	step [201/255], loss=39.0764
	step [202/255], loss=38.0073
	step [203/255], loss=39.3103
	step [204/255], loss=38.0733
	step [205/255], loss=37.6249
	step [206/255], loss=37.6526
	step [207/255], loss=38.8728
	step [208/255], loss=39.7572
	step [209/255], loss=38.5464
	step [210/255], loss=38.1384
	step [211/255], loss=37.6579
	step [212/255], loss=37.1138
	step [213/255], loss=37.8335
	step [214/255], loss=39.7782
	step [215/255], loss=39.4350
	step [216/255], loss=38.6270
	step [217/255], loss=37.2712
	step [218/255], loss=38.0678
	step [219/255], loss=38.3136
	step [220/255], loss=38.8092
	step [221/255], loss=39.5086
	step [222/255], loss=38.4822
	step [223/255], loss=36.6961
	step [224/255], loss=37.9122
	step [225/255], loss=40.1979
	step [226/255], loss=35.9559
	step [227/255], loss=35.9092
	step [228/255], loss=35.4844
	step [229/255], loss=37.4112
	step [230/255], loss=36.7138
	step [231/255], loss=36.9114
	step [232/255], loss=38.9752
	step [233/255], loss=37.1185
	step [234/255], loss=35.9900
	step [235/255], loss=36.8699
	step [236/255], loss=35.6505
	step [237/255], loss=37.7136
	step [238/255], loss=35.7203
	step [239/255], loss=36.8785
	step [240/255], loss=36.8278
	step [241/255], loss=35.8922
	step [242/255], loss=36.0764
	step [243/255], loss=37.3033
	step [244/255], loss=34.8996
	step [245/255], loss=35.9622
	step [246/255], loss=36.8665
	step [247/255], loss=38.1771
	step [248/255], loss=35.5450
	step [249/255], loss=37.8664
	step [250/255], loss=36.8732
	step [251/255], loss=35.4778
	step [252/255], loss=35.5343
	step [253/255], loss=34.9298
	step [254/255], loss=36.3099
	step [255/255], loss=28.5496
	Evaluating
	loss=0.1747, precision=0.1445, recall=0.9964, f1=0.2524
saving model as: 0_saved_model.pth
Training epoch 3
	step [1/255], loss=36.9376
	step [2/255], loss=35.9046
	step [3/255], loss=34.7242
	step [4/255], loss=36.1081
	step [5/255], loss=36.1747
	step [6/255], loss=37.1584
	step [7/255], loss=35.4807
	step [8/255], loss=34.1324
	step [9/255], loss=35.2297
	step [10/255], loss=34.8848
	step [11/255], loss=34.8367
	step [12/255], loss=36.0670
	step [13/255], loss=35.2540
	step [14/255], loss=35.3595
	step [15/255], loss=33.9719
	step [16/255], loss=36.5166
	step [17/255], loss=36.7602
	step [18/255], loss=37.5658
	step [19/255], loss=33.4012
	step [20/255], loss=33.9912
	step [21/255], loss=33.7167
	step [22/255], loss=33.2980
	step [23/255], loss=34.5825
	step [24/255], loss=34.6324
	step [25/255], loss=32.4116
	step [26/255], loss=32.1141
	step [27/255], loss=36.5078
	step [28/255], loss=33.8276
	step [29/255], loss=34.5216
	step [30/255], loss=34.7141
	step [31/255], loss=33.6950
	step [32/255], loss=34.1397
	step [33/255], loss=33.7154
	step [34/255], loss=34.0133
	step [35/255], loss=35.2764
	step [36/255], loss=34.5167
	step [37/255], loss=33.4672
	step [38/255], loss=37.3592
	step [39/255], loss=32.4069
	step [40/255], loss=35.1289
	step [41/255], loss=34.4529
	step [42/255], loss=32.3532
	step [43/255], loss=33.9342
	step [44/255], loss=33.9392
	step [45/255], loss=32.5043
	step [46/255], loss=32.8173
	step [47/255], loss=33.5980
	step [48/255], loss=32.9779
	step [49/255], loss=32.5351
	step [50/255], loss=33.0987
	step [51/255], loss=32.0745
	step [52/255], loss=33.4829
	step [53/255], loss=33.1682
	step [54/255], loss=32.3080
	step [55/255], loss=33.9086
	step [56/255], loss=32.7087
	step [57/255], loss=34.7046
	step [58/255], loss=34.0010
	step [59/255], loss=31.8307
	step [60/255], loss=32.3143
	step [61/255], loss=31.0228
	step [62/255], loss=33.2497
	step [63/255], loss=33.0297
	step [64/255], loss=33.9160
	step [65/255], loss=32.5214
	step [66/255], loss=32.0471
	step [67/255], loss=33.7830
	step [68/255], loss=34.0242
	step [69/255], loss=31.5901
	step [70/255], loss=32.4700
	step [71/255], loss=34.4983
	step [72/255], loss=34.0241
	step [73/255], loss=32.9524
	step [74/255], loss=33.9787
	step [75/255], loss=32.0431
	step [76/255], loss=35.4835
	step [77/255], loss=32.3950
	step [78/255], loss=31.7915
	step [79/255], loss=32.4949
	step [80/255], loss=31.7089
	step [81/255], loss=30.7885
	step [82/255], loss=32.2971
	step [83/255], loss=33.5248
	step [84/255], loss=32.9292
	step [85/255], loss=30.1997
	step [86/255], loss=31.9602
	step [87/255], loss=31.8902
	step [88/255], loss=30.9447
	step [89/255], loss=31.7255
	step [90/255], loss=31.8646
	step [91/255], loss=29.8101
	step [92/255], loss=31.4213
	step [93/255], loss=30.4999
	step [94/255], loss=31.0921
	step [95/255], loss=31.4048
	step [96/255], loss=31.6904
	step [97/255], loss=31.0318
	step [98/255], loss=30.8353
	step [99/255], loss=30.3839
	step [100/255], loss=31.5765
	step [101/255], loss=30.7268
	step [102/255], loss=30.7482
	step [103/255], loss=32.0449
	step [104/255], loss=30.3011
	step [105/255], loss=31.2007
	step [106/255], loss=31.4408
	step [107/255], loss=31.6217
	step [108/255], loss=30.8267
	step [109/255], loss=29.3180
	step [110/255], loss=32.4402
	step [111/255], loss=29.3773
	step [112/255], loss=30.9770
	step [113/255], loss=29.7769
	step [114/255], loss=30.5210
	step [115/255], loss=30.6573
	step [116/255], loss=30.1812
	step [117/255], loss=29.3178
	step [118/255], loss=29.9747
	step [119/255], loss=30.5417
	step [120/255], loss=29.1001
	step [121/255], loss=29.3902
	step [122/255], loss=30.3489
	step [123/255], loss=31.3902
	step [124/255], loss=30.1940
	step [125/255], loss=30.6293
	step [126/255], loss=29.4862
	step [127/255], loss=30.0363
	step [128/255], loss=31.1895
	step [129/255], loss=29.5746
	step [130/255], loss=29.3876
	step [131/255], loss=30.0835
	step [132/255], loss=32.6927
	step [133/255], loss=32.8568
	step [134/255], loss=29.5655
	step [135/255], loss=31.6607
	step [136/255], loss=29.8540
	step [137/255], loss=30.4361
	step [138/255], loss=28.7568
	step [139/255], loss=30.4026
	step [140/255], loss=29.3983
	step [141/255], loss=29.1100
	step [142/255], loss=29.7661
	step [143/255], loss=29.5985
	step [144/255], loss=29.1758
	step [145/255], loss=28.8586
	step [146/255], loss=29.7457
	step [147/255], loss=29.0488
	step [148/255], loss=27.3229
	step [149/255], loss=26.8096
	step [150/255], loss=28.6801
	step [151/255], loss=28.1468
	step [152/255], loss=30.7258
	step [153/255], loss=30.1578
	step [154/255], loss=29.2832
	step [155/255], loss=29.4513
	step [156/255], loss=28.1828
	step [157/255], loss=28.4536
	step [158/255], loss=28.2397
	step [159/255], loss=27.7563
	step [160/255], loss=27.0672
	step [161/255], loss=29.3156
	step [162/255], loss=27.6446
	step [163/255], loss=27.5263
	step [164/255], loss=29.5059
	step [165/255], loss=28.2783
	step [166/255], loss=31.9708
	step [167/255], loss=28.9431
	step [168/255], loss=29.1094
	step [169/255], loss=28.4410
	step [170/255], loss=29.0561
	step [171/255], loss=28.4619
	step [172/255], loss=28.2100
	step [173/255], loss=30.3638
	step [174/255], loss=29.6052
	step [175/255], loss=27.8258
	step [176/255], loss=30.1980
	step [177/255], loss=28.7975
	step [178/255], loss=28.4591
	step [179/255], loss=30.3029
	step [180/255], loss=28.7422
	step [181/255], loss=27.0494
	step [182/255], loss=27.7954
	step [183/255], loss=27.5539
	step [184/255], loss=28.3935
	step [185/255], loss=29.3979
	step [186/255], loss=27.6181
	step [187/255], loss=26.9035
	step [188/255], loss=28.6423
	step [189/255], loss=26.0558
	step [190/255], loss=27.4835
	step [191/255], loss=28.2654
	step [192/255], loss=28.9018
	step [193/255], loss=26.0021
	step [194/255], loss=27.4657
	step [195/255], loss=26.4938
	step [196/255], loss=27.6447
	step [197/255], loss=26.8059
	step [198/255], loss=26.8336
	step [199/255], loss=25.9086
	step [200/255], loss=26.2924
	step [201/255], loss=28.1411
	step [202/255], loss=27.3694
	step [203/255], loss=26.3297
	step [204/255], loss=28.4480
	step [205/255], loss=25.6362
	step [206/255], loss=26.7906
	step [207/255], loss=28.0589
	step [208/255], loss=29.9263
	step [209/255], loss=26.9130
	step [210/255], loss=27.7366
	step [211/255], loss=27.3063
	step [212/255], loss=28.0213
	step [213/255], loss=26.9460
	step [214/255], loss=25.4977
	step [215/255], loss=26.7402
	step [216/255], loss=27.5533
	step [217/255], loss=27.6705
	step [218/255], loss=27.6065
	step [219/255], loss=28.7603
	step [220/255], loss=25.8950
	step [221/255], loss=27.0599
	step [222/255], loss=26.1700
	step [223/255], loss=27.2130
	step [224/255], loss=26.4567
	step [225/255], loss=25.0263
	step [226/255], loss=26.0242
	step [227/255], loss=27.0081
	step [228/255], loss=26.9352
	step [229/255], loss=28.2881
	step [230/255], loss=26.7151
	step [231/255], loss=27.4916
	step [232/255], loss=25.9779
	step [233/255], loss=27.5591
	step [234/255], loss=27.0120
	step [235/255], loss=26.2774
	step [236/255], loss=26.3770
	step [237/255], loss=26.2612
	step [238/255], loss=27.1140
	step [239/255], loss=28.5645
	step [240/255], loss=27.4282
	step [241/255], loss=26.1426
	step [242/255], loss=25.7154
	step [243/255], loss=26.4010
	step [244/255], loss=24.9502
	step [245/255], loss=26.3900
	step [246/255], loss=25.9809
	step [247/255], loss=23.8776
	step [248/255], loss=24.6553
	step [249/255], loss=28.0051
	step [250/255], loss=24.7270
	step [251/255], loss=26.9990
	step [252/255], loss=26.3078
	step [253/255], loss=24.8615
	step [254/255], loss=24.9473
	step [255/255], loss=22.7103
	Evaluating
	loss=0.1214, precision=0.1494, recall=0.9961, f1=0.2598
saving model as: 0_saved_model.pth
Training epoch 4
	step [1/255], loss=27.9463
	step [2/255], loss=25.4858
	step [3/255], loss=25.9562
	step [4/255], loss=25.7413
	step [5/255], loss=25.7712
	step [6/255], loss=24.6380
	step [7/255], loss=24.8230
	step [8/255], loss=28.4355
	step [9/255], loss=28.4974
	step [10/255], loss=26.8577
	step [11/255], loss=25.0477
	step [12/255], loss=25.0520
	step [13/255], loss=25.2117
	step [14/255], loss=24.8157
	step [15/255], loss=25.9297
	step [16/255], loss=25.3632
	step [17/255], loss=25.0434
	step [18/255], loss=24.9859
	step [19/255], loss=25.9346
	step [20/255], loss=25.4226
	step [21/255], loss=24.4625
	step [22/255], loss=25.4189
	step [23/255], loss=23.2875
	step [24/255], loss=23.4941
	step [25/255], loss=23.8972
	step [26/255], loss=25.4190
	step [27/255], loss=25.3793
	step [28/255], loss=24.7190
	step [29/255], loss=26.6534
	step [30/255], loss=25.6797
	step [31/255], loss=24.9700
	step [32/255], loss=24.2869
	step [33/255], loss=23.7370
	step [34/255], loss=23.7374
	step [35/255], loss=24.1789
	step [36/255], loss=26.1236
	step [37/255], loss=23.0743
	step [38/255], loss=24.6910
	step [39/255], loss=27.5240
	step [40/255], loss=24.2226
	step [41/255], loss=27.0529
	step [42/255], loss=23.4568
	step [43/255], loss=26.4149
	step [44/255], loss=23.3275
	step [45/255], loss=24.9174
	step [46/255], loss=23.7021
	step [47/255], loss=24.6839
	step [48/255], loss=23.9697
	step [49/255], loss=23.8528
	step [50/255], loss=23.5488
	step [51/255], loss=24.2905
	step [52/255], loss=24.8031
	step [53/255], loss=24.0781
	step [54/255], loss=23.9944
	step [55/255], loss=24.6208
	step [56/255], loss=24.8549
	step [57/255], loss=23.8425
	step [58/255], loss=22.8128
	step [59/255], loss=23.5824
	step [60/255], loss=23.7326
	step [61/255], loss=23.5164
	step [62/255], loss=24.7245
	step [63/255], loss=22.6474
	step [64/255], loss=25.0646
	step [65/255], loss=23.3077
	step [66/255], loss=24.7635
	step [67/255], loss=23.7366
	step [68/255], loss=23.9025
	step [69/255], loss=22.8432
	step [70/255], loss=25.6911
	step [71/255], loss=23.9114
	step [72/255], loss=23.4811
	step [73/255], loss=23.6914
	step [74/255], loss=24.0076
	step [75/255], loss=22.3298
	step [76/255], loss=22.3783
	step [77/255], loss=24.2470
	step [78/255], loss=22.1609
	step [79/255], loss=24.4886
	step [80/255], loss=23.9278
	step [81/255], loss=22.1782
	step [82/255], loss=22.3289
	step [83/255], loss=23.7586
	step [84/255], loss=24.8384
	step [85/255], loss=22.6915
	step [86/255], loss=26.1091
	step [87/255], loss=22.9860
	step [88/255], loss=26.1394
	step [89/255], loss=23.9784
	step [90/255], loss=23.7711
	step [91/255], loss=21.8133
	step [92/255], loss=23.4407
	step [93/255], loss=23.9950
	step [94/255], loss=23.4717
	step [95/255], loss=24.5069
	step [96/255], loss=24.6949
	step [97/255], loss=24.7153
	step [98/255], loss=23.1662
	step [99/255], loss=23.7216
	step [100/255], loss=21.9067
	step [101/255], loss=24.7099
	step [102/255], loss=22.3360
	step [103/255], loss=22.4043
	step [104/255], loss=23.5170
	step [105/255], loss=21.5783
	step [106/255], loss=21.6625
	step [107/255], loss=21.6973
	step [108/255], loss=23.6320
	step [109/255], loss=24.5106
	step [110/255], loss=24.8279
	step [111/255], loss=21.0621
	step [112/255], loss=23.7180
	step [113/255], loss=20.8881
	step [114/255], loss=24.4804
	step [115/255], loss=24.3525
	step [116/255], loss=23.1949
	step [117/255], loss=24.8486
	step [118/255], loss=22.6744
	step [119/255], loss=22.5959
	step [120/255], loss=20.8974
	step [121/255], loss=23.5583
	step [122/255], loss=22.9509
	step [123/255], loss=22.9985
	step [124/255], loss=24.2874
	step [125/255], loss=22.9624
	step [126/255], loss=22.6927
	step [127/255], loss=21.4074
	step [128/255], loss=22.6264
	step [129/255], loss=21.1701
	step [130/255], loss=22.4601
	step [131/255], loss=22.5445
	step [132/255], loss=22.7170
	step [133/255], loss=24.3178
	step [134/255], loss=22.0881
	step [135/255], loss=22.4022
	step [136/255], loss=21.5208
	step [137/255], loss=22.4942
	step [138/255], loss=21.2789
	step [139/255], loss=22.9502
	step [140/255], loss=22.1956
	step [141/255], loss=22.6655
	step [142/255], loss=22.7452
	step [143/255], loss=21.4035
	step [144/255], loss=22.6814
	step [145/255], loss=22.2586
	step [146/255], loss=23.2712
	step [147/255], loss=22.2647
	step [148/255], loss=20.0321
	step [149/255], loss=22.4003
	step [150/255], loss=22.3918
	step [151/255], loss=20.3909
	step [152/255], loss=21.9481
	step [153/255], loss=21.4473
	step [154/255], loss=21.8418
	step [155/255], loss=21.5243
	step [156/255], loss=21.9472
	step [157/255], loss=20.4415
	step [158/255], loss=20.9486
	step [159/255], loss=22.0301
	step [160/255], loss=20.8924
	step [161/255], loss=20.5298
	step [162/255], loss=20.8208
	step [163/255], loss=23.1300
	step [164/255], loss=21.2077
	step [165/255], loss=20.4666
	step [166/255], loss=20.7008
	step [167/255], loss=21.1104
	step [168/255], loss=22.0871
	step [169/255], loss=20.2154
	step [170/255], loss=21.8274
	step [171/255], loss=23.2937
	step [172/255], loss=21.1015
	step [173/255], loss=22.0918
	step [174/255], loss=20.9674
	step [175/255], loss=21.1537
	step [176/255], loss=22.3979
	step [177/255], loss=20.6303
	step [178/255], loss=23.0514
	step [179/255], loss=22.7693
	step [180/255], loss=20.9067
	step [181/255], loss=22.1465
	step [182/255], loss=21.5108
	step [183/255], loss=23.1989
	step [184/255], loss=21.9157
	step [185/255], loss=21.7970
	step [186/255], loss=21.5488
	step [187/255], loss=21.3907
	step [188/255], loss=20.2384
	step [189/255], loss=19.1933
	step [190/255], loss=21.8998
	step [191/255], loss=19.8670
	step [192/255], loss=20.9745
	step [193/255], loss=21.7317
	step [194/255], loss=21.9828
	step [195/255], loss=20.3871
	step [196/255], loss=21.5137
	step [197/255], loss=22.2984
	step [198/255], loss=20.9827
	step [199/255], loss=22.8267
	step [200/255], loss=20.6897
	step [201/255], loss=21.2346
	step [202/255], loss=20.5788
	step [203/255], loss=22.3592
	step [204/255], loss=20.3419
	step [205/255], loss=21.2659
	step [206/255], loss=21.1644
	step [207/255], loss=22.4054
	step [208/255], loss=21.6877
	step [209/255], loss=21.2214
	step [210/255], loss=20.7572
	step [211/255], loss=21.0009
	step [212/255], loss=20.9825
	step [213/255], loss=19.4118
	step [214/255], loss=20.4084
	step [215/255], loss=19.9948
	step [216/255], loss=20.8875
	step [217/255], loss=22.2962
	step [218/255], loss=19.2305
	step [219/255], loss=21.8272
	step [220/255], loss=19.7164
	step [221/255], loss=20.7154
	step [222/255], loss=21.3272
	step [223/255], loss=19.7122
	step [224/255], loss=21.8439
	step [225/255], loss=21.2295
	step [226/255], loss=19.5180
	step [227/255], loss=20.3925
	step [228/255], loss=18.8769
	step [229/255], loss=20.3916
	step [230/255], loss=22.4951
	step [231/255], loss=19.9398
	step [232/255], loss=20.2022
	step [233/255], loss=22.4325
	step [234/255], loss=20.4076
	step [235/255], loss=19.3876
	step [236/255], loss=21.3769
	step [237/255], loss=20.4525
	step [238/255], loss=18.3041
	step [239/255], loss=19.1746
	step [240/255], loss=20.7522
	step [241/255], loss=20.0194
	step [242/255], loss=21.0939
	step [243/255], loss=19.1352
	step [244/255], loss=18.0217
	step [245/255], loss=20.3708
	step [246/255], loss=19.6014
	step [247/255], loss=19.6768
	step [248/255], loss=20.2131
	step [249/255], loss=21.2275
	step [250/255], loss=21.4979
	step [251/255], loss=20.9522
	step [252/255], loss=18.2405
	step [253/255], loss=19.6631
	step [254/255], loss=20.2318
	step [255/255], loss=15.8907
	Evaluating
	loss=0.0966, precision=0.1461, recall=0.9959, f1=0.2548
Training epoch 5
	step [1/255], loss=19.8825
	step [2/255], loss=20.6232
	step [3/255], loss=19.0818
	step [4/255], loss=20.3522
	step [5/255], loss=19.9563
	step [6/255], loss=18.8756
	step [7/255], loss=21.2975
	step [8/255], loss=19.3459
	step [9/255], loss=18.8638
	step [10/255], loss=20.7693
	step [11/255], loss=20.2141
	step [12/255], loss=20.7323
	step [13/255], loss=18.4480
	step [14/255], loss=18.8479
	step [15/255], loss=19.8129
	step [16/255], loss=20.6748
	step [17/255], loss=18.5099
	step [18/255], loss=19.5112
	step [19/255], loss=19.4779
	step [20/255], loss=20.8056
	step [21/255], loss=21.3123
	step [22/255], loss=20.7301
	step [23/255], loss=18.5633
	step [24/255], loss=19.7137
	step [25/255], loss=19.4962
	step [26/255], loss=20.8697
	step [27/255], loss=20.9444
	step [28/255], loss=18.6207
	step [29/255], loss=19.1724
	step [30/255], loss=19.9144
	step [31/255], loss=20.0691
	step [32/255], loss=18.1945
	step [33/255], loss=19.3184
	step [34/255], loss=18.7253
	step [35/255], loss=17.7220
	step [36/255], loss=18.8356
	step [37/255], loss=17.9666
	step [38/255], loss=18.9336
	step [39/255], loss=21.5833
	step [40/255], loss=18.1048
	step [41/255], loss=19.5279
	step [42/255], loss=18.9192
	step [43/255], loss=17.6089
	step [44/255], loss=21.9046
	step [45/255], loss=19.6469
	step [46/255], loss=18.8218
	step [47/255], loss=20.6427
	step [48/255], loss=18.2855
	step [49/255], loss=18.1324
	step [50/255], loss=17.8876
	step [51/255], loss=22.1964
	step [52/255], loss=19.9017
	step [53/255], loss=17.8530
	step [54/255], loss=18.3843
	step [55/255], loss=17.4785
	step [56/255], loss=18.1833
	step [57/255], loss=20.3730
	step [58/255], loss=17.7504
	step [59/255], loss=18.0571
	step [60/255], loss=23.4506
	step [61/255], loss=18.9087
	step [62/255], loss=17.8869
	step [63/255], loss=18.4248
	step [64/255], loss=17.3671
	step [65/255], loss=17.6711
	step [66/255], loss=20.3677
	step [67/255], loss=18.7323
	step [68/255], loss=17.6583
	step [69/255], loss=19.4570
	step [70/255], loss=18.1574
	step [71/255], loss=18.4522
	step [72/255], loss=18.5034
	step [73/255], loss=18.7712
	step [74/255], loss=19.1635
	step [75/255], loss=18.1810
	step [76/255], loss=17.6629
	step [77/255], loss=19.0517
	step [78/255], loss=17.0783
	step [79/255], loss=16.1841
	step [80/255], loss=20.2654
	step [81/255], loss=21.1077
	step [82/255], loss=16.6048
	step [83/255], loss=18.5300
	step [84/255], loss=18.7275
	step [85/255], loss=19.3808
	step [86/255], loss=18.9485
	step [87/255], loss=19.9529
	step [88/255], loss=18.3722
	step [89/255], loss=17.2803
	step [90/255], loss=18.5168
	step [91/255], loss=18.9283
	step [92/255], loss=18.5430
	step [93/255], loss=17.1260
	step [94/255], loss=16.6646
	step [95/255], loss=19.2511
	step [96/255], loss=17.9989
	step [97/255], loss=19.3998
	step [98/255], loss=18.5683
	step [99/255], loss=17.7744
	step [100/255], loss=17.4226
	step [101/255], loss=17.9725
	step [102/255], loss=20.2050
	step [103/255], loss=16.4274
	step [104/255], loss=19.1044
	step [105/255], loss=17.6710
	step [106/255], loss=18.5835
	step [107/255], loss=17.5199
	step [108/255], loss=16.9012
	step [109/255], loss=17.6090
	step [110/255], loss=18.8013
	step [111/255], loss=16.8845
	step [112/255], loss=18.6248
	step [113/255], loss=18.8619
	step [114/255], loss=19.4546
	step [115/255], loss=17.9255
	step [116/255], loss=20.1758
	step [117/255], loss=16.3260
	step [118/255], loss=17.4650
	step [119/255], loss=17.8553
	step [120/255], loss=16.7548
	step [121/255], loss=18.7307
	step [122/255], loss=17.2395
	step [123/255], loss=17.3293
	step [124/255], loss=17.5637
	step [125/255], loss=18.1258
	step [126/255], loss=17.9366
	step [127/255], loss=19.3451
	step [128/255], loss=19.0760
	step [129/255], loss=17.6206
	step [130/255], loss=18.2375
	step [131/255], loss=17.5251
	step [132/255], loss=18.9629
	step [133/255], loss=19.3434
	step [134/255], loss=18.0911
	step [135/255], loss=18.7054
	step [136/255], loss=17.4006
	step [137/255], loss=17.0036
	step [138/255], loss=19.1302
	step [139/255], loss=18.6825
	step [140/255], loss=15.9960
	step [141/255], loss=17.7416
	step [142/255], loss=18.7091
	step [143/255], loss=19.1279
	step [144/255], loss=18.7911
	step [145/255], loss=17.7932
	step [146/255], loss=17.7771
	step [147/255], loss=18.4185
	step [148/255], loss=17.3590
	step [149/255], loss=17.3551
	step [150/255], loss=17.4356
	step [151/255], loss=19.3453
	step [152/255], loss=16.2124
	step [153/255], loss=18.9809
	step [154/255], loss=19.4195
	step [155/255], loss=18.7580
	step [156/255], loss=16.2521
	step [157/255], loss=17.9585
	step [158/255], loss=16.2765
	step [159/255], loss=17.5000
	step [160/255], loss=18.3974
	step [161/255], loss=16.6027
	step [162/255], loss=17.9818
	step [163/255], loss=17.5667
	step [164/255], loss=15.7464
	step [165/255], loss=17.6059
	step [166/255], loss=17.3998
	step [167/255], loss=18.1160
	step [168/255], loss=16.7617
	step [169/255], loss=16.0453
	step [170/255], loss=17.3815
	step [171/255], loss=17.3538
	step [172/255], loss=17.2179
	step [173/255], loss=18.5761
	step [174/255], loss=17.7348
	step [175/255], loss=17.5584
	step [176/255], loss=17.2003
	step [177/255], loss=17.9263
	step [178/255], loss=17.5388
	step [179/255], loss=16.2724
	step [180/255], loss=18.5102
	step [181/255], loss=16.9024
	step [182/255], loss=18.2805
	step [183/255], loss=18.5277
	step [184/255], loss=17.1404
	step [185/255], loss=17.9722
	step [186/255], loss=19.7946
	step [187/255], loss=17.1392
	step [188/255], loss=16.6657
	step [189/255], loss=18.8653
	step [190/255], loss=16.7834
	step [191/255], loss=17.5348
	step [192/255], loss=18.5637
	step [193/255], loss=17.3292
	step [194/255], loss=17.4140
	step [195/255], loss=16.8220
	step [196/255], loss=15.1974
	step [197/255], loss=17.1857
	step [198/255], loss=17.1034
	step [199/255], loss=14.6861
	step [200/255], loss=16.5460
	step [201/255], loss=17.4123
	step [202/255], loss=17.1214
	step [203/255], loss=16.6341
	step [204/255], loss=16.5361
	step [205/255], loss=17.8251
	step [206/255], loss=17.6666
	step [207/255], loss=15.9686
	step [208/255], loss=16.0965
	step [209/255], loss=15.0189
	step [210/255], loss=15.6959
	step [211/255], loss=19.3154
	step [212/255], loss=16.8418
	step [213/255], loss=16.6158
	step [214/255], loss=16.0707
	step [215/255], loss=15.5847
	step [216/255], loss=17.3628
	step [217/255], loss=17.1848
	step [218/255], loss=17.0650
	step [219/255], loss=16.1240
	step [220/255], loss=16.6706
	step [221/255], loss=18.4421
	step [222/255], loss=17.9247
	step [223/255], loss=16.2450
	step [224/255], loss=15.4187
	step [225/255], loss=15.7027
	step [226/255], loss=15.7460
	step [227/255], loss=17.1268
	step [228/255], loss=17.9592
	step [229/255], loss=16.7200
	step [230/255], loss=17.3306
	step [231/255], loss=16.2124
	step [232/255], loss=16.5851
	step [233/255], loss=17.8654
	step [234/255], loss=16.3122
	step [235/255], loss=15.7646
	step [236/255], loss=14.4521
	step [237/255], loss=15.9345
	step [238/255], loss=19.0131
	step [239/255], loss=14.9496
	step [240/255], loss=18.0350
	step [241/255], loss=16.6773
	step [242/255], loss=15.4280
	step [243/255], loss=16.6248
	step [244/255], loss=19.0665
	step [245/255], loss=20.0657
	step [246/255], loss=14.7725
	step [247/255], loss=16.3176
	step [248/255], loss=14.7457
	step [249/255], loss=16.1535
	step [250/255], loss=15.7762
	step [251/255], loss=15.6724
	step [252/255], loss=15.8357
	step [253/255], loss=15.6441
	step [254/255], loss=15.2455
	step [255/255], loss=16.4251
	Evaluating
	loss=0.0679, precision=0.1738, recall=0.9953, f1=0.2960
saving model as: 0_saved_model.pth
Training epoch 6
	step [1/255], loss=18.9962
	step [2/255], loss=15.3349
	step [3/255], loss=17.2509
	step [4/255], loss=16.8221
	step [5/255], loss=17.2173
	step [6/255], loss=17.0186
	step [7/255], loss=15.8347
	step [8/255], loss=17.3227
	step [9/255], loss=14.8115
	step [10/255], loss=16.2125
	step [11/255], loss=17.0029
	step [12/255], loss=15.8765
	step [13/255], loss=15.0377
	step [14/255], loss=14.9116
	step [15/255], loss=16.8127
	step [16/255], loss=15.1409
	step [17/255], loss=15.3340
	step [18/255], loss=18.8791
	step [19/255], loss=16.2474
	step [20/255], loss=15.6357
	step [21/255], loss=15.7398
	step [22/255], loss=17.3063
	step [23/255], loss=16.8116
	step [24/255], loss=15.7784
	step [25/255], loss=15.7763
	step [26/255], loss=14.4124
	step [27/255], loss=16.8473
	step [28/255], loss=17.0183
	step [29/255], loss=18.1151
	step [30/255], loss=16.3447
	step [31/255], loss=18.5668
	step [32/255], loss=17.0507
	step [33/255], loss=15.3961
	step [34/255], loss=16.3131
	step [35/255], loss=16.3122
	step [36/255], loss=16.5811
	step [37/255], loss=14.8242
	step [38/255], loss=15.8969
	step [39/255], loss=17.1346
	step [40/255], loss=17.8228
	step [41/255], loss=15.9052
	step [42/255], loss=16.8458
	step [43/255], loss=15.3395
	step [44/255], loss=15.2337
	step [45/255], loss=15.6383
	step [46/255], loss=13.9478
	step [47/255], loss=16.4369
	step [48/255], loss=14.5773
	step [49/255], loss=15.1671
	step [50/255], loss=15.0256
	step [51/255], loss=16.1553
	step [52/255], loss=14.8584
	step [53/255], loss=16.3300
	step [54/255], loss=14.2344
	step [55/255], loss=16.4132
	step [56/255], loss=13.8900
	step [57/255], loss=16.1778
	step [58/255], loss=16.4102
	step [59/255], loss=14.3177
	step [60/255], loss=14.5734
	step [61/255], loss=14.8723
	step [62/255], loss=14.0145
	step [63/255], loss=16.1501
	step [64/255], loss=15.5575
	step [65/255], loss=17.0359
	step [66/255], loss=15.9654
	step [67/255], loss=16.6292
	step [68/255], loss=15.0477
	step [69/255], loss=13.6649
	step [70/255], loss=13.4309
	step [71/255], loss=16.0154
	step [72/255], loss=15.2486
	step [73/255], loss=14.0757
	step [74/255], loss=15.0669
	step [75/255], loss=17.3820
	step [76/255], loss=16.5967
	step [77/255], loss=15.8879
	step [78/255], loss=15.7481
	step [79/255], loss=15.1076
	step [80/255], loss=15.7587
	step [81/255], loss=15.0513
	step [82/255], loss=16.2892
	step [83/255], loss=15.3707
	step [84/255], loss=15.5465
	step [85/255], loss=15.5152
	step [86/255], loss=15.3129
	step [87/255], loss=16.6903
	step [88/255], loss=14.0679
	step [89/255], loss=13.6090
	step [90/255], loss=13.8307
	step [91/255], loss=14.4847
	step [92/255], loss=14.8074
	step [93/255], loss=18.0148
	step [94/255], loss=14.7583
	step [95/255], loss=15.6428
	step [96/255], loss=14.9496
	step [97/255], loss=15.0581
	step [98/255], loss=14.1359
	step [99/255], loss=17.0602
	step [100/255], loss=15.9735
	step [101/255], loss=16.1634
	step [102/255], loss=17.5618
	step [103/255], loss=15.5467
	step [104/255], loss=15.5554
	step [105/255], loss=13.7382
	step [106/255], loss=15.8669
	step [107/255], loss=13.8981
	step [108/255], loss=15.0328
	step [109/255], loss=15.0341
	step [110/255], loss=15.8759
	step [111/255], loss=16.2674
	step [112/255], loss=14.3741
	step [113/255], loss=14.5100
	step [114/255], loss=14.2070
	step [115/255], loss=14.4771
	step [116/255], loss=13.7293
	step [117/255], loss=16.3596
	step [118/255], loss=17.6275
	step [119/255], loss=13.7503
	step [120/255], loss=12.9186
	step [121/255], loss=14.5968
	step [122/255], loss=14.5708
	step [123/255], loss=14.9091
	step [124/255], loss=14.5932
	step [125/255], loss=14.4438
	step [126/255], loss=14.6192
	step [127/255], loss=15.0752
	step [128/255], loss=16.6772
	step [129/255], loss=14.6179
	step [130/255], loss=12.9863
	step [131/255], loss=15.4195
	step [132/255], loss=14.2595
	step [133/255], loss=14.4093
	step [134/255], loss=17.2004
	step [135/255], loss=17.3771
	step [136/255], loss=17.2208
	step [137/255], loss=14.6883
	step [138/255], loss=14.9814
	step [139/255], loss=15.4541
	step [140/255], loss=13.6675
	step [141/255], loss=14.6254
	step [142/255], loss=15.7371
	step [143/255], loss=15.7185
	step [144/255], loss=13.9765
	step [145/255], loss=14.7216
	step [146/255], loss=13.8984
	step [147/255], loss=16.0490
	step [148/255], loss=13.2553
	step [149/255], loss=14.0487
	step [150/255], loss=13.9296
	step [151/255], loss=15.2109
	step [152/255], loss=15.1850
	step [153/255], loss=14.0389
	step [154/255], loss=16.3350
	step [155/255], loss=15.0928
	step [156/255], loss=16.2078
	step [157/255], loss=14.4007
	step [158/255], loss=14.2745
	step [159/255], loss=15.6432
	step [160/255], loss=14.7760
	step [161/255], loss=13.9405
	step [162/255], loss=14.3006
	step [163/255], loss=14.6109
	step [164/255], loss=15.8871
	step [165/255], loss=13.0842
	step [166/255], loss=12.9563
	step [167/255], loss=14.1107
	step [168/255], loss=14.4578
	step [169/255], loss=14.8108
	step [170/255], loss=14.0128
	step [171/255], loss=14.1759
	step [172/255], loss=12.4154
	step [173/255], loss=15.0208
	step [174/255], loss=13.6068
	step [175/255], loss=14.1838
	step [176/255], loss=14.2739
	step [177/255], loss=17.4854
	step [178/255], loss=14.0720
	step [179/255], loss=13.5056
	step [180/255], loss=14.9270
	step [181/255], loss=14.6212
	step [182/255], loss=15.6082
	step [183/255], loss=14.2634
	step [184/255], loss=14.7734
	step [185/255], loss=14.8722
	step [186/255], loss=15.2946
	step [187/255], loss=14.7281
	step [188/255], loss=13.5848
	step [189/255], loss=14.1642
	step [190/255], loss=13.0328
	step [191/255], loss=13.2898
	step [192/255], loss=13.8364
	step [193/255], loss=14.6044
	step [194/255], loss=15.6329
	step [195/255], loss=12.5607
	step [196/255], loss=14.3833
	step [197/255], loss=13.5562
	step [198/255], loss=12.8391
	step [199/255], loss=12.2875
	step [200/255], loss=16.5962
	step [201/255], loss=14.6692
	step [202/255], loss=13.5243
	step [203/255], loss=13.7333
	step [204/255], loss=15.5208
	step [205/255], loss=15.7368
	step [206/255], loss=13.5672
	step [207/255], loss=14.2981
	step [208/255], loss=14.6288
	step [209/255], loss=13.2867
	step [210/255], loss=13.5594
	step [211/255], loss=15.7337
	step [212/255], loss=13.8798
	step [213/255], loss=14.4258
	step [214/255], loss=12.9209
	step [215/255], loss=12.8169
	step [216/255], loss=17.0365
	step [217/255], loss=14.3262
	step [218/255], loss=13.5789
	step [219/255], loss=13.5236
	step [220/255], loss=12.3530
	step [221/255], loss=12.4167
	step [222/255], loss=13.8816
	step [223/255], loss=18.1295
	step [224/255], loss=12.4259
	step [225/255], loss=14.9962
	step [226/255], loss=13.8448
	step [227/255], loss=13.7985
	step [228/255], loss=14.9180
	step [229/255], loss=14.3280
	step [230/255], loss=12.8408
	step [231/255], loss=14.2672
	step [232/255], loss=14.3631
	step [233/255], loss=13.4692
	step [234/255], loss=13.5276
	step [235/255], loss=13.8845
	step [236/255], loss=14.1223
	step [237/255], loss=13.8970
	step [238/255], loss=14.1162
	step [239/255], loss=16.1168
	step [240/255], loss=14.1619
	step [241/255], loss=14.2855
	step [242/255], loss=15.3343
	step [243/255], loss=14.1356
	step [244/255], loss=12.0961
	step [245/255], loss=14.6503
	step [246/255], loss=14.9073
	step [247/255], loss=13.4123
	step [248/255], loss=13.0008
	step [249/255], loss=12.7566
	step [250/255], loss=14.4727
	step [251/255], loss=14.3422
	step [252/255], loss=12.2180
	step [253/255], loss=14.2918
	step [254/255], loss=15.6529
	step [255/255], loss=12.9378
	Evaluating
	loss=0.0620, precision=0.1533, recall=0.9965, f1=0.2658
Training epoch 7
	step [1/255], loss=14.1451
	step [2/255], loss=14.1706
	step [3/255], loss=13.0943
	step [4/255], loss=15.9859
	step [5/255], loss=11.7387
	step [6/255], loss=13.3504
	step [7/255], loss=12.9650
	step [8/255], loss=14.4376
	step [9/255], loss=15.2656
	step [10/255], loss=11.8930
	step [11/255], loss=14.9168
	step [12/255], loss=14.5676
	step [13/255], loss=14.5615
	step [14/255], loss=14.8879
	step [15/255], loss=13.9563
	step [16/255], loss=12.6351
	step [17/255], loss=14.1244
	step [18/255], loss=13.2988
	step [19/255], loss=14.7651
	step [20/255], loss=14.7281
	step [21/255], loss=13.4705
	step [22/255], loss=15.0991
	step [23/255], loss=12.4357
	step [24/255], loss=13.2809
	step [25/255], loss=13.3688
	step [26/255], loss=12.2508
	step [27/255], loss=12.5509
	step [28/255], loss=15.0780
	step [29/255], loss=13.8742
	step [30/255], loss=11.5291
	step [31/255], loss=12.6951
	step [32/255], loss=15.4410
	step [33/255], loss=13.7907
	step [34/255], loss=12.9718
	step [35/255], loss=17.3495
	step [36/255], loss=12.7557
	step [37/255], loss=12.6589
	step [38/255], loss=13.3438
	step [39/255], loss=14.5115
	step [40/255], loss=14.8215
	step [41/255], loss=13.9628
	step [42/255], loss=13.9423
	step [43/255], loss=12.8279
	step [44/255], loss=11.1145
	step [45/255], loss=13.5848
	step [46/255], loss=13.2819
	step [47/255], loss=13.3589
	step [48/255], loss=14.4688
	step [49/255], loss=13.8682
	step [50/255], loss=14.6292
	step [51/255], loss=12.5192
	step [52/255], loss=12.6718
	step [53/255], loss=13.6879
	step [54/255], loss=14.7163
	step [55/255], loss=12.6526
	step [56/255], loss=12.8744
	step [57/255], loss=15.6341
	step [58/255], loss=13.5578
	step [59/255], loss=14.0277
	step [60/255], loss=13.4110
	step [61/255], loss=13.4837
	step [62/255], loss=12.1235
	step [63/255], loss=14.6281
	step [64/255], loss=13.4303
	step [65/255], loss=14.1368
	step [66/255], loss=13.9291
	step [67/255], loss=13.3328
	step [68/255], loss=13.7260
	step [69/255], loss=16.3696
	step [70/255], loss=12.4052
	step [71/255], loss=12.2119
	step [72/255], loss=13.5643
	step [73/255], loss=12.1095
	step [74/255], loss=11.3969
	step [75/255], loss=13.5288
	step [76/255], loss=12.5938
	step [77/255], loss=12.9780
	step [78/255], loss=13.6161
	step [79/255], loss=11.7213
	step [80/255], loss=13.5282
	step [81/255], loss=12.6687
	step [82/255], loss=13.2303
	step [83/255], loss=13.6457
	step [84/255], loss=15.2232
	step [85/255], loss=13.0528
	step [86/255], loss=13.3464
	step [87/255], loss=13.1848
	step [88/255], loss=13.2303
	step [89/255], loss=13.2726
	step [90/255], loss=13.9952
	step [91/255], loss=14.1034
	step [92/255], loss=13.3582
	step [93/255], loss=12.4127
	step [94/255], loss=13.0556
	step [95/255], loss=13.5980
	step [96/255], loss=13.4414
	step [97/255], loss=12.7083
	step [98/255], loss=12.7570
	step [99/255], loss=11.7569
	step [100/255], loss=14.9520
	step [101/255], loss=14.6233
	step [102/255], loss=16.2254
	step [103/255], loss=12.7401
	step [104/255], loss=13.4354
	step [105/255], loss=13.1211
	step [106/255], loss=12.1641
	step [107/255], loss=12.7366
	step [108/255], loss=13.2612
	step [109/255], loss=11.7585
	step [110/255], loss=11.7641
	step [111/255], loss=12.6655
	step [112/255], loss=13.9187
	step [113/255], loss=14.6447
	step [114/255], loss=13.3843
	step [115/255], loss=12.9244
	step [116/255], loss=13.5747
	step [117/255], loss=10.5647
	step [118/255], loss=12.2937
	step [119/255], loss=12.6287
	step [120/255], loss=11.2996
	step [121/255], loss=12.4999
	step [122/255], loss=13.6442
	step [123/255], loss=12.4124
	step [124/255], loss=12.6013
	step [125/255], loss=13.0800
	step [126/255], loss=15.3984
	step [127/255], loss=12.8048
	step [128/255], loss=13.6283
	step [129/255], loss=13.8940
	step [130/255], loss=13.4641
	step [131/255], loss=12.9011
	step [132/255], loss=14.0077
	step [133/255], loss=12.5791
	step [134/255], loss=13.2363
	step [135/255], loss=11.1423
	step [136/255], loss=12.4744
	step [137/255], loss=13.0927
	step [138/255], loss=10.8923
	step [139/255], loss=10.3545
	step [140/255], loss=12.6760
	step [141/255], loss=12.9136
	step [142/255], loss=13.7576
	step [143/255], loss=13.2915
	step [144/255], loss=15.5097
	step [145/255], loss=13.6460
	step [146/255], loss=12.4710
	step [147/255], loss=14.5132
	step [148/255], loss=11.2469
	step [149/255], loss=11.4630
	step [150/255], loss=12.9393
	step [151/255], loss=12.7687
	step [152/255], loss=11.6451
	step [153/255], loss=14.4672
	step [154/255], loss=11.8242
	step [155/255], loss=12.6422
	step [156/255], loss=11.8385
	step [157/255], loss=11.3877
	step [158/255], loss=13.2891
	step [159/255], loss=11.5999
	step [160/255], loss=11.3975
	step [161/255], loss=15.6262
	step [162/255], loss=12.6184
	step [163/255], loss=13.4827
	step [164/255], loss=13.2170
	step [165/255], loss=12.0753
	step [166/255], loss=13.7548
	step [167/255], loss=12.0646
	step [168/255], loss=12.6852
	step [169/255], loss=12.0139
	step [170/255], loss=11.9902
	step [171/255], loss=14.0245
	step [172/255], loss=12.6928
	step [173/255], loss=12.0081
	step [174/255], loss=13.4402
	step [175/255], loss=11.7733
	step [176/255], loss=14.4579
	step [177/255], loss=12.8454
	step [178/255], loss=13.0631
	step [179/255], loss=12.5641
	step [180/255], loss=12.3920
	step [181/255], loss=10.9431
	step [182/255], loss=12.7490
	step [183/255], loss=12.3796
	step [184/255], loss=12.0002
	step [185/255], loss=11.7232
	step [186/255], loss=14.1132
	step [187/255], loss=12.1873
	step [188/255], loss=11.8864
	step [189/255], loss=11.3857
	step [190/255], loss=12.3932
	step [191/255], loss=14.5204
	step [192/255], loss=12.5255
	step [193/255], loss=12.0467
	step [194/255], loss=11.6641
	step [195/255], loss=13.9948
	step [196/255], loss=13.9671
	step [197/255], loss=11.8651
	step [198/255], loss=10.7254
	step [199/255], loss=13.8411
	step [200/255], loss=11.4964
	step [201/255], loss=13.7044
	step [202/255], loss=12.7307
	step [203/255], loss=12.1548
	step [204/255], loss=12.5986
	step [205/255], loss=11.8369
	step [206/255], loss=10.8923
	step [207/255], loss=11.6877
	step [208/255], loss=13.9168
	step [209/255], loss=12.2200
	step [210/255], loss=11.4840
	step [211/255], loss=14.0043
	step [212/255], loss=12.1106
	step [213/255], loss=14.6793
	step [214/255], loss=11.4965
	step [215/255], loss=10.1138
	step [216/255], loss=14.8123
	step [217/255], loss=10.8610
	step [218/255], loss=11.7485
	step [219/255], loss=10.7427
	step [220/255], loss=12.2889
	step [221/255], loss=12.8575
	step [222/255], loss=12.4691
	step [223/255], loss=12.3417
	step [224/255], loss=11.7263
	step [225/255], loss=12.2950
	step [226/255], loss=13.2608
	step [227/255], loss=15.3468
	step [228/255], loss=11.7109
	step [229/255], loss=11.5179
	step [230/255], loss=13.4034
	step [231/255], loss=12.3615
	step [232/255], loss=11.2187
	step [233/255], loss=10.6142
	step [234/255], loss=11.9088
	step [235/255], loss=11.0879
	step [236/255], loss=11.6127
	step [237/255], loss=12.6590
	step [238/255], loss=12.6160
	step [239/255], loss=11.1045
	step [240/255], loss=14.7795
	step [241/255], loss=12.5346
	step [242/255], loss=12.1578
	step [243/255], loss=12.2785
	step [244/255], loss=12.7737
	step [245/255], loss=12.5301
	step [246/255], loss=13.8250
	step [247/255], loss=12.5274
	step [248/255], loss=11.1357
	step [249/255], loss=12.9404
	step [250/255], loss=12.5997
	step [251/255], loss=13.2290
	step [252/255], loss=12.6090
	step [253/255], loss=9.6403
	step [254/255], loss=13.3984
	step [255/255], loss=11.2274
	Evaluating
	loss=0.0443, precision=0.2013, recall=0.9943, f1=0.3349
saving model as: 0_saved_model.pth
Training epoch 8
	step [1/255], loss=12.6627
	step [2/255], loss=11.7865
	step [3/255], loss=9.8339
	step [4/255], loss=11.2338
	step [5/255], loss=13.7691
	step [6/255], loss=10.5706
	step [7/255], loss=10.5964
	step [8/255], loss=12.3337
	step [9/255], loss=12.0909
	step [10/255], loss=12.9335
	step [11/255], loss=11.5837
	step [12/255], loss=12.0723
	step [13/255], loss=11.8415
	step [14/255], loss=11.4047
	step [15/255], loss=12.4707
	step [16/255], loss=12.4102
	step [17/255], loss=12.0029
	step [18/255], loss=11.7519
	step [19/255], loss=14.2404
	step [20/255], loss=14.0065
	step [21/255], loss=12.1745
	step [22/255], loss=11.0302
	step [23/255], loss=11.3055
	step [24/255], loss=12.7561
	step [25/255], loss=10.9506
	step [26/255], loss=12.7009
	step [27/255], loss=14.3369
	step [28/255], loss=13.4308
	step [29/255], loss=12.1661
	step [30/255], loss=12.2144
	step [31/255], loss=12.4871
	step [32/255], loss=11.3839
	step [33/255], loss=10.6643
	step [34/255], loss=11.8475
	step [35/255], loss=12.5436
	step [36/255], loss=13.9181
	step [37/255], loss=10.7624
	step [38/255], loss=11.8790
	step [39/255], loss=11.9660
	step [40/255], loss=11.7068
	step [41/255], loss=12.8506
	step [42/255], loss=10.3008
	step [43/255], loss=11.7602
	step [44/255], loss=11.4336
	step [45/255], loss=13.4839
	step [46/255], loss=12.1752
	step [47/255], loss=11.0918
	step [48/255], loss=12.4788
	step [49/255], loss=12.2573
	step [50/255], loss=13.6134
	step [51/255], loss=12.9119
	step [52/255], loss=10.8317
	step [53/255], loss=11.2905
	step [54/255], loss=10.6745
	step [55/255], loss=10.3333
	step [56/255], loss=11.7714
	step [57/255], loss=10.9492
	step [58/255], loss=12.2778
	step [59/255], loss=11.5240
	step [60/255], loss=12.1842
	step [61/255], loss=11.0661
	step [62/255], loss=12.4540
	step [63/255], loss=13.4033
	step [64/255], loss=11.2016
	step [65/255], loss=11.5683
	step [66/255], loss=12.7177
	step [67/255], loss=11.0931
	step [68/255], loss=11.7722
	step [69/255], loss=9.6982
	step [70/255], loss=13.0220
	step [71/255], loss=10.3684
	step [72/255], loss=12.2052
	step [73/255], loss=12.3820
	step [74/255], loss=10.9153
	step [75/255], loss=10.7418
	step [76/255], loss=12.5388
	step [77/255], loss=10.7135
	step [78/255], loss=9.4110
	step [79/255], loss=11.2588
	step [80/255], loss=12.3395
	step [81/255], loss=12.5467
	step [82/255], loss=12.6297
	step [83/255], loss=12.5166
	step [84/255], loss=11.2840
	step [85/255], loss=13.3423
	step [86/255], loss=13.1130
	step [87/255], loss=14.1424
	step [88/255], loss=13.9735
	step [89/255], loss=11.5682
	step [90/255], loss=12.5102
	step [91/255], loss=11.7056
	step [92/255], loss=12.8649
	step [93/255], loss=11.5268
	step [94/255], loss=11.6331
	step [95/255], loss=12.6936
	step [96/255], loss=11.6076
	step [97/255], loss=11.5858
	step [98/255], loss=11.8424
	step [99/255], loss=10.8808
	step [100/255], loss=12.7414
	step [101/255], loss=12.6054
	step [102/255], loss=12.6172
	step [103/255], loss=12.1545
	step [104/255], loss=11.2315
	step [105/255], loss=10.9426
	step [106/255], loss=11.8695
	step [107/255], loss=13.2611
	step [108/255], loss=13.7742
	step [109/255], loss=11.7745
	step [110/255], loss=11.6890
	step [111/255], loss=11.6885
	step [112/255], loss=9.6883
	step [113/255], loss=11.7678
	step [114/255], loss=11.9633
	step [115/255], loss=10.9742
	step [116/255], loss=12.7127
	step [117/255], loss=11.7656
	step [118/255], loss=12.0508
	step [119/255], loss=13.1920
	step [120/255], loss=11.6064
	step [121/255], loss=14.0044
	step [122/255], loss=12.4286
	step [123/255], loss=12.1182
	step [124/255], loss=10.6802
	step [125/255], loss=11.8667
	step [126/255], loss=11.3413
	step [127/255], loss=12.3287
	step [128/255], loss=14.1113
	step [129/255], loss=11.0089
	step [130/255], loss=12.8526
	step [131/255], loss=13.5492
	step [132/255], loss=13.6142
	step [133/255], loss=12.0486
	step [134/255], loss=12.3408
	step [135/255], loss=12.2276
	step [136/255], loss=9.4906
	step [137/255], loss=11.2897
	step [138/255], loss=10.3076
	step [139/255], loss=10.3708
	step [140/255], loss=11.9509
	step [141/255], loss=13.3698
	step [142/255], loss=10.0338
	step [143/255], loss=11.9650
	step [144/255], loss=11.0096
	step [145/255], loss=12.1426
	step [146/255], loss=9.5947
	step [147/255], loss=12.0909
	step [148/255], loss=9.9892
	step [149/255], loss=10.6492
	step [150/255], loss=10.5559
	step [151/255], loss=9.7469
	step [152/255], loss=11.3900
	step [153/255], loss=11.2318
	step [154/255], loss=10.1968
	step [155/255], loss=10.3659
	step [156/255], loss=12.2994
	step [157/255], loss=11.2183
	step [158/255], loss=10.8523
	step [159/255], loss=11.9417
	step [160/255], loss=12.8226
	step [161/255], loss=12.5446
	step [162/255], loss=11.4400
	step [163/255], loss=12.5753
	step [164/255], loss=12.2221
	step [165/255], loss=10.4808
	step [166/255], loss=13.5466
	step [167/255], loss=10.7984
	step [168/255], loss=10.9294
	step [169/255], loss=12.1878
	step [170/255], loss=11.1267
	step [171/255], loss=11.2478
	step [172/255], loss=10.9513
	step [173/255], loss=10.5529
	step [174/255], loss=9.6531
	step [175/255], loss=12.0265
	step [176/255], loss=11.6783
	step [177/255], loss=11.7863
	step [178/255], loss=13.0393
	step [179/255], loss=11.6885
	step [180/255], loss=10.4073
	step [181/255], loss=10.9047
	step [182/255], loss=11.3253
	step [183/255], loss=11.9888
	step [184/255], loss=10.6630
	step [185/255], loss=12.3113
	step [186/255], loss=10.0064
	step [187/255], loss=10.7385
	step [188/255], loss=11.5274
	step [189/255], loss=10.9134
	step [190/255], loss=10.2067
	step [191/255], loss=12.3765
	step [192/255], loss=9.5857
	step [193/255], loss=10.7374
	step [194/255], loss=11.0801
	step [195/255], loss=9.0436
	step [196/255], loss=10.8187
	step [197/255], loss=11.7087
	step [198/255], loss=9.6407
	step [199/255], loss=11.0671
	step [200/255], loss=11.3696
	step [201/255], loss=10.5931
	step [202/255], loss=11.1362
	step [203/255], loss=10.8574
	step [204/255], loss=12.9635
	step [205/255], loss=14.0945
	step [206/255], loss=11.5364
	step [207/255], loss=12.1004
	step [208/255], loss=12.1329
	step [209/255], loss=10.7874
	step [210/255], loss=11.3142
	step [211/255], loss=11.4916
	step [212/255], loss=8.6792
	step [213/255], loss=11.4695
	step [214/255], loss=11.4059
	step [215/255], loss=11.2675
	step [216/255], loss=11.4002
	step [217/255], loss=12.8825
	step [218/255], loss=12.1899
	step [219/255], loss=9.7839
	step [220/255], loss=11.9926
	step [221/255], loss=10.3783
	step [222/255], loss=10.5621
	step [223/255], loss=11.8424
	step [224/255], loss=11.2706
	step [225/255], loss=11.2445
	step [226/255], loss=11.9300
	step [227/255], loss=11.4421
	step [228/255], loss=11.1935
	step [229/255], loss=10.5157
	step [230/255], loss=11.0526
	step [231/255], loss=10.3270
	step [232/255], loss=8.9446
	step [233/255], loss=10.0249
	step [234/255], loss=11.5983
	step [235/255], loss=10.9909
	step [236/255], loss=10.9927
	step [237/255], loss=10.8667
	step [238/255], loss=9.6026
	step [239/255], loss=11.3349
	step [240/255], loss=10.9219
	step [241/255], loss=10.0859
	step [242/255], loss=9.3555
	step [243/255], loss=14.8760
	step [244/255], loss=10.0417
	step [245/255], loss=10.0600
	step [246/255], loss=10.5537
	step [247/255], loss=9.9416
	step [248/255], loss=11.8384
	step [249/255], loss=10.2049
	step [250/255], loss=10.0438
	step [251/255], loss=11.2826
	step [252/255], loss=11.6600
	step [253/255], loss=9.2165
	step [254/255], loss=11.5244
	step [255/255], loss=7.9673
	Evaluating
	loss=0.0397, precision=0.2014, recall=0.9948, f1=0.3349
saving model as: 0_saved_model.pth
Training epoch 9
	step [1/255], loss=9.6817
	step [2/255], loss=8.4266
	step [3/255], loss=10.3312
	step [4/255], loss=11.8517
	step [5/255], loss=9.3587
	step [6/255], loss=10.3434
	step [7/255], loss=10.3872
	step [8/255], loss=10.0881
	step [9/255], loss=9.6241
	step [10/255], loss=9.3433
	step [11/255], loss=12.9362
	step [12/255], loss=13.6475
	step [13/255], loss=9.9505
	step [14/255], loss=11.7708
	step [15/255], loss=13.5453
	step [16/255], loss=11.1119
	step [17/255], loss=12.4831
	step [18/255], loss=9.3190
	step [19/255], loss=10.0901
	step [20/255], loss=12.2136
	step [21/255], loss=10.9613
	step [22/255], loss=9.0454
	step [23/255], loss=10.4872
	step [24/255], loss=10.3949
	step [25/255], loss=10.8612
	step [26/255], loss=10.1333
	step [27/255], loss=8.9826
	step [28/255], loss=11.0008
	step [29/255], loss=12.4416
	step [30/255], loss=11.9162
	step [31/255], loss=10.1601
	step [32/255], loss=11.2444
	step [33/255], loss=11.5087
	step [34/255], loss=9.9317
	step [35/255], loss=10.3033
	step [36/255], loss=10.5945
	step [37/255], loss=10.2165
	step [38/255], loss=11.6347
	step [39/255], loss=11.9279
	step [40/255], loss=9.9417
	step [41/255], loss=11.1165
	step [42/255], loss=11.2360
	step [43/255], loss=11.4394
	step [44/255], loss=9.7104
	step [45/255], loss=11.2850
	step [46/255], loss=9.8270
	step [47/255], loss=10.6001
	step [48/255], loss=12.3567
	step [49/255], loss=10.9261
	step [50/255], loss=11.1850
	step [51/255], loss=9.6900
	step [52/255], loss=11.4908
	step [53/255], loss=10.4787
	step [54/255], loss=11.4987
	step [55/255], loss=11.1898
	step [56/255], loss=10.6020
	step [57/255], loss=12.1693
	step [58/255], loss=13.2415
	step [59/255], loss=10.9628
	step [60/255], loss=10.1750
	step [61/255], loss=10.5362
	step [62/255], loss=13.6621
	step [63/255], loss=9.3943
	step [64/255], loss=11.3367
	step [65/255], loss=9.2885
	step [66/255], loss=10.7687
	step [67/255], loss=8.8174
	step [68/255], loss=10.8942
	step [69/255], loss=8.4813
	step [70/255], loss=10.5071
	step [71/255], loss=11.9713
	step [72/255], loss=9.9832
	step [73/255], loss=9.6172
	step [74/255], loss=12.0715
	step [75/255], loss=9.0881
	step [76/255], loss=10.4547
	step [77/255], loss=10.0488
	step [78/255], loss=11.0567
	step [79/255], loss=10.3351
	step [80/255], loss=11.1719
	step [81/255], loss=11.5371
	step [82/255], loss=11.4581
	step [83/255], loss=10.3375
	step [84/255], loss=11.3119
	step [85/255], loss=11.9292
	step [86/255], loss=12.9846
	step [87/255], loss=10.7377
	step [88/255], loss=10.8504
	step [89/255], loss=10.2836
	step [90/255], loss=8.9900
	step [91/255], loss=10.8313
	step [92/255], loss=9.3839
	step [93/255], loss=9.8710
	step [94/255], loss=12.4549
	step [95/255], loss=10.8142
	step [96/255], loss=9.7418
	step [97/255], loss=11.6551
	step [98/255], loss=9.5206
	step [99/255], loss=9.1859
	step [100/255], loss=9.0577
	step [101/255], loss=10.3201
	step [102/255], loss=12.7583
	step [103/255], loss=10.1312
	step [104/255], loss=11.4750
	step [105/255], loss=11.5150
	step [106/255], loss=13.6625
	step [107/255], loss=11.6983
	step [108/255], loss=10.6030
	step [109/255], loss=9.8756
	step [110/255], loss=9.7756
	step [111/255], loss=11.4042
	step [112/255], loss=10.9288
	step [113/255], loss=12.1859
	step [114/255], loss=12.1226
	step [115/255], loss=9.2516
	step [116/255], loss=9.6289
	step [117/255], loss=10.0331
	step [118/255], loss=9.9597
	step [119/255], loss=11.2984
	step [120/255], loss=8.2735
	step [121/255], loss=8.6206
	step [122/255], loss=10.9805
	step [123/255], loss=9.9532
	step [124/255], loss=8.5540
	step [125/255], loss=11.0025
	step [126/255], loss=11.5306
	step [127/255], loss=9.9855
	step [128/255], loss=9.3124
	step [129/255], loss=11.2779
	step [130/255], loss=10.7304
	step [131/255], loss=11.7771
	step [132/255], loss=10.3249
	step [133/255], loss=10.8217
	step [134/255], loss=11.2940
	step [135/255], loss=9.2436
	step [136/255], loss=13.3960
	step [137/255], loss=9.3667
	step [138/255], loss=9.6356
	step [139/255], loss=11.9662
	step [140/255], loss=12.9879
	step [141/255], loss=12.3348
	step [142/255], loss=9.7852
	step [143/255], loss=9.6421
	step [144/255], loss=11.8338
	step [145/255], loss=10.9002
	step [146/255], loss=10.1348
	step [147/255], loss=9.3973
	step [148/255], loss=10.8605
	step [149/255], loss=9.5789
	step [150/255], loss=10.3337
	step [151/255], loss=9.8054
	step [152/255], loss=10.9097
	step [153/255], loss=10.6003
	step [154/255], loss=9.9394
	step [155/255], loss=10.7483
	step [156/255], loss=8.7078
	step [157/255], loss=9.9103
	step [158/255], loss=11.9041
	step [159/255], loss=11.1455
	step [160/255], loss=11.2342
	step [161/255], loss=9.9127
	step [162/255], loss=10.6588
	step [163/255], loss=11.2432
	step [164/255], loss=8.1795
	step [165/255], loss=10.8405
	step [166/255], loss=9.3293
	step [167/255], loss=11.1706
	step [168/255], loss=9.1133
	step [169/255], loss=12.1814
	step [170/255], loss=10.0814
	step [171/255], loss=12.7870
	step [172/255], loss=9.1819
	step [173/255], loss=10.3107
	step [174/255], loss=10.8932
	step [175/255], loss=10.0803
	step [176/255], loss=11.6242
	step [177/255], loss=10.1273
	step [178/255], loss=11.6297
	step [179/255], loss=10.5475
	step [180/255], loss=10.8362
	step [181/255], loss=10.2270
	step [182/255], loss=13.9073
	step [183/255], loss=9.3972
	step [184/255], loss=9.3352
	step [185/255], loss=9.9581
	step [186/255], loss=10.1134
	step [187/255], loss=12.8691
	step [188/255], loss=11.0602
	step [189/255], loss=11.9973
	step [190/255], loss=11.7135
	step [191/255], loss=11.5318
	step [192/255], loss=11.6115
	step [193/255], loss=10.5863
	step [194/255], loss=9.2981
	step [195/255], loss=11.1964
	step [196/255], loss=10.6146
	step [197/255], loss=9.3291
	step [198/255], loss=9.3117
	step [199/255], loss=9.5408
	step [200/255], loss=9.6961
	step [201/255], loss=9.9027
	step [202/255], loss=10.8010
	step [203/255], loss=10.7479
	step [204/255], loss=10.7060
	step [205/255], loss=9.2316
	step [206/255], loss=10.6831
	step [207/255], loss=11.3319
	step [208/255], loss=9.5829
	step [209/255], loss=10.9557
	step [210/255], loss=12.1322
	step [211/255], loss=9.3316
	step [212/255], loss=10.1337
	step [213/255], loss=9.2807
	step [214/255], loss=11.2560
	step [215/255], loss=10.7180
	step [216/255], loss=11.3694
	step [217/255], loss=9.2791
	step [218/255], loss=9.4921
	step [219/255], loss=9.5182
	step [220/255], loss=11.8086
	step [221/255], loss=11.6398
	step [222/255], loss=11.1857
	step [223/255], loss=9.2077
	step [224/255], loss=8.9712
	step [225/255], loss=10.8211
	step [226/255], loss=8.6611
	step [227/255], loss=9.0537
	step [228/255], loss=10.6421
	step [229/255], loss=10.5399
	step [230/255], loss=12.0341
	step [231/255], loss=11.6998
	step [232/255], loss=11.3407
	step [233/255], loss=9.8731
	step [234/255], loss=11.5212
	step [235/255], loss=9.9533
	step [236/255], loss=10.4208
	step [237/255], loss=10.1984
	step [238/255], loss=10.4235
	step [239/255], loss=9.0564
	step [240/255], loss=9.4740
	step [241/255], loss=8.7658
	step [242/255], loss=8.6402
	step [243/255], loss=9.7705
	step [244/255], loss=9.6241
	step [245/255], loss=12.6749
	step [246/255], loss=12.2787
	step [247/255], loss=8.8588
	step [248/255], loss=11.0572
	step [249/255], loss=9.4972
	step [250/255], loss=9.1111
	step [251/255], loss=10.7073
	step [252/255], loss=10.9236
	step [253/255], loss=9.3952
	step [254/255], loss=9.5693
	step [255/255], loss=7.5692
	Evaluating
	loss=0.0387, precision=0.1626, recall=0.9957, f1=0.2795
Training epoch 10
	step [1/255], loss=9.6642
	step [2/255], loss=8.6656
	step [3/255], loss=10.4130
	step [4/255], loss=9.7815
	step [5/255], loss=9.7081
	step [6/255], loss=9.3825
	step [7/255], loss=9.7805
	step [8/255], loss=11.4567
	step [9/255], loss=10.2763
	step [10/255], loss=11.0896
	step [11/255], loss=9.9037
	step [12/255], loss=9.8807
	step [13/255], loss=8.7792
	step [14/255], loss=11.3248
	step [15/255], loss=9.4766
	step [16/255], loss=10.8977
	step [17/255], loss=9.4892
	step [18/255], loss=9.4120
	step [19/255], loss=12.0982
	step [20/255], loss=9.3013
	step [21/255], loss=8.6831
	step [22/255], loss=9.1978
	step [23/255], loss=9.5940
	step [24/255], loss=10.7645
	step [25/255], loss=9.3941
	step [26/255], loss=9.5640
	step [27/255], loss=11.4750
	step [28/255], loss=9.4790
	step [29/255], loss=10.0457
	step [30/255], loss=10.5535
	step [31/255], loss=10.0240
	step [32/255], loss=11.4664
	step [33/255], loss=9.0099
	step [34/255], loss=11.0813
	step [35/255], loss=8.9422
	step [36/255], loss=10.5526
	step [37/255], loss=9.4452
	step [38/255], loss=9.2243
	step [39/255], loss=9.2108
	step [40/255], loss=9.4666
	step [41/255], loss=10.2877
	step [42/255], loss=13.7831
	step [43/255], loss=8.4863
	step [44/255], loss=10.2536
	step [45/255], loss=10.9823
	step [46/255], loss=9.3544
	step [47/255], loss=9.0282
	step [48/255], loss=10.3068
	step [49/255], loss=11.1384
	step [50/255], loss=10.3901
	step [51/255], loss=11.0747
	step [52/255], loss=9.4697
	step [53/255], loss=11.1872
	step [54/255], loss=8.1213
	step [55/255], loss=8.8057
	step [56/255], loss=12.2244
	step [57/255], loss=9.6149
	step [58/255], loss=7.8999
	step [59/255], loss=8.8962
	step [60/255], loss=10.7257
	step [61/255], loss=11.1224
	step [62/255], loss=9.8461
	step [63/255], loss=9.0380
	step [64/255], loss=8.5902
	step [65/255], loss=10.1554
	step [66/255], loss=11.1275
	step [67/255], loss=9.1841
	step [68/255], loss=10.0984
	step [69/255], loss=9.4230
	step [70/255], loss=10.4970
	step [71/255], loss=10.4860
	step [72/255], loss=9.8052
	step [73/255], loss=11.9294
	step [74/255], loss=9.3899
	step [75/255], loss=8.8265
	step [76/255], loss=10.9027
	step [77/255], loss=10.3999
	step [78/255], loss=9.7423
	step [79/255], loss=10.5168
	step [80/255], loss=8.9198
	step [81/255], loss=10.0296
	step [82/255], loss=11.4201
	step [83/255], loss=9.3460
	step [84/255], loss=10.8360
	step [85/255], loss=8.4216
	step [86/255], loss=11.9634
	step [87/255], loss=12.9265
	step [88/255], loss=11.9732
	step [89/255], loss=10.6744
	step [90/255], loss=8.9079
	step [91/255], loss=10.1183
	step [92/255], loss=8.8472
	step [93/255], loss=11.5235
	step [94/255], loss=10.7466
	step [95/255], loss=9.8227
	step [96/255], loss=8.6342
	step [97/255], loss=9.3854
	step [98/255], loss=9.8701
	step [99/255], loss=9.9637
	step [100/255], loss=11.5951
	step [101/255], loss=10.3074
	step [102/255], loss=9.3639
	step [103/255], loss=11.5067
	step [104/255], loss=8.4228
	step [105/255], loss=9.2668
	step [106/255], loss=9.8227
	step [107/255], loss=9.4761
	step [108/255], loss=10.3290
	step [109/255], loss=10.4107
	step [110/255], loss=10.3828
	step [111/255], loss=9.1350
	step [112/255], loss=8.4671
	step [113/255], loss=8.6726
	step [114/255], loss=9.3927
	step [115/255], loss=10.9322
	step [116/255], loss=9.8383
	step [117/255], loss=9.2247
	step [118/255], loss=12.4235
	step [119/255], loss=10.4256
	step [120/255], loss=9.9413
	step [121/255], loss=10.0390
	step [122/255], loss=9.4920
	step [123/255], loss=10.3982
	step [124/255], loss=10.9726
	step [125/255], loss=9.1497
	step [126/255], loss=10.9044
	step [127/255], loss=7.9819
	step [128/255], loss=8.8320
	step [129/255], loss=9.0423
	step [130/255], loss=8.8412
	step [131/255], loss=10.2044
	step [132/255], loss=8.5624
	step [133/255], loss=9.1233
	step [134/255], loss=10.1927
	step [135/255], loss=8.3586
	step [136/255], loss=7.7095
	step [137/255], loss=9.2214
	step [138/255], loss=9.3490
	step [139/255], loss=12.4029
	step [140/255], loss=8.6867
	step [141/255], loss=9.8420
	step [142/255], loss=9.9758
	step [143/255], loss=8.8752
	step [144/255], loss=9.1897
	step [145/255], loss=8.0738
	step [146/255], loss=8.9334
	step [147/255], loss=9.6142
	step [148/255], loss=9.5480
	step [149/255], loss=9.6364
	step [150/255], loss=9.1467
	step [151/255], loss=10.0416
	step [152/255], loss=10.0905
	step [153/255], loss=8.2494
	step [154/255], loss=9.1740
	step [155/255], loss=9.5367
	step [156/255], loss=9.1695
	step [157/255], loss=9.0341
	step [158/255], loss=10.0881
	step [159/255], loss=9.5061
	step [160/255], loss=9.3102
	step [161/255], loss=10.7779
	step [162/255], loss=11.4018
	step [163/255], loss=9.2172
	step [164/255], loss=11.5306
	step [165/255], loss=9.8960
	step [166/255], loss=9.2290
	step [167/255], loss=10.3726
	step [168/255], loss=11.2491
	step [169/255], loss=10.8684
	step [170/255], loss=9.6156
	step [171/255], loss=8.7993
	step [172/255], loss=9.6296
	step [173/255], loss=8.6198
	step [174/255], loss=7.8184
	step [175/255], loss=9.2274
	step [176/255], loss=7.8698
	step [177/255], loss=11.2618
	step [178/255], loss=10.8992
	step [179/255], loss=8.9613
	step [180/255], loss=9.0802
	step [181/255], loss=10.0025
	step [182/255], loss=10.7983
	step [183/255], loss=9.7704
	step [184/255], loss=10.4192
	step [185/255], loss=9.0406
	step [186/255], loss=10.6679
	step [187/255], loss=10.4563
	step [188/255], loss=8.5242
	step [189/255], loss=9.3496
	step [190/255], loss=9.5881
	step [191/255], loss=11.1855
	step [192/255], loss=10.6095
	step [193/255], loss=10.2029
	step [194/255], loss=8.1734
	step [195/255], loss=10.3096
	step [196/255], loss=9.9746
	step [197/255], loss=10.2956
	step [198/255], loss=11.0559
	step [199/255], loss=10.5180
	step [200/255], loss=11.8204
	step [201/255], loss=9.5442
	step [202/255], loss=10.6721
	step [203/255], loss=9.7009
	step [204/255], loss=10.3983
	step [205/255], loss=8.4786
	step [206/255], loss=9.6825
	step [207/255], loss=8.2169
	step [208/255], loss=7.8983
	step [209/255], loss=9.0306
	step [210/255], loss=8.0092
	step [211/255], loss=11.2221
	step [212/255], loss=8.2575
	step [213/255], loss=10.8675
	step [214/255], loss=11.6149
	step [215/255], loss=8.5799
	step [216/255], loss=9.0457
	step [217/255], loss=9.9628
	step [218/255], loss=8.8081
	step [219/255], loss=8.6541
	step [220/255], loss=12.1437
	step [221/255], loss=9.1571
	step [222/255], loss=8.4177
	step [223/255], loss=8.2114
	step [224/255], loss=13.3117
	step [225/255], loss=11.0320
	step [226/255], loss=10.6831
	step [227/255], loss=8.8293
	step [228/255], loss=9.1307
	step [229/255], loss=8.5887
	step [230/255], loss=11.0697
	step [231/255], loss=9.4261
	step [232/255], loss=9.0212
	step [233/255], loss=8.7986
	step [234/255], loss=10.2429
	step [235/255], loss=8.5282
	step [236/255], loss=10.2054
	step [237/255], loss=9.0185
	step [238/255], loss=9.4218
	step [239/255], loss=9.4300
	step [240/255], loss=7.9910
	step [241/255], loss=7.9550
	step [242/255], loss=10.0469
	step [243/255], loss=10.3141
	step [244/255], loss=11.8199
	step [245/255], loss=10.3036
	step [246/255], loss=9.7663
	step [247/255], loss=12.0382
	step [248/255], loss=8.7894
	step [249/255], loss=9.5098
	step [250/255], loss=9.5904
	step [251/255], loss=8.4058
	step [252/255], loss=9.5901
	step [253/255], loss=9.1129
	step [254/255], loss=11.7476
	step [255/255], loss=7.8930
	Evaluating
	loss=0.0328, precision=0.1800, recall=0.9951, f1=0.3049
Training epoch 11
	step [1/255], loss=10.0678
	step [2/255], loss=8.7038
	step [3/255], loss=9.9853
	step [4/255], loss=8.2559
	step [5/255], loss=10.4160
	step [6/255], loss=8.9605
	step [7/255], loss=9.3172
	step [8/255], loss=10.4749
	step [9/255], loss=8.6811
	step [10/255], loss=9.4219
	step [11/255], loss=9.3553
	step [12/255], loss=7.8314
	step [13/255], loss=9.1212
	step [14/255], loss=8.9358
	step [15/255], loss=10.4452
	step [16/255], loss=8.4057
	step [17/255], loss=10.9899
	step [18/255], loss=9.7907
	step [19/255], loss=11.2347
	step [20/255], loss=9.5418
	step [21/255], loss=9.1380
	step [22/255], loss=7.9595
	step [23/255], loss=8.3098
	step [24/255], loss=9.4871
	step [25/255], loss=9.4328
	step [26/255], loss=9.5741
	step [27/255], loss=8.7761
	step [28/255], loss=8.5460
	step [29/255], loss=7.8703
	step [30/255], loss=7.8041
	step [31/255], loss=11.5784
	step [32/255], loss=9.1656
	step [33/255], loss=9.6054
	step [34/255], loss=10.2936
	step [35/255], loss=8.5165
	step [36/255], loss=8.7082
	step [37/255], loss=11.2111
	step [38/255], loss=9.9884
	step [39/255], loss=10.4144
	step [40/255], loss=8.8493
	step [41/255], loss=10.6039
	step [42/255], loss=8.8305
	step [43/255], loss=9.2457
	step [44/255], loss=9.9061
	step [45/255], loss=9.5868
	step [46/255], loss=9.5271
	step [47/255], loss=7.6647
	step [48/255], loss=10.2517
	step [49/255], loss=8.7852
	step [50/255], loss=8.7089
	step [51/255], loss=9.9291
	step [52/255], loss=9.6444
	step [53/255], loss=8.1361
	step [54/255], loss=7.4827
	step [55/255], loss=10.5243
	step [56/255], loss=10.4382
	step [57/255], loss=7.9046
	step [58/255], loss=8.3266
	step [59/255], loss=7.9411
	step [60/255], loss=8.8844
	step [61/255], loss=8.4270
	step [62/255], loss=9.1793
	step [63/255], loss=10.7452
	step [64/255], loss=8.2712
	step [65/255], loss=9.8061
	step [66/255], loss=9.5280
	step [67/255], loss=9.6349
	step [68/255], loss=9.6383
	step [69/255], loss=8.7996
	step [70/255], loss=9.5383
	step [71/255], loss=10.0094
	step [72/255], loss=9.2394
	step [73/255], loss=8.0242
	step [74/255], loss=8.1459
	step [75/255], loss=8.9028
	step [76/255], loss=7.8691
	step [77/255], loss=8.4118
	step [78/255], loss=8.2486
	step [79/255], loss=9.9466
	step [80/255], loss=10.5709
	step [81/255], loss=8.6730
	step [82/255], loss=8.9490
	step [83/255], loss=7.7489
	step [84/255], loss=9.7457
	step [85/255], loss=8.7036
	step [86/255], loss=9.3123
	step [87/255], loss=9.7070
	step [88/255], loss=10.9129
	step [89/255], loss=7.8034
	step [90/255], loss=8.1603
	step [91/255], loss=9.0626
	step [92/255], loss=10.2690
	step [93/255], loss=10.0669
	step [94/255], loss=9.3361
	step [95/255], loss=8.0904
	step [96/255], loss=9.9410
	step [97/255], loss=8.2747
	step [98/255], loss=9.4543
	step [99/255], loss=9.8833
	step [100/255], loss=9.1629
	step [101/255], loss=10.1353
	step [102/255], loss=9.2951
	step [103/255], loss=8.8243
	step [104/255], loss=8.0068
	step [105/255], loss=9.5617
	step [106/255], loss=9.6432
	step [107/255], loss=7.5869
	step [108/255], loss=10.2927
	step [109/255], loss=9.9140
	step [110/255], loss=8.3468
	step [111/255], loss=8.5410
	step [112/255], loss=7.4847
	step [113/255], loss=9.6382
	step [114/255], loss=9.4816
	step [115/255], loss=9.9283
	step [116/255], loss=10.4545
	step [117/255], loss=9.7088
	step [118/255], loss=9.3639
	step [119/255], loss=10.2759
	step [120/255], loss=8.3767
	step [121/255], loss=8.3740
	step [122/255], loss=9.9218
	step [123/255], loss=8.8431
	step [124/255], loss=10.2068
	step [125/255], loss=8.3424
	step [126/255], loss=10.1478
	step [127/255], loss=8.6557
	step [128/255], loss=8.3443
	step [129/255], loss=9.1620
	step [130/255], loss=9.0508
	step [131/255], loss=7.5928
	step [132/255], loss=7.5240
	step [133/255], loss=11.1259
	step [134/255], loss=9.2412
	step [135/255], loss=8.7986
	step [136/255], loss=9.6272
	step [137/255], loss=8.8495
	step [138/255], loss=9.2765
	step [139/255], loss=9.3705
	step [140/255], loss=7.8573
	step [141/255], loss=10.8128
	step [142/255], loss=9.5639
	step [143/255], loss=9.5309
	step [144/255], loss=8.9102
	step [145/255], loss=8.5303
	step [146/255], loss=12.1094
	step [147/255], loss=8.4649
	step [148/255], loss=8.9979
	step [149/255], loss=10.4735
	step [150/255], loss=9.5829
	step [151/255], loss=10.3641
	step [152/255], loss=8.7395
	step [153/255], loss=10.2081
	step [154/255], loss=9.1240
	step [155/255], loss=11.2740
	step [156/255], loss=10.9175
	step [157/255], loss=7.2072
	step [158/255], loss=8.4862
	step [159/255], loss=10.9898
	step [160/255], loss=9.2058
	step [161/255], loss=8.3575
	step [162/255], loss=9.0859
	step [163/255], loss=10.6767
	step [164/255], loss=9.4959
	step [165/255], loss=14.5787
	step [166/255], loss=8.0832
	step [167/255], loss=10.4996
	step [168/255], loss=9.3185
	step [169/255], loss=9.8637
	step [170/255], loss=10.3858
	step [171/255], loss=9.0008
	step [172/255], loss=8.2563
	step [173/255], loss=8.6878
	step [174/255], loss=8.4321
	step [175/255], loss=8.8625
	step [176/255], loss=9.7628
	step [177/255], loss=8.5783
	step [178/255], loss=8.8996
	step [179/255], loss=10.0012
	step [180/255], loss=7.6853
	step [181/255], loss=9.9934
	step [182/255], loss=9.1334
	step [183/255], loss=7.1330
	step [184/255], loss=9.8329
	step [185/255], loss=10.7737
	step [186/255], loss=9.1461
	step [187/255], loss=8.1023
	step [188/255], loss=9.0384
	step [189/255], loss=9.4180
	step [190/255], loss=8.9887
	step [191/255], loss=10.1049
	step [192/255], loss=8.8939
	step [193/255], loss=10.2172
	step [194/255], loss=8.0377
	step [195/255], loss=8.8457
	step [196/255], loss=8.9985
	step [197/255], loss=12.4173
	step [198/255], loss=8.9146
	step [199/255], loss=8.3115
	step [200/255], loss=8.3233
	step [201/255], loss=8.0587
	step [202/255], loss=10.1258
	step [203/255], loss=9.5233
	step [204/255], loss=9.5409
	step [205/255], loss=7.9230
	step [206/255], loss=7.5064
	step [207/255], loss=10.0708
	step [208/255], loss=9.6170
	step [209/255], loss=11.1044
	step [210/255], loss=8.0907
	step [211/255], loss=9.7898
	step [212/255], loss=8.4041
	step [213/255], loss=8.0511
	step [214/255], loss=10.0376
	step [215/255], loss=9.9807
	step [216/255], loss=8.2654
	step [217/255], loss=8.3247
	step [218/255], loss=8.6156
	step [219/255], loss=8.6167
	step [220/255], loss=10.4506
	step [221/255], loss=10.7337
	step [222/255], loss=9.1477
	step [223/255], loss=8.1358
	step [224/255], loss=8.8966
	step [225/255], loss=8.5021
	step [226/255], loss=10.2152
	step [227/255], loss=8.5844
	step [228/255], loss=8.9966
	step [229/255], loss=7.8121
	step [230/255], loss=9.8850
	step [231/255], loss=7.5488
	step [232/255], loss=9.6044
	step [233/255], loss=9.6644
	step [234/255], loss=7.5567
	step [235/255], loss=7.7975
	step [236/255], loss=8.3719
	step [237/255], loss=11.3583
	step [238/255], loss=9.6913
	step [239/255], loss=9.7740
	step [240/255], loss=8.3793
	step [241/255], loss=9.4245
	step [242/255], loss=11.0228
	step [243/255], loss=8.2685
	step [244/255], loss=7.8067
	step [245/255], loss=9.2701
	step [246/255], loss=6.8658
	step [247/255], loss=8.9227
	step [248/255], loss=11.4169
	step [249/255], loss=8.9173
	step [250/255], loss=10.0997
	step [251/255], loss=7.3776
	step [252/255], loss=9.2671
	step [253/255], loss=9.7282
	step [254/255], loss=8.5567
	step [255/255], loss=8.2637
	Evaluating
	loss=0.0334, precision=0.1625, recall=0.9959, f1=0.2794
Training epoch 12
	step [1/255], loss=9.4764
	step [2/255], loss=9.0921
	step [3/255], loss=9.8580
	step [4/255], loss=10.4262
	step [5/255], loss=8.8474
	step [6/255], loss=7.8187
	step [7/255], loss=11.3998
	step [8/255], loss=8.8989
	step [9/255], loss=8.6963
	step [10/255], loss=8.4377
	step [11/255], loss=8.9239
	step [12/255], loss=7.3171
	step [13/255], loss=10.1489
	step [14/255], loss=8.2012
	step [15/255], loss=7.3189
	step [16/255], loss=10.2572
	step [17/255], loss=7.1336
	step [18/255], loss=10.7189
	step [19/255], loss=9.8977
	step [20/255], loss=7.0732
	step [21/255], loss=10.2254
	step [22/255], loss=8.3291
	step [23/255], loss=7.5626
	step [24/255], loss=8.7197
	step [25/255], loss=9.5338
	step [26/255], loss=8.2768
	step [27/255], loss=8.9068
	step [28/255], loss=9.3127
	step [29/255], loss=6.8681
	step [30/255], loss=7.7463
	step [31/255], loss=7.7803
	step [32/255], loss=8.4922
	step [33/255], loss=6.8063
	step [34/255], loss=7.6913
	step [35/255], loss=10.6259
	step [36/255], loss=8.8718
	step [37/255], loss=9.1663
	step [38/255], loss=8.0350
	step [39/255], loss=9.1788
	step [40/255], loss=7.4607
	step [41/255], loss=11.5743
	step [42/255], loss=8.3256
	step [43/255], loss=7.8861
	step [44/255], loss=9.1164
	step [45/255], loss=8.2099
	step [46/255], loss=7.4799
	step [47/255], loss=8.3845
	step [48/255], loss=7.7814
	step [49/255], loss=10.2044
	step [50/255], loss=8.0801
	step [51/255], loss=7.6463
	step [52/255], loss=7.9548
	step [53/255], loss=7.4435
	step [54/255], loss=8.6225
	step [55/255], loss=8.7284
	step [56/255], loss=9.2477
	step [57/255], loss=9.1797
	step [58/255], loss=9.2068
	step [59/255], loss=8.3617
	step [60/255], loss=8.1768
	step [61/255], loss=9.2462
	step [62/255], loss=11.0530
	step [63/255], loss=10.3076
	step [64/255], loss=8.3434
	step [65/255], loss=9.9281
	step [66/255], loss=9.5325
	step [67/255], loss=9.9822
	step [68/255], loss=7.3373
	step [69/255], loss=10.8563
	step [70/255], loss=9.2184
	step [71/255], loss=9.9429
	step [72/255], loss=9.9001
	step [73/255], loss=8.2423
	step [74/255], loss=9.4930
	step [75/255], loss=7.7588
	step [76/255], loss=7.5509
	step [77/255], loss=9.0206
	step [78/255], loss=8.9652
	step [79/255], loss=8.3289
	step [80/255], loss=9.8231
	step [81/255], loss=8.0901
	step [82/255], loss=9.4069
	step [83/255], loss=8.0475
	step [84/255], loss=7.5703
	step [85/255], loss=7.5037
	step [86/255], loss=8.9130
	step [87/255], loss=8.1423
	step [88/255], loss=7.4649
	step [89/255], loss=12.2448
	step [90/255], loss=8.5392
	step [91/255], loss=8.6624
	step [92/255], loss=10.6132
	step [93/255], loss=8.4186
	step [94/255], loss=7.4202
	step [95/255], loss=7.8459
	step [96/255], loss=8.7489
	step [97/255], loss=11.8585
	step [98/255], loss=11.0726
	step [99/255], loss=8.6112
	step [100/255], loss=9.5861
	step [101/255], loss=9.6277
	step [102/255], loss=9.3858
	step [103/255], loss=8.1253
	step [104/255], loss=9.9687
	step [105/255], loss=10.1573
	step [106/255], loss=9.3766
	step [107/255], loss=10.8579
	step [108/255], loss=6.6803
	step [109/255], loss=9.1002
	step [110/255], loss=9.2847
	step [111/255], loss=7.8799
	step [112/255], loss=8.6562
	step [113/255], loss=9.1229
	step [114/255], loss=10.1670
	step [115/255], loss=7.5157
	step [116/255], loss=8.6686
	step [117/255], loss=9.0833
	step [118/255], loss=9.4526
	step [119/255], loss=6.8778
	step [120/255], loss=7.5978
	step [121/255], loss=12.3277
	step [122/255], loss=7.9191
	step [123/255], loss=9.7225
	step [124/255], loss=8.0824
	step [125/255], loss=9.8733
	step [126/255], loss=7.3934
	step [127/255], loss=8.7388
	step [128/255], loss=8.9635
	step [129/255], loss=8.2776
	step [130/255], loss=7.5048
	step [131/255], loss=9.4953
	step [132/255], loss=7.9049
	step [133/255], loss=6.9948
	step [134/255], loss=9.4482
	step [135/255], loss=8.8660
	step [136/255], loss=8.0027
	step [137/255], loss=8.5294
	step [138/255], loss=7.2119
	step [139/255], loss=9.1718
	step [140/255], loss=9.5890
	step [141/255], loss=10.1318
	step [142/255], loss=8.9078
	step [143/255], loss=8.4400
	step [144/255], loss=8.7351
	step [145/255], loss=7.2532
	step [146/255], loss=8.9468
	step [147/255], loss=9.4432
	step [148/255], loss=9.5456
	step [149/255], loss=7.2645
	step [150/255], loss=8.2136
	step [151/255], loss=8.2178
	step [152/255], loss=8.7184
	step [153/255], loss=9.1156
	step [154/255], loss=9.4429
	step [155/255], loss=9.5162
	step [156/255], loss=7.2331
	step [157/255], loss=12.5409
	step [158/255], loss=8.1914
	step [159/255], loss=8.0969
	step [160/255], loss=10.1009
	step [161/255], loss=7.6638
	step [162/255], loss=7.6503
	step [163/255], loss=7.6713
	step [164/255], loss=9.2681
	step [165/255], loss=10.3828
	step [166/255], loss=7.6410
	step [167/255], loss=7.0471
	step [168/255], loss=10.2319
	step [169/255], loss=9.7647
	step [170/255], loss=9.3467
	step [171/255], loss=8.6198
	step [172/255], loss=8.6360
	step [173/255], loss=7.6820
	step [174/255], loss=6.9202
	step [175/255], loss=7.7901
	step [176/255], loss=7.7123
	step [177/255], loss=10.1855
	step [178/255], loss=6.9153
	step [179/255], loss=7.2530
	step [180/255], loss=11.0499
	step [181/255], loss=10.7916
	step [182/255], loss=9.0852
	step [183/255], loss=7.9230
	step [184/255], loss=9.9161
	step [185/255], loss=9.6842
	step [186/255], loss=11.2969
	step [187/255], loss=9.2234
	step [188/255], loss=9.3768
	step [189/255], loss=9.4956
	step [190/255], loss=8.6603
	step [191/255], loss=9.0264
	step [192/255], loss=8.8752
	step [193/255], loss=8.5323
	step [194/255], loss=9.2307
	step [195/255], loss=9.1111
	step [196/255], loss=7.9964
	step [197/255], loss=7.2714
	step [198/255], loss=9.4452
	step [199/255], loss=10.6156
	step [200/255], loss=7.9829
	step [201/255], loss=6.9731
	step [202/255], loss=9.2607
	step [203/255], loss=9.9759
	step [204/255], loss=7.6419
	step [205/255], loss=8.8198
	step [206/255], loss=10.8946
	step [207/255], loss=9.6920
	step [208/255], loss=7.6205
	step [209/255], loss=8.1012
	step [210/255], loss=10.2494
	step [211/255], loss=9.6381
	step [212/255], loss=8.7813
	step [213/255], loss=7.2954
	step [214/255], loss=8.9132
	step [215/255], loss=8.3896
	step [216/255], loss=7.3231
	step [217/255], loss=8.0902
	step [218/255], loss=8.3311
	step [219/255], loss=8.1125
	step [220/255], loss=7.7897
	step [221/255], loss=7.1711
	step [222/255], loss=9.5287
	step [223/255], loss=8.9594
	step [224/255], loss=8.7368
	step [225/255], loss=7.8317
	step [226/255], loss=11.4346
	step [227/255], loss=9.7181
	step [228/255], loss=8.9220
	step [229/255], loss=8.1694
	step [230/255], loss=9.0384
	step [231/255], loss=8.7153
	step [232/255], loss=8.3000
	step [233/255], loss=8.7279
	step [234/255], loss=10.4897
	step [235/255], loss=8.9664
	step [236/255], loss=10.4183
	step [237/255], loss=8.2472
	step [238/255], loss=8.6061
	step [239/255], loss=9.5248
	step [240/255], loss=8.9837
	step [241/255], loss=10.3197
	step [242/255], loss=7.6393
	step [243/255], loss=8.5132
	step [244/255], loss=7.6013
	step [245/255], loss=8.7709
	step [246/255], loss=7.8446
	step [247/255], loss=8.2276
	step [248/255], loss=8.5087
	step [249/255], loss=5.8355
	step [250/255], loss=9.6243
	step [251/255], loss=7.9090
	step [252/255], loss=7.5383
	step [253/255], loss=9.2147
	step [254/255], loss=10.8004
	step [255/255], loss=7.8689
	Evaluating
	loss=0.0283, precision=0.2039, recall=0.9947, f1=0.3384
saving model as: 0_saved_model.pth
Training epoch 13
	step [1/255], loss=8.6018
	step [2/255], loss=8.9721
	step [3/255], loss=10.5778
	step [4/255], loss=12.3272
	step [5/255], loss=8.9387
	step [6/255], loss=7.7465
	step [7/255], loss=7.3342
	step [8/255], loss=9.2363
	step [9/255], loss=7.5166
	step [10/255], loss=7.8021
	step [11/255], loss=8.6151
	step [12/255], loss=8.4298
	step [13/255], loss=9.6130
	step [14/255], loss=8.6734
	step [15/255], loss=10.6506
	step [16/255], loss=9.6095
	step [17/255], loss=11.1685
	step [18/255], loss=8.1294
	step [19/255], loss=8.2009
	step [20/255], loss=8.4577
	step [21/255], loss=8.6369
	step [22/255], loss=7.8954
	step [23/255], loss=7.5430
	step [24/255], loss=8.4259
	step [25/255], loss=8.8215
	step [26/255], loss=9.1010
	step [27/255], loss=8.7936
	step [28/255], loss=7.5209
	step [29/255], loss=8.5255
	step [30/255], loss=9.6020
	step [31/255], loss=11.2253
	step [32/255], loss=7.5027
	step [33/255], loss=8.4058
	step [34/255], loss=7.1254
	step [35/255], loss=8.4663
	step [36/255], loss=9.2872
	step [37/255], loss=8.5596
	step [38/255], loss=9.6911
	step [39/255], loss=10.5393
	step [40/255], loss=7.8915
	step [41/255], loss=7.7814
	step [42/255], loss=6.5106
	step [43/255], loss=10.1743
	step [44/255], loss=8.0625
	step [45/255], loss=7.8370
	step [46/255], loss=8.7694
	step [47/255], loss=10.0537
	step [48/255], loss=9.6888
	step [49/255], loss=8.7583
	step [50/255], loss=8.3799
	step [51/255], loss=8.2581
	step [52/255], loss=7.7134
	step [53/255], loss=6.7685
	step [54/255], loss=8.5169
	step [55/255], loss=8.1232
	step [56/255], loss=8.5535
	step [57/255], loss=8.9969
	step [58/255], loss=8.0026
	step [59/255], loss=7.7914
	step [60/255], loss=7.0743
	step [61/255], loss=7.1063
	step [62/255], loss=10.9511
	step [63/255], loss=8.0995
	step [64/255], loss=7.2738
	step [65/255], loss=8.8402
	step [66/255], loss=8.8128
	step [67/255], loss=7.8776
	step [68/255], loss=9.3326
	step [69/255], loss=8.5649
	step [70/255], loss=8.2401
	step [71/255], loss=9.1387
	step [72/255], loss=7.1675
	step [73/255], loss=8.0646
	step [74/255], loss=10.1367
	step [75/255], loss=11.4952
	step [76/255], loss=10.4024
	step [77/255], loss=7.4695
	step [78/255], loss=7.2727
	step [79/255], loss=7.8342
	step [80/255], loss=7.9836
	step [81/255], loss=12.0083
	step [82/255], loss=10.5524
	step [83/255], loss=7.6261
	step [84/255], loss=8.3961
	step [85/255], loss=8.7563
	step [86/255], loss=10.9611
	step [87/255], loss=7.0078
	step [88/255], loss=9.8020
	step [89/255], loss=7.9280
	step [90/255], loss=9.6045
	step [91/255], loss=7.6203
	step [92/255], loss=7.9922
	step [93/255], loss=9.3367
	step [94/255], loss=9.1656
	step [95/255], loss=11.9596
	step [96/255], loss=8.4287
	step [97/255], loss=9.6334
	step [98/255], loss=7.3926
	step [99/255], loss=8.3818
	step [100/255], loss=8.4965
	step [101/255], loss=8.7669
	step [102/255], loss=10.2769
	step [103/255], loss=8.6148
	step [104/255], loss=8.8238
	step [105/255], loss=8.9736
	step [106/255], loss=9.1114
	step [107/255], loss=10.3340
	step [108/255], loss=8.6304
	step [109/255], loss=8.1896
	step [110/255], loss=6.2753
	step [111/255], loss=7.9582
	step [112/255], loss=8.2116
	step [113/255], loss=8.5070
	step [114/255], loss=9.6451
	step [115/255], loss=8.4490
	step [116/255], loss=9.9272
	step [117/255], loss=9.8020
	step [118/255], loss=7.4743
	step [119/255], loss=7.0531
	step [120/255], loss=7.3469
	step [121/255], loss=9.4259
	step [122/255], loss=9.1602
	step [123/255], loss=8.5599
	step [124/255], loss=8.9188
	step [125/255], loss=8.1034
	step [126/255], loss=9.1950
	step [127/255], loss=8.2005
	step [128/255], loss=8.5872
	step [129/255], loss=11.5680
	step [130/255], loss=10.0890
	step [131/255], loss=11.1918
	step [132/255], loss=8.8153
	step [133/255], loss=10.1705
	step [134/255], loss=7.5963
	step [135/255], loss=7.9601
	step [136/255], loss=7.8744
	step [137/255], loss=10.4052
	step [138/255], loss=7.3966
	step [139/255], loss=9.6298
	step [140/255], loss=8.5908
	step [141/255], loss=8.7372
	step [142/255], loss=8.6908
	step [143/255], loss=7.7764
	step [144/255], loss=6.0034
	step [145/255], loss=8.2506
	step [146/255], loss=10.8227
	step [147/255], loss=8.7484
	step [148/255], loss=7.3751
	step [149/255], loss=8.0754
	step [150/255], loss=8.5638
	step [151/255], loss=8.2695
	step [152/255], loss=8.2027
	step [153/255], loss=8.4893
	step [154/255], loss=7.6883
	step [155/255], loss=7.1731
	step [156/255], loss=8.8351
	step [157/255], loss=8.5607
	step [158/255], loss=10.8251
	step [159/255], loss=7.0811
	step [160/255], loss=7.7281
	step [161/255], loss=9.0151
	step [162/255], loss=6.6488
	step [163/255], loss=8.1409
	step [164/255], loss=9.7827
	step [165/255], loss=9.2997
	step [166/255], loss=9.9833
	step [167/255], loss=6.6519
	step [168/255], loss=6.8778
	step [169/255], loss=7.1790
	step [170/255], loss=8.0670
	step [171/255], loss=8.0722
	step [172/255], loss=7.1953
	step [173/255], loss=7.9258
	step [174/255], loss=9.5177
	step [175/255], loss=10.1528
	step [176/255], loss=7.7515
	step [177/255], loss=8.3886
	step [178/255], loss=8.8237
	step [179/255], loss=9.2939
	step [180/255], loss=8.2484
	step [181/255], loss=8.1247
	step [182/255], loss=6.7725
	step [183/255], loss=10.2241
	step [184/255], loss=6.7383
	step [185/255], loss=10.0904
	step [186/255], loss=7.1649
	step [187/255], loss=10.3018
	step [188/255], loss=8.5410
	step [189/255], loss=9.2959
	step [190/255], loss=7.0917
	step [191/255], loss=7.9089
	step [192/255], loss=6.9832
	step [193/255], loss=8.1531
	step [194/255], loss=8.0354
	step [195/255], loss=7.3487
	step [196/255], loss=7.9821
	step [197/255], loss=7.5136
	step [198/255], loss=9.2740
	step [199/255], loss=8.1719
	step [200/255], loss=7.8735
	step [201/255], loss=7.2891
	step [202/255], loss=8.2151
	step [203/255], loss=7.9690
	step [204/255], loss=7.5774
	step [205/255], loss=8.0084
	step [206/255], loss=9.1916
	step [207/255], loss=7.8725
	step [208/255], loss=7.0905
	step [209/255], loss=7.0810
	step [210/255], loss=9.9259
	step [211/255], loss=8.4975
	step [212/255], loss=7.9745
	step [213/255], loss=9.6505
	step [214/255], loss=8.9658
	step [215/255], loss=8.8122
	step [216/255], loss=7.7112
	step [217/255], loss=8.1860
	step [218/255], loss=6.7317
	step [219/255], loss=8.2844
	step [220/255], loss=7.7647
	step [221/255], loss=9.0953
	step [222/255], loss=7.8061
	step [223/255], loss=8.0390
	step [224/255], loss=5.5905
	step [225/255], loss=8.7570
	step [226/255], loss=7.2238
	step [227/255], loss=8.1015
	step [228/255], loss=10.1088
	step [229/255], loss=7.7042
	step [230/255], loss=7.5672
	step [231/255], loss=7.3733
	step [232/255], loss=9.2582
	step [233/255], loss=8.0391
	step [234/255], loss=7.7627
	step [235/255], loss=7.6971
	step [236/255], loss=7.7278
	step [237/255], loss=8.1212
	step [238/255], loss=7.4693
	step [239/255], loss=8.0664
	step [240/255], loss=8.3577
	step [241/255], loss=7.8969
	step [242/255], loss=7.7750
	step [243/255], loss=9.4341
	step [244/255], loss=6.9753
	step [245/255], loss=7.1532
	step [246/255], loss=8.0403
	step [247/255], loss=7.2901
	step [248/255], loss=8.5549
	step [249/255], loss=7.4673
	step [250/255], loss=8.4525
	step [251/255], loss=10.3436
	step [252/255], loss=8.0730
	step [253/255], loss=7.3776
	step [254/255], loss=11.1077
	step [255/255], loss=8.7975
	Evaluating
	loss=0.0294, precision=0.1611, recall=0.9960, f1=0.2774
Training epoch 14
	step [1/255], loss=7.5509
	step [2/255], loss=7.6626
	step [3/255], loss=8.6360
	step [4/255], loss=8.5087
	step [5/255], loss=8.9312
	step [6/255], loss=8.2977
	step [7/255], loss=7.0629
	step [8/255], loss=6.7555
	step [9/255], loss=7.9629
	step [10/255], loss=8.4434
	step [11/255], loss=9.1211
	step [12/255], loss=6.2166
	step [13/255], loss=6.9528
	step [14/255], loss=7.3695
	step [15/255], loss=8.4822
	step [16/255], loss=9.2886
	step [17/255], loss=7.1604
	step [18/255], loss=7.2265
	step [19/255], loss=8.7780
	step [20/255], loss=7.6304
	step [21/255], loss=8.3457
	step [22/255], loss=7.8014
	step [23/255], loss=9.3475
	step [24/255], loss=9.1306
	step [25/255], loss=7.7584
	step [26/255], loss=9.4216
	step [27/255], loss=7.8746
	step [28/255], loss=8.4495
	step [29/255], loss=8.6461
	step [30/255], loss=11.6993
	step [31/255], loss=9.3030
	step [32/255], loss=6.5505
	step [33/255], loss=8.0122
	step [34/255], loss=9.2298
	step [35/255], loss=7.5652
	step [36/255], loss=8.0149
	step [37/255], loss=8.7368
	step [38/255], loss=8.3920
	step [39/255], loss=11.4079
	step [40/255], loss=7.7709
	step [41/255], loss=7.4669
	step [42/255], loss=7.4931
	step [43/255], loss=6.2855
	step [44/255], loss=7.7168
	step [45/255], loss=7.8384
	step [46/255], loss=8.6730
	step [47/255], loss=8.6581
	step [48/255], loss=9.7266
	step [49/255], loss=9.2785
	step [50/255], loss=8.4876
	step [51/255], loss=7.7700
	step [52/255], loss=7.5399
	step [53/255], loss=8.4320
	step [54/255], loss=7.7513
	step [55/255], loss=6.9594
	step [56/255], loss=6.8768
	step [57/255], loss=8.0212
	step [58/255], loss=10.4371
	step [59/255], loss=9.1428
	step [60/255], loss=7.7278
	step [61/255], loss=10.8404
	step [62/255], loss=10.0094
	step [63/255], loss=7.8740
	step [64/255], loss=8.7818
	step [65/255], loss=8.3589
	step [66/255], loss=7.2343
	step [67/255], loss=8.8282
	step [68/255], loss=9.3201
	step [69/255], loss=6.5675
	step [70/255], loss=9.4311
	step [71/255], loss=7.7342
	step [72/255], loss=9.3090
	step [73/255], loss=7.7068
	step [74/255], loss=9.4175
	step [75/255], loss=6.7613
	step [76/255], loss=8.5499
	step [77/255], loss=7.7758
	step [78/255], loss=6.2769
	step [79/255], loss=7.8382
	step [80/255], loss=8.7249
	step [81/255], loss=11.1740
	step [82/255], loss=8.0114
	step [83/255], loss=8.0585
	step [84/255], loss=7.4858
	step [85/255], loss=8.1708
	step [86/255], loss=7.6585
	step [87/255], loss=9.1572
	step [88/255], loss=7.5349
	step [89/255], loss=8.9481
	step [90/255], loss=9.0056
	step [91/255], loss=7.4878
	step [92/255], loss=8.6445
	step [93/255], loss=7.9353
	step [94/255], loss=8.9155
	step [95/255], loss=9.3577
	step [96/255], loss=7.6297
	step [97/255], loss=10.0373
	step [98/255], loss=11.4452
	step [99/255], loss=9.1821
	step [100/255], loss=10.0238
	step [101/255], loss=9.2388
	step [102/255], loss=8.4447
	step [103/255], loss=7.7054
	step [104/255], loss=7.3438
	step [105/255], loss=7.2786
	step [106/255], loss=8.5851
	step [107/255], loss=8.8716
	step [108/255], loss=9.1150
	step [109/255], loss=8.6288
	step [110/255], loss=8.0027
	step [111/255], loss=9.4299
	step [112/255], loss=8.5740
	step [113/255], loss=6.8709
	step [114/255], loss=7.2077
	step [115/255], loss=6.6645
	step [116/255], loss=9.6380
	step [117/255], loss=8.8496
	step [118/255], loss=8.3138
	step [119/255], loss=8.3234
	step [120/255], loss=7.3133
	step [121/255], loss=8.8090
	step [122/255], loss=7.7079
	step [123/255], loss=7.5480
	step [124/255], loss=8.3311
	step [125/255], loss=6.8623
	step [126/255], loss=8.1202
	step [127/255], loss=7.6433
	step [128/255], loss=7.1150
	step [129/255], loss=7.5148
	step [130/255], loss=6.9508
	step [131/255], loss=7.8088
	step [132/255], loss=8.0910
	step [133/255], loss=7.5654
	step [134/255], loss=7.7088
	step [135/255], loss=9.0351
	step [136/255], loss=7.1476
	step [137/255], loss=9.5185
	step [138/255], loss=6.3958
	step [139/255], loss=7.0071
	step [140/255], loss=7.7162
	step [141/255], loss=8.1826
	step [142/255], loss=6.5268
	step [143/255], loss=9.0112
	step [144/255], loss=7.8178
	step [145/255], loss=6.4633
	step [146/255], loss=8.6144
	step [147/255], loss=8.5320
	step [148/255], loss=9.1410
	step [149/255], loss=7.1180
	step [150/255], loss=6.9706
	step [151/255], loss=8.1916
	step [152/255], loss=8.4650
	step [153/255], loss=6.8209
	step [154/255], loss=6.4438
	step [155/255], loss=7.3979
	step [156/255], loss=8.0785
	step [157/255], loss=7.9917
	step [158/255], loss=9.0703
	step [159/255], loss=9.9645
	step [160/255], loss=10.5328
	step [161/255], loss=8.4104
	step [162/255], loss=9.2729
	step [163/255], loss=7.1528
	step [164/255], loss=7.5642
	step [165/255], loss=7.6831
	step [166/255], loss=10.4358
	step [167/255], loss=7.4038
	step [168/255], loss=7.8954
	step [169/255], loss=6.5379
	step [170/255], loss=6.5810
	step [171/255], loss=8.9664
	step [172/255], loss=9.1850
	step [173/255], loss=7.9553
	step [174/255], loss=8.8975
	step [175/255], loss=7.9293
	step [176/255], loss=8.9508
	step [177/255], loss=6.7194
	step [178/255], loss=6.6282
	step [179/255], loss=6.9859
	step [180/255], loss=7.2842
	step [181/255], loss=8.3656
	step [182/255], loss=9.7664
	step [183/255], loss=8.4749
	step [184/255], loss=8.1535
	step [185/255], loss=7.4187
	step [186/255], loss=7.3566
	step [187/255], loss=8.8612
	step [188/255], loss=7.5058
	step [189/255], loss=9.2146
	step [190/255], loss=7.9843
	step [191/255], loss=8.0182
	step [192/255], loss=7.4340
	step [193/255], loss=7.9156
	step [194/255], loss=7.2517
	step [195/255], loss=8.8906
	step [196/255], loss=10.3050
	step [197/255], loss=8.9214
	step [198/255], loss=9.8407
	step [199/255], loss=6.5926
	step [200/255], loss=7.6241
	step [201/255], loss=7.6033
	step [202/255], loss=8.5416
	step [203/255], loss=7.6434
	step [204/255], loss=8.6268
	step [205/255], loss=8.1515
	step [206/255], loss=9.2596
	step [207/255], loss=6.5922
	step [208/255], loss=8.5293
	step [209/255], loss=6.8551
	step [210/255], loss=8.4296
	step [211/255], loss=7.4558
	step [212/255], loss=10.0007
	step [213/255], loss=7.1270
	step [214/255], loss=7.6941
	step [215/255], loss=7.8306
	step [216/255], loss=6.9931
	step [217/255], loss=6.8823
	step [218/255], loss=6.4238
	step [219/255], loss=8.3835
	step [220/255], loss=6.9528
	step [221/255], loss=7.1438
	step [222/255], loss=10.0008
	step [223/255], loss=10.0126
	step [224/255], loss=7.2213
	step [225/255], loss=8.2615
	step [226/255], loss=6.8810
	step [227/255], loss=8.1366
	step [228/255], loss=10.0384
	step [229/255], loss=7.2967
	step [230/255], loss=7.3500
	step [231/255], loss=7.5982
	step [232/255], loss=5.5627
	step [233/255], loss=8.7638
	step [234/255], loss=7.5108
	step [235/255], loss=9.5911
	step [236/255], loss=9.1042
	step [237/255], loss=9.1673
	step [238/255], loss=7.5365
	step [239/255], loss=7.9712
	step [240/255], loss=8.6070
	step [241/255], loss=8.6096
	step [242/255], loss=7.5685
	step [243/255], loss=8.4895
	step [244/255], loss=7.0969
	step [245/255], loss=10.3428
	step [246/255], loss=8.6605
	step [247/255], loss=7.5665
	step [248/255], loss=7.3527
	step [249/255], loss=9.0271
	step [250/255], loss=7.9930
	step [251/255], loss=7.5903
	step [252/255], loss=6.5214
	step [253/255], loss=9.1417
	step [254/255], loss=7.9012
	step [255/255], loss=6.5052
	Evaluating
	loss=0.0331, precision=0.1454, recall=0.9966, f1=0.2538
Training epoch 15
	step [1/255], loss=9.1776
	step [2/255], loss=7.8230
	step [3/255], loss=10.2331
	step [4/255], loss=8.0485
	step [5/255], loss=7.2761
	step [6/255], loss=8.0973
	step [7/255], loss=7.0727
	step [8/255], loss=7.3738
	step [9/255], loss=6.3462
	step [10/255], loss=8.3795
	step [11/255], loss=8.5788
	step [12/255], loss=8.9149
	step [13/255], loss=9.0460
	step [14/255], loss=9.0296
	step [15/255], loss=7.3331
	step [16/255], loss=9.3116
	step [17/255], loss=8.3255
	step [18/255], loss=6.7502
	step [19/255], loss=8.0240
	step [20/255], loss=9.8949
	step [21/255], loss=5.8259
	step [22/255], loss=7.2714
	step [23/255], loss=9.0425
	step [24/255], loss=8.4277
	step [25/255], loss=8.6110
	step [26/255], loss=6.6937
	step [27/255], loss=6.5408
	step [28/255], loss=6.5679
	step [29/255], loss=8.1241
	step [30/255], loss=7.2437
	step [31/255], loss=7.6089
	step [32/255], loss=8.7718
	step [33/255], loss=9.3645
	step [34/255], loss=8.5595
	step [35/255], loss=9.1963
	step [36/255], loss=7.5096
	step [37/255], loss=6.2549
	step [38/255], loss=6.2370
	step [39/255], loss=7.4753
	step [40/255], loss=7.7746
	step [41/255], loss=8.4437
	step [42/255], loss=9.4017
	step [43/255], loss=11.2619
	step [44/255], loss=8.1576
	step [45/255], loss=8.0315
	step [46/255], loss=9.3931
	step [47/255], loss=7.9044
	step [48/255], loss=6.4215
	step [49/255], loss=7.5817
	step [50/255], loss=7.5089
	step [51/255], loss=7.0404
	step [52/255], loss=8.8466
	step [53/255], loss=8.3006
	step [54/255], loss=7.7078
	step [55/255], loss=9.4512
	step [56/255], loss=7.5665
	step [57/255], loss=7.7098
	step [58/255], loss=7.3829
	step [59/255], loss=7.8385
	step [60/255], loss=8.2354
	step [61/255], loss=8.2472
	step [62/255], loss=7.9447
	step [63/255], loss=6.6731
	step [64/255], loss=8.8082
	step [65/255], loss=7.1050
	step [66/255], loss=6.5317
	step [67/255], loss=9.3521
	step [68/255], loss=6.5415
	step [69/255], loss=7.8678
	step [70/255], loss=9.4651
	step [71/255], loss=7.8493
	step [72/255], loss=6.7030
	step [73/255], loss=7.6722
	step [74/255], loss=8.0667
	step [75/255], loss=8.7875
	step [76/255], loss=8.7321
	step [77/255], loss=11.1268
	step [78/255], loss=9.9149
	step [79/255], loss=7.6956
	step [80/255], loss=8.2388
	step [81/255], loss=7.6131
	step [82/255], loss=6.7569
	step [83/255], loss=8.0021
	step [84/255], loss=9.3960
	step [85/255], loss=7.7495
	step [86/255], loss=8.9696
	step [87/255], loss=10.9620
	step [88/255], loss=6.0590
	step [89/255], loss=6.7268
	step [90/255], loss=9.8511
	step [91/255], loss=8.2597
	step [92/255], loss=7.6884
	step [93/255], loss=7.6964
	step [94/255], loss=7.3382
	step [95/255], loss=9.2926
	step [96/255], loss=9.3726
	step [97/255], loss=7.4021
	step [98/255], loss=6.2655
	step [99/255], loss=11.9754
	step [100/255], loss=7.7709
	step [101/255], loss=8.2589
	step [102/255], loss=7.6907
	step [103/255], loss=8.0641
	step [104/255], loss=7.9288
	step [105/255], loss=7.5556
	step [106/255], loss=7.1334
	step [107/255], loss=9.4878
	step [108/255], loss=6.6794
	step [109/255], loss=7.4485
	step [110/255], loss=6.2158
	step [111/255], loss=6.8905
	step [112/255], loss=8.3551
	step [113/255], loss=10.8841
	step [114/255], loss=9.1357
	step [115/255], loss=8.3085
	step [116/255], loss=7.7059
	step [117/255], loss=8.1139
	step [118/255], loss=8.8860
	step [119/255], loss=7.4974
	step [120/255], loss=6.6262
	step [121/255], loss=8.8432
	step [122/255], loss=6.9547
	step [123/255], loss=8.2288
	step [124/255], loss=7.3585
	step [125/255], loss=6.2985
	step [126/255], loss=6.6067
	step [127/255], loss=6.7035
	step [128/255], loss=7.1239
	step [129/255], loss=8.8831
	step [130/255], loss=8.0767
	step [131/255], loss=7.1358
	step [132/255], loss=6.9394
	step [133/255], loss=8.7567
	step [134/255], loss=7.6578
	step [135/255], loss=8.1508
	step [136/255], loss=7.2094
	step [137/255], loss=8.3266
	step [138/255], loss=9.9272
	step [139/255], loss=9.7265
	step [140/255], loss=7.5807
	step [141/255], loss=7.8719
	step [142/255], loss=5.9500
	step [143/255], loss=11.3100
	step [144/255], loss=7.0579
	step [145/255], loss=7.5118
	step [146/255], loss=8.1433
	step [147/255], loss=7.0084
	step [148/255], loss=8.7929
	step [149/255], loss=7.5559
	step [150/255], loss=8.8791
	step [151/255], loss=7.6606
	step [152/255], loss=6.5960
	step [153/255], loss=7.3085
	step [154/255], loss=8.4167
	step [155/255], loss=10.9234
	step [156/255], loss=6.8276
	step [157/255], loss=8.0062
	step [158/255], loss=8.0177
	step [159/255], loss=8.1152
	step [160/255], loss=8.4479
	step [161/255], loss=8.4327
	step [162/255], loss=7.7338
	step [163/255], loss=7.5321
	step [164/255], loss=8.4160
	step [165/255], loss=7.5194
	step [166/255], loss=7.0323
	step [167/255], loss=7.8392
	step [168/255], loss=9.2839
	step [169/255], loss=7.1512
	step [170/255], loss=8.3624
	step [171/255], loss=7.2730
	step [172/255], loss=6.4719
	step [173/255], loss=7.1184
	step [174/255], loss=10.2934
	step [175/255], loss=7.5129
	step [176/255], loss=6.8440
	step [177/255], loss=9.0412
	step [178/255], loss=7.1978
	step [179/255], loss=8.1927
	step [180/255], loss=6.7814
	step [181/255], loss=7.7052
	step [182/255], loss=8.6465
	step [183/255], loss=7.4527
	step [184/255], loss=7.3204
	step [185/255], loss=8.8701
	step [186/255], loss=7.0073
	step [187/255], loss=7.6433
	step [188/255], loss=6.5990
	step [189/255], loss=7.9488
	step [190/255], loss=6.3237
	step [191/255], loss=7.4383
	step [192/255], loss=6.7744
	step [193/255], loss=7.3761
	step [194/255], loss=9.3732
	step [195/255], loss=8.4435
	step [196/255], loss=8.7429
	step [197/255], loss=6.4683
	step [198/255], loss=7.9241
	step [199/255], loss=9.4152
	step [200/255], loss=6.8569
	step [201/255], loss=8.5462
	step [202/255], loss=7.9973
	step [203/255], loss=7.4798
	step [204/255], loss=7.3554
	step [205/255], loss=8.6316
	step [206/255], loss=8.8064
	step [207/255], loss=7.8195
	step [208/255], loss=9.5390
	step [209/255], loss=8.8062
	step [210/255], loss=6.5112
	step [211/255], loss=6.5626
	step [212/255], loss=7.6308
	step [213/255], loss=8.9585
	step [214/255], loss=7.1364
	step [215/255], loss=6.9976
	step [216/255], loss=8.5125
	step [217/255], loss=5.4111
	step [218/255], loss=7.5108
	step [219/255], loss=9.2471
	step [220/255], loss=7.5252
	step [221/255], loss=6.6795
	step [222/255], loss=8.7790
	step [223/255], loss=8.0605
	step [224/255], loss=7.4302
	step [225/255], loss=6.5655
	step [226/255], loss=9.6056
	step [227/255], loss=6.5190
	step [228/255], loss=8.9886
	step [229/255], loss=8.5068
	step [230/255], loss=7.9139
	step [231/255], loss=6.4601
	step [232/255], loss=8.6639
	step [233/255], loss=8.2489
	step [234/255], loss=8.0336
	step [235/255], loss=9.8094
	step [236/255], loss=8.5298
	step [237/255], loss=8.7158
	step [238/255], loss=7.2730
	step [239/255], loss=5.9413
	step [240/255], loss=8.4512
	step [241/255], loss=8.2608
	step [242/255], loss=6.8900
	step [243/255], loss=7.0489
	step [244/255], loss=6.6276
	step [245/255], loss=9.5075
	step [246/255], loss=8.8611
	step [247/255], loss=5.9370
	step [248/255], loss=7.3232
	step [249/255], loss=7.0234
	step [250/255], loss=5.5650
	step [251/255], loss=5.8611
	step [252/255], loss=8.2105
	step [253/255], loss=9.4542
	step [254/255], loss=10.3520
	step [255/255], loss=5.8290
	Evaluating
	loss=0.0277, precision=0.1754, recall=0.9956, f1=0.2982
Training epoch 16
	step [1/255], loss=6.3520
	step [2/255], loss=7.0421
	step [3/255], loss=8.9036
	step [4/255], loss=8.0704
	step [5/255], loss=7.4579
	step [6/255], loss=7.9238
	step [7/255], loss=8.1253
	step [8/255], loss=7.2094
	step [9/255], loss=6.6310
	step [10/255], loss=7.4178
	step [11/255], loss=6.7936
	step [12/255], loss=7.7203
	step [13/255], loss=6.4416
	step [14/255], loss=9.4736
	step [15/255], loss=7.3744
	step [16/255], loss=8.7426
	step [17/255], loss=7.4404
	step [18/255], loss=8.5426
	step [19/255], loss=8.3097
	step [20/255], loss=7.2633
	step [21/255], loss=7.1177
	step [22/255], loss=6.9575
	step [23/255], loss=6.3969
	step [24/255], loss=9.2389
	step [25/255], loss=10.1636
	step [26/255], loss=9.3534
	step [27/255], loss=10.4349
	step [28/255], loss=6.2599
	step [29/255], loss=6.9170
	step [30/255], loss=7.2540
	step [31/255], loss=6.3105
	step [32/255], loss=7.5134
	step [33/255], loss=6.9399
	step [34/255], loss=9.3014
	step [35/255], loss=6.5728
	step [36/255], loss=6.3052
	step [37/255], loss=6.6742
	step [38/255], loss=7.2409
	step [39/255], loss=7.5146
	step [40/255], loss=8.6267
	step [41/255], loss=8.5189
	step [42/255], loss=8.3244
	step [43/255], loss=8.7490
	step [44/255], loss=8.2621
	step [45/255], loss=9.0113
	step [46/255], loss=6.4489
	step [47/255], loss=7.3803
	step [48/255], loss=8.0634
	step [49/255], loss=8.9351
	step [50/255], loss=7.8398
	step [51/255], loss=6.2635
	step [52/255], loss=7.6656
	step [53/255], loss=8.0707
	step [54/255], loss=9.4261
	step [55/255], loss=7.4136
	step [56/255], loss=8.0220
	step [57/255], loss=7.6867
	step [58/255], loss=6.6751
	step [59/255], loss=7.2938
	step [60/255], loss=7.2814
	step [61/255], loss=8.0273
	step [62/255], loss=8.5364
	step [63/255], loss=6.8851
	step [64/255], loss=7.4757
	step [65/255], loss=8.0373
	step [66/255], loss=9.2271
	step [67/255], loss=7.2077
	step [68/255], loss=8.1999
	step [69/255], loss=6.8971
	step [70/255], loss=7.0829
	step [71/255], loss=7.6078
	step [72/255], loss=8.3123
	step [73/255], loss=5.8870
	step [74/255], loss=6.3438
	step [75/255], loss=7.8228
	step [76/255], loss=7.0150
	step [77/255], loss=10.1410
	step [78/255], loss=8.5754
	step [79/255], loss=7.4361
	step [80/255], loss=7.1616
	step [81/255], loss=6.5817
	step [82/255], loss=5.4232
	step [83/255], loss=6.7071
	step [84/255], loss=7.6447
	step [85/255], loss=6.8084
	step [86/255], loss=10.1681
	step [87/255], loss=8.4802
	step [88/255], loss=7.8613
	step [89/255], loss=10.1660
	step [90/255], loss=8.5165
	step [91/255], loss=9.7766
	step [92/255], loss=6.5506
	step [93/255], loss=7.8139
	step [94/255], loss=7.3211
	step [95/255], loss=9.4674
	step [96/255], loss=8.4316
	step [97/255], loss=8.2054
	step [98/255], loss=7.2235
	step [99/255], loss=8.5338
	step [100/255], loss=6.6806
	step [101/255], loss=7.9882
	step [102/255], loss=5.8048
	step [103/255], loss=6.9085
	step [104/255], loss=7.1856
	step [105/255], loss=7.3944
	step [106/255], loss=7.8130
	step [107/255], loss=8.4602
	step [108/255], loss=8.7178
	step [109/255], loss=7.3619
	step [110/255], loss=6.6973
	step [111/255], loss=6.3050
	step [112/255], loss=9.9270
	step [113/255], loss=7.4042
	step [114/255], loss=7.4275
	step [115/255], loss=7.2055
	step [116/255], loss=7.2409
	step [117/255], loss=8.9345
	step [118/255], loss=7.1726
	step [119/255], loss=8.1492
	step [120/255], loss=6.9910
	step [121/255], loss=8.9138
	step [122/255], loss=7.6853
	step [123/255], loss=11.0054
	step [124/255], loss=8.3130
	step [125/255], loss=9.2116
	step [126/255], loss=7.6543
	step [127/255], loss=7.6755
	step [128/255], loss=6.8600
	step [129/255], loss=9.5695
	step [130/255], loss=8.0544
	step [131/255], loss=7.5932
	step [132/255], loss=7.2999
	step [133/255], loss=6.4619
	step [134/255], loss=8.1468
	step [135/255], loss=8.4812
	step [136/255], loss=6.2386
	step [137/255], loss=7.5110
	step [138/255], loss=14.3061
	step [139/255], loss=8.2908
	step [140/255], loss=8.9230
	step [141/255], loss=6.4234
	step [142/255], loss=7.2347
	step [143/255], loss=6.6400
	step [144/255], loss=9.1600
	step [145/255], loss=8.4530
	step [146/255], loss=8.2199
	step [147/255], loss=7.2958
	step [148/255], loss=7.9087
	step [149/255], loss=7.0313
	step [150/255], loss=7.7837
	step [151/255], loss=7.2907
	step [152/255], loss=8.8894
	step [153/255], loss=7.3523
	step [154/255], loss=8.1585
	step [155/255], loss=8.2918
	step [156/255], loss=5.2175
	step [157/255], loss=7.7838
	step [158/255], loss=6.6636
	step [159/255], loss=7.8601
	step [160/255], loss=8.4398
	step [161/255], loss=7.4389
	step [162/255], loss=5.7303
	step [163/255], loss=5.9901
	step [164/255], loss=6.0003
	step [165/255], loss=6.7893
	step [166/255], loss=5.5815
	step [167/255], loss=7.0758
	step [168/255], loss=8.3243
	step [169/255], loss=7.0408
	step [170/255], loss=7.2227
	step [171/255], loss=6.6134
	step [172/255], loss=8.1472
	step [173/255], loss=6.3295
	step [174/255], loss=7.7725
	step [175/255], loss=7.6243
	step [176/255], loss=8.9903
	step [177/255], loss=6.3808
	step [178/255], loss=8.1100
	step [179/255], loss=10.2615
	step [180/255], loss=6.3301
	step [181/255], loss=7.2838
	step [182/255], loss=8.7237
	step [183/255], loss=7.5980
	step [184/255], loss=7.0985
	step [185/255], loss=7.5363
	step [186/255], loss=7.2398
	step [187/255], loss=6.9781
	step [188/255], loss=11.6413
	step [189/255], loss=6.2145
	step [190/255], loss=6.8288
	step [191/255], loss=9.0824
	step [192/255], loss=8.1914
	step [193/255], loss=6.6424
	step [194/255], loss=6.4559
	step [195/255], loss=7.8722
	step [196/255], loss=9.1664
	step [197/255], loss=7.9718
	step [198/255], loss=8.1650
	step [199/255], loss=7.2309
	step [200/255], loss=6.9800
	step [201/255], loss=6.9036
	step [202/255], loss=6.4449
	step [203/255], loss=6.5572
	step [204/255], loss=7.5063
	step [205/255], loss=6.5931
	step [206/255], loss=7.1749
	step [207/255], loss=8.9071
	step [208/255], loss=6.1638
	step [209/255], loss=6.0984
	step [210/255], loss=7.3147
	step [211/255], loss=7.0017
	step [212/255], loss=6.5035
	step [213/255], loss=7.4725
	step [214/255], loss=7.9939
	step [215/255], loss=6.2786
	step [216/255], loss=10.0442
	step [217/255], loss=6.8452
	step [218/255], loss=8.5408
	step [219/255], loss=5.7983
	step [220/255], loss=7.4013
	step [221/255], loss=6.9803
	step [222/255], loss=7.1560
	step [223/255], loss=6.6098
	step [224/255], loss=8.7870
	step [225/255], loss=6.4650
	step [226/255], loss=6.6561
	step [227/255], loss=8.4638
	step [228/255], loss=8.0625
	step [229/255], loss=11.2664
	step [230/255], loss=7.5324
	step [231/255], loss=6.2077
	step [232/255], loss=8.0565
	step [233/255], loss=9.3750
	step [234/255], loss=7.2435
	step [235/255], loss=7.9172
	step [236/255], loss=8.4981
	step [237/255], loss=9.2427
	step [238/255], loss=7.7848
	step [239/255], loss=7.0379
	step [240/255], loss=7.0717
	step [241/255], loss=7.3688
	step [242/255], loss=6.4166
	step [243/255], loss=8.3411
	step [244/255], loss=6.8699
	step [245/255], loss=7.7103
	step [246/255], loss=7.3635
	step [247/255], loss=5.6337
	step [248/255], loss=5.8551
	step [249/255], loss=6.3159
	step [250/255], loss=7.2410
	step [251/255], loss=8.2980
	step [252/255], loss=8.7659
	step [253/255], loss=7.1171
	step [254/255], loss=8.3895
	step [255/255], loss=5.0339
	Evaluating
	loss=0.0293, precision=0.1537, recall=0.9948, f1=0.2662
Training epoch 17
	step [1/255], loss=9.5666
	step [2/255], loss=6.7541
	step [3/255], loss=6.0134
	step [4/255], loss=7.3568
	step [5/255], loss=7.3468
	step [6/255], loss=9.3996
	step [7/255], loss=6.7719
	step [8/255], loss=6.9816
	step [9/255], loss=8.5929
	step [10/255], loss=7.1967
	step [11/255], loss=8.3233
	step [12/255], loss=6.2016
	step [13/255], loss=8.0344
	step [14/255], loss=5.7881
	step [15/255], loss=5.9100
	step [16/255], loss=10.0149
	step [17/255], loss=6.5498
	step [18/255], loss=8.4549
	step [19/255], loss=6.7041
	step [20/255], loss=7.9271
	step [21/255], loss=7.8015
	step [22/255], loss=7.8264
	step [23/255], loss=7.8117
	step [24/255], loss=8.4667
	step [25/255], loss=6.8501
	step [26/255], loss=7.4698
	step [27/255], loss=7.0504
	step [28/255], loss=5.9495
	step [29/255], loss=6.8252
	step [30/255], loss=5.4098
	step [31/255], loss=8.8973
	step [32/255], loss=6.7083
	step [33/255], loss=7.6532
	step [34/255], loss=6.3464
	step [35/255], loss=8.4896
	step [36/255], loss=7.2205
	step [37/255], loss=6.5495
	step [38/255], loss=9.1502
	step [39/255], loss=6.1460
	step [40/255], loss=6.7982
	step [41/255], loss=8.6284
	step [42/255], loss=7.3481
	step [43/255], loss=9.5861
	step [44/255], loss=8.9432
	step [45/255], loss=7.1348
	step [46/255], loss=7.7711
	step [47/255], loss=8.9200
	step [48/255], loss=6.5667
	step [49/255], loss=7.2283
	step [50/255], loss=6.5109
	step [51/255], loss=9.8239
	step [52/255], loss=7.0417
	step [53/255], loss=6.5569
	step [54/255], loss=7.0204
	step [55/255], loss=8.0241
	step [56/255], loss=7.3999
	step [57/255], loss=8.6336
	step [58/255], loss=7.0634
	step [59/255], loss=6.8274
	step [60/255], loss=7.6901
	step [61/255], loss=6.7190
	step [62/255], loss=7.3191
	step [63/255], loss=8.1273
	step [64/255], loss=7.7109
	step [65/255], loss=7.1846
	step [66/255], loss=7.4478
	step [67/255], loss=7.6514
	step [68/255], loss=6.1914
	step [69/255], loss=7.0938
	step [70/255], loss=8.6471
	step [71/255], loss=9.3645
	step [72/255], loss=7.0827
	step [73/255], loss=7.7249
	step [74/255], loss=8.3100
	step [75/255], loss=8.9350
	step [76/255], loss=7.2996
	step [77/255], loss=7.7280
	step [78/255], loss=6.3527
	step [79/255], loss=7.8418
	step [80/255], loss=6.7751
	step [81/255], loss=8.6505
	step [82/255], loss=8.8614
	step [83/255], loss=8.2121
	step [84/255], loss=6.5071
	step [85/255], loss=8.3644
	step [86/255], loss=7.4846
	step [87/255], loss=4.9757
	step [88/255], loss=6.9603
	step [89/255], loss=7.3607
	step [90/255], loss=7.2656
	step [91/255], loss=7.3950
	step [92/255], loss=7.7618
	step [93/255], loss=7.6398
	step [94/255], loss=8.9426
	step [95/255], loss=7.2026
	step [96/255], loss=5.8385
	step [97/255], loss=7.0485
	step [98/255], loss=7.4862
	step [99/255], loss=6.1685
	step [100/255], loss=8.1925
	step [101/255], loss=7.6314
	step [102/255], loss=6.9850
	step [103/255], loss=7.2922
	step [104/255], loss=7.1010
	step [105/255], loss=6.5806
	step [106/255], loss=7.4726
	step [107/255], loss=9.2756
	step [108/255], loss=8.1021
	step [109/255], loss=10.5043
	step [110/255], loss=7.0400
	step [111/255], loss=8.6868
	step [112/255], loss=9.9628
	step [113/255], loss=7.5348
	step [114/255], loss=8.0346
	step [115/255], loss=8.6212
	step [116/255], loss=6.6854
	step [117/255], loss=6.5632
	step [118/255], loss=7.0906
	step [119/255], loss=9.0943
	step [120/255], loss=6.9298
	step [121/255], loss=7.6765
	step [122/255], loss=9.8965
	step [123/255], loss=7.0932
	step [124/255], loss=7.4290
	step [125/255], loss=8.7576
	step [126/255], loss=7.1435
	step [127/255], loss=7.3814
	step [128/255], loss=8.6487
	step [129/255], loss=8.6103
	step [130/255], loss=7.6582
	step [131/255], loss=8.1685
	step [132/255], loss=6.1438
	step [133/255], loss=7.8086
	step [134/255], loss=7.2165
	step [135/255], loss=7.1647
	step [136/255], loss=7.8474
	step [137/255], loss=8.0382
	step [138/255], loss=6.7307
	step [139/255], loss=8.0413
	step [140/255], loss=6.3297
	step [141/255], loss=7.4345
	step [142/255], loss=6.6415
	step [143/255], loss=7.1722
	step [144/255], loss=7.0591
	step [145/255], loss=8.3007
	step [146/255], loss=7.3106
	step [147/255], loss=8.9769
	step [148/255], loss=5.8037
	step [149/255], loss=8.3247
	step [150/255], loss=8.5496
	step [151/255], loss=6.6339
	step [152/255], loss=7.0785
	step [153/255], loss=6.5816
	step [154/255], loss=7.0191
	step [155/255], loss=6.4693
	step [156/255], loss=9.2417
	step [157/255], loss=7.6931
	step [158/255], loss=6.5944
	step [159/255], loss=9.5089
	step [160/255], loss=7.4223
	step [161/255], loss=6.5104
	step [162/255], loss=8.3603
	step [163/255], loss=9.4991
	step [164/255], loss=6.7458
	step [165/255], loss=7.7912
	step [166/255], loss=9.0611
	step [167/255], loss=6.4341
	step [168/255], loss=7.1669
	step [169/255], loss=9.2479
	step [170/255], loss=7.7862
	step [171/255], loss=8.4405
	step [172/255], loss=7.0203
	step [173/255], loss=5.8216
	step [174/255], loss=6.6868
	step [175/255], loss=5.8727
	step [176/255], loss=8.0203
	step [177/255], loss=7.6661
	step [178/255], loss=8.9035
	step [179/255], loss=5.4539
	step [180/255], loss=6.0965
	step [181/255], loss=7.4999
	step [182/255], loss=7.4569
	step [183/255], loss=4.8902
	step [184/255], loss=7.0791
	step [185/255], loss=9.2555
	step [186/255], loss=8.3828
	step [187/255], loss=9.7190
	step [188/255], loss=7.8387
	step [189/255], loss=8.2667
	step [190/255], loss=7.6806
	step [191/255], loss=8.3515
	step [192/255], loss=7.0842
	step [193/255], loss=6.8875
	step [194/255], loss=6.5213
	step [195/255], loss=8.0624
	step [196/255], loss=8.7948
	step [197/255], loss=7.1439
	step [198/255], loss=6.6470
	step [199/255], loss=7.1627
	step [200/255], loss=8.5433
	step [201/255], loss=6.8767
	step [202/255], loss=7.6687
	step [203/255], loss=6.4908
	step [204/255], loss=6.7383
	step [205/255], loss=6.7001
	step [206/255], loss=7.8032
	step [207/255], loss=7.8855
	step [208/255], loss=5.1437
	step [209/255], loss=7.9733
	step [210/255], loss=6.9090
	step [211/255], loss=7.1198
	step [212/255], loss=8.2363
	step [213/255], loss=7.3886
	step [214/255], loss=8.2251
	step [215/255], loss=9.2822
	step [216/255], loss=8.7504
	step [217/255], loss=7.1137
	step [218/255], loss=8.1857
	step [219/255], loss=8.5265
	step [220/255], loss=7.7786
	step [221/255], loss=8.2083
	step [222/255], loss=8.2526
	step [223/255], loss=7.0000
	step [224/255], loss=9.5991
	step [225/255], loss=6.5632
	step [226/255], loss=6.7634
	step [227/255], loss=7.4126
	step [228/255], loss=8.8616
	step [229/255], loss=6.9042
	step [230/255], loss=7.0368
	step [231/255], loss=5.9315
	step [232/255], loss=8.2138
	step [233/255], loss=6.3948
	step [234/255], loss=7.2884
	step [235/255], loss=6.3280
	step [236/255], loss=6.4599
	step [237/255], loss=6.4600
	step [238/255], loss=6.7161
	step [239/255], loss=5.7162
	step [240/255], loss=7.5325
	step [241/255], loss=7.3974
	step [242/255], loss=8.4751
	step [243/255], loss=6.5638
	step [244/255], loss=7.1373
	step [245/255], loss=6.7391
	step [246/255], loss=5.9180
	step [247/255], loss=8.5170
	step [248/255], loss=7.9706
	step [249/255], loss=6.7578
	step [250/255], loss=8.5793
	step [251/255], loss=7.3186
	step [252/255], loss=7.0272
	step [253/255], loss=5.6833
	step [254/255], loss=7.0183
	step [255/255], loss=4.7537
	Evaluating
	loss=0.0324, precision=0.1373, recall=0.9964, f1=0.2414
Training epoch 18
	step [1/255], loss=8.4791
	step [2/255], loss=6.6741
	step [3/255], loss=8.1421
	step [4/255], loss=6.1093
	step [5/255], loss=7.9527
	step [6/255], loss=8.7252
	step [7/255], loss=6.3739
	step [8/255], loss=8.0945
	step [9/255], loss=7.4898
	step [10/255], loss=7.9184
	step [11/255], loss=6.0676
	step [12/255], loss=6.3827
	step [13/255], loss=7.7684
	step [14/255], loss=6.6258
	step [15/255], loss=7.8139
	step [16/255], loss=6.5364
	step [17/255], loss=7.9681
	step [18/255], loss=8.1834
	step [19/255], loss=7.0471
	step [20/255], loss=7.5178
	step [21/255], loss=7.0443
	step [22/255], loss=8.0127
	step [23/255], loss=6.9443
	step [24/255], loss=8.9176
	step [25/255], loss=7.6862
	step [26/255], loss=8.2354
	step [27/255], loss=7.9150
	step [28/255], loss=6.9199
	step [29/255], loss=6.7970
	step [30/255], loss=8.0450
	step [31/255], loss=8.0914
	step [32/255], loss=7.5768
	step [33/255], loss=7.3220
	step [34/255], loss=6.7921
	step [35/255], loss=9.4346
	step [36/255], loss=6.2210
	step [37/255], loss=9.2956
	step [38/255], loss=5.8660
	step [39/255], loss=8.5100
	step [40/255], loss=9.6687
	step [41/255], loss=7.6025
	step [42/255], loss=7.6584
	step [43/255], loss=8.9697
	step [44/255], loss=7.5591
	step [45/255], loss=5.8244
	step [46/255], loss=6.8595
	step [47/255], loss=6.6384
	step [48/255], loss=6.9966
	step [49/255], loss=7.7129
	step [50/255], loss=5.9124
	step [51/255], loss=7.4400
	step [52/255], loss=8.6680
	step [53/255], loss=7.1852
	step [54/255], loss=6.1609
	step [55/255], loss=8.4506
	step [56/255], loss=9.4907
	step [57/255], loss=7.6352
	step [58/255], loss=5.9804
	step [59/255], loss=8.1179
	step [60/255], loss=8.0106
	step [61/255], loss=7.0631
	step [62/255], loss=8.0701
	step [63/255], loss=6.1132
	step [64/255], loss=7.3522
	step [65/255], loss=7.0628
	step [66/255], loss=7.2659
	step [67/255], loss=8.5761
	step [68/255], loss=8.4163
	step [69/255], loss=7.2137
	step [70/255], loss=6.1013
	step [71/255], loss=6.5852
	step [72/255], loss=5.0804
	step [73/255], loss=8.9129
	step [74/255], loss=8.4728
	step [75/255], loss=8.8335
	step [76/255], loss=6.4124
	step [77/255], loss=8.6012
	step [78/255], loss=7.8805
	step [79/255], loss=7.4556
	step [80/255], loss=6.4655
	step [81/255], loss=6.0178
	step [82/255], loss=8.5206
	step [83/255], loss=8.0276
	step [84/255], loss=8.3278
	step [85/255], loss=5.7499
	step [86/255], loss=5.9911
	step [87/255], loss=5.9160
	step [88/255], loss=6.7028
	step [89/255], loss=6.5052
	step [90/255], loss=7.2357
	step [91/255], loss=7.2450
	step [92/255], loss=6.6551
	step [93/255], loss=9.0195
	step [94/255], loss=7.4122
	step [95/255], loss=6.1619
	step [96/255], loss=8.4789
	step [97/255], loss=8.5968
	step [98/255], loss=7.3551
	step [99/255], loss=6.3341
	step [100/255], loss=7.0200
	step [101/255], loss=7.7828
	step [102/255], loss=7.4697
	step [103/255], loss=9.4385
	step [104/255], loss=8.1582
	step [105/255], loss=5.6354
	step [106/255], loss=7.0807
	step [107/255], loss=6.8196
	step [108/255], loss=6.1798
	step [109/255], loss=6.0030
	step [110/255], loss=6.6762
	step [111/255], loss=8.6690
	step [112/255], loss=6.4725
	step [113/255], loss=8.1037
	step [114/255], loss=7.4089
	step [115/255], loss=8.2114
	step [116/255], loss=8.1524
	step [117/255], loss=6.8884
	step [118/255], loss=8.0372
	step [119/255], loss=7.4851
	step [120/255], loss=6.6417
	step [121/255], loss=7.6114
	step [122/255], loss=7.6129
	step [123/255], loss=6.2822
	step [124/255], loss=8.8774
	step [125/255], loss=7.0140
	step [126/255], loss=7.7022
	step [127/255], loss=6.4636
	step [128/255], loss=8.2650
	step [129/255], loss=5.6182
	step [130/255], loss=6.2090
	step [131/255], loss=7.1833
	step [132/255], loss=7.6176
	step [133/255], loss=6.9190
	step [134/255], loss=6.7879
	step [135/255], loss=6.5647
	step [136/255], loss=5.6003
	step [137/255], loss=6.6675
	step [138/255], loss=6.7867
	step [139/255], loss=8.1327
	step [140/255], loss=8.4658
	step [141/255], loss=6.8714
	step [142/255], loss=9.7777
	step [143/255], loss=5.9107
	step [144/255], loss=6.1194
	step [145/255], loss=6.3622
	step [146/255], loss=7.7231
	step [147/255], loss=8.0444
	step [148/255], loss=6.2935
	step [149/255], loss=9.0838
	step [150/255], loss=6.9003
	step [151/255], loss=6.1767
	step [152/255], loss=6.4898
	step [153/255], loss=9.9423
	step [154/255], loss=7.5172
	step [155/255], loss=8.1077
	step [156/255], loss=7.0959
	step [157/255], loss=7.3112
	step [158/255], loss=5.7165
	step [159/255], loss=6.8724
	step [160/255], loss=7.5863
	step [161/255], loss=7.0972
	step [162/255], loss=7.4624
	step [163/255], loss=7.4245
	step [164/255], loss=10.4125
	step [165/255], loss=7.1392
	step [166/255], loss=6.8045
	step [167/255], loss=6.3564
	step [168/255], loss=7.5911
	step [169/255], loss=8.7082
	step [170/255], loss=5.7066
	step [171/255], loss=6.1024
	step [172/255], loss=7.8410
	step [173/255], loss=7.0731
	step [174/255], loss=6.8050
	step [175/255], loss=7.7937
	step [176/255], loss=7.2781
	step [177/255], loss=6.7257
	step [178/255], loss=8.4826
	step [179/255], loss=7.9523
	step [180/255], loss=7.0431
	step [181/255], loss=5.6290
	step [182/255], loss=7.0527
	step [183/255], loss=6.1390
	step [184/255], loss=6.5206
	step [185/255], loss=5.9749
	step [186/255], loss=6.1692
	step [187/255], loss=7.6104
	step [188/255], loss=7.6252
	step [189/255], loss=5.8855
	step [190/255], loss=10.8683
	step [191/255], loss=6.3745
	step [192/255], loss=7.7493
	step [193/255], loss=7.5517
	step [194/255], loss=8.5636
	step [195/255], loss=6.3500
	step [196/255], loss=6.7083
	step [197/255], loss=7.0500
	step [198/255], loss=6.5717
	step [199/255], loss=6.0076
	step [200/255], loss=6.4874
	step [201/255], loss=7.3207
	step [202/255], loss=7.5088
	step [203/255], loss=8.7521
	step [204/255], loss=8.5394
	step [205/255], loss=5.8810
	step [206/255], loss=6.0920
	step [207/255], loss=7.5478
	step [208/255], loss=7.3799
	step [209/255], loss=8.4383
	step [210/255], loss=7.4116
	step [211/255], loss=8.6488
	step [212/255], loss=5.6583
	step [213/255], loss=10.4998
	step [214/255], loss=8.7182
	step [215/255], loss=6.2898
	step [216/255], loss=5.5986
	step [217/255], loss=6.8413
	step [218/255], loss=6.4288
	step [219/255], loss=8.7825
	step [220/255], loss=7.7502
	step [221/255], loss=6.6812
	step [222/255], loss=6.1251
	step [223/255], loss=8.5738
	step [224/255], loss=8.5376
	step [225/255], loss=6.5074
	step [226/255], loss=7.3560
	step [227/255], loss=8.5459
	step [228/255], loss=7.3409
	step [229/255], loss=5.6618
	step [230/255], loss=7.7160
	step [231/255], loss=7.3800
	step [232/255], loss=6.3264
	step [233/255], loss=6.3996
	step [234/255], loss=7.0295
	step [235/255], loss=7.5295
	step [236/255], loss=9.1743
	step [237/255], loss=7.3313
	step [238/255], loss=8.2591
	step [239/255], loss=6.9705
	step [240/255], loss=6.6001
	step [241/255], loss=7.2677
	step [242/255], loss=6.2469
	step [243/255], loss=9.8023
	step [244/255], loss=6.4456
	step [245/255], loss=6.9443
	step [246/255], loss=6.1949
	step [247/255], loss=7.2328
	step [248/255], loss=7.0156
	step [249/255], loss=7.9793
	step [250/255], loss=7.6019
	step [251/255], loss=7.3111
	step [252/255], loss=6.0108
	step [253/255], loss=7.2501
	step [254/255], loss=6.1322
	step [255/255], loss=5.4011
	Evaluating
	loss=0.0273, precision=0.1582, recall=0.9961, f1=0.2730
Training epoch 19
	step [1/255], loss=7.2557
	step [2/255], loss=7.5148
	step [3/255], loss=8.8444
	step [4/255], loss=7.6289
	step [5/255], loss=6.1983
	step [6/255], loss=6.0188
	step [7/255], loss=5.7318
	step [8/255], loss=6.5364
	step [9/255], loss=6.7752
	step [10/255], loss=6.9398
	step [11/255], loss=6.7256
	step [12/255], loss=7.1010
	step [13/255], loss=8.2521
	step [14/255], loss=7.8057
	step [15/255], loss=8.1948
	step [16/255], loss=10.0841
	step [17/255], loss=6.0322
	step [18/255], loss=7.3953
	step [19/255], loss=7.2144
	step [20/255], loss=8.4464
	step [21/255], loss=4.9555
	step [22/255], loss=6.5446
	step [23/255], loss=5.4652
	step [24/255], loss=5.8352
	step [25/255], loss=7.3363
	step [26/255], loss=7.4288
	step [27/255], loss=7.9374
	step [28/255], loss=7.5474
	step [29/255], loss=7.1327
	step [30/255], loss=6.6723
	step [31/255], loss=9.5352
	step [32/255], loss=6.6315
	step [33/255], loss=6.2014
	step [34/255], loss=8.5356
	step [35/255], loss=6.3577
	step [36/255], loss=8.5630
	step [37/255], loss=6.0696
	step [38/255], loss=6.6340
	step [39/255], loss=6.9281
	step [40/255], loss=7.3769
	step [41/255], loss=7.4246
	step [42/255], loss=7.6529
	step [43/255], loss=6.9424
	step [44/255], loss=5.7860
	step [45/255], loss=5.5719
	step [46/255], loss=7.7889
	step [47/255], loss=9.4220
	step [48/255], loss=6.1996
	step [49/255], loss=7.2633
	step [50/255], loss=5.4720
	step [51/255], loss=8.6334
	step [52/255], loss=7.0090
	step [53/255], loss=5.7425
	step [54/255], loss=9.2098
	step [55/255], loss=7.1033
	step [56/255], loss=6.7891
	step [57/255], loss=7.2574
	step [58/255], loss=5.7450
	step [59/255], loss=6.8508
	step [60/255], loss=6.2126
	step [61/255], loss=7.5900
	step [62/255], loss=7.0384
	step [63/255], loss=8.0861
	step [64/255], loss=7.5553
	step [65/255], loss=6.6832
	step [66/255], loss=7.9939
	step [67/255], loss=8.4763
	step [68/255], loss=6.3416
	step [69/255], loss=6.2203
	step [70/255], loss=7.7648
	step [71/255], loss=7.9129
	step [72/255], loss=8.7137
	step [73/255], loss=8.4463
	step [74/255], loss=6.5751
	step [75/255], loss=6.9750
	step [76/255], loss=8.8013
	step [77/255], loss=6.7277
	step [78/255], loss=6.0222
	step [79/255], loss=6.8588
	step [80/255], loss=7.6925
	step [81/255], loss=9.1079
	step [82/255], loss=7.0825
	step [83/255], loss=6.6456
	step [84/255], loss=7.6262
	step [85/255], loss=6.8123
	step [86/255], loss=7.3881
	step [87/255], loss=6.1568
	step [88/255], loss=6.7851
	step [89/255], loss=7.1979
	step [90/255], loss=8.7017
	step [91/255], loss=6.3816
	step [92/255], loss=7.5348
	step [93/255], loss=6.9893
	step [94/255], loss=6.6527
	step [95/255], loss=7.5219
	step [96/255], loss=8.6902
	step [97/255], loss=7.4016
	step [98/255], loss=7.1928
	step [99/255], loss=6.5359
	step [100/255], loss=5.6620
	step [101/255], loss=7.6796
	step [102/255], loss=7.0253
	step [103/255], loss=6.2350
	step [104/255], loss=5.9723
	step [105/255], loss=5.5929
	step [106/255], loss=6.8115
	step [107/255], loss=8.4236
	step [108/255], loss=7.0309
	step [109/255], loss=7.0371
	step [110/255], loss=8.7663
	step [111/255], loss=6.7122
	step [112/255], loss=7.4094
	step [113/255], loss=6.4261
	step [114/255], loss=7.4527
	step [115/255], loss=5.7758
	step [116/255], loss=6.4533
	step [117/255], loss=6.6729
	step [118/255], loss=7.8632
	step [119/255], loss=8.9915
	step [120/255], loss=6.5490
	step [121/255], loss=6.8188
	step [122/255], loss=6.4061
	step [123/255], loss=6.6703
	step [124/255], loss=9.3322
	step [125/255], loss=6.4278
	step [126/255], loss=8.5235
	step [127/255], loss=6.6757
	step [128/255], loss=9.3861
	step [129/255], loss=7.0006
	step [130/255], loss=7.9369
	step [131/255], loss=7.2701
	step [132/255], loss=6.1589
	step [133/255], loss=9.1726
	step [134/255], loss=5.8553
	step [135/255], loss=6.9334
	step [136/255], loss=8.2278
	step [137/255], loss=7.3404
	step [138/255], loss=6.6895
	step [139/255], loss=6.8889
	step [140/255], loss=7.7224
	step [141/255], loss=8.9628
	step [142/255], loss=7.6797
	step [143/255], loss=6.4509
	step [144/255], loss=7.2357
	step [145/255], loss=8.6660
	step [146/255], loss=6.6541
	step [147/255], loss=10.4448
	step [148/255], loss=9.7253
	step [149/255], loss=8.0628
	step [150/255], loss=8.5883
	step [151/255], loss=6.5269
	step [152/255], loss=7.4032
	step [153/255], loss=8.1500
	step [154/255], loss=7.1483
	step [155/255], loss=6.3362
	step [156/255], loss=6.2134
	step [157/255], loss=8.0007
	step [158/255], loss=5.5299
	step [159/255], loss=6.7706
	step [160/255], loss=7.1747
	step [161/255], loss=7.3339
	step [162/255], loss=7.4085
	step [163/255], loss=8.6580
	step [164/255], loss=5.9337
	step [165/255], loss=6.9019
	step [166/255], loss=6.9708
	step [167/255], loss=6.8942
	step [168/255], loss=7.3461
	step [169/255], loss=6.9791
	step [170/255], loss=5.9320
	step [171/255], loss=6.6885
	step [172/255], loss=8.0884
	step [173/255], loss=7.6936
	step [174/255], loss=9.3113
	step [175/255], loss=7.9253
	step [176/255], loss=7.2624
	step [177/255], loss=7.1282
	step [178/255], loss=8.2297
	step [179/255], loss=5.5539
	step [180/255], loss=7.6201
	step [181/255], loss=6.9421
	step [182/255], loss=6.9379
	step [183/255], loss=7.7770
	step [184/255], loss=7.7268
	step [185/255], loss=8.3678
	step [186/255], loss=7.5157
	step [187/255], loss=7.4082
	step [188/255], loss=6.0226
	step [189/255], loss=7.6523
	step [190/255], loss=6.2885
	step [191/255], loss=7.5786
	step [192/255], loss=5.8707
	step [193/255], loss=6.6580
	step [194/255], loss=7.7426
	step [195/255], loss=6.7845
	step [196/255], loss=6.8286
	step [197/255], loss=6.9235
	step [198/255], loss=7.1401
	step [199/255], loss=6.0551
	step [200/255], loss=6.0499
	step [201/255], loss=7.3500
	step [202/255], loss=7.7194
	step [203/255], loss=5.5462
	step [204/255], loss=8.5604
	step [205/255], loss=5.9253
	step [206/255], loss=9.0012
	step [207/255], loss=7.3383
	step [208/255], loss=6.2803
	step [209/255], loss=7.3809
	step [210/255], loss=6.0320
	step [211/255], loss=5.1639
	step [212/255], loss=6.3397
	step [213/255], loss=6.9191
	step [214/255], loss=8.1963
	step [215/255], loss=5.3313
	step [216/255], loss=6.5319
	step [217/255], loss=6.3922
	step [218/255], loss=5.6589
	step [219/255], loss=6.8555
	step [220/255], loss=6.2203
	step [221/255], loss=6.6849
	step [222/255], loss=7.8301
	step [223/255], loss=8.2494
	step [224/255], loss=5.2269
	step [225/255], loss=5.5616
	step [226/255], loss=7.7578
	step [227/255], loss=7.9563
	step [228/255], loss=5.9588
	step [229/255], loss=7.6799
	step [230/255], loss=7.8848
	step [231/255], loss=8.1942
	step [232/255], loss=7.7339
	step [233/255], loss=7.0476
	step [234/255], loss=5.3461
	step [235/255], loss=6.7731
	step [236/255], loss=5.5249
	step [237/255], loss=7.0874
	step [238/255], loss=5.9422
	step [239/255], loss=6.8224
	step [240/255], loss=7.5527
	step [241/255], loss=5.9196
	step [242/255], loss=7.3916
	step [243/255], loss=8.3844
	step [244/255], loss=8.1563
	step [245/255], loss=7.3545
	step [246/255], loss=6.7005
	step [247/255], loss=7.0283
	step [248/255], loss=8.4020
	step [249/255], loss=6.6691
	step [250/255], loss=6.9976
	step [251/255], loss=6.9476
	step [252/255], loss=6.3930
	step [253/255], loss=5.3779
	step [254/255], loss=8.1696
	step [255/255], loss=5.6535
	Evaluating
	loss=0.0249, precision=0.1699, recall=0.9946, f1=0.2903
Training epoch 20
	step [1/255], loss=8.0640
	step [2/255], loss=6.1182
	step [3/255], loss=7.1030
	step [4/255], loss=6.8345
	step [5/255], loss=7.4227
	step [6/255], loss=6.4630
	step [7/255], loss=5.8648
	step [8/255], loss=8.2706
	step [9/255], loss=5.5741
	step [10/255], loss=7.6484
	step [11/255], loss=6.9386
	step [12/255], loss=7.3146
	step [13/255], loss=6.8479
	step [14/255], loss=6.3042
	step [15/255], loss=8.2760
	step [16/255], loss=6.9334
	step [17/255], loss=8.5319
	step [18/255], loss=8.3817
	step [19/255], loss=7.0573
	step [20/255], loss=6.5695
	step [21/255], loss=7.2516
	step [22/255], loss=6.9093
	step [23/255], loss=6.1519
	step [24/255], loss=8.1145
	step [25/255], loss=6.3931
	step [26/255], loss=6.6121
	step [27/255], loss=6.6754
	step [28/255], loss=7.8294
	step [29/255], loss=8.0665
	step [30/255], loss=6.9180
	step [31/255], loss=6.9919
	step [32/255], loss=7.4183
	step [33/255], loss=6.2454
	step [34/255], loss=7.1184
	step [35/255], loss=6.6107
	step [36/255], loss=7.2828
	step [37/255], loss=6.4777
	step [38/255], loss=5.4551
	step [39/255], loss=6.6379
	step [40/255], loss=8.0508
	step [41/255], loss=7.2838
	step [42/255], loss=8.1297
	step [43/255], loss=6.6247
	step [44/255], loss=6.3341
	step [45/255], loss=8.4741
	step [46/255], loss=5.3704
	step [47/255], loss=5.3736
	step [48/255], loss=8.1300
	step [49/255], loss=7.4664
	step [50/255], loss=6.4025
	step [51/255], loss=5.4603
	step [52/255], loss=7.2131
	step [53/255], loss=7.7290
	step [54/255], loss=6.4973
	step [55/255], loss=7.5219
	step [56/255], loss=5.7937
	step [57/255], loss=6.0200
	step [58/255], loss=5.8524
	step [59/255], loss=6.4251
	step [60/255], loss=8.1416
	step [61/255], loss=6.8206
	step [62/255], loss=7.4792
	step [63/255], loss=7.5642
	step [64/255], loss=8.0246
	step [65/255], loss=7.7076
	step [66/255], loss=8.1725
	step [67/255], loss=7.2761
	step [68/255], loss=5.2295
	step [69/255], loss=5.8231
	step [70/255], loss=7.5872
	step [71/255], loss=5.8906
	step [72/255], loss=7.2057
	step [73/255], loss=9.0441
	step [74/255], loss=6.0347
	step [75/255], loss=5.9884
	step [76/255], loss=6.2576
	step [77/255], loss=6.7443
	step [78/255], loss=6.7927
	step [79/255], loss=5.6157
	step [80/255], loss=7.2094
	step [81/255], loss=7.0626
	step [82/255], loss=7.8541
	step [83/255], loss=8.3429
	step [84/255], loss=7.3368
	step [85/255], loss=6.1941
	step [86/255], loss=7.3195
	step [87/255], loss=6.8089
	step [88/255], loss=8.6732
	step [89/255], loss=5.9809
	step [90/255], loss=5.4529
	step [91/255], loss=8.1650
	step [92/255], loss=7.4322
	step [93/255], loss=6.6371
	step [94/255], loss=7.1713
	step [95/255], loss=6.3951
	step [96/255], loss=6.4239
	step [97/255], loss=5.5520
	step [98/255], loss=5.3770
	step [99/255], loss=6.3342
	step [100/255], loss=8.0674
	step [101/255], loss=6.3951
	step [102/255], loss=6.9604
	step [103/255], loss=6.8202
	step [104/255], loss=8.7641
	step [105/255], loss=7.5840
	step [106/255], loss=7.3544
	step [107/255], loss=7.9622
	step [108/255], loss=6.4173
	step [109/255], loss=6.1971
	step [110/255], loss=6.1244
	step [111/255], loss=7.3542
	step [112/255], loss=7.5872
	step [113/255], loss=9.5220
	step [114/255], loss=7.8938
	step [115/255], loss=7.8442
	step [116/255], loss=8.0446
	step [117/255], loss=5.9883
	step [118/255], loss=7.9448
	step [119/255], loss=6.1298
	step [120/255], loss=6.8215
	step [121/255], loss=6.4208
	step [122/255], loss=5.7032
	step [123/255], loss=5.2512
	step [124/255], loss=6.2122
	step [125/255], loss=6.8073
	step [126/255], loss=6.2749
	step [127/255], loss=6.2986
	step [128/255], loss=8.1323
	step [129/255], loss=6.7385
	step [130/255], loss=5.1517
	step [131/255], loss=7.3975
	step [132/255], loss=7.8257
	step [133/255], loss=5.7534
	step [134/255], loss=6.8304
	step [135/255], loss=6.4859
	step [136/255], loss=5.4348
	step [137/255], loss=5.4665
	step [138/255], loss=5.8674
	step [139/255], loss=6.7406
	step [140/255], loss=5.3265
	step [141/255], loss=6.8785
	step [142/255], loss=8.0103
	step [143/255], loss=8.2289
	step [144/255], loss=7.0460
	step [145/255], loss=7.3380
	step [146/255], loss=6.5741
	step [147/255], loss=8.0352
	step [148/255], loss=6.0642
	step [149/255], loss=7.4399
	step [150/255], loss=7.3140
	step [151/255], loss=6.1314
	step [152/255], loss=8.4950
	step [153/255], loss=6.2551
	step [154/255], loss=6.8980
	step [155/255], loss=7.3529
	step [156/255], loss=8.7323
	step [157/255], loss=5.9319
	step [158/255], loss=6.4977
	step [159/255], loss=7.0125
	step [160/255], loss=6.3446
	step [161/255], loss=7.8021
	step [162/255], loss=7.4322
	step [163/255], loss=7.2672
	step [164/255], loss=5.5496
	step [165/255], loss=9.2590
	step [166/255], loss=5.9835
	step [167/255], loss=5.9486
	step [168/255], loss=7.4605
	step [169/255], loss=7.8492
	step [170/255], loss=8.0195
	step [171/255], loss=6.5533
	step [172/255], loss=6.4794
	step [173/255], loss=8.7922
	step [174/255], loss=7.8499
	step [175/255], loss=6.6489
	step [176/255], loss=8.6380
	step [177/255], loss=6.6299
	step [178/255], loss=8.0518
	step [179/255], loss=5.8177
	step [180/255], loss=6.5870
	step [181/255], loss=8.2473
	step [182/255], loss=6.6012
	step [183/255], loss=5.5799
	step [184/255], loss=7.2577
	step [185/255], loss=6.4612
	step [186/255], loss=6.0290
	step [187/255], loss=9.4812
	step [188/255], loss=5.3395
	step [189/255], loss=7.3449
	step [190/255], loss=7.3797
	step [191/255], loss=7.2093
	step [192/255], loss=7.2156
	step [193/255], loss=6.1851
	step [194/255], loss=7.2381
	step [195/255], loss=7.8029
	step [196/255], loss=6.7585
	step [197/255], loss=7.3517
	step [198/255], loss=7.2896
	step [199/255], loss=6.1454
	step [200/255], loss=7.8841
	step [201/255], loss=9.1770
	step [202/255], loss=6.9564
	step [203/255], loss=7.0187
	step [204/255], loss=6.6582
	step [205/255], loss=7.6828
	step [206/255], loss=7.2349
	step [207/255], loss=7.6335
	step [208/255], loss=7.2436
	step [209/255], loss=7.0596
	step [210/255], loss=6.9261
	step [211/255], loss=7.5601
	step [212/255], loss=7.5079
	step [213/255], loss=8.1867
	step [214/255], loss=7.9443
	step [215/255], loss=6.8491
	step [216/255], loss=5.5852
	step [217/255], loss=7.8933
	step [218/255], loss=7.1470
	step [219/255], loss=7.7809
	step [220/255], loss=6.7322
	step [221/255], loss=7.2564
	step [222/255], loss=6.0457
	step [223/255], loss=5.6952
	step [224/255], loss=6.1549
	step [225/255], loss=7.4876
	step [226/255], loss=6.8534
	step [227/255], loss=7.9767
	step [228/255], loss=7.7552
	step [229/255], loss=6.6765
	step [230/255], loss=5.9332
	step [231/255], loss=7.9949
	step [232/255], loss=6.8275
	step [233/255], loss=8.7771
	step [234/255], loss=6.5818
	step [235/255], loss=5.8441
	step [236/255], loss=8.6565
	step [237/255], loss=8.8301
	step [238/255], loss=6.3630
	step [239/255], loss=7.0131
	step [240/255], loss=7.7639
	step [241/255], loss=7.3899
	step [242/255], loss=6.8001
	step [243/255], loss=7.5478
	step [244/255], loss=6.2959
	step [245/255], loss=6.0793
	step [246/255], loss=6.2590
	step [247/255], loss=7.5804
	step [248/255], loss=6.5880
	step [249/255], loss=7.1730
	step [250/255], loss=8.8929
	step [251/255], loss=5.7879
	step [252/255], loss=7.4667
	step [253/255], loss=7.6861
	step [254/255], loss=7.2360
	step [255/255], loss=5.4087
	Evaluating
	loss=0.0274, precision=0.1570, recall=0.9958, f1=0.2712
Training epoch 21
	step [1/255], loss=7.4707
	step [2/255], loss=7.4254
	step [3/255], loss=7.3711
	step [4/255], loss=6.0422
	step [5/255], loss=5.9462
	step [6/255], loss=6.1577
	step [7/255], loss=7.2476
	step [8/255], loss=5.1817
	step [9/255], loss=6.1160
	step [10/255], loss=9.0535
	step [11/255], loss=7.5957
	step [12/255], loss=5.8564
	step [13/255], loss=6.9847
	step [14/255], loss=7.5008
	step [15/255], loss=7.5173
	step [16/255], loss=6.0243
	step [17/255], loss=7.8724
	step [18/255], loss=6.8076
	step [19/255], loss=6.6308
	step [20/255], loss=7.4965
	step [21/255], loss=5.0713
	step [22/255], loss=5.8714
	step [23/255], loss=6.0946
	step [24/255], loss=6.3656
	step [25/255], loss=8.0254
	step [26/255], loss=7.0362
	step [27/255], loss=7.8853
	step [28/255], loss=5.4485
	step [29/255], loss=6.6812
	step [30/255], loss=7.2239
	step [31/255], loss=5.8129
	step [32/255], loss=6.0578
	step [33/255], loss=5.8225
	step [34/255], loss=6.4172
	step [35/255], loss=6.9291
	step [36/255], loss=7.0982
	step [37/255], loss=6.6144
	step [38/255], loss=4.8288
	step [39/255], loss=7.5008
	step [40/255], loss=5.9826
	step [41/255], loss=6.7881
	step [42/255], loss=6.6352
	step [43/255], loss=6.5615
	step [44/255], loss=8.3681
	step [45/255], loss=5.5097
	step [46/255], loss=7.2432
	step [47/255], loss=7.6605
	step [48/255], loss=5.4086
	step [49/255], loss=6.2720
	step [50/255], loss=7.3689
	step [51/255], loss=5.9100
	step [52/255], loss=6.7723
	step [53/255], loss=7.2390
	step [54/255], loss=4.8249
	step [55/255], loss=8.6815
	step [56/255], loss=7.9618
	step [57/255], loss=8.0475
	step [58/255], loss=7.0237
	step [59/255], loss=9.0408
	step [60/255], loss=6.8278
	step [61/255], loss=6.1421
	step [62/255], loss=6.3528
	step [63/255], loss=5.7141
	step [64/255], loss=5.8403
	step [65/255], loss=7.0823
	step [66/255], loss=6.7993
	step [67/255], loss=6.2761
	step [68/255], loss=5.5921
	step [69/255], loss=6.4949
	step [70/255], loss=7.6237
	step [71/255], loss=7.9216
	step [72/255], loss=6.8805
	step [73/255], loss=7.6412
	step [74/255], loss=6.5840
	step [75/255], loss=6.1698
	step [76/255], loss=9.4212
	step [77/255], loss=6.2488
	step [78/255], loss=6.6529
	step [79/255], loss=5.6435
	step [80/255], loss=7.7483
	step [81/255], loss=6.9450
	step [82/255], loss=7.0846
	step [83/255], loss=6.9534
	step [84/255], loss=8.6572
	step [85/255], loss=7.0338
	step [86/255], loss=7.7475
	step [87/255], loss=6.2078
	step [88/255], loss=6.6565
	step [89/255], loss=7.3409
	step [90/255], loss=6.3517
	step [91/255], loss=8.7469
	step [92/255], loss=7.1372
	step [93/255], loss=8.0456
	step [94/255], loss=7.4973
	step [95/255], loss=7.6226
	step [96/255], loss=6.9874
	step [97/255], loss=7.4662
	step [98/255], loss=8.9535
	step [99/255], loss=8.3617
	step [100/255], loss=6.5219
	step [101/255], loss=7.3474
	step [102/255], loss=7.8438
	step [103/255], loss=7.7217
	step [104/255], loss=5.9883
	step [105/255], loss=7.8426
	step [106/255], loss=5.7120
	step [107/255], loss=7.8520
	step [108/255], loss=7.4531
	step [109/255], loss=7.4545
	step [110/255], loss=7.0322
	step [111/255], loss=8.2455
	step [112/255], loss=10.2965
	step [113/255], loss=5.9862
	step [114/255], loss=5.7543
	step [115/255], loss=6.1629
	step [116/255], loss=9.4774
	step [117/255], loss=6.8390
	step [118/255], loss=6.8654
	step [119/255], loss=7.8190
	step [120/255], loss=5.4511
	step [121/255], loss=6.2366
	step [122/255], loss=7.5514
	step [123/255], loss=5.7190
	step [124/255], loss=9.9034
	step [125/255], loss=7.9752
	step [126/255], loss=7.2719
	step [127/255], loss=5.3840
	step [128/255], loss=5.5692
	step [129/255], loss=7.1147
	step [130/255], loss=6.7908
	step [131/255], loss=9.9365
	step [132/255], loss=6.3686
	step [133/255], loss=5.7652
	step [134/255], loss=8.0846
	step [135/255], loss=6.3724
	step [136/255], loss=6.1074
	step [137/255], loss=6.2760
	step [138/255], loss=5.9391
	step [139/255], loss=6.8409
	step [140/255], loss=5.5032
	step [141/255], loss=7.4612
	step [142/255], loss=5.1260
	step [143/255], loss=5.3954
	step [144/255], loss=7.7042
	step [145/255], loss=5.7763
	step [146/255], loss=6.3628
	step [147/255], loss=7.3189
	step [148/255], loss=5.4222
	step [149/255], loss=6.9550
	step [150/255], loss=7.4449
	step [151/255], loss=7.1732
	step [152/255], loss=6.3071
	step [153/255], loss=5.6419
	step [154/255], loss=7.4793
	step [155/255], loss=7.6949
	step [156/255], loss=5.5253
	step [157/255], loss=6.9101
	step [158/255], loss=6.6409
	step [159/255], loss=5.3273
	step [160/255], loss=8.7259
	step [161/255], loss=5.3272
	step [162/255], loss=8.5375
	step [163/255], loss=6.5485
	step [164/255], loss=5.6219
	step [165/255], loss=8.7091
	step [166/255], loss=7.3455
	step [167/255], loss=7.0727
	step [168/255], loss=7.9790
	step [169/255], loss=6.3365
	step [170/255], loss=6.4769
	step [171/255], loss=7.3274
	step [172/255], loss=7.4462
	step [173/255], loss=6.5840
	step [174/255], loss=8.2667
	step [175/255], loss=5.4710
	step [176/255], loss=6.3772
	step [177/255], loss=9.4525
	step [178/255], loss=6.3837
	step [179/255], loss=5.6503
	step [180/255], loss=7.0077
	step [181/255], loss=7.9366
	step [182/255], loss=6.4540
	step [183/255], loss=5.4031
	step [184/255], loss=7.9480
	step [185/255], loss=7.4588
	step [186/255], loss=5.9709
	step [187/255], loss=7.1724
	step [188/255], loss=6.4038
	step [189/255], loss=8.5913
	step [190/255], loss=6.7509
	step [191/255], loss=4.5805
	step [192/255], loss=4.9416
	step [193/255], loss=7.7499
	step [194/255], loss=7.2911
	step [195/255], loss=6.6068
	step [196/255], loss=8.3341
	step [197/255], loss=6.2090
	step [198/255], loss=5.9264
	step [199/255], loss=7.9298
	step [200/255], loss=7.8180
	step [201/255], loss=6.1215
	step [202/255], loss=6.2515
	step [203/255], loss=6.7453
	step [204/255], loss=6.4263
	step [205/255], loss=6.2382
	step [206/255], loss=6.7296
	step [207/255], loss=5.1222
	step [208/255], loss=5.2060
	step [209/255], loss=8.5515
	step [210/255], loss=5.7233
	step [211/255], loss=5.2776
	step [212/255], loss=7.9088
	step [213/255], loss=8.1481
	step [214/255], loss=7.8342
	step [215/255], loss=6.3225
	step [216/255], loss=5.4669
	step [217/255], loss=7.6863
	step [218/255], loss=6.5455
	step [219/255], loss=6.3910
	step [220/255], loss=6.4215
	step [221/255], loss=7.9381
	step [222/255], loss=5.6783
	step [223/255], loss=7.4593
	step [224/255], loss=6.5722
	step [225/255], loss=7.7146
	step [226/255], loss=7.5414
	step [227/255], loss=6.8758
	step [228/255], loss=4.5705
	step [229/255], loss=6.6383
	step [230/255], loss=7.0332
	step [231/255], loss=8.2327
	step [232/255], loss=6.8391
	step [233/255], loss=7.5302
	step [234/255], loss=8.8048
	step [235/255], loss=6.0041
	step [236/255], loss=7.1436
	step [237/255], loss=8.0563
	step [238/255], loss=7.9689
	step [239/255], loss=7.2312
	step [240/255], loss=6.3211
	step [241/255], loss=6.2868
	step [242/255], loss=7.3119
	step [243/255], loss=7.4487
	step [244/255], loss=6.2017
	step [245/255], loss=7.6844
	step [246/255], loss=6.1118
	step [247/255], loss=6.6517
	step [248/255], loss=6.5686
	step [249/255], loss=8.1732
	step [250/255], loss=8.4969
	step [251/255], loss=4.7710
	step [252/255], loss=6.7716
	step [253/255], loss=7.0824
	step [254/255], loss=5.6986
	step [255/255], loss=4.4804
	Evaluating
	loss=0.0244, precision=0.1717, recall=0.9959, f1=0.2930
Training epoch 22
	step [1/255], loss=6.3353
	step [2/255], loss=6.2098
	step [3/255], loss=7.8260
	step [4/255], loss=6.1162
	step [5/255], loss=8.7739
	step [6/255], loss=8.0788
	step [7/255], loss=6.4809
	step [8/255], loss=6.9170
	step [9/255], loss=5.8279
	step [10/255], loss=8.1298
	step [11/255], loss=7.0826
	step [12/255], loss=6.7891
	step [13/255], loss=6.4158
	step [14/255], loss=7.2188
	step [15/255], loss=6.5290
	step [16/255], loss=5.1879
	step [17/255], loss=5.7020
	step [18/255], loss=6.9774
	step [19/255], loss=8.1786
	step [20/255], loss=8.1283
	step [21/255], loss=6.5473
	step [22/255], loss=6.2202
	step [23/255], loss=5.9796
	step [24/255], loss=5.2377
	step [25/255], loss=6.4076
	step [26/255], loss=7.2649
	step [27/255], loss=6.8664
	step [28/255], loss=5.6412
	step [29/255], loss=6.6386
	step [30/255], loss=6.2005
	step [31/255], loss=6.1758
	step [32/255], loss=7.6748
	step [33/255], loss=6.1280
	step [34/255], loss=6.6606
	step [35/255], loss=9.3610
	step [36/255], loss=5.6990
	step [37/255], loss=6.0141
	step [38/255], loss=5.7873
	step [39/255], loss=5.0897
	step [40/255], loss=6.9969
	step [41/255], loss=9.7176
	step [42/255], loss=6.5020
	step [43/255], loss=6.1659
	step [44/255], loss=6.7493
	step [45/255], loss=7.6227
	step [46/255], loss=7.3149
	step [47/255], loss=7.7154
	step [48/255], loss=7.0144
	step [49/255], loss=6.3338
	step [50/255], loss=8.3336
	step [51/255], loss=6.2150
	step [52/255], loss=6.5086
	step [53/255], loss=7.4594
	step [54/255], loss=6.4540
	step [55/255], loss=6.2175
	step [56/255], loss=6.5126
	step [57/255], loss=6.5414
	step [58/255], loss=5.9801
	step [59/255], loss=7.0327
	step [60/255], loss=5.8875
	step [61/255], loss=6.5693
	step [62/255], loss=9.4222
	step [63/255], loss=6.6346
	step [64/255], loss=6.4690
	step [65/255], loss=6.0807
	step [66/255], loss=6.9824
	step [67/255], loss=6.6064
	step [68/255], loss=6.4987
	step [69/255], loss=7.1886
	step [70/255], loss=4.9512
	step [71/255], loss=7.9550
	step [72/255], loss=6.5991
	step [73/255], loss=7.9169
	step [74/255], loss=5.7115
	step [75/255], loss=7.3235
	step [76/255], loss=5.8999
	step [77/255], loss=7.8571
	step [78/255], loss=5.4472
	step [79/255], loss=6.3903
	step [80/255], loss=6.5344
	step [81/255], loss=5.6609
	step [82/255], loss=7.5165
	step [83/255], loss=7.7648
	step [84/255], loss=6.9236
	step [85/255], loss=5.5922
	step [86/255], loss=6.1258
	step [87/255], loss=7.9890
	step [88/255], loss=6.3685
	step [89/255], loss=6.4261
	step [90/255], loss=6.4490
	step [91/255], loss=6.2727
	step [92/255], loss=6.1444
	step [93/255], loss=7.0168
	step [94/255], loss=8.7387
	step [95/255], loss=8.3403
	step [96/255], loss=5.1318
	step [97/255], loss=6.4906
	step [98/255], loss=5.6328
	step [99/255], loss=5.7189
	step [100/255], loss=6.5222
	step [101/255], loss=7.4280
	step [102/255], loss=6.1248
	step [103/255], loss=7.9249
	step [104/255], loss=7.4561
	step [105/255], loss=5.3243
	step [106/255], loss=5.6835
	step [107/255], loss=5.9854
	step [108/255], loss=6.3926
	step [109/255], loss=7.5973
	step [110/255], loss=6.8789
	step [111/255], loss=6.5047
	step [112/255], loss=6.0819
	step [113/255], loss=6.0396
	step [114/255], loss=6.1863
	step [115/255], loss=5.4204
	step [116/255], loss=5.1430
	step [117/255], loss=5.6604
	step [118/255], loss=6.3953
	step [119/255], loss=7.7966
	step [120/255], loss=6.8437
	step [121/255], loss=6.0855
	step [122/255], loss=6.5915
	step [123/255], loss=7.7190
	step [124/255], loss=7.0898
	step [125/255], loss=6.1636
	step [126/255], loss=7.3911
	step [127/255], loss=6.5051
	step [128/255], loss=5.8768
	step [129/255], loss=6.5920
	step [130/255], loss=7.9794
	step [131/255], loss=5.5379
	step [132/255], loss=6.2307
	step [133/255], loss=7.1844
	step [134/255], loss=6.6150
	step [135/255], loss=6.9031
	step [136/255], loss=6.4679
	step [137/255], loss=7.2556
	step [138/255], loss=6.8701
	step [139/255], loss=7.0467
	step [140/255], loss=7.7558
	step [141/255], loss=5.8552
	step [142/255], loss=5.7687
	step [143/255], loss=6.7254
	step [144/255], loss=5.5318
	step [145/255], loss=6.1271
	step [146/255], loss=6.8393
	step [147/255], loss=6.8531
	step [148/255], loss=6.4802
	step [149/255], loss=5.7636
	step [150/255], loss=5.5761
	step [151/255], loss=6.1931
	step [152/255], loss=5.6239
	step [153/255], loss=6.8116
	step [154/255], loss=6.2319
	step [155/255], loss=6.9265
	step [156/255], loss=7.1429
	step [157/255], loss=8.3250
	step [158/255], loss=6.9442
	step [159/255], loss=5.3110
	step [160/255], loss=6.8082
	step [161/255], loss=6.4404
	step [162/255], loss=7.2740
	step [163/255], loss=7.5352
	step [164/255], loss=5.2611
	step [165/255], loss=6.8553
	step [166/255], loss=6.1848
	step [167/255], loss=5.0802
	step [168/255], loss=7.2872
	step [169/255], loss=7.5234
	step [170/255], loss=7.2044
	step [171/255], loss=7.6024
	step [172/255], loss=5.8749
	step [173/255], loss=5.8735
	step [174/255], loss=6.7052
	step [175/255], loss=6.7514
	step [176/255], loss=7.0648
	step [177/255], loss=9.5725
	step [178/255], loss=5.4247
	step [179/255], loss=6.3664
	step [180/255], loss=5.8068
	step [181/255], loss=4.2212
	step [182/255], loss=10.2375
	step [183/255], loss=5.7461
	step [184/255], loss=8.4443
	step [185/255], loss=6.4946
	step [186/255], loss=7.0338
	step [187/255], loss=7.2582
	step [188/255], loss=6.2128
	step [189/255], loss=6.2484
	step [190/255], loss=6.9366
	step [191/255], loss=6.8324
	step [192/255], loss=6.7633
	step [193/255], loss=7.0292
	step [194/255], loss=6.6658
	step [195/255], loss=6.6049
	step [196/255], loss=7.0570
	step [197/255], loss=4.9782
	step [198/255], loss=7.0727
	step [199/255], loss=5.4507
	step [200/255], loss=6.8590
	step [201/255], loss=7.4326
	step [202/255], loss=6.2902
	step [203/255], loss=7.2398
	step [204/255], loss=7.0374
	step [205/255], loss=5.8117
	step [206/255], loss=7.2172
	step [207/255], loss=8.5674
	step [208/255], loss=5.6378
	step [209/255], loss=5.7747
	step [210/255], loss=6.5260
	step [211/255], loss=6.8817
	step [212/255], loss=8.9716
	step [213/255], loss=7.1522
	step [214/255], loss=6.8253
	step [215/255], loss=5.7924
	step [216/255], loss=5.2409
	step [217/255], loss=6.6220
	step [218/255], loss=6.5876
	step [219/255], loss=6.6236
	step [220/255], loss=5.7744
	step [221/255], loss=8.9682
	step [222/255], loss=5.8257
	step [223/255], loss=6.1579
	step [224/255], loss=6.1371
	step [225/255], loss=5.9539
	step [226/255], loss=6.8810
	step [227/255], loss=5.9714
	step [228/255], loss=6.6089
	step [229/255], loss=5.6088
	step [230/255], loss=6.4951
	step [231/255], loss=7.4210
	step [232/255], loss=6.8348
	step [233/255], loss=8.7889
	step [234/255], loss=7.6056
	step [235/255], loss=6.5842
	step [236/255], loss=6.2214
	step [237/255], loss=6.2579
	step [238/255], loss=7.7000
	step [239/255], loss=6.2563
	step [240/255], loss=5.4613
	step [241/255], loss=7.2667
	step [242/255], loss=6.6163
	step [243/255], loss=6.8119
	step [244/255], loss=8.2185
	step [245/255], loss=8.8550
	step [246/255], loss=6.9737
	step [247/255], loss=5.8623
	step [248/255], loss=6.5256
	step [249/255], loss=9.6292
	step [250/255], loss=5.2509
	step [251/255], loss=6.8874
	step [252/255], loss=5.6656
	step [253/255], loss=7.8617
	step [254/255], loss=7.5817
	step [255/255], loss=5.1771
	Evaluating
	loss=0.0255, precision=0.1696, recall=0.9945, f1=0.2897
Training epoch 23
	step [1/255], loss=6.6310
	step [2/255], loss=5.6134
	step [3/255], loss=6.8809
	step [4/255], loss=6.6025
	step [5/255], loss=6.1910
	step [6/255], loss=8.4923
	step [7/255], loss=6.7121
	step [8/255], loss=6.9589
	step [9/255], loss=7.4193
	step [10/255], loss=7.0840
	step [11/255], loss=8.7234
	step [12/255], loss=5.2636
	step [13/255], loss=7.0907
	step [14/255], loss=5.6969
	step [15/255], loss=8.1063
	step [16/255], loss=6.1960
	step [17/255], loss=4.9597
	step [18/255], loss=7.7471
	step [19/255], loss=7.0503
	step [20/255], loss=7.3051
	step [21/255], loss=5.7604
	step [22/255], loss=6.6976
	step [23/255], loss=6.0559
	step [24/255], loss=6.2364
	step [25/255], loss=8.2939
	step [26/255], loss=5.6613
	step [27/255], loss=6.7202
	step [28/255], loss=7.4881
	step [29/255], loss=6.5631
	step [30/255], loss=6.8439
	step [31/255], loss=5.7932
	step [32/255], loss=5.7503
	step [33/255], loss=6.7033
	step [34/255], loss=7.4353
	step [35/255], loss=5.8518
	step [36/255], loss=6.3358
	step [37/255], loss=7.1080
	step [38/255], loss=7.6022
	step [39/255], loss=7.0849
	step [40/255], loss=8.5396
	step [41/255], loss=5.4142
	step [42/255], loss=7.8915
	step [43/255], loss=6.7099
	step [44/255], loss=7.5057
	step [45/255], loss=5.7635
	step [46/255], loss=6.5271
	step [47/255], loss=5.5299
	step [48/255], loss=7.4011
	step [49/255], loss=7.7867
	step [50/255], loss=4.6222
	step [51/255], loss=6.1721
	step [52/255], loss=6.6333
	step [53/255], loss=5.3415
	step [54/255], loss=8.5615
	step [55/255], loss=7.3862
	step [56/255], loss=8.0538
	step [57/255], loss=7.0969
	step [58/255], loss=6.5190
	step [59/255], loss=9.5212
	step [60/255], loss=8.4296
	step [61/255], loss=8.5085
	step [62/255], loss=6.1903
	step [63/255], loss=6.7230
	step [64/255], loss=7.1901
	step [65/255], loss=7.5471
	step [66/255], loss=5.7171
	step [67/255], loss=4.6598
	step [68/255], loss=5.9988
	step [69/255], loss=6.3028
	step [70/255], loss=6.7018
	step [71/255], loss=6.0398
	step [72/255], loss=6.3015
	step [73/255], loss=7.7774
	step [74/255], loss=6.0812
	step [75/255], loss=5.6481
	step [76/255], loss=7.3907
	step [77/255], loss=5.6894
	step [78/255], loss=6.5454
	step [79/255], loss=7.1180
	step [80/255], loss=4.6983
	step [81/255], loss=7.7359
	step [82/255], loss=5.8090
	step [83/255], loss=4.9646
	step [84/255], loss=7.9196
	step [85/255], loss=6.4396
	step [86/255], loss=6.5516
	step [87/255], loss=6.5078
	step [88/255], loss=6.5035
	step [89/255], loss=7.1751
	step [90/255], loss=6.5369
	step [91/255], loss=8.9681
	step [92/255], loss=6.4943
	step [93/255], loss=6.2159
	step [94/255], loss=8.0386
	step [95/255], loss=6.6046
	step [96/255], loss=9.5515
	step [97/255], loss=7.9879
	step [98/255], loss=6.8648
	step [99/255], loss=5.7088
	step [100/255], loss=6.6683
	step [101/255], loss=7.0482
	step [102/255], loss=6.1980
	step [103/255], loss=8.3543
	step [104/255], loss=7.2415
	step [105/255], loss=6.1291
	step [106/255], loss=5.7644
	step [107/255], loss=5.1702
	step [108/255], loss=4.4040
	step [109/255], loss=5.7380
	step [110/255], loss=5.1430
	step [111/255], loss=5.9377
	step [112/255], loss=9.7639
	step [113/255], loss=6.6949
	step [114/255], loss=6.3424
	step [115/255], loss=6.7126
	step [116/255], loss=4.8653
	step [117/255], loss=7.3373
	step [118/255], loss=5.9468
	step [119/255], loss=5.5188
	step [120/255], loss=5.2847
	step [121/255], loss=5.4336
	step [122/255], loss=7.6353
	step [123/255], loss=6.1283
	step [124/255], loss=6.9141
	step [125/255], loss=6.4033
	step [126/255], loss=6.5566
	step [127/255], loss=6.5388
	step [128/255], loss=8.2782
	step [129/255], loss=5.1345
	step [130/255], loss=5.7651
	step [131/255], loss=6.3187
	step [132/255], loss=7.3779
	step [133/255], loss=7.0684
	step [134/255], loss=5.9160
	step [135/255], loss=6.4663
	step [136/255], loss=5.7744
	step [137/255], loss=6.1275
	step [138/255], loss=7.2458
	step [139/255], loss=7.0223
	step [140/255], loss=6.7573
	step [141/255], loss=6.1534
	step [142/255], loss=4.9193
	step [143/255], loss=8.5346
	step [144/255], loss=6.2349
	step [145/255], loss=5.5153
	step [146/255], loss=7.7452
	step [147/255], loss=6.1979
	step [148/255], loss=7.0641
	step [149/255], loss=6.3680
	step [150/255], loss=8.0126
	step [151/255], loss=5.7746
	step [152/255], loss=8.4276
	step [153/255], loss=6.5087
	step [154/255], loss=6.3661
	step [155/255], loss=5.6465
	step [156/255], loss=5.9552
	step [157/255], loss=5.0967
	step [158/255], loss=6.0207
	step [159/255], loss=6.4354
	step [160/255], loss=7.3326
	step [161/255], loss=5.1151
	step [162/255], loss=7.4664
	step [163/255], loss=5.7918
	step [164/255], loss=5.2580
	step [165/255], loss=5.8123
	step [166/255], loss=7.6379
	step [167/255], loss=7.9192
	step [168/255], loss=7.5330
	step [169/255], loss=7.3735
	step [170/255], loss=5.8740
	step [171/255], loss=7.1270
	step [172/255], loss=6.6849
	step [173/255], loss=5.9929
	step [174/255], loss=6.7347
	step [175/255], loss=6.9547
	step [176/255], loss=6.4144
	step [177/255], loss=7.7639
	step [178/255], loss=6.6564
	step [179/255], loss=5.9782
	step [180/255], loss=6.4934
	step [181/255], loss=5.9845
	step [182/255], loss=5.3194
	step [183/255], loss=6.7731
	step [184/255], loss=7.4897
	step [185/255], loss=6.7950
	step [186/255], loss=7.2230
	step [187/255], loss=6.5995
	step [188/255], loss=7.1588
	step [189/255], loss=5.8583
	step [190/255], loss=5.7892
	step [191/255], loss=5.3609
	step [192/255], loss=6.5240
	step [193/255], loss=6.2477
	step [194/255], loss=6.1369
	step [195/255], loss=6.9704
	step [196/255], loss=6.6856
	step [197/255], loss=6.2575
	step [198/255], loss=7.2222
	step [199/255], loss=7.3160
	step [200/255], loss=6.1421
	step [201/255], loss=6.9283
	step [202/255], loss=7.6947
	step [203/255], loss=5.8743
	step [204/255], loss=5.9675
	step [205/255], loss=5.0676
	step [206/255], loss=8.0229
	step [207/255], loss=5.6553
	step [208/255], loss=6.0239
	step [209/255], loss=6.6807
	step [210/255], loss=6.0125
	step [211/255], loss=6.6653
	step [212/255], loss=5.8692
	step [213/255], loss=6.9457
	step [214/255], loss=5.9882
	step [215/255], loss=6.8541
	step [216/255], loss=6.2211
	step [217/255], loss=5.8409
	step [218/255], loss=5.5555
	step [219/255], loss=6.4766
	step [220/255], loss=6.5241
	step [221/255], loss=5.6657
	step [222/255], loss=9.0519
	step [223/255], loss=6.6638
	step [224/255], loss=7.2678
	step [225/255], loss=6.5231
	step [226/255], loss=8.1461
	step [227/255], loss=6.6981
	step [228/255], loss=6.5304
	step [229/255], loss=5.7776
	step [230/255], loss=6.3833
	step [231/255], loss=4.7882
	step [232/255], loss=6.5050
	step [233/255], loss=6.8324
	step [234/255], loss=5.9203
	step [235/255], loss=5.4668
	step [236/255], loss=7.1041
	step [237/255], loss=7.0955
	step [238/255], loss=7.1210
	step [239/255], loss=5.8635
	step [240/255], loss=5.8190
	step [241/255], loss=9.2504
	step [242/255], loss=7.4545
	step [243/255], loss=6.1935
	step [244/255], loss=7.4021
	step [245/255], loss=6.6531
	step [246/255], loss=5.1741
	step [247/255], loss=6.0012
	step [248/255], loss=7.4074
	step [249/255], loss=4.7790
	step [250/255], loss=7.0418
	step [251/255], loss=6.4743
	step [252/255], loss=6.0068
	step [253/255], loss=6.2363
	step [254/255], loss=6.0538
	step [255/255], loss=5.8948
	Evaluating
	loss=0.0180, precision=0.2268, recall=0.9924, f1=0.3692
saving model as: 0_saved_model.pth
Training epoch 24
	step [1/255], loss=7.4002
	step [2/255], loss=6.5299
	step [3/255], loss=5.6449
	step [4/255], loss=5.8976
	step [5/255], loss=7.4825
	step [6/255], loss=6.2499
	step [7/255], loss=4.8887
	step [8/255], loss=5.3417
	step [9/255], loss=7.2340
	step [10/255], loss=6.8247
	step [11/255], loss=6.5924
	step [12/255], loss=5.4874
	step [13/255], loss=6.3947
	step [14/255], loss=6.6004
	step [15/255], loss=6.1475
	step [16/255], loss=6.7582
	step [17/255], loss=6.7400
	step [18/255], loss=6.5667
	step [19/255], loss=6.4150
	step [20/255], loss=6.0357
	step [21/255], loss=6.8023
	step [22/255], loss=6.0949
	step [23/255], loss=6.6757
	step [24/255], loss=5.7857
	step [25/255], loss=5.5222
	step [26/255], loss=6.7311
	step [27/255], loss=7.4833
	step [28/255], loss=7.1339
	step [29/255], loss=6.6045
	step [30/255], loss=5.8742
	step [31/255], loss=6.3669
	step [32/255], loss=7.0264
	step [33/255], loss=4.8680
	step [34/255], loss=5.5428
	step [35/255], loss=4.9779
	step [36/255], loss=6.7202
	step [37/255], loss=5.6471
	step [38/255], loss=5.6967
	step [39/255], loss=7.4425
	step [40/255], loss=8.6743
	step [41/255], loss=5.2642
	step [42/255], loss=6.6761
	step [43/255], loss=7.6275
	step [44/255], loss=6.0584
	step [45/255], loss=5.5218
	step [46/255], loss=6.4565
	step [47/255], loss=6.4542
	step [48/255], loss=7.9346
	step [49/255], loss=7.0178
	step [50/255], loss=5.2923
	step [51/255], loss=6.5377
	step [52/255], loss=6.4533
	step [53/255], loss=6.9132
	step [54/255], loss=5.9604
	step [55/255], loss=7.0033
	step [56/255], loss=5.9492
	step [57/255], loss=6.8403
	step [58/255], loss=7.2349
	step [59/255], loss=6.0891
	step [60/255], loss=6.8628
	step [61/255], loss=6.7524
	step [62/255], loss=5.2663
	step [63/255], loss=5.7300
	step [64/255], loss=5.5575
	step [65/255], loss=5.6192
	step [66/255], loss=7.1631
	step [67/255], loss=5.4273
	step [68/255], loss=7.0895
	step [69/255], loss=5.7446
	step [70/255], loss=6.2548
	step [71/255], loss=7.7610
	step [72/255], loss=6.0253
	step [73/255], loss=5.0135
	step [74/255], loss=7.0244
	step [75/255], loss=7.4401
	step [76/255], loss=5.7215
	step [77/255], loss=6.7461
	step [78/255], loss=7.2459
	step [79/255], loss=6.1175
	step [80/255], loss=6.7664
	step [81/255], loss=5.9137
	step [82/255], loss=4.9814
	step [83/255], loss=5.3893
	step [84/255], loss=5.6260
	step [85/255], loss=6.4001
	step [86/255], loss=8.1293
	step [87/255], loss=5.0867
	step [88/255], loss=6.4492
	step [89/255], loss=7.7777
	step [90/255], loss=7.3102
	step [91/255], loss=6.6086
	step [92/255], loss=7.0145
	step [93/255], loss=6.3653
	step [94/255], loss=5.4714
	step [95/255], loss=5.9564
	step [96/255], loss=6.7960
	step [97/255], loss=7.0946
	step [98/255], loss=6.6068
	step [99/255], loss=5.1356
	step [100/255], loss=7.1457
	step [101/255], loss=5.7765
	step [102/255], loss=6.7807
	step [103/255], loss=6.8988
	step [104/255], loss=6.5050
	step [105/255], loss=6.3315
	step [106/255], loss=5.7281
	step [107/255], loss=5.6476
	step [108/255], loss=6.1619
	step [109/255], loss=5.8011
	step [110/255], loss=7.0197
	step [111/255], loss=7.8771
	step [112/255], loss=5.5451
	step [113/255], loss=5.3805
	step [114/255], loss=6.1288
	step [115/255], loss=7.6880
	step [116/255], loss=6.8835
	step [117/255], loss=6.8973
	step [118/255], loss=7.7765
	step [119/255], loss=6.6280
	step [120/255], loss=7.1249
	step [121/255], loss=6.1876
	step [122/255], loss=5.7599
	step [123/255], loss=5.9855
	step [124/255], loss=6.3523
	step [125/255], loss=5.6316
	step [126/255], loss=6.8343
	step [127/255], loss=5.5767
	step [128/255], loss=6.4857
	step [129/255], loss=8.0608
	step [130/255], loss=7.4606
	step [131/255], loss=7.8169
	step [132/255], loss=7.5674
	step [133/255], loss=7.6739
	step [134/255], loss=7.5569
	step [135/255], loss=6.1835
	step [136/255], loss=6.2103
	step [137/255], loss=8.6388
	step [138/255], loss=7.1704
	step [139/255], loss=6.5836
	step [140/255], loss=6.1846
	step [141/255], loss=6.2836
	step [142/255], loss=5.8307
	step [143/255], loss=6.3761
	step [144/255], loss=6.3813
	step [145/255], loss=5.1351
	step [146/255], loss=5.8082
	step [147/255], loss=6.5959
	step [148/255], loss=6.5260
	step [149/255], loss=6.2878
	step [150/255], loss=7.3717
	step [151/255], loss=6.8875
	step [152/255], loss=6.2832
	step [153/255], loss=6.0986
	step [154/255], loss=5.9188
	step [155/255], loss=6.7394
	step [156/255], loss=6.2819
	step [157/255], loss=6.2017
	step [158/255], loss=5.2691
	step [159/255], loss=5.6167
	step [160/255], loss=5.9829
	step [161/255], loss=8.1735
	step [162/255], loss=8.4955
	step [163/255], loss=6.0087
	step [164/255], loss=8.0942
	step [165/255], loss=5.4140
	step [166/255], loss=6.3001
	step [167/255], loss=8.3541
	step [168/255], loss=6.5930
	step [169/255], loss=6.2770
	step [170/255], loss=6.3378
	step [171/255], loss=6.0678
	step [172/255], loss=4.9594
	step [173/255], loss=7.7616
	step [174/255], loss=6.9114
	step [175/255], loss=5.7245
	step [176/255], loss=6.6290
	step [177/255], loss=6.1365
	step [178/255], loss=6.6268
	step [179/255], loss=6.6279
	step [180/255], loss=7.8642
	step [181/255], loss=6.6920
	step [182/255], loss=7.0318
	step [183/255], loss=6.9370
	step [184/255], loss=5.8023
	step [185/255], loss=6.0085
	step [186/255], loss=5.6376
	step [187/255], loss=5.5907
	step [188/255], loss=5.9865
	step [189/255], loss=5.8010
	step [190/255], loss=7.0767
	step [191/255], loss=5.9208
	step [192/255], loss=7.5123
	step [193/255], loss=6.0653
	step [194/255], loss=7.2280
	step [195/255], loss=6.8062
	step [196/255], loss=7.3840
	step [197/255], loss=6.5624
	step [198/255], loss=6.8018
	step [199/255], loss=4.9252
	step [200/255], loss=7.4486
	step [201/255], loss=5.2507
	step [202/255], loss=7.4122
	step [203/255], loss=6.4039
	step [204/255], loss=9.6143
	step [205/255], loss=5.7708
	step [206/255], loss=7.9813
	step [207/255], loss=6.0771
	step [208/255], loss=7.1933
	step [209/255], loss=9.6924
	step [210/255], loss=7.2231
	step [211/255], loss=7.4249
	step [212/255], loss=5.9670
	step [213/255], loss=6.8171
	step [214/255], loss=7.1867
	step [215/255], loss=5.8699
	step [216/255], loss=6.4932
	step [217/255], loss=7.0849
	step [218/255], loss=6.8209
	step [219/255], loss=6.3927
	step [220/255], loss=6.7742
	step [221/255], loss=5.8431
	step [222/255], loss=6.9313
	step [223/255], loss=6.1029
	step [224/255], loss=6.3603
	step [225/255], loss=7.0956
	step [226/255], loss=7.5057
	step [227/255], loss=4.8591
	step [228/255], loss=5.3068
	step [229/255], loss=5.7826
	step [230/255], loss=4.9447
	step [231/255], loss=6.8948
	step [232/255], loss=5.6221
	step [233/255], loss=6.6009
	step [234/255], loss=5.7514
	step [235/255], loss=5.8945
	step [236/255], loss=6.7655
	step [237/255], loss=5.3784
	step [238/255], loss=6.3828
	step [239/255], loss=6.3075
	step [240/255], loss=6.8824
	step [241/255], loss=5.7069
	step [242/255], loss=7.6087
	step [243/255], loss=6.5036
	step [244/255], loss=8.3513
	step [245/255], loss=5.1980
	step [246/255], loss=6.2083
	step [247/255], loss=6.4962
	step [248/255], loss=7.7407
	step [249/255], loss=6.0324
	step [250/255], loss=7.9882
	step [251/255], loss=6.2210
	step [252/255], loss=6.0057
	step [253/255], loss=6.3009
	step [254/255], loss=6.5459
	step [255/255], loss=5.3643
	Evaluating
	loss=0.0204, precision=0.1970, recall=0.9936, f1=0.3288
Training epoch 25
	step [1/255], loss=5.9525
	step [2/255], loss=10.8825
	step [3/255], loss=5.8754
	step [4/255], loss=5.6121
	step [5/255], loss=6.5520
	step [6/255], loss=6.8468
	step [7/255], loss=6.8371
	step [8/255], loss=7.9332
	step [9/255], loss=7.5628
	step [10/255], loss=6.1794
	step [11/255], loss=6.7684
	step [12/255], loss=7.4724
	step [13/255], loss=5.5493
	step [14/255], loss=6.5877
	step [15/255], loss=5.9108
	step [16/255], loss=7.1226
	step [17/255], loss=5.9684
	step [18/255], loss=6.0822
	step [19/255], loss=6.1520
	step [20/255], loss=6.1019
	step [21/255], loss=7.1308
	step [22/255], loss=6.2284
	step [23/255], loss=7.5372
	step [24/255], loss=6.3322
	step [25/255], loss=6.6067
	step [26/255], loss=5.6274
	step [27/255], loss=7.3085
	step [28/255], loss=6.7664
	step [29/255], loss=6.1748
	step [30/255], loss=5.9463
	step [31/255], loss=5.8650
	step [32/255], loss=6.3679
	step [33/255], loss=7.2744
	step [34/255], loss=6.8166
	step [35/255], loss=4.9730
	step [36/255], loss=6.4012
	step [37/255], loss=6.1410
	step [38/255], loss=6.6038
	step [39/255], loss=7.7347
	step [40/255], loss=5.8292
	step [41/255], loss=5.2984
	step [42/255], loss=5.2176
	step [43/255], loss=6.3508
	step [44/255], loss=5.7592
	step [45/255], loss=5.3561
	step [46/255], loss=6.4379
	step [47/255], loss=5.2017
	step [48/255], loss=7.8269
	step [49/255], loss=6.9882
	step [50/255], loss=5.6710
	step [51/255], loss=6.5349
	step [52/255], loss=6.6378
	step [53/255], loss=7.2344
	step [54/255], loss=6.1253
	step [55/255], loss=5.9745
	step [56/255], loss=5.9521
	step [57/255], loss=6.4112
	step [58/255], loss=9.2186
	step [59/255], loss=9.6565
	step [60/255], loss=7.3955
	step [61/255], loss=6.1355
	step [62/255], loss=5.8451
	step [63/255], loss=6.4024
	step [64/255], loss=6.1607
	step [65/255], loss=4.6153
	step [66/255], loss=4.9062
	step [67/255], loss=7.3571
	step [68/255], loss=6.8061
	step [69/255], loss=6.9183
	step [70/255], loss=4.7018
	step [71/255], loss=6.8583
	step [72/255], loss=6.4119
	step [73/255], loss=8.1500
	step [74/255], loss=6.7395
	step [75/255], loss=7.0729
	step [76/255], loss=7.2036
	step [77/255], loss=5.8971
	step [78/255], loss=6.3817
	step [79/255], loss=5.7331
	step [80/255], loss=5.1799
	step [81/255], loss=5.1397
	step [82/255], loss=7.6388
	step [83/255], loss=6.7586
	step [84/255], loss=6.7636
	step [85/255], loss=6.1127
	step [86/255], loss=6.3320
	step [87/255], loss=5.9647
	step [88/255], loss=5.8890
	step [89/255], loss=5.6016
	step [90/255], loss=5.8926
	step [91/255], loss=6.3447
	step [92/255], loss=6.2858
	step [93/255], loss=5.8161
	step [94/255], loss=6.0811
	step [95/255], loss=7.1691
	step [96/255], loss=5.8916
	step [97/255], loss=5.5172
	step [98/255], loss=6.9440
	step [99/255], loss=6.8990
	step [100/255], loss=8.4783
	step [101/255], loss=6.5826
	step [102/255], loss=6.7312
	step [103/255], loss=6.5476
	step [104/255], loss=7.0171
	step [105/255], loss=6.5896
	step [106/255], loss=8.6296
	step [107/255], loss=5.0346
	step [108/255], loss=7.1303
	step [109/255], loss=7.5190
	step [110/255], loss=6.7219
	step [111/255], loss=6.8505
	step [112/255], loss=6.3100
	step [113/255], loss=6.9251
	step [114/255], loss=5.9659
	step [115/255], loss=6.6685
	step [116/255], loss=5.2096
	step [117/255], loss=6.2632
	step [118/255], loss=6.2887
	step [119/255], loss=6.9660
	step [120/255], loss=6.2808
	step [121/255], loss=5.7672
	step [122/255], loss=6.2747
	step [123/255], loss=7.7809
	step [124/255], loss=5.6712
	step [125/255], loss=6.9895
	step [126/255], loss=5.5947
	step [127/255], loss=5.7330
	step [128/255], loss=6.2745
	step [129/255], loss=5.9905
	step [130/255], loss=7.7340
	step [131/255], loss=7.2985
	step [132/255], loss=6.0817
	step [133/255], loss=6.1636
	step [134/255], loss=5.2897
	step [135/255], loss=6.1224
	step [136/255], loss=5.5440
	step [137/255], loss=5.8210
	step [138/255], loss=5.4946
	step [139/255], loss=6.4986
	step [140/255], loss=7.5035
	step [141/255], loss=6.5492
	step [142/255], loss=5.7507
	step [143/255], loss=6.1811
	step [144/255], loss=6.2856
	step [145/255], loss=5.5810
	step [146/255], loss=6.5717
	step [147/255], loss=7.1008
	step [148/255], loss=5.5091
	step [149/255], loss=5.0772
	step [150/255], loss=5.5982
	step [151/255], loss=4.9807
	step [152/255], loss=7.0681
	step [153/255], loss=6.6235
	step [154/255], loss=6.1962
	step [155/255], loss=6.2270
	step [156/255], loss=7.1999
	step [157/255], loss=6.8403
	step [158/255], loss=6.2541
	step [159/255], loss=4.6148
	step [160/255], loss=5.0685
	step [161/255], loss=7.2984
	step [162/255], loss=4.8761
	step [163/255], loss=4.6513
	step [164/255], loss=6.9952
	step [165/255], loss=5.9586
	step [166/255], loss=5.6216
	step [167/255], loss=6.4623
	step [168/255], loss=5.1975
	step [169/255], loss=5.5793
	step [170/255], loss=5.1588
	step [171/255], loss=5.7957
	step [172/255], loss=7.4944
	step [173/255], loss=5.0312
	step [174/255], loss=6.5590
	step [175/255], loss=6.8677
	step [176/255], loss=5.9712
	step [177/255], loss=6.8525
	step [178/255], loss=7.9827
	step [179/255], loss=5.2746
	step [180/255], loss=5.6730
	step [181/255], loss=6.9678
	step [182/255], loss=5.8909
	step [183/255], loss=5.4390
	step [184/255], loss=5.9281
	step [185/255], loss=7.1889
	step [186/255], loss=5.8500
	step [187/255], loss=6.5069
	step [188/255], loss=5.9467
	step [189/255], loss=8.1779
	step [190/255], loss=6.8028
	step [191/255], loss=6.0484
	step [192/255], loss=6.7514
	step [193/255], loss=7.0336
	step [194/255], loss=7.4501
	step [195/255], loss=8.2281
	step [196/255], loss=8.6926
	step [197/255], loss=6.4280
	step [198/255], loss=7.2480
	step [199/255], loss=5.8550
	step [200/255], loss=7.6531
	step [201/255], loss=5.5926
	step [202/255], loss=7.2479
	step [203/255], loss=6.9901
	step [204/255], loss=6.5402
	step [205/255], loss=5.6487
	step [206/255], loss=6.6104
	step [207/255], loss=5.9759
	step [208/255], loss=6.1104
	step [209/255], loss=5.8319
	step [210/255], loss=6.4822
	step [211/255], loss=4.8545
	step [212/255], loss=5.0991
	step [213/255], loss=4.8553
	step [214/255], loss=7.7284
	step [215/255], loss=6.4366
	step [216/255], loss=5.2464
	step [217/255], loss=5.9182
	step [218/255], loss=6.5544
	step [219/255], loss=5.5282
	step [220/255], loss=6.7138
	step [221/255], loss=6.4994
	step [222/255], loss=5.9274
	step [223/255], loss=6.3129
	step [224/255], loss=7.9343
	step [225/255], loss=5.1999
	step [226/255], loss=6.3927
	step [227/255], loss=6.4353
	step [228/255], loss=6.6004
	step [229/255], loss=4.5329
	step [230/255], loss=5.8197
	step [231/255], loss=6.2765
	step [232/255], loss=5.9804
	step [233/255], loss=5.5874
	step [234/255], loss=5.4793
	step [235/255], loss=6.1865
	step [236/255], loss=8.5789
	step [237/255], loss=6.6308
	step [238/255], loss=6.0858
	step [239/255], loss=5.8279
	step [240/255], loss=7.0225
	step [241/255], loss=5.1214
	step [242/255], loss=6.3095
	step [243/255], loss=6.2689
	step [244/255], loss=5.0926
	step [245/255], loss=5.8708
	step [246/255], loss=4.7156
	step [247/255], loss=6.8136
	step [248/255], loss=6.7032
	step [249/255], loss=6.1673
	step [250/255], loss=5.0468
	step [251/255], loss=7.1775
	step [252/255], loss=5.1289
	step [253/255], loss=7.1253
	step [254/255], loss=5.9180
	step [255/255], loss=5.9301
	Evaluating
	loss=0.0192, precision=0.2144, recall=0.9937, f1=0.3527
Training epoch 26
	step [1/255], loss=7.3719
	step [2/255], loss=6.0578
	step [3/255], loss=6.4624
	step [4/255], loss=7.5796
	step [5/255], loss=5.8000
	step [6/255], loss=4.8511
	step [7/255], loss=5.0587
	step [8/255], loss=7.6590
	step [9/255], loss=7.0047
	step [10/255], loss=7.3845
	step [11/255], loss=5.5493
	step [12/255], loss=5.9988
	step [13/255], loss=5.3044
	step [14/255], loss=7.1757
	step [15/255], loss=6.8140
	step [16/255], loss=6.7834
	step [17/255], loss=6.5291
	step [18/255], loss=5.8547
	step [19/255], loss=5.8748
	step [20/255], loss=5.5164
	step [21/255], loss=7.1261
	step [22/255], loss=6.0054
	step [23/255], loss=5.4902
	step [24/255], loss=6.2456
	step [25/255], loss=6.5499
	step [26/255], loss=7.1017
	step [27/255], loss=4.5932
	step [28/255], loss=6.4658
	step [29/255], loss=5.9381
	step [30/255], loss=6.8243
	step [31/255], loss=7.7735
	step [32/255], loss=5.3403
	step [33/255], loss=5.6793
	step [34/255], loss=7.2113
	step [35/255], loss=7.3116
	step [36/255], loss=5.8450
	step [37/255], loss=5.5830
	step [38/255], loss=5.3743
	step [39/255], loss=6.3202
	step [40/255], loss=5.5530
	step [41/255], loss=5.2490
	step [42/255], loss=8.2200
	step [43/255], loss=6.3330
	step [44/255], loss=4.8812
	step [45/255], loss=7.3475
	step [46/255], loss=5.8835
	step [47/255], loss=5.0933
	step [48/255], loss=6.3988
	step [49/255], loss=5.6042
	step [50/255], loss=6.5393
	step [51/255], loss=6.0249
	step [52/255], loss=7.8282
	step [53/255], loss=5.2306
	step [54/255], loss=6.7151
	step [55/255], loss=6.5425
	step [56/255], loss=5.1838
	step [57/255], loss=6.6641
	step [58/255], loss=5.1107
	step [59/255], loss=5.8284
	step [60/255], loss=6.2065
	step [61/255], loss=6.1225
	step [62/255], loss=8.8223
	step [63/255], loss=5.7723
	step [64/255], loss=6.6241
	step [65/255], loss=5.7821
	step [66/255], loss=5.6315
	step [67/255], loss=5.5160
	step [68/255], loss=4.8132
	step [69/255], loss=5.1053
	step [70/255], loss=5.4043
	step [71/255], loss=5.5032
	step [72/255], loss=6.7446
	step [73/255], loss=4.6533
	step [74/255], loss=6.6000
	step [75/255], loss=6.1689
	step [76/255], loss=5.9171
	step [77/255], loss=5.9494
	step [78/255], loss=6.0597
	step [79/255], loss=4.5493
	step [80/255], loss=5.8947
	step [81/255], loss=5.5645
	step [82/255], loss=5.6873
	step [83/255], loss=7.3676
	step [84/255], loss=7.1380
	step [85/255], loss=4.4442
	step [86/255], loss=6.0729
	step [87/255], loss=4.8475
	step [88/255], loss=6.0839
	step [89/255], loss=4.5564
	step [90/255], loss=5.9077
	step [91/255], loss=5.6700
	step [92/255], loss=4.5793
	step [93/255], loss=4.0056
	step [94/255], loss=5.3999
	step [95/255], loss=5.9192
	step [96/255], loss=4.9599
	step [97/255], loss=4.7583
	step [98/255], loss=5.5246
	step [99/255], loss=7.5567
	step [100/255], loss=5.6535
	step [101/255], loss=6.6367
	step [102/255], loss=5.4684
	step [103/255], loss=7.2145
	step [104/255], loss=6.8642
	step [105/255], loss=6.8894
	step [106/255], loss=5.4560
	step [107/255], loss=6.0165
	step [108/255], loss=7.2039
	step [109/255], loss=6.1072
	step [110/255], loss=4.2429
	step [111/255], loss=5.7536
	step [112/255], loss=5.5037
	step [113/255], loss=6.3707
	step [114/255], loss=7.2510
	step [115/255], loss=5.4162
	step [116/255], loss=5.8768
	step [117/255], loss=5.7594
	step [118/255], loss=5.7315
	step [119/255], loss=6.1584
	step [120/255], loss=5.7876
	step [121/255], loss=5.9042
	step [122/255], loss=8.2949
	step [123/255], loss=5.6927
	step [124/255], loss=7.2641
	step [125/255], loss=6.9300
	step [126/255], loss=5.1890
	step [127/255], loss=5.4851
	step [128/255], loss=6.0562
	step [129/255], loss=6.0687
	step [130/255], loss=5.1907
	step [131/255], loss=6.6424
	step [132/255], loss=6.9281
	step [133/255], loss=6.7845
	step [134/255], loss=5.3737
	step [135/255], loss=4.9864
	step [136/255], loss=6.9319
	step [137/255], loss=4.8369
	step [138/255], loss=8.8587
	step [139/255], loss=5.7236
	step [140/255], loss=7.3367
	step [141/255], loss=7.9638
	step [142/255], loss=6.7356
	step [143/255], loss=7.7563
	step [144/255], loss=6.4005
	step [145/255], loss=7.6787
	step [146/255], loss=6.0897
	step [147/255], loss=5.5644
	step [148/255], loss=7.0715
	step [149/255], loss=6.7767
	step [150/255], loss=6.9954
	step [151/255], loss=7.2970
	step [152/255], loss=7.6857
	step [153/255], loss=6.5736
	step [154/255], loss=5.4238
	step [155/255], loss=6.2663
	step [156/255], loss=6.1630
	step [157/255], loss=7.6578
	step [158/255], loss=7.3399
	step [159/255], loss=5.2315
	step [160/255], loss=6.2970
	step [161/255], loss=6.2705
	step [162/255], loss=4.0171
	step [163/255], loss=5.1117
	step [164/255], loss=6.2043
	step [165/255], loss=6.7821
	step [166/255], loss=8.4884
	step [167/255], loss=6.2332
	step [168/255], loss=5.3202
	step [169/255], loss=7.7966
	step [170/255], loss=5.3128
	step [171/255], loss=5.9519
	step [172/255], loss=5.7107
	step [173/255], loss=5.9369
	step [174/255], loss=6.8320
	step [175/255], loss=8.2490
	step [176/255], loss=5.7565
	step [177/255], loss=6.4660
	step [178/255], loss=6.1862
	step [179/255], loss=6.3037
	step [180/255], loss=5.8854
	step [181/255], loss=5.6351
	step [182/255], loss=7.2645
	step [183/255], loss=6.2668
	step [184/255], loss=9.3876
	step [185/255], loss=6.3873
	step [186/255], loss=6.0964
	step [187/255], loss=5.2251
	step [188/255], loss=6.8767
	step [189/255], loss=5.0931
	step [190/255], loss=6.1058
	step [191/255], loss=6.6567
	step [192/255], loss=5.6586
	step [193/255], loss=7.7252
	step [194/255], loss=5.2781
	step [195/255], loss=5.7351
	step [196/255], loss=7.8769
	step [197/255], loss=7.6675
	step [198/255], loss=6.4255
	step [199/255], loss=5.5088
	step [200/255], loss=5.5821
	step [201/255], loss=6.1639
	step [202/255], loss=6.1783
	step [203/255], loss=5.6050
	step [204/255], loss=6.1204
	step [205/255], loss=6.2356
	step [206/255], loss=6.1394
	step [207/255], loss=7.6232
	step [208/255], loss=8.3207
	step [209/255], loss=4.8886
	step [210/255], loss=6.1639
	step [211/255], loss=6.0782
	step [212/255], loss=7.3124
	step [213/255], loss=8.3691
	step [214/255], loss=5.8998
	step [215/255], loss=6.2396
	step [216/255], loss=6.2474
	step [217/255], loss=6.2265
	step [218/255], loss=5.7641
	step [219/255], loss=5.4370
	step [220/255], loss=6.2444
	step [221/255], loss=7.7181
	step [222/255], loss=6.9337
	step [223/255], loss=7.1036
	step [224/255], loss=5.8952
	step [225/255], loss=6.2217
	step [226/255], loss=6.2537
	step [227/255], loss=6.6541
	step [228/255], loss=4.7265
	step [229/255], loss=6.2466
	step [230/255], loss=6.8419
	step [231/255], loss=7.3845
	step [232/255], loss=5.2759
	step [233/255], loss=5.4711
	step [234/255], loss=6.7596
	step [235/255], loss=5.9179
	step [236/255], loss=4.8529
	step [237/255], loss=7.1614
	step [238/255], loss=7.6732
	step [239/255], loss=6.1458
	step [240/255], loss=6.2782
	step [241/255], loss=6.1175
	step [242/255], loss=7.4340
	step [243/255], loss=5.5456
	step [244/255], loss=5.6591
	step [245/255], loss=5.3434
	step [246/255], loss=6.2340
	step [247/255], loss=5.4736
	step [248/255], loss=7.1563
	step [249/255], loss=7.3370
	step [250/255], loss=5.0278
	step [251/255], loss=5.5041
	step [252/255], loss=8.5878
	step [253/255], loss=6.1183
	step [254/255], loss=5.3888
	step [255/255], loss=4.7941
	Evaluating
	loss=0.0200, precision=0.1964, recall=0.9942, f1=0.3279
Training epoch 27
	step [1/255], loss=8.1690
	step [2/255], loss=6.0412
	step [3/255], loss=5.9431
	step [4/255], loss=4.6529
	step [5/255], loss=6.2559
	step [6/255], loss=5.8713
	step [7/255], loss=6.0749
	step [8/255], loss=5.5477
	step [9/255], loss=6.5175
	step [10/255], loss=7.5151
	step [11/255], loss=6.0002
	step [12/255], loss=5.4487
	step [13/255], loss=4.7811
	step [14/255], loss=7.0229
	step [15/255], loss=5.6994
	step [16/255], loss=6.8226
	step [17/255], loss=5.5502
	step [18/255], loss=5.0538
	step [19/255], loss=5.5006
	step [20/255], loss=6.0680
	step [21/255], loss=5.9278
	step [22/255], loss=5.1932
	step [23/255], loss=7.5760
	step [24/255], loss=6.2123
	step [25/255], loss=5.9848
	step [26/255], loss=5.9784
	step [27/255], loss=5.9550
	step [28/255], loss=6.1855
	step [29/255], loss=6.6300
	step [30/255], loss=6.4961
	step [31/255], loss=6.1484
	step [32/255], loss=6.0646
	step [33/255], loss=7.6584
	step [34/255], loss=5.3185
	step [35/255], loss=5.4009
	step [36/255], loss=5.6872
	step [37/255], loss=7.3941
	step [38/255], loss=5.1523
	step [39/255], loss=7.0529
	step [40/255], loss=6.8741
	step [41/255], loss=5.7503
	step [42/255], loss=6.8390
	step [43/255], loss=5.1258
	step [44/255], loss=5.9403
	step [45/255], loss=7.3934
	step [46/255], loss=6.1601
	step [47/255], loss=6.1254
	step [48/255], loss=5.8072
	step [49/255], loss=6.5693
	step [50/255], loss=5.8500
	step [51/255], loss=5.6337
	step [52/255], loss=6.3873
	step [53/255], loss=6.4812
	step [54/255], loss=5.7213
	step [55/255], loss=5.7354
	step [56/255], loss=6.1899
	step [57/255], loss=6.1120
	step [58/255], loss=6.0801
	step [59/255], loss=6.1797
	step [60/255], loss=7.0168
	step [61/255], loss=7.5309
	step [62/255], loss=7.5312
	step [63/255], loss=5.2822
	step [64/255], loss=6.4761
	step [65/255], loss=6.8228
	step [66/255], loss=5.8097
	step [67/255], loss=6.9321
	step [68/255], loss=5.7191
	step [69/255], loss=7.1584
	step [70/255], loss=7.6653
	step [71/255], loss=5.7300
	step [72/255], loss=5.9754
	step [73/255], loss=6.7381
	step [74/255], loss=7.6774
	step [75/255], loss=5.8386
	step [76/255], loss=5.9792
	step [77/255], loss=6.0472
	step [78/255], loss=4.0572
	step [79/255], loss=7.1780
	step [80/255], loss=6.0232
	step [81/255], loss=6.3918
	step [82/255], loss=5.9715
	step [83/255], loss=5.2556
	step [84/255], loss=6.0664
	step [85/255], loss=6.3218
	step [86/255], loss=7.0721
	step [87/255], loss=8.2545
	step [88/255], loss=4.6856
	step [89/255], loss=6.0704
	step [90/255], loss=6.8638
	step [91/255], loss=5.4442
	step [92/255], loss=6.3101
	step [93/255], loss=5.6788
	step [94/255], loss=5.5856
	step [95/255], loss=7.2930
	step [96/255], loss=7.5829
	step [97/255], loss=5.0465
	step [98/255], loss=5.8918
	step [99/255], loss=6.2629
	step [100/255], loss=6.6499
	step [101/255], loss=6.7707
	step [102/255], loss=6.2127
	step [103/255], loss=5.4424
	step [104/255], loss=5.7811
	step [105/255], loss=5.9550
	step [106/255], loss=5.6639
	step [107/255], loss=5.6308
	step [108/255], loss=5.3216
	step [109/255], loss=6.5328
	step [110/255], loss=6.2000
	step [111/255], loss=5.6750
	step [112/255], loss=5.8259
	step [113/255], loss=5.6792
	step [114/255], loss=5.7591
	step [115/255], loss=6.2024
	step [116/255], loss=5.1926
	step [117/255], loss=6.7103
	step [118/255], loss=5.8948
	step [119/255], loss=6.1779
	step [120/255], loss=5.4844
	step [121/255], loss=6.3117
	step [122/255], loss=5.6683
	step [123/255], loss=5.5162
	step [124/255], loss=6.4736
	step [125/255], loss=7.1203
	step [126/255], loss=4.7084
	step [127/255], loss=6.4135
	step [128/255], loss=6.5000
	step [129/255], loss=6.1017
	step [130/255], loss=6.8155
	step [131/255], loss=5.9680
	step [132/255], loss=6.9962
	step [133/255], loss=6.2966
	step [134/255], loss=6.0272
	step [135/255], loss=6.5907
	step [136/255], loss=6.7975
	step [137/255], loss=7.3604
	step [138/255], loss=5.7666
	step [139/255], loss=5.2811
	step [140/255], loss=6.4270
	step [141/255], loss=6.5129
	step [142/255], loss=6.4749
	step [143/255], loss=4.8211
	step [144/255], loss=6.9751
	step [145/255], loss=6.8113
	step [146/255], loss=7.5812
	step [147/255], loss=5.3341
	step [148/255], loss=7.0927
	step [149/255], loss=5.7935
	step [150/255], loss=7.2292
	step [151/255], loss=5.6700
	step [152/255], loss=6.0436
	step [153/255], loss=5.9536
	step [154/255], loss=5.0900
	step [155/255], loss=7.9644
	step [156/255], loss=5.7844
	step [157/255], loss=5.6820
	step [158/255], loss=5.0068
	step [159/255], loss=8.0095
	step [160/255], loss=7.1806
	step [161/255], loss=6.8264
	step [162/255], loss=6.0276
	step [163/255], loss=6.5580
	step [164/255], loss=6.8233
	step [165/255], loss=5.8695
	step [166/255], loss=4.9839
	step [167/255], loss=5.5737
	step [168/255], loss=6.1429
	step [169/255], loss=5.1265
	step [170/255], loss=5.2509
	step [171/255], loss=6.7733
	step [172/255], loss=5.8660
	step [173/255], loss=8.8856
	step [174/255], loss=6.2837
	step [175/255], loss=7.9230
	step [176/255], loss=5.3409
	step [177/255], loss=5.4624
	step [178/255], loss=5.4578
	step [179/255], loss=4.7672
	step [180/255], loss=5.1251
	step [181/255], loss=6.2630
	step [182/255], loss=5.7626
	step [183/255], loss=6.5819
	step [184/255], loss=5.5875
	step [185/255], loss=5.9903
	step [186/255], loss=5.1377
	step [187/255], loss=6.1388
	step [188/255], loss=5.8509
	step [189/255], loss=5.3647
	step [190/255], loss=5.9889
	step [191/255], loss=5.3032
	step [192/255], loss=5.7072
	step [193/255], loss=4.3166
	step [194/255], loss=7.9993
	step [195/255], loss=6.5361
	step [196/255], loss=4.8315
	step [197/255], loss=6.2291
	step [198/255], loss=5.8463
	step [199/255], loss=5.1231
	step [200/255], loss=6.7981
	step [201/255], loss=6.3627
	step [202/255], loss=5.4790
	step [203/255], loss=4.9716
	step [204/255], loss=5.6489
	step [205/255], loss=4.9954
	step [206/255], loss=5.3060
	step [207/255], loss=4.6968
	step [208/255], loss=5.9221
	step [209/255], loss=5.5811
	step [210/255], loss=6.0818
	step [211/255], loss=5.4188
	step [212/255], loss=6.6748
	step [213/255], loss=5.6243
	step [214/255], loss=7.7465
	step [215/255], loss=6.2432
	step [216/255], loss=6.5662
	step [217/255], loss=4.8626
	step [218/255], loss=8.4846
	step [219/255], loss=7.9639
	step [220/255], loss=6.1001
	step [221/255], loss=5.2678
	step [222/255], loss=6.2265
	step [223/255], loss=5.6985
	step [224/255], loss=5.7093
	step [225/255], loss=5.1430
	step [226/255], loss=4.7515
	step [227/255], loss=6.6363
	step [228/255], loss=7.4871
	step [229/255], loss=6.6169
	step [230/255], loss=6.0397
	step [231/255], loss=5.9253
	step [232/255], loss=5.4256
	step [233/255], loss=5.3246
	step [234/255], loss=5.5697
	step [235/255], loss=6.7097
	step [236/255], loss=6.1733
	step [237/255], loss=5.2568
	step [238/255], loss=6.3772
	step [239/255], loss=8.1540
	step [240/255], loss=6.4729
	step [241/255], loss=5.1956
	step [242/255], loss=5.4131
	step [243/255], loss=5.8989
	step [244/255], loss=4.8857
	step [245/255], loss=5.4324
	step [246/255], loss=6.1836
	step [247/255], loss=5.5171
	step [248/255], loss=5.9080
	step [249/255], loss=6.7895
	step [250/255], loss=5.3035
	step [251/255], loss=5.2558
	step [252/255], loss=5.2840
	step [253/255], loss=5.4766
	step [254/255], loss=5.3424
	step [255/255], loss=4.7181
	Evaluating
	loss=0.0178, precision=0.2265, recall=0.9926, f1=0.3688
Training epoch 28
	step [1/255], loss=4.9852
	step [2/255], loss=5.5498
	step [3/255], loss=6.3902
	step [4/255], loss=6.2247
	step [5/255], loss=6.0722
	step [6/255], loss=7.1808
	step [7/255], loss=7.0226
	step [8/255], loss=7.4016
	step [9/255], loss=5.3593
	step [10/255], loss=6.8128
	step [11/255], loss=4.9746
	step [12/255], loss=6.2488
	step [13/255], loss=5.9490
	step [14/255], loss=5.5333
	step [15/255], loss=4.7867
	step [16/255], loss=6.1021
	step [17/255], loss=8.1249
	step [18/255], loss=4.9865
	step [19/255], loss=5.5418
	step [20/255], loss=5.2117
	step [21/255], loss=4.2669
	step [22/255], loss=5.3268
	step [23/255], loss=10.0801
	step [24/255], loss=7.3266
	step [25/255], loss=6.5838
	step [26/255], loss=6.0925
	step [27/255], loss=7.5968
	step [28/255], loss=5.6766
	step [29/255], loss=6.7565
	step [30/255], loss=5.9517
	step [31/255], loss=6.2523
	step [32/255], loss=5.1597
	step [33/255], loss=6.3115
	step [34/255], loss=5.1791
	step [35/255], loss=5.3457
	step [36/255], loss=6.9314
	step [37/255], loss=6.7439
	step [38/255], loss=6.7569
	step [39/255], loss=5.5629
	step [40/255], loss=7.3605
	step [41/255], loss=6.1172
	step [42/255], loss=5.4471
	step [43/255], loss=5.8741
	step [44/255], loss=5.1042
	step [45/255], loss=7.5711
	step [46/255], loss=8.3545
	step [47/255], loss=5.2882
	step [48/255], loss=5.3308
	step [49/255], loss=5.5595
	step [50/255], loss=6.5716
	step [51/255], loss=5.5133
	step [52/255], loss=5.0287
	step [53/255], loss=5.8260
	step [54/255], loss=6.6105
	step [55/255], loss=5.5404
	step [56/255], loss=5.9004
	step [57/255], loss=5.5159
	step [58/255], loss=7.1802
	step [59/255], loss=5.7504
	step [60/255], loss=5.9597
	step [61/255], loss=7.7158
	step [62/255], loss=7.4419
	step [63/255], loss=6.0131
	step [64/255], loss=5.5195
	step [65/255], loss=5.5833
	step [66/255], loss=5.9816
	step [67/255], loss=5.2700
	step [68/255], loss=5.6852
	step [69/255], loss=5.5228
	step [70/255], loss=6.2116
	step [71/255], loss=6.4051
	step [72/255], loss=8.1311
	step [73/255], loss=5.9843
	step [74/255], loss=4.8468
	step [75/255], loss=5.9621
	step [76/255], loss=5.8743
	step [77/255], loss=5.8742
	step [78/255], loss=7.6628
	step [79/255], loss=4.8471
	step [80/255], loss=5.1181
	step [81/255], loss=6.5440
	step [82/255], loss=5.6442
	step [83/255], loss=6.7662
	step [84/255], loss=5.8099
	step [85/255], loss=7.5015
	step [86/255], loss=5.8600
	step [87/255], loss=5.1275
	step [88/255], loss=5.6715
	step [89/255], loss=6.6978
	step [90/255], loss=6.9351
	step [91/255], loss=6.6465
	step [92/255], loss=5.9088
	step [93/255], loss=7.3905
	step [94/255], loss=8.5028
	step [95/255], loss=6.0533
	step [96/255], loss=7.8240
	step [97/255], loss=5.3981
	step [98/255], loss=4.2386
	step [99/255], loss=4.9634
	step [100/255], loss=5.3390
	step [101/255], loss=5.6512
	step [102/255], loss=6.5362
	step [103/255], loss=6.3412
	step [104/255], loss=6.6386
	step [105/255], loss=6.3106
	step [106/255], loss=7.2723
	step [107/255], loss=5.3582
	step [108/255], loss=6.1890
	step [109/255], loss=6.6273
	step [110/255], loss=7.2484
	step [111/255], loss=6.6876
	step [112/255], loss=6.1219
	step [113/255], loss=6.2047
	step [114/255], loss=6.3000
	step [115/255], loss=7.3196
	step [116/255], loss=7.3310
	step [117/255], loss=6.0690
	step [118/255], loss=5.4477
	step [119/255], loss=6.0296
	step [120/255], loss=5.9750
	step [121/255], loss=4.6185
	step [122/255], loss=6.7702
	step [123/255], loss=4.4775
	step [124/255], loss=4.6775
	step [125/255], loss=5.2051
	step [126/255], loss=5.7197
	step [127/255], loss=6.8477
	step [128/255], loss=6.1405
	step [129/255], loss=5.1828
	step [130/255], loss=5.7683
	step [131/255], loss=4.4008
	step [132/255], loss=5.3225
	step [133/255], loss=6.0403
	step [134/255], loss=6.6628
	step [135/255], loss=4.9944
	step [136/255], loss=6.5309
	step [137/255], loss=6.3984
	step [138/255], loss=5.5285
	step [139/255], loss=5.0458
	step [140/255], loss=5.6065
	step [141/255], loss=5.7519
	step [142/255], loss=5.7649
	step [143/255], loss=5.2030
	step [144/255], loss=5.3508
	step [145/255], loss=5.2579
	step [146/255], loss=5.9510
	step [147/255], loss=5.2053
	step [148/255], loss=6.5191
	step [149/255], loss=5.2148
	step [150/255], loss=5.0822
	step [151/255], loss=6.4806
	step [152/255], loss=5.9296
	step [153/255], loss=5.0178
	step [154/255], loss=4.9839
	step [155/255], loss=6.1301
	step [156/255], loss=4.6331
	step [157/255], loss=8.5158
	step [158/255], loss=6.3226
	step [159/255], loss=6.4305
	step [160/255], loss=6.5591
	step [161/255], loss=6.7158
	step [162/255], loss=4.8452
	step [163/255], loss=7.5552
	step [164/255], loss=5.0826
	step [165/255], loss=6.7292
	step [166/255], loss=6.1857
	step [167/255], loss=5.2930
	step [168/255], loss=5.9685
	step [169/255], loss=6.5281
	step [170/255], loss=5.2007
	step [171/255], loss=5.2983
	step [172/255], loss=6.2240
	step [173/255], loss=5.4581
	step [174/255], loss=6.4124
	step [175/255], loss=7.0947
	step [176/255], loss=6.2807
	step [177/255], loss=5.7470
	step [178/255], loss=5.4408
	step [179/255], loss=5.2034
	step [180/255], loss=7.6601
	step [181/255], loss=6.2322
	step [182/255], loss=5.0788
	step [183/255], loss=5.1133
	step [184/255], loss=6.8735
	step [185/255], loss=5.6461
	step [186/255], loss=5.5822
	step [187/255], loss=6.8320
	step [188/255], loss=4.8387
	step [189/255], loss=6.8631
	step [190/255], loss=8.1723
	step [191/255], loss=5.4555
	step [192/255], loss=5.1101
	step [193/255], loss=4.5583
	step [194/255], loss=6.6502
	step [195/255], loss=7.3591
	step [196/255], loss=5.4191
	step [197/255], loss=6.3493
	step [198/255], loss=5.9737
	step [199/255], loss=7.2691
	step [200/255], loss=5.8685
	step [201/255], loss=5.7832
	step [202/255], loss=5.0257
	step [203/255], loss=8.6799
	step [204/255], loss=6.6143
	step [205/255], loss=6.6573
	step [206/255], loss=5.1643
	step [207/255], loss=5.3737
	step [208/255], loss=5.6587
	step [209/255], loss=5.8181
	step [210/255], loss=4.1445
	step [211/255], loss=7.3225
	step [212/255], loss=4.4791
	step [213/255], loss=5.3607
	step [214/255], loss=6.7341
	step [215/255], loss=5.5439
	step [216/255], loss=5.9707
	step [217/255], loss=5.7930
	step [218/255], loss=5.7186
	step [219/255], loss=6.1043
	step [220/255], loss=7.4593
	step [221/255], loss=5.2019
	step [222/255], loss=6.4942
	step [223/255], loss=6.8955
	step [224/255], loss=5.6530
	step [225/255], loss=7.8093
	step [226/255], loss=4.3937
	step [227/255], loss=4.3199
	step [228/255], loss=7.0277
	step [229/255], loss=5.9511
	step [230/255], loss=6.0543
	step [231/255], loss=7.3719
	step [232/255], loss=7.6198
	step [233/255], loss=6.6783
	step [234/255], loss=5.4798
	step [235/255], loss=5.7625
	step [236/255], loss=5.2589
	step [237/255], loss=5.6425
	step [238/255], loss=5.4639
	step [239/255], loss=5.7374
	step [240/255], loss=5.0556
	step [241/255], loss=5.5321
	step [242/255], loss=5.8105
	step [243/255], loss=7.8022
	step [244/255], loss=6.6761
	step [245/255], loss=6.6717
	step [246/255], loss=6.0015
	step [247/255], loss=5.7475
	step [248/255], loss=6.7334
	step [249/255], loss=4.6177
	step [250/255], loss=5.3650
	step [251/255], loss=5.2234
	step [252/255], loss=4.6725
	step [253/255], loss=5.7297
	step [254/255], loss=5.5301
	step [255/255], loss=4.1285
	Evaluating
	loss=0.0181, precision=0.2195, recall=0.9928, f1=0.3595
Training epoch 29
	step [1/255], loss=6.2566
	step [2/255], loss=6.4118
	step [3/255], loss=4.9688
	step [4/255], loss=5.5581
	step [5/255], loss=5.7841
	step [6/255], loss=5.1614
	step [7/255], loss=4.9406
	step [8/255], loss=5.2315
	step [9/255], loss=7.6649
	step [10/255], loss=6.6014
	step [11/255], loss=5.9851
	step [12/255], loss=6.0436
	step [13/255], loss=5.1685
	step [14/255], loss=7.1556
	step [15/255], loss=5.5505
	step [16/255], loss=5.5527
	step [17/255], loss=4.9193
	step [18/255], loss=4.6437
	step [19/255], loss=6.0071
	step [20/255], loss=4.7114
	step [21/255], loss=6.4268
	step [22/255], loss=6.1813
	step [23/255], loss=5.7241
	step [24/255], loss=5.5924
	step [25/255], loss=5.8729
	step [26/255], loss=6.2155
	step [27/255], loss=5.6664
	step [28/255], loss=5.6018
	step [29/255], loss=5.2593
	step [30/255], loss=6.2134
	step [31/255], loss=4.8564
	step [32/255], loss=7.0415
	step [33/255], loss=6.8125
	step [34/255], loss=4.9656
	step [35/255], loss=7.1237
	step [36/255], loss=5.4098
	step [37/255], loss=6.6890
	step [38/255], loss=6.8346
	step [39/255], loss=6.2038
	step [40/255], loss=5.9564
	step [41/255], loss=6.0209
	step [42/255], loss=4.4780
	step [43/255], loss=5.3797
	step [44/255], loss=5.7560
	step [45/255], loss=6.2533
	step [46/255], loss=6.2542
	step [47/255], loss=6.6015
	step [48/255], loss=8.1737
	step [49/255], loss=4.9649
	step [50/255], loss=5.7247
	step [51/255], loss=5.7334
	step [52/255], loss=6.8262
	step [53/255], loss=7.0488
	step [54/255], loss=6.2845
	step [55/255], loss=5.2422
	step [56/255], loss=7.6571
	step [57/255], loss=5.5023
	step [58/255], loss=5.2554
	step [59/255], loss=5.9315
	step [60/255], loss=6.6914
	step [61/255], loss=4.5247
	step [62/255], loss=6.5803
	step [63/255], loss=6.7888
	step [64/255], loss=4.8763
	step [65/255], loss=5.6909
	step [66/255], loss=5.2324
	step [67/255], loss=6.5855
	step [68/255], loss=5.2547
	step [69/255], loss=5.9528
	step [70/255], loss=6.6569
	step [71/255], loss=6.4144
	step [72/255], loss=6.5933
	step [73/255], loss=7.0758
	step [74/255], loss=4.6221
	step [75/255], loss=5.2977
	step [76/255], loss=5.5328
	step [77/255], loss=6.0469
	step [78/255], loss=5.8689
	step [79/255], loss=5.9668
	step [80/255], loss=5.0404
	step [81/255], loss=6.2698
	step [82/255], loss=4.9366
	step [83/255], loss=6.6328
	step [84/255], loss=6.9449
	step [85/255], loss=4.3542
	step [86/255], loss=7.3412
	step [87/255], loss=8.0123
	step [88/255], loss=4.8367
	step [89/255], loss=7.6947
	step [90/255], loss=5.7654
	step [91/255], loss=7.8801
	step [92/255], loss=6.2020
	step [93/255], loss=7.0546
	step [94/255], loss=6.0405
	step [95/255], loss=5.1953
	step [96/255], loss=6.6811
	step [97/255], loss=6.3255
	step [98/255], loss=7.3824
	step [99/255], loss=6.6510
	step [100/255], loss=5.9870
	step [101/255], loss=6.4271
	step [102/255], loss=6.2202
	step [103/255], loss=5.2985
	step [104/255], loss=6.2795
	step [105/255], loss=7.0226
	step [106/255], loss=5.8456
	step [107/255], loss=6.1561
	step [108/255], loss=4.5074
	step [109/255], loss=6.1172
	step [110/255], loss=5.5014
	step [111/255], loss=5.4884
	step [112/255], loss=5.2549
	step [113/255], loss=5.5689
	step [114/255], loss=7.0846
	step [115/255], loss=7.0867
	step [116/255], loss=5.5718
	step [117/255], loss=4.8667
	step [118/255], loss=6.1084
	step [119/255], loss=5.6128
	step [120/255], loss=5.4305
	step [121/255], loss=6.1455
	step [122/255], loss=5.0216
	step [123/255], loss=5.5986
	step [124/255], loss=7.3860
	step [125/255], loss=4.9891
	step [126/255], loss=5.6481
	step [127/255], loss=6.7097
	step [128/255], loss=4.9822
	step [129/255], loss=3.9485
	step [130/255], loss=6.8931
	step [131/255], loss=5.5421
	step [132/255], loss=6.3686
	step [133/255], loss=7.0050
	step [134/255], loss=6.7526
	step [135/255], loss=6.6912
	step [136/255], loss=5.1744
	step [137/255], loss=5.7045
	step [138/255], loss=7.0060
	step [139/255], loss=5.6099
	step [140/255], loss=7.5203
	step [141/255], loss=7.8822
	step [142/255], loss=4.5942
	step [143/255], loss=5.6179
	step [144/255], loss=7.1584
	step [145/255], loss=5.3589
	step [146/255], loss=6.0531
	step [147/255], loss=4.9704
	step [148/255], loss=5.7850
	step [149/255], loss=5.7632
	step [150/255], loss=7.7071
	step [151/255], loss=5.2239
	step [152/255], loss=6.0998
	step [153/255], loss=5.6948
	step [154/255], loss=5.2146
	step [155/255], loss=5.6169
	step [156/255], loss=5.9103
	step [157/255], loss=6.0838
	step [158/255], loss=6.6012
	step [159/255], loss=5.7631
	step [160/255], loss=6.0318
	step [161/255], loss=7.0528
	step [162/255], loss=5.9411
	step [163/255], loss=5.7458
	step [164/255], loss=5.4203
	step [165/255], loss=6.8362
	step [166/255], loss=6.1095
	step [167/255], loss=6.1069
	step [168/255], loss=5.6121
	step [169/255], loss=6.2591
	step [170/255], loss=8.6213
	step [171/255], loss=6.5031
	step [172/255], loss=5.8837
	step [173/255], loss=4.8100
	step [174/255], loss=5.7858
	step [175/255], loss=6.7643
	step [176/255], loss=5.4695
	step [177/255], loss=5.5595
	step [178/255], loss=5.1173
	step [179/255], loss=6.0249
	step [180/255], loss=5.5167
	step [181/255], loss=5.8867
	step [182/255], loss=5.7724
	step [183/255], loss=6.9368
	step [184/255], loss=6.3641
	step [185/255], loss=5.3021
	step [186/255], loss=7.0619
	step [187/255], loss=4.6309
	step [188/255], loss=5.5846
	step [189/255], loss=6.7724
	step [190/255], loss=6.5171
	step [191/255], loss=6.3688
	step [192/255], loss=4.6503
	step [193/255], loss=6.1363
	step [194/255], loss=5.3465
	step [195/255], loss=5.5084
	step [196/255], loss=5.5742
	step [197/255], loss=7.4445
	step [198/255], loss=5.5734
	step [199/255], loss=5.1970
	step [200/255], loss=4.5233
	step [201/255], loss=4.7461
	step [202/255], loss=7.0836
	step [203/255], loss=6.3160
	step [204/255], loss=4.9716
	step [205/255], loss=5.9023
	step [206/255], loss=5.7325
	step [207/255], loss=4.7756
	step [208/255], loss=5.5057
	step [209/255], loss=5.4422
	step [210/255], loss=6.4472
	step [211/255], loss=5.0136
	step [212/255], loss=5.7129
	step [213/255], loss=4.9893
	step [214/255], loss=4.7199
	step [215/255], loss=6.2888
	step [216/255], loss=4.9682
	step [217/255], loss=5.4110
	step [218/255], loss=4.7842
	step [219/255], loss=5.0175
	step [220/255], loss=5.8020
	step [221/255], loss=5.7110
	step [222/255], loss=5.1894
	step [223/255], loss=5.7529
	step [224/255], loss=5.7242
	step [225/255], loss=7.6992
	step [226/255], loss=6.3914
	step [227/255], loss=4.5587
	step [228/255], loss=5.6431
	step [229/255], loss=4.4685
	step [230/255], loss=5.3257
	step [231/255], loss=5.2937
	step [232/255], loss=4.6031
	step [233/255], loss=6.0999
	step [234/255], loss=6.1177
	step [235/255], loss=5.8482
	step [236/255], loss=5.2690
	step [237/255], loss=6.4579
	step [238/255], loss=6.2582
	step [239/255], loss=7.2711
	step [240/255], loss=5.3809
	step [241/255], loss=5.9881
	step [242/255], loss=4.1292
	step [243/255], loss=5.1755
	step [244/255], loss=5.0969
	step [245/255], loss=4.1830
	step [246/255], loss=5.3887
	step [247/255], loss=5.5762
	step [248/255], loss=6.3330
	step [249/255], loss=4.6440
	step [250/255], loss=4.6802
	step [251/255], loss=5.2351
	step [252/255], loss=5.5432
	step [253/255], loss=6.9705
	step [254/255], loss=5.9908
	step [255/255], loss=6.0001
	Evaluating
	loss=0.0217, precision=0.1932, recall=0.9934, f1=0.3235
Training epoch 30
	step [1/255], loss=5.9966
	step [2/255], loss=4.2563
	step [3/255], loss=4.9409
	step [4/255], loss=4.5185
	step [5/255], loss=5.9199
	step [6/255], loss=5.6323
	step [7/255], loss=5.4511
	step [8/255], loss=6.3703
	step [9/255], loss=4.2648
	step [10/255], loss=5.3151
	step [11/255], loss=6.2677
	step [12/255], loss=6.1716
	step [13/255], loss=5.2700
	step [14/255], loss=5.4876
	step [15/255], loss=7.1407
	step [16/255], loss=6.7239
	step [17/255], loss=4.6391
	step [18/255], loss=5.5353
	step [19/255], loss=7.2906
	step [20/255], loss=5.7417
	step [21/255], loss=5.5109
	step [22/255], loss=5.5565
	step [23/255], loss=4.6644
	step [24/255], loss=5.1605
	step [25/255], loss=5.5358
	step [26/255], loss=6.3881
	step [27/255], loss=4.4446
	step [28/255], loss=5.6319
	step [29/255], loss=4.7635
	step [30/255], loss=6.7711
	step [31/255], loss=5.7990
	step [32/255], loss=5.8129
	step [33/255], loss=5.9595
	step [34/255], loss=6.6170
	step [35/255], loss=5.7592
	step [36/255], loss=6.5203
	step [37/255], loss=4.9635
	step [38/255], loss=6.6277
	step [39/255], loss=5.8587
	step [40/255], loss=5.2958
	step [41/255], loss=6.4259
	step [42/255], loss=5.7148
	step [43/255], loss=4.4849
	step [44/255], loss=4.6914
	step [45/255], loss=7.7136
	step [46/255], loss=5.1840
	step [47/255], loss=5.5090
	step [48/255], loss=5.6986
	step [49/255], loss=4.9512
	step [50/255], loss=6.5056
	step [51/255], loss=5.5392
	step [52/255], loss=4.4311
	step [53/255], loss=6.2512
	step [54/255], loss=6.0601
	step [55/255], loss=6.0887
	step [56/255], loss=5.8656
	step [57/255], loss=5.2042
	step [58/255], loss=5.3557
	step [59/255], loss=5.7849
	step [60/255], loss=5.3849
	step [61/255], loss=6.4639
	step [62/255], loss=6.1080
	step [63/255], loss=4.4987
	step [64/255], loss=7.5144
	step [65/255], loss=5.9665
	step [66/255], loss=7.8916
	step [67/255], loss=4.8021
	step [68/255], loss=5.6053
	step [69/255], loss=5.8002
	step [70/255], loss=6.2772
	step [71/255], loss=6.0376
	step [72/255], loss=6.5151
	step [73/255], loss=5.4013
	step [74/255], loss=3.8656
	step [75/255], loss=6.6812
	step [76/255], loss=6.8855
	step [77/255], loss=4.2638
	step [78/255], loss=5.5108
	step [79/255], loss=5.1992
	step [80/255], loss=5.5378
	step [81/255], loss=4.6550
	step [82/255], loss=6.4391
	step [83/255], loss=6.6540
	step [84/255], loss=4.1773
	step [85/255], loss=5.5325
	step [86/255], loss=5.5483
	step [87/255], loss=5.8186
	step [88/255], loss=5.8360
	step [89/255], loss=4.8433
	step [90/255], loss=6.9791
	step [91/255], loss=5.5042
	step [92/255], loss=6.6957
	step [93/255], loss=6.5385
	step [94/255], loss=5.2609
	step [95/255], loss=6.7552
	step [96/255], loss=6.8878
	step [97/255], loss=4.9797
	step [98/255], loss=5.2189
	step [99/255], loss=6.9311
	step [100/255], loss=6.1941
	step [101/255], loss=7.4784
	step [102/255], loss=5.5826
	step [103/255], loss=5.8962
	step [104/255], loss=5.8845
	step [105/255], loss=6.5029
	step [106/255], loss=5.6087
	step [107/255], loss=6.0175
	step [108/255], loss=5.6892
	step [109/255], loss=7.5870
	step [110/255], loss=6.2137
	step [111/255], loss=5.1909
	step [112/255], loss=6.2804
	step [113/255], loss=4.4055
	step [114/255], loss=4.8281
	step [115/255], loss=4.6038
	step [116/255], loss=7.0634
	step [117/255], loss=4.8642
	step [118/255], loss=4.8716
	step [119/255], loss=5.0229
	step [120/255], loss=5.4732
	step [121/255], loss=5.5777
	step [122/255], loss=6.6753
	step [123/255], loss=5.1264
	step [124/255], loss=5.7466
	step [125/255], loss=5.6362
	step [126/255], loss=3.9778
	step [127/255], loss=7.0106
	step [128/255], loss=7.3732
	step [129/255], loss=5.7914
	step [130/255], loss=4.9052
	step [131/255], loss=5.2450
	step [132/255], loss=5.5822
	step [133/255], loss=5.0149
	step [134/255], loss=6.0155
	step [135/255], loss=5.9858
	step [136/255], loss=5.3909
	step [137/255], loss=6.9466
	step [138/255], loss=7.1329
	step [139/255], loss=5.7621
	step [140/255], loss=4.8686
	step [141/255], loss=5.2643
	step [142/255], loss=6.0423
	step [143/255], loss=5.7554
	step [144/255], loss=5.6050
	step [145/255], loss=5.6408
	step [146/255], loss=6.4860
	step [147/255], loss=6.3003
	step [148/255], loss=5.6399
	step [149/255], loss=5.8025
	step [150/255], loss=4.8535
	step [151/255], loss=5.4292
	step [152/255], loss=4.8588
	step [153/255], loss=5.1026
	step [154/255], loss=5.5945
	step [155/255], loss=6.1390
	step [156/255], loss=6.4962
	step [157/255], loss=5.4342
	step [158/255], loss=5.3489
	step [159/255], loss=6.2588
	step [160/255], loss=4.8225
	step [161/255], loss=6.7532
	step [162/255], loss=5.8164
	step [163/255], loss=4.0146
	step [164/255], loss=5.3339
	step [165/255], loss=5.5579
	step [166/255], loss=5.8920
	step [167/255], loss=6.8340
	step [168/255], loss=5.0839
	step [169/255], loss=5.8814
	step [170/255], loss=5.7950
	step [171/255], loss=5.9978
	step [172/255], loss=5.1115
	step [173/255], loss=5.0562
	step [174/255], loss=5.9158
	step [175/255], loss=5.6524
	step [176/255], loss=6.5614
	step [177/255], loss=6.5161
	step [178/255], loss=6.1874
	step [179/255], loss=6.8066
	step [180/255], loss=7.1639
	step [181/255], loss=6.8878
	step [182/255], loss=5.1848
	step [183/255], loss=5.5250
	step [184/255], loss=6.9955
	step [185/255], loss=5.8558
	step [186/255], loss=6.5809
	step [187/255], loss=5.5977
	step [188/255], loss=6.1906
	step [189/255], loss=5.1742
	step [190/255], loss=7.2974
	step [191/255], loss=6.5295
	step [192/255], loss=4.7724
	step [193/255], loss=6.1384
	step [194/255], loss=5.4964
	step [195/255], loss=5.6283
	step [196/255], loss=4.9848
	step [197/255], loss=6.7858
	step [198/255], loss=5.4272
	step [199/255], loss=7.1783
	step [200/255], loss=5.8214
	step [201/255], loss=5.0312
	step [202/255], loss=5.0352
	step [203/255], loss=6.5942
	step [204/255], loss=6.2652
	step [205/255], loss=5.7688
	step [206/255], loss=5.9032
	step [207/255], loss=6.1355
	step [208/255], loss=7.7733
	step [209/255], loss=5.6171
	step [210/255], loss=5.1282
	step [211/255], loss=6.0372
	step [212/255], loss=5.2916
	step [213/255], loss=6.0312
	step [214/255], loss=5.0120
	step [215/255], loss=5.5610
	step [216/255], loss=4.7866
	step [217/255], loss=6.2884
	step [218/255], loss=4.7895
	step [219/255], loss=6.1105
	step [220/255], loss=4.9514
	step [221/255], loss=6.7764
	step [222/255], loss=5.6898
	step [223/255], loss=5.5307
	step [224/255], loss=6.2827
	step [225/255], loss=5.1193
	step [226/255], loss=5.5795
	step [227/255], loss=5.3186
	step [228/255], loss=4.7132
	step [229/255], loss=6.3383
	step [230/255], loss=6.3698
	step [231/255], loss=4.6298
	step [232/255], loss=5.8757
	step [233/255], loss=5.9684
	step [234/255], loss=5.6335
	step [235/255], loss=4.9656
	step [236/255], loss=6.2241
	step [237/255], loss=6.3042
	step [238/255], loss=4.4723
	step [239/255], loss=4.4136
	step [240/255], loss=4.6275
	step [241/255], loss=5.1021
	step [242/255], loss=4.7532
	step [243/255], loss=5.5668
	step [244/255], loss=5.6324
	step [245/255], loss=6.8917
	step [246/255], loss=4.4537
	step [247/255], loss=5.8981
	step [248/255], loss=5.3744
	step [249/255], loss=5.9483
	step [250/255], loss=6.7893
	step [251/255], loss=6.2570
	step [252/255], loss=6.5345
	step [253/255], loss=5.5983
	step [254/255], loss=6.0739
	step [255/255], loss=4.8853
	Evaluating
	loss=0.0177, precision=0.2085, recall=0.9927, f1=0.3447
Training finished
best_f1: 0.36917208313216177
directing: X rim_enhanced: False test_id 1
removed wrong scan: weights_X_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_218_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_46_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_49_xwqg-A00121_2019-05-15.npy
# all image files: 12135 # all weight files in weight_dir: 9175 # image files with weight 9141
removed wrong scan: weights_X_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_218_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_46_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_49_xwqg-A00121_2019-05-15.npy
# all image files: 12135 # all weight files in weight_dir: 2707 # image files with weight 2691
train: /ibex/scratch/projects/c2052/air_tube_seg/training_samples_enhanced/X 9141
Using 4 GPUs
Going to train epochs [1-30]
Training epoch 1
	step [1/191], loss=641.6699
	step [2/191], loss=433.4783
	step [3/191], loss=268.4555
	step [4/191], loss=214.4073
	step [5/191], loss=173.5705
	step [6/191], loss=160.0515
	step [7/191], loss=156.1369
	step [8/191], loss=147.8272
	step [9/191], loss=146.7996
	step [10/191], loss=143.6921
	step [11/191], loss=141.7090
	step [12/191], loss=139.4613
	step [13/191], loss=139.8670
	step [14/191], loss=138.3135
	step [15/191], loss=137.8033
	step [16/191], loss=136.7564
	step [17/191], loss=137.2459
	step [18/191], loss=136.2398
	step [19/191], loss=136.8432
	step [20/191], loss=135.5532
	step [21/191], loss=133.3634
	step [22/191], loss=132.4311
	step [23/191], loss=131.9241
	step [24/191], loss=130.9652
	step [25/191], loss=129.7114
	step [26/191], loss=129.9188
	step [27/191], loss=129.0699
	step [28/191], loss=127.8634
	step [29/191], loss=126.3951
	step [30/191], loss=125.6410
	step [31/191], loss=124.9510
	step [32/191], loss=123.9850
	step [33/191], loss=124.8490
	step [34/191], loss=123.8274
	step [35/191], loss=122.4951
	step [36/191], loss=122.1612
	step [37/191], loss=119.7706
	step [38/191], loss=121.3028
	step [39/191], loss=121.2709
	step [40/191], loss=119.1360
	step [41/191], loss=118.5089
	step [42/191], loss=117.1439
	step [43/191], loss=116.4454
	step [44/191], loss=115.7834
	step [45/191], loss=114.9404
	step [46/191], loss=114.0803
	step [47/191], loss=113.0783
	step [48/191], loss=112.8995
	step [49/191], loss=113.1704
	step [50/191], loss=111.9882
	step [51/191], loss=109.7658
	step [52/191], loss=110.1594
	step [53/191], loss=109.8007
	step [54/191], loss=109.5163
	step [55/191], loss=109.9236
	step [56/191], loss=107.0064
	step [57/191], loss=106.3156
	step [58/191], loss=107.0257
	step [59/191], loss=105.3375
	step [60/191], loss=105.5850
	step [61/191], loss=104.2134
	step [62/191], loss=104.9261
	step [63/191], loss=105.0688
	step [64/191], loss=104.1721
	step [65/191], loss=102.8969
	step [66/191], loss=103.4007
	step [67/191], loss=101.2902
	step [68/191], loss=100.5889
	step [69/191], loss=99.7380
	step [70/191], loss=100.6124
	step [71/191], loss=99.8874
	step [72/191], loss=99.1430
	step [73/191], loss=98.6844
	step [74/191], loss=98.0305
	step [75/191], loss=97.7556
	step [76/191], loss=98.7234
	step [77/191], loss=96.8442
	step [78/191], loss=98.0921
	step [79/191], loss=98.1229
	step [80/191], loss=96.1995
	step [81/191], loss=96.1701
	step [82/191], loss=96.0257
	step [83/191], loss=96.9601
	step [84/191], loss=94.9848
	step [85/191], loss=95.6667
	step [86/191], loss=92.9697
	step [87/191], loss=92.7814
	step [88/191], loss=93.0215
	step [89/191], loss=91.4430
	step [90/191], loss=92.0443
	step [91/191], loss=92.1106
	step [92/191], loss=92.9039
	step [93/191], loss=90.4928
	step [94/191], loss=91.7096
	step [95/191], loss=90.2274
	step [96/191], loss=90.7837
	step [97/191], loss=89.9645
	step [98/191], loss=89.4763
	step [99/191], loss=90.0473
	step [100/191], loss=89.6218
	step [101/191], loss=88.9192
	step [102/191], loss=87.5195
	step [103/191], loss=89.5189
	step [104/191], loss=89.0956
	step [105/191], loss=87.0032
	step [106/191], loss=86.9440
	step [107/191], loss=87.1510
	step [108/191], loss=88.0847
	step [109/191], loss=86.1623
	step [110/191], loss=86.9520
	step [111/191], loss=86.8536
	step [112/191], loss=88.4670
	step [113/191], loss=86.8540
	step [114/191], loss=86.1313
	step [115/191], loss=86.4939
	step [116/191], loss=85.9113
	step [117/191], loss=85.8163
	step [118/191], loss=84.7852
	step [119/191], loss=84.8154
	step [120/191], loss=83.9147
	step [121/191], loss=85.3913
	step [122/191], loss=86.2697
	step [123/191], loss=85.4208
	step [124/191], loss=85.4164
	step [125/191], loss=85.9302
	step [126/191], loss=84.4881
	step [127/191], loss=83.0659
	step [128/191], loss=82.7325
	step [129/191], loss=83.4729
	step [130/191], loss=82.8350
	step [131/191], loss=82.6832
	step [132/191], loss=83.4207
	step [133/191], loss=81.8127
	step [134/191], loss=81.9120
	step [135/191], loss=82.3447
	step [136/191], loss=81.3089
	step [137/191], loss=82.0904
	step [138/191], loss=80.7176
	step [139/191], loss=82.2188
	step [140/191], loss=82.6001
	step [141/191], loss=80.4668
	step [142/191], loss=80.8987
	step [143/191], loss=81.1446
	step [144/191], loss=82.5021
	step [145/191], loss=81.0511
	step [146/191], loss=81.7335
	step [147/191], loss=80.2794
	step [148/191], loss=80.3550
	step [149/191], loss=79.5151
	step [150/191], loss=78.2491
	step [151/191], loss=80.9622
	step [152/191], loss=78.6621
	step [153/191], loss=76.9727
	step [154/191], loss=80.3743
	step [155/191], loss=79.5925
	step [156/191], loss=79.2546
	step [157/191], loss=78.9499
	step [158/191], loss=79.0378
	step [159/191], loss=79.4477
	step [160/191], loss=79.1663
	step [161/191], loss=79.7641
	step [162/191], loss=78.4322
	step [163/191], loss=78.2467
	step [164/191], loss=76.2892
	step [165/191], loss=78.2665
	step [166/191], loss=77.5481
	step [167/191], loss=76.3672
	step [168/191], loss=76.6941
	step [169/191], loss=76.3842
	step [170/191], loss=76.9212
	step [171/191], loss=75.9665
	step [172/191], loss=78.8106
	step [173/191], loss=75.2155
	step [174/191], loss=75.5203
	step [175/191], loss=75.4364
	step [176/191], loss=75.5842
	step [177/191], loss=77.3395
	step [178/191], loss=76.3146
	step [179/191], loss=77.5014
	step [180/191], loss=76.1313
	step [181/191], loss=76.5246
	step [182/191], loss=76.8674
	step [183/191], loss=75.8916
	step [184/191], loss=74.9488
	step [185/191], loss=74.8805
	step [186/191], loss=75.7941
	step [187/191], loss=76.6967
	step [188/191], loss=75.4173
	step [189/191], loss=76.0358
	step [190/191], loss=74.2719
	step [191/191], loss=32.9941
	Evaluating
	loss=0.3733, precision=0.1715, recall=0.9947, f1=0.2925
saving model as: 1_saved_model.pth
Training epoch 2
	step [1/191], loss=73.6537
	step [2/191], loss=75.7074
	step [3/191], loss=74.0961
	step [4/191], loss=75.1824
	step [5/191], loss=73.9620
	step [6/191], loss=73.8097
	step [7/191], loss=72.9587
	step [8/191], loss=72.7027
	step [9/191], loss=73.6024
	step [10/191], loss=72.7639
	step [11/191], loss=73.9917
	step [12/191], loss=73.6246
	step [13/191], loss=73.0224
	step [14/191], loss=72.1927
	step [15/191], loss=72.9729
	step [16/191], loss=73.2799
	step [17/191], loss=72.9580
	step [18/191], loss=72.0628
	step [19/191], loss=72.5050
	step [20/191], loss=73.1265
	step [21/191], loss=73.4949
	step [22/191], loss=71.8949
	step [23/191], loss=72.1432
	step [24/191], loss=71.1103
	step [25/191], loss=71.3230
	step [26/191], loss=71.6274
	step [27/191], loss=71.5288
	step [28/191], loss=73.3238
	step [29/191], loss=72.8719
	step [30/191], loss=70.5188
	step [31/191], loss=70.9738
	step [32/191], loss=71.1561
	step [33/191], loss=71.2102
	step [34/191], loss=71.2562
	step [35/191], loss=71.0787
	step [36/191], loss=70.1549
	step [37/191], loss=69.8489
	step [38/191], loss=71.1534
	step [39/191], loss=70.5660
	step [40/191], loss=69.5608
	step [41/191], loss=69.7455
	step [42/191], loss=71.0718
	step [43/191], loss=69.6478
	step [44/191], loss=69.5150
	step [45/191], loss=69.0055
	step [46/191], loss=68.7092
	step [47/191], loss=70.0378
	step [48/191], loss=69.8876
	step [49/191], loss=68.8463
	step [50/191], loss=70.0622
	step [51/191], loss=69.8911
	step [52/191], loss=69.1864
	step [53/191], loss=67.6109
	step [54/191], loss=68.1409
	step [55/191], loss=68.4122
	step [56/191], loss=68.8330
	step [57/191], loss=67.9016
	step [58/191], loss=67.9943
	step [59/191], loss=67.8568
	step [60/191], loss=68.2913
	step [61/191], loss=68.6890
	step [62/191], loss=68.2064
	step [63/191], loss=68.1958
	step [64/191], loss=67.7479
	step [65/191], loss=66.8582
	step [66/191], loss=66.7576
	step [67/191], loss=67.9148
	step [68/191], loss=66.6832
	step [69/191], loss=66.7485
	step [70/191], loss=66.8939
	step [71/191], loss=67.3150
	step [72/191], loss=66.8158
	step [73/191], loss=68.2024
	step [74/191], loss=66.5734
	step [75/191], loss=66.3931
	step [76/191], loss=69.1427
	step [77/191], loss=65.9888
	step [78/191], loss=66.6201
	step [79/191], loss=65.4603
	step [80/191], loss=67.6579
	step [81/191], loss=65.9983
	step [82/191], loss=65.9622
	step [83/191], loss=65.8263
	step [84/191], loss=67.3556
	step [85/191], loss=65.6980
	step [86/191], loss=66.0500
	step [87/191], loss=67.1691
	step [88/191], loss=67.7480
	step [89/191], loss=65.6187
	step [90/191], loss=65.0689
	step [91/191], loss=65.5127
	step [92/191], loss=65.0774
	step [93/191], loss=63.8740
	step [94/191], loss=63.5766
	step [95/191], loss=65.3526
	step [96/191], loss=65.0636
	step [97/191], loss=66.6060
	step [98/191], loss=64.2172
	step [99/191], loss=65.9615
	step [100/191], loss=64.4147
	step [101/191], loss=63.4548
	step [102/191], loss=65.3135
	step [103/191], loss=65.2405
	step [104/191], loss=64.1078
	step [105/191], loss=64.7071
	step [106/191], loss=64.5530
	step [107/191], loss=64.0564
	step [108/191], loss=63.6759
	step [109/191], loss=62.3384
	step [110/191], loss=63.2870
	step [111/191], loss=63.6339
	step [112/191], loss=62.8027
	step [113/191], loss=62.8686
	step [114/191], loss=64.2961
	step [115/191], loss=62.9793
	step [116/191], loss=63.2788
	step [117/191], loss=63.8826
	step [118/191], loss=62.8288
	step [119/191], loss=62.4106
	step [120/191], loss=62.2659
	step [121/191], loss=64.3172
	step [122/191], loss=62.3964
	step [123/191], loss=63.5606
	step [124/191], loss=62.3396
	step [125/191], loss=63.1488
	step [126/191], loss=60.4113
	step [127/191], loss=62.2794
	step [128/191], loss=61.4683
	step [129/191], loss=63.3942
	step [130/191], loss=62.0003
	step [131/191], loss=63.7806
	step [132/191], loss=62.0915
	step [133/191], loss=60.8072
	step [134/191], loss=60.9369
	step [135/191], loss=61.1754
	step [136/191], loss=60.6758
	step [137/191], loss=60.2824
	step [138/191], loss=60.3472
	step [139/191], loss=60.3669
	step [140/191], loss=61.0528
	step [141/191], loss=61.1762
	step [142/191], loss=60.1312
	step [143/191], loss=61.9178
	step [144/191], loss=61.3809
	step [145/191], loss=60.0476
	step [146/191], loss=61.5510
	step [147/191], loss=60.7943
	step [148/191], loss=59.1775
	step [149/191], loss=59.4760
	step [150/191], loss=60.0744
	step [151/191], loss=61.0769
	step [152/191], loss=60.8328
	step [153/191], loss=61.2225
	step [154/191], loss=59.5203
	step [155/191], loss=60.8248
	step [156/191], loss=58.8295
	step [157/191], loss=58.5115
	step [158/191], loss=59.8129
	step [159/191], loss=60.6611
	step [160/191], loss=58.3298
	step [161/191], loss=60.8339
	step [162/191], loss=59.7947
	step [163/191], loss=57.9071
	step [164/191], loss=57.7419
	step [165/191], loss=59.3164
	step [166/191], loss=58.2869
	step [167/191], loss=58.9322
	step [168/191], loss=58.5515
	step [169/191], loss=59.2313
	step [170/191], loss=60.0484
	step [171/191], loss=58.5730
	step [172/191], loss=58.1126
	step [173/191], loss=59.5442
	step [174/191], loss=57.9026
	step [175/191], loss=57.8097
	step [176/191], loss=58.0971
	step [177/191], loss=57.1108
	step [178/191], loss=58.9140
	step [179/191], loss=57.5381
	step [180/191], loss=57.3053
	step [181/191], loss=57.7433
	step [182/191], loss=59.7095
	step [183/191], loss=56.3063
	step [184/191], loss=57.4519
	step [185/191], loss=57.3635
	step [186/191], loss=57.1846
	step [187/191], loss=56.6824
	step [188/191], loss=56.5630
	step [189/191], loss=56.6585
	step [190/191], loss=55.2103
	step [191/191], loss=26.4427
	Evaluating
	loss=0.2773, precision=0.2068, recall=0.9944, f1=0.3424
saving model as: 1_saved_model.pth
Training epoch 3
	step [1/191], loss=56.3209
	step [2/191], loss=57.0956
	step [3/191], loss=55.9801
	step [4/191], loss=56.2145
	step [5/191], loss=55.8990
	step [6/191], loss=57.3853
	step [7/191], loss=56.5054
	step [8/191], loss=58.4739
	step [9/191], loss=57.3054
	step [10/191], loss=56.5469
	step [11/191], loss=55.5390
	step [12/191], loss=56.7578
	step [13/191], loss=55.2424
	step [14/191], loss=57.0565
	step [15/191], loss=55.7283
	step [16/191], loss=54.6723
	step [17/191], loss=56.5601
	step [18/191], loss=54.4217
	step [19/191], loss=55.2407
	step [20/191], loss=54.4519
	step [21/191], loss=58.2152
	step [22/191], loss=57.4928
	step [23/191], loss=55.7494
	step [24/191], loss=55.2901
	step [25/191], loss=55.7984
	step [26/191], loss=56.4177
	step [27/191], loss=54.6572
	step [28/191], loss=53.9342
	step [29/191], loss=55.6472
	step [30/191], loss=53.7623
	step [31/191], loss=56.2927
	step [32/191], loss=54.0809
	step [33/191], loss=52.9431
	step [34/191], loss=52.6134
	step [35/191], loss=55.3428
	step [36/191], loss=53.2507
	step [37/191], loss=55.5390
	step [38/191], loss=54.5716
	step [39/191], loss=53.9226
	step [40/191], loss=53.9146
	step [41/191], loss=53.4043
	step [42/191], loss=55.7463
	step [43/191], loss=52.8946
	step [44/191], loss=53.0299
	step [45/191], loss=54.1356
	step [46/191], loss=53.0988
	step [47/191], loss=52.9284
	step [48/191], loss=52.7792
	step [49/191], loss=52.4375
	step [50/191], loss=54.6792
	step [51/191], loss=53.4349
	step [52/191], loss=53.0957
	step [53/191], loss=53.6867
	step [54/191], loss=52.0671
	step [55/191], loss=52.4471
	step [56/191], loss=52.1544
	step [57/191], loss=52.2121
	step [58/191], loss=52.0457
	step [59/191], loss=52.8971
	step [60/191], loss=52.9284
	step [61/191], loss=53.6015
	step [62/191], loss=52.7721
	step [63/191], loss=51.3548
	step [64/191], loss=51.2826
	step [65/191], loss=51.0784
	step [66/191], loss=52.7391
	step [67/191], loss=50.4529
	step [68/191], loss=52.3830
	step [69/191], loss=51.1215
	step [70/191], loss=51.8358
	step [71/191], loss=52.2291
	step [72/191], loss=52.4916
	step [73/191], loss=50.8365
	step [74/191], loss=53.5665
	step [75/191], loss=51.8100
	step [76/191], loss=51.4001
	step [77/191], loss=50.6922
	step [78/191], loss=50.9973
	step [79/191], loss=50.5423
	step [80/191], loss=51.9772
	step [81/191], loss=54.3893
	step [82/191], loss=50.3683
	step [83/191], loss=51.7170
	step [84/191], loss=51.0627
	step [85/191], loss=49.7733
	step [86/191], loss=51.3479
	step [87/191], loss=52.3467
	step [88/191], loss=50.7674
	step [89/191], loss=50.8436
	step [90/191], loss=49.9160
	step [91/191], loss=49.7300
	step [92/191], loss=51.8897
	step [93/191], loss=49.8882
	step [94/191], loss=50.6584
	step [95/191], loss=50.1989
	step [96/191], loss=50.5005
	step [97/191], loss=49.9709
	step [98/191], loss=49.1611
	step [99/191], loss=50.6252
	step [100/191], loss=51.4253
	step [101/191], loss=50.0423
	step [102/191], loss=48.8531
	step [103/191], loss=48.9278
	step [104/191], loss=52.1131
	step [105/191], loss=49.0734
	step [106/191], loss=49.0520
	step [107/191], loss=49.6456
	step [108/191], loss=48.6898
	step [109/191], loss=49.3600
	step [110/191], loss=48.5271
	step [111/191], loss=50.5664
	step [112/191], loss=48.5874
	step [113/191], loss=49.3357
	step [114/191], loss=48.9167
	step [115/191], loss=48.7534
	step [116/191], loss=48.4540
	step [117/191], loss=48.3387
	step [118/191], loss=48.6815
	step [119/191], loss=48.2914
	step [120/191], loss=49.2585
	step [121/191], loss=49.6523
	step [122/191], loss=49.1247
	step [123/191], loss=49.0393
	step [124/191], loss=47.9845
	step [125/191], loss=48.0329
	step [126/191], loss=49.5460
	step [127/191], loss=47.9654
	step [128/191], loss=48.4859
	step [129/191], loss=47.7518
	step [130/191], loss=47.5420
	step [131/191], loss=46.9254
	step [132/191], loss=49.4763
	step [133/191], loss=47.4296
	step [134/191], loss=49.1684
	step [135/191], loss=47.8239
	step [136/191], loss=49.2641
	step [137/191], loss=47.6745
	step [138/191], loss=48.0904
	step [139/191], loss=47.8022
	step [140/191], loss=47.2595
	step [141/191], loss=46.4577
	step [142/191], loss=47.2893
	step [143/191], loss=47.7601
	step [144/191], loss=47.3628
	step [145/191], loss=46.9443
	step [146/191], loss=47.9693
	step [147/191], loss=46.3833
	step [148/191], loss=47.3081
	step [149/191], loss=45.7293
	step [150/191], loss=47.3813
	step [151/191], loss=47.0642
	step [152/191], loss=45.8487
	step [153/191], loss=47.1935
	step [154/191], loss=46.5886
	step [155/191], loss=47.7006
	step [156/191], loss=47.8025
	step [157/191], loss=46.4078
	step [158/191], loss=46.0949
	step [159/191], loss=46.1308
	step [160/191], loss=45.4503
	step [161/191], loss=48.0226
	step [162/191], loss=46.5903
	step [163/191], loss=45.9639
	step [164/191], loss=45.5513
	step [165/191], loss=46.5231
	step [166/191], loss=46.6456
	step [167/191], loss=44.9204
	step [168/191], loss=45.0082
	step [169/191], loss=46.0788
	step [170/191], loss=46.6021
	step [171/191], loss=45.3696
	step [172/191], loss=46.2519
	step [173/191], loss=47.2005
	step [174/191], loss=46.6034
	step [175/191], loss=44.1091
	step [176/191], loss=44.9589
	step [177/191], loss=46.2268
	step [178/191], loss=45.4048
	step [179/191], loss=45.6878
	step [180/191], loss=46.3959
	step [181/191], loss=45.5799
	step [182/191], loss=45.6746
	step [183/191], loss=44.3319
	step [184/191], loss=44.5507
	step [185/191], loss=45.2075
	step [186/191], loss=44.5670
	step [187/191], loss=44.8520
	step [188/191], loss=45.8193
	step [189/191], loss=46.5482
	step [190/191], loss=44.8458
	step [191/191], loss=20.0668
	Evaluating
	loss=0.2141, precision=0.1933, recall=0.9950, f1=0.3237
Training epoch 4
	step [1/191], loss=45.0692
	step [2/191], loss=43.8016
	step [3/191], loss=44.3691
	step [4/191], loss=43.1515
	step [5/191], loss=43.3581
	step [6/191], loss=44.7004
	step [7/191], loss=44.4179
	step [8/191], loss=44.2198
	step [9/191], loss=43.8471
	step [10/191], loss=43.6756
	step [11/191], loss=43.5741
	step [12/191], loss=42.8920
	step [13/191], loss=46.0152
	step [14/191], loss=44.5291
	step [15/191], loss=42.1881
	step [16/191], loss=43.6218
	step [17/191], loss=44.7836
	step [18/191], loss=44.6031
	step [19/191], loss=42.8825
	step [20/191], loss=42.5653
	step [21/191], loss=43.8009
	step [22/191], loss=44.8133
	step [23/191], loss=44.0235
	step [24/191], loss=43.0105
	step [25/191], loss=43.6128
	step [26/191], loss=42.9307
	step [27/191], loss=43.4140
	step [28/191], loss=43.4859
	step [29/191], loss=42.0823
	step [30/191], loss=42.7281
	step [31/191], loss=42.4141
	step [32/191], loss=41.3166
	step [33/191], loss=44.0359
	step [34/191], loss=43.1516
	step [35/191], loss=42.1504
	step [36/191], loss=43.2928
	step [37/191], loss=44.0647
	step [38/191], loss=43.3498
	step [39/191], loss=43.0383
	step [40/191], loss=42.9179
	step [41/191], loss=42.5959
	step [42/191], loss=42.0004
	step [43/191], loss=41.8786
	step [44/191], loss=42.5106
	step [45/191], loss=42.0409
	step [46/191], loss=40.7188
	step [47/191], loss=41.6492
	step [48/191], loss=41.9599
	step [49/191], loss=41.8617
	step [50/191], loss=43.7560
	step [51/191], loss=42.1863
	step [52/191], loss=43.8326
	step [53/191], loss=42.1316
	step [54/191], loss=41.3867
	step [55/191], loss=41.3106
	step [56/191], loss=43.2572
	step [57/191], loss=42.9068
	step [58/191], loss=41.3714
	step [59/191], loss=41.2242
	step [60/191], loss=41.9420
	step [61/191], loss=40.1759
	step [62/191], loss=40.2473
	step [63/191], loss=40.1636
	step [64/191], loss=40.1505
	step [65/191], loss=41.8966
	step [66/191], loss=40.9440
	step [67/191], loss=39.8053
	step [68/191], loss=40.0942
	step [69/191], loss=40.0614
	step [70/191], loss=40.3963
	step [71/191], loss=40.7182
	step [72/191], loss=40.6071
	step [73/191], loss=41.0845
	step [74/191], loss=42.3592
	step [75/191], loss=40.3271
	step [76/191], loss=39.5623
	step [77/191], loss=42.9087
	step [78/191], loss=39.9793
	step [79/191], loss=41.5658
	step [80/191], loss=40.6083
	step [81/191], loss=39.8772
	step [82/191], loss=39.6649
	step [83/191], loss=40.8732
	step [84/191], loss=40.5915
	step [85/191], loss=40.1324
	step [86/191], loss=39.3834
	step [87/191], loss=39.3467
	step [88/191], loss=40.7768
	step [89/191], loss=40.5189
	step [90/191], loss=40.1511
	step [91/191], loss=39.0155
	step [92/191], loss=39.7649
	step [93/191], loss=40.1835
	step [94/191], loss=41.4425
	step [95/191], loss=40.0364
	step [96/191], loss=43.0904
	step [97/191], loss=39.2931
	step [98/191], loss=39.7938
	step [99/191], loss=39.8343
	step [100/191], loss=39.2968
	step [101/191], loss=38.2778
	step [102/191], loss=42.8281
	step [103/191], loss=38.6782
	step [104/191], loss=37.9498
	step [105/191], loss=39.2064
	step [106/191], loss=39.1532
	step [107/191], loss=39.3750
	step [108/191], loss=39.0263
	step [109/191], loss=39.0479
	step [110/191], loss=38.6706
	step [111/191], loss=38.8228
	step [112/191], loss=38.0420
	step [113/191], loss=38.9173
	step [114/191], loss=38.7648
	step [115/191], loss=39.5904
	step [116/191], loss=38.4254
	step [117/191], loss=39.3383
	step [118/191], loss=39.1392
	step [119/191], loss=40.4739
	step [120/191], loss=38.7959
	step [121/191], loss=38.1102
	step [122/191], loss=39.6191
	step [123/191], loss=39.5440
	step [124/191], loss=38.8430
	step [125/191], loss=39.2779
	step [126/191], loss=37.4314
	step [127/191], loss=39.6141
	step [128/191], loss=40.0846
	step [129/191], loss=37.9877
	step [130/191], loss=38.3978
	step [131/191], loss=39.2378
	step [132/191], loss=39.0264
	step [133/191], loss=37.3061
	step [134/191], loss=37.5571
	step [135/191], loss=38.7757
	step [136/191], loss=37.8068
	step [137/191], loss=37.4138
	step [138/191], loss=38.3067
	step [139/191], loss=38.3681
	step [140/191], loss=36.9191
	step [141/191], loss=37.9547
	step [142/191], loss=37.7652
	step [143/191], loss=36.8345
	step [144/191], loss=39.5862
	step [145/191], loss=37.7617
	step [146/191], loss=37.3586
	step [147/191], loss=36.2696
	step [148/191], loss=37.8888
	step [149/191], loss=37.5982
	step [150/191], loss=38.4632
	step [151/191], loss=37.2831
	step [152/191], loss=37.7627
	step [153/191], loss=37.7209
	step [154/191], loss=36.5625
	step [155/191], loss=36.1854
	step [156/191], loss=36.7223
	step [157/191], loss=36.1392
	step [158/191], loss=38.2528
	step [159/191], loss=38.5678
	step [160/191], loss=37.3187
	step [161/191], loss=36.6838
	step [162/191], loss=35.8874
	step [163/191], loss=37.1640
	step [164/191], loss=37.0067
	step [165/191], loss=35.9419
	step [166/191], loss=37.1802
	step [167/191], loss=36.7725
	step [168/191], loss=37.5990
	step [169/191], loss=38.4397
	step [170/191], loss=39.2108
	step [171/191], loss=37.4692
	step [172/191], loss=36.2840
	step [173/191], loss=36.9353
	step [174/191], loss=36.0807
	step [175/191], loss=35.6952
	step [176/191], loss=36.4809
	step [177/191], loss=35.4431
	step [178/191], loss=36.1784
	step [179/191], loss=36.1863
	step [180/191], loss=36.5240
	step [181/191], loss=35.7540
	step [182/191], loss=36.2953
	step [183/191], loss=35.8431
	step [184/191], loss=35.9532
	step [185/191], loss=36.1708
	step [186/191], loss=37.5591
	step [187/191], loss=35.4597
	step [188/191], loss=35.3484
	step [189/191], loss=35.0424
	step [190/191], loss=34.8589
	step [191/191], loss=15.7259
	Evaluating
	loss=0.1682, precision=0.2045, recall=0.9945, f1=0.3393
Training epoch 5
	step [1/191], loss=35.3341
	step [2/191], loss=37.1388
	step [3/191], loss=34.2524
	step [4/191], loss=35.4281
	step [5/191], loss=37.4202
	step [6/191], loss=36.2845
	step [7/191], loss=34.6491
	step [8/191], loss=34.5715
	step [9/191], loss=34.2263
	step [10/191], loss=34.6031
	step [11/191], loss=35.8932
	step [12/191], loss=34.5057
	step [13/191], loss=35.1342
	step [14/191], loss=35.1365
	step [15/191], loss=35.0739
	step [16/191], loss=35.3780
	step [17/191], loss=33.6979
	step [18/191], loss=36.3220
	step [19/191], loss=35.9837
	step [20/191], loss=36.3100
	step [21/191], loss=34.5022
	step [22/191], loss=33.1910
	step [23/191], loss=34.6392
	step [24/191], loss=35.3417
	step [25/191], loss=34.7221
	step [26/191], loss=34.2127
	step [27/191], loss=33.2529
	step [28/191], loss=34.1410
	step [29/191], loss=37.0980
	step [30/191], loss=34.6061
	step [31/191], loss=35.2964
	step [32/191], loss=35.8490
	step [33/191], loss=35.2456
	step [34/191], loss=35.8465
	step [35/191], loss=34.5889
	step [36/191], loss=34.2895
	step [37/191], loss=34.7684
	step [38/191], loss=33.1377
	step [39/191], loss=35.2796
	step [40/191], loss=32.4257
	step [41/191], loss=35.2814
	step [42/191], loss=36.7927
	step [43/191], loss=33.6741
	step [44/191], loss=33.6806
	step [45/191], loss=33.1804
	step [46/191], loss=33.9665
	step [47/191], loss=33.1898
	step [48/191], loss=33.3650
	step [49/191], loss=32.4938
	step [50/191], loss=33.6463
	step [51/191], loss=34.2916
	step [52/191], loss=34.5668
	step [53/191], loss=33.2656
	step [54/191], loss=34.2835
	step [55/191], loss=33.6626
	step [56/191], loss=33.3174
	step [57/191], loss=34.9607
	step [58/191], loss=34.6233
	step [59/191], loss=33.6288
	step [60/191], loss=32.7157
	step [61/191], loss=33.4866
	step [62/191], loss=34.0352
	step [63/191], loss=33.9126
	step [64/191], loss=32.8079
	step [65/191], loss=33.3986
	step [66/191], loss=32.9176
	step [67/191], loss=31.3913
	step [68/191], loss=33.5246
	step [69/191], loss=32.1998
	step [70/191], loss=32.2701
	step [71/191], loss=31.4702
	step [72/191], loss=32.0274
	step [73/191], loss=32.3034
	step [74/191], loss=31.4756
	step [75/191], loss=33.8937
	step [76/191], loss=33.5373
	step [77/191], loss=34.7293
	step [78/191], loss=34.2365
	step [79/191], loss=34.0145
	step [80/191], loss=34.0207
	step [81/191], loss=32.7170
	step [82/191], loss=34.2235
	step [83/191], loss=33.9457
	step [84/191], loss=31.8721
	step [85/191], loss=33.6713
	step [86/191], loss=32.7516
	step [87/191], loss=31.6838
	step [88/191], loss=32.4471
	step [89/191], loss=31.5141
	step [90/191], loss=33.3754
	step [91/191], loss=32.0676
	step [92/191], loss=31.9751
	step [93/191], loss=32.6014
	step [94/191], loss=31.4919
	step [95/191], loss=34.2645
	step [96/191], loss=31.0583
	step [97/191], loss=31.8174
	step [98/191], loss=33.2510
	step [99/191], loss=32.1982
	step [100/191], loss=32.5771
	step [101/191], loss=32.2846
	step [102/191], loss=31.7205
	step [103/191], loss=32.7299
	step [104/191], loss=31.6925
	step [105/191], loss=31.2445
	step [106/191], loss=32.8606
	step [107/191], loss=32.2319
	step [108/191], loss=32.0965
	step [109/191], loss=31.7092
	step [110/191], loss=30.8524
	step [111/191], loss=32.6618
	step [112/191], loss=31.2967
	step [113/191], loss=31.3491
	step [114/191], loss=30.7804
	step [115/191], loss=30.4843
	step [116/191], loss=31.0020
	step [117/191], loss=31.1694
	step [118/191], loss=32.1086
	step [119/191], loss=30.8281
	step [120/191], loss=32.7306
	step [121/191], loss=31.1970
	step [122/191], loss=32.1838
	step [123/191], loss=30.7599
	step [124/191], loss=31.0352
	step [125/191], loss=30.4982
	step [126/191], loss=31.9002
	step [127/191], loss=31.0814
	step [128/191], loss=34.0906
	step [129/191], loss=30.5504
	step [130/191], loss=30.6003
	step [131/191], loss=30.5127
	step [132/191], loss=31.7022
	step [133/191], loss=30.6366
	step [134/191], loss=31.5014
	step [135/191], loss=31.7946
	step [136/191], loss=30.5474
	step [137/191], loss=29.8423
	step [138/191], loss=29.8323
	step [139/191], loss=31.3989
	step [140/191], loss=31.5320
	step [141/191], loss=29.7747
	step [142/191], loss=32.1662
	step [143/191], loss=30.5374
	step [144/191], loss=29.9277
	step [145/191], loss=30.4588
	step [146/191], loss=29.5206
	step [147/191], loss=31.6380
	step [148/191], loss=31.1537
	step [149/191], loss=29.7695
	step [150/191], loss=30.9008
	step [151/191], loss=30.9979
	step [152/191], loss=30.5105
	step [153/191], loss=30.5280
	step [154/191], loss=30.1169
	step [155/191], loss=29.8135
	step [156/191], loss=30.1158
	step [157/191], loss=29.9383
	step [158/191], loss=29.1756
	step [159/191], loss=31.1237
	step [160/191], loss=30.3917
	step [161/191], loss=31.2497
	step [162/191], loss=30.0021
	step [163/191], loss=30.6797
	step [164/191], loss=30.5789
	step [165/191], loss=30.7102
	step [166/191], loss=29.8966
	step [167/191], loss=30.9802
	step [168/191], loss=29.2489
	step [169/191], loss=29.5851
	step [170/191], loss=31.8545
	step [171/191], loss=29.4150
	step [172/191], loss=32.1922
	step [173/191], loss=29.8097
	step [174/191], loss=29.0045
	step [175/191], loss=30.1919
	step [176/191], loss=31.5327
	step [177/191], loss=31.5277
	step [178/191], loss=29.9683
	step [179/191], loss=29.8583
	step [180/191], loss=28.1692
	step [181/191], loss=28.7241
	step [182/191], loss=28.8116
	step [183/191], loss=29.7484
	step [184/191], loss=30.3067
	step [185/191], loss=30.2956
	step [186/191], loss=30.1762
	step [187/191], loss=28.9347
	step [188/191], loss=28.9175
	step [189/191], loss=30.1378
	step [190/191], loss=28.8858
	step [191/191], loss=12.8024
	Evaluating
	loss=0.1328, precision=0.2400, recall=0.9929, f1=0.3866
saving model as: 1_saved_model.pth
Training epoch 6
	step [1/191], loss=29.3493
	step [2/191], loss=29.7047
	step [3/191], loss=28.3117
	step [4/191], loss=29.1018
	step [5/191], loss=28.6783
	step [6/191], loss=30.2020
	step [7/191], loss=27.2730
	step [8/191], loss=28.7525
	step [9/191], loss=31.0047
	step [10/191], loss=29.0535
	step [11/191], loss=28.4111
	step [12/191], loss=28.8970
	step [13/191], loss=29.1595
	step [14/191], loss=29.0306
	step [15/191], loss=29.3103
	step [16/191], loss=29.9475
	step [17/191], loss=28.4438
	step [18/191], loss=28.8863
	step [19/191], loss=28.7341
	step [20/191], loss=28.1475
	step [21/191], loss=28.5027
	step [22/191], loss=28.7939
	step [23/191], loss=28.7421
	step [24/191], loss=29.2025
	step [25/191], loss=28.5038
	step [26/191], loss=28.9649
	step [27/191], loss=27.5981
	step [28/191], loss=27.9108
	step [29/191], loss=28.6035
	step [30/191], loss=28.7931
	step [31/191], loss=28.0862
	step [32/191], loss=28.4033
	step [33/191], loss=29.2977
	step [34/191], loss=29.3657
	step [35/191], loss=28.6708
	step [36/191], loss=30.6071
	step [37/191], loss=27.5593
	step [38/191], loss=27.8982
	step [39/191], loss=28.1288
	step [40/191], loss=27.1560
	step [41/191], loss=27.7940
	step [42/191], loss=29.0425
	step [43/191], loss=29.4704
	step [44/191], loss=28.5044
	step [45/191], loss=29.6158
	step [46/191], loss=27.4006
	step [47/191], loss=28.0738
	step [48/191], loss=27.8860
	step [49/191], loss=28.6397
	step [50/191], loss=27.1668
	step [51/191], loss=27.4557
	step [52/191], loss=28.9796
	step [53/191], loss=30.0410
	step [54/191], loss=27.7626
	step [55/191], loss=29.3211
	step [56/191], loss=28.0546
	step [57/191], loss=27.6383
	step [58/191], loss=30.0591
	step [59/191], loss=28.8911
	step [60/191], loss=27.8434
	step [61/191], loss=27.9122
	step [62/191], loss=28.5980
	step [63/191], loss=27.3451
	step [64/191], loss=28.0941
	step [65/191], loss=27.6425
	step [66/191], loss=28.4574
	step [67/191], loss=28.3737
	step [68/191], loss=26.8065
	step [69/191], loss=26.9613
	step [70/191], loss=26.7222
	step [71/191], loss=27.1393
	step [72/191], loss=27.8961
	step [73/191], loss=27.3020
	step [74/191], loss=26.4171
	step [75/191], loss=28.2531
	step [76/191], loss=28.1975
	step [77/191], loss=28.2170
	step [78/191], loss=26.8890
	step [79/191], loss=27.6015
	step [80/191], loss=27.4856
	step [81/191], loss=27.2955
	step [82/191], loss=26.9715
	step [83/191], loss=26.7852
	step [84/191], loss=25.4026
	step [85/191], loss=27.6173
	step [86/191], loss=27.7915
	step [87/191], loss=28.4361
	step [88/191], loss=27.8530
	step [89/191], loss=27.2172
	step [90/191], loss=27.1217
	step [91/191], loss=27.1442
	step [92/191], loss=26.4208
	step [93/191], loss=26.2932
	step [94/191], loss=26.2012
	step [95/191], loss=25.2251
	step [96/191], loss=25.0671
	step [97/191], loss=25.9511
	step [98/191], loss=27.4810
	step [99/191], loss=26.7689
	step [100/191], loss=25.9810
	step [101/191], loss=26.8450
	step [102/191], loss=26.0512
	step [103/191], loss=27.9225
	step [104/191], loss=28.0764
	step [105/191], loss=27.2878
	step [106/191], loss=25.7692
	step [107/191], loss=26.1370
	step [108/191], loss=25.1624
	step [109/191], loss=25.5222
	step [110/191], loss=28.3776
	step [111/191], loss=27.2846
	step [112/191], loss=27.0453
	step [113/191], loss=26.5996
	step [114/191], loss=25.4505
	step [115/191], loss=27.1996
	step [116/191], loss=25.4802
	step [117/191], loss=27.8123
	step [118/191], loss=25.1787
	step [119/191], loss=25.4301
	step [120/191], loss=26.9252
	step [121/191], loss=26.2164
	step [122/191], loss=25.5432
	step [123/191], loss=25.0686
	step [124/191], loss=26.8333
	step [125/191], loss=25.9678
	step [126/191], loss=26.8577
	step [127/191], loss=26.9372
	step [128/191], loss=26.1203
	step [129/191], loss=26.0715
	step [130/191], loss=25.4203
	step [131/191], loss=24.7295
	step [132/191], loss=25.2078
	step [133/191], loss=26.1776
	step [134/191], loss=25.1809
	step [135/191], loss=24.1686
	step [136/191], loss=26.0411
	step [137/191], loss=25.0927
	step [138/191], loss=27.0316
	step [139/191], loss=25.6944
	step [140/191], loss=25.7263
	step [141/191], loss=26.8482
	step [142/191], loss=26.0371
	step [143/191], loss=25.7769
	step [144/191], loss=25.4989
	step [145/191], loss=25.8760
	step [146/191], loss=25.0359
	step [147/191], loss=25.6937
	step [148/191], loss=25.0035
	step [149/191], loss=26.4942
	step [150/191], loss=24.9052
	step [151/191], loss=26.4097
	step [152/191], loss=24.7531
	step [153/191], loss=25.0878
	step [154/191], loss=25.9634
	step [155/191], loss=26.2041
	step [156/191], loss=26.0354
	step [157/191], loss=24.7041
	step [158/191], loss=25.4631
	step [159/191], loss=24.7181
	step [160/191], loss=26.3506
	step [161/191], loss=25.3099
	step [162/191], loss=25.1702
	step [163/191], loss=25.7333
	step [164/191], loss=24.4590
	step [165/191], loss=24.9013
	step [166/191], loss=26.5524
	step [167/191], loss=25.8341
	step [168/191], loss=24.3951
	step [169/191], loss=24.5376
	step [170/191], loss=24.4797
	step [171/191], loss=24.2027
	step [172/191], loss=23.6955
	step [173/191], loss=23.7876
	step [174/191], loss=23.9668
	step [175/191], loss=25.9384
	step [176/191], loss=25.4170
	step [177/191], loss=24.9881
	step [178/191], loss=24.7398
	step [179/191], loss=24.4123
	step [180/191], loss=24.8077
	step [181/191], loss=25.3650
	step [182/191], loss=26.1676
	step [183/191], loss=24.5658
	step [184/191], loss=24.5063
	step [185/191], loss=24.8172
	step [186/191], loss=24.8415
	step [187/191], loss=25.1323
	step [188/191], loss=24.2147
	step [189/191], loss=24.6496
	step [190/191], loss=23.5793
	step [191/191], loss=10.7278
	Evaluating
	loss=0.1109, precision=0.2324, recall=0.9931, f1=0.3766
Training epoch 7
	step [1/191], loss=24.8887
	step [2/191], loss=24.4342
	step [3/191], loss=25.9733
	step [4/191], loss=23.7069
	step [5/191], loss=25.9982
	step [6/191], loss=25.1167
	step [7/191], loss=24.5953
	step [8/191], loss=24.2330
	step [9/191], loss=23.6677
	step [10/191], loss=25.4151
	step [11/191], loss=24.0517
	step [12/191], loss=23.3727
	step [13/191], loss=23.6621
	step [14/191], loss=24.5552
	step [15/191], loss=23.9575
	step [16/191], loss=25.6346
	step [17/191], loss=24.7958
	step [18/191], loss=23.1802
	step [19/191], loss=24.6615
	step [20/191], loss=23.6080
	step [21/191], loss=25.7371
	step [22/191], loss=24.2146
	step [23/191], loss=24.7351
	step [24/191], loss=24.5743
	step [25/191], loss=25.2309
	step [26/191], loss=23.8030
	step [27/191], loss=23.7820
	step [28/191], loss=22.4982
	step [29/191], loss=23.2984
	step [30/191], loss=23.8351
	step [31/191], loss=24.2445
	step [32/191], loss=23.7932
	step [33/191], loss=23.9541
	step [34/191], loss=24.5307
	step [35/191], loss=23.0658
	step [36/191], loss=24.1794
	step [37/191], loss=24.4427
	step [38/191], loss=23.8589
	step [39/191], loss=23.5495
	step [40/191], loss=23.6754
	step [41/191], loss=23.5330
	step [42/191], loss=23.7527
	step [43/191], loss=23.6411
	step [44/191], loss=23.7335
	step [45/191], loss=24.2840
	step [46/191], loss=23.0102
	step [47/191], loss=23.7527
	step [48/191], loss=22.6184
	step [49/191], loss=22.8819
	step [50/191], loss=24.1204
	step [51/191], loss=24.5966
	step [52/191], loss=22.3456
	step [53/191], loss=23.6710
	step [54/191], loss=24.6169
	step [55/191], loss=23.5667
	step [56/191], loss=22.9168
	step [57/191], loss=22.4247
	step [58/191], loss=24.5465
	step [59/191], loss=22.3625
	step [60/191], loss=23.5734
	step [61/191], loss=25.5269
	step [62/191], loss=23.7184
	step [63/191], loss=23.4340
	step [64/191], loss=24.3922
	step [65/191], loss=22.0797
	step [66/191], loss=23.1737
	step [67/191], loss=23.3961
	step [68/191], loss=23.1574
	step [69/191], loss=23.5836
	step [70/191], loss=22.9469
	step [71/191], loss=24.6367
	step [72/191], loss=21.2001
	step [73/191], loss=22.3041
	step [74/191], loss=21.3891
	step [75/191], loss=22.8273
	step [76/191], loss=24.0856
	step [77/191], loss=23.2748
	step [78/191], loss=22.1838
	step [79/191], loss=23.3732
	step [80/191], loss=22.4993
	step [81/191], loss=22.8244
	step [82/191], loss=23.6098
	step [83/191], loss=22.8106
	step [84/191], loss=22.7685
	step [85/191], loss=22.3605
	step [86/191], loss=22.5067
	step [87/191], loss=22.2342
	step [88/191], loss=23.0005
	step [89/191], loss=22.6038
	step [90/191], loss=24.1637
	step [91/191], loss=24.8086
	step [92/191], loss=23.3863
	step [93/191], loss=22.8159
	step [94/191], loss=22.8831
	step [95/191], loss=22.7900
	step [96/191], loss=22.5228
	step [97/191], loss=24.4074
	step [98/191], loss=22.0660
	step [99/191], loss=22.1269
	step [100/191], loss=23.6988
	step [101/191], loss=20.5883
	step [102/191], loss=23.1421
	step [103/191], loss=22.6981
	step [104/191], loss=21.5504
	step [105/191], loss=23.6595
	step [106/191], loss=22.3776
	step [107/191], loss=21.5268
	step [108/191], loss=21.9424
	step [109/191], loss=21.9210
	step [110/191], loss=21.4928
	step [111/191], loss=23.3427
	step [112/191], loss=24.0854
	step [113/191], loss=22.4193
	step [114/191], loss=22.7292
	step [115/191], loss=21.5954
	step [116/191], loss=21.4879
	step [117/191], loss=22.8468
	step [118/191], loss=22.1715
	step [119/191], loss=21.3296
	step [120/191], loss=23.8670
	step [121/191], loss=22.5212
	step [122/191], loss=23.7615
	step [123/191], loss=20.3991
	step [124/191], loss=23.7625
	step [125/191], loss=24.1632
	step [126/191], loss=21.7400
	step [127/191], loss=22.8968
	step [128/191], loss=22.3452
	step [129/191], loss=21.8626
	step [130/191], loss=21.5078
	step [131/191], loss=20.6080
	step [132/191], loss=21.3483
	step [133/191], loss=21.8999
	step [134/191], loss=23.1707
	step [135/191], loss=21.6368
	step [136/191], loss=21.5448
	step [137/191], loss=23.4251
	step [138/191], loss=21.5435
	step [139/191], loss=22.2671
	step [140/191], loss=22.4613
	step [141/191], loss=22.6312
	step [142/191], loss=21.4851
	step [143/191], loss=20.6211
	step [144/191], loss=21.3695
	step [145/191], loss=22.5783
	step [146/191], loss=22.8114
	step [147/191], loss=21.6875
	step [148/191], loss=22.8909
	step [149/191], loss=22.0324
	step [150/191], loss=23.2554
	step [151/191], loss=20.6032
	step [152/191], loss=22.2513
	step [153/191], loss=21.6406
	step [154/191], loss=21.1452
	step [155/191], loss=22.9685
	step [156/191], loss=23.2657
	step [157/191], loss=20.9956
	step [158/191], loss=22.7543
	step [159/191], loss=22.5658
	step [160/191], loss=20.9591
	step [161/191], loss=21.6352
	step [162/191], loss=21.4286
	step [163/191], loss=21.0707
	step [164/191], loss=21.3971
	step [165/191], loss=22.1789
	step [166/191], loss=22.0369
	step [167/191], loss=20.7881
	step [168/191], loss=22.6631
	step [169/191], loss=21.5737
	step [170/191], loss=20.4084
	step [171/191], loss=21.5312
	step [172/191], loss=22.6868
	step [173/191], loss=21.6455
	step [174/191], loss=20.5028
	step [175/191], loss=21.3055
	step [176/191], loss=21.1191
	step [177/191], loss=20.8008
	step [178/191], loss=22.9826
	step [179/191], loss=21.2211
	step [180/191], loss=20.5975
	step [181/191], loss=20.3727
	step [182/191], loss=20.0342
	step [183/191], loss=22.2860
	step [184/191], loss=20.4028
	step [185/191], loss=22.5986
	step [186/191], loss=21.1611
	step [187/191], loss=20.4779
	step [188/191], loss=20.7112
	step [189/191], loss=21.2024
	step [190/191], loss=22.3899
	step [191/191], loss=10.0784
	Evaluating
	loss=0.0943, precision=0.2081, recall=0.9936, f1=0.3441
Training epoch 8
	step [1/191], loss=20.2970
	step [2/191], loss=20.7144
	step [3/191], loss=22.4612
	step [4/191], loss=22.2341
	step [5/191], loss=21.3852
	step [6/191], loss=20.8161
	step [7/191], loss=21.2732
	step [8/191], loss=21.1137
	step [9/191], loss=19.7762
	step [10/191], loss=21.0175
	step [11/191], loss=20.5436
	step [12/191], loss=21.2256
	step [13/191], loss=20.8760
	step [14/191], loss=22.2836
	step [15/191], loss=20.7208
	step [16/191], loss=20.6059
	step [17/191], loss=20.2372
	step [18/191], loss=21.0714
	step [19/191], loss=20.8333
	step [20/191], loss=21.0506
	step [21/191], loss=22.3899
	step [22/191], loss=20.9225
	step [23/191], loss=22.3831
	step [24/191], loss=20.4094
	step [25/191], loss=21.5705
	step [26/191], loss=20.4683
	step [27/191], loss=20.9131
	step [28/191], loss=19.6651
	step [29/191], loss=20.6089
	step [30/191], loss=18.5603
	step [31/191], loss=22.7686
	step [32/191], loss=21.1893
	step [33/191], loss=21.2418
	step [34/191], loss=20.2044
	step [35/191], loss=20.7992
	step [36/191], loss=20.1437
	step [37/191], loss=21.6084
	step [38/191], loss=20.2947
	step [39/191], loss=19.5377
	step [40/191], loss=21.7996
	step [41/191], loss=19.6111
	step [42/191], loss=19.9512
	step [43/191], loss=20.4849
	step [44/191], loss=21.4873
	step [45/191], loss=21.5761
	step [46/191], loss=18.3845
	step [47/191], loss=19.8722
	step [48/191], loss=20.4453
	step [49/191], loss=21.5490
	step [50/191], loss=20.9383
	step [51/191], loss=21.5298
	step [52/191], loss=19.4665
	step [53/191], loss=23.3756
	step [54/191], loss=20.4492
	step [55/191], loss=21.2000
	step [56/191], loss=20.7287
	step [57/191], loss=20.2362
	step [58/191], loss=20.7985
	step [59/191], loss=21.3961
	step [60/191], loss=20.0513
	step [61/191], loss=20.1096
	step [62/191], loss=21.4362
	step [63/191], loss=19.6762
	step [64/191], loss=19.4577
	step [65/191], loss=18.9896
	step [66/191], loss=20.5319
	step [67/191], loss=19.8403
	step [68/191], loss=21.8231
	step [69/191], loss=20.8279
	step [70/191], loss=19.0754
	step [71/191], loss=20.3564
	step [72/191], loss=21.1358
	step [73/191], loss=19.5859
	step [74/191], loss=20.5811
	step [75/191], loss=20.1997
	step [76/191], loss=19.6496
	step [77/191], loss=19.3800
	step [78/191], loss=20.0460
	step [79/191], loss=20.5849
	step [80/191], loss=20.4933
	step [81/191], loss=20.1952
	step [82/191], loss=20.0132
	step [83/191], loss=20.4871
	step [84/191], loss=19.8151
	step [85/191], loss=19.6772
	step [86/191], loss=19.2844
	step [87/191], loss=21.4097
	step [88/191], loss=19.2068
	step [89/191], loss=20.9480
	step [90/191], loss=20.0827
	step [91/191], loss=18.7140
	step [92/191], loss=19.8869
	step [93/191], loss=19.4465
	step [94/191], loss=18.8008
	step [95/191], loss=21.4958
	step [96/191], loss=19.5677
	step [97/191], loss=19.0935
	step [98/191], loss=20.3964
	step [99/191], loss=19.3671
	step [100/191], loss=20.0758
	step [101/191], loss=19.7999
	step [102/191], loss=19.6064
	step [103/191], loss=19.5735
	step [104/191], loss=20.4934
	step [105/191], loss=20.7527
	step [106/191], loss=18.9932
	step [107/191], loss=21.3817
	step [108/191], loss=18.0136
	step [109/191], loss=19.6438
	step [110/191], loss=20.5967
	step [111/191], loss=19.3605
	step [112/191], loss=18.4397
	step [113/191], loss=19.2219
	step [114/191], loss=20.0377
	step [115/191], loss=18.8994
	step [116/191], loss=19.9275
	step [117/191], loss=18.7552
	step [118/191], loss=18.4796
	step [119/191], loss=18.0557
	step [120/191], loss=19.6715
	step [121/191], loss=19.3894
	step [122/191], loss=19.6537
	step [123/191], loss=19.2266
	step [124/191], loss=19.3026
	step [125/191], loss=20.3243
	step [126/191], loss=19.5914
	step [127/191], loss=19.0266
	step [128/191], loss=19.3520
	step [129/191], loss=19.7103
	step [130/191], loss=19.2420
	step [131/191], loss=18.4579
	step [132/191], loss=19.3023
	step [133/191], loss=19.4305
	step [134/191], loss=18.6277
	step [135/191], loss=18.3942
	step [136/191], loss=20.3881
	step [137/191], loss=18.4757
	step [138/191], loss=18.5662
	step [139/191], loss=19.0464
	step [140/191], loss=19.6767
	step [141/191], loss=21.2630
	step [142/191], loss=18.1916
	step [143/191], loss=17.5975
	step [144/191], loss=18.5196
	step [145/191], loss=20.1570
	step [146/191], loss=18.8996
	step [147/191], loss=20.9292
	step [148/191], loss=21.0375
	step [149/191], loss=19.2741
	step [150/191], loss=18.5462
	step [151/191], loss=18.8124
	step [152/191], loss=19.8273
	step [153/191], loss=19.0021
	step [154/191], loss=19.4742
	step [155/191], loss=18.6064
	step [156/191], loss=17.6491
	step [157/191], loss=19.1572
	step [158/191], loss=19.0819
	step [159/191], loss=18.1325
	step [160/191], loss=19.1109
	step [161/191], loss=17.7831
	step [162/191], loss=18.2642
	step [163/191], loss=19.9174
	step [164/191], loss=18.5884
	step [165/191], loss=19.1140
	step [166/191], loss=18.2298
	step [167/191], loss=19.4319
	step [168/191], loss=20.0084
	step [169/191], loss=18.2719
	step [170/191], loss=19.3467
	step [171/191], loss=18.6594
	step [172/191], loss=18.0552
	step [173/191], loss=18.9103
	step [174/191], loss=19.5578
	step [175/191], loss=19.1736
	step [176/191], loss=18.7441
	step [177/191], loss=19.4266
	step [178/191], loss=18.7179
	step [179/191], loss=19.8341
	step [180/191], loss=19.6580
	step [181/191], loss=18.0635
	step [182/191], loss=19.7410
	step [183/191], loss=19.4998
	step [184/191], loss=17.9307
	step [185/191], loss=19.7476
	step [186/191], loss=17.5697
	step [187/191], loss=17.4597
	step [188/191], loss=18.5578
	step [189/191], loss=18.8279
	step [190/191], loss=18.8400
	step [191/191], loss=8.3835
	Evaluating
	loss=0.0792, precision=0.2313, recall=0.9937, f1=0.3753
Training epoch 9
	step [1/191], loss=18.1541
	step [2/191], loss=19.3247
	step [3/191], loss=18.6901
	step [4/191], loss=18.3000
	step [5/191], loss=18.4134
	step [6/191], loss=18.1985
	step [7/191], loss=18.0936
	step [8/191], loss=19.4289
	step [9/191], loss=19.9840
	step [10/191], loss=18.2562
	step [11/191], loss=18.8006
	step [12/191], loss=16.5529
	step [13/191], loss=18.1505
	step [14/191], loss=16.5046
	step [15/191], loss=18.4672
	step [16/191], loss=19.4560
	step [17/191], loss=17.9142
	step [18/191], loss=20.6063
	step [19/191], loss=18.0693
	step [20/191], loss=18.2836
	step [21/191], loss=19.1065
	step [22/191], loss=19.3604
	step [23/191], loss=18.8417
	step [24/191], loss=17.0443
	step [25/191], loss=18.7701
	step [26/191], loss=17.4256
	step [27/191], loss=17.7097
	step [28/191], loss=17.5173
	step [29/191], loss=18.8641
	step [30/191], loss=17.9887
	step [31/191], loss=16.6161
	step [32/191], loss=19.6839
	step [33/191], loss=18.3421
	step [34/191], loss=19.9413
	step [35/191], loss=17.0554
	step [36/191], loss=17.4079
	step [37/191], loss=18.8531
	step [38/191], loss=17.9890
	step [39/191], loss=18.8105
	step [40/191], loss=17.3891
	step [41/191], loss=17.3024
	step [42/191], loss=17.5893
	step [43/191], loss=18.3889
	step [44/191], loss=19.0957
	step [45/191], loss=19.5475
	step [46/191], loss=17.9902
	step [47/191], loss=19.5458
	step [48/191], loss=16.9551
	step [49/191], loss=15.9623
	step [50/191], loss=16.9872
	step [51/191], loss=17.6661
	step [52/191], loss=18.9106
	step [53/191], loss=18.0495
	step [54/191], loss=18.7105
	step [55/191], loss=17.7939
	step [56/191], loss=17.4303
	step [57/191], loss=18.7445
	step [58/191], loss=18.1225
	step [59/191], loss=17.8731
	step [60/191], loss=17.0596
	step [61/191], loss=18.0708
	step [62/191], loss=17.2800
	step [63/191], loss=16.6497
	step [64/191], loss=17.3638
	step [65/191], loss=18.7354
	step [66/191], loss=18.1383
	step [67/191], loss=16.2880
	step [68/191], loss=18.1728
	step [69/191], loss=18.8701
	step [70/191], loss=17.8841
	step [71/191], loss=16.8460
	step [72/191], loss=17.2958
	step [73/191], loss=17.2409
	step [74/191], loss=18.6287
	step [75/191], loss=19.4621
	step [76/191], loss=17.9984
	step [77/191], loss=17.6531
	step [78/191], loss=18.1880
	step [79/191], loss=16.5782
	step [80/191], loss=17.7774
	step [81/191], loss=18.1356
	step [82/191], loss=16.7904
	step [83/191], loss=17.6827
	step [84/191], loss=17.2194
	step [85/191], loss=19.9887
	step [86/191], loss=18.1673
	step [87/191], loss=18.2794
	step [88/191], loss=18.2364
	step [89/191], loss=17.9055
	step [90/191], loss=16.5960
	step [91/191], loss=16.6969
	step [92/191], loss=18.3414
	step [93/191], loss=16.9170
	step [94/191], loss=18.1570
	step [95/191], loss=18.3171
	step [96/191], loss=16.6194
	step [97/191], loss=17.2922
	step [98/191], loss=18.4316
	step [99/191], loss=17.3054
	step [100/191], loss=17.6885
	step [101/191], loss=19.3910
	step [102/191], loss=17.4007
	step [103/191], loss=16.2103
	step [104/191], loss=17.8841
	step [105/191], loss=16.7972
	step [106/191], loss=19.0933
	step [107/191], loss=16.6307
	step [108/191], loss=17.2470
	step [109/191], loss=17.1489
	step [110/191], loss=16.9254
	step [111/191], loss=16.8862
	step [112/191], loss=16.5044
	step [113/191], loss=18.4668
	step [114/191], loss=18.0049
	step [115/191], loss=15.9940
	step [116/191], loss=17.6569
	step [117/191], loss=16.3228
	step [118/191], loss=17.9508
	step [119/191], loss=16.2222
	step [120/191], loss=16.9503
	step [121/191], loss=18.3277
	step [122/191], loss=16.9456
	step [123/191], loss=16.0183
	step [124/191], loss=18.4074
	step [125/191], loss=18.6390
	step [126/191], loss=18.0586
	step [127/191], loss=16.2923
	step [128/191], loss=16.0001
	step [129/191], loss=17.9049
	step [130/191], loss=17.6609
	step [131/191], loss=14.6552
	step [132/191], loss=17.9773
	step [133/191], loss=17.7266
	step [134/191], loss=17.1447
	step [135/191], loss=15.6748
	step [136/191], loss=15.6421
	step [137/191], loss=16.1177
	step [138/191], loss=15.4358
	step [139/191], loss=15.3854
	step [140/191], loss=16.5155
	step [141/191], loss=17.5806
	step [142/191], loss=17.4559
	step [143/191], loss=16.2638
	step [144/191], loss=16.9728
	step [145/191], loss=16.8789
	step [146/191], loss=16.1578
	step [147/191], loss=17.1587
	step [148/191], loss=17.9866
	step [149/191], loss=16.6349
	step [150/191], loss=17.0446
	step [151/191], loss=16.4960
	step [152/191], loss=17.4417
	step [153/191], loss=15.9529
	step [154/191], loss=17.2343
	step [155/191], loss=16.3749
	step [156/191], loss=16.2277
	step [157/191], loss=17.5006
	step [158/191], loss=16.0169
	step [159/191], loss=17.2097
	step [160/191], loss=17.5228
	step [161/191], loss=18.1067
	step [162/191], loss=15.7418
	step [163/191], loss=17.0179
	step [164/191], loss=17.0980
	step [165/191], loss=14.6313
	step [166/191], loss=15.8824
	step [167/191], loss=17.0165
	step [168/191], loss=16.5118
	step [169/191], loss=16.9288
	step [170/191], loss=17.0963
	step [171/191], loss=16.2637
	step [172/191], loss=16.2920
	step [173/191], loss=16.8131
	step [174/191], loss=16.7082
	step [175/191], loss=18.1575
	step [176/191], loss=16.0751
	step [177/191], loss=16.6785
	step [178/191], loss=16.5044
	step [179/191], loss=18.7309
	step [180/191], loss=16.7706
	step [181/191], loss=17.0707
	step [182/191], loss=15.8975
	step [183/191], loss=16.6403
	step [184/191], loss=17.1265
	step [185/191], loss=15.5979
	step [186/191], loss=16.4103
	step [187/191], loss=15.8545
	step [188/191], loss=15.9443
	step [189/191], loss=17.2767
	step [190/191], loss=16.8897
	step [191/191], loss=7.0413
	Evaluating
	loss=0.0693, precision=0.2251, recall=0.9943, f1=0.3670
Training epoch 10
	step [1/191], loss=17.0849
	step [2/191], loss=14.8703
	step [3/191], loss=15.3833
	step [4/191], loss=17.4061
	step [5/191], loss=15.9973
	step [6/191], loss=18.6928
	step [7/191], loss=17.3505
	step [8/191], loss=17.1820
	step [9/191], loss=15.9698
	step [10/191], loss=15.2419
	step [11/191], loss=16.5298
	step [12/191], loss=17.3397
	step [13/191], loss=16.5529
	step [14/191], loss=17.5319
	step [15/191], loss=14.9363
	step [16/191], loss=16.1528
	step [17/191], loss=16.6512
	step [18/191], loss=16.1856
	step [19/191], loss=16.6051
	step [20/191], loss=17.2827
	step [21/191], loss=17.2542
	step [22/191], loss=16.7515
	step [23/191], loss=14.7028
	step [24/191], loss=16.2004
	step [25/191], loss=15.9950
	step [26/191], loss=15.3082
	step [27/191], loss=16.6477
	step [28/191], loss=15.7975
	step [29/191], loss=15.4264
	step [30/191], loss=17.7809
	step [31/191], loss=16.1450
	step [32/191], loss=16.5052
	step [33/191], loss=16.6172
	step [34/191], loss=16.3408
	step [35/191], loss=16.8750
	step [36/191], loss=15.4922
	step [37/191], loss=15.1049
	step [38/191], loss=16.7389
	step [39/191], loss=16.3589
	step [40/191], loss=15.4560
	step [41/191], loss=14.7426
	step [42/191], loss=16.8170
	step [43/191], loss=14.8373
	step [44/191], loss=15.4046
	step [45/191], loss=16.0063
	step [46/191], loss=16.5280
	step [47/191], loss=15.6684
	step [48/191], loss=17.5153
	step [49/191], loss=15.1067
	step [50/191], loss=14.3003
	step [51/191], loss=14.6475
	step [52/191], loss=14.5406
	step [53/191], loss=15.9346
	step [54/191], loss=14.7358
	step [55/191], loss=15.5196
	step [56/191], loss=15.7113
	step [57/191], loss=16.3765
	step [58/191], loss=16.4794
	step [59/191], loss=16.5448
	step [60/191], loss=16.2091
	step [61/191], loss=16.8488
	step [62/191], loss=15.7887
	step [63/191], loss=14.8572
	step [64/191], loss=16.2190
	step [65/191], loss=14.6281
	step [66/191], loss=15.2272
	step [67/191], loss=14.7107
	step [68/191], loss=16.6467
	step [69/191], loss=17.6543
	step [70/191], loss=16.2395
	step [71/191], loss=15.3669
	step [72/191], loss=15.4326
	step [73/191], loss=16.2718
	step [74/191], loss=15.1575
	step [75/191], loss=17.3583
	step [76/191], loss=16.1044
	step [77/191], loss=14.8283
	step [78/191], loss=16.7441
	step [79/191], loss=16.8732
	step [80/191], loss=15.9660
	step [81/191], loss=15.0993
	step [82/191], loss=16.1789
	step [83/191], loss=15.9758
	step [84/191], loss=16.4306
	step [85/191], loss=15.0836
	step [86/191], loss=15.9051
	step [87/191], loss=16.5348
	step [88/191], loss=16.4347
	step [89/191], loss=16.4006
	step [90/191], loss=14.6471
	step [91/191], loss=16.2478
	step [92/191], loss=15.2281
	step [93/191], loss=15.7161
	step [94/191], loss=16.0391
	step [95/191], loss=14.3378
	step [96/191], loss=15.6756
	step [97/191], loss=14.9010
	step [98/191], loss=15.4083
	step [99/191], loss=15.6496
	step [100/191], loss=15.0396
	step [101/191], loss=15.8971
	step [102/191], loss=14.8150
	step [103/191], loss=15.3423
	step [104/191], loss=14.4181
	step [105/191], loss=14.8982
	step [106/191], loss=16.5448
	step [107/191], loss=16.0418
	step [108/191], loss=16.7305
	step [109/191], loss=15.8685
	step [110/191], loss=16.7291
	step [111/191], loss=16.0281
	step [112/191], loss=15.8942
	step [113/191], loss=14.8082
	step [114/191], loss=14.5937
	step [115/191], loss=15.1030
	step [116/191], loss=14.8539
	step [117/191], loss=14.9754
	step [118/191], loss=15.3592
	step [119/191], loss=15.3514
	step [120/191], loss=16.8312
	step [121/191], loss=15.4193
	step [122/191], loss=14.4598
	step [123/191], loss=14.7416
	step [124/191], loss=16.3691
	step [125/191], loss=15.2274
	step [126/191], loss=15.5888
	step [127/191], loss=14.3117
	step [128/191], loss=14.5139
	step [129/191], loss=15.4565
	step [130/191], loss=14.8942
	step [131/191], loss=16.1977
	step [132/191], loss=15.1359
	step [133/191], loss=15.8726
	step [134/191], loss=16.0963
	step [135/191], loss=14.9135
	step [136/191], loss=16.8045
	step [137/191], loss=14.4121
	step [138/191], loss=14.3048
	step [139/191], loss=15.1597
	step [140/191], loss=17.3740
	step [141/191], loss=14.8756
	step [142/191], loss=15.8392
	step [143/191], loss=15.5045
	step [144/191], loss=14.7828
	step [145/191], loss=14.4873
	step [146/191], loss=16.0594
	step [147/191], loss=16.1822
	step [148/191], loss=14.8971
	step [149/191], loss=14.5863
	step [150/191], loss=15.7277
	step [151/191], loss=14.7446
	step [152/191], loss=15.1311
	step [153/191], loss=15.5442
	step [154/191], loss=15.6668
	step [155/191], loss=14.3114
	step [156/191], loss=14.5231
	step [157/191], loss=15.0919
	step [158/191], loss=15.5636
	step [159/191], loss=15.5893
	step [160/191], loss=15.0895
	step [161/191], loss=13.4748
	step [162/191], loss=15.1735
	step [163/191], loss=15.6522
	step [164/191], loss=15.1826
	step [165/191], loss=16.0422
	step [166/191], loss=16.3419
	step [167/191], loss=14.8209
	step [168/191], loss=16.0994
	step [169/191], loss=13.6573
	step [170/191], loss=15.5930
	step [171/191], loss=13.7579
	step [172/191], loss=15.9620
	step [173/191], loss=14.4120
	step [174/191], loss=13.5574
	step [175/191], loss=15.1081
	step [176/191], loss=15.2287
	step [177/191], loss=15.2847
	step [178/191], loss=14.3086
	step [179/191], loss=15.8097
	step [180/191], loss=14.5457
	step [181/191], loss=14.1858
	step [182/191], loss=15.1544
	step [183/191], loss=15.3739
	step [184/191], loss=15.5769
	step [185/191], loss=14.1578
	step [186/191], loss=15.8303
	step [187/191], loss=16.6159
	step [188/191], loss=15.6546
	step [189/191], loss=13.8376
	step [190/191], loss=14.3517
	step [191/191], loss=7.1976
	Evaluating
	loss=0.0595, precision=0.2262, recall=0.9930, f1=0.3685
Training epoch 11
	step [1/191], loss=14.8736
	step [2/191], loss=13.8783
	step [3/191], loss=15.0295
	step [4/191], loss=13.7787
	step [5/191], loss=15.6239
	step [6/191], loss=16.4969
	step [7/191], loss=13.8715
	step [8/191], loss=14.1092
	step [9/191], loss=14.6088
	step [10/191], loss=14.9626
	step [11/191], loss=14.4417
	step [12/191], loss=15.3046
	step [13/191], loss=14.9134
	step [14/191], loss=14.7317
	step [15/191], loss=13.6333
	step [16/191], loss=14.8231
	step [17/191], loss=15.5100
	step [18/191], loss=13.8173
	step [19/191], loss=15.1035
	step [20/191], loss=14.8784
	step [21/191], loss=14.7299
	step [22/191], loss=15.8261
	step [23/191], loss=14.6260
	step [24/191], loss=14.4072
	step [25/191], loss=14.4666
	step [26/191], loss=13.2436
	step [27/191], loss=13.4426
	step [28/191], loss=14.8091
	step [29/191], loss=14.3625
	step [30/191], loss=14.9136
	step [31/191], loss=13.7056
	step [32/191], loss=14.8353
	step [33/191], loss=13.6822
	step [34/191], loss=13.0614
	step [35/191], loss=15.4573
	step [36/191], loss=14.5731
	step [37/191], loss=13.6075
	step [38/191], loss=13.6979
	step [39/191], loss=14.6633
	step [40/191], loss=14.1747
	step [41/191], loss=12.8223
	step [42/191], loss=14.9965
	step [43/191], loss=16.2989
	step [44/191], loss=13.3554
	step [45/191], loss=14.1194
	step [46/191], loss=13.6975
	step [47/191], loss=13.8684
	step [48/191], loss=13.9804
	step [49/191], loss=14.0491
	step [50/191], loss=14.9373
	step [51/191], loss=13.1876
	step [52/191], loss=13.5055
	step [53/191], loss=14.5673
	step [54/191], loss=13.2538
	step [55/191], loss=15.0886
	step [56/191], loss=15.8392
	step [57/191], loss=14.1588
	step [58/191], loss=13.2681
	step [59/191], loss=14.2218
	step [60/191], loss=14.0290
	step [61/191], loss=15.9194
	step [62/191], loss=15.8605
	step [63/191], loss=13.7760
	step [64/191], loss=13.2525
	step [65/191], loss=17.1321
	step [66/191], loss=14.1953
	step [67/191], loss=14.1970
	step [68/191], loss=14.9480
	step [69/191], loss=14.2508
	step [70/191], loss=16.0944
	step [71/191], loss=15.2301
	step [72/191], loss=14.0801
	step [73/191], loss=13.2525
	step [74/191], loss=13.8885
	step [75/191], loss=15.2671
	step [76/191], loss=14.6041
	step [77/191], loss=14.2832
	step [78/191], loss=13.4719
	step [79/191], loss=15.8065
	step [80/191], loss=15.7071
	step [81/191], loss=13.1036
	step [82/191], loss=13.6745
	step [83/191], loss=15.6211
	step [84/191], loss=15.1169
	step [85/191], loss=13.4595
	step [86/191], loss=15.7812
	step [87/191], loss=14.0791
	step [88/191], loss=13.1989
	step [89/191], loss=18.0805
	step [90/191], loss=14.3208
	step [91/191], loss=13.6182
	step [92/191], loss=14.5831
	step [93/191], loss=14.0041
	step [94/191], loss=14.3927
	step [95/191], loss=14.6094
	step [96/191], loss=16.9200
	step [97/191], loss=14.5322
	step [98/191], loss=14.5979
	step [99/191], loss=14.0016
	step [100/191], loss=13.5361
	step [101/191], loss=13.4179
	step [102/191], loss=14.4674
	step [103/191], loss=14.1489
	step [104/191], loss=14.4450
	step [105/191], loss=12.4436
	step [106/191], loss=13.9650
	step [107/191], loss=14.0139
	step [108/191], loss=14.5545
	step [109/191], loss=14.1448
	step [110/191], loss=13.0847
	step [111/191], loss=13.4049
	step [112/191], loss=13.8609
	step [113/191], loss=12.7050
	step [114/191], loss=13.2969
	step [115/191], loss=14.2595
	step [116/191], loss=13.8695
	step [117/191], loss=15.0327
	step [118/191], loss=14.2340
	step [119/191], loss=12.6745
	step [120/191], loss=13.6770
	step [121/191], loss=14.4283
	step [122/191], loss=13.5680
	step [123/191], loss=13.9561
	step [124/191], loss=13.8657
	step [125/191], loss=13.6026
	step [126/191], loss=13.1189
	step [127/191], loss=13.2086
	step [128/191], loss=12.9189
	step [129/191], loss=12.5802
	step [130/191], loss=14.6174
	step [131/191], loss=13.0386
	step [132/191], loss=12.6164
	step [133/191], loss=12.7381
	step [134/191], loss=12.5861
	step [135/191], loss=14.3954
	step [136/191], loss=13.2328
	step [137/191], loss=14.1479
	step [138/191], loss=15.1834
	step [139/191], loss=13.8077
	step [140/191], loss=12.7930
	step [141/191], loss=14.7665
	step [142/191], loss=15.1670
	step [143/191], loss=12.3193
	step [144/191], loss=12.7133
	step [145/191], loss=12.7896
	step [146/191], loss=12.7684
	step [147/191], loss=13.3495
	step [148/191], loss=14.1979
	step [149/191], loss=13.0550
	step [150/191], loss=13.4102
	step [151/191], loss=14.5331
	step [152/191], loss=12.5197
	step [153/191], loss=13.9207
	step [154/191], loss=14.3345
	step [155/191], loss=13.0482
	step [156/191], loss=13.4677
	step [157/191], loss=14.2002
	step [158/191], loss=12.6946
	step [159/191], loss=13.4087
	step [160/191], loss=15.2770
	step [161/191], loss=14.2539
	step [162/191], loss=14.6900
	step [163/191], loss=13.5840
	step [164/191], loss=13.1311
	step [165/191], loss=12.8514
	step [166/191], loss=13.3316
	step [167/191], loss=12.9367
	step [168/191], loss=15.1269
	step [169/191], loss=13.4690
	step [170/191], loss=14.1359
	step [171/191], loss=14.4768
	step [172/191], loss=13.1882
	step [173/191], loss=14.8820
	step [174/191], loss=14.7160
	step [175/191], loss=13.8508
	step [176/191], loss=13.9470
	step [177/191], loss=12.9252
	step [178/191], loss=13.7703
	step [179/191], loss=13.1531
	step [180/191], loss=14.1928
	step [181/191], loss=13.9757
	step [182/191], loss=13.0495
	step [183/191], loss=13.4214
	step [184/191], loss=13.9177
	step [185/191], loss=14.2859
	step [186/191], loss=13.5618
	step [187/191], loss=13.8814
	step [188/191], loss=13.1606
	step [189/191], loss=13.3790
	step [190/191], loss=14.9523
	step [191/191], loss=6.2986
	Evaluating
	loss=0.0562, precision=0.2194, recall=0.9925, f1=0.3593
Training epoch 12
	step [1/191], loss=14.3934
	step [2/191], loss=13.3923
	step [3/191], loss=14.6914
	step [4/191], loss=13.6475
	step [5/191], loss=14.8622
	step [6/191], loss=13.2484
	step [7/191], loss=13.9012
	step [8/191], loss=13.8829
	step [9/191], loss=13.1919
	step [10/191], loss=14.2447
	step [11/191], loss=13.9897
	step [12/191], loss=13.2051
	step [13/191], loss=13.4236
	step [14/191], loss=14.2434
	step [15/191], loss=13.5472
	step [16/191], loss=13.9645
	step [17/191], loss=13.9685
	step [18/191], loss=14.5754
	step [19/191], loss=12.9980
	step [20/191], loss=12.9988
	step [21/191], loss=13.4255
	step [22/191], loss=13.0210
	step [23/191], loss=13.4087
	step [24/191], loss=13.3026
	step [25/191], loss=14.3004
	step [26/191], loss=13.6666
	step [27/191], loss=13.2664
	step [28/191], loss=13.7384
	step [29/191], loss=11.5802
	step [30/191], loss=12.6935
	step [31/191], loss=14.9302
	step [32/191], loss=11.7028
	step [33/191], loss=13.4290
	step [34/191], loss=13.2850
	step [35/191], loss=13.5882
	step [36/191], loss=12.3589
	step [37/191], loss=11.4237
	step [38/191], loss=12.5303
	step [39/191], loss=12.2958
	step [40/191], loss=13.2417
	step [41/191], loss=12.6885
	step [42/191], loss=11.6853
	step [43/191], loss=12.5154
	step [44/191], loss=13.5947
	step [45/191], loss=12.8614
	step [46/191], loss=13.9362
	step [47/191], loss=13.6934
	step [48/191], loss=12.4031
	step [49/191], loss=13.9815
	step [50/191], loss=12.9949
	step [51/191], loss=12.9261
	step [52/191], loss=12.3465
	step [53/191], loss=13.1449
	step [54/191], loss=14.0112
	step [55/191], loss=13.1977
	step [56/191], loss=13.0050
	step [57/191], loss=14.0364
	step [58/191], loss=13.6441
	step [59/191], loss=14.6445
	step [60/191], loss=12.5297
	step [61/191], loss=13.6785
	step [62/191], loss=13.4337
	step [63/191], loss=13.6840
	step [64/191], loss=13.6334
	step [65/191], loss=13.1728
	step [66/191], loss=13.7950
	step [67/191], loss=13.8863
	step [68/191], loss=12.7084
	step [69/191], loss=15.9390
	step [70/191], loss=14.6459
	step [71/191], loss=13.5714
	step [72/191], loss=12.7556
	step [73/191], loss=12.0284
	step [74/191], loss=13.2747
	step [75/191], loss=12.1234
	step [76/191], loss=13.3387
	step [77/191], loss=11.5396
	step [78/191], loss=14.0548
	step [79/191], loss=13.0881
	step [80/191], loss=11.7725
	step [81/191], loss=12.9094
	step [82/191], loss=14.5488
	step [83/191], loss=13.4594
	step [84/191], loss=13.2391
	step [85/191], loss=14.0839
	step [86/191], loss=13.3642
	step [87/191], loss=13.3263
	step [88/191], loss=12.5906
	step [89/191], loss=12.5257
	step [90/191], loss=12.7829
	step [91/191], loss=13.1746
	step [92/191], loss=13.1684
	step [93/191], loss=12.6276
	step [94/191], loss=12.8287
	step [95/191], loss=11.6300
	step [96/191], loss=14.4798
	step [97/191], loss=13.2049
	step [98/191], loss=12.3205
	step [99/191], loss=12.5151
	step [100/191], loss=13.8371
	step [101/191], loss=11.7934
	step [102/191], loss=13.3370
	step [103/191], loss=11.7585
	step [104/191], loss=12.5837
	step [105/191], loss=12.9750
	step [106/191], loss=13.3480
	step [107/191], loss=12.6141
	step [108/191], loss=15.3633
	step [109/191], loss=12.6141
	step [110/191], loss=12.5164
	step [111/191], loss=12.4810
	step [112/191], loss=12.7925
	step [113/191], loss=12.2870
	step [114/191], loss=12.8332
	step [115/191], loss=12.8827
	step [116/191], loss=11.3548
	step [117/191], loss=14.6910
	step [118/191], loss=11.9620
	step [119/191], loss=13.0902
	step [120/191], loss=11.6173
	step [121/191], loss=12.7176
	step [122/191], loss=12.0415
	step [123/191], loss=14.5503
	step [124/191], loss=13.1962
	step [125/191], loss=12.5486
	step [126/191], loss=12.6940
	step [127/191], loss=13.4735
	step [128/191], loss=12.6774
	step [129/191], loss=13.4096
	step [130/191], loss=13.9959
	step [131/191], loss=12.6237
	step [132/191], loss=13.2838
	step [133/191], loss=13.0812
	step [134/191], loss=12.6219
	step [135/191], loss=13.1761
	step [136/191], loss=12.7514
	step [137/191], loss=12.7481
	step [138/191], loss=12.5673
	step [139/191], loss=11.5895
	step [140/191], loss=13.6205
	step [141/191], loss=12.5287
	step [142/191], loss=12.0354
	step [143/191], loss=13.2201
	step [144/191], loss=11.5109
	step [145/191], loss=13.5576
	step [146/191], loss=12.8488
	step [147/191], loss=14.6633
	step [148/191], loss=13.6776
	step [149/191], loss=11.8955
	step [150/191], loss=12.6842
	step [151/191], loss=13.4796
	step [152/191], loss=12.0497
	step [153/191], loss=12.8088
	step [154/191], loss=12.0641
	step [155/191], loss=11.9291
	step [156/191], loss=12.0294
	step [157/191], loss=13.0303
	step [158/191], loss=11.9104
	step [159/191], loss=13.5328
	step [160/191], loss=12.1974
	step [161/191], loss=13.1987
	step [162/191], loss=14.4020
	step [163/191], loss=12.5494
	step [164/191], loss=10.9153
	step [165/191], loss=12.4151
	step [166/191], loss=13.1037
	step [167/191], loss=13.0845
	step [168/191], loss=12.1310
	step [169/191], loss=12.8499
	step [170/191], loss=13.5074
	step [171/191], loss=11.8959
	step [172/191], loss=11.4454
	step [173/191], loss=11.4712
	step [174/191], loss=12.1170
	step [175/191], loss=11.0912
	step [176/191], loss=11.3235
	step [177/191], loss=11.4730
	step [178/191], loss=14.3852
	step [179/191], loss=11.8498
	step [180/191], loss=12.0237
	step [181/191], loss=13.1245
	step [182/191], loss=12.0809
	step [183/191], loss=13.6912
	step [184/191], loss=11.8153
	step [185/191], loss=12.2777
	step [186/191], loss=12.4407
	step [187/191], loss=12.7602
	step [188/191], loss=12.8881
	step [189/191], loss=13.1306
	step [190/191], loss=12.4587
	step [191/191], loss=5.3910
	Evaluating
	loss=0.0507, precision=0.2223, recall=0.9940, f1=0.3634
Training epoch 13
	step [1/191], loss=12.7971
	step [2/191], loss=11.6832
	step [3/191], loss=12.6621
	step [4/191], loss=12.7174
	step [5/191], loss=13.0960
	step [6/191], loss=12.3861
	step [7/191], loss=10.9971
	step [8/191], loss=11.8642
	step [9/191], loss=12.2479
	step [10/191], loss=12.9676
	step [11/191], loss=11.6883
	step [12/191], loss=12.9305
	step [13/191], loss=13.9850
	step [14/191], loss=12.1519
	step [15/191], loss=10.5891
	step [16/191], loss=13.3206
	step [17/191], loss=10.4552
	step [18/191], loss=12.8532
	step [19/191], loss=11.2455
	step [20/191], loss=11.9751
	step [21/191], loss=14.0374
	step [22/191], loss=13.4072
	step [23/191], loss=13.4448
	step [24/191], loss=11.4261
	step [25/191], loss=11.5167
	step [26/191], loss=11.4156
	step [27/191], loss=11.0020
	step [28/191], loss=11.8906
	step [29/191], loss=11.9480
	step [30/191], loss=12.3417
	step [31/191], loss=11.7521
	step [32/191], loss=12.5685
	step [33/191], loss=11.9914
	step [34/191], loss=12.3212
	step [35/191], loss=12.2193
	step [36/191], loss=11.4254
	step [37/191], loss=11.3890
	step [38/191], loss=11.2178
	step [39/191], loss=12.9709
	step [40/191], loss=14.2307
	step [41/191], loss=13.2738
	step [42/191], loss=11.6184
	step [43/191], loss=11.6827
	step [44/191], loss=11.4052
	step [45/191], loss=11.4392
	step [46/191], loss=11.1022
	step [47/191], loss=12.4857
	step [48/191], loss=13.2594
	step [49/191], loss=12.1170
	step [50/191], loss=12.3900
	step [51/191], loss=10.7922
	step [52/191], loss=13.1237
	step [53/191], loss=12.9201
	step [54/191], loss=11.8292
	step [55/191], loss=12.8459
	step [56/191], loss=11.1760
	step [57/191], loss=12.1673
	step [58/191], loss=11.5984
	step [59/191], loss=11.4058
	step [60/191], loss=14.8176
	step [61/191], loss=11.6826
	step [62/191], loss=13.2425
	step [63/191], loss=12.1970
	step [64/191], loss=11.5068
	step [65/191], loss=11.6469
	step [66/191], loss=10.7218
	step [67/191], loss=12.2122
	step [68/191], loss=11.0664
	step [69/191], loss=12.1858
	step [70/191], loss=11.3247
	step [71/191], loss=12.4014
	step [72/191], loss=12.3819
	step [73/191], loss=12.4621
	step [74/191], loss=13.5769
	step [75/191], loss=12.1614
	step [76/191], loss=10.7052
	step [77/191], loss=11.1704
	step [78/191], loss=10.9111
	step [79/191], loss=11.2040
	step [80/191], loss=12.5260
	step [81/191], loss=11.8027
	step [82/191], loss=13.0121
	step [83/191], loss=13.3954
	step [84/191], loss=11.9659
	step [85/191], loss=12.0516
	step [86/191], loss=11.4428
	step [87/191], loss=11.0098
	step [88/191], loss=11.8314
	step [89/191], loss=12.1015
	step [90/191], loss=11.4523
	step [91/191], loss=11.9827
	step [92/191], loss=11.5342
	step [93/191], loss=10.7252
	step [94/191], loss=11.5260
	step [95/191], loss=12.4378
	step [96/191], loss=12.8375
	step [97/191], loss=11.1746
	step [98/191], loss=12.0217
	step [99/191], loss=13.0978
	step [100/191], loss=11.4031
	step [101/191], loss=10.7641
	step [102/191], loss=12.1220
	step [103/191], loss=13.1333
	step [104/191], loss=12.7982
	step [105/191], loss=12.5060
	step [106/191], loss=11.4396
	step [107/191], loss=12.0073
	step [108/191], loss=11.4937
	step [109/191], loss=13.4588
	step [110/191], loss=11.9946
	step [111/191], loss=13.3216
	step [112/191], loss=12.0006
	step [113/191], loss=11.8902
	step [114/191], loss=12.1970
	step [115/191], loss=13.6729
	step [116/191], loss=10.5048
	step [117/191], loss=11.2999
	step [118/191], loss=12.6998
	step [119/191], loss=11.4608
	step [120/191], loss=13.2390
	step [121/191], loss=10.9000
	step [122/191], loss=13.2641
	step [123/191], loss=12.8319
	step [124/191], loss=10.5700
	step [125/191], loss=12.1971
	step [126/191], loss=10.9848
	step [127/191], loss=11.1845
	step [128/191], loss=10.7961
	step [129/191], loss=11.1116
	step [130/191], loss=11.5748
	step [131/191], loss=11.8104
	step [132/191], loss=12.1282
	step [133/191], loss=10.0948
	step [134/191], loss=11.9207
	step [135/191], loss=11.4640
	step [136/191], loss=13.5967
	step [137/191], loss=10.6676
	step [138/191], loss=12.1127
	step [139/191], loss=10.3470
	step [140/191], loss=11.0609
	step [141/191], loss=11.0844
	step [142/191], loss=11.9189
	step [143/191], loss=10.6548
	step [144/191], loss=13.0558
	step [145/191], loss=10.8514
	step [146/191], loss=11.4505
	step [147/191], loss=12.3232
	step [148/191], loss=10.8122
	step [149/191], loss=12.0164
	step [150/191], loss=11.8217
	step [151/191], loss=11.7799
	step [152/191], loss=10.9690
	step [153/191], loss=10.9737
	step [154/191], loss=12.7567
	step [155/191], loss=12.0064
	step [156/191], loss=12.1311
	step [157/191], loss=10.5997
	step [158/191], loss=11.2713
	step [159/191], loss=11.9642
	step [160/191], loss=10.9986
	step [161/191], loss=12.5824
	step [162/191], loss=11.2737
	step [163/191], loss=10.8710
	step [164/191], loss=11.6817
	step [165/191], loss=13.5389
	step [166/191], loss=10.9569
	step [167/191], loss=10.8348
	step [168/191], loss=12.2700
	step [169/191], loss=11.4261
	step [170/191], loss=11.9242
	step [171/191], loss=13.1001
	step [172/191], loss=12.3114
	step [173/191], loss=11.5860
	step [174/191], loss=13.1987
	step [175/191], loss=11.5935
	step [176/191], loss=12.3057
	step [177/191], loss=11.2081
	step [178/191], loss=11.8882
	step [179/191], loss=11.3920
	step [180/191], loss=12.3497
	step [181/191], loss=11.4804
	step [182/191], loss=11.0263
	step [183/191], loss=11.6557
	step [184/191], loss=11.5167
	step [185/191], loss=11.2097
	step [186/191], loss=12.9939
	step [187/191], loss=9.4434
	step [188/191], loss=11.5487
	step [189/191], loss=12.4898
	step [190/191], loss=11.1051
	step [191/191], loss=5.0329
	Evaluating
	loss=0.0418, precision=0.2627, recall=0.9910, f1=0.4153
saving model as: 1_saved_model.pth
Training epoch 14
	step [1/191], loss=13.9708
	step [2/191], loss=11.1363
	step [3/191], loss=12.0142
	step [4/191], loss=12.8930
	step [5/191], loss=12.0213
	step [6/191], loss=12.8224
	step [7/191], loss=10.7740
	step [8/191], loss=10.4095
	step [9/191], loss=11.9416
	step [10/191], loss=11.5424
	step [11/191], loss=12.6239
	step [12/191], loss=10.8002
	step [13/191], loss=11.2814
	step [14/191], loss=12.2771
	step [15/191], loss=12.3205
	step [16/191], loss=11.1537
	step [17/191], loss=11.9611
	step [18/191], loss=12.9489
	step [19/191], loss=11.5425
	step [20/191], loss=10.9551
	step [21/191], loss=12.8270
	step [22/191], loss=12.2338
	step [23/191], loss=12.3149
	step [24/191], loss=11.5029
	step [25/191], loss=11.0425
	step [26/191], loss=11.6507
	step [27/191], loss=11.0891
	step [28/191], loss=10.2717
	step [29/191], loss=11.2358
	step [30/191], loss=11.4119
	step [31/191], loss=12.5586
	step [32/191], loss=11.1974
	step [33/191], loss=10.3157
	step [34/191], loss=11.4161
	step [35/191], loss=12.1544
	step [36/191], loss=11.2322
	step [37/191], loss=10.6512
	step [38/191], loss=13.0289
	step [39/191], loss=12.7726
	step [40/191], loss=11.4587
	step [41/191], loss=10.6502
	step [42/191], loss=10.9943
	step [43/191], loss=11.4748
	step [44/191], loss=11.5145
	step [45/191], loss=11.5662
	step [46/191], loss=12.0797
	step [47/191], loss=11.4518
	step [48/191], loss=11.2986
	step [49/191], loss=10.6546
	step [50/191], loss=11.6433
	step [51/191], loss=12.1470
	step [52/191], loss=11.7529
	step [53/191], loss=11.1607
	step [54/191], loss=11.2461
	step [55/191], loss=10.9630
	step [56/191], loss=10.8180
	step [57/191], loss=11.6812
	step [58/191], loss=11.5089
	step [59/191], loss=10.8688
	step [60/191], loss=11.7429
	step [61/191], loss=11.8603
	step [62/191], loss=10.9087
	step [63/191], loss=9.8937
	step [64/191], loss=12.4387
	step [65/191], loss=10.8878
	step [66/191], loss=12.1698
	step [67/191], loss=10.7761
	step [68/191], loss=10.0303
	step [69/191], loss=10.9642
	step [70/191], loss=12.3236
	step [71/191], loss=11.6391
	step [72/191], loss=11.2483
	step [73/191], loss=9.7624
	step [74/191], loss=12.4328
	step [75/191], loss=11.5116
	step [76/191], loss=11.1813
	step [77/191], loss=11.2926
	step [78/191], loss=10.8714
	step [79/191], loss=10.0397
	step [80/191], loss=11.3757
	step [81/191], loss=10.7936
	step [82/191], loss=10.3711
	step [83/191], loss=11.2856
	step [84/191], loss=12.4853
	step [85/191], loss=10.2007
	step [86/191], loss=10.7504
	step [87/191], loss=11.3768
	step [88/191], loss=11.2181
	step [89/191], loss=10.6215
	step [90/191], loss=12.1881
	step [91/191], loss=10.9775
	step [92/191], loss=10.8555
	step [93/191], loss=9.4723
	step [94/191], loss=10.4908
	step [95/191], loss=10.2638
	step [96/191], loss=12.1121
	step [97/191], loss=11.1947
	step [98/191], loss=11.9165
	step [99/191], loss=12.4224
	step [100/191], loss=11.6151
	step [101/191], loss=11.2213
	step [102/191], loss=10.3792
	step [103/191], loss=11.6122
	step [104/191], loss=10.8298
	step [105/191], loss=10.0358
	step [106/191], loss=10.9457
	step [107/191], loss=11.6461
	step [108/191], loss=10.8294
	step [109/191], loss=12.5617
	step [110/191], loss=12.3522
	step [111/191], loss=9.5689
	step [112/191], loss=10.0479
	step [113/191], loss=11.4999
	step [114/191], loss=10.0470
	step [115/191], loss=11.6218
	step [116/191], loss=11.5237
	step [117/191], loss=10.3399
	step [118/191], loss=12.6631
	step [119/191], loss=11.3888
	step [120/191], loss=9.8408
	step [121/191], loss=10.2961
	step [122/191], loss=9.7910
	step [123/191], loss=11.1181
	step [124/191], loss=10.5965
	step [125/191], loss=10.0685
	step [126/191], loss=11.5737
	step [127/191], loss=10.6053
	step [128/191], loss=10.0085
	step [129/191], loss=11.6519
	step [130/191], loss=10.6888
	step [131/191], loss=11.3919
	step [132/191], loss=10.7842
	step [133/191], loss=10.7667
	step [134/191], loss=10.2855
	step [135/191], loss=8.9506
	step [136/191], loss=10.4945
	step [137/191], loss=11.0344
	step [138/191], loss=10.8063
	step [139/191], loss=11.4364
	step [140/191], loss=10.6354
	step [141/191], loss=10.0356
	step [142/191], loss=10.7343
	step [143/191], loss=11.2240
	step [144/191], loss=9.5280
	step [145/191], loss=10.4256
	step [146/191], loss=11.5161
	step [147/191], loss=10.2909
	step [148/191], loss=10.2667
	step [149/191], loss=10.3782
	step [150/191], loss=11.9600
	step [151/191], loss=11.4272
	step [152/191], loss=10.2576
	step [153/191], loss=9.9547
	step [154/191], loss=9.4191
	step [155/191], loss=11.9283
	step [156/191], loss=10.9682
	step [157/191], loss=11.1760
	step [158/191], loss=8.9851
	step [159/191], loss=11.3220
	step [160/191], loss=10.2903
	step [161/191], loss=11.9799
	step [162/191], loss=12.5432
	step [163/191], loss=9.9656
	step [164/191], loss=9.8328
	step [165/191], loss=10.2544
	step [166/191], loss=10.3041
	step [167/191], loss=11.8524
	step [168/191], loss=11.8926
	step [169/191], loss=11.0082
	step [170/191], loss=11.3571
	step [171/191], loss=10.4025
	step [172/191], loss=11.3032
	step [173/191], loss=10.7461
	step [174/191], loss=10.0321
	step [175/191], loss=10.9461
	step [176/191], loss=9.8874
	step [177/191], loss=11.3343
	step [178/191], loss=10.3231
	step [179/191], loss=12.2883
	step [180/191], loss=11.0497
	step [181/191], loss=11.1418
	step [182/191], loss=12.2849
	step [183/191], loss=9.4800
	step [184/191], loss=10.6142
	step [185/191], loss=10.9723
	step [186/191], loss=10.5738
	step [187/191], loss=10.3838
	step [188/191], loss=10.9501
	step [189/191], loss=10.3785
	step [190/191], loss=10.7337
	step [191/191], loss=5.3365
	Evaluating
	loss=0.0447, precision=0.2141, recall=0.9933, f1=0.3523
Training epoch 15
	step [1/191], loss=8.7797
	step [2/191], loss=11.5603
	step [3/191], loss=11.3344
	step [4/191], loss=9.3357
	step [5/191], loss=10.2507
	step [6/191], loss=10.9809
	step [7/191], loss=10.3903
	step [8/191], loss=10.1599
	step [9/191], loss=10.9830
	step [10/191], loss=11.2244
	step [11/191], loss=10.9085
	step [12/191], loss=10.7955
	step [13/191], loss=11.1177
	step [14/191], loss=11.2167
	step [15/191], loss=9.8958
	step [16/191], loss=9.1158
	step [17/191], loss=11.5582
	step [18/191], loss=9.9985
	step [19/191], loss=10.7260
	step [20/191], loss=11.2668
	step [21/191], loss=10.7764
	step [22/191], loss=11.1754
	step [23/191], loss=11.2064
	step [24/191], loss=10.5462
	step [25/191], loss=10.6813
	step [26/191], loss=11.0749
	step [27/191], loss=10.3469
	step [28/191], loss=10.1224
	step [29/191], loss=9.8023
	step [30/191], loss=10.8861
	step [31/191], loss=10.1671
	step [32/191], loss=11.8596
	step [33/191], loss=11.0686
	step [34/191], loss=9.8638
	step [35/191], loss=9.6403
	step [36/191], loss=10.3710
	step [37/191], loss=11.1278
	step [38/191], loss=9.5587
	step [39/191], loss=10.3041
	step [40/191], loss=10.2435
	step [41/191], loss=10.8606
	step [42/191], loss=10.4628
	step [43/191], loss=11.1504
	step [44/191], loss=10.5224
	step [45/191], loss=10.3073
	step [46/191], loss=10.3146
	step [47/191], loss=10.9528
	step [48/191], loss=10.8658
	step [49/191], loss=9.4009
	step [50/191], loss=10.0643
	step [51/191], loss=11.1247
	step [52/191], loss=10.8504
	step [53/191], loss=9.9032
	step [54/191], loss=10.4074
	step [55/191], loss=9.5127
	step [56/191], loss=12.0718
	step [57/191], loss=10.6198
	step [58/191], loss=10.8081
	step [59/191], loss=11.6914
	step [60/191], loss=9.9081
	step [61/191], loss=11.8474
	step [62/191], loss=11.1176
	step [63/191], loss=10.9079
	step [64/191], loss=11.0703
	step [65/191], loss=10.1502
	step [66/191], loss=11.8412
	step [67/191], loss=11.5790
	step [68/191], loss=9.9989
	step [69/191], loss=10.0420
	step [70/191], loss=11.6354
	step [71/191], loss=9.7936
	step [72/191], loss=9.7870
	step [73/191], loss=10.3506
	step [74/191], loss=11.0569
	step [75/191], loss=10.2136
	step [76/191], loss=11.2647
	step [77/191], loss=11.5266
	step [78/191], loss=11.0563
	step [79/191], loss=9.9319
	step [80/191], loss=10.8288
	step [81/191], loss=10.4423
	step [82/191], loss=9.7770
	step [83/191], loss=12.3972
	step [84/191], loss=9.3685
	step [85/191], loss=9.8716
	step [86/191], loss=10.3327
	step [87/191], loss=8.8195
	step [88/191], loss=10.3804
	step [89/191], loss=11.6222
	step [90/191], loss=9.8848
	step [91/191], loss=11.2201
	step [92/191], loss=11.2129
	step [93/191], loss=10.1106
	step [94/191], loss=9.7838
	step [95/191], loss=9.3545
	step [96/191], loss=11.5343
	step [97/191], loss=10.1096
	step [98/191], loss=11.2854
	step [99/191], loss=9.7163
	step [100/191], loss=9.5977
	step [101/191], loss=11.1052
	step [102/191], loss=9.9956
	step [103/191], loss=11.1296
	step [104/191], loss=11.2717
	step [105/191], loss=9.5032
	step [106/191], loss=11.8406
	step [107/191], loss=9.4877
	step [108/191], loss=11.0616
	step [109/191], loss=10.3906
	step [110/191], loss=11.1700
	step [111/191], loss=9.4778
	step [112/191], loss=9.8534
	step [113/191], loss=11.9951
	step [114/191], loss=9.6231
	step [115/191], loss=10.2663
	step [116/191], loss=10.4818
	step [117/191], loss=11.2200
	step [118/191], loss=10.4485
	step [119/191], loss=9.5416
	step [120/191], loss=11.0583
	step [121/191], loss=9.3083
	step [122/191], loss=10.6127
	step [123/191], loss=10.9327
	step [124/191], loss=9.9620
	step [125/191], loss=11.7679
	step [126/191], loss=10.1942
	step [127/191], loss=10.1200
	step [128/191], loss=8.7245
	step [129/191], loss=10.9872
	step [130/191], loss=11.1827
	step [131/191], loss=8.8417
	step [132/191], loss=9.1237
	step [133/191], loss=9.4664
	step [134/191], loss=10.2242
	step [135/191], loss=10.5291
	step [136/191], loss=10.6522
	step [137/191], loss=10.2705
	step [138/191], loss=9.9214
	step [139/191], loss=10.6307
	step [140/191], loss=8.7407
	step [141/191], loss=9.4117
	step [142/191], loss=12.4182
	step [143/191], loss=10.5327
	step [144/191], loss=10.3294
	step [145/191], loss=9.4013
	step [146/191], loss=9.9540
	step [147/191], loss=9.9768
	step [148/191], loss=10.4092
	step [149/191], loss=10.1544
	step [150/191], loss=10.3541
	step [151/191], loss=9.5952
	step [152/191], loss=10.0817
	step [153/191], loss=9.6381
	step [154/191], loss=9.0933
	step [155/191], loss=10.5749
	step [156/191], loss=10.9280
	step [157/191], loss=10.8251
	step [158/191], loss=11.8634
	step [159/191], loss=10.7593
	step [160/191], loss=9.0347
	step [161/191], loss=10.6500
	step [162/191], loss=10.8961
	step [163/191], loss=11.2062
	step [164/191], loss=11.9932
	step [165/191], loss=10.5556
	step [166/191], loss=10.6793
	step [167/191], loss=8.8902
	step [168/191], loss=11.2336
	step [169/191], loss=9.8668
	step [170/191], loss=9.4713
	step [171/191], loss=10.6324
	step [172/191], loss=10.5931
	step [173/191], loss=11.9341
	step [174/191], loss=9.5506
	step [175/191], loss=9.8411
	step [176/191], loss=9.1028
	step [177/191], loss=11.2744
	step [178/191], loss=9.2528
	step [179/191], loss=10.7626
	step [180/191], loss=9.3961
	step [181/191], loss=10.0880
	step [182/191], loss=9.1281
	step [183/191], loss=9.7392
	step [184/191], loss=11.4012
	step [185/191], loss=10.2370
	step [186/191], loss=10.4757
	step [187/191], loss=8.4760
	step [188/191], loss=10.8442
	step [189/191], loss=9.6018
	step [190/191], loss=10.6648
	step [191/191], loss=4.9455
	Evaluating
	loss=0.0383, precision=0.2427, recall=0.9924, f1=0.3901
Training epoch 16
	step [1/191], loss=9.7317
	step [2/191], loss=10.4360
	step [3/191], loss=11.5022
	step [4/191], loss=11.2141
	step [5/191], loss=10.8607
	step [6/191], loss=10.3092
	step [7/191], loss=9.9085
	step [8/191], loss=8.3341
	step [9/191], loss=8.9194
	step [10/191], loss=10.9431
	step [11/191], loss=11.9878
	step [12/191], loss=9.8874
	step [13/191], loss=10.1415
	step [14/191], loss=10.1780
	step [15/191], loss=10.0933
	step [16/191], loss=8.7782
	step [17/191], loss=9.9371
	step [18/191], loss=10.5541
	step [19/191], loss=9.3602
	step [20/191], loss=10.0341
	step [21/191], loss=9.8609
	step [22/191], loss=9.2891
	step [23/191], loss=10.9753
	step [24/191], loss=9.5994
	step [25/191], loss=8.9654
	step [26/191], loss=9.5482
	step [27/191], loss=9.3833
	step [28/191], loss=9.7034
	step [29/191], loss=9.8589
	step [30/191], loss=11.0983
	step [31/191], loss=10.0894
	step [32/191], loss=11.0209
	step [33/191], loss=9.8570
	step [34/191], loss=10.2659
	step [35/191], loss=9.3295
	step [36/191], loss=11.0598
	step [37/191], loss=9.7603
	step [38/191], loss=10.9953
	step [39/191], loss=10.7313
	step [40/191], loss=9.6640
	step [41/191], loss=9.2490
	step [42/191], loss=9.2696
	step [43/191], loss=10.1226
	step [44/191], loss=9.8714
	step [45/191], loss=9.5803
	step [46/191], loss=9.8348
	step [47/191], loss=10.0000
	step [48/191], loss=11.8702
	step [49/191], loss=10.1362
	step [50/191], loss=8.6841
	step [51/191], loss=10.1331
	step [52/191], loss=10.4327
	step [53/191], loss=10.4636
	step [54/191], loss=10.9506
	step [55/191], loss=8.8197
	step [56/191], loss=9.3900
	step [57/191], loss=9.5979
	step [58/191], loss=9.5816
	step [59/191], loss=8.8417
	step [60/191], loss=9.5133
	step [61/191], loss=10.3472
	step [62/191], loss=9.0331
	step [63/191], loss=10.5638
	step [64/191], loss=9.2201
	step [65/191], loss=10.4757
	step [66/191], loss=9.7141
	step [67/191], loss=11.3070
	step [68/191], loss=10.4585
	step [69/191], loss=9.4641
	step [70/191], loss=11.2559
	step [71/191], loss=10.1848
	step [72/191], loss=9.3642
	step [73/191], loss=10.8610
	step [74/191], loss=10.1533
	step [75/191], loss=9.8459
	step [76/191], loss=8.3273
	step [77/191], loss=9.3651
	step [78/191], loss=9.5555
	step [79/191], loss=11.2857
	step [80/191], loss=8.8706
	step [81/191], loss=9.0187
	step [82/191], loss=10.5429
	step [83/191], loss=8.9865
	step [84/191], loss=10.8213
	step [85/191], loss=10.0118
	step [86/191], loss=9.7801
	step [87/191], loss=9.2134
	step [88/191], loss=9.3929
	step [89/191], loss=8.8723
	step [90/191], loss=11.4405
	step [91/191], loss=10.8802
	step [92/191], loss=10.6889
	step [93/191], loss=9.8727
	step [94/191], loss=8.8070
	step [95/191], loss=9.8591
	step [96/191], loss=9.2562
	step [97/191], loss=9.2182
	step [98/191], loss=9.8714
	step [99/191], loss=10.4439
	step [100/191], loss=9.0659
	step [101/191], loss=10.4732
	step [102/191], loss=9.4627
	step [103/191], loss=9.2140
	step [104/191], loss=10.1317
	step [105/191], loss=9.3979
	step [106/191], loss=9.7282
	step [107/191], loss=9.2441
	step [108/191], loss=10.4353
	step [109/191], loss=9.6317
	step [110/191], loss=9.1074
	step [111/191], loss=10.7953
	step [112/191], loss=9.5581
	step [113/191], loss=8.9929
	step [114/191], loss=9.6304
	step [115/191], loss=9.5218
	step [116/191], loss=10.6527
	step [117/191], loss=10.4439
	step [118/191], loss=9.5709
	step [119/191], loss=10.1574
	step [120/191], loss=9.9019
	step [121/191], loss=8.6877
	step [122/191], loss=12.2723
	step [123/191], loss=8.8540
	step [124/191], loss=9.6877
	step [125/191], loss=9.2882
	step [126/191], loss=9.5339
	step [127/191], loss=9.9916
	step [128/191], loss=9.5234
	step [129/191], loss=9.5891
	step [130/191], loss=10.0982
	step [131/191], loss=10.0230
	step [132/191], loss=8.5466
	step [133/191], loss=8.8628
	step [134/191], loss=9.3024
	step [135/191], loss=9.8282
	step [136/191], loss=9.7908
	step [137/191], loss=9.9089
	step [138/191], loss=10.1655
	step [139/191], loss=8.6097
	step [140/191], loss=9.6259
	step [141/191], loss=9.6365
	step [142/191], loss=8.9508
	step [143/191], loss=9.2054
	step [144/191], loss=9.8533
	step [145/191], loss=10.0166
	step [146/191], loss=8.4383
	step [147/191], loss=8.8684
	step [148/191], loss=9.2870
	step [149/191], loss=10.1099
	step [150/191], loss=9.5845
	step [151/191], loss=9.9311
	step [152/191], loss=10.5654
	step [153/191], loss=10.2604
	step [154/191], loss=8.8967
	step [155/191], loss=8.8060
	step [156/191], loss=9.9668
	step [157/191], loss=9.6798
	step [158/191], loss=9.2811
	step [159/191], loss=9.0177
	step [160/191], loss=8.5847
	step [161/191], loss=9.4931
	step [162/191], loss=9.0770
	step [163/191], loss=8.6149
	step [164/191], loss=9.3691
	step [165/191], loss=9.0711
	step [166/191], loss=9.0029
	step [167/191], loss=9.5948
	step [168/191], loss=9.4635
	step [169/191], loss=10.1855
	step [170/191], loss=9.3970
	step [171/191], loss=10.9022
	step [172/191], loss=9.6939
	step [173/191], loss=8.9920
	step [174/191], loss=9.2356
	step [175/191], loss=9.7829
	step [176/191], loss=9.6937
	step [177/191], loss=9.9238
	step [178/191], loss=8.2211
	step [179/191], loss=9.2170
	step [180/191], loss=9.5601
	step [181/191], loss=8.9661
	step [182/191], loss=8.4033
	step [183/191], loss=10.7758
	step [184/191], loss=10.9014
	step [185/191], loss=9.6589
	step [186/191], loss=9.8337
	step [187/191], loss=10.1813
	step [188/191], loss=8.5555
	step [189/191], loss=9.5520
	step [190/191], loss=8.4591
	step [191/191], loss=4.5785
	Evaluating
	loss=0.0332, precision=0.2776, recall=0.9898, f1=0.4336
saving model as: 1_saved_model.pth
Training epoch 17
	step [1/191], loss=8.7631
	step [2/191], loss=9.3505
	step [3/191], loss=9.7167
	step [4/191], loss=10.5020
	step [5/191], loss=9.5660
	step [6/191], loss=9.3766
	step [7/191], loss=9.5508
	step [8/191], loss=9.7468
	step [9/191], loss=11.3378
	step [10/191], loss=9.3748
	step [11/191], loss=9.8032
	step [12/191], loss=10.1047
	step [13/191], loss=9.2607
	step [14/191], loss=9.9179
	step [15/191], loss=10.0509
	step [16/191], loss=8.8639
	step [17/191], loss=9.1059
	step [18/191], loss=9.4320
	step [19/191], loss=8.9258
	step [20/191], loss=9.4885
	step [21/191], loss=9.8288
	step [22/191], loss=8.6663
	step [23/191], loss=9.9673
	step [24/191], loss=9.9618
	step [25/191], loss=8.9573
	step [26/191], loss=8.6122
	step [27/191], loss=9.0809
	step [28/191], loss=8.4873
	step [29/191], loss=11.9827
	step [30/191], loss=8.8000
	step [31/191], loss=9.4346
	step [32/191], loss=9.8358
	step [33/191], loss=10.0395
	step [34/191], loss=8.6729
	step [35/191], loss=10.1118
	step [36/191], loss=9.5856
	step [37/191], loss=10.3457
	step [38/191], loss=9.0209
	step [39/191], loss=8.7567
	step [40/191], loss=8.9063
	step [41/191], loss=9.5357
	step [42/191], loss=10.1436
	step [43/191], loss=8.2548
	step [44/191], loss=8.9668
	step [45/191], loss=8.3220
	step [46/191], loss=8.3878
	step [47/191], loss=9.3194
	step [48/191], loss=9.2839
	step [49/191], loss=9.8507
	step [50/191], loss=9.3750
	step [51/191], loss=9.2562
	step [52/191], loss=8.7694
	step [53/191], loss=10.8416
	step [54/191], loss=10.9108
	step [55/191], loss=11.0228
	step [56/191], loss=9.0121
	step [57/191], loss=8.0136
	step [58/191], loss=8.1761
	step [59/191], loss=9.6698
	step [60/191], loss=9.4969
	step [61/191], loss=9.1119
	step [62/191], loss=8.5100
	step [63/191], loss=8.6202
	step [64/191], loss=8.4621
	step [65/191], loss=8.4312
	step [66/191], loss=7.9343
	step [67/191], loss=8.9989
	step [68/191], loss=8.4278
	step [69/191], loss=9.6763
	step [70/191], loss=9.5478
	step [71/191], loss=9.5069
	step [72/191], loss=10.9159
	step [73/191], loss=10.2645
	step [74/191], loss=9.5205
	step [75/191], loss=9.5096
	step [76/191], loss=9.3361
	step [77/191], loss=9.5446
	step [78/191], loss=7.8837
	step [79/191], loss=10.2182
	step [80/191], loss=8.0169
	step [81/191], loss=8.4402
	step [82/191], loss=9.7468
	step [83/191], loss=9.5667
	step [84/191], loss=9.0626
	step [85/191], loss=10.0245
	step [86/191], loss=8.9743
	step [87/191], loss=9.7827
	step [88/191], loss=8.3163
	step [89/191], loss=10.4097
	step [90/191], loss=9.1950
	step [91/191], loss=9.1553
	step [92/191], loss=8.3024
	step [93/191], loss=10.3311
	step [94/191], loss=9.9161
	step [95/191], loss=8.8673
	step [96/191], loss=10.3246
	step [97/191], loss=9.7691
	step [98/191], loss=10.1878
	step [99/191], loss=9.6968
	step [100/191], loss=8.9179
	step [101/191], loss=8.7567
	step [102/191], loss=8.7251
	step [103/191], loss=8.4094
	step [104/191], loss=9.1265
	step [105/191], loss=8.9916
	step [106/191], loss=8.2231
	step [107/191], loss=7.9374
	step [108/191], loss=9.2214
	step [109/191], loss=8.2277
	step [110/191], loss=9.0844
	step [111/191], loss=8.6998
	step [112/191], loss=9.3723
	step [113/191], loss=7.8726
	step [114/191], loss=9.5067
	step [115/191], loss=8.3021
	step [116/191], loss=8.0365
	step [117/191], loss=9.1058
	step [118/191], loss=8.5001
	step [119/191], loss=9.0401
	step [120/191], loss=9.5629
	step [121/191], loss=9.9360
	step [122/191], loss=7.7813
	step [123/191], loss=7.6773
	step [124/191], loss=8.7684
	step [125/191], loss=9.5669
	step [126/191], loss=11.6268
	step [127/191], loss=10.1054
	step [128/191], loss=8.7801
	step [129/191], loss=9.0770
	step [130/191], loss=8.7877
	step [131/191], loss=9.1746
	step [132/191], loss=12.7912
	step [133/191], loss=9.3186
	step [134/191], loss=8.9909
	step [135/191], loss=8.3005
	step [136/191], loss=9.7965
	step [137/191], loss=8.8904
	step [138/191], loss=8.1114
	step [139/191], loss=9.5229
	step [140/191], loss=9.7281
	step [141/191], loss=9.8218
	step [142/191], loss=9.5220
	step [143/191], loss=8.4201
	step [144/191], loss=9.0606
	step [145/191], loss=9.7387
	step [146/191], loss=10.4692
	step [147/191], loss=9.0741
	step [148/191], loss=9.2003
	step [149/191], loss=8.5565
	step [150/191], loss=8.1567
	step [151/191], loss=7.9275
	step [152/191], loss=9.1006
	step [153/191], loss=9.7859
	step [154/191], loss=9.1514
	step [155/191], loss=9.4747
	step [156/191], loss=9.1216
	step [157/191], loss=7.9790
	step [158/191], loss=9.6335
	step [159/191], loss=9.7517
	step [160/191], loss=9.3616
	step [161/191], loss=8.3882
	step [162/191], loss=8.6730
	step [163/191], loss=8.6004
	step [164/191], loss=8.8928
	step [165/191], loss=9.6865
	step [166/191], loss=10.4513
	step [167/191], loss=8.8689
	step [168/191], loss=8.9209
	step [169/191], loss=8.0997
	step [170/191], loss=8.8110
	step [171/191], loss=8.4452
	step [172/191], loss=8.6492
	step [173/191], loss=9.9223
	step [174/191], loss=9.8045
	step [175/191], loss=10.0970
	step [176/191], loss=9.5134
	step [177/191], loss=8.7804
	step [178/191], loss=9.3040
	step [179/191], loss=8.9148
	step [180/191], loss=9.5435
	step [181/191], loss=8.0882
	step [182/191], loss=10.1289
	step [183/191], loss=8.5786
	step [184/191], loss=9.5831
	step [185/191], loss=9.7698
	step [186/191], loss=10.0426
	step [187/191], loss=8.6946
	step [188/191], loss=8.9620
	step [189/191], loss=8.6531
	step [190/191], loss=8.3754
	step [191/191], loss=4.0869
	Evaluating
	loss=0.0275, precision=0.3325, recall=0.9841, f1=0.4971
saving model as: 1_saved_model.pth
Training epoch 18
	step [1/191], loss=8.8517
	step [2/191], loss=7.8695
	step [3/191], loss=9.3005
	step [4/191], loss=11.7710
	step [5/191], loss=8.9173
	step [6/191], loss=8.3647
	step [7/191], loss=7.7967
	step [8/191], loss=9.6872
	step [9/191], loss=8.6351
	step [10/191], loss=8.5795
	step [11/191], loss=8.3063
	step [12/191], loss=9.4839
	step [13/191], loss=10.0448
	step [14/191], loss=8.1255
	step [15/191], loss=9.3109
	step [16/191], loss=7.9405
	step [17/191], loss=8.3888
	step [18/191], loss=9.0270
	step [19/191], loss=8.5150
	step [20/191], loss=9.2178
	step [21/191], loss=8.2436
	step [22/191], loss=8.8689
	step [23/191], loss=8.6542
	step [24/191], loss=9.8665
	step [25/191], loss=8.6096
	step [26/191], loss=9.5543
	step [27/191], loss=8.0616
	step [28/191], loss=8.9734
	step [29/191], loss=8.2066
	step [30/191], loss=8.3463
	step [31/191], loss=8.0138
	step [32/191], loss=8.7211
	step [33/191], loss=9.1051
	step [34/191], loss=9.0262
	step [35/191], loss=8.2698
	step [36/191], loss=9.0330
	step [37/191], loss=7.8290
	step [38/191], loss=9.3133
	step [39/191], loss=8.9637
	step [40/191], loss=7.7002
	step [41/191], loss=8.5656
	step [42/191], loss=7.4306
	step [43/191], loss=8.6229
	step [44/191], loss=8.9606
	step [45/191], loss=8.2738
	step [46/191], loss=9.3262
	step [47/191], loss=8.0400
	step [48/191], loss=8.5079
	step [49/191], loss=6.8601
	step [50/191], loss=9.0985
	step [51/191], loss=8.6154
	step [52/191], loss=10.8465
	step [53/191], loss=9.0290
	step [54/191], loss=9.7781
	step [55/191], loss=8.2048
	step [56/191], loss=8.9583
	step [57/191], loss=7.7654
	step [58/191], loss=8.8227
	step [59/191], loss=8.6622
	step [60/191], loss=8.6810
	step [61/191], loss=8.6835
	step [62/191], loss=7.9101
	step [63/191], loss=9.2057
	step [64/191], loss=9.3065
	step [65/191], loss=8.5671
	step [66/191], loss=9.5602
	step [67/191], loss=8.8606
	step [68/191], loss=11.4265
	step [69/191], loss=8.3508
	step [70/191], loss=7.8975
	step [71/191], loss=7.4768
	step [72/191], loss=9.6280
	step [73/191], loss=9.0543
	step [74/191], loss=8.8170
	step [75/191], loss=7.7014
	step [76/191], loss=7.6459
	step [77/191], loss=8.9136
	step [78/191], loss=10.4477
	step [79/191], loss=9.4298
	step [80/191], loss=7.7133
	step [81/191], loss=9.0148
	step [82/191], loss=8.0636
	step [83/191], loss=9.5806
	step [84/191], loss=9.0533
	step [85/191], loss=9.1843
	step [86/191], loss=8.9492
	step [87/191], loss=8.1283
	step [88/191], loss=9.2536
	step [89/191], loss=8.5096
	step [90/191], loss=9.9617
	step [91/191], loss=7.9631
	step [92/191], loss=9.0263
	step [93/191], loss=7.9309
	step [94/191], loss=7.5383
	step [95/191], loss=8.3748
	step [96/191], loss=8.7356
	step [97/191], loss=8.1939
	step [98/191], loss=7.8761
	step [99/191], loss=8.4015
	step [100/191], loss=7.3660
	step [101/191], loss=7.8336
	step [102/191], loss=8.0677
	step [103/191], loss=8.1106
	step [104/191], loss=8.2775
	step [105/191], loss=8.9617
	step [106/191], loss=8.1506
	step [107/191], loss=7.7394
	step [108/191], loss=8.7545
	step [109/191], loss=10.5551
	step [110/191], loss=8.4028
	step [111/191], loss=7.9150
	step [112/191], loss=7.8785
	step [113/191], loss=8.9303
	step [114/191], loss=10.0018
	step [115/191], loss=8.7722
	step [116/191], loss=9.3016
	step [117/191], loss=8.3834
	step [118/191], loss=9.1597
	step [119/191], loss=9.0963
	step [120/191], loss=9.5600
	step [121/191], loss=8.5489
	step [122/191], loss=7.9571
	step [123/191], loss=8.9237
	step [124/191], loss=9.6753
	step [125/191], loss=9.9083
	step [126/191], loss=9.4631
	step [127/191], loss=10.2889
	step [128/191], loss=9.1258
	step [129/191], loss=9.0839
	step [130/191], loss=8.6873
	step [131/191], loss=9.6366
	step [132/191], loss=8.1516
	step [133/191], loss=9.0896
	step [134/191], loss=8.6666
	step [135/191], loss=9.2201
	step [136/191], loss=7.9463
	step [137/191], loss=9.1275
	step [138/191], loss=8.0738
	step [139/191], loss=9.1426
	step [140/191], loss=7.6052
	step [141/191], loss=7.7653
	step [142/191], loss=8.3548
	step [143/191], loss=7.5263
	step [144/191], loss=9.1939
	step [145/191], loss=9.8211
	step [146/191], loss=8.3958
	step [147/191], loss=8.8317
	step [148/191], loss=9.2054
	step [149/191], loss=8.9664
	step [150/191], loss=8.2923
	step [151/191], loss=10.0287
	step [152/191], loss=8.9562
	step [153/191], loss=9.5654
	step [154/191], loss=9.1061
	step [155/191], loss=8.0855
	step [156/191], loss=9.5447
	step [157/191], loss=7.6623
	step [158/191], loss=8.3946
	step [159/191], loss=8.5899
	step [160/191], loss=8.2962
	step [161/191], loss=8.1243
	step [162/191], loss=9.8589
	step [163/191], loss=8.6682
	step [164/191], loss=9.1824
	step [165/191], loss=8.3694
	step [166/191], loss=8.0468
	step [167/191], loss=7.5903
	step [168/191], loss=8.6536
	step [169/191], loss=8.2053
	step [170/191], loss=8.5332
	step [171/191], loss=8.2363
	step [172/191], loss=8.8516
	step [173/191], loss=9.2568
	step [174/191], loss=8.2928
	step [175/191], loss=9.7423
	step [176/191], loss=8.7428
	step [177/191], loss=8.3965
	step [178/191], loss=8.2145
	step [179/191], loss=8.7729
	step [180/191], loss=7.8871
	step [181/191], loss=7.7721
	step [182/191], loss=7.1755
	step [183/191], loss=8.7286
	step [184/191], loss=8.9850
	step [185/191], loss=10.3800
	step [186/191], loss=9.6081
	step [187/191], loss=8.2331
	step [188/191], loss=8.3522
	step [189/191], loss=7.2766
	step [190/191], loss=7.8937
	step [191/191], loss=3.1790
	Evaluating
	loss=0.0267, precision=0.3095, recall=0.9876, f1=0.4713
Training epoch 19
	step [1/191], loss=8.9190
	step [2/191], loss=7.6389
	step [3/191], loss=8.3941
	step [4/191], loss=7.7087
	step [5/191], loss=9.6222
	step [6/191], loss=9.1305
	step [7/191], loss=8.5458
	step [8/191], loss=8.0684
	step [9/191], loss=8.6406
	step [10/191], loss=8.7161
	step [11/191], loss=8.4966
	step [12/191], loss=8.2980
	step [13/191], loss=8.1463
	step [14/191], loss=8.9385
	step [15/191], loss=8.7324
	step [16/191], loss=8.7402
	step [17/191], loss=8.4187
	step [18/191], loss=9.1221
	step [19/191], loss=9.4717
	step [20/191], loss=8.4879
	step [21/191], loss=7.7962
	step [22/191], loss=7.8267
	step [23/191], loss=7.9554
	step [24/191], loss=8.8401
	step [25/191], loss=8.1356
	step [26/191], loss=7.5712
	step [27/191], loss=8.8157
	step [28/191], loss=7.6937
	step [29/191], loss=9.4299
	step [30/191], loss=9.5740
	step [31/191], loss=8.6755
	step [32/191], loss=8.3860
	step [33/191], loss=9.2239
	step [34/191], loss=8.2925
	step [35/191], loss=8.3556
	step [36/191], loss=9.0541
	step [37/191], loss=9.2439
	step [38/191], loss=9.4545
	step [39/191], loss=7.3893
	step [40/191], loss=9.6778
	step [41/191], loss=9.1120
	step [42/191], loss=7.9799
	step [43/191], loss=8.4110
	step [44/191], loss=8.6211
	step [45/191], loss=7.7281
	step [46/191], loss=8.9200
	step [47/191], loss=7.9711
	step [48/191], loss=8.5010
	step [49/191], loss=8.6252
	step [50/191], loss=8.4860
	step [51/191], loss=8.6565
	step [52/191], loss=8.5034
	step [53/191], loss=8.1509
	step [54/191], loss=7.7146
	step [55/191], loss=8.3283
	step [56/191], loss=8.7326
	step [57/191], loss=8.2808
	step [58/191], loss=8.8461
	step [59/191], loss=8.1155
	step [60/191], loss=8.6622
	step [61/191], loss=9.4410
	step [62/191], loss=8.1795
	step [63/191], loss=7.5246
	step [64/191], loss=8.5228
	step [65/191], loss=8.4103
	step [66/191], loss=8.1959
	step [67/191], loss=7.2031
	step [68/191], loss=7.9043
	step [69/191], loss=9.4464
	step [70/191], loss=8.5541
	step [71/191], loss=8.9541
	step [72/191], loss=7.8834
	step [73/191], loss=9.5157
	step [74/191], loss=7.6831
	step [75/191], loss=7.6992
	step [76/191], loss=8.6189
	step [77/191], loss=9.6960
	step [78/191], loss=8.2324
	step [79/191], loss=8.0681
	step [80/191], loss=9.3784
	step [81/191], loss=9.4235
	step [82/191], loss=7.4893
	step [83/191], loss=8.0614
	step [84/191], loss=7.8577
	step [85/191], loss=9.9015
	step [86/191], loss=8.5092
	step [87/191], loss=7.5502
	step [88/191], loss=9.3608
	step [89/191], loss=7.3276
	step [90/191], loss=7.7015
	step [91/191], loss=8.2434
	step [92/191], loss=7.9168
	step [93/191], loss=8.3791
	step [94/191], loss=8.2976
	step [95/191], loss=7.4011
	step [96/191], loss=7.7175
	step [97/191], loss=9.2527
	step [98/191], loss=7.1946
	step [99/191], loss=7.6922
	step [100/191], loss=7.3826
	step [101/191], loss=8.1966
	step [102/191], loss=10.1960
	step [103/191], loss=8.1724
	step [104/191], loss=8.3786
	step [105/191], loss=9.1897
	step [106/191], loss=7.6057
	step [107/191], loss=7.4575
	step [108/191], loss=8.0758
	step [109/191], loss=7.5919
	step [110/191], loss=7.1810
	step [111/191], loss=8.1115
	step [112/191], loss=8.3199
	step [113/191], loss=7.1791
	step [114/191], loss=7.6740
	step [115/191], loss=7.5534
	step [116/191], loss=9.2483
	step [117/191], loss=9.9030
	step [118/191], loss=9.5837
	step [119/191], loss=8.8370
	step [120/191], loss=7.4732
	step [121/191], loss=7.5062
	step [122/191], loss=9.5941
	step [123/191], loss=8.4600
	step [124/191], loss=8.0288
	step [125/191], loss=7.8404
	step [126/191], loss=8.0910
	step [127/191], loss=8.8443
	step [128/191], loss=8.3899
	step [129/191], loss=7.8674
	step [130/191], loss=7.1981
	step [131/191], loss=7.6337
	step [132/191], loss=6.9448
	step [133/191], loss=7.6185
	step [134/191], loss=8.5808
	step [135/191], loss=8.5939
	step [136/191], loss=8.6336
	step [137/191], loss=7.5060
	step [138/191], loss=7.5534
	step [139/191], loss=7.9889
	step [140/191], loss=7.8774
	step [141/191], loss=7.8677
	step [142/191], loss=8.5889
	step [143/191], loss=8.5255
	step [144/191], loss=8.0498
	step [145/191], loss=8.2230
	step [146/191], loss=7.2222
	step [147/191], loss=8.7860
	step [148/191], loss=7.6145
	step [149/191], loss=7.9626
	step [150/191], loss=7.1137
	step [151/191], loss=8.0896
	step [152/191], loss=8.5359
	step [153/191], loss=7.7933
	step [154/191], loss=8.3356
	step [155/191], loss=8.3140
	step [156/191], loss=7.1588
	step [157/191], loss=8.2983
	step [158/191], loss=8.2058
	step [159/191], loss=7.3205
	step [160/191], loss=8.2606
	step [161/191], loss=7.6045
	step [162/191], loss=7.6927
	step [163/191], loss=7.4285
	step [164/191], loss=8.8813
	step [165/191], loss=8.7140
	step [166/191], loss=7.3103
	step [167/191], loss=7.9036
	step [168/191], loss=6.4817
	step [169/191], loss=8.1107
	step [170/191], loss=7.7963
	step [171/191], loss=9.2197
	step [172/191], loss=8.6266
	step [173/191], loss=8.7768
	step [174/191], loss=7.5341
	step [175/191], loss=7.5100
	step [176/191], loss=8.6063
	step [177/191], loss=7.3143
	step [178/191], loss=8.6318
	step [179/191], loss=8.1319
	step [180/191], loss=8.4435
	step [181/191], loss=9.1504
	step [182/191], loss=7.6625
	step [183/191], loss=7.1482
	step [184/191], loss=7.4468
	step [185/191], loss=7.8226
	step [186/191], loss=7.0010
	step [187/191], loss=7.2446
	step [188/191], loss=7.7462
	step [189/191], loss=7.5235
	step [190/191], loss=8.2826
	step [191/191], loss=4.0047
	Evaluating
	loss=0.0295, precision=0.2617, recall=0.9910, f1=0.4141
Training epoch 20
	step [1/191], loss=6.9810
	step [2/191], loss=6.3106
	step [3/191], loss=7.0966
	step [4/191], loss=8.3746
	step [5/191], loss=8.4097
	step [6/191], loss=9.1064
	step [7/191], loss=8.9610
	step [8/191], loss=9.2171
	step [9/191], loss=8.4046
	step [10/191], loss=7.3125
	step [11/191], loss=7.9080
	step [12/191], loss=6.7706
	step [13/191], loss=6.9104
	step [14/191], loss=8.4197
	step [15/191], loss=8.5618
	step [16/191], loss=9.7920
	step [17/191], loss=7.0042
	step [18/191], loss=7.5146
	step [19/191], loss=8.1371
	step [20/191], loss=7.8766
	step [21/191], loss=8.3163
	step [22/191], loss=8.3136
	step [23/191], loss=7.9490
	step [24/191], loss=8.0018
	step [25/191], loss=8.3420
	step [26/191], loss=7.9819
	step [27/191], loss=7.2014
	step [28/191], loss=8.9207
	step [29/191], loss=8.8186
	step [30/191], loss=8.8285
	step [31/191], loss=9.5048
	step [32/191], loss=7.7278
	step [33/191], loss=8.5796
	step [34/191], loss=8.6090
	step [35/191], loss=8.0830
	step [36/191], loss=8.6801
	step [37/191], loss=7.9366
	step [38/191], loss=7.0770
	step [39/191], loss=8.7605
	step [40/191], loss=8.5936
	step [41/191], loss=9.0345
	step [42/191], loss=8.4117
	step [43/191], loss=7.1527
	step [44/191], loss=6.8592
	step [45/191], loss=6.9658
	step [46/191], loss=8.0335
	step [47/191], loss=8.8054
	step [48/191], loss=8.7536
	step [49/191], loss=6.6910
	step [50/191], loss=8.2607
	step [51/191], loss=7.4851
	step [52/191], loss=8.2622
	step [53/191], loss=8.2312
	step [54/191], loss=7.6681
	step [55/191], loss=7.6041
	step [56/191], loss=7.5594
	step [57/191], loss=7.7413
	step [58/191], loss=7.1711
	step [59/191], loss=7.8571
	step [60/191], loss=8.0434
	step [61/191], loss=6.7563
	step [62/191], loss=7.3242
	step [63/191], loss=6.5385
	step [64/191], loss=8.5571
	step [65/191], loss=7.1254
	step [66/191], loss=7.4414
	step [67/191], loss=8.4847
	step [68/191], loss=7.1314
	step [69/191], loss=8.1829
	step [70/191], loss=8.5870
	step [71/191], loss=6.9094
	step [72/191], loss=7.1449
	step [73/191], loss=7.5133
	step [74/191], loss=8.4959
	step [75/191], loss=7.7847
	step [76/191], loss=6.8246
	step [77/191], loss=8.4582
	step [78/191], loss=8.3044
	step [79/191], loss=7.4829
	step [80/191], loss=9.1200
	step [81/191], loss=8.6556
	step [82/191], loss=8.8044
	step [83/191], loss=8.6448
	step [84/191], loss=8.5976
	step [85/191], loss=8.7030
	step [86/191], loss=9.3674
	step [87/191], loss=7.1309
	step [88/191], loss=6.9577
	step [89/191], loss=7.9590
	step [90/191], loss=7.9248
	step [91/191], loss=8.2283
	step [92/191], loss=6.3517
	step [93/191], loss=7.8567
	step [94/191], loss=7.2672
	step [95/191], loss=7.1165
	step [96/191], loss=9.0644
	step [97/191], loss=8.1408
	step [98/191], loss=8.5375
	step [99/191], loss=7.4437
	step [100/191], loss=7.9651
	step [101/191], loss=7.9569
	step [102/191], loss=7.5625
	step [103/191], loss=7.9717
	step [104/191], loss=6.7916
	step [105/191], loss=8.4821
	step [106/191], loss=7.8032
	step [107/191], loss=7.7165
	step [108/191], loss=8.5616
	step [109/191], loss=7.9451
	step [110/191], loss=7.7252
	step [111/191], loss=8.4437
	step [112/191], loss=6.9061
	step [113/191], loss=7.3054
	step [114/191], loss=7.8936
	step [115/191], loss=7.7464
	step [116/191], loss=7.6609
	step [117/191], loss=6.7309
	step [118/191], loss=6.8534
	step [119/191], loss=8.0412
	step [120/191], loss=9.6835
	step [121/191], loss=9.4768
	step [122/191], loss=7.7770
	step [123/191], loss=7.4918
	step [124/191], loss=7.8296
	step [125/191], loss=8.8286
	step [126/191], loss=7.8820
	step [127/191], loss=7.4862
	step [128/191], loss=7.4010
	step [129/191], loss=8.5574
	step [130/191], loss=8.3702
	step [131/191], loss=8.1309
	step [132/191], loss=7.6974
	step [133/191], loss=8.8467
	step [134/191], loss=7.2260
	step [135/191], loss=7.6686
	step [136/191], loss=7.2949
	step [137/191], loss=7.9564
	step [138/191], loss=8.0464
	step [139/191], loss=8.9829
	step [140/191], loss=7.8930
	step [141/191], loss=7.0594
	step [142/191], loss=8.3421
	step [143/191], loss=6.9614
	step [144/191], loss=7.4821
	step [145/191], loss=8.2708
	step [146/191], loss=7.6245
	step [147/191], loss=8.2123
	step [148/191], loss=7.3721
	step [149/191], loss=8.0284
	step [150/191], loss=8.0977
	step [151/191], loss=6.9660
	step [152/191], loss=7.5938
	step [153/191], loss=7.5423
	step [154/191], loss=8.6160
	step [155/191], loss=6.9432
	step [156/191], loss=7.4396
	step [157/191], loss=6.8907
	step [158/191], loss=7.9120
	step [159/191], loss=7.6172
	step [160/191], loss=7.9301
	step [161/191], loss=7.7841
	step [162/191], loss=7.6046
	step [163/191], loss=7.9684
	step [164/191], loss=7.6099
	step [165/191], loss=8.5237
	step [166/191], loss=7.0637
	step [167/191], loss=7.7709
	step [168/191], loss=7.3392
	step [169/191], loss=6.8953
	step [170/191], loss=7.0879
	step [171/191], loss=7.0339
	step [172/191], loss=8.8346
	step [173/191], loss=7.2082
	step [174/191], loss=8.7843
	step [175/191], loss=8.4625
	step [176/191], loss=8.0949
	step [177/191], loss=7.3212
	step [178/191], loss=7.3747
	step [179/191], loss=7.4852
	step [180/191], loss=8.6873
	step [181/191], loss=8.3606
	step [182/191], loss=7.9266
	step [183/191], loss=7.6286
	step [184/191], loss=7.3154
	step [185/191], loss=7.0078
	step [186/191], loss=7.6652
	step [187/191], loss=7.8664
	step [188/191], loss=8.0423
	step [189/191], loss=8.0390
	step [190/191], loss=7.0785
	step [191/191], loss=2.9830
	Evaluating
	loss=0.0290, precision=0.2743, recall=0.9896, f1=0.4296
Training epoch 21
	step [1/191], loss=7.9311
	step [2/191], loss=7.7085
	step [3/191], loss=7.9800
	step [4/191], loss=7.3533
	step [5/191], loss=7.8360
	step [6/191], loss=7.8402
	step [7/191], loss=7.3995
	step [8/191], loss=9.3018
	step [9/191], loss=6.9344
	step [10/191], loss=7.1653
	step [11/191], loss=8.8202
	step [12/191], loss=7.1634
	step [13/191], loss=7.2294
	step [14/191], loss=7.9240
	step [15/191], loss=9.2728
	step [16/191], loss=8.4639
	step [17/191], loss=7.1688
	step [18/191], loss=7.9975
	step [19/191], loss=7.2590
	step [20/191], loss=9.0213
	step [21/191], loss=6.5127
	step [22/191], loss=7.3105
	step [23/191], loss=8.3271
	step [24/191], loss=7.7673
	step [25/191], loss=6.7143
	step [26/191], loss=7.2536
	step [27/191], loss=6.9477
	step [28/191], loss=8.6649
	step [29/191], loss=7.9680
	step [30/191], loss=7.0973
	step [31/191], loss=7.6477
	step [32/191], loss=7.6725
	step [33/191], loss=8.5157
	step [34/191], loss=6.5039
	step [35/191], loss=7.0271
	step [36/191], loss=7.2168
	step [37/191], loss=7.5833
	step [38/191], loss=7.6286
	step [39/191], loss=6.2296
	step [40/191], loss=9.3411
	step [41/191], loss=7.1147
	step [42/191], loss=7.3245
	step [43/191], loss=8.6955
	step [44/191], loss=7.6103
	step [45/191], loss=8.6632
	step [46/191], loss=6.4147
	step [47/191], loss=7.3507
	step [48/191], loss=7.6512
	step [49/191], loss=7.6514
	step [50/191], loss=7.7977
	step [51/191], loss=8.3125
	step [52/191], loss=7.3858
	step [53/191], loss=7.3307
	step [54/191], loss=7.9325
	step [55/191], loss=7.0045
	step [56/191], loss=7.8691
	step [57/191], loss=7.3988
	step [58/191], loss=7.4746
	step [59/191], loss=8.1461
	step [60/191], loss=6.7743
	step [61/191], loss=8.3083
	step [62/191], loss=8.6303
	step [63/191], loss=6.7420
	step [64/191], loss=7.6727
	step [65/191], loss=8.0817
	step [66/191], loss=7.1441
	step [67/191], loss=7.7796
	step [68/191], loss=7.9046
	step [69/191], loss=7.4277
	step [70/191], loss=7.0032
	step [71/191], loss=7.1954
	step [72/191], loss=6.8223
	step [73/191], loss=7.6548
	step [74/191], loss=9.5931
	step [75/191], loss=7.2201
	step [76/191], loss=8.6210
	step [77/191], loss=7.3776
	step [78/191], loss=6.8725
	step [79/191], loss=7.0772
	step [80/191], loss=6.9832
	step [81/191], loss=7.3565
	step [82/191], loss=7.9164
	step [83/191], loss=8.0051
	step [84/191], loss=7.6533
	step [85/191], loss=6.6036
	step [86/191], loss=7.4464
	step [87/191], loss=6.7629
	step [88/191], loss=6.9693
	step [89/191], loss=5.9892
	step [90/191], loss=7.3422
	step [91/191], loss=6.5078
	step [92/191], loss=7.8378
	step [93/191], loss=8.3745
	step [94/191], loss=8.2092
	step [95/191], loss=8.1585
	step [96/191], loss=7.4718
	step [97/191], loss=6.0268
	step [98/191], loss=6.3208
	step [99/191], loss=6.8733
	step [100/191], loss=7.4844
	step [101/191], loss=7.8014
	step [102/191], loss=8.0495
	step [103/191], loss=6.9022
	step [104/191], loss=7.1890
	step [105/191], loss=8.2204
	step [106/191], loss=8.2188
	step [107/191], loss=7.4667
	step [108/191], loss=6.5234
	step [109/191], loss=7.3076
	step [110/191], loss=6.4419
	step [111/191], loss=7.1181
	step [112/191], loss=7.3410
	step [113/191], loss=7.2274
	step [114/191], loss=7.5232
	step [115/191], loss=7.4173
	step [116/191], loss=8.4626
	step [117/191], loss=9.1261
	step [118/191], loss=5.9883
	step [119/191], loss=8.1466
	step [120/191], loss=7.7882
	step [121/191], loss=7.0625
	step [122/191], loss=9.5580
	step [123/191], loss=7.6750
	step [124/191], loss=8.7527
	step [125/191], loss=6.4984
	step [126/191], loss=7.4796
	step [127/191], loss=7.3377
	step [128/191], loss=7.2113
	step [129/191], loss=6.9088
	step [130/191], loss=7.8290
	step [131/191], loss=8.2801
	step [132/191], loss=5.9391
	step [133/191], loss=6.1234
	step [134/191], loss=6.9360
	step [135/191], loss=7.9688
	step [136/191], loss=7.3034
	step [137/191], loss=6.9518
	step [138/191], loss=7.8601
	step [139/191], loss=7.2038
	step [140/191], loss=7.5356
	step [141/191], loss=6.6405
	step [142/191], loss=7.7024
	step [143/191], loss=7.1284
	step [144/191], loss=8.0291
	step [145/191], loss=6.8232
	step [146/191], loss=7.1765
	step [147/191], loss=6.6969
	step [148/191], loss=6.5262
	step [149/191], loss=7.4504
	step [150/191], loss=7.0742
	step [151/191], loss=8.0273
	step [152/191], loss=7.4994
	step [153/191], loss=8.1999
	step [154/191], loss=8.4411
	step [155/191], loss=6.7266
	step [156/191], loss=7.2293
	step [157/191], loss=8.6510
	step [158/191], loss=7.7942
	step [159/191], loss=7.6502
	step [160/191], loss=6.6524
	step [161/191], loss=8.8355
	step [162/191], loss=8.5765
	step [163/191], loss=8.0557
	step [164/191], loss=8.3478
	step [165/191], loss=7.5816
	step [166/191], loss=7.7353
	step [167/191], loss=7.7936
	step [168/191], loss=7.5565
	step [169/191], loss=6.5925
	step [170/191], loss=7.1815
	step [171/191], loss=8.0507
	step [172/191], loss=8.3277
	step [173/191], loss=7.8197
	step [174/191], loss=6.3632
	step [175/191], loss=7.6446
	step [176/191], loss=9.6551
	step [177/191], loss=7.1336
	step [178/191], loss=6.9448
	step [179/191], loss=6.7480
	step [180/191], loss=7.8976
	step [181/191], loss=7.4835
	step [182/191], loss=7.4707
	step [183/191], loss=7.6837
	step [184/191], loss=6.5602
	step [185/191], loss=7.1775
	step [186/191], loss=7.3361
	step [187/191], loss=9.0683
	step [188/191], loss=7.9107
	step [189/191], loss=7.8444
	step [190/191], loss=7.6951
	step [191/191], loss=3.3311
	Evaluating
	loss=0.0292, precision=0.2419, recall=0.9910, f1=0.3889
Training epoch 22
	step [1/191], loss=6.7563
	step [2/191], loss=7.4701
	step [3/191], loss=7.8364
	step [4/191], loss=7.4022
	step [5/191], loss=7.1987
	step [6/191], loss=8.5434
	step [7/191], loss=6.9269
	step [8/191], loss=8.4857
	step [9/191], loss=7.0632
	step [10/191], loss=6.7130
	step [11/191], loss=6.8419
	step [12/191], loss=7.5410
	step [13/191], loss=7.1474
	step [14/191], loss=7.1584
	step [15/191], loss=6.9574
	step [16/191], loss=8.3471
	step [17/191], loss=6.5597
	step [18/191], loss=8.3828
	step [19/191], loss=7.4659
	step [20/191], loss=7.0392
	step [21/191], loss=7.5282
	step [22/191], loss=7.4604
	step [23/191], loss=8.5199
	step [24/191], loss=5.9386
	step [25/191], loss=6.4183
	step [26/191], loss=7.1689
	step [27/191], loss=7.9123
	step [28/191], loss=7.2924
	step [29/191], loss=7.0401
	step [30/191], loss=6.4674
	step [31/191], loss=8.0307
	step [32/191], loss=7.6936
	step [33/191], loss=6.5640
	step [34/191], loss=6.5827
	step [35/191], loss=7.4396
	step [36/191], loss=8.1952
	step [37/191], loss=8.2819
	step [38/191], loss=8.2420
	step [39/191], loss=7.5140
	step [40/191], loss=6.1367
	step [41/191], loss=7.6550
	step [42/191], loss=7.4344
	step [43/191], loss=6.9854
	step [44/191], loss=5.9887
	step [45/191], loss=6.5327
	step [46/191], loss=7.6543
	step [47/191], loss=6.8677
	step [48/191], loss=6.6271
	step [49/191], loss=7.4631
	step [50/191], loss=6.4899
	step [51/191], loss=6.6573
	step [52/191], loss=7.5189
	step [53/191], loss=7.6902
	step [54/191], loss=7.8624
	step [55/191], loss=6.2830
	step [56/191], loss=6.4246
	step [57/191], loss=7.7587
	step [58/191], loss=7.5878
	step [59/191], loss=8.0116
	step [60/191], loss=8.7815
	step [61/191], loss=7.3770
	step [62/191], loss=6.9866
	step [63/191], loss=6.0732
	step [64/191], loss=6.8737
	step [65/191], loss=7.4325
	step [66/191], loss=7.0537
	step [67/191], loss=7.2433
	step [68/191], loss=7.8963
	step [69/191], loss=6.9786
	step [70/191], loss=6.9823
	step [71/191], loss=6.3345
	step [72/191], loss=5.7247
	step [73/191], loss=7.0499
	step [74/191], loss=7.8902
	step [75/191], loss=7.4020
	step [76/191], loss=6.7176
	step [77/191], loss=7.2493
	step [78/191], loss=6.6081
	step [79/191], loss=7.2930
	step [80/191], loss=8.0525
	step [81/191], loss=6.1120
	step [82/191], loss=7.1784
	step [83/191], loss=9.0123
	step [84/191], loss=6.5056
	step [85/191], loss=8.0276
	step [86/191], loss=7.0034
	step [87/191], loss=7.5816
	step [88/191], loss=6.6155
	step [89/191], loss=7.3148
	step [90/191], loss=6.5712
	step [91/191], loss=7.3549
	step [92/191], loss=7.1987
	step [93/191], loss=7.6470
	step [94/191], loss=5.5893
	step [95/191], loss=7.5525
	step [96/191], loss=7.7795
	step [97/191], loss=7.0481
	step [98/191], loss=6.1450
	step [99/191], loss=6.7175
	step [100/191], loss=8.6368
	step [101/191], loss=6.7722
	step [102/191], loss=7.5250
	step [103/191], loss=7.1296
	step [104/191], loss=7.2923
	step [105/191], loss=7.8240
	step [106/191], loss=7.8515
	step [107/191], loss=7.2051
	step [108/191], loss=6.8572
	step [109/191], loss=6.1145
	step [110/191], loss=8.4879
	step [111/191], loss=7.2893
	step [112/191], loss=7.1773
	step [113/191], loss=7.6267
	step [114/191], loss=6.8479
	step [115/191], loss=6.6419
	step [116/191], loss=7.6033
	step [117/191], loss=8.0830
	step [118/191], loss=6.3343
	step [119/191], loss=7.5253
	step [120/191], loss=7.2794
	step [121/191], loss=5.6423
	step [122/191], loss=7.1426
	step [123/191], loss=6.2975
	step [124/191], loss=8.5828
	step [125/191], loss=7.1520
	step [126/191], loss=8.2116
	step [127/191], loss=7.2207
	step [128/191], loss=7.2537
	step [129/191], loss=7.9525
	step [130/191], loss=6.9068
	step [131/191], loss=7.1796
	step [132/191], loss=6.5851
	step [133/191], loss=6.9748
	step [134/191], loss=5.9665
	step [135/191], loss=5.8402
	step [136/191], loss=7.9692
	step [137/191], loss=6.6084
	step [138/191], loss=6.8240
	step [139/191], loss=6.9008
	step [140/191], loss=7.6764
	step [141/191], loss=7.3952
	step [142/191], loss=7.6369
	step [143/191], loss=8.0907
	step [144/191], loss=6.9251
	step [145/191], loss=6.8438
	step [146/191], loss=7.5224
	step [147/191], loss=8.1150
	step [148/191], loss=7.2370
	step [149/191], loss=7.2638
	step [150/191], loss=7.3359
	step [151/191], loss=6.4181
	step [152/191], loss=6.9586
	step [153/191], loss=7.9389
	step [154/191], loss=7.0588
	step [155/191], loss=8.0101
	step [156/191], loss=7.4078
	step [157/191], loss=8.2019
	step [158/191], loss=7.2884
	step [159/191], loss=7.1618
	step [160/191], loss=7.6626
	step [161/191], loss=6.4494
	step [162/191], loss=7.0171
	step [163/191], loss=6.0016
	step [164/191], loss=7.7839
	step [165/191], loss=7.3653
	step [166/191], loss=6.6708
	step [167/191], loss=7.0573
	step [168/191], loss=6.6274
	step [169/191], loss=7.7371
	step [170/191], loss=6.7699
	step [171/191], loss=6.3615
	step [172/191], loss=6.1694
	step [173/191], loss=6.9500
	step [174/191], loss=7.8610
	step [175/191], loss=7.0488
	step [176/191], loss=7.7728
	step [177/191], loss=7.3584
	step [178/191], loss=7.2998
	step [179/191], loss=7.8750
	step [180/191], loss=7.1759
	step [181/191], loss=7.0058
	step [182/191], loss=7.7629
	step [183/191], loss=7.5595
	step [184/191], loss=7.2677
	step [185/191], loss=6.9336
	step [186/191], loss=7.4771
	step [187/191], loss=7.6969
	step [188/191], loss=7.5662
	step [189/191], loss=6.7585
	step [190/191], loss=6.7184
	step [191/191], loss=3.2337
	Evaluating
	loss=0.0218, precision=0.3329, recall=0.9862, f1=0.4977
saving model as: 1_saved_model.pth
Training epoch 23
	step [1/191], loss=6.6005
	step [2/191], loss=7.3386
	step [3/191], loss=7.2457
	step [4/191], loss=5.8301
	step [5/191], loss=7.6579
	step [6/191], loss=6.2707
	step [7/191], loss=6.5200
	step [8/191], loss=7.4121
	step [9/191], loss=6.9903
	step [10/191], loss=6.2262
	step [11/191], loss=6.9691
	step [12/191], loss=6.1761
	step [13/191], loss=6.7730
	step [14/191], loss=6.4392
	step [15/191], loss=6.8701
	step [16/191], loss=7.2802
	step [17/191], loss=6.5799
	step [18/191], loss=8.1966
	step [19/191], loss=6.9450
	step [20/191], loss=7.4959
	step [21/191], loss=7.3472
	step [22/191], loss=6.5552
	step [23/191], loss=6.8274
	step [24/191], loss=7.1632
	step [25/191], loss=6.8655
	step [26/191], loss=7.8320
	step [27/191], loss=7.1971
	step [28/191], loss=6.5662
	step [29/191], loss=6.1036
	step [30/191], loss=7.0009
	step [31/191], loss=6.4205
	step [32/191], loss=6.8649
	step [33/191], loss=5.7872
	step [34/191], loss=7.7716
	step [35/191], loss=6.5572
	step [36/191], loss=6.6119
	step [37/191], loss=7.0943
	step [38/191], loss=6.7850
	step [39/191], loss=6.4867
	step [40/191], loss=7.8997
	step [41/191], loss=7.6741
	step [42/191], loss=6.4490
	step [43/191], loss=6.1912
	step [44/191], loss=6.6613
	step [45/191], loss=7.4385
	step [46/191], loss=7.0413
	step [47/191], loss=5.9768
	step [48/191], loss=7.1789
	step [49/191], loss=6.4930
	step [50/191], loss=6.9393
	step [51/191], loss=6.2037
	step [52/191], loss=6.9108
	step [53/191], loss=6.3746
	step [54/191], loss=7.0309
	step [55/191], loss=8.1621
	step [56/191], loss=7.7367
	step [57/191], loss=6.1975
	step [58/191], loss=6.3240
	step [59/191], loss=7.3097
	step [60/191], loss=7.6082
	step [61/191], loss=6.8028
	step [62/191], loss=7.6970
	step [63/191], loss=6.4554
	step [64/191], loss=7.1051
	step [65/191], loss=5.9257
	step [66/191], loss=6.2806
	step [67/191], loss=6.5868
	step [68/191], loss=6.7207
	step [69/191], loss=7.3338
	step [70/191], loss=7.7756
	step [71/191], loss=6.0198
	step [72/191], loss=7.1060
	step [73/191], loss=6.1845
	step [74/191], loss=7.1780
	step [75/191], loss=6.8740
	step [76/191], loss=7.4591
	step [77/191], loss=7.0237
	step [78/191], loss=7.4417
	step [79/191], loss=6.4634
	step [80/191], loss=6.8754
	step [81/191], loss=7.5103
	step [82/191], loss=6.0633
	step [83/191], loss=6.7626
	step [84/191], loss=5.2727
	step [85/191], loss=7.4586
	step [86/191], loss=8.1502
	step [87/191], loss=6.4470
	step [88/191], loss=6.3079
	step [89/191], loss=6.9523
	step [90/191], loss=6.8114
	step [91/191], loss=8.3399
	step [92/191], loss=6.8891
	step [93/191], loss=6.5034
	step [94/191], loss=6.4640
	step [95/191], loss=6.8565
	step [96/191], loss=6.4734
	step [97/191], loss=7.5744
	step [98/191], loss=6.2914
	step [99/191], loss=6.1658
	step [100/191], loss=6.5718
	step [101/191], loss=7.4377
	step [102/191], loss=7.2369
	step [103/191], loss=6.3033
	step [104/191], loss=6.9982
	step [105/191], loss=5.2381
	step [106/191], loss=8.3100
	step [107/191], loss=6.1789
	step [108/191], loss=6.8040
	step [109/191], loss=7.7422
	step [110/191], loss=6.8442
	step [111/191], loss=6.5570
	step [112/191], loss=7.4306
	step [113/191], loss=6.9538
	step [114/191], loss=8.2461
	step [115/191], loss=7.0612
	step [116/191], loss=6.1387
	step [117/191], loss=8.5134
	step [118/191], loss=7.2072
	step [119/191], loss=8.0486
	step [120/191], loss=7.1910
	step [121/191], loss=7.6661
	step [122/191], loss=8.9375
	step [123/191], loss=7.0351
	step [124/191], loss=6.0487
	step [125/191], loss=6.8587
	step [126/191], loss=7.1457
	step [127/191], loss=7.8862
	step [128/191], loss=6.1110
	step [129/191], loss=6.1810
	step [130/191], loss=7.8119
	step [131/191], loss=8.3135
	step [132/191], loss=6.3175
	step [133/191], loss=6.7767
	step [134/191], loss=6.6486
	step [135/191], loss=6.8322
	step [136/191], loss=6.9440
	step [137/191], loss=8.1578
	step [138/191], loss=7.9390
	step [139/191], loss=6.2632
	step [140/191], loss=7.2484
	step [141/191], loss=6.8492
	step [142/191], loss=6.5268
	step [143/191], loss=7.2507
	step [144/191], loss=7.0780
	step [145/191], loss=6.0590
	step [146/191], loss=6.7375
	step [147/191], loss=7.4523
	step [148/191], loss=7.1247
	step [149/191], loss=7.6575
	step [150/191], loss=7.3037
	step [151/191], loss=6.4159
	step [152/191], loss=7.0792
	step [153/191], loss=6.2726
	step [154/191], loss=7.0633
	step [155/191], loss=6.1275
	step [156/191], loss=6.1142
	step [157/191], loss=7.2004
	step [158/191], loss=6.2612
	step [159/191], loss=5.8925
	step [160/191], loss=7.5834
	step [161/191], loss=7.0029
	step [162/191], loss=7.9321
	step [163/191], loss=6.1915
	step [164/191], loss=6.7756
	step [165/191], loss=6.4453
	step [166/191], loss=6.7901
	step [167/191], loss=6.9354
	step [168/191], loss=6.7518
	step [169/191], loss=7.8903
	step [170/191], loss=6.7099
	step [171/191], loss=6.8318
	step [172/191], loss=7.2724
	step [173/191], loss=7.9396
	step [174/191], loss=7.1573
	step [175/191], loss=7.0798
	step [176/191], loss=7.2119
	step [177/191], loss=8.3684
	step [178/191], loss=7.4892
	step [179/191], loss=6.5550
	step [180/191], loss=6.3414
	step [181/191], loss=6.6781
	step [182/191], loss=6.6108
	step [183/191], loss=6.8186
	step [184/191], loss=7.0431
	step [185/191], loss=7.5493
	step [186/191], loss=6.6797
	step [187/191], loss=6.2828
	step [188/191], loss=7.8943
	step [189/191], loss=6.4894
	step [190/191], loss=6.1593
	step [191/191], loss=3.3906
	Evaluating
	loss=0.0265, precision=0.2665, recall=0.9898, f1=0.4200
Training epoch 24
	step [1/191], loss=6.5618
	step [2/191], loss=6.7780
	step [3/191], loss=6.9693
	step [4/191], loss=6.0132
	step [5/191], loss=7.3278
	step [6/191], loss=7.3857
	step [7/191], loss=6.3532
	step [8/191], loss=6.9185
	step [9/191], loss=6.8814
	step [10/191], loss=7.0279
	step [11/191], loss=7.3456
	step [12/191], loss=5.8196
	step [13/191], loss=7.2161
	step [14/191], loss=7.5788
	step [15/191], loss=6.2750
	step [16/191], loss=7.1374
	step [17/191], loss=6.4983
	step [18/191], loss=7.9033
	step [19/191], loss=7.4491
	step [20/191], loss=7.0247
	step [21/191], loss=6.6952
	step [22/191], loss=7.8424
	step [23/191], loss=7.1579
	step [24/191], loss=7.0687
	step [25/191], loss=6.1236
	step [26/191], loss=6.4518
	step [27/191], loss=7.1705
	step [28/191], loss=6.5674
	step [29/191], loss=8.2087
	step [30/191], loss=5.9586
	step [31/191], loss=6.2975
	step [32/191], loss=7.1427
	step [33/191], loss=5.7450
	step [34/191], loss=6.4214
	step [35/191], loss=7.3882
	step [36/191], loss=6.4411
	step [37/191], loss=6.1924
	step [38/191], loss=8.3474
	step [39/191], loss=7.0304
	step [40/191], loss=6.2808
	step [41/191], loss=7.9020
	step [42/191], loss=6.9055
	step [43/191], loss=8.2981
	step [44/191], loss=6.0591
	step [45/191], loss=7.6620
	step [46/191], loss=6.1980
	step [47/191], loss=6.5808
	step [48/191], loss=6.1476
	step [49/191], loss=7.2568
	step [50/191], loss=6.7912
	step [51/191], loss=5.9624
	step [52/191], loss=6.3566
	step [53/191], loss=5.9235
	step [54/191], loss=6.5256
	step [55/191], loss=6.9903
	step [56/191], loss=7.4743
	step [57/191], loss=6.3921
	step [58/191], loss=6.2544
	step [59/191], loss=6.1006
	step [60/191], loss=6.0636
	step [61/191], loss=6.3279
	step [62/191], loss=5.5235
	step [63/191], loss=5.8871
	step [64/191], loss=6.6089
	step [65/191], loss=5.6782
	step [66/191], loss=6.6463
	step [67/191], loss=7.0048
	step [68/191], loss=6.2616
	step [69/191], loss=6.6341
	step [70/191], loss=9.0901
	step [71/191], loss=5.6659
	step [72/191], loss=7.3637
	step [73/191], loss=7.5196
	step [74/191], loss=7.2556
	step [75/191], loss=7.0625
	step [76/191], loss=6.3922
	step [77/191], loss=7.2159
	step [78/191], loss=4.9350
	step [79/191], loss=6.5539
	step [80/191], loss=8.9165
	step [81/191], loss=6.7022
	step [82/191], loss=6.6440
	step [83/191], loss=6.3042
	step [84/191], loss=5.9502
	step [85/191], loss=5.3108
	step [86/191], loss=6.7794
	step [87/191], loss=6.9901
	step [88/191], loss=6.0001
	step [89/191], loss=8.3364
	step [90/191], loss=7.3950
	step [91/191], loss=6.3038
	step [92/191], loss=7.4356
	step [93/191], loss=6.3051
	step [94/191], loss=6.5603
	step [95/191], loss=7.2001
	step [96/191], loss=6.1545
	step [97/191], loss=6.0578
	step [98/191], loss=6.4952
	step [99/191], loss=5.3904
	step [100/191], loss=6.7799
	step [101/191], loss=6.3077
	step [102/191], loss=7.0160
	step [103/191], loss=6.5003
	step [104/191], loss=6.1284
	step [105/191], loss=7.5617
	step [106/191], loss=7.3605
	step [107/191], loss=6.8297
	step [108/191], loss=6.8681
	step [109/191], loss=6.3439
	step [110/191], loss=6.4589
	step [111/191], loss=6.0973
	step [112/191], loss=6.5312
	step [113/191], loss=6.5584
	step [114/191], loss=6.5673
	step [115/191], loss=6.7677
	step [116/191], loss=7.3380
	step [117/191], loss=6.2959
	step [118/191], loss=6.1719
	step [119/191], loss=6.1106
	step [120/191], loss=6.6019
	step [121/191], loss=5.9847
	step [122/191], loss=5.3204
	step [123/191], loss=6.6173
	step [124/191], loss=6.7910
	step [125/191], loss=7.2712
	step [126/191], loss=6.2323
	step [127/191], loss=7.0727
	step [128/191], loss=7.7645
	step [129/191], loss=6.6911
	step [130/191], loss=6.6447
	step [131/191], loss=7.3521
	step [132/191], loss=6.8635
	step [133/191], loss=6.4541
	step [134/191], loss=5.8287
	step [135/191], loss=5.7329
	step [136/191], loss=5.9341
	step [137/191], loss=7.2961
	step [138/191], loss=6.6831
	step [139/191], loss=5.3741
	step [140/191], loss=5.7661
	step [141/191], loss=7.2745
	step [142/191], loss=5.6159
	step [143/191], loss=6.7755
	step [144/191], loss=5.2101
	step [145/191], loss=7.1767
	step [146/191], loss=6.1995
	step [147/191], loss=7.0703
	step [148/191], loss=6.3097
	step [149/191], loss=5.8065
	step [150/191], loss=6.7310
	step [151/191], loss=6.1198
	step [152/191], loss=6.2930
	step [153/191], loss=7.0733
	step [154/191], loss=7.0260
	step [155/191], loss=6.9364
	step [156/191], loss=6.6740
	step [157/191], loss=5.6937
	step [158/191], loss=7.2354
	step [159/191], loss=7.6771
	step [160/191], loss=6.3438
	step [161/191], loss=6.0486
	step [162/191], loss=6.4937
	step [163/191], loss=5.8392
	step [164/191], loss=6.2042
	step [165/191], loss=6.6116
	step [166/191], loss=6.7590
	step [167/191], loss=6.6208
	step [168/191], loss=8.3647
	step [169/191], loss=7.1918
	step [170/191], loss=7.2559
	step [171/191], loss=7.3064
	step [172/191], loss=7.6453
	step [173/191], loss=7.9572
	step [174/191], loss=6.2217
	step [175/191], loss=6.7637
	step [176/191], loss=6.2182
	step [177/191], loss=6.6513
	step [178/191], loss=6.9888
	step [179/191], loss=6.2866
	step [180/191], loss=7.9274
	step [181/191], loss=5.4857
	step [182/191], loss=6.0559
	step [183/191], loss=6.4007
	step [184/191], loss=5.7814
	step [185/191], loss=6.9785
	step [186/191], loss=7.1852
	step [187/191], loss=7.0991
	step [188/191], loss=6.9691
	step [189/191], loss=6.1741
	step [190/191], loss=6.0197
	step [191/191], loss=2.4286
	Evaluating
	loss=0.0220, precision=0.2994, recall=0.9879, f1=0.4595
Training epoch 25
	step [1/191], loss=7.1862
	step [2/191], loss=6.4992
	step [3/191], loss=6.5630
	step [4/191], loss=7.5174
	step [5/191], loss=6.7013
	step [6/191], loss=6.3949
	step [7/191], loss=5.9528
	step [8/191], loss=7.0584
	step [9/191], loss=7.7610
	step [10/191], loss=6.5658
	step [11/191], loss=7.4905
	step [12/191], loss=6.5218
	step [13/191], loss=5.4377
	step [14/191], loss=6.3998
	step [15/191], loss=6.0310
	step [16/191], loss=6.8313
	step [17/191], loss=6.5494
	step [18/191], loss=5.9380
	step [19/191], loss=7.0625
	step [20/191], loss=6.7163
	step [21/191], loss=6.8522
	step [22/191], loss=7.2631
	step [23/191], loss=7.2477
	step [24/191], loss=6.7171
	step [25/191], loss=7.1672
	step [26/191], loss=7.9420
	step [27/191], loss=6.9912
	step [28/191], loss=6.1493
	step [29/191], loss=6.6653
	step [30/191], loss=6.6112
	step [31/191], loss=5.9540
	step [32/191], loss=5.9614
	step [33/191], loss=6.2451
	step [34/191], loss=6.7095
	step [35/191], loss=6.3978
	step [36/191], loss=6.2394
	step [37/191], loss=6.2928
	step [38/191], loss=6.7968
	step [39/191], loss=6.1637
	step [40/191], loss=6.9088
	step [41/191], loss=5.8230
	step [42/191], loss=6.2511
	step [43/191], loss=6.4536
	step [44/191], loss=7.0125
	step [45/191], loss=5.4752
	step [46/191], loss=7.0245
	step [47/191], loss=6.0761
	step [48/191], loss=6.4235
	step [49/191], loss=7.1846
	step [50/191], loss=5.4644
	step [51/191], loss=6.3235
	step [52/191], loss=6.4738
	step [53/191], loss=6.7052
	step [54/191], loss=7.1380
	step [55/191], loss=6.7827
	step [56/191], loss=7.1979
	step [57/191], loss=6.4794
	step [58/191], loss=6.6190
	step [59/191], loss=5.6231
	step [60/191], loss=6.6223
	step [61/191], loss=6.9879
	step [62/191], loss=6.7974
	step [63/191], loss=6.1348
	step [64/191], loss=7.2241
	step [65/191], loss=6.1764
	step [66/191], loss=6.1831
	step [67/191], loss=7.0436
	step [68/191], loss=5.9449
	step [69/191], loss=6.7493
	step [70/191], loss=7.1989
	step [71/191], loss=7.6810
	step [72/191], loss=7.2669
	step [73/191], loss=6.2000
	step [74/191], loss=7.0273
	step [75/191], loss=6.0126
	step [76/191], loss=7.9066
	step [77/191], loss=6.9125
	step [78/191], loss=5.7169
	step [79/191], loss=6.6848
	step [80/191], loss=6.1666
	step [81/191], loss=8.1761
	step [82/191], loss=6.1270
	step [83/191], loss=6.6412
	step [84/191], loss=6.6887
	step [85/191], loss=7.0594
	step [86/191], loss=6.0637
	step [87/191], loss=6.8327
	step [88/191], loss=6.6414
	step [89/191], loss=7.2994
	step [90/191], loss=6.7394
	step [91/191], loss=5.4730
	step [92/191], loss=5.9508
	step [93/191], loss=6.3461
	step [94/191], loss=7.2774
	step [95/191], loss=6.1323
	step [96/191], loss=6.4450
	step [97/191], loss=6.9743
	step [98/191], loss=7.2705
	step [99/191], loss=7.0436
	step [100/191], loss=5.8310
	step [101/191], loss=5.7730
	step [102/191], loss=6.3284
	step [103/191], loss=5.8618
	step [104/191], loss=7.4174
	step [105/191], loss=6.3971
	step [106/191], loss=6.6054
	step [107/191], loss=6.7350
	step [108/191], loss=6.2027
	step [109/191], loss=6.1096
	step [110/191], loss=6.0256
	step [111/191], loss=5.6980
	step [112/191], loss=6.3429
	step [113/191], loss=6.6721
	step [114/191], loss=6.1048
	step [115/191], loss=6.1872
	step [116/191], loss=5.7585
	step [117/191], loss=6.3468
	step [118/191], loss=6.6926
	step [119/191], loss=7.8813
	step [120/191], loss=6.5616
	step [121/191], loss=6.7761
	step [122/191], loss=6.2087
	step [123/191], loss=6.6281
	step [124/191], loss=5.1964
	step [125/191], loss=5.8262
	step [126/191], loss=7.4508
	step [127/191], loss=6.1251
	step [128/191], loss=5.9799
	step [129/191], loss=5.7031
	step [130/191], loss=6.8017
	step [131/191], loss=5.7692
	step [132/191], loss=6.9776
	step [133/191], loss=6.9182
	step [134/191], loss=6.9085
	step [135/191], loss=7.9464
	step [136/191], loss=5.7965
	step [137/191], loss=7.7720
	step [138/191], loss=7.5578
	step [139/191], loss=5.6574
	step [140/191], loss=5.5354
	step [141/191], loss=5.9984
	step [142/191], loss=7.1633
	step [143/191], loss=6.0234
	step [144/191], loss=5.6060
	step [145/191], loss=5.5007
	step [146/191], loss=7.1808
	step [147/191], loss=6.8835
	step [148/191], loss=5.8464
	step [149/191], loss=6.1479
	step [150/191], loss=6.0133
	step [151/191], loss=6.0847
	step [152/191], loss=5.7605
	step [153/191], loss=6.7845
	step [154/191], loss=5.8005
	step [155/191], loss=7.4050
	step [156/191], loss=6.8070
	step [157/191], loss=6.4016
	step [158/191], loss=6.6624
	step [159/191], loss=6.6839
	step [160/191], loss=5.4358
	step [161/191], loss=7.9047
	step [162/191], loss=5.7019
	step [163/191], loss=7.1626
	step [164/191], loss=5.9805
	step [165/191], loss=6.1848
	step [166/191], loss=6.3009
	step [167/191], loss=5.6943
	step [168/191], loss=5.8857
	step [169/191], loss=6.1175
	step [170/191], loss=7.6862
	step [171/191], loss=5.5129
	step [172/191], loss=7.0258
	step [173/191], loss=6.3394
	step [174/191], loss=6.9179
	step [175/191], loss=5.7533
	step [176/191], loss=5.7135
	step [177/191], loss=6.8414
	step [178/191], loss=6.4328
	step [179/191], loss=7.3022
	step [180/191], loss=6.1706
	step [181/191], loss=6.3693
	step [182/191], loss=5.5990
	step [183/191], loss=7.2495
	step [184/191], loss=6.7980
	step [185/191], loss=6.0980
	step [186/191], loss=4.8566
	step [187/191], loss=6.9888
	step [188/191], loss=5.4240
	step [189/191], loss=6.9744
	step [190/191], loss=6.1413
	step [191/191], loss=3.0039
	Evaluating
	loss=0.0252, precision=0.2682, recall=0.9902, f1=0.4220
Training epoch 26
	step [1/191], loss=7.0330
	step [2/191], loss=6.3427
	step [3/191], loss=5.8408
	step [4/191], loss=4.7597
	step [5/191], loss=6.1455
	step [6/191], loss=6.1045
	step [7/191], loss=6.6759
	step [8/191], loss=6.1930
	step [9/191], loss=5.6046
	step [10/191], loss=6.8465
	step [11/191], loss=6.7564
	step [12/191], loss=6.2625
	step [13/191], loss=5.6553
	step [14/191], loss=7.3978
	step [15/191], loss=6.0546
	step [16/191], loss=6.2111
	step [17/191], loss=7.1178
	step [18/191], loss=6.8681
	step [19/191], loss=6.0409
	step [20/191], loss=5.2623
	step [21/191], loss=6.5723
	step [22/191], loss=6.8441
	step [23/191], loss=5.8852
	step [24/191], loss=6.2193
	step [25/191], loss=6.6357
	step [26/191], loss=6.4344
	step [27/191], loss=6.0719
	step [28/191], loss=6.5942
	step [29/191], loss=6.3129
	step [30/191], loss=6.2772
	step [31/191], loss=6.1065
	step [32/191], loss=5.3658
	step [33/191], loss=6.9551
	step [34/191], loss=5.8039
	step [35/191], loss=5.8866
	step [36/191], loss=7.6234
	step [37/191], loss=5.7306
	step [38/191], loss=5.6251
	step [39/191], loss=6.8089
	step [40/191], loss=6.7017
	step [41/191], loss=5.5441
	step [42/191], loss=6.1003
	step [43/191], loss=7.3958
	step [44/191], loss=5.1157
	step [45/191], loss=5.2545
	step [46/191], loss=6.5146
	step [47/191], loss=6.6327
	step [48/191], loss=5.0032
	step [49/191], loss=6.3338
	step [50/191], loss=5.3256
	step [51/191], loss=6.4115
	step [52/191], loss=6.8640
	step [53/191], loss=6.0910
	step [54/191], loss=5.8579
	step [55/191], loss=6.3478
	step [56/191], loss=5.8901
	step [57/191], loss=6.1121
	step [58/191], loss=5.1730
	step [59/191], loss=5.3594
	step [60/191], loss=7.2598
	step [61/191], loss=6.6256
	step [62/191], loss=6.2392
	step [63/191], loss=6.5406
	step [64/191], loss=6.5368
	step [65/191], loss=6.0219
	step [66/191], loss=6.5419
	step [67/191], loss=6.3514
	step [68/191], loss=6.2314
	step [69/191], loss=7.0604
	step [70/191], loss=6.5176
	step [71/191], loss=6.5410
	step [72/191], loss=5.4922
	step [73/191], loss=6.4437
	step [74/191], loss=5.9645
	step [75/191], loss=5.6451
	step [76/191], loss=6.9664
	step [77/191], loss=6.2973
	step [78/191], loss=6.2560
	step [79/191], loss=7.1112
	step [80/191], loss=6.8386
	step [81/191], loss=5.7105
	step [82/191], loss=5.2755
	step [83/191], loss=5.6012
	step [84/191], loss=6.1092
	step [85/191], loss=7.3051
	step [86/191], loss=5.9437
	step [87/191], loss=5.8244
	step [88/191], loss=6.4488
	step [89/191], loss=6.7215
	step [90/191], loss=5.9516
	step [91/191], loss=5.9336
	step [92/191], loss=7.0099
	step [93/191], loss=5.7756
	step [94/191], loss=5.4839
	step [95/191], loss=6.7993
	step [96/191], loss=6.5271
	step [97/191], loss=6.8900
	step [98/191], loss=6.7368
	step [99/191], loss=7.3894
	step [100/191], loss=6.3518
	step [101/191], loss=6.0477
	step [102/191], loss=4.9055
	step [103/191], loss=6.4951
	step [104/191], loss=7.7395
	step [105/191], loss=5.5030
	step [106/191], loss=5.4674
	step [107/191], loss=6.2302
	step [108/191], loss=5.3900
	step [109/191], loss=6.9496
	step [110/191], loss=5.8274
	step [111/191], loss=6.1246
	step [112/191], loss=7.5718
	step [113/191], loss=6.6855
	step [114/191], loss=5.5963
	step [115/191], loss=6.6080
	step [116/191], loss=6.5970
	step [117/191], loss=6.3946
	step [118/191], loss=5.7935
	step [119/191], loss=5.9523
	step [120/191], loss=5.4904
	step [121/191], loss=5.9141
	step [122/191], loss=7.4130
	step [123/191], loss=6.3723
	step [124/191], loss=6.4217
	step [125/191], loss=5.7563
	step [126/191], loss=5.7997
	step [127/191], loss=5.7690
	step [128/191], loss=6.5018
	step [129/191], loss=7.2166
	step [130/191], loss=5.8175
	step [131/191], loss=5.3728
	step [132/191], loss=6.4013
	step [133/191], loss=6.6710
	step [134/191], loss=7.4082
	step [135/191], loss=6.2198
	step [136/191], loss=7.6942
	step [137/191], loss=5.8314
	step [138/191], loss=6.5599
	step [139/191], loss=6.8724
	step [140/191], loss=5.8610
	step [141/191], loss=6.6983
	step [142/191], loss=5.9792
	step [143/191], loss=5.8779
	step [144/191], loss=7.1682
	step [145/191], loss=7.0890
	step [146/191], loss=6.6979
	step [147/191], loss=6.4433
	step [148/191], loss=6.1600
	step [149/191], loss=6.0830
	step [150/191], loss=6.1488
	step [151/191], loss=6.1229
	step [152/191], loss=5.9691
	step [153/191], loss=6.4192
	step [154/191], loss=5.5893
	step [155/191], loss=5.8261
	step [156/191], loss=5.1938
	step [157/191], loss=5.3608
	step [158/191], loss=7.6084
	step [159/191], loss=5.8120
	step [160/191], loss=6.5504
	step [161/191], loss=6.2353
	step [162/191], loss=7.0590
	step [163/191], loss=6.3901
	step [164/191], loss=6.7326
	step [165/191], loss=5.4478
	step [166/191], loss=6.4590
	step [167/191], loss=5.7275
	step [168/191], loss=6.9159
	step [169/191], loss=5.3476
	step [170/191], loss=6.4782
	step [171/191], loss=5.7952
	step [172/191], loss=6.8938
	step [173/191], loss=6.0768
	step [174/191], loss=5.6508
	step [175/191], loss=6.4482
	step [176/191], loss=6.4382
	step [177/191], loss=6.1891
	step [178/191], loss=6.2922
	step [179/191], loss=7.0748
	step [180/191], loss=5.2043
	step [181/191], loss=6.3498
	step [182/191], loss=5.8140
	step [183/191], loss=5.9509
	step [184/191], loss=6.1078
	step [185/191], loss=5.6561
	step [186/191], loss=6.7822
	step [187/191], loss=6.8634
	step [188/191], loss=5.7271
	step [189/191], loss=6.1308
	step [190/191], loss=6.5176
	step [191/191], loss=2.5711
	Evaluating
	loss=0.0259, precision=0.2541, recall=0.9916, f1=0.4046
Training epoch 27
	step [1/191], loss=5.9399
	step [2/191], loss=5.3410
	step [3/191], loss=5.9350
	step [4/191], loss=5.1883
	step [5/191], loss=5.7709
	step [6/191], loss=6.4343
	step [7/191], loss=5.2559
	step [8/191], loss=5.7208
	step [9/191], loss=7.4268
	step [10/191], loss=5.9112
	step [11/191], loss=5.9005
	step [12/191], loss=6.3445
	step [13/191], loss=5.4985
	step [14/191], loss=6.6235
	step [15/191], loss=5.8394
	step [16/191], loss=5.6476
	step [17/191], loss=5.4413
	step [18/191], loss=4.8540
	step [19/191], loss=6.1930
	step [20/191], loss=6.3839
	step [21/191], loss=7.0361
	step [22/191], loss=5.3261
	step [23/191], loss=5.7463
	step [24/191], loss=6.8885
	step [25/191], loss=6.6944
	step [26/191], loss=8.3595
	step [27/191], loss=7.0726
	step [28/191], loss=5.3315
	step [29/191], loss=6.7565
	step [30/191], loss=6.3850
	step [31/191], loss=5.5860
	step [32/191], loss=5.9780
	step [33/191], loss=6.7950
	step [34/191], loss=5.9866
	step [35/191], loss=6.4378
	step [36/191], loss=6.2521
	step [37/191], loss=5.7373
	step [38/191], loss=5.2667
	step [39/191], loss=5.7634
	step [40/191], loss=5.8231
	step [41/191], loss=5.3656
	step [42/191], loss=5.9306
	step [43/191], loss=6.1246
	step [44/191], loss=5.7276
	step [45/191], loss=5.8523
	step [46/191], loss=6.3225
	step [47/191], loss=5.8868
	step [48/191], loss=6.7228
	step [49/191], loss=6.4650
	step [50/191], loss=6.7208
	step [51/191], loss=6.3730
	step [52/191], loss=4.9545
	step [53/191], loss=5.8761
	step [54/191], loss=4.9898
	step [55/191], loss=5.9289
	step [56/191], loss=6.4233
	step [57/191], loss=6.7524
	step [58/191], loss=5.7567
	step [59/191], loss=6.1496
	step [60/191], loss=6.3029
	step [61/191], loss=5.7373
	step [62/191], loss=6.2882
	step [63/191], loss=5.2698
	step [64/191], loss=6.0067
	step [65/191], loss=6.2232
	step [66/191], loss=6.0059
	step [67/191], loss=6.1188
	step [68/191], loss=4.6430
	step [69/191], loss=6.1062
	step [70/191], loss=5.8934
	step [71/191], loss=5.9569
	step [72/191], loss=5.4752
	step [73/191], loss=6.5091
	step [74/191], loss=6.4767
	step [75/191], loss=6.1734
	step [76/191], loss=5.8521
	step [77/191], loss=6.7228
	step [78/191], loss=5.8428
	step [79/191], loss=6.9489
	step [80/191], loss=6.6644
	step [81/191], loss=5.7427
	step [82/191], loss=5.4666
	step [83/191], loss=5.6711
	step [84/191], loss=6.8021
	step [85/191], loss=6.6439
	step [86/191], loss=6.8025
	step [87/191], loss=5.5156
	step [88/191], loss=5.5203
	step [89/191], loss=7.0718
	step [90/191], loss=4.9611
	step [91/191], loss=5.8239
	step [92/191], loss=5.9771
	step [93/191], loss=6.0652
	step [94/191], loss=5.9211
	step [95/191], loss=5.6910
	step [96/191], loss=6.0034
	step [97/191], loss=6.5460
	step [98/191], loss=5.5070
	step [99/191], loss=5.7613
	step [100/191], loss=5.0732
	step [101/191], loss=5.6805
	step [102/191], loss=5.4769
	step [103/191], loss=6.3866
	step [104/191], loss=6.1653
	step [105/191], loss=6.4954
	step [106/191], loss=6.2080
	step [107/191], loss=5.6846
	step [108/191], loss=6.1070
	step [109/191], loss=5.4574
	step [110/191], loss=5.6128
	step [111/191], loss=6.1434
	step [112/191], loss=5.9877
	step [113/191], loss=6.7895
	step [114/191], loss=5.9963
	step [115/191], loss=5.4266
	step [116/191], loss=6.9732
	step [117/191], loss=5.9132
	step [118/191], loss=5.1986
	step [119/191], loss=6.1389
	step [120/191], loss=6.8708
	step [121/191], loss=5.5430
	step [122/191], loss=5.7866
	step [123/191], loss=6.4274
	step [124/191], loss=5.4220
	step [125/191], loss=6.7060
	step [126/191], loss=4.5808
	step [127/191], loss=5.6048
	step [128/191], loss=5.1182
	step [129/191], loss=6.0780
	step [130/191], loss=6.0747
	step [131/191], loss=5.3643
	step [132/191], loss=7.5181
	step [133/191], loss=6.5401
	step [134/191], loss=5.7972
	step [135/191], loss=5.6899
	step [136/191], loss=5.8768
	step [137/191], loss=5.9044
	step [138/191], loss=6.2910
	step [139/191], loss=5.8803
	step [140/191], loss=6.3654
	step [141/191], loss=5.8861
	step [142/191], loss=6.4865
	step [143/191], loss=7.1759
	step [144/191], loss=5.8936
	step [145/191], loss=5.8742
	step [146/191], loss=5.8495
	step [147/191], loss=6.4534
	step [148/191], loss=5.0890
	step [149/191], loss=4.7627
	step [150/191], loss=5.9547
	step [151/191], loss=7.0209
	step [152/191], loss=5.6745
	step [153/191], loss=6.4279
	step [154/191], loss=6.5317
	step [155/191], loss=6.0726
	step [156/191], loss=6.3712
	step [157/191], loss=5.2802
	step [158/191], loss=6.6489
	step [159/191], loss=6.1549
	step [160/191], loss=5.9137
	step [161/191], loss=5.9158
	step [162/191], loss=6.8171
	step [163/191], loss=6.7067
	step [164/191], loss=6.0677
	step [165/191], loss=6.1431
	step [166/191], loss=6.1403
	step [167/191], loss=5.7226
	step [168/191], loss=6.3018
	step [169/191], loss=4.9261
	step [170/191], loss=5.3001
	step [171/191], loss=6.3241
	step [172/191], loss=6.7364
	step [173/191], loss=6.3502
	step [174/191], loss=6.6553
	step [175/191], loss=6.6255
	step [176/191], loss=5.9791
	step [177/191], loss=6.7990
	step [178/191], loss=4.7606
	step [179/191], loss=5.8271
	step [180/191], loss=5.1016
	step [181/191], loss=5.5114
	step [182/191], loss=5.3330
	step [183/191], loss=5.3735
	step [184/191], loss=5.6544
	step [185/191], loss=4.6570
	step [186/191], loss=5.9716
	step [187/191], loss=5.3108
	step [188/191], loss=6.1521
	step [189/191], loss=6.0737
	step [190/191], loss=5.7676
	step [191/191], loss=3.2735
	Evaluating
	loss=0.0188, precision=0.3291, recall=0.9866, f1=0.4936
Training epoch 28
	step [1/191], loss=6.2546
	step [2/191], loss=6.8120
	step [3/191], loss=5.8610
	step [4/191], loss=6.9530
	step [5/191], loss=6.5098
	step [6/191], loss=5.6924
	step [7/191], loss=5.2202
	step [8/191], loss=5.9168
	step [9/191], loss=4.3848
	step [10/191], loss=6.2578
	step [11/191], loss=6.1278
	step [12/191], loss=5.2109
	step [13/191], loss=5.8348
	step [14/191], loss=5.9882
	step [15/191], loss=5.8377
	step [16/191], loss=5.3564
	step [17/191], loss=5.9593
	step [18/191], loss=6.8156
	step [19/191], loss=6.5078
	step [20/191], loss=6.3077
	step [21/191], loss=5.5112
	step [22/191], loss=5.5847
	step [23/191], loss=6.0491
	step [24/191], loss=5.9178
	step [25/191], loss=6.0866
	step [26/191], loss=5.0559
	step [27/191], loss=6.4924
	step [28/191], loss=5.2874
	step [29/191], loss=6.8830
	step [30/191], loss=5.4335
	step [31/191], loss=6.1091
	step [32/191], loss=6.5389
	step [33/191], loss=6.9032
	step [34/191], loss=4.2495
	step [35/191], loss=6.4203
	step [36/191], loss=6.8621
	step [37/191], loss=5.7412
	step [38/191], loss=5.2931
	step [39/191], loss=5.4370
	step [40/191], loss=6.0287
	step [41/191], loss=5.5263
	step [42/191], loss=5.0052
	step [43/191], loss=4.8268
	step [44/191], loss=5.8876
	step [45/191], loss=5.8664
	step [46/191], loss=5.7970
	step [47/191], loss=6.6805
	step [48/191], loss=5.7899
	step [49/191], loss=5.7984
	step [50/191], loss=5.9377
	step [51/191], loss=6.0865
	step [52/191], loss=5.5519
	step [53/191], loss=6.4697
	step [54/191], loss=5.5021
	step [55/191], loss=5.6085
	step [56/191], loss=6.1394
	step [57/191], loss=5.8684
	step [58/191], loss=5.1846
	step [59/191], loss=5.4202
	step [60/191], loss=5.6091
	step [61/191], loss=5.9954
	step [62/191], loss=6.0581
	step [63/191], loss=5.0297
	step [64/191], loss=5.6268
	step [65/191], loss=5.2036
	step [66/191], loss=5.1252
	step [67/191], loss=4.7704
	step [68/191], loss=5.6730
	step [69/191], loss=5.9617
	step [70/191], loss=5.6254
	step [71/191], loss=6.3767
	step [72/191], loss=6.5628
	step [73/191], loss=5.4636
	step [74/191], loss=5.6921
	step [75/191], loss=6.2775
	step [76/191], loss=6.3148
	step [77/191], loss=6.1775
	step [78/191], loss=5.9839
	step [79/191], loss=6.2722
	step [80/191], loss=5.0881
	step [81/191], loss=6.0824
	step [82/191], loss=6.7894
	step [83/191], loss=5.7637
	step [84/191], loss=5.4530
	step [85/191], loss=5.2988
	step [86/191], loss=6.0499
	step [87/191], loss=5.7716
	step [88/191], loss=5.8289
	step [89/191], loss=7.0671
	step [90/191], loss=6.2435
	step [91/191], loss=6.7161
	step [92/191], loss=5.4030
	step [93/191], loss=4.9752
	step [94/191], loss=4.7250
	step [95/191], loss=5.3363
	step [96/191], loss=6.2119
	step [97/191], loss=6.6191
	step [98/191], loss=6.4909
	step [99/191], loss=6.9121
	step [100/191], loss=6.9393
	step [101/191], loss=6.4481
	step [102/191], loss=5.9199
	step [103/191], loss=5.9973
	step [104/191], loss=6.3383
	step [105/191], loss=4.8256
	step [106/191], loss=5.8141
	step [107/191], loss=5.5051
	step [108/191], loss=5.8247
	step [109/191], loss=4.9785
	step [110/191], loss=6.0810
	step [111/191], loss=6.4253
	step [112/191], loss=6.6796
	step [113/191], loss=6.1252
	step [114/191], loss=6.8294
	step [115/191], loss=6.6453
	step [116/191], loss=5.3694
	step [117/191], loss=6.3752
	step [118/191], loss=4.9365
	step [119/191], loss=5.8697
	step [120/191], loss=6.5829
	step [121/191], loss=6.5457
	step [122/191], loss=6.1682
	step [123/191], loss=5.4179
	step [124/191], loss=7.5063
	step [125/191], loss=5.9750
	step [126/191], loss=6.4850
	step [127/191], loss=7.2581
	step [128/191], loss=5.6125
	step [129/191], loss=5.8201
	step [130/191], loss=5.0741
	step [131/191], loss=5.4915
	step [132/191], loss=7.0091
	step [133/191], loss=6.1062
	step [134/191], loss=6.1467
	step [135/191], loss=6.1089
	step [136/191], loss=5.7348
	step [137/191], loss=5.8232
	step [138/191], loss=6.2318
	step [139/191], loss=6.0019
	step [140/191], loss=6.7392
	step [141/191], loss=6.0318
	step [142/191], loss=5.7882
	step [143/191], loss=5.9743
	step [144/191], loss=6.3887
	step [145/191], loss=6.2037
	step [146/191], loss=5.1585
	step [147/191], loss=6.5897
	step [148/191], loss=5.8544
	step [149/191], loss=5.2725
	step [150/191], loss=5.7470
	step [151/191], loss=6.1018
	step [152/191], loss=5.9946
	step [153/191], loss=6.5360
	step [154/191], loss=6.5290
	step [155/191], loss=6.8692
	step [156/191], loss=5.6086
	step [157/191], loss=5.5775
	step [158/191], loss=5.7013
	step [159/191], loss=5.0482
	step [160/191], loss=5.8315
	step [161/191], loss=5.5700
	step [162/191], loss=5.9523
	step [163/191], loss=5.8434
	step [164/191], loss=5.9870
	step [165/191], loss=5.9888
	step [166/191], loss=6.9890
	step [167/191], loss=5.8929
	step [168/191], loss=5.9654
	step [169/191], loss=6.0625
	step [170/191], loss=6.4956
	step [171/191], loss=6.2274
	step [172/191], loss=6.8635
	step [173/191], loss=5.4903
	step [174/191], loss=5.4538
	step [175/191], loss=5.9312
	step [176/191], loss=5.8916
	step [177/191], loss=5.0511
	step [178/191], loss=5.4876
	step [179/191], loss=5.3787
	step [180/191], loss=6.2105
	step [181/191], loss=6.0690
	step [182/191], loss=6.1676
	step [183/191], loss=7.1146
	step [184/191], loss=4.9493
	step [185/191], loss=5.6017
	step [186/191], loss=5.5908
	step [187/191], loss=5.2242
	step [188/191], loss=5.1027
	step [189/191], loss=5.4127
	step [190/191], loss=4.8142
	step [191/191], loss=2.9072
	Evaluating
	loss=0.0148, precision=0.3676, recall=0.9815, f1=0.5349
saving model as: 1_saved_model.pth
Training epoch 29
	step [1/191], loss=5.2041
	step [2/191], loss=5.8925
	step [3/191], loss=5.4997
	step [4/191], loss=5.7957
	step [5/191], loss=6.2750
	step [6/191], loss=5.3709
	step [7/191], loss=5.1039
	step [8/191], loss=5.9182
	step [9/191], loss=6.8620
	step [10/191], loss=5.0427
	step [11/191], loss=5.5690
	step [12/191], loss=6.0262
	step [13/191], loss=5.8521
	step [14/191], loss=6.5650
	step [15/191], loss=6.0171
	step [16/191], loss=6.9529
	step [17/191], loss=5.3101
	step [18/191], loss=5.2710
	step [19/191], loss=5.0161
	step [20/191], loss=6.6575
	step [21/191], loss=6.1728
	step [22/191], loss=5.0329
	step [23/191], loss=6.0176
	step [24/191], loss=5.6085
	step [25/191], loss=5.3774
	step [26/191], loss=6.1325
	step [27/191], loss=5.5412
	step [28/191], loss=6.8150
	step [29/191], loss=7.0398
	step [30/191], loss=5.7774
	step [31/191], loss=6.4915
	step [32/191], loss=6.3004
	step [33/191], loss=4.5471
	step [34/191], loss=5.4436
	step [35/191], loss=6.3221
	step [36/191], loss=6.1569
	step [37/191], loss=5.7620
	step [38/191], loss=5.1102
	step [39/191], loss=5.6002
	step [40/191], loss=5.3807
	step [41/191], loss=6.0886
	step [42/191], loss=5.6492
	step [43/191], loss=5.2052
	step [44/191], loss=5.5554
	step [45/191], loss=6.5281
	step [46/191], loss=6.9537
	step [47/191], loss=6.2216
	step [48/191], loss=6.3920
	step [49/191], loss=5.4376
	step [50/191], loss=5.6837
	step [51/191], loss=5.8732
	step [52/191], loss=5.6927
	step [53/191], loss=5.9029
	step [54/191], loss=5.3251
	step [55/191], loss=5.6043
	step [56/191], loss=5.7427
	step [57/191], loss=5.0415
	step [58/191], loss=5.7620
	step [59/191], loss=5.6745
	step [60/191], loss=5.7415
	step [61/191], loss=5.6726
	step [62/191], loss=5.5504
	step [63/191], loss=5.6601
	step [64/191], loss=5.3649
	step [65/191], loss=5.8291
	step [66/191], loss=5.7403
	step [67/191], loss=6.9061
	step [68/191], loss=5.8318
	step [69/191], loss=6.8372
	step [70/191], loss=6.1025
	step [71/191], loss=6.1218
	step [72/191], loss=5.7462
	step [73/191], loss=6.0728
	step [74/191], loss=5.4716
	step [75/191], loss=5.5626
	step [76/191], loss=5.7210
	step [77/191], loss=5.9640
	step [78/191], loss=5.2459
	step [79/191], loss=6.2441
	step [80/191], loss=4.7469
	step [81/191], loss=4.8756
	step [82/191], loss=5.6828
	step [83/191], loss=5.4562
	step [84/191], loss=5.6217
	step [85/191], loss=5.0175
	step [86/191], loss=6.2074
	step [87/191], loss=5.9548
	step [88/191], loss=6.5309
	step [89/191], loss=5.2905
	step [90/191], loss=5.3608
	step [91/191], loss=6.0110
	step [92/191], loss=5.8172
	step [93/191], loss=5.7255
	step [94/191], loss=5.7173
	step [95/191], loss=5.5342
	step [96/191], loss=4.7612
	step [97/191], loss=5.4389
	step [98/191], loss=5.9315
	step [99/191], loss=5.0902
	step [100/191], loss=4.5794
	step [101/191], loss=5.8040
	step [102/191], loss=4.5464
	step [103/191], loss=5.6543
	step [104/191], loss=5.1689
	step [105/191], loss=5.4705
	step [106/191], loss=7.3559
	step [107/191], loss=6.4096
	step [108/191], loss=5.2582
	step [109/191], loss=6.2226
	step [110/191], loss=5.5858
	step [111/191], loss=6.3635
	step [112/191], loss=7.2412
	step [113/191], loss=5.7382
	step [114/191], loss=5.5577
	step [115/191], loss=5.9128
	step [116/191], loss=4.7988
	step [117/191], loss=5.4578
	step [118/191], loss=5.6745
	step [119/191], loss=5.3810
	step [120/191], loss=5.8399
	step [121/191], loss=6.4937
	step [122/191], loss=6.0864
	step [123/191], loss=4.9704
	step [124/191], loss=5.1262
	step [125/191], loss=6.1548
	step [126/191], loss=6.2050
	step [127/191], loss=5.9664
	step [128/191], loss=6.0198
	step [129/191], loss=6.0000
	step [130/191], loss=7.5016
	step [131/191], loss=5.6727
	step [132/191], loss=5.7118
	step [133/191], loss=5.0090
	step [134/191], loss=5.9976
	step [135/191], loss=6.3647
	step [136/191], loss=5.6330
	step [137/191], loss=5.9686
	step [138/191], loss=5.3318
	step [139/191], loss=5.5248
	step [140/191], loss=6.2437
	step [141/191], loss=5.7165
	step [142/191], loss=5.9960
	step [143/191], loss=6.1076
	step [144/191], loss=5.5772
	step [145/191], loss=5.2083
	step [146/191], loss=5.9109
	step [147/191], loss=5.9784
	step [148/191], loss=5.3383
	step [149/191], loss=6.4901
	step [150/191], loss=5.1522
	step [151/191], loss=4.7623
	step [152/191], loss=6.5097
	step [153/191], loss=5.2663
	step [154/191], loss=6.1594
	step [155/191], loss=5.9446
	step [156/191], loss=6.6514
	step [157/191], loss=5.6256
	step [158/191], loss=4.9415
	step [159/191], loss=5.5515
	step [160/191], loss=5.7030
	step [161/191], loss=5.8857
	step [162/191], loss=5.9888
	step [163/191], loss=5.4995
	step [164/191], loss=6.6466
	step [165/191], loss=5.8213
	step [166/191], loss=5.4579
	step [167/191], loss=5.9300
	step [168/191], loss=5.3938
	step [169/191], loss=5.1394
	step [170/191], loss=5.8759
	step [171/191], loss=5.7601
	step [172/191], loss=5.9434
	step [173/191], loss=5.3333
	step [174/191], loss=4.8647
	step [175/191], loss=4.8592
	step [176/191], loss=5.6880
	step [177/191], loss=6.3875
	step [178/191], loss=4.4273
	step [179/191], loss=5.1755
	step [180/191], loss=5.8086
	step [181/191], loss=5.5494
	step [182/191], loss=6.4600
	step [183/191], loss=4.4721
	step [184/191], loss=5.3996
	step [185/191], loss=5.0847
	step [186/191], loss=5.6985
	step [187/191], loss=5.9871
	step [188/191], loss=6.0809
	step [189/191], loss=6.0112
	step [190/191], loss=5.2381
	step [191/191], loss=2.2746
	Evaluating
	loss=0.0192, precision=0.3249, recall=0.9873, f1=0.4890
Training epoch 30
	step [1/191], loss=5.9347
	step [2/191], loss=5.9209
	step [3/191], loss=6.3867
	step [4/191], loss=5.1876
	step [5/191], loss=5.5330
	step [6/191], loss=5.7237
	step [7/191], loss=5.4125
	step [8/191], loss=6.4737
	step [9/191], loss=5.1809
	step [10/191], loss=4.5869
	step [11/191], loss=5.3243
	step [12/191], loss=5.6061
	step [13/191], loss=5.9093
	step [14/191], loss=5.8343
	step [15/191], loss=5.1359
	step [16/191], loss=4.3050
	step [17/191], loss=5.7797
	step [18/191], loss=5.4310
	step [19/191], loss=5.5754
	step [20/191], loss=5.4352
	step [21/191], loss=4.6524
	step [22/191], loss=5.0095
	step [23/191], loss=5.5897
	step [24/191], loss=4.9222
	step [25/191], loss=5.5845
	step [26/191], loss=6.2367
	step [27/191], loss=5.9756
	step [28/191], loss=5.1020
	step [29/191], loss=5.6234
	step [30/191], loss=6.1385
	step [31/191], loss=5.7562
	step [32/191], loss=5.4851
	step [33/191], loss=5.8714
	step [34/191], loss=5.0001
	step [35/191], loss=4.2365
	step [36/191], loss=4.5762
	step [37/191], loss=5.5027
	step [38/191], loss=5.7321
	step [39/191], loss=5.5433
	step [40/191], loss=5.7379
	step [41/191], loss=5.5683
	step [42/191], loss=5.0292
	step [43/191], loss=5.7963
	step [44/191], loss=4.8484
	step [45/191], loss=5.2369
	step [46/191], loss=4.9248
	step [47/191], loss=6.4832
	step [48/191], loss=5.2037
	step [49/191], loss=5.5094
	step [50/191], loss=6.0287
	step [51/191], loss=6.0153
	step [52/191], loss=6.0639
	step [53/191], loss=5.9294
	step [54/191], loss=5.8108
	step [55/191], loss=5.0272
	step [56/191], loss=5.5050
	step [57/191], loss=4.9355
	step [58/191], loss=5.6448
	step [59/191], loss=5.2480
	step [60/191], loss=5.1584
	step [61/191], loss=6.1673
	step [62/191], loss=5.7461
	step [63/191], loss=5.5286
	step [64/191], loss=5.6537
	step [65/191], loss=6.2874
	step [66/191], loss=6.1059
	step [67/191], loss=4.9634
	step [68/191], loss=6.4126
	step [69/191], loss=5.5134
	step [70/191], loss=6.0554
	step [71/191], loss=6.0190
	step [72/191], loss=5.3618
	step [73/191], loss=5.5739
	step [74/191], loss=5.7406
	step [75/191], loss=5.6716
	step [76/191], loss=6.0379
	step [77/191], loss=5.3279
	step [78/191], loss=5.6517
	step [79/191], loss=5.4316
	step [80/191], loss=5.6824
	step [81/191], loss=6.1263
	step [82/191], loss=5.5927
	step [83/191], loss=6.4574
	step [84/191], loss=6.3723
	step [85/191], loss=5.4989
	step [86/191], loss=5.8832
	step [87/191], loss=5.4985
	step [88/191], loss=5.6971
	step [89/191], loss=5.2937
	step [90/191], loss=5.9888
	step [91/191], loss=5.7521
	step [92/191], loss=5.7699
	step [93/191], loss=5.4308
	step [94/191], loss=6.2258
	step [95/191], loss=5.7151
	step [96/191], loss=5.2235
	step [97/191], loss=4.9363
	step [98/191], loss=5.0147
	step [99/191], loss=5.7990
	step [100/191], loss=6.5561
	step [101/191], loss=5.0274
	step [102/191], loss=5.1341
	step [103/191], loss=4.7652
	step [104/191], loss=5.8054
	step [105/191], loss=5.6120
	step [106/191], loss=5.2697
	step [107/191], loss=5.4344
	step [108/191], loss=5.2685
	step [109/191], loss=5.3417
	step [110/191], loss=4.8896
	step [111/191], loss=5.5081
	step [112/191], loss=5.9366
	step [113/191], loss=4.8634
	step [114/191], loss=5.7150
	step [115/191], loss=6.0958
	step [116/191], loss=5.2193
	step [117/191], loss=5.4178
	step [118/191], loss=5.8107
	step [119/191], loss=6.1307
	step [120/191], loss=5.1032
	step [121/191], loss=5.2594
	step [122/191], loss=5.0815
	step [123/191], loss=6.2445
	step [124/191], loss=5.3926
	step [125/191], loss=5.8022
	step [126/191], loss=5.5553
	step [127/191], loss=5.7030
	step [128/191], loss=5.1094
	step [129/191], loss=5.6849
	step [130/191], loss=5.4828
	step [131/191], loss=5.4576
	step [132/191], loss=4.5095
	step [133/191], loss=4.9106
	step [134/191], loss=6.1053
	step [135/191], loss=5.9929
	step [136/191], loss=5.2485
	step [137/191], loss=5.7380
	step [138/191], loss=6.1484
	step [139/191], loss=6.4510
	step [140/191], loss=4.2766
	step [141/191], loss=5.5492
	step [142/191], loss=6.0062
	step [143/191], loss=5.6206
	step [144/191], loss=6.1534
	step [145/191], loss=5.3758
	step [146/191], loss=5.7496
	step [147/191], loss=5.2452
	step [148/191], loss=4.8638
	step [149/191], loss=5.2180
	step [150/191], loss=5.5008
	step [151/191], loss=5.4978
	step [152/191], loss=4.9672
	step [153/191], loss=4.8977
	step [154/191], loss=5.4879
	step [155/191], loss=5.2093
	step [156/191], loss=5.2013
	step [157/191], loss=6.1112
	step [158/191], loss=5.1426
	step [159/191], loss=5.1971
	step [160/191], loss=4.9401
	step [161/191], loss=5.5265
	step [162/191], loss=5.5209
	step [163/191], loss=5.5034
	step [164/191], loss=6.2105
	step [165/191], loss=5.5699
	step [166/191], loss=5.2293
	step [167/191], loss=5.3068
	step [168/191], loss=5.8471
	step [169/191], loss=4.8284
	step [170/191], loss=5.5970
	step [171/191], loss=6.6212
	step [172/191], loss=5.4439
	step [173/191], loss=6.4070
	step [174/191], loss=5.5520
	step [175/191], loss=4.9569
	step [176/191], loss=4.6347
	step [177/191], loss=6.8675
	step [178/191], loss=5.1031
	step [179/191], loss=6.0340
	step [180/191], loss=4.3296
	step [181/191], loss=5.5253
	step [182/191], loss=5.2377
	step [183/191], loss=5.2510
	step [184/191], loss=6.0063
	step [185/191], loss=6.4303
	step [186/191], loss=5.6285
	step [187/191], loss=5.2325
	step [188/191], loss=5.2345
	step [189/191], loss=5.7842
	step [190/191], loss=5.1049
	step [191/191], loss=2.4667
	Evaluating
	loss=0.0170, precision=0.3497, recall=0.9851, f1=0.5161
Training finished
best_f1: 0.5349122225162035
directing: Y rim_enhanced: False test_id 1
removed wrong scan: weights_Y_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_171_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_299_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_87_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_241_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_224_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_336_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_350_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_244_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_373_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_308_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_285_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_261_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_243_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_274_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_188_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_218_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_387_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_360_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_372_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_316_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_320_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_160_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_253_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_254_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_204_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_349_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_149_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_283_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_300_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_333_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_221_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_159_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_215_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_363_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_329_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_368_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_254_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_282_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_170_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_211_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_260_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_258_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_231_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_222_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_313_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_282_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_259_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_263_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_177_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_190_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_281_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_187_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_261_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_245_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_315_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_286_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_184_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_312_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_302_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_276_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_157_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_236_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_331_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_214_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_212_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_356_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_167_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_383_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_255_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_224_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_264_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_343_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_375_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_168_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_318_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_367_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_220_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_163_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_255_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_351_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_305_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_330_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_181_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_229_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_309_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_225_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_164_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_242_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_233_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_346_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_326_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_230_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_371_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_275_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_377_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_273_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_197_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_151_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_199_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_332_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_239_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_307_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_380_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_321_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_265_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_150_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_201_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_291_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_353_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_246_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_322_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_310_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_352_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_262_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_297_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_338_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_319_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_179_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_236_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_239_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_215_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_218_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_253_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_293_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_303_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_158_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_203_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_381_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_317_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_226_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_189_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_270_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_294_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_354_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_359_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_266_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_292_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_314_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_218_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_238_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_200_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_262_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_229_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_169_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_271_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_147_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_288_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_244_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_347_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_248_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_323_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_335_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_278_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_233_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_306_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_209_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_365_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_259_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_378_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_298_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_256_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_276_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_193_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_384_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_208_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_237_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_221_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_258_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_339_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_232_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_325_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_238_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_290_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_231_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_369_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_280_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_337_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_216_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_304_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_250_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_194_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_301_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_324_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_340_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_162_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_295_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_366_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_370_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_362_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_379_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_269_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_180_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_289_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_296_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_183_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_271_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_226_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_237_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_341_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_161_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_358_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_254_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_235_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_334_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_376_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_287_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_252_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_205_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_348_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_249_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_251_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_186_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_257_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_80_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_243_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_279_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_268_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_230_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_206_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_165_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_207_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_227_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_260_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_155_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_225_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_311_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_327_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_344_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_267_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_256_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_328_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_153_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_257_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_152_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_176_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_342_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_222_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_263_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_385_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_156_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_386_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_196_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_243_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_195_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_277_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_252_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_232_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_220_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_388_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_210_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_241_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_265_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_146_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_266_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_284_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_355_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_174_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_225_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_345_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_272_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_205_xwqg-B00034_2020-04-03.npy
# all image files: 20333 # all weight files in weight_dir: 15614 # image files with weight 15579
removed wrong scan: weights_Y_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_171_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_299_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_87_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_241_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_224_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_336_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_350_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_244_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_373_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_308_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_285_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_261_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_243_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_274_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_188_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_218_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_387_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_360_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_372_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_316_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_320_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_160_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_253_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_254_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_204_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_349_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_149_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_283_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_300_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_333_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_221_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_159_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_215_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_363_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_329_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_368_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_254_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_282_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_170_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_211_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_260_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_258_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_231_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_222_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_313_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_282_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_259_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_263_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_177_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_190_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_281_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_187_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_261_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_245_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_315_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_286_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_184_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_312_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_302_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_276_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_157_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_236_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_331_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_214_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_212_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_356_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_167_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_383_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_255_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_224_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_264_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_343_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_375_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_168_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_318_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_367_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_220_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_163_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_255_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_351_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_305_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_330_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_181_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_229_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_309_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_225_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_164_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_242_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_233_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_346_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_326_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_230_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_371_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_275_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_377_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_273_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_197_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_151_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_199_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_332_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_239_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_307_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_380_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_321_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_265_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_150_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_201_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_291_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_353_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_246_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_322_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_310_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_352_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_262_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_297_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_338_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_319_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_179_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_236_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_239_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_215_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_218_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_253_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_293_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_303_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_158_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_203_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_381_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_317_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_226_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_189_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_270_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_294_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_354_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_359_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_266_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_292_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_314_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_218_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_238_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_200_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_262_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_229_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_169_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_271_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_147_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_288_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_244_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_347_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_248_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_323_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_335_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_278_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_233_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_306_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_209_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_365_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_259_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_378_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_298_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_256_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_276_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_193_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_384_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_208_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_237_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_221_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_258_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_339_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_232_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_325_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_238_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_290_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_231_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_369_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_280_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_337_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_216_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_304_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_250_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_194_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_301_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_324_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_340_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_162_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_295_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_366_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_370_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_362_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_379_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_269_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_180_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_289_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_296_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_183_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_271_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_226_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_237_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_341_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_161_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_358_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_254_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_235_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_334_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_376_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_287_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_252_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_205_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_348_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_249_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_251_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_186_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_257_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_80_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_243_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_279_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_268_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_230_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_206_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_165_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_207_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_227_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_260_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_155_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_225_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_311_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_327_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_344_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_267_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_256_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_328_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_153_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_257_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_152_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_176_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_342_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_222_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_263_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_385_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_156_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_386_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_196_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_243_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_195_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_277_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_252_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_232_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_220_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_388_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_210_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_241_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_265_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_146_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_266_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_284_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_355_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_174_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_225_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_345_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_272_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_205_xwqg-B00034_2020-04-03.npy
# all image files: 20333 # all weight files in weight_dir: 4469 # image files with weight 4451
train: /ibex/scratch/projects/c2052/air_tube_seg/training_samples_enhanced/Y 15579
Using 4 GPUs
Going to train epochs [1-30]
Training epoch 1
	step [1/325], loss=225.2248
	step [2/325], loss=199.0883
	step [3/325], loss=191.4377
	step [4/325], loss=186.2304
	step [5/325], loss=183.3751
	step [6/325], loss=180.0375
	step [7/325], loss=176.8522
	step [8/325], loss=172.6010
	step [9/325], loss=171.5904
	step [10/325], loss=169.4735
	step [11/325], loss=168.2986
	step [12/325], loss=165.4395
	step [13/325], loss=160.8622
	step [14/325], loss=160.3040
	step [15/325], loss=158.0403
	step [16/325], loss=156.5207
	step [17/325], loss=152.4939
	step [18/325], loss=151.8551
	step [19/325], loss=151.3121
	step [20/325], loss=148.3865
	step [21/325], loss=146.3526
	step [22/325], loss=143.9232
	step [23/325], loss=143.9015
	step [24/325], loss=141.2365
	step [25/325], loss=142.0100
	step [26/325], loss=138.2315
	step [27/325], loss=136.6292
	step [28/325], loss=135.4115
	step [29/325], loss=135.8540
	step [30/325], loss=130.8870
	step [31/325], loss=131.0499
	step [32/325], loss=131.2195
	step [33/325], loss=129.5940
	step [34/325], loss=128.0333
	step [35/325], loss=127.5632
	step [36/325], loss=127.0252
	step [37/325], loss=125.6149
	step [38/325], loss=125.0085
	step [39/325], loss=123.0343
	step [40/325], loss=119.2920
	step [41/325], loss=119.7684
	step [42/325], loss=118.5887
	step [43/325], loss=118.2479
	step [44/325], loss=117.3617
	step [45/325], loss=113.2654
	step [46/325], loss=113.5684
	step [47/325], loss=113.0956
	step [48/325], loss=112.4249
	step [49/325], loss=111.1421
	step [50/325], loss=110.9385
	step [51/325], loss=109.5045
	step [52/325], loss=107.7682
	step [53/325], loss=107.7206
	step [54/325], loss=107.6110
	step [55/325], loss=106.0966
	step [56/325], loss=107.0406
	step [57/325], loss=103.7635
	step [58/325], loss=103.6948
	step [59/325], loss=104.7455
	step [60/325], loss=101.2065
	step [61/325], loss=103.0259
	step [62/325], loss=100.1027
	step [63/325], loss=103.0370
	step [64/325], loss=100.0971
	step [65/325], loss=100.1296
	step [66/325], loss=99.7446
	step [67/325], loss=97.8700
	step [68/325], loss=97.5948
	step [69/325], loss=97.9268
	step [70/325], loss=97.2760
	step [71/325], loss=95.7202
	step [72/325], loss=96.4536
	step [73/325], loss=96.9674
	step [74/325], loss=95.7326
	step [75/325], loss=96.0300
	step [76/325], loss=93.9090
	step [77/325], loss=92.6349
	step [78/325], loss=93.8160
	step [79/325], loss=94.7191
	step [80/325], loss=93.6111
	step [81/325], loss=92.4871
	step [82/325], loss=92.3128
	step [83/325], loss=92.7482
	step [84/325], loss=92.8056
	step [85/325], loss=92.9132
	step [86/325], loss=91.8247
	step [87/325], loss=91.7380
	step [88/325], loss=91.4038
	step [89/325], loss=91.0997
	step [90/325], loss=90.6916
	step [91/325], loss=90.7954
	step [92/325], loss=89.3485
	step [93/325], loss=89.2958
	step [94/325], loss=89.3619
	step [95/325], loss=90.3102
	step [96/325], loss=88.3427
	step [97/325], loss=88.4500
	step [98/325], loss=88.1130
	step [99/325], loss=89.3101
	step [100/325], loss=87.4812
	step [101/325], loss=88.4931
	step [102/325], loss=87.8811
	step [103/325], loss=85.7997
	step [104/325], loss=87.6639
	step [105/325], loss=88.0952
	step [106/325], loss=86.0106
	step [107/325], loss=87.5655
	step [108/325], loss=87.1131
	step [109/325], loss=85.6870
	step [110/325], loss=88.1980
	step [111/325], loss=87.9193
	step [112/325], loss=86.8798
	step [113/325], loss=85.2526
	step [114/325], loss=85.1084
	step [115/325], loss=86.2474
	step [116/325], loss=87.1215
	step [117/325], loss=84.3500
	step [118/325], loss=86.8288
	step [119/325], loss=86.0256
	step [120/325], loss=85.3993
	step [121/325], loss=86.7968
	step [122/325], loss=84.7258
	step [123/325], loss=83.2180
	step [124/325], loss=84.5282
	step [125/325], loss=86.7355
	step [126/325], loss=83.8598
	step [127/325], loss=85.2292
	step [128/325], loss=83.7707
	step [129/325], loss=83.6397
	step [130/325], loss=84.8873
	step [131/325], loss=83.3145
	step [132/325], loss=83.5162
	step [133/325], loss=83.0788
	step [134/325], loss=82.1712
	step [135/325], loss=81.3011
	step [136/325], loss=83.1167
	step [137/325], loss=81.5994
	step [138/325], loss=82.0005
	step [139/325], loss=80.5075
	step [140/325], loss=81.4010
	step [141/325], loss=82.9274
	step [142/325], loss=81.7969
	step [143/325], loss=81.4629
	step [144/325], loss=82.0931
	step [145/325], loss=81.2534
	step [146/325], loss=80.7924
	step [147/325], loss=80.2407
	step [148/325], loss=81.1808
	step [149/325], loss=79.2142
	step [150/325], loss=80.5942
	step [151/325], loss=78.5427
	step [152/325], loss=80.5016
	step [153/325], loss=80.3160
	step [154/325], loss=80.4345
	step [155/325], loss=77.8532
	step [156/325], loss=80.5581
	step [157/325], loss=79.4873
	step [158/325], loss=78.6658
	step [159/325], loss=78.6275
	step [160/325], loss=79.0776
	step [161/325], loss=79.2125
	step [162/325], loss=78.6287
	step [163/325], loss=77.8012
	step [164/325], loss=77.7947
	step [165/325], loss=78.6464
	step [166/325], loss=77.3728
	step [167/325], loss=78.3067
	step [168/325], loss=77.1727
	step [169/325], loss=78.4114
	step [170/325], loss=76.3493
	step [171/325], loss=76.0156
	step [172/325], loss=76.6270
	step [173/325], loss=77.0597
	step [174/325], loss=76.6957
	step [175/325], loss=77.5288
	step [176/325], loss=77.5904
	step [177/325], loss=76.7012
	step [178/325], loss=76.6796
	step [179/325], loss=76.5939
	step [180/325], loss=76.3957
	step [181/325], loss=75.4592
	step [182/325], loss=75.5702
	step [183/325], loss=75.6406
	step [184/325], loss=76.2153
	step [185/325], loss=74.6423
	step [186/325], loss=74.4996
	step [187/325], loss=74.6089
	step [188/325], loss=75.7784
	step [189/325], loss=76.2717
	step [190/325], loss=76.5947
	step [191/325], loss=75.5241
	step [192/325], loss=74.2124
	step [193/325], loss=74.2521
	step [194/325], loss=74.7490
	step [195/325], loss=74.3619
	step [196/325], loss=74.4293
	step [197/325], loss=75.2670
	step [198/325], loss=74.2428
	step [199/325], loss=74.2091
	step [200/325], loss=73.5121
	step [201/325], loss=73.2449
	step [202/325], loss=73.0952
	step [203/325], loss=72.9443
	step [204/325], loss=72.7979
	step [205/325], loss=73.5174
	step [206/325], loss=72.5702
	step [207/325], loss=72.5449
	step [208/325], loss=73.3805
	step [209/325], loss=72.6535
	step [210/325], loss=71.9984
	step [211/325], loss=73.3700
	step [212/325], loss=72.5179
	step [213/325], loss=72.5447
	step [214/325], loss=72.6772
	step [215/325], loss=72.5919
	step [216/325], loss=71.5628
	step [217/325], loss=71.9757
	step [218/325], loss=71.9024
	step [219/325], loss=73.1094
	step [220/325], loss=73.0356
	step [221/325], loss=71.8078
	step [222/325], loss=70.4385
	step [223/325], loss=70.7001
	step [224/325], loss=71.7051
	step [225/325], loss=72.0336
	step [226/325], loss=70.4353
	step [227/325], loss=71.6000
	step [228/325], loss=70.4798
	step [229/325], loss=71.9153
	step [230/325], loss=71.1955
	step [231/325], loss=70.2713
	step [232/325], loss=70.2822
	step [233/325], loss=70.8380
	step [234/325], loss=71.0608
	step [235/325], loss=68.4478
	step [236/325], loss=68.7157
	step [237/325], loss=69.7772
	step [238/325], loss=69.4820
	step [239/325], loss=70.9746
	step [240/325], loss=69.6638
	step [241/325], loss=69.5331
	step [242/325], loss=69.1580
	step [243/325], loss=67.8722
	step [244/325], loss=69.6552
	step [245/325], loss=69.4794
	step [246/325], loss=68.6472
	step [247/325], loss=70.2365
	step [248/325], loss=68.6896
	step [249/325], loss=69.2315
	step [250/325], loss=67.3373
	step [251/325], loss=69.7519
	step [252/325], loss=67.9899
	step [253/325], loss=68.4729
	step [254/325], loss=66.4143
	step [255/325], loss=68.6674
	step [256/325], loss=66.8810
	step [257/325], loss=68.0704
	step [258/325], loss=69.0130
	step [259/325], loss=67.4788
	step [260/325], loss=66.8261
	step [261/325], loss=68.1302
	step [262/325], loss=66.5146
	step [263/325], loss=67.3193
	step [264/325], loss=66.6361
	step [265/325], loss=68.3450
	step [266/325], loss=66.3583
	step [267/325], loss=67.9508
	step [268/325], loss=67.2345
	step [269/325], loss=66.8246
	step [270/325], loss=65.8303
	step [271/325], loss=65.7962
	step [272/325], loss=65.7213
	step [273/325], loss=67.6508
	step [274/325], loss=66.2672
	step [275/325], loss=66.7867
	step [276/325], loss=66.1850
	step [277/325], loss=68.2947
	step [278/325], loss=66.8764
	step [279/325], loss=67.4367
	step [280/325], loss=64.6231
	step [281/325], loss=66.5104
	step [282/325], loss=64.8769
	step [283/325], loss=66.4770
	step [284/325], loss=67.3140
	step [285/325], loss=65.2705
	step [286/325], loss=65.2057
	step [287/325], loss=65.3248
	step [288/325], loss=66.0782
	step [289/325], loss=66.4697
	step [290/325], loss=64.9427
	step [291/325], loss=64.7623
	step [292/325], loss=65.5024
	step [293/325], loss=65.3095
	step [294/325], loss=64.5354
	step [295/325], loss=63.8736
	step [296/325], loss=63.6184
	step [297/325], loss=63.5179
	step [298/325], loss=63.9881
	step [299/325], loss=65.2697
	step [300/325], loss=63.5308
	step [301/325], loss=64.9505
	step [302/325], loss=64.5143
	step [303/325], loss=64.2905
	step [304/325], loss=63.4208
	step [305/325], loss=64.7801
	step [306/325], loss=64.4421
	step [307/325], loss=63.4647
	step [308/325], loss=63.6189
	step [309/325], loss=62.6542
	step [310/325], loss=63.2295
	step [311/325], loss=62.5875
	step [312/325], loss=64.5159
	step [313/325], loss=63.4026
	step [314/325], loss=61.9959
	step [315/325], loss=61.6793
	step [316/325], loss=64.4323
	step [317/325], loss=62.4783
	step [318/325], loss=64.4856
	step [319/325], loss=62.8726
	step [320/325], loss=64.0845
	step [321/325], loss=61.7304
	step [322/325], loss=61.5363
	step [323/325], loss=61.7069
	step [324/325], loss=62.3771
	step [325/325], loss=34.8071
	Evaluating
	loss=0.3036, precision=0.1756, recall=0.9960, f1=0.2986
saving model as: 1_saved_model.pth
Training epoch 2
	step [1/325], loss=60.7375
	step [2/325], loss=61.1354
	step [3/325], loss=62.7386
	step [4/325], loss=62.6053
	step [5/325], loss=61.8155
	step [6/325], loss=61.7735
	step [7/325], loss=61.5234
	step [8/325], loss=61.8359
	step [9/325], loss=60.2508
	step [10/325], loss=60.5201
	step [11/325], loss=62.9941
	step [12/325], loss=59.9890
	step [13/325], loss=61.5434
	step [14/325], loss=60.1195
	step [15/325], loss=59.3852
	step [16/325], loss=60.5648
	step [17/325], loss=62.1634
	step [18/325], loss=61.4300
	step [19/325], loss=59.8942
	step [20/325], loss=59.2074
	step [21/325], loss=60.6280
	step [22/325], loss=61.0513
	step [23/325], loss=58.5990
	step [24/325], loss=59.8179
	step [25/325], loss=59.1271
	step [26/325], loss=59.2718
	step [27/325], loss=60.2134
	step [28/325], loss=59.7751
	step [29/325], loss=60.2283
	step [30/325], loss=59.7775
	step [31/325], loss=58.9721
	step [32/325], loss=59.9805
	step [33/325], loss=59.2783
	step [34/325], loss=59.4964
	step [35/325], loss=59.6964
	step [36/325], loss=58.4306
	step [37/325], loss=58.7268
	step [38/325], loss=59.0754
	step [39/325], loss=57.8111
	step [40/325], loss=58.4030
	step [41/325], loss=57.4842
	step [42/325], loss=58.8148
	step [43/325], loss=57.1645
	step [44/325], loss=59.1648
	step [45/325], loss=57.7778
	step [46/325], loss=57.7512
	step [47/325], loss=58.3364
	step [48/325], loss=56.9623
	step [49/325], loss=58.5830
	step [50/325], loss=58.2842
	step [51/325], loss=58.1615
	step [52/325], loss=58.7099
	step [53/325], loss=57.2121
	step [54/325], loss=58.1847
	step [55/325], loss=57.5903
	step [56/325], loss=56.9307
	step [57/325], loss=55.9617
	step [58/325], loss=55.7985
	step [59/325], loss=57.3745
	step [60/325], loss=56.0007
	step [61/325], loss=56.2682
	step [62/325], loss=57.1426
	step [63/325], loss=57.2321
	step [64/325], loss=56.6373
	step [65/325], loss=56.1141
	step [66/325], loss=55.6963
	step [67/325], loss=59.7727
	step [68/325], loss=55.7765
	step [69/325], loss=56.2013
	step [70/325], loss=56.9008
	step [71/325], loss=56.4075
	step [72/325], loss=56.4748
	step [73/325], loss=56.4808
	step [74/325], loss=56.5115
	step [75/325], loss=57.9693
	step [76/325], loss=55.1065
	step [77/325], loss=56.9991
	step [78/325], loss=56.2466
	step [79/325], loss=56.3358
	step [80/325], loss=56.4143
	step [81/325], loss=57.1278
	step [82/325], loss=55.7525
	step [83/325], loss=55.0394
	step [84/325], loss=55.6334
	step [85/325], loss=55.0223
	step [86/325], loss=55.6296
	step [87/325], loss=57.3996
	step [88/325], loss=55.7685
	step [89/325], loss=54.7240
	step [90/325], loss=55.2182
	step [91/325], loss=55.4803
	step [92/325], loss=56.1990
	step [93/325], loss=54.8655
	step [94/325], loss=55.3163
	step [95/325], loss=54.6450
	step [96/325], loss=55.5844
	step [97/325], loss=55.6575
	step [98/325], loss=54.5678
	step [99/325], loss=53.5246
	step [100/325], loss=53.7324
	step [101/325], loss=54.4230
	step [102/325], loss=55.3048
	step [103/325], loss=54.5280
	step [104/325], loss=56.0080
	step [105/325], loss=55.1817
	step [106/325], loss=53.9004
	step [107/325], loss=53.4047
	step [108/325], loss=53.4092
	step [109/325], loss=54.3559
	step [110/325], loss=52.5212
	step [111/325], loss=54.2334
	step [112/325], loss=52.6225
	step [113/325], loss=53.4431
	step [114/325], loss=52.7393
	step [115/325], loss=54.2244
	step [116/325], loss=53.0566
	step [117/325], loss=54.3524
	step [118/325], loss=52.5512
	step [119/325], loss=52.1417
	step [120/325], loss=51.3057
	step [121/325], loss=52.0277
	step [122/325], loss=52.3429
	step [123/325], loss=53.6878
	step [124/325], loss=53.1020
	step [125/325], loss=52.7788
	step [126/325], loss=52.7608
	step [127/325], loss=51.3856
	step [128/325], loss=51.0439
	step [129/325], loss=51.8994
	step [130/325], loss=52.5100
	step [131/325], loss=52.4845
	step [132/325], loss=53.1654
	step [133/325], loss=52.4092
	step [134/325], loss=51.5919
	step [135/325], loss=51.1561
	step [136/325], loss=52.5092
	step [137/325], loss=51.1816
	step [138/325], loss=53.2636
	step [139/325], loss=50.7949
	step [140/325], loss=50.8157
	step [141/325], loss=52.2719
	step [142/325], loss=51.4918
	step [143/325], loss=51.3527
	step [144/325], loss=50.9364
	step [145/325], loss=51.7659
	step [146/325], loss=50.7618
	step [147/325], loss=50.4350
	step [148/325], loss=52.7115
	step [149/325], loss=52.0263
	step [150/325], loss=50.7737
	step [151/325], loss=52.0711
	step [152/325], loss=50.8273
	step [153/325], loss=49.7592
	step [154/325], loss=50.8342
	step [155/325], loss=50.4130
	step [156/325], loss=48.5455
	step [157/325], loss=49.3767
	step [158/325], loss=50.9543
	step [159/325], loss=49.1447
	step [160/325], loss=49.6722
	step [161/325], loss=50.8584
	step [162/325], loss=49.9778
	step [163/325], loss=49.1000
	step [164/325], loss=49.0646
	step [165/325], loss=51.9423
	step [166/325], loss=49.1645
	step [167/325], loss=51.4480
	step [168/325], loss=49.7593
	step [169/325], loss=50.8922
	step [170/325], loss=50.0126
	step [171/325], loss=48.4863
	step [172/325], loss=50.9497
	step [173/325], loss=49.7262
	step [174/325], loss=49.8182
	step [175/325], loss=49.1312
	step [176/325], loss=48.5372
	step [177/325], loss=50.8354
	step [178/325], loss=50.1470
	step [179/325], loss=49.7437
	step [180/325], loss=47.9138
	step [181/325], loss=48.8186
	step [182/325], loss=48.7382
	step [183/325], loss=49.6075
	step [184/325], loss=48.1491
	step [185/325], loss=48.3759
	step [186/325], loss=47.9904
	step [187/325], loss=48.3049
	step [188/325], loss=47.7153
	step [189/325], loss=49.3435
	step [190/325], loss=47.0821
	step [191/325], loss=49.1751
	step [192/325], loss=48.0504
	step [193/325], loss=47.9893
	step [194/325], loss=48.5282
	step [195/325], loss=48.9731
	step [196/325], loss=47.5547
	step [197/325], loss=49.4341
	step [198/325], loss=46.8350
	step [199/325], loss=47.1810
	step [200/325], loss=47.9435
	step [201/325], loss=49.6770
	step [202/325], loss=47.1396
	step [203/325], loss=48.5004
	step [204/325], loss=46.1366
	step [205/325], loss=47.4152
	step [206/325], loss=47.1878
	step [207/325], loss=47.5518
	step [208/325], loss=47.5643
	step [209/325], loss=46.8755
	step [210/325], loss=46.9012
	step [211/325], loss=46.5482
	step [212/325], loss=46.2638
	step [213/325], loss=46.7989
	step [214/325], loss=46.3456
	step [215/325], loss=45.5066
	step [216/325], loss=46.6516
	step [217/325], loss=46.8836
	step [218/325], loss=47.2130
	step [219/325], loss=46.9386
	step [220/325], loss=46.6215
	step [221/325], loss=45.5724
	step [222/325], loss=46.2534
	step [223/325], loss=45.8536
	step [224/325], loss=47.7975
	step [225/325], loss=45.7723
	step [226/325], loss=46.3451
	step [227/325], loss=45.5136
	step [228/325], loss=46.8382
	step [229/325], loss=45.4184
	step [230/325], loss=45.8262
	step [231/325], loss=46.3221
	step [232/325], loss=47.5406
	step [233/325], loss=45.3726
	step [234/325], loss=45.8595
	step [235/325], loss=45.6154
	step [236/325], loss=47.1011
	step [237/325], loss=44.9643
	step [238/325], loss=44.7255
	step [239/325], loss=46.3227
	step [240/325], loss=45.9375
	step [241/325], loss=43.8588
	step [242/325], loss=45.4070
	step [243/325], loss=45.3842
	step [244/325], loss=44.1566
	step [245/325], loss=44.7291
	step [246/325], loss=44.0415
	step [247/325], loss=44.8137
	step [248/325], loss=45.8986
	step [249/325], loss=45.3817
	step [250/325], loss=46.1587
	step [251/325], loss=44.5232
	step [252/325], loss=44.4431
	step [253/325], loss=45.1330
	step [254/325], loss=46.7358
	step [255/325], loss=43.8869
	step [256/325], loss=42.9147
	step [257/325], loss=42.9253
	step [258/325], loss=45.0087
	step [259/325], loss=43.3893
	step [260/325], loss=44.8907
	step [261/325], loss=43.5550
	step [262/325], loss=44.7416
	step [263/325], loss=43.0031
	step [264/325], loss=46.1383
	step [265/325], loss=45.2789
	step [266/325], loss=45.7285
	step [267/325], loss=43.4957
	step [268/325], loss=43.1863
	step [269/325], loss=44.0520
	step [270/325], loss=43.2015
	step [271/325], loss=43.2150
	step [272/325], loss=43.4904
	step [273/325], loss=43.3364
	step [274/325], loss=44.5932
	step [275/325], loss=43.2783
	step [276/325], loss=42.7233
	step [277/325], loss=43.5471
	step [278/325], loss=43.1199
	step [279/325], loss=42.8236
	step [280/325], loss=43.5503
	step [281/325], loss=43.8190
	step [282/325], loss=42.0643
	step [283/325], loss=43.1328
	step [284/325], loss=41.5604
	step [285/325], loss=43.0354
	step [286/325], loss=43.6830
	step [287/325], loss=43.2905
	step [288/325], loss=43.8774
	step [289/325], loss=41.1349
	step [290/325], loss=42.9118
	step [291/325], loss=42.9860
	step [292/325], loss=41.7667
	step [293/325], loss=41.9269
	step [294/325], loss=43.1053
	step [295/325], loss=42.8477
	step [296/325], loss=41.9826
	step [297/325], loss=42.3455
	step [298/325], loss=42.5500
	step [299/325], loss=43.0448
	step [300/325], loss=42.9357
	step [301/325], loss=43.4688
	step [302/325], loss=40.8339
	step [303/325], loss=41.8227
	step [304/325], loss=41.5762
	step [305/325], loss=40.6558
	step [306/325], loss=41.3384
	step [307/325], loss=40.5953
	step [308/325], loss=43.0193
	step [309/325], loss=40.6943
	step [310/325], loss=39.9747
	step [311/325], loss=40.6880
	step [312/325], loss=40.4804
	step [313/325], loss=43.1922
	step [314/325], loss=42.1938
	step [315/325], loss=40.3704
	step [316/325], loss=40.9078
	step [317/325], loss=40.2516
	step [318/325], loss=40.8445
	step [319/325], loss=40.8369
	step [320/325], loss=40.5442
	step [321/325], loss=41.2778
	step [322/325], loss=41.4402
	step [323/325], loss=40.8326
	step [324/325], loss=41.1025
	step [325/325], loss=22.8862
	Evaluating
	loss=0.2039, precision=0.1675, recall=0.9965, f1=0.2868
Training epoch 3
	step [1/325], loss=39.0721
	step [2/325], loss=40.6513
	step [3/325], loss=39.5053
	step [4/325], loss=40.2401
	step [5/325], loss=41.8195
	step [6/325], loss=41.6555
	step [7/325], loss=39.3567
	step [8/325], loss=41.3047
	step [9/325], loss=41.8151
	step [10/325], loss=40.3885
	step [11/325], loss=40.6852
	step [12/325], loss=41.7132
	step [13/325], loss=40.8532
	step [14/325], loss=40.1721
	step [15/325], loss=41.5020
	step [16/325], loss=42.1325
	step [17/325], loss=41.1626
	step [18/325], loss=39.7544
	step [19/325], loss=39.4133
	step [20/325], loss=40.5206
	step [21/325], loss=40.5486
	step [22/325], loss=39.1087
	step [23/325], loss=39.5016
	step [24/325], loss=39.5298
	step [25/325], loss=40.9510
	step [26/325], loss=39.7890
	step [27/325], loss=40.4603
	step [28/325], loss=38.8926
	step [29/325], loss=38.6301
	step [30/325], loss=39.2905
	step [31/325], loss=38.5603
	step [32/325], loss=37.5147
	step [33/325], loss=39.3767
	step [34/325], loss=40.7594
	step [35/325], loss=40.3369
	step [36/325], loss=39.4827
	step [37/325], loss=40.2040
	step [38/325], loss=39.4697
	step [39/325], loss=39.0639
	step [40/325], loss=38.9999
	step [41/325], loss=39.2500
	step [42/325], loss=39.1002
	step [43/325], loss=41.1533
	step [44/325], loss=38.5274
	step [45/325], loss=38.4216
	step [46/325], loss=38.8810
	step [47/325], loss=39.7549
	step [48/325], loss=38.3726
	step [49/325], loss=37.9482
	step [50/325], loss=37.5032
	step [51/325], loss=38.9420
	step [52/325], loss=38.7823
	step [53/325], loss=40.3455
	step [54/325], loss=37.2846
	step [55/325], loss=38.7270
	step [56/325], loss=43.0849
	step [57/325], loss=36.9968
	step [58/325], loss=39.0950
	step [59/325], loss=42.1056
	step [60/325], loss=38.1455
	step [61/325], loss=41.6103
	step [62/325], loss=40.1751
	step [63/325], loss=39.4332
	step [64/325], loss=39.7866
	step [65/325], loss=40.1113
	step [66/325], loss=40.5211
	step [67/325], loss=38.5315
	step [68/325], loss=37.4419
	step [69/325], loss=38.8278
	step [70/325], loss=39.3304
	step [71/325], loss=37.7602
	step [72/325], loss=39.3993
	step [73/325], loss=38.1969
	step [74/325], loss=38.1855
	step [75/325], loss=37.1287
	step [76/325], loss=37.6192
	step [77/325], loss=36.4782
	step [78/325], loss=37.9072
	step [79/325], loss=37.4445
	step [80/325], loss=38.3128
	step [81/325], loss=37.5073
	step [82/325], loss=38.9407
	step [83/325], loss=38.6628
	step [84/325], loss=40.4781
	step [85/325], loss=37.9698
	step [86/325], loss=37.7873
	step [87/325], loss=40.4340
	step [88/325], loss=37.2512
	step [89/325], loss=36.3638
	step [90/325], loss=38.4118
	step [91/325], loss=36.4269
	step [92/325], loss=35.6078
	step [93/325], loss=37.0122
	step [94/325], loss=37.2883
	step [95/325], loss=37.4739
	step [96/325], loss=35.9500
	step [97/325], loss=36.6638
	step [98/325], loss=35.2512
	step [99/325], loss=34.1943
	step [100/325], loss=35.2225
	step [101/325], loss=37.2468
	step [102/325], loss=35.4678
	step [103/325], loss=36.3235
	step [104/325], loss=37.2162
	step [105/325], loss=37.0687
	step [106/325], loss=34.0308
	step [107/325], loss=36.8049
	step [108/325], loss=35.7350
	step [109/325], loss=34.6946
	step [110/325], loss=36.7664
	step [111/325], loss=34.7737
	step [112/325], loss=35.8058
	step [113/325], loss=34.4681
	step [114/325], loss=35.4978
	step [115/325], loss=35.0487
	step [116/325], loss=35.9811
	step [117/325], loss=35.5012
	step [118/325], loss=34.5189
	step [119/325], loss=34.3452
	step [120/325], loss=36.3785
	step [121/325], loss=36.5693
	step [122/325], loss=34.3705
	step [123/325], loss=35.3870
	step [124/325], loss=35.4596
	step [125/325], loss=34.8311
	step [126/325], loss=35.8920
	step [127/325], loss=34.6740
	step [128/325], loss=35.2324
	step [129/325], loss=34.3529
	step [130/325], loss=33.6564
	step [131/325], loss=34.4287
	step [132/325], loss=34.1129
	step [133/325], loss=35.4251
	step [134/325], loss=35.4385
	step [135/325], loss=35.2087
	step [136/325], loss=34.7190
	step [137/325], loss=35.0864
	step [138/325], loss=34.4932
	step [139/325], loss=34.9025
	step [140/325], loss=34.2696
	step [141/325], loss=34.6737
	step [142/325], loss=34.1568
	step [143/325], loss=34.3228
	step [144/325], loss=35.6399
	step [145/325], loss=34.2926
	step [146/325], loss=34.9973
	step [147/325], loss=33.5723
	step [148/325], loss=35.2954
	step [149/325], loss=32.7395
	step [150/325], loss=35.5058
	step [151/325], loss=32.2707
	step [152/325], loss=35.8628
	step [153/325], loss=34.1111
	step [154/325], loss=33.1038
	step [155/325], loss=35.9573
	step [156/325], loss=33.5036
	step [157/325], loss=34.1123
	step [158/325], loss=34.0296
	step [159/325], loss=33.9786
	step [160/325], loss=34.2893
	step [161/325], loss=32.5351
	step [162/325], loss=35.3602
	step [163/325], loss=32.5329
	step [164/325], loss=32.8230
	step [165/325], loss=32.9813
	step [166/325], loss=34.0934
	step [167/325], loss=33.4417
	step [168/325], loss=33.1695
	step [169/325], loss=34.0142
	step [170/325], loss=33.5650
	step [171/325], loss=34.3508
	step [172/325], loss=33.7535
	step [173/325], loss=33.8478
	step [174/325], loss=34.3868
	step [175/325], loss=32.0301
	step [176/325], loss=34.9518
	step [177/325], loss=35.2199
	step [178/325], loss=32.5407
	step [179/325], loss=33.1867
	step [180/325], loss=32.6579
	step [181/325], loss=31.7537
	step [182/325], loss=32.8564
	step [183/325], loss=33.2419
	step [184/325], loss=33.4437
	step [185/325], loss=33.9820
	step [186/325], loss=34.0465
	step [187/325], loss=32.4861
	step [188/325], loss=33.4567
	step [189/325], loss=33.0189
	step [190/325], loss=33.2118
	step [191/325], loss=32.8870
	step [192/325], loss=33.9902
	step [193/325], loss=32.5527
	step [194/325], loss=32.7640
	step [195/325], loss=32.3572
	step [196/325], loss=31.1842
	step [197/325], loss=33.1985
	step [198/325], loss=34.0793
	step [199/325], loss=31.8737
	step [200/325], loss=35.2286
	step [201/325], loss=31.2999
	step [202/325], loss=30.0927
	step [203/325], loss=32.1389
	step [204/325], loss=31.0984
	step [205/325], loss=32.2634
	step [206/325], loss=30.7341
	step [207/325], loss=34.2342
	step [208/325], loss=33.3842
	step [209/325], loss=33.2608
	step [210/325], loss=31.9918
	step [211/325], loss=31.4951
	step [212/325], loss=31.3870
	step [213/325], loss=30.5563
	step [214/325], loss=31.9720
	step [215/325], loss=32.5301
	step [216/325], loss=32.0496
	step [217/325], loss=31.3974
	step [218/325], loss=31.3921
	step [219/325], loss=32.3090
	step [220/325], loss=31.9144
	step [221/325], loss=31.4292
	step [222/325], loss=32.2889
	step [223/325], loss=31.6068
	step [224/325], loss=30.0686
	step [225/325], loss=30.9074
	step [226/325], loss=34.0468
	step [227/325], loss=30.7014
	step [228/325], loss=30.8523
	step [229/325], loss=33.0545
	step [230/325], loss=31.3456
	step [231/325], loss=30.4832
	step [232/325], loss=32.4642
	step [233/325], loss=31.6886
	step [234/325], loss=30.1120
	step [235/325], loss=31.2970
	step [236/325], loss=30.5671
	step [237/325], loss=31.5052
	step [238/325], loss=31.0149
	step [239/325], loss=30.6577
	step [240/325], loss=29.9686
	step [241/325], loss=31.3520
	step [242/325], loss=31.4457
	step [243/325], loss=29.9963
	step [244/325], loss=31.0808
	step [245/325], loss=31.9983
	step [246/325], loss=29.9256
	step [247/325], loss=30.8774
	step [248/325], loss=29.5167
	step [249/325], loss=29.3730
	step [250/325], loss=30.2719
	step [251/325], loss=29.8689
	step [252/325], loss=30.1849
	step [253/325], loss=29.7934
	step [254/325], loss=31.6345
	step [255/325], loss=31.0235
	step [256/325], loss=30.0889
	step [257/325], loss=29.9376
	step [258/325], loss=30.0478
	step [259/325], loss=31.1265
	step [260/325], loss=30.1647
	step [261/325], loss=29.7934
	step [262/325], loss=29.2434
	step [263/325], loss=29.6201
	step [264/325], loss=28.9134
	step [265/325], loss=29.3252
	step [266/325], loss=30.2234
	step [267/325], loss=31.6138
	step [268/325], loss=31.0177
	step [269/325], loss=29.8354
	step [270/325], loss=30.6009
	step [271/325], loss=30.6188
	step [272/325], loss=30.8016
	step [273/325], loss=29.4187
	step [274/325], loss=29.1628
	step [275/325], loss=30.5282
	step [276/325], loss=30.4799
	step [277/325], loss=27.8896
	step [278/325], loss=28.7905
	step [279/325], loss=30.9544
	step [280/325], loss=29.2612
	step [281/325], loss=29.1786
	step [282/325], loss=29.0629
	step [283/325], loss=30.8258
	step [284/325], loss=28.9956
	step [285/325], loss=29.6702
	step [286/325], loss=29.6577
	step [287/325], loss=29.4782
	step [288/325], loss=29.6352
	step [289/325], loss=29.4466
	step [290/325], loss=30.9358
	step [291/325], loss=28.9312
	step [292/325], loss=30.7480
	step [293/325], loss=29.1401
	step [294/325], loss=29.2001
	step [295/325], loss=29.1574
	step [296/325], loss=28.4864
	step [297/325], loss=29.2243
	step [298/325], loss=29.8311
	step [299/325], loss=27.5742
	step [300/325], loss=29.1643
	step [301/325], loss=28.1179
	step [302/325], loss=28.3146
	step [303/325], loss=28.8282
	step [304/325], loss=29.2076
	step [305/325], loss=28.6847
	step [306/325], loss=28.5970
	step [307/325], loss=29.9690
	step [308/325], loss=28.6735
	step [309/325], loss=28.8957
	step [310/325], loss=30.9010
	step [311/325], loss=28.6029
	step [312/325], loss=30.1036
	step [313/325], loss=28.4308
	step [314/325], loss=30.2097
	step [315/325], loss=28.7637
	step [316/325], loss=28.2050
	step [317/325], loss=27.0263
	step [318/325], loss=28.4953
	step [319/325], loss=28.1222
	step [320/325], loss=28.5126
	step [321/325], loss=28.5459
	step [322/325], loss=27.9778
	step [323/325], loss=29.7962
	step [324/325], loss=28.4769
	step [325/325], loss=15.6726
	Evaluating
	loss=0.1385, precision=0.1718, recall=0.9965, f1=0.2931
Training epoch 4
	step [1/325], loss=28.5330
	step [2/325], loss=28.5860
	step [3/325], loss=28.2518
	step [4/325], loss=27.9746
	step [5/325], loss=28.1208
	step [6/325], loss=27.9294
	step [7/325], loss=29.0056
	step [8/325], loss=27.8467
	step [9/325], loss=27.3803
	step [10/325], loss=28.0075
	step [11/325], loss=28.1792
	step [12/325], loss=27.2434
	step [13/325], loss=28.0851
	step [14/325], loss=28.0410
	step [15/325], loss=27.6305
	step [16/325], loss=28.8895
	step [17/325], loss=28.3157
	step [18/325], loss=27.7300
	step [19/325], loss=26.9656
	step [20/325], loss=27.0397
	step [21/325], loss=27.0196
	step [22/325], loss=28.2886
	step [23/325], loss=27.1264
	step [24/325], loss=28.0868
	step [25/325], loss=27.9359
	step [26/325], loss=28.6320
	step [27/325], loss=26.0671
	step [28/325], loss=27.3135
	step [29/325], loss=27.6117
	step [30/325], loss=27.1982
	step [31/325], loss=27.3782
	step [32/325], loss=27.8928
	step [33/325], loss=28.2212
	step [34/325], loss=28.3682
	step [35/325], loss=25.5146
	step [36/325], loss=26.8470
	step [37/325], loss=26.9748
	step [38/325], loss=27.3907
	step [39/325], loss=27.0303
	step [40/325], loss=27.2725
	step [41/325], loss=27.3779
	step [42/325], loss=26.5598
	step [43/325], loss=27.7281
	step [44/325], loss=26.8135
	step [45/325], loss=26.0910
	step [46/325], loss=27.0592
	step [47/325], loss=25.6757
	step [48/325], loss=27.3000
	step [49/325], loss=26.1842
	step [50/325], loss=27.6884
	step [51/325], loss=27.1911
	step [52/325], loss=27.0231
	step [53/325], loss=28.4119
	step [54/325], loss=26.9230
	step [55/325], loss=26.2125
	step [56/325], loss=27.1012
	step [57/325], loss=25.8025
	step [58/325], loss=27.3912
	step [59/325], loss=25.2239
	step [60/325], loss=26.5512
	step [61/325], loss=25.7882
	step [62/325], loss=26.7913
	step [63/325], loss=27.1078
	step [64/325], loss=26.7946
	step [65/325], loss=28.2286
	step [66/325], loss=26.1221
	step [67/325], loss=25.8088
	step [68/325], loss=26.9018
	step [69/325], loss=28.1904
	step [70/325], loss=25.7264
	step [71/325], loss=25.9030
	step [72/325], loss=26.7899
	step [73/325], loss=25.9050
	step [74/325], loss=27.1914
	step [75/325], loss=25.3317
	step [76/325], loss=26.3925
	step [77/325], loss=26.7401
	step [78/325], loss=25.9130
	step [79/325], loss=25.7248
	step [80/325], loss=26.0631
	step [81/325], loss=25.6803
	step [82/325], loss=25.4171
	step [83/325], loss=26.4790
	step [84/325], loss=25.9223
	step [85/325], loss=25.4972
	step [86/325], loss=26.0713
	step [87/325], loss=26.9018
	step [88/325], loss=26.2361
	step [89/325], loss=25.9916
	step [90/325], loss=26.5376
	step [91/325], loss=26.9360
	step [92/325], loss=23.9343
	step [93/325], loss=27.5276
	step [94/325], loss=25.2022
	step [95/325], loss=25.1591
	step [96/325], loss=26.5466
	step [97/325], loss=25.9290
	step [98/325], loss=25.1500
	step [99/325], loss=26.6488
	step [100/325], loss=23.9140
	step [101/325], loss=25.9163
	step [102/325], loss=24.7103
	step [103/325], loss=24.4411
	step [104/325], loss=25.7170
	step [105/325], loss=26.9990
	step [106/325], loss=24.2759
	step [107/325], loss=25.2477
	step [108/325], loss=25.5717
	step [109/325], loss=23.4822
	step [110/325], loss=26.4881
	step [111/325], loss=23.4423
	step [112/325], loss=23.7584
	step [113/325], loss=24.2870
	step [114/325], loss=24.3315
	step [115/325], loss=24.2756
	step [116/325], loss=25.0723
	step [117/325], loss=23.3848
	step [118/325], loss=24.4927
	step [119/325], loss=23.4185
	step [120/325], loss=25.8741
	step [121/325], loss=25.2053
	step [122/325], loss=24.8505
	step [123/325], loss=24.6411
	step [124/325], loss=25.1398
	step [125/325], loss=25.5207
	step [126/325], loss=25.6210
	step [127/325], loss=25.0621
	step [128/325], loss=24.4643
	step [129/325], loss=24.0455
	step [130/325], loss=25.5509
	step [131/325], loss=25.4426
	step [132/325], loss=23.3713
	step [133/325], loss=23.7713
	step [134/325], loss=24.9367
	step [135/325], loss=23.5623
	step [136/325], loss=23.8625
	step [137/325], loss=25.9520
	step [138/325], loss=23.2624
	step [139/325], loss=23.3363
	step [140/325], loss=23.9249
	step [141/325], loss=23.3979
	step [142/325], loss=24.0954
	step [143/325], loss=24.0082
	step [144/325], loss=23.8100
	step [145/325], loss=24.5843
	step [146/325], loss=24.3353
	step [147/325], loss=24.0388
	step [148/325], loss=24.9409
	step [149/325], loss=23.5794
	step [150/325], loss=24.2768
	step [151/325], loss=23.2927
	step [152/325], loss=24.6630
	step [153/325], loss=23.3277
	step [154/325], loss=23.9906
	step [155/325], loss=25.8623
	step [156/325], loss=25.0029
	step [157/325], loss=24.8519
	step [158/325], loss=23.9305
	step [159/325], loss=24.0561
	step [160/325], loss=23.0147
	step [161/325], loss=24.6322
	step [162/325], loss=22.5584
	step [163/325], loss=24.6270
	step [164/325], loss=25.0016
	step [165/325], loss=23.7379
	step [166/325], loss=23.6768
	step [167/325], loss=25.3613
	step [168/325], loss=23.1395
	step [169/325], loss=22.9216
	step [170/325], loss=23.4296
	step [171/325], loss=23.2595
	step [172/325], loss=24.5629
	step [173/325], loss=24.3099
	step [174/325], loss=24.1946
	step [175/325], loss=23.5850
	step [176/325], loss=23.7887
	step [177/325], loss=22.3228
	step [178/325], loss=23.4015
	step [179/325], loss=22.7394
	step [180/325], loss=23.8137
	step [181/325], loss=22.6248
	step [182/325], loss=22.0011
	step [183/325], loss=21.3948
	step [184/325], loss=21.3712
	step [185/325], loss=24.1796
	step [186/325], loss=23.3782
	step [187/325], loss=23.3832
	step [188/325], loss=21.6717
	step [189/325], loss=23.1787
	step [190/325], loss=21.5241
	step [191/325], loss=22.1254
	step [192/325], loss=22.8169
	step [193/325], loss=22.5062
	step [194/325], loss=22.7960
	step [195/325], loss=22.7703
	step [196/325], loss=22.8060
	step [197/325], loss=23.9062
	step [198/325], loss=21.5102
	step [199/325], loss=22.1664
	step [200/325], loss=21.6938
	step [201/325], loss=22.7937
	step [202/325], loss=22.1420
	step [203/325], loss=22.5631
	step [204/325], loss=21.6229
	step [205/325], loss=20.3777
	step [206/325], loss=24.5655
	step [207/325], loss=22.7406
	step [208/325], loss=21.7783
	step [209/325], loss=22.0221
	step [210/325], loss=24.4135
	step [211/325], loss=22.8475
	step [212/325], loss=23.3387
	step [213/325], loss=22.1691
	step [214/325], loss=22.6148
	step [215/325], loss=22.1024
	step [216/325], loss=22.1683
	step [217/325], loss=22.7890
	step [218/325], loss=22.5856
	step [219/325], loss=22.4901
	step [220/325], loss=20.4439
	step [221/325], loss=21.5640
	step [222/325], loss=23.0004
	step [223/325], loss=22.4523
	step [224/325], loss=22.2520
	step [225/325], loss=23.6171
	step [226/325], loss=22.2570
	step [227/325], loss=23.0427
	step [228/325], loss=23.0199
	step [229/325], loss=21.1280
	step [230/325], loss=21.3948
	step [231/325], loss=24.0734
	step [232/325], loss=22.5502
	step [233/325], loss=21.7297
	step [234/325], loss=22.4059
	step [235/325], loss=22.8908
	step [236/325], loss=21.1651
	step [237/325], loss=21.4354
	step [238/325], loss=21.4808
	step [239/325], loss=21.3202
	step [240/325], loss=21.9265
	step [241/325], loss=24.0689
	step [242/325], loss=20.6210
	step [243/325], loss=21.3252
	step [244/325], loss=22.7837
	step [245/325], loss=23.3293
	step [246/325], loss=25.1022
	step [247/325], loss=22.8146
	step [248/325], loss=21.0852
	step [249/325], loss=21.6620
	step [250/325], loss=21.7810
	step [251/325], loss=21.6534
	step [252/325], loss=20.4461
	step [253/325], loss=21.1413
	step [254/325], loss=20.5346
	step [255/325], loss=22.9846
	step [256/325], loss=21.5477
	step [257/325], loss=23.2470
	step [258/325], loss=21.8687
	step [259/325], loss=21.8944
	step [260/325], loss=21.9531
	step [261/325], loss=22.1388
	step [262/325], loss=22.4702
	step [263/325], loss=22.1218
	step [264/325], loss=20.7975
	step [265/325], loss=22.0395
	step [266/325], loss=21.3291
	step [267/325], loss=20.6677
	step [268/325], loss=21.2243
	step [269/325], loss=21.5012
	step [270/325], loss=22.4988
	step [271/325], loss=20.5943
	step [272/325], loss=20.1455
	step [273/325], loss=21.6311
	step [274/325], loss=22.2315
	step [275/325], loss=21.4263
	step [276/325], loss=20.6783
	step [277/325], loss=21.8645
	step [278/325], loss=20.4462
	step [279/325], loss=22.1039
	step [280/325], loss=20.7314
	step [281/325], loss=21.4192
	step [282/325], loss=21.3939
	step [283/325], loss=21.0538
	step [284/325], loss=20.4893
	step [285/325], loss=20.6816
	step [286/325], loss=21.6525
	step [287/325], loss=21.2932
	step [288/325], loss=20.4836
	step [289/325], loss=20.1444
	step [290/325], loss=21.9813
	step [291/325], loss=20.9119
	step [292/325], loss=21.6811
	step [293/325], loss=21.2161
	step [294/325], loss=20.9776
	step [295/325], loss=23.6377
	step [296/325], loss=21.8809
	step [297/325], loss=21.6670
	step [298/325], loss=24.0342
	step [299/325], loss=24.2002
	step [300/325], loss=21.8451
	step [301/325], loss=19.9907
	step [302/325], loss=20.6712
	step [303/325], loss=19.8041
	step [304/325], loss=20.9695
	step [305/325], loss=20.2178
	step [306/325], loss=21.2622
	step [307/325], loss=21.7639
	step [308/325], loss=20.3763
	step [309/325], loss=21.4163
	step [310/325], loss=19.5487
	step [311/325], loss=19.9200
	step [312/325], loss=20.2087
	step [313/325], loss=20.3822
	step [314/325], loss=21.1500
	step [315/325], loss=21.1382
	step [316/325], loss=19.9858
	step [317/325], loss=21.0239
	step [318/325], loss=19.4327
	step [319/325], loss=20.8177
	step [320/325], loss=19.5679
	step [321/325], loss=19.9296
	step [322/325], loss=20.5472
	step [323/325], loss=22.1863
	step [324/325], loss=19.6477
	step [325/325], loss=11.6745
	Evaluating
	loss=0.0927, precision=0.2061, recall=0.9952, f1=0.3415
saving model as: 1_saved_model.pth
Training epoch 5
	step [1/325], loss=21.0202
	step [2/325], loss=20.0929
	step [3/325], loss=22.5327
	step [4/325], loss=20.5086
	step [5/325], loss=19.1550
	step [6/325], loss=20.4805
	step [7/325], loss=20.0944
	step [8/325], loss=21.8307
	step [9/325], loss=20.4174
	step [10/325], loss=19.1698
	step [11/325], loss=22.2461
	step [12/325], loss=19.3096
	step [13/325], loss=20.6891
	step [14/325], loss=20.4333
	step [15/325], loss=20.2908
	step [16/325], loss=20.2003
	step [17/325], loss=18.7198
	step [18/325], loss=19.5121
	step [19/325], loss=21.2412
	step [20/325], loss=19.8155
	step [21/325], loss=21.6072
	step [22/325], loss=19.6107
	step [23/325], loss=19.0709
	step [24/325], loss=19.8349
	step [25/325], loss=20.1010
	step [26/325], loss=19.5968
	step [27/325], loss=21.0308
	step [28/325], loss=21.0927
	step [29/325], loss=20.2071
	step [30/325], loss=19.7729
	step [31/325], loss=19.2809
	step [32/325], loss=19.9251
	step [33/325], loss=18.9548
	step [34/325], loss=20.6089
	step [35/325], loss=18.7939
	step [36/325], loss=18.9863
	step [37/325], loss=19.0019
	step [38/325], loss=19.0732
	step [39/325], loss=18.3754
	step [40/325], loss=20.9791
	step [41/325], loss=20.6234
	step [42/325], loss=17.9421
	step [43/325], loss=19.7500
	step [44/325], loss=19.9317
	step [45/325], loss=19.9446
	step [46/325], loss=19.8939
	step [47/325], loss=19.7571
	step [48/325], loss=20.2290
	step [49/325], loss=18.7086
	step [50/325], loss=18.7254
	step [51/325], loss=18.5937
	step [52/325], loss=18.7715
	step [53/325], loss=20.6812
	step [54/325], loss=18.6764
	step [55/325], loss=20.0981
	step [56/325], loss=19.5597
	step [57/325], loss=18.2285
	step [58/325], loss=19.6640
	step [59/325], loss=21.4317
	step [60/325], loss=18.7067
	step [61/325], loss=20.5444
	step [62/325], loss=19.4583
	step [63/325], loss=20.3262
	step [64/325], loss=17.4801
	step [65/325], loss=19.1886
	step [66/325], loss=20.7380
	step [67/325], loss=19.8441
	step [68/325], loss=17.6485
	step [69/325], loss=21.1611
	step [70/325], loss=19.0198
	step [71/325], loss=19.0323
	step [72/325], loss=19.3885
	step [73/325], loss=18.5803
	step [74/325], loss=19.4319
	step [75/325], loss=19.3415
	step [76/325], loss=19.9829
	step [77/325], loss=17.6639
	step [78/325], loss=18.9241
	step [79/325], loss=20.4132
	step [80/325], loss=19.0717
	step [81/325], loss=18.6810
	step [82/325], loss=19.1014
	step [83/325], loss=18.8830
	step [84/325], loss=19.4097
	step [85/325], loss=19.4938
	step [86/325], loss=18.6662
	step [87/325], loss=18.9268
	step [88/325], loss=18.7184
	step [89/325], loss=18.2537
	step [90/325], loss=18.2257
	step [91/325], loss=19.4735
	step [92/325], loss=16.7962
	step [93/325], loss=19.1691
	step [94/325], loss=17.8908
	step [95/325], loss=17.8701
	step [96/325], loss=18.4844
	step [97/325], loss=17.1103
	step [98/325], loss=18.1962
	step [99/325], loss=18.0480
	step [100/325], loss=18.6967
	step [101/325], loss=18.6789
	step [102/325], loss=18.3531
	step [103/325], loss=19.7313
	step [104/325], loss=19.3482
	step [105/325], loss=18.7222
	step [106/325], loss=19.6749
	step [107/325], loss=18.2932
	step [108/325], loss=18.0179
	step [109/325], loss=19.5949
	step [110/325], loss=18.6786
	step [111/325], loss=17.8976
	step [112/325], loss=18.9532
	step [113/325], loss=15.8351
	step [114/325], loss=17.3264
	step [115/325], loss=19.3759
	step [116/325], loss=18.1645
	step [117/325], loss=18.1660
	step [118/325], loss=19.1271
	step [119/325], loss=19.4664
	step [120/325], loss=17.0745
	step [121/325], loss=19.4276
	step [122/325], loss=18.0642
	step [123/325], loss=17.5217
	step [124/325], loss=17.5325
	step [125/325], loss=19.4574
	step [126/325], loss=18.3852
	step [127/325], loss=18.8020
	step [128/325], loss=19.0083
	step [129/325], loss=17.1194
	step [130/325], loss=16.7213
	step [131/325], loss=19.3129
	step [132/325], loss=16.6834
	step [133/325], loss=16.8254
	step [134/325], loss=17.4325
	step [135/325], loss=16.2066
	step [136/325], loss=16.1362
	step [137/325], loss=16.8755
	step [138/325], loss=18.7937
	step [139/325], loss=16.6125
	step [140/325], loss=19.8290
	step [141/325], loss=18.9485
	step [142/325], loss=17.5230
	step [143/325], loss=18.4944
	step [144/325], loss=17.9081
	step [145/325], loss=17.0399
	step [146/325], loss=15.8851
	step [147/325], loss=17.7112
	step [148/325], loss=18.3567
	step [149/325], loss=16.9539
	step [150/325], loss=18.8293
	step [151/325], loss=16.6436
	step [152/325], loss=16.5408
	step [153/325], loss=17.0410
	step [154/325], loss=18.1247
	step [155/325], loss=17.9624
	step [156/325], loss=18.3732
	step [157/325], loss=17.0832
	step [158/325], loss=17.9979
	step [159/325], loss=17.2779
	step [160/325], loss=18.2994
	step [161/325], loss=17.7048
	step [162/325], loss=16.7376
	step [163/325], loss=17.4346
	step [164/325], loss=17.3948
	step [165/325], loss=16.2473
	step [166/325], loss=17.9762
	step [167/325], loss=17.3321
	step [168/325], loss=18.4610
	step [169/325], loss=18.1329
	step [170/325], loss=17.2233
	step [171/325], loss=17.3138
	step [172/325], loss=17.2275
	step [173/325], loss=17.8277
	step [174/325], loss=16.0209
	step [175/325], loss=18.7351
	step [176/325], loss=17.9709
	step [177/325], loss=17.9807
	step [178/325], loss=15.5857
	step [179/325], loss=17.0192
	step [180/325], loss=17.5340
	step [181/325], loss=17.3417
	step [182/325], loss=16.6603
	step [183/325], loss=18.3110
	step [184/325], loss=17.3025
	step [185/325], loss=17.4179
	step [186/325], loss=18.4919
	step [187/325], loss=18.3652
	step [188/325], loss=16.9181
	step [189/325], loss=16.7216
	step [190/325], loss=18.6414
	step [191/325], loss=17.7294
	step [192/325], loss=18.3401
	step [193/325], loss=16.6140
	step [194/325], loss=17.0411
	step [195/325], loss=17.1710
	step [196/325], loss=19.2804
	step [197/325], loss=18.0972
	step [198/325], loss=18.1143
	step [199/325], loss=18.3004
	step [200/325], loss=16.9980
	step [201/325], loss=16.5814
	step [202/325], loss=16.5223
	step [203/325], loss=16.5672
	step [204/325], loss=16.8852
	step [205/325], loss=18.1247
	step [206/325], loss=18.7690
	step [207/325], loss=15.7607
	step [208/325], loss=17.3195
	step [209/325], loss=18.3031
	step [210/325], loss=16.8789
	step [211/325], loss=16.8633
	step [212/325], loss=16.0602
	step [213/325], loss=16.7161
	step [214/325], loss=15.8937
	step [215/325], loss=16.1086
	step [216/325], loss=16.3788
	step [217/325], loss=16.5407
	step [218/325], loss=16.2801
	step [219/325], loss=17.1720
	step [220/325], loss=16.0846
	step [221/325], loss=15.9767
	step [222/325], loss=15.7684
	step [223/325], loss=17.8045
	step [224/325], loss=16.3358
	step [225/325], loss=16.6820
	step [226/325], loss=18.5042
	step [227/325], loss=16.9456
	step [228/325], loss=17.2756
	step [229/325], loss=17.7985
	step [230/325], loss=17.1891
	step [231/325], loss=16.9873
	step [232/325], loss=16.1780
	step [233/325], loss=18.3612
	step [234/325], loss=15.6397
	step [235/325], loss=15.5758
	step [236/325], loss=16.3479
	step [237/325], loss=17.4322
	step [238/325], loss=16.4655
	step [239/325], loss=16.5479
	step [240/325], loss=18.9781
	step [241/325], loss=17.2364
	step [242/325], loss=16.9452
	step [243/325], loss=15.7475
	step [244/325], loss=15.4770
	step [245/325], loss=16.8644
	step [246/325], loss=17.1843
	step [247/325], loss=16.5222
	step [248/325], loss=18.9062
	step [249/325], loss=17.8729
	step [250/325], loss=15.8800
	step [251/325], loss=17.3761
	step [252/325], loss=17.0734
	step [253/325], loss=17.6504
	step [254/325], loss=17.8329
	step [255/325], loss=15.4563
	step [256/325], loss=16.6361
	step [257/325], loss=16.0065
	step [258/325], loss=16.0985
	step [259/325], loss=15.4293
	step [260/325], loss=15.6899
	step [261/325], loss=15.0161
	step [262/325], loss=17.2067
	step [263/325], loss=15.5954
	step [264/325], loss=16.7518
	step [265/325], loss=15.0780
	step [266/325], loss=16.8740
	step [267/325], loss=15.6638
	step [268/325], loss=17.9206
	step [269/325], loss=16.8426
	step [270/325], loss=17.9033
	step [271/325], loss=17.5828
	step [272/325], loss=16.8343
	step [273/325], loss=16.6286
	step [274/325], loss=17.1948
	step [275/325], loss=14.4831
	step [276/325], loss=15.3262
	step [277/325], loss=14.9266
	step [278/325], loss=19.0575
	step [279/325], loss=17.9662
	step [280/325], loss=14.6973
	step [281/325], loss=16.9476
	step [282/325], loss=15.0143
	step [283/325], loss=17.3915
	step [284/325], loss=17.1247
	step [285/325], loss=15.3399
	step [286/325], loss=17.0735
	step [287/325], loss=16.7074
	step [288/325], loss=15.4874
	step [289/325], loss=15.3085
	step [290/325], loss=15.6472
	step [291/325], loss=16.7487
	step [292/325], loss=16.1699
	step [293/325], loss=16.9042
	step [294/325], loss=14.9743
	step [295/325], loss=15.8050
	step [296/325], loss=16.2784
	step [297/325], loss=15.1625
	step [298/325], loss=15.2073
	step [299/325], loss=17.1300
	step [300/325], loss=15.6694
	step [301/325], loss=17.1009
	step [302/325], loss=14.6736
	step [303/325], loss=15.0922
	step [304/325], loss=16.5724
	step [305/325], loss=14.7425
	step [306/325], loss=15.2844
	step [307/325], loss=14.7824
	step [308/325], loss=15.2388
	step [309/325], loss=15.1095
	step [310/325], loss=15.8042
	step [311/325], loss=14.7240
	step [312/325], loss=18.8738
	step [313/325], loss=13.5968
	step [314/325], loss=14.4234
	step [315/325], loss=17.9847
	step [316/325], loss=15.4988
	step [317/325], loss=16.1628
	step [318/325], loss=16.3152
	step [319/325], loss=16.4502
	step [320/325], loss=16.7690
	step [321/325], loss=15.2636
	step [322/325], loss=16.6294
	step [323/325], loss=15.2565
	step [324/325], loss=16.1805
	step [325/325], loss=8.8500
	Evaluating
	loss=0.0706, precision=0.1993, recall=0.9956, f1=0.3321
Training epoch 6
	step [1/325], loss=15.2118
	step [2/325], loss=16.1180
	step [3/325], loss=16.0896
	step [4/325], loss=14.8383
	step [5/325], loss=16.2516
	step [6/325], loss=15.6992
	step [7/325], loss=14.5439
	step [8/325], loss=14.5431
	step [9/325], loss=15.0695
	step [10/325], loss=14.3220
	step [11/325], loss=15.3214
	step [12/325], loss=15.0763
	step [13/325], loss=15.2780
	step [14/325], loss=15.7125
	step [15/325], loss=14.6781
	step [16/325], loss=15.9570
	step [17/325], loss=14.9477
	step [18/325], loss=15.3913
	step [19/325], loss=15.5957
	step [20/325], loss=14.8855
	step [21/325], loss=16.1027
	step [22/325], loss=14.0158
	step [23/325], loss=16.6782
	step [24/325], loss=15.5446
	step [25/325], loss=16.8507
	step [26/325], loss=13.9852
	step [27/325], loss=15.3573
	step [28/325], loss=14.0032
	step [29/325], loss=14.8177
	step [30/325], loss=14.9259
	step [31/325], loss=14.6702
	step [32/325], loss=14.9342
	step [33/325], loss=14.4951
	step [34/325], loss=13.9648
	step [35/325], loss=14.7063
	step [36/325], loss=14.7889
	step [37/325], loss=13.1338
	step [38/325], loss=15.1731
	step [39/325], loss=14.1356
	step [40/325], loss=14.9912
	step [41/325], loss=14.9868
	step [42/325], loss=16.6311
	step [43/325], loss=16.2025
	step [44/325], loss=15.2123
	step [45/325], loss=14.7243
	step [46/325], loss=16.1522
	step [47/325], loss=14.1459
	step [48/325], loss=14.7821
	step [49/325], loss=16.9470
	step [50/325], loss=15.9298
	step [51/325], loss=13.8478
	step [52/325], loss=16.8376
	step [53/325], loss=14.3921
	step [54/325], loss=15.2306
	step [55/325], loss=15.3310
	step [56/325], loss=14.2056
	step [57/325], loss=15.5843
	step [58/325], loss=16.2170
	step [59/325], loss=16.4766
	step [60/325], loss=15.9395
	step [61/325], loss=14.4997
	step [62/325], loss=14.5122
	step [63/325], loss=15.2211
	step [64/325], loss=15.9337
	step [65/325], loss=14.4924
	step [66/325], loss=14.1901
	step [67/325], loss=14.6188
	step [68/325], loss=13.4268
	step [69/325], loss=14.4243
	step [70/325], loss=14.8305
	step [71/325], loss=16.3573
	step [72/325], loss=15.4493
	step [73/325], loss=16.0035
	step [74/325], loss=15.7099
	step [75/325], loss=15.4199
	step [76/325], loss=16.1194
	step [77/325], loss=14.0709
	step [78/325], loss=15.3154
	step [79/325], loss=15.8980
	step [80/325], loss=14.5069
	step [81/325], loss=13.4987
	step [82/325], loss=13.2742
	step [83/325], loss=15.0972
	step [84/325], loss=13.7081
	step [85/325], loss=15.1610
	step [86/325], loss=13.9070
	step [87/325], loss=14.5098
	step [88/325], loss=15.9608
	step [89/325], loss=14.7422
	step [90/325], loss=16.5393
	step [91/325], loss=14.1873
	step [92/325], loss=14.7506
	step [93/325], loss=14.9499
	step [94/325], loss=14.4026
	step [95/325], loss=14.2690
	step [96/325], loss=15.1467
	step [97/325], loss=15.6910
	step [98/325], loss=14.6245
	step [99/325], loss=15.6217
	step [100/325], loss=17.1833
	step [101/325], loss=13.1599
	step [102/325], loss=17.2738
	step [103/325], loss=14.3167
	step [104/325], loss=14.6016
	step [105/325], loss=15.7834
	step [106/325], loss=14.4540
	step [107/325], loss=14.6319
	step [108/325], loss=14.1930
	step [109/325], loss=16.1690
	step [110/325], loss=15.2957
	step [111/325], loss=12.8431
	step [112/325], loss=13.8791
	step [113/325], loss=15.2730
	step [114/325], loss=15.8904
	step [115/325], loss=15.0920
	step [116/325], loss=12.7815
	step [117/325], loss=17.3178
	step [118/325], loss=13.4957
	step [119/325], loss=17.4861
	step [120/325], loss=15.9030
	step [121/325], loss=15.7806
	step [122/325], loss=13.1336
	step [123/325], loss=15.2853
	step [124/325], loss=14.8053
	step [125/325], loss=13.2212
	step [126/325], loss=13.9027
	step [127/325], loss=19.7267
	step [128/325], loss=13.7947
	step [129/325], loss=14.9813
	step [130/325], loss=15.6444
	step [131/325], loss=15.0313
	step [132/325], loss=15.4447
	step [133/325], loss=13.8343
	step [134/325], loss=16.0155
	step [135/325], loss=14.1843
	step [136/325], loss=12.8786
	step [137/325], loss=15.8092
	step [138/325], loss=16.3358
	step [139/325], loss=12.8867
	step [140/325], loss=14.3372
	step [141/325], loss=14.6644
	step [142/325], loss=14.4488
	step [143/325], loss=13.9393
	step [144/325], loss=13.6111
	step [145/325], loss=15.1205
	step [146/325], loss=14.4126
	step [147/325], loss=14.2184
	step [148/325], loss=13.4878
	step [149/325], loss=14.0657
	step [150/325], loss=13.4660
	step [151/325], loss=13.8938
	step [152/325], loss=12.7867
	step [153/325], loss=13.9593
	step [154/325], loss=14.8101
	step [155/325], loss=13.4972
	step [156/325], loss=15.4114
	step [157/325], loss=12.9877
	step [158/325], loss=14.2713
	step [159/325], loss=14.5168
	step [160/325], loss=15.0193
	step [161/325], loss=13.3431
	step [162/325], loss=12.4050
	step [163/325], loss=12.7712
	step [164/325], loss=14.9029
	step [165/325], loss=14.4802
	step [166/325], loss=13.3077
	step [167/325], loss=12.5280
	step [168/325], loss=15.0681
	step [169/325], loss=15.4076
	step [170/325], loss=13.0150
	step [171/325], loss=13.7178
	step [172/325], loss=12.9024
	step [173/325], loss=12.8771
	step [174/325], loss=15.7075
	step [175/325], loss=13.4309
	step [176/325], loss=13.8223
	step [177/325], loss=14.5609
	step [178/325], loss=14.1398
	step [179/325], loss=11.6957
	step [180/325], loss=15.5400
	step [181/325], loss=14.1734
	step [182/325], loss=12.7201
	step [183/325], loss=13.7344
	step [184/325], loss=13.1902
	step [185/325], loss=14.9341
	step [186/325], loss=13.3996
	step [187/325], loss=13.2337
	step [188/325], loss=14.4629
	step [189/325], loss=13.2117
	step [190/325], loss=13.4188
	step [191/325], loss=14.2935
	step [192/325], loss=12.4618
	step [193/325], loss=14.7845
	step [194/325], loss=14.7179
	step [195/325], loss=14.5075
	step [196/325], loss=12.0933
	step [197/325], loss=13.2747
	step [198/325], loss=14.6918
	step [199/325], loss=14.9126
	step [200/325], loss=15.1285
	step [201/325], loss=12.8789
	step [202/325], loss=14.6925
	step [203/325], loss=14.0077
	step [204/325], loss=14.4908
	step [205/325], loss=13.7320
	step [206/325], loss=13.4278
	step [207/325], loss=12.8939
	step [208/325], loss=12.8949
	step [209/325], loss=14.8878
	step [210/325], loss=14.6366
	step [211/325], loss=15.0814
	step [212/325], loss=13.6862
	step [213/325], loss=14.6387
	step [214/325], loss=15.0668
	step [215/325], loss=13.5763
	step [216/325], loss=14.4478
	step [217/325], loss=13.0676
	step [218/325], loss=14.9190
	step [219/325], loss=13.9262
	step [220/325], loss=14.1474
	step [221/325], loss=15.1757
	step [222/325], loss=14.2829
	step [223/325], loss=12.9267
	step [224/325], loss=15.9768
	step [225/325], loss=12.3322
	step [226/325], loss=13.0767
	step [227/325], loss=12.9141
	step [228/325], loss=12.5534
	step [229/325], loss=13.7325
	step [230/325], loss=14.5156
	step [231/325], loss=12.0010
	step [232/325], loss=13.7499
	step [233/325], loss=12.1474
	step [234/325], loss=12.4670
	step [235/325], loss=12.6247
	step [236/325], loss=13.3020
	step [237/325], loss=13.6723
	step [238/325], loss=14.1226
	step [239/325], loss=12.4100
	step [240/325], loss=13.6038
	step [241/325], loss=14.7891
	step [242/325], loss=12.6183
	step [243/325], loss=12.0543
	step [244/325], loss=14.3834
	step [245/325], loss=13.8491
	step [246/325], loss=13.7790
	step [247/325], loss=13.0352
	step [248/325], loss=13.0754
	step [249/325], loss=14.1245
	step [250/325], loss=12.2831
	step [251/325], loss=13.6206
	step [252/325], loss=14.7163
	step [253/325], loss=13.1401
	step [254/325], loss=13.2064
	step [255/325], loss=14.1261
	step [256/325], loss=12.5114
	step [257/325], loss=12.7700
	step [258/325], loss=13.1077
	step [259/325], loss=13.5323
	step [260/325], loss=12.8245
	step [261/325], loss=12.7841
	step [262/325], loss=11.4599
	step [263/325], loss=13.0795
	step [264/325], loss=12.2834
	step [265/325], loss=12.3071
	step [266/325], loss=12.0746
	step [267/325], loss=12.7861
	step [268/325], loss=13.2810
	step [269/325], loss=12.6663
	step [270/325], loss=13.8950
	step [271/325], loss=12.8326
	step [272/325], loss=13.2260
	step [273/325], loss=13.3789
	step [274/325], loss=12.1661
	step [275/325], loss=12.6919
	step [276/325], loss=14.1916
	step [277/325], loss=12.2314
	step [278/325], loss=12.6788
	step [279/325], loss=13.9184
	step [280/325], loss=12.4679
	step [281/325], loss=13.1150
	step [282/325], loss=12.5753
	step [283/325], loss=12.8792
	step [284/325], loss=12.1220
	step [285/325], loss=13.7363
	step [286/325], loss=12.7944
	step [287/325], loss=12.1602
	step [288/325], loss=12.4226
	step [289/325], loss=12.7840
	step [290/325], loss=14.3305
	step [291/325], loss=13.7245
	step [292/325], loss=12.5897
	step [293/325], loss=11.9842
	step [294/325], loss=13.1652
	step [295/325], loss=12.7473
	step [296/325], loss=11.3081
	step [297/325], loss=11.8425
	step [298/325], loss=12.9452
	step [299/325], loss=13.1231
	step [300/325], loss=12.7494
	step [301/325], loss=13.0141
	step [302/325], loss=12.1612
	step [303/325], loss=12.6821
	step [304/325], loss=11.9320
	step [305/325], loss=14.0405
	step [306/325], loss=13.7709
	step [307/325], loss=13.6308
	step [308/325], loss=12.0210
	step [309/325], loss=12.2592
	step [310/325], loss=13.2024
	step [311/325], loss=11.9298
	step [312/325], loss=12.9785
	step [313/325], loss=13.0199
	step [314/325], loss=15.4812
	step [315/325], loss=14.0360
	step [316/325], loss=13.5233
	step [317/325], loss=12.7632
	step [318/325], loss=12.5065
	step [319/325], loss=15.4115
	step [320/325], loss=13.2694
	step [321/325], loss=11.1770
	step [322/325], loss=12.4684
	step [323/325], loss=12.8894
	step [324/325], loss=11.8957
	step [325/325], loss=6.8489
	Evaluating
	loss=0.0507, precision=0.2236, recall=0.9945, f1=0.3651
saving model as: 1_saved_model.pth
Training epoch 7
	step [1/325], loss=14.8872
	step [2/325], loss=11.5900
	step [3/325], loss=14.7259
	step [4/325], loss=14.3391
	step [5/325], loss=11.6890
	step [6/325], loss=11.8653
	step [7/325], loss=14.2312
	step [8/325], loss=12.5913
	step [9/325], loss=11.8174
	step [10/325], loss=11.6015
	step [11/325], loss=11.5324
	step [12/325], loss=14.0400
	step [13/325], loss=13.8391
	step [14/325], loss=11.0206
	step [15/325], loss=11.7502
	step [16/325], loss=12.8417
	step [17/325], loss=13.2334
	step [18/325], loss=13.3519
	step [19/325], loss=13.6273
	step [20/325], loss=14.1577
	step [21/325], loss=12.5777
	step [22/325], loss=11.2494
	step [23/325], loss=13.0532
	step [24/325], loss=12.1609
	step [25/325], loss=14.3473
	step [26/325], loss=11.2660
	step [27/325], loss=14.5423
	step [28/325], loss=12.2229
	step [29/325], loss=13.0505
	step [30/325], loss=12.4132
	step [31/325], loss=12.9688
	step [32/325], loss=12.3424
	step [33/325], loss=12.3541
	step [34/325], loss=11.4156
	step [35/325], loss=12.9870
	step [36/325], loss=12.8927
	step [37/325], loss=12.5319
	step [38/325], loss=12.7624
	step [39/325], loss=14.1834
	step [40/325], loss=12.3058
	step [41/325], loss=14.6078
	step [42/325], loss=13.2802
	step [43/325], loss=11.8179
	step [44/325], loss=11.3455
	step [45/325], loss=10.7866
	step [46/325], loss=12.3576
	step [47/325], loss=11.0292
	step [48/325], loss=12.2042
	step [49/325], loss=14.6840
	step [50/325], loss=11.9679
	step [51/325], loss=11.9887
	step [52/325], loss=12.0026
	step [53/325], loss=11.5450
	step [54/325], loss=12.9675
	step [55/325], loss=10.8983
	step [56/325], loss=12.9753
	step [57/325], loss=14.0295
	step [58/325], loss=12.5311
	step [59/325], loss=13.3033
	step [60/325], loss=12.5147
	step [61/325], loss=13.9423
	step [62/325], loss=11.8977
	step [63/325], loss=12.7044
	step [64/325], loss=10.9930
	step [65/325], loss=10.9684
	step [66/325], loss=13.1783
	step [67/325], loss=12.2619
	step [68/325], loss=11.6617
	step [69/325], loss=12.8117
	step [70/325], loss=12.3543
	step [71/325], loss=11.4343
	step [72/325], loss=11.4893
	step [73/325], loss=12.2981
	step [74/325], loss=12.9143
	step [75/325], loss=13.6904
	step [76/325], loss=12.0649
	step [77/325], loss=13.1752
	step [78/325], loss=13.6982
	step [79/325], loss=12.2275
	step [80/325], loss=12.7119
	step [81/325], loss=14.1418
	step [82/325], loss=12.1067
	step [83/325], loss=12.8119
	step [84/325], loss=12.1249
	step [85/325], loss=11.8507
	step [86/325], loss=13.8415
	step [87/325], loss=12.8166
	step [88/325], loss=13.4402
	step [89/325], loss=12.8244
	step [90/325], loss=12.4718
	step [91/325], loss=12.6618
	step [92/325], loss=12.2400
	step [93/325], loss=12.0813
	step [94/325], loss=13.1719
	step [95/325], loss=13.6248
	step [96/325], loss=11.8915
	step [97/325], loss=12.5513
	step [98/325], loss=11.6241
	step [99/325], loss=11.6208
	step [100/325], loss=12.8610
	step [101/325], loss=11.9866
	step [102/325], loss=13.1988
	step [103/325], loss=11.2555
	step [104/325], loss=13.3504
	step [105/325], loss=14.3586
	step [106/325], loss=12.3222
	step [107/325], loss=11.9849
	step [108/325], loss=10.7289
	step [109/325], loss=12.6035
	step [110/325], loss=11.6406
	step [111/325], loss=11.8755
	step [112/325], loss=11.6287
	step [113/325], loss=13.7517
	step [114/325], loss=9.6591
	step [115/325], loss=11.9955
	step [116/325], loss=10.5358
	step [117/325], loss=11.7391
	step [118/325], loss=12.7778
	step [119/325], loss=14.2897
	step [120/325], loss=13.6302
	step [121/325], loss=11.5536
	step [122/325], loss=11.5381
	step [123/325], loss=10.9377
	step [124/325], loss=12.6491
	step [125/325], loss=11.8187
	step [126/325], loss=11.5215
	step [127/325], loss=12.2822
	step [128/325], loss=12.7252
	step [129/325], loss=10.9477
	step [130/325], loss=14.0835
	step [131/325], loss=10.0004
	step [132/325], loss=10.8461
	step [133/325], loss=12.1665
	step [134/325], loss=12.1664
	step [135/325], loss=11.2732
	step [136/325], loss=11.9269
	step [137/325], loss=12.0229
	step [138/325], loss=12.6392
	step [139/325], loss=11.2268
	step [140/325], loss=13.0652
	step [141/325], loss=12.9294
	step [142/325], loss=11.2312
	step [143/325], loss=11.8988
	step [144/325], loss=13.4893
	step [145/325], loss=10.7851
	step [146/325], loss=11.3677
	step [147/325], loss=11.2001
	step [148/325], loss=11.3546
	step [149/325], loss=12.1871
	step [150/325], loss=10.2495
	step [151/325], loss=12.7959
	step [152/325], loss=12.6601
	step [153/325], loss=10.5221
	step [154/325], loss=11.5511
	step [155/325], loss=13.6529
	step [156/325], loss=11.7772
	step [157/325], loss=9.8348
	step [158/325], loss=12.4088
	step [159/325], loss=13.2083
	step [160/325], loss=10.3153
	step [161/325], loss=11.2530
	step [162/325], loss=12.3919
	step [163/325], loss=11.1032
	step [164/325], loss=10.5479
	step [165/325], loss=12.5523
	step [166/325], loss=10.2179
	step [167/325], loss=12.9185
	step [168/325], loss=12.5679
	step [169/325], loss=11.1433
	step [170/325], loss=10.3905
	step [171/325], loss=13.4415
	step [172/325], loss=12.5823
	step [173/325], loss=11.7503
	step [174/325], loss=13.7898
	step [175/325], loss=12.3174
	step [176/325], loss=12.2312
	step [177/325], loss=12.0133
	step [178/325], loss=12.3955
	step [179/325], loss=12.1284
	step [180/325], loss=10.0962
	step [181/325], loss=10.4701
	step [182/325], loss=11.5628
	step [183/325], loss=12.7864
	step [184/325], loss=13.2635
	step [185/325], loss=11.4703
	step [186/325], loss=10.7220
	step [187/325], loss=11.8342
	step [188/325], loss=13.1366
	step [189/325], loss=11.5784
	step [190/325], loss=11.7170
	step [191/325], loss=11.5939
	step [192/325], loss=10.9467
	step [193/325], loss=10.1947
	step [194/325], loss=10.9825
	step [195/325], loss=12.6834
	step [196/325], loss=11.6246
	step [197/325], loss=10.4428
	step [198/325], loss=12.3508
	step [199/325], loss=11.9272
	step [200/325], loss=12.3256
	step [201/325], loss=10.3888
	step [202/325], loss=11.4282
	step [203/325], loss=10.9087
	step [204/325], loss=11.0846
	step [205/325], loss=11.1834
	step [206/325], loss=11.5507
	step [207/325], loss=13.1441
	step [208/325], loss=10.9916
	step [209/325], loss=11.6679
	step [210/325], loss=12.2960
	step [211/325], loss=12.4808
	step [212/325], loss=12.1262
	step [213/325], loss=12.2844
	step [214/325], loss=12.6060
	step [215/325], loss=12.7209
	step [216/325], loss=13.5725
	step [217/325], loss=11.3144
	step [218/325], loss=11.1326
	step [219/325], loss=12.0391
	step [220/325], loss=11.9300
	step [221/325], loss=12.7798
	step [222/325], loss=10.2931
	step [223/325], loss=11.5417
	step [224/325], loss=12.3145
	step [225/325], loss=11.1194
	step [226/325], loss=11.0824
	step [227/325], loss=12.9598
	step [228/325], loss=10.7873
	step [229/325], loss=9.9960
	step [230/325], loss=11.7713
	step [231/325], loss=11.9891
	step [232/325], loss=9.7045
	step [233/325], loss=11.0579
	step [234/325], loss=10.7195
	step [235/325], loss=11.2193
	step [236/325], loss=11.4592
	step [237/325], loss=10.5009
	step [238/325], loss=10.3340
	step [239/325], loss=12.4935
	step [240/325], loss=10.8077
	step [241/325], loss=10.1386
	step [242/325], loss=11.8090
	step [243/325], loss=14.5386
	step [244/325], loss=10.6045
	step [245/325], loss=12.8353
	step [246/325], loss=11.3831
	step [247/325], loss=9.4016
	step [248/325], loss=10.9111
	step [249/325], loss=11.4501
	step [250/325], loss=11.7085
	step [251/325], loss=11.2605
	step [252/325], loss=10.9402
	step [253/325], loss=10.9381
	step [254/325], loss=10.3868
	step [255/325], loss=11.4998
	step [256/325], loss=10.9164
	step [257/325], loss=11.2247
	step [258/325], loss=11.7769
	step [259/325], loss=11.3047
	step [260/325], loss=10.4114
	step [261/325], loss=11.6560
	step [262/325], loss=13.5935
	step [263/325], loss=11.3064
	step [264/325], loss=11.9044
	step [265/325], loss=11.4554
	step [266/325], loss=10.9868
	step [267/325], loss=14.3053
	step [268/325], loss=10.5661
	step [269/325], loss=13.1232
	step [270/325], loss=12.7286
	step [271/325], loss=10.7869
	step [272/325], loss=11.4647
	step [273/325], loss=10.5996
	step [274/325], loss=9.4789
	step [275/325], loss=11.2446
	step [276/325], loss=10.1848
	step [277/325], loss=11.7062
	step [278/325], loss=10.7444
	step [279/325], loss=11.9284
	step [280/325], loss=12.9681
	step [281/325], loss=10.2044
	step [282/325], loss=10.1290
	step [283/325], loss=10.5833
	step [284/325], loss=9.5955
	step [285/325], loss=11.1913
	step [286/325], loss=11.8152
	step [287/325], loss=11.9007
	step [288/325], loss=11.8849
	step [289/325], loss=10.4649
	step [290/325], loss=12.4108
	step [291/325], loss=10.1657
	step [292/325], loss=14.7274
	step [293/325], loss=12.2856
	step [294/325], loss=10.2207
	step [295/325], loss=9.9542
	step [296/325], loss=10.4626
	step [297/325], loss=10.3380
	step [298/325], loss=12.6891
	step [299/325], loss=10.4228
	step [300/325], loss=10.8631
	step [301/325], loss=9.8237
	step [302/325], loss=11.0454
	step [303/325], loss=11.2311
	step [304/325], loss=12.3513
	step [305/325], loss=10.6764
	step [306/325], loss=10.4798
	step [307/325], loss=11.6154
	step [308/325], loss=8.9189
	step [309/325], loss=11.7366
	step [310/325], loss=10.1933
	step [311/325], loss=10.6657
	step [312/325], loss=9.3795
	step [313/325], loss=12.5259
	step [314/325], loss=9.3311
	step [315/325], loss=9.3001
	step [316/325], loss=11.0039
	step [317/325], loss=9.3110
	step [318/325], loss=10.0286
	step [319/325], loss=11.7085
	step [320/325], loss=11.7881
	step [321/325], loss=9.8822
	step [322/325], loss=11.1570
	step [323/325], loss=10.7508
	step [324/325], loss=10.4999
	step [325/325], loss=5.7207
	Evaluating
	loss=0.0449, precision=0.1876, recall=0.9957, f1=0.3158
Training epoch 8
	step [1/325], loss=9.9293
	step [2/325], loss=10.0720
	step [3/325], loss=8.9634
	step [4/325], loss=12.4989
	step [5/325], loss=12.0998
	step [6/325], loss=9.5874
	step [7/325], loss=10.4106
	step [8/325], loss=11.2370
	step [9/325], loss=10.3021
	step [10/325], loss=8.9764
	step [11/325], loss=10.8249
	step [12/325], loss=9.6399
	step [13/325], loss=12.3810
	step [14/325], loss=11.0780
	step [15/325], loss=10.6812
	step [16/325], loss=9.8136
	step [17/325], loss=10.2236
	step [18/325], loss=10.3286
	step [19/325], loss=10.8957
	step [20/325], loss=11.2473
	step [21/325], loss=10.5613
	step [22/325], loss=10.9621
	step [23/325], loss=9.6707
	step [24/325], loss=10.7082
	step [25/325], loss=9.9906
	step [26/325], loss=11.8917
	step [27/325], loss=11.3030
	step [28/325], loss=10.3228
	step [29/325], loss=9.9061
	step [30/325], loss=11.1938
	step [31/325], loss=9.8711
	step [32/325], loss=10.1817
	step [33/325], loss=10.0099
	step [34/325], loss=8.9031
	step [35/325], loss=14.1590
	step [36/325], loss=10.8008
	step [37/325], loss=10.3193
	step [38/325], loss=11.8509
	step [39/325], loss=9.8102
	step [40/325], loss=11.7011
	step [41/325], loss=9.8329
	step [42/325], loss=10.4850
	step [43/325], loss=11.1157
	step [44/325], loss=12.5639
	step [45/325], loss=9.8077
	step [46/325], loss=10.9106
	step [47/325], loss=11.7330
	step [48/325], loss=11.7713
	step [49/325], loss=9.7129
	step [50/325], loss=13.2079
	step [51/325], loss=11.2120
	step [52/325], loss=9.7732
	step [53/325], loss=10.5161
	step [54/325], loss=9.7866
	step [55/325], loss=9.3373
	step [56/325], loss=9.8461
	step [57/325], loss=10.4128
	step [58/325], loss=13.4247
	step [59/325], loss=14.0422
	step [60/325], loss=10.0322
	step [61/325], loss=10.8852
	step [62/325], loss=9.8689
	step [63/325], loss=9.9218
	step [64/325], loss=10.2298
	step [65/325], loss=11.1423
	step [66/325], loss=10.7967
	step [67/325], loss=11.0458
	step [68/325], loss=11.8489
	step [69/325], loss=9.0081
	step [70/325], loss=10.0412
	step [71/325], loss=10.4251
	step [72/325], loss=10.4471
	step [73/325], loss=10.1319
	step [74/325], loss=10.0391
	step [75/325], loss=10.2522
	step [76/325], loss=11.7087
	step [77/325], loss=10.6083
	step [78/325], loss=11.0193
	step [79/325], loss=10.8253
	step [80/325], loss=10.8287
	step [81/325], loss=13.8093
	step [82/325], loss=9.4172
	step [83/325], loss=11.4452
	step [84/325], loss=10.4973
	step [85/325], loss=8.2984
	step [86/325], loss=9.6200
	step [87/325], loss=12.0233
	step [88/325], loss=10.0153
	step [89/325], loss=11.5865
	step [90/325], loss=11.0608
	step [91/325], loss=10.9995
	step [92/325], loss=9.9035
	step [93/325], loss=10.8082
	step [94/325], loss=9.7471
	step [95/325], loss=11.1868
	step [96/325], loss=10.6464
	step [97/325], loss=11.4322
	step [98/325], loss=8.9909
	step [99/325], loss=10.2171
	step [100/325], loss=9.8517
	step [101/325], loss=9.5998
	step [102/325], loss=10.0393
	step [103/325], loss=11.3947
	step [104/325], loss=10.0259
	step [105/325], loss=11.1316
	step [106/325], loss=12.1609
	step [107/325], loss=8.8718
	step [108/325], loss=9.1341
	step [109/325], loss=10.3582
	step [110/325], loss=11.5021
	step [111/325], loss=10.0796
	step [112/325], loss=11.9022
	step [113/325], loss=10.7214
	step [114/325], loss=9.9528
	step [115/325], loss=11.7006
	step [116/325], loss=11.7609
	step [117/325], loss=9.4572
	step [118/325], loss=9.6280
	step [119/325], loss=8.7323
	step [120/325], loss=11.4664
	step [121/325], loss=10.4638
	step [122/325], loss=11.1448
	step [123/325], loss=11.8908
	step [124/325], loss=11.0509
	step [125/325], loss=10.1237
	step [126/325], loss=10.2680
	step [127/325], loss=10.4855
	step [128/325], loss=11.6779
	step [129/325], loss=11.0641
	step [130/325], loss=10.5494
	step [131/325], loss=11.4086
	step [132/325], loss=9.7201
	step [133/325], loss=10.3606
	step [134/325], loss=11.0605
	step [135/325], loss=10.0960
	step [136/325], loss=10.0631
	step [137/325], loss=9.8842
	step [138/325], loss=10.0968
	step [139/325], loss=9.5473
	step [140/325], loss=9.5224
	step [141/325], loss=11.4228
	step [142/325], loss=10.8348
	step [143/325], loss=11.0441
	step [144/325], loss=10.1565
	step [145/325], loss=10.5019
	step [146/325], loss=10.9925
	step [147/325], loss=9.2742
	step [148/325], loss=10.3347
	step [149/325], loss=8.9230
	step [150/325], loss=9.6938
	step [151/325], loss=10.1342
	step [152/325], loss=10.3532
	step [153/325], loss=11.1314
	step [154/325], loss=11.3606
	step [155/325], loss=11.1035
	step [156/325], loss=9.0755
	step [157/325], loss=9.4777
	step [158/325], loss=9.4108
	step [159/325], loss=11.9679
	step [160/325], loss=11.1083
	step [161/325], loss=9.4863
	step [162/325], loss=8.9363
	step [163/325], loss=9.6121
	step [164/325], loss=11.3480
	step [165/325], loss=11.1428
	step [166/325], loss=10.9925
	step [167/325], loss=10.3815
	step [168/325], loss=9.7332
	step [169/325], loss=12.2388
	step [170/325], loss=10.3332
	step [171/325], loss=9.6606
	step [172/325], loss=9.6857
	step [173/325], loss=9.0186
	step [174/325], loss=11.5012
	step [175/325], loss=9.3253
	step [176/325], loss=10.4465
	step [177/325], loss=8.6474
	step [178/325], loss=11.2978
	step [179/325], loss=8.4329
	step [180/325], loss=9.0580
	step [181/325], loss=10.2248
	step [182/325], loss=7.8684
	step [183/325], loss=9.6152
	step [184/325], loss=10.5745
	step [185/325], loss=9.5566
	step [186/325], loss=9.3448
	step [187/325], loss=9.9567
	step [188/325], loss=10.6479
	step [189/325], loss=11.2250
	step [190/325], loss=7.9211
	step [191/325], loss=10.3322
	step [192/325], loss=11.3024
	step [193/325], loss=11.0453
	step [194/325], loss=8.9725
	step [195/325], loss=9.6125
	step [196/325], loss=10.3750
	step [197/325], loss=9.0361
	step [198/325], loss=10.6376
	step [199/325], loss=9.2160
	step [200/325], loss=10.7794
	step [201/325], loss=10.9014
	step [202/325], loss=8.5459
	step [203/325], loss=11.1748
	step [204/325], loss=9.0844
	step [205/325], loss=11.3044
	step [206/325], loss=10.0874
	step [207/325], loss=11.6056
	step [208/325], loss=10.8016
	step [209/325], loss=8.5048
	step [210/325], loss=9.1071
	step [211/325], loss=9.9838
	step [212/325], loss=11.1745
	step [213/325], loss=9.7754
	step [214/325], loss=8.8681
	step [215/325], loss=10.5552
	step [216/325], loss=10.2160
	step [217/325], loss=8.3394
	step [218/325], loss=9.9406
	step [219/325], loss=8.4657
	step [220/325], loss=10.1334
	step [221/325], loss=10.6862
	step [222/325], loss=10.3525
	step [223/325], loss=9.9294
	step [224/325], loss=9.7862
	step [225/325], loss=9.5497
	step [226/325], loss=9.8619
	step [227/325], loss=8.5752
	step [228/325], loss=9.9522
	step [229/325], loss=9.6292
	step [230/325], loss=8.5969
	step [231/325], loss=10.4726
	step [232/325], loss=9.9034
	step [233/325], loss=9.0164
	step [234/325], loss=11.2814
	step [235/325], loss=9.8843
	step [236/325], loss=9.8414
	step [237/325], loss=11.2213
	step [238/325], loss=10.4381
	step [239/325], loss=10.2309
	step [240/325], loss=8.3963
	step [241/325], loss=10.4488
	step [242/325], loss=10.2998
	step [243/325], loss=11.7900
	step [244/325], loss=11.1772
	step [245/325], loss=9.7696
	step [246/325], loss=9.7813
	step [247/325], loss=8.6752
	step [248/325], loss=9.3973
	step [249/325], loss=9.1619
	step [250/325], loss=9.1238
	step [251/325], loss=9.6712
	step [252/325], loss=8.5246
	step [253/325], loss=10.4521
	step [254/325], loss=8.5681
	step [255/325], loss=11.9061
	step [256/325], loss=10.3495
	step [257/325], loss=10.1913
	step [258/325], loss=9.0480
	step [259/325], loss=8.2917
	step [260/325], loss=8.2935
	step [261/325], loss=9.6543
	step [262/325], loss=10.2755
	step [263/325], loss=9.9388
	step [264/325], loss=10.1848
	step [265/325], loss=9.0044
	step [266/325], loss=8.6303
	step [267/325], loss=8.6474
	step [268/325], loss=11.2783
	step [269/325], loss=9.7201
	step [270/325], loss=8.3946
	step [271/325], loss=9.6667
	step [272/325], loss=11.2102
	step [273/325], loss=10.3009
	step [274/325], loss=9.9389
	step [275/325], loss=11.4784
	step [276/325], loss=8.6533
	step [277/325], loss=10.5229
	step [278/325], loss=10.7398
	step [279/325], loss=10.3987
	step [280/325], loss=9.4928
	step [281/325], loss=9.6277
	step [282/325], loss=9.5293
	step [283/325], loss=8.8529
	step [284/325], loss=9.4423
	step [285/325], loss=9.1959
	step [286/325], loss=7.7017
	step [287/325], loss=7.2859
	step [288/325], loss=10.3196
	step [289/325], loss=10.8016
	step [290/325], loss=9.9769
	step [291/325], loss=10.1624
	step [292/325], loss=9.5941
	step [293/325], loss=9.4864
	step [294/325], loss=8.7813
	step [295/325], loss=7.9049
	step [296/325], loss=8.7998
	step [297/325], loss=8.4087
	step [298/325], loss=9.8869
	step [299/325], loss=8.1130
	step [300/325], loss=9.3133
	step [301/325], loss=9.0872
	step [302/325], loss=8.7053
	step [303/325], loss=10.4405
	step [304/325], loss=11.6313
	step [305/325], loss=9.0532
	step [306/325], loss=10.5657
	step [307/325], loss=9.0990
	step [308/325], loss=9.6435
	step [309/325], loss=8.9368
	step [310/325], loss=9.8825
	step [311/325], loss=12.9439
	step [312/325], loss=8.7685
	step [313/325], loss=10.1713
	step [314/325], loss=10.1525
	step [315/325], loss=9.7331
	step [316/325], loss=10.1253
	step [317/325], loss=9.5725
	step [318/325], loss=9.5696
	step [319/325], loss=11.7121
	step [320/325], loss=8.6956
	step [321/325], loss=8.8437
	step [322/325], loss=8.2038
	step [323/325], loss=7.1338
	step [324/325], loss=8.4166
	step [325/325], loss=5.7303
	Evaluating
	loss=0.0359, precision=0.2096, recall=0.9953, f1=0.3462
Training epoch 9
	step [1/325], loss=8.1597
	step [2/325], loss=9.6335
	step [3/325], loss=9.9426
	step [4/325], loss=10.9158
	step [5/325], loss=10.2467
	step [6/325], loss=10.2073
	step [7/325], loss=8.6401
	step [8/325], loss=10.6930
	step [9/325], loss=11.1666
	step [10/325], loss=9.5198
	step [11/325], loss=7.6943
	step [12/325], loss=9.8156
	step [13/325], loss=8.7206
	step [14/325], loss=9.4401
	step [15/325], loss=10.8824
	step [16/325], loss=10.3844
	step [17/325], loss=9.3065
	step [18/325], loss=10.3481
	step [19/325], loss=8.2942
	step [20/325], loss=8.6290
	step [21/325], loss=9.7060
	step [22/325], loss=8.3317
	step [23/325], loss=9.4339
	step [24/325], loss=11.5157
	step [25/325], loss=10.0634
	step [26/325], loss=9.6846
	step [27/325], loss=9.3288
	step [28/325], loss=8.4030
	step [29/325], loss=9.7197
	step [30/325], loss=9.8923
	step [31/325], loss=9.5090
	step [32/325], loss=10.3861
	step [33/325], loss=10.3674
	step [34/325], loss=9.0547
	step [35/325], loss=10.4737
	step [36/325], loss=9.0070
	step [37/325], loss=9.6485
	step [38/325], loss=10.0579
	step [39/325], loss=10.1206
	step [40/325], loss=8.3956
	step [41/325], loss=8.9669
	step [42/325], loss=10.4916
	step [43/325], loss=9.5451
	step [44/325], loss=8.8238
	step [45/325], loss=9.4158
	step [46/325], loss=11.5175
	step [47/325], loss=10.4160
	step [48/325], loss=9.5608
	step [49/325], loss=8.4129
	step [50/325], loss=9.8356
	step [51/325], loss=8.5382
	step [52/325], loss=8.6523
	step [53/325], loss=9.4947
	step [54/325], loss=10.1305
	step [55/325], loss=9.2405
	step [56/325], loss=10.7366
	step [57/325], loss=10.4863
	step [58/325], loss=7.6653
	step [59/325], loss=9.0089
	step [60/325], loss=10.0421
	step [61/325], loss=7.9833
	step [62/325], loss=10.7103
	step [63/325], loss=9.0082
	step [64/325], loss=11.2574
	step [65/325], loss=9.2979
	step [66/325], loss=10.7273
	step [67/325], loss=10.9408
	step [68/325], loss=10.5726
	step [69/325], loss=9.1867
	step [70/325], loss=8.0985
	step [71/325], loss=8.2510
	step [72/325], loss=8.4435
	step [73/325], loss=11.0857
	step [74/325], loss=8.4316
	step [75/325], loss=11.7339
	step [76/325], loss=10.5878
	step [77/325], loss=9.7727
	step [78/325], loss=9.6663
	step [79/325], loss=9.0663
	step [80/325], loss=9.5775
	step [81/325], loss=8.2389
	step [82/325], loss=10.0576
	step [83/325], loss=8.8885
	step [84/325], loss=9.4714
	step [85/325], loss=12.4341
	step [86/325], loss=11.0791
	step [87/325], loss=9.3383
	step [88/325], loss=10.0037
	step [89/325], loss=8.2938
	step [90/325], loss=9.3335
	step [91/325], loss=8.5000
	step [92/325], loss=8.4650
	step [93/325], loss=8.2976
	step [94/325], loss=12.9405
	step [95/325], loss=9.1041
	step [96/325], loss=7.6322
	step [97/325], loss=8.4277
	step [98/325], loss=12.4357
	step [99/325], loss=8.9400
	step [100/325], loss=9.3589
	step [101/325], loss=8.7043
	step [102/325], loss=8.0576
	step [103/325], loss=10.3604
	step [104/325], loss=8.5630
	step [105/325], loss=8.7085
	step [106/325], loss=9.8345
	step [107/325], loss=10.0337
	step [108/325], loss=13.4411
	step [109/325], loss=9.3369
	step [110/325], loss=9.4835
	step [111/325], loss=8.4654
	step [112/325], loss=9.1845
	step [113/325], loss=9.6764
	step [114/325], loss=8.3644
	step [115/325], loss=10.3271
	step [116/325], loss=9.4866
	step [117/325], loss=9.4418
	step [118/325], loss=8.9225
	step [119/325], loss=9.4352
	step [120/325], loss=9.0761
	step [121/325], loss=8.1447
	step [122/325], loss=9.2816
	step [123/325], loss=10.9538
	step [124/325], loss=8.6084
	step [125/325], loss=8.9510
	step [126/325], loss=8.7770
	step [127/325], loss=9.0199
	step [128/325], loss=8.3079
	step [129/325], loss=8.0029
	step [130/325], loss=8.0119
	step [131/325], loss=8.7804
	step [132/325], loss=7.8230
	step [133/325], loss=9.7887
	step [134/325], loss=8.6216
	step [135/325], loss=9.6611
	step [136/325], loss=10.3229
	step [137/325], loss=10.5778
	step [138/325], loss=9.0182
	step [139/325], loss=9.4195
	step [140/325], loss=8.5891
	step [141/325], loss=8.6734
	step [142/325], loss=9.8999
	step [143/325], loss=10.1246
	step [144/325], loss=8.9026
	step [145/325], loss=8.9863
	step [146/325], loss=7.8464
	step [147/325], loss=9.1017
	step [148/325], loss=9.1318
	step [149/325], loss=8.3644
	step [150/325], loss=8.1557
	step [151/325], loss=8.5870
	step [152/325], loss=8.4061
	step [153/325], loss=8.9158
	step [154/325], loss=7.5451
	step [155/325], loss=7.6972
	step [156/325], loss=10.4264
	step [157/325], loss=10.7273
	step [158/325], loss=8.6133
	step [159/325], loss=7.6722
	step [160/325], loss=10.4794
	step [161/325], loss=8.6878
	step [162/325], loss=8.9994
	step [163/325], loss=8.9600
	step [164/325], loss=9.2412
	step [165/325], loss=10.2243
	step [166/325], loss=9.3851
	step [167/325], loss=8.8780
	step [168/325], loss=9.7068
	step [169/325], loss=8.6575
	step [170/325], loss=9.9011
	step [171/325], loss=8.5504
	step [172/325], loss=8.6358
	step [173/325], loss=9.7198
	step [174/325], loss=8.5275
	step [175/325], loss=8.2776
	step [176/325], loss=9.3728
	step [177/325], loss=7.9810
	step [178/325], loss=8.6285
	step [179/325], loss=7.8263
	step [180/325], loss=7.5314
	step [181/325], loss=7.2404
	step [182/325], loss=8.4560
	step [183/325], loss=8.0901
	step [184/325], loss=7.8164
	step [185/325], loss=8.8105
	step [186/325], loss=8.1145
	step [187/325], loss=10.4056
	step [188/325], loss=9.5960
	step [189/325], loss=8.5008
	step [190/325], loss=9.5385
	step [191/325], loss=9.9942
	step [192/325], loss=7.8452
	step [193/325], loss=8.5570
	step [194/325], loss=8.7120
	step [195/325], loss=7.8139
	step [196/325], loss=9.0171
	step [197/325], loss=9.9406
	step [198/325], loss=7.7437
	step [199/325], loss=8.4534
	step [200/325], loss=8.2874
	step [201/325], loss=7.8474
	step [202/325], loss=8.6374
	step [203/325], loss=10.4192
	step [204/325], loss=8.5510
	step [205/325], loss=9.3919
	step [206/325], loss=8.7622
	step [207/325], loss=9.1433
	step [208/325], loss=9.1869
	step [209/325], loss=8.8656
	step [210/325], loss=9.2743
	step [211/325], loss=7.9706
	step [212/325], loss=8.9882
	step [213/325], loss=8.1087
	step [214/325], loss=8.0453
	step [215/325], loss=7.1137
	step [216/325], loss=9.8314
	step [217/325], loss=9.7548
	step [218/325], loss=10.0166
	step [219/325], loss=9.1236
	step [220/325], loss=9.8594
	step [221/325], loss=8.6391
	step [222/325], loss=8.7004
	step [223/325], loss=9.6530
	step [224/325], loss=8.5060
	step [225/325], loss=8.6394
	step [226/325], loss=7.5289
	step [227/325], loss=9.2990
	step [228/325], loss=8.6723
	step [229/325], loss=9.5250
	step [230/325], loss=6.8763
	step [231/325], loss=9.9106
	step [232/325], loss=8.0782
	step [233/325], loss=7.7989
	step [234/325], loss=7.3523
	step [235/325], loss=8.7440
	step [236/325], loss=9.1165
	step [237/325], loss=8.0611
	step [238/325], loss=10.8258
	step [239/325], loss=9.8945
	step [240/325], loss=8.1876
	step [241/325], loss=9.7215
	step [242/325], loss=7.5582
	step [243/325], loss=8.3641
	step [244/325], loss=8.3563
	step [245/325], loss=9.1171
	step [246/325], loss=8.8910
	step [247/325], loss=8.2592
	step [248/325], loss=9.7220
	step [249/325], loss=8.3906
	step [250/325], loss=8.1515
	step [251/325], loss=9.0170
	step [252/325], loss=12.5646
	step [253/325], loss=9.8070
	step [254/325], loss=8.1488
	step [255/325], loss=8.0911
	step [256/325], loss=9.0816
	step [257/325], loss=8.5860
	step [258/325], loss=7.4280
	step [259/325], loss=9.3293
	step [260/325], loss=10.0429
	step [261/325], loss=8.3251
	step [262/325], loss=8.1781
	step [263/325], loss=8.8577
	step [264/325], loss=7.2814
	step [265/325], loss=6.8696
	step [266/325], loss=8.8342
	step [267/325], loss=9.5517
	step [268/325], loss=8.1202
	step [269/325], loss=10.8258
	step [270/325], loss=8.9703
	step [271/325], loss=10.5855
	step [272/325], loss=8.6399
	step [273/325], loss=9.3848
	step [274/325], loss=7.2428
	step [275/325], loss=7.9914
	step [276/325], loss=8.2500
	step [277/325], loss=8.6170
	step [278/325], loss=8.7442
	step [279/325], loss=8.1684
	step [280/325], loss=9.0889
	step [281/325], loss=7.8137
	step [282/325], loss=7.3334
	step [283/325], loss=9.9459
	step [284/325], loss=8.4714
	step [285/325], loss=10.6267
	step [286/325], loss=9.2694
	step [287/325], loss=8.8446
	step [288/325], loss=11.4312
	step [289/325], loss=7.6554
	step [290/325], loss=8.4241
	step [291/325], loss=8.2984
	step [292/325], loss=8.0531
	step [293/325], loss=8.2386
	step [294/325], loss=9.5374
	step [295/325], loss=10.7694
	step [296/325], loss=9.1337
	step [297/325], loss=9.1362
	step [298/325], loss=8.0585
	step [299/325], loss=9.3923
	step [300/325], loss=8.8405
	step [301/325], loss=9.7988
	step [302/325], loss=8.1536
	step [303/325], loss=8.5456
	step [304/325], loss=8.6783
	step [305/325], loss=9.1571
	step [306/325], loss=9.2791
	step [307/325], loss=9.4510
	step [308/325], loss=7.7785
	step [309/325], loss=8.0585
	step [310/325], loss=9.8982
	step [311/325], loss=8.1612
	step [312/325], loss=9.2172
	step [313/325], loss=8.7591
	step [314/325], loss=8.4983
	step [315/325], loss=7.6994
	step [316/325], loss=7.8368
	step [317/325], loss=7.5065
	step [318/325], loss=8.5155
	step [319/325], loss=6.9091
	step [320/325], loss=8.0492
	step [321/325], loss=9.5427
	step [322/325], loss=9.1829
	step [323/325], loss=8.7639
	step [324/325], loss=9.3017
	step [325/325], loss=4.0515
	Evaluating
	loss=0.0352, precision=0.1816, recall=0.9965, f1=0.3072
Training epoch 10
	step [1/325], loss=12.0833
	step [2/325], loss=7.8459
	step [3/325], loss=8.9350
	step [4/325], loss=11.1594
	step [5/325], loss=7.7513
	step [6/325], loss=8.5765
	step [7/325], loss=9.4915
	step [8/325], loss=8.5143
	step [9/325], loss=8.0301
	step [10/325], loss=7.0083
	step [11/325], loss=8.4079
	step [12/325], loss=8.0972
	step [13/325], loss=9.1214
	step [14/325], loss=7.6469
	step [15/325], loss=9.1209
	step [16/325], loss=7.1873
	step [17/325], loss=9.4342
	step [18/325], loss=7.4686
	step [19/325], loss=9.6287
	step [20/325], loss=9.3243
	step [21/325], loss=8.9610
	step [22/325], loss=12.0203
	step [23/325], loss=9.2608
	step [24/325], loss=9.3255
	step [25/325], loss=8.1120
	step [26/325], loss=8.6992
	step [27/325], loss=8.5131
	step [28/325], loss=10.8103
	step [29/325], loss=8.7218
	step [30/325], loss=8.6372
	step [31/325], loss=8.4213
	step [32/325], loss=9.3864
	step [33/325], loss=8.8454
	step [34/325], loss=9.1061
	step [35/325], loss=8.9684
	step [36/325], loss=7.4206
	step [37/325], loss=9.9465
	step [38/325], loss=8.7433
	step [39/325], loss=10.1651
	step [40/325], loss=9.4118
	step [41/325], loss=7.5291
	step [42/325], loss=9.6123
	step [43/325], loss=10.1654
	step [44/325], loss=9.5376
	step [45/325], loss=8.2123
	step [46/325], loss=9.0146
	step [47/325], loss=6.8408
	step [48/325], loss=8.4596
	step [49/325], loss=10.4750
	step [50/325], loss=8.1598
	step [51/325], loss=9.6210
	step [52/325], loss=7.9496
	step [53/325], loss=9.0266
	step [54/325], loss=8.2391
	step [55/325], loss=8.3563
	step [56/325], loss=8.7998
	step [57/325], loss=6.5267
	step [58/325], loss=9.5806
	step [59/325], loss=8.5992
	step [60/325], loss=7.9790
	step [61/325], loss=7.7029
	step [62/325], loss=7.7760
	step [63/325], loss=7.7232
	step [64/325], loss=9.3138
	step [65/325], loss=6.6732
	step [66/325], loss=10.3569
	step [67/325], loss=8.7161
	step [68/325], loss=8.3043
	step [69/325], loss=10.2178
	step [70/325], loss=9.5240
	step [71/325], loss=9.1965
	step [72/325], loss=9.3457
	step [73/325], loss=8.0930
	step [74/325], loss=8.1351
	step [75/325], loss=8.7304
	step [76/325], loss=7.9926
	step [77/325], loss=9.3837
	step [78/325], loss=8.8184
	step [79/325], loss=9.2636
	step [80/325], loss=7.9156
	step [81/325], loss=8.0416
	step [82/325], loss=7.2997
	step [83/325], loss=10.2988
	step [84/325], loss=8.3984
	step [85/325], loss=6.7751
	step [86/325], loss=7.1129
	step [87/325], loss=8.9359
	step [88/325], loss=6.9202
	step [89/325], loss=8.7353
	step [90/325], loss=8.1097
	step [91/325], loss=8.2632
	step [92/325], loss=9.5712
	step [93/325], loss=7.8590
	step [94/325], loss=7.7258
	step [95/325], loss=7.3017
	step [96/325], loss=7.7465
	step [97/325], loss=9.3071
	step [98/325], loss=6.8370
	step [99/325], loss=9.6425
	step [100/325], loss=9.7512
	step [101/325], loss=7.7101
	step [102/325], loss=7.3478
	step [103/325], loss=7.6412
	step [104/325], loss=9.8520
	step [105/325], loss=9.9996
	step [106/325], loss=8.9665
	step [107/325], loss=8.3665
	step [108/325], loss=7.2109
	step [109/325], loss=8.0386
	step [110/325], loss=7.5742
	step [111/325], loss=7.0057
	step [112/325], loss=9.0794
	step [113/325], loss=8.9674
	step [114/325], loss=7.3444
	step [115/325], loss=8.6325
	step [116/325], loss=8.7540
	step [117/325], loss=8.6037
	step [118/325], loss=8.1527
	step [119/325], loss=7.6920
	step [120/325], loss=8.6416
	step [121/325], loss=7.1450
	step [122/325], loss=7.4827
	step [123/325], loss=10.0901
	step [124/325], loss=8.5054
	step [125/325], loss=7.5895
	step [126/325], loss=8.9268
	step [127/325], loss=7.8839
	step [128/325], loss=9.6798
	step [129/325], loss=9.5179
	step [130/325], loss=11.2163
	step [131/325], loss=8.7134
	step [132/325], loss=8.7361
	step [133/325], loss=8.7862
	step [134/325], loss=8.8045
	step [135/325], loss=7.4032
	step [136/325], loss=8.3144
	step [137/325], loss=7.9624
	step [138/325], loss=7.9746
	step [139/325], loss=6.7080
	step [140/325], loss=7.1149
	step [141/325], loss=9.6069
	step [142/325], loss=10.1161
	step [143/325], loss=9.1722
	step [144/325], loss=6.5565
	step [145/325], loss=7.9735
	step [146/325], loss=8.3141
	step [147/325], loss=7.6111
	step [148/325], loss=8.0212
	step [149/325], loss=8.2185
	step [150/325], loss=6.4592
	step [151/325], loss=7.5644
	step [152/325], loss=7.1691
	step [153/325], loss=7.9724
	step [154/325], loss=8.6343
	step [155/325], loss=9.3373
	step [156/325], loss=9.2420
	step [157/325], loss=8.1543
	step [158/325], loss=9.7549
	step [159/325], loss=7.0732
	step [160/325], loss=10.3512
	step [161/325], loss=9.7637
	step [162/325], loss=9.9171
	step [163/325], loss=9.9543
	step [164/325], loss=8.9184
	step [165/325], loss=7.5380
	step [166/325], loss=7.1738
	step [167/325], loss=9.0256
	step [168/325], loss=8.2581
	step [169/325], loss=8.4932
	step [170/325], loss=7.9627
	step [171/325], loss=8.8701
	step [172/325], loss=8.9230
	step [173/325], loss=8.9570
	step [174/325], loss=8.0109
	step [175/325], loss=6.6913
	step [176/325], loss=9.6129
	step [177/325], loss=6.6739
	step [178/325], loss=7.3509
	step [179/325], loss=8.8687
	step [180/325], loss=8.9932
	step [181/325], loss=8.0019
	step [182/325], loss=7.3801
	step [183/325], loss=8.3162
	step [184/325], loss=7.7326
	step [185/325], loss=8.7290
	step [186/325], loss=9.2362
	step [187/325], loss=6.9416
	step [188/325], loss=7.0062
	step [189/325], loss=7.2543
	step [190/325], loss=7.7463
	step [191/325], loss=8.1880
	step [192/325], loss=7.4106
	step [193/325], loss=7.9241
	step [194/325], loss=8.2760
	step [195/325], loss=8.8014
	step [196/325], loss=6.9477
	step [197/325], loss=9.3665
	step [198/325], loss=8.3618
	step [199/325], loss=7.4194
	step [200/325], loss=7.5059
	step [201/325], loss=7.4458
	step [202/325], loss=8.5160
	step [203/325], loss=8.3269
	step [204/325], loss=8.3393
	step [205/325], loss=7.5987
	step [206/325], loss=8.0355
	step [207/325], loss=8.4939
	step [208/325], loss=8.0771
	step [209/325], loss=7.7287
	step [210/325], loss=6.6345
	step [211/325], loss=8.0553
	step [212/325], loss=7.6626
	step [213/325], loss=7.7671
	step [214/325], loss=8.0673
	step [215/325], loss=8.4129
	step [216/325], loss=6.1453
	step [217/325], loss=8.1172
	step [218/325], loss=8.0507
	step [219/325], loss=7.6836
	step [220/325], loss=8.5433
	step [221/325], loss=9.7685
	step [222/325], loss=7.5522
	step [223/325], loss=7.7917
	step [224/325], loss=7.3903
	step [225/325], loss=7.3883
	step [226/325], loss=8.0464
	step [227/325], loss=7.0662
	step [228/325], loss=7.1096
	step [229/325], loss=8.3623
	step [230/325], loss=6.6096
	step [231/325], loss=8.9528
	step [232/325], loss=8.2458
	step [233/325], loss=8.3234
	step [234/325], loss=6.8244
	step [235/325], loss=7.9350
	step [236/325], loss=7.5778
	step [237/325], loss=10.5361
	step [238/325], loss=7.4316
	step [239/325], loss=7.4393
	step [240/325], loss=9.2211
	step [241/325], loss=7.2377
	step [242/325], loss=7.7130
	step [243/325], loss=6.8461
	step [244/325], loss=6.9555
	step [245/325], loss=9.0380
	step [246/325], loss=7.7547
	step [247/325], loss=8.6660
	step [248/325], loss=7.4770
	step [249/325], loss=8.9727
	step [250/325], loss=8.6932
	step [251/325], loss=7.5326
	step [252/325], loss=7.6996
	step [253/325], loss=7.9747
	step [254/325], loss=7.2684
	step [255/325], loss=6.6882
	step [256/325], loss=8.9179
	step [257/325], loss=7.5821
	step [258/325], loss=8.4347
	step [259/325], loss=9.0450
	step [260/325], loss=7.6493
	step [261/325], loss=8.5060
	step [262/325], loss=8.1803
	step [263/325], loss=8.8169
	step [264/325], loss=7.9479
	step [265/325], loss=8.2542
	step [266/325], loss=9.0647
	step [267/325], loss=7.9299
	step [268/325], loss=9.4291
	step [269/325], loss=8.2302
	step [270/325], loss=7.7805
	step [271/325], loss=8.1514
	step [272/325], loss=6.4509
	step [273/325], loss=7.7972
	step [274/325], loss=7.4229
	step [275/325], loss=9.3700
	step [276/325], loss=9.6898
	step [277/325], loss=7.5767
	step [278/325], loss=8.6975
	step [279/325], loss=8.3823
	step [280/325], loss=7.4263
	step [281/325], loss=9.1006
	step [282/325], loss=6.8163
	step [283/325], loss=9.4411
	step [284/325], loss=7.9776
	step [285/325], loss=9.9192
	step [286/325], loss=7.1627
	step [287/325], loss=8.6608
	step [288/325], loss=7.9798
	step [289/325], loss=7.2973
	step [290/325], loss=7.4911
	step [291/325], loss=6.5973
	step [292/325], loss=8.0088
	step [293/325], loss=9.2984
	step [294/325], loss=7.5564
	step [295/325], loss=8.8373
	step [296/325], loss=6.7012
	step [297/325], loss=7.9697
	step [298/325], loss=7.0972
	step [299/325], loss=8.1348
	step [300/325], loss=7.9451
	step [301/325], loss=6.9585
	step [302/325], loss=7.9956
	step [303/325], loss=7.1620
	step [304/325], loss=9.3580
	step [305/325], loss=8.7916
	step [306/325], loss=7.0466
	step [307/325], loss=8.8948
	step [308/325], loss=8.1901
	step [309/325], loss=6.6883
	step [310/325], loss=9.6403
	step [311/325], loss=8.4833
	step [312/325], loss=6.6233
	step [313/325], loss=7.4990
	step [314/325], loss=9.4000
	step [315/325], loss=9.7554
	step [316/325], loss=7.9482
	step [317/325], loss=8.7569
	step [318/325], loss=8.1859
	step [319/325], loss=8.7526
	step [320/325], loss=7.5346
	step [321/325], loss=6.5478
	step [322/325], loss=8.4515
	step [323/325], loss=7.9913
	step [324/325], loss=6.9678
	step [325/325], loss=4.0359
	Evaluating
	loss=0.0297, precision=0.1974, recall=0.9960, f1=0.3294
Training epoch 11
	step [1/325], loss=8.8942
	step [2/325], loss=7.6816
	step [3/325], loss=8.6454
	step [4/325], loss=7.9803
	step [5/325], loss=9.0077
	step [6/325], loss=8.6489
	step [7/325], loss=9.1547
	step [8/325], loss=8.3910
	step [9/325], loss=8.8577
	step [10/325], loss=8.0585
	step [11/325], loss=7.3290
	step [12/325], loss=7.8357
	step [13/325], loss=10.4066
	step [14/325], loss=7.4982
	step [15/325], loss=8.1793
	step [16/325], loss=8.0498
	step [17/325], loss=7.7977
	step [18/325], loss=6.6019
	step [19/325], loss=7.8938
	step [20/325], loss=6.2437
	step [21/325], loss=6.9266
	step [22/325], loss=9.4598
	step [23/325], loss=7.9458
	step [24/325], loss=8.7110
	step [25/325], loss=6.1546
	step [26/325], loss=7.5935
	step [27/325], loss=8.5415
	step [28/325], loss=6.7307
	step [29/325], loss=8.8742
	step [30/325], loss=8.6651
	step [31/325], loss=7.3599
	step [32/325], loss=6.6199
	step [33/325], loss=8.3940
	step [34/325], loss=9.4045
	step [35/325], loss=8.9986
	step [36/325], loss=7.5325
	step [37/325], loss=7.9405
	step [38/325], loss=7.0178
	step [39/325], loss=9.0761
	step [40/325], loss=7.9079
	step [41/325], loss=6.8785
	step [42/325], loss=7.5571
	step [43/325], loss=7.8752
	step [44/325], loss=6.2694
	step [45/325], loss=9.1790
	step [46/325], loss=7.5806
	step [47/325], loss=7.3912
	step [48/325], loss=8.6336
	step [49/325], loss=6.9626
	step [50/325], loss=9.1350
	step [51/325], loss=10.7900
	step [52/325], loss=8.7623
	step [53/325], loss=8.8491
	step [54/325], loss=8.2772
	step [55/325], loss=8.3768
	step [56/325], loss=7.2331
	step [57/325], loss=7.3950
	step [58/325], loss=7.6092
	step [59/325], loss=7.6565
	step [60/325], loss=7.5418
	step [61/325], loss=8.7026
	step [62/325], loss=8.1781
	step [63/325], loss=6.4857
	step [64/325], loss=8.1924
	step [65/325], loss=7.8564
	step [66/325], loss=6.6870
	step [67/325], loss=8.1284
	step [68/325], loss=7.3946
	step [69/325], loss=7.0431
	step [70/325], loss=7.8161
	step [71/325], loss=6.8590
	step [72/325], loss=8.4889
	step [73/325], loss=7.7816
	step [74/325], loss=9.0629
	step [75/325], loss=8.4491
	step [76/325], loss=9.5216
	step [77/325], loss=9.8580
	step [78/325], loss=9.0452
	step [79/325], loss=6.9249
	step [80/325], loss=6.2647
	step [81/325], loss=7.3420
	step [82/325], loss=8.6587
	step [83/325], loss=7.9928
	step [84/325], loss=7.7290
	step [85/325], loss=7.6880
	step [86/325], loss=7.3297
	step [87/325], loss=6.3758
	step [88/325], loss=7.2191
	step [89/325], loss=7.1453
	step [90/325], loss=8.3138
	step [91/325], loss=8.0651
	step [92/325], loss=6.8591
	step [93/325], loss=7.6599
	step [94/325], loss=8.0708
	step [95/325], loss=7.2606
	step [96/325], loss=8.0283
	step [97/325], loss=7.5354
	step [98/325], loss=8.0445
	step [99/325], loss=7.4141
	step [100/325], loss=7.3422
	step [101/325], loss=8.3799
	step [102/325], loss=7.4721
	step [103/325], loss=5.9633
	step [104/325], loss=6.9039
	step [105/325], loss=7.4907
	step [106/325], loss=7.6651
	step [107/325], loss=7.8927
	step [108/325], loss=6.7078
	step [109/325], loss=8.7438
	step [110/325], loss=8.0721
	step [111/325], loss=6.8629
	step [112/325], loss=8.2471
	step [113/325], loss=6.3477
	step [114/325], loss=7.2368
	step [115/325], loss=7.1145
	step [116/325], loss=7.2549
	step [117/325], loss=7.9003
	step [118/325], loss=8.4225
	step [119/325], loss=8.6971
	step [120/325], loss=7.1252
	step [121/325], loss=7.7863
	step [122/325], loss=8.7009
	step [123/325], loss=6.0581
	step [124/325], loss=6.7478
	step [125/325], loss=7.5333
	step [126/325], loss=7.4033
	step [127/325], loss=8.9191
	step [128/325], loss=7.4332
	step [129/325], loss=8.5211
	step [130/325], loss=7.7899
	step [131/325], loss=6.9894
	step [132/325], loss=6.6627
	step [133/325], loss=6.5631
	step [134/325], loss=8.4766
	step [135/325], loss=6.9322
	step [136/325], loss=7.1379
	step [137/325], loss=9.3255
	step [138/325], loss=6.6366
	step [139/325], loss=5.9002
	step [140/325], loss=7.0148
	step [141/325], loss=8.1742
	step [142/325], loss=7.2199
	step [143/325], loss=6.9031
	step [144/325], loss=8.4549
	step [145/325], loss=7.5027
	step [146/325], loss=7.3770
	step [147/325], loss=7.3222
	step [148/325], loss=6.1452
	step [149/325], loss=7.0384
	step [150/325], loss=6.8667
	step [151/325], loss=8.1014
	step [152/325], loss=11.4067
	step [153/325], loss=5.6717
	step [154/325], loss=8.2385
	step [155/325], loss=10.0757
	step [156/325], loss=7.3249
	step [157/325], loss=9.2517
	step [158/325], loss=7.9179
	step [159/325], loss=6.9131
	step [160/325], loss=7.5837
	step [161/325], loss=7.8822
	step [162/325], loss=7.5721
	step [163/325], loss=8.9956
	step [164/325], loss=7.6460
	step [165/325], loss=8.0350
	step [166/325], loss=7.2394
	step [167/325], loss=7.5493
	step [168/325], loss=7.9357
	step [169/325], loss=8.3437
	step [170/325], loss=8.5690
	step [171/325], loss=6.3760
	step [172/325], loss=7.7968
	step [173/325], loss=8.7306
	step [174/325], loss=8.2879
	step [175/325], loss=7.1252
	step [176/325], loss=9.7441
	step [177/325], loss=6.2694
	step [178/325], loss=8.1893
	step [179/325], loss=7.1034
	step [180/325], loss=7.2605
	step [181/325], loss=7.5629
	step [182/325], loss=7.0968
	step [183/325], loss=8.0323
	step [184/325], loss=8.4719
	step [185/325], loss=7.8800
	step [186/325], loss=7.1496
	step [187/325], loss=8.3585
	step [188/325], loss=7.1634
	step [189/325], loss=8.2494
	step [190/325], loss=7.8794
	step [191/325], loss=8.8861
	step [192/325], loss=6.1488
	step [193/325], loss=6.5106
	step [194/325], loss=7.8168
	step [195/325], loss=5.9037
	step [196/325], loss=8.0408
	step [197/325], loss=7.8616
	step [198/325], loss=8.0917
	step [199/325], loss=8.9614
	step [200/325], loss=9.6171
	step [201/325], loss=6.7087
	step [202/325], loss=7.9511
	step [203/325], loss=7.1994
	step [204/325], loss=7.0324
	step [205/325], loss=8.2415
	step [206/325], loss=6.8553
	step [207/325], loss=8.1414
	step [208/325], loss=6.1960
	step [209/325], loss=6.9727
	step [210/325], loss=7.7539
	step [211/325], loss=6.0863
	step [212/325], loss=8.1147
	step [213/325], loss=6.7072
	step [214/325], loss=7.1590
	step [215/325], loss=9.4250
	step [216/325], loss=7.1634
	step [217/325], loss=8.9019
	step [218/325], loss=8.4395
	step [219/325], loss=9.7209
	step [220/325], loss=8.6429
	step [221/325], loss=6.4345
	step [222/325], loss=7.0444
	step [223/325], loss=8.3324
	step [224/325], loss=6.9832
	step [225/325], loss=8.0222
	step [226/325], loss=7.3028
	step [227/325], loss=10.3183
	step [228/325], loss=8.5736
	step [229/325], loss=6.3302
	step [230/325], loss=8.3039
	step [231/325], loss=7.1539
	step [232/325], loss=7.2093
	step [233/325], loss=7.2259
	step [234/325], loss=6.7024
	step [235/325], loss=9.2024
	step [236/325], loss=7.5024
	step [237/325], loss=6.5225
	step [238/325], loss=9.2859
	step [239/325], loss=6.2423
	step [240/325], loss=7.3424
	step [241/325], loss=8.9825
	step [242/325], loss=7.7626
	step [243/325], loss=7.1063
	step [244/325], loss=7.1117
	step [245/325], loss=8.8999
	step [246/325], loss=6.4390
	step [247/325], loss=9.0309
	step [248/325], loss=10.3089
	step [249/325], loss=7.3128
	step [250/325], loss=5.7844
	step [251/325], loss=7.4165
	step [252/325], loss=7.2170
	step [253/325], loss=7.8707
	step [254/325], loss=6.6851
	step [255/325], loss=7.2762
	step [256/325], loss=8.7798
	step [257/325], loss=7.0413
	step [258/325], loss=7.7533
	step [259/325], loss=9.4834
	step [260/325], loss=7.3449
	step [261/325], loss=8.0862
	step [262/325], loss=7.1178
	step [263/325], loss=6.8547
	step [264/325], loss=6.3955
	step [265/325], loss=8.2146
	step [266/325], loss=9.8655
	step [267/325], loss=7.9409
	step [268/325], loss=6.1532
	step [269/325], loss=6.7936
	step [270/325], loss=6.1161
	step [271/325], loss=7.5998
	step [272/325], loss=7.6702
	step [273/325], loss=6.9870
	step [274/325], loss=7.1395
	step [275/325], loss=7.5143
	step [276/325], loss=7.8310
	step [277/325], loss=7.0307
	step [278/325], loss=6.6131
	step [279/325], loss=7.9934
	step [280/325], loss=8.0820
	step [281/325], loss=8.5825
	step [282/325], loss=8.2948
	step [283/325], loss=7.6669
	step [284/325], loss=7.0804
	step [285/325], loss=6.9735
	step [286/325], loss=7.8759
	step [287/325], loss=7.2272
	step [288/325], loss=8.2085
	step [289/325], loss=7.1965
	step [290/325], loss=7.1681
	step [291/325], loss=6.2775
	step [292/325], loss=6.5533
	step [293/325], loss=7.8365
	step [294/325], loss=7.3925
	step [295/325], loss=6.7454
	step [296/325], loss=5.8940
	step [297/325], loss=5.7014
	step [298/325], loss=8.1554
	step [299/325], loss=7.4660
	step [300/325], loss=8.4771
	step [301/325], loss=10.7253
	step [302/325], loss=8.1900
	step [303/325], loss=7.1565
	step [304/325], loss=8.1652
	step [305/325], loss=8.4356
	step [306/325], loss=7.3815
	step [307/325], loss=7.5514
	step [308/325], loss=6.9153
	step [309/325], loss=8.2240
	step [310/325], loss=6.7800
	step [311/325], loss=7.1333
	step [312/325], loss=7.4319
	step [313/325], loss=7.0454
	step [314/325], loss=5.9999
	step [315/325], loss=8.6778
	step [316/325], loss=7.6398
	step [317/325], loss=6.5704
	step [318/325], loss=7.7303
	step [319/325], loss=6.0538
	step [320/325], loss=8.9768
	step [321/325], loss=7.6582
	step [322/325], loss=8.6449
	step [323/325], loss=7.6149
	step [324/325], loss=7.3458
	step [325/325], loss=4.3325
	Evaluating
	loss=0.0288, precision=0.1839, recall=0.9960, f1=0.3105
Training epoch 12
	step [1/325], loss=7.2815
	step [2/325], loss=6.6667
	step [3/325], loss=7.9395
	step [4/325], loss=7.1723
	step [5/325], loss=6.7576
	step [6/325], loss=7.1500
	step [7/325], loss=7.3659
	step [8/325], loss=7.0169
	step [9/325], loss=7.8411
	step [10/325], loss=7.0981
	step [11/325], loss=7.3004
	step [12/325], loss=5.7942
	step [13/325], loss=7.1047
	step [14/325], loss=6.7232
	step [15/325], loss=8.5006
	step [16/325], loss=6.7882
	step [17/325], loss=8.0618
	step [18/325], loss=7.5158
	step [19/325], loss=6.2148
	step [20/325], loss=7.6041
	step [21/325], loss=6.3632
	step [22/325], loss=5.2840
	step [23/325], loss=7.4242
	step [24/325], loss=7.1536
	step [25/325], loss=6.0175
	step [26/325], loss=9.6124
	step [27/325], loss=7.5626
	step [28/325], loss=7.0557
	step [29/325], loss=7.0150
	step [30/325], loss=6.5441
	step [31/325], loss=7.9955
	step [32/325], loss=6.3517
	step [33/325], loss=7.8787
	step [34/325], loss=6.7665
	step [35/325], loss=9.1168
	step [36/325], loss=7.0109
	step [37/325], loss=8.6904
	step [38/325], loss=7.1692
	step [39/325], loss=8.2457
	step [40/325], loss=7.4150
	step [41/325], loss=5.2287
	step [42/325], loss=6.8892
	step [43/325], loss=6.8476
	step [44/325], loss=6.6326
	step [45/325], loss=7.5120
	step [46/325], loss=6.8482
	step [47/325], loss=7.6133
	step [48/325], loss=7.1815
	step [49/325], loss=7.9750
	step [50/325], loss=6.3477
	step [51/325], loss=8.6282
	step [52/325], loss=7.2015
	step [53/325], loss=5.9152
	step [54/325], loss=6.1275
	step [55/325], loss=7.1200
	step [56/325], loss=6.9094
	step [57/325], loss=8.5079
	step [58/325], loss=7.9264
	step [59/325], loss=7.6658
	step [60/325], loss=7.8243
	step [61/325], loss=7.9199
	step [62/325], loss=6.0094
	step [63/325], loss=7.1785
	step [64/325], loss=7.1347
	step [65/325], loss=7.0287
	step [66/325], loss=5.7503
	step [67/325], loss=8.5340
	step [68/325], loss=6.7162
	step [69/325], loss=8.8336
	step [70/325], loss=6.1309
	step [71/325], loss=7.9168
	step [72/325], loss=7.4391
	step [73/325], loss=8.5158
	step [74/325], loss=6.9512
	step [75/325], loss=7.5565
	step [76/325], loss=9.1471
	step [77/325], loss=6.0746
	step [78/325], loss=7.3184
	step [79/325], loss=9.1805
	step [80/325], loss=8.4642
	step [81/325], loss=7.7913
	step [82/325], loss=6.9204
	step [83/325], loss=7.2956
	step [84/325], loss=7.8558
	step [85/325], loss=7.3550
	step [86/325], loss=6.7159
	step [87/325], loss=7.5813
	step [88/325], loss=5.9562
	step [89/325], loss=8.0619
	step [90/325], loss=8.8578
	step [91/325], loss=7.4912
	step [92/325], loss=6.1897
	step [93/325], loss=6.6788
	step [94/325], loss=6.9187
	step [95/325], loss=6.7372
	step [96/325], loss=5.7807
	step [97/325], loss=5.8650
	step [98/325], loss=8.4670
	step [99/325], loss=7.6263
	step [100/325], loss=7.7595
	step [101/325], loss=8.8201
	step [102/325], loss=8.1326
	step [103/325], loss=6.3728
	step [104/325], loss=6.7106
	step [105/325], loss=6.6018
	step [106/325], loss=8.2628
	step [107/325], loss=7.2893
	step [108/325], loss=9.8703
	step [109/325], loss=8.3897
	step [110/325], loss=6.4777
	step [111/325], loss=7.1034
	step [112/325], loss=5.7221
	step [113/325], loss=9.3236
	step [114/325], loss=7.8397
	step [115/325], loss=7.6153
	step [116/325], loss=6.8214
	step [117/325], loss=5.6594
	step [118/325], loss=8.9630
	step [119/325], loss=7.0831
	step [120/325], loss=8.0115
	step [121/325], loss=7.0107
	step [122/325], loss=8.4620
	step [123/325], loss=10.7976
	step [124/325], loss=6.6532
	step [125/325], loss=9.9757
	step [126/325], loss=6.8617
	step [127/325], loss=7.2265
	step [128/325], loss=7.9830
	step [129/325], loss=6.8025
	step [130/325], loss=7.0738
	step [131/325], loss=7.2933
	step [132/325], loss=5.8821
	step [133/325], loss=7.7020
	step [134/325], loss=6.2691
	step [135/325], loss=7.2271
	step [136/325], loss=7.2928
	step [137/325], loss=7.0168
	step [138/325], loss=5.4220
	step [139/325], loss=7.4931
	step [140/325], loss=7.7918
	step [141/325], loss=7.4898
	step [142/325], loss=6.9395
	step [143/325], loss=7.2081
	step [144/325], loss=7.5479
	step [145/325], loss=6.9755
	step [146/325], loss=7.0263
	step [147/325], loss=6.9474
	step [148/325], loss=6.4244
	step [149/325], loss=9.5834
	step [150/325], loss=7.0131
	step [151/325], loss=8.3505
	step [152/325], loss=5.8187
	step [153/325], loss=8.2527
	step [154/325], loss=6.1272
	step [155/325], loss=7.8859
	step [156/325], loss=6.7946
	step [157/325], loss=6.9726
	step [158/325], loss=5.5075
	step [159/325], loss=6.4114
	step [160/325], loss=6.3003
	step [161/325], loss=6.4444
	step [162/325], loss=11.0369
	step [163/325], loss=8.2609
	step [164/325], loss=7.5816
	step [165/325], loss=7.6625
	step [166/325], loss=7.1366
	step [167/325], loss=7.3791
	step [168/325], loss=6.3208
	step [169/325], loss=8.0067
	step [170/325], loss=9.3234
	step [171/325], loss=7.6033
	step [172/325], loss=8.2915
	step [173/325], loss=7.8462
	step [174/325], loss=6.7771
	step [175/325], loss=6.7550
	step [176/325], loss=6.1133
	step [177/325], loss=5.4548
	step [178/325], loss=7.6363
	step [179/325], loss=8.0921
	step [180/325], loss=7.3676
	step [181/325], loss=6.4309
	step [182/325], loss=6.3711
	step [183/325], loss=8.5500
	step [184/325], loss=6.4216
	step [185/325], loss=7.2918
	step [186/325], loss=7.9840
	step [187/325], loss=10.1903
	step [188/325], loss=6.4458
	step [189/325], loss=7.5271
	step [190/325], loss=9.6251
	step [191/325], loss=6.6320
	step [192/325], loss=7.0154
	step [193/325], loss=6.1450
	step [194/325], loss=8.1384
	step [195/325], loss=6.6505
	step [196/325], loss=5.6703
	step [197/325], loss=6.7936
	step [198/325], loss=7.4612
	step [199/325], loss=8.2709
	step [200/325], loss=6.5684
	step [201/325], loss=7.9107
	step [202/325], loss=5.9138
	step [203/325], loss=6.5732
	step [204/325], loss=5.9869
	step [205/325], loss=8.8227
	step [206/325], loss=7.9463
	step [207/325], loss=7.1297
	step [208/325], loss=7.0142
	step [209/325], loss=7.7750
	step [210/325], loss=6.3172
	step [211/325], loss=8.9637
	step [212/325], loss=6.0594
	step [213/325], loss=5.7337
	step [214/325], loss=5.7773
	step [215/325], loss=7.4162
	step [216/325], loss=7.2574
	step [217/325], loss=7.9605
	step [218/325], loss=6.3942
	step [219/325], loss=6.3367
	step [220/325], loss=8.4168
	step [221/325], loss=7.5201
	step [222/325], loss=8.6864
	step [223/325], loss=8.2840
	step [224/325], loss=6.3780
	step [225/325], loss=6.2149
	step [226/325], loss=7.4919
	step [227/325], loss=7.3785
	step [228/325], loss=7.0903
	step [229/325], loss=6.9814
	step [230/325], loss=7.5564
	step [231/325], loss=5.9791
	step [232/325], loss=8.5253
	step [233/325], loss=6.7997
	step [234/325], loss=7.3142
	step [235/325], loss=6.3596
	step [236/325], loss=6.9748
	step [237/325], loss=5.8788
	step [238/325], loss=5.7727
	step [239/325], loss=8.2337
	step [240/325], loss=7.0937
	step [241/325], loss=6.4366
	step [242/325], loss=7.7323
	step [243/325], loss=7.2478
	step [244/325], loss=7.6331
	step [245/325], loss=8.2487
	step [246/325], loss=8.3407
	step [247/325], loss=8.0968
	step [248/325], loss=6.0171
	step [249/325], loss=5.5698
	step [250/325], loss=7.0134
	step [251/325], loss=7.2649
	step [252/325], loss=6.2785
	step [253/325], loss=8.0543
	step [254/325], loss=5.5668
	step [255/325], loss=6.1684
	step [256/325], loss=8.0204
	step [257/325], loss=7.0912
	step [258/325], loss=6.2287
	step [259/325], loss=6.3707
	step [260/325], loss=7.0460
	step [261/325], loss=7.4709
	step [262/325], loss=6.8208
	step [263/325], loss=6.7281
	step [264/325], loss=8.4720
	step [265/325], loss=7.4225
	step [266/325], loss=8.7818
	step [267/325], loss=6.3896
	step [268/325], loss=10.6676
	step [269/325], loss=7.6697
	step [270/325], loss=7.1035
	step [271/325], loss=7.8956
	step [272/325], loss=9.4036
	step [273/325], loss=6.5717
	step [274/325], loss=7.4812
	step [275/325], loss=6.8823
	step [276/325], loss=6.1081
	step [277/325], loss=8.1002
	step [278/325], loss=8.2304
	step [279/325], loss=6.6276
	step [280/325], loss=8.8843
	step [281/325], loss=6.0702
	step [282/325], loss=5.9196
	step [283/325], loss=7.7529
	step [284/325], loss=7.1723
	step [285/325], loss=8.6381
	step [286/325], loss=6.9197
	step [287/325], loss=6.6088
	step [288/325], loss=6.6146
	step [289/325], loss=7.3279
	step [290/325], loss=7.1508
	step [291/325], loss=7.8841
	step [292/325], loss=5.8255
	step [293/325], loss=6.8180
	step [294/325], loss=5.2323
	step [295/325], loss=8.8949
	step [296/325], loss=8.9297
	step [297/325], loss=6.7059
	step [298/325], loss=6.3280
	step [299/325], loss=5.4241
	step [300/325], loss=6.6136
	step [301/325], loss=7.9300
	step [302/325], loss=6.4828
	step [303/325], loss=5.9311
	step [304/325], loss=6.9482
	step [305/325], loss=6.5030
	step [306/325], loss=7.1505
	step [307/325], loss=5.4592
	step [308/325], loss=7.4058
	step [309/325], loss=7.7074
	step [310/325], loss=5.9371
	step [311/325], loss=7.8179
	step [312/325], loss=7.7189
	step [313/325], loss=6.6563
	step [314/325], loss=6.9082
	step [315/325], loss=6.3161
	step [316/325], loss=7.5881
	step [317/325], loss=7.0592
	step [318/325], loss=7.2069
	step [319/325], loss=8.2809
	step [320/325], loss=8.2093
	step [321/325], loss=9.4048
	step [322/325], loss=6.1403
	step [323/325], loss=6.9643
	step [324/325], loss=7.2331
	step [325/325], loss=3.5672
	Evaluating
	loss=0.0296, precision=0.1653, recall=0.9966, f1=0.2836
Training epoch 13
	step [1/325], loss=7.3483
	step [2/325], loss=5.7443
	step [3/325], loss=6.5543
	step [4/325], loss=6.3770
	step [5/325], loss=7.5280
	step [6/325], loss=6.7089
	step [7/325], loss=7.7005
	step [8/325], loss=5.7367
	step [9/325], loss=7.3213
	step [10/325], loss=8.6082
	step [11/325], loss=6.2873
	step [12/325], loss=8.5861
	step [13/325], loss=7.4477
	step [14/325], loss=6.8040
	step [15/325], loss=7.1462
	step [16/325], loss=7.3288
	step [17/325], loss=6.6788
	step [18/325], loss=8.6616
	step [19/325], loss=6.1770
	step [20/325], loss=6.7437
	step [21/325], loss=8.5092
	step [22/325], loss=7.2922
	step [23/325], loss=7.0152
	step [24/325], loss=6.3912
	step [25/325], loss=10.1204
	step [26/325], loss=6.6822
	step [27/325], loss=8.2642
	step [28/325], loss=6.7889
	step [29/325], loss=8.0375
	step [30/325], loss=7.0362
	step [31/325], loss=5.4938
	step [32/325], loss=6.4093
	step [33/325], loss=5.7848
	step [34/325], loss=6.8637
	step [35/325], loss=8.2040
	step [36/325], loss=6.1775
	step [37/325], loss=7.0884
	step [38/325], loss=6.6300
	step [39/325], loss=6.4812
	step [40/325], loss=6.8684
	step [41/325], loss=8.0795
	step [42/325], loss=8.8133
	step [43/325], loss=7.8611
	step [44/325], loss=7.1413
	step [45/325], loss=7.2777
	step [46/325], loss=6.8310
	step [47/325], loss=6.5022
	step [48/325], loss=9.2207
	step [49/325], loss=6.8597
	step [50/325], loss=7.7928
	step [51/325], loss=8.5492
	step [52/325], loss=8.1387
	step [53/325], loss=6.6235
	step [54/325], loss=6.4519
	step [55/325], loss=6.7659
	step [56/325], loss=7.1550
	step [57/325], loss=6.0329
	step [58/325], loss=6.7533
	step [59/325], loss=6.9292
	step [60/325], loss=6.5541
	step [61/325], loss=7.1813
	step [62/325], loss=7.0276
	step [63/325], loss=5.7863
	step [64/325], loss=8.2765
	step [65/325], loss=7.0525
	step [66/325], loss=5.9479
	step [67/325], loss=7.4280
	step [68/325], loss=8.2156
	step [69/325], loss=6.7469
	step [70/325], loss=7.8290
	step [71/325], loss=5.3219
	step [72/325], loss=6.4406
	step [73/325], loss=6.7505
	step [74/325], loss=7.7675
	step [75/325], loss=6.3084
	step [76/325], loss=5.8613
	step [77/325], loss=7.0613
	step [78/325], loss=6.3065
	step [79/325], loss=6.9177
	step [80/325], loss=8.3093
	step [81/325], loss=8.4168
	step [82/325], loss=6.3172
	step [83/325], loss=7.2415
	step [84/325], loss=6.8326
	step [85/325], loss=7.6934
	step [86/325], loss=6.5769
	step [87/325], loss=8.9454
	step [88/325], loss=6.7162
	step [89/325], loss=6.6721
	step [90/325], loss=5.9941
	step [91/325], loss=5.9428
	step [92/325], loss=7.3212
	step [93/325], loss=7.8321
	step [94/325], loss=8.2440
	step [95/325], loss=7.6147
	step [96/325], loss=6.9732
	step [97/325], loss=7.1829
	step [98/325], loss=6.0717
	step [99/325], loss=6.1392
	step [100/325], loss=6.8492
	step [101/325], loss=7.3703
	step [102/325], loss=6.2738
	step [103/325], loss=7.8400
	step [104/325], loss=7.2340
	step [105/325], loss=6.1907
	step [106/325], loss=7.1268
	step [107/325], loss=7.2775
	step [108/325], loss=7.0180
	step [109/325], loss=6.4405
	step [110/325], loss=7.0205
	step [111/325], loss=6.9052
	step [112/325], loss=6.8454
	step [113/325], loss=7.5065
	step [114/325], loss=6.7184
	step [115/325], loss=5.0776
	step [116/325], loss=8.7923
	step [117/325], loss=7.7364
	step [118/325], loss=8.6297
	step [119/325], loss=8.2690
	step [120/325], loss=7.5340
	step [121/325], loss=7.0439
	step [122/325], loss=7.8091
	step [123/325], loss=6.8474
	step [124/325], loss=6.6727
	step [125/325], loss=6.7610
	step [126/325], loss=8.6925
	step [127/325], loss=6.1308
	step [128/325], loss=8.9774
	step [129/325], loss=6.5661
	step [130/325], loss=6.8985
	step [131/325], loss=6.4997
	step [132/325], loss=7.9831
	step [133/325], loss=5.8060
	step [134/325], loss=5.0715
	step [135/325], loss=6.4981
	step [136/325], loss=6.1223
	step [137/325], loss=7.2170
	step [138/325], loss=6.8094
	step [139/325], loss=7.3284
	step [140/325], loss=6.7819
	step [141/325], loss=6.7675
	step [142/325], loss=5.6424
	step [143/325], loss=6.8476
	step [144/325], loss=7.7176
	step [145/325], loss=7.6134
	step [146/325], loss=6.4671
	step [147/325], loss=8.3010
	step [148/325], loss=6.8020
	step [149/325], loss=8.8177
	step [150/325], loss=7.5734
	step [151/325], loss=7.0259
	step [152/325], loss=6.0311
	step [153/325], loss=5.5225
	step [154/325], loss=8.0074
	step [155/325], loss=9.8391
	step [156/325], loss=6.8592
	step [157/325], loss=6.6209
	step [158/325], loss=5.6175
	step [159/325], loss=5.9530
	step [160/325], loss=7.4971
	step [161/325], loss=6.8620
	step [162/325], loss=10.4001
	step [163/325], loss=8.0271
	step [164/325], loss=6.2253
	step [165/325], loss=7.4349
	step [166/325], loss=7.2679
	step [167/325], loss=6.6877
	step [168/325], loss=8.0899
	step [169/325], loss=8.4484
	step [170/325], loss=4.3722
	step [171/325], loss=5.8858
	step [172/325], loss=7.3512
	step [173/325], loss=6.2687
	step [174/325], loss=5.0167
	step [175/325], loss=6.8830
	step [176/325], loss=6.0090
	step [177/325], loss=6.6779
	step [178/325], loss=6.6888
	step [179/325], loss=8.1840
	step [180/325], loss=7.0885
	step [181/325], loss=6.2688
	step [182/325], loss=6.5136
	step [183/325], loss=5.8472
	step [184/325], loss=9.1106
	step [185/325], loss=7.0845
	step [186/325], loss=6.8041
	step [187/325], loss=5.6055
	step [188/325], loss=6.0444
	step [189/325], loss=6.9025
	step [190/325], loss=6.0788
	step [191/325], loss=7.4795
	step [192/325], loss=5.5001
	step [193/325], loss=5.7946
	step [194/325], loss=9.2330
	step [195/325], loss=5.5066
	step [196/325], loss=7.5087
	step [197/325], loss=7.4639
	step [198/325], loss=6.2146
	step [199/325], loss=6.3551
	step [200/325], loss=7.3186
	step [201/325], loss=6.0593
	step [202/325], loss=6.5428
	step [203/325], loss=6.3086
	step [204/325], loss=6.3177
	step [205/325], loss=5.2435
	step [206/325], loss=7.7099
	step [207/325], loss=7.9220
	step [208/325], loss=11.1590
	step [209/325], loss=5.4934
	step [210/325], loss=5.8689
	step [211/325], loss=7.7161
	step [212/325], loss=6.6480
	step [213/325], loss=8.3683
	step [214/325], loss=7.8565
	step [215/325], loss=8.2144
	step [216/325], loss=5.7119
	step [217/325], loss=5.7474
	step [218/325], loss=6.5647
	step [219/325], loss=7.9428
	step [220/325], loss=7.9998
	step [221/325], loss=7.3723
	step [222/325], loss=5.6737
	step [223/325], loss=7.2928
	step [224/325], loss=6.6104
	step [225/325], loss=8.6175
	step [226/325], loss=6.7236
	step [227/325], loss=7.3300
	step [228/325], loss=7.2287
	step [229/325], loss=6.6357
	step [230/325], loss=7.1627
	step [231/325], loss=7.6927
	step [232/325], loss=8.3944
	step [233/325], loss=6.4559
	step [234/325], loss=6.2824
	step [235/325], loss=6.9304
	step [236/325], loss=6.8429
	step [237/325], loss=8.5260
	step [238/325], loss=5.9135
	step [239/325], loss=5.6698
	step [240/325], loss=6.4705
	step [241/325], loss=7.4698
	step [242/325], loss=8.4243
	step [243/325], loss=8.3298
	step [244/325], loss=7.5911
	step [245/325], loss=7.8547
	step [246/325], loss=6.6176
	step [247/325], loss=5.9625
	step [248/325], loss=7.7970
	step [249/325], loss=9.9878
	step [250/325], loss=6.2625
	step [251/325], loss=6.5728
	step [252/325], loss=6.6505
	step [253/325], loss=8.3846
	step [254/325], loss=6.1034
	step [255/325], loss=6.9191
	step [256/325], loss=8.1734
	step [257/325], loss=8.6500
	step [258/325], loss=8.5312
	step [259/325], loss=6.1753
	step [260/325], loss=6.9820
	step [261/325], loss=6.9329
	step [262/325], loss=7.4062
	step [263/325], loss=8.1263
	step [264/325], loss=6.1950
	step [265/325], loss=6.9233
	step [266/325], loss=6.9095
	step [267/325], loss=6.1787
	step [268/325], loss=6.4438
	step [269/325], loss=7.9479
	step [270/325], loss=6.0893
	step [271/325], loss=5.7498
	step [272/325], loss=5.9793
	step [273/325], loss=5.4422
	step [274/325], loss=6.4104
	step [275/325], loss=6.6842
	step [276/325], loss=8.9474
	step [277/325], loss=6.3780
	step [278/325], loss=7.3052
	step [279/325], loss=7.2651
	step [280/325], loss=6.1002
	step [281/325], loss=6.4655
	step [282/325], loss=5.6820
	step [283/325], loss=5.6136
	step [284/325], loss=5.7603
	step [285/325], loss=5.8155
	step [286/325], loss=6.1922
	step [287/325], loss=7.6990
	step [288/325], loss=7.1774
	step [289/325], loss=7.7674
	step [290/325], loss=6.8223
	step [291/325], loss=6.7231
	step [292/325], loss=6.8833
	step [293/325], loss=6.8598
	step [294/325], loss=6.8413
	step [295/325], loss=7.3162
	step [296/325], loss=6.5994
	step [297/325], loss=6.2353
	step [298/325], loss=8.6006
	step [299/325], loss=7.0436
	step [300/325], loss=8.3866
	step [301/325], loss=7.4435
	step [302/325], loss=6.5768
	step [303/325], loss=6.9882
	step [304/325], loss=8.6330
	step [305/325], loss=6.9767
	step [306/325], loss=7.8453
	step [307/325], loss=5.9679
	step [308/325], loss=5.7795
	step [309/325], loss=5.6294
	step [310/325], loss=9.1011
	step [311/325], loss=8.0072
	step [312/325], loss=7.7638
	step [313/325], loss=7.3316
	step [314/325], loss=6.6923
	step [315/325], loss=7.1433
	step [316/325], loss=6.8775
	step [317/325], loss=7.0813
	step [318/325], loss=7.6440
	step [319/325], loss=6.4388
	step [320/325], loss=5.8428
	step [321/325], loss=6.0526
	step [322/325], loss=6.4377
	step [323/325], loss=5.6817
	step [324/325], loss=6.8187
	step [325/325], loss=4.3949
	Evaluating
	loss=0.0228, precision=0.2091, recall=0.9955, f1=0.3455
Training epoch 14
	step [1/325], loss=6.1224
	step [2/325], loss=7.3738
	step [3/325], loss=7.7668
	step [4/325], loss=5.6612
	step [5/325], loss=6.3782
	step [6/325], loss=7.2651
	step [7/325], loss=5.9269
	step [8/325], loss=5.5336
	step [9/325], loss=7.5011
	step [10/325], loss=8.7035
	step [11/325], loss=7.7889
	step [12/325], loss=6.2749
	step [13/325], loss=7.6261
	step [14/325], loss=6.6307
	step [15/325], loss=7.8977
	step [16/325], loss=7.0959
	step [17/325], loss=7.1095
	step [18/325], loss=7.1321
	step [19/325], loss=5.9489
	step [20/325], loss=6.2007
	step [21/325], loss=5.9732
	step [22/325], loss=7.5157
	step [23/325], loss=7.3611
	step [24/325], loss=5.6355
	step [25/325], loss=7.4680
	step [26/325], loss=7.6788
	step [27/325], loss=5.7461
	step [28/325], loss=6.1401
	step [29/325], loss=5.1104
	step [30/325], loss=6.0294
	step [31/325], loss=5.4766
	step [32/325], loss=6.6953
	step [33/325], loss=7.1213
	step [34/325], loss=7.6405
	step [35/325], loss=7.1728
	step [36/325], loss=7.0262
	step [37/325], loss=7.3654
	step [38/325], loss=6.4777
	step [39/325], loss=6.0032
	step [40/325], loss=5.4534
	step [41/325], loss=4.4378
	step [42/325], loss=7.5072
	step [43/325], loss=5.6733
	step [44/325], loss=6.3995
	step [45/325], loss=8.4352
	step [46/325], loss=8.5546
	step [47/325], loss=6.6902
	step [48/325], loss=9.9534
	step [49/325], loss=6.4867
	step [50/325], loss=7.1463
	step [51/325], loss=5.9198
	step [52/325], loss=7.5548
	step [53/325], loss=6.6086
	step [54/325], loss=7.9919
	step [55/325], loss=7.1332
	step [56/325], loss=6.7113
	step [57/325], loss=5.2964
	step [58/325], loss=6.9302
	step [59/325], loss=6.4988
	step [60/325], loss=6.5379
	step [61/325], loss=6.1148
	step [62/325], loss=7.1647
	step [63/325], loss=8.2940
	step [64/325], loss=7.8976
	step [65/325], loss=7.5371
	step [66/325], loss=6.8896
	step [67/325], loss=5.5438
	step [68/325], loss=6.8945
	step [69/325], loss=6.2750
	step [70/325], loss=7.0433
	step [71/325], loss=8.2504
	step [72/325], loss=6.3279
	step [73/325], loss=6.9594
	step [74/325], loss=6.6905
	step [75/325], loss=6.1450
	step [76/325], loss=7.9887
	step [77/325], loss=6.2288
	step [78/325], loss=7.0149
	step [79/325], loss=5.3230
	step [80/325], loss=6.4311
	step [81/325], loss=7.2742
	step [82/325], loss=8.1323
	step [83/325], loss=7.1882
	step [84/325], loss=6.9119
	step [85/325], loss=5.4532
	step [86/325], loss=7.1361
	step [87/325], loss=7.4016
	step [88/325], loss=7.9284
	step [89/325], loss=7.4600
	step [90/325], loss=6.5824
	step [91/325], loss=5.9199
	step [92/325], loss=6.0418
	step [93/325], loss=6.8925
	step [94/325], loss=6.3160
	step [95/325], loss=6.4149
	step [96/325], loss=5.5241
	step [97/325], loss=6.3102
	step [98/325], loss=7.7586
	step [99/325], loss=5.6134
	step [100/325], loss=6.7845
	step [101/325], loss=6.2278
	step [102/325], loss=5.3081
	step [103/325], loss=6.4379
	step [104/325], loss=7.7664
	step [105/325], loss=5.4214
	step [106/325], loss=6.4896
	step [107/325], loss=6.6685
	step [108/325], loss=8.8367
	step [109/325], loss=7.2809
	step [110/325], loss=8.4839
	step [111/325], loss=6.1722
	step [112/325], loss=7.1869
	step [113/325], loss=8.0158
	step [114/325], loss=9.2017
	step [115/325], loss=8.4018
	step [116/325], loss=7.9201
	step [117/325], loss=7.1168
	step [118/325], loss=6.4578
	step [119/325], loss=6.5521
	step [120/325], loss=7.2292
	step [121/325], loss=5.6870
	step [122/325], loss=5.4574
	step [123/325], loss=6.7665
	step [124/325], loss=6.2836
	step [125/325], loss=8.6340
	step [126/325], loss=7.4147
	step [127/325], loss=6.0481
	step [128/325], loss=7.0960
	step [129/325], loss=6.3540
	step [130/325], loss=6.0632
	step [131/325], loss=6.3005
	step [132/325], loss=6.1362
	step [133/325], loss=6.7347
	step [134/325], loss=6.4565
	step [135/325], loss=6.4068
	step [136/325], loss=5.8265
	step [137/325], loss=5.9593
	step [138/325], loss=5.1078
	step [139/325], loss=5.7339
	step [140/325], loss=8.3355
	step [141/325], loss=5.6027
	step [142/325], loss=6.7835
	step [143/325], loss=6.3860
	step [144/325], loss=7.0107
	step [145/325], loss=7.1693
	step [146/325], loss=5.9331
	step [147/325], loss=7.0938
	step [148/325], loss=5.0552
	step [149/325], loss=9.1409
	step [150/325], loss=6.0586
	step [151/325], loss=6.4586
	step [152/325], loss=6.6801
	step [153/325], loss=5.4012
	step [154/325], loss=5.9240
	step [155/325], loss=6.4300
	step [156/325], loss=6.1910
	step [157/325], loss=5.6826
	step [158/325], loss=5.6531
	step [159/325], loss=7.8872
	step [160/325], loss=7.2834
	step [161/325], loss=7.7401
	step [162/325], loss=6.6451
	step [163/325], loss=7.4818
	step [164/325], loss=6.1333
	step [165/325], loss=7.4527
	step [166/325], loss=7.0272
	step [167/325], loss=6.2725
	step [168/325], loss=7.9282
	step [169/325], loss=4.9548
	step [170/325], loss=6.0917
	step [171/325], loss=6.4021
	step [172/325], loss=7.3554
	step [173/325], loss=6.7594
	step [174/325], loss=6.2811
	step [175/325], loss=6.6332
	step [176/325], loss=6.7415
	step [177/325], loss=5.6328
	step [178/325], loss=6.9627
	step [179/325], loss=6.3413
	step [180/325], loss=5.3440
	step [181/325], loss=7.5263
	step [182/325], loss=7.4076
	step [183/325], loss=5.9604
	step [184/325], loss=8.1954
	step [185/325], loss=5.0204
	step [186/325], loss=6.4036
	step [187/325], loss=6.8584
	step [188/325], loss=5.6017
	step [189/325], loss=6.1981
	step [190/325], loss=6.3371
	step [191/325], loss=7.1533
	step [192/325], loss=6.3437
	step [193/325], loss=7.3414
	step [194/325], loss=7.6103
	step [195/325], loss=4.6430
	step [196/325], loss=7.0254
	step [197/325], loss=5.9450
	step [198/325], loss=5.6596
	step [199/325], loss=6.4049
	step [200/325], loss=7.8763
	step [201/325], loss=7.3869
	step [202/325], loss=7.2412
	step [203/325], loss=5.7311
	step [204/325], loss=6.6375
	step [205/325], loss=6.2074
	step [206/325], loss=6.0891
	step [207/325], loss=6.1241
	step [208/325], loss=6.5924
	step [209/325], loss=6.7453
	step [210/325], loss=5.2155
	step [211/325], loss=6.4521
	step [212/325], loss=5.6394
	step [213/325], loss=6.1582
	step [214/325], loss=5.1127
	step [215/325], loss=12.0940
	step [216/325], loss=7.3786
	step [217/325], loss=6.7352
	step [218/325], loss=6.2816
	step [219/325], loss=5.6095
	step [220/325], loss=7.7595
	step [221/325], loss=7.8004
	step [222/325], loss=6.6800
	step [223/325], loss=6.1393
	step [224/325], loss=5.7249
	step [225/325], loss=5.4249
	step [226/325], loss=6.6401
	step [227/325], loss=5.2064
	step [228/325], loss=10.7725
	step [229/325], loss=6.3015
	step [230/325], loss=6.0669
	step [231/325], loss=6.6485
	step [232/325], loss=9.1508
	step [233/325], loss=6.0072
	step [234/325], loss=7.1545
	step [235/325], loss=5.3829
	step [236/325], loss=6.9318
	step [237/325], loss=5.8773
	step [238/325], loss=5.2318
	step [239/325], loss=6.0101
	step [240/325], loss=7.1337
	step [241/325], loss=8.1230
	step [242/325], loss=6.2649
	step [243/325], loss=6.5895
	step [244/325], loss=7.0240
	step [245/325], loss=6.0886
	step [246/325], loss=7.3898
	step [247/325], loss=7.0733
	step [248/325], loss=6.6120
	step [249/325], loss=6.4741
	step [250/325], loss=6.4853
	step [251/325], loss=6.8339
	step [252/325], loss=8.4336
	step [253/325], loss=7.6362
	step [254/325], loss=6.6538
	step [255/325], loss=7.4269
	step [256/325], loss=7.1686
	step [257/325], loss=6.2763
	step [258/325], loss=6.2578
	step [259/325], loss=5.7344
	step [260/325], loss=6.5766
	step [261/325], loss=5.8042
	step [262/325], loss=5.8140
	step [263/325], loss=5.8694
	step [264/325], loss=5.7192
	step [265/325], loss=8.3069
	step [266/325], loss=5.4468
	step [267/325], loss=7.6812
	step [268/325], loss=7.1772
	step [269/325], loss=6.6749
	step [270/325], loss=6.0704
	step [271/325], loss=6.6327
	step [272/325], loss=6.0466
	step [273/325], loss=8.1695
	step [274/325], loss=5.9697
	step [275/325], loss=5.7728
	step [276/325], loss=6.2420
	step [277/325], loss=5.8520
	step [278/325], loss=5.6603
	step [279/325], loss=4.9174
	step [280/325], loss=4.7991
	step [281/325], loss=4.4887
	step [282/325], loss=5.4222
	step [283/325], loss=6.8155
	step [284/325], loss=7.3472
	step [285/325], loss=6.5389
	step [286/325], loss=7.6055
	step [287/325], loss=6.2874
	step [288/325], loss=7.0786
	step [289/325], loss=7.8736
	step [290/325], loss=7.6127
	step [291/325], loss=6.4513
	step [292/325], loss=7.1836
	step [293/325], loss=5.8772
	step [294/325], loss=5.1698
	step [295/325], loss=8.5358
	step [296/325], loss=4.9638
	step [297/325], loss=5.3605
	step [298/325], loss=6.5027
	step [299/325], loss=6.6290
	step [300/325], loss=7.5674
	step [301/325], loss=6.1497
	step [302/325], loss=6.2296
	step [303/325], loss=7.9460
	step [304/325], loss=6.2133
	step [305/325], loss=5.6198
	step [306/325], loss=6.3866
	step [307/325], loss=6.9359
	step [308/325], loss=6.1764
	step [309/325], loss=5.7410
	step [310/325], loss=7.3335
	step [311/325], loss=8.4888
	step [312/325], loss=7.5068
	step [313/325], loss=6.3710
	step [314/325], loss=8.4445
	step [315/325], loss=6.6977
	step [316/325], loss=6.1329
	step [317/325], loss=6.3903
	step [318/325], loss=6.2952
	step [319/325], loss=5.9165
	step [320/325], loss=7.2764
	step [321/325], loss=5.4134
	step [322/325], loss=5.9512
	step [323/325], loss=7.0548
	step [324/325], loss=6.0183
	step [325/325], loss=4.2419
	Evaluating
	loss=0.0198, precision=0.2249, recall=0.9947, f1=0.3668
saving model as: 1_saved_model.pth
Training epoch 15
	step [1/325], loss=6.6115
	step [2/325], loss=6.7476
	step [3/325], loss=6.3065
	step [4/325], loss=7.0367
	step [5/325], loss=7.5551
	step [6/325], loss=6.4682
	step [7/325], loss=5.2377
	step [8/325], loss=5.7506
	step [9/325], loss=5.5265
	step [10/325], loss=8.4158
	step [11/325], loss=7.4718
	step [12/325], loss=5.7885
	step [13/325], loss=6.3628
	step [14/325], loss=7.8278
	step [15/325], loss=6.3729
	step [16/325], loss=7.4658
	step [17/325], loss=7.1807
	step [18/325], loss=6.6973
	step [19/325], loss=5.7160
	step [20/325], loss=6.4002
	step [21/325], loss=8.1026
	step [22/325], loss=7.5839
	step [23/325], loss=6.6000
	step [24/325], loss=5.5850
	step [25/325], loss=5.9437
	step [26/325], loss=6.0253
	step [27/325], loss=6.0225
	step [28/325], loss=5.7396
	step [29/325], loss=5.4960
	step [30/325], loss=7.2979
	step [31/325], loss=5.9233
	step [32/325], loss=5.4844
	step [33/325], loss=6.2946
	step [34/325], loss=7.2699
	step [35/325], loss=6.4786
	step [36/325], loss=5.2276
	step [37/325], loss=5.9528
	step [38/325], loss=7.1380
	step [39/325], loss=6.1371
	step [40/325], loss=10.1353
	step [41/325], loss=8.2171
	step [42/325], loss=7.0201
	step [43/325], loss=6.5072
	step [44/325], loss=7.8352
	step [45/325], loss=6.1395
	step [46/325], loss=8.0354
	step [47/325], loss=6.4346
	step [48/325], loss=5.6731
	step [49/325], loss=5.8723
	step [50/325], loss=6.4909
	step [51/325], loss=6.2876
	step [52/325], loss=8.7797
	step [53/325], loss=5.2439
	step [54/325], loss=6.2489
	step [55/325], loss=7.0380
	step [56/325], loss=7.6237
	step [57/325], loss=8.0556
	step [58/325], loss=6.8770
	step [59/325], loss=7.3359
	step [60/325], loss=6.4112
	step [61/325], loss=6.3184
	step [62/325], loss=6.6431
	step [63/325], loss=6.0272
	step [64/325], loss=5.3900
	step [65/325], loss=6.6478
	step [66/325], loss=6.6230
	step [67/325], loss=6.8980
	step [68/325], loss=7.7035
	step [69/325], loss=5.1725
	step [70/325], loss=5.9484
	step [71/325], loss=6.0066
	step [72/325], loss=6.1050
	step [73/325], loss=6.4307
	step [74/325], loss=5.8332
	step [75/325], loss=5.7446
	step [76/325], loss=5.5523
	step [77/325], loss=8.9277
	step [78/325], loss=7.2354
	step [79/325], loss=5.2339
	step [80/325], loss=5.8084
	step [81/325], loss=6.3943
	step [82/325], loss=7.7039
	step [83/325], loss=5.9250
	step [84/325], loss=9.0402
	step [85/325], loss=5.9926
	step [86/325], loss=8.8060
	step [87/325], loss=6.8414
	step [88/325], loss=6.6357
	step [89/325], loss=4.6666
	step [90/325], loss=6.4598
	step [91/325], loss=6.0148
	step [92/325], loss=5.9995
	step [93/325], loss=7.4666
	step [94/325], loss=7.0601
	step [95/325], loss=6.2158
	step [96/325], loss=5.3330
	step [97/325], loss=7.1859
	step [98/325], loss=5.5848
	step [99/325], loss=6.2628
	step [100/325], loss=6.3875
	step [101/325], loss=7.2551
	step [102/325], loss=6.9158
	step [103/325], loss=6.4602
	step [104/325], loss=5.2803
	step [105/325], loss=6.9977
	step [106/325], loss=6.4986
	step [107/325], loss=4.8220
	step [108/325], loss=6.5067
	step [109/325], loss=6.4849
	step [110/325], loss=7.3160
	step [111/325], loss=6.3584
	step [112/325], loss=6.0082
	step [113/325], loss=6.0442
	step [114/325], loss=6.1382
	step [115/325], loss=6.9600
	step [116/325], loss=5.4192
	step [117/325], loss=6.1409
	step [118/325], loss=5.2783
	step [119/325], loss=5.9366
	step [120/325], loss=6.2811
	step [121/325], loss=5.4407
	step [122/325], loss=6.6380
	step [123/325], loss=5.7441
	step [124/325], loss=6.9301
	step [125/325], loss=5.6673
	step [126/325], loss=4.7943
	step [127/325], loss=7.6401
	step [128/325], loss=7.8727
	step [129/325], loss=7.8928
	step [130/325], loss=5.8085
	step [131/325], loss=5.5248
	step [132/325], loss=7.5685
	step [133/325], loss=7.0200
	step [134/325], loss=6.9803
	step [135/325], loss=7.1636
	step [136/325], loss=6.3249
	step [137/325], loss=5.1996
	step [138/325], loss=7.9015
	step [139/325], loss=5.6684
	step [140/325], loss=6.4498
	step [141/325], loss=4.8549
	step [142/325], loss=6.8517
	step [143/325], loss=6.3596
	step [144/325], loss=6.4721
	step [145/325], loss=7.5116
	step [146/325], loss=6.4588
	step [147/325], loss=5.2753
	step [148/325], loss=8.7760
	step [149/325], loss=7.9982
	step [150/325], loss=4.7466
	step [151/325], loss=6.4629
	step [152/325], loss=6.8127
	step [153/325], loss=5.1394
	step [154/325], loss=6.4851
	step [155/325], loss=7.2299
	step [156/325], loss=5.3245
	step [157/325], loss=5.6630
	step [158/325], loss=5.3381
	step [159/325], loss=5.8641
	step [160/325], loss=7.6761
	step [161/325], loss=6.7720
	step [162/325], loss=4.8375
	step [163/325], loss=8.3582
	step [164/325], loss=6.2083
	step [165/325], loss=7.0429
	step [166/325], loss=6.6278
	step [167/325], loss=6.4816
	step [168/325], loss=4.7475
	step [169/325], loss=7.0613
	step [170/325], loss=5.6770
	step [171/325], loss=5.7245
	step [172/325], loss=6.8467
	step [173/325], loss=8.1238
	step [174/325], loss=6.5529
	step [175/325], loss=5.8788
	step [176/325], loss=6.2468
	step [177/325], loss=5.4338
	step [178/325], loss=4.7636
	step [179/325], loss=8.9792
	step [180/325], loss=4.3407
	step [181/325], loss=6.0517
	step [182/325], loss=7.1004
	step [183/325], loss=6.2438
	step [184/325], loss=5.5008
	step [185/325], loss=6.2688
	step [186/325], loss=6.0470
	step [187/325], loss=7.7361
	step [188/325], loss=5.7019
	step [189/325], loss=6.2075
	step [190/325], loss=5.8534
	step [191/325], loss=5.9115
	step [192/325], loss=5.1648
	step [193/325], loss=6.4356
	step [194/325], loss=5.7040
	step [195/325], loss=6.4736
	step [196/325], loss=7.1995
	step [197/325], loss=7.1784
	step [198/325], loss=6.1879
	step [199/325], loss=6.3470
	step [200/325], loss=8.0154
	step [201/325], loss=6.0170
	step [202/325], loss=6.6718
	step [203/325], loss=6.2832
	step [204/325], loss=7.5508
	step [205/325], loss=7.2842
	step [206/325], loss=4.9893
	step [207/325], loss=8.4334
	step [208/325], loss=5.8491
	step [209/325], loss=6.9058
	step [210/325], loss=6.7647
	step [211/325], loss=9.6255
	step [212/325], loss=5.5566
	step [213/325], loss=5.3622
	step [214/325], loss=5.6650
	step [215/325], loss=6.6566
	step [216/325], loss=6.7222
	step [217/325], loss=7.5784
	step [218/325], loss=5.9937
	step [219/325], loss=4.4903
	step [220/325], loss=6.6760
	step [221/325], loss=7.2564
	step [222/325], loss=6.1651
	step [223/325], loss=7.2698
	step [224/325], loss=5.3610
	step [225/325], loss=6.4376
	step [226/325], loss=5.5196
	step [227/325], loss=6.0908
	step [228/325], loss=6.9013
	step [229/325], loss=6.4687
	step [230/325], loss=6.1033
	step [231/325], loss=5.4378
	step [232/325], loss=5.9071
	step [233/325], loss=6.3949
	step [234/325], loss=5.8548
	step [235/325], loss=7.0141
	step [236/325], loss=5.8745
	step [237/325], loss=6.0661
	step [238/325], loss=4.8414
	step [239/325], loss=4.9996
	step [240/325], loss=4.2282
	step [241/325], loss=8.4746
	step [242/325], loss=9.0917
	step [243/325], loss=5.8299
	step [244/325], loss=5.4823
	step [245/325], loss=6.3615
	step [246/325], loss=7.0130
	step [247/325], loss=6.6499
	step [248/325], loss=6.6724
	step [249/325], loss=6.8962
	step [250/325], loss=5.2025
	step [251/325], loss=6.3232
	step [252/325], loss=7.1259
	step [253/325], loss=7.4739
	step [254/325], loss=6.0415
	step [255/325], loss=7.2619
	step [256/325], loss=8.2027
	step [257/325], loss=5.2034
	step [258/325], loss=7.2890
	step [259/325], loss=6.0653
	step [260/325], loss=6.5246
	step [261/325], loss=5.1408
	step [262/325], loss=7.4011
	step [263/325], loss=6.1909
	step [264/325], loss=7.6568
	step [265/325], loss=5.9490
	step [266/325], loss=7.4449
	step [267/325], loss=6.0122
	step [268/325], loss=5.1457
	step [269/325], loss=6.7533
	step [270/325], loss=6.1491
	step [271/325], loss=5.5226
	step [272/325], loss=6.2926
	step [273/325], loss=6.1549
	step [274/325], loss=6.4685
	step [275/325], loss=6.0293
	step [276/325], loss=7.3073
	step [277/325], loss=5.7024
	step [278/325], loss=6.7324
	step [279/325], loss=6.2433
	step [280/325], loss=7.3505
	step [281/325], loss=6.8735
	step [282/325], loss=5.5446
	step [283/325], loss=6.1851
	step [284/325], loss=6.4691
	step [285/325], loss=6.7458
	step [286/325], loss=6.3975
	step [287/325], loss=4.9335
	step [288/325], loss=6.7694
	step [289/325], loss=6.9157
	step [290/325], loss=6.5747
	step [291/325], loss=6.1668
	step [292/325], loss=7.5034
	step [293/325], loss=5.7361
	step [294/325], loss=6.9886
	step [295/325], loss=5.5671
	step [296/325], loss=5.1162
	step [297/325], loss=6.3334
	step [298/325], loss=7.3076
	step [299/325], loss=5.5955
	step [300/325], loss=7.6990
	step [301/325], loss=6.4590
	step [302/325], loss=6.1547
	step [303/325], loss=9.3031
	step [304/325], loss=6.0496
	step [305/325], loss=6.2347
	step [306/325], loss=6.0105
	step [307/325], loss=5.4602
	step [308/325], loss=6.5458
	step [309/325], loss=4.9475
	step [310/325], loss=5.4940
	step [311/325], loss=5.7416
	step [312/325], loss=5.5033
	step [313/325], loss=7.4952
	step [314/325], loss=7.6171
	step [315/325], loss=6.3039
	step [316/325], loss=7.6196
	step [317/325], loss=7.5568
	step [318/325], loss=6.7380
	step [319/325], loss=8.6019
	step [320/325], loss=6.2502
	step [321/325], loss=7.3112
	step [322/325], loss=6.2833
	step [323/325], loss=7.1111
	step [324/325], loss=7.4200
	step [325/325], loss=3.5359
	Evaluating
	loss=0.0246, precision=0.1769, recall=0.9962, f1=0.3004
Training epoch 16
	step [1/325], loss=6.3977
	step [2/325], loss=6.2825
	step [3/325], loss=5.9783
	step [4/325], loss=5.9039
	step [5/325], loss=5.6778
	step [6/325], loss=7.3754
	step [7/325], loss=7.2475
	step [8/325], loss=5.9076
	step [9/325], loss=5.9341
	step [10/325], loss=5.5426
	step [11/325], loss=6.2930
	step [12/325], loss=6.0112
	step [13/325], loss=6.5234
	step [14/325], loss=7.2390
	step [15/325], loss=5.7832
	step [16/325], loss=5.7370
	step [17/325], loss=5.6603
	step [18/325], loss=7.9147
	step [19/325], loss=5.9450
	step [20/325], loss=7.2377
	step [21/325], loss=7.6863
	step [22/325], loss=6.8419
	step [23/325], loss=6.0592
	step [24/325], loss=7.1003
	step [25/325], loss=4.6760
	step [26/325], loss=4.9792
	step [27/325], loss=5.8647
	step [28/325], loss=6.3825
	step [29/325], loss=5.7579
	step [30/325], loss=5.2946
	step [31/325], loss=5.7737
	step [32/325], loss=6.2764
	step [33/325], loss=5.9092
	step [34/325], loss=5.7271
	step [35/325], loss=6.9982
	step [36/325], loss=6.8715
	step [37/325], loss=5.9662
	step [38/325], loss=6.3090
	step [39/325], loss=5.8054
	step [40/325], loss=7.0175
	step [41/325], loss=5.9433
	step [42/325], loss=5.6257
	step [43/325], loss=5.3239
	step [44/325], loss=6.0853
	step [45/325], loss=6.8426
	step [46/325], loss=8.9456
	step [47/325], loss=6.5288
	step [48/325], loss=6.7169
	step [49/325], loss=6.4846
	step [50/325], loss=5.9401
	step [51/325], loss=5.3308
	step [52/325], loss=5.1914
	step [53/325], loss=5.9715
	step [54/325], loss=6.6600
	step [55/325], loss=5.8791
	step [56/325], loss=6.9618
	step [57/325], loss=6.5970
	step [58/325], loss=6.0772
	step [59/325], loss=5.3978
	step [60/325], loss=5.7741
	step [61/325], loss=7.1003
	step [62/325], loss=6.7405
	step [63/325], loss=8.4425
	step [64/325], loss=6.4176
	step [65/325], loss=10.5603
	step [66/325], loss=8.2199
	step [67/325], loss=6.0435
	step [68/325], loss=6.2135
	step [69/325], loss=5.8717
	step [70/325], loss=5.4664
	step [71/325], loss=6.1884
	step [72/325], loss=5.8706
	step [73/325], loss=5.3404
	step [74/325], loss=7.1783
	step [75/325], loss=5.7050
	step [76/325], loss=7.3543
	step [77/325], loss=6.7501
	step [78/325], loss=6.1206
	step [79/325], loss=7.0295
	step [80/325], loss=5.8966
	step [81/325], loss=6.9718
	step [82/325], loss=5.4413
	step [83/325], loss=6.1274
	step [84/325], loss=6.0541
	step [85/325], loss=8.1376
	step [86/325], loss=5.2297
	step [87/325], loss=5.1770
	step [88/325], loss=5.4483
	step [89/325], loss=8.0070
	step [90/325], loss=4.9959
	step [91/325], loss=6.1859
	step [92/325], loss=6.0217
	step [93/325], loss=5.4500
	step [94/325], loss=7.0921
	step [95/325], loss=5.8237
	step [96/325], loss=6.7731
	step [97/325], loss=6.7412
	step [98/325], loss=10.0046
	step [99/325], loss=6.7158
	step [100/325], loss=4.5984
	step [101/325], loss=4.4565
	step [102/325], loss=5.3673
	step [103/325], loss=6.0122
	step [104/325], loss=7.7897
	step [105/325], loss=8.2683
	step [106/325], loss=7.1999
	step [107/325], loss=6.3230
	step [108/325], loss=5.8483
	step [109/325], loss=6.4675
	step [110/325], loss=6.9696
	step [111/325], loss=6.8652
	step [112/325], loss=6.6293
	step [113/325], loss=5.9672
	step [114/325], loss=4.8405
	step [115/325], loss=7.3911
	step [116/325], loss=5.6343
	step [117/325], loss=5.7085
	step [118/325], loss=6.7021
	step [119/325], loss=8.0766
	step [120/325], loss=7.7998
	step [121/325], loss=7.0325
	step [122/325], loss=6.5463
	step [123/325], loss=5.3027
	step [124/325], loss=7.0883
	step [125/325], loss=6.4562
	step [126/325], loss=5.7182
	step [127/325], loss=5.9447
	step [128/325], loss=4.8729
	step [129/325], loss=6.6191
	step [130/325], loss=7.6688
	step [131/325], loss=6.2061
	step [132/325], loss=5.3528
	step [133/325], loss=8.0993
	step [134/325], loss=5.6744
	step [135/325], loss=6.5548
	step [136/325], loss=6.9491
	step [137/325], loss=5.6413
	step [138/325], loss=6.5957
	step [139/325], loss=6.2782
	step [140/325], loss=5.1765
	step [141/325], loss=5.1001
	step [142/325], loss=6.6531
	step [143/325], loss=6.2157
	step [144/325], loss=8.0520
	step [145/325], loss=6.0276
	step [146/325], loss=8.1186
	step [147/325], loss=6.5422
	step [148/325], loss=7.1346
	step [149/325], loss=6.7790
	step [150/325], loss=5.9033
	step [151/325], loss=5.2711
	step [152/325], loss=4.7228
	step [153/325], loss=7.0742
	step [154/325], loss=7.8446
	step [155/325], loss=6.4104
	step [156/325], loss=6.2133
	step [157/325], loss=5.8390
	step [158/325], loss=5.9061
	step [159/325], loss=5.3874
	step [160/325], loss=5.9159
	step [161/325], loss=6.3840
	step [162/325], loss=6.3116
	step [163/325], loss=6.9034
	step [164/325], loss=4.8457
	step [165/325], loss=6.2841
	step [166/325], loss=6.1735
	step [167/325], loss=4.9965
	step [168/325], loss=5.6104
	step [169/325], loss=3.9096
	step [170/325], loss=5.9880
	step [171/325], loss=7.0641
	step [172/325], loss=6.2884
	step [173/325], loss=7.3823
	step [174/325], loss=5.3013
	step [175/325], loss=6.2545
	step [176/325], loss=6.1618
	step [177/325], loss=6.6726
	step [178/325], loss=5.9321
	step [179/325], loss=8.5241
	step [180/325], loss=5.1185
	step [181/325], loss=5.8692
	step [182/325], loss=5.2004
	step [183/325], loss=6.7985
	step [184/325], loss=4.9164
	step [185/325], loss=6.3498
	step [186/325], loss=6.2372
	step [187/325], loss=5.1158
	step [188/325], loss=6.2164
	step [189/325], loss=6.0046
	step [190/325], loss=6.9618
	step [191/325], loss=5.8008
	step [192/325], loss=6.3548
	step [193/325], loss=8.7748
	step [194/325], loss=7.0784
	step [195/325], loss=6.4473
	step [196/325], loss=6.7987
	step [197/325], loss=6.4539
	step [198/325], loss=6.8845
	step [199/325], loss=5.6764
	step [200/325], loss=7.6430
	step [201/325], loss=5.3314
	step [202/325], loss=6.3120
	step [203/325], loss=6.3278
	step [204/325], loss=4.9949
	step [205/325], loss=5.3849
	step [206/325], loss=4.8283
	step [207/325], loss=7.5965
	step [208/325], loss=7.1738
	step [209/325], loss=6.5232
	step [210/325], loss=6.1698
	step [211/325], loss=6.8973
	step [212/325], loss=5.4009
	step [213/325], loss=6.2373
	step [214/325], loss=6.6034
	step [215/325], loss=5.6078
	step [216/325], loss=6.8870
	step [217/325], loss=5.7385
	step [218/325], loss=5.3626
	step [219/325], loss=6.5673
	step [220/325], loss=6.8744
	step [221/325], loss=5.1493
	step [222/325], loss=5.9594
	step [223/325], loss=5.8036
	step [224/325], loss=7.5316
	step [225/325], loss=6.3611
	step [226/325], loss=7.4185
	step [227/325], loss=5.7114
	step [228/325], loss=5.8950
	step [229/325], loss=4.7880
	step [230/325], loss=7.2553
	step [231/325], loss=6.1590
	step [232/325], loss=5.8858
	step [233/325], loss=8.0021
	step [234/325], loss=5.1350
	step [235/325], loss=6.1546
	step [236/325], loss=6.3772
	step [237/325], loss=4.9659
	step [238/325], loss=6.1503
	step [239/325], loss=5.7832
	step [240/325], loss=8.5274
	step [241/325], loss=5.3564
	step [242/325], loss=4.6351
	step [243/325], loss=6.2584
	step [244/325], loss=5.6914
	step [245/325], loss=5.9543
	step [246/325], loss=5.6553
	step [247/325], loss=5.7214
	step [248/325], loss=6.3593
	step [249/325], loss=5.6546
	step [250/325], loss=7.6698
	step [251/325], loss=6.8701
	step [252/325], loss=6.1038
	step [253/325], loss=6.4533
	step [254/325], loss=6.6545
	step [255/325], loss=6.8189
	step [256/325], loss=6.6688
	step [257/325], loss=6.0811
	step [258/325], loss=7.1594
	step [259/325], loss=5.8777
	step [260/325], loss=6.5241
	step [261/325], loss=5.6073
	step [262/325], loss=6.6471
	step [263/325], loss=5.4748
	step [264/325], loss=6.3884
	step [265/325], loss=6.9656
	step [266/325], loss=6.2187
	step [267/325], loss=5.4333
	step [268/325], loss=6.6395
	step [269/325], loss=7.2162
	step [270/325], loss=5.4302
	step [271/325], loss=6.8730
	step [272/325], loss=6.8044
	step [273/325], loss=5.1108
	step [274/325], loss=5.6428
	step [275/325], loss=7.1119
	step [276/325], loss=5.1391
	step [277/325], loss=5.8124
	step [278/325], loss=6.6566
	step [279/325], loss=6.7315
	step [280/325], loss=8.0400
	step [281/325], loss=5.3931
	step [282/325], loss=5.2405
	step [283/325], loss=5.7192
	step [284/325], loss=4.7645
	step [285/325], loss=7.0384
	step [286/325], loss=4.7315
	step [287/325], loss=5.5106
	step [288/325], loss=6.0763
	step [289/325], loss=7.6295
	step [290/325], loss=7.6821
	step [291/325], loss=5.2162
	step [292/325], loss=5.8174
	step [293/325], loss=5.4519
	step [294/325], loss=5.2463
	step [295/325], loss=5.0896
	step [296/325], loss=5.9177
	step [297/325], loss=5.8919
	step [298/325], loss=6.2286
	step [299/325], loss=6.2659
	step [300/325], loss=7.1253
	step [301/325], loss=5.3165
	step [302/325], loss=6.2781
	step [303/325], loss=5.8289
	step [304/325], loss=6.0364
	step [305/325], loss=6.3632
	step [306/325], loss=5.5937
	step [307/325], loss=6.0133
	step [308/325], loss=6.5995
	step [309/325], loss=5.3610
	step [310/325], loss=6.9138
	step [311/325], loss=5.5489
	step [312/325], loss=4.4140
	step [313/325], loss=5.0040
	step [314/325], loss=5.6354
	step [315/325], loss=6.0539
	step [316/325], loss=5.9228
	step [317/325], loss=5.3387
	step [318/325], loss=5.9491
	step [319/325], loss=7.0737
	step [320/325], loss=7.0748
	step [321/325], loss=6.6429
	step [322/325], loss=5.1845
	step [323/325], loss=6.6311
	step [324/325], loss=6.7737
	step [325/325], loss=2.8481
	Evaluating
	loss=0.0226, precision=0.1887, recall=0.9958, f1=0.3173
Training epoch 17
	step [1/325], loss=4.8440
	step [2/325], loss=6.6894
	step [3/325], loss=5.2944
	step [4/325], loss=9.2696
	step [5/325], loss=6.5841
	step [6/325], loss=7.4534
	step [7/325], loss=5.7478
	step [8/325], loss=5.4103
	step [9/325], loss=5.6598
	step [10/325], loss=5.7225
	step [11/325], loss=5.8454
	step [12/325], loss=5.8616
	step [13/325], loss=4.2670
	step [14/325], loss=4.7277
	step [15/325], loss=7.0906
	step [16/325], loss=7.2535
	step [17/325], loss=5.1075
	step [18/325], loss=6.4082
	step [19/325], loss=7.1475
	step [20/325], loss=6.1886
	step [21/325], loss=7.8680
	step [22/325], loss=4.7440
	step [23/325], loss=5.2452
	step [24/325], loss=5.3833
	step [25/325], loss=6.7856
	step [26/325], loss=6.1901
	step [27/325], loss=6.5992
	step [28/325], loss=6.6156
	step [29/325], loss=5.2740
	step [30/325], loss=5.1846
	step [31/325], loss=6.3063
	step [32/325], loss=5.3673
	step [33/325], loss=7.6140
	step [34/325], loss=6.9577
	step [35/325], loss=6.7135
	step [36/325], loss=6.6405
	step [37/325], loss=5.7552
	step [38/325], loss=4.5498
	step [39/325], loss=6.1022
	step [40/325], loss=5.8349
	step [41/325], loss=5.8071
	step [42/325], loss=5.8095
	step [43/325], loss=4.1578
	step [44/325], loss=8.3104
	step [45/325], loss=6.9367
	step [46/325], loss=5.9554
	step [47/325], loss=6.4913
	step [48/325], loss=7.5911
	step [49/325], loss=5.8538
	step [50/325], loss=6.6416
	step [51/325], loss=6.1935
	step [52/325], loss=6.0683
	step [53/325], loss=5.5470
	step [54/325], loss=5.3559
	step [55/325], loss=5.7938
	step [56/325], loss=5.2254
	step [57/325], loss=4.4021
	step [58/325], loss=5.3327
	step [59/325], loss=6.8867
	step [60/325], loss=5.6644
	step [61/325], loss=7.0735
	step [62/325], loss=4.8470
	step [63/325], loss=4.8864
	step [64/325], loss=5.5049
	step [65/325], loss=6.2339
	step [66/325], loss=5.1842
	step [67/325], loss=7.7826
	step [68/325], loss=5.2816
	step [69/325], loss=7.8655
	step [70/325], loss=7.3093
	step [71/325], loss=6.9601
	step [72/325], loss=7.1575
	step [73/325], loss=5.2342
	step [74/325], loss=6.0149
	step [75/325], loss=7.3133
	step [76/325], loss=7.2044
	step [77/325], loss=5.9901
	step [78/325], loss=6.8244
	step [79/325], loss=8.1149
	step [80/325], loss=5.8157
	step [81/325], loss=5.6609
	step [82/325], loss=5.2205
	step [83/325], loss=5.1516
	step [84/325], loss=5.7998
	step [85/325], loss=5.9949
	step [86/325], loss=5.1820
	step [87/325], loss=5.3479
	step [88/325], loss=6.4459
	step [89/325], loss=5.6874
	step [90/325], loss=7.3104
	step [91/325], loss=7.3462
	step [92/325], loss=6.4160
	step [93/325], loss=8.1313
	step [94/325], loss=5.9081
	step [95/325], loss=6.6769
	step [96/325], loss=6.1094
	step [97/325], loss=6.2270
	step [98/325], loss=7.4273
	step [99/325], loss=5.8443
	step [100/325], loss=6.6258
	step [101/325], loss=6.6372
	step [102/325], loss=5.7889
	step [103/325], loss=7.6451
	step [104/325], loss=5.7154
	step [105/325], loss=7.4341
	step [106/325], loss=7.5901
	step [107/325], loss=5.3816
	step [108/325], loss=7.6428
	step [109/325], loss=7.1366
	step [110/325], loss=6.1558
	step [111/325], loss=6.5207
	step [112/325], loss=6.9369
	step [113/325], loss=5.5704
	step [114/325], loss=5.6097
	step [115/325], loss=7.5910
	step [116/325], loss=6.2742
	step [117/325], loss=6.2444
	step [118/325], loss=6.2150
	step [119/325], loss=5.6532
	step [120/325], loss=4.4545
	step [121/325], loss=4.8829
	step [122/325], loss=5.0858
	step [123/325], loss=6.2729
	step [124/325], loss=7.1525
	step [125/325], loss=5.1415
	step [126/325], loss=5.6592
	step [127/325], loss=5.1230
	step [128/325], loss=8.1546
	step [129/325], loss=5.1930
	step [130/325], loss=6.2290
	step [131/325], loss=5.0352
	step [132/325], loss=4.6684
	step [133/325], loss=6.1230
	step [134/325], loss=5.2066
	step [135/325], loss=5.5004
	step [136/325], loss=5.9161
	step [137/325], loss=6.7683
	step [138/325], loss=5.3115
	step [139/325], loss=6.9256
	step [140/325], loss=4.8515
	step [141/325], loss=5.0353
	step [142/325], loss=7.1155
	step [143/325], loss=6.4961
	step [144/325], loss=6.3772
	step [145/325], loss=6.3500
	step [146/325], loss=7.0484
	step [147/325], loss=6.6543
	step [148/325], loss=5.0748
	step [149/325], loss=7.3506
	step [150/325], loss=4.6588
	step [151/325], loss=6.8382
	step [152/325], loss=5.5939
	step [153/325], loss=6.6537
	step [154/325], loss=5.1976
	step [155/325], loss=4.8205
	step [156/325], loss=4.8539
	step [157/325], loss=5.0602
	step [158/325], loss=5.5089
	step [159/325], loss=6.6412
	step [160/325], loss=4.3023
	step [161/325], loss=7.2230
	step [162/325], loss=5.4642
	step [163/325], loss=6.5490
	step [164/325], loss=5.8599
	step [165/325], loss=4.8123
	step [166/325], loss=6.6668
	step [167/325], loss=5.6788
	step [168/325], loss=7.4377
	step [169/325], loss=5.5278
	step [170/325], loss=6.3965
	step [171/325], loss=5.9121
	step [172/325], loss=5.1132
	step [173/325], loss=4.2192
	step [174/325], loss=6.5374
	step [175/325], loss=6.5756
	step [176/325], loss=6.3134
	step [177/325], loss=5.7636
	step [178/325], loss=5.4388
	step [179/325], loss=5.0899
	step [180/325], loss=5.8268
	step [181/325], loss=6.0516
	step [182/325], loss=5.4617
	step [183/325], loss=6.7906
	step [184/325], loss=5.1438
	step [185/325], loss=5.9722
	step [186/325], loss=6.2157
	step [187/325], loss=6.0804
	step [188/325], loss=5.6306
	step [189/325], loss=5.7096
	step [190/325], loss=7.0166
	step [191/325], loss=6.2340
	step [192/325], loss=6.8837
	step [193/325], loss=5.2564
	step [194/325], loss=5.5214
	step [195/325], loss=4.8554
	step [196/325], loss=7.1819
	step [197/325], loss=6.2166
	step [198/325], loss=8.3940
	step [199/325], loss=5.9586
	step [200/325], loss=4.3049
	step [201/325], loss=6.5315
	step [202/325], loss=7.8940
	step [203/325], loss=6.3711
	step [204/325], loss=6.4129
	step [205/325], loss=4.4483
	step [206/325], loss=8.2499
	step [207/325], loss=5.5151
	step [208/325], loss=7.8477
	step [209/325], loss=5.5047
	step [210/325], loss=6.1818
	step [211/325], loss=5.1384
	step [212/325], loss=7.1668
	step [213/325], loss=6.3739
	step [214/325], loss=4.9894
	step [215/325], loss=5.4313
	step [216/325], loss=5.8279
	step [217/325], loss=4.8025
	step [218/325], loss=7.6892
	step [219/325], loss=5.2997
	step [220/325], loss=7.1996
	step [221/325], loss=8.0239
	step [222/325], loss=6.5826
	step [223/325], loss=6.6737
	step [224/325], loss=6.6767
	step [225/325], loss=6.9770
	step [226/325], loss=4.5777
	step [227/325], loss=7.3608
	step [228/325], loss=5.3366
	step [229/325], loss=6.3385
	step [230/325], loss=5.5360
	step [231/325], loss=5.8769
	step [232/325], loss=5.7980
	step [233/325], loss=6.1377
	step [234/325], loss=5.8622
	step [235/325], loss=5.9925
	step [236/325], loss=5.9255
	step [237/325], loss=5.0800
	step [238/325], loss=7.2741
	step [239/325], loss=5.2456
	step [240/325], loss=7.3107
	step [241/325], loss=5.1725
	step [242/325], loss=4.9469
	step [243/325], loss=5.0997
	step [244/325], loss=7.0875
	step [245/325], loss=5.3119
	step [246/325], loss=7.1113
	step [247/325], loss=4.7435
	step [248/325], loss=5.3129
	step [249/325], loss=7.7743
	step [250/325], loss=5.6491
	step [251/325], loss=8.2519
	step [252/325], loss=6.3546
	step [253/325], loss=5.9899
	step [254/325], loss=5.9514
	step [255/325], loss=5.6105
	step [256/325], loss=6.3921
	step [257/325], loss=8.0917
	step [258/325], loss=7.4053
	step [259/325], loss=7.8318
	step [260/325], loss=5.9250
	step [261/325], loss=7.5946
	step [262/325], loss=6.6693
	step [263/325], loss=6.1478
	step [264/325], loss=5.8013
	step [265/325], loss=4.4979
	step [266/325], loss=5.2315
	step [267/325], loss=8.3520
	step [268/325], loss=5.6766
	step [269/325], loss=6.4979
	step [270/325], loss=5.3057
	step [271/325], loss=5.8424
	step [272/325], loss=7.1928
	step [273/325], loss=5.2529
	step [274/325], loss=4.7624
	step [275/325], loss=5.3744
	step [276/325], loss=5.3459
	step [277/325], loss=6.3138
	step [278/325], loss=6.0977
	step [279/325], loss=6.8313
	step [280/325], loss=5.5539
	step [281/325], loss=4.8050
	step [282/325], loss=5.2560
	step [283/325], loss=5.7811
	step [284/325], loss=5.0980
	step [285/325], loss=3.9436
	step [286/325], loss=6.2505
	step [287/325], loss=6.4812
	step [288/325], loss=5.3692
	step [289/325], loss=6.8264
	step [290/325], loss=6.4038
	step [291/325], loss=5.1341
	step [292/325], loss=5.0221
	step [293/325], loss=5.3155
	step [294/325], loss=5.7769
	step [295/325], loss=6.5454
	step [296/325], loss=6.8904
	step [297/325], loss=7.1598
	step [298/325], loss=5.3464
	step [299/325], loss=5.4955
	step [300/325], loss=5.4381
	step [301/325], loss=6.2874
	step [302/325], loss=5.1413
	step [303/325], loss=6.0902
	step [304/325], loss=6.6270
	step [305/325], loss=5.5057
	step [306/325], loss=7.5763
	step [307/325], loss=5.9716
	step [308/325], loss=6.5056
	step [309/325], loss=6.8125
	step [310/325], loss=7.0522
	step [311/325], loss=6.4836
	step [312/325], loss=5.9510
	step [313/325], loss=6.3905
	step [314/325], loss=6.6958
	step [315/325], loss=6.2555
	step [316/325], loss=5.4405
	step [317/325], loss=7.0463
	step [318/325], loss=4.8714
	step [319/325], loss=4.1541
	step [320/325], loss=8.1774
	step [321/325], loss=9.1260
	step [322/325], loss=6.3429
	step [323/325], loss=5.5438
	step [324/325], loss=7.4012
	step [325/325], loss=3.2610
	Evaluating
	loss=0.0201, precision=0.1997, recall=0.9958, f1=0.3327
Training epoch 18
	step [1/325], loss=4.7676
	step [2/325], loss=6.8430
	step [3/325], loss=5.8996
	step [4/325], loss=6.8554
	step [5/325], loss=6.6880
	step [6/325], loss=5.1721
	step [7/325], loss=6.1304
	step [8/325], loss=7.2642
	step [9/325], loss=9.7744
	step [10/325], loss=6.9992
	step [11/325], loss=6.7633
	step [12/325], loss=4.4555
	step [13/325], loss=5.2023
	step [14/325], loss=6.0628
	step [15/325], loss=7.0949
	step [16/325], loss=5.7461
	step [17/325], loss=7.8269
	step [18/325], loss=4.4310
	step [19/325], loss=6.1047
	step [20/325], loss=6.3040
	step [21/325], loss=5.2245
	step [22/325], loss=5.8353
	step [23/325], loss=6.4521
	step [24/325], loss=5.4972
	step [25/325], loss=5.8054
	step [26/325], loss=6.4193
	step [27/325], loss=4.5333
	step [28/325], loss=6.8389
	step [29/325], loss=7.7638
	step [30/325], loss=6.9840
	step [31/325], loss=5.1785
	step [32/325], loss=6.1133
	step [33/325], loss=5.8641
	step [34/325], loss=5.4629
	step [35/325], loss=6.4286
	step [36/325], loss=6.8004
	step [37/325], loss=6.9199
	step [38/325], loss=6.0662
	step [39/325], loss=5.3608
	step [40/325], loss=4.8951
	step [41/325], loss=5.3637
	step [42/325], loss=6.9847
	step [43/325], loss=5.9337
	step [44/325], loss=5.0968
	step [45/325], loss=4.5681
	step [46/325], loss=6.1534
	step [47/325], loss=7.2622
	step [48/325], loss=4.7842
	step [49/325], loss=4.9944
	step [50/325], loss=6.4594
	step [51/325], loss=6.6197
	step [52/325], loss=4.8426
	step [53/325], loss=6.7510
	step [54/325], loss=5.7667
	step [55/325], loss=5.6042
	step [56/325], loss=7.9589
	step [57/325], loss=5.5530
	step [58/325], loss=5.9052
	step [59/325], loss=5.6737
	step [60/325], loss=7.8941
	step [61/325], loss=7.0072
	step [62/325], loss=6.4339
	step [63/325], loss=5.9367
	step [64/325], loss=5.9794
	step [65/325], loss=4.8054
	step [66/325], loss=6.6815
	step [67/325], loss=5.3758
	step [68/325], loss=5.1382
	step [69/325], loss=5.2662
	step [70/325], loss=6.1054
	step [71/325], loss=6.7314
	step [72/325], loss=5.8332
	step [73/325], loss=6.1405
	step [74/325], loss=6.7972
	step [75/325], loss=6.7381
	step [76/325], loss=7.0298
	step [77/325], loss=5.1174
	step [78/325], loss=5.9299
	step [79/325], loss=7.6086
	step [80/325], loss=4.7357
	step [81/325], loss=5.3067
	step [82/325], loss=6.5446
	step [83/325], loss=4.8485
	step [84/325], loss=6.5752
	step [85/325], loss=5.9721
	step [86/325], loss=5.8394
	step [87/325], loss=6.2473
	step [88/325], loss=4.6393
	step [89/325], loss=5.9233
	step [90/325], loss=5.8620
	step [91/325], loss=6.7079
	step [92/325], loss=4.5962
	step [93/325], loss=3.9178
	step [94/325], loss=6.4134
	step [95/325], loss=6.7591
	step [96/325], loss=5.2340
	step [97/325], loss=5.6156
	step [98/325], loss=5.7090
	step [99/325], loss=5.9686
	step [100/325], loss=4.8678
	step [101/325], loss=7.7151
	step [102/325], loss=3.7447
	step [103/325], loss=6.0017
	step [104/325], loss=4.6718
	step [105/325], loss=6.5435
	step [106/325], loss=6.5861
	step [107/325], loss=5.9146
	step [108/325], loss=4.5530
	step [109/325], loss=6.1164
	step [110/325], loss=5.9451
	step [111/325], loss=5.2863
	step [112/325], loss=6.5092
	step [113/325], loss=4.2212
	step [114/325], loss=6.7820
	step [115/325], loss=5.9196
	step [116/325], loss=5.8174
	step [117/325], loss=5.9714
	step [118/325], loss=5.2298
	step [119/325], loss=4.6301
	step [120/325], loss=6.9957
	step [121/325], loss=5.4960
	step [122/325], loss=6.2101
	step [123/325], loss=4.9768
	step [124/325], loss=4.7950
	step [125/325], loss=4.7244
	step [126/325], loss=5.2177
	step [127/325], loss=6.1055
	step [128/325], loss=5.0824
	step [129/325], loss=6.1091
	step [130/325], loss=7.7868
	step [131/325], loss=6.6139
	step [132/325], loss=7.6509
	step [133/325], loss=5.6347
	step [134/325], loss=8.6125
	step [135/325], loss=5.4207
	step [136/325], loss=6.1309
	step [137/325], loss=4.9464
	step [138/325], loss=5.8895
	step [139/325], loss=4.5998
	step [140/325], loss=8.4155
	step [141/325], loss=5.5655
	step [142/325], loss=5.7076
	step [143/325], loss=6.1639
	step [144/325], loss=6.7293
	step [145/325], loss=7.5669
	step [146/325], loss=6.0647
	step [147/325], loss=7.0378
	step [148/325], loss=4.7586
	step [149/325], loss=6.6366
	step [150/325], loss=6.1443
	step [151/325], loss=5.2133
	step [152/325], loss=4.7462
	step [153/325], loss=5.7994
	step [154/325], loss=6.7785
	step [155/325], loss=5.6717
	step [156/325], loss=5.2103
	step [157/325], loss=6.3513
	step [158/325], loss=6.1571
	step [159/325], loss=5.7471
	step [160/325], loss=5.0288
	step [161/325], loss=6.9217
	step [162/325], loss=6.8005
	step [163/325], loss=5.4330
	step [164/325], loss=4.7222
	step [165/325], loss=5.9493
	step [166/325], loss=5.9651
	step [167/325], loss=6.3580
	step [168/325], loss=5.1934
	step [169/325], loss=6.6754
	step [170/325], loss=4.5409
	step [171/325], loss=6.6371
	step [172/325], loss=5.7268
	step [173/325], loss=5.2688
	step [174/325], loss=5.6860
	step [175/325], loss=6.2895
	step [176/325], loss=7.9440
	step [177/325], loss=4.8434
	step [178/325], loss=5.4304
	step [179/325], loss=6.2500
	step [180/325], loss=5.3865
	step [181/325], loss=5.8501
	step [182/325], loss=6.5042
	step [183/325], loss=6.2042
	step [184/325], loss=6.3745
	step [185/325], loss=8.2212
	step [186/325], loss=6.7814
	step [187/325], loss=4.5781
	step [188/325], loss=4.3276
	step [189/325], loss=5.0271
	step [190/325], loss=6.5278
	step [191/325], loss=6.0660
	step [192/325], loss=4.8718
	step [193/325], loss=5.9591
	step [194/325], loss=6.2035
	step [195/325], loss=4.2482
	step [196/325], loss=4.8128
	step [197/325], loss=5.6036
	step [198/325], loss=6.6686
	step [199/325], loss=5.7478
	step [200/325], loss=6.0340
	step [201/325], loss=6.3999
	step [202/325], loss=5.4212
	step [203/325], loss=6.2358
	step [204/325], loss=5.8414
	step [205/325], loss=6.2005
	step [206/325], loss=6.0295
	step [207/325], loss=6.3039
	step [208/325], loss=6.2633
	step [209/325], loss=6.2396
	step [210/325], loss=5.9655
	step [211/325], loss=6.7310
	step [212/325], loss=5.5442
	step [213/325], loss=5.3630
	step [214/325], loss=5.3931
	step [215/325], loss=4.9162
	step [216/325], loss=7.5063
	step [217/325], loss=5.2474
	step [218/325], loss=5.4993
	step [219/325], loss=5.2392
	step [220/325], loss=5.8951
	step [221/325], loss=5.2049
	step [222/325], loss=6.5926
	step [223/325], loss=4.4655
	step [224/325], loss=5.3284
	step [225/325], loss=6.4087
	step [226/325], loss=7.0590
	step [227/325], loss=5.6041
	step [228/325], loss=5.2453
	step [229/325], loss=7.2885
	step [230/325], loss=6.5840
	step [231/325], loss=5.1366
	step [232/325], loss=5.7370
	step [233/325], loss=6.6682
	step [234/325], loss=5.6391
	step [235/325], loss=5.1204
	step [236/325], loss=5.1104
	step [237/325], loss=5.9903
	step [238/325], loss=7.2708
	step [239/325], loss=6.6747
	step [240/325], loss=7.7963
	step [241/325], loss=5.6438
	step [242/325], loss=5.5337
	step [243/325], loss=4.8374
	step [244/325], loss=6.0143
	step [245/325], loss=5.2727
	step [246/325], loss=6.8212
	step [247/325], loss=5.5032
	step [248/325], loss=4.6173
	step [249/325], loss=7.2155
	step [250/325], loss=8.4277
	step [251/325], loss=5.2913
	step [252/325], loss=5.4889
	step [253/325], loss=5.7477
	step [254/325], loss=7.4705
	step [255/325], loss=5.8036
	step [256/325], loss=9.0812
	step [257/325], loss=5.3061
	step [258/325], loss=5.6560
	step [259/325], loss=6.0976
	step [260/325], loss=6.9242
	step [261/325], loss=6.1072
	step [262/325], loss=5.6302
	step [263/325], loss=7.8969
	step [264/325], loss=6.4270
	step [265/325], loss=5.4059
	step [266/325], loss=5.0384
	step [267/325], loss=4.8605
	step [268/325], loss=6.5095
	step [269/325], loss=5.0622
	step [270/325], loss=5.1869
	step [271/325], loss=5.7893
	step [272/325], loss=5.9124
	step [273/325], loss=5.4978
	step [274/325], loss=6.7184
	step [275/325], loss=5.2051
	step [276/325], loss=6.6799
	step [277/325], loss=6.2463
	step [278/325], loss=5.9471
	step [279/325], loss=5.7578
	step [280/325], loss=6.2258
	step [281/325], loss=4.6358
	step [282/325], loss=5.6381
	step [283/325], loss=7.6064
	step [284/325], loss=4.2406
	step [285/325], loss=6.6331
	step [286/325], loss=6.5932
	step [287/325], loss=6.2689
	step [288/325], loss=4.1912
	step [289/325], loss=5.6752
	step [290/325], loss=5.4211
	step [291/325], loss=5.8308
	step [292/325], loss=5.1214
	step [293/325], loss=5.6825
	step [294/325], loss=5.9031
	step [295/325], loss=7.3014
	step [296/325], loss=5.7813
	step [297/325], loss=7.9674
	step [298/325], loss=8.4422
	step [299/325], loss=5.8576
	step [300/325], loss=5.3330
	step [301/325], loss=6.0770
	step [302/325], loss=6.8025
	step [303/325], loss=5.2718
	step [304/325], loss=5.1842
	step [305/325], loss=6.8696
	step [306/325], loss=5.5948
	step [307/325], loss=5.5394
	step [308/325], loss=6.7121
	step [309/325], loss=4.2818
	step [310/325], loss=5.6208
	step [311/325], loss=6.8274
	step [312/325], loss=5.8778
	step [313/325], loss=6.8106
	step [314/325], loss=7.2840
	step [315/325], loss=7.2733
	step [316/325], loss=6.2993
	step [317/325], loss=5.3248
	step [318/325], loss=7.0926
	step [319/325], loss=6.0723
	step [320/325], loss=6.6543
	step [321/325], loss=5.3401
	step [322/325], loss=5.6625
	step [323/325], loss=6.1722
	step [324/325], loss=6.4711
	step [325/325], loss=5.2996
	Evaluating
	loss=0.0197, precision=0.2243, recall=0.9942, f1=0.3661
Training epoch 19
	step [1/325], loss=5.7516
	step [2/325], loss=5.4439
	step [3/325], loss=7.2640
	step [4/325], loss=4.5605
	step [5/325], loss=5.6445
	step [6/325], loss=5.5484
	step [7/325], loss=6.0668
	step [8/325], loss=5.8657
	step [9/325], loss=5.3959
	step [10/325], loss=6.2792
	step [11/325], loss=4.6377
	step [12/325], loss=5.5346
	step [13/325], loss=6.2404
	step [14/325], loss=6.0201
	step [15/325], loss=5.0934
	step [16/325], loss=5.8618
	step [17/325], loss=6.9496
	step [18/325], loss=5.0090
	step [19/325], loss=7.0784
	step [20/325], loss=8.1878
	step [21/325], loss=6.5081
	step [22/325], loss=7.1772
	step [23/325], loss=4.6820
	step [24/325], loss=5.6437
	step [25/325], loss=5.2617
	step [26/325], loss=5.2201
	step [27/325], loss=6.7076
	step [28/325], loss=6.0377
	step [29/325], loss=5.0377
	step [30/325], loss=5.1432
	step [31/325], loss=6.4572
	step [32/325], loss=5.2937
	step [33/325], loss=8.3373
	step [34/325], loss=6.6938
	step [35/325], loss=4.6872
	step [36/325], loss=5.4779
	step [37/325], loss=6.3627
	step [38/325], loss=4.5001
	step [39/325], loss=5.4355
	step [40/325], loss=4.4273
	step [41/325], loss=5.4875
	step [42/325], loss=6.7721
	step [43/325], loss=6.4544
	step [44/325], loss=6.8428
	step [45/325], loss=5.4291
	step [46/325], loss=5.6070
	step [47/325], loss=6.4167
	step [48/325], loss=5.0471
	step [49/325], loss=5.6863
	step [50/325], loss=6.6220
	step [51/325], loss=7.3812
	step [52/325], loss=5.2399
	step [53/325], loss=4.6668
	step [54/325], loss=5.6902
	step [55/325], loss=6.4600
	step [56/325], loss=5.5949
	step [57/325], loss=4.5542
	step [58/325], loss=7.9343
	step [59/325], loss=6.0658
	step [60/325], loss=7.8075
	step [61/325], loss=6.4681
	step [62/325], loss=4.9229
	step [63/325], loss=5.8717
	step [64/325], loss=6.6261
	step [65/325], loss=5.3923
	step [66/325], loss=6.9022
	step [67/325], loss=5.5065
	step [68/325], loss=5.8542
	step [69/325], loss=5.8775
	step [70/325], loss=5.5334
	step [71/325], loss=5.7434
	step [72/325], loss=5.2136
	step [73/325], loss=7.3382
	step [74/325], loss=5.0181
	step [75/325], loss=5.2522
	step [76/325], loss=5.3215
	step [77/325], loss=4.8253
	step [78/325], loss=5.0369
	step [79/325], loss=5.5459
	step [80/325], loss=6.9062
	step [81/325], loss=6.0290
	step [82/325], loss=4.9008
	step [83/325], loss=5.7914
	step [84/325], loss=6.1141
	step [85/325], loss=6.2950
	step [86/325], loss=5.1439
	step [87/325], loss=6.0666
	step [88/325], loss=5.0485
	step [89/325], loss=6.1246
	step [90/325], loss=6.6071
	step [91/325], loss=6.4754
	step [92/325], loss=5.7606
	step [93/325], loss=5.6417
	step [94/325], loss=8.1869
	step [95/325], loss=4.6753
	step [96/325], loss=6.3875
	step [97/325], loss=6.4571
	step [98/325], loss=4.8923
	step [99/325], loss=5.4985
	step [100/325], loss=4.9834
	step [101/325], loss=6.2711
	step [102/325], loss=4.1275
	step [103/325], loss=6.3835
	step [104/325], loss=6.4582
	step [105/325], loss=6.8264
	step [106/325], loss=4.4068
	step [107/325], loss=6.2253
	step [108/325], loss=4.8078
	step [109/325], loss=5.1709
	step [110/325], loss=6.6156
	step [111/325], loss=7.1609
	step [112/325], loss=5.8644
	step [113/325], loss=5.5477
	step [114/325], loss=6.4744
	step [115/325], loss=5.7066
	step [116/325], loss=5.5379
	step [117/325], loss=6.5795
	step [118/325], loss=6.8863
	step [119/325], loss=5.9549
	step [120/325], loss=3.9353
	step [121/325], loss=5.3470
	step [122/325], loss=6.4002
	step [123/325], loss=5.2376
	step [124/325], loss=6.1003
	step [125/325], loss=7.3291
	step [126/325], loss=6.9215
	step [127/325], loss=5.2701
	step [128/325], loss=6.8969
	step [129/325], loss=5.3918
	step [130/325], loss=6.9410
	step [131/325], loss=6.4156
	step [132/325], loss=6.2362
	step [133/325], loss=4.4781
	step [134/325], loss=5.4271
	step [135/325], loss=5.4822
	step [136/325], loss=5.8534
	step [137/325], loss=4.5296
	step [138/325], loss=7.9826
	step [139/325], loss=5.7499
	step [140/325], loss=5.7818
	step [141/325], loss=4.9776
	step [142/325], loss=4.9183
	step [143/325], loss=5.6886
	step [144/325], loss=5.1044
	step [145/325], loss=6.2278
	step [146/325], loss=4.8481
	step [147/325], loss=6.1412
	step [148/325], loss=6.2383
	step [149/325], loss=8.4135
	step [150/325], loss=5.3849
	step [151/325], loss=3.7565
	step [152/325], loss=4.7752
	step [153/325], loss=6.3310
	step [154/325], loss=5.1844
	step [155/325], loss=5.5953
	step [156/325], loss=4.6731
	step [157/325], loss=5.9854
	step [158/325], loss=4.2135
	step [159/325], loss=6.0122
	step [160/325], loss=5.8241
	step [161/325], loss=6.0050
	step [162/325], loss=6.8879
	step [163/325], loss=5.5084
	step [164/325], loss=6.1449
	step [165/325], loss=4.3208
	step [166/325], loss=5.8399
	step [167/325], loss=5.9869
	step [168/325], loss=5.3619
	step [169/325], loss=5.3337
	step [170/325], loss=4.2605
	step [171/325], loss=6.4888
	step [172/325], loss=5.0023
	step [173/325], loss=6.0680
	step [174/325], loss=4.7238
	step [175/325], loss=5.5806
	step [176/325], loss=7.4051
	step [177/325], loss=5.1433
	step [178/325], loss=5.9494
	step [179/325], loss=5.4105
	step [180/325], loss=7.9810
	step [181/325], loss=6.9044
	step [182/325], loss=6.0775
	step [183/325], loss=4.9950
	step [184/325], loss=5.9609
	step [185/325], loss=5.9345
	step [186/325], loss=6.7377
	step [187/325], loss=8.0499
	step [188/325], loss=5.5025
	step [189/325], loss=6.2911
	step [190/325], loss=4.3015
	step [191/325], loss=6.5491
	step [192/325], loss=4.1386
	step [193/325], loss=5.4604
	step [194/325], loss=5.3452
	step [195/325], loss=5.5269
	step [196/325], loss=5.9006
	step [197/325], loss=4.9216
	step [198/325], loss=5.3019
	step [199/325], loss=6.2152
	step [200/325], loss=6.1085
	step [201/325], loss=4.8119
	step [202/325], loss=4.8160
	step [203/325], loss=5.2922
	step [204/325], loss=6.5557
	step [205/325], loss=4.3438
	step [206/325], loss=5.6340
	step [207/325], loss=7.0531
	step [208/325], loss=4.8813
	step [209/325], loss=5.9065
	step [210/325], loss=7.9239
	step [211/325], loss=5.5348
	step [212/325], loss=6.9819
	step [213/325], loss=6.2800
	step [214/325], loss=6.8842
	step [215/325], loss=6.3418
	step [216/325], loss=6.8440
	step [217/325], loss=6.1738
	step [218/325], loss=5.9078
	step [219/325], loss=5.7036
	step [220/325], loss=5.2876
	step [221/325], loss=8.9711
	step [222/325], loss=5.4668
	step [223/325], loss=6.6694
	step [224/325], loss=6.2446
	step [225/325], loss=5.1987
	step [226/325], loss=7.7109
	step [227/325], loss=5.0244
	step [228/325], loss=5.7823
	step [229/325], loss=5.7369
	step [230/325], loss=5.6477
	step [231/325], loss=5.5322
	step [232/325], loss=6.5711
	step [233/325], loss=6.6664
	step [234/325], loss=4.7890
	step [235/325], loss=5.0109
	step [236/325], loss=5.7005
	step [237/325], loss=5.2339
	step [238/325], loss=5.3975
	step [239/325], loss=5.9613
	step [240/325], loss=6.4856
	step [241/325], loss=6.2910
	step [242/325], loss=4.0051
	step [243/325], loss=6.2825
	step [244/325], loss=4.7254
	step [245/325], loss=5.3156
	step [246/325], loss=5.9366
	step [247/325], loss=7.0846
	step [248/325], loss=5.2731
	step [249/325], loss=8.0559
	step [250/325], loss=6.3555
	step [251/325], loss=4.8010
	step [252/325], loss=4.9174
	step [253/325], loss=5.8570
	step [254/325], loss=5.4981
	step [255/325], loss=5.4006
	step [256/325], loss=5.1178
	step [257/325], loss=5.2638
	step [258/325], loss=6.5036
	step [259/325], loss=5.6052
	step [260/325], loss=3.4302
	step [261/325], loss=6.3414
	step [262/325], loss=5.2121
	step [263/325], loss=6.9920
	step [264/325], loss=5.3450
	step [265/325], loss=6.4817
	step [266/325], loss=5.8787
	step [267/325], loss=5.5153
	step [268/325], loss=6.9673
	step [269/325], loss=5.7236
	step [270/325], loss=5.2620
	step [271/325], loss=6.3251
	step [272/325], loss=5.0117
	step [273/325], loss=7.5730
	step [274/325], loss=6.0170
	step [275/325], loss=6.1877
	step [276/325], loss=5.5515
	step [277/325], loss=5.5904
	step [278/325], loss=5.6502
	step [279/325], loss=5.1868
	step [280/325], loss=6.3010
	step [281/325], loss=5.4826
	step [282/325], loss=6.0864
	step [283/325], loss=6.1350
	step [284/325], loss=4.9997
	step [285/325], loss=6.8340
	step [286/325], loss=5.4661
	step [287/325], loss=6.1807
	step [288/325], loss=5.7763
	step [289/325], loss=5.1089
	step [290/325], loss=3.9892
	step [291/325], loss=4.6744
	step [292/325], loss=5.4107
	step [293/325], loss=5.5959
	step [294/325], loss=5.6954
	step [295/325], loss=5.6517
	step [296/325], loss=5.2311
	step [297/325], loss=6.1855
	step [298/325], loss=6.5956
	step [299/325], loss=6.0734
	step [300/325], loss=4.9473
	step [301/325], loss=5.3639
	step [302/325], loss=6.1373
	step [303/325], loss=6.4026
	step [304/325], loss=6.7594
	step [305/325], loss=5.8665
	step [306/325], loss=7.0835
	step [307/325], loss=6.2331
	step [308/325], loss=5.6699
	step [309/325], loss=4.8888
	step [310/325], loss=6.2202
	step [311/325], loss=5.7411
	step [312/325], loss=5.4926
	step [313/325], loss=5.2716
	step [314/325], loss=7.2639
	step [315/325], loss=6.3337
	step [316/325], loss=5.0324
	step [317/325], loss=5.1932
	step [318/325], loss=5.9356
	step [319/325], loss=5.3627
	step [320/325], loss=6.5073
	step [321/325], loss=5.8841
	step [322/325], loss=5.8191
	step [323/325], loss=6.9928
	step [324/325], loss=5.4062
	step [325/325], loss=2.6605
	Evaluating
	loss=0.0215, precision=0.2008, recall=0.9953, f1=0.3342
Training epoch 20
	step [1/325], loss=6.5395
	step [2/325], loss=5.7579
	step [3/325], loss=4.3685
	step [4/325], loss=6.2264
	step [5/325], loss=5.7675
	step [6/325], loss=6.2465
	step [7/325], loss=6.0901
	step [8/325], loss=5.3233
	step [9/325], loss=6.4649
	step [10/325], loss=5.0062
	step [11/325], loss=6.2975
	step [12/325], loss=6.2749
	step [13/325], loss=5.8325
	step [14/325], loss=5.3040
	step [15/325], loss=6.5219
	step [16/325], loss=5.4519
	step [17/325], loss=6.2634
	step [18/325], loss=6.0865
	step [19/325], loss=5.9450
	step [20/325], loss=5.8269
	step [21/325], loss=4.8886
	step [22/325], loss=4.6556
	step [23/325], loss=5.6854
	step [24/325], loss=8.6053
	step [25/325], loss=6.3593
	step [26/325], loss=7.9799
	step [27/325], loss=5.1728
	step [28/325], loss=7.4949
	step [29/325], loss=5.4949
	step [30/325], loss=5.2091
	step [31/325], loss=6.0924
	step [32/325], loss=5.9555
	step [33/325], loss=6.3166
	step [34/325], loss=6.4307
	step [35/325], loss=5.5503
	step [36/325], loss=6.5886
	step [37/325], loss=5.6371
	step [38/325], loss=4.9029
	step [39/325], loss=6.6138
	step [40/325], loss=6.1630
	step [41/325], loss=4.7404
	step [42/325], loss=7.0024
	step [43/325], loss=6.4768
	step [44/325], loss=4.8294
	step [45/325], loss=5.5812
	step [46/325], loss=6.5872
	step [47/325], loss=5.8208
	step [48/325], loss=5.9668
	step [49/325], loss=5.5862
	step [50/325], loss=5.3894
	step [51/325], loss=4.9159
	step [52/325], loss=7.5184
	step [53/325], loss=5.4030
	step [54/325], loss=6.3421
	step [55/325], loss=4.6878
	step [56/325], loss=5.5547
	step [57/325], loss=5.4987
	step [58/325], loss=5.7944
	step [59/325], loss=3.8119
	step [60/325], loss=4.1301
	step [61/325], loss=7.1811
	step [62/325], loss=5.0885
	step [63/325], loss=8.6652
	step [64/325], loss=4.9112
	step [65/325], loss=4.8371
	step [66/325], loss=5.1328
	step [67/325], loss=6.6722
	step [68/325], loss=6.3206
	step [69/325], loss=5.0315
	step [70/325], loss=5.3062
	step [71/325], loss=5.1768
	step [72/325], loss=7.3853
	step [73/325], loss=7.7398
	step [74/325], loss=6.8278
	step [75/325], loss=5.4197
	step [76/325], loss=6.2418
	step [77/325], loss=6.7482
	step [78/325], loss=8.0233
	step [79/325], loss=5.2479
	step [80/325], loss=6.4419
	step [81/325], loss=4.6177
	step [82/325], loss=5.4373
	step [83/325], loss=6.2969
	step [84/325], loss=5.6066
	step [85/325], loss=7.2392
	step [86/325], loss=5.8303
	step [87/325], loss=4.8316
	step [88/325], loss=7.0794
	step [89/325], loss=6.0153
	step [90/325], loss=4.2713
	step [91/325], loss=5.3956
	step [92/325], loss=5.0975
	step [93/325], loss=6.0272
	step [94/325], loss=5.6005
	step [95/325], loss=5.2776
	step [96/325], loss=5.5162
	step [97/325], loss=5.8326
	step [98/325], loss=4.0582
	step [99/325], loss=4.4857
	step [100/325], loss=5.9708
	step [101/325], loss=7.2930
	step [102/325], loss=5.3446
	step [103/325], loss=5.5675
	step [104/325], loss=5.2294
	step [105/325], loss=4.7252
	step [106/325], loss=6.0968
	step [107/325], loss=7.0844
	step [108/325], loss=6.1929
	step [109/325], loss=6.3628
	step [110/325], loss=5.8196
	step [111/325], loss=6.2811
	step [112/325], loss=5.3782
	step [113/325], loss=5.0984
	step [114/325], loss=7.3304
	step [115/325], loss=4.6112
	step [116/325], loss=6.8324
	step [117/325], loss=5.0429
	step [118/325], loss=7.0146
	step [119/325], loss=4.7329
	step [120/325], loss=7.5736
	step [121/325], loss=6.3388
	step [122/325], loss=5.8928
	step [123/325], loss=7.0162
	step [124/325], loss=5.0529
	step [125/325], loss=5.8392
	step [126/325], loss=5.7780
	step [127/325], loss=6.0316
	step [128/325], loss=4.2826
	step [129/325], loss=5.5501
	step [130/325], loss=4.4295
	step [131/325], loss=4.9632
	step [132/325], loss=4.8951
	step [133/325], loss=6.4339
	step [134/325], loss=5.3621
	step [135/325], loss=6.3703
	step [136/325], loss=5.3377
	step [137/325], loss=5.5397
	step [138/325], loss=4.4654
	step [139/325], loss=6.0368
	step [140/325], loss=6.6745
	step [141/325], loss=5.5483
	step [142/325], loss=4.1847
	step [143/325], loss=5.5096
	step [144/325], loss=4.6918
	step [145/325], loss=6.0667
	step [146/325], loss=4.9656
	step [147/325], loss=6.8516
	step [148/325], loss=6.1189
	step [149/325], loss=4.4721
	step [150/325], loss=4.3365
	step [151/325], loss=5.2821
	step [152/325], loss=5.7214
	step [153/325], loss=4.9941
	step [154/325], loss=4.6087
	step [155/325], loss=5.3847
	step [156/325], loss=5.8160
	step [157/325], loss=5.7023
	step [158/325], loss=7.6970
	step [159/325], loss=5.0700
	step [160/325], loss=6.0153
	step [161/325], loss=5.0530
	step [162/325], loss=5.7006
	step [163/325], loss=5.6305
	step [164/325], loss=5.9931
	step [165/325], loss=5.9111
	step [166/325], loss=5.4550
	step [167/325], loss=5.6987
	step [168/325], loss=5.4386
	step [169/325], loss=6.6051
	step [170/325], loss=6.1356
	step [171/325], loss=5.5766
	step [172/325], loss=6.2973
	step [173/325], loss=6.5807
	step [174/325], loss=5.8660
	step [175/325], loss=5.8718
	step [176/325], loss=5.1945
	step [177/325], loss=5.3061
	step [178/325], loss=4.9466
	step [179/325], loss=5.3430
	step [180/325], loss=4.8956
	step [181/325], loss=5.7850
	step [182/325], loss=5.5238
	step [183/325], loss=7.0029
	step [184/325], loss=6.0259
	step [185/325], loss=4.8480
	step [186/325], loss=4.4557
	step [187/325], loss=5.7904
	step [188/325], loss=6.7759
	step [189/325], loss=7.2594
	step [190/325], loss=5.0486
	step [191/325], loss=5.2671
	step [192/325], loss=7.0887
	step [193/325], loss=6.6110
	step [194/325], loss=4.8708
	step [195/325], loss=4.6270
	step [196/325], loss=6.5296
	step [197/325], loss=6.2820
	step [198/325], loss=6.7069
	step [199/325], loss=4.7836
	step [200/325], loss=5.5700
	step [201/325], loss=5.3659
	step [202/325], loss=5.3415
	step [203/325], loss=6.1828
	step [204/325], loss=5.2044
	step [205/325], loss=4.6083
	step [206/325], loss=4.4665
	step [207/325], loss=5.5537
	step [208/325], loss=4.5946
	step [209/325], loss=3.9721
	step [210/325], loss=5.8313
	step [211/325], loss=6.4315
	step [212/325], loss=5.6996
	step [213/325], loss=6.7467
	step [214/325], loss=4.5175
	step [215/325], loss=5.2547
	step [216/325], loss=5.6695
	step [217/325], loss=3.8508
	step [218/325], loss=5.2563
	step [219/325], loss=5.2204
	step [220/325], loss=7.6804
	step [221/325], loss=4.7657
	step [222/325], loss=4.2181
	step [223/325], loss=6.5833
	step [224/325], loss=3.9792
	step [225/325], loss=4.0447
	step [226/325], loss=5.9072
	step [227/325], loss=5.9296
	step [228/325], loss=4.8543
	step [229/325], loss=4.0896
	step [230/325], loss=4.6662
	step [231/325], loss=5.0064
	step [232/325], loss=7.8170
	step [233/325], loss=6.4141
	step [234/325], loss=4.7097
	step [235/325], loss=4.8225
	step [236/325], loss=7.6199
	step [237/325], loss=6.2070
	step [238/325], loss=4.9727
	step [239/325], loss=5.4153
	step [240/325], loss=6.6394
	step [241/325], loss=6.1253
	step [242/325], loss=4.5774
	step [243/325], loss=6.5483
	step [244/325], loss=4.9176
	step [245/325], loss=6.8908
	step [246/325], loss=5.7918
	step [247/325], loss=6.3157
	step [248/325], loss=5.2667
	step [249/325], loss=7.6819
	step [250/325], loss=4.1676
	step [251/325], loss=6.6518
	step [252/325], loss=4.9068
	step [253/325], loss=4.5738
	step [254/325], loss=6.3239
	step [255/325], loss=5.3207
	step [256/325], loss=4.8572
	step [257/325], loss=4.5720
	step [258/325], loss=4.9242
	step [259/325], loss=6.1773
	step [260/325], loss=4.3869
	step [261/325], loss=7.9164
	step [262/325], loss=5.5869
	step [263/325], loss=6.0227
	step [264/325], loss=6.2031
	step [265/325], loss=5.5531
	step [266/325], loss=4.8323
	step [267/325], loss=6.0817
	step [268/325], loss=6.0330
	step [269/325], loss=5.4885
	step [270/325], loss=7.1942
	step [271/325], loss=5.6588
	step [272/325], loss=6.0201
	step [273/325], loss=4.9735
	step [274/325], loss=5.7819
	step [275/325], loss=5.5318
	step [276/325], loss=5.7124
	step [277/325], loss=5.3204
	step [278/325], loss=5.7834
	step [279/325], loss=6.1409
	step [280/325], loss=6.5283
	step [281/325], loss=5.3723
	step [282/325], loss=5.1046
	step [283/325], loss=4.9576
	step [284/325], loss=4.7499
	step [285/325], loss=5.4423
	step [286/325], loss=6.5242
	step [287/325], loss=4.3428
	step [288/325], loss=5.5980
	step [289/325], loss=5.5758
	step [290/325], loss=5.2111
	step [291/325], loss=4.8873
	step [292/325], loss=6.4880
	step [293/325], loss=4.8395
	step [294/325], loss=6.3442
	step [295/325], loss=5.5897
	step [296/325], loss=6.3025
	step [297/325], loss=5.3189
	step [298/325], loss=6.0275
	step [299/325], loss=5.1855
	step [300/325], loss=7.8618
	step [301/325], loss=4.7150
	step [302/325], loss=5.5154
	step [303/325], loss=5.4197
	step [304/325], loss=5.4050
	step [305/325], loss=4.9531
	step [306/325], loss=5.9146
	step [307/325], loss=6.4327
	step [308/325], loss=4.8880
	step [309/325], loss=4.8332
	step [310/325], loss=5.2863
	step [311/325], loss=4.1884
	step [312/325], loss=5.2735
	step [313/325], loss=5.9936
	step [314/325], loss=6.5599
	step [315/325], loss=7.7721
	step [316/325], loss=5.2974
	step [317/325], loss=5.5209
	step [318/325], loss=6.0635
	step [319/325], loss=5.4069
	step [320/325], loss=4.2939
	step [321/325], loss=4.6859
	step [322/325], loss=6.2235
	step [323/325], loss=6.6618
	step [324/325], loss=5.2587
	step [325/325], loss=2.2154
	Evaluating
	loss=0.0191, precision=0.2125, recall=0.9954, f1=0.3502
Training epoch 21
	step [1/325], loss=5.9789
	step [2/325], loss=4.1153
	step [3/325], loss=5.3078
	step [4/325], loss=5.3662
	step [5/325], loss=5.4056
	step [6/325], loss=5.9479
	step [7/325], loss=4.8228
	step [8/325], loss=5.8296
	step [9/325], loss=6.4280
	step [10/325], loss=7.0193
	step [11/325], loss=6.3252
	step [12/325], loss=5.5345
	step [13/325], loss=5.2652
	step [14/325], loss=5.5518
	step [15/325], loss=6.1090
	step [16/325], loss=5.7839
	step [17/325], loss=6.1872
	step [18/325], loss=6.1508
	step [19/325], loss=5.8896
	step [20/325], loss=4.4073
	step [21/325], loss=4.0640
	step [22/325], loss=6.6743
	step [23/325], loss=5.2201
	step [24/325], loss=5.9629
	step [25/325], loss=6.2840
	step [26/325], loss=6.6499
	step [27/325], loss=5.8940
	step [28/325], loss=4.9402
	step [29/325], loss=5.9272
	step [30/325], loss=5.8558
	step [31/325], loss=4.6966
	step [32/325], loss=5.0823
	step [33/325], loss=5.3416
	step [34/325], loss=4.5674
	step [35/325], loss=6.2674
	step [36/325], loss=5.6787
	step [37/325], loss=4.4270
	step [38/325], loss=4.8330
	step [39/325], loss=4.0104
	step [40/325], loss=5.6635
	step [41/325], loss=6.4785
	step [42/325], loss=4.1307
	step [43/325], loss=5.1005
	step [44/325], loss=6.1353
	step [45/325], loss=5.4824
	step [46/325], loss=5.1342
	step [47/325], loss=4.7458
	step [48/325], loss=4.0352
	step [49/325], loss=5.9507
	step [50/325], loss=6.9196
	step [51/325], loss=4.9012
	step [52/325], loss=5.4098
	step [53/325], loss=5.6291
	step [54/325], loss=5.8954
	step [55/325], loss=6.3004
	step [56/325], loss=4.1774
	step [57/325], loss=5.0126
	step [58/325], loss=4.6002
	step [59/325], loss=5.1837
	step [60/325], loss=6.6823
	step [61/325], loss=6.2676
	step [62/325], loss=5.4460
	step [63/325], loss=5.3463
	step [64/325], loss=5.5203
	step [65/325], loss=4.4077
	step [66/325], loss=6.9484
	step [67/325], loss=6.0186
	step [68/325], loss=5.2432
	step [69/325], loss=4.3177
	step [70/325], loss=5.5638
	step [71/325], loss=4.8299
	step [72/325], loss=6.0818
	step [73/325], loss=6.0075
	step [74/325], loss=5.0973
	step [75/325], loss=6.3217
	step [76/325], loss=6.1164
	step [77/325], loss=5.8030
	step [78/325], loss=5.4514
	step [79/325], loss=5.7096
	step [80/325], loss=5.5718
	step [81/325], loss=5.8221
	step [82/325], loss=7.6129
	step [83/325], loss=4.5643
	step [84/325], loss=5.4942
	step [85/325], loss=5.1506
	step [86/325], loss=5.3722
	step [87/325], loss=5.6303
	step [88/325], loss=5.5184
	step [89/325], loss=5.5287
	step [90/325], loss=4.5474
	step [91/325], loss=4.7430
	step [92/325], loss=5.4582
	step [93/325], loss=5.3231
	step [94/325], loss=7.2687
	step [95/325], loss=4.2617
	step [96/325], loss=5.5484
	step [97/325], loss=5.1755
	step [98/325], loss=5.4849
	step [99/325], loss=5.2223
	step [100/325], loss=4.7501
	step [101/325], loss=5.8328
	step [102/325], loss=6.1690
	step [103/325], loss=5.5333
	step [104/325], loss=5.5300
	step [105/325], loss=6.4975
	step [106/325], loss=5.4635
	step [107/325], loss=4.8124
	step [108/325], loss=5.9827
	step [109/325], loss=5.8682
	step [110/325], loss=4.9390
	step [111/325], loss=6.3957
	step [112/325], loss=5.5614
	step [113/325], loss=5.5841
	step [114/325], loss=6.1377
	step [115/325], loss=6.0213
	step [116/325], loss=6.7332
	step [117/325], loss=6.6469
	step [118/325], loss=5.5598
	step [119/325], loss=5.4817
	step [120/325], loss=5.5636
	step [121/325], loss=6.5971
	step [122/325], loss=5.1039
	step [123/325], loss=6.1006
	step [124/325], loss=6.4133
	step [125/325], loss=6.7577
	step [126/325], loss=6.2237
	step [127/325], loss=5.9673
	step [128/325], loss=5.7249
	step [129/325], loss=6.6475
	step [130/325], loss=6.1914
	step [131/325], loss=4.5543
	step [132/325], loss=5.7840
	step [133/325], loss=6.4681
	step [134/325], loss=5.6760
	step [135/325], loss=4.7196
	step [136/325], loss=4.8638
	step [137/325], loss=3.9445
	step [138/325], loss=4.6347
	step [139/325], loss=5.8838
	step [140/325], loss=6.0239
	step [141/325], loss=9.6623
	step [142/325], loss=4.8038
	step [143/325], loss=5.3894
	step [144/325], loss=5.1708
	step [145/325], loss=7.5745
	step [146/325], loss=5.1236
	step [147/325], loss=5.9860
	step [148/325], loss=4.8647
	step [149/325], loss=6.2582
	step [150/325], loss=7.4793
	step [151/325], loss=5.5991
	step [152/325], loss=5.7978
	step [153/325], loss=6.8011
	step [154/325], loss=4.4682
	step [155/325], loss=5.5649
	step [156/325], loss=4.5098
	step [157/325], loss=6.4925
	step [158/325], loss=5.4001
	step [159/325], loss=6.8804
	step [160/325], loss=6.1930
	step [161/325], loss=4.6998
	step [162/325], loss=5.7138
	step [163/325], loss=4.9024
	step [164/325], loss=6.2927
	step [165/325], loss=5.5041
	step [166/325], loss=4.8565
	step [167/325], loss=4.6258
	step [168/325], loss=4.9693
	step [169/325], loss=6.1707
	step [170/325], loss=5.6691
	step [171/325], loss=6.0784
	step [172/325], loss=5.5288
	step [173/325], loss=4.9773
	step [174/325], loss=5.8673
	step [175/325], loss=6.9983
	step [176/325], loss=5.6141
	step [177/325], loss=5.8954
	step [178/325], loss=5.1050
	step [179/325], loss=5.6024
	step [180/325], loss=5.2564
	step [181/325], loss=7.6077
	step [182/325], loss=4.9636
	step [183/325], loss=5.1162
	step [184/325], loss=5.7881
	step [185/325], loss=5.2439
	step [186/325], loss=5.4539
	step [187/325], loss=6.6849
	step [188/325], loss=4.4542
	step [189/325], loss=5.9893
	step [190/325], loss=5.2762
	step [191/325], loss=5.0059
	step [192/325], loss=4.5759
	step [193/325], loss=5.5475
	step [194/325], loss=5.7338
	step [195/325], loss=4.4206
	step [196/325], loss=8.6789
	step [197/325], loss=5.7503
	step [198/325], loss=5.5332
	step [199/325], loss=4.9864
	step [200/325], loss=6.0115
	step [201/325], loss=5.7197
	step [202/325], loss=4.9600
	step [203/325], loss=5.5998
	step [204/325], loss=6.4377
	step [205/325], loss=5.9091
	step [206/325], loss=5.5830
	step [207/325], loss=5.5482
	step [208/325], loss=5.5588
	step [209/325], loss=5.9318
	step [210/325], loss=5.8808
	step [211/325], loss=5.9899
	step [212/325], loss=6.8946
	step [213/325], loss=4.0011
	step [214/325], loss=4.1889
	step [215/325], loss=5.1654
	step [216/325], loss=5.4770
	step [217/325], loss=6.1643
	step [218/325], loss=4.1144
	step [219/325], loss=6.1721
	step [220/325], loss=5.2276
	step [221/325], loss=5.7790
	step [222/325], loss=4.6816
	step [223/325], loss=6.0107
	step [224/325], loss=5.4508
	step [225/325], loss=6.2913
	step [226/325], loss=6.0739
	step [227/325], loss=5.2516
	step [228/325], loss=5.9368
	step [229/325], loss=5.9246
	step [230/325], loss=5.6582
	step [231/325], loss=5.9288
	step [232/325], loss=5.6297
	step [233/325], loss=5.9214
	step [234/325], loss=5.8817
	step [235/325], loss=5.1364
	step [236/325], loss=5.0550
	step [237/325], loss=5.8618
	step [238/325], loss=4.8142
	step [239/325], loss=4.9920
	step [240/325], loss=5.4347
	step [241/325], loss=5.2432
	step [242/325], loss=7.5440
	step [243/325], loss=5.0225
	step [244/325], loss=6.9765
	step [245/325], loss=6.5264
	step [246/325], loss=4.5653
	step [247/325], loss=5.4861
	step [248/325], loss=6.2268
	step [249/325], loss=5.9290
	step [250/325], loss=4.4712
	step [251/325], loss=5.4743
	step [252/325], loss=4.8196
	step [253/325], loss=3.4254
	step [254/325], loss=5.0638
	step [255/325], loss=4.5731
	step [256/325], loss=5.4396
	step [257/325], loss=4.2750
	step [258/325], loss=5.7921
	step [259/325], loss=7.2546
	step [260/325], loss=4.3117
	step [261/325], loss=4.5759
	step [262/325], loss=5.3892
	step [263/325], loss=6.0860
	step [264/325], loss=5.1448
	step [265/325], loss=5.8303
	step [266/325], loss=5.8499
	step [267/325], loss=5.5027
	step [268/325], loss=4.8076
	step [269/325], loss=6.1333
	step [270/325], loss=4.9135
	step [271/325], loss=4.0555
	step [272/325], loss=7.0799
	step [273/325], loss=6.6519
	step [274/325], loss=4.8076
	step [275/325], loss=3.6017
	step [276/325], loss=6.2496
	step [277/325], loss=4.2478
	step [278/325], loss=6.2218
	step [279/325], loss=5.1549
	step [280/325], loss=4.3543
	step [281/325], loss=5.4474
	step [282/325], loss=6.5470
	step [283/325], loss=6.0239
	step [284/325], loss=5.1159
	step [285/325], loss=6.3514
	step [286/325], loss=5.6286
	step [287/325], loss=4.9782
	step [288/325], loss=5.0424
	step [289/325], loss=4.8467
	step [290/325], loss=5.7707
	step [291/325], loss=4.4823
	step [292/325], loss=8.5224
	step [293/325], loss=5.2972
	step [294/325], loss=6.4259
	step [295/325], loss=6.2243
	step [296/325], loss=5.4976
	step [297/325], loss=5.8368
	step [298/325], loss=5.4204
	step [299/325], loss=5.7452
	step [300/325], loss=3.9678
	step [301/325], loss=5.5579
	step [302/325], loss=4.9575
	step [303/325], loss=4.6532
	step [304/325], loss=7.0924
	step [305/325], loss=3.8853
	step [306/325], loss=5.1932
	step [307/325], loss=6.7241
	step [308/325], loss=5.3977
	step [309/325], loss=5.2172
	step [310/325], loss=7.3578
	step [311/325], loss=6.2429
	step [312/325], loss=4.9812
	step [313/325], loss=5.5332
	step [314/325], loss=5.9139
	step [315/325], loss=5.6684
	step [316/325], loss=5.8095
	step [317/325], loss=5.3528
	step [318/325], loss=4.3260
	step [319/325], loss=5.7373
	step [320/325], loss=4.8828
	step [321/325], loss=6.2315
	step [322/325], loss=4.7968
	step [323/325], loss=5.5223
	step [324/325], loss=4.8799
	step [325/325], loss=3.3376
	Evaluating
	loss=0.0175, precision=0.2272, recall=0.9943, f1=0.3699
saving model as: 1_saved_model.pth
Training epoch 22
	step [1/325], loss=4.3999
	step [2/325], loss=7.5882
	step [3/325], loss=4.9621
	step [4/325], loss=6.1197
	step [5/325], loss=4.5488
	step [6/325], loss=4.8968
	step [7/325], loss=5.4015
	step [8/325], loss=4.8272
	step [9/325], loss=3.4965
	step [10/325], loss=5.0676
	step [11/325], loss=5.2906
	step [12/325], loss=5.5388
	step [13/325], loss=5.1610
	step [14/325], loss=6.1883
	step [15/325], loss=5.3487
	step [16/325], loss=7.1003
	step [17/325], loss=4.3916
	step [18/325], loss=7.2096
	step [19/325], loss=5.7151
	step [20/325], loss=5.4641
	step [21/325], loss=4.1770
	step [22/325], loss=5.2434
	step [23/325], loss=5.7914
	step [24/325], loss=4.9495
	step [25/325], loss=5.8183
	step [26/325], loss=6.1206
	step [27/325], loss=4.3733
	step [28/325], loss=5.9898
	step [29/325], loss=4.2549
	step [30/325], loss=6.1497
	step [31/325], loss=5.3005
	step [32/325], loss=5.0622
	step [33/325], loss=5.3684
	step [34/325], loss=5.0396
	step [35/325], loss=5.1447
	step [36/325], loss=5.9928
	step [37/325], loss=5.2433
	step [38/325], loss=5.0437
	step [39/325], loss=5.8911
	step [40/325], loss=5.8099
	step [41/325], loss=5.9355
	step [42/325], loss=6.5732
	step [43/325], loss=7.3348
	step [44/325], loss=4.2778
	step [45/325], loss=5.4683
	step [46/325], loss=5.6060
	step [47/325], loss=5.3326
	step [48/325], loss=5.3965
	step [49/325], loss=4.9086
	step [50/325], loss=6.2315
	step [51/325], loss=5.0021
	step [52/325], loss=5.2871
	step [53/325], loss=5.1317
	step [54/325], loss=6.3529
	step [55/325], loss=5.7446
	step [56/325], loss=4.5788
	step [57/325], loss=6.3747
	step [58/325], loss=4.6854
	step [59/325], loss=5.6837
	step [60/325], loss=5.6353
	step [61/325], loss=6.0749
	step [62/325], loss=4.9605
	step [63/325], loss=5.2887
	step [64/325], loss=5.1857
	step [65/325], loss=5.4393
	step [66/325], loss=5.0902
	step [67/325], loss=5.4483
	step [68/325], loss=5.6899
	step [69/325], loss=5.7540
	step [70/325], loss=5.2109
	step [71/325], loss=5.5757
	step [72/325], loss=6.1740
	step [73/325], loss=4.9585
	step [74/325], loss=5.8204
	step [75/325], loss=4.7089
	step [76/325], loss=4.4334
	step [77/325], loss=5.2318
	step [78/325], loss=4.7944
	step [79/325], loss=3.7853
	step [80/325], loss=5.6917
	step [81/325], loss=5.7582
	step [82/325], loss=5.4144
	step [83/325], loss=5.2812
	step [84/325], loss=8.7060
	step [85/325], loss=5.1350
	step [86/325], loss=4.0578
	step [87/325], loss=5.3135
	step [88/325], loss=5.8096
	step [89/325], loss=5.6987
	step [90/325], loss=5.1057
	step [91/325], loss=5.4569
	step [92/325], loss=4.2872
	step [93/325], loss=4.7204
	step [94/325], loss=4.6445
	step [95/325], loss=6.0179
	step [96/325], loss=5.3922
	step [97/325], loss=4.5994
	step [98/325], loss=6.8697
	step [99/325], loss=7.7485
	step [100/325], loss=5.6489
	step [101/325], loss=5.9834
	step [102/325], loss=4.8634
	step [103/325], loss=5.5740
	step [104/325], loss=5.4813
	step [105/325], loss=6.0478
	step [106/325], loss=5.0145
	step [107/325], loss=5.1383
	step [108/325], loss=4.2429
	step [109/325], loss=5.2050
	step [110/325], loss=5.3804
	step [111/325], loss=5.0624
	step [112/325], loss=5.9130
	step [113/325], loss=5.3954
	step [114/325], loss=7.2313
	step [115/325], loss=4.0195
	step [116/325], loss=6.3373
	step [117/325], loss=4.9055
	step [118/325], loss=5.6261
	step [119/325], loss=5.6083
	step [120/325], loss=4.9765
	step [121/325], loss=5.3470
	step [122/325], loss=5.2619
	step [123/325], loss=5.7156
	step [124/325], loss=5.5220
	step [125/325], loss=5.7498
	step [126/325], loss=6.1689
	step [127/325], loss=6.3667
	step [128/325], loss=4.9210
	step [129/325], loss=5.1211
	step [130/325], loss=4.5513
	step [131/325], loss=4.3816
	step [132/325], loss=4.8145
	step [133/325], loss=6.5105
	step [134/325], loss=5.1449
	step [135/325], loss=5.2536
	step [136/325], loss=6.4972
	step [137/325], loss=5.6221
	step [138/325], loss=3.8638
	step [139/325], loss=6.4371
	step [140/325], loss=5.9825
	step [141/325], loss=5.4135
	step [142/325], loss=5.4565
	step [143/325], loss=6.1921
	step [144/325], loss=5.3640
	step [145/325], loss=6.0506
	step [146/325], loss=6.0545
	step [147/325], loss=4.4948
	step [148/325], loss=4.9257
	step [149/325], loss=6.5136
	step [150/325], loss=6.5651
	step [151/325], loss=5.7488
	step [152/325], loss=4.6661
	step [153/325], loss=4.6297
	step [154/325], loss=4.4624
	step [155/325], loss=5.8216
	step [156/325], loss=4.4217
	step [157/325], loss=5.0171
	step [158/325], loss=5.0515
	step [159/325], loss=5.7977
	step [160/325], loss=5.3066
	step [161/325], loss=5.3101
	step [162/325], loss=6.7483
	step [163/325], loss=5.0512
	step [164/325], loss=4.7839
	step [165/325], loss=5.6676
	step [166/325], loss=5.1041
	step [167/325], loss=5.2320
	step [168/325], loss=4.2996
	step [169/325], loss=6.6717
	step [170/325], loss=5.2940
	step [171/325], loss=4.5650
	step [172/325], loss=4.3048
	step [173/325], loss=4.8107
	step [174/325], loss=5.6220
	step [175/325], loss=5.8707
	step [176/325], loss=6.1844
	step [177/325], loss=6.0519
	step [178/325], loss=6.3503
	step [179/325], loss=5.8453
	step [180/325], loss=5.7747
	step [181/325], loss=6.4366
	step [182/325], loss=5.8684
	step [183/325], loss=4.7254
	step [184/325], loss=6.1432
	step [185/325], loss=5.4775
	step [186/325], loss=5.6276
	step [187/325], loss=7.2408
	step [188/325], loss=9.3919
	step [189/325], loss=5.5183
	step [190/325], loss=5.7089
	step [191/325], loss=5.9387
	step [192/325], loss=5.2363
	step [193/325], loss=5.2310
	step [194/325], loss=4.5580
	step [195/325], loss=5.4750
	step [196/325], loss=4.5284
	step [197/325], loss=5.5348
	step [198/325], loss=5.0144
	step [199/325], loss=4.4823
	step [200/325], loss=5.4268
	step [201/325], loss=7.1014
	step [202/325], loss=4.8891
	step [203/325], loss=5.6416
	step [204/325], loss=4.6050
	step [205/325], loss=4.8545
	step [206/325], loss=5.6613
	step [207/325], loss=4.7308
	step [208/325], loss=5.8619
	step [209/325], loss=6.7306
	step [210/325], loss=5.4488
	step [211/325], loss=7.0511
	step [212/325], loss=6.6477
	step [213/325], loss=4.3553
	step [214/325], loss=5.0354
	step [215/325], loss=7.4430
	step [216/325], loss=4.5830
	step [217/325], loss=5.0934
	step [218/325], loss=4.4873
	step [219/325], loss=9.0090
	step [220/325], loss=6.9357
	step [221/325], loss=5.4433
	step [222/325], loss=4.0847
	step [223/325], loss=6.2835
	step [224/325], loss=5.9717
	step [225/325], loss=5.5823
	step [226/325], loss=6.9511
	step [227/325], loss=5.9332
	step [228/325], loss=6.0026
	step [229/325], loss=5.2932
	step [230/325], loss=5.7578
	step [231/325], loss=4.3007
	step [232/325], loss=6.2618
	step [233/325], loss=6.0379
	step [234/325], loss=6.9395
	step [235/325], loss=5.0267
	step [236/325], loss=6.6272
	step [237/325], loss=4.9534
	step [238/325], loss=5.7192
	step [239/325], loss=5.4767
	step [240/325], loss=5.2270
	step [241/325], loss=5.7074
	step [242/325], loss=5.8646
	step [243/325], loss=4.7962
	step [244/325], loss=4.7559
	step [245/325], loss=5.8402
	step [246/325], loss=4.8270
	step [247/325], loss=6.1819
	step [248/325], loss=6.3879
	step [249/325], loss=4.6411
	step [250/325], loss=4.3422
	step [251/325], loss=6.2704
	step [252/325], loss=5.6179
	step [253/325], loss=4.0917
	step [254/325], loss=7.3417
	step [255/325], loss=5.4303
	step [256/325], loss=5.6172
	step [257/325], loss=5.1559
	step [258/325], loss=5.6443
	step [259/325], loss=5.7194
	step [260/325], loss=6.6056
	step [261/325], loss=4.4461
	step [262/325], loss=5.1846
	step [263/325], loss=3.7356
	step [264/325], loss=5.2277
	step [265/325], loss=5.1737
	step [266/325], loss=6.3128
	step [267/325], loss=4.8574
	step [268/325], loss=5.8710
	step [269/325], loss=5.5613
	step [270/325], loss=4.8677
	step [271/325], loss=5.4346
	step [272/325], loss=5.0777
	step [273/325], loss=5.8200
	step [274/325], loss=5.1732
	step [275/325], loss=6.2722
	step [276/325], loss=5.0230
	step [277/325], loss=5.5284
	step [278/325], loss=5.1857
	step [279/325], loss=7.5426
	step [280/325], loss=4.8864
	step [281/325], loss=5.4902
	step [282/325], loss=5.8236
	step [283/325], loss=6.4040
	step [284/325], loss=5.5244
	step [285/325], loss=4.5979
	step [286/325], loss=5.3712
	step [287/325], loss=6.1921
	step [288/325], loss=4.3446
	step [289/325], loss=5.1493
	step [290/325], loss=6.4628
	step [291/325], loss=6.9551
	step [292/325], loss=5.4399
	step [293/325], loss=4.7163
	step [294/325], loss=6.5595
	step [295/325], loss=4.6090
	step [296/325], loss=4.8352
	step [297/325], loss=4.8733
	step [298/325], loss=4.9953
	step [299/325], loss=5.0611
	step [300/325], loss=5.1389
	step [301/325], loss=5.0707
	step [302/325], loss=4.7547
	step [303/325], loss=7.4136
	step [304/325], loss=6.0245
	step [305/325], loss=5.4498
	step [306/325], loss=6.0704
	step [307/325], loss=5.1801
	step [308/325], loss=5.4340
	step [309/325], loss=5.7755
	step [310/325], loss=4.8727
	step [311/325], loss=6.1524
	step [312/325], loss=5.4454
	step [313/325], loss=6.0780
	step [314/325], loss=5.2973
	step [315/325], loss=6.2598
	step [316/325], loss=5.0986
	step [317/325], loss=4.3517
	step [318/325], loss=5.7136
	step [319/325], loss=4.5548
	step [320/325], loss=4.8707
	step [321/325], loss=6.2253
	step [322/325], loss=6.2850
	step [323/325], loss=4.9445
	step [324/325], loss=6.8736
	step [325/325], loss=3.9667
	Evaluating
	loss=0.0188, precision=0.2129, recall=0.9951, f1=0.3508
Training epoch 23
	step [1/325], loss=6.6742
	step [2/325], loss=4.6728
	step [3/325], loss=6.1317
	step [4/325], loss=4.4002
	step [5/325], loss=5.2432
	step [6/325], loss=5.3288
	step [7/325], loss=4.9714
	step [8/325], loss=6.2309
	step [9/325], loss=4.6923
	step [10/325], loss=5.9889
	step [11/325], loss=5.3751
	step [12/325], loss=6.2418
	step [13/325], loss=4.7576
	step [14/325], loss=4.9983
	step [15/325], loss=5.5445
	step [16/325], loss=4.9994
	step [17/325], loss=4.6403
	step [18/325], loss=5.6900
	step [19/325], loss=5.2822
	step [20/325], loss=4.0190
	step [21/325], loss=4.5932
	step [22/325], loss=5.0084
	step [23/325], loss=5.8116
	step [24/325], loss=4.5963
	step [25/325], loss=5.0922
	step [26/325], loss=5.2288
	step [27/325], loss=4.0756
	step [28/325], loss=5.6399
	step [29/325], loss=5.2664
	step [30/325], loss=6.0366
	step [31/325], loss=4.9879
	step [32/325], loss=5.5205
	step [33/325], loss=6.5547
	step [34/325], loss=7.1734
	step [35/325], loss=7.0431
	step [36/325], loss=5.3235
	step [37/325], loss=4.9051
	step [38/325], loss=5.0820
	step [39/325], loss=5.0791
	step [40/325], loss=5.4713
	step [41/325], loss=3.7408
	step [42/325], loss=3.9686
	step [43/325], loss=7.0340
	step [44/325], loss=4.9239
	step [45/325], loss=5.8149
	step [46/325], loss=5.2768
	step [47/325], loss=4.4316
	step [48/325], loss=4.4612
	step [49/325], loss=5.2559
	step [50/325], loss=6.8417
	step [51/325], loss=4.0930
	step [52/325], loss=6.0398
	step [53/325], loss=4.5805
	step [54/325], loss=5.5884
	step [55/325], loss=5.9375
	step [56/325], loss=6.1245
	step [57/325], loss=5.6638
	step [58/325], loss=6.6214
	step [59/325], loss=5.0627
	step [60/325], loss=5.1740
	step [61/325], loss=5.1939
	step [62/325], loss=6.4266
	step [63/325], loss=5.0566
	step [64/325], loss=5.1970
	step [65/325], loss=4.8328
	step [66/325], loss=6.0506
	step [67/325], loss=5.8167
	step [68/325], loss=4.7291
	step [69/325], loss=5.0382
	step [70/325], loss=5.7531
	step [71/325], loss=7.7529
	step [72/325], loss=4.7268
	step [73/325], loss=6.2760
	step [74/325], loss=5.5865
	step [75/325], loss=4.8248
	step [76/325], loss=5.1944
	step [77/325], loss=4.5786
	step [78/325], loss=4.6550
	step [79/325], loss=5.6702
	step [80/325], loss=4.6732
	step [81/325], loss=6.8716
	step [82/325], loss=6.1737
	step [83/325], loss=6.0980
	step [84/325], loss=5.7415
	step [85/325], loss=5.5566
	step [86/325], loss=4.7975
	step [87/325], loss=6.2514
	step [88/325], loss=5.6488
	step [89/325], loss=5.4528
	step [90/325], loss=4.6658
	step [91/325], loss=6.7712
	step [92/325], loss=5.7480
	step [93/325], loss=5.6193
	step [94/325], loss=6.1345
	step [95/325], loss=5.3010
	step [96/325], loss=4.7754
	step [97/325], loss=5.9789
	step [98/325], loss=5.3768
	step [99/325], loss=5.0529
	step [100/325], loss=4.8710
	step [101/325], loss=5.7321
	step [102/325], loss=7.6547
	step [103/325], loss=7.2524
	step [104/325], loss=5.4278
	step [105/325], loss=5.2965
	step [106/325], loss=5.3791
	step [107/325], loss=6.1216
	step [108/325], loss=5.8602
	step [109/325], loss=5.7407
	step [110/325], loss=4.9557
	step [111/325], loss=5.5490
	step [112/325], loss=5.0568
	step [113/325], loss=5.2513
	step [114/325], loss=6.2006
	step [115/325], loss=5.4835
	step [116/325], loss=7.6259
	step [117/325], loss=4.7457
	step [118/325], loss=5.1441
	step [119/325], loss=4.7286
	step [120/325], loss=5.3193
	step [121/325], loss=5.1685
	step [122/325], loss=5.3437
	step [123/325], loss=5.1652
	step [124/325], loss=5.1595
	step [125/325], loss=5.3175
	step [126/325], loss=5.8197
	step [127/325], loss=4.2990
	step [128/325], loss=6.4966
	step [129/325], loss=6.3273
	step [130/325], loss=5.1386
	step [131/325], loss=4.6832
	step [132/325], loss=4.6640
	step [133/325], loss=5.9770
	step [134/325], loss=5.9023
	step [135/325], loss=5.3586
	step [136/325], loss=6.2856
	step [137/325], loss=5.6103
	step [138/325], loss=5.7546
	step [139/325], loss=4.4871
	step [140/325], loss=5.2834
	step [141/325], loss=5.1830
	step [142/325], loss=5.1816
	step [143/325], loss=6.2545
	step [144/325], loss=6.9777
	step [145/325], loss=4.3601
	step [146/325], loss=6.2054
	step [147/325], loss=3.7864
	step [148/325], loss=5.9900
	step [149/325], loss=4.7269
	step [150/325], loss=5.1247
	step [151/325], loss=4.8823
	step [152/325], loss=4.5597
	step [153/325], loss=6.2226
	step [154/325], loss=5.9560
	step [155/325], loss=6.5825
	step [156/325], loss=4.3485
	step [157/325], loss=5.1863
	step [158/325], loss=6.3750
	step [159/325], loss=4.2196
	step [160/325], loss=4.7420
	step [161/325], loss=4.6788
	step [162/325], loss=4.2787
	step [163/325], loss=4.6084
	step [164/325], loss=5.0629
	step [165/325], loss=4.5372
	step [166/325], loss=5.4509
	step [167/325], loss=6.1993
	step [168/325], loss=6.7791
	step [169/325], loss=5.9738
	step [170/325], loss=5.9875
	step [171/325], loss=4.5769
	step [172/325], loss=4.6063
	step [173/325], loss=4.7672
	step [174/325], loss=5.3647
	step [175/325], loss=5.2719
	step [176/325], loss=4.3672
	step [177/325], loss=4.2768
	step [178/325], loss=4.7588
	step [179/325], loss=5.4204
	step [180/325], loss=7.2634
	step [181/325], loss=6.6063
	step [182/325], loss=5.4918
	step [183/325], loss=3.6746
	step [184/325], loss=4.4623
	step [185/325], loss=4.9588
	step [186/325], loss=4.8658
	step [187/325], loss=5.5485
	step [188/325], loss=4.4954
	step [189/325], loss=4.6318
	step [190/325], loss=6.8353
	step [191/325], loss=4.8215
	step [192/325], loss=4.4839
	step [193/325], loss=4.3518
	step [194/325], loss=4.2733
	step [195/325], loss=4.6392
	step [196/325], loss=4.9946
	step [197/325], loss=4.5622
	step [198/325], loss=4.5609
	step [199/325], loss=4.2827
	step [200/325], loss=5.6291
	step [201/325], loss=4.5997
	step [202/325], loss=5.2829
	step [203/325], loss=6.8547
	step [204/325], loss=6.0359
	step [205/325], loss=4.6851
	step [206/325], loss=5.7548
	step [207/325], loss=4.5826
	step [208/325], loss=4.9669
	step [209/325], loss=6.0084
	step [210/325], loss=4.3982
	step [211/325], loss=5.2953
	step [212/325], loss=5.2443
	step [213/325], loss=6.1575
	step [214/325], loss=5.8617
	step [215/325], loss=5.2544
	step [216/325], loss=4.9922
	step [217/325], loss=5.3433
	step [218/325], loss=5.8836
	step [219/325], loss=5.9228
	step [220/325], loss=5.6683
	step [221/325], loss=4.5178
	step [222/325], loss=7.1428
	step [223/325], loss=4.7100
	step [224/325], loss=4.6675
	step [225/325], loss=4.7210
	step [226/325], loss=4.2445
	step [227/325], loss=4.9972
	step [228/325], loss=5.6171
	step [229/325], loss=4.1058
	step [230/325], loss=7.2891
	step [231/325], loss=4.3349
	step [232/325], loss=5.3951
	step [233/325], loss=6.4648
	step [234/325], loss=5.9878
	step [235/325], loss=5.3503
	step [236/325], loss=6.4341
	step [237/325], loss=4.6687
	step [238/325], loss=4.8430
	step [239/325], loss=6.1347
	step [240/325], loss=5.0771
	step [241/325], loss=4.9506
	step [242/325], loss=5.3123
	step [243/325], loss=7.3634
	step [244/325], loss=5.4088
	step [245/325], loss=6.9534
	step [246/325], loss=5.8718
	step [247/325], loss=6.1590
	step [248/325], loss=4.8264
	step [249/325], loss=5.6501
	step [250/325], loss=5.6761
	step [251/325], loss=4.2435
	step [252/325], loss=4.8689
	step [253/325], loss=7.0072
	step [254/325], loss=4.2299
	step [255/325], loss=6.2442
	step [256/325], loss=5.3132
	step [257/325], loss=5.0159
	step [258/325], loss=6.8182
	step [259/325], loss=5.5391
	step [260/325], loss=5.7098
	step [261/325], loss=5.9312
	step [262/325], loss=5.2729
	step [263/325], loss=8.2444
	step [264/325], loss=5.2488
	step [265/325], loss=6.3034
	step [266/325], loss=5.3409
	step [267/325], loss=4.8038
	step [268/325], loss=4.7417
	step [269/325], loss=5.3454
	step [270/325], loss=6.6182
	step [271/325], loss=6.0886
	step [272/325], loss=4.6569
	step [273/325], loss=6.5456
	step [274/325], loss=4.9993
	step [275/325], loss=6.0842
	step [276/325], loss=4.9591
	step [277/325], loss=6.8056
	step [278/325], loss=4.7216
	step [279/325], loss=5.0068
	step [280/325], loss=4.9732
	step [281/325], loss=5.0782
	step [282/325], loss=6.0375
	step [283/325], loss=5.2362
	step [284/325], loss=5.3135
	step [285/325], loss=6.3981
	step [286/325], loss=6.2217
	step [287/325], loss=5.8104
	step [288/325], loss=4.9246
	step [289/325], loss=5.7527
	step [290/325], loss=5.4910
	step [291/325], loss=5.0716
	step [292/325], loss=5.2031
	step [293/325], loss=5.7004
	step [294/325], loss=4.4698
	step [295/325], loss=6.5730
	step [296/325], loss=5.1237
	step [297/325], loss=5.5418
	step [298/325], loss=5.6153
	step [299/325], loss=5.0048
	step [300/325], loss=7.1339
	step [301/325], loss=4.7162
	step [302/325], loss=4.9540
	step [303/325], loss=5.3750
	step [304/325], loss=4.7798
	step [305/325], loss=5.2591
	step [306/325], loss=3.8220
	step [307/325], loss=4.0960
	step [308/325], loss=4.7618
	step [309/325], loss=5.1526
	step [310/325], loss=5.1911
	step [311/325], loss=3.9415
	step [312/325], loss=5.3091
	step [313/325], loss=6.3681
	step [314/325], loss=4.6368
	step [315/325], loss=4.8630
	step [316/325], loss=5.0167
	step [317/325], loss=5.8207
	step [318/325], loss=4.6521
	step [319/325], loss=5.6283
	step [320/325], loss=4.1160
	step [321/325], loss=5.0683
	step [322/325], loss=5.3049
	step [323/325], loss=6.2409
	step [324/325], loss=5.1206
	step [325/325], loss=4.8947
	Evaluating
	loss=0.0174, precision=0.2140, recall=0.9942, f1=0.3523
Training epoch 24
	step [1/325], loss=4.2800
	step [2/325], loss=5.2397
	step [3/325], loss=6.2054
	step [4/325], loss=5.2246
	step [5/325], loss=4.8111
	step [6/325], loss=5.7809
	step [7/325], loss=4.7120
	step [8/325], loss=4.9497
	step [9/325], loss=4.6988
	step [10/325], loss=5.2526
	step [11/325], loss=4.7597
	step [12/325], loss=4.4701
	step [13/325], loss=4.9918
	step [14/325], loss=6.5663
	step [15/325], loss=5.9104
	step [16/325], loss=4.7155
	step [17/325], loss=6.4876
	step [18/325], loss=5.8809
	step [19/325], loss=5.1782
	step [20/325], loss=5.2449
	step [21/325], loss=4.1995
	step [22/325], loss=4.3349
	step [23/325], loss=5.9108
	step [24/325], loss=7.5162
	step [25/325], loss=4.9839
	step [26/325], loss=4.8892
	step [27/325], loss=5.1789
	step [28/325], loss=4.5603
	step [29/325], loss=5.2141
	step [30/325], loss=6.2228
	step [31/325], loss=6.5558
	step [32/325], loss=4.9990
	step [33/325], loss=5.0923
	step [34/325], loss=6.2563
	step [35/325], loss=6.5167
	step [36/325], loss=6.1913
	step [37/325], loss=6.0380
	step [38/325], loss=5.6597
	step [39/325], loss=5.7676
	step [40/325], loss=4.8928
	step [41/325], loss=4.4198
	step [42/325], loss=5.3822
	step [43/325], loss=5.7032
	step [44/325], loss=7.0154
	step [45/325], loss=5.8645
	step [46/325], loss=4.8858
	step [47/325], loss=4.6330
	step [48/325], loss=4.7763
	step [49/325], loss=4.5560
	step [50/325], loss=5.6654
	step [51/325], loss=5.7623
	step [52/325], loss=5.0663
	step [53/325], loss=5.8978
	step [54/325], loss=4.5698
	step [55/325], loss=3.6587
	step [56/325], loss=5.3862
	step [57/325], loss=4.0120
	step [58/325], loss=5.9303
	step [59/325], loss=5.7000
	step [60/325], loss=5.7675
	step [61/325], loss=6.2151
	step [62/325], loss=4.8033
	step [63/325], loss=5.7895
	step [64/325], loss=4.9575
	step [65/325], loss=5.0218
	step [66/325], loss=4.9976
	step [67/325], loss=4.1591
	step [68/325], loss=5.5735
	step [69/325], loss=5.5412
	step [70/325], loss=5.6616
	step [71/325], loss=5.8991
	step [72/325], loss=4.8497
	step [73/325], loss=5.9101
	step [74/325], loss=6.2245
	step [75/325], loss=6.0700
	step [76/325], loss=5.0696
	step [77/325], loss=4.3670
	step [78/325], loss=5.4625
	step [79/325], loss=5.0072
	step [80/325], loss=5.2875
	step [81/325], loss=5.5224
	step [82/325], loss=5.6216
	step [83/325], loss=7.4397
	step [84/325], loss=6.0558
	step [85/325], loss=5.2434
	step [86/325], loss=5.4045
	step [87/325], loss=5.7119
	step [88/325], loss=5.3109
	step [89/325], loss=6.3040
	step [90/325], loss=4.8920
	step [91/325], loss=6.0380
	step [92/325], loss=3.9467
	step [93/325], loss=6.2040
	step [94/325], loss=6.2272
	step [95/325], loss=4.7462
	step [96/325], loss=3.9632
	step [97/325], loss=5.0725
	step [98/325], loss=5.2919
	step [99/325], loss=4.4139
	step [100/325], loss=5.0771
	step [101/325], loss=5.6131
	step [102/325], loss=6.5544
	step [103/325], loss=5.5527
	step [104/325], loss=5.7969
	step [105/325], loss=5.2475
	step [106/325], loss=5.4500
	step [107/325], loss=5.0471
	step [108/325], loss=4.7864
	step [109/325], loss=4.2691
	step [110/325], loss=4.9324
	step [111/325], loss=4.3394
	step [112/325], loss=7.4306
	step [113/325], loss=4.3961
	step [114/325], loss=5.8685
	step [115/325], loss=4.8761
	step [116/325], loss=3.7780
	step [117/325], loss=4.5611
	step [118/325], loss=5.4662
	step [119/325], loss=6.1371
	step [120/325], loss=4.0574
	step [121/325], loss=5.0131
	step [122/325], loss=4.7990
	step [123/325], loss=6.1179
	step [124/325], loss=5.7633
	step [125/325], loss=5.8985
	step [126/325], loss=4.9650
	step [127/325], loss=5.7736
	step [128/325], loss=6.0235
	step [129/325], loss=4.6699
	step [130/325], loss=5.2397
	step [131/325], loss=4.3057
	step [132/325], loss=5.1993
	step [133/325], loss=5.5649
	step [134/325], loss=4.5202
	step [135/325], loss=5.1075
	step [136/325], loss=4.7891
	step [137/325], loss=5.0598
	step [138/325], loss=6.6926
	step [139/325], loss=5.3589
	step [140/325], loss=4.4384
	step [141/325], loss=4.7809
	step [142/325], loss=5.2549
	step [143/325], loss=5.8320
	step [144/325], loss=5.5634
	step [145/325], loss=4.7821
	step [146/325], loss=6.4080
	step [147/325], loss=6.4511
	step [148/325], loss=4.9197
	step [149/325], loss=4.5383
	step [150/325], loss=5.1268
	step [151/325], loss=5.9020
	step [152/325], loss=5.1810
	step [153/325], loss=3.2888
	step [154/325], loss=4.9106
	step [155/325], loss=5.7409
	step [156/325], loss=5.3151
	step [157/325], loss=6.2769
	step [158/325], loss=4.3115
	step [159/325], loss=5.6233
	step [160/325], loss=5.2631
	step [161/325], loss=3.7465
	step [162/325], loss=5.8244
	step [163/325], loss=5.1275
	step [164/325], loss=5.8093
	step [165/325], loss=4.7846
	step [166/325], loss=4.6072
	step [167/325], loss=4.3322
	step [168/325], loss=4.4250
	step [169/325], loss=6.0732
	step [170/325], loss=5.7322
	step [171/325], loss=4.6076
	step [172/325], loss=5.7824
	step [173/325], loss=6.1128
	step [174/325], loss=5.1262
	step [175/325], loss=5.1337
	step [176/325], loss=5.5342
	step [177/325], loss=4.7794
	step [178/325], loss=5.1695
	step [179/325], loss=3.8404
	step [180/325], loss=5.6453
	step [181/325], loss=5.1447
	step [182/325], loss=6.0024
	step [183/325], loss=5.2943
	step [184/325], loss=5.5042
	step [185/325], loss=4.4693
	step [186/325], loss=5.1663
	step [187/325], loss=5.3447
	step [188/325], loss=5.1800
	step [189/325], loss=4.9260
	step [190/325], loss=6.7495
	step [191/325], loss=4.6504
	step [192/325], loss=5.2983
	step [193/325], loss=6.3605
	step [194/325], loss=5.8125
	step [195/325], loss=4.3254
	step [196/325], loss=4.6709
	step [197/325], loss=4.6546
	step [198/325], loss=5.7142
	step [199/325], loss=5.7675
	step [200/325], loss=4.4373
	step [201/325], loss=5.0512
	step [202/325], loss=4.6731
	step [203/325], loss=5.6098
	step [204/325], loss=6.6650
	step [205/325], loss=6.4922
	step [206/325], loss=5.3812
	step [207/325], loss=5.3435
	step [208/325], loss=5.0974
	step [209/325], loss=7.6641
	step [210/325], loss=5.9414
	step [211/325], loss=4.8502
	step [212/325], loss=4.6557
	step [213/325], loss=4.6510
	step [214/325], loss=5.1717
	step [215/325], loss=4.8836
	step [216/325], loss=5.1156
	step [217/325], loss=6.2138
	step [218/325], loss=6.5184
	step [219/325], loss=5.1102
	step [220/325], loss=3.9346
	step [221/325], loss=5.1566
	step [222/325], loss=5.8104
	step [223/325], loss=4.7791
	step [224/325], loss=4.5869
	step [225/325], loss=6.2371
	step [226/325], loss=5.3739
	step [227/325], loss=4.6421
	step [228/325], loss=4.4657
	step [229/325], loss=5.3189
	step [230/325], loss=5.7786
	step [231/325], loss=4.7760
	step [232/325], loss=5.2496
	step [233/325], loss=3.9832
	step [234/325], loss=4.4173
	step [235/325], loss=4.9621
	step [236/325], loss=5.2116
	step [237/325], loss=4.5387
	step [238/325], loss=5.5091
	step [239/325], loss=5.5119
	step [240/325], loss=4.5754
	step [241/325], loss=4.6415
	step [242/325], loss=5.7876
	step [243/325], loss=3.9833
	step [244/325], loss=5.3208
	step [245/325], loss=5.3885
	step [246/325], loss=6.3625
	step [247/325], loss=5.4898
	step [248/325], loss=5.0060
	step [249/325], loss=5.8050
	step [250/325], loss=5.1837
	step [251/325], loss=7.2049
	step [252/325], loss=5.4173
	step [253/325], loss=3.9912
	step [254/325], loss=5.0238
	step [255/325], loss=6.4701
	step [256/325], loss=5.2637
	step [257/325], loss=5.3289
	step [258/325], loss=5.8227
	step [259/325], loss=5.2081
	step [260/325], loss=4.2063
	step [261/325], loss=4.5462
	step [262/325], loss=4.6035
	step [263/325], loss=4.2247
	step [264/325], loss=4.9898
	step [265/325], loss=6.0674
	step [266/325], loss=6.4237
	step [267/325], loss=7.3243
	step [268/325], loss=5.1737
	step [269/325], loss=4.8353
	step [270/325], loss=5.3178
	step [271/325], loss=4.8770
	step [272/325], loss=6.1766
	step [273/325], loss=5.9529
	step [274/325], loss=4.9175
	step [275/325], loss=3.6436
	step [276/325], loss=5.1552
	step [277/325], loss=3.9693
	step [278/325], loss=5.5936
	step [279/325], loss=5.2687
	step [280/325], loss=5.1514
	step [281/325], loss=5.1162
	step [282/325], loss=5.5133
	step [283/325], loss=6.2255
	step [284/325], loss=5.0615
	step [285/325], loss=3.7928
	step [286/325], loss=6.6136
	step [287/325], loss=4.7528
	step [288/325], loss=5.3540
	step [289/325], loss=5.2917
	step [290/325], loss=5.2204
	step [291/325], loss=4.6825
	step [292/325], loss=4.8900
	step [293/325], loss=5.3677
	step [294/325], loss=4.5102
	step [295/325], loss=5.1488
	step [296/325], loss=5.4145
	step [297/325], loss=5.5791
	step [298/325], loss=5.8995
	step [299/325], loss=5.4588
	step [300/325], loss=4.8101
	step [301/325], loss=5.1329
	step [302/325], loss=5.4013
	step [303/325], loss=5.7410
	step [304/325], loss=6.2372
	step [305/325], loss=4.5362
	step [306/325], loss=4.6566
	step [307/325], loss=5.9085
	step [308/325], loss=5.4830
	step [309/325], loss=6.8064
	step [310/325], loss=6.1590
	step [311/325], loss=5.0344
	step [312/325], loss=6.1244
	step [313/325], loss=4.7432
	step [314/325], loss=6.5042
	step [315/325], loss=5.1737
	step [316/325], loss=5.4925
	step [317/325], loss=5.1684
	step [318/325], loss=4.8641
	step [319/325], loss=5.1242
	step [320/325], loss=4.5237
	step [321/325], loss=4.3747
	step [322/325], loss=3.9252
	step [323/325], loss=5.1047
	step [324/325], loss=5.3450
	step [325/325], loss=2.9105
	Evaluating
	loss=0.0177, precision=0.2350, recall=0.9939, f1=0.3802
saving model as: 1_saved_model.pth
Training epoch 25
	step [1/325], loss=6.1961
	step [2/325], loss=5.8792
	step [3/325], loss=6.5679
	step [4/325], loss=5.7855
	step [5/325], loss=5.0279
	step [6/325], loss=4.8922
	step [7/325], loss=5.2889
	step [8/325], loss=4.6264
	step [9/325], loss=5.6378
	step [10/325], loss=5.2727
	step [11/325], loss=5.8343
	step [12/325], loss=4.4395
	step [13/325], loss=4.2743
	step [14/325], loss=4.8936
	step [15/325], loss=6.1619
	step [16/325], loss=5.5015
	step [17/325], loss=5.5479
	step [18/325], loss=5.3234
	step [19/325], loss=5.5665
	step [20/325], loss=5.7090
	step [21/325], loss=5.5625
	step [22/325], loss=6.3684
	step [23/325], loss=5.4648
	step [24/325], loss=5.3401
	step [25/325], loss=5.6462
	step [26/325], loss=5.8142
	step [27/325], loss=6.3397
	step [28/325], loss=5.2616
	step [29/325], loss=4.4624
	step [30/325], loss=4.2391
	step [31/325], loss=5.3045
	step [32/325], loss=6.6671
	step [33/325], loss=5.6393
	step [34/325], loss=4.7511
	step [35/325], loss=5.1523
	step [36/325], loss=4.1919
	step [37/325], loss=5.5619
	step [38/325], loss=6.0664
	step [39/325], loss=5.0625
	step [40/325], loss=5.4019
	step [41/325], loss=5.6779
	step [42/325], loss=5.4884
	step [43/325], loss=4.8997
	step [44/325], loss=4.9889
	step [45/325], loss=5.9851
	step [46/325], loss=5.2263
	step [47/325], loss=5.3157
	step [48/325], loss=4.1152
	step [49/325], loss=7.7545
	step [50/325], loss=5.3391
	step [51/325], loss=5.0339
	step [52/325], loss=6.1810
	step [53/325], loss=5.0297
	step [54/325], loss=5.4101
	step [55/325], loss=5.0947
	step [56/325], loss=4.0248
	step [57/325], loss=4.4344
	step [58/325], loss=5.7046
	step [59/325], loss=6.5043
	step [60/325], loss=5.1282
	step [61/325], loss=6.8921
	step [62/325], loss=4.0874
	step [63/325], loss=4.1834
	step [64/325], loss=4.8482
	step [65/325], loss=5.9589
	step [66/325], loss=4.7766
	step [67/325], loss=5.4855
	step [68/325], loss=5.5814
	step [69/325], loss=5.2429
	step [70/325], loss=6.1150
	step [71/325], loss=5.8584
	step [72/325], loss=7.6322
	step [73/325], loss=3.9954
	step [74/325], loss=4.8038
	step [75/325], loss=4.1682
	step [76/325], loss=7.0598
	step [77/325], loss=4.9586
	step [78/325], loss=5.0433
	step [79/325], loss=5.0231
	step [80/325], loss=5.0345
	step [81/325], loss=3.8806
	step [82/325], loss=4.2792
	step [83/325], loss=5.0223
	step [84/325], loss=4.9180
	step [85/325], loss=5.6343
	step [86/325], loss=4.5532
	step [87/325], loss=5.3563
	step [88/325], loss=5.8242
	step [89/325], loss=5.8410
	step [90/325], loss=5.3211
	step [91/325], loss=6.2227
	step [92/325], loss=4.8785
	step [93/325], loss=4.7345
	step [94/325], loss=4.5425
	step [95/325], loss=4.2398
	step [96/325], loss=5.4270
	step [97/325], loss=5.9905
	step [98/325], loss=5.3297
	step [99/325], loss=4.8235
	step [100/325], loss=5.1063
	step [101/325], loss=4.2651
	step [102/325], loss=4.7208
	step [103/325], loss=4.1844
	step [104/325], loss=4.9898
	step [105/325], loss=6.6146
	step [106/325], loss=5.3242
	step [107/325], loss=4.0779
	step [108/325], loss=4.7424
	step [109/325], loss=6.3036
	step [110/325], loss=5.8189
	step [111/325], loss=6.7962
	step [112/325], loss=4.6487
	step [113/325], loss=6.1760
	step [114/325], loss=5.7373
	step [115/325], loss=5.7943
	step [116/325], loss=5.6306
	step [117/325], loss=6.0744
	step [118/325], loss=4.7660
	step [119/325], loss=4.8326
	step [120/325], loss=5.0153
	step [121/325], loss=5.1476
	step [122/325], loss=3.9982
	step [123/325], loss=5.2549
	step [124/325], loss=3.8487
	step [125/325], loss=4.0581
	step [126/325], loss=5.3649
	step [127/325], loss=4.9973
	step [128/325], loss=6.4316
	step [129/325], loss=3.9529
	step [130/325], loss=5.5165
	step [131/325], loss=4.9789
	step [132/325], loss=7.6039
	step [133/325], loss=4.6628
	step [134/325], loss=5.8834
	step [135/325], loss=4.8574
	step [136/325], loss=4.3189
	step [137/325], loss=4.7146
	step [138/325], loss=5.2351
	step [139/325], loss=6.2103
	step [140/325], loss=4.4768
	step [141/325], loss=4.8787
	step [142/325], loss=5.2545
	step [143/325], loss=5.1802
	step [144/325], loss=5.1287
	step [145/325], loss=4.2534
	step [146/325], loss=4.1614
	step [147/325], loss=5.9683
	step [148/325], loss=4.9369
	step [149/325], loss=5.4092
	step [150/325], loss=4.5184
	step [151/325], loss=4.0779
	step [152/325], loss=4.0030
	step [153/325], loss=4.2474
	step [154/325], loss=5.9364
	step [155/325], loss=5.9753
	step [156/325], loss=5.3067
	step [157/325], loss=5.1400
	step [158/325], loss=6.1409
	step [159/325], loss=5.0571
	step [160/325], loss=5.7005
	step [161/325], loss=5.4578
	step [162/325], loss=5.2777
	step [163/325], loss=7.4813
	step [164/325], loss=3.9967
	step [165/325], loss=5.2404
	step [166/325], loss=4.7456
	step [167/325], loss=5.3674
	step [168/325], loss=5.0498
	step [169/325], loss=5.1167
	step [170/325], loss=5.3103
	step [171/325], loss=6.7178
	step [172/325], loss=6.0729
	step [173/325], loss=5.4987
	step [174/325], loss=4.8526
	step [175/325], loss=5.2047
	step [176/325], loss=5.0203
	step [177/325], loss=5.1921
	step [178/325], loss=4.6189
	step [179/325], loss=4.2995
	step [180/325], loss=6.1556
	step [181/325], loss=5.9082
	step [182/325], loss=5.4561
	step [183/325], loss=4.8574
	step [184/325], loss=4.8533
	step [185/325], loss=6.0042
	step [186/325], loss=4.9907
	step [187/325], loss=4.2559
	step [188/325], loss=6.4746
	step [189/325], loss=3.6277
	step [190/325], loss=5.1571
	step [191/325], loss=4.0694
	step [192/325], loss=5.4569
	step [193/325], loss=4.1211
	step [194/325], loss=5.3174
	step [195/325], loss=4.3753
	step [196/325], loss=4.8609
	step [197/325], loss=4.9618
	step [198/325], loss=4.1187
	step [199/325], loss=5.0110
	step [200/325], loss=4.9808
	step [201/325], loss=6.8910
	step [202/325], loss=4.3136
	step [203/325], loss=6.1818
	step [204/325], loss=4.9348
	step [205/325], loss=4.1545
	step [206/325], loss=5.9919
	step [207/325], loss=5.9968
	step [208/325], loss=4.5720
	step [209/325], loss=4.8841
	step [210/325], loss=6.1198
	step [211/325], loss=4.8432
	step [212/325], loss=4.3618
	step [213/325], loss=4.9998
	step [214/325], loss=8.4574
	step [215/325], loss=4.6938
	step [216/325], loss=4.4404
	step [217/325], loss=5.1970
	step [218/325], loss=3.8981
	step [219/325], loss=4.9878
	step [220/325], loss=4.3254
	step [221/325], loss=5.0049
	step [222/325], loss=5.0495
	step [223/325], loss=5.3500
	step [224/325], loss=3.9813
	step [225/325], loss=7.2107
	step [226/325], loss=5.3171
	step [227/325], loss=4.8628
	step [228/325], loss=4.2594
	step [229/325], loss=4.7617
	step [230/325], loss=4.3941
	step [231/325], loss=4.1142
	step [232/325], loss=4.6644
	step [233/325], loss=5.4431
	step [234/325], loss=4.6022
	step [235/325], loss=5.0148
	step [236/325], loss=4.1615
	step [237/325], loss=4.7310
	step [238/325], loss=6.0903
	step [239/325], loss=4.2439
	step [240/325], loss=6.0024
	step [241/325], loss=4.9756
	step [242/325], loss=5.1497
	step [243/325], loss=6.4872
	step [244/325], loss=5.4261
	step [245/325], loss=4.6034
	step [246/325], loss=4.8402
	step [247/325], loss=4.6225
	step [248/325], loss=6.3592
	step [249/325], loss=5.4275
	step [250/325], loss=5.1561
	step [251/325], loss=5.7816
	step [252/325], loss=4.9886
	step [253/325], loss=4.6239
	step [254/325], loss=4.8046
	step [255/325], loss=5.4053
	step [256/325], loss=3.9764
	step [257/325], loss=4.8660
	step [258/325], loss=5.3157
	step [259/325], loss=5.2978
	step [260/325], loss=5.1529
	step [261/325], loss=5.3403
	step [262/325], loss=4.2843
	step [263/325], loss=4.7514
	step [264/325], loss=4.4900
	step [265/325], loss=3.3989
	step [266/325], loss=4.2145
	step [267/325], loss=6.2083
	step [268/325], loss=5.5520
	step [269/325], loss=5.2995
	step [270/325], loss=4.8110
	step [271/325], loss=5.5784
	step [272/325], loss=4.9289
	step [273/325], loss=5.0571
	step [274/325], loss=5.8436
	step [275/325], loss=4.6758
	step [276/325], loss=6.1615
	step [277/325], loss=4.6781
	step [278/325], loss=6.4256
	step [279/325], loss=5.0982
	step [280/325], loss=4.9816
	step [281/325], loss=4.7931
	step [282/325], loss=4.9717
	step [283/325], loss=4.9123
	step [284/325], loss=5.6110
	step [285/325], loss=5.0384
	step [286/325], loss=5.9757
	step [287/325], loss=4.2237
	step [288/325], loss=5.7764
	step [289/325], loss=6.4336
	step [290/325], loss=6.3472
	step [291/325], loss=3.8813
	step [292/325], loss=4.6808
	step [293/325], loss=5.6346
	step [294/325], loss=5.2077
	step [295/325], loss=5.1618
	step [296/325], loss=4.2459
	step [297/325], loss=4.9268
	step [298/325], loss=6.7471
	step [299/325], loss=5.4533
	step [300/325], loss=5.2928
	step [301/325], loss=5.0905
	step [302/325], loss=5.1036
	step [303/325], loss=5.0703
	step [304/325], loss=4.5474
	step [305/325], loss=5.2690
	step [306/325], loss=4.4918
	step [307/325], loss=4.8416
	step [308/325], loss=5.8045
	step [309/325], loss=4.8563
	step [310/325], loss=7.7614
	step [311/325], loss=5.0447
	step [312/325], loss=4.4408
	step [313/325], loss=4.4295
	step [314/325], loss=5.1548
	step [315/325], loss=5.6304
	step [316/325], loss=4.5430
	step [317/325], loss=4.9363
	step [318/325], loss=6.0190
	step [319/325], loss=4.7067
	step [320/325], loss=4.0709
	step [321/325], loss=5.9953
	step [322/325], loss=6.8366
	step [323/325], loss=4.4955
	step [324/325], loss=5.3884
	step [325/325], loss=3.6420
	Evaluating
	loss=0.0194, precision=0.1913, recall=0.9953, f1=0.3209
Training epoch 26
	step [1/325], loss=5.3968
	step [2/325], loss=4.8084
	step [3/325], loss=5.8097
	step [4/325], loss=4.1838
	step [5/325], loss=5.4651
	step [6/325], loss=5.0147
	step [7/325], loss=5.8879
	step [8/325], loss=6.1597
	step [9/325], loss=5.3276
	step [10/325], loss=4.0465
	step [11/325], loss=4.9654
	step [12/325], loss=4.2307
	step [13/325], loss=5.9699
	step [14/325], loss=5.2568
	step [15/325], loss=4.6617
	step [16/325], loss=5.4162
	step [17/325], loss=5.6507
	step [18/325], loss=5.0234
	step [19/325], loss=4.3260
	step [20/325], loss=5.5179
	step [21/325], loss=4.5290
	step [22/325], loss=3.7920
	step [23/325], loss=5.1695
	step [24/325], loss=5.5799
	step [25/325], loss=5.2883
	step [26/325], loss=4.3347
	step [27/325], loss=5.7853
	step [28/325], loss=4.7015
	step [29/325], loss=5.5681
	step [30/325], loss=5.1982
	step [31/325], loss=4.7831
	step [32/325], loss=6.1975
	step [33/325], loss=5.8482
	step [34/325], loss=3.9124
	step [35/325], loss=6.6339
	step [36/325], loss=5.3941
	step [37/325], loss=5.3688
	step [38/325], loss=4.7962
	step [39/325], loss=4.6163
	step [40/325], loss=4.0611
	step [41/325], loss=4.6501
	step [42/325], loss=6.8473
	step [43/325], loss=5.6098
	step [44/325], loss=6.1630
	step [45/325], loss=4.8502
	step [46/325], loss=5.2520
	step [47/325], loss=4.7503
	step [48/325], loss=5.2625
	step [49/325], loss=5.9987
	step [50/325], loss=4.9983
	step [51/325], loss=4.7411
	step [52/325], loss=5.0156
	step [53/325], loss=3.9123
	step [54/325], loss=5.2101
	step [55/325], loss=4.7861
	step [56/325], loss=5.9343
	step [57/325], loss=4.9155
	step [58/325], loss=4.5753
	step [59/325], loss=6.9833
	step [60/325], loss=5.8804
	step [61/325], loss=8.0345
	step [62/325], loss=5.8809
	step [63/325], loss=5.7070
	step [64/325], loss=4.5100
	step [65/325], loss=4.6801
	step [66/325], loss=4.7582
	step [67/325], loss=5.1220
	step [68/325], loss=5.4462
	step [69/325], loss=4.7938
	step [70/325], loss=5.0994
	step [71/325], loss=4.9308
	step [72/325], loss=4.6970
	step [73/325], loss=4.9191
	step [74/325], loss=5.3827
	step [75/325], loss=6.3716
	step [76/325], loss=5.6962
	step [77/325], loss=6.5090
	step [78/325], loss=5.2171
	step [79/325], loss=5.6686
	step [80/325], loss=5.4600
	step [81/325], loss=5.1147
	step [82/325], loss=4.7886
	step [83/325], loss=4.8194
	step [84/325], loss=4.5671
	step [85/325], loss=4.9179
	step [86/325], loss=3.8915
	step [87/325], loss=4.9629
	step [88/325], loss=4.4881
	step [89/325], loss=4.3382
	step [90/325], loss=5.2728
	step [91/325], loss=5.0240
	step [92/325], loss=5.8373
	step [93/325], loss=4.9266
	step [94/325], loss=4.4943
	step [95/325], loss=4.5161
	step [96/325], loss=4.9746
	step [97/325], loss=3.8896
	step [98/325], loss=5.7979
	step [99/325], loss=5.2296
	step [100/325], loss=4.6173
	step [101/325], loss=5.9539
	step [102/325], loss=4.2372
	step [103/325], loss=3.8511
	step [104/325], loss=5.3784
	step [105/325], loss=4.5928
	step [106/325], loss=5.4721
	step [107/325], loss=5.3606
	step [108/325], loss=4.3206
	step [109/325], loss=5.4252
	step [110/325], loss=4.6403
	step [111/325], loss=3.5717
	step [112/325], loss=4.5612
	step [113/325], loss=6.0412
	step [114/325], loss=5.8834
	step [115/325], loss=5.5666
	step [116/325], loss=4.5664
	step [117/325], loss=4.5975
	step [118/325], loss=6.2407
	step [119/325], loss=6.1105
	step [120/325], loss=6.0779
	step [121/325], loss=4.2396
	step [122/325], loss=4.5424
	step [123/325], loss=5.9659
	step [124/325], loss=4.7337
	step [125/325], loss=5.0636
	step [126/325], loss=5.3790
	step [127/325], loss=4.7213
	step [128/325], loss=4.8898
	step [129/325], loss=6.0021
	step [130/325], loss=5.0165
	step [131/325], loss=5.4730
	step [132/325], loss=5.0532
	step [133/325], loss=6.8401
	step [134/325], loss=4.9877
	step [135/325], loss=5.1195
	step [136/325], loss=4.7691
	step [137/325], loss=5.1545
	step [138/325], loss=4.5448
	step [139/325], loss=4.5694
	step [140/325], loss=5.0723
	step [141/325], loss=5.2142
	step [142/325], loss=6.3502
	step [143/325], loss=4.0570
	step [144/325], loss=3.8439
	step [145/325], loss=4.8292
	step [146/325], loss=6.9766
	step [147/325], loss=5.0057
	step [148/325], loss=5.7970
	step [149/325], loss=5.7718
	step [150/325], loss=4.8706
	step [151/325], loss=4.4699
	step [152/325], loss=3.9786
	step [153/325], loss=4.9920
	step [154/325], loss=4.9709
	step [155/325], loss=6.0124
	step [156/325], loss=4.4959
	step [157/325], loss=5.3605
	step [158/325], loss=5.4270
	step [159/325], loss=5.4651
	step [160/325], loss=5.6169
	step [161/325], loss=5.2716
	step [162/325], loss=5.2613
	step [163/325], loss=5.3857
	step [164/325], loss=4.9117
	step [165/325], loss=4.7517
	step [166/325], loss=5.2497
	step [167/325], loss=5.0846
	step [168/325], loss=4.2291
	step [169/325], loss=4.3430
	step [170/325], loss=4.6560
	step [171/325], loss=4.9404
	step [172/325], loss=4.0076
	step [173/325], loss=6.2251
	step [174/325], loss=5.0211
	step [175/325], loss=5.9368
	step [176/325], loss=4.1960
	step [177/325], loss=4.6084
	step [178/325], loss=4.8065
	step [179/325], loss=6.0809
	step [180/325], loss=5.1280
	step [181/325], loss=3.0841
	step [182/325], loss=4.0577
	step [183/325], loss=4.8363
	step [184/325], loss=4.4572
	step [185/325], loss=5.0778
	step [186/325], loss=4.3130
	step [187/325], loss=5.7323
	step [188/325], loss=5.3130
	step [189/325], loss=4.1700
	step [190/325], loss=4.9231
	step [191/325], loss=4.6797
	step [192/325], loss=5.4490
	step [193/325], loss=5.6352
	step [194/325], loss=4.6218
	step [195/325], loss=6.5600
	step [196/325], loss=4.9933
	step [197/325], loss=4.9166
	step [198/325], loss=5.1232
	step [199/325], loss=5.4284
	step [200/325], loss=5.6299
	step [201/325], loss=4.6462
	step [202/325], loss=5.1859
	step [203/325], loss=4.9208
	step [204/325], loss=4.7632
	step [205/325], loss=4.5556
	step [206/325], loss=4.9403
	step [207/325], loss=4.6223
	step [208/325], loss=5.6477
	step [209/325], loss=4.6525
	step [210/325], loss=4.7627
	step [211/325], loss=4.2804
	step [212/325], loss=4.2689
	step [213/325], loss=5.4398
	step [214/325], loss=5.3016
	step [215/325], loss=5.7287
	step [216/325], loss=5.6344
	step [217/325], loss=6.5105
	step [218/325], loss=4.2592
	step [219/325], loss=5.3390
	step [220/325], loss=5.0159
	step [221/325], loss=5.0008
	step [222/325], loss=5.0073
	step [223/325], loss=5.0392
	step [224/325], loss=4.6836
	step [225/325], loss=4.8159
	step [226/325], loss=5.3883
	step [227/325], loss=4.3019
	step [228/325], loss=5.2607
	step [229/325], loss=4.6791
	step [230/325], loss=4.1188
	step [231/325], loss=4.2854
	step [232/325], loss=5.2482
	step [233/325], loss=4.7261
	step [234/325], loss=4.9310
	step [235/325], loss=5.2180
	step [236/325], loss=5.0297
	step [237/325], loss=4.6001
	step [238/325], loss=5.7057
	step [239/325], loss=4.5066
	step [240/325], loss=4.1786
	step [241/325], loss=5.5028
	step [242/325], loss=4.8998
	step [243/325], loss=4.3813
	step [244/325], loss=5.9918
	step [245/325], loss=4.9161
	step [246/325], loss=5.8128
	step [247/325], loss=4.8361
	step [248/325], loss=4.5606
	step [249/325], loss=4.7109
	step [250/325], loss=4.0017
	step [251/325], loss=4.9009
	step [252/325], loss=5.3653
	step [253/325], loss=5.2657
	step [254/325], loss=4.2301
	step [255/325], loss=6.0527
	step [256/325], loss=4.8773
	step [257/325], loss=5.9293
	step [258/325], loss=4.4070
	step [259/325], loss=4.1376
	step [260/325], loss=4.8959
	step [261/325], loss=5.8480
	step [262/325], loss=5.1854
	step [263/325], loss=5.8387
	step [264/325], loss=4.4561
	step [265/325], loss=6.3891
	step [266/325], loss=4.6644
	step [267/325], loss=5.3261
	step [268/325], loss=6.5585
	step [269/325], loss=6.1250
	step [270/325], loss=6.3088
	step [271/325], loss=5.6747
	step [272/325], loss=5.3063
	step [273/325], loss=5.4535
	step [274/325], loss=5.7100
	step [275/325], loss=4.6965
	step [276/325], loss=3.7216
	step [277/325], loss=5.8177
	step [278/325], loss=5.2709
	step [279/325], loss=4.9479
	step [280/325], loss=6.7265
	step [281/325], loss=4.5817
	step [282/325], loss=4.3940
	step [283/325], loss=5.2608
	step [284/325], loss=5.0419
	step [285/325], loss=4.9694
	step [286/325], loss=5.8886
	step [287/325], loss=6.2374
	step [288/325], loss=4.6028
	step [289/325], loss=4.6196
	step [290/325], loss=4.0419
	step [291/325], loss=4.9558
	step [292/325], loss=4.9704
	step [293/325], loss=5.2231
	step [294/325], loss=4.2394
	step [295/325], loss=4.2130
	step [296/325], loss=4.4594
	step [297/325], loss=4.0958
	step [298/325], loss=4.5802
	step [299/325], loss=7.1069
	step [300/325], loss=5.4865
	step [301/325], loss=3.8504
	step [302/325], loss=4.4657
	step [303/325], loss=3.6655
	step [304/325], loss=4.5673
	step [305/325], loss=4.9337
	step [306/325], loss=5.5046
	step [307/325], loss=5.1844
	step [308/325], loss=6.4400
	step [309/325], loss=6.5703
	step [310/325], loss=4.8258
	step [311/325], loss=4.6247
	step [312/325], loss=4.3182
	step [313/325], loss=9.3460
	step [314/325], loss=6.0766
	step [315/325], loss=5.3790
	step [316/325], loss=5.8451
	step [317/325], loss=5.2936
	step [318/325], loss=4.5070
	step [319/325], loss=6.3745
	step [320/325], loss=6.4689
	step [321/325], loss=5.8232
	step [322/325], loss=5.1127
	step [323/325], loss=4.6735
	step [324/325], loss=5.2800
	step [325/325], loss=3.3407
	Evaluating
	loss=0.0183, precision=0.2029, recall=0.9945, f1=0.3370
Training epoch 27
	step [1/325], loss=5.3324
	step [2/325], loss=4.1855
	step [3/325], loss=5.6747
	step [4/325], loss=5.8352
	step [5/325], loss=6.1331
	step [6/325], loss=4.5884
	step [7/325], loss=5.2892
	step [8/325], loss=4.6270
	step [9/325], loss=4.9942
	step [10/325], loss=4.1438
	step [11/325], loss=4.4685
	step [12/325], loss=5.3822
	step [13/325], loss=5.0555
	step [14/325], loss=5.2376
	step [15/325], loss=4.5610
	step [16/325], loss=4.5313
	step [17/325], loss=4.1388
	step [18/325], loss=6.7010
	step [19/325], loss=3.7188
	step [20/325], loss=5.9929
	step [21/325], loss=6.1758
	step [22/325], loss=5.0670
	step [23/325], loss=4.3091
	step [24/325], loss=5.0130
	step [25/325], loss=4.8691
	step [26/325], loss=5.6240
	step [27/325], loss=5.1847
	step [28/325], loss=4.2190
	step [29/325], loss=5.2887
	step [30/325], loss=5.3951
	step [31/325], loss=4.8601
	step [32/325], loss=4.5889
	step [33/325], loss=4.3825
	step [34/325], loss=6.7281
	step [35/325], loss=5.7583
	step [36/325], loss=4.4951
	step [37/325], loss=4.2835
	step [38/325], loss=6.3665
	step [39/325], loss=3.6633
	step [40/325], loss=4.6127
	step [41/325], loss=4.9723
	step [42/325], loss=4.6202
	step [43/325], loss=5.1670
	step [44/325], loss=4.4715
	step [45/325], loss=4.2275
	step [46/325], loss=5.4964
	step [47/325], loss=5.7030
	step [48/325], loss=7.0612
	step [49/325], loss=3.7173
	step [50/325], loss=5.1386
	step [51/325], loss=4.1377
	step [52/325], loss=5.4201
	step [53/325], loss=4.3064
	step [54/325], loss=5.8631
	step [55/325], loss=6.2242
	step [56/325], loss=3.9791
	step [57/325], loss=4.5517
	step [58/325], loss=4.7764
	step [59/325], loss=5.6438
	step [60/325], loss=5.4526
	step [61/325], loss=5.0140
	step [62/325], loss=4.7175
	step [63/325], loss=3.8139
	step [64/325], loss=4.6388
	step [65/325], loss=5.4625
	step [66/325], loss=4.1148
	step [67/325], loss=3.5960
	step [68/325], loss=5.4990
	step [69/325], loss=5.0269
	step [70/325], loss=6.4213
	step [71/325], loss=4.9732
	step [72/325], loss=5.9196
	step [73/325], loss=4.5374
	step [74/325], loss=4.5392
	step [75/325], loss=5.0726
	step [76/325], loss=5.1434
	step [77/325], loss=5.3722
	step [78/325], loss=6.0823
	step [79/325], loss=5.4015
	step [80/325], loss=4.3905
	step [81/325], loss=4.8056
	step [82/325], loss=4.3262
	step [83/325], loss=4.7068
	step [84/325], loss=5.9541
	step [85/325], loss=4.6059
	step [86/325], loss=4.2595
	step [87/325], loss=6.1172
	step [88/325], loss=4.1911
	step [89/325], loss=5.9367
	step [90/325], loss=5.2731
	step [91/325], loss=5.6754
	step [92/325], loss=5.2603
	step [93/325], loss=4.7801
	step [94/325], loss=4.9718
	step [95/325], loss=5.3994
	step [96/325], loss=5.9868
	step [97/325], loss=5.3523
	step [98/325], loss=6.6751
	step [99/325], loss=5.1233
	step [100/325], loss=4.9585
	step [101/325], loss=5.3271
	step [102/325], loss=4.7340
	step [103/325], loss=4.9364
	step [104/325], loss=5.0414
	step [105/325], loss=4.5967
	step [106/325], loss=5.5375
	step [107/325], loss=4.8993
	step [108/325], loss=5.1153
	step [109/325], loss=6.1236
	step [110/325], loss=4.4997
	step [111/325], loss=5.4861
	step [112/325], loss=3.2522
	step [113/325], loss=4.5539
	step [114/325], loss=4.7612
	step [115/325], loss=4.8796
	step [116/325], loss=4.9003
	step [117/325], loss=4.5370
	step [118/325], loss=6.6040
	step [119/325], loss=6.2466
	step [120/325], loss=3.9642
	step [121/325], loss=5.3615
	step [122/325], loss=5.6453
	step [123/325], loss=5.5568
	step [124/325], loss=6.3750
	step [125/325], loss=4.8880
	step [126/325], loss=4.9883
	step [127/325], loss=4.9112
	step [128/325], loss=5.1556
	step [129/325], loss=5.5727
	step [130/325], loss=5.3913
	step [131/325], loss=4.9687
	step [132/325], loss=5.2052
	step [133/325], loss=5.5433
	step [134/325], loss=4.3833
	step [135/325], loss=5.8932
	step [136/325], loss=5.3032
	step [137/325], loss=4.3889
	step [138/325], loss=3.9078
	step [139/325], loss=4.7404
	step [140/325], loss=5.3550
	step [141/325], loss=4.0756
	step [142/325], loss=5.8360
	step [143/325], loss=4.7398
	step [144/325], loss=4.6224
	step [145/325], loss=4.8535
	step [146/325], loss=5.4346
	step [147/325], loss=4.8183
	step [148/325], loss=4.3383
	step [149/325], loss=4.9113
	step [150/325], loss=5.0993
	step [151/325], loss=5.0687
	step [152/325], loss=5.2425
	step [153/325], loss=4.6575
	step [154/325], loss=5.9386
	step [155/325], loss=4.7565
	step [156/325], loss=4.6952
	step [157/325], loss=5.6737
	step [158/325], loss=6.9317
	step [159/325], loss=5.6321
	step [160/325], loss=5.5299
	step [161/325], loss=4.6166
	step [162/325], loss=6.2998
	step [163/325], loss=3.8565
	step [164/325], loss=4.6024
	step [165/325], loss=7.1854
	step [166/325], loss=5.0281
	step [167/325], loss=4.6412
	step [168/325], loss=5.7330
	step [169/325], loss=6.2819
	step [170/325], loss=4.6777
	step [171/325], loss=4.0501
	step [172/325], loss=4.7027
	step [173/325], loss=3.8933
	step [174/325], loss=3.7698
	step [175/325], loss=3.7501
	step [176/325], loss=5.2719
	step [177/325], loss=6.0407
	step [178/325], loss=3.8181
	step [179/325], loss=5.6882
	step [180/325], loss=4.7607
	step [181/325], loss=5.7399
	step [182/325], loss=5.3994
	step [183/325], loss=4.3804
	step [184/325], loss=5.3720
	step [185/325], loss=4.5222
	step [186/325], loss=4.6322
	step [187/325], loss=4.0677
	step [188/325], loss=4.8782
	step [189/325], loss=4.7976
	step [190/325], loss=3.8433
	step [191/325], loss=3.9848
	step [192/325], loss=4.1797
	step [193/325], loss=5.7833
	step [194/325], loss=3.8961
	step [195/325], loss=5.0757
	step [196/325], loss=4.5717
	step [197/325], loss=5.0178
	step [198/325], loss=4.5666
	step [199/325], loss=4.8970
	step [200/325], loss=4.7806
	step [201/325], loss=5.0760
	step [202/325], loss=6.0257
	step [203/325], loss=4.4669
	step [204/325], loss=3.6314
	step [205/325], loss=6.1531
	step [206/325], loss=4.8051
	step [207/325], loss=6.0994
	step [208/325], loss=4.7246
	step [209/325], loss=4.3191
	step [210/325], loss=5.6801
	step [211/325], loss=4.7858
	step [212/325], loss=7.4948
	step [213/325], loss=4.4794
	step [214/325], loss=3.4788
	step [215/325], loss=3.9301
	step [216/325], loss=4.8992
	step [217/325], loss=7.4583
	step [218/325], loss=4.2407
	step [219/325], loss=5.7612
	step [220/325], loss=3.7918
	step [221/325], loss=5.9801
	step [222/325], loss=5.2145
	step [223/325], loss=5.3966
	step [224/325], loss=4.3884
	step [225/325], loss=5.1783
	step [226/325], loss=4.5763
	step [227/325], loss=5.2367
	step [228/325], loss=5.2309
	step [229/325], loss=6.0282
	step [230/325], loss=4.5618
	step [231/325], loss=5.6244
	step [232/325], loss=3.6049
	step [233/325], loss=4.1790
	step [234/325], loss=4.9518
	step [235/325], loss=4.8517
	step [236/325], loss=3.5673
	step [237/325], loss=4.4296
	step [238/325], loss=3.9947
	step [239/325], loss=4.8379
	step [240/325], loss=3.8674
	step [241/325], loss=5.2934
	step [242/325], loss=5.4917
	step [243/325], loss=4.3520
	step [244/325], loss=5.3278
	step [245/325], loss=4.8632
	step [246/325], loss=4.9700
	step [247/325], loss=4.7204
	step [248/325], loss=4.9436
	step [249/325], loss=4.2285
	step [250/325], loss=4.7113
	step [251/325], loss=5.2556
	step [252/325], loss=4.9188
	step [253/325], loss=5.1155
	step [254/325], loss=4.8401
	step [255/325], loss=3.6926
	step [256/325], loss=4.6728
	step [257/325], loss=6.2810
	step [258/325], loss=5.8293
	step [259/325], loss=4.4516
	step [260/325], loss=4.2576
	step [261/325], loss=4.0599
	step [262/325], loss=5.2686
	step [263/325], loss=5.2306
	step [264/325], loss=5.2230
	step [265/325], loss=5.0049
	step [266/325], loss=5.1627
	step [267/325], loss=5.5058
	step [268/325], loss=6.2480
	step [269/325], loss=4.3413
	step [270/325], loss=4.8545
	step [271/325], loss=4.4667
	step [272/325], loss=4.9982
	step [273/325], loss=5.1105
	step [274/325], loss=4.1846
	step [275/325], loss=4.7169
	step [276/325], loss=5.2057
	step [277/325], loss=4.7961
	step [278/325], loss=5.6995
	step [279/325], loss=5.4817
	step [280/325], loss=5.1861
	step [281/325], loss=6.9104
	step [282/325], loss=5.5593
	step [283/325], loss=4.7281
	step [284/325], loss=4.0368
	step [285/325], loss=4.9663
	step [286/325], loss=5.1580
	step [287/325], loss=4.7027
	step [288/325], loss=4.8678
	step [289/325], loss=4.3533
	step [290/325], loss=5.9408
	step [291/325], loss=5.5951
	step [292/325], loss=5.7244
	step [293/325], loss=5.2911
	step [294/325], loss=4.4872
	step [295/325], loss=5.4867
	step [296/325], loss=4.5575
	step [297/325], loss=5.6702
	step [298/325], loss=4.9786
	step [299/325], loss=5.6922
	step [300/325], loss=4.4672
	step [301/325], loss=5.3020
	step [302/325], loss=5.9393
	step [303/325], loss=4.2603
	step [304/325], loss=4.4838
	step [305/325], loss=5.3839
	step [306/325], loss=4.7603
	step [307/325], loss=4.5775
	step [308/325], loss=4.9604
	step [309/325], loss=6.1275
	step [310/325], loss=5.7539
	step [311/325], loss=5.1320
	step [312/325], loss=5.2972
	step [313/325], loss=5.1702
	step [314/325], loss=4.8522
	step [315/325], loss=5.5432
	step [316/325], loss=4.8279
	step [317/325], loss=4.3834
	step [318/325], loss=4.9768
	step [319/325], loss=5.1824
	step [320/325], loss=4.8398
	step [321/325], loss=5.8225
	step [322/325], loss=4.0386
	step [323/325], loss=6.3162
	step [324/325], loss=6.1352
	step [325/325], loss=3.3726
	Evaluating
	loss=0.0214, precision=0.1876, recall=0.9958, f1=0.3157
Training epoch 28
	step [1/325], loss=5.1383
	step [2/325], loss=4.6976
	step [3/325], loss=5.8306
	step [4/325], loss=4.8366
	step [5/325], loss=5.2916
	step [6/325], loss=4.6937
	step [7/325], loss=4.2938
	step [8/325], loss=5.1598
	step [9/325], loss=6.3484
	step [10/325], loss=4.9987
	step [11/325], loss=6.1889
	step [12/325], loss=4.9021
	step [13/325], loss=5.2519
	step [14/325], loss=4.7786
	step [15/325], loss=5.8031
	step [16/325], loss=5.5574
	step [17/325], loss=5.2491
	step [18/325], loss=4.5636
	step [19/325], loss=3.4187
	step [20/325], loss=4.5453
	step [21/325], loss=5.0494
	step [22/325], loss=3.7244
	step [23/325], loss=4.2911
	step [24/325], loss=5.5523
	step [25/325], loss=4.9487
	step [26/325], loss=5.1819
	step [27/325], loss=5.9285
	step [28/325], loss=5.0993
	step [29/325], loss=4.1082
	step [30/325], loss=5.5538
	step [31/325], loss=4.7383
	step [32/325], loss=4.2404
	step [33/325], loss=5.1409
	step [34/325], loss=5.6822
	step [35/325], loss=4.7554
	step [36/325], loss=4.7604
	step [37/325], loss=4.1863
	step [38/325], loss=4.5475
	step [39/325], loss=5.5325
	step [40/325], loss=6.8069
	step [41/325], loss=6.3294
	step [42/325], loss=6.2731
	step [43/325], loss=5.2273
	step [44/325], loss=4.9487
	step [45/325], loss=5.1045
	step [46/325], loss=5.4663
	step [47/325], loss=5.4147
	step [48/325], loss=6.4571
	step [49/325], loss=4.0659
	step [50/325], loss=5.6300
	step [51/325], loss=7.1855
	step [52/325], loss=5.6332
	step [53/325], loss=6.4468
	step [54/325], loss=4.1612
	step [55/325], loss=4.8334
	step [56/325], loss=5.0968
	step [57/325], loss=4.7106
	step [58/325], loss=5.0097
	step [59/325], loss=8.7202
	step [60/325], loss=4.7729
	step [61/325], loss=6.4800
	step [62/325], loss=5.2608
	step [63/325], loss=4.8572
	step [64/325], loss=3.8179
	step [65/325], loss=4.7333
	step [66/325], loss=4.3404
	step [67/325], loss=5.0563
	step [68/325], loss=4.7313
	step [69/325], loss=5.4261
	step [70/325], loss=5.5731
	step [71/325], loss=4.8673
	step [72/325], loss=5.3230
	step [73/325], loss=5.1525
	step [74/325], loss=4.6627
	step [75/325], loss=5.4723
	step [76/325], loss=4.9896
	step [77/325], loss=5.4286
	step [78/325], loss=4.4594
	step [79/325], loss=5.8078
	step [80/325], loss=4.8876
	step [81/325], loss=4.8683
	step [82/325], loss=5.4213
	step [83/325], loss=4.5910
	step [84/325], loss=4.6756
	step [85/325], loss=4.7472
	step [86/325], loss=5.2426
	step [87/325], loss=4.4884
	step [88/325], loss=4.6148
	step [89/325], loss=4.7505
	step [90/325], loss=4.7437
	step [91/325], loss=5.0935
	step [92/325], loss=5.5758
	step [93/325], loss=6.2283
	step [94/325], loss=4.8702
	step [95/325], loss=5.1419
	step [96/325], loss=3.8480
	step [97/325], loss=4.3609
	step [98/325], loss=4.7462
	step [99/325], loss=4.6352
	step [100/325], loss=5.6345
	step [101/325], loss=5.1717
	step [102/325], loss=5.7888
	step [103/325], loss=4.9188
	step [104/325], loss=4.9906
	step [105/325], loss=5.8895
	step [106/325], loss=4.9185
	step [107/325], loss=4.6319
	step [108/325], loss=3.7079
	step [109/325], loss=5.6417
	step [110/325], loss=5.0543
	step [111/325], loss=4.7945
	step [112/325], loss=5.5117
	step [113/325], loss=3.8197
	step [114/325], loss=4.0416
	step [115/325], loss=3.9690
	step [116/325], loss=5.2510
	step [117/325], loss=6.2092
	step [118/325], loss=5.7668
	step [119/325], loss=4.5508
	step [120/325], loss=6.3962
	step [121/325], loss=5.2021
	step [122/325], loss=4.3286
	step [123/325], loss=6.1999
	step [124/325], loss=4.4037
	step [125/325], loss=4.4229
	step [126/325], loss=3.7343
	step [127/325], loss=4.8718
	step [128/325], loss=3.6904
	step [129/325], loss=4.7250
	step [130/325], loss=5.2156
	step [131/325], loss=4.8743
	step [132/325], loss=5.4209
	step [133/325], loss=4.1001
	step [134/325], loss=5.3047
	step [135/325], loss=3.6694
	step [136/325], loss=6.4407
	step [137/325], loss=3.8724
	step [138/325], loss=4.8174
	step [139/325], loss=5.3519
	step [140/325], loss=6.1315
	step [141/325], loss=5.0757
	step [142/325], loss=5.1184
	step [143/325], loss=4.3608
	step [144/325], loss=5.0647
	step [145/325], loss=5.0785
	step [146/325], loss=4.2882
	step [147/325], loss=5.3089
	step [148/325], loss=5.0759
	step [149/325], loss=6.0270
	step [150/325], loss=5.4195
	step [151/325], loss=4.7667
	step [152/325], loss=4.3530
	step [153/325], loss=4.5029
	step [154/325], loss=4.2320
	step [155/325], loss=4.9053
	step [156/325], loss=5.0934
	step [157/325], loss=5.7154
	step [158/325], loss=5.0415
	step [159/325], loss=5.0177
	step [160/325], loss=4.6483
	step [161/325], loss=5.7633
	step [162/325], loss=3.7645
	step [163/325], loss=4.8662
	step [164/325], loss=4.5553
	step [165/325], loss=4.0904
	step [166/325], loss=4.6996
	step [167/325], loss=4.8766
	step [168/325], loss=4.7881
	step [169/325], loss=3.2024
	step [170/325], loss=4.7398
	step [171/325], loss=3.5365
	step [172/325], loss=6.4794
	step [173/325], loss=6.2929
	step [174/325], loss=4.3230
	step [175/325], loss=4.9433
	step [176/325], loss=4.6823
	step [177/325], loss=4.2973
	step [178/325], loss=4.5743
	step [179/325], loss=4.6718
	step [180/325], loss=4.9183
	step [181/325], loss=5.1094
	step [182/325], loss=4.7333
	step [183/325], loss=5.2375
	step [184/325], loss=5.7185
	step [185/325], loss=5.5658
	step [186/325], loss=4.9533
	step [187/325], loss=5.2446
	step [188/325], loss=5.1510
	step [189/325], loss=5.6762
	step [190/325], loss=5.6928
	step [191/325], loss=5.2725
	step [192/325], loss=5.3363
	step [193/325], loss=4.0153
	step [194/325], loss=5.0760
	step [195/325], loss=4.5877
	step [196/325], loss=4.2445
	step [197/325], loss=6.0830
	step [198/325], loss=4.6861
	step [199/325], loss=6.5149
	step [200/325], loss=4.5204
	step [201/325], loss=4.5687
	step [202/325], loss=4.1110
	step [203/325], loss=6.5565
	step [204/325], loss=3.8716
	step [205/325], loss=6.4667
	step [206/325], loss=4.6689
	step [207/325], loss=4.0338
	step [208/325], loss=3.8865
	step [209/325], loss=6.2381
	step [210/325], loss=4.7494
	step [211/325], loss=4.5912
	step [212/325], loss=4.3955
	step [213/325], loss=4.2291
	step [214/325], loss=4.7829
	step [215/325], loss=4.4336
	step [216/325], loss=4.0150
	step [217/325], loss=4.9594
	step [218/325], loss=4.3021
	step [219/325], loss=3.4546
	step [220/325], loss=4.6257
	step [221/325], loss=5.7567
	step [222/325], loss=4.1241
	step [223/325], loss=5.7397
	step [224/325], loss=4.5879
	step [225/325], loss=4.8935
	step [226/325], loss=4.6062
	step [227/325], loss=4.0094
	step [228/325], loss=7.3146
	step [229/325], loss=4.0107
	step [230/325], loss=4.8055
	step [231/325], loss=4.8998
	step [232/325], loss=4.4766
	step [233/325], loss=3.7525
	step [234/325], loss=5.0206
	step [235/325], loss=5.6771
	step [236/325], loss=4.4383
	step [237/325], loss=4.6490
	step [238/325], loss=3.5849
	step [239/325], loss=5.0027
	step [240/325], loss=4.8453
	step [241/325], loss=4.8675
	step [242/325], loss=5.3913
	step [243/325], loss=4.0643
	step [244/325], loss=4.1659
	step [245/325], loss=5.3657
	step [246/325], loss=4.9305
	step [247/325], loss=5.2361
	step [248/325], loss=5.1087
	step [249/325], loss=4.5080
	step [250/325], loss=5.1807
	step [251/325], loss=5.5649
	step [252/325], loss=5.3270
	step [253/325], loss=5.9912
	step [254/325], loss=5.1889
	step [255/325], loss=5.7177
	step [256/325], loss=4.0088
	step [257/325], loss=5.2017
	step [258/325], loss=4.8566
	step [259/325], loss=4.5666
	step [260/325], loss=5.5672
	step [261/325], loss=5.3263
	step [262/325], loss=4.4008
	step [263/325], loss=5.5778
	step [264/325], loss=4.2551
	step [265/325], loss=5.1829
	step [266/325], loss=5.5348
	step [267/325], loss=5.2491
	step [268/325], loss=4.5872
	step [269/325], loss=5.5486
	step [270/325], loss=5.2476
	step [271/325], loss=4.0848
	step [272/325], loss=4.0297
	step [273/325], loss=6.8510
	step [274/325], loss=5.9659
	step [275/325], loss=4.2605
	step [276/325], loss=4.2284
	step [277/325], loss=4.9273
	step [278/325], loss=6.1934
	step [279/325], loss=5.2379
	step [280/325], loss=3.7495
	step [281/325], loss=4.2934
	step [282/325], loss=5.1588
	step [283/325], loss=5.2194
	step [284/325], loss=5.2454
	step [285/325], loss=4.9233
	step [286/325], loss=5.0768
	step [287/325], loss=4.7311
	step [288/325], loss=4.8456
	step [289/325], loss=4.1341
	step [290/325], loss=5.1129
	step [291/325], loss=5.0773
	step [292/325], loss=3.5205
	step [293/325], loss=4.7455
	step [294/325], loss=6.4099
	step [295/325], loss=5.4739
	step [296/325], loss=5.3143
	step [297/325], loss=5.4522
	step [298/325], loss=5.3170
	step [299/325], loss=4.2204
	step [300/325], loss=5.1095
	step [301/325], loss=4.9269
	step [302/325], loss=4.8228
	step [303/325], loss=5.4887
	step [304/325], loss=4.2708
	step [305/325], loss=4.2909
	step [306/325], loss=4.5944
	step [307/325], loss=5.5680
	step [308/325], loss=5.0442
	step [309/325], loss=4.7773
	step [310/325], loss=5.0830
	step [311/325], loss=3.2293
	step [312/325], loss=4.9161
	step [313/325], loss=4.8067
	step [314/325], loss=4.5187
	step [315/325], loss=3.8697
	step [316/325], loss=5.0691
	step [317/325], loss=3.9778
	step [318/325], loss=4.7955
	step [319/325], loss=5.1023
	step [320/325], loss=5.5865
	step [321/325], loss=4.9567
	step [322/325], loss=4.3433
	step [323/325], loss=5.5069
	step [324/325], loss=4.7235
	step [325/325], loss=3.3809
	Evaluating
	loss=0.0167, precision=0.2301, recall=0.9938, f1=0.3736
Training epoch 29
	step [1/325], loss=5.6246
	step [2/325], loss=5.8299
	step [3/325], loss=6.1982
	step [4/325], loss=6.3195
	step [5/325], loss=5.2473
	step [6/325], loss=5.1990
	step [7/325], loss=4.9411
	step [8/325], loss=4.6069
	step [9/325], loss=5.7526
	step [10/325], loss=4.7216
	step [11/325], loss=6.2851
	step [12/325], loss=4.3509
	step [13/325], loss=4.0215
	step [14/325], loss=4.9253
	step [15/325], loss=5.0108
	step [16/325], loss=3.3107
	step [17/325], loss=6.4727
	step [18/325], loss=4.9808
	step [19/325], loss=4.6991
	step [20/325], loss=4.8854
	step [21/325], loss=4.8418
	step [22/325], loss=4.8232
	step [23/325], loss=4.3983
	step [24/325], loss=3.9656
	step [25/325], loss=6.3237
	step [26/325], loss=4.6128
	step [27/325], loss=5.4635
	step [28/325], loss=5.1976
	step [29/325], loss=4.7957
	step [30/325], loss=4.2675
	step [31/325], loss=4.4524
	step [32/325], loss=5.3552
	step [33/325], loss=4.5989
	step [34/325], loss=3.7416
	step [35/325], loss=5.3653
	step [36/325], loss=5.7064
	step [37/325], loss=4.5650
	step [38/325], loss=4.0460
	step [39/325], loss=4.6330
	step [40/325], loss=5.3685
	step [41/325], loss=4.6076
	step [42/325], loss=4.9400
	step [43/325], loss=5.0627
	step [44/325], loss=3.9193
	step [45/325], loss=4.6997
	step [46/325], loss=5.3894
	step [47/325], loss=3.8135
	step [48/325], loss=5.9031
	step [49/325], loss=4.5868
	step [50/325], loss=4.0340
	step [51/325], loss=4.4091
	step [52/325], loss=4.1878
	step [53/325], loss=4.8782
	step [54/325], loss=4.5388
	step [55/325], loss=7.6502
	step [56/325], loss=4.5206
	step [57/325], loss=4.9743
	step [58/325], loss=3.9614
	step [59/325], loss=5.5908
	step [60/325], loss=5.1317
	step [61/325], loss=4.8948
	step [62/325], loss=4.2659
	step [63/325], loss=4.7596
	step [64/325], loss=4.7543
	step [65/325], loss=4.6897
	step [66/325], loss=3.8488
	step [67/325], loss=5.3422
	step [68/325], loss=5.3838
	step [69/325], loss=4.8960
	step [70/325], loss=4.3937
	step [71/325], loss=5.0185
	step [72/325], loss=4.2554
	step [73/325], loss=5.7229
	step [74/325], loss=5.0358
	step [75/325], loss=4.6927
	step [76/325], loss=4.4303
	step [77/325], loss=4.7954
	step [78/325], loss=4.3816
	step [79/325], loss=5.4159
	step [80/325], loss=4.9402
	step [81/325], loss=4.5353
	step [82/325], loss=4.2962
	step [83/325], loss=4.8940
	step [84/325], loss=4.7962
	step [85/325], loss=5.0286
	step [86/325], loss=4.0703
	step [87/325], loss=4.1946
	step [88/325], loss=5.0338
	step [89/325], loss=4.0732
	step [90/325], loss=4.9771
	step [91/325], loss=4.5364
	step [92/325], loss=3.2567
	step [93/325], loss=4.0980
	step [94/325], loss=4.4833
	step [95/325], loss=6.9590
	step [96/325], loss=5.4724
	step [97/325], loss=4.5248
	step [98/325], loss=4.6610
	step [99/325], loss=5.4493
	step [100/325], loss=4.9486
	step [101/325], loss=4.3894
	step [102/325], loss=5.4982
	step [103/325], loss=4.9698
	step [104/325], loss=5.4219
	step [105/325], loss=4.6947
	step [106/325], loss=5.2450
	step [107/325], loss=5.3842
	step [108/325], loss=5.4941
	step [109/325], loss=5.0599
	step [110/325], loss=5.1927
	step [111/325], loss=5.2976
	step [112/325], loss=4.7608
	step [113/325], loss=4.4357
	step [114/325], loss=5.6501
	step [115/325], loss=4.7488
	step [116/325], loss=5.2964
	step [117/325], loss=5.4631
	step [118/325], loss=5.5826
	step [119/325], loss=5.8316
	step [120/325], loss=4.8950
	step [121/325], loss=4.9913
	step [122/325], loss=4.7638
	step [123/325], loss=4.6053
	step [124/325], loss=4.2190
	step [125/325], loss=3.7201
	step [126/325], loss=4.2661
	step [127/325], loss=5.3706
	step [128/325], loss=4.3999
	step [129/325], loss=4.4363
	step [130/325], loss=5.3488
	step [131/325], loss=4.0343
	step [132/325], loss=4.3935
	step [133/325], loss=6.2453
	step [134/325], loss=5.7645
	step [135/325], loss=4.3007
	step [136/325], loss=4.8093
	step [137/325], loss=5.1442
	step [138/325], loss=5.1778
	step [139/325], loss=4.2875
	step [140/325], loss=4.9075
	step [141/325], loss=5.1239
	step [142/325], loss=3.8291
	step [143/325], loss=4.8478
	step [144/325], loss=5.3495
	step [145/325], loss=5.2547
	step [146/325], loss=4.3204
	step [147/325], loss=4.4358
	step [148/325], loss=4.3320
	step [149/325], loss=5.5855
	step [150/325], loss=4.1254
	step [151/325], loss=4.8178
	step [152/325], loss=4.8165
	step [153/325], loss=4.8924
	step [154/325], loss=4.8211
	step [155/325], loss=4.5028
	step [156/325], loss=4.1812
	step [157/325], loss=4.7318
	step [158/325], loss=4.6371
	step [159/325], loss=4.0473
	step [160/325], loss=3.7356
	step [161/325], loss=4.3990
	step [162/325], loss=5.5611
	step [163/325], loss=3.6673
	step [164/325], loss=4.7960
	step [165/325], loss=4.7689
	step [166/325], loss=4.6665
	step [167/325], loss=6.3737
	step [168/325], loss=5.1026
	step [169/325], loss=4.9666
	step [170/325], loss=4.8908
	step [171/325], loss=4.6695
	step [172/325], loss=4.8488
	step [173/325], loss=5.2201
	step [174/325], loss=4.6102
	step [175/325], loss=4.5345
	step [176/325], loss=7.0578
	step [177/325], loss=4.7227
	step [178/325], loss=3.9824
	step [179/325], loss=7.4008
	step [180/325], loss=4.5785
	step [181/325], loss=6.8143
	step [182/325], loss=4.7460
	step [183/325], loss=5.0231
	step [184/325], loss=6.3036
	step [185/325], loss=4.3600
	step [186/325], loss=5.0857
	step [187/325], loss=4.9694
	step [188/325], loss=4.8309
	step [189/325], loss=4.4568
	step [190/325], loss=4.3389
	step [191/325], loss=3.6046
	step [192/325], loss=7.4672
	step [193/325], loss=5.5735
	step [194/325], loss=5.1362
	step [195/325], loss=3.8666
	step [196/325], loss=4.0010
	step [197/325], loss=5.3989
	step [198/325], loss=5.4542
	step [199/325], loss=5.6425
	step [200/325], loss=4.9681
	step [201/325], loss=5.5419
	step [202/325], loss=4.1475
	step [203/325], loss=4.8078
	step [204/325], loss=3.3662
	step [205/325], loss=5.3890
	step [206/325], loss=4.1981
	step [207/325], loss=3.9554
	step [208/325], loss=3.5959
	step [209/325], loss=5.0911
	step [210/325], loss=5.2395
	step [211/325], loss=4.2479
	step [212/325], loss=5.1387
	step [213/325], loss=4.6674
	step [214/325], loss=5.6402
	step [215/325], loss=4.3096
	step [216/325], loss=4.9598
	step [217/325], loss=4.3978
	step [218/325], loss=4.4918
	step [219/325], loss=4.4479
	step [220/325], loss=6.0448
	step [221/325], loss=4.5929
	step [222/325], loss=4.3461
	step [223/325], loss=4.8818
	step [224/325], loss=4.3852
	step [225/325], loss=5.4452
	step [226/325], loss=4.4798
	step [227/325], loss=5.0837
	step [228/325], loss=4.6445
	step [229/325], loss=3.4583
	step [230/325], loss=4.4057
	step [231/325], loss=4.6592
	step [232/325], loss=4.1462
	step [233/325], loss=4.5732
	step [234/325], loss=4.0126
	step [235/325], loss=4.5706
	step [236/325], loss=4.9180
	step [237/325], loss=5.0643
	step [238/325], loss=3.9767
	step [239/325], loss=5.3054
	step [240/325], loss=5.5539
	step [241/325], loss=4.2584
	step [242/325], loss=5.0833
	step [243/325], loss=4.2770
	step [244/325], loss=4.2517
	step [245/325], loss=4.5588
	step [246/325], loss=4.5629
	step [247/325], loss=3.7804
	step [248/325], loss=4.6286
	step [249/325], loss=4.0022
	step [250/325], loss=4.3470
	step [251/325], loss=4.4997
	step [252/325], loss=4.6464
	step [253/325], loss=4.4354
	step [254/325], loss=5.2878
	step [255/325], loss=6.3414
	step [256/325], loss=3.7279
	step [257/325], loss=3.8629
	step [258/325], loss=4.3250
	step [259/325], loss=4.7295
	step [260/325], loss=4.4538
	step [261/325], loss=5.7387
	step [262/325], loss=5.8512
	step [263/325], loss=5.0034
	step [264/325], loss=6.2343
	step [265/325], loss=3.9283
	step [266/325], loss=5.2654
	step [267/325], loss=4.5192
	step [268/325], loss=5.5634
	step [269/325], loss=5.0471
	step [270/325], loss=6.0796
	step [271/325], loss=5.3891
	step [272/325], loss=3.7111
	step [273/325], loss=4.0301
	step [274/325], loss=5.1934
	step [275/325], loss=4.4571
	step [276/325], loss=5.8057
	step [277/325], loss=4.3181
	step [278/325], loss=4.8255
	step [279/325], loss=3.3566
	step [280/325], loss=5.3180
	step [281/325], loss=4.6675
	step [282/325], loss=4.0708
	step [283/325], loss=5.0888
	step [284/325], loss=3.6401
	step [285/325], loss=5.7929
	step [286/325], loss=4.1658
	step [287/325], loss=5.3584
	step [288/325], loss=5.3426
	step [289/325], loss=3.3913
	step [290/325], loss=4.9858
	step [291/325], loss=6.9853
	step [292/325], loss=4.6464
	step [293/325], loss=4.3837
	step [294/325], loss=4.0592
	step [295/325], loss=3.9507
	step [296/325], loss=6.2327
	step [297/325], loss=4.5750
	step [298/325], loss=5.1371
	step [299/325], loss=4.1283
	step [300/325], loss=4.4052
	step [301/325], loss=4.5416
	step [302/325], loss=5.0224
	step [303/325], loss=4.4212
	step [304/325], loss=4.8458
	step [305/325], loss=7.2993
	step [306/325], loss=5.2789
	step [307/325], loss=4.8007
	step [308/325], loss=3.9318
	step [309/325], loss=4.1387
	step [310/325], loss=5.0458
	step [311/325], loss=4.8202
	step [312/325], loss=2.9949
	step [313/325], loss=5.0440
	step [314/325], loss=4.7474
	step [315/325], loss=4.4649
	step [316/325], loss=6.1677
	step [317/325], loss=4.5325
	step [318/325], loss=4.8199
	step [319/325], loss=4.9744
	step [320/325], loss=5.3226
	step [321/325], loss=5.2640
	step [322/325], loss=5.1629
	step [323/325], loss=4.7502
	step [324/325], loss=4.7516
	step [325/325], loss=2.0779
	Evaluating
	loss=0.0165, precision=0.2170, recall=0.9943, f1=0.3562
Training epoch 30
	step [1/325], loss=4.6433
	step [2/325], loss=5.3501
	step [3/325], loss=5.1095
	step [4/325], loss=6.0378
	step [5/325], loss=4.6483
	step [6/325], loss=4.8498
	step [7/325], loss=4.6963
	step [8/325], loss=4.4299
	step [9/325], loss=3.6887
	step [10/325], loss=4.8351
	step [11/325], loss=6.1284
	step [12/325], loss=5.7991
	step [13/325], loss=3.9866
	step [14/325], loss=3.8316
	step [15/325], loss=4.7266
	step [16/325], loss=5.2457
	step [17/325], loss=4.4425
	step [18/325], loss=4.5252
	step [19/325], loss=4.3859
	step [20/325], loss=4.0384
	step [21/325], loss=4.6446
	step [22/325], loss=5.1097
	step [23/325], loss=4.6346
	step [24/325], loss=4.3177
	step [25/325], loss=4.0797
	step [26/325], loss=3.4826
	step [27/325], loss=5.0023
	step [28/325], loss=4.7503
	step [29/325], loss=4.4260
	step [30/325], loss=5.7725
	step [31/325], loss=5.6542
	step [32/325], loss=5.4201
	step [33/325], loss=3.8178
	step [34/325], loss=6.3008
	step [35/325], loss=4.8837
	step [36/325], loss=4.8173
	step [37/325], loss=4.6173
	step [38/325], loss=4.8250
	step [39/325], loss=5.0034
	step [40/325], loss=4.6104
	step [41/325], loss=4.7995
	step [42/325], loss=4.5456
	step [43/325], loss=5.1336
	step [44/325], loss=4.2374
	step [45/325], loss=4.5974
	step [46/325], loss=5.5252
	step [47/325], loss=4.7829
	step [48/325], loss=5.5061
	step [49/325], loss=4.1961
	step [50/325], loss=5.3225
	step [51/325], loss=3.9814
	step [52/325], loss=4.2324
	step [53/325], loss=4.2890
	step [54/325], loss=5.2355
	step [55/325], loss=4.7315
	step [56/325], loss=3.5892
	step [57/325], loss=4.2143
	step [58/325], loss=4.2943
	step [59/325], loss=4.8820
	step [60/325], loss=4.7959
	step [61/325], loss=4.6046
	step [62/325], loss=5.2147
	step [63/325], loss=5.0585
	step [64/325], loss=4.3701
	step [65/325], loss=5.3715
	step [66/325], loss=5.2439
	step [67/325], loss=5.4317
	step [68/325], loss=4.9152
	step [69/325], loss=4.8730
	step [70/325], loss=4.1613
	step [71/325], loss=4.4656
	step [72/325], loss=4.9623
	step [73/325], loss=4.8605
	step [74/325], loss=4.2605
	step [75/325], loss=4.0741
	step [76/325], loss=5.6099
	step [77/325], loss=5.0789
	step [78/325], loss=4.1090
	step [79/325], loss=5.0076
	step [80/325], loss=4.0024
	step [81/325], loss=4.7560
	step [82/325], loss=4.6254
	step [83/325], loss=5.2265
	step [84/325], loss=4.9931
	step [85/325], loss=5.5268
	step [86/325], loss=6.1601
	step [87/325], loss=3.7597
	step [88/325], loss=4.8962
	step [89/325], loss=4.6383
	step [90/325], loss=5.1289
	step [91/325], loss=4.5870
	step [92/325], loss=5.7986
	step [93/325], loss=3.8079
	step [94/325], loss=4.5410
	step [95/325], loss=5.0608
	step [96/325], loss=4.8317
	step [97/325], loss=5.9134
	step [98/325], loss=4.1163
	step [99/325], loss=5.0959
	step [100/325], loss=5.8169
	step [101/325], loss=5.1794
	step [102/325], loss=3.7779
	step [103/325], loss=4.2181
	step [104/325], loss=5.2656
	step [105/325], loss=3.7761
	step [106/325], loss=4.2077
	step [107/325], loss=4.2448
	step [108/325], loss=4.5293
	step [109/325], loss=4.0686
	step [110/325], loss=4.7081
	step [111/325], loss=4.1065
	step [112/325], loss=5.1321
	step [113/325], loss=4.5486
	step [114/325], loss=5.5861
	step [115/325], loss=4.3111
	step [116/325], loss=4.3896
	step [117/325], loss=4.5578
	step [118/325], loss=5.5854
	step [119/325], loss=5.3083
	step [120/325], loss=5.8714
	step [121/325], loss=4.3240
	step [122/325], loss=4.6125
	step [123/325], loss=3.8650
	step [124/325], loss=5.0945
	step [125/325], loss=4.4315
	step [126/325], loss=4.2893
	step [127/325], loss=5.3092
	step [128/325], loss=4.4560
	step [129/325], loss=4.9956
	step [130/325], loss=4.8694
	step [131/325], loss=4.9920
	step [132/325], loss=4.7876
	step [133/325], loss=4.2111
	step [134/325], loss=5.5749
	step [135/325], loss=3.9716
	step [136/325], loss=5.1673
	step [137/325], loss=4.6979
	step [138/325], loss=5.1716
	step [139/325], loss=5.2939
	step [140/325], loss=5.2207
	step [141/325], loss=2.9665
	step [142/325], loss=5.3172
	step [143/325], loss=4.8552
	step [144/325], loss=4.6010
	step [145/325], loss=4.2876
	step [146/325], loss=4.9417
	step [147/325], loss=4.9335
	step [148/325], loss=6.0205
	step [149/325], loss=4.8564
	step [150/325], loss=4.2910
	step [151/325], loss=3.3790
	step [152/325], loss=6.1298
	step [153/325], loss=4.4875
	step [154/325], loss=5.9951
	step [155/325], loss=4.4995
	step [156/325], loss=4.6334
	step [157/325], loss=4.6804
	step [158/325], loss=5.4040
	step [159/325], loss=4.3146
	step [160/325], loss=5.5171
	step [161/325], loss=3.6045
	step [162/325], loss=4.4075
	step [163/325], loss=5.3331
	step [164/325], loss=4.8581
	step [165/325], loss=5.7277
	step [166/325], loss=4.0995
	step [167/325], loss=4.3363
	step [168/325], loss=4.0719
	step [169/325], loss=5.1498
	step [170/325], loss=4.2069
	step [171/325], loss=5.1415
	step [172/325], loss=4.9694
	step [173/325], loss=6.1589
	step [174/325], loss=4.3985
	step [175/325], loss=4.9117
	step [176/325], loss=4.6526
	step [177/325], loss=4.5878
	step [178/325], loss=4.0929
	step [179/325], loss=5.5585
	step [180/325], loss=4.0815
	step [181/325], loss=3.7621
	step [182/325], loss=4.2567
	step [183/325], loss=4.7535
	step [184/325], loss=5.1183
	step [185/325], loss=4.8088
	step [186/325], loss=4.1340
	step [187/325], loss=5.6357
	step [188/325], loss=5.1450
	step [189/325], loss=5.8238
	step [190/325], loss=3.9562
	step [191/325], loss=4.7511
	step [192/325], loss=6.2426
	step [193/325], loss=3.9034
	step [194/325], loss=5.4683
	step [195/325], loss=4.7466
	step [196/325], loss=3.8244
	step [197/325], loss=4.1206
	step [198/325], loss=5.2745
	step [199/325], loss=5.4982
	step [200/325], loss=4.6377
	step [201/325], loss=4.2918
	step [202/325], loss=3.9738
	step [203/325], loss=4.8423
	step [204/325], loss=3.9832
	step [205/325], loss=5.4255
	step [206/325], loss=4.9032
	step [207/325], loss=6.0608
	step [208/325], loss=4.6246
	step [209/325], loss=4.5615
	step [210/325], loss=4.7308
	step [211/325], loss=4.0066
	step [212/325], loss=4.1391
	step [213/325], loss=4.5713
	step [214/325], loss=5.5437
	step [215/325], loss=4.4081
	step [216/325], loss=4.4627
	step [217/325], loss=5.2193
	step [218/325], loss=4.3494
	step [219/325], loss=4.2799
	step [220/325], loss=3.7286
	step [221/325], loss=5.4298
	step [222/325], loss=4.6469
	step [223/325], loss=5.3653
	step [224/325], loss=5.7602
	step [225/325], loss=4.1407
	step [226/325], loss=4.5245
	step [227/325], loss=5.2346
	step [228/325], loss=4.7378
	step [229/325], loss=4.2311
	step [230/325], loss=4.1211
	step [231/325], loss=5.3427
	step [232/325], loss=4.1794
	step [233/325], loss=4.9919
	step [234/325], loss=4.6921
	step [235/325], loss=4.1880
	step [236/325], loss=4.4101
	step [237/325], loss=4.3635
	step [238/325], loss=5.6172
	step [239/325], loss=4.6415
	step [240/325], loss=4.7215
	step [241/325], loss=4.1625
	step [242/325], loss=4.3536
	step [243/325], loss=4.5378
	step [244/325], loss=4.8650
	step [245/325], loss=3.6016
	step [246/325], loss=4.7811
	step [247/325], loss=3.8632
	step [248/325], loss=4.0219
	step [249/325], loss=4.4539
	step [250/325], loss=5.2780
	step [251/325], loss=4.2654
	step [252/325], loss=4.8313
	step [253/325], loss=4.3322
	step [254/325], loss=3.9985
	step [255/325], loss=3.7785
	step [256/325], loss=4.6024
	step [257/325], loss=4.8610
	step [258/325], loss=4.6757
	step [259/325], loss=6.8206
	step [260/325], loss=4.8497
	step [261/325], loss=4.3706
	step [262/325], loss=5.4489
	step [263/325], loss=4.1899
	step [264/325], loss=3.7206
	step [265/325], loss=3.8265
	step [266/325], loss=3.4657
	step [267/325], loss=4.9102
	step [268/325], loss=4.2433
	step [269/325], loss=5.6961
	step [270/325], loss=5.0239
	step [271/325], loss=5.1172
	step [272/325], loss=4.6796
	step [273/325], loss=4.6859
	step [274/325], loss=6.2206
	step [275/325], loss=5.1438
	step [276/325], loss=4.1717
	step [277/325], loss=6.3974
	step [278/325], loss=4.4385
	step [279/325], loss=4.4774
	step [280/325], loss=6.4263
	step [281/325], loss=4.9575
	step [282/325], loss=4.9505
	step [283/325], loss=4.0388
	step [284/325], loss=4.3191
	step [285/325], loss=4.8665
	step [286/325], loss=5.9167
	step [287/325], loss=5.5825
	step [288/325], loss=4.6633
	step [289/325], loss=4.9750
	step [290/325], loss=4.2319
	step [291/325], loss=4.8532
	step [292/325], loss=5.4492
	step [293/325], loss=5.1353
	step [294/325], loss=4.1084
	step [295/325], loss=6.5324
	step [296/325], loss=5.0214
	step [297/325], loss=4.6547
	step [298/325], loss=4.5072
	step [299/325], loss=5.5590
	step [300/325], loss=4.5179
	step [301/325], loss=4.4008
	step [302/325], loss=5.9344
	step [303/325], loss=4.3219
	step [304/325], loss=3.5538
	step [305/325], loss=4.2398
	step [306/325], loss=4.5715
	step [307/325], loss=4.8952
	step [308/325], loss=4.8223
	step [309/325], loss=4.1675
	step [310/325], loss=3.6928
	step [311/325], loss=4.6674
	step [312/325], loss=5.2490
	step [313/325], loss=4.9978
	step [314/325], loss=4.6014
	step [315/325], loss=3.9725
	step [316/325], loss=4.6277
	step [317/325], loss=4.9307
	step [318/325], loss=4.5222
	step [319/325], loss=5.4626
	step [320/325], loss=4.3388
	step [321/325], loss=5.1381
	step [322/325], loss=4.4196
	step [323/325], loss=3.8843
	step [324/325], loss=5.7524
	step [325/325], loss=2.2059
	Evaluating
	loss=0.0168, precision=0.2115, recall=0.9938, f1=0.3487
Training finished
best_f1: 0.3801867619951102
directing: Z rim_enhanced: False test_id 1
removed wrong scan: weights_Z_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_292_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_299_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_58_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_265_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_271_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_300_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_206_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_252_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_294_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_46_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_291_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_298_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_276_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_262_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_293_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_295_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_278_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_87_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_214_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_297_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_154_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_282_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_218_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_254_xwqg-B00034_2020-04-03.npy
# all image files: 15579 # all weight files in weight_dir: 12179 # image files with weight 12153
removed wrong scan: weights_Z_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_292_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_299_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_58_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_265_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_271_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_300_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_206_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_252_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_294_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_46_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_291_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_298_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_276_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_262_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_293_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_295_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_278_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_87_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_214_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_297_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_154_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_282_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_218_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_254_xwqg-B00034_2020-04-03.npy
# all image files: 15579 # all weight files in weight_dir: 3340 # image files with weight 3331
train: /ibex/scratch/projects/c2052/air_tube_seg/training_samples_enhanced/Z 12153
Using 4 GPUs
Going to train epochs [1-30]
Training epoch 1
	step [1/254], loss=196.4518
	step [2/254], loss=150.4855
	step [3/254], loss=148.4719
	step [4/254], loss=143.2092
	step [5/254], loss=139.8187
	step [6/254], loss=138.2139
	step [7/254], loss=135.8221
	step [8/254], loss=133.8956
	step [9/254], loss=132.7216
	step [10/254], loss=129.7542
	step [11/254], loss=127.4712
	step [12/254], loss=124.1637
	step [13/254], loss=123.5028
	step [14/254], loss=121.9118
	step [15/254], loss=119.3883
	step [16/254], loss=120.1531
	step [17/254], loss=118.6093
	step [18/254], loss=115.6710
	step [19/254], loss=114.4523
	step [20/254], loss=113.3105
	step [21/254], loss=111.9059
	step [22/254], loss=111.3612
	step [23/254], loss=109.3576
	step [24/254], loss=109.3621
	step [25/254], loss=106.8206
	step [26/254], loss=106.5390
	step [27/254], loss=105.1422
	step [28/254], loss=103.6276
	step [29/254], loss=102.3339
	step [30/254], loss=103.0826
	step [31/254], loss=101.2488
	step [32/254], loss=101.9807
	step [33/254], loss=99.5809
	step [34/254], loss=99.6348
	step [35/254], loss=100.2117
	step [36/254], loss=97.9477
	step [37/254], loss=96.7358
	step [38/254], loss=95.2834
	step [39/254], loss=95.7733
	step [40/254], loss=94.9310
	step [41/254], loss=93.0806
	step [42/254], loss=92.0776
	step [43/254], loss=92.6141
	step [44/254], loss=92.0559
	step [45/254], loss=90.0146
	step [46/254], loss=89.7614
	step [47/254], loss=88.4805
	step [48/254], loss=88.3688
	step [49/254], loss=87.9768
	step [50/254], loss=87.3326
	step [51/254], loss=85.8090
	step [52/254], loss=86.5423
	step [53/254], loss=85.3598
	step [54/254], loss=86.8509
	step [55/254], loss=85.3251
	step [56/254], loss=84.4666
	step [57/254], loss=84.5445
	step [58/254], loss=82.3899
	step [59/254], loss=82.8972
	step [60/254], loss=84.5109
	step [61/254], loss=83.6009
	step [62/254], loss=83.6034
	step [63/254], loss=81.4145
	step [64/254], loss=81.2349
	step [65/254], loss=80.8619
	step [66/254], loss=81.6442
	step [67/254], loss=79.0414
	step [68/254], loss=80.1868
	step [69/254], loss=82.9389
	step [70/254], loss=79.3291
	step [71/254], loss=81.3816
	step [72/254], loss=78.6535
	step [73/254], loss=79.8494
	step [74/254], loss=78.1049
	step [75/254], loss=77.9898
	step [76/254], loss=77.8927
	step [77/254], loss=76.8310
	step [78/254], loss=75.8392
	step [79/254], loss=77.3849
	step [80/254], loss=77.7572
	step [81/254], loss=78.1413
	step [82/254], loss=77.1640
	step [83/254], loss=78.6569
	step [84/254], loss=76.2424
	step [85/254], loss=74.7008
	step [86/254], loss=74.8321
	step [87/254], loss=74.7438
	step [88/254], loss=75.3772
	step [89/254], loss=75.7754
	step [90/254], loss=72.9889
	step [91/254], loss=72.8923
	step [92/254], loss=73.7540
	step [93/254], loss=73.9685
	step [94/254], loss=74.4322
	step [95/254], loss=73.8464
	step [96/254], loss=74.0530
	step [97/254], loss=72.3078
	step [98/254], loss=73.0162
	step [99/254], loss=72.6508
	step [100/254], loss=73.3672
	step [101/254], loss=72.6778
	step [102/254], loss=74.0835
	step [103/254], loss=70.7761
	step [104/254], loss=72.2220
	step [105/254], loss=71.9887
	step [106/254], loss=71.1887
	step [107/254], loss=71.2223
	step [108/254], loss=70.6093
	step [109/254], loss=69.7779
	step [110/254], loss=70.1874
	step [111/254], loss=68.5671
	step [112/254], loss=71.0300
	step [113/254], loss=68.6035
	step [114/254], loss=69.5220
	step [115/254], loss=69.7431
	step [116/254], loss=70.6052
	step [117/254], loss=70.0358
	step [118/254], loss=68.9243
	step [119/254], loss=69.5202
	step [120/254], loss=68.5553
	step [121/254], loss=70.2786
	step [122/254], loss=67.6744
	step [123/254], loss=69.4427
	step [124/254], loss=69.8974
	step [125/254], loss=69.9535
	step [126/254], loss=69.8519
	step [127/254], loss=70.8075
	step [128/254], loss=68.6665
	step [129/254], loss=67.0266
	step [130/254], loss=68.1624
	step [131/254], loss=66.3727
	step [132/254], loss=66.8468
	step [133/254], loss=67.8605
	step [134/254], loss=66.5430
	step [135/254], loss=67.0923
	step [136/254], loss=66.5083
	step [137/254], loss=64.7209
	step [138/254], loss=65.5182
	step [139/254], loss=65.8718
	step [140/254], loss=66.7595
	step [141/254], loss=65.1878
	step [142/254], loss=67.2187
	step [143/254], loss=66.7489
	step [144/254], loss=65.1695
	step [145/254], loss=65.0802
	step [146/254], loss=66.5797
	step [147/254], loss=66.7295
	step [148/254], loss=64.4859
	step [149/254], loss=64.3522
	step [150/254], loss=65.1572
	step [151/254], loss=63.9770
	step [152/254], loss=64.1835
	step [153/254], loss=65.6101
	step [154/254], loss=63.9003
	step [155/254], loss=66.2809
	step [156/254], loss=64.0025
	step [157/254], loss=63.7930
	step [158/254], loss=64.2621
	step [159/254], loss=64.0031
	step [160/254], loss=64.7898
	step [161/254], loss=64.6995
	step [162/254], loss=64.4322
	step [163/254], loss=63.2363
	step [164/254], loss=64.0243
	step [165/254], loss=62.7396
	step [166/254], loss=63.8534
	step [167/254], loss=64.2214
	step [168/254], loss=62.7568
	step [169/254], loss=62.7039
	step [170/254], loss=63.5827
	step [171/254], loss=64.8931
	step [172/254], loss=63.6733
	step [173/254], loss=60.9182
	step [174/254], loss=63.4003
	step [175/254], loss=61.2050
	step [176/254], loss=62.3375
	step [177/254], loss=61.9603
	step [178/254], loss=62.3866
	step [179/254], loss=61.6798
	step [180/254], loss=61.7005
	step [181/254], loss=61.0470
	step [182/254], loss=60.6761
	step [183/254], loss=62.0582
	step [184/254], loss=60.5323
	step [185/254], loss=61.8609
	step [186/254], loss=59.2031
	step [187/254], loss=61.6295
	step [188/254], loss=60.4287
	step [189/254], loss=61.1458
	step [190/254], loss=63.2728
	step [191/254], loss=61.3980
	step [192/254], loss=61.2479
	step [193/254], loss=61.5225
	step [194/254], loss=59.7488
	step [195/254], loss=59.7607
	step [196/254], loss=61.0733
	step [197/254], loss=61.3020
	step [198/254], loss=57.5837
	step [199/254], loss=59.3951
	step [200/254], loss=59.6502
	step [201/254], loss=58.5180
	step [202/254], loss=58.4663
	step [203/254], loss=58.6148
	step [204/254], loss=61.4906
	step [205/254], loss=59.0123
	step [206/254], loss=60.0169
	step [207/254], loss=58.1650
	step [208/254], loss=59.6895
	step [209/254], loss=59.1289
	step [210/254], loss=59.4351
	step [211/254], loss=58.0044
	step [212/254], loss=58.1038
	step [213/254], loss=56.3298
	step [214/254], loss=60.9236
	step [215/254], loss=56.9442
	step [216/254], loss=58.2372
	step [217/254], loss=57.2415
	step [218/254], loss=58.4900
	step [219/254], loss=59.3672
	step [220/254], loss=58.1164
	step [221/254], loss=57.2284
	step [222/254], loss=57.6568
	step [223/254], loss=56.0896
	step [224/254], loss=58.1074
	step [225/254], loss=57.2101
	step [226/254], loss=58.5069
	step [227/254], loss=57.4093
	step [228/254], loss=56.6844
	step [229/254], loss=55.3924
	step [230/254], loss=57.5310
	step [231/254], loss=55.5149
	step [232/254], loss=56.0408
	step [233/254], loss=58.1033
	step [234/254], loss=55.0482
	step [235/254], loss=54.3565
	step [236/254], loss=56.9470
	step [237/254], loss=55.7907
	step [238/254], loss=56.8671
	step [239/254], loss=55.4130
	step [240/254], loss=55.7029
	step [241/254], loss=55.6792
	step [242/254], loss=54.6644
	step [243/254], loss=56.4335
	step [244/254], loss=54.9779
	step [245/254], loss=57.0536
	step [246/254], loss=56.0113
	step [247/254], loss=54.6714
	step [248/254], loss=54.5327
	step [249/254], loss=54.3867
	step [250/254], loss=56.2040
	step [251/254], loss=56.8550
	step [252/254], loss=56.5386
	step [253/254], loss=53.6468
	step [254/254], loss=10.6468
	Evaluating
	loss=0.2728, precision=0.1692, recall=0.9953, f1=0.2893
saving model as: 1_saved_model.pth
Training epoch 2
	step [1/254], loss=54.5868
	step [2/254], loss=54.7795
	step [3/254], loss=55.0825
	step [4/254], loss=54.7103
	step [5/254], loss=53.8848
	step [6/254], loss=52.9919
	step [7/254], loss=54.6542
	step [8/254], loss=52.9283
	step [9/254], loss=53.3159
	step [10/254], loss=54.8269
	step [11/254], loss=52.1520
	step [12/254], loss=53.5488
	step [13/254], loss=55.6348
	step [14/254], loss=54.1742
	step [15/254], loss=53.1605
	step [16/254], loss=53.6170
	step [17/254], loss=52.5452
	step [18/254], loss=53.5070
	step [19/254], loss=53.4777
	step [20/254], loss=52.5128
	step [21/254], loss=53.8165
	step [22/254], loss=52.4692
	step [23/254], loss=52.0786
	step [24/254], loss=55.2348
	step [25/254], loss=53.2595
	step [26/254], loss=51.7330
	step [27/254], loss=52.1895
	step [28/254], loss=52.5106
	step [29/254], loss=54.1802
	step [30/254], loss=51.0615
	step [31/254], loss=51.3639
	step [32/254], loss=50.8086
	step [33/254], loss=50.6969
	step [34/254], loss=52.9037
	step [35/254], loss=52.6958
	step [36/254], loss=52.1566
	step [37/254], loss=50.7947
	step [38/254], loss=50.9025
	step [39/254], loss=52.3955
	step [40/254], loss=52.1443
	step [41/254], loss=51.1521
	step [42/254], loss=50.7911
	step [43/254], loss=49.9909
	step [44/254], loss=51.4464
	step [45/254], loss=51.9547
	step [46/254], loss=50.7284
	step [47/254], loss=51.8206
	step [48/254], loss=50.9908
	step [49/254], loss=51.6521
	step [50/254], loss=49.9929
	step [51/254], loss=49.7088
	step [52/254], loss=51.6803
	step [53/254], loss=51.3234
	step [54/254], loss=50.7772
	step [55/254], loss=48.7100
	step [56/254], loss=48.0226
	step [57/254], loss=51.9089
	step [58/254], loss=50.2944
	step [59/254], loss=49.1544
	step [60/254], loss=50.0863
	step [61/254], loss=50.8683
	step [62/254], loss=50.1160
	step [63/254], loss=48.2216
	step [64/254], loss=48.7121
	step [65/254], loss=49.1370
	step [66/254], loss=48.7158
	step [67/254], loss=51.6423
	step [68/254], loss=48.9651
	step [69/254], loss=49.6461
	step [70/254], loss=49.8953
	step [71/254], loss=49.4128
	step [72/254], loss=49.3184
	step [73/254], loss=48.6638
	step [74/254], loss=49.7618
	step [75/254], loss=48.7791
	step [76/254], loss=47.1305
	step [77/254], loss=49.6668
	step [78/254], loss=49.7141
	step [79/254], loss=47.0403
	step [80/254], loss=48.9192
	step [81/254], loss=49.6688
	step [82/254], loss=50.5513
	step [83/254], loss=47.9649
	step [84/254], loss=46.2368
	step [85/254], loss=46.2695
	step [86/254], loss=47.0519
	step [87/254], loss=48.3401
	step [88/254], loss=47.7011
	step [89/254], loss=49.0018
	step [90/254], loss=48.8287
	step [91/254], loss=46.7873
	step [92/254], loss=46.5310
	step [93/254], loss=46.6704
	step [94/254], loss=48.5567
	step [95/254], loss=45.9877
	step [96/254], loss=46.9902
	step [97/254], loss=46.3816
	step [98/254], loss=46.8524
	step [99/254], loss=46.5193
	step [100/254], loss=46.7298
	step [101/254], loss=48.3600
	step [102/254], loss=47.5060
	step [103/254], loss=48.6847
	step [104/254], loss=46.1004
	step [105/254], loss=46.1167
	step [106/254], loss=45.2061
	step [107/254], loss=45.7557
	step [108/254], loss=45.4405
	step [109/254], loss=44.9953
	step [110/254], loss=47.4052
	step [111/254], loss=45.3181
	step [112/254], loss=44.4147
	step [113/254], loss=47.1953
	step [114/254], loss=45.6204
	step [115/254], loss=45.3829
	step [116/254], loss=44.5166
	step [117/254], loss=44.5973
	step [118/254], loss=46.6938
	step [119/254], loss=45.1147
	step [120/254], loss=44.8910
	step [121/254], loss=45.5034
	step [122/254], loss=45.3478
	step [123/254], loss=44.9978
	step [124/254], loss=43.4057
	step [125/254], loss=46.0608
	step [126/254], loss=46.8322
	step [127/254], loss=44.4197
	step [128/254], loss=43.8632
	step [129/254], loss=45.2651
	step [130/254], loss=43.4439
	step [131/254], loss=43.9099
	step [132/254], loss=44.3605
	step [133/254], loss=44.0354
	step [134/254], loss=45.3133
	step [135/254], loss=44.0516
	step [136/254], loss=45.2960
	step [137/254], loss=43.1081
	step [138/254], loss=44.0023
	step [139/254], loss=43.8791
	step [140/254], loss=44.5411
	step [141/254], loss=43.9340
	step [142/254], loss=43.0204
	step [143/254], loss=44.9319
	step [144/254], loss=44.5221
	step [145/254], loss=43.0869
	step [146/254], loss=43.1961
	step [147/254], loss=43.5156
	step [148/254], loss=43.0766
	step [149/254], loss=41.7465
	step [150/254], loss=43.8785
	step [151/254], loss=43.7630
	step [152/254], loss=44.6860
	step [153/254], loss=44.9305
	step [154/254], loss=42.4520
	step [155/254], loss=41.7869
	step [156/254], loss=41.5942
	step [157/254], loss=43.0081
	step [158/254], loss=40.3725
	step [159/254], loss=42.3672
	step [160/254], loss=42.1414
	step [161/254], loss=40.7910
	step [162/254], loss=42.5557
	step [163/254], loss=44.0111
	step [164/254], loss=44.9117
	step [165/254], loss=41.8880
	step [166/254], loss=42.3808
	step [167/254], loss=41.3300
	step [168/254], loss=41.9162
	step [169/254], loss=41.2936
	step [170/254], loss=42.0162
	step [171/254], loss=41.5780
	step [172/254], loss=41.1244
	step [173/254], loss=40.5993
	step [174/254], loss=42.7171
	step [175/254], loss=41.4349
	step [176/254], loss=42.3156
	step [177/254], loss=41.6644
	step [178/254], loss=40.9293
	step [179/254], loss=41.4840
	step [180/254], loss=40.8695
	step [181/254], loss=40.1727
	step [182/254], loss=41.5095
	step [183/254], loss=41.5962
	step [184/254], loss=39.6369
	step [185/254], loss=41.6928
	step [186/254], loss=40.9374
	step [187/254], loss=41.1887
	step [188/254], loss=39.4443
	step [189/254], loss=39.6864
	step [190/254], loss=39.9188
	step [191/254], loss=39.9895
	step [192/254], loss=42.6131
	step [193/254], loss=39.3095
	step [194/254], loss=39.5683
	step [195/254], loss=39.6655
	step [196/254], loss=39.7718
	step [197/254], loss=39.9224
	step [198/254], loss=39.5192
	step [199/254], loss=38.7316
	step [200/254], loss=39.4112
	step [201/254], loss=40.8169
	step [202/254], loss=41.2693
	step [203/254], loss=40.7310
	step [204/254], loss=42.3566
	step [205/254], loss=38.8960
	step [206/254], loss=40.9897
	step [207/254], loss=40.2460
	step [208/254], loss=39.6667
	step [209/254], loss=40.0989
	step [210/254], loss=39.4090
	step [211/254], loss=38.7415
	step [212/254], loss=38.8565
	step [213/254], loss=39.7982
	step [214/254], loss=39.4466
	step [215/254], loss=40.1128
	step [216/254], loss=39.9981
	step [217/254], loss=39.5103
	step [218/254], loss=39.4668
	step [219/254], loss=38.4929
	step [220/254], loss=37.9593
	step [221/254], loss=40.4990
	step [222/254], loss=38.5342
	step [223/254], loss=39.5019
	step [224/254], loss=39.1157
	step [225/254], loss=38.4876
	step [226/254], loss=38.4889
	step [227/254], loss=39.8051
	step [228/254], loss=38.5722
	step [229/254], loss=37.9215
	step [230/254], loss=37.7829
	step [231/254], loss=36.5039
	step [232/254], loss=36.7621
	step [233/254], loss=38.1273
	step [234/254], loss=38.6456
	step [235/254], loss=38.4580
	step [236/254], loss=37.5923
	step [237/254], loss=38.4878
	step [238/254], loss=36.2812
	step [239/254], loss=40.3401
	step [240/254], loss=39.2999
	step [241/254], loss=36.4212
	step [242/254], loss=37.2744
	step [243/254], loss=37.9912
	step [244/254], loss=36.0906
	step [245/254], loss=38.1058
	step [246/254], loss=37.1775
	step [247/254], loss=38.6438
	step [248/254], loss=38.2553
	step [249/254], loss=36.4640
	step [250/254], loss=37.4362
	step [251/254], loss=37.2290
	step [252/254], loss=36.6318
	step [253/254], loss=37.7107
	step [254/254], loss=7.3515
	Evaluating
	loss=0.1813, precision=0.1979, recall=0.9945, f1=0.3301
saving model as: 1_saved_model.pth
Training epoch 3
	step [1/254], loss=36.9395
	step [2/254], loss=35.7301
	step [3/254], loss=36.3499
	step [4/254], loss=37.0862
	step [5/254], loss=37.1940
	step [6/254], loss=40.1233
	step [7/254], loss=36.2628
	step [8/254], loss=36.5430
	step [9/254], loss=36.2859
	step [10/254], loss=35.8674
	step [11/254], loss=36.3941
	step [12/254], loss=35.8448
	step [13/254], loss=36.3161
	step [14/254], loss=36.1129
	step [15/254], loss=37.2594
	step [16/254], loss=37.3804
	step [17/254], loss=35.4405
	step [18/254], loss=36.2987
	step [19/254], loss=36.0882
	step [20/254], loss=33.8602
	step [21/254], loss=37.1618
	step [22/254], loss=35.1853
	step [23/254], loss=35.9974
	step [24/254], loss=36.9687
	step [25/254], loss=37.5991
	step [26/254], loss=35.6271
	step [27/254], loss=35.9244
	step [28/254], loss=35.7008
	step [29/254], loss=36.1162
	step [30/254], loss=36.3638
	step [31/254], loss=35.6319
	step [32/254], loss=36.9763
	step [33/254], loss=35.1259
	step [34/254], loss=35.9604
	step [35/254], loss=37.4482
	step [36/254], loss=35.4576
	step [37/254], loss=34.6996
	step [38/254], loss=33.4947
	step [39/254], loss=36.3909
	step [40/254], loss=35.4980
	step [41/254], loss=35.9380
	step [42/254], loss=35.8611
	step [43/254], loss=36.2515
	step [44/254], loss=35.9153
	step [45/254], loss=35.4795
	step [46/254], loss=33.7406
	step [47/254], loss=34.4208
	step [48/254], loss=33.0923
	step [49/254], loss=34.6653
	step [50/254], loss=34.5335
	step [51/254], loss=37.3795
	step [52/254], loss=34.8984
	step [53/254], loss=32.9197
	step [54/254], loss=33.5621
	step [55/254], loss=35.1649
	step [56/254], loss=35.0289
	step [57/254], loss=34.4498
	step [58/254], loss=34.7597
	step [59/254], loss=34.0720
	step [60/254], loss=33.6403
	step [61/254], loss=34.6895
	step [62/254], loss=34.3855
	step [63/254], loss=33.3114
	step [64/254], loss=35.3662
	step [65/254], loss=34.7579
	step [66/254], loss=34.6065
	step [67/254], loss=33.9797
	step [68/254], loss=32.2728
	step [69/254], loss=35.7040
	step [70/254], loss=33.3804
	step [71/254], loss=31.9605
	step [72/254], loss=32.9734
	step [73/254], loss=32.2169
	step [74/254], loss=35.1735
	step [75/254], loss=34.6410
	step [76/254], loss=34.3516
	step [77/254], loss=33.7800
	step [78/254], loss=33.4390
	step [79/254], loss=34.2919
	step [80/254], loss=32.8917
	step [81/254], loss=33.2497
	step [82/254], loss=33.8823
	step [83/254], loss=33.4524
	step [84/254], loss=33.0612
	step [85/254], loss=33.0259
	step [86/254], loss=31.8540
	step [87/254], loss=32.3776
	step [88/254], loss=32.6442
	step [89/254], loss=32.9628
	step [90/254], loss=34.1696
	step [91/254], loss=33.8656
	step [92/254], loss=32.9718
	step [93/254], loss=33.2475
	step [94/254], loss=33.4923
	step [95/254], loss=33.2040
	step [96/254], loss=31.2556
	step [97/254], loss=32.4787
	step [98/254], loss=30.6268
	step [99/254], loss=32.3298
	step [100/254], loss=33.0415
	step [101/254], loss=33.0809
	step [102/254], loss=32.6963
	step [103/254], loss=31.8925
	step [104/254], loss=32.4279
	step [105/254], loss=30.3507
	step [106/254], loss=31.5184
	step [107/254], loss=33.5466
	step [108/254], loss=32.2666
	step [109/254], loss=32.6417
	step [110/254], loss=33.4341
	step [111/254], loss=30.8275
	step [112/254], loss=30.0310
	step [113/254], loss=31.4494
	step [114/254], loss=32.5131
	step [115/254], loss=31.1156
	step [116/254], loss=31.3876
	step [117/254], loss=33.5039
	step [118/254], loss=31.6165
	step [119/254], loss=31.1700
	step [120/254], loss=32.7083
	step [121/254], loss=29.5896
	step [122/254], loss=30.1471
	step [123/254], loss=30.9624
	step [124/254], loss=31.1366
	step [125/254], loss=31.6695
	step [126/254], loss=31.2365
	step [127/254], loss=31.0470
	step [128/254], loss=31.4364
	step [129/254], loss=31.6491
	step [130/254], loss=31.7833
	step [131/254], loss=30.1186
	step [132/254], loss=30.4296
	step [133/254], loss=30.6199
	step [134/254], loss=32.0382
	step [135/254], loss=30.5159
	step [136/254], loss=29.7170
	step [137/254], loss=29.5723
	step [138/254], loss=30.0698
	step [139/254], loss=31.9070
	step [140/254], loss=30.0628
	step [141/254], loss=30.1531
	step [142/254], loss=30.6202
	step [143/254], loss=29.6711
	step [144/254], loss=29.5930
	step [145/254], loss=31.5844
	step [146/254], loss=29.3520
	step [147/254], loss=30.6286
	step [148/254], loss=29.4058
	step [149/254], loss=29.4382
	step [150/254], loss=28.1166
	step [151/254], loss=29.0890
	step [152/254], loss=28.3019
	step [153/254], loss=30.1048
	step [154/254], loss=29.0376
	step [155/254], loss=27.4882
	step [156/254], loss=27.5811
	step [157/254], loss=29.4763
	step [158/254], loss=28.3245
	step [159/254], loss=29.3314
	step [160/254], loss=31.3291
	step [161/254], loss=29.2937
	step [162/254], loss=30.5846
	step [163/254], loss=29.3115
	step [164/254], loss=28.8142
	step [165/254], loss=28.4295
	step [166/254], loss=27.6269
	step [167/254], loss=28.0975
	step [168/254], loss=31.8236
	step [169/254], loss=29.2564
	step [170/254], loss=29.5139
	step [171/254], loss=27.9672
	step [172/254], loss=31.6866
	step [173/254], loss=28.6525
	step [174/254], loss=28.6704
	step [175/254], loss=28.9104
	step [176/254], loss=31.2570
	step [177/254], loss=29.7163
	step [178/254], loss=27.9372
	step [179/254], loss=29.3762
	step [180/254], loss=29.2288
	step [181/254], loss=29.5551
	step [182/254], loss=29.2333
	step [183/254], loss=29.0979
	step [184/254], loss=29.6454
	step [185/254], loss=28.0584
	step [186/254], loss=27.8091
	step [187/254], loss=28.0588
	step [188/254], loss=30.1161
	step [189/254], loss=28.1783
	step [190/254], loss=28.2185
	step [191/254], loss=28.7495
	step [192/254], loss=26.1924
	step [193/254], loss=27.2300
	step [194/254], loss=27.9815
	step [195/254], loss=26.6947
	step [196/254], loss=27.2174
	step [197/254], loss=27.2367
	step [198/254], loss=28.3535
	step [199/254], loss=27.3134
	step [200/254], loss=28.0366
	step [201/254], loss=28.1038
	step [202/254], loss=27.3695
	step [203/254], loss=27.0709
	step [204/254], loss=27.5461
	step [205/254], loss=27.8294
	step [206/254], loss=27.3392
	step [207/254], loss=27.3718
	step [208/254], loss=28.6939
	step [209/254], loss=26.7743
	step [210/254], loss=29.0240
	step [211/254], loss=26.5957
	step [212/254], loss=29.0261
	step [213/254], loss=27.5545
	step [214/254], loss=27.9377
	step [215/254], loss=26.7763
	step [216/254], loss=26.7513
	step [217/254], loss=27.0257
	step [218/254], loss=27.3247
	step [219/254], loss=26.8992
	step [220/254], loss=26.7272
	step [221/254], loss=27.0528
	step [222/254], loss=25.7183
	step [223/254], loss=28.2410
	step [224/254], loss=26.9619
	step [225/254], loss=27.8243
	step [226/254], loss=25.8371
	step [227/254], loss=26.0622
	step [228/254], loss=27.0459
	step [229/254], loss=27.2852
	step [230/254], loss=26.8526
	step [231/254], loss=26.1056
	step [232/254], loss=26.2839
	step [233/254], loss=26.2175
	step [234/254], loss=25.1281
	step [235/254], loss=26.7142
	step [236/254], loss=25.7246
	step [237/254], loss=27.4617
	step [238/254], loss=25.0520
	step [239/254], loss=26.5848
	step [240/254], loss=24.9856
	step [241/254], loss=26.2896
	step [242/254], loss=25.7829
	step [243/254], loss=25.5695
	step [244/254], loss=25.9373
	step [245/254], loss=25.7484
	step [246/254], loss=25.5084
	step [247/254], loss=27.3672
	step [248/254], loss=26.5770
	step [249/254], loss=26.7922
	step [250/254], loss=26.4546
	step [251/254], loss=25.5466
	step [252/254], loss=26.8033
	step [253/254], loss=25.2441
	step [254/254], loss=4.9211
	Evaluating
	loss=0.1227, precision=0.2056, recall=0.9942, f1=0.3407
saving model as: 1_saved_model.pth
Training epoch 4
	step [1/254], loss=24.2807
	step [2/254], loss=27.6543
	step [3/254], loss=24.6112
	step [4/254], loss=24.2521
	step [5/254], loss=26.5494
	step [6/254], loss=27.1451
	step [7/254], loss=25.7123
	step [8/254], loss=27.0468
	step [9/254], loss=25.4129
	step [10/254], loss=24.1149
	step [11/254], loss=24.4141
	step [12/254], loss=23.1961
	step [13/254], loss=24.2525
	step [14/254], loss=24.4453
	step [15/254], loss=23.8550
	step [16/254], loss=26.3459
	step [17/254], loss=25.6647
	step [18/254], loss=24.3422
	step [19/254], loss=25.7286
	step [20/254], loss=24.2244
	step [21/254], loss=24.5317
	step [22/254], loss=24.7923
	step [23/254], loss=25.7890
	step [24/254], loss=25.5182
	step [25/254], loss=25.9794
	step [26/254], loss=24.9420
	step [27/254], loss=24.9971
	step [28/254], loss=24.8470
	step [29/254], loss=25.4893
	step [30/254], loss=26.2779
	step [31/254], loss=24.6035
	step [32/254], loss=23.6116
	step [33/254], loss=26.2437
	step [34/254], loss=22.6616
	step [35/254], loss=24.4383
	step [36/254], loss=23.7928
	step [37/254], loss=27.0413
	step [38/254], loss=23.9976
	step [39/254], loss=26.0949
	step [40/254], loss=22.9141
	step [41/254], loss=24.7390
	step [42/254], loss=24.5602
	step [43/254], loss=23.1201
	step [44/254], loss=24.0882
	step [45/254], loss=25.4359
	step [46/254], loss=24.0695
	step [47/254], loss=23.4497
	step [48/254], loss=24.3827
	step [49/254], loss=23.5676
	step [50/254], loss=25.4349
	step [51/254], loss=25.2482
	step [52/254], loss=24.2564
	step [53/254], loss=24.5971
	step [54/254], loss=23.5634
	step [55/254], loss=23.4761
	step [56/254], loss=23.7995
	step [57/254], loss=22.6589
	step [58/254], loss=24.0117
	step [59/254], loss=24.1232
	step [60/254], loss=24.0979
	step [61/254], loss=23.4311
	step [62/254], loss=22.4413
	step [63/254], loss=25.0175
	step [64/254], loss=22.2069
	step [65/254], loss=23.7170
	step [66/254], loss=23.1137
	step [67/254], loss=22.6856
	step [68/254], loss=21.7014
	step [69/254], loss=23.6438
	step [70/254], loss=22.7944
	step [71/254], loss=24.2856
	step [72/254], loss=22.8952
	step [73/254], loss=22.2008
	step [74/254], loss=22.0542
	step [75/254], loss=22.9357
	step [76/254], loss=23.2678
	step [77/254], loss=22.5127
	step [78/254], loss=21.9451
	step [79/254], loss=23.3833
	step [80/254], loss=22.4889
	step [81/254], loss=25.0915
	step [82/254], loss=24.5972
	step [83/254], loss=24.2020
	step [84/254], loss=23.1333
	step [85/254], loss=24.2254
	step [86/254], loss=22.7762
	step [87/254], loss=23.0837
	step [88/254], loss=24.3874
	step [89/254], loss=23.9878
	step [90/254], loss=23.9675
	step [91/254], loss=23.2007
	step [92/254], loss=22.1775
	step [93/254], loss=25.3437
	step [94/254], loss=22.5515
	step [95/254], loss=21.8320
	step [96/254], loss=22.9285
	step [97/254], loss=22.0897
	step [98/254], loss=21.6356
	step [99/254], loss=21.5177
	step [100/254], loss=21.6316
	step [101/254], loss=22.1547
	step [102/254], loss=22.7301
	step [103/254], loss=23.8729
	step [104/254], loss=21.5932
	step [105/254], loss=23.1391
	step [106/254], loss=23.1094
	step [107/254], loss=20.4398
	step [108/254], loss=24.9026
	step [109/254], loss=21.4346
	step [110/254], loss=22.6414
	step [111/254], loss=23.9360
	step [112/254], loss=22.4403
	step [113/254], loss=19.9932
	step [114/254], loss=22.4132
	step [115/254], loss=21.4765
	step [116/254], loss=21.0317
	step [117/254], loss=20.7493
	step [118/254], loss=21.7990
	step [119/254], loss=22.6257
	step [120/254], loss=23.1788
	step [121/254], loss=22.0004
	step [122/254], loss=22.7621
	step [123/254], loss=20.5833
	step [124/254], loss=22.1759
	step [125/254], loss=21.8548
	step [126/254], loss=24.4588
	step [127/254], loss=21.9660
	step [128/254], loss=22.2427
	step [129/254], loss=20.5538
	step [130/254], loss=21.7313
	step [131/254], loss=22.2734
	step [132/254], loss=20.9926
	step [133/254], loss=21.4912
	step [134/254], loss=21.3820
	step [135/254], loss=23.8821
	step [136/254], loss=21.5490
	step [137/254], loss=21.6576
	step [138/254], loss=21.3104
	step [139/254], loss=22.3868
	step [140/254], loss=20.7423
	step [141/254], loss=20.9026
	step [142/254], loss=22.1337
	step [143/254], loss=21.8589
	step [144/254], loss=21.4060
	step [145/254], loss=20.4380
	step [146/254], loss=21.3299
	step [147/254], loss=20.7332
	step [148/254], loss=20.7401
	step [149/254], loss=21.1324
	step [150/254], loss=21.0982
	step [151/254], loss=20.8496
	step [152/254], loss=20.4738
	step [153/254], loss=21.0219
	step [154/254], loss=20.9591
	step [155/254], loss=21.1590
	step [156/254], loss=20.8079
	step [157/254], loss=21.7402
	step [158/254], loss=21.0212
	step [159/254], loss=23.1066
	step [160/254], loss=19.7114
	step [161/254], loss=21.6965
	step [162/254], loss=19.9172
	step [163/254], loss=19.9088
	step [164/254], loss=19.8692
	step [165/254], loss=20.6714
	step [166/254], loss=20.0111
	step [167/254], loss=20.9285
	step [168/254], loss=21.5838
	step [169/254], loss=19.7183
	step [170/254], loss=20.4013
	step [171/254], loss=19.1224
	step [172/254], loss=21.3144
	step [173/254], loss=19.9344
	step [174/254], loss=20.7086
	step [175/254], loss=18.4604
	step [176/254], loss=20.7194
	step [177/254], loss=21.8858
	step [178/254], loss=21.2435
	step [179/254], loss=21.2768
	step [180/254], loss=19.1112
	step [181/254], loss=21.6410
	step [182/254], loss=21.1170
	step [183/254], loss=19.6802
	step [184/254], loss=20.4275
	step [185/254], loss=19.9714
	step [186/254], loss=20.9383
	step [187/254], loss=20.9812
	step [188/254], loss=21.2966
	step [189/254], loss=19.4712
	step [190/254], loss=23.5180
	step [191/254], loss=20.1579
	step [192/254], loss=21.4047
	step [193/254], loss=21.2530
	step [194/254], loss=19.7486
	step [195/254], loss=21.3352
	step [196/254], loss=21.7619
	step [197/254], loss=19.2327
	step [198/254], loss=20.4423
	step [199/254], loss=20.5843
	step [200/254], loss=21.7731
	step [201/254], loss=23.6999
	step [202/254], loss=18.8085
	step [203/254], loss=18.8940
	step [204/254], loss=19.9699
	step [205/254], loss=21.8570
	step [206/254], loss=20.6283
	step [207/254], loss=19.6788
	step [208/254], loss=20.2396
	step [209/254], loss=18.6673
	step [210/254], loss=18.5372
	step [211/254], loss=20.2883
	step [212/254], loss=18.7872
	step [213/254], loss=21.0545
	step [214/254], loss=18.7730
	step [215/254], loss=21.6732
	step [216/254], loss=19.9895
	step [217/254], loss=19.2837
	step [218/254], loss=20.5948
	step [219/254], loss=19.2339
	step [220/254], loss=21.3213
	step [221/254], loss=21.8327
	step [222/254], loss=19.0019
	step [223/254], loss=19.8565
	step [224/254], loss=19.0659
	step [225/254], loss=19.5544
	step [226/254], loss=20.0145
	step [227/254], loss=20.0449
	step [228/254], loss=19.7593
	step [229/254], loss=18.2926
	step [230/254], loss=21.2635
	step [231/254], loss=21.0645
	step [232/254], loss=19.8949
	step [233/254], loss=19.4935
	step [234/254], loss=19.0255
	step [235/254], loss=18.2299
	step [236/254], loss=20.0055
	step [237/254], loss=20.0030
	step [238/254], loss=18.8701
	step [239/254], loss=22.8545
	step [240/254], loss=19.3150
	step [241/254], loss=19.5441
	step [242/254], loss=19.0391
	step [243/254], loss=19.8262
	step [244/254], loss=20.9253
	step [245/254], loss=20.3216
	step [246/254], loss=18.6154
	step [247/254], loss=18.4433
	step [248/254], loss=19.8907
	step [249/254], loss=19.5119
	step [250/254], loss=18.9101
	step [251/254], loss=17.5481
	step [252/254], loss=17.5436
	step [253/254], loss=18.7196
	step [254/254], loss=2.8972
	Evaluating
	loss=0.0839, precision=0.2572, recall=0.9918, f1=0.4085
saving model as: 1_saved_model.pth
Training epoch 5
	step [1/254], loss=16.7626
	step [2/254], loss=18.1125
	step [3/254], loss=20.3397
	step [4/254], loss=20.2269
	step [5/254], loss=17.6875
	step [6/254], loss=17.4257
	step [7/254], loss=18.7005
	step [8/254], loss=19.9492
	step [9/254], loss=17.5152
	step [10/254], loss=17.5848
	step [11/254], loss=19.8572
	step [12/254], loss=19.7125
	step [13/254], loss=18.6175
	step [14/254], loss=19.2428
	step [15/254], loss=17.4490
	step [16/254], loss=19.6006
	step [17/254], loss=18.1659
	step [18/254], loss=17.9367
	step [19/254], loss=20.0670
	step [20/254], loss=18.5707
	step [21/254], loss=19.0282
	step [22/254], loss=19.1476
	step [23/254], loss=17.9647
	step [24/254], loss=19.3016
	step [25/254], loss=19.2886
	step [26/254], loss=17.8011
	step [27/254], loss=18.0039
	step [28/254], loss=18.1238
	step [29/254], loss=17.6202
	step [30/254], loss=18.2661
	step [31/254], loss=18.1849
	step [32/254], loss=18.0652
	step [33/254], loss=18.9965
	step [34/254], loss=19.1954
	step [35/254], loss=21.1214
	step [36/254], loss=17.9563
	step [37/254], loss=18.5475
	step [38/254], loss=17.4021
	step [39/254], loss=18.3015
	step [40/254], loss=16.9641
	step [41/254], loss=18.7584
	step [42/254], loss=19.9554
	step [43/254], loss=18.0095
	step [44/254], loss=19.1880
	step [45/254], loss=18.5943
	step [46/254], loss=19.4994
	step [47/254], loss=17.1491
	step [48/254], loss=18.2242
	step [49/254], loss=19.7106
	step [50/254], loss=17.6783
	step [51/254], loss=18.5413
	step [52/254], loss=17.9719
	step [53/254], loss=20.3953
	step [54/254], loss=18.2449
	step [55/254], loss=18.1657
	step [56/254], loss=18.0788
	step [57/254], loss=17.4812
	step [58/254], loss=18.1152
	step [59/254], loss=17.1606
	step [60/254], loss=19.9298
	step [61/254], loss=17.4868
	step [62/254], loss=18.9222
	step [63/254], loss=17.2470
	step [64/254], loss=17.7201
	step [65/254], loss=18.9856
	step [66/254], loss=18.9557
	step [67/254], loss=17.4737
	step [68/254], loss=18.1556
	step [69/254], loss=18.8647
	step [70/254], loss=18.9258
	step [71/254], loss=18.4482
	step [72/254], loss=17.9066
	step [73/254], loss=17.7317
	step [74/254], loss=18.3754
	step [75/254], loss=18.6946
	step [76/254], loss=16.8336
	step [77/254], loss=21.6992
	step [78/254], loss=16.7378
	step [79/254], loss=18.5404
	step [80/254], loss=18.6422
	step [81/254], loss=17.9597
	step [82/254], loss=17.6365
	step [83/254], loss=17.7490
	step [84/254], loss=17.1790
	step [85/254], loss=16.9630
	step [86/254], loss=21.2380
	step [87/254], loss=16.3138
	step [88/254], loss=19.4342
	step [89/254], loss=18.5102
	step [90/254], loss=18.8224
	step [91/254], loss=17.0045
	step [92/254], loss=18.7391
	step [93/254], loss=18.7083
	step [94/254], loss=17.8374
	step [95/254], loss=16.8302
	step [96/254], loss=18.0108
	step [97/254], loss=17.0565
	step [98/254], loss=17.6391
	step [99/254], loss=18.0958
	step [100/254], loss=17.7717
	step [101/254], loss=17.5737
	step [102/254], loss=17.7018
	step [103/254], loss=18.7277
	step [104/254], loss=18.4837
	step [105/254], loss=17.0413
	step [106/254], loss=19.0914
	step [107/254], loss=18.7660
	step [108/254], loss=17.2211
	step [109/254], loss=16.2328
	step [110/254], loss=18.7350
	step [111/254], loss=16.8733
	step [112/254], loss=16.6136
	step [113/254], loss=18.2190
	step [114/254], loss=16.2123
	step [115/254], loss=17.3000
	step [116/254], loss=17.5033
	step [117/254], loss=16.7191
	step [118/254], loss=15.6911
	step [119/254], loss=17.1049
	step [120/254], loss=15.4768
	step [121/254], loss=15.7243
	step [122/254], loss=17.5258
	step [123/254], loss=20.2520
	step [124/254], loss=18.8524
	step [125/254], loss=16.4683
	step [126/254], loss=17.4776
	step [127/254], loss=17.4986
	step [128/254], loss=17.9987
	step [129/254], loss=18.0899
	step [130/254], loss=16.6993
	step [131/254], loss=15.9128
	step [132/254], loss=16.1609
	step [133/254], loss=18.1263
	step [134/254], loss=16.6973
	step [135/254], loss=17.1997
	step [136/254], loss=16.0375
	step [137/254], loss=15.5222
	step [138/254], loss=15.5115
	step [139/254], loss=16.0506
	step [140/254], loss=15.6549
	step [141/254], loss=15.6231
	step [142/254], loss=16.5804
	step [143/254], loss=18.0779
	step [144/254], loss=15.7475
	step [145/254], loss=16.4598
	step [146/254], loss=17.3934
	step [147/254], loss=17.5710
	step [148/254], loss=18.2898
	step [149/254], loss=17.5880
	step [150/254], loss=16.7648
	step [151/254], loss=15.8453
	step [152/254], loss=17.1063
	step [153/254], loss=15.0928
	step [154/254], loss=15.4568
	step [155/254], loss=16.3317
	step [156/254], loss=16.5066
	step [157/254], loss=17.7247
	step [158/254], loss=14.5946
	step [159/254], loss=15.7840
	step [160/254], loss=15.5411
	step [161/254], loss=16.1869
	step [162/254], loss=18.3662
	step [163/254], loss=15.5729
	step [164/254], loss=16.6505
	step [165/254], loss=16.2694
	step [166/254], loss=16.7618
	step [167/254], loss=15.5793
	step [168/254], loss=18.8158
	step [169/254], loss=16.2281
	step [170/254], loss=16.1752
	step [171/254], loss=16.8475
	step [172/254], loss=16.2447
	step [173/254], loss=17.2356
	step [174/254], loss=15.7028
	step [175/254], loss=16.2515
	step [176/254], loss=15.1183
	step [177/254], loss=15.8197
	step [178/254], loss=15.6635
	step [179/254], loss=16.5393
	step [180/254], loss=17.5794
	step [181/254], loss=17.6676
	step [182/254], loss=15.8183
	step [183/254], loss=15.0142
	step [184/254], loss=16.8566
	step [185/254], loss=13.6457
	step [186/254], loss=15.3182
	step [187/254], loss=16.1813
	step [188/254], loss=17.9222
	step [189/254], loss=15.5590
	step [190/254], loss=14.9832
	step [191/254], loss=14.4550
	step [192/254], loss=16.1727
	step [193/254], loss=16.3012
	step [194/254], loss=15.9209
	step [195/254], loss=15.9368
	step [196/254], loss=17.1948
	step [197/254], loss=16.8451
	step [198/254], loss=15.8741
	step [199/254], loss=16.5964
	step [200/254], loss=16.1013
	step [201/254], loss=14.8666
	step [202/254], loss=14.4705
	step [203/254], loss=16.0819
	step [204/254], loss=15.0018
	step [205/254], loss=17.3634
	step [206/254], loss=18.0187
	step [207/254], loss=15.6883
	step [208/254], loss=15.6788
	step [209/254], loss=14.2682
	step [210/254], loss=16.2046
	step [211/254], loss=15.5645
	step [212/254], loss=13.8450
	step [213/254], loss=13.3661
	step [214/254], loss=14.8316
	step [215/254], loss=16.3777
	step [216/254], loss=17.2006
	step [217/254], loss=15.9390
	step [218/254], loss=15.1341
	step [219/254], loss=13.6288
	step [220/254], loss=17.0740
	step [221/254], loss=16.5782
	step [222/254], loss=15.5811
	step [223/254], loss=13.3138
	step [224/254], loss=17.4746
	step [225/254], loss=15.3181
	step [226/254], loss=15.8068
	step [227/254], loss=15.2509
	step [228/254], loss=16.8768
	step [229/254], loss=14.5830
	step [230/254], loss=15.6385
	step [231/254], loss=15.5271
	step [232/254], loss=14.4547
	step [233/254], loss=13.5727
	step [234/254], loss=14.9053
	step [235/254], loss=13.8313
	step [236/254], loss=13.3442
	step [237/254], loss=13.2999
	step [238/254], loss=14.4154
	step [239/254], loss=18.1835
	step [240/254], loss=14.9181
	step [241/254], loss=16.6129
	step [242/254], loss=15.5028
	step [243/254], loss=16.1297
	step [244/254], loss=16.4317
	step [245/254], loss=15.0549
	step [246/254], loss=14.2984
	step [247/254], loss=15.0430
	step [248/254], loss=16.7599
	step [249/254], loss=16.0980
	step [250/254], loss=14.9173
	step [251/254], loss=17.5256
	step [252/254], loss=14.6785
	step [253/254], loss=15.2356
	step [254/254], loss=3.0597
	Evaluating
	loss=0.0681, precision=0.2115, recall=0.9943, f1=0.3488
Training epoch 6
	step [1/254], loss=13.7001
	step [2/254], loss=14.3583
	step [3/254], loss=14.2111
	step [4/254], loss=15.0771
	step [5/254], loss=14.8683
	step [6/254], loss=15.5792
	step [7/254], loss=15.6011
	step [8/254], loss=14.8295
	step [9/254], loss=13.9971
	step [10/254], loss=15.1998
	step [11/254], loss=15.7333
	step [12/254], loss=16.6569
	step [13/254], loss=13.0014
	step [14/254], loss=14.3078
	step [15/254], loss=14.0354
	step [16/254], loss=12.7193
	step [17/254], loss=16.2520
	step [18/254], loss=15.6398
	step [19/254], loss=15.2161
	step [20/254], loss=15.1304
	step [21/254], loss=13.7384
	step [22/254], loss=13.7755
	step [23/254], loss=15.8471
	step [24/254], loss=15.9891
	step [25/254], loss=15.2300
	step [26/254], loss=15.2179
	step [27/254], loss=16.5167
	step [28/254], loss=14.2865
	step [29/254], loss=13.5274
	step [30/254], loss=16.1918
	step [31/254], loss=14.8767
	step [32/254], loss=16.0944
	step [33/254], loss=16.0205
	step [34/254], loss=16.9049
	step [35/254], loss=14.5279
	step [36/254], loss=14.2579
	step [37/254], loss=15.1126
	step [38/254], loss=12.9894
	step [39/254], loss=14.0310
	step [40/254], loss=14.6209
	step [41/254], loss=14.3026
	step [42/254], loss=15.8030
	step [43/254], loss=13.4318
	step [44/254], loss=14.0088
	step [45/254], loss=13.1423
	step [46/254], loss=14.3982
	step [47/254], loss=12.6946
	step [48/254], loss=15.1453
	step [49/254], loss=16.6203
	step [50/254], loss=13.5387
	step [51/254], loss=13.2939
	step [52/254], loss=14.2409
	step [53/254], loss=13.8516
	step [54/254], loss=14.1194
	step [55/254], loss=15.0050
	step [56/254], loss=14.1791
	step [57/254], loss=13.8077
	step [58/254], loss=15.5740
	step [59/254], loss=15.1559
	step [60/254], loss=13.8509
	step [61/254], loss=14.6274
	step [62/254], loss=16.3782
	step [63/254], loss=16.5965
	step [64/254], loss=14.8458
	step [65/254], loss=15.1434
	step [66/254], loss=14.8648
	step [67/254], loss=13.9687
	step [68/254], loss=15.4998
	step [69/254], loss=14.2167
	step [70/254], loss=15.0255
	step [71/254], loss=14.0831
	step [72/254], loss=14.1309
	step [73/254], loss=14.1036
	step [74/254], loss=16.3327
	step [75/254], loss=16.3783
	step [76/254], loss=15.4312
	step [77/254], loss=15.2579
	step [78/254], loss=14.1498
	step [79/254], loss=14.7122
	step [80/254], loss=16.5699
	step [81/254], loss=14.5505
	step [82/254], loss=14.1322
	step [83/254], loss=14.8010
	step [84/254], loss=14.6807
	step [85/254], loss=13.1606
	step [86/254], loss=13.9186
	step [87/254], loss=13.5000
	step [88/254], loss=12.2745
	step [89/254], loss=14.8923
	step [90/254], loss=16.3428
	step [91/254], loss=13.4584
	step [92/254], loss=15.3095
	step [93/254], loss=12.1572
	step [94/254], loss=12.4547
	step [95/254], loss=14.6412
	step [96/254], loss=14.3273
	step [97/254], loss=15.7492
	step [98/254], loss=13.0325
	step [99/254], loss=14.6414
	step [100/254], loss=12.9618
	step [101/254], loss=13.8751
	step [102/254], loss=12.6747
	step [103/254], loss=13.0854
	step [104/254], loss=12.9450
	step [105/254], loss=14.8265
	step [106/254], loss=16.5698
	step [107/254], loss=13.9831
	step [108/254], loss=14.2978
	step [109/254], loss=15.2963
	step [110/254], loss=13.9711
	step [111/254], loss=15.6356
	step [112/254], loss=13.4268
	step [113/254], loss=13.9427
	step [114/254], loss=13.0210
	step [115/254], loss=17.1445
	step [116/254], loss=14.8406
	step [117/254], loss=13.8744
	step [118/254], loss=17.4574
	step [119/254], loss=12.8675
	step [120/254], loss=14.1306
	step [121/254], loss=12.5950
	step [122/254], loss=16.9346
	step [123/254], loss=14.3245
	step [124/254], loss=14.3316
	step [125/254], loss=13.5082
	step [126/254], loss=14.6473
	step [127/254], loss=13.8525
	step [128/254], loss=14.6984
	step [129/254], loss=14.4047
	step [130/254], loss=13.2583
	step [131/254], loss=12.5171
	step [132/254], loss=15.0085
	step [133/254], loss=15.2680
	step [134/254], loss=11.9909
	step [135/254], loss=13.6341
	step [136/254], loss=13.5219
	step [137/254], loss=13.5009
	step [138/254], loss=13.9360
	step [139/254], loss=14.7295
	step [140/254], loss=14.2726
	step [141/254], loss=15.2233
	step [142/254], loss=14.4962
	step [143/254], loss=14.4624
	step [144/254], loss=14.1002
	step [145/254], loss=16.4700
	step [146/254], loss=14.4879
	step [147/254], loss=13.1040
	step [148/254], loss=12.6946
	step [149/254], loss=14.3499
	step [150/254], loss=12.9068
	step [151/254], loss=14.3802
	step [152/254], loss=13.2411
	step [153/254], loss=13.4219
	step [154/254], loss=13.6105
	step [155/254], loss=15.2522
	step [156/254], loss=13.1121
	step [157/254], loss=14.2065
	step [158/254], loss=13.5636
	step [159/254], loss=15.9624
	step [160/254], loss=13.3786
	step [161/254], loss=13.0414
	step [162/254], loss=15.5811
	step [163/254], loss=14.1214
	step [164/254], loss=13.6248
	step [165/254], loss=12.2732
	step [166/254], loss=12.8957
	step [167/254], loss=13.4398
	step [168/254], loss=12.8791
	step [169/254], loss=14.3226
	step [170/254], loss=12.5773
	step [171/254], loss=13.9775
	step [172/254], loss=12.1700
	step [173/254], loss=13.9732
	step [174/254], loss=13.2791
	step [175/254], loss=14.1476
	step [176/254], loss=13.3236
	step [177/254], loss=12.4875
	step [178/254], loss=16.1505
	step [179/254], loss=13.3082
	step [180/254], loss=13.0383
	step [181/254], loss=14.3416
	step [182/254], loss=11.8081
	step [183/254], loss=16.6804
	step [184/254], loss=13.2608
	step [185/254], loss=12.3391
	step [186/254], loss=13.9999
	step [187/254], loss=13.8592
	step [188/254], loss=12.2998
	step [189/254], loss=12.7297
	step [190/254], loss=14.8683
	step [191/254], loss=11.7284
	step [192/254], loss=11.3860
	step [193/254], loss=13.0526
	step [194/254], loss=13.6262
	step [195/254], loss=11.2552
	step [196/254], loss=15.6974
	step [197/254], loss=13.7526
	step [198/254], loss=13.0854
	step [199/254], loss=13.3848
	step [200/254], loss=13.2846
	step [201/254], loss=14.0332
	step [202/254], loss=12.3634
	step [203/254], loss=13.0229
	step [204/254], loss=12.3349
	step [205/254], loss=12.9532
	step [206/254], loss=13.9953
	step [207/254], loss=12.7817
	step [208/254], loss=13.8090
	step [209/254], loss=14.2919
	step [210/254], loss=13.1772
	step [211/254], loss=12.4178
	step [212/254], loss=12.0714
	step [213/254], loss=13.2204
	step [214/254], loss=12.8272
	step [215/254], loss=13.4741
	step [216/254], loss=13.5162
	step [217/254], loss=14.3039
	step [218/254], loss=14.4588
	step [219/254], loss=12.0981
	step [220/254], loss=13.9525
	step [221/254], loss=13.6536
	step [222/254], loss=12.3483
	step [223/254], loss=12.8580
	step [224/254], loss=11.9993
	step [225/254], loss=11.5200
	step [226/254], loss=11.8847
	step [227/254], loss=12.7457
	step [228/254], loss=12.4122
	step [229/254], loss=11.7263
	step [230/254], loss=12.3735
	step [231/254], loss=13.6892
	step [232/254], loss=14.8430
	step [233/254], loss=11.2687
	step [234/254], loss=12.6357
	step [235/254], loss=12.6089
	step [236/254], loss=11.7894
	step [237/254], loss=13.6256
	step [238/254], loss=14.3176
	step [239/254], loss=12.5731
	step [240/254], loss=11.9629
	step [241/254], loss=12.7035
	step [242/254], loss=14.0363
	step [243/254], loss=13.7646
	step [244/254], loss=13.3766
	step [245/254], loss=13.3562
	step [246/254], loss=13.3310
	step [247/254], loss=12.4479
	step [248/254], loss=11.6971
	step [249/254], loss=12.4053
	step [250/254], loss=12.9314
	step [251/254], loss=13.8270
	step [252/254], loss=15.6982
	step [253/254], loss=12.5879
	step [254/254], loss=2.4162
	Evaluating
	loss=0.0536, precision=0.2122, recall=0.9936, f1=0.3498
Training epoch 7
	step [1/254], loss=13.1300
	step [2/254], loss=12.8541
	step [3/254], loss=12.2124
	step [4/254], loss=11.6957
	step [5/254], loss=12.5201
	step [6/254], loss=12.7176
	step [7/254], loss=11.5703
	step [8/254], loss=13.9698
	step [9/254], loss=12.0993
	step [10/254], loss=11.9128
	step [11/254], loss=13.1203
	step [12/254], loss=13.8567
	step [13/254], loss=12.0102
	step [14/254], loss=12.9916
	step [15/254], loss=13.1136
	step [16/254], loss=11.7969
	step [17/254], loss=12.5412
	step [18/254], loss=12.9897
	step [19/254], loss=11.0302
	step [20/254], loss=13.7036
	step [21/254], loss=10.7770
	step [22/254], loss=12.2768
	step [23/254], loss=14.3915
	step [24/254], loss=12.4599
	step [25/254], loss=12.0187
	step [26/254], loss=12.8023
	step [27/254], loss=14.2381
	step [28/254], loss=14.9666
	step [29/254], loss=14.7933
	step [30/254], loss=10.3714
	step [31/254], loss=12.8710
	step [32/254], loss=13.0636
	step [33/254], loss=11.5310
	step [34/254], loss=11.0785
	step [35/254], loss=11.2092
	step [36/254], loss=11.2276
	step [37/254], loss=15.5584
	step [38/254], loss=12.6632
	step [39/254], loss=11.6753
	step [40/254], loss=12.9079
	step [41/254], loss=11.8103
	step [42/254], loss=10.9758
	step [43/254], loss=11.4791
	step [44/254], loss=12.3935
	step [45/254], loss=15.1906
	step [46/254], loss=11.7682
	step [47/254], loss=12.3831
	step [48/254], loss=11.2734
	step [49/254], loss=11.5866
	step [50/254], loss=13.9751
	step [51/254], loss=14.2818
	step [52/254], loss=11.9086
	step [53/254], loss=11.7162
	step [54/254], loss=11.7367
	step [55/254], loss=12.8226
	step [56/254], loss=12.8400
	step [57/254], loss=14.1445
	step [58/254], loss=11.5392
	step [59/254], loss=13.1168
	step [60/254], loss=12.1876
	step [61/254], loss=14.3806
	step [62/254], loss=11.1090
	step [63/254], loss=13.4405
	step [64/254], loss=11.8033
	step [65/254], loss=10.4534
	step [66/254], loss=10.5883
	step [67/254], loss=12.5180
	step [68/254], loss=12.0530
	step [69/254], loss=13.9871
	step [70/254], loss=12.8507
	step [71/254], loss=12.6832
	step [72/254], loss=12.3190
	step [73/254], loss=13.0775
	step [74/254], loss=13.0518
	step [75/254], loss=11.3923
	step [76/254], loss=12.2855
	step [77/254], loss=13.3962
	step [78/254], loss=10.7915
	step [79/254], loss=12.0385
	step [80/254], loss=10.8796
	step [81/254], loss=12.6498
	step [82/254], loss=10.7021
	step [83/254], loss=10.8940
	step [84/254], loss=11.2621
	step [85/254], loss=12.4209
	step [86/254], loss=12.9175
	step [87/254], loss=11.5290
	step [88/254], loss=11.7491
	step [89/254], loss=11.9651
	step [90/254], loss=13.6698
	step [91/254], loss=12.2041
	step [92/254], loss=13.1367
	step [93/254], loss=10.3387
	step [94/254], loss=11.8794
	step [95/254], loss=13.0758
	step [96/254], loss=12.4531
	step [97/254], loss=12.5009
	step [98/254], loss=12.4892
	step [99/254], loss=10.7305
	step [100/254], loss=11.2689
	step [101/254], loss=11.5472
	step [102/254], loss=11.5724
	step [103/254], loss=13.4189
	step [104/254], loss=14.7922
	step [105/254], loss=11.5808
	step [106/254], loss=12.5341
	step [107/254], loss=13.1414
	step [108/254], loss=11.8180
	step [109/254], loss=14.1276
	step [110/254], loss=12.4872
	step [111/254], loss=12.7786
	step [112/254], loss=13.7224
	step [113/254], loss=12.7638
	step [114/254], loss=12.4159
	step [115/254], loss=10.8320
	step [116/254], loss=12.0244
	step [117/254], loss=11.2214
	step [118/254], loss=12.2299
	step [119/254], loss=12.1383
	step [120/254], loss=13.0129
	step [121/254], loss=12.1237
	step [122/254], loss=10.8561
	step [123/254], loss=13.0978
	step [124/254], loss=11.3584
	step [125/254], loss=13.3468
	step [126/254], loss=12.1806
	step [127/254], loss=12.6125
	step [128/254], loss=11.3677
	step [129/254], loss=12.3109
	step [130/254], loss=12.2944
	step [131/254], loss=12.8011
	step [132/254], loss=11.0467
	step [133/254], loss=12.1566
	step [134/254], loss=13.9878
	step [135/254], loss=12.6508
	step [136/254], loss=12.3314
	step [137/254], loss=10.8563
	step [138/254], loss=10.6587
	step [139/254], loss=12.6713
	step [140/254], loss=12.5279
	step [141/254], loss=12.0068
	step [142/254], loss=10.9932
	step [143/254], loss=12.1465
	step [144/254], loss=10.9830
	step [145/254], loss=11.8457
	step [146/254], loss=10.2139
	step [147/254], loss=13.6330
	step [148/254], loss=12.0601
	step [149/254], loss=11.6058
	step [150/254], loss=13.3177
	step [151/254], loss=11.7409
	step [152/254], loss=11.3275
	step [153/254], loss=9.9891
	step [154/254], loss=10.5205
	step [155/254], loss=13.8075
	step [156/254], loss=10.8212
	step [157/254], loss=12.6290
	step [158/254], loss=12.5962
	step [159/254], loss=11.1274
	step [160/254], loss=11.3927
	step [161/254], loss=13.2971
	step [162/254], loss=11.8342
	step [163/254], loss=11.3267
	step [164/254], loss=10.6415
	step [165/254], loss=12.0687
	step [166/254], loss=14.4512
	step [167/254], loss=13.1061
	step [168/254], loss=11.6835
	step [169/254], loss=12.9160
	step [170/254], loss=11.7786
	step [171/254], loss=11.8186
	step [172/254], loss=12.8307
	step [173/254], loss=12.0053
	step [174/254], loss=10.7380
	step [175/254], loss=12.6267
	step [176/254], loss=11.9055
	step [177/254], loss=10.1878
	step [178/254], loss=12.5294
	step [179/254], loss=14.6293
	step [180/254], loss=12.6278
	step [181/254], loss=11.6915
	step [182/254], loss=10.8116
	step [183/254], loss=11.4754
	step [184/254], loss=11.7691
	step [185/254], loss=12.5253
	step [186/254], loss=11.3873
	step [187/254], loss=13.5630
	step [188/254], loss=10.0926
	step [189/254], loss=11.8162
	step [190/254], loss=11.0270
	step [191/254], loss=11.9916
	step [192/254], loss=15.0626
	step [193/254], loss=11.0559
	step [194/254], loss=11.8383
	step [195/254], loss=13.1640
	step [196/254], loss=11.2751
	step [197/254], loss=10.8914
	step [198/254], loss=10.6583
	step [199/254], loss=12.0186
	step [200/254], loss=12.1677
	step [201/254], loss=10.3684
	step [202/254], loss=10.6240
	step [203/254], loss=10.8361
	step [204/254], loss=10.5498
	step [205/254], loss=12.1473
	step [206/254], loss=9.2294
	step [207/254], loss=11.3366
	step [208/254], loss=11.6265
	step [209/254], loss=10.4715
	step [210/254], loss=12.8777
	step [211/254], loss=12.3090
	step [212/254], loss=12.2701
	step [213/254], loss=11.2370
	step [214/254], loss=11.2114
	step [215/254], loss=11.5797
	step [216/254], loss=13.0187
	step [217/254], loss=11.9807
	step [218/254], loss=11.1503
	step [219/254], loss=10.6124
	step [220/254], loss=12.0990
	step [221/254], loss=11.3810
	step [222/254], loss=10.2536
	step [223/254], loss=10.9669
	step [224/254], loss=11.3996
	step [225/254], loss=12.2807
	step [226/254], loss=10.2055
	step [227/254], loss=10.8348
	step [228/254], loss=10.1754
	step [229/254], loss=11.9961
	step [230/254], loss=10.4147
	step [231/254], loss=11.6999
	step [232/254], loss=10.9699
	step [233/254], loss=11.0734
	step [234/254], loss=9.1623
	step [235/254], loss=11.9244
	step [236/254], loss=10.7751
	step [237/254], loss=12.3624
	step [238/254], loss=10.3575
	step [239/254], loss=11.4261
	step [240/254], loss=11.0988
	step [241/254], loss=10.8296
	step [242/254], loss=13.2043
	step [243/254], loss=10.6200
	step [244/254], loss=11.0727
	step [245/254], loss=11.6994
	step [246/254], loss=11.1390
	step [247/254], loss=12.9323
	step [248/254], loss=12.1904
	step [249/254], loss=9.6728
	step [250/254], loss=12.3558
	step [251/254], loss=12.6309
	step [252/254], loss=11.8276
	step [253/254], loss=12.0903
	step [254/254], loss=1.8520
	Evaluating
	loss=0.0447, precision=0.2155, recall=0.9929, f1=0.3542
Training epoch 8
	step [1/254], loss=11.4944
	step [2/254], loss=12.5331
	step [3/254], loss=11.6859
	step [4/254], loss=12.0607
	step [5/254], loss=11.5575
	step [6/254], loss=10.0813
	step [7/254], loss=11.6331
	step [8/254], loss=10.6055
	step [9/254], loss=10.1199
	step [10/254], loss=10.7420
	step [11/254], loss=13.7374
	step [12/254], loss=11.4010
	step [13/254], loss=11.2835
	step [14/254], loss=12.7122
	step [15/254], loss=9.3651
	step [16/254], loss=10.8362
	step [17/254], loss=11.2137
	step [18/254], loss=10.8586
	step [19/254], loss=11.0162
	step [20/254], loss=11.1129
	step [21/254], loss=10.6614
	step [22/254], loss=11.7922
	step [23/254], loss=11.5172
	step [24/254], loss=10.4401
	step [25/254], loss=13.2450
	step [26/254], loss=11.2271
	step [27/254], loss=10.9444
	step [28/254], loss=10.2935
	step [29/254], loss=10.3774
	step [30/254], loss=11.5372
	step [31/254], loss=10.1867
	step [32/254], loss=11.1458
	step [33/254], loss=11.2926
	step [34/254], loss=11.0896
	step [35/254], loss=12.1093
	step [36/254], loss=11.7416
	step [37/254], loss=10.0019
	step [38/254], loss=10.7779
	step [39/254], loss=9.0758
	step [40/254], loss=9.9555
	step [41/254], loss=9.0742
	step [42/254], loss=10.5637
	step [43/254], loss=12.4572
	step [44/254], loss=11.9646
	step [45/254], loss=13.3142
	step [46/254], loss=10.8166
	step [47/254], loss=12.1792
	step [48/254], loss=9.6970
	step [49/254], loss=11.4049
	step [50/254], loss=10.5149
	step [51/254], loss=11.4765
	step [52/254], loss=12.1386
	step [53/254], loss=11.8252
	step [54/254], loss=10.6010
	step [55/254], loss=9.0750
	step [56/254], loss=10.8976
	step [57/254], loss=12.9770
	step [58/254], loss=10.3624
	step [59/254], loss=10.6736
	step [60/254], loss=12.1518
	step [61/254], loss=11.2876
	step [62/254], loss=9.6007
	step [63/254], loss=12.0558
	step [64/254], loss=11.0980
	step [65/254], loss=12.2486
	step [66/254], loss=9.9640
	step [67/254], loss=11.1876
	step [68/254], loss=9.7947
	step [69/254], loss=10.6573
	step [70/254], loss=12.8371
	step [71/254], loss=11.0154
	step [72/254], loss=8.9154
	step [73/254], loss=11.5061
	step [74/254], loss=10.6565
	step [75/254], loss=11.3191
	step [76/254], loss=11.6571
	step [77/254], loss=10.4920
	step [78/254], loss=13.4034
	step [79/254], loss=10.7746
	step [80/254], loss=10.2852
	step [81/254], loss=10.1777
	step [82/254], loss=12.3394
	step [83/254], loss=8.5963
	step [84/254], loss=10.6414
	step [85/254], loss=10.1325
	step [86/254], loss=11.0246
	step [87/254], loss=12.2450
	step [88/254], loss=9.8321
	step [89/254], loss=9.5451
	step [90/254], loss=12.4888
	step [91/254], loss=12.2029
	step [92/254], loss=12.6790
	step [93/254], loss=12.4081
	step [94/254], loss=10.5635
	step [95/254], loss=11.4019
	step [96/254], loss=9.7930
	step [97/254], loss=12.6132
	step [98/254], loss=10.1061
	step [99/254], loss=11.1287
	step [100/254], loss=10.1992
	step [101/254], loss=10.9800
	step [102/254], loss=12.1853
	step [103/254], loss=11.2869
	step [104/254], loss=10.6214
	step [105/254], loss=10.4040
	step [106/254], loss=10.2899
	step [107/254], loss=10.3752
	step [108/254], loss=9.7497
	step [109/254], loss=10.3000
	step [110/254], loss=9.5578
	step [111/254], loss=11.1619
	step [112/254], loss=10.5663
	step [113/254], loss=11.1649
	step [114/254], loss=9.3833
	step [115/254], loss=10.2652
	step [116/254], loss=10.3062
	step [117/254], loss=10.7164
	step [118/254], loss=10.4463
	step [119/254], loss=13.5895
	step [120/254], loss=11.4541
	step [121/254], loss=9.7640
	step [122/254], loss=10.3881
	step [123/254], loss=10.1328
	step [124/254], loss=11.3616
	step [125/254], loss=9.1470
	step [126/254], loss=10.1797
	step [127/254], loss=10.0873
	step [128/254], loss=9.3600
	step [129/254], loss=11.8765
	step [130/254], loss=11.3790
	step [131/254], loss=10.8061
	step [132/254], loss=10.0038
	step [133/254], loss=10.5854
	step [134/254], loss=11.0700
	step [135/254], loss=8.7009
	step [136/254], loss=11.6535
	step [137/254], loss=10.7065
	step [138/254], loss=10.1530
	step [139/254], loss=11.5904
	step [140/254], loss=10.5759
	step [141/254], loss=11.2759
	step [142/254], loss=11.5396
	step [143/254], loss=11.4893
	step [144/254], loss=11.0069
	step [145/254], loss=12.1909
	step [146/254], loss=10.2283
	step [147/254], loss=9.6299
	step [148/254], loss=10.4610
	step [149/254], loss=9.9361
	step [150/254], loss=13.5379
	step [151/254], loss=9.8661
	step [152/254], loss=10.7685
	step [153/254], loss=12.2911
	step [154/254], loss=10.1790
	step [155/254], loss=9.5114
	step [156/254], loss=13.0525
	step [157/254], loss=9.3659
	step [158/254], loss=10.1863
	step [159/254], loss=8.9881
	step [160/254], loss=10.7921
	step [161/254], loss=9.4662
	step [162/254], loss=9.6660
	step [163/254], loss=10.7563
	step [164/254], loss=8.7455
	step [165/254], loss=11.8200
	step [166/254], loss=12.1793
	step [167/254], loss=9.4812
	step [168/254], loss=9.6901
	step [169/254], loss=9.6599
	step [170/254], loss=11.5704
	step [171/254], loss=10.9447
	step [172/254], loss=9.1999
	step [173/254], loss=11.6614
	step [174/254], loss=10.6647
	step [175/254], loss=9.6923
	step [176/254], loss=8.6889
	step [177/254], loss=9.5071
	step [178/254], loss=9.9813
	step [179/254], loss=10.1453
	step [180/254], loss=10.7397
	step [181/254], loss=9.4876
	step [182/254], loss=9.5319
	step [183/254], loss=10.9743
	step [184/254], loss=9.5706
	step [185/254], loss=7.9341
	step [186/254], loss=8.9993
	step [187/254], loss=9.7658
	step [188/254], loss=10.6343
	step [189/254], loss=10.4302
	step [190/254], loss=10.8397
	step [191/254], loss=9.1281
	step [192/254], loss=10.7060
	step [193/254], loss=12.0328
	step [194/254], loss=10.4005
	step [195/254], loss=8.2286
	step [196/254], loss=10.4066
	step [197/254], loss=9.4700
	step [198/254], loss=11.5262
	step [199/254], loss=10.2170
	step [200/254], loss=10.0420
	step [201/254], loss=9.8928
	step [202/254], loss=10.9170
	step [203/254], loss=10.4010
	step [204/254], loss=11.1929
	step [205/254], loss=11.3057
	step [206/254], loss=9.8708
	step [207/254], loss=11.2822
	step [208/254], loss=9.9697
	step [209/254], loss=11.5936
	step [210/254], loss=10.8451
	step [211/254], loss=10.1104
	step [212/254], loss=12.1899
	step [213/254], loss=10.8168
	step [214/254], loss=10.9768
	step [215/254], loss=10.0088
	step [216/254], loss=10.5484
	step [217/254], loss=9.1807
	step [218/254], loss=11.6295
	step [219/254], loss=11.8569
	step [220/254], loss=10.5709
	step [221/254], loss=10.4641
	step [222/254], loss=11.2287
	step [223/254], loss=13.2103
	step [224/254], loss=9.3830
	step [225/254], loss=9.9320
	step [226/254], loss=9.8328
	step [227/254], loss=10.6766
	step [228/254], loss=9.6949
	step [229/254], loss=11.7964
	step [230/254], loss=11.3617
	step [231/254], loss=10.1655
	step [232/254], loss=11.1998
	step [233/254], loss=10.6280
	step [234/254], loss=8.7918
	step [235/254], loss=10.1921
	step [236/254], loss=10.7316
	step [237/254], loss=9.2909
	step [238/254], loss=10.7634
	step [239/254], loss=9.0562
	step [240/254], loss=9.8864
	step [241/254], loss=9.5008
	step [242/254], loss=10.5491
	step [243/254], loss=11.5415
	step [244/254], loss=9.6817
	step [245/254], loss=10.8623
	step [246/254], loss=9.8413
	step [247/254], loss=10.5771
	step [248/254], loss=9.3682
	step [249/254], loss=9.2404
	step [250/254], loss=9.7011
	step [251/254], loss=10.4481
	step [252/254], loss=8.5471
	step [253/254], loss=11.3285
	step [254/254], loss=1.6485
	Evaluating
	loss=0.0354, precision=0.2567, recall=0.9915, f1=0.4079
Training epoch 9
	step [1/254], loss=12.2404
	step [2/254], loss=9.7796
	step [3/254], loss=11.3372
	step [4/254], loss=9.6485
	step [5/254], loss=11.9393
	step [6/254], loss=8.7649
	step [7/254], loss=9.5639
	step [8/254], loss=10.9827
	step [9/254], loss=10.5333
	step [10/254], loss=10.0648
	step [11/254], loss=10.7640
	step [12/254], loss=10.4187
	step [13/254], loss=10.1519
	step [14/254], loss=9.4662
	step [15/254], loss=9.8041
	step [16/254], loss=14.4252
	step [17/254], loss=9.7513
	step [18/254], loss=10.8206
	step [19/254], loss=10.0247
	step [20/254], loss=8.4246
	step [21/254], loss=9.6175
	step [22/254], loss=12.2422
	step [23/254], loss=8.9833
	step [24/254], loss=12.5182
	step [25/254], loss=9.2905
	step [26/254], loss=10.6253
	step [27/254], loss=11.8152
	step [28/254], loss=9.7455
	step [29/254], loss=10.1238
	step [30/254], loss=11.0239
	step [31/254], loss=10.4661
	step [32/254], loss=9.3665
	step [33/254], loss=11.2796
	step [34/254], loss=9.6556
	step [35/254], loss=9.1122
	step [36/254], loss=12.4949
	step [37/254], loss=8.2116
	step [38/254], loss=10.2898
	step [39/254], loss=9.2974
	step [40/254], loss=9.9418
	step [41/254], loss=11.3464
	step [42/254], loss=10.1829
	step [43/254], loss=8.7910
	step [44/254], loss=8.3926
	step [45/254], loss=10.6104
	step [46/254], loss=10.6287
	step [47/254], loss=8.6036
	step [48/254], loss=8.8885
	step [49/254], loss=11.0294
	step [50/254], loss=9.8142
	step [51/254], loss=9.2340
	step [52/254], loss=8.6785
	step [53/254], loss=11.7563
	step [54/254], loss=7.7457
	step [55/254], loss=9.5405
	step [56/254], loss=8.9257
	step [57/254], loss=8.2785
	step [58/254], loss=11.0839
	step [59/254], loss=10.9389
	step [60/254], loss=8.6573
	step [61/254], loss=8.1829
	step [62/254], loss=10.2726
	step [63/254], loss=9.3096
	step [64/254], loss=7.6665
	step [65/254], loss=9.4702
	step [66/254], loss=10.3104
	step [67/254], loss=11.1798
	step [68/254], loss=9.4286
	step [69/254], loss=9.3594
	step [70/254], loss=8.6245
	step [71/254], loss=8.4480
	step [72/254], loss=8.3572
	step [73/254], loss=9.1912
	step [74/254], loss=9.2904
	step [75/254], loss=7.9790
	step [76/254], loss=8.7979
	step [77/254], loss=10.1164
	step [78/254], loss=10.6418
	step [79/254], loss=11.4229
	step [80/254], loss=10.5535
	step [81/254], loss=9.4866
	step [82/254], loss=11.1566
	step [83/254], loss=8.0401
	step [84/254], loss=7.8175
	step [85/254], loss=10.3813
	step [86/254], loss=9.3846
	step [87/254], loss=9.2181
	step [88/254], loss=11.0125
	step [89/254], loss=10.9423
	step [90/254], loss=10.4339
	step [91/254], loss=10.0724
	step [92/254], loss=10.0910
	step [93/254], loss=8.3652
	step [94/254], loss=11.0602
	step [95/254], loss=10.2115
	step [96/254], loss=10.1934
	step [97/254], loss=9.9505
	step [98/254], loss=10.3302
	step [99/254], loss=9.1311
	step [100/254], loss=10.6673
	step [101/254], loss=10.6753
	step [102/254], loss=8.9575
	step [103/254], loss=10.7448
	step [104/254], loss=11.2451
	step [105/254], loss=10.3116
	step [106/254], loss=8.9418
	step [107/254], loss=7.8863
	step [108/254], loss=10.4708
	step [109/254], loss=8.0709
	step [110/254], loss=9.3033
	step [111/254], loss=9.0178
	step [112/254], loss=9.2587
	step [113/254], loss=10.2005
	step [114/254], loss=8.9276
	step [115/254], loss=10.1514
	step [116/254], loss=10.1843
	step [117/254], loss=9.8309
	step [118/254], loss=12.5703
	step [119/254], loss=9.6516
	step [120/254], loss=9.3577
	step [121/254], loss=8.9909
	step [122/254], loss=10.5459
	step [123/254], loss=10.1736
	step [124/254], loss=9.2628
	step [125/254], loss=12.1871
	step [126/254], loss=10.0624
	step [127/254], loss=9.3275
	step [128/254], loss=9.4352
	step [129/254], loss=10.2602
	step [130/254], loss=10.5091
	step [131/254], loss=10.1013
	step [132/254], loss=8.8469
	step [133/254], loss=9.6777
	step [134/254], loss=8.2129
	step [135/254], loss=8.9733
	step [136/254], loss=11.8608
	step [137/254], loss=9.9864
	step [138/254], loss=9.0783
	step [139/254], loss=8.5651
	step [140/254], loss=9.5733
	step [141/254], loss=9.1025
	step [142/254], loss=12.7622
	step [143/254], loss=9.3020
	step [144/254], loss=9.0210
	step [145/254], loss=10.1007
	step [146/254], loss=10.0424
	step [147/254], loss=10.0525
	step [148/254], loss=10.2637
	step [149/254], loss=10.9194
	step [150/254], loss=10.3824
	step [151/254], loss=10.7605
	step [152/254], loss=8.7288
	step [153/254], loss=10.5470
	step [154/254], loss=8.2645
	step [155/254], loss=8.9158
	step [156/254], loss=10.1213
	step [157/254], loss=8.4866
	step [158/254], loss=8.6354
	step [159/254], loss=8.5373
	step [160/254], loss=7.9026
	step [161/254], loss=11.8486
	step [162/254], loss=9.0953
	step [163/254], loss=10.0743
	step [164/254], loss=8.8849
	step [165/254], loss=9.6459
	step [166/254], loss=7.7834
	step [167/254], loss=9.7033
	step [168/254], loss=8.7644
	step [169/254], loss=7.5202
	step [170/254], loss=10.4019
	step [171/254], loss=10.5197
	step [172/254], loss=8.6705
	step [173/254], loss=10.6651
	step [174/254], loss=9.0033
	step [175/254], loss=11.8231
	step [176/254], loss=10.7686
	step [177/254], loss=9.4282
	step [178/254], loss=10.3539
	step [179/254], loss=10.2597
	step [180/254], loss=8.7279
	step [181/254], loss=11.0743
	step [182/254], loss=8.1615
	step [183/254], loss=8.2905
	step [184/254], loss=9.6532
	step [185/254], loss=10.4492
	step [186/254], loss=9.9300
	step [187/254], loss=8.3566
	step [188/254], loss=7.8636
	step [189/254], loss=8.3637
	step [190/254], loss=10.7402
	step [191/254], loss=8.8844
	step [192/254], loss=8.8971
	step [193/254], loss=9.4784
	step [194/254], loss=9.6047
	step [195/254], loss=8.2277
	step [196/254], loss=10.9718
	step [197/254], loss=10.0525
	step [198/254], loss=10.1123
	step [199/254], loss=8.5748
	step [200/254], loss=7.5263
	step [201/254], loss=9.9473
	step [202/254], loss=9.3697
	step [203/254], loss=9.6799
	step [204/254], loss=8.7662
	step [205/254], loss=7.6385
	step [206/254], loss=11.5654
	step [207/254], loss=8.2757
	step [208/254], loss=11.0636
	step [209/254], loss=10.0540
	step [210/254], loss=11.9711
	step [211/254], loss=8.1623
	step [212/254], loss=10.1197
	step [213/254], loss=10.7784
	step [214/254], loss=10.2010
	step [215/254], loss=10.6277
	step [216/254], loss=11.2315
	step [217/254], loss=10.6593
	step [218/254], loss=9.1948
	step [219/254], loss=11.0256
	step [220/254], loss=10.5645
	step [221/254], loss=9.4463
	step [222/254], loss=9.5259
	step [223/254], loss=9.1138
	step [224/254], loss=8.1855
	step [225/254], loss=11.3955
	step [226/254], loss=9.4230
	step [227/254], loss=9.9841
	step [228/254], loss=10.5025
	step [229/254], loss=10.3682
	step [230/254], loss=7.9765
	step [231/254], loss=10.1512
	step [232/254], loss=9.6239
	step [233/254], loss=9.9828
	step [234/254], loss=11.0033
	step [235/254], loss=8.1487
	step [236/254], loss=7.5834
	step [237/254], loss=9.7092
	step [238/254], loss=8.8784
	step [239/254], loss=9.0338
	step [240/254], loss=13.2896
	step [241/254], loss=7.4541
	step [242/254], loss=8.3820
	step [243/254], loss=9.8675
	step [244/254], loss=9.8368
	step [245/254], loss=8.6743
	step [246/254], loss=8.5714
	step [247/254], loss=9.0738
	step [248/254], loss=11.5606
	step [249/254], loss=10.4621
	step [250/254], loss=8.6606
	step [251/254], loss=9.7415
	step [252/254], loss=8.9693
	step [253/254], loss=9.3154
	step [254/254], loss=2.6359
	Evaluating
	loss=0.0326, precision=0.2475, recall=0.9922, f1=0.3961
Training epoch 10
	step [1/254], loss=8.8213
	step [2/254], loss=8.1183
	step [3/254], loss=9.1595
	step [4/254], loss=7.7624
	step [5/254], loss=8.1739
	step [6/254], loss=8.8141
	step [7/254], loss=11.6522
	step [8/254], loss=9.6589
	step [9/254], loss=13.4467
	step [10/254], loss=6.9732
	step [11/254], loss=8.4127
	step [12/254], loss=9.1666
	step [13/254], loss=9.8677
	step [14/254], loss=10.3973
	step [15/254], loss=8.2705
	step [16/254], loss=8.0262
	step [17/254], loss=10.2709
	step [18/254], loss=13.5998
	step [19/254], loss=8.8611
	step [20/254], loss=10.7668
	step [21/254], loss=9.2140
	step [22/254], loss=8.3769
	step [23/254], loss=9.5660
	step [24/254], loss=10.3330
	step [25/254], loss=10.2770
	step [26/254], loss=9.5881
	step [27/254], loss=8.9426
	step [28/254], loss=8.4406
	step [29/254], loss=8.7261
	step [30/254], loss=7.7740
	step [31/254], loss=8.7101
	step [32/254], loss=9.5295
	step [33/254], loss=11.5621
	step [34/254], loss=8.7244
	step [35/254], loss=9.5275
	step [36/254], loss=8.4555
	step [37/254], loss=9.8892
	step [38/254], loss=8.4180
	step [39/254], loss=10.3908
	step [40/254], loss=9.2452
	step [41/254], loss=9.5367
	step [42/254], loss=8.5085
	step [43/254], loss=7.9305
	step [44/254], loss=10.6290
	step [45/254], loss=11.5646
	step [46/254], loss=9.7781
	step [47/254], loss=9.6135
	step [48/254], loss=8.5239
	step [49/254], loss=7.8273
	step [50/254], loss=8.0113
	step [51/254], loss=9.2897
	step [52/254], loss=10.6417
	step [53/254], loss=7.2816
	step [54/254], loss=11.4613
	step [55/254], loss=7.2326
	step [56/254], loss=8.7975
	step [57/254], loss=8.3958
	step [58/254], loss=10.9178
	step [59/254], loss=8.0314
	step [60/254], loss=9.9929
	step [61/254], loss=7.3996
	step [62/254], loss=9.2663
	step [63/254], loss=9.5186
	step [64/254], loss=9.9105
	step [65/254], loss=9.8774
	step [66/254], loss=9.5031
	step [67/254], loss=10.4190
	step [68/254], loss=10.5115
	step [69/254], loss=9.2276
	step [70/254], loss=9.0538
	step [71/254], loss=8.6371
	step [72/254], loss=9.9874
	step [73/254], loss=8.0821
	step [74/254], loss=7.9799
	step [75/254], loss=8.7556
	step [76/254], loss=8.2134
	step [77/254], loss=8.8599
	step [78/254], loss=9.4824
	step [79/254], loss=9.2419
	step [80/254], loss=8.2466
	step [81/254], loss=9.4841
	step [82/254], loss=7.1176
	step [83/254], loss=9.6763
	step [84/254], loss=7.7353
	step [85/254], loss=7.7683
	step [86/254], loss=9.9428
	step [87/254], loss=7.0145
	step [88/254], loss=9.9662
	step [89/254], loss=8.4412
	step [90/254], loss=8.2125
	step [91/254], loss=8.1000
	step [92/254], loss=9.9267
	step [93/254], loss=9.1076
	step [94/254], loss=11.8401
	step [95/254], loss=10.5070
	step [96/254], loss=7.8181
	step [97/254], loss=6.8615
	step [98/254], loss=7.4536
	step [99/254], loss=8.3057
	step [100/254], loss=10.5686
	step [101/254], loss=8.4852
	step [102/254], loss=6.8427
	step [103/254], loss=8.2338
	step [104/254], loss=9.1567
	step [105/254], loss=8.3035
	step [106/254], loss=10.3349
	step [107/254], loss=9.7092
	step [108/254], loss=8.4097
	step [109/254], loss=8.5868
	step [110/254], loss=7.8280
	step [111/254], loss=7.8983
	step [112/254], loss=9.0476
	step [113/254], loss=10.9395
	step [114/254], loss=8.0975
	step [115/254], loss=10.8327
	step [116/254], loss=9.1783
	step [117/254], loss=9.4796
	step [118/254], loss=8.9662
	step [119/254], loss=8.6365
	step [120/254], loss=9.6665
	step [121/254], loss=10.6719
	step [122/254], loss=7.3573
	step [123/254], loss=9.0664
	step [124/254], loss=9.3892
	step [125/254], loss=8.0199
	step [126/254], loss=8.6474
	step [127/254], loss=10.7275
	step [128/254], loss=7.1974
	step [129/254], loss=8.5926
	step [130/254], loss=11.4396
	step [131/254], loss=7.0094
	step [132/254], loss=8.3118
	step [133/254], loss=10.3678
	step [134/254], loss=8.3026
	step [135/254], loss=9.5355
	step [136/254], loss=9.7527
	step [137/254], loss=7.9560
	step [138/254], loss=10.7766
	step [139/254], loss=8.7099
	step [140/254], loss=10.3416
	step [141/254], loss=9.0170
	step [142/254], loss=8.1974
	step [143/254], loss=7.9907
	step [144/254], loss=9.3216
	step [145/254], loss=9.5982
	step [146/254], loss=8.2297
	step [147/254], loss=9.2622
	step [148/254], loss=8.8702
	step [149/254], loss=9.6572
	step [150/254], loss=9.1103
	step [151/254], loss=8.0312
	step [152/254], loss=7.9020
	step [153/254], loss=9.2792
	step [154/254], loss=9.3057
	step [155/254], loss=8.9693
	step [156/254], loss=9.4412
	step [157/254], loss=9.3100
	step [158/254], loss=11.1083
	step [159/254], loss=9.3149
	step [160/254], loss=8.1594
	step [161/254], loss=8.9218
	step [162/254], loss=7.4395
	step [163/254], loss=8.1282
	step [164/254], loss=10.2944
	step [165/254], loss=7.4545
	step [166/254], loss=9.6401
	step [167/254], loss=7.3213
	step [168/254], loss=8.3109
	step [169/254], loss=8.4247
	step [170/254], loss=9.1594
	step [171/254], loss=8.6787
	step [172/254], loss=8.9085
	step [173/254], loss=10.9346
	step [174/254], loss=9.6065
	step [175/254], loss=8.6540
	step [176/254], loss=7.7113
	step [177/254], loss=9.4516
	step [178/254], loss=9.2844
	step [179/254], loss=9.8191
	step [180/254], loss=8.8229
	step [181/254], loss=8.6475
	step [182/254], loss=8.4504
	step [183/254], loss=9.8959
	step [184/254], loss=8.3824
	step [185/254], loss=8.4298
	step [186/254], loss=9.4606
	step [187/254], loss=6.9119
	step [188/254], loss=6.1494
	step [189/254], loss=9.4842
	step [190/254], loss=9.7743
	step [191/254], loss=9.5022
	step [192/254], loss=8.6860
	step [193/254], loss=12.2948
	step [194/254], loss=9.2013
	step [195/254], loss=8.8282
	step [196/254], loss=10.0386
	step [197/254], loss=11.1343
	step [198/254], loss=9.1542
	step [199/254], loss=9.2099
	step [200/254], loss=8.8614
	step [201/254], loss=8.3788
	step [202/254], loss=9.1299
	step [203/254], loss=9.3421
	step [204/254], loss=8.9773
	step [205/254], loss=10.0395
	step [206/254], loss=11.2428
	step [207/254], loss=10.9370
	step [208/254], loss=7.1487
	step [209/254], loss=8.9655
	step [210/254], loss=8.8964
	step [211/254], loss=9.5110
	step [212/254], loss=8.7598
	step [213/254], loss=8.8006
	step [214/254], loss=8.8116
	step [215/254], loss=9.4015
	step [216/254], loss=9.4481
	step [217/254], loss=11.0775
	step [218/254], loss=8.1545
	step [219/254], loss=9.0730
	step [220/254], loss=10.4032
	step [221/254], loss=9.2538
	step [222/254], loss=8.7453
	step [223/254], loss=7.1556
	step [224/254], loss=9.0551
	step [225/254], loss=9.4808
	step [226/254], loss=8.6332
	step [227/254], loss=7.6055
	step [228/254], loss=9.3900
	step [229/254], loss=8.6318
	step [230/254], loss=8.1924
	step [231/254], loss=11.1296
	step [232/254], loss=8.0851
	step [233/254], loss=7.7386
	step [234/254], loss=11.3065
	step [235/254], loss=9.0347
	step [236/254], loss=7.3144
	step [237/254], loss=8.8501
	step [238/254], loss=9.3557
	step [239/254], loss=8.2897
	step [240/254], loss=10.2305
	step [241/254], loss=9.1214
	step [242/254], loss=8.1599
	step [243/254], loss=7.4059
	step [244/254], loss=9.5046
	step [245/254], loss=8.1663
	step [246/254], loss=8.7706
	step [247/254], loss=8.3439
	step [248/254], loss=9.0392
	step [249/254], loss=10.3898
	step [250/254], loss=8.3008
	step [251/254], loss=9.2958
	step [252/254], loss=9.6160
	step [253/254], loss=6.1668
	step [254/254], loss=1.7467
	Evaluating
	loss=0.0313, precision=0.2388, recall=0.9930, f1=0.3851
Training epoch 11
	step [1/254], loss=8.5101
	step [2/254], loss=10.5079
	step [3/254], loss=7.5593
	step [4/254], loss=7.3956
	step [5/254], loss=9.2050
	step [6/254], loss=8.5147
	step [7/254], loss=8.3737
	step [8/254], loss=9.5339
	step [9/254], loss=7.2914
	step [10/254], loss=8.7341
	step [11/254], loss=8.6164
	step [12/254], loss=7.6182
	step [13/254], loss=8.6654
	step [14/254], loss=9.0718
	step [15/254], loss=9.4659
	step [16/254], loss=9.5946
	step [17/254], loss=9.5336
	step [18/254], loss=7.8777
	step [19/254], loss=8.6033
	step [20/254], loss=9.4069
	step [21/254], loss=7.5219
	step [22/254], loss=7.8185
	step [23/254], loss=8.0076
	step [24/254], loss=8.6484
	step [25/254], loss=8.2703
	step [26/254], loss=9.1707
	step [27/254], loss=9.2527
	step [28/254], loss=9.1913
	step [29/254], loss=8.5872
	step [30/254], loss=10.3240
	step [31/254], loss=8.3991
	step [32/254], loss=9.1273
	step [33/254], loss=7.0755
	step [34/254], loss=10.7535
	step [35/254], loss=8.2481
	step [36/254], loss=9.0136
	step [37/254], loss=8.1217
	step [38/254], loss=8.6455
	step [39/254], loss=8.8017
	step [40/254], loss=7.6642
	step [41/254], loss=7.4583
	step [42/254], loss=9.4877
	step [43/254], loss=8.8996
	step [44/254], loss=9.6492
	step [45/254], loss=10.3997
	step [46/254], loss=6.8822
	step [47/254], loss=7.5477
	step [48/254], loss=7.4102
	step [49/254], loss=7.5896
	step [50/254], loss=9.4469
	step [51/254], loss=8.6039
	step [52/254], loss=8.5595
	step [53/254], loss=8.5038
	step [54/254], loss=9.0464
	step [55/254], loss=6.7491
	step [56/254], loss=7.6722
	step [57/254], loss=7.6660
	step [58/254], loss=10.6027
	step [59/254], loss=8.1496
	step [60/254], loss=8.5450
	step [61/254], loss=11.0589
	step [62/254], loss=9.2905
	step [63/254], loss=7.7593
	step [64/254], loss=7.8704
	step [65/254], loss=7.2456
	step [66/254], loss=9.7712
	step [67/254], loss=9.5677
	step [68/254], loss=7.0512
	step [69/254], loss=7.8717
	step [70/254], loss=7.5514
	step [71/254], loss=8.5211
	step [72/254], loss=9.3293
	step [73/254], loss=7.9825
	step [74/254], loss=8.3495
	step [75/254], loss=7.2783
	step [76/254], loss=9.8471
	step [77/254], loss=9.8555
	step [78/254], loss=12.1037
	step [79/254], loss=8.0709
	step [80/254], loss=8.7344
	step [81/254], loss=11.8016
	step [82/254], loss=7.2236
	step [83/254], loss=9.2151
	step [84/254], loss=9.5384
	step [85/254], loss=8.0608
	step [86/254], loss=10.3718
	step [87/254], loss=9.8634
	step [88/254], loss=8.8350
	step [89/254], loss=9.3376
	step [90/254], loss=8.2849
	step [91/254], loss=7.9059
	step [92/254], loss=9.1375
	step [93/254], loss=9.0011
	step [94/254], loss=9.5266
	step [95/254], loss=9.4014
	step [96/254], loss=7.5409
	step [97/254], loss=8.0334
	step [98/254], loss=8.1119
	step [99/254], loss=7.7061
	step [100/254], loss=10.7241
	step [101/254], loss=7.9332
	step [102/254], loss=8.4028
	step [103/254], loss=11.6323
	step [104/254], loss=7.4670
	step [105/254], loss=9.5509
	step [106/254], loss=7.6798
	step [107/254], loss=7.1647
	step [108/254], loss=7.9819
	step [109/254], loss=8.9110
	step [110/254], loss=8.4096
	step [111/254], loss=7.5089
	step [112/254], loss=8.5397
	step [113/254], loss=7.8271
	step [114/254], loss=8.4707
	step [115/254], loss=7.7886
	step [116/254], loss=9.1882
	step [117/254], loss=9.9894
	step [118/254], loss=9.4852
	step [119/254], loss=8.6386
	step [120/254], loss=8.9447
	step [121/254], loss=9.3309
	step [122/254], loss=9.5461
	step [123/254], loss=9.4396
	step [124/254], loss=9.9412
	step [125/254], loss=7.7533
	step [126/254], loss=8.4075
	step [127/254], loss=7.7038
	step [128/254], loss=7.6081
	step [129/254], loss=8.2839
	step [130/254], loss=7.9762
	step [131/254], loss=9.1129
	step [132/254], loss=6.4197
	step [133/254], loss=8.1803
	step [134/254], loss=8.6464
	step [135/254], loss=7.7138
	step [136/254], loss=6.0479
	step [137/254], loss=10.0615
	step [138/254], loss=7.2358
	step [139/254], loss=8.1076
	step [140/254], loss=7.5556
	step [141/254], loss=9.2242
	step [142/254], loss=9.3188
	step [143/254], loss=7.6562
	step [144/254], loss=9.4885
	step [145/254], loss=7.7277
	step [146/254], loss=8.1069
	step [147/254], loss=10.4299
	step [148/254], loss=8.9472
	step [149/254], loss=8.1242
	step [150/254], loss=8.7838
	step [151/254], loss=8.7299
	step [152/254], loss=9.3124
	step [153/254], loss=8.7710
	step [154/254], loss=8.1377
	step [155/254], loss=8.8524
	step [156/254], loss=10.6970
	step [157/254], loss=9.1803
	step [158/254], loss=9.7148
	step [159/254], loss=10.3479
	step [160/254], loss=8.9127
	step [161/254], loss=8.5374
	step [162/254], loss=7.7767
	step [163/254], loss=7.5076
	step [164/254], loss=8.9686
	step [165/254], loss=8.0639
	step [166/254], loss=12.0939
	step [167/254], loss=7.8734
	step [168/254], loss=8.6514
	step [169/254], loss=7.8848
	step [170/254], loss=7.4299
	step [171/254], loss=12.5775
	step [172/254], loss=8.2654
	step [173/254], loss=8.4421
	step [174/254], loss=9.1696
	step [175/254], loss=10.1150
	step [176/254], loss=7.9740
	step [177/254], loss=9.1623
	step [178/254], loss=8.3420
	step [179/254], loss=7.5337
	step [180/254], loss=7.6219
	step [181/254], loss=9.5926
	step [182/254], loss=8.6299
	step [183/254], loss=7.3916
	step [184/254], loss=9.9685
	step [185/254], loss=9.3231
	step [186/254], loss=7.1673
	step [187/254], loss=6.9858
	step [188/254], loss=9.3737
	step [189/254], loss=8.1367
	step [190/254], loss=6.8291
	step [191/254], loss=6.5867
	step [192/254], loss=10.4311
	step [193/254], loss=7.5367
	step [194/254], loss=8.1524
	step [195/254], loss=9.4568
	step [196/254], loss=7.0646
	step [197/254], loss=8.5886
	step [198/254], loss=7.9979
	step [199/254], loss=9.0274
	step [200/254], loss=8.8080
	step [201/254], loss=7.7572
	step [202/254], loss=9.5305
	step [203/254], loss=11.3311
	step [204/254], loss=7.3330
	step [205/254], loss=7.6566
	step [206/254], loss=6.6793
	step [207/254], loss=8.6948
	step [208/254], loss=10.2139
	step [209/254], loss=9.0806
	step [210/254], loss=8.4812
	step [211/254], loss=7.4902
	step [212/254], loss=9.3420
	step [213/254], loss=8.4252
	step [214/254], loss=8.2390
	step [215/254], loss=7.2209
	step [216/254], loss=7.5486
	step [217/254], loss=11.8980
	step [218/254], loss=6.5871
	step [219/254], loss=9.2311
	step [220/254], loss=7.0848
	step [221/254], loss=7.4694
	step [222/254], loss=7.0628
	step [223/254], loss=6.8414
	step [224/254], loss=8.3647
	step [225/254], loss=9.4650
	step [226/254], loss=9.1569
	step [227/254], loss=7.9634
	step [228/254], loss=8.4672
	step [229/254], loss=9.5819
	step [230/254], loss=8.0613
	step [231/254], loss=9.5398
	step [232/254], loss=7.7688
	step [233/254], loss=9.0678
	step [234/254], loss=8.9973
	step [235/254], loss=9.0916
	step [236/254], loss=7.8334
	step [237/254], loss=8.4822
	step [238/254], loss=8.5815
	step [239/254], loss=7.3071
	step [240/254], loss=7.7199
	step [241/254], loss=8.7905
	step [242/254], loss=7.1285
	step [243/254], loss=9.0665
	step [244/254], loss=7.3381
	step [245/254], loss=7.8976
	step [246/254], loss=7.9156
	step [247/254], loss=8.3296
	step [248/254], loss=8.5874
	step [249/254], loss=8.6886
	step [250/254], loss=7.7165
	step [251/254], loss=8.5611
	step [252/254], loss=9.5454
	step [253/254], loss=8.4911
	step [254/254], loss=1.0376
	Evaluating
	loss=0.0400, precision=0.1700, recall=0.9961, f1=0.2904
Training epoch 12
	step [1/254], loss=7.7018
	step [2/254], loss=8.4379
	step [3/254], loss=10.3015
	step [4/254], loss=7.3093
	step [5/254], loss=10.4215
	step [6/254], loss=8.0979
	step [7/254], loss=10.3664
	step [8/254], loss=7.5880
	step [9/254], loss=9.0100
	step [10/254], loss=7.1618
	step [11/254], loss=7.2674
	step [12/254], loss=8.0330
	step [13/254], loss=10.9766
	step [14/254], loss=8.1322
	step [15/254], loss=9.2718
	step [16/254], loss=7.1208
	step [17/254], loss=8.9468
	step [18/254], loss=6.7152
	step [19/254], loss=9.1961
	step [20/254], loss=8.0169
	step [21/254], loss=9.4679
	step [22/254], loss=7.9801
	step [23/254], loss=7.4218
	step [24/254], loss=7.2010
	step [25/254], loss=8.4659
	step [26/254], loss=8.2215
	step [27/254], loss=7.7103
	step [28/254], loss=8.4868
	step [29/254], loss=7.1404
	step [30/254], loss=7.8304
	step [31/254], loss=8.9271
	step [32/254], loss=8.3100
	step [33/254], loss=9.8149
	step [34/254], loss=9.0504
	step [35/254], loss=8.0001
	step [36/254], loss=11.0805
	step [37/254], loss=9.0084
	step [38/254], loss=8.0448
	step [39/254], loss=8.7161
	step [40/254], loss=8.7225
	step [41/254], loss=8.5953
	step [42/254], loss=6.5461
	step [43/254], loss=9.0482
	step [44/254], loss=6.6637
	step [45/254], loss=6.5784
	step [46/254], loss=7.7004
	step [47/254], loss=9.8860
	step [48/254], loss=8.0654
	step [49/254], loss=6.7919
	step [50/254], loss=7.5187
	step [51/254], loss=6.6517
	step [52/254], loss=9.5869
	step [53/254], loss=6.3183
	step [54/254], loss=7.3366
	step [55/254], loss=9.8177
	step [56/254], loss=7.1325
	step [57/254], loss=9.3100
	step [58/254], loss=8.7481
	step [59/254], loss=7.5246
	step [60/254], loss=8.3045
	step [61/254], loss=9.4032
	step [62/254], loss=9.5687
	step [63/254], loss=8.4146
	step [64/254], loss=7.2507
	step [65/254], loss=8.1833
	step [66/254], loss=8.2748
	step [67/254], loss=7.2043
	step [68/254], loss=7.9335
	step [69/254], loss=7.7592
	step [70/254], loss=8.4733
	step [71/254], loss=8.9079
	step [72/254], loss=11.8733
	step [73/254], loss=8.7632
	step [74/254], loss=8.5523
	step [75/254], loss=7.1035
	step [76/254], loss=7.3580
	step [77/254], loss=7.5780
	step [78/254], loss=7.0335
	step [79/254], loss=8.6719
	step [80/254], loss=8.2266
	step [81/254], loss=8.9377
	step [82/254], loss=7.2200
	step [83/254], loss=9.2478
	step [84/254], loss=10.1215
	step [85/254], loss=7.6814
	step [86/254], loss=9.8668
	step [87/254], loss=8.6918
	step [88/254], loss=7.5913
	step [89/254], loss=10.7426
	step [90/254], loss=7.9723
	step [91/254], loss=8.4079
	step [92/254], loss=7.2677
	step [93/254], loss=7.2946
	step [94/254], loss=7.2495
	step [95/254], loss=8.4184
	step [96/254], loss=7.7033
	step [97/254], loss=7.2381
	step [98/254], loss=7.8348
	step [99/254], loss=8.4331
	step [100/254], loss=7.8096
	step [101/254], loss=8.1699
	step [102/254], loss=7.6727
	step [103/254], loss=8.5460
	step [104/254], loss=8.4731
	step [105/254], loss=7.1182
	step [106/254], loss=10.4036
	step [107/254], loss=5.6954
	step [108/254], loss=9.0914
	step [109/254], loss=7.0364
	step [110/254], loss=7.4107
	step [111/254], loss=6.9345
	step [112/254], loss=8.7629
	step [113/254], loss=7.7315
	step [114/254], loss=7.4288
	step [115/254], loss=7.6107
	step [116/254], loss=7.7490
	step [117/254], loss=8.0201
	step [118/254], loss=8.4200
	step [119/254], loss=7.9932
	step [120/254], loss=7.8295
	step [121/254], loss=7.6958
	step [122/254], loss=8.5708
	step [123/254], loss=8.1024
	step [124/254], loss=10.1843
	step [125/254], loss=8.0413
	step [126/254], loss=7.3708
	step [127/254], loss=8.8078
	step [128/254], loss=7.8500
	step [129/254], loss=9.4680
	step [130/254], loss=7.8982
	step [131/254], loss=8.0169
	step [132/254], loss=7.4590
	step [133/254], loss=7.4883
	step [134/254], loss=7.8046
	step [135/254], loss=7.5457
	step [136/254], loss=8.6013
	step [137/254], loss=6.3114
	step [138/254], loss=8.2415
	step [139/254], loss=6.8388
	step [140/254], loss=7.7645
	step [141/254], loss=8.6303
	step [142/254], loss=7.3802
	step [143/254], loss=6.9075
	step [144/254], loss=9.8938
	step [145/254], loss=9.5115
	step [146/254], loss=8.7518
	step [147/254], loss=7.0729
	step [148/254], loss=8.2780
	step [149/254], loss=8.2664
	step [150/254], loss=8.9673
	step [151/254], loss=7.4745
	step [152/254], loss=7.0651
	step [153/254], loss=7.4810
	step [154/254], loss=7.1913
	step [155/254], loss=10.1133
	step [156/254], loss=7.5536
	step [157/254], loss=9.8528
	step [158/254], loss=8.8661
	step [159/254], loss=8.0732
	step [160/254], loss=8.3996
	step [161/254], loss=6.8214
	step [162/254], loss=8.2520
	step [163/254], loss=8.1438
	step [164/254], loss=5.9062
	step [165/254], loss=8.2124
	step [166/254], loss=12.2979
	step [167/254], loss=7.3171
	step [168/254], loss=8.4749
	step [169/254], loss=7.5614
	step [170/254], loss=7.0547
	step [171/254], loss=8.1538
	step [172/254], loss=6.8552
	step [173/254], loss=8.2280
	step [174/254], loss=9.0809
	step [175/254], loss=8.3876
	step [176/254], loss=7.8595
	step [177/254], loss=7.3052
	step [178/254], loss=8.9639
	step [179/254], loss=7.1076
	step [180/254], loss=7.2384
	step [181/254], loss=7.1266
	step [182/254], loss=8.1868
	step [183/254], loss=8.2530
	step [184/254], loss=8.9316
	step [185/254], loss=8.4108
	step [186/254], loss=8.4476
	step [187/254], loss=7.2074
	step [188/254], loss=9.6073
	step [189/254], loss=10.3459
	step [190/254], loss=8.9658
	step [191/254], loss=7.8745
	step [192/254], loss=9.3751
	step [193/254], loss=7.6109
	step [194/254], loss=6.7651
	step [195/254], loss=8.1961
	step [196/254], loss=7.0889
	step [197/254], loss=9.5924
	step [198/254], loss=8.7747
	step [199/254], loss=8.6073
	step [200/254], loss=7.0857
	step [201/254], loss=7.1441
	step [202/254], loss=9.0700
	step [203/254], loss=7.6799
	step [204/254], loss=6.9925
	step [205/254], loss=7.8451
	step [206/254], loss=8.2463
	step [207/254], loss=7.5407
	step [208/254], loss=9.9346
	step [209/254], loss=8.7684
	step [210/254], loss=7.4203
	step [211/254], loss=6.6436
	step [212/254], loss=8.1600
	step [213/254], loss=8.0242
	step [214/254], loss=11.8272
	step [215/254], loss=8.2585
	step [216/254], loss=6.9283
	step [217/254], loss=8.1936
	step [218/254], loss=9.2976
	step [219/254], loss=7.9865
	step [220/254], loss=6.6385
	step [221/254], loss=7.1076
	step [222/254], loss=8.0831
	step [223/254], loss=8.8763
	step [224/254], loss=9.9940
	step [225/254], loss=6.8850
	step [226/254], loss=7.1488
	step [227/254], loss=8.1894
	step [228/254], loss=8.1954
	step [229/254], loss=7.8548
	step [230/254], loss=7.5661
	step [231/254], loss=7.5654
	step [232/254], loss=8.0054
	step [233/254], loss=8.6219
	step [234/254], loss=7.4095
	step [235/254], loss=8.0158
	step [236/254], loss=7.1958
	step [237/254], loss=9.7192
	step [238/254], loss=10.3119
	step [239/254], loss=8.7128
	step [240/254], loss=7.9496
	step [241/254], loss=9.4856
	step [242/254], loss=6.7195
	step [243/254], loss=7.5235
	step [244/254], loss=6.8512
	step [245/254], loss=8.3077
	step [246/254], loss=6.8834
	step [247/254], loss=5.4750
	step [248/254], loss=9.3593
	step [249/254], loss=9.7555
	step [250/254], loss=6.3873
	step [251/254], loss=7.8398
	step [252/254], loss=7.3115
	step [253/254], loss=6.9821
	step [254/254], loss=1.8464
	Evaluating
	loss=0.0264, precision=0.2621, recall=0.9920, f1=0.4147
saving model as: 1_saved_model.pth
Training epoch 13
	step [1/254], loss=6.9987
	step [2/254], loss=8.8487
	step [3/254], loss=6.4168
	step [4/254], loss=7.2466
	step [5/254], loss=6.5650
	step [6/254], loss=7.8583
	step [7/254], loss=7.1895
	step [8/254], loss=8.5047
	step [9/254], loss=8.3751
	step [10/254], loss=6.4323
	step [11/254], loss=7.3795
	step [12/254], loss=8.2706
	step [13/254], loss=8.9134
	step [14/254], loss=8.1918
	step [15/254], loss=8.3450
	step [16/254], loss=8.1569
	step [17/254], loss=9.5638
	step [18/254], loss=6.7849
	step [19/254], loss=8.6402
	step [20/254], loss=8.1443
	step [21/254], loss=7.6333
	step [22/254], loss=9.8921
	step [23/254], loss=7.9136
	step [24/254], loss=9.4290
	step [25/254], loss=6.3323
	step [26/254], loss=6.6381
	step [27/254], loss=7.5692
	step [28/254], loss=8.5937
	step [29/254], loss=7.8963
	step [30/254], loss=7.2884
	step [31/254], loss=7.0257
	step [32/254], loss=6.3678
	step [33/254], loss=6.8498
	step [34/254], loss=6.2840
	step [35/254], loss=7.4112
	step [36/254], loss=7.9063
	step [37/254], loss=7.0574
	step [38/254], loss=6.6529
	step [39/254], loss=14.8068
	step [40/254], loss=7.4724
	step [41/254], loss=8.6504
	step [42/254], loss=8.9319
	step [43/254], loss=7.1852
	step [44/254], loss=6.9678
	step [45/254], loss=7.7478
	step [46/254], loss=9.0892
	step [47/254], loss=8.9432
	step [48/254], loss=7.4183
	step [49/254], loss=9.0298
	step [50/254], loss=8.0402
	step [51/254], loss=6.7973
	step [52/254], loss=7.3733
	step [53/254], loss=8.8804
	step [54/254], loss=7.6253
	step [55/254], loss=5.5085
	step [56/254], loss=8.4950
	step [57/254], loss=6.4626
	step [58/254], loss=10.9516
	step [59/254], loss=9.9965
	step [60/254], loss=7.5983
	step [61/254], loss=6.8219
	step [62/254], loss=9.5671
	step [63/254], loss=8.4174
	step [64/254], loss=7.9224
	step [65/254], loss=7.1878
	step [66/254], loss=7.0757
	step [67/254], loss=6.7344
	step [68/254], loss=7.3284
	step [69/254], loss=9.8808
	step [70/254], loss=7.9296
	step [71/254], loss=9.8406
	step [72/254], loss=7.9058
	step [73/254], loss=9.1508
	step [74/254], loss=7.0031
	step [75/254], loss=7.7252
	step [76/254], loss=7.5539
	step [77/254], loss=7.6184
	step [78/254], loss=7.4785
	step [79/254], loss=7.9919
	step [80/254], loss=6.0507
	step [81/254], loss=8.2171
	step [82/254], loss=8.3586
	step [83/254], loss=8.1026
	step [84/254], loss=7.4610
	step [85/254], loss=7.8493
	step [86/254], loss=7.4933
	step [87/254], loss=8.2757
	step [88/254], loss=9.1497
	step [89/254], loss=8.6124
	step [90/254], loss=8.4746
	step [91/254], loss=9.7131
	step [92/254], loss=9.6152
	step [93/254], loss=8.1090
	step [94/254], loss=7.0534
	step [95/254], loss=9.6673
	step [96/254], loss=8.1375
	step [97/254], loss=7.3439
	step [98/254], loss=8.7805
	step [99/254], loss=7.5915
	step [100/254], loss=9.0498
	step [101/254], loss=6.3220
	step [102/254], loss=9.0222
	step [103/254], loss=7.3177
	step [104/254], loss=7.7422
	step [105/254], loss=7.6360
	step [106/254], loss=7.9052
	step [107/254], loss=6.7092
	step [108/254], loss=8.5875
	step [109/254], loss=6.7159
	step [110/254], loss=8.5660
	step [111/254], loss=8.5721
	step [112/254], loss=7.5951
	step [113/254], loss=8.5079
	step [114/254], loss=9.4354
	step [115/254], loss=7.9838
	step [116/254], loss=8.0870
	step [117/254], loss=6.7211
	step [118/254], loss=8.5668
	step [119/254], loss=9.6949
	step [120/254], loss=7.4931
	step [121/254], loss=8.2239
	step [122/254], loss=10.1079
	step [123/254], loss=9.0499
	step [124/254], loss=7.7479
	step [125/254], loss=5.7266
	step [126/254], loss=10.0957
	step [127/254], loss=7.2865
	step [128/254], loss=8.5131
	step [129/254], loss=6.7228
	step [130/254], loss=6.8550
	step [131/254], loss=8.6979
	step [132/254], loss=5.2866
	step [133/254], loss=8.0667
	step [134/254], loss=7.7402
	step [135/254], loss=6.6799
	step [136/254], loss=6.4255
	step [137/254], loss=6.6924
	step [138/254], loss=7.0798
	step [139/254], loss=7.9268
	step [140/254], loss=11.5165
	step [141/254], loss=7.5137
	step [142/254], loss=8.2538
	step [143/254], loss=8.0461
	step [144/254], loss=9.8402
	step [145/254], loss=6.8795
	step [146/254], loss=6.5230
	step [147/254], loss=6.7844
	step [148/254], loss=7.5280
	step [149/254], loss=7.7061
	step [150/254], loss=6.8957
	step [151/254], loss=7.0373
	step [152/254], loss=7.2768
	step [153/254], loss=7.6062
	step [154/254], loss=7.5707
	step [155/254], loss=7.0963
	step [156/254], loss=7.7193
	step [157/254], loss=8.1335
	step [158/254], loss=8.3108
	step [159/254], loss=8.5671
	step [160/254], loss=7.2001
	step [161/254], loss=8.5126
	step [162/254], loss=8.7375
	step [163/254], loss=7.4615
	step [164/254], loss=7.1384
	step [165/254], loss=7.3493
	step [166/254], loss=7.4673
	step [167/254], loss=8.0058
	step [168/254], loss=6.4065
	step [169/254], loss=6.5399
	step [170/254], loss=8.5689
	step [171/254], loss=8.0542
	step [172/254], loss=5.8294
	step [173/254], loss=6.1430
	step [174/254], loss=8.5087
	step [175/254], loss=7.6069
	step [176/254], loss=6.5524
	step [177/254], loss=8.2660
	step [178/254], loss=8.5374
	step [179/254], loss=7.3923
	step [180/254], loss=8.6985
	step [181/254], loss=6.5803
	step [182/254], loss=7.8582
	step [183/254], loss=6.8592
	step [184/254], loss=6.4070
	step [185/254], loss=6.3673
	step [186/254], loss=7.4116
	step [187/254], loss=9.7449
	step [188/254], loss=8.0690
	step [189/254], loss=8.4049
	step [190/254], loss=7.4932
	step [191/254], loss=7.8857
	step [192/254], loss=7.8612
	step [193/254], loss=7.6302
	step [194/254], loss=7.9335
	step [195/254], loss=8.5577
	step [196/254], loss=8.1632
	step [197/254], loss=6.7339
	step [198/254], loss=8.6747
	step [199/254], loss=7.3060
	step [200/254], loss=6.6848
	step [201/254], loss=8.3583
	step [202/254], loss=8.2356
	step [203/254], loss=7.4474
	step [204/254], loss=6.3014
	step [205/254], loss=8.9757
	step [206/254], loss=5.9035
	step [207/254], loss=8.4297
	step [208/254], loss=7.1796
	step [209/254], loss=8.0384
	step [210/254], loss=6.8420
	step [211/254], loss=7.1905
	step [212/254], loss=7.7401
	step [213/254], loss=7.7714
	step [214/254], loss=8.2513
	step [215/254], loss=6.2218
	step [216/254], loss=6.6696
	step [217/254], loss=8.8044
	step [218/254], loss=10.2443
	step [219/254], loss=9.2773
	step [220/254], loss=8.0814
	step [221/254], loss=5.9611
	step [222/254], loss=7.2304
	step [223/254], loss=7.0612
	step [224/254], loss=7.0885
	step [225/254], loss=6.4734
	step [226/254], loss=6.6308
	step [227/254], loss=9.5469
	step [228/254], loss=5.6164
	step [229/254], loss=6.8603
	step [230/254], loss=8.0800
	step [231/254], loss=8.9096
	step [232/254], loss=9.0553
	step [233/254], loss=7.0699
	step [234/254], loss=6.9776
	step [235/254], loss=7.4547
	step [236/254], loss=7.2616
	step [237/254], loss=7.5700
	step [238/254], loss=7.6446
	step [239/254], loss=9.5196
	step [240/254], loss=10.0435
	step [241/254], loss=6.6731
	step [242/254], loss=6.9090
	step [243/254], loss=8.0536
	step [244/254], loss=8.5971
	step [245/254], loss=8.8323
	step [246/254], loss=8.4979
	step [247/254], loss=9.4358
	step [248/254], loss=5.8856
	step [249/254], loss=7.7094
	step [250/254], loss=7.8850
	step [251/254], loss=7.0921
	step [252/254], loss=7.2499
	step [253/254], loss=6.4460
	step [254/254], loss=1.2644
	Evaluating
	loss=0.0273, precision=0.2177, recall=0.9926, f1=0.3570
Training epoch 14
	step [1/254], loss=7.7250
	step [2/254], loss=11.2853
	step [3/254], loss=5.9283
	step [4/254], loss=7.9221
	step [5/254], loss=8.0318
	step [6/254], loss=8.1085
	step [7/254], loss=6.0103
	step [8/254], loss=7.0484
	step [9/254], loss=7.8012
	step [10/254], loss=9.6683
	step [11/254], loss=7.5391
	step [12/254], loss=7.0591
	step [13/254], loss=7.4735
	step [14/254], loss=9.4389
	step [15/254], loss=7.6650
	step [16/254], loss=8.0544
	step [17/254], loss=7.9750
	step [18/254], loss=7.2464
	step [19/254], loss=6.1538
	step [20/254], loss=7.5067
	step [21/254], loss=6.5099
	step [22/254], loss=7.7943
	step [23/254], loss=6.5363
	step [24/254], loss=8.0315
	step [25/254], loss=5.9380
	step [26/254], loss=6.8317
	step [27/254], loss=6.7187
	step [28/254], loss=6.0969
	step [29/254], loss=7.9414
	step [30/254], loss=9.6936
	step [31/254], loss=6.9806
	step [32/254], loss=7.0988
	step [33/254], loss=7.3663
	step [34/254], loss=6.5510
	step [35/254], loss=7.1250
	step [36/254], loss=6.1839
	step [37/254], loss=8.8856
	step [38/254], loss=8.4155
	step [39/254], loss=7.4253
	step [40/254], loss=7.1747
	step [41/254], loss=8.8685
	step [42/254], loss=7.2238
	step [43/254], loss=8.0750
	step [44/254], loss=6.2928
	step [45/254], loss=8.3724
	step [46/254], loss=6.3628
	step [47/254], loss=6.6110
	step [48/254], loss=5.3577
	step [49/254], loss=8.4474
	step [50/254], loss=6.6175
	step [51/254], loss=11.0284
	step [52/254], loss=7.5295
	step [53/254], loss=6.9763
	step [54/254], loss=8.4621
	step [55/254], loss=7.8547
	step [56/254], loss=6.9192
	step [57/254], loss=6.6183
	step [58/254], loss=7.2164
	step [59/254], loss=7.2452
	step [60/254], loss=8.5184
	step [61/254], loss=7.8471
	step [62/254], loss=6.8613
	step [63/254], loss=9.4458
	step [64/254], loss=7.3483
	step [65/254], loss=5.9003
	step [66/254], loss=8.8206
	step [67/254], loss=7.4485
	step [68/254], loss=7.1351
	step [69/254], loss=9.0180
	step [70/254], loss=7.1023
	step [71/254], loss=7.0154
	step [72/254], loss=7.1556
	step [73/254], loss=8.6872
	step [74/254], loss=8.1020
	step [75/254], loss=7.3789
	step [76/254], loss=7.2845
	step [77/254], loss=7.1144
	step [78/254], loss=8.3930
	step [79/254], loss=7.7724
	step [80/254], loss=7.7471
	step [81/254], loss=6.6071
	step [82/254], loss=7.1148
	step [83/254], loss=6.2047
	step [84/254], loss=7.0530
	step [85/254], loss=8.6209
	step [86/254], loss=7.0578
	step [87/254], loss=9.6348
	step [88/254], loss=6.6266
	step [89/254], loss=6.8471
	step [90/254], loss=8.8360
	step [91/254], loss=7.2585
	step [92/254], loss=7.7102
	step [93/254], loss=8.2469
	step [94/254], loss=9.2558
	step [95/254], loss=6.0773
	step [96/254], loss=7.0352
	step [97/254], loss=6.4800
	step [98/254], loss=7.7158
	step [99/254], loss=8.1276
	step [100/254], loss=6.6388
	step [101/254], loss=7.6723
	step [102/254], loss=7.8502
	step [103/254], loss=6.6766
	step [104/254], loss=7.9017
	step [105/254], loss=9.3962
	step [106/254], loss=7.1656
	step [107/254], loss=8.0382
	step [108/254], loss=8.7902
	step [109/254], loss=8.9654
	step [110/254], loss=6.3197
	step [111/254], loss=7.3825
	step [112/254], loss=7.7711
	step [113/254], loss=9.5841
	step [114/254], loss=6.5234
	step [115/254], loss=7.0715
	step [116/254], loss=7.9768
	step [117/254], loss=7.9897
	step [118/254], loss=7.5188
	step [119/254], loss=7.9544
	step [120/254], loss=6.3545
	step [121/254], loss=7.0686
	step [122/254], loss=7.6117
	step [123/254], loss=8.4722
	step [124/254], loss=9.0401
	step [125/254], loss=6.8285
	step [126/254], loss=7.6671
	step [127/254], loss=8.4885
	step [128/254], loss=8.9104
	step [129/254], loss=7.0870
	step [130/254], loss=7.0363
	step [131/254], loss=6.8362
	step [132/254], loss=6.7260
	step [133/254], loss=7.1627
	step [134/254], loss=9.9206
	step [135/254], loss=6.0195
	step [136/254], loss=7.3624
	step [137/254], loss=8.1798
	step [138/254], loss=7.5834
	step [139/254], loss=6.5478
	step [140/254], loss=6.0546
	step [141/254], loss=8.7630
	step [142/254], loss=5.9889
	step [143/254], loss=6.1691
	step [144/254], loss=6.1680
	step [145/254], loss=7.2756
	step [146/254], loss=6.5786
	step [147/254], loss=7.4851
	step [148/254], loss=8.3050
	step [149/254], loss=7.4996
	step [150/254], loss=7.8276
	step [151/254], loss=7.9213
	step [152/254], loss=6.1600
	step [153/254], loss=7.3792
	step [154/254], loss=8.5574
	step [155/254], loss=7.8463
	step [156/254], loss=7.6990
	step [157/254], loss=8.3725
	step [158/254], loss=7.3935
	step [159/254], loss=8.0195
	step [160/254], loss=7.3364
	step [161/254], loss=8.4345
	step [162/254], loss=6.8304
	step [163/254], loss=6.6367
	step [164/254], loss=8.0774
	step [165/254], loss=8.3267
	step [166/254], loss=8.4096
	step [167/254], loss=7.5875
	step [168/254], loss=8.3455
	step [169/254], loss=8.4384
	step [170/254], loss=7.1284
	step [171/254], loss=6.2039
	step [172/254], loss=8.4790
	step [173/254], loss=7.4244
	step [174/254], loss=7.4615
	step [175/254], loss=6.4305
	step [176/254], loss=6.7518
	step [177/254], loss=8.8483
	step [178/254], loss=7.8643
	step [179/254], loss=6.4764
	step [180/254], loss=8.5113
	step [181/254], loss=7.1931
	step [182/254], loss=5.9353
	step [183/254], loss=8.0257
	step [184/254], loss=8.8424
	step [185/254], loss=7.2254
	step [186/254], loss=7.7925
	step [187/254], loss=7.0118
	step [188/254], loss=7.1621
	step [189/254], loss=8.0465
	step [190/254], loss=6.2833
	step [191/254], loss=6.7486
	step [192/254], loss=9.0475
	step [193/254], loss=7.0742
	step [194/254], loss=5.3570
	step [195/254], loss=8.3080
	step [196/254], loss=8.3148
	step [197/254], loss=6.4546
	step [198/254], loss=7.8830
	step [199/254], loss=7.3811
	step [200/254], loss=7.3348
	step [201/254], loss=6.8155
	step [202/254], loss=9.4900
	step [203/254], loss=8.8423
	step [204/254], loss=8.4800
	step [205/254], loss=6.6980
	step [206/254], loss=7.5682
	step [207/254], loss=8.3988
	step [208/254], loss=7.1729
	step [209/254], loss=8.9182
	step [210/254], loss=8.9355
	step [211/254], loss=5.4501
	step [212/254], loss=6.2229
	step [213/254], loss=5.9324
	step [214/254], loss=5.7633
	step [215/254], loss=7.1755
	step [216/254], loss=7.3757
	step [217/254], loss=8.6800
	step [218/254], loss=9.2603
	step [219/254], loss=6.0936
	step [220/254], loss=7.5255
	step [221/254], loss=7.2561
	step [222/254], loss=7.0902
	step [223/254], loss=5.7860
	step [224/254], loss=7.5540
	step [225/254], loss=7.3576
	step [226/254], loss=7.9499
	step [227/254], loss=7.7140
	step [228/254], loss=6.0888
	step [229/254], loss=7.0625
	step [230/254], loss=8.0978
	step [231/254], loss=8.0552
	step [232/254], loss=8.4338
	step [233/254], loss=7.1958
	step [234/254], loss=7.7308
	step [235/254], loss=7.4788
	step [236/254], loss=7.9929
	step [237/254], loss=6.6957
	step [238/254], loss=8.1295
	step [239/254], loss=7.3329
	step [240/254], loss=5.5707
	step [241/254], loss=6.8124
	step [242/254], loss=9.0458
	step [243/254], loss=7.8169
	step [244/254], loss=7.7479
	step [245/254], loss=5.7966
	step [246/254], loss=7.8803
	step [247/254], loss=9.6416
	step [248/254], loss=7.8844
	step [249/254], loss=8.5669
	step [250/254], loss=6.4513
	step [251/254], loss=8.4899
	step [252/254], loss=7.2366
	step [253/254], loss=7.0893
	step [254/254], loss=2.1309
	Evaluating
	loss=0.0206, precision=0.2803, recall=0.9907, f1=0.4369
saving model as: 1_saved_model.pth
Training epoch 15
	step [1/254], loss=8.9983
	step [2/254], loss=6.4310
	step [3/254], loss=8.6411
	step [4/254], loss=7.4718
	step [5/254], loss=6.3388
	step [6/254], loss=6.9281
	step [7/254], loss=8.6267
	step [8/254], loss=8.3685
	step [9/254], loss=7.4122
	step [10/254], loss=7.3823
	step [11/254], loss=7.5974
	step [12/254], loss=7.2245
	step [13/254], loss=7.5362
	step [14/254], loss=7.2367
	step [15/254], loss=8.6556
	step [16/254], loss=5.7846
	step [17/254], loss=7.5239
	step [18/254], loss=10.9011
	step [19/254], loss=6.4622
	step [20/254], loss=6.5161
	step [21/254], loss=6.5217
	step [22/254], loss=7.4561
	step [23/254], loss=7.4331
	step [24/254], loss=7.5368
	step [25/254], loss=8.7966
	step [26/254], loss=6.5800
	step [27/254], loss=6.5435
	step [28/254], loss=6.4003
	step [29/254], loss=7.0615
	step [30/254], loss=7.9407
	step [31/254], loss=6.1963
	step [32/254], loss=5.3564
	step [33/254], loss=6.3583
	step [34/254], loss=9.7230
	step [35/254], loss=7.7069
	step [36/254], loss=9.1178
	step [37/254], loss=6.4720
	step [38/254], loss=6.9134
	step [39/254], loss=8.5533
	step [40/254], loss=7.8665
	step [41/254], loss=7.4017
	step [42/254], loss=7.7307
	step [43/254], loss=7.1829
	step [44/254], loss=6.3489
	step [45/254], loss=6.7032
	step [46/254], loss=8.8199
	step [47/254], loss=8.8879
	step [48/254], loss=7.2158
	step [49/254], loss=5.8339
	step [50/254], loss=6.7313
	step [51/254], loss=7.6582
	step [52/254], loss=6.3421
	step [53/254], loss=6.7560
	step [54/254], loss=7.4578
	step [55/254], loss=5.6281
	step [56/254], loss=7.1111
	step [57/254], loss=7.2452
	step [58/254], loss=5.8065
	step [59/254], loss=6.9724
	step [60/254], loss=7.0089
	step [61/254], loss=5.2216
	step [62/254], loss=8.2393
	step [63/254], loss=6.7064
	step [64/254], loss=6.3948
	step [65/254], loss=6.2113
	step [66/254], loss=7.8372
	step [67/254], loss=7.8700
	step [68/254], loss=6.9201
	step [69/254], loss=8.0514
	step [70/254], loss=8.2341
	step [71/254], loss=8.1164
	step [72/254], loss=7.6899
	step [73/254], loss=5.9427
	step [74/254], loss=8.8321
	step [75/254], loss=8.6912
	step [76/254], loss=7.4684
	step [77/254], loss=6.8830
	step [78/254], loss=6.4692
	step [79/254], loss=6.2526
	step [80/254], loss=7.1141
	step [81/254], loss=7.3751
	step [82/254], loss=7.3708
	step [83/254], loss=7.4080
	step [84/254], loss=8.5976
	step [85/254], loss=6.5578
	step [86/254], loss=7.2255
	step [87/254], loss=6.6637
	step [88/254], loss=7.2977
	step [89/254], loss=7.1891
	step [90/254], loss=7.7277
	step [91/254], loss=5.4264
	step [92/254], loss=7.1739
	step [93/254], loss=6.9676
	step [94/254], loss=7.5224
	step [95/254], loss=6.9360
	step [96/254], loss=7.0065
	step [97/254], loss=8.6358
	step [98/254], loss=5.5777
	step [99/254], loss=6.3412
	step [100/254], loss=6.6281
	step [101/254], loss=7.1987
	step [102/254], loss=7.5784
	step [103/254], loss=8.3063
	step [104/254], loss=9.7307
	step [105/254], loss=5.6176
	step [106/254], loss=8.8573
	step [107/254], loss=7.8123
	step [108/254], loss=6.4847
	step [109/254], loss=5.1899
	step [110/254], loss=6.4966
	step [111/254], loss=7.6547
	step [112/254], loss=7.1382
	step [113/254], loss=7.4236
	step [114/254], loss=6.7332
	step [115/254], loss=6.1552
	step [116/254], loss=9.9010
	step [117/254], loss=6.5134
	step [118/254], loss=7.5881
	step [119/254], loss=8.1496
	step [120/254], loss=10.8442
	step [121/254], loss=7.7665
	step [122/254], loss=7.5038
	step [123/254], loss=7.3710
	step [124/254], loss=7.6286
	step [125/254], loss=6.9445
	step [126/254], loss=7.6929
	step [127/254], loss=11.5454
	step [128/254], loss=7.1412
	step [129/254], loss=7.5032
	step [130/254], loss=6.6210
	step [131/254], loss=8.4919
	step [132/254], loss=7.2139
	step [133/254], loss=8.8705
	step [134/254], loss=7.8218
	step [135/254], loss=6.0078
	step [136/254], loss=7.6203
	step [137/254], loss=6.7597
	step [138/254], loss=8.2648
	step [139/254], loss=6.5825
	step [140/254], loss=6.3663
	step [141/254], loss=7.4251
	step [142/254], loss=7.1847
	step [143/254], loss=7.3216
	step [144/254], loss=7.5539
	step [145/254], loss=6.7822
	step [146/254], loss=7.4303
	step [147/254], loss=5.3270
	step [148/254], loss=8.0159
	step [149/254], loss=6.5901
	step [150/254], loss=6.8838
	step [151/254], loss=6.3588
	step [152/254], loss=5.8717
	step [153/254], loss=6.3521
	step [154/254], loss=6.4523
	step [155/254], loss=6.1341
	step [156/254], loss=10.0959
	step [157/254], loss=5.5158
	step [158/254], loss=6.2353
	step [159/254], loss=8.9883
	step [160/254], loss=8.7841
	step [161/254], loss=6.2773
	step [162/254], loss=7.0072
	step [163/254], loss=7.3606
	step [164/254], loss=8.3579
	step [165/254], loss=7.8603
	step [166/254], loss=6.1798
	step [167/254], loss=6.2058
	step [168/254], loss=10.7566
	step [169/254], loss=8.6186
	step [170/254], loss=10.0948
	step [171/254], loss=6.3506
	step [172/254], loss=5.4604
	step [173/254], loss=8.1586
	step [174/254], loss=5.8539
	step [175/254], loss=8.5232
	step [176/254], loss=5.9995
	step [177/254], loss=6.8839
	step [178/254], loss=9.1937
	step [179/254], loss=7.7366
	step [180/254], loss=6.0833
	step [181/254], loss=6.8138
	step [182/254], loss=6.9435
	step [183/254], loss=7.3556
	step [184/254], loss=6.6718
	step [185/254], loss=7.1929
	step [186/254], loss=6.7581
	step [187/254], loss=6.8250
	step [188/254], loss=8.2972
	step [189/254], loss=6.0342
	step [190/254], loss=6.7095
	step [191/254], loss=8.4562
	step [192/254], loss=8.7976
	step [193/254], loss=6.1230
	step [194/254], loss=6.7257
	step [195/254], loss=8.6908
	step [196/254], loss=7.5614
	step [197/254], loss=7.6121
	step [198/254], loss=7.1530
	step [199/254], loss=6.6087
	step [200/254], loss=6.5834
	step [201/254], loss=8.1122
	step [202/254], loss=8.0623
	step [203/254], loss=5.8637
	step [204/254], loss=7.0597
	step [205/254], loss=5.5915
	step [206/254], loss=7.1772
	step [207/254], loss=10.7451
	step [208/254], loss=7.0621
	step [209/254], loss=6.5958
	step [210/254], loss=7.1244
	step [211/254], loss=6.5695
	step [212/254], loss=7.0243
	step [213/254], loss=6.6194
	step [214/254], loss=6.4362
	step [215/254], loss=6.5683
	step [216/254], loss=9.6291
	step [217/254], loss=7.5349
	step [218/254], loss=7.6920
	step [219/254], loss=8.3439
	step [220/254], loss=8.0688
	step [221/254], loss=7.1592
	step [222/254], loss=7.9754
	step [223/254], loss=6.8692
	step [224/254], loss=7.5292
	step [225/254], loss=6.7727
	step [226/254], loss=6.6775
	step [227/254], loss=8.3391
	step [228/254], loss=7.3616
	step [229/254], loss=8.1582
	step [230/254], loss=8.9183
	step [231/254], loss=6.9559
	step [232/254], loss=7.4777
	step [233/254], loss=5.9811
	step [234/254], loss=5.6812
	step [235/254], loss=7.0642
	step [236/254], loss=7.8103
	step [237/254], loss=5.9893
	step [238/254], loss=5.1390
	step [239/254], loss=5.7572
	step [240/254], loss=10.7533
	step [241/254], loss=7.4069
	step [242/254], loss=8.2381
	step [243/254], loss=5.9536
	step [244/254], loss=6.1071
	step [245/254], loss=7.3093
	step [246/254], loss=8.0920
	step [247/254], loss=8.7108
	step [248/254], loss=6.0534
	step [249/254], loss=7.5006
	step [250/254], loss=8.7164
	step [251/254], loss=8.3193
	step [252/254], loss=6.8158
	step [253/254], loss=6.3742
	step [254/254], loss=1.0857
	Evaluating
	loss=0.0233, precision=0.2392, recall=0.9925, f1=0.3855
Training epoch 16
	step [1/254], loss=6.8430
	step [2/254], loss=6.3765
	step [3/254], loss=7.2974
	step [4/254], loss=6.4527
	step [5/254], loss=7.0259
	step [6/254], loss=9.2199
	step [7/254], loss=6.4147
	step [8/254], loss=6.5476
	step [9/254], loss=7.6547
	step [10/254], loss=5.2014
	step [11/254], loss=8.3811
	step [12/254], loss=6.7428
	step [13/254], loss=7.3983
	step [14/254], loss=6.9418
	step [15/254], loss=7.0934
	step [16/254], loss=8.9699
	step [17/254], loss=6.8743
	step [18/254], loss=6.8438
	step [19/254], loss=8.3006
	step [20/254], loss=6.8998
	step [21/254], loss=7.7394
	step [22/254], loss=8.2472
	step [23/254], loss=7.4687
	step [24/254], loss=6.3881
	step [25/254], loss=7.9905
	step [26/254], loss=8.5201
	step [27/254], loss=5.5637
	step [28/254], loss=8.0897
	step [29/254], loss=9.8410
	step [30/254], loss=7.4121
	step [31/254], loss=8.6975
	step [32/254], loss=6.8599
	step [33/254], loss=5.7988
	step [34/254], loss=7.0767
	step [35/254], loss=7.1995
	step [36/254], loss=6.6823
	step [37/254], loss=6.5490
	step [38/254], loss=8.0985
	step [39/254], loss=6.9248
	step [40/254], loss=6.6343
	step [41/254], loss=6.7080
	step [42/254], loss=6.8529
	step [43/254], loss=7.8236
	step [44/254], loss=9.2702
	step [45/254], loss=7.1359
	step [46/254], loss=6.3531
	step [47/254], loss=7.1037
	step [48/254], loss=6.8385
	step [49/254], loss=7.6870
	step [50/254], loss=6.3500
	step [51/254], loss=7.0472
	step [52/254], loss=6.4934
	step [53/254], loss=7.2855
	step [54/254], loss=8.8822
	step [55/254], loss=5.9446
	step [56/254], loss=6.7916
	step [57/254], loss=6.9918
	step [58/254], loss=7.4863
	step [59/254], loss=7.6719
	step [60/254], loss=6.1389
	step [61/254], loss=7.0119
	step [62/254], loss=8.0212
	step [63/254], loss=7.3897
	step [64/254], loss=7.1034
	step [65/254], loss=6.5881
	step [66/254], loss=6.6623
	step [67/254], loss=7.2706
	step [68/254], loss=6.4393
	step [69/254], loss=7.6295
	step [70/254], loss=9.4152
	step [71/254], loss=7.3198
	step [72/254], loss=9.5221
	step [73/254], loss=7.1771
	step [74/254], loss=7.6732
	step [75/254], loss=7.2993
	step [76/254], loss=7.1293
	step [77/254], loss=6.6130
	step [78/254], loss=5.5026
	step [79/254], loss=6.2051
	step [80/254], loss=8.6048
	step [81/254], loss=5.1811
	step [82/254], loss=8.6450
	step [83/254], loss=8.7031
	step [84/254], loss=6.5382
	step [85/254], loss=7.9698
	step [86/254], loss=6.9857
	step [87/254], loss=6.9713
	step [88/254], loss=7.8277
	step [89/254], loss=6.7923
	step [90/254], loss=7.8306
	step [91/254], loss=6.5102
	step [92/254], loss=7.4989
	step [93/254], loss=6.9626
	step [94/254], loss=7.8913
	step [95/254], loss=6.0697
	step [96/254], loss=7.1256
	step [97/254], loss=6.4211
	step [98/254], loss=6.8598
	step [99/254], loss=7.2009
	step [100/254], loss=6.2302
	step [101/254], loss=7.1848
	step [102/254], loss=8.1999
	step [103/254], loss=6.2151
	step [104/254], loss=6.1755
	step [105/254], loss=8.2860
	step [106/254], loss=7.9960
	step [107/254], loss=8.1012
	step [108/254], loss=9.8860
	step [109/254], loss=6.7542
	step [110/254], loss=6.8003
	step [111/254], loss=8.6983
	step [112/254], loss=9.5159
	step [113/254], loss=5.8422
	step [114/254], loss=8.4915
	step [115/254], loss=7.0257
	step [116/254], loss=6.8373
	step [117/254], loss=6.5276
	step [118/254], loss=5.5995
	step [119/254], loss=5.8495
	step [120/254], loss=8.5890
	step [121/254], loss=8.2520
	step [122/254], loss=7.9232
	step [123/254], loss=7.9529
	step [124/254], loss=7.0611
	step [125/254], loss=7.2114
	step [126/254], loss=5.4994
	step [127/254], loss=6.9553
	step [128/254], loss=6.0874
	step [129/254], loss=7.5594
	step [130/254], loss=8.0182
	step [131/254], loss=7.5622
	step [132/254], loss=7.2252
	step [133/254], loss=6.6009
	step [134/254], loss=7.0700
	step [135/254], loss=8.7908
	step [136/254], loss=8.1103
	step [137/254], loss=5.8181
	step [138/254], loss=6.4063
	step [139/254], loss=6.9604
	step [140/254], loss=7.6707
	step [141/254], loss=7.9493
	step [142/254], loss=7.2579
	step [143/254], loss=9.1367
	step [144/254], loss=6.1888
	step [145/254], loss=5.9922
	step [146/254], loss=7.2837
	step [147/254], loss=7.1414
	step [148/254], loss=7.8964
	step [149/254], loss=7.1026
	step [150/254], loss=5.1942
	step [151/254], loss=7.2934
	step [152/254], loss=9.0495
	step [153/254], loss=5.9989
	step [154/254], loss=6.7486
	step [155/254], loss=6.4015
	step [156/254], loss=5.4762
	step [157/254], loss=6.3120
	step [158/254], loss=6.1274
	step [159/254], loss=8.1163
	step [160/254], loss=9.4474
	step [161/254], loss=6.7045
	step [162/254], loss=6.0182
	step [163/254], loss=5.7311
	step [164/254], loss=7.2661
	step [165/254], loss=6.4336
	step [166/254], loss=6.0950
	step [167/254], loss=6.7177
	step [168/254], loss=9.0017
	step [169/254], loss=6.7047
	step [170/254], loss=6.9557
	step [171/254], loss=7.5636
	step [172/254], loss=6.5350
	step [173/254], loss=7.6657
	step [174/254], loss=5.4838
	step [175/254], loss=6.8268
	step [176/254], loss=5.2109
	step [177/254], loss=6.8828
	step [178/254], loss=6.3843
	step [179/254], loss=6.7143
	step [180/254], loss=6.3826
	step [181/254], loss=6.1576
	step [182/254], loss=7.1251
	step [183/254], loss=6.5716
	step [184/254], loss=8.6058
	step [185/254], loss=6.5558
	step [186/254], loss=7.4572
	step [187/254], loss=7.4597
	step [188/254], loss=7.8058
	step [189/254], loss=6.3433
	step [190/254], loss=7.8766
	step [191/254], loss=6.3306
	step [192/254], loss=6.0447
	step [193/254], loss=6.3561
	step [194/254], loss=7.0414
	step [195/254], loss=7.4979
	step [196/254], loss=6.5259
	step [197/254], loss=7.8250
	step [198/254], loss=8.0641
	step [199/254], loss=6.9361
	step [200/254], loss=6.2126
	step [201/254], loss=7.2370
	step [202/254], loss=5.4032
	step [203/254], loss=6.3567
	step [204/254], loss=5.7801
	step [205/254], loss=6.7461
	step [206/254], loss=6.6085
	step [207/254], loss=8.2339
	step [208/254], loss=9.7341
	step [209/254], loss=5.8493
	step [210/254], loss=7.6955
	step [211/254], loss=7.5465
	step [212/254], loss=7.6194
	step [213/254], loss=6.8129
	step [214/254], loss=8.4765
	step [215/254], loss=7.4934
	step [216/254], loss=6.8109
	step [217/254], loss=7.5939
	step [218/254], loss=6.3319
	step [219/254], loss=5.5252
	step [220/254], loss=6.5638
	step [221/254], loss=7.5849
	step [222/254], loss=5.8996
	step [223/254], loss=8.1622
	step [224/254], loss=6.7239
	step [225/254], loss=5.5620
	step [226/254], loss=6.5550
	step [227/254], loss=8.5359
	step [228/254], loss=6.2367
	step [229/254], loss=7.2881
	step [230/254], loss=8.6923
	step [231/254], loss=8.5439
	step [232/254], loss=7.3601
	step [233/254], loss=5.6342
	step [234/254], loss=7.4695
	step [235/254], loss=7.4550
	step [236/254], loss=8.7629
	step [237/254], loss=5.8627
	step [238/254], loss=6.3937
	step [239/254], loss=6.7704
	step [240/254], loss=9.0477
	step [241/254], loss=6.4645
	step [242/254], loss=6.9737
	step [243/254], loss=6.5753
	step [244/254], loss=6.7345
	step [245/254], loss=7.3830
	step [246/254], loss=5.8907
	step [247/254], loss=7.3208
	step [248/254], loss=5.3847
	step [249/254], loss=7.0567
	step [250/254], loss=5.5266
	step [251/254], loss=6.8725
	step [252/254], loss=6.4795
	step [253/254], loss=5.6177
	step [254/254], loss=1.3410
	Evaluating
	loss=0.0208, precision=0.2769, recall=0.9912, f1=0.4328
Training epoch 17
	step [1/254], loss=9.8632
	step [2/254], loss=6.1661
	step [3/254], loss=6.6950
	step [4/254], loss=8.0692
	step [5/254], loss=6.5763
	step [6/254], loss=7.2554
	step [7/254], loss=8.7094
	step [8/254], loss=9.2326
	step [9/254], loss=8.3322
	step [10/254], loss=9.3965
	step [11/254], loss=8.2957
	step [12/254], loss=6.5133
	step [13/254], loss=6.3650
	step [14/254], loss=6.4061
	step [15/254], loss=5.9942
	step [16/254], loss=7.2304
	step [17/254], loss=7.1262
	step [18/254], loss=6.9980
	step [19/254], loss=7.1084
	step [20/254], loss=6.1700
	step [21/254], loss=5.2243
	step [22/254], loss=6.1115
	step [23/254], loss=8.1742
	step [24/254], loss=5.7974
	step [25/254], loss=7.8024
	step [26/254], loss=8.3549
	step [27/254], loss=7.5225
	step [28/254], loss=6.7976
	step [29/254], loss=8.7584
	step [30/254], loss=6.5773
	step [31/254], loss=7.5844
	step [32/254], loss=6.2860
	step [33/254], loss=7.6507
	step [34/254], loss=5.3310
	step [35/254], loss=6.7537
	step [36/254], loss=6.1976
	step [37/254], loss=6.9341
	step [38/254], loss=8.7084
	step [39/254], loss=7.1192
	step [40/254], loss=8.1876
	step [41/254], loss=6.0829
	step [42/254], loss=6.8948
	step [43/254], loss=6.0426
	step [44/254], loss=9.8723
	step [45/254], loss=4.9295
	step [46/254], loss=5.1648
	step [47/254], loss=5.9803
	step [48/254], loss=7.0806
	step [49/254], loss=7.5455
	step [50/254], loss=9.7713
	step [51/254], loss=6.0939
	step [52/254], loss=7.0756
	step [53/254], loss=6.9392
	step [54/254], loss=6.9589
	step [55/254], loss=6.0051
	step [56/254], loss=5.5757
	step [57/254], loss=5.6461
	step [58/254], loss=4.5130
	step [59/254], loss=10.9543
	step [60/254], loss=5.9075
	step [61/254], loss=6.0650
	step [62/254], loss=7.0891
	step [63/254], loss=7.4546
	step [64/254], loss=7.1590
	step [65/254], loss=5.0909
	step [66/254], loss=7.8416
	step [67/254], loss=8.7079
	step [68/254], loss=6.6901
	step [69/254], loss=8.8022
	step [70/254], loss=5.6362
	step [71/254], loss=6.6296
	step [72/254], loss=7.0049
	step [73/254], loss=5.5536
	step [74/254], loss=6.7622
	step [75/254], loss=5.5999
	step [76/254], loss=5.9881
	step [77/254], loss=8.2494
	step [78/254], loss=6.8010
	step [79/254], loss=6.5578
	step [80/254], loss=9.5096
	step [81/254], loss=7.8030
	step [82/254], loss=6.4966
	step [83/254], loss=4.8820
	step [84/254], loss=7.9883
	step [85/254], loss=6.9880
	step [86/254], loss=8.6463
	step [87/254], loss=7.3143
	step [88/254], loss=6.8727
	step [89/254], loss=8.1283
	step [90/254], loss=7.4963
	step [91/254], loss=7.9863
	step [92/254], loss=7.5177
	step [93/254], loss=7.1857
	step [94/254], loss=7.0115
	step [95/254], loss=6.8460
	step [96/254], loss=7.4919
	step [97/254], loss=7.1894
	step [98/254], loss=7.7327
	step [99/254], loss=8.6552
	step [100/254], loss=8.0426
	step [101/254], loss=6.4269
	step [102/254], loss=7.3745
	step [103/254], loss=6.9233
	step [104/254], loss=7.5002
	step [105/254], loss=6.4500
	step [106/254], loss=6.2918
	step [107/254], loss=8.0923
	step [108/254], loss=6.7645
	step [109/254], loss=6.6638
	step [110/254], loss=7.1688
	step [111/254], loss=7.8061
	step [112/254], loss=5.7592
	step [113/254], loss=6.0766
	step [114/254], loss=5.2604
	step [115/254], loss=5.2731
	step [116/254], loss=6.2054
	step [117/254], loss=8.6799
	step [118/254], loss=7.2349
	step [119/254], loss=6.2405
	step [120/254], loss=6.1674
	step [121/254], loss=6.7764
	step [122/254], loss=7.9844
	step [123/254], loss=7.5386
	step [124/254], loss=7.5465
	step [125/254], loss=6.2271
	step [126/254], loss=5.5179
	step [127/254], loss=8.9624
	step [128/254], loss=5.5511
	step [129/254], loss=5.6567
	step [130/254], loss=6.6360
	step [131/254], loss=6.6788
	step [132/254], loss=7.7122
	step [133/254], loss=6.1831
	step [134/254], loss=8.0208
	step [135/254], loss=5.1387
	step [136/254], loss=6.5063
	step [137/254], loss=8.0486
	step [138/254], loss=5.8409
	step [139/254], loss=7.4840
	step [140/254], loss=8.4854
	step [141/254], loss=5.6673
	step [142/254], loss=6.5377
	step [143/254], loss=7.6532
	step [144/254], loss=5.6653
	step [145/254], loss=7.8647
	step [146/254], loss=5.3713
	step [147/254], loss=6.6458
	step [148/254], loss=7.9421
	step [149/254], loss=6.0656
	step [150/254], loss=7.9520
	step [151/254], loss=7.3211
	step [152/254], loss=7.1364
	step [153/254], loss=6.7852
	step [154/254], loss=5.6492
	step [155/254], loss=6.1789
	step [156/254], loss=7.0723
	step [157/254], loss=6.7389
	step [158/254], loss=6.1423
	step [159/254], loss=5.7993
	step [160/254], loss=6.9403
	step [161/254], loss=6.1232
	step [162/254], loss=7.6845
	step [163/254], loss=6.8712
	step [164/254], loss=6.0940
	step [165/254], loss=6.1611
	step [166/254], loss=5.7600
	step [167/254], loss=7.9505
	step [168/254], loss=6.3284
	step [169/254], loss=7.0757
	step [170/254], loss=6.6384
	step [171/254], loss=8.2203
	step [172/254], loss=7.6830
	step [173/254], loss=6.3631
	step [174/254], loss=7.1150
	step [175/254], loss=5.1261
	step [176/254], loss=6.8171
	step [177/254], loss=6.1776
	step [178/254], loss=6.6849
	step [179/254], loss=6.6966
	step [180/254], loss=6.2637
	step [181/254], loss=6.4480
	step [182/254], loss=6.9447
	step [183/254], loss=7.2456
	step [184/254], loss=5.8221
	step [185/254], loss=7.6446
	step [186/254], loss=7.1503
	step [187/254], loss=7.3399
	step [188/254], loss=7.6831
	step [189/254], loss=5.6899
	step [190/254], loss=9.0353
	step [191/254], loss=5.0030
	step [192/254], loss=6.5514
	step [193/254], loss=7.1854
	step [194/254], loss=7.0378
	step [195/254], loss=6.7838
	step [196/254], loss=6.2506
	step [197/254], loss=7.0197
	step [198/254], loss=8.4783
	step [199/254], loss=6.6808
	step [200/254], loss=5.8460
	step [201/254], loss=7.0466
	step [202/254], loss=7.8696
	step [203/254], loss=7.1494
	step [204/254], loss=6.6054
	step [205/254], loss=7.6660
	step [206/254], loss=6.3680
	step [207/254], loss=7.8225
	step [208/254], loss=7.6122
	step [209/254], loss=5.5941
	step [210/254], loss=5.0890
	step [211/254], loss=7.3436
	step [212/254], loss=8.8183
	step [213/254], loss=8.6078
	step [214/254], loss=7.8532
	step [215/254], loss=5.5649
	step [216/254], loss=8.0808
	step [217/254], loss=6.2082
	step [218/254], loss=6.1197
	step [219/254], loss=6.2635
	step [220/254], loss=6.1673
	step [221/254], loss=7.1341
	step [222/254], loss=7.0186
	step [223/254], loss=7.5995
	step [224/254], loss=6.2754
	step [225/254], loss=7.0244
	step [226/254], loss=8.4469
	step [227/254], loss=6.3236
	step [228/254], loss=6.6633
	step [229/254], loss=8.0904
	step [230/254], loss=6.0888
	step [231/254], loss=5.7217
	step [232/254], loss=7.1117
	step [233/254], loss=6.7595
	step [234/254], loss=7.8458
	step [235/254], loss=5.7709
	step [236/254], loss=5.2517
	step [237/254], loss=7.3485
	step [238/254], loss=8.4065
	step [239/254], loss=6.2366
	step [240/254], loss=7.2660
	step [241/254], loss=7.4970
	step [242/254], loss=8.6260
	step [243/254], loss=7.2054
	step [244/254], loss=7.0706
	step [245/254], loss=7.2247
	step [246/254], loss=7.3871
	step [247/254], loss=7.4511
	step [248/254], loss=6.5134
	step [249/254], loss=7.7773
	step [250/254], loss=6.7963
	step [251/254], loss=5.4774
	step [252/254], loss=7.7519
	step [253/254], loss=7.2666
	step [254/254], loss=1.0742
	Evaluating
	loss=0.0275, precision=0.2054, recall=0.9940, f1=0.3405
Training epoch 18
	step [1/254], loss=7.3702
	step [2/254], loss=6.1249
	step [3/254], loss=6.8060
	step [4/254], loss=5.3006
	step [5/254], loss=8.7487
	step [6/254], loss=7.7307
	step [7/254], loss=6.4384
	step [8/254], loss=5.7261
	step [9/254], loss=6.5502
	step [10/254], loss=7.0796
	step [11/254], loss=6.3570
	step [12/254], loss=5.6735
	step [13/254], loss=5.8676
	step [14/254], loss=6.5921
	step [15/254], loss=6.2696
	step [16/254], loss=6.6859
	step [17/254], loss=8.0845
	step [18/254], loss=6.9213
	step [19/254], loss=7.2657
	step [20/254], loss=5.0801
	step [21/254], loss=7.1299
	step [22/254], loss=7.4070
	step [23/254], loss=6.2454
	step [24/254], loss=6.1826
	step [25/254], loss=5.7503
	step [26/254], loss=5.6942
	step [27/254], loss=7.4373
	step [28/254], loss=6.3857
	step [29/254], loss=6.3034
	step [30/254], loss=7.2088
	step [31/254], loss=8.9474
	step [32/254], loss=7.0350
	step [33/254], loss=7.2363
	step [34/254], loss=7.2702
	step [35/254], loss=5.2139
	step [36/254], loss=6.4196
	step [37/254], loss=6.6979
	step [38/254], loss=5.4672
	step [39/254], loss=8.8844
	step [40/254], loss=5.5318
	step [41/254], loss=9.2832
	step [42/254], loss=7.7908
	step [43/254], loss=7.8597
	step [44/254], loss=7.4656
	step [45/254], loss=6.2612
	step [46/254], loss=6.3237
	step [47/254], loss=6.8259
	step [48/254], loss=6.4307
	step [49/254], loss=7.6063
	step [50/254], loss=5.5833
	step [51/254], loss=6.1178
	step [52/254], loss=7.4524
	step [53/254], loss=8.1870
	step [54/254], loss=6.8505
	step [55/254], loss=7.7969
	step [56/254], loss=6.0245
	step [57/254], loss=6.8169
	step [58/254], loss=6.6437
	step [59/254], loss=7.8297
	step [60/254], loss=5.8125
	step [61/254], loss=8.4636
	step [62/254], loss=6.0879
	step [63/254], loss=7.4714
	step [64/254], loss=5.8297
	step [65/254], loss=5.5343
	step [66/254], loss=7.2891
	step [67/254], loss=6.7065
	step [68/254], loss=6.3719
	step [69/254], loss=8.3749
	step [70/254], loss=5.5203
	step [71/254], loss=7.0977
	step [72/254], loss=7.6041
	step [73/254], loss=6.2863
	step [74/254], loss=8.5802
	step [75/254], loss=6.5527
	step [76/254], loss=7.3679
	step [77/254], loss=7.6732
	step [78/254], loss=5.8320
	step [79/254], loss=8.5225
	step [80/254], loss=7.1807
	step [81/254], loss=7.2050
	step [82/254], loss=6.9194
	step [83/254], loss=7.0181
	step [84/254], loss=7.0876
	step [85/254], loss=7.4524
	step [86/254], loss=8.8234
	step [87/254], loss=6.0626
	step [88/254], loss=6.6869
	step [89/254], loss=9.9640
	step [90/254], loss=6.2227
	step [91/254], loss=6.5789
	step [92/254], loss=5.4312
	step [93/254], loss=5.5362
	step [94/254], loss=4.8085
	step [95/254], loss=6.7533
	step [96/254], loss=6.8722
	step [97/254], loss=8.2265
	step [98/254], loss=6.0377
	step [99/254], loss=4.9805
	step [100/254], loss=5.9954
	step [101/254], loss=7.2613
	step [102/254], loss=8.9706
	step [103/254], loss=8.0645
	step [104/254], loss=6.1118
	step [105/254], loss=6.4017
	step [106/254], loss=7.9355
	step [107/254], loss=5.7397
	step [108/254], loss=6.3737
	step [109/254], loss=5.7193
	step [110/254], loss=7.3862
	step [111/254], loss=6.5900
	step [112/254], loss=5.8597
	step [113/254], loss=5.7356
	step [114/254], loss=6.7157
	step [115/254], loss=7.2409
	step [116/254], loss=6.5061
	step [117/254], loss=5.9995
	step [118/254], loss=6.1033
	step [119/254], loss=6.7169
	step [120/254], loss=7.0422
	step [121/254], loss=6.3070
	step [122/254], loss=7.1952
	step [123/254], loss=7.8925
	step [124/254], loss=8.0842
	step [125/254], loss=7.6866
	step [126/254], loss=7.0964
	step [127/254], loss=5.7168
	step [128/254], loss=6.9869
	step [129/254], loss=6.0795
	step [130/254], loss=8.1558
	step [131/254], loss=6.9070
	step [132/254], loss=6.8519
	step [133/254], loss=7.4666
	step [134/254], loss=9.2814
	step [135/254], loss=7.8176
	step [136/254], loss=5.8943
	step [137/254], loss=5.0606
	step [138/254], loss=6.3329
	step [139/254], loss=5.0675
	step [140/254], loss=6.2185
	step [141/254], loss=5.1835
	step [142/254], loss=8.6295
	step [143/254], loss=7.2761
	step [144/254], loss=5.5425
	step [145/254], loss=8.2677
	step [146/254], loss=5.4599
	step [147/254], loss=7.4902
	step [148/254], loss=7.5901
	step [149/254], loss=6.8111
	step [150/254], loss=6.0363
	step [151/254], loss=5.4426
	step [152/254], loss=7.8193
	step [153/254], loss=7.5463
	step [154/254], loss=7.2203
	step [155/254], loss=5.2003
	step [156/254], loss=5.9451
	step [157/254], loss=5.4983
	step [158/254], loss=6.2128
	step [159/254], loss=6.5718
	step [160/254], loss=6.8065
	step [161/254], loss=7.9102
	step [162/254], loss=6.4142
	step [163/254], loss=6.0708
	step [164/254], loss=6.0296
	step [165/254], loss=7.6388
	step [166/254], loss=6.1156
	step [167/254], loss=6.1348
	step [168/254], loss=6.5600
	step [169/254], loss=6.0265
	step [170/254], loss=5.8116
	step [171/254], loss=6.7027
	step [172/254], loss=7.9584
	step [173/254], loss=6.9628
	step [174/254], loss=8.1813
	step [175/254], loss=6.4979
	step [176/254], loss=6.9249
	step [177/254], loss=7.1460
	step [178/254], loss=5.1461
	step [179/254], loss=8.4904
	step [180/254], loss=5.8922
	step [181/254], loss=6.8872
	step [182/254], loss=8.3320
	step [183/254], loss=5.8177
	step [184/254], loss=7.6612
	step [185/254], loss=5.7718
	step [186/254], loss=7.0245
	step [187/254], loss=6.9494
	step [188/254], loss=5.9901
	step [189/254], loss=6.8448
	step [190/254], loss=6.4647
	step [191/254], loss=7.7397
	step [192/254], loss=7.4458
	step [193/254], loss=6.9507
	step [194/254], loss=6.2035
	step [195/254], loss=7.5956
	step [196/254], loss=5.3474
	step [197/254], loss=7.5828
	step [198/254], loss=6.8498
	step [199/254], loss=6.0824
	step [200/254], loss=6.8523
	step [201/254], loss=7.5266
	step [202/254], loss=6.6445
	step [203/254], loss=5.4279
	step [204/254], loss=6.9772
	step [205/254], loss=7.9135
	step [206/254], loss=6.0633
	step [207/254], loss=7.4177
	step [208/254], loss=6.1784
	step [209/254], loss=6.2164
	step [210/254], loss=5.7148
	step [211/254], loss=7.2620
	step [212/254], loss=6.3146
	step [213/254], loss=6.4890
	step [214/254], loss=6.5046
	step [215/254], loss=5.9271
	step [216/254], loss=5.9679
	step [217/254], loss=7.5519
	step [218/254], loss=5.9010
	step [219/254], loss=9.5570
	step [220/254], loss=5.1479
	step [221/254], loss=6.7207
	step [222/254], loss=7.7114
	step [223/254], loss=6.8067
	step [224/254], loss=6.6576
	step [225/254], loss=5.6888
	step [226/254], loss=7.6680
	step [227/254], loss=6.4687
	step [228/254], loss=6.4476
	step [229/254], loss=7.3595
	step [230/254], loss=5.2417
	step [231/254], loss=8.3988
	step [232/254], loss=7.1477
	step [233/254], loss=7.0757
	step [234/254], loss=5.0580
	step [235/254], loss=7.8038
	step [236/254], loss=5.7744
	step [237/254], loss=6.1776
	step [238/254], loss=6.9987
	step [239/254], loss=7.8056
	step [240/254], loss=6.5427
	step [241/254], loss=7.2757
	step [242/254], loss=8.2007
	step [243/254], loss=6.8291
	step [244/254], loss=6.2466
	step [245/254], loss=6.0856
	step [246/254], loss=7.4268
	step [247/254], loss=7.0657
	step [248/254], loss=6.0234
	step [249/254], loss=6.7287
	step [250/254], loss=8.1985
	step [251/254], loss=6.4056
	step [252/254], loss=6.2679
	step [253/254], loss=5.9584
	step [254/254], loss=1.8874
	Evaluating
	loss=0.0212, precision=0.2557, recall=0.9922, f1=0.4066
Training epoch 19
	step [1/254], loss=7.5707
	step [2/254], loss=6.1656
	step [3/254], loss=7.7108
	step [4/254], loss=6.6840
	step [5/254], loss=6.8276
	step [6/254], loss=7.9459
	step [7/254], loss=6.0280
	step [8/254], loss=6.5411
	step [9/254], loss=6.7074
	step [10/254], loss=8.8136
	step [11/254], loss=6.3020
	step [12/254], loss=7.0881
	step [13/254], loss=6.9180
	step [14/254], loss=7.2992
	step [15/254], loss=6.2041
	step [16/254], loss=7.7159
	step [17/254], loss=7.2570
	step [18/254], loss=7.3031
	step [19/254], loss=6.6173
	step [20/254], loss=6.7786
	step [21/254], loss=5.2905
	step [22/254], loss=4.9277
	step [23/254], loss=6.5633
	step [24/254], loss=7.8513
	step [25/254], loss=5.7562
	step [26/254], loss=8.2744
	step [27/254], loss=6.0709
	step [28/254], loss=6.5507
	step [29/254], loss=6.9801
	step [30/254], loss=5.5294
	step [31/254], loss=5.6588
	step [32/254], loss=7.5357
	step [33/254], loss=6.3632
	step [34/254], loss=6.1844
	step [35/254], loss=7.4992
	step [36/254], loss=5.6794
	step [37/254], loss=7.3000
	step [38/254], loss=7.4155
	step [39/254], loss=6.8302
	step [40/254], loss=6.6953
	step [41/254], loss=6.7103
	step [42/254], loss=5.8623
	step [43/254], loss=7.3031
	step [44/254], loss=7.1636
	step [45/254], loss=7.2845
	step [46/254], loss=6.9276
	step [47/254], loss=7.4645
	step [48/254], loss=7.7971
	step [49/254], loss=6.0699
	step [50/254], loss=10.1120
	step [51/254], loss=6.5321
	step [52/254], loss=6.3653
	step [53/254], loss=5.2582
	step [54/254], loss=6.5887
	step [55/254], loss=6.3135
	step [56/254], loss=9.3698
	step [57/254], loss=5.6762
	step [58/254], loss=9.2370
	step [59/254], loss=5.4788
	step [60/254], loss=5.8506
	step [61/254], loss=7.9422
	step [62/254], loss=7.6565
	step [63/254], loss=5.6105
	step [64/254], loss=6.0410
	step [65/254], loss=7.1049
	step [66/254], loss=8.7451
	step [67/254], loss=6.6002
	step [68/254], loss=8.3664
	step [69/254], loss=5.0643
	step [70/254], loss=7.6211
	step [71/254], loss=5.1207
	step [72/254], loss=8.7898
	step [73/254], loss=7.3266
	step [74/254], loss=6.5670
	step [75/254], loss=5.4493
	step [76/254], loss=7.6760
	step [77/254], loss=7.8222
	step [78/254], loss=6.7747
	step [79/254], loss=6.2520
	step [80/254], loss=4.6594
	step [81/254], loss=5.4623
	step [82/254], loss=6.3696
	step [83/254], loss=5.8305
	step [84/254], loss=7.1276
	step [85/254], loss=5.1380
	step [86/254], loss=6.3983
	step [87/254], loss=6.1629
	step [88/254], loss=6.5257
	step [89/254], loss=6.0061
	step [90/254], loss=5.9600
	step [91/254], loss=6.8289
	step [92/254], loss=6.3224
	step [93/254], loss=8.7490
	step [94/254], loss=6.7112
	step [95/254], loss=7.3354
	step [96/254], loss=5.7639
	step [97/254], loss=7.5482
	step [98/254], loss=6.1036
	step [99/254], loss=5.5816
	step [100/254], loss=7.1221
	step [101/254], loss=7.6475
	step [102/254], loss=8.9360
	step [103/254], loss=7.3449
	step [104/254], loss=6.4921
	step [105/254], loss=7.1814
	step [106/254], loss=7.0205
	step [107/254], loss=8.1323
	step [108/254], loss=6.1816
	step [109/254], loss=6.6764
	step [110/254], loss=8.5025
	step [111/254], loss=5.4093
	step [112/254], loss=5.8672
	step [113/254], loss=5.8341
	step [114/254], loss=6.9237
	step [115/254], loss=4.9284
	step [116/254], loss=5.9172
	step [117/254], loss=4.6263
	step [118/254], loss=5.7725
	step [119/254], loss=6.5789
	step [120/254], loss=7.1217
	step [121/254], loss=6.3964
	step [122/254], loss=5.0464
	step [123/254], loss=7.0638
	step [124/254], loss=6.0884
	step [125/254], loss=6.7835
	step [126/254], loss=5.5792
	step [127/254], loss=5.4845
	step [128/254], loss=6.1175
	step [129/254], loss=6.6117
	step [130/254], loss=6.8516
	step [131/254], loss=6.0642
	step [132/254], loss=5.6500
	step [133/254], loss=6.8405
	step [134/254], loss=6.6208
	step [135/254], loss=8.7226
	step [136/254], loss=8.6893
	step [137/254], loss=5.9579
	step [138/254], loss=5.7930
	step [139/254], loss=5.4632
	step [140/254], loss=5.5218
	step [141/254], loss=6.4090
	step [142/254], loss=8.5778
	step [143/254], loss=5.3095
	step [144/254], loss=6.5409
	step [145/254], loss=6.1307
	step [146/254], loss=6.9133
	step [147/254], loss=6.1003
	step [148/254], loss=5.9361
	step [149/254], loss=6.9231
	step [150/254], loss=5.8469
	step [151/254], loss=6.4988
	step [152/254], loss=6.0649
	step [153/254], loss=6.7768
	step [154/254], loss=7.4500
	step [155/254], loss=5.1191
	step [156/254], loss=5.2953
	step [157/254], loss=6.0643
	step [158/254], loss=6.5424
	step [159/254], loss=6.7822
	step [160/254], loss=6.8146
	step [161/254], loss=7.1997
	step [162/254], loss=4.4782
	step [163/254], loss=5.5308
	step [164/254], loss=5.1158
	step [165/254], loss=5.0129
	step [166/254], loss=6.0185
	step [167/254], loss=7.8022
	step [168/254], loss=7.1288
	step [169/254], loss=8.1910
	step [170/254], loss=6.2774
	step [171/254], loss=7.7135
	step [172/254], loss=6.8098
	step [173/254], loss=6.9487
	step [174/254], loss=8.0492
	step [175/254], loss=7.1938
	step [176/254], loss=6.3873
	step [177/254], loss=6.1722
	step [178/254], loss=8.1768
	step [179/254], loss=7.0329
	step [180/254], loss=6.6249
	step [181/254], loss=8.7599
	step [182/254], loss=6.9932
	step [183/254], loss=7.1899
	step [184/254], loss=6.5596
	step [185/254], loss=6.9383
	step [186/254], loss=7.4060
	step [187/254], loss=7.4124
	step [188/254], loss=7.2910
	step [189/254], loss=7.0916
	step [190/254], loss=5.0980
	step [191/254], loss=7.4934
	step [192/254], loss=8.2000
	step [193/254], loss=5.6437
	step [194/254], loss=5.5984
	step [195/254], loss=7.2673
	step [196/254], loss=6.2791
	step [197/254], loss=7.4648
	step [198/254], loss=4.9913
	step [199/254], loss=6.8080
	step [200/254], loss=5.2806
	step [201/254], loss=6.0879
	step [202/254], loss=6.4245
	step [203/254], loss=6.6037
	step [204/254], loss=6.4210
	step [205/254], loss=6.5484
	step [206/254], loss=7.2720
	step [207/254], loss=6.1080
	step [208/254], loss=5.3192
	step [209/254], loss=8.0051
	step [210/254], loss=8.4019
	step [211/254], loss=5.8458
	step [212/254], loss=6.7595
	step [213/254], loss=4.6149
	step [214/254], loss=5.6867
	step [215/254], loss=7.0477
	step [216/254], loss=5.6675
	step [217/254], loss=9.8012
	step [218/254], loss=6.8472
	step [219/254], loss=6.0550
	step [220/254], loss=6.2991
	step [221/254], loss=8.6103
	step [222/254], loss=5.8026
	step [223/254], loss=7.1260
	step [224/254], loss=6.8944
	step [225/254], loss=6.0265
	step [226/254], loss=9.2237
	step [227/254], loss=7.3885
	step [228/254], loss=6.6808
	step [229/254], loss=6.6698
	step [230/254], loss=7.6533
	step [231/254], loss=6.1643
	step [232/254], loss=7.1606
	step [233/254], loss=5.9708
	step [234/254], loss=7.0156
	step [235/254], loss=7.8867
	step [236/254], loss=6.3261
	step [237/254], loss=6.5623
	step [238/254], loss=6.4532
	step [239/254], loss=4.3399
	step [240/254], loss=6.4051
	step [241/254], loss=6.2529
	step [242/254], loss=5.0262
	step [243/254], loss=6.0864
	step [244/254], loss=8.1531
	step [245/254], loss=7.5009
	step [246/254], loss=7.7161
	step [247/254], loss=8.6369
	step [248/254], loss=6.6635
	step [249/254], loss=7.1226
	step [250/254], loss=5.9607
	step [251/254], loss=5.6218
	step [252/254], loss=5.7736
	step [253/254], loss=8.0661
	step [254/254], loss=1.6446
	Evaluating
	loss=0.0254, precision=0.1889, recall=0.9931, f1=0.3175
Training epoch 20
	step [1/254], loss=6.5375
	step [2/254], loss=9.1743
	step [3/254], loss=6.2696
	step [4/254], loss=7.0229
	step [5/254], loss=6.9313
	step [6/254], loss=6.5033
	step [7/254], loss=6.8874
	step [8/254], loss=5.7972
	step [9/254], loss=6.2931
	step [10/254], loss=5.5517
	step [11/254], loss=8.7180
	step [12/254], loss=7.9531
	step [13/254], loss=5.9705
	step [14/254], loss=5.6054
	step [15/254], loss=6.8266
	step [16/254], loss=7.9865
	step [17/254], loss=6.0644
	step [18/254], loss=6.9827
	step [19/254], loss=6.8981
	step [20/254], loss=5.8985
	step [21/254], loss=6.3688
	step [22/254], loss=6.3671
	step [23/254], loss=6.5597
	step [24/254], loss=6.2675
	step [25/254], loss=5.9445
	step [26/254], loss=7.2807
	step [27/254], loss=5.7348
	step [28/254], loss=6.0970
	step [29/254], loss=5.0058
	step [30/254], loss=7.2999
	step [31/254], loss=5.2075
	step [32/254], loss=5.8107
	step [33/254], loss=6.7779
	step [34/254], loss=6.6024
	step [35/254], loss=4.9923
	step [36/254], loss=7.4286
	step [37/254], loss=6.1481
	step [38/254], loss=5.6309
	step [39/254], loss=6.5348
	step [40/254], loss=5.1279
	step [41/254], loss=5.9740
	step [42/254], loss=7.0467
	step [43/254], loss=6.9579
	step [44/254], loss=7.3385
	step [45/254], loss=6.3638
	step [46/254], loss=7.0399
	step [47/254], loss=7.3425
	step [48/254], loss=5.9456
	step [49/254], loss=8.2700
	step [50/254], loss=7.0709
	step [51/254], loss=6.0294
	step [52/254], loss=6.0482
	step [53/254], loss=5.6394
	step [54/254], loss=4.8255
	step [55/254], loss=6.1736
	step [56/254], loss=7.8509
	step [57/254], loss=6.0949
	step [58/254], loss=5.7643
	step [59/254], loss=5.7902
	step [60/254], loss=6.3539
	step [61/254], loss=6.6070
	step [62/254], loss=6.9724
	step [63/254], loss=9.8679
	step [64/254], loss=6.6900
	step [65/254], loss=7.5131
	step [66/254], loss=7.6836
	step [67/254], loss=7.7264
	step [68/254], loss=7.0468
	step [69/254], loss=8.7737
	step [70/254], loss=6.0369
	step [71/254], loss=5.4405
	step [72/254], loss=7.7263
	step [73/254], loss=6.1755
	step [74/254], loss=7.1348
	step [75/254], loss=6.5214
	step [76/254], loss=7.7740
	step [77/254], loss=8.1734
	step [78/254], loss=5.6260
	step [79/254], loss=5.4855
	step [80/254], loss=8.2580
	step [81/254], loss=7.3695
	step [82/254], loss=6.5716
	step [83/254], loss=5.6913
	step [84/254], loss=5.9976
	step [85/254], loss=6.6624
	step [86/254], loss=6.9551
	step [87/254], loss=7.6794
	step [88/254], loss=6.1090
	step [89/254], loss=6.1401
	step [90/254], loss=6.2367
	step [91/254], loss=10.0256
	step [92/254], loss=6.0616
	step [93/254], loss=6.5797
	step [94/254], loss=6.3938
	step [95/254], loss=6.4499
	step [96/254], loss=5.9872
	step [97/254], loss=6.9601
	step [98/254], loss=5.7249
	step [99/254], loss=4.8448
	step [100/254], loss=5.8023
	step [101/254], loss=6.8979
	step [102/254], loss=6.4057
	step [103/254], loss=5.9413
	step [104/254], loss=5.5385
	step [105/254], loss=7.1723
	step [106/254], loss=5.7174
	step [107/254], loss=6.8247
	step [108/254], loss=5.9910
	step [109/254], loss=6.7107
	step [110/254], loss=6.4786
	step [111/254], loss=7.8123
	step [112/254], loss=6.8192
	step [113/254], loss=5.9356
	step [114/254], loss=5.9016
	step [115/254], loss=6.2869
	step [116/254], loss=6.7335
	step [117/254], loss=8.9368
	step [118/254], loss=6.6274
	step [119/254], loss=7.3287
	step [120/254], loss=6.6595
	step [121/254], loss=6.0534
	step [122/254], loss=4.8432
	step [123/254], loss=7.3168
	step [124/254], loss=6.0169
	step [125/254], loss=5.0939
	step [126/254], loss=6.8094
	step [127/254], loss=5.6418
	step [128/254], loss=5.9897
	step [129/254], loss=6.1709
	step [130/254], loss=7.9450
	step [131/254], loss=6.0300
	step [132/254], loss=6.3571
	step [133/254], loss=6.9111
	step [134/254], loss=5.4583
	step [135/254], loss=6.9600
	step [136/254], loss=5.5402
	step [137/254], loss=5.8848
	step [138/254], loss=6.8288
	step [139/254], loss=8.3804
	step [140/254], loss=6.2042
	step [141/254], loss=6.0240
	step [142/254], loss=7.0063
	step [143/254], loss=5.9641
	step [144/254], loss=7.7667
	step [145/254], loss=6.4033
	step [146/254], loss=5.1647
	step [147/254], loss=7.4282
	step [148/254], loss=6.4196
	step [149/254], loss=5.8777
	step [150/254], loss=7.4722
	step [151/254], loss=4.4099
	step [152/254], loss=6.9914
	step [153/254], loss=6.5812
	step [154/254], loss=5.9512
	step [155/254], loss=8.3902
	step [156/254], loss=5.5012
	step [157/254], loss=5.9598
	step [158/254], loss=6.2076
	step [159/254], loss=5.4528
	step [160/254], loss=6.7683
	step [161/254], loss=6.6509
	step [162/254], loss=7.2732
	step [163/254], loss=4.8899
	step [164/254], loss=7.1758
	step [165/254], loss=5.3154
	step [166/254], loss=6.5047
	step [167/254], loss=5.4175
	step [168/254], loss=6.3985
	step [169/254], loss=5.7970
	step [170/254], loss=6.1639
	step [171/254], loss=6.8181
	step [172/254], loss=6.6090
	step [173/254], loss=5.4599
	step [174/254], loss=6.8139
	step [175/254], loss=5.0020
	step [176/254], loss=9.0096
	step [177/254], loss=7.1907
	step [178/254], loss=6.9386
	step [179/254], loss=8.4338
	step [180/254], loss=6.0113
	step [181/254], loss=5.7152
	step [182/254], loss=6.0230
	step [183/254], loss=5.9199
	step [184/254], loss=7.1813
	step [185/254], loss=6.8257
	step [186/254], loss=7.4600
	step [187/254], loss=6.4224
	step [188/254], loss=5.1184
	step [189/254], loss=5.7504
	step [190/254], loss=6.8459
	step [191/254], loss=4.5546
	step [192/254], loss=5.8842
	step [193/254], loss=7.2394
	step [194/254], loss=6.2708
	step [195/254], loss=8.1152
	step [196/254], loss=7.4922
	step [197/254], loss=5.8075
	step [198/254], loss=5.1587
	step [199/254], loss=5.7482
	step [200/254], loss=8.1467
	step [201/254], loss=8.7183
	step [202/254], loss=6.3820
	step [203/254], loss=8.1476
	step [204/254], loss=6.5149
	step [205/254], loss=6.6967
	step [206/254], loss=5.8311
	step [207/254], loss=5.8708
	step [208/254], loss=6.0613
	step [209/254], loss=5.9749
	step [210/254], loss=6.2521
	step [211/254], loss=7.6468
	step [212/254], loss=7.8435
	step [213/254], loss=5.6844
	step [214/254], loss=6.6883
	step [215/254], loss=7.1015
	step [216/254], loss=6.5959
	step [217/254], loss=7.3426
	step [218/254], loss=5.2959
	step [219/254], loss=6.7973
	step [220/254], loss=6.0490
	step [221/254], loss=7.4311
	step [222/254], loss=5.7576
	step [223/254], loss=6.2283
	step [224/254], loss=6.4913
	step [225/254], loss=8.3708
	step [226/254], loss=5.7071
	step [227/254], loss=7.0193
	step [228/254], loss=7.6694
	step [229/254], loss=8.1451
	step [230/254], loss=6.3305
	step [231/254], loss=6.1090
	step [232/254], loss=4.7263
	step [233/254], loss=6.1820
	step [234/254], loss=6.3199
	step [235/254], loss=7.5048
	step [236/254], loss=6.2721
	step [237/254], loss=5.6058
	step [238/254], loss=5.6699
	step [239/254], loss=7.2588
	step [240/254], loss=6.1694
	step [241/254], loss=7.6552
	step [242/254], loss=6.5177
	step [243/254], loss=5.2243
	step [244/254], loss=5.3998
	step [245/254], loss=5.4697
	step [246/254], loss=6.2933
	step [247/254], loss=6.1658
	step [248/254], loss=8.0443
	step [249/254], loss=5.7311
	step [250/254], loss=6.1155
	step [251/254], loss=6.9291
	step [252/254], loss=5.5803
	step [253/254], loss=5.8581
	step [254/254], loss=1.7431
	Evaluating
	loss=0.0271, precision=0.2040, recall=0.9940, f1=0.3386
Training epoch 21
	step [1/254], loss=6.3558
	step [2/254], loss=7.1586
	step [3/254], loss=6.5013
	step [4/254], loss=5.1115
	step [5/254], loss=7.1332
	step [6/254], loss=6.6121
	step [7/254], loss=6.6625
	step [8/254], loss=5.1546
	step [9/254], loss=4.6437
	step [10/254], loss=7.1806
	step [11/254], loss=7.4696
	step [12/254], loss=7.8347
	step [13/254], loss=5.7669
	step [14/254], loss=4.7521
	step [15/254], loss=7.4463
	step [16/254], loss=5.8274
	step [17/254], loss=6.2413
	step [18/254], loss=4.8047
	step [19/254], loss=6.8398
	step [20/254], loss=6.1858
	step [21/254], loss=5.8736
	step [22/254], loss=5.8180
	step [23/254], loss=6.1996
	step [24/254], loss=5.8112
	step [25/254], loss=5.5573
	step [26/254], loss=6.2164
	step [27/254], loss=6.1901
	step [28/254], loss=8.0747
	step [29/254], loss=5.9689
	step [30/254], loss=7.6290
	step [31/254], loss=7.3093
	step [32/254], loss=6.5153
	step [33/254], loss=5.9085
	step [34/254], loss=6.6815
	step [35/254], loss=6.9595
	step [36/254], loss=4.8412
	step [37/254], loss=8.2768
	step [38/254], loss=5.6937
	step [39/254], loss=5.4135
	step [40/254], loss=7.3532
	step [41/254], loss=5.8997
	step [42/254], loss=5.5331
	step [43/254], loss=6.5083
	step [44/254], loss=6.1904
	step [45/254], loss=5.9522
	step [46/254], loss=4.9195
	step [47/254], loss=4.9880
	step [48/254], loss=8.3066
	step [49/254], loss=8.8094
	step [50/254], loss=7.8185
	step [51/254], loss=6.4470
	step [52/254], loss=6.6842
	step [53/254], loss=5.9449
	step [54/254], loss=6.2949
	step [55/254], loss=7.0504
	step [56/254], loss=6.6931
	step [57/254], loss=6.5061
	step [58/254], loss=6.2948
	step [59/254], loss=7.2718
	step [60/254], loss=6.4466
	step [61/254], loss=7.3027
	step [62/254], loss=5.1215
	step [63/254], loss=6.4533
	step [64/254], loss=6.6893
	step [65/254], loss=5.3110
	step [66/254], loss=6.3624
	step [67/254], loss=5.6902
	step [68/254], loss=6.8976
	step [69/254], loss=8.5057
	step [70/254], loss=5.2195
	step [71/254], loss=5.9431
	step [72/254], loss=6.6351
	step [73/254], loss=7.1972
	step [74/254], loss=6.3720
	step [75/254], loss=6.2219
	step [76/254], loss=6.6611
	step [77/254], loss=5.1517
	step [78/254], loss=6.0159
	step [79/254], loss=6.5455
	step [80/254], loss=5.0525
	step [81/254], loss=5.5753
	step [82/254], loss=5.7831
	step [83/254], loss=7.0532
	step [84/254], loss=6.7139
	step [85/254], loss=6.2328
	step [86/254], loss=7.8670
	step [87/254], loss=6.0721
	step [88/254], loss=6.6432
	step [89/254], loss=8.0268
	step [90/254], loss=7.0162
	step [91/254], loss=6.2961
	step [92/254], loss=7.4282
	step [93/254], loss=7.2798
	step [94/254], loss=6.3181
	step [95/254], loss=5.4856
	step [96/254], loss=7.2572
	step [97/254], loss=7.5119
	step [98/254], loss=7.3471
	step [99/254], loss=6.4781
	step [100/254], loss=9.7331
	step [101/254], loss=6.3440
	step [102/254], loss=5.3363
	step [103/254], loss=7.1304
	step [104/254], loss=6.1913
	step [105/254], loss=8.0786
	step [106/254], loss=5.4514
	step [107/254], loss=5.0056
	step [108/254], loss=4.6924
	step [109/254], loss=7.1428
	step [110/254], loss=7.8182
	step [111/254], loss=5.4521
	step [112/254], loss=7.8382
	step [113/254], loss=5.6797
	step [114/254], loss=5.0202
	step [115/254], loss=6.5321
	step [116/254], loss=7.3160
	step [117/254], loss=5.2696
	step [118/254], loss=8.8457
	step [119/254], loss=6.0764
	step [120/254], loss=5.4942
	step [121/254], loss=5.7192
	step [122/254], loss=7.0252
	step [123/254], loss=6.2898
	step [124/254], loss=6.5922
	step [125/254], loss=7.1050
	step [126/254], loss=6.3480
	step [127/254], loss=5.9579
	step [128/254], loss=5.9844
	step [129/254], loss=5.2271
	step [130/254], loss=7.2816
	step [131/254], loss=5.6912
	step [132/254], loss=8.7226
	step [133/254], loss=6.7734
	step [134/254], loss=6.4740
	step [135/254], loss=5.2801
	step [136/254], loss=5.8111
	step [137/254], loss=5.0372
	step [138/254], loss=6.3369
	step [139/254], loss=5.5564
	step [140/254], loss=4.5691
	step [141/254], loss=6.4759
	step [142/254], loss=7.4376
	step [143/254], loss=6.0007
	step [144/254], loss=7.1086
	step [145/254], loss=7.5991
	step [146/254], loss=6.9148
	step [147/254], loss=6.1136
	step [148/254], loss=6.1966
	step [149/254], loss=6.5116
	step [150/254], loss=6.4582
	step [151/254], loss=7.7654
	step [152/254], loss=5.9979
	step [153/254], loss=5.6265
	step [154/254], loss=7.1934
	step [155/254], loss=6.7653
	step [156/254], loss=6.1796
	step [157/254], loss=6.2839
	step [158/254], loss=4.5839
	step [159/254], loss=6.0667
	step [160/254], loss=6.3140
	step [161/254], loss=8.8712
	step [162/254], loss=6.5837
	step [163/254], loss=6.2889
	step [164/254], loss=7.5776
	step [165/254], loss=5.4818
	step [166/254], loss=5.5708
	step [167/254], loss=6.0212
	step [168/254], loss=6.5065
	step [169/254], loss=6.5732
	step [170/254], loss=7.0033
	step [171/254], loss=5.5647
	step [172/254], loss=6.9662
	step [173/254], loss=4.7981
	step [174/254], loss=7.5707
	step [175/254], loss=6.4157
	step [176/254], loss=6.0183
	step [177/254], loss=4.6666
	step [178/254], loss=7.5154
	step [179/254], loss=5.4778
	step [180/254], loss=7.6720
	step [181/254], loss=6.6631
	step [182/254], loss=5.3803
	step [183/254], loss=6.5466
	step [184/254], loss=6.1593
	step [185/254], loss=7.6826
	step [186/254], loss=7.1931
	step [187/254], loss=5.2238
	step [188/254], loss=5.0511
	step [189/254], loss=5.1578
	step [190/254], loss=5.4512
	step [191/254], loss=6.4751
	step [192/254], loss=6.1110
	step [193/254], loss=6.0201
	step [194/254], loss=5.8029
	step [195/254], loss=6.8490
	step [196/254], loss=6.5033
	step [197/254], loss=7.1333
	step [198/254], loss=7.3064
	step [199/254], loss=6.6585
	step [200/254], loss=6.2918
	step [201/254], loss=5.9608
	step [202/254], loss=8.3236
	step [203/254], loss=6.5416
	step [204/254], loss=6.4894
	step [205/254], loss=5.7032
	step [206/254], loss=8.2044
	step [207/254], loss=4.9843
	step [208/254], loss=7.5834
	step [209/254], loss=7.0138
	step [210/254], loss=6.5824
	step [211/254], loss=4.5619
	step [212/254], loss=4.2289
	step [213/254], loss=5.7881
	step [214/254], loss=6.2577
	step [215/254], loss=4.3940
	step [216/254], loss=10.5872
	step [217/254], loss=5.7621
	step [218/254], loss=6.7452
	step [219/254], loss=5.3175
	step [220/254], loss=6.7827
	step [221/254], loss=5.2544
	step [222/254], loss=6.2612
	step [223/254], loss=5.8935
	step [224/254], loss=6.4280
	step [225/254], loss=7.0357
	step [226/254], loss=5.9551
	step [227/254], loss=6.0235
	step [228/254], loss=8.2657
	step [229/254], loss=6.3821
	step [230/254], loss=7.3469
	step [231/254], loss=5.5527
	step [232/254], loss=4.8771
	step [233/254], loss=6.5021
	step [234/254], loss=7.0002
	step [235/254], loss=9.0749
	step [236/254], loss=6.5869
	step [237/254], loss=5.8863
	step [238/254], loss=5.6216
	step [239/254], loss=7.1689
	step [240/254], loss=6.5461
	step [241/254], loss=7.1682
	step [242/254], loss=4.8200
	step [243/254], loss=5.6193
	step [244/254], loss=7.2015
	step [245/254], loss=5.9971
	step [246/254], loss=5.9878
	step [247/254], loss=8.1259
	step [248/254], loss=6.8982
	step [249/254], loss=7.0263
	step [250/254], loss=5.7222
	step [251/254], loss=6.0997
	step [252/254], loss=5.8155
	step [253/254], loss=5.0317
	step [254/254], loss=1.1907
	Evaluating
	loss=0.0190, precision=0.2602, recall=0.9904, f1=0.4122
Training epoch 22
	step [1/254], loss=5.8907
	step [2/254], loss=5.3177
	step [3/254], loss=4.7386
	step [4/254], loss=7.7501
	step [5/254], loss=8.4315
	step [6/254], loss=6.5245
	step [7/254], loss=6.0588
	step [8/254], loss=9.2952
	step [9/254], loss=7.1562
	step [10/254], loss=6.0930
	step [11/254], loss=8.0425
	step [12/254], loss=5.9645
	step [13/254], loss=6.1178
	step [14/254], loss=6.2161
	step [15/254], loss=6.6146
	step [16/254], loss=6.8646
	step [17/254], loss=7.6643
	step [18/254], loss=5.7090
	step [19/254], loss=5.8514
	step [20/254], loss=7.6297
	step [21/254], loss=6.6732
	step [22/254], loss=5.4216
	step [23/254], loss=7.9133
	step [24/254], loss=5.8620
	step [25/254], loss=5.6706
	step [26/254], loss=5.7562
	step [27/254], loss=5.4976
	step [28/254], loss=7.2508
	step [29/254], loss=5.4202
	step [30/254], loss=4.9909
	step [31/254], loss=6.4998
	step [32/254], loss=6.1869
	step [33/254], loss=5.6148
	step [34/254], loss=5.4512
	step [35/254], loss=6.3797
	step [36/254], loss=4.8377
	step [37/254], loss=5.6483
	step [38/254], loss=5.4491
	step [39/254], loss=5.9258
	step [40/254], loss=4.8430
	step [41/254], loss=6.6321
	step [42/254], loss=4.8543
	step [43/254], loss=8.3379
	step [44/254], loss=5.0372
	step [45/254], loss=5.4592
	step [46/254], loss=5.6474
	step [47/254], loss=5.1191
	step [48/254], loss=7.5404
	step [49/254], loss=6.2914
	step [50/254], loss=6.4677
	step [51/254], loss=6.8468
	step [52/254], loss=5.5942
	step [53/254], loss=7.3612
	step [54/254], loss=5.7414
	step [55/254], loss=7.7700
	step [56/254], loss=5.5044
	step [57/254], loss=6.8474
	step [58/254], loss=5.1767
	step [59/254], loss=5.1977
	step [60/254], loss=6.5819
	step [61/254], loss=5.5469
	step [62/254], loss=5.3629
	step [63/254], loss=6.3642
	step [64/254], loss=6.4535
	step [65/254], loss=4.9687
	step [66/254], loss=7.5121
	step [67/254], loss=6.8669
	step [68/254], loss=6.0780
	step [69/254], loss=8.0048
	step [70/254], loss=6.9680
	step [71/254], loss=5.3513
	step [72/254], loss=10.0970
	step [73/254], loss=4.5012
	step [74/254], loss=5.8919
	step [75/254], loss=7.5431
	step [76/254], loss=5.7594
	step [77/254], loss=7.2295
	step [78/254], loss=5.6142
	step [79/254], loss=6.6812
	step [80/254], loss=6.2493
	step [81/254], loss=8.8704
	step [82/254], loss=5.2994
	step [83/254], loss=5.9611
	step [84/254], loss=5.9477
	step [85/254], loss=6.4790
	step [86/254], loss=6.3589
	step [87/254], loss=6.2894
	step [88/254], loss=5.1341
	step [89/254], loss=5.7899
	step [90/254], loss=5.0053
	step [91/254], loss=4.5512
	step [92/254], loss=6.4616
	step [93/254], loss=8.7164
	step [94/254], loss=3.8363
	step [95/254], loss=6.8819
	step [96/254], loss=4.5860
	step [97/254], loss=6.1142
	step [98/254], loss=7.4902
	step [99/254], loss=6.3915
	step [100/254], loss=8.2883
	step [101/254], loss=6.5565
	step [102/254], loss=6.2686
	step [103/254], loss=6.8469
	step [104/254], loss=5.6188
	step [105/254], loss=9.5855
	step [106/254], loss=6.6387
	step [107/254], loss=6.1938
	step [108/254], loss=6.6700
	step [109/254], loss=7.2519
	step [110/254], loss=6.4860
	step [111/254], loss=6.1393
	step [112/254], loss=6.7089
	step [113/254], loss=6.0584
	step [114/254], loss=4.8416
	step [115/254], loss=6.9469
	step [116/254], loss=7.9100
	step [117/254], loss=7.1498
	step [118/254], loss=6.8696
	step [119/254], loss=7.4542
	step [120/254], loss=7.2482
	step [121/254], loss=6.0673
	step [122/254], loss=5.8651
	step [123/254], loss=6.8032
	step [124/254], loss=4.9680
	step [125/254], loss=7.8127
	step [126/254], loss=5.2820
	step [127/254], loss=6.1092
	step [128/254], loss=6.1159
	step [129/254], loss=6.2791
	step [130/254], loss=6.1087
	step [131/254], loss=5.8718
	step [132/254], loss=6.0886
	step [133/254], loss=6.2352
	step [134/254], loss=6.6049
	step [135/254], loss=7.4366
	step [136/254], loss=5.8962
	step [137/254], loss=7.2296
	step [138/254], loss=6.2267
	step [139/254], loss=5.2173
	step [140/254], loss=6.1987
	step [141/254], loss=4.9127
	step [142/254], loss=7.6107
	step [143/254], loss=6.1465
	step [144/254], loss=5.0311
	step [145/254], loss=7.4471
	step [146/254], loss=5.7432
	step [147/254], loss=5.9989
	step [148/254], loss=5.3697
	step [149/254], loss=6.0962
	step [150/254], loss=6.3583
	step [151/254], loss=8.2637
	step [152/254], loss=4.3137
	step [153/254], loss=5.3627
	step [154/254], loss=7.5357
	step [155/254], loss=6.3893
	step [156/254], loss=7.2684
	step [157/254], loss=6.1138
	step [158/254], loss=5.9223
	step [159/254], loss=6.6581
	step [160/254], loss=6.1612
	step [161/254], loss=6.3460
	step [162/254], loss=7.8378
	step [163/254], loss=7.5167
	step [164/254], loss=5.8858
	step [165/254], loss=6.2431
	step [166/254], loss=6.8134
	step [167/254], loss=5.5773
	step [168/254], loss=7.9813
	step [169/254], loss=5.1174
	step [170/254], loss=6.2692
	step [171/254], loss=5.8738
	step [172/254], loss=5.8084
	step [173/254], loss=6.3170
	step [174/254], loss=6.2123
	step [175/254], loss=7.9650
	step [176/254], loss=6.0311
	step [177/254], loss=5.0678
	step [178/254], loss=4.2483
	step [179/254], loss=6.4915
	step [180/254], loss=5.2880
	step [181/254], loss=4.9782
	step [182/254], loss=5.5053
	step [183/254], loss=6.6930
	step [184/254], loss=5.4707
	step [185/254], loss=7.5610
	step [186/254], loss=6.4247
	step [187/254], loss=5.8386
	step [188/254], loss=6.8818
	step [189/254], loss=7.5148
	step [190/254], loss=5.0734
	step [191/254], loss=5.6498
	step [192/254], loss=7.5351
	step [193/254], loss=5.9903
	step [194/254], loss=6.1802
	step [195/254], loss=5.6557
	step [196/254], loss=5.3548
	step [197/254], loss=6.1424
	step [198/254], loss=7.3846
	step [199/254], loss=5.7914
	step [200/254], loss=7.7609
	step [201/254], loss=6.1630
	step [202/254], loss=7.3990
	step [203/254], loss=4.8670
	step [204/254], loss=5.0184
	step [205/254], loss=7.3574
	step [206/254], loss=7.3782
	step [207/254], loss=6.4418
	step [208/254], loss=6.4425
	step [209/254], loss=6.2989
	step [210/254], loss=5.8497
	step [211/254], loss=5.4106
	step [212/254], loss=5.3624
	step [213/254], loss=6.4003
	step [214/254], loss=7.7128
	step [215/254], loss=5.4983
	step [216/254], loss=5.6841
	step [217/254], loss=8.2758
	step [218/254], loss=5.9002
	step [219/254], loss=6.6900
	step [220/254], loss=5.3903
	step [221/254], loss=7.4826
	step [222/254], loss=6.5771
	step [223/254], loss=6.6591
	step [224/254], loss=6.5844
	step [225/254], loss=4.6159
	step [226/254], loss=7.8814
	step [227/254], loss=6.1111
	step [228/254], loss=4.5975
	step [229/254], loss=5.6531
	step [230/254], loss=8.6111
	step [231/254], loss=5.0911
	step [232/254], loss=5.9698
	step [233/254], loss=5.5770
	step [234/254], loss=6.2683
	step [235/254], loss=5.5380
	step [236/254], loss=5.0105
	step [237/254], loss=6.5952
	step [238/254], loss=5.2708
	step [239/254], loss=7.0387
	step [240/254], loss=5.5085
	step [241/254], loss=5.2380
	step [242/254], loss=4.8836
	step [243/254], loss=4.8743
	step [244/254], loss=6.8496
	step [245/254], loss=7.8437
	step [246/254], loss=5.0066
	step [247/254], loss=6.3011
	step [248/254], loss=7.2553
	step [249/254], loss=6.4418
	step [250/254], loss=5.9507
	step [251/254], loss=7.3064
	step [252/254], loss=7.1479
	step [253/254], loss=6.4263
	step [254/254], loss=1.3560
	Evaluating
	loss=0.0251, precision=0.2053, recall=0.9943, f1=0.3403
Training epoch 23
	step [1/254], loss=5.4406
	step [2/254], loss=6.3947
	step [3/254], loss=5.4695
	step [4/254], loss=9.3660
	step [5/254], loss=7.3228
	step [6/254], loss=5.5531
	step [7/254], loss=7.1571
	step [8/254], loss=5.0397
	step [9/254], loss=5.4131
	step [10/254], loss=6.9708
	step [11/254], loss=6.5303
	step [12/254], loss=5.9784
	step [13/254], loss=5.8292
	step [14/254], loss=7.8616
	step [15/254], loss=6.1649
	step [16/254], loss=6.4711
	step [17/254], loss=6.2562
	step [18/254], loss=6.5686
	step [19/254], loss=7.6518
	step [20/254], loss=5.2712
	step [21/254], loss=5.4386
	step [22/254], loss=5.9625
	step [23/254], loss=5.8532
	step [24/254], loss=5.0923
	step [25/254], loss=6.6896
	step [26/254], loss=6.3526
	step [27/254], loss=6.7770
	step [28/254], loss=7.5841
	step [29/254], loss=7.0254
	step [30/254], loss=5.2173
	step [31/254], loss=5.5597
	step [32/254], loss=7.2921
	step [33/254], loss=5.7574
	step [34/254], loss=5.6763
	step [35/254], loss=4.5647
	step [36/254], loss=6.5182
	step [37/254], loss=7.2999
	step [38/254], loss=6.3833
	step [39/254], loss=8.7272
	step [40/254], loss=5.0659
	step [41/254], loss=5.4484
	step [42/254], loss=5.9861
	step [43/254], loss=5.9644
	step [44/254], loss=7.4998
	step [45/254], loss=6.0957
	step [46/254], loss=5.6842
	step [47/254], loss=6.2676
	step [48/254], loss=5.8584
	step [49/254], loss=6.9387
	step [50/254], loss=7.1059
	step [51/254], loss=5.7833
	step [52/254], loss=6.5944
	step [53/254], loss=6.1043
	step [54/254], loss=5.2536
	step [55/254], loss=5.9629
	step [56/254], loss=7.5296
	step [57/254], loss=6.5403
	step [58/254], loss=4.8502
	step [59/254], loss=6.5500
	step [60/254], loss=6.4800
	step [61/254], loss=7.2774
	step [62/254], loss=4.8716
	step [63/254], loss=6.3717
	step [64/254], loss=6.3489
	step [65/254], loss=5.4645
	step [66/254], loss=6.0435
	step [67/254], loss=6.3463
	step [68/254], loss=6.7775
	step [69/254], loss=6.9007
	step [70/254], loss=7.5699
	step [71/254], loss=6.7270
	step [72/254], loss=6.5277
	step [73/254], loss=7.7538
	step [74/254], loss=7.4081
	step [75/254], loss=5.6706
	step [76/254], loss=5.9971
	step [77/254], loss=4.9580
	step [78/254], loss=5.2901
	step [79/254], loss=7.4946
	step [80/254], loss=5.6256
	step [81/254], loss=4.9942
	step [82/254], loss=6.0799
	step [83/254], loss=4.5818
	step [84/254], loss=5.3715
	step [85/254], loss=4.8677
	step [86/254], loss=6.7110
	step [87/254], loss=5.5023
	step [88/254], loss=7.6861
	step [89/254], loss=6.6187
	step [90/254], loss=4.9639
	step [91/254], loss=5.1331
	step [92/254], loss=5.8769
	step [93/254], loss=5.4843
	step [94/254], loss=5.1876
	step [95/254], loss=6.3327
	step [96/254], loss=5.5346
	step [97/254], loss=7.8287
	step [98/254], loss=5.4708
	step [99/254], loss=6.0767
	step [100/254], loss=6.4430
	step [101/254], loss=7.3115
	step [102/254], loss=5.5354
	step [103/254], loss=6.2354
	step [104/254], loss=5.5986
	step [105/254], loss=5.3213
	step [106/254], loss=6.4638
	step [107/254], loss=6.8634
	step [108/254], loss=5.2286
	step [109/254], loss=4.6533
	step [110/254], loss=5.8391
	step [111/254], loss=5.8930
	step [112/254], loss=7.4634
	step [113/254], loss=5.7669
	step [114/254], loss=5.7695
	step [115/254], loss=7.6714
	step [116/254], loss=5.7951
	step [117/254], loss=6.2544
	step [118/254], loss=7.7302
	step [119/254], loss=5.9084
	step [120/254], loss=5.7751
	step [121/254], loss=6.6144
	step [122/254], loss=7.4011
	step [123/254], loss=4.6290
	step [124/254], loss=5.2070
	step [125/254], loss=4.9728
	step [126/254], loss=5.7875
	step [127/254], loss=6.1079
	step [128/254], loss=5.4629
	step [129/254], loss=6.5125
	step [130/254], loss=5.7801
	step [131/254], loss=6.8603
	step [132/254], loss=7.4761
	step [133/254], loss=6.8186
	step [134/254], loss=6.3418
	step [135/254], loss=7.4103
	step [136/254], loss=5.2279
	step [137/254], loss=6.0774
	step [138/254], loss=5.3147
	step [139/254], loss=5.2147
	step [140/254], loss=7.2750
	step [141/254], loss=7.1050
	step [142/254], loss=6.2208
	step [143/254], loss=4.9615
	step [144/254], loss=7.1481
	step [145/254], loss=5.1218
	step [146/254], loss=5.7230
	step [147/254], loss=5.4458
	step [148/254], loss=4.9351
	step [149/254], loss=5.1797
	step [150/254], loss=6.3192
	step [151/254], loss=8.0349
	step [152/254], loss=6.3822
	step [153/254], loss=5.7520
	step [154/254], loss=5.9691
	step [155/254], loss=6.6967
	step [156/254], loss=5.1593
	step [157/254], loss=6.3281
	step [158/254], loss=7.3899
	step [159/254], loss=6.2380
	step [160/254], loss=6.1753
	step [161/254], loss=7.0797
	step [162/254], loss=8.3835
	step [163/254], loss=6.9781
	step [164/254], loss=7.7202
	step [165/254], loss=6.4970
	step [166/254], loss=5.2792
	step [167/254], loss=5.7421
	step [168/254], loss=5.7177
	step [169/254], loss=6.1424
	step [170/254], loss=8.1177
	step [171/254], loss=4.0969
	step [172/254], loss=5.3455
	step [173/254], loss=5.4098
	step [174/254], loss=6.2513
	step [175/254], loss=4.7383
	step [176/254], loss=5.5372
	step [177/254], loss=4.2096
	step [178/254], loss=4.6154
	step [179/254], loss=5.2921
	step [180/254], loss=6.8296
	step [181/254], loss=8.4601
	step [182/254], loss=5.9259
	step [183/254], loss=6.6978
	step [184/254], loss=6.2264
	step [185/254], loss=6.2371
	step [186/254], loss=5.2483
	step [187/254], loss=4.7044
	step [188/254], loss=5.6700
	step [189/254], loss=6.3428
	step [190/254], loss=5.6698
	step [191/254], loss=5.7953
	step [192/254], loss=7.1986
	step [193/254], loss=7.1801
	step [194/254], loss=6.4070
	step [195/254], loss=5.1146
	step [196/254], loss=6.5604
	step [197/254], loss=8.2518
	step [198/254], loss=5.2360
	step [199/254], loss=5.2339
	step [200/254], loss=5.2335
	step [201/254], loss=6.0497
	step [202/254], loss=6.8376
	step [203/254], loss=6.7641
	step [204/254], loss=6.9822
	step [205/254], loss=6.3436
	step [206/254], loss=6.1473
	step [207/254], loss=6.3774
	step [208/254], loss=5.1153
	step [209/254], loss=6.3602
	step [210/254], loss=5.4417
	step [211/254], loss=6.7554
	step [212/254], loss=6.5245
	step [213/254], loss=5.3155
	step [214/254], loss=8.7089
	step [215/254], loss=5.6932
	step [216/254], loss=6.7446
	step [217/254], loss=6.9645
	step [218/254], loss=5.8191
	step [219/254], loss=5.8524
	step [220/254], loss=6.1818
	step [221/254], loss=7.9202
	step [222/254], loss=5.1061
	step [223/254], loss=7.1537
	step [224/254], loss=5.0883
	step [225/254], loss=6.4825
	step [226/254], loss=7.0921
	step [227/254], loss=6.8382
	step [228/254], loss=5.5688
	step [229/254], loss=5.5000
	step [230/254], loss=5.1622
	step [231/254], loss=6.2702
	step [232/254], loss=5.4142
	step [233/254], loss=6.1430
	step [234/254], loss=6.9856
	step [235/254], loss=7.5272
	step [236/254], loss=6.7484
	step [237/254], loss=5.1849
	step [238/254], loss=7.3438
	step [239/254], loss=6.3143
	step [240/254], loss=7.0127
	step [241/254], loss=6.0696
	step [242/254], loss=5.6839
	step [243/254], loss=6.2528
	step [244/254], loss=6.7926
	step [245/254], loss=7.4330
	step [246/254], loss=5.6273
	step [247/254], loss=7.8546
	step [248/254], loss=5.7866
	step [249/254], loss=8.2326
	step [250/254], loss=8.2964
	step [251/254], loss=7.2872
	step [252/254], loss=5.8347
	step [253/254], loss=4.7397
	step [254/254], loss=1.3165
	Evaluating
	loss=0.0264, precision=0.1850, recall=0.9938, f1=0.3119
Training epoch 24
	step [1/254], loss=5.3541
	step [2/254], loss=7.3376
	step [3/254], loss=6.0324
	step [4/254], loss=7.0506
	step [5/254], loss=5.4479
	step [6/254], loss=4.9525
	step [7/254], loss=7.1819
	step [8/254], loss=4.7099
	step [9/254], loss=5.2505
	step [10/254], loss=6.0000
	step [11/254], loss=5.4575
	step [12/254], loss=9.0220
	step [13/254], loss=5.3986
	step [14/254], loss=5.7263
	step [15/254], loss=5.9871
	step [16/254], loss=6.3150
	step [17/254], loss=5.1731
	step [18/254], loss=4.6926
	step [19/254], loss=6.7140
	step [20/254], loss=5.3538
	step [21/254], loss=5.2723
	step [22/254], loss=4.9598
	step [23/254], loss=6.9331
	step [24/254], loss=4.4885
	step [25/254], loss=6.0635
	step [26/254], loss=5.0018
	step [27/254], loss=6.1110
	step [28/254], loss=9.4019
	step [29/254], loss=6.0467
	step [30/254], loss=5.4611
	step [31/254], loss=6.1855
	step [32/254], loss=6.9301
	step [33/254], loss=5.1296
	step [34/254], loss=7.0201
	step [35/254], loss=6.6749
	step [36/254], loss=4.7807
	step [37/254], loss=6.6946
	step [38/254], loss=7.3506
	step [39/254], loss=5.0479
	step [40/254], loss=3.6711
	step [41/254], loss=6.3963
	step [42/254], loss=7.0002
	step [43/254], loss=6.9244
	step [44/254], loss=6.6722
	step [45/254], loss=5.2413
	step [46/254], loss=4.7002
	step [47/254], loss=5.1178
	step [48/254], loss=5.9127
	step [49/254], loss=5.4518
	step [50/254], loss=5.8328
	step [51/254], loss=6.1072
	step [52/254], loss=5.7180
	step [53/254], loss=7.3856
	step [54/254], loss=6.0936
	step [55/254], loss=6.0435
	step [56/254], loss=6.3202
	step [57/254], loss=5.5178
	step [58/254], loss=5.7012
	step [59/254], loss=5.6014
	step [60/254], loss=7.2295
	step [61/254], loss=5.3456
	step [62/254], loss=5.3959
	step [63/254], loss=6.6364
	step [64/254], loss=6.1239
	step [65/254], loss=5.9932
	step [66/254], loss=6.7510
	step [67/254], loss=6.6813
	step [68/254], loss=6.8294
	step [69/254], loss=6.4790
	step [70/254], loss=6.0697
	step [71/254], loss=6.2132
	step [72/254], loss=6.6164
	step [73/254], loss=5.6774
	step [74/254], loss=5.1476
	step [75/254], loss=4.5976
	step [76/254], loss=5.9860
	step [77/254], loss=4.2885
	step [78/254], loss=6.1622
	step [79/254], loss=6.4518
	step [80/254], loss=7.5237
	step [81/254], loss=5.9405
	step [82/254], loss=6.1199
	step [83/254], loss=7.0263
	step [84/254], loss=5.7666
	step [85/254], loss=5.5889
	step [86/254], loss=6.2467
	step [87/254], loss=6.9914
	step [88/254], loss=7.0803
	step [89/254], loss=6.4685
	step [90/254], loss=6.3228
	step [91/254], loss=4.8842
	step [92/254], loss=4.6749
	step [93/254], loss=7.1375
	step [94/254], loss=6.6298
	step [95/254], loss=6.1595
	step [96/254], loss=5.3430
	step [97/254], loss=5.2985
	step [98/254], loss=5.8527
	step [99/254], loss=8.5944
	step [100/254], loss=5.0656
	step [101/254], loss=4.8437
	step [102/254], loss=5.5501
	step [103/254], loss=6.1641
	step [104/254], loss=7.2347
	step [105/254], loss=4.8118
	step [106/254], loss=6.9947
	step [107/254], loss=6.3866
	step [108/254], loss=7.4184
	step [109/254], loss=6.7326
	step [110/254], loss=4.7334
	step [111/254], loss=5.7172
	step [112/254], loss=8.0358
	step [113/254], loss=5.1293
	step [114/254], loss=7.1371
	step [115/254], loss=5.1402
	step [116/254], loss=7.1823
	step [117/254], loss=4.9424
	step [118/254], loss=7.1436
	step [119/254], loss=7.4706
	step [120/254], loss=6.3277
	step [121/254], loss=5.9369
	step [122/254], loss=6.1346
	step [123/254], loss=5.4848
	step [124/254], loss=5.5688
	step [125/254], loss=5.4577
	step [126/254], loss=6.1571
	step [127/254], loss=6.3679
	step [128/254], loss=7.9219
	step [129/254], loss=8.6100
	step [130/254], loss=5.0225
	step [131/254], loss=6.4151
	step [132/254], loss=7.6856
	step [133/254], loss=7.0600
	step [134/254], loss=5.6536
	step [135/254], loss=6.8594
	step [136/254], loss=6.8086
	step [137/254], loss=5.2735
	step [138/254], loss=4.6322
	step [139/254], loss=5.7220
	step [140/254], loss=4.5203
	step [141/254], loss=5.6444
	step [142/254], loss=6.6928
	step [143/254], loss=4.5982
	step [144/254], loss=6.2399
	step [145/254], loss=6.4678
	step [146/254], loss=6.2917
	step [147/254], loss=6.8598
	step [148/254], loss=7.1707
	step [149/254], loss=6.6638
	step [150/254], loss=5.6424
	step [151/254], loss=5.8464
	step [152/254], loss=6.4407
	step [153/254], loss=5.6342
	step [154/254], loss=6.2745
	step [155/254], loss=6.8896
	step [156/254], loss=6.6348
	step [157/254], loss=6.5169
	step [158/254], loss=6.8538
	step [159/254], loss=7.0897
	step [160/254], loss=6.5930
	step [161/254], loss=5.8528
	step [162/254], loss=8.7255
	step [163/254], loss=5.8510
	step [164/254], loss=6.2683
	step [165/254], loss=4.9731
	step [166/254], loss=5.8774
	step [167/254], loss=4.9274
	step [168/254], loss=6.8626
	step [169/254], loss=5.2159
	step [170/254], loss=9.5132
	step [171/254], loss=4.6257
	step [172/254], loss=5.5488
	step [173/254], loss=5.5367
	step [174/254], loss=6.9625
	step [175/254], loss=4.8451
	step [176/254], loss=4.8275
	step [177/254], loss=5.5638
	step [178/254], loss=6.9302
	step [179/254], loss=7.8754
	step [180/254], loss=9.5360
	step [181/254], loss=4.7552
	step [182/254], loss=7.1696
	step [183/254], loss=7.3104
	step [184/254], loss=6.1540
	step [185/254], loss=5.2542
	step [186/254], loss=5.6658
	step [187/254], loss=6.8242
	step [188/254], loss=5.4989
	step [189/254], loss=8.2739
	step [190/254], loss=6.4944
	step [191/254], loss=5.1562
	step [192/254], loss=6.9210
	step [193/254], loss=5.4108
	step [194/254], loss=6.1867
	step [195/254], loss=5.6840
	step [196/254], loss=5.3933
	step [197/254], loss=4.6388
	step [198/254], loss=7.5243
	step [199/254], loss=5.5834
	step [200/254], loss=4.6114
	step [201/254], loss=5.9859
	step [202/254], loss=6.5358
	step [203/254], loss=5.2564
	step [204/254], loss=6.7730
	step [205/254], loss=8.0554
	step [206/254], loss=6.5524
	step [207/254], loss=5.3690
	step [208/254], loss=6.7905
	step [209/254], loss=6.5657
	step [210/254], loss=7.3501
	step [211/254], loss=6.7820
	step [212/254], loss=5.6275
	step [213/254], loss=5.7203
	step [214/254], loss=5.4409
	step [215/254], loss=6.1929
	step [216/254], loss=6.2259
	step [217/254], loss=5.6898
	step [218/254], loss=6.1877
	step [219/254], loss=6.2377
	step [220/254], loss=6.9884
	step [221/254], loss=6.9235
	step [222/254], loss=5.8449
	step [223/254], loss=5.5037
	step [224/254], loss=6.4742
	step [225/254], loss=7.2579
	step [226/254], loss=5.7072
	step [227/254], loss=5.0720
	step [228/254], loss=5.6480
	step [229/254], loss=7.4060
	step [230/254], loss=6.5643
	step [231/254], loss=5.5777
	step [232/254], loss=5.4760
	step [233/254], loss=6.5350
	step [234/254], loss=5.5384
	step [235/254], loss=6.8319
	step [236/254], loss=8.1166
	step [237/254], loss=5.1561
	step [238/254], loss=5.8801
	step [239/254], loss=6.5016
	step [240/254], loss=7.4467
	step [241/254], loss=5.5632
	step [242/254], loss=7.2435
	step [243/254], loss=5.3851
	step [244/254], loss=6.7226
	step [245/254], loss=4.9817
	step [246/254], loss=5.9263
	step [247/254], loss=8.1043
	step [248/254], loss=5.1515
	step [249/254], loss=7.0386
	step [250/254], loss=4.9677
	step [251/254], loss=5.5791
	step [252/254], loss=5.6122
	step [253/254], loss=7.4551
	step [254/254], loss=1.1995
	Evaluating
	loss=0.0206, precision=0.2469, recall=0.9920, f1=0.3954
Training epoch 25
	step [1/254], loss=5.6489
	step [2/254], loss=4.8738
	step [3/254], loss=6.0403
	step [4/254], loss=6.1345
	step [5/254], loss=6.0930
	step [6/254], loss=6.6732
	step [7/254], loss=6.7225
	step [8/254], loss=4.7501
	step [9/254], loss=6.5569
	step [10/254], loss=5.6626
	step [11/254], loss=5.9371
	step [12/254], loss=5.6167
	step [13/254], loss=5.6916
	step [14/254], loss=7.1830
	step [15/254], loss=7.8758
	step [16/254], loss=5.9554
	step [17/254], loss=6.1604
	step [18/254], loss=4.9317
	step [19/254], loss=5.8849
	step [20/254], loss=6.9024
	step [21/254], loss=7.0206
	step [22/254], loss=5.3030
	step [23/254], loss=7.0511
	step [24/254], loss=5.5142
	step [25/254], loss=5.7816
	step [26/254], loss=6.3526
	step [27/254], loss=5.5533
	step [28/254], loss=5.0406
	step [29/254], loss=6.4849
	step [30/254], loss=5.1978
	step [31/254], loss=4.5821
	step [32/254], loss=6.2166
	step [33/254], loss=6.1223
	step [34/254], loss=5.4460
	step [35/254], loss=5.7409
	step [36/254], loss=6.0400
	step [37/254], loss=5.2186
	step [38/254], loss=5.6674
	step [39/254], loss=5.1485
	step [40/254], loss=5.7498
	step [41/254], loss=5.8491
	step [42/254], loss=6.2252
	step [43/254], loss=6.1608
	step [44/254], loss=7.4353
	step [45/254], loss=6.1560
	step [46/254], loss=5.8810
	step [47/254], loss=5.1957
	step [48/254], loss=5.7437
	step [49/254], loss=4.8334
	step [50/254], loss=4.7559
	step [51/254], loss=6.4234
	step [52/254], loss=4.4157
	step [53/254], loss=6.3577
	step [54/254], loss=6.3033
	step [55/254], loss=6.5941
	step [56/254], loss=6.8025
	step [57/254], loss=3.7819
	step [58/254], loss=5.2387
	step [59/254], loss=6.2549
	step [60/254], loss=7.4532
	step [61/254], loss=7.4271
	step [62/254], loss=5.4537
	step [63/254], loss=5.4109
	step [64/254], loss=5.0851
	step [65/254], loss=6.9634
	step [66/254], loss=5.2439
	step [67/254], loss=6.2535
	step [68/254], loss=5.4694
	step [69/254], loss=5.9978
	step [70/254], loss=6.2801
	step [71/254], loss=5.1838
	step [72/254], loss=7.7394
	step [73/254], loss=5.7791
	step [74/254], loss=6.2596
	step [75/254], loss=5.4951
	step [76/254], loss=5.7373
	step [77/254], loss=6.3772
	step [78/254], loss=5.5708
	step [79/254], loss=6.3512
	step [80/254], loss=5.2600
	step [81/254], loss=5.7876
	step [82/254], loss=5.3751
	step [83/254], loss=7.3692
	step [84/254], loss=6.9892
	step [85/254], loss=5.8751
	step [86/254], loss=7.2555
	step [87/254], loss=6.7737
	step [88/254], loss=7.3138
	step [89/254], loss=6.3703
	step [90/254], loss=6.0891
	step [91/254], loss=5.5888
	step [92/254], loss=6.4605
	step [93/254], loss=5.6638
	step [94/254], loss=6.0050
	step [95/254], loss=4.6831
	step [96/254], loss=6.0157
	step [97/254], loss=5.4520
	step [98/254], loss=6.8287
	step [99/254], loss=5.8126
	step [100/254], loss=8.9810
	step [101/254], loss=5.6461
	step [102/254], loss=5.2391
	step [103/254], loss=6.8875
	step [104/254], loss=5.8565
	step [105/254], loss=5.7553
	step [106/254], loss=5.4919
	step [107/254], loss=6.5630
	step [108/254], loss=6.1196
	step [109/254], loss=7.7690
	step [110/254], loss=6.7504
	step [111/254], loss=6.2462
	step [112/254], loss=6.5538
	step [113/254], loss=5.8411
	step [114/254], loss=4.5952
	step [115/254], loss=5.1850
	step [116/254], loss=7.7080
	step [117/254], loss=5.9293
	step [118/254], loss=6.2314
	step [119/254], loss=6.2052
	step [120/254], loss=6.5496
	step [121/254], loss=5.9139
	step [122/254], loss=5.6028
	step [123/254], loss=6.4705
	step [124/254], loss=4.5870
	step [125/254], loss=6.1820
	step [126/254], loss=6.4334
	step [127/254], loss=6.1537
	step [128/254], loss=6.8109
	step [129/254], loss=6.3230
	step [130/254], loss=5.0992
	step [131/254], loss=9.0214
	step [132/254], loss=6.3002
	step [133/254], loss=4.8930
	step [134/254], loss=7.1204
	step [135/254], loss=5.7067
	step [136/254], loss=4.3888
	step [137/254], loss=5.9410
	step [138/254], loss=6.2255
	step [139/254], loss=6.1337
	step [140/254], loss=5.8191
	step [141/254], loss=6.6491
	step [142/254], loss=6.1513
	step [143/254], loss=6.1842
	step [144/254], loss=6.4342
	step [145/254], loss=7.1462
	step [146/254], loss=7.1133
	step [147/254], loss=5.5458
	step [148/254], loss=7.1161
	step [149/254], loss=5.6753
	step [150/254], loss=6.1925
	step [151/254], loss=4.7691
	step [152/254], loss=5.9960
	step [153/254], loss=5.2114
	step [154/254], loss=5.2638
	step [155/254], loss=7.3049
	step [156/254], loss=5.7303
	step [157/254], loss=6.0614
	step [158/254], loss=5.5672
	step [159/254], loss=5.2568
	step [160/254], loss=6.2094
	step [161/254], loss=5.5111
	step [162/254], loss=5.4315
	step [163/254], loss=5.0405
	step [164/254], loss=6.1145
	step [165/254], loss=6.1900
	step [166/254], loss=4.5297
	step [167/254], loss=6.6069
	step [168/254], loss=5.8818
	step [169/254], loss=6.4088
	step [170/254], loss=5.8469
	step [171/254], loss=6.7468
	step [172/254], loss=5.1858
	step [173/254], loss=6.1171
	step [174/254], loss=7.8355
	step [175/254], loss=6.0774
	step [176/254], loss=5.7162
	step [177/254], loss=5.9420
	step [178/254], loss=6.4478
	step [179/254], loss=5.8675
	step [180/254], loss=5.9927
	step [181/254], loss=6.4221
	step [182/254], loss=5.0409
	step [183/254], loss=6.7897
	step [184/254], loss=6.4278
	step [185/254], loss=6.3230
	step [186/254], loss=7.1232
	step [187/254], loss=4.0430
	step [188/254], loss=6.1756
	step [189/254], loss=5.3059
	step [190/254], loss=5.0312
	step [191/254], loss=6.4414
	step [192/254], loss=6.2624
	step [193/254], loss=6.5300
	step [194/254], loss=6.2548
	step [195/254], loss=5.3799
	step [196/254], loss=6.3454
	step [197/254], loss=6.8079
	step [198/254], loss=6.1154
	step [199/254], loss=6.6251
	step [200/254], loss=5.7067
	step [201/254], loss=5.4086
	step [202/254], loss=5.8971
	step [203/254], loss=6.5176
	step [204/254], loss=5.4078
	step [205/254], loss=4.7380
	step [206/254], loss=7.5000
	step [207/254], loss=7.9708
	step [208/254], loss=6.1052
	step [209/254], loss=5.6579
	step [210/254], loss=5.8417
	step [211/254], loss=5.1627
	step [212/254], loss=4.8924
	step [213/254], loss=6.1789
	step [214/254], loss=5.8696
	step [215/254], loss=7.9420
	step [216/254], loss=6.0879
	step [217/254], loss=5.4698
	step [218/254], loss=5.3924
	step [219/254], loss=4.2911
	step [220/254], loss=6.1730
	step [221/254], loss=4.6115
	step [222/254], loss=4.3788
	step [223/254], loss=4.8803
	step [224/254], loss=7.3087
	step [225/254], loss=5.1370
	step [226/254], loss=5.1164
	step [227/254], loss=8.5910
	step [228/254], loss=5.5093
	step [229/254], loss=7.5213
	step [230/254], loss=6.0919
	step [231/254], loss=5.6854
	step [232/254], loss=6.3054
	step [233/254], loss=5.0413
	step [234/254], loss=6.5460
	step [235/254], loss=6.5972
	step [236/254], loss=6.2711
	step [237/254], loss=7.3146
	step [238/254], loss=6.0767
	step [239/254], loss=6.6577
	step [240/254], loss=6.1020
	step [241/254], loss=5.7662
	step [242/254], loss=5.3640
	step [243/254], loss=6.1127
	step [244/254], loss=5.9366
	step [245/254], loss=5.2674
	step [246/254], loss=4.6172
	step [247/254], loss=5.5492
	step [248/254], loss=5.3527
	step [249/254], loss=5.9636
	step [250/254], loss=4.4346
	step [251/254], loss=6.3188
	step [252/254], loss=7.1658
	step [253/254], loss=7.2710
	step [254/254], loss=1.6131
	Evaluating
	loss=0.0198, precision=0.2423, recall=0.9920, f1=0.3894
Training epoch 26
	step [1/254], loss=6.4470
	step [2/254], loss=6.0281
	step [3/254], loss=5.1289
	step [4/254], loss=6.5265
	step [5/254], loss=6.6212
	step [6/254], loss=5.6855
	step [7/254], loss=4.8774
	step [8/254], loss=8.2080
	step [9/254], loss=6.7846
	step [10/254], loss=5.7794
	step [11/254], loss=4.5342
	step [12/254], loss=5.3851
	step [13/254], loss=5.5041
	step [14/254], loss=4.5682
	step [15/254], loss=5.6200
	step [16/254], loss=7.0753
	step [17/254], loss=5.4312
	step [18/254], loss=5.8450
	step [19/254], loss=5.6064
	step [20/254], loss=5.5860
	step [21/254], loss=5.1811
	step [22/254], loss=6.1782
	step [23/254], loss=5.7837
	step [24/254], loss=6.5922
	step [25/254], loss=6.2453
	step [26/254], loss=4.5557
	step [27/254], loss=5.3829
	step [28/254], loss=6.5576
	step [29/254], loss=6.1876
	step [30/254], loss=5.4736
	step [31/254], loss=6.2055
	step [32/254], loss=7.1107
	step [33/254], loss=6.0793
	step [34/254], loss=8.0067
	step [35/254], loss=5.3569
	step [36/254], loss=5.5666
	step [37/254], loss=5.6401
	step [38/254], loss=6.2079
	step [39/254], loss=4.8933
	step [40/254], loss=5.1850
	step [41/254], loss=5.4056
	step [42/254], loss=6.2577
	step [43/254], loss=6.2334
	step [44/254], loss=5.9902
	step [45/254], loss=6.6530
	step [46/254], loss=5.2104
	step [47/254], loss=5.8821
	step [48/254], loss=7.6728
	step [49/254], loss=5.6065
	step [50/254], loss=6.1469
	step [51/254], loss=5.6415
	step [52/254], loss=7.2752
	step [53/254], loss=5.3470
	step [54/254], loss=5.5203
	step [55/254], loss=5.7993
	step [56/254], loss=6.7519
	step [57/254], loss=5.9212
	step [58/254], loss=5.7812
	step [59/254], loss=5.5989
	step [60/254], loss=5.3403
	step [61/254], loss=6.1696
	step [62/254], loss=5.3622
	step [63/254], loss=4.3904
	step [64/254], loss=7.0789
	step [65/254], loss=5.7873
	step [66/254], loss=4.8052
	step [67/254], loss=6.3023
	step [68/254], loss=7.5832
	step [69/254], loss=5.8528
	step [70/254], loss=6.0022
	step [71/254], loss=4.8867
	step [72/254], loss=6.9749
	step [73/254], loss=7.0675
	step [74/254], loss=7.0532
	step [75/254], loss=7.1863
	step [76/254], loss=6.7204
	step [77/254], loss=8.3778
	step [78/254], loss=6.0321
	step [79/254], loss=9.1318
	step [80/254], loss=6.9986
	step [81/254], loss=6.4378
	step [82/254], loss=5.2149
	step [83/254], loss=5.9383
	step [84/254], loss=7.0515
	step [85/254], loss=6.2100
	step [86/254], loss=6.2238
	step [87/254], loss=5.3945
	step [88/254], loss=5.5882
	step [89/254], loss=5.2396
	step [90/254], loss=6.1483
	step [91/254], loss=5.0635
	step [92/254], loss=4.3189
	step [93/254], loss=5.4461
	step [94/254], loss=5.9727
	step [95/254], loss=5.8448
	step [96/254], loss=4.8927
	step [97/254], loss=7.8265
	step [98/254], loss=4.6469
	step [99/254], loss=5.1237
	step [100/254], loss=7.3059
	step [101/254], loss=5.8751
	step [102/254], loss=6.1576
	step [103/254], loss=5.8176
	step [104/254], loss=5.4503
	step [105/254], loss=7.0862
	step [106/254], loss=5.3439
	step [107/254], loss=5.9388
	step [108/254], loss=7.0149
	step [109/254], loss=5.3047
	step [110/254], loss=7.1824
	step [111/254], loss=7.1285
	step [112/254], loss=6.0492
	step [113/254], loss=6.2146
	step [114/254], loss=6.5433
	step [115/254], loss=6.1454
	step [116/254], loss=5.7669
	step [117/254], loss=7.0078
	step [118/254], loss=5.4359
	step [119/254], loss=3.8993
	step [120/254], loss=5.0073
	step [121/254], loss=6.3419
	step [122/254], loss=4.9643
	step [123/254], loss=6.6221
	step [124/254], loss=7.0779
	step [125/254], loss=6.1210
	step [126/254], loss=5.7729
	step [127/254], loss=6.2406
	step [128/254], loss=4.8466
	step [129/254], loss=5.8191
	step [130/254], loss=5.4896
	step [131/254], loss=5.4768
	step [132/254], loss=5.3201
	step [133/254], loss=5.1722
	step [134/254], loss=5.9644
	step [135/254], loss=6.3827
	step [136/254], loss=7.4071
	step [137/254], loss=5.6537
	step [138/254], loss=5.5011
	step [139/254], loss=7.4427
	step [140/254], loss=6.0298
	step [141/254], loss=5.5625
	step [142/254], loss=6.0878
	step [143/254], loss=8.6037
	step [144/254], loss=8.1634
	step [145/254], loss=5.3826
	step [146/254], loss=4.3594
	step [147/254], loss=6.3204
	step [148/254], loss=4.6309
	step [149/254], loss=5.5650
	step [150/254], loss=5.0905
	step [151/254], loss=5.9788
	step [152/254], loss=6.2155
	step [153/254], loss=5.4414
	step [154/254], loss=3.7614
	step [155/254], loss=6.7755
	step [156/254], loss=5.0135
	step [157/254], loss=6.1257
	step [158/254], loss=6.6826
	step [159/254], loss=6.6705
	step [160/254], loss=6.0455
	step [161/254], loss=5.8025
	step [162/254], loss=4.5260
	step [163/254], loss=6.1680
	step [164/254], loss=6.4393
	step [165/254], loss=5.6889
	step [166/254], loss=6.1521
	step [167/254], loss=6.3409
	step [168/254], loss=5.3699
	step [169/254], loss=6.8469
	step [170/254], loss=5.1629
	step [171/254], loss=5.6591
	step [172/254], loss=5.0898
	step [173/254], loss=5.1654
	step [174/254], loss=5.3896
	step [175/254], loss=4.6527
	step [176/254], loss=6.0722
	step [177/254], loss=7.1376
	step [178/254], loss=7.8111
	step [179/254], loss=6.9036
	step [180/254], loss=5.3160
	step [181/254], loss=4.3688
	step [182/254], loss=5.0209
	step [183/254], loss=5.7201
	step [184/254], loss=6.1627
	step [185/254], loss=5.5825
	step [186/254], loss=5.0752
	step [187/254], loss=5.7624
	step [188/254], loss=5.6468
	step [189/254], loss=5.1797
	step [190/254], loss=5.6945
	step [191/254], loss=6.2567
	step [192/254], loss=5.3959
	step [193/254], loss=7.6196
	step [194/254], loss=5.7902
	step [195/254], loss=5.4685
	step [196/254], loss=5.8299
	step [197/254], loss=5.7606
	step [198/254], loss=4.8760
	step [199/254], loss=6.4697
	step [200/254], loss=5.9649
	step [201/254], loss=5.6825
	step [202/254], loss=4.5170
	step [203/254], loss=5.4198
	step [204/254], loss=6.6798
	step [205/254], loss=5.5067
	step [206/254], loss=5.7287
	step [207/254], loss=5.3539
	step [208/254], loss=5.5075
	step [209/254], loss=4.6533
	step [210/254], loss=5.2039
	step [211/254], loss=7.3593
	step [212/254], loss=5.3606
	step [213/254], loss=8.3671
	step [214/254], loss=5.3889
	step [215/254], loss=5.9387
	step [216/254], loss=5.6039
	step [217/254], loss=6.0242
	step [218/254], loss=5.2707
	step [219/254], loss=7.0088
	step [220/254], loss=5.8414
	step [221/254], loss=5.5772
	step [222/254], loss=6.1123
	step [223/254], loss=5.7654
	step [224/254], loss=5.4382
	step [225/254], loss=4.9115
	step [226/254], loss=7.6296
	step [227/254], loss=5.7127
	step [228/254], loss=5.3171
	step [229/254], loss=6.9567
	step [230/254], loss=6.5017
	step [231/254], loss=6.3894
	step [232/254], loss=5.9996
	step [233/254], loss=5.0674
	step [234/254], loss=8.7594
	step [235/254], loss=6.1252
	step [236/254], loss=7.3882
	step [237/254], loss=5.5077
	step [238/254], loss=6.4356
	step [239/254], loss=4.2609
	step [240/254], loss=4.5403
	step [241/254], loss=6.5860
	step [242/254], loss=6.8173
	step [243/254], loss=6.4043
	step [244/254], loss=6.1133
	step [245/254], loss=6.4131
	step [246/254], loss=5.6153
	step [247/254], loss=5.3898
	step [248/254], loss=5.4466
	step [249/254], loss=3.7304
	step [250/254], loss=6.4181
	step [251/254], loss=6.8531
	step [252/254], loss=5.1483
	step [253/254], loss=4.0887
	step [254/254], loss=1.5497
	Evaluating
	loss=0.0176, precision=0.2713, recall=0.9896, f1=0.4259
Training epoch 27
	step [1/254], loss=6.0703
	step [2/254], loss=5.1169
	step [3/254], loss=6.7596
	step [4/254], loss=5.0680
	step [5/254], loss=5.6551
	step [6/254], loss=5.8579
	step [7/254], loss=6.1054
	step [8/254], loss=5.0026
	step [9/254], loss=8.0503
	step [10/254], loss=5.0690
	step [11/254], loss=5.1987
	step [12/254], loss=6.2760
	step [13/254], loss=6.5864
	step [14/254], loss=5.5270
	step [15/254], loss=5.6386
	step [16/254], loss=5.6131
	step [17/254], loss=5.0744
	step [18/254], loss=6.0750
	step [19/254], loss=6.4987
	step [20/254], loss=6.6601
	step [21/254], loss=5.5664
	step [22/254], loss=7.0114
	step [23/254], loss=6.0706
	step [24/254], loss=5.6695
	step [25/254], loss=6.8352
	step [26/254], loss=8.4050
	step [27/254], loss=4.8803
	step [28/254], loss=5.4335
	step [29/254], loss=5.8996
	step [30/254], loss=7.1236
	step [31/254], loss=5.9404
	step [32/254], loss=5.3058
	step [33/254], loss=5.8192
	step [34/254], loss=6.9022
	step [35/254], loss=4.6399
	step [36/254], loss=5.7046
	step [37/254], loss=6.0490
	step [38/254], loss=5.2928
	step [39/254], loss=5.0767
	step [40/254], loss=5.1375
	step [41/254], loss=6.3044
	step [42/254], loss=6.2489
	step [43/254], loss=5.5591
	step [44/254], loss=5.1515
	step [45/254], loss=4.5726
	step [46/254], loss=6.0203
	step [47/254], loss=5.2857
	step [48/254], loss=5.1333
	step [49/254], loss=5.2315
	step [50/254], loss=4.2349
	step [51/254], loss=5.9300
	step [52/254], loss=5.9528
	step [53/254], loss=8.4889
	step [54/254], loss=5.0678
	step [55/254], loss=5.5348
	step [56/254], loss=6.2984
	step [57/254], loss=6.2267
	step [58/254], loss=5.6097
	step [59/254], loss=5.6513
	step [60/254], loss=5.2205
	step [61/254], loss=5.1364
	step [62/254], loss=5.5871
	step [63/254], loss=4.9977
	step [64/254], loss=4.7175
	step [65/254], loss=5.7881
	step [66/254], loss=4.7845
	step [67/254], loss=6.9152
	step [68/254], loss=5.1215
	step [69/254], loss=6.8303
	step [70/254], loss=6.2250
	step [71/254], loss=5.7107
	step [72/254], loss=5.4561
	step [73/254], loss=4.5905
	step [74/254], loss=6.2514
	step [75/254], loss=6.7390
	step [76/254], loss=6.9740
	step [77/254], loss=4.3237
	step [78/254], loss=5.7092
	step [79/254], loss=4.7040
	step [80/254], loss=4.9624
	step [81/254], loss=8.0999
	step [82/254], loss=4.7916
	step [83/254], loss=6.1186
	step [84/254], loss=5.7462
	step [85/254], loss=5.9982
	step [86/254], loss=7.7582
	step [87/254], loss=6.7775
	step [88/254], loss=4.8926
	step [89/254], loss=5.3884
	step [90/254], loss=5.3924
	step [91/254], loss=3.8409
	step [92/254], loss=6.7978
	step [93/254], loss=5.7333
	step [94/254], loss=5.8408
	step [95/254], loss=5.8274
	step [96/254], loss=6.5028
	step [97/254], loss=4.5726
	step [98/254], loss=6.5015
	step [99/254], loss=4.3508
	step [100/254], loss=7.6348
	step [101/254], loss=5.9867
	step [102/254], loss=5.9375
	step [103/254], loss=5.1074
	step [104/254], loss=7.3101
	step [105/254], loss=5.7200
	step [106/254], loss=5.2534
	step [107/254], loss=5.5358
	step [108/254], loss=6.5262
	step [109/254], loss=4.9573
	step [110/254], loss=5.8835
	step [111/254], loss=5.1425
	step [112/254], loss=4.7292
	step [113/254], loss=5.4945
	step [114/254], loss=5.7289
	step [115/254], loss=6.3229
	step [116/254], loss=5.3626
	step [117/254], loss=4.8443
	step [118/254], loss=7.7335
	step [119/254], loss=4.9358
	step [120/254], loss=3.0093
	step [121/254], loss=6.4931
	step [122/254], loss=5.7016
	step [123/254], loss=5.8975
	step [124/254], loss=6.1918
	step [125/254], loss=5.3101
	step [126/254], loss=5.0372
	step [127/254], loss=6.5810
	step [128/254], loss=6.0582
	step [129/254], loss=6.7368
	step [130/254], loss=4.3205
	step [131/254], loss=6.3244
	step [132/254], loss=5.8175
	step [133/254], loss=5.4774
	step [134/254], loss=5.8331
	step [135/254], loss=4.9394
	step [136/254], loss=6.5656
	step [137/254], loss=6.3610
	step [138/254], loss=5.8460
	step [139/254], loss=5.9613
	step [140/254], loss=6.4515
	step [141/254], loss=6.2383
	step [142/254], loss=4.2412
	step [143/254], loss=6.8184
	step [144/254], loss=6.1102
	step [145/254], loss=5.4535
	step [146/254], loss=6.9364
	step [147/254], loss=6.6863
	step [148/254], loss=6.1527
	step [149/254], loss=6.4817
	step [150/254], loss=5.9777
	step [151/254], loss=5.2168
	step [152/254], loss=6.3366
	step [153/254], loss=5.0201
	step [154/254], loss=6.5603
	step [155/254], loss=5.6947
	step [156/254], loss=5.2074
	step [157/254], loss=4.6502
	step [158/254], loss=5.4499
	step [159/254], loss=5.2216
	step [160/254], loss=5.7306
	step [161/254], loss=6.4519
	step [162/254], loss=7.6436
	step [163/254], loss=5.6322
	step [164/254], loss=5.9453
	step [165/254], loss=5.4095
	step [166/254], loss=5.4256
	step [167/254], loss=7.2144
	step [168/254], loss=7.0517
	step [169/254], loss=5.9960
	step [170/254], loss=7.3667
	step [171/254], loss=5.7648
	step [172/254], loss=6.3627
	step [173/254], loss=6.2108
	step [174/254], loss=6.6532
	step [175/254], loss=5.5228
	step [176/254], loss=4.5291
	step [177/254], loss=6.6743
	step [178/254], loss=4.9045
	step [179/254], loss=5.4106
	step [180/254], loss=7.1960
	step [181/254], loss=5.8020
	step [182/254], loss=6.2189
	step [183/254], loss=5.0163
	step [184/254], loss=3.8965
	step [185/254], loss=6.1631
	step [186/254], loss=5.9737
	step [187/254], loss=6.3318
	step [188/254], loss=6.6586
	step [189/254], loss=5.0524
	step [190/254], loss=9.1968
	step [191/254], loss=7.2917
	step [192/254], loss=5.6013
	step [193/254], loss=4.6104
	step [194/254], loss=6.2016
	step [195/254], loss=4.8921
	step [196/254], loss=8.8553
	step [197/254], loss=4.4553
	step [198/254], loss=6.7207
	step [199/254], loss=4.9969
	step [200/254], loss=5.6811
	step [201/254], loss=5.5966
	step [202/254], loss=7.7689
	step [203/254], loss=7.3718
	step [204/254], loss=5.7307
	step [205/254], loss=6.0860
	step [206/254], loss=6.4107
	step [207/254], loss=4.8637
	step [208/254], loss=5.9624
	step [209/254], loss=6.1263
	step [210/254], loss=6.8667
	step [211/254], loss=6.7849
	step [212/254], loss=5.3761
	step [213/254], loss=6.1434
	step [214/254], loss=4.4782
	step [215/254], loss=5.2371
	step [216/254], loss=6.2302
	step [217/254], loss=6.4394
	step [218/254], loss=4.3617
	step [219/254], loss=5.5066
	step [220/254], loss=6.1072
	step [221/254], loss=4.4360
	step [222/254], loss=6.1091
	step [223/254], loss=6.2817
	step [224/254], loss=5.4850
	step [225/254], loss=6.6249
	step [226/254], loss=5.9788
	step [227/254], loss=5.3763
	step [228/254], loss=6.1106
	step [229/254], loss=5.0903
	step [230/254], loss=7.4627
	step [231/254], loss=6.5169
	step [232/254], loss=6.5054
	step [233/254], loss=7.2578
	step [234/254], loss=5.3715
	step [235/254], loss=5.1136
	step [236/254], loss=5.1769
	step [237/254], loss=4.9921
	step [238/254], loss=6.4210
	step [239/254], loss=6.5273
	step [240/254], loss=6.1353
	step [241/254], loss=5.5842
	step [242/254], loss=4.8299
	step [243/254], loss=5.4057
	step [244/254], loss=5.3618
	step [245/254], loss=4.4351
	step [246/254], loss=6.6068
	step [247/254], loss=5.2816
	step [248/254], loss=6.9306
	step [249/254], loss=7.4693
	step [250/254], loss=5.8077
	step [251/254], loss=4.7294
	step [252/254], loss=6.1191
	step [253/254], loss=7.5300
	step [254/254], loss=0.6293
	Evaluating
	loss=0.0228, precision=0.2111, recall=0.9911, f1=0.3481
Training epoch 28
	step [1/254], loss=6.2558
	step [2/254], loss=5.7973
	step [3/254], loss=5.5273
	step [4/254], loss=6.3881
	step [5/254], loss=5.3965
	step [6/254], loss=5.1306
	step [7/254], loss=7.4914
	step [8/254], loss=5.8278
	step [9/254], loss=5.8352
	step [10/254], loss=4.9680
	step [11/254], loss=7.6771
	step [12/254], loss=5.7504
	step [13/254], loss=5.0981
	step [14/254], loss=5.3543
	step [15/254], loss=5.0911
	step [16/254], loss=6.2289
	step [17/254], loss=4.1802
	step [18/254], loss=5.6875
	step [19/254], loss=7.1267
	step [20/254], loss=5.3403
	step [21/254], loss=5.5563
	step [22/254], loss=3.6870
	step [23/254], loss=6.9811
	step [24/254], loss=7.1804
	step [25/254], loss=5.9957
	step [26/254], loss=5.3310
	step [27/254], loss=6.7613
	step [28/254], loss=6.7065
	step [29/254], loss=6.7079
	step [30/254], loss=6.8739
	step [31/254], loss=6.5015
	step [32/254], loss=5.4663
	step [33/254], loss=4.8655
	step [34/254], loss=4.7637
	step [35/254], loss=5.7081
	step [36/254], loss=5.7309
	step [37/254], loss=6.5036
	step [38/254], loss=6.6022
	step [39/254], loss=4.3005
	step [40/254], loss=5.0010
	step [41/254], loss=5.1823
	step [42/254], loss=5.1421
	step [43/254], loss=5.7507
	step [44/254], loss=7.1957
	step [45/254], loss=4.2472
	step [46/254], loss=5.8250
	step [47/254], loss=6.7453
	step [48/254], loss=6.1809
	step [49/254], loss=5.3868
	step [50/254], loss=5.4579
	step [51/254], loss=6.3653
	step [52/254], loss=5.4558
	step [53/254], loss=5.8408
	step [54/254], loss=5.5506
	step [55/254], loss=4.9457
	step [56/254], loss=4.7568
	step [57/254], loss=4.9972
	step [58/254], loss=7.0730
	step [59/254], loss=5.8363
	step [60/254], loss=5.8362
	step [61/254], loss=5.0238
	step [62/254], loss=6.0216
	step [63/254], loss=4.8050
	step [64/254], loss=6.4472
	step [65/254], loss=6.7019
	step [66/254], loss=6.2312
	step [67/254], loss=6.0330
	step [68/254], loss=3.8710
	step [69/254], loss=7.0872
	step [70/254], loss=5.3453
	step [71/254], loss=5.8445
	step [72/254], loss=4.8864
	step [73/254], loss=5.7748
	step [74/254], loss=7.0164
	step [75/254], loss=5.3662
	step [76/254], loss=6.9074
	step [77/254], loss=5.8111
	step [78/254], loss=5.4718
	step [79/254], loss=6.3971
	step [80/254], loss=6.5447
	step [81/254], loss=5.4104
	step [82/254], loss=5.0182
	step [83/254], loss=4.5035
	step [84/254], loss=7.1542
	step [85/254], loss=5.1115
	step [86/254], loss=5.3350
	step [87/254], loss=5.9454
	step [88/254], loss=5.6729
	step [89/254], loss=5.6443
	step [90/254], loss=6.7510
	step [91/254], loss=5.0468
	step [92/254], loss=5.3472
	step [93/254], loss=5.8366
	step [94/254], loss=4.6920
	step [95/254], loss=6.6111
	step [96/254], loss=4.8587
	step [97/254], loss=4.8453
	step [98/254], loss=6.1821
	step [99/254], loss=5.2212
	step [100/254], loss=5.7821
	step [101/254], loss=5.3112
	step [102/254], loss=5.4420
	step [103/254], loss=6.2219
	step [104/254], loss=5.1840
	step [105/254], loss=5.9418
	step [106/254], loss=5.9883
	step [107/254], loss=5.8476
	step [108/254], loss=5.3129
	step [109/254], loss=5.4226
	step [110/254], loss=6.4129
	step [111/254], loss=6.2325
	step [112/254], loss=5.1399
	step [113/254], loss=6.1208
	step [114/254], loss=3.6725
	step [115/254], loss=5.9308
	step [116/254], loss=5.9815
	step [117/254], loss=6.8535
	step [118/254], loss=4.3745
	step [119/254], loss=5.1165
	step [120/254], loss=6.8301
	step [121/254], loss=5.3292
	step [122/254], loss=7.2703
	step [123/254], loss=5.0486
	step [124/254], loss=6.1332
	step [125/254], loss=5.6000
	step [126/254], loss=5.7261
	step [127/254], loss=4.6272
	step [128/254], loss=6.9928
	step [129/254], loss=6.2633
	step [130/254], loss=5.0350
	step [131/254], loss=5.6336
	step [132/254], loss=4.4981
	step [133/254], loss=5.9362
	step [134/254], loss=6.2427
	step [135/254], loss=5.9151
	step [136/254], loss=5.2744
	step [137/254], loss=7.0718
	step [138/254], loss=5.7918
	step [139/254], loss=6.1171
	step [140/254], loss=4.8630
	step [141/254], loss=6.0418
	step [142/254], loss=6.2384
	step [143/254], loss=4.3897
	step [144/254], loss=5.4526
	step [145/254], loss=4.9003
	step [146/254], loss=5.7714
	step [147/254], loss=4.9121
	step [148/254], loss=5.4324
	step [149/254], loss=6.7827
	step [150/254], loss=5.6614
	step [151/254], loss=5.2081
	step [152/254], loss=6.7339
	step [153/254], loss=5.3342
	step [154/254], loss=4.9715
	step [155/254], loss=6.8984
	step [156/254], loss=4.8048
	step [157/254], loss=5.7668
	step [158/254], loss=5.4549
	step [159/254], loss=7.9234
	step [160/254], loss=7.1815
	step [161/254], loss=6.8697
	step [162/254], loss=6.7827
	step [163/254], loss=5.2238
	step [164/254], loss=5.5487
	step [165/254], loss=5.8445
	step [166/254], loss=4.8077
	step [167/254], loss=7.9855
	step [168/254], loss=5.7863
	step [169/254], loss=6.0898
	step [170/254], loss=7.3116
	step [171/254], loss=5.6592
	step [172/254], loss=5.0897
	step [173/254], loss=5.1740
	step [174/254], loss=6.1171
	step [175/254], loss=4.6712
	step [176/254], loss=5.7539
	step [177/254], loss=5.5738
	step [178/254], loss=5.6862
	step [179/254], loss=5.8037
	step [180/254], loss=5.7913
	step [181/254], loss=6.2973
	step [182/254], loss=5.8839
	step [183/254], loss=5.6651
	step [184/254], loss=4.5944
	step [185/254], loss=5.9979
	step [186/254], loss=4.9016
	step [187/254], loss=5.5196
	step [188/254], loss=5.2920
	step [189/254], loss=5.6791
	step [190/254], loss=6.8558
	step [191/254], loss=6.4926
	step [192/254], loss=6.8365
	step [193/254], loss=6.1687
	step [194/254], loss=5.0491
	step [195/254], loss=7.0497
	step [196/254], loss=6.0043
	step [197/254], loss=5.5324
	step [198/254], loss=6.7324
	step [199/254], loss=7.2473
	step [200/254], loss=6.3394
	step [201/254], loss=6.6491
	step [202/254], loss=4.6486
	step [203/254], loss=6.4590
	step [204/254], loss=5.0161
	step [205/254], loss=6.8282
	step [206/254], loss=5.3973
	step [207/254], loss=6.1148
	step [208/254], loss=6.2294
	step [209/254], loss=6.8619
	step [210/254], loss=6.6488
	step [211/254], loss=5.0183
	step [212/254], loss=8.0775
	step [213/254], loss=5.4308
	step [214/254], loss=6.5224
	step [215/254], loss=6.6085
	step [216/254], loss=5.2272
	step [217/254], loss=5.5078
	step [218/254], loss=4.6035
	step [219/254], loss=5.1314
	step [220/254], loss=5.5386
	step [221/254], loss=7.8199
	step [222/254], loss=5.0140
	step [223/254], loss=6.8872
	step [224/254], loss=6.0319
	step [225/254], loss=5.6753
	step [226/254], loss=5.4523
	step [227/254], loss=4.3184
	step [228/254], loss=7.0926
	step [229/254], loss=5.5756
	step [230/254], loss=5.9581
	step [231/254], loss=6.0995
	step [232/254], loss=6.3205
	step [233/254], loss=5.4500
	step [234/254], loss=6.4967
	step [235/254], loss=6.2265
	step [236/254], loss=5.4016
	step [237/254], loss=4.9824
	step [238/254], loss=5.3038
	step [239/254], loss=6.2060
	step [240/254], loss=5.9982
	step [241/254], loss=5.3630
	step [242/254], loss=4.8381
	step [243/254], loss=7.1986
	step [244/254], loss=4.9134
	step [245/254], loss=6.4401
	step [246/254], loss=7.1801
	step [247/254], loss=6.5094
	step [248/254], loss=5.8938
	step [249/254], loss=5.0500
	step [250/254], loss=5.0497
	step [251/254], loss=5.4625
	step [252/254], loss=6.1088
	step [253/254], loss=5.3910
	step [254/254], loss=1.2000
	Evaluating
	loss=0.0159, precision=0.2925, recall=0.9878, f1=0.4514
saving model as: 1_saved_model.pth
Training epoch 29
	step [1/254], loss=5.6617
	step [2/254], loss=6.7361
	step [3/254], loss=6.3203
	step [4/254], loss=6.4960
	step [5/254], loss=5.8288
	step [6/254], loss=5.6380
	step [7/254], loss=4.5389
	step [8/254], loss=5.9000
	step [9/254], loss=4.2058
	step [10/254], loss=6.9558
	step [11/254], loss=5.5432
	step [12/254], loss=6.0556
	step [13/254], loss=7.0282
	step [14/254], loss=5.5495
	step [15/254], loss=4.9184
	step [16/254], loss=4.8538
	step [17/254], loss=6.0917
	step [18/254], loss=4.3376
	step [19/254], loss=5.9240
	step [20/254], loss=5.2856
	step [21/254], loss=5.7438
	step [22/254], loss=5.4638
	step [23/254], loss=5.8255
	step [24/254], loss=3.9459
	step [25/254], loss=5.0812
	step [26/254], loss=5.5292
	step [27/254], loss=6.0993
	step [28/254], loss=6.1776
	step [29/254], loss=4.4357
	step [30/254], loss=4.3510
	step [31/254], loss=6.5937
	step [32/254], loss=8.7436
	step [33/254], loss=7.0909
	step [34/254], loss=6.4527
	step [35/254], loss=5.5069
	step [36/254], loss=7.4025
	step [37/254], loss=6.8364
	step [38/254], loss=5.4266
	step [39/254], loss=5.0897
	step [40/254], loss=6.6529
	step [41/254], loss=5.5541
	step [42/254], loss=5.7751
	step [43/254], loss=5.8284
	step [44/254], loss=6.3784
	step [45/254], loss=7.0379
	step [46/254], loss=5.0269
	step [47/254], loss=5.5211
	step [48/254], loss=6.3385
	step [49/254], loss=6.1094
	step [50/254], loss=5.6691
	step [51/254], loss=6.1373
	step [52/254], loss=5.2015
	step [53/254], loss=5.5454
	step [54/254], loss=5.7556
	step [55/254], loss=3.8089
	step [56/254], loss=7.3627
	step [57/254], loss=7.0293
	step [58/254], loss=8.2897
	step [59/254], loss=5.3414
	step [60/254], loss=4.6565
	step [61/254], loss=5.3401
	step [62/254], loss=6.4353
	step [63/254], loss=7.0387
	step [64/254], loss=5.4858
	step [65/254], loss=5.7402
	step [66/254], loss=5.1185
	step [67/254], loss=7.2375
	step [68/254], loss=6.2371
	step [69/254], loss=4.3938
	step [70/254], loss=5.4635
	step [71/254], loss=5.6884
	step [72/254], loss=6.4480
	step [73/254], loss=5.8537
	step [74/254], loss=4.0818
	step [75/254], loss=7.5448
	step [76/254], loss=5.6768
	step [77/254], loss=6.4949
	step [78/254], loss=6.6582
	step [79/254], loss=5.9027
	step [80/254], loss=5.5123
	step [81/254], loss=5.6702
	step [82/254], loss=5.7707
	step [83/254], loss=5.8437
	step [84/254], loss=4.9104
	step [85/254], loss=5.7026
	step [86/254], loss=4.8540
	step [87/254], loss=5.2254
	step [88/254], loss=5.3214
	step [89/254], loss=6.8437
	step [90/254], loss=4.0001
	step [91/254], loss=5.5528
	step [92/254], loss=5.7447
	step [93/254], loss=6.0727
	step [94/254], loss=5.3424
	step [95/254], loss=5.4048
	step [96/254], loss=7.7065
	step [97/254], loss=4.6292
	step [98/254], loss=7.6934
	step [99/254], loss=4.5907
	step [100/254], loss=5.8561
	step [101/254], loss=5.2030
	step [102/254], loss=7.4914
	step [103/254], loss=4.8161
	step [104/254], loss=4.8072
	step [105/254], loss=6.3804
	step [106/254], loss=5.7777
	step [107/254], loss=5.7294
	step [108/254], loss=5.8093
	step [109/254], loss=6.9386
	step [110/254], loss=7.1004
	step [111/254], loss=6.3509
	step [112/254], loss=6.4280
	step [113/254], loss=6.0475
	step [114/254], loss=5.4159
	step [115/254], loss=5.1572
	step [116/254], loss=5.8734
	step [117/254], loss=5.7475
	step [118/254], loss=5.2104
	step [119/254], loss=5.4967
	step [120/254], loss=6.3634
	step [121/254], loss=6.5653
	step [122/254], loss=5.0510
	step [123/254], loss=5.7935
	step [124/254], loss=5.8629
	step [125/254], loss=6.3553
	step [126/254], loss=4.8920
	step [127/254], loss=5.3040
	step [128/254], loss=6.0726
	step [129/254], loss=7.1342
	step [130/254], loss=5.8863
	step [131/254], loss=5.3043
	step [132/254], loss=5.7838
	step [133/254], loss=5.0887
	step [134/254], loss=6.4641
	step [135/254], loss=6.6560
	step [136/254], loss=5.0958
	step [137/254], loss=4.9263
	step [138/254], loss=5.3769
	step [139/254], loss=3.7863
	step [140/254], loss=6.2837
	step [141/254], loss=5.1443
	step [142/254], loss=5.5570
	step [143/254], loss=5.9668
	step [144/254], loss=6.6856
	step [145/254], loss=4.9556
	step [146/254], loss=6.1784
	step [147/254], loss=6.7816
	step [148/254], loss=5.5065
	step [149/254], loss=5.4594
	step [150/254], loss=5.4529
	step [151/254], loss=4.8657
	step [152/254], loss=5.2451
	step [153/254], loss=6.6911
	step [154/254], loss=4.8974
	step [155/254], loss=6.1902
	step [156/254], loss=7.4604
	step [157/254], loss=5.0122
	step [158/254], loss=6.5802
	step [159/254], loss=3.9475
	step [160/254], loss=6.3981
	step [161/254], loss=6.0533
	step [162/254], loss=6.4784
	step [163/254], loss=5.5195
	step [164/254], loss=5.7890
	step [165/254], loss=5.7045
	step [166/254], loss=4.9581
	step [167/254], loss=6.4983
	step [168/254], loss=4.8800
	step [169/254], loss=4.6917
	step [170/254], loss=5.3403
	step [171/254], loss=5.1923
	step [172/254], loss=5.5083
	step [173/254], loss=6.7332
	step [174/254], loss=7.8744
	step [175/254], loss=6.2838
	step [176/254], loss=5.4454
	step [177/254], loss=5.6179
	step [178/254], loss=5.5265
	step [179/254], loss=5.3012
	step [180/254], loss=5.2960
	step [181/254], loss=4.5989
	step [182/254], loss=5.5585
	step [183/254], loss=5.0521
	step [184/254], loss=5.9816
	step [185/254], loss=6.5689
	step [186/254], loss=5.2191
	step [187/254], loss=7.1702
	step [188/254], loss=4.9774
	step [189/254], loss=5.0367
	step [190/254], loss=5.9659
	step [191/254], loss=6.5851
	step [192/254], loss=5.4895
	step [193/254], loss=5.3288
	step [194/254], loss=5.4910
	step [195/254], loss=5.1240
	step [196/254], loss=5.1073
	step [197/254], loss=6.4157
	step [198/254], loss=8.2332
	step [199/254], loss=5.8574
	step [200/254], loss=5.6724
	step [201/254], loss=6.0330
	step [202/254], loss=4.3820
	step [203/254], loss=5.9132
	step [204/254], loss=4.5611
	step [205/254], loss=6.2042
	step [206/254], loss=6.3825
	step [207/254], loss=4.9811
	step [208/254], loss=5.7551
	step [209/254], loss=4.7742
	step [210/254], loss=5.4319
	step [211/254], loss=5.0263
	step [212/254], loss=5.7981
	step [213/254], loss=6.1680
	step [214/254], loss=5.5520
	step [215/254], loss=4.7496
	step [216/254], loss=6.2161
	step [217/254], loss=4.7089
	step [218/254], loss=5.6330
	step [219/254], loss=5.1176
	step [220/254], loss=5.3129
	step [221/254], loss=6.0666
	step [222/254], loss=4.6371
	step [223/254], loss=5.6535
	step [224/254], loss=4.9107
	step [225/254], loss=6.0917
	step [226/254], loss=5.4521
	step [227/254], loss=4.0641
	step [228/254], loss=5.2928
	step [229/254], loss=4.9733
	step [230/254], loss=5.4018
	step [231/254], loss=4.4086
	step [232/254], loss=7.0080
	step [233/254], loss=5.5187
	step [234/254], loss=5.4026
	step [235/254], loss=6.5096
	step [236/254], loss=4.7166
	step [237/254], loss=5.6586
	step [238/254], loss=4.9320
	step [239/254], loss=6.3543
	step [240/254], loss=8.1358
	step [241/254], loss=6.0173
	step [242/254], loss=6.1861
	step [243/254], loss=6.4524
	step [244/254], loss=5.2387
	step [245/254], loss=5.0781
	step [246/254], loss=4.6007
	step [247/254], loss=6.3247
	step [248/254], loss=6.1541
	step [249/254], loss=4.7209
	step [250/254], loss=4.2922
	step [251/254], loss=6.2796
	step [252/254], loss=7.5465
	step [253/254], loss=5.3782
	step [254/254], loss=1.0581
	Evaluating
	loss=0.0202, precision=0.2494, recall=0.9904, f1=0.3984
Training epoch 30
	step [1/254], loss=7.1479
	step [2/254], loss=7.4031
	step [3/254], loss=5.6824
	step [4/254], loss=5.6728
	step [5/254], loss=5.8492
	step [6/254], loss=5.8748
	step [7/254], loss=5.2464
	step [8/254], loss=4.3773
	step [9/254], loss=5.0539
	step [10/254], loss=5.7284
	step [11/254], loss=6.4251
	step [12/254], loss=4.7118
	step [13/254], loss=6.6264
	step [14/254], loss=6.4913
	step [15/254], loss=5.6375
	step [16/254], loss=5.9918
	step [17/254], loss=6.7205
	step [18/254], loss=5.3244
	step [19/254], loss=6.5185
	step [20/254], loss=5.6908
	step [21/254], loss=6.5924
	step [22/254], loss=4.8310
	step [23/254], loss=5.2796
	step [24/254], loss=6.6433
	step [25/254], loss=6.4965
	step [26/254], loss=7.6569
	step [27/254], loss=6.3859
	step [28/254], loss=5.2691
	step [29/254], loss=5.9363
	step [30/254], loss=4.8031
	step [31/254], loss=4.7675
	step [32/254], loss=5.7629
	step [33/254], loss=5.6662
	step [34/254], loss=5.7794
	step [35/254], loss=6.8484
	step [36/254], loss=6.1858
	step [37/254], loss=4.6318
	step [38/254], loss=4.8022
	step [39/254], loss=5.6123
	step [40/254], loss=5.3622
	step [41/254], loss=5.4874
	step [42/254], loss=5.3314
	step [43/254], loss=5.9207
	step [44/254], loss=5.2999
	step [45/254], loss=6.0115
	step [46/254], loss=5.9842
	step [47/254], loss=6.2974
	step [48/254], loss=6.3295
	step [49/254], loss=5.3359
	step [50/254], loss=5.4378
	step [51/254], loss=5.1932
	step [52/254], loss=4.4484
	step [53/254], loss=5.4048
	step [54/254], loss=5.2121
	step [55/254], loss=5.4911
	step [56/254], loss=6.0531
	step [57/254], loss=6.1818
	step [58/254], loss=4.7489
	step [59/254], loss=5.8284
	step [60/254], loss=4.4633
	step [61/254], loss=5.6594
	step [62/254], loss=5.2176
	step [63/254], loss=5.6853
	step [64/254], loss=4.9356
	step [65/254], loss=6.8761
	step [66/254], loss=5.6388
	step [67/254], loss=5.2205
	step [68/254], loss=5.3725
	step [69/254], loss=6.0463
	step [70/254], loss=5.3713
	step [71/254], loss=5.9806
	step [72/254], loss=4.7273
	step [73/254], loss=5.8840
	step [74/254], loss=5.0037
	step [75/254], loss=5.2843
	step [76/254], loss=5.8942
	step [77/254], loss=4.9114
	step [78/254], loss=5.9411
	step [79/254], loss=4.9081
	step [80/254], loss=5.7965
	step [81/254], loss=6.6488
	step [82/254], loss=6.2394
	step [83/254], loss=5.7288
	step [84/254], loss=5.1899
	step [85/254], loss=5.8034
	step [86/254], loss=6.0487
	step [87/254], loss=5.8015
	step [88/254], loss=4.5830
	step [89/254], loss=5.8753
	step [90/254], loss=5.8548
	step [91/254], loss=4.9918
	step [92/254], loss=6.1383
	step [93/254], loss=6.6265
	step [94/254], loss=5.4782
	step [95/254], loss=5.3374
	step [96/254], loss=6.7527
	step [97/254], loss=6.3424
	step [98/254], loss=5.9641
	step [99/254], loss=6.0398
	step [100/254], loss=5.3197
	step [101/254], loss=4.6520
	step [102/254], loss=4.8375
	step [103/254], loss=5.3622
	step [104/254], loss=6.4273
	step [105/254], loss=5.9991
	step [106/254], loss=6.3516
	step [107/254], loss=4.6328
	step [108/254], loss=6.8793
	step [109/254], loss=5.7997
	step [110/254], loss=7.3835
	step [111/254], loss=6.3931
	step [112/254], loss=6.6860
	step [113/254], loss=4.8413
	step [114/254], loss=6.4555
	step [115/254], loss=7.1818
	step [116/254], loss=5.0389
	step [117/254], loss=5.6214
	step [118/254], loss=5.6702
	step [119/254], loss=4.9055
	step [120/254], loss=5.0832
	step [121/254], loss=5.5735
	step [122/254], loss=5.3828
	step [123/254], loss=5.2480
	step [124/254], loss=5.7149
	step [125/254], loss=5.5993
	step [126/254], loss=7.0424
	step [127/254], loss=4.3857
	step [128/254], loss=5.4159
	step [129/254], loss=5.5810
	step [130/254], loss=4.8171
	step [131/254], loss=6.4647
	step [132/254], loss=5.4758
	step [133/254], loss=5.4349
	step [134/254], loss=5.2621
	step [135/254], loss=4.2739
	step [136/254], loss=5.3359
	step [137/254], loss=5.7341
	step [138/254], loss=5.6844
	step [139/254], loss=4.3938
	step [140/254], loss=5.7796
	step [141/254], loss=5.5643
	step [142/254], loss=6.6683
	step [143/254], loss=6.3267
	step [144/254], loss=5.6619
	step [145/254], loss=4.5621
	step [146/254], loss=5.5391
	step [147/254], loss=4.9600
	step [148/254], loss=5.0197
	step [149/254], loss=5.0863
	step [150/254], loss=5.2580
	step [151/254], loss=5.7109
	step [152/254], loss=5.3066
	step [153/254], loss=6.4019
	step [154/254], loss=5.7267
	step [155/254], loss=4.2426
	step [156/254], loss=5.6739
	step [157/254], loss=5.7614
	step [158/254], loss=5.0845
	step [159/254], loss=5.2746
	step [160/254], loss=4.9606
	step [161/254], loss=5.2807
	step [162/254], loss=5.1993
	step [163/254], loss=5.4390
	step [164/254], loss=4.2768
	step [165/254], loss=6.4278
	step [166/254], loss=5.5072
	step [167/254], loss=5.1824
	step [168/254], loss=5.1989
	step [169/254], loss=4.4346
	step [170/254], loss=6.4388
	step [171/254], loss=5.4723
	step [172/254], loss=4.4171
	step [173/254], loss=4.1943
	step [174/254], loss=5.2672
	step [175/254], loss=5.3837
	step [176/254], loss=6.1690
	step [177/254], loss=6.3988
	step [178/254], loss=6.0932
	step [179/254], loss=5.5513
	step [180/254], loss=4.7863
	step [181/254], loss=5.5642
	step [182/254], loss=4.4631
	step [183/254], loss=5.2853
	step [184/254], loss=5.0551
	step [185/254], loss=7.1768
	step [186/254], loss=4.3799
	step [187/254], loss=4.9125
	step [188/254], loss=5.5690
	step [189/254], loss=5.9027
	step [190/254], loss=6.0609
	step [191/254], loss=4.5326
	step [192/254], loss=5.0539
	step [193/254], loss=4.7940
	step [194/254], loss=5.6291
	step [195/254], loss=6.1521
	step [196/254], loss=6.2393
	step [197/254], loss=4.7302
	step [198/254], loss=5.8115
	step [199/254], loss=7.5931
	step [200/254], loss=5.6707
	step [201/254], loss=8.0891
	step [202/254], loss=4.5936
	step [203/254], loss=5.4256
	step [204/254], loss=5.3434
	step [205/254], loss=5.1522
	step [206/254], loss=5.2384
	step [207/254], loss=5.0019
	step [208/254], loss=6.4480
	step [209/254], loss=4.4135
	step [210/254], loss=5.5142
	step [211/254], loss=6.3139
	step [212/254], loss=5.9023
	step [213/254], loss=4.8536
	step [214/254], loss=4.8800
	step [215/254], loss=7.0058
	step [216/254], loss=5.1011
	step [217/254], loss=4.8805
	step [218/254], loss=6.1805
	step [219/254], loss=5.4655
	step [220/254], loss=7.0585
	step [221/254], loss=8.0888
	step [222/254], loss=5.0212
	step [223/254], loss=5.8338
	step [224/254], loss=4.8046
	step [225/254], loss=6.4246
	step [226/254], loss=4.7315
	step [227/254], loss=6.2239
	step [228/254], loss=5.3120
	step [229/254], loss=5.4998
	step [230/254], loss=5.5210
	step [231/254], loss=5.6990
	step [232/254], loss=6.2633
	step [233/254], loss=5.1960
	step [234/254], loss=5.3249
	step [235/254], loss=4.8465
	step [236/254], loss=7.4666
	step [237/254], loss=5.2945
	step [238/254], loss=4.3654
	step [239/254], loss=5.5331
	step [240/254], loss=4.6552
	step [241/254], loss=5.9335
	step [242/254], loss=5.6598
	step [243/254], loss=5.4417
	step [244/254], loss=5.2666
	step [245/254], loss=5.6650
	step [246/254], loss=4.7158
	step [247/254], loss=5.6291
	step [248/254], loss=6.6220
	step [249/254], loss=5.1154
	step [250/254], loss=4.3466
	step [251/254], loss=5.2372
	step [252/254], loss=5.5664
	step [253/254], loss=5.2874
	step [254/254], loss=1.9697
	Evaluating
	loss=0.0146, precision=0.2967, recall=0.9871, f1=0.4563
saving model as: 1_saved_model.pth
Training finished
best_f1: 0.4562794262021792
directing: X rim_enhanced: False test_id 2
removed wrong scan: weights_X_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_218_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_46_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_49_xwqg-A00121_2019-05-15.npy
# all image files: 12135 # all weight files in weight_dir: 9348 # image files with weight 9316
removed wrong scan: weights_X_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_218_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_46_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_49_xwqg-A00121_2019-05-15.npy
# all image files: 12135 # all weight files in weight_dir: 2534 # image files with weight 2516
train: /ibex/scratch/projects/c2052/air_tube_seg/training_samples_enhanced/X 9316
Using 4 GPUs
Going to train epochs [1-30]
Training epoch 1
	step [1/195], loss=175.1440
	step [2/195], loss=159.8600
	step [3/195], loss=155.1568
	step [4/195], loss=151.2596
	step [5/195], loss=149.1779
	step [6/195], loss=145.8494
	step [7/195], loss=142.9545
	step [8/195], loss=141.3857
	step [9/195], loss=140.6594
	step [10/195], loss=136.7869
	step [11/195], loss=136.9434
	step [12/195], loss=134.7274
	step [13/195], loss=134.0628
	step [14/195], loss=133.1998
	step [15/195], loss=130.5618
	step [16/195], loss=129.5228
	step [17/195], loss=128.6727
	step [18/195], loss=129.2363
	step [19/195], loss=126.0838
	step [20/195], loss=127.0844
	step [21/195], loss=124.5745
	step [22/195], loss=123.6822
	step [23/195], loss=121.3902
	step [24/195], loss=121.8567
	step [25/195], loss=121.6707
	step [26/195], loss=120.9918
	step [27/195], loss=119.1146
	step [28/195], loss=117.5021
	step [29/195], loss=117.7992
	step [30/195], loss=116.2008
	step [31/195], loss=115.3642
	step [32/195], loss=114.7719
	step [33/195], loss=113.5780
	step [34/195], loss=113.0464
	step [35/195], loss=113.0992
	step [36/195], loss=110.0787
	step [37/195], loss=111.8288
	step [38/195], loss=109.5733
	step [39/195], loss=110.6415
	step [40/195], loss=109.8095
	step [41/195], loss=110.0036
	step [42/195], loss=108.1910
	step [43/195], loss=107.0482
	step [44/195], loss=106.2059
	step [45/195], loss=106.2099
	step [46/195], loss=104.8609
	step [47/195], loss=106.4802
	step [48/195], loss=104.0941
	step [49/195], loss=105.4028
	step [50/195], loss=104.8577
	step [51/195], loss=103.8512
	step [52/195], loss=104.2934
	step [53/195], loss=100.9273
	step [54/195], loss=101.4912
	step [55/195], loss=102.3484
	step [56/195], loss=101.2677
	step [57/195], loss=100.4793
	step [58/195], loss=100.5635
	step [59/195], loss=101.8573
	step [60/195], loss=100.1293
	step [61/195], loss=100.2486
	step [62/195], loss=98.6432
	step [63/195], loss=99.0143
	step [64/195], loss=99.5983
	step [65/195], loss=98.7455
	step [66/195], loss=98.0328
	step [67/195], loss=98.0483
	step [68/195], loss=97.6467
	step [69/195], loss=96.5659
	step [70/195], loss=96.1522
	step [71/195], loss=96.8965
	step [72/195], loss=95.7098
	step [73/195], loss=95.3475
	step [74/195], loss=96.2758
	step [75/195], loss=94.7913
	step [76/195], loss=96.3115
	step [77/195], loss=95.0263
	step [78/195], loss=94.2057
	step [79/195], loss=94.4705
	step [80/195], loss=94.5786
	step [81/195], loss=94.7994
	step [82/195], loss=93.8399
	step [83/195], loss=94.2221
	step [84/195], loss=93.3731
	step [85/195], loss=93.5020
	step [86/195], loss=92.9801
	step [87/195], loss=92.6185
	step [88/195], loss=92.5169
	step [89/195], loss=91.2323
	step [90/195], loss=92.0055
	step [91/195], loss=91.8892
	step [92/195], loss=91.1704
	step [93/195], loss=90.2709
	step [94/195], loss=91.5039
	step [95/195], loss=90.7293
	step [96/195], loss=90.4329
	step [97/195], loss=92.1064
	step [98/195], loss=90.2242
	step [99/195], loss=90.3106
	step [100/195], loss=91.3877
	step [101/195], loss=90.0655
	step [102/195], loss=90.3862
	step [103/195], loss=91.3328
	step [104/195], loss=89.0259
	step [105/195], loss=88.1635
	step [106/195], loss=89.2041
	step [107/195], loss=88.1104
	step [108/195], loss=88.4686
	step [109/195], loss=88.7562
	step [110/195], loss=87.1183
	step [111/195], loss=88.6781
	step [112/195], loss=87.9546
	step [113/195], loss=87.3971
	step [114/195], loss=88.3184
	step [115/195], loss=88.3062
	step [116/195], loss=87.9151
	step [117/195], loss=86.8369
	step [118/195], loss=86.7287
	step [119/195], loss=86.5566
	step [120/195], loss=86.7176
	step [121/195], loss=87.1327
	step [122/195], loss=86.4416
	step [123/195], loss=86.5949
	step [124/195], loss=85.9546
	step [125/195], loss=85.9694
	step [126/195], loss=85.5000
	step [127/195], loss=85.1313
	step [128/195], loss=85.4077
	step [129/195], loss=85.2801
	step [130/195], loss=85.5514
	step [131/195], loss=83.5369
	step [132/195], loss=84.4638
	step [133/195], loss=84.2800
	step [134/195], loss=84.0107
	step [135/195], loss=84.5923
	step [136/195], loss=85.8539
	step [137/195], loss=84.8917
	step [138/195], loss=84.6153
	step [139/195], loss=83.3715
	step [140/195], loss=83.6310
	step [141/195], loss=83.2866
	step [142/195], loss=84.4565
	step [143/195], loss=83.2905
	step [144/195], loss=83.2693
	step [145/195], loss=84.2369
	step [146/195], loss=82.1319
	step [147/195], loss=82.8561
	step [148/195], loss=81.5114
	step [149/195], loss=84.2839
	step [150/195], loss=83.2242
	step [151/195], loss=81.8902
	step [152/195], loss=81.3820
	step [153/195], loss=82.3781
	step [154/195], loss=81.6351
	step [155/195], loss=80.5583
	step [156/195], loss=81.2480
	step [157/195], loss=80.9350
	step [158/195], loss=81.6031
	step [159/195], loss=80.1365
	step [160/195], loss=80.6776
	step [161/195], loss=81.0562
	step [162/195], loss=80.0493
	step [163/195], loss=79.8079
	step [164/195], loss=80.5580
	step [165/195], loss=79.7386
	step [166/195], loss=81.5490
	step [167/195], loss=81.3443
	step [168/195], loss=80.4836
	step [169/195], loss=80.0036
	step [170/195], loss=79.8438
	step [171/195], loss=79.4668
	step [172/195], loss=79.6107
	step [173/195], loss=79.0600
	step [174/195], loss=78.7787
	step [175/195], loss=79.5367
	step [176/195], loss=78.5041
	step [177/195], loss=77.5823
	step [178/195], loss=77.7460
	step [179/195], loss=79.5947
	step [180/195], loss=79.6370
	step [181/195], loss=78.0642
	step [182/195], loss=77.6094
	step [183/195], loss=77.9190
	step [184/195], loss=78.9286
	step [185/195], loss=77.1343
	step [186/195], loss=79.1266
	step [187/195], loss=78.0080
	step [188/195], loss=77.5947
	step [189/195], loss=77.1522
	step [190/195], loss=78.4316
	step [191/195], loss=77.6553
	step [192/195], loss=77.1561
	step [193/195], loss=77.4930
	step [194/195], loss=75.3567
	step [195/195], loss=7.8139
	Evaluating
	loss=0.3943, precision=0.1503, recall=0.9977, f1=0.2612
saving model as: 2_saved_model.pth
Training epoch 2
	step [1/195], loss=76.6229
	step [2/195], loss=76.0310
	step [3/195], loss=77.7862
	step [4/195], loss=76.6956
	step [5/195], loss=75.8167
	step [6/195], loss=75.8650
	step [7/195], loss=75.4388
	step [8/195], loss=75.6632
	step [9/195], loss=75.4568
	step [10/195], loss=76.6726
	step [11/195], loss=76.0027
	step [12/195], loss=76.0528
	step [13/195], loss=74.7330
	step [14/195], loss=74.2501
	step [15/195], loss=74.4056
	step [16/195], loss=74.6581
	step [17/195], loss=74.2764
	step [18/195], loss=73.9318
	step [19/195], loss=73.2862
	step [20/195], loss=74.1513
	step [21/195], loss=74.1596
	step [22/195], loss=74.3355
	step [23/195], loss=74.0148
	step [24/195], loss=74.5978
	step [25/195], loss=73.8346
	step [26/195], loss=74.5431
	step [27/195], loss=73.6301
	step [28/195], loss=72.2680
	step [29/195], loss=72.3748
	step [30/195], loss=72.7868
	step [31/195], loss=73.9591
	step [32/195], loss=73.3892
	step [33/195], loss=72.6247
	step [34/195], loss=72.6627
	step [35/195], loss=71.9706
	step [36/195], loss=72.4801
	step [37/195], loss=71.3087
	step [38/195], loss=72.0256
	step [39/195], loss=72.6207
	step [40/195], loss=73.0882
	step [41/195], loss=72.0559
	step [42/195], loss=70.4289
	step [43/195], loss=70.9512
	step [44/195], loss=71.2427
	step [45/195], loss=71.4648
	step [46/195], loss=72.2040
	step [47/195], loss=70.9901
	step [48/195], loss=71.9149
	step [49/195], loss=71.0990
	step [50/195], loss=70.7944
	step [51/195], loss=71.1060
	step [52/195], loss=70.3488
	step [53/195], loss=71.2567
	step [54/195], loss=72.8934
	step [55/195], loss=70.2141
	step [56/195], loss=70.2318
	step [57/195], loss=71.5823
	step [58/195], loss=70.7565
	step [59/195], loss=70.1472
	step [60/195], loss=71.2151
	step [61/195], loss=70.4825
	step [62/195], loss=71.0300
	step [63/195], loss=70.6964
	step [64/195], loss=69.4278
	step [65/195], loss=69.5792
	step [66/195], loss=68.7823
	step [67/195], loss=69.8071
	step [68/195], loss=69.4431
	step [69/195], loss=69.2107
	step [70/195], loss=69.2951
	step [71/195], loss=68.8295
	step [72/195], loss=68.9688
	step [73/195], loss=69.0727
	step [74/195], loss=71.1986
	step [75/195], loss=68.4839
	step [76/195], loss=69.3213
	step [77/195], loss=68.4325
	step [78/195], loss=68.1869
	step [79/195], loss=68.3828
	step [80/195], loss=68.3549
	step [81/195], loss=68.3824
	step [82/195], loss=67.6371
	step [83/195], loss=66.0429
	step [84/195], loss=67.6720
	step [85/195], loss=67.5912
	step [86/195], loss=68.2825
	step [87/195], loss=67.2072
	step [88/195], loss=66.4321
	step [89/195], loss=67.1322
	step [90/195], loss=67.6169
	step [91/195], loss=66.5615
	step [92/195], loss=67.0061
	step [93/195], loss=66.3440
	step [94/195], loss=66.5899
	step [95/195], loss=66.6308
	step [96/195], loss=66.8064
	step [97/195], loss=65.6004
	step [98/195], loss=66.4022
	step [99/195], loss=66.9063
	step [100/195], loss=65.6569
	step [101/195], loss=66.2552
	step [102/195], loss=65.0974
	step [103/195], loss=64.4618
	step [104/195], loss=64.0248
	step [105/195], loss=65.3994
	step [106/195], loss=65.8206
	step [107/195], loss=66.0169
	step [108/195], loss=65.4988
	step [109/195], loss=64.7207
	step [110/195], loss=63.8719
	step [111/195], loss=65.1504
	step [112/195], loss=64.5362
	step [113/195], loss=64.0397
	step [114/195], loss=65.1330
	step [115/195], loss=65.4974
	step [116/195], loss=63.3980
	step [117/195], loss=65.2103
	step [118/195], loss=64.1477
	step [119/195], loss=64.1599
	step [120/195], loss=64.5067
	step [121/195], loss=63.0797
	step [122/195], loss=65.3907
	step [123/195], loss=65.0297
	step [124/195], loss=64.4809
	step [125/195], loss=63.3156
	step [126/195], loss=63.2176
	step [127/195], loss=63.6021
	step [128/195], loss=62.9724
	step [129/195], loss=62.9716
	step [130/195], loss=62.2572
	step [131/195], loss=64.2464
	step [132/195], loss=62.7697
	step [133/195], loss=62.5882
	step [134/195], loss=61.6559
	step [135/195], loss=64.3480
	step [136/195], loss=61.4061
	step [137/195], loss=61.3781
	step [138/195], loss=62.9241
	step [139/195], loss=62.9306
	step [140/195], loss=62.5785
	step [141/195], loss=61.2000
	step [142/195], loss=62.0762
	step [143/195], loss=62.2745
	step [144/195], loss=61.6562
	step [145/195], loss=60.2577
	step [146/195], loss=61.0111
	step [147/195], loss=61.4164
	step [148/195], loss=59.7195
	step [149/195], loss=61.4744
	step [150/195], loss=61.9632
	step [151/195], loss=61.6883
	step [152/195], loss=61.4051
	step [153/195], loss=60.8856
	step [154/195], loss=61.1483
	step [155/195], loss=60.8250
	step [156/195], loss=59.8195
	step [157/195], loss=61.0130
	step [158/195], loss=61.6001
	step [159/195], loss=61.3387
	step [160/195], loss=60.3591
	step [161/195], loss=60.8100
	step [162/195], loss=61.8659
	step [163/195], loss=59.4870
	step [164/195], loss=59.6002
	step [165/195], loss=58.7763
	step [166/195], loss=61.3263
	step [167/195], loss=60.8949
	step [168/195], loss=60.2336
	step [169/195], loss=60.0638
	step [170/195], loss=60.7615
	step [171/195], loss=58.3783
	step [172/195], loss=59.7990
	step [173/195], loss=59.6455
	step [174/195], loss=59.6669
	step [175/195], loss=60.1005
	step [176/195], loss=59.1552
	step [177/195], loss=59.7735
	step [178/195], loss=60.3157
	step [179/195], loss=58.4481
	step [180/195], loss=60.0854
	step [181/195], loss=57.7888
	step [182/195], loss=59.6571
	step [183/195], loss=57.9230
	step [184/195], loss=58.1141
	step [185/195], loss=58.4274
	step [186/195], loss=57.8513
	step [187/195], loss=58.1546
	step [188/195], loss=57.1430
	step [189/195], loss=57.7899
	step [190/195], loss=58.9441
	step [191/195], loss=57.3071
	step [192/195], loss=57.3850
	step [193/195], loss=58.3529
	step [194/195], loss=58.0894
	step [195/195], loss=4.9906
	Evaluating
	loss=0.2878, precision=0.1739, recall=0.9975, f1=0.2961
saving model as: 2_saved_model.pth
Training epoch 3
	step [1/195], loss=57.4111
	step [2/195], loss=58.1010
	step [3/195], loss=58.0818
	step [4/195], loss=57.0657
	step [5/195], loss=56.3149
	step [6/195], loss=56.8124
	step [7/195], loss=56.9053
	step [8/195], loss=57.3441
	step [9/195], loss=56.8564
	step [10/195], loss=55.9900
	step [11/195], loss=57.0907
	step [12/195], loss=55.3038
	step [13/195], loss=57.4376
	step [14/195], loss=55.5247
	step [15/195], loss=55.4753
	step [16/195], loss=54.7513
	step [17/195], loss=56.7045
	step [18/195], loss=55.9009
	step [19/195], loss=55.3786
	step [20/195], loss=56.5448
	step [21/195], loss=55.2023
	step [22/195], loss=54.3234
	step [23/195], loss=56.0379
	step [24/195], loss=55.4121
	step [25/195], loss=54.3004
	step [26/195], loss=56.1022
	step [27/195], loss=56.6670
	step [28/195], loss=54.0454
	step [29/195], loss=55.2813
	step [30/195], loss=54.0143
	step [31/195], loss=54.1106
	step [32/195], loss=55.1316
	step [33/195], loss=55.2532
	step [34/195], loss=54.7558
	step [35/195], loss=57.3140
	step [36/195], loss=53.5269
	step [37/195], loss=54.9753
	step [38/195], loss=53.2411
	step [39/195], loss=54.3804
	step [40/195], loss=54.1075
	step [41/195], loss=53.9745
	step [42/195], loss=53.4548
	step [43/195], loss=53.8286
	step [44/195], loss=53.8377
	step [45/195], loss=53.5353
	step [46/195], loss=52.3455
	step [47/195], loss=53.7452
	step [48/195], loss=54.3446
	step [49/195], loss=54.4788
	step [50/195], loss=54.1331
	step [51/195], loss=53.9882
	step [52/195], loss=52.3437
	step [53/195], loss=52.5091
	step [54/195], loss=52.1365
	step [55/195], loss=52.7457
	step [56/195], loss=53.0528
	step [57/195], loss=52.7371
	step [58/195], loss=52.0015
	step [59/195], loss=52.2638
	step [60/195], loss=53.6687
	step [61/195], loss=52.2996
	step [62/195], loss=52.8374
	step [63/195], loss=52.9969
	step [64/195], loss=50.7345
	step [65/195], loss=50.8428
	step [66/195], loss=52.1606
	step [67/195], loss=51.4085
	step [68/195], loss=50.9445
	step [69/195], loss=52.0707
	step [70/195], loss=52.6709
	step [71/195], loss=50.9757
	step [72/195], loss=52.0876
	step [73/195], loss=51.4994
	step [74/195], loss=52.0166
	step [75/195], loss=50.6531
	step [76/195], loss=51.1618
	step [77/195], loss=50.8988
	step [78/195], loss=49.8584
	step [79/195], loss=50.5400
	step [80/195], loss=49.5931
	step [81/195], loss=50.9329
	step [82/195], loss=49.2673
	step [83/195], loss=50.5831
	step [84/195], loss=49.5650
	step [85/195], loss=50.5660
	step [86/195], loss=51.3185
	step [87/195], loss=50.2537
	step [88/195], loss=48.5717
	step [89/195], loss=50.0855
	step [90/195], loss=51.5311
	step [91/195], loss=52.2205
	step [92/195], loss=48.8512
	step [93/195], loss=50.5939
	step [94/195], loss=48.8643
	step [95/195], loss=48.5890
	step [96/195], loss=48.5137
	step [97/195], loss=47.4448
	step [98/195], loss=48.3951
	step [99/195], loss=50.7183
	step [100/195], loss=49.5803
	step [101/195], loss=49.0727
	step [102/195], loss=48.5209
	step [103/195], loss=48.5840
	step [104/195], loss=49.6741
	step [105/195], loss=47.2915
	step [106/195], loss=50.1656
	step [107/195], loss=49.5033
	step [108/195], loss=49.2540
	step [109/195], loss=48.3811
	step [110/195], loss=48.2880
	step [111/195], loss=49.8523
	step [112/195], loss=51.0645
	step [113/195], loss=49.6817
	step [114/195], loss=49.9870
	step [115/195], loss=48.4272
	step [116/195], loss=50.0472
	step [117/195], loss=47.9096
	step [118/195], loss=48.0621
	step [119/195], loss=49.0307
	step [120/195], loss=47.7454
	step [121/195], loss=49.5536
	step [122/195], loss=47.6328
	step [123/195], loss=47.4566
	step [124/195], loss=46.6994
	step [125/195], loss=47.5668
	step [126/195], loss=46.5185
	step [127/195], loss=46.4368
	step [128/195], loss=46.8320
	step [129/195], loss=47.7837
	step [130/195], loss=46.9418
	step [131/195], loss=47.9101
	step [132/195], loss=46.0433
	step [133/195], loss=46.1388
	step [134/195], loss=46.6276
	step [135/195], loss=46.4602
	step [136/195], loss=45.5391
	step [137/195], loss=48.1756
	step [138/195], loss=48.0026
	step [139/195], loss=46.5370
	step [140/195], loss=46.9108
	step [141/195], loss=47.0202
	step [142/195], loss=47.9817
	step [143/195], loss=45.6788
	step [144/195], loss=47.3433
	step [145/195], loss=47.1041
	step [146/195], loss=45.4862
	step [147/195], loss=46.5357
	step [148/195], loss=44.6717
	step [149/195], loss=46.0833
	step [150/195], loss=46.5604
	step [151/195], loss=46.7370
	step [152/195], loss=45.5285
	step [153/195], loss=44.6620
	step [154/195], loss=46.1810
	step [155/195], loss=47.0283
	step [156/195], loss=45.8857
	step [157/195], loss=45.9355
	step [158/195], loss=45.2509
	step [159/195], loss=44.8706
	step [160/195], loss=45.6546
	step [161/195], loss=45.4876
	step [162/195], loss=44.1614
	step [163/195], loss=44.3683
	step [164/195], loss=44.3085
	step [165/195], loss=44.6242
	step [166/195], loss=45.1823
	step [167/195], loss=44.3230
	step [168/195], loss=44.5607
	step [169/195], loss=44.7935
	step [170/195], loss=44.2491
	step [171/195], loss=46.5278
	step [172/195], loss=45.4212
	step [173/195], loss=44.9774
	step [174/195], loss=43.8766
	step [175/195], loss=44.2752
	step [176/195], loss=45.4773
	step [177/195], loss=45.5621
	step [178/195], loss=44.6207
	step [179/195], loss=45.3434
	step [180/195], loss=43.8114
	step [181/195], loss=44.7734
	step [182/195], loss=44.0137
	step [183/195], loss=43.5571
	step [184/195], loss=42.6503
	step [185/195], loss=44.5781
	step [186/195], loss=42.0314
	step [187/195], loss=43.1968
	step [188/195], loss=43.1810
	step [189/195], loss=42.6922
	step [190/195], loss=42.5828
	step [191/195], loss=42.7668
	step [192/195], loss=43.6076
	step [193/195], loss=42.6154
	step [194/195], loss=43.2926
	step [195/195], loss=4.0493
	Evaluating
	loss=0.2106, precision=0.1772, recall=0.9974, f1=0.3009
saving model as: 2_saved_model.pth
Training epoch 4
	step [1/195], loss=43.7238
	step [2/195], loss=42.5088
	step [3/195], loss=42.7086
	step [4/195], loss=43.0281
	step [5/195], loss=43.2128
	step [6/195], loss=42.9646
	step [7/195], loss=42.5242
	step [8/195], loss=43.7825
	step [9/195], loss=41.5775
	step [10/195], loss=43.1166
	step [11/195], loss=43.0500
	step [12/195], loss=44.2849
	step [13/195], loss=41.9769
	step [14/195], loss=42.9590
	step [15/195], loss=42.5104
	step [16/195], loss=42.2892
	step [17/195], loss=43.5927
	step [18/195], loss=41.8547
	step [19/195], loss=42.0599
	step [20/195], loss=42.1835
	step [21/195], loss=42.8899
	step [22/195], loss=41.6703
	step [23/195], loss=41.4114
	step [24/195], loss=40.8749
	step [25/195], loss=42.3931
	step [26/195], loss=41.6639
	step [27/195], loss=41.7151
	step [28/195], loss=40.2448
	step [29/195], loss=41.4520
	step [30/195], loss=41.3644
	step [31/195], loss=40.5221
	step [32/195], loss=40.6959
	step [33/195], loss=40.7624
	step [34/195], loss=40.6125
	step [35/195], loss=41.8280
	step [36/195], loss=40.6179
	step [37/195], loss=41.1857
	step [38/195], loss=41.7368
	step [39/195], loss=40.8174
	step [40/195], loss=41.2722
	step [41/195], loss=41.1390
	step [42/195], loss=40.4471
	step [43/195], loss=41.2288
	step [44/195], loss=40.6003
	step [45/195], loss=40.5771
	step [46/195], loss=41.5765
	step [47/195], loss=40.8488
	step [48/195], loss=41.9038
	step [49/195], loss=40.1148
	step [50/195], loss=39.6910
	step [51/195], loss=41.2894
	step [52/195], loss=42.3719
	step [53/195], loss=40.0236
	step [54/195], loss=39.1696
	step [55/195], loss=38.6782
	step [56/195], loss=40.4621
	step [57/195], loss=39.4903
	step [58/195], loss=40.2740
	step [59/195], loss=39.1533
	step [60/195], loss=39.2538
	step [61/195], loss=39.1992
	step [62/195], loss=41.5228
	step [63/195], loss=39.2222
	step [64/195], loss=39.7484
	step [65/195], loss=39.4537
	step [66/195], loss=38.1017
	step [67/195], loss=39.4919
	step [68/195], loss=39.7129
	step [69/195], loss=40.0382
	step [70/195], loss=39.6999
	step [71/195], loss=40.3069
	step [72/195], loss=40.2980
	step [73/195], loss=38.1278
	step [74/195], loss=38.1492
	step [75/195], loss=40.0386
	step [76/195], loss=38.9349
	step [77/195], loss=38.8537
	step [78/195], loss=39.0541
	step [79/195], loss=38.0579
	step [80/195], loss=38.8528
	step [81/195], loss=39.2034
	step [82/195], loss=38.2654
	step [83/195], loss=37.2176
	step [84/195], loss=38.9644
	step [85/195], loss=39.5563
	step [86/195], loss=38.1300
	step [87/195], loss=39.4386
	step [88/195], loss=38.2929
	step [89/195], loss=38.5256
	step [90/195], loss=38.8227
	step [91/195], loss=37.2865
	step [92/195], loss=38.6557
	step [93/195], loss=38.9086
	step [94/195], loss=38.1918
	step [95/195], loss=37.4319
	step [96/195], loss=38.1890
	step [97/195], loss=39.5830
	step [98/195], loss=36.4118
	step [99/195], loss=37.3495
	step [100/195], loss=38.2204
	step [101/195], loss=36.8750
	step [102/195], loss=37.0764
	step [103/195], loss=37.1445
	step [104/195], loss=37.3443
	step [105/195], loss=38.2844
	step [106/195], loss=37.1140
	step [107/195], loss=37.1141
	step [108/195], loss=35.9411
	step [109/195], loss=37.0197
	step [110/195], loss=36.7658
	step [111/195], loss=36.0279
	step [112/195], loss=37.6452
	step [113/195], loss=36.6321
	step [114/195], loss=36.1182
	step [115/195], loss=37.8838
	step [116/195], loss=37.2109
	step [117/195], loss=36.2346
	step [118/195], loss=36.3183
	step [119/195], loss=36.6508
	step [120/195], loss=38.5822
	step [121/195], loss=36.0699
	step [122/195], loss=37.8648
	step [123/195], loss=36.5719
	step [124/195], loss=36.9121
	step [125/195], loss=36.6527
	step [126/195], loss=36.5169
	step [127/195], loss=37.0067
	step [128/195], loss=36.0961
	step [129/195], loss=35.7297
	step [130/195], loss=36.9033
	step [131/195], loss=36.5940
	step [132/195], loss=36.0562
	step [133/195], loss=36.7381
	step [134/195], loss=37.6725
	step [135/195], loss=35.0964
	step [136/195], loss=35.9672
	step [137/195], loss=35.1478
	step [138/195], loss=37.0508
	step [139/195], loss=37.1203
	step [140/195], loss=37.1418
	step [141/195], loss=36.1743
	step [142/195], loss=35.8448
	step [143/195], loss=36.1490
	step [144/195], loss=36.8995
	step [145/195], loss=38.7001
	step [146/195], loss=35.9383
	step [147/195], loss=36.8081
	step [148/195], loss=35.0468
	step [149/195], loss=37.1087
	step [150/195], loss=36.6343
	step [151/195], loss=34.2501
	step [152/195], loss=34.3638
	step [153/195], loss=35.8031
	step [154/195], loss=36.9064
	step [155/195], loss=36.3608
	step [156/195], loss=35.1508
	step [157/195], loss=36.0351
	step [158/195], loss=36.1920
	step [159/195], loss=35.3528
	step [160/195], loss=34.9692
	step [161/195], loss=36.4531
	step [162/195], loss=34.9262
	step [163/195], loss=34.8171
	step [164/195], loss=35.1483
	step [165/195], loss=34.2784
	step [166/195], loss=35.7513
	step [167/195], loss=34.6296
	step [168/195], loss=36.4258
	step [169/195], loss=36.2415
	step [170/195], loss=34.8465
	step [171/195], loss=35.9645
	step [172/195], loss=34.8382
	step [173/195], loss=34.9997
	step [174/195], loss=33.7933
	step [175/195], loss=34.4327
	step [176/195], loss=33.4088
	step [177/195], loss=34.5219
	step [178/195], loss=33.6999
	step [179/195], loss=34.4053
	step [180/195], loss=36.9262
	step [181/195], loss=34.4415
	step [182/195], loss=34.2654
	step [183/195], loss=33.8780
	step [184/195], loss=35.1866
	step [185/195], loss=33.5363
	step [186/195], loss=33.4440
	step [187/195], loss=34.4956
	step [188/195], loss=33.9586
	step [189/195], loss=34.0178
	step [190/195], loss=33.9018
	step [191/195], loss=34.5001
	step [192/195], loss=33.6749
	step [193/195], loss=33.3362
	step [194/195], loss=33.7557
	step [195/195], loss=2.9432
	Evaluating
	loss=0.1687, precision=0.1739, recall=0.9975, f1=0.2962
Training epoch 5
	step [1/195], loss=32.8288
	step [2/195], loss=34.0846
	step [3/195], loss=32.4222
	step [4/195], loss=33.7193
	step [5/195], loss=33.2309
	step [6/195], loss=34.2844
	step [7/195], loss=34.5421
	step [8/195], loss=32.7235
	step [9/195], loss=32.6938
	step [10/195], loss=32.7307
	step [11/195], loss=33.4861
	step [12/195], loss=32.8287
	step [13/195], loss=33.5738
	step [14/195], loss=33.8694
	step [15/195], loss=34.2532
	step [16/195], loss=32.5227
	step [17/195], loss=33.0939
	step [18/195], loss=32.6551
	step [19/195], loss=32.2841
	step [20/195], loss=32.6274
	step [21/195], loss=33.4455
	step [22/195], loss=31.7808
	step [23/195], loss=32.4266
	step [24/195], loss=32.8425
	step [25/195], loss=31.9114
	step [26/195], loss=31.9194
	step [27/195], loss=34.2128
	step [28/195], loss=31.0366
	step [29/195], loss=33.1274
	step [30/195], loss=32.4201
	step [31/195], loss=33.0197
	step [32/195], loss=33.4523
	step [33/195], loss=31.0120
	step [34/195], loss=31.9203
	step [35/195], loss=31.8058
	step [36/195], loss=32.7889
	step [37/195], loss=32.8304
	step [38/195], loss=31.6569
	step [39/195], loss=33.0268
	step [40/195], loss=32.9560
	step [41/195], loss=31.8644
	step [42/195], loss=32.6546
	step [43/195], loss=32.6978
	step [44/195], loss=31.2531
	step [45/195], loss=32.4206
	step [46/195], loss=31.6874
	step [47/195], loss=31.3601
	step [48/195], loss=32.7297
	step [49/195], loss=30.1843
	step [50/195], loss=31.9920
	step [51/195], loss=31.0385
	step [52/195], loss=31.8738
	step [53/195], loss=32.7997
	step [54/195], loss=31.3015
	step [55/195], loss=33.2654
	step [56/195], loss=31.6780
	step [57/195], loss=31.2465
	step [58/195], loss=29.2761
	step [59/195], loss=31.6834
	step [60/195], loss=31.4273
	step [61/195], loss=32.4427
	step [62/195], loss=31.2624
	step [63/195], loss=31.6477
	step [64/195], loss=31.2054
	step [65/195], loss=30.5584
	step [66/195], loss=30.4816
	step [67/195], loss=30.8742
	step [68/195], loss=31.6186
	step [69/195], loss=31.9057
	step [70/195], loss=30.9882
	step [71/195], loss=32.1100
	step [72/195], loss=33.4143
	step [73/195], loss=30.8638
	step [74/195], loss=30.0786
	step [75/195], loss=32.7503
	step [76/195], loss=30.1788
	step [77/195], loss=30.0185
	step [78/195], loss=30.1632
	step [79/195], loss=30.1731
	step [80/195], loss=30.2437
	step [81/195], loss=31.8604
	step [82/195], loss=31.6727
	step [83/195], loss=31.0082
	step [84/195], loss=31.0105
	step [85/195], loss=29.9453
	step [86/195], loss=30.7510
	step [87/195], loss=30.2235
	step [88/195], loss=30.1596
	step [89/195], loss=30.7943
	step [90/195], loss=30.8373
	step [91/195], loss=31.6507
	step [92/195], loss=30.2380
	step [93/195], loss=31.6845
	step [94/195], loss=29.7024
	step [95/195], loss=30.0934
	step [96/195], loss=31.2490
	step [97/195], loss=31.1266
	step [98/195], loss=29.5256
	step [99/195], loss=29.1007
	step [100/195], loss=31.5302
	step [101/195], loss=31.0643
	step [102/195], loss=29.1311
	step [103/195], loss=30.1182
	step [104/195], loss=29.8022
	step [105/195], loss=30.3052
	step [106/195], loss=29.3684
	step [107/195], loss=29.6120
	step [108/195], loss=28.6485
	step [109/195], loss=30.4588
	step [110/195], loss=29.6778
	step [111/195], loss=29.5095
	step [112/195], loss=29.7684
	step [113/195], loss=29.7494
	step [114/195], loss=29.0000
	step [115/195], loss=28.9690
	step [116/195], loss=29.0463
	step [117/195], loss=29.7818
	step [118/195], loss=29.6413
	step [119/195], loss=28.7555
	step [120/195], loss=29.1990
	step [121/195], loss=29.6548
	step [122/195], loss=29.2231
	step [123/195], loss=29.7041
	step [124/195], loss=29.6383
	step [125/195], loss=28.9198
	step [126/195], loss=28.6022
	step [127/195], loss=29.5714
	step [128/195], loss=28.3330
	step [129/195], loss=29.5196
	step [130/195], loss=29.4272
	step [131/195], loss=28.0466
	step [132/195], loss=29.1472
	step [133/195], loss=28.1354
	step [134/195], loss=28.8111
	step [135/195], loss=28.7690
	step [136/195], loss=28.9344
	step [137/195], loss=28.5873
	step [138/195], loss=27.3292
	step [139/195], loss=29.7860
	step [140/195], loss=28.7680
	step [141/195], loss=29.9993
	step [142/195], loss=27.8653
	step [143/195], loss=29.1134
	step [144/195], loss=27.4011
	step [145/195], loss=28.8450
	step [146/195], loss=30.0258
	step [147/195], loss=27.2387
	step [148/195], loss=29.0346
	step [149/195], loss=27.6339
	step [150/195], loss=27.3674
	step [151/195], loss=29.5105
	step [152/195], loss=28.1702
	step [153/195], loss=27.4990
	step [154/195], loss=27.9796
	step [155/195], loss=29.9448
	step [156/195], loss=28.8013
	step [157/195], loss=27.7705
	step [158/195], loss=29.1499
	step [159/195], loss=27.1359
	step [160/195], loss=27.8092
	step [161/195], loss=26.8960
	step [162/195], loss=28.8049
	step [163/195], loss=28.8399
	step [164/195], loss=27.3644
	step [165/195], loss=26.5494
	step [166/195], loss=27.2851
	step [167/195], loss=28.1666
	step [168/195], loss=26.8315
	step [169/195], loss=26.5876
	step [170/195], loss=29.2137
	step [171/195], loss=27.6331
	step [172/195], loss=27.7837
	step [173/195], loss=27.7430
	step [174/195], loss=27.6295
	step [175/195], loss=27.3563
	step [176/195], loss=26.5285
	step [177/195], loss=27.5079
	step [178/195], loss=30.2796
	step [179/195], loss=28.3892
	step [180/195], loss=27.7180
	step [181/195], loss=28.4062
	step [182/195], loss=26.9278
	step [183/195], loss=28.1079
	step [184/195], loss=27.0150
	step [185/195], loss=27.0904
	step [186/195], loss=27.2885
	step [187/195], loss=28.8309
	step [188/195], loss=29.5813
	step [189/195], loss=27.9622
	step [190/195], loss=26.2026
	step [191/195], loss=26.5429
	step [192/195], loss=26.8978
	step [193/195], loss=27.5468
	step [194/195], loss=26.9390
	step [195/195], loss=2.5826
	Evaluating
	loss=0.1360, precision=0.1791, recall=0.9975, f1=0.3037
saving model as: 2_saved_model.pth
Training epoch 6
	step [1/195], loss=26.4257
	step [2/195], loss=27.8375
	step [3/195], loss=25.6914
	step [4/195], loss=25.6322
	step [5/195], loss=26.7950
	step [6/195], loss=27.6858
	step [7/195], loss=26.1275
	step [8/195], loss=27.0304
	step [9/195], loss=26.4460
	step [10/195], loss=25.1790
	step [11/195], loss=26.6524
	step [12/195], loss=26.8551
	step [13/195], loss=27.0018
	step [14/195], loss=25.7507
	step [15/195], loss=26.2310
	step [16/195], loss=26.9188
	step [17/195], loss=25.6414
	step [18/195], loss=24.7814
	step [19/195], loss=25.8942
	step [20/195], loss=29.0335
	step [21/195], loss=25.7721
	step [22/195], loss=26.2065
	step [23/195], loss=25.2323
	step [24/195], loss=25.8712
	step [25/195], loss=25.4447
	step [26/195], loss=27.9985
	step [27/195], loss=27.0558
	step [28/195], loss=25.8258
	step [29/195], loss=25.9006
	step [30/195], loss=26.8483
	step [31/195], loss=26.0052
	step [32/195], loss=26.8166
	step [33/195], loss=25.5852
	step [34/195], loss=28.7865
	step [35/195], loss=25.9586
	step [36/195], loss=28.4076
	step [37/195], loss=26.1806
	step [38/195], loss=26.2269
	step [39/195], loss=25.0110
	step [40/195], loss=25.3650
	step [41/195], loss=26.0450
	step [42/195], loss=26.2181
	step [43/195], loss=24.2561
	step [44/195], loss=26.2485
	step [45/195], loss=24.8682
	step [46/195], loss=25.1586
	step [47/195], loss=26.7188
	step [48/195], loss=25.0192
	step [49/195], loss=25.9388
	step [50/195], loss=24.1171
	step [51/195], loss=26.1905
	step [52/195], loss=25.3909
	step [53/195], loss=24.4224
	step [54/195], loss=26.3121
	step [55/195], loss=25.7836
	step [56/195], loss=25.5679
	step [57/195], loss=25.5170
	step [58/195], loss=25.4760
	step [59/195], loss=26.7938
	step [60/195], loss=25.0743
	step [61/195], loss=25.8551
	step [62/195], loss=27.0085
	step [63/195], loss=24.7316
	step [64/195], loss=26.2419
	step [65/195], loss=25.5169
	step [66/195], loss=25.0630
	step [67/195], loss=24.8447
	step [68/195], loss=24.6715
	step [69/195], loss=25.6459
	step [70/195], loss=24.6787
	step [71/195], loss=23.8441
	step [72/195], loss=25.3824
	step [73/195], loss=24.3792
	step [74/195], loss=24.0925
	step [75/195], loss=23.4361
	step [76/195], loss=24.7932
	step [77/195], loss=23.8501
	step [78/195], loss=24.6649
	step [79/195], loss=24.5430
	step [80/195], loss=24.3895
	step [81/195], loss=23.8323
	step [82/195], loss=25.9386
	step [83/195], loss=24.4204
	step [84/195], loss=23.8073
	step [85/195], loss=25.3124
	step [86/195], loss=23.7874
	step [87/195], loss=24.2377
	step [88/195], loss=23.2896
	step [89/195], loss=24.5346
	step [90/195], loss=22.6774
	step [91/195], loss=24.2033
	step [92/195], loss=24.4454
	step [93/195], loss=23.7821
	step [94/195], loss=23.4852
	step [95/195], loss=23.6731
	step [96/195], loss=24.1419
	step [97/195], loss=23.4226
	step [98/195], loss=25.2134
	step [99/195], loss=24.8435
	step [100/195], loss=24.5856
	step [101/195], loss=25.6780
	step [102/195], loss=24.0266
	step [103/195], loss=23.6119
	step [104/195], loss=24.6635
	step [105/195], loss=23.3647
	step [106/195], loss=23.8571
	step [107/195], loss=25.4999
	step [108/195], loss=26.0842
	step [109/195], loss=23.9041
	step [110/195], loss=23.4763
	step [111/195], loss=24.5802
	step [112/195], loss=23.0348
	step [113/195], loss=24.3002
	step [114/195], loss=23.7344
	step [115/195], loss=24.1751
	step [116/195], loss=23.3850
	step [117/195], loss=22.6673
	step [118/195], loss=23.4809
	step [119/195], loss=24.7810
	step [120/195], loss=25.4970
	step [121/195], loss=22.4021
	step [122/195], loss=22.7871
	step [123/195], loss=22.7509
	step [124/195], loss=23.5971
	step [125/195], loss=24.5137
	step [126/195], loss=25.1699
	step [127/195], loss=23.4259
	step [128/195], loss=23.8730
	step [129/195], loss=24.0259
	step [130/195], loss=22.5602
	step [131/195], loss=23.0553
	step [132/195], loss=22.9265
	step [133/195], loss=23.4720
	step [134/195], loss=24.5417
	step [135/195], loss=25.6949
	step [136/195], loss=23.0958
	step [137/195], loss=22.5412
	step [138/195], loss=23.3377
	step [139/195], loss=22.6917
	step [140/195], loss=23.1055
	step [141/195], loss=22.9077
	step [142/195], loss=21.8446
	step [143/195], loss=22.8738
	step [144/195], loss=24.1013
	step [145/195], loss=25.8497
	step [146/195], loss=22.4434
	step [147/195], loss=22.9312
	step [148/195], loss=22.2334
	step [149/195], loss=24.6713
	step [150/195], loss=21.5080
	step [151/195], loss=24.5590
	step [152/195], loss=23.8127
	step [153/195], loss=21.6713
	step [154/195], loss=23.7982
	step [155/195], loss=22.3717
	step [156/195], loss=22.3720
	step [157/195], loss=22.9025
	step [158/195], loss=23.7798
	step [159/195], loss=23.3483
	step [160/195], loss=23.9840
	step [161/195], loss=23.8559
	step [162/195], loss=23.5610
	step [163/195], loss=23.3240
	step [164/195], loss=23.0223
	step [165/195], loss=24.2982
	step [166/195], loss=22.9663
	step [167/195], loss=23.3933
	step [168/195], loss=22.4971
	step [169/195], loss=24.5570
	step [170/195], loss=21.1777
	step [171/195], loss=24.1251
	step [172/195], loss=22.6021
	step [173/195], loss=21.8129
	step [174/195], loss=22.5006
	step [175/195], loss=21.6098
	step [176/195], loss=21.6778
	step [177/195], loss=23.7928
	step [178/195], loss=22.1634
	step [179/195], loss=21.5559
	step [180/195], loss=22.4317
	step [181/195], loss=21.4447
	step [182/195], loss=22.9776
	step [183/195], loss=23.5560
	step [184/195], loss=23.1704
	step [185/195], loss=24.3647
	step [186/195], loss=23.3819
	step [187/195], loss=22.6542
	step [188/195], loss=22.6056
	step [189/195], loss=22.1908
	step [190/195], loss=23.6352
	step [191/195], loss=24.6137
	step [192/195], loss=22.4036
	step [193/195], loss=22.6915
	step [194/195], loss=21.5640
	step [195/195], loss=2.0497
	Evaluating
	loss=0.0997, precision=0.2519, recall=0.9955, f1=0.4021
saving model as: 2_saved_model.pth
Training epoch 7
	step [1/195], loss=24.1247
	step [2/195], loss=21.8642
	step [3/195], loss=21.4390
	step [4/195], loss=22.9742
	step [5/195], loss=22.2762
	step [6/195], loss=22.9228
	step [7/195], loss=24.1000
	step [8/195], loss=20.9733
	step [9/195], loss=24.2811
	step [10/195], loss=22.0997
	step [11/195], loss=21.6102
	step [12/195], loss=21.3063
	step [13/195], loss=22.5343
	step [14/195], loss=21.5248
	step [15/195], loss=21.8193
	step [16/195], loss=23.1847
	step [17/195], loss=22.7822
	step [18/195], loss=21.7843
	step [19/195], loss=22.3552
	step [20/195], loss=22.2210
	step [21/195], loss=20.2338
	step [22/195], loss=21.5162
	step [23/195], loss=20.9119
	step [24/195], loss=22.5391
	step [25/195], loss=20.9171
	step [26/195], loss=22.5520
	step [27/195], loss=21.9220
	step [28/195], loss=20.4551
	step [29/195], loss=22.6603
	step [30/195], loss=22.0959
	step [31/195], loss=22.1723
	step [32/195], loss=22.0135
	step [33/195], loss=22.2217
	step [34/195], loss=22.3143
	step [35/195], loss=21.3086
	step [36/195], loss=22.4029
	step [37/195], loss=19.8976
	step [38/195], loss=22.1425
	step [39/195], loss=21.4571
	step [40/195], loss=20.6946
	step [41/195], loss=21.3784
	step [42/195], loss=20.8759
	step [43/195], loss=21.3273
	step [44/195], loss=21.3315
	step [45/195], loss=20.0018
	step [46/195], loss=21.1332
	step [47/195], loss=22.3716
	step [48/195], loss=20.1914
	step [49/195], loss=20.4780
	step [50/195], loss=21.1563
	step [51/195], loss=20.8429
	step [52/195], loss=19.9790
	step [53/195], loss=20.0847
	step [54/195], loss=21.2638
	step [55/195], loss=21.5957
	step [56/195], loss=20.2637
	step [57/195], loss=20.0285
	step [58/195], loss=21.5061
	step [59/195], loss=21.2059
	step [60/195], loss=20.2706
	step [61/195], loss=21.4017
	step [62/195], loss=19.9462
	step [63/195], loss=20.9360
	step [64/195], loss=18.7050
	step [65/195], loss=21.2617
	step [66/195], loss=20.9538
	step [67/195], loss=21.2048
	step [68/195], loss=20.0484
	step [69/195], loss=21.3115
	step [70/195], loss=19.8645
	step [71/195], loss=20.1964
	step [72/195], loss=19.0398
	step [73/195], loss=19.1336
	step [74/195], loss=19.5390
	step [75/195], loss=20.4086
	step [76/195], loss=22.5178
	step [77/195], loss=19.9149
	step [78/195], loss=20.5327
	step [79/195], loss=21.8177
	step [80/195], loss=21.8154
	step [81/195], loss=19.6613
	step [82/195], loss=20.9254
	step [83/195], loss=20.4624
	step [84/195], loss=20.9429
	step [85/195], loss=21.4872
	step [86/195], loss=20.7396
	step [87/195], loss=20.8814
	step [88/195], loss=20.0050
	step [89/195], loss=19.1710
	step [90/195], loss=20.3021
	step [91/195], loss=23.7161
	step [92/195], loss=20.4782
	step [93/195], loss=21.4878
	step [94/195], loss=20.6169
	step [95/195], loss=20.9410
	step [96/195], loss=21.0658
	step [97/195], loss=20.4530
	step [98/195], loss=20.4975
	step [99/195], loss=20.7652
	step [100/195], loss=20.5305
	step [101/195], loss=19.7766
	step [102/195], loss=20.2002
	step [103/195], loss=21.2662
	step [104/195], loss=19.7880
	step [105/195], loss=19.5381
	step [106/195], loss=19.6902
	step [107/195], loss=20.5911
	step [108/195], loss=19.8854
	step [109/195], loss=19.9276
	step [110/195], loss=19.1555
	step [111/195], loss=19.8495
	step [112/195], loss=19.9496
	step [113/195], loss=19.3348
	step [114/195], loss=20.7318
	step [115/195], loss=19.7980
	step [116/195], loss=20.3764
	step [117/195], loss=19.6890
	step [118/195], loss=21.0621
	step [119/195], loss=18.4758
	step [120/195], loss=19.9923
	step [121/195], loss=20.7258
	step [122/195], loss=19.8617
	step [123/195], loss=19.5989
	step [124/195], loss=21.8589
	step [125/195], loss=20.4050
	step [126/195], loss=19.2427
	step [127/195], loss=20.1869
	step [128/195], loss=19.5311
	step [129/195], loss=18.2472
	step [130/195], loss=19.5687
	step [131/195], loss=22.2310
	step [132/195], loss=20.7606
	step [133/195], loss=20.5900
	step [134/195], loss=21.7645
	step [135/195], loss=19.0913
	step [136/195], loss=19.3289
	step [137/195], loss=20.4220
	step [138/195], loss=20.1262
	step [139/195], loss=20.9857
	step [140/195], loss=19.6824
	step [141/195], loss=18.6374
	step [142/195], loss=18.9264
	step [143/195], loss=20.8910
	step [144/195], loss=20.1978
	step [145/195], loss=19.2530
	step [146/195], loss=20.0970
	step [147/195], loss=18.9302
	step [148/195], loss=19.2750
	step [149/195], loss=18.5796
	step [150/195], loss=19.7625
	step [151/195], loss=20.3723
	step [152/195], loss=19.5120
	step [153/195], loss=20.1703
	step [154/195], loss=18.5051
	step [155/195], loss=19.0543
	step [156/195], loss=19.8905
	step [157/195], loss=18.8376
	step [158/195], loss=18.5024
	step [159/195], loss=19.7602
	step [160/195], loss=19.9936
	step [161/195], loss=21.4028
	step [162/195], loss=18.7812
	step [163/195], loss=19.1485
	step [164/195], loss=18.8075
	step [165/195], loss=17.6532
	step [166/195], loss=19.1513
	step [167/195], loss=18.4340
	step [168/195], loss=18.3848
	step [169/195], loss=17.0276
	step [170/195], loss=19.2019
	step [171/195], loss=19.7928
	step [172/195], loss=17.7745
	step [173/195], loss=19.1629
	step [174/195], loss=19.9018
	step [175/195], loss=18.7265
	step [176/195], loss=19.0229
	step [177/195], loss=19.7024
	step [178/195], loss=17.9045
	step [179/195], loss=15.7371
	step [180/195], loss=18.6872
	step [181/195], loss=18.2516
	step [182/195], loss=18.5581
	step [183/195], loss=17.4858
	step [184/195], loss=17.3872
	step [185/195], loss=20.1254
	step [186/195], loss=18.1456
	step [187/195], loss=17.9918
	step [188/195], loss=19.7269
	step [189/195], loss=18.0102
	step [190/195], loss=17.7871
	step [191/195], loss=19.0687
	step [192/195], loss=19.5457
	step [193/195], loss=18.6185
	step [194/195], loss=19.3172
	step [195/195], loss=1.7508
	Evaluating
	loss=0.0864, precision=0.2018, recall=0.9969, f1=0.3357
Training epoch 8
	step [1/195], loss=18.3851
	step [2/195], loss=18.8448
	step [3/195], loss=18.3162
	step [4/195], loss=20.3942
	step [5/195], loss=18.1015
	step [6/195], loss=16.8339
	step [7/195], loss=18.8297
	step [8/195], loss=18.1498
	step [9/195], loss=18.4111
	step [10/195], loss=17.7894
	step [11/195], loss=18.5570
	step [12/195], loss=17.9989
	step [13/195], loss=19.8668
	step [14/195], loss=18.5182
	step [15/195], loss=19.1894
	step [16/195], loss=19.8375
	step [17/195], loss=17.2234
	step [18/195], loss=17.9676
	step [19/195], loss=18.1270
	step [20/195], loss=19.1221
	step [21/195], loss=17.8945
	step [22/195], loss=18.0841
	step [23/195], loss=18.8295
	step [24/195], loss=18.5621
	step [25/195], loss=18.2905
	step [26/195], loss=17.4972
	step [27/195], loss=17.8413
	step [28/195], loss=19.6961
	step [29/195], loss=18.3290
	step [30/195], loss=18.7178
	step [31/195], loss=18.2259
	step [32/195], loss=17.8515
	step [33/195], loss=17.2335
	step [34/195], loss=17.4117
	step [35/195], loss=18.1879
	step [36/195], loss=18.0235
	step [37/195], loss=17.8947
	step [38/195], loss=18.0086
	step [39/195], loss=18.8895
	step [40/195], loss=17.8242
	step [41/195], loss=21.5100
	step [42/195], loss=17.5649
	step [43/195], loss=16.9804
	step [44/195], loss=17.6973
	step [45/195], loss=17.3808
	step [46/195], loss=18.5481
	step [47/195], loss=18.2909
	step [48/195], loss=17.7280
	step [49/195], loss=19.5945
	step [50/195], loss=17.3455
	step [51/195], loss=17.7902
	step [52/195], loss=17.1932
	step [53/195], loss=17.8768
	step [54/195], loss=17.4469
	step [55/195], loss=17.2255
	step [56/195], loss=17.1938
	step [57/195], loss=17.5093
	step [58/195], loss=18.3695
	step [59/195], loss=19.6433
	step [60/195], loss=17.0165
	step [61/195], loss=16.8567
	step [62/195], loss=16.9373
	step [63/195], loss=18.9245
	step [64/195], loss=17.8618
	step [65/195], loss=18.6756
	step [66/195], loss=17.7071
	step [67/195], loss=18.0395
	step [68/195], loss=17.3982
	step [69/195], loss=19.1371
	step [70/195], loss=18.3191
	step [71/195], loss=16.9025
	step [72/195], loss=17.9242
	step [73/195], loss=18.0498
	step [74/195], loss=18.6119
	step [75/195], loss=17.4821
	step [76/195], loss=19.5904
	step [77/195], loss=17.4200
	step [78/195], loss=17.4842
	step [79/195], loss=18.0039
	step [80/195], loss=19.0557
	step [81/195], loss=17.0681
	step [82/195], loss=18.3629
	step [83/195], loss=17.5497
	step [84/195], loss=17.0848
	step [85/195], loss=16.2846
	step [86/195], loss=18.6886
	step [87/195], loss=17.2838
	step [88/195], loss=17.7860
	step [89/195], loss=19.3538
	step [90/195], loss=17.2480
	step [91/195], loss=16.7758
	step [92/195], loss=19.0346
	step [93/195], loss=17.8730
	step [94/195], loss=15.2332
	step [95/195], loss=18.7973
	step [96/195], loss=19.2224
	step [97/195], loss=17.8630
	step [98/195], loss=17.0017
	step [99/195], loss=17.0834
	step [100/195], loss=17.3975
	step [101/195], loss=16.8771
	step [102/195], loss=17.4442
	step [103/195], loss=19.0657
	step [104/195], loss=16.6866
	step [105/195], loss=16.8782
	step [106/195], loss=16.4940
	step [107/195], loss=16.4070
	step [108/195], loss=18.5325
	step [109/195], loss=16.5862
	step [110/195], loss=17.0191
	step [111/195], loss=17.3376
	step [112/195], loss=16.0755
	step [113/195], loss=17.7071
	step [114/195], loss=17.2755
	step [115/195], loss=16.3355
	step [116/195], loss=17.8697
	step [117/195], loss=16.6526
	step [118/195], loss=17.4425
	step [119/195], loss=17.4758
	step [120/195], loss=17.3026
	step [121/195], loss=16.0119
	step [122/195], loss=16.3539
	step [123/195], loss=16.4722
	step [124/195], loss=17.5902
	step [125/195], loss=16.6798
	step [126/195], loss=17.5119
	step [127/195], loss=16.1942
	step [128/195], loss=16.5648
	step [129/195], loss=17.4410
	step [130/195], loss=16.4472
	step [131/195], loss=16.8143
	step [132/195], loss=16.2679
	step [133/195], loss=16.2676
	step [134/195], loss=16.7564
	step [135/195], loss=16.5004
	step [136/195], loss=16.4326
	step [137/195], loss=15.5896
	step [138/195], loss=16.7392
	step [139/195], loss=16.3152
	step [140/195], loss=15.8152
	step [141/195], loss=16.1632
	step [142/195], loss=15.7906
	step [143/195], loss=16.7923
	step [144/195], loss=17.0822
	step [145/195], loss=15.6998
	step [146/195], loss=16.3355
	step [147/195], loss=16.1329
	step [148/195], loss=15.8036
	step [149/195], loss=15.4254
	step [150/195], loss=15.9014
	step [151/195], loss=16.6636
	step [152/195], loss=17.5315
	step [153/195], loss=15.9796
	step [154/195], loss=17.5518
	step [155/195], loss=15.8457
	step [156/195], loss=16.3409
	step [157/195], loss=17.4892
	step [158/195], loss=17.8798
	step [159/195], loss=16.3753
	step [160/195], loss=18.1125
	step [161/195], loss=17.2129
	step [162/195], loss=18.2904
	step [163/195], loss=16.2448
	step [164/195], loss=16.9729
	step [165/195], loss=15.4145
	step [166/195], loss=16.4470
	step [167/195], loss=15.3212
	step [168/195], loss=14.9106
	step [169/195], loss=16.7011
	step [170/195], loss=16.8698
	step [171/195], loss=15.7433
	step [172/195], loss=16.1614
	step [173/195], loss=15.9974
	step [174/195], loss=16.7346
	step [175/195], loss=15.8656
	step [176/195], loss=17.3039
	step [177/195], loss=17.6475
	step [178/195], loss=15.8278
	step [179/195], loss=16.2852
	step [180/195], loss=16.3008
	step [181/195], loss=17.4804
	step [182/195], loss=16.7560
	step [183/195], loss=15.3338
	step [184/195], loss=15.9050
	step [185/195], loss=18.3391
	step [186/195], loss=15.3262
	step [187/195], loss=16.9466
	step [188/195], loss=15.2942
	step [189/195], loss=16.0251
	step [190/195], loss=17.7938
	step [191/195], loss=16.2792
	step [192/195], loss=18.0614
	step [193/195], loss=16.0856
	step [194/195], loss=15.0497
	step [195/195], loss=1.6921
	Evaluating
	loss=0.0770, precision=0.1703, recall=0.9977, f1=0.2909
Training epoch 9
	step [1/195], loss=16.0492
	step [2/195], loss=15.4523
	step [3/195], loss=16.1725
	step [4/195], loss=16.6045
	step [5/195], loss=17.8159
	step [6/195], loss=15.8982
	step [7/195], loss=15.8060
	step [8/195], loss=15.8603
	step [9/195], loss=16.6984
	step [10/195], loss=16.3184
	step [11/195], loss=16.2981
	step [12/195], loss=16.0799
	step [13/195], loss=15.0225
	step [14/195], loss=15.4475
	step [15/195], loss=16.6272
	step [16/195], loss=14.7867
	step [17/195], loss=16.1005
	step [18/195], loss=15.3749
	step [19/195], loss=15.7359
	step [20/195], loss=18.1965
	step [21/195], loss=17.8954
	step [22/195], loss=17.1301
	step [23/195], loss=15.2776
	step [24/195], loss=16.9372
	step [25/195], loss=17.4997
	step [26/195], loss=16.2679
	step [27/195], loss=14.6855
	step [28/195], loss=16.3027
	step [29/195], loss=16.0598
	step [30/195], loss=16.2605
	step [31/195], loss=15.7488
	step [32/195], loss=15.6739
	step [33/195], loss=14.5115
	step [34/195], loss=17.4739
	step [35/195], loss=15.9437
	step [36/195], loss=15.7485
	step [37/195], loss=14.4822
	step [38/195], loss=15.1883
	step [39/195], loss=18.7828
	step [40/195], loss=18.2850
	step [41/195], loss=15.2780
	step [42/195], loss=14.9007
	step [43/195], loss=15.7416
	step [44/195], loss=15.9584
	step [45/195], loss=14.2713
	step [46/195], loss=15.8325
	step [47/195], loss=15.3517
	step [48/195], loss=14.4300
	step [49/195], loss=16.7446
	step [50/195], loss=16.4103
	step [51/195], loss=14.7518
	step [52/195], loss=15.7874
	step [53/195], loss=15.2761
	step [54/195], loss=16.1320
	step [55/195], loss=15.7764
	step [56/195], loss=15.9027
	step [57/195], loss=15.8799
	step [58/195], loss=15.8420
	step [59/195], loss=16.0923
	step [60/195], loss=15.3720
	step [61/195], loss=15.5870
	step [62/195], loss=14.8260
	step [63/195], loss=14.7361
	step [64/195], loss=14.7239
	step [65/195], loss=15.0356
	step [66/195], loss=18.3419
	step [67/195], loss=15.8049
	step [68/195], loss=15.5685
	step [69/195], loss=16.0132
	step [70/195], loss=14.9529
	step [71/195], loss=14.7088
	step [72/195], loss=16.5634
	step [73/195], loss=16.8349
	step [74/195], loss=15.6005
	step [75/195], loss=15.1321
	step [76/195], loss=15.2220
	step [77/195], loss=15.7970
	step [78/195], loss=15.2866
	step [79/195], loss=16.2626
	step [80/195], loss=15.9575
	step [81/195], loss=14.9485
	step [82/195], loss=14.1259
	step [83/195], loss=16.5479
	step [84/195], loss=15.1452
	step [85/195], loss=16.7412
	step [86/195], loss=13.7871
	step [87/195], loss=14.0979
	step [88/195], loss=13.8409
	step [89/195], loss=14.6837
	step [90/195], loss=15.2823
	step [91/195], loss=15.6592
	step [92/195], loss=15.1557
	step [93/195], loss=14.0011
	step [94/195], loss=14.5232
	step [95/195], loss=16.0969
	step [96/195], loss=15.8936
	step [97/195], loss=14.2330
	step [98/195], loss=15.9046
	step [99/195], loss=14.6253
	step [100/195], loss=13.3300
	step [101/195], loss=15.2101
	step [102/195], loss=15.6137
	step [103/195], loss=14.0142
	step [104/195], loss=15.4570
	step [105/195], loss=16.2325
	step [106/195], loss=15.5805
	step [107/195], loss=14.0664
	step [108/195], loss=14.2254
	step [109/195], loss=16.0289
	step [110/195], loss=16.8085
	step [111/195], loss=14.6985
	step [112/195], loss=14.0610
	step [113/195], loss=15.2854
	step [114/195], loss=15.4860
	step [115/195], loss=13.3170
	step [116/195], loss=14.8114
	step [117/195], loss=15.2415
	step [118/195], loss=14.2088
	step [119/195], loss=14.4265
	step [120/195], loss=14.9283
	step [121/195], loss=13.8437
	step [122/195], loss=14.3087
	step [123/195], loss=15.6209
	step [124/195], loss=15.7605
	step [125/195], loss=15.1305
	step [126/195], loss=15.3374
	step [127/195], loss=16.3861
	step [128/195], loss=13.2881
	step [129/195], loss=15.2508
	step [130/195], loss=14.3751
	step [131/195], loss=14.7270
	step [132/195], loss=15.9894
	step [133/195], loss=16.3682
	step [134/195], loss=15.1159
	step [135/195], loss=14.2978
	step [136/195], loss=14.6736
	step [137/195], loss=15.3006
	step [138/195], loss=15.2541
	step [139/195], loss=15.9160
	step [140/195], loss=16.2799
	step [141/195], loss=15.6831
	step [142/195], loss=14.9493
	step [143/195], loss=13.5773
	step [144/195], loss=15.0276
	step [145/195], loss=14.6999
	step [146/195], loss=14.4468
	step [147/195], loss=14.9006
	step [148/195], loss=14.0164
	step [149/195], loss=14.7858
	step [150/195], loss=14.2934
	step [151/195], loss=15.9981
	step [152/195], loss=15.1523
	step [153/195], loss=14.3725
	step [154/195], loss=14.3354
	step [155/195], loss=15.4710
	step [156/195], loss=14.8008
	step [157/195], loss=15.2886
	step [158/195], loss=15.9481
	step [159/195], loss=15.5090
	step [160/195], loss=14.7909
	step [161/195], loss=14.2770
	step [162/195], loss=15.9350
	step [163/195], loss=15.8906
	step [164/195], loss=15.1718
	step [165/195], loss=14.3933
	step [166/195], loss=15.2079
	step [167/195], loss=13.6133
	step [168/195], loss=14.2266
	step [169/195], loss=17.3128
	step [170/195], loss=15.8110
	step [171/195], loss=16.0797
	step [172/195], loss=14.2359
	step [173/195], loss=16.1081
	step [174/195], loss=14.5141
	step [175/195], loss=14.9589
	step [176/195], loss=15.1427
	step [177/195], loss=14.8327
	step [178/195], loss=16.8043
	step [179/195], loss=14.5614
	step [180/195], loss=15.1701
	step [181/195], loss=14.1000
	step [182/195], loss=13.7710
	step [183/195], loss=13.6983
	step [184/195], loss=13.9497
	step [185/195], loss=13.3683
	step [186/195], loss=16.4952
	step [187/195], loss=13.7529
	step [188/195], loss=14.6434
	step [189/195], loss=15.0079
	step [190/195], loss=14.0745
	step [191/195], loss=13.7698
	step [192/195], loss=14.7459
	step [193/195], loss=16.3036
	step [194/195], loss=16.4908
	step [195/195], loss=1.6496
	Evaluating
	loss=0.0667, precision=0.1932, recall=0.9975, f1=0.3237
Training epoch 10
	step [1/195], loss=15.4503
	step [2/195], loss=14.0621
	step [3/195], loss=15.4004
	step [4/195], loss=14.7314
	step [5/195], loss=14.6034
	step [6/195], loss=14.4313
	step [7/195], loss=13.3784
	step [8/195], loss=13.3930
	step [9/195], loss=15.2972
	step [10/195], loss=14.1844
	step [11/195], loss=14.2398
	step [12/195], loss=13.7894
	step [13/195], loss=15.3804
	step [14/195], loss=15.3917
	step [15/195], loss=14.7290
	step [16/195], loss=13.5214
	step [17/195], loss=14.6420
	step [18/195], loss=13.6350
	step [19/195], loss=15.2464
	step [20/195], loss=14.8175
	step [21/195], loss=13.7450
	step [22/195], loss=12.3961
	step [23/195], loss=12.3379
	step [24/195], loss=13.5535
	step [25/195], loss=14.1744
	step [26/195], loss=13.5364
	step [27/195], loss=13.6634
	step [28/195], loss=14.0400
	step [29/195], loss=14.7593
	step [30/195], loss=15.0955
	step [31/195], loss=17.3380
	step [32/195], loss=13.6939
	step [33/195], loss=13.2342
	step [34/195], loss=15.0105
	step [35/195], loss=13.1005
	step [36/195], loss=14.0409
	step [37/195], loss=14.0463
	step [38/195], loss=16.1884
	step [39/195], loss=13.9343
	step [40/195], loss=17.3098
	step [41/195], loss=14.2657
	step [42/195], loss=14.8322
	step [43/195], loss=13.5550
	step [44/195], loss=13.4194
	step [45/195], loss=14.3923
	step [46/195], loss=15.1098
	step [47/195], loss=16.4678
	step [48/195], loss=13.8594
	step [49/195], loss=14.3221
	step [50/195], loss=14.7187
	step [51/195], loss=13.0909
	step [52/195], loss=12.6826
	step [53/195], loss=13.2986
	step [54/195], loss=14.6967
	step [55/195], loss=15.4584
	step [56/195], loss=15.2340
	step [57/195], loss=13.4092
	step [58/195], loss=13.5741
	step [59/195], loss=15.0761
	step [60/195], loss=13.6580
	step [61/195], loss=13.9008
	step [62/195], loss=12.6546
	step [63/195], loss=13.9514
	step [64/195], loss=13.7592
	step [65/195], loss=14.5211
	step [66/195], loss=13.9145
	step [67/195], loss=12.9331
	step [68/195], loss=12.2926
	step [69/195], loss=14.2976
	step [70/195], loss=14.4127
	step [71/195], loss=13.4593
	step [72/195], loss=13.9796
	step [73/195], loss=13.9722
	step [74/195], loss=14.8195
	step [75/195], loss=13.3363
	step [76/195], loss=15.1845
	step [77/195], loss=12.7370
	step [78/195], loss=11.9282
	step [79/195], loss=13.2848
	step [80/195], loss=13.9421
	step [81/195], loss=14.3116
	step [82/195], loss=13.1923
	step [83/195], loss=13.4434
	step [84/195], loss=13.9043
	step [85/195], loss=16.0609
	step [86/195], loss=14.8203
	step [87/195], loss=13.9728
	step [88/195], loss=14.9266
	step [89/195], loss=14.2731
	step [90/195], loss=14.9375
	step [91/195], loss=14.0943
	step [92/195], loss=13.2903
	step [93/195], loss=13.7206
	step [94/195], loss=14.1368
	step [95/195], loss=12.8377
	step [96/195], loss=15.1802
	step [97/195], loss=14.0265
	step [98/195], loss=14.1763
	step [99/195], loss=12.4505
	step [100/195], loss=13.3945
	step [101/195], loss=13.6854
	step [102/195], loss=15.1014
	step [103/195], loss=13.2262
	step [104/195], loss=13.6768
	step [105/195], loss=13.5295
	step [106/195], loss=16.1281
	step [107/195], loss=11.5798
	step [108/195], loss=13.8450
	step [109/195], loss=14.5563
	step [110/195], loss=13.3648
	step [111/195], loss=15.5896
	step [112/195], loss=12.9976
	step [113/195], loss=12.5427
	step [114/195], loss=12.4869
	step [115/195], loss=13.1926
	step [116/195], loss=13.2142
	step [117/195], loss=12.2279
	step [118/195], loss=15.0532
	step [119/195], loss=14.4149
	step [120/195], loss=14.1281
	step [121/195], loss=12.6818
	step [122/195], loss=14.4527
	step [123/195], loss=13.1074
	step [124/195], loss=13.9414
	step [125/195], loss=13.8182
	step [126/195], loss=13.2014
	step [127/195], loss=13.1246
	step [128/195], loss=12.6835
	step [129/195], loss=13.2180
	step [130/195], loss=13.3089
	step [131/195], loss=13.0729
	step [132/195], loss=13.3817
	step [133/195], loss=12.8184
	step [134/195], loss=13.4041
	step [135/195], loss=13.4114
	step [136/195], loss=13.7685
	step [137/195], loss=12.6513
	step [138/195], loss=12.6772
	step [139/195], loss=13.5732
	step [140/195], loss=12.3982
	step [141/195], loss=14.3938
	step [142/195], loss=12.8119
	step [143/195], loss=13.9991
	step [144/195], loss=13.6245
	step [145/195], loss=14.7562
	step [146/195], loss=14.7751
	step [147/195], loss=12.8617
	step [148/195], loss=13.2450
	step [149/195], loss=12.4887
	step [150/195], loss=13.1917
	step [151/195], loss=12.9070
	step [152/195], loss=13.8311
	step [153/195], loss=13.0985
	step [154/195], loss=13.2037
	step [155/195], loss=15.5301
	step [156/195], loss=14.6774
	step [157/195], loss=12.4981
	step [158/195], loss=12.4278
	step [159/195], loss=13.1923
	step [160/195], loss=14.2816
	step [161/195], loss=14.3790
	step [162/195], loss=13.0386
	step [163/195], loss=13.2911
	step [164/195], loss=14.9593
	step [165/195], loss=16.1146
	step [166/195], loss=12.9278
	step [167/195], loss=13.8085
	step [168/195], loss=14.3783
	step [169/195], loss=13.4452
	step [170/195], loss=13.7080
	step [171/195], loss=13.4410
	step [172/195], loss=14.0877
	step [173/195], loss=13.6478
	step [174/195], loss=12.8051
	step [175/195], loss=13.4180
	step [176/195], loss=13.3559
	step [177/195], loss=12.0456
	step [178/195], loss=12.4400
	step [179/195], loss=13.2894
	step [180/195], loss=13.3546
	step [181/195], loss=12.6978
	step [182/195], loss=14.2392
	step [183/195], loss=12.8708
	step [184/195], loss=12.9055
	step [185/195], loss=12.8687
	step [186/195], loss=14.0004
	step [187/195], loss=13.6400
	step [188/195], loss=12.6206
	step [189/195], loss=13.1438
	step [190/195], loss=12.5399
	step [191/195], loss=12.7040
	step [192/195], loss=13.6849
	step [193/195], loss=13.4992
	step [194/195], loss=12.6865
	step [195/195], loss=1.2519
	Evaluating
	loss=0.0538, precision=0.2283, recall=0.9966, f1=0.3715
Training epoch 11
	step [1/195], loss=13.4633
	step [2/195], loss=14.3172
	step [3/195], loss=13.3211
	step [4/195], loss=12.3069
	step [5/195], loss=12.8507
	step [6/195], loss=12.1412
	step [7/195], loss=13.1453
	step [8/195], loss=14.1350
	step [9/195], loss=13.0321
	step [10/195], loss=13.0769
	step [11/195], loss=13.1078
	step [12/195], loss=11.8293
	step [13/195], loss=12.7143
	step [14/195], loss=12.2263
	step [15/195], loss=13.8454
	step [16/195], loss=12.4726
	step [17/195], loss=14.6521
	step [18/195], loss=12.1475
	step [19/195], loss=14.5857
	step [20/195], loss=13.1789
	step [21/195], loss=13.4433
	step [22/195], loss=13.2222
	step [23/195], loss=12.9889
	step [24/195], loss=12.7195
	step [25/195], loss=11.5406
	step [26/195], loss=11.6146
	step [27/195], loss=12.4539
	step [28/195], loss=13.3151
	step [29/195], loss=13.1279
	step [30/195], loss=13.0917
	step [31/195], loss=13.0328
	step [32/195], loss=12.4624
	step [33/195], loss=12.2694
	step [34/195], loss=12.6895
	step [35/195], loss=11.6962
	step [36/195], loss=13.6841
	step [37/195], loss=12.7629
	step [38/195], loss=14.6735
	step [39/195], loss=12.0347
	step [40/195], loss=12.2572
	step [41/195], loss=12.1264
	step [42/195], loss=13.4794
	step [43/195], loss=12.4924
	step [44/195], loss=13.4722
	step [45/195], loss=14.3104
	step [46/195], loss=14.1785
	step [47/195], loss=12.9868
	step [48/195], loss=12.7122
	step [49/195], loss=14.3255
	step [50/195], loss=11.4688
	step [51/195], loss=11.6446
	step [52/195], loss=12.4022
	step [53/195], loss=12.3818
	step [54/195], loss=12.0037
	step [55/195], loss=13.4639
	step [56/195], loss=14.1135
	step [57/195], loss=11.9589
	step [58/195], loss=14.3246
	step [59/195], loss=13.1934
	step [60/195], loss=13.2839
	step [61/195], loss=12.9010
	step [62/195], loss=11.6219
	step [63/195], loss=13.3868
	step [64/195], loss=13.1320
	step [65/195], loss=14.2801
	step [66/195], loss=12.6641
	step [67/195], loss=13.5631
	step [68/195], loss=11.7172
	step [69/195], loss=13.2937
	step [70/195], loss=12.0333
	step [71/195], loss=12.3244
	step [72/195], loss=14.2090
	step [73/195], loss=13.3465
	step [74/195], loss=11.7727
	step [75/195], loss=13.4344
	step [76/195], loss=12.4095
	step [77/195], loss=13.0557
	step [78/195], loss=13.6587
	step [79/195], loss=12.8629
	step [80/195], loss=12.1024
	step [81/195], loss=14.7992
	step [82/195], loss=11.6475
	step [83/195], loss=13.1463
	step [84/195], loss=12.0347
	step [85/195], loss=13.3151
	step [86/195], loss=12.9107
	step [87/195], loss=13.5128
	step [88/195], loss=11.4861
	step [89/195], loss=14.1637
	step [90/195], loss=13.9201
	step [91/195], loss=13.3170
	step [92/195], loss=13.2299
	step [93/195], loss=11.5622
	step [94/195], loss=10.5229
	step [95/195], loss=11.7953
	step [96/195], loss=13.0495
	step [97/195], loss=12.5608
	step [98/195], loss=11.5228
	step [99/195], loss=11.2316
	step [100/195], loss=13.1926
	step [101/195], loss=14.6831
	step [102/195], loss=13.1765
	step [103/195], loss=12.7326
	step [104/195], loss=12.1138
	step [105/195], loss=12.2208
	step [106/195], loss=10.7575
	step [107/195], loss=13.4414
	step [108/195], loss=11.8464
	step [109/195], loss=11.1829
	step [110/195], loss=11.7730
	step [111/195], loss=13.0539
	step [112/195], loss=13.2295
	step [113/195], loss=13.0206
	step [114/195], loss=12.2873
	step [115/195], loss=12.9879
	step [116/195], loss=11.8184
	step [117/195], loss=12.7115
	step [118/195], loss=11.5122
	step [119/195], loss=13.2978
	step [120/195], loss=11.8510
	step [121/195], loss=10.8739
	step [122/195], loss=12.8592
	step [123/195], loss=12.6111
	step [124/195], loss=14.2167
	step [125/195], loss=12.2806
	step [126/195], loss=11.8527
	step [127/195], loss=15.4559
	step [128/195], loss=12.2466
	step [129/195], loss=10.7738
	step [130/195], loss=14.1501
	step [131/195], loss=12.0890
	step [132/195], loss=12.7605
	step [133/195], loss=11.7558
	step [134/195], loss=12.2040
	step [135/195], loss=12.7662
	step [136/195], loss=11.9507
	step [137/195], loss=13.7896
	step [138/195], loss=11.9812
	step [139/195], loss=13.4477
	step [140/195], loss=12.5421
	step [141/195], loss=14.3414
	step [142/195], loss=12.5787
	step [143/195], loss=12.8209
	step [144/195], loss=11.4172
	step [145/195], loss=12.7784
	step [146/195], loss=11.6942
	step [147/195], loss=12.6811
	step [148/195], loss=11.7937
	step [149/195], loss=12.7691
	step [150/195], loss=12.2041
	step [151/195], loss=13.1258
	step [152/195], loss=11.7921
	step [153/195], loss=12.5034
	step [154/195], loss=11.3320
	step [155/195], loss=12.2251
	step [156/195], loss=13.0889
	step [157/195], loss=12.2739
	step [158/195], loss=12.4330
	step [159/195], loss=12.1783
	step [160/195], loss=12.0275
	step [161/195], loss=12.9027
	step [162/195], loss=11.3512
	step [163/195], loss=12.2373
	step [164/195], loss=13.6399
	step [165/195], loss=13.5239
	step [166/195], loss=11.9930
	step [167/195], loss=12.3318
	step [168/195], loss=14.9347
	step [169/195], loss=12.7397
	step [170/195], loss=12.2730
	step [171/195], loss=11.3424
	step [172/195], loss=10.8265
	step [173/195], loss=12.7362
	step [174/195], loss=13.6398
	step [175/195], loss=10.9056
	step [176/195], loss=11.3426
	step [177/195], loss=12.5564
	step [178/195], loss=11.8155
	step [179/195], loss=12.4886
	step [180/195], loss=11.8066
	step [181/195], loss=11.6102
	step [182/195], loss=12.6431
	step [183/195], loss=10.8604
	step [184/195], loss=13.1867
	step [185/195], loss=12.1830
	step [186/195], loss=11.5075
	step [187/195], loss=12.7493
	step [188/195], loss=12.1024
	step [189/195], loss=10.9063
	step [190/195], loss=12.1510
	step [191/195], loss=14.3314
	step [192/195], loss=11.6924
	step [193/195], loss=11.8906
	step [194/195], loss=12.5694
	step [195/195], loss=1.2298
	Evaluating
	loss=0.0629, precision=0.1618, recall=0.9978, f1=0.2785
Training epoch 12
	step [1/195], loss=11.4737
	step [2/195], loss=12.0149
	step [3/195], loss=13.2812
	step [4/195], loss=13.1901
	step [5/195], loss=14.1988
	step [6/195], loss=12.0007
	step [7/195], loss=11.1318
	step [8/195], loss=14.2318
	step [9/195], loss=12.1236
	step [10/195], loss=12.7598
	step [11/195], loss=12.3601
	step [12/195], loss=13.5816
	step [13/195], loss=12.3861
	step [14/195], loss=11.2780
	step [15/195], loss=12.6133
	step [16/195], loss=12.5976
	step [17/195], loss=12.5673
	step [18/195], loss=11.0688
	step [19/195], loss=10.9140
	step [20/195], loss=12.2171
	step [21/195], loss=10.5294
	step [22/195], loss=12.2409
	step [23/195], loss=13.0590
	step [24/195], loss=10.6241
	step [25/195], loss=12.8249
	step [26/195], loss=12.1075
	step [27/195], loss=11.6034
	step [28/195], loss=12.1730
	step [29/195], loss=12.8195
	step [30/195], loss=11.4851
	step [31/195], loss=12.7202
	step [32/195], loss=11.6251
	step [33/195], loss=11.5276
	step [34/195], loss=10.2832
	step [35/195], loss=11.8206
	step [36/195], loss=13.0533
	step [37/195], loss=11.9290
	step [38/195], loss=11.8710
	step [39/195], loss=11.3188
	step [40/195], loss=12.2120
	step [41/195], loss=13.1484
	step [42/195], loss=12.1839
	step [43/195], loss=10.9113
	step [44/195], loss=11.3442
	step [45/195], loss=12.3247
	step [46/195], loss=11.8214
	step [47/195], loss=12.2191
	step [48/195], loss=11.3848
	step [49/195], loss=10.9773
	step [50/195], loss=11.5420
	step [51/195], loss=11.9608
	step [52/195], loss=12.8580
	step [53/195], loss=13.7249
	step [54/195], loss=11.8163
	step [55/195], loss=11.5632
	step [56/195], loss=11.5235
	step [57/195], loss=13.8820
	step [58/195], loss=11.3896
	step [59/195], loss=12.0793
	step [60/195], loss=12.0084
	step [61/195], loss=11.8524
	step [62/195], loss=11.3491
	step [63/195], loss=11.6109
	step [64/195], loss=12.0062
	step [65/195], loss=12.6623
	step [66/195], loss=13.6434
	step [67/195], loss=13.5316
	step [68/195], loss=10.5286
	step [69/195], loss=11.0040
	step [70/195], loss=12.2142
	step [71/195], loss=11.6386
	step [72/195], loss=11.7247
	step [73/195], loss=11.1719
	step [74/195], loss=10.1746
	step [75/195], loss=11.5851
	step [76/195], loss=11.4790
	step [77/195], loss=9.7120
	step [78/195], loss=11.3834
	step [79/195], loss=11.3416
	step [80/195], loss=10.7610
	step [81/195], loss=12.8413
	step [82/195], loss=11.2937
	step [83/195], loss=11.2688
	step [84/195], loss=9.9563
	step [85/195], loss=12.0014
	step [86/195], loss=12.4605
	step [87/195], loss=10.2915
	step [88/195], loss=13.2469
	step [89/195], loss=10.8599
	step [90/195], loss=10.8115
	step [91/195], loss=12.0187
	step [92/195], loss=10.6881
	step [93/195], loss=12.8600
	step [94/195], loss=12.1199
	step [95/195], loss=14.4199
	step [96/195], loss=11.1465
	step [97/195], loss=12.4590
	step [98/195], loss=10.4193
	step [99/195], loss=11.5026
	step [100/195], loss=11.4117
	step [101/195], loss=12.3693
	step [102/195], loss=14.3214
	step [103/195], loss=12.4656
	step [104/195], loss=11.5237
	step [105/195], loss=10.1677
	step [106/195], loss=11.3964
	step [107/195], loss=11.2058
	step [108/195], loss=10.5326
	step [109/195], loss=11.7300
	step [110/195], loss=11.7064
	step [111/195], loss=12.0368
	step [112/195], loss=11.9077
	step [113/195], loss=11.5398
	step [114/195], loss=11.7713
	step [115/195], loss=12.6024
	step [116/195], loss=12.5133
	step [117/195], loss=10.5529
	step [118/195], loss=9.8217
	step [119/195], loss=12.6919
	step [120/195], loss=12.2110
	step [121/195], loss=10.9299
	step [122/195], loss=12.4672
	step [123/195], loss=12.3070
	step [124/195], loss=12.0986
	step [125/195], loss=11.7341
	step [126/195], loss=9.2803
	step [127/195], loss=12.5156
	step [128/195], loss=11.3896
	step [129/195], loss=11.8917
	step [130/195], loss=12.0346
	step [131/195], loss=13.0308
	step [132/195], loss=11.3245
	step [133/195], loss=10.2591
	step [134/195], loss=12.1703
	step [135/195], loss=12.2932
	step [136/195], loss=12.2148
	step [137/195], loss=12.7917
	step [138/195], loss=12.4711
	step [139/195], loss=10.9514
	step [140/195], loss=12.3058
	step [141/195], loss=12.1330
	step [142/195], loss=12.3772
	step [143/195], loss=11.6014
	step [144/195], loss=11.5368
	step [145/195], loss=11.4632
	step [146/195], loss=10.5862
	step [147/195], loss=12.8396
	step [148/195], loss=11.4207
	step [149/195], loss=10.5155
	step [150/195], loss=11.5773
	step [151/195], loss=12.6498
	step [152/195], loss=11.7674
	step [153/195], loss=13.4940
	step [154/195], loss=10.8532
	step [155/195], loss=10.3689
	step [156/195], loss=10.8415
	step [157/195], loss=12.5773
	step [158/195], loss=13.5503
	step [159/195], loss=11.3543
	step [160/195], loss=10.8085
	step [161/195], loss=11.2866
	step [162/195], loss=11.8125
	step [163/195], loss=10.7949
	step [164/195], loss=12.2838
	step [165/195], loss=12.2314
	step [166/195], loss=11.7711
	step [167/195], loss=11.2572
	step [168/195], loss=11.2515
	step [169/195], loss=10.7911
	step [170/195], loss=12.8973
	step [171/195], loss=11.8218
	step [172/195], loss=11.8981
	step [173/195], loss=10.0509
	step [174/195], loss=10.8815
	step [175/195], loss=12.7036
	step [176/195], loss=10.1872
	step [177/195], loss=11.0733
	step [178/195], loss=11.1505
	step [179/195], loss=11.3880
	step [180/195], loss=11.7379
	step [181/195], loss=11.4352
	step [182/195], loss=9.7308
	step [183/195], loss=10.7182
	step [184/195], loss=11.2083
	step [185/195], loss=12.2141
	step [186/195], loss=11.7569
	step [187/195], loss=10.9479
	step [188/195], loss=10.4958
	step [189/195], loss=11.0550
	step [190/195], loss=11.6223
	step [191/195], loss=12.7838
	step [192/195], loss=10.4523
	step [193/195], loss=11.0297
	step [194/195], loss=11.3580
	step [195/195], loss=1.4264
	Evaluating
	loss=0.0454, precision=0.2174, recall=0.9967, f1=0.3569
Training epoch 13
	step [1/195], loss=10.4877
	step [2/195], loss=12.3813
	step [3/195], loss=12.2849
	step [4/195], loss=10.2336
	step [5/195], loss=11.1874
	step [6/195], loss=12.1122
	step [7/195], loss=10.1175
	step [8/195], loss=11.4419
	step [9/195], loss=11.9891
	step [10/195], loss=12.8645
	step [11/195], loss=11.7725
	step [12/195], loss=9.3282
	step [13/195], loss=11.7134
	step [14/195], loss=11.2889
	step [15/195], loss=11.2084
	step [16/195], loss=10.3666
	step [17/195], loss=11.6565
	step [18/195], loss=12.0280
	step [19/195], loss=12.2419
	step [20/195], loss=10.6706
	step [21/195], loss=11.0548
	step [22/195], loss=12.4583
	step [23/195], loss=12.0062
	step [24/195], loss=12.2282
	step [25/195], loss=10.4788
	step [26/195], loss=11.7613
	step [27/195], loss=10.7091
	step [28/195], loss=9.6949
	step [29/195], loss=10.2543
	step [30/195], loss=10.9913
	step [31/195], loss=14.3128
	step [32/195], loss=11.1599
	step [33/195], loss=10.8482
	step [34/195], loss=11.4225
	step [35/195], loss=11.2449
	step [36/195], loss=13.0687
	step [37/195], loss=11.3209
	step [38/195], loss=11.1964
	step [39/195], loss=10.7726
	step [40/195], loss=11.2666
	step [41/195], loss=13.2552
	step [42/195], loss=10.8720
	step [43/195], loss=11.1950
	step [44/195], loss=12.9576
	step [45/195], loss=10.5173
	step [46/195], loss=12.8786
	step [47/195], loss=12.2492
	step [48/195], loss=13.0904
	step [49/195], loss=11.2036
	step [50/195], loss=10.5982
	step [51/195], loss=10.5114
	step [52/195], loss=10.1161
	step [53/195], loss=12.0254
	step [54/195], loss=11.2150
	step [55/195], loss=8.9110
	step [56/195], loss=11.6326
	step [57/195], loss=11.4314
	step [58/195], loss=10.0639
	step [59/195], loss=12.3729
	step [60/195], loss=9.0544
	step [61/195], loss=10.6569
	step [62/195], loss=10.2092
	step [63/195], loss=10.5174
	step [64/195], loss=12.1917
	step [65/195], loss=10.5429
	step [66/195], loss=11.2282
	step [67/195], loss=12.1534
	step [68/195], loss=9.8257
	step [69/195], loss=12.3662
	step [70/195], loss=10.6225
	step [71/195], loss=12.8438
	step [72/195], loss=12.7286
	step [73/195], loss=12.3404
	step [74/195], loss=9.8206
	step [75/195], loss=11.2686
	step [76/195], loss=11.4706
	step [77/195], loss=12.1110
	step [78/195], loss=11.9645
	step [79/195], loss=11.1083
	step [80/195], loss=10.4196
	step [81/195], loss=12.7519
	step [82/195], loss=9.9789
	step [83/195], loss=9.9360
	step [84/195], loss=11.2745
	step [85/195], loss=10.5709
	step [86/195], loss=10.5111
	step [87/195], loss=10.6369
	step [88/195], loss=11.4036
	step [89/195], loss=9.4485
	step [90/195], loss=12.1507
	step [91/195], loss=11.8976
	step [92/195], loss=11.7620
	step [93/195], loss=9.9898
	step [94/195], loss=11.5263
	step [95/195], loss=10.6317
	step [96/195], loss=10.7619
	step [97/195], loss=10.3268
	step [98/195], loss=11.5628
	step [99/195], loss=13.7312
	step [100/195], loss=10.5434
	step [101/195], loss=9.0762
	step [102/195], loss=10.4362
	step [103/195], loss=12.0525
	step [104/195], loss=10.9331
	step [105/195], loss=10.8688
	step [106/195], loss=11.2173
	step [107/195], loss=10.1193
	step [108/195], loss=11.5515
	step [109/195], loss=9.9817
	step [110/195], loss=9.3546
	step [111/195], loss=9.5759
	step [112/195], loss=10.4508
	step [113/195], loss=11.9480
	step [114/195], loss=11.5355
	step [115/195], loss=10.8514
	step [116/195], loss=10.5066
	step [117/195], loss=11.1815
	step [118/195], loss=9.8910
	step [119/195], loss=10.3113
	step [120/195], loss=11.6920
	step [121/195], loss=12.0436
	step [122/195], loss=11.9645
	step [123/195], loss=12.7696
	step [124/195], loss=10.4994
	step [125/195], loss=11.1635
	step [126/195], loss=10.7861
	step [127/195], loss=10.4396
	step [128/195], loss=10.9567
	step [129/195], loss=10.0880
	step [130/195], loss=9.9793
	step [131/195], loss=11.5961
	step [132/195], loss=11.8886
	step [133/195], loss=11.3886
	step [134/195], loss=9.8921
	step [135/195], loss=10.0230
	step [136/195], loss=10.0049
	step [137/195], loss=10.0210
	step [138/195], loss=9.8596
	step [139/195], loss=12.1528
	step [140/195], loss=12.9307
	step [141/195], loss=10.8256
	step [142/195], loss=10.6645
	step [143/195], loss=11.1322
	step [144/195], loss=10.9066
	step [145/195], loss=11.3339
	step [146/195], loss=10.6920
	step [147/195], loss=11.0996
	step [148/195], loss=12.3300
	step [149/195], loss=9.7933
	step [150/195], loss=10.9215
	step [151/195], loss=9.9408
	step [152/195], loss=10.4898
	step [153/195], loss=8.9812
	step [154/195], loss=10.0118
	step [155/195], loss=11.6883
	step [156/195], loss=12.1341
	step [157/195], loss=12.1089
	step [158/195], loss=10.3690
	step [159/195], loss=9.9617
	step [160/195], loss=10.8181
	step [161/195], loss=11.3044
	step [162/195], loss=10.9012
	step [163/195], loss=10.7335
	step [164/195], loss=10.2899
	step [165/195], loss=9.7812
	step [166/195], loss=10.5642
	step [167/195], loss=9.2715
	step [168/195], loss=11.0006
	step [169/195], loss=12.3550
	step [170/195], loss=9.8873
	step [171/195], loss=12.2032
	step [172/195], loss=11.2193
	step [173/195], loss=12.1865
	step [174/195], loss=10.4292
	step [175/195], loss=10.1343
	step [176/195], loss=10.5013
	step [177/195], loss=11.1098
	step [178/195], loss=11.9175
	step [179/195], loss=9.8461
	step [180/195], loss=9.3618
	step [181/195], loss=10.6382
	step [182/195], loss=10.9446
	step [183/195], loss=10.5231
	step [184/195], loss=11.8478
	step [185/195], loss=9.7478
	step [186/195], loss=11.6056
	step [187/195], loss=10.1702
	step [188/195], loss=11.1187
	step [189/195], loss=10.8311
	step [190/195], loss=10.0487
	step [191/195], loss=11.6052
	step [192/195], loss=10.0012
	step [193/195], loss=10.1020
	step [194/195], loss=10.3439
	step [195/195], loss=1.1036
	Evaluating
	loss=0.0397, precision=0.2561, recall=0.9957, f1=0.4074
saving model as: 2_saved_model.pth
Training epoch 14
	step [1/195], loss=9.2881
	step [2/195], loss=10.2549
	step [3/195], loss=10.5322
	step [4/195], loss=11.5519
	step [5/195], loss=11.6878
	step [6/195], loss=9.2510
	step [7/195], loss=10.1869
	step [8/195], loss=10.0914
	step [9/195], loss=11.2406
	step [10/195], loss=10.8339
	step [11/195], loss=11.8330
	step [12/195], loss=11.5777
	step [13/195], loss=11.2864
	step [14/195], loss=12.8013
	step [15/195], loss=9.3412
	step [16/195], loss=9.7488
	step [17/195], loss=11.6679
	step [18/195], loss=10.4611
	step [19/195], loss=8.7858
	step [20/195], loss=10.2697
	step [21/195], loss=11.6091
	step [22/195], loss=8.6176
	step [23/195], loss=8.9916
	step [24/195], loss=10.1803
	step [25/195], loss=10.4242
	step [26/195], loss=11.4490
	step [27/195], loss=10.8752
	step [28/195], loss=10.7272
	step [29/195], loss=10.7115
	step [30/195], loss=11.1498
	step [31/195], loss=11.3601
	step [32/195], loss=10.2250
	step [33/195], loss=10.8440
	step [34/195], loss=10.8336
	step [35/195], loss=11.4454
	step [36/195], loss=11.0778
	step [37/195], loss=10.0416
	step [38/195], loss=10.7868
	step [39/195], loss=12.1918
	step [40/195], loss=11.2484
	step [41/195], loss=11.4512
	step [42/195], loss=9.9342
	step [43/195], loss=11.2534
	step [44/195], loss=10.6735
	step [45/195], loss=10.4915
	step [46/195], loss=10.1311
	step [47/195], loss=9.7616
	step [48/195], loss=10.3990
	step [49/195], loss=9.4090
	step [50/195], loss=10.1077
	step [51/195], loss=9.5222
	step [52/195], loss=9.8356
	step [53/195], loss=12.5431
	step [54/195], loss=10.8903
	step [55/195], loss=9.7041
	step [56/195], loss=9.9248
	step [57/195], loss=9.6824
	step [58/195], loss=9.6810
	step [59/195], loss=10.1819
	step [60/195], loss=10.4827
	step [61/195], loss=10.0523
	step [62/195], loss=11.7751
	step [63/195], loss=13.3906
	step [64/195], loss=11.1858
	step [65/195], loss=10.9830
	step [66/195], loss=11.1551
	step [67/195], loss=10.7553
	step [68/195], loss=9.4092
	step [69/195], loss=11.8302
	step [70/195], loss=11.1138
	step [71/195], loss=11.9360
	step [72/195], loss=11.4406
	step [73/195], loss=10.7833
	step [74/195], loss=10.7052
	step [75/195], loss=9.8729
	step [76/195], loss=11.5702
	step [77/195], loss=9.8972
	step [78/195], loss=11.8492
	step [79/195], loss=10.9930
	step [80/195], loss=9.1915
	step [81/195], loss=11.0692
	step [82/195], loss=10.3236
	step [83/195], loss=10.7388
	step [84/195], loss=9.9884
	step [85/195], loss=9.8294
	step [86/195], loss=9.0068
	step [87/195], loss=9.6098
	step [88/195], loss=11.6577
	step [89/195], loss=10.8165
	step [90/195], loss=9.7905
	step [91/195], loss=9.8466
	step [92/195], loss=11.5038
	step [93/195], loss=9.9624
	step [94/195], loss=9.8220
	step [95/195], loss=9.7468
	step [96/195], loss=9.2166
	step [97/195], loss=10.7163
	step [98/195], loss=10.8731
	step [99/195], loss=10.8852
	step [100/195], loss=11.5058
	step [101/195], loss=12.9572
	step [102/195], loss=11.1521
	step [103/195], loss=9.1495
	step [104/195], loss=10.1923
	step [105/195], loss=11.1069
	step [106/195], loss=9.6271
	step [107/195], loss=12.4167
	step [108/195], loss=9.3326
	step [109/195], loss=11.0624
	step [110/195], loss=11.5181
	step [111/195], loss=10.6690
	step [112/195], loss=10.5430
	step [113/195], loss=9.9323
	step [114/195], loss=9.7793
	step [115/195], loss=9.5100
	step [116/195], loss=10.4221
	step [117/195], loss=10.9562
	step [118/195], loss=10.2720
	step [119/195], loss=11.7755
	step [120/195], loss=11.1810
	step [121/195], loss=10.7360
	step [122/195], loss=11.1251
	step [123/195], loss=10.5164
	step [124/195], loss=10.3950
	step [125/195], loss=10.5592
	step [126/195], loss=11.5477
	step [127/195], loss=9.5345
	step [128/195], loss=10.4338
	step [129/195], loss=9.6414
	step [130/195], loss=10.3171
	step [131/195], loss=9.9181
	step [132/195], loss=9.4743
	step [133/195], loss=9.0171
	step [134/195], loss=9.5682
	step [135/195], loss=10.3341
	step [136/195], loss=8.6395
	step [137/195], loss=9.0311
	step [138/195], loss=9.4518
	step [139/195], loss=10.7410
	step [140/195], loss=10.4554
	step [141/195], loss=11.2815
	step [142/195], loss=11.0248
	step [143/195], loss=9.4293
	step [144/195], loss=11.5019
	step [145/195], loss=11.4471
	step [146/195], loss=9.2335
	step [147/195], loss=10.2293
	step [148/195], loss=9.7875
	step [149/195], loss=10.4012
	step [150/195], loss=8.6102
	step [151/195], loss=11.6553
	step [152/195], loss=12.0280
	step [153/195], loss=9.8606
	step [154/195], loss=10.9322
	step [155/195], loss=11.1297
	step [156/195], loss=10.2838
	step [157/195], loss=10.1411
	step [158/195], loss=10.6443
	step [159/195], loss=9.2034
	step [160/195], loss=11.2980
	step [161/195], loss=9.3897
	step [162/195], loss=9.8672
	step [163/195], loss=10.8367
	step [164/195], loss=9.4934
	step [165/195], loss=9.5380
	step [166/195], loss=10.5442
	step [167/195], loss=12.0608
	step [168/195], loss=10.3609
	step [169/195], loss=8.9835
	step [170/195], loss=8.7895
	step [171/195], loss=9.8643
	step [172/195], loss=10.3379
	step [173/195], loss=9.4500
	step [174/195], loss=8.6245
	step [175/195], loss=10.5152
	step [176/195], loss=8.7623
	step [177/195], loss=9.9919
	step [178/195], loss=9.3242
	step [179/195], loss=9.3338
	step [180/195], loss=10.9287
	step [181/195], loss=9.9974
	step [182/195], loss=10.5715
	step [183/195], loss=12.1049
	step [184/195], loss=8.8400
	step [185/195], loss=9.5447
	step [186/195], loss=9.8839
	step [187/195], loss=10.7115
	step [188/195], loss=10.3722
	step [189/195], loss=10.1165
	step [190/195], loss=11.4369
	step [191/195], loss=9.6776
	step [192/195], loss=9.1036
	step [193/195], loss=10.0078
	step [194/195], loss=9.4452
	step [195/195], loss=1.3965
	Evaluating
	loss=0.0406, precision=0.2306, recall=0.9963, f1=0.3745
Training epoch 15
	step [1/195], loss=9.6102
	step [2/195], loss=10.1120
	step [3/195], loss=10.6973
	step [4/195], loss=10.9877
	step [5/195], loss=10.7460
	step [6/195], loss=11.3147
	step [7/195], loss=9.7274
	step [8/195], loss=9.6544
	step [9/195], loss=9.7526
	step [10/195], loss=9.3537
	step [11/195], loss=9.5983
	step [12/195], loss=9.6642
	step [13/195], loss=11.3813
	step [14/195], loss=9.8405
	step [15/195], loss=8.4172
	step [16/195], loss=10.8970
	step [17/195], loss=12.5540
	step [18/195], loss=10.0984
	step [19/195], loss=9.8130
	step [20/195], loss=9.7854
	step [21/195], loss=9.0033
	step [22/195], loss=10.1563
	step [23/195], loss=10.1308
	step [24/195], loss=10.8574
	step [25/195], loss=9.0154
	step [26/195], loss=9.5920
	step [27/195], loss=11.6271
	step [28/195], loss=8.5738
	step [29/195], loss=10.5868
	step [30/195], loss=10.2403
	step [31/195], loss=10.3967
	step [32/195], loss=10.1675
	step [33/195], loss=11.3351
	step [34/195], loss=9.7417
	step [35/195], loss=9.6002
	step [36/195], loss=9.1739
	step [37/195], loss=9.3379
	step [38/195], loss=11.2309
	step [39/195], loss=11.2716
	step [40/195], loss=9.4923
	step [41/195], loss=10.1922
	step [42/195], loss=9.3536
	step [43/195], loss=10.3516
	step [44/195], loss=8.9364
	step [45/195], loss=10.3755
	step [46/195], loss=9.3735
	step [47/195], loss=11.0151
	step [48/195], loss=11.2711
	step [49/195], loss=9.4190
	step [50/195], loss=9.7112
	step [51/195], loss=9.6296
	step [52/195], loss=9.2222
	step [53/195], loss=9.4114
	step [54/195], loss=10.5870
	step [55/195], loss=10.0571
	step [56/195], loss=9.0354
	step [57/195], loss=9.0318
	step [58/195], loss=11.3520
	step [59/195], loss=9.2226
	step [60/195], loss=8.9995
	step [61/195], loss=11.0652
	step [62/195], loss=9.9815
	step [63/195], loss=9.5921
	step [64/195], loss=9.3893
	step [65/195], loss=9.1831
	step [66/195], loss=9.5075
	step [67/195], loss=10.0397
	step [68/195], loss=9.5951
	step [69/195], loss=10.4060
	step [70/195], loss=9.0034
	step [71/195], loss=11.1535
	step [72/195], loss=8.5786
	step [73/195], loss=9.3376
	step [74/195], loss=9.8919
	step [75/195], loss=9.3143
	step [76/195], loss=10.0386
	step [77/195], loss=9.2423
	step [78/195], loss=9.9303
	step [79/195], loss=10.4053
	step [80/195], loss=10.3719
	step [81/195], loss=11.4393
	step [82/195], loss=9.5326
	step [83/195], loss=10.9156
	step [84/195], loss=10.8524
	step [85/195], loss=12.4985
	step [86/195], loss=11.3348
	step [87/195], loss=9.7640
	step [88/195], loss=9.1863
	step [89/195], loss=10.2707
	step [90/195], loss=9.2396
	step [91/195], loss=9.7178
	step [92/195], loss=11.7866
	step [93/195], loss=10.0722
	step [94/195], loss=10.3354
	step [95/195], loss=9.5006
	step [96/195], loss=9.6224
	step [97/195], loss=8.9139
	step [98/195], loss=11.0294
	step [99/195], loss=9.2177
	step [100/195], loss=9.4582
	step [101/195], loss=10.0506
	step [102/195], loss=11.0849
	step [103/195], loss=9.6716
	step [104/195], loss=9.6314
	step [105/195], loss=9.7427
	step [106/195], loss=9.2464
	step [107/195], loss=9.5060
	step [108/195], loss=10.4878
	step [109/195], loss=10.0073
	step [110/195], loss=10.6266
	step [111/195], loss=9.4356
	step [112/195], loss=10.0304
	step [113/195], loss=8.9072
	step [114/195], loss=10.8526
	step [115/195], loss=11.5358
	step [116/195], loss=10.1461
	step [117/195], loss=10.2055
	step [118/195], loss=10.5324
	step [119/195], loss=9.1628
	step [120/195], loss=9.4746
	step [121/195], loss=10.1368
	step [122/195], loss=9.7877
	step [123/195], loss=9.2385
	step [124/195], loss=12.4697
	step [125/195], loss=8.7206
	step [126/195], loss=8.7510
	step [127/195], loss=10.2045
	step [128/195], loss=7.7072
	step [129/195], loss=9.9960
	step [130/195], loss=9.8899
	step [131/195], loss=12.2257
	step [132/195], loss=9.7480
	step [133/195], loss=10.6037
	step [134/195], loss=9.6649
	step [135/195], loss=9.1079
	step [136/195], loss=9.3600
	step [137/195], loss=11.0232
	step [138/195], loss=8.7289
	step [139/195], loss=10.0600
	step [140/195], loss=10.4676
	step [141/195], loss=7.8203
	step [142/195], loss=10.8742
	step [143/195], loss=10.3855
	step [144/195], loss=9.4030
	step [145/195], loss=8.8264
	step [146/195], loss=9.5686
	step [147/195], loss=9.7231
	step [148/195], loss=11.2699
	step [149/195], loss=9.8959
	step [150/195], loss=9.3506
	step [151/195], loss=10.1059
	step [152/195], loss=9.2173
	step [153/195], loss=8.2428
	step [154/195], loss=8.7585
	step [155/195], loss=9.5602
	step [156/195], loss=9.9748
	step [157/195], loss=10.6383
	step [158/195], loss=9.5016
	step [159/195], loss=11.3937
	step [160/195], loss=8.2112
	step [161/195], loss=9.7108
	step [162/195], loss=10.2868
	step [163/195], loss=9.7771
	step [164/195], loss=9.4910
	step [165/195], loss=10.4680
	step [166/195], loss=9.7526
	step [167/195], loss=9.7382
	step [168/195], loss=8.3017
	step [169/195], loss=10.2125
	step [170/195], loss=8.8450
	step [171/195], loss=9.2647
	step [172/195], loss=9.5330
	step [173/195], loss=9.4454
	step [174/195], loss=8.9752
	step [175/195], loss=10.7373
	step [176/195], loss=9.7198
	step [177/195], loss=10.1305
	step [178/195], loss=10.1059
	step [179/195], loss=9.1638
	step [180/195], loss=9.1371
	step [181/195], loss=9.8512
	step [182/195], loss=10.0927
	step [183/195], loss=9.9911
	step [184/195], loss=10.2941
	step [185/195], loss=8.9537
	step [186/195], loss=9.0002
	step [187/195], loss=9.9921
	step [188/195], loss=9.2819
	step [189/195], loss=8.3043
	step [190/195], loss=9.5315
	step [191/195], loss=8.0935
	step [192/195], loss=9.1958
	step [193/195], loss=9.5247
	step [194/195], loss=10.0584
	step [195/195], loss=0.8017
	Evaluating
	loss=0.0340, precision=0.2460, recall=0.9957, f1=0.3945
Training epoch 16
	step [1/195], loss=11.1455
	step [2/195], loss=9.8431
	step [3/195], loss=10.9606
	step [4/195], loss=10.3450
	step [5/195], loss=10.4384
	step [6/195], loss=9.5798
	step [7/195], loss=9.1775
	step [8/195], loss=10.8176
	step [9/195], loss=9.0987
	step [10/195], loss=10.3955
	step [11/195], loss=10.6116
	step [12/195], loss=9.0829
	step [13/195], loss=11.0588
	step [14/195], loss=8.6307
	step [15/195], loss=9.5056
	step [16/195], loss=8.6602
	step [17/195], loss=9.6100
	step [18/195], loss=12.3136
	step [19/195], loss=10.5427
	step [20/195], loss=9.2586
	step [21/195], loss=10.1353
	step [22/195], loss=9.5057
	step [23/195], loss=10.4011
	step [24/195], loss=10.1704
	step [25/195], loss=9.3761
	step [26/195], loss=10.3773
	step [27/195], loss=8.7833
	step [28/195], loss=10.9977
	step [29/195], loss=10.0529
	step [30/195], loss=9.9904
	step [31/195], loss=9.2310
	step [32/195], loss=7.6619
	step [33/195], loss=8.7184
	step [34/195], loss=8.3467
	step [35/195], loss=9.3541
	step [36/195], loss=9.5772
	step [37/195], loss=8.8727
	step [38/195], loss=9.5946
	step [39/195], loss=8.2962
	step [40/195], loss=8.4664
	step [41/195], loss=9.8170
	step [42/195], loss=9.6146
	step [43/195], loss=9.9208
	step [44/195], loss=11.7491
	step [45/195], loss=8.8986
	step [46/195], loss=10.1089
	step [47/195], loss=9.7934
	step [48/195], loss=8.3788
	step [49/195], loss=10.0706
	step [50/195], loss=10.3039
	step [51/195], loss=8.4351
	step [52/195], loss=10.1251
	step [53/195], loss=8.5846
	step [54/195], loss=9.8725
	step [55/195], loss=8.5710
	step [56/195], loss=9.5739
	step [57/195], loss=11.1925
	step [58/195], loss=9.2028
	step [59/195], loss=9.3483
	step [60/195], loss=9.7503
	step [61/195], loss=8.6133
	step [62/195], loss=10.5083
	step [63/195], loss=9.2377
	step [64/195], loss=11.1829
	step [65/195], loss=9.8226
	step [66/195], loss=9.4490
	step [67/195], loss=9.1134
	step [68/195], loss=10.8264
	step [69/195], loss=10.0528
	step [70/195], loss=9.7482
	step [71/195], loss=9.7425
	step [72/195], loss=8.9033
	step [73/195], loss=9.2453
	step [74/195], loss=10.1238
	step [75/195], loss=11.0912
	step [76/195], loss=9.9894
	step [77/195], loss=9.4773
	step [78/195], loss=9.9487
	step [79/195], loss=8.8678
	step [80/195], loss=9.3369
	step [81/195], loss=9.8643
	step [82/195], loss=9.1786
	step [83/195], loss=9.3816
	step [84/195], loss=9.0485
	step [85/195], loss=9.6761
	step [86/195], loss=9.5768
	step [87/195], loss=9.8468
	step [88/195], loss=8.7056
	step [89/195], loss=9.3961
	step [90/195], loss=10.4980
	step [91/195], loss=9.4297
	step [92/195], loss=8.6327
	step [93/195], loss=8.5562
	step [94/195], loss=9.0385
	step [95/195], loss=7.9711
	step [96/195], loss=7.7994
	step [97/195], loss=9.0700
	step [98/195], loss=8.9960
	step [99/195], loss=8.9432
	step [100/195], loss=9.2982
	step [101/195], loss=10.2102
	step [102/195], loss=9.7862
	step [103/195], loss=10.8389
	step [104/195], loss=10.0518
	step [105/195], loss=9.8952
	step [106/195], loss=9.6734
	step [107/195], loss=10.5107
	step [108/195], loss=8.8209
	step [109/195], loss=10.3410
	step [110/195], loss=10.0124
	step [111/195], loss=9.2944
	step [112/195], loss=8.6855
	step [113/195], loss=8.5808
	step [114/195], loss=11.1782
	step [115/195], loss=8.6507
	step [116/195], loss=8.7817
	step [117/195], loss=8.8629
	step [118/195], loss=10.1833
	step [119/195], loss=9.4078
	step [120/195], loss=9.4149
	step [121/195], loss=9.1643
	step [122/195], loss=7.9926
	step [123/195], loss=8.9313
	step [124/195], loss=8.4601
	step [125/195], loss=8.6440
	step [126/195], loss=9.1241
	step [127/195], loss=7.9290
	step [128/195], loss=9.8183
	step [129/195], loss=9.0158
	step [130/195], loss=10.0105
	step [131/195], loss=10.6631
	step [132/195], loss=9.5726
	step [133/195], loss=8.7504
	step [134/195], loss=8.4563
	step [135/195], loss=8.9808
	step [136/195], loss=9.3961
	step [137/195], loss=8.8601
	step [138/195], loss=9.9334
	step [139/195], loss=9.6850
	step [140/195], loss=8.5660
	step [141/195], loss=9.2165
	step [142/195], loss=8.7315
	step [143/195], loss=9.0910
	step [144/195], loss=8.9221
	step [145/195], loss=11.0062
	step [146/195], loss=8.6163
	step [147/195], loss=9.0502
	step [148/195], loss=10.1549
	step [149/195], loss=7.9685
	step [150/195], loss=7.8014
	step [151/195], loss=10.2654
	step [152/195], loss=8.5897
	step [153/195], loss=9.4699
	step [154/195], loss=10.2763
	step [155/195], loss=8.9072
	step [156/195], loss=10.3300
	step [157/195], loss=9.9986
	step [158/195], loss=9.6421
	step [159/195], loss=12.1582
	step [160/195], loss=9.3646
	step [161/195], loss=8.5049
	step [162/195], loss=8.2599
	step [163/195], loss=10.1633
	step [164/195], loss=8.8755
	step [165/195], loss=9.9636
	step [166/195], loss=9.9024
	step [167/195], loss=8.7587
	step [168/195], loss=9.9738
	step [169/195], loss=9.6278
	step [170/195], loss=9.2754
	step [171/195], loss=8.2637
	step [172/195], loss=9.3543
	step [173/195], loss=10.3164
	step [174/195], loss=9.1615
	step [175/195], loss=9.1427
	step [176/195], loss=9.7144
	step [177/195], loss=8.9414
	step [178/195], loss=8.7727
	step [179/195], loss=8.0203
	step [180/195], loss=8.8078
	step [181/195], loss=8.2668
	step [182/195], loss=8.9677
	step [183/195], loss=8.7173
	step [184/195], loss=9.6347
	step [185/195], loss=9.2174
	step [186/195], loss=9.1271
	step [187/195], loss=8.3306
	step [188/195], loss=8.4404
	step [189/195], loss=8.1838
	step [190/195], loss=11.0396
	step [191/195], loss=9.6755
	step [192/195], loss=9.3891
	step [193/195], loss=10.0387
	step [194/195], loss=10.3059
	step [195/195], loss=1.0957
	Evaluating
	loss=0.0322, precision=0.2763, recall=0.9949, f1=0.4325
saving model as: 2_saved_model.pth
Training epoch 17
	step [1/195], loss=8.9683
	step [2/195], loss=9.6977
	step [3/195], loss=7.3951
	step [4/195], loss=8.9084
	step [5/195], loss=8.1213
	step [6/195], loss=9.9496
	step [7/195], loss=9.6780
	step [8/195], loss=9.5006
	step [9/195], loss=9.4252
	step [10/195], loss=9.2741
	step [11/195], loss=9.2140
	step [12/195], loss=8.9348
	step [13/195], loss=9.4369
	step [14/195], loss=9.2973
	step [15/195], loss=10.0529
	step [16/195], loss=9.0505
	step [17/195], loss=9.2336
	step [18/195], loss=10.0173
	step [19/195], loss=9.6307
	step [20/195], loss=7.4104
	step [21/195], loss=10.2120
	step [22/195], loss=10.6008
	step [23/195], loss=8.6933
	step [24/195], loss=8.4045
	step [25/195], loss=8.7000
	step [26/195], loss=8.2637
	step [27/195], loss=11.0549
	step [28/195], loss=8.6838
	step [29/195], loss=7.7321
	step [30/195], loss=9.0552
	step [31/195], loss=9.3909
	step [32/195], loss=8.8170
	step [33/195], loss=8.9212
	step [34/195], loss=9.6832
	step [35/195], loss=10.1139
	step [36/195], loss=9.1663
	step [37/195], loss=9.8061
	step [38/195], loss=10.0126
	step [39/195], loss=8.5397
	step [40/195], loss=10.9076
	step [41/195], loss=8.9118
	step [42/195], loss=10.0419
	step [43/195], loss=8.8469
	step [44/195], loss=10.0591
	step [45/195], loss=7.8338
	step [46/195], loss=9.1643
	step [47/195], loss=10.1617
	step [48/195], loss=7.7618
	step [49/195], loss=8.9226
	step [50/195], loss=8.3748
	step [51/195], loss=8.6388
	step [52/195], loss=8.5942
	step [53/195], loss=8.6362
	step [54/195], loss=7.4817
	step [55/195], loss=10.3890
	step [56/195], loss=8.4195
	step [57/195], loss=8.1540
	step [58/195], loss=9.6104
	step [59/195], loss=9.1658
	step [60/195], loss=8.9940
	step [61/195], loss=8.3655
	step [62/195], loss=9.2039
	step [63/195], loss=9.5444
	step [64/195], loss=9.7134
	step [65/195], loss=8.9773
	step [66/195], loss=7.9513
	step [67/195], loss=8.9072
	step [68/195], loss=9.6552
	step [69/195], loss=10.2482
	step [70/195], loss=8.2323
	step [71/195], loss=7.3257
	step [72/195], loss=9.5600
	step [73/195], loss=8.6933
	step [74/195], loss=10.3253
	step [75/195], loss=10.1253
	step [76/195], loss=9.2534
	step [77/195], loss=9.2954
	step [78/195], loss=9.1334
	step [79/195], loss=9.8191
	step [80/195], loss=8.8167
	step [81/195], loss=8.9706
	step [82/195], loss=9.8210
	step [83/195], loss=9.0127
	step [84/195], loss=9.8765
	step [85/195], loss=7.7373
	step [86/195], loss=8.2953
	step [87/195], loss=8.4074
	step [88/195], loss=9.6355
	step [89/195], loss=10.1924
	step [90/195], loss=10.7023
	step [91/195], loss=10.6942
	step [92/195], loss=9.4439
	step [93/195], loss=8.4005
	step [94/195], loss=9.2433
	step [95/195], loss=8.8519
	step [96/195], loss=9.4862
	step [97/195], loss=9.5624
	step [98/195], loss=9.5419
	step [99/195], loss=9.4769
	step [100/195], loss=10.6744
	step [101/195], loss=10.4851
	step [102/195], loss=9.5732
	step [103/195], loss=8.4720
	step [104/195], loss=8.6374
	step [105/195], loss=8.1211
	step [106/195], loss=9.5393
	step [107/195], loss=9.5317
	step [108/195], loss=9.6939
	step [109/195], loss=7.4238
	step [110/195], loss=7.6238
	step [111/195], loss=10.3145
	step [112/195], loss=9.2863
	step [113/195], loss=9.5788
	step [114/195], loss=9.6918
	step [115/195], loss=8.3672
	step [116/195], loss=9.6025
	step [117/195], loss=8.4396
	step [118/195], loss=8.3871
	step [119/195], loss=8.7025
	step [120/195], loss=8.5882
	step [121/195], loss=8.7262
	step [122/195], loss=8.0006
	step [123/195], loss=7.9853
	step [124/195], loss=8.6101
	step [125/195], loss=9.7358
	step [126/195], loss=7.6539
	step [127/195], loss=9.5683
	step [128/195], loss=8.1242
	step [129/195], loss=10.2871
	step [130/195], loss=9.7812
	step [131/195], loss=8.7896
	step [132/195], loss=9.0754
	step [133/195], loss=8.9720
	step [134/195], loss=9.5320
	step [135/195], loss=9.7132
	step [136/195], loss=7.5450
	step [137/195], loss=8.2117
	step [138/195], loss=10.2019
	step [139/195], loss=10.5739
	step [140/195], loss=7.9968
	step [141/195], loss=7.7104
	step [142/195], loss=7.8241
	step [143/195], loss=8.1361
	step [144/195], loss=7.9104
	step [145/195], loss=8.9890
	step [146/195], loss=7.5577
	step [147/195], loss=8.6048
	step [148/195], loss=9.0694
	step [149/195], loss=7.6791
	step [150/195], loss=9.3616
	step [151/195], loss=7.7260
	step [152/195], loss=9.5987
	step [153/195], loss=8.5202
	step [154/195], loss=9.4577
	step [155/195], loss=9.7382
	step [156/195], loss=7.9387
	step [157/195], loss=9.7457
	step [158/195], loss=8.1132
	step [159/195], loss=9.2912
	step [160/195], loss=9.3222
	step [161/195], loss=9.3054
	step [162/195], loss=8.5003
	step [163/195], loss=8.2826
	step [164/195], loss=9.6814
	step [165/195], loss=9.2620
	step [166/195], loss=8.4024
	step [167/195], loss=9.0027
	step [168/195], loss=8.8914
	step [169/195], loss=8.0635
	step [170/195], loss=7.8512
	step [171/195], loss=9.9103
	step [172/195], loss=8.1714
	step [173/195], loss=9.6942
	step [174/195], loss=8.5747
	step [175/195], loss=8.7910
	step [176/195], loss=8.7294
	step [177/195], loss=9.6773
	step [178/195], loss=8.6280
	step [179/195], loss=11.9164
	step [180/195], loss=8.2940
	step [181/195], loss=9.5835
	step [182/195], loss=8.7901
	step [183/195], loss=8.6346
	step [184/195], loss=10.0006
	step [185/195], loss=9.1527
	step [186/195], loss=9.9069
	step [187/195], loss=8.6980
	step [188/195], loss=8.2299
	step [189/195], loss=9.1322
	step [190/195], loss=8.4933
	step [191/195], loss=6.9545
	step [192/195], loss=8.7331
	step [193/195], loss=9.0757
	step [194/195], loss=7.6610
	step [195/195], loss=1.7367
	Evaluating
	loss=0.0308, precision=0.2694, recall=0.9952, f1=0.4241
Training epoch 18
	step [1/195], loss=8.6496
	step [2/195], loss=9.6140
	step [3/195], loss=9.2727
	step [4/195], loss=9.6225
	step [5/195], loss=8.2847
	step [6/195], loss=8.6878
	step [7/195], loss=8.2342
	step [8/195], loss=9.3798
	step [9/195], loss=9.0091
	step [10/195], loss=8.3831
	step [11/195], loss=9.0512
	step [12/195], loss=6.9577
	step [13/195], loss=8.1443
	step [14/195], loss=7.6050
	step [15/195], loss=8.8205
	step [16/195], loss=9.1118
	step [17/195], loss=8.3789
	step [18/195], loss=9.5946
	step [19/195], loss=8.0240
	step [20/195], loss=8.7737
	step [21/195], loss=8.0688
	step [22/195], loss=8.0218
	step [23/195], loss=8.7745
	step [24/195], loss=8.9425
	step [25/195], loss=8.5608
	step [26/195], loss=7.1184
	step [27/195], loss=7.8989
	step [28/195], loss=8.8334
	step [29/195], loss=7.5977
	step [30/195], loss=8.4504
	step [31/195], loss=8.4429
	step [32/195], loss=9.8037
	step [33/195], loss=9.4300
	step [34/195], loss=9.5291
	step [35/195], loss=10.4560
	step [36/195], loss=9.1588
	step [37/195], loss=7.3956
	step [38/195], loss=9.0358
	step [39/195], loss=8.3304
	step [40/195], loss=8.7866
	step [41/195], loss=8.0831
	step [42/195], loss=9.7044
	step [43/195], loss=8.1160
	step [44/195], loss=8.7048
	step [45/195], loss=9.8848
	step [46/195], loss=9.1444
	step [47/195], loss=7.7814
	step [48/195], loss=9.4311
	step [49/195], loss=8.9897
	step [50/195], loss=8.7753
	step [51/195], loss=8.6691
	step [52/195], loss=9.2718
	step [53/195], loss=9.5402
	step [54/195], loss=9.3180
	step [55/195], loss=9.0925
	step [56/195], loss=8.1658
	step [57/195], loss=8.4658
	step [58/195], loss=9.0829
	step [59/195], loss=7.7940
	step [60/195], loss=7.3563
	step [61/195], loss=9.4923
	step [62/195], loss=7.8099
	step [63/195], loss=10.4700
	step [64/195], loss=10.1907
	step [65/195], loss=9.6135
	step [66/195], loss=7.1034
	step [67/195], loss=9.7782
	step [68/195], loss=7.9961
	step [69/195], loss=8.0023
	step [70/195], loss=10.7912
	step [71/195], loss=9.5659
	step [72/195], loss=7.8833
	step [73/195], loss=9.4304
	step [74/195], loss=8.5895
	step [75/195], loss=8.8693
	step [76/195], loss=6.8354
	step [77/195], loss=9.0408
	step [78/195], loss=8.5052
	step [79/195], loss=11.0496
	step [80/195], loss=8.2734
	step [81/195], loss=8.0736
	step [82/195], loss=8.8653
	step [83/195], loss=10.0757
	step [84/195], loss=8.1261
	step [85/195], loss=7.7861
	step [86/195], loss=8.7302
	step [87/195], loss=8.2986
	step [88/195], loss=8.8510
	step [89/195], loss=9.2744
	step [90/195], loss=10.1622
	step [91/195], loss=9.0518
	step [92/195], loss=8.0415
	step [93/195], loss=8.3905
	step [94/195], loss=8.6754
	step [95/195], loss=8.4790
	step [96/195], loss=9.1524
	step [97/195], loss=7.8514
	step [98/195], loss=10.2302
	step [99/195], loss=7.9416
	step [100/195], loss=8.6814
	step [101/195], loss=7.6883
	step [102/195], loss=8.5491
	step [103/195], loss=8.1687
	step [104/195], loss=8.0012
	step [105/195], loss=7.4220
	step [106/195], loss=9.2652
	step [107/195], loss=8.4878
	step [108/195], loss=8.8032
	step [109/195], loss=8.2763
	step [110/195], loss=7.5969
	step [111/195], loss=9.8573
	step [112/195], loss=8.2351
	step [113/195], loss=8.7197
	step [114/195], loss=7.3974
	step [115/195], loss=8.4213
	step [116/195], loss=6.9949
	step [117/195], loss=8.2173
	step [118/195], loss=10.8982
	step [119/195], loss=7.7300
	step [120/195], loss=7.8855
	step [121/195], loss=8.4247
	step [122/195], loss=8.6595
	step [123/195], loss=8.3414
	step [124/195], loss=10.0732
	step [125/195], loss=9.4991
	step [126/195], loss=7.8548
	step [127/195], loss=7.3905
	step [128/195], loss=9.0790
	step [129/195], loss=8.3808
	step [130/195], loss=6.8791
	step [131/195], loss=8.6376
	step [132/195], loss=7.1071
	step [133/195], loss=8.7326
	step [134/195], loss=8.8131
	step [135/195], loss=8.2501
	step [136/195], loss=7.7548
	step [137/195], loss=8.1236
	step [138/195], loss=8.0136
	step [139/195], loss=9.0222
	step [140/195], loss=9.2735
	step [141/195], loss=9.5801
	step [142/195], loss=6.5949
	step [143/195], loss=8.9422
	step [144/195], loss=7.9479
	step [145/195], loss=7.3154
	step [146/195], loss=8.5791
	step [147/195], loss=9.8568
	step [148/195], loss=8.9717
	step [149/195], loss=8.2202
	step [150/195], loss=10.2535
	step [151/195], loss=7.8402
	step [152/195], loss=8.7651
	step [153/195], loss=8.9790
	step [154/195], loss=10.5762
	step [155/195], loss=8.9167
	step [156/195], loss=7.8751
	step [157/195], loss=9.0787
	step [158/195], loss=10.0532
	step [159/195], loss=7.5269
	step [160/195], loss=8.7386
	step [161/195], loss=8.4425
	step [162/195], loss=9.3996
	step [163/195], loss=7.3365
	step [164/195], loss=9.8977
	step [165/195], loss=8.5478
	step [166/195], loss=8.0781
	step [167/195], loss=8.4098
	step [168/195], loss=9.3198
	step [169/195], loss=9.5666
	step [170/195], loss=8.5531
	step [171/195], loss=8.7440
	step [172/195], loss=8.5958
	step [173/195], loss=9.1629
	step [174/195], loss=7.5120
	step [175/195], loss=8.5733
	step [176/195], loss=8.2224
	step [177/195], loss=8.1972
	step [178/195], loss=9.0588
	step [179/195], loss=9.1687
	step [180/195], loss=7.5250
	step [181/195], loss=8.2157
	step [182/195], loss=8.7840
	step [183/195], loss=8.3882
	step [184/195], loss=11.9851
	step [185/195], loss=10.1580
	step [186/195], loss=7.3003
	step [187/195], loss=8.6704
	step [188/195], loss=7.9998
	step [189/195], loss=8.3240
	step [190/195], loss=7.6256
	step [191/195], loss=8.9297
	step [192/195], loss=9.0778
	step [193/195], loss=7.3256
	step [194/195], loss=7.5328
	step [195/195], loss=0.7438
	Evaluating
	loss=0.0358, precision=0.2310, recall=0.9961, f1=0.3751
Training epoch 19
	step [1/195], loss=9.5021
	step [2/195], loss=9.1248
	step [3/195], loss=7.9219
	step [4/195], loss=8.9644
	step [5/195], loss=8.0309
	step [6/195], loss=8.2366
	step [7/195], loss=8.9065
	step [8/195], loss=9.3126
	step [9/195], loss=7.9590
	step [10/195], loss=7.9762
	step [11/195], loss=9.0317
	step [12/195], loss=8.9759
	step [13/195], loss=8.1416
	step [14/195], loss=9.9526
	step [15/195], loss=7.7458
	step [16/195], loss=9.0038
	step [17/195], loss=8.1479
	step [18/195], loss=8.2725
	step [19/195], loss=8.4998
	step [20/195], loss=7.2150
	step [21/195], loss=8.3933
	step [22/195], loss=9.5099
	step [23/195], loss=7.7640
	step [24/195], loss=9.2537
	step [25/195], loss=7.9349
	step [26/195], loss=8.4849
	step [27/195], loss=9.1574
	step [28/195], loss=7.8549
	step [29/195], loss=8.9732
	step [30/195], loss=8.8772
	step [31/195], loss=8.8820
	step [32/195], loss=9.3608
	step [33/195], loss=9.0221
	step [34/195], loss=7.5325
	step [35/195], loss=8.3860
	step [36/195], loss=7.8703
	step [37/195], loss=9.8216
	step [38/195], loss=9.0665
	step [39/195], loss=8.8584
	step [40/195], loss=6.9958
	step [41/195], loss=8.7253
	step [42/195], loss=8.3739
	step [43/195], loss=9.6450
	step [44/195], loss=8.9204
	step [45/195], loss=7.8509
	step [46/195], loss=6.8038
	step [47/195], loss=8.7365
	step [48/195], loss=7.6321
	step [49/195], loss=8.0836
	step [50/195], loss=10.2941
	step [51/195], loss=9.2454
	step [52/195], loss=9.4788
	step [53/195], loss=9.2372
	step [54/195], loss=8.4523
	step [55/195], loss=8.9998
	step [56/195], loss=8.4453
	step [57/195], loss=8.8143
	step [58/195], loss=7.2995
	step [59/195], loss=8.1365
	step [60/195], loss=9.1419
	step [61/195], loss=7.4716
	step [62/195], loss=7.6952
	step [63/195], loss=7.5890
	step [64/195], loss=7.4234
	step [65/195], loss=9.0490
	step [66/195], loss=8.8216
	step [67/195], loss=8.9856
	step [68/195], loss=7.6855
	step [69/195], loss=9.1449
	step [70/195], loss=9.5025
	step [71/195], loss=8.6690
	step [72/195], loss=8.1444
	step [73/195], loss=10.0109
	step [74/195], loss=8.4308
	step [75/195], loss=8.6888
	step [76/195], loss=7.9494
	step [77/195], loss=9.5480
	step [78/195], loss=8.4007
	step [79/195], loss=9.1156
	step [80/195], loss=8.2600
	step [81/195], loss=7.9254
	step [82/195], loss=8.0002
	step [83/195], loss=8.2240
	step [84/195], loss=7.5899
	step [85/195], loss=8.7759
	step [86/195], loss=8.1219
	step [87/195], loss=8.5314
	step [88/195], loss=7.4934
	step [89/195], loss=7.3305
	step [90/195], loss=7.0617
	step [91/195], loss=8.4419
	step [92/195], loss=8.9564
	step [93/195], loss=7.3073
	step [94/195], loss=7.5779
	step [95/195], loss=7.4153
	step [96/195], loss=7.9265
	step [97/195], loss=8.2958
	step [98/195], loss=8.7286
	step [99/195], loss=6.4624
	step [100/195], loss=8.2774
	step [101/195], loss=7.3067
	step [102/195], loss=8.6686
	step [103/195], loss=7.7527
	step [104/195], loss=8.4790
	step [105/195], loss=7.6123
	step [106/195], loss=11.1281
	step [107/195], loss=8.8933
	step [108/195], loss=7.3959
	step [109/195], loss=9.7216
	step [110/195], loss=8.0673
	step [111/195], loss=8.3155
	step [112/195], loss=8.1390
	step [113/195], loss=9.1709
	step [114/195], loss=9.0985
	step [115/195], loss=7.0924
	step [116/195], loss=9.1300
	step [117/195], loss=7.5193
	step [118/195], loss=8.0168
	step [119/195], loss=7.9211
	step [120/195], loss=9.3094
	step [121/195], loss=7.6266
	step [122/195], loss=8.9294
	step [123/195], loss=7.3098
	step [124/195], loss=8.7430
	step [125/195], loss=6.9964
	step [126/195], loss=8.8537
	step [127/195], loss=8.7313
	step [128/195], loss=7.8336
	step [129/195], loss=7.0997
	step [130/195], loss=7.9735
	step [131/195], loss=8.6409
	step [132/195], loss=7.7528
	step [133/195], loss=9.8782
	step [134/195], loss=7.7535
	step [135/195], loss=8.1844
	step [136/195], loss=10.6935
	step [137/195], loss=8.6423
	step [138/195], loss=8.6033
	step [139/195], loss=7.7628
	step [140/195], loss=8.9985
	step [141/195], loss=9.6050
	step [142/195], loss=9.6137
	step [143/195], loss=9.6203
	step [144/195], loss=8.3658
	step [145/195], loss=6.5340
	step [146/195], loss=8.7827
	step [147/195], loss=8.6509
	step [148/195], loss=7.9183
	step [149/195], loss=7.7201
	step [150/195], loss=8.9550
	step [151/195], loss=7.8101
	step [152/195], loss=6.6015
	step [153/195], loss=8.5002
	step [154/195], loss=8.3577
	step [155/195], loss=8.3518
	step [156/195], loss=7.7078
	step [157/195], loss=8.6003
	step [158/195], loss=9.1907
	step [159/195], loss=9.6498
	step [160/195], loss=9.4764
	step [161/195], loss=8.6799
	step [162/195], loss=7.9836
	step [163/195], loss=9.0206
	step [164/195], loss=8.1160
	step [165/195], loss=8.4856
	step [166/195], loss=8.6980
	step [167/195], loss=6.9283
	step [168/195], loss=7.7375
	step [169/195], loss=6.4952
	step [170/195], loss=7.8722
	step [171/195], loss=7.3477
	step [172/195], loss=8.9093
	step [173/195], loss=8.7110
	step [174/195], loss=9.2456
	step [175/195], loss=8.3739
	step [176/195], loss=6.8872
	step [177/195], loss=8.4045
	step [178/195], loss=8.4276
	step [179/195], loss=10.4214
	step [180/195], loss=7.7986
	step [181/195], loss=8.2049
	step [182/195], loss=6.7734
	step [183/195], loss=6.8993
	step [184/195], loss=6.7253
	step [185/195], loss=8.0402
	step [186/195], loss=9.2972
	step [187/195], loss=8.0017
	step [188/195], loss=7.3627
	step [189/195], loss=7.3530
	step [190/195], loss=8.0294
	step [191/195], loss=8.9637
	step [192/195], loss=8.6878
	step [193/195], loss=6.5052
	step [194/195], loss=7.4205
	step [195/195], loss=0.8323
	Evaluating
	loss=0.0386, precision=0.2090, recall=0.9966, f1=0.3455
Training epoch 20
	step [1/195], loss=10.0344
	step [2/195], loss=8.4662
	step [3/195], loss=7.8930
	step [4/195], loss=8.4804
	step [5/195], loss=7.7205
	step [6/195], loss=7.2043
	step [7/195], loss=7.4809
	step [8/195], loss=6.8059
	step [9/195], loss=7.4416
	step [10/195], loss=8.8024
	step [11/195], loss=9.5379
	step [12/195], loss=8.0976
	step [13/195], loss=8.8613
	step [14/195], loss=7.2412
	step [15/195], loss=6.8141
	step [16/195], loss=7.3445
	step [17/195], loss=7.8949
	step [18/195], loss=7.4276
	step [19/195], loss=9.5171
	step [20/195], loss=9.9126
	step [21/195], loss=8.4559
	step [22/195], loss=7.5791
	step [23/195], loss=6.8437
	step [24/195], loss=9.0481
	step [25/195], loss=9.2888
	step [26/195], loss=7.6568
	step [27/195], loss=7.5072
	step [28/195], loss=7.0001
	step [29/195], loss=8.7709
	step [30/195], loss=9.3431
	step [31/195], loss=8.4617
	step [32/195], loss=8.0432
	step [33/195], loss=6.6600
	step [34/195], loss=8.8910
	step [35/195], loss=7.5057
	step [36/195], loss=7.7115
	step [37/195], loss=8.2611
	step [38/195], loss=7.3229
	step [39/195], loss=9.4651
	step [40/195], loss=8.7533
	step [41/195], loss=7.8446
	step [42/195], loss=7.3624
	step [43/195], loss=6.9707
	step [44/195], loss=9.3794
	step [45/195], loss=7.7570
	step [46/195], loss=8.1879
	step [47/195], loss=8.2075
	step [48/195], loss=6.8925
	step [49/195], loss=8.2870
	step [50/195], loss=9.0837
	step [51/195], loss=7.9798
	step [52/195], loss=9.4024
	step [53/195], loss=7.9085
	step [54/195], loss=7.4733
	step [55/195], loss=6.7080
	step [56/195], loss=7.6957
	step [57/195], loss=7.1448
	step [58/195], loss=8.3604
	step [59/195], loss=6.5496
	step [60/195], loss=7.3473
	step [61/195], loss=9.8517
	step [62/195], loss=6.9116
	step [63/195], loss=7.5336
	step [64/195], loss=6.9986
	step [65/195], loss=7.4356
	step [66/195], loss=8.6137
	step [67/195], loss=8.1695
	step [68/195], loss=8.2119
	step [69/195], loss=7.5930
	step [70/195], loss=7.8093
	step [71/195], loss=8.0228
	step [72/195], loss=8.5261
	step [73/195], loss=9.4439
	step [74/195], loss=7.2866
	step [75/195], loss=8.1625
	step [76/195], loss=8.3865
	step [77/195], loss=7.5999
	step [78/195], loss=7.4671
	step [79/195], loss=8.7299
	step [80/195], loss=7.0955
	step [81/195], loss=7.4190
	step [82/195], loss=7.9859
	step [83/195], loss=8.5353
	step [84/195], loss=8.6341
	step [85/195], loss=7.7780
	step [86/195], loss=8.6585
	step [87/195], loss=7.9082
	step [88/195], loss=8.8081
	step [89/195], loss=8.1132
	step [90/195], loss=8.3997
	step [91/195], loss=7.3883
	step [92/195], loss=8.7871
	step [93/195], loss=8.4110
	step [94/195], loss=8.3293
	step [95/195], loss=6.9276
	step [96/195], loss=6.8188
	step [97/195], loss=7.8566
	step [98/195], loss=8.5790
	step [99/195], loss=8.6995
	step [100/195], loss=8.6635
	step [101/195], loss=8.1765
	step [102/195], loss=7.4232
	step [103/195], loss=8.3178
	step [104/195], loss=7.5769
	step [105/195], loss=7.0079
	step [106/195], loss=7.1738
	step [107/195], loss=8.8052
	step [108/195], loss=6.4674
	step [109/195], loss=7.6378
	step [110/195], loss=7.6420
	step [111/195], loss=7.0833
	step [112/195], loss=7.9089
	step [113/195], loss=9.0792
	step [114/195], loss=7.5791
	step [115/195], loss=8.2032
	step [116/195], loss=9.2376
	step [117/195], loss=9.1266
	step [118/195], loss=7.9119
	step [119/195], loss=7.4195
	step [120/195], loss=8.6751
	step [121/195], loss=8.9596
	step [122/195], loss=8.9903
	step [123/195], loss=8.7048
	step [124/195], loss=7.6744
	step [125/195], loss=8.1805
	step [126/195], loss=7.2702
	step [127/195], loss=7.8616
	step [128/195], loss=9.2042
	step [129/195], loss=7.7355
	step [130/195], loss=9.6055
	step [131/195], loss=8.3905
	step [132/195], loss=8.7048
	step [133/195], loss=10.1492
	step [134/195], loss=7.7312
	step [135/195], loss=7.8768
	step [136/195], loss=7.4852
	step [137/195], loss=8.6437
	step [138/195], loss=8.2662
	step [139/195], loss=7.7173
	step [140/195], loss=8.3893
	step [141/195], loss=7.8657
	step [142/195], loss=6.9756
	step [143/195], loss=6.8799
	step [144/195], loss=8.3602
	step [145/195], loss=7.1341
	step [146/195], loss=9.3951
	step [147/195], loss=6.9428
	step [148/195], loss=8.1349
	step [149/195], loss=8.0991
	step [150/195], loss=7.5607
	step [151/195], loss=8.2882
	step [152/195], loss=7.8015
	step [153/195], loss=8.1446
	step [154/195], loss=7.4530
	step [155/195], loss=6.4756
	step [156/195], loss=7.5224
	step [157/195], loss=8.2629
	step [158/195], loss=7.4125
	step [159/195], loss=7.8212
	step [160/195], loss=7.3124
	step [161/195], loss=9.3264
	step [162/195], loss=7.5193
	step [163/195], loss=8.2919
	step [164/195], loss=7.2770
	step [165/195], loss=8.9737
	step [166/195], loss=8.6038
	step [167/195], loss=7.8239
	step [168/195], loss=7.1751
	step [169/195], loss=7.2737
	step [170/195], loss=7.9529
	step [171/195], loss=7.1958
	step [172/195], loss=8.0604
	step [173/195], loss=7.5949
	step [174/195], loss=7.4390
	step [175/195], loss=7.3367
	step [176/195], loss=8.5349
	step [177/195], loss=7.8372
	step [178/195], loss=7.6736
	step [179/195], loss=6.9765
	step [180/195], loss=7.0079
	step [181/195], loss=8.6865
	step [182/195], loss=9.1928
	step [183/195], loss=9.0508
	step [184/195], loss=7.5707
	step [185/195], loss=8.5880
	step [186/195], loss=7.8295
	step [187/195], loss=7.7180
	step [188/195], loss=8.1052
	step [189/195], loss=7.9087
	step [190/195], loss=8.0999
	step [191/195], loss=6.5976
	step [192/195], loss=7.2195
	step [193/195], loss=10.3250
	step [194/195], loss=7.3550
	step [195/195], loss=0.6684
	Evaluating
	loss=0.0311, precision=0.2509, recall=0.9954, f1=0.4007
Training epoch 21
	step [1/195], loss=8.5212
	step [2/195], loss=8.4581
	step [3/195], loss=8.5983
	step [4/195], loss=6.6532
	step [5/195], loss=6.8845
	step [6/195], loss=8.5229
	step [7/195], loss=7.2017
	step [8/195], loss=8.9247
	step [9/195], loss=7.8919
	step [10/195], loss=7.7813
	step [11/195], loss=9.4862
	step [12/195], loss=8.7268
	step [13/195], loss=9.3300
	step [14/195], loss=7.3564
	step [15/195], loss=8.3439
	step [16/195], loss=8.8583
	step [17/195], loss=8.6276
	step [18/195], loss=10.1194
	step [19/195], loss=8.2636
	step [20/195], loss=8.0625
	step [21/195], loss=7.3853
	step [22/195], loss=7.6539
	step [23/195], loss=6.6068
	step [24/195], loss=6.9090
	step [25/195], loss=6.2007
	step [26/195], loss=7.0276
	step [27/195], loss=7.6951
	step [28/195], loss=7.9086
	step [29/195], loss=9.0155
	step [30/195], loss=6.8759
	step [31/195], loss=7.2933
	step [32/195], loss=6.7940
	step [33/195], loss=8.4918
	step [34/195], loss=7.6035
	step [35/195], loss=7.5093
	step [36/195], loss=7.8442
	step [37/195], loss=8.3100
	step [38/195], loss=8.4050
	step [39/195], loss=7.8293
	step [40/195], loss=7.1351
	step [41/195], loss=7.5394
	step [42/195], loss=8.3841
	step [43/195], loss=7.6169
	step [44/195], loss=7.0133
	step [45/195], loss=8.9867
	step [46/195], loss=9.4272
	step [47/195], loss=7.9723
	step [48/195], loss=8.2845
	step [49/195], loss=6.9301
	step [50/195], loss=7.2745
	step [51/195], loss=7.8799
	step [52/195], loss=6.8390
	step [53/195], loss=8.2302
	step [54/195], loss=8.1104
	step [55/195], loss=8.9469
	step [56/195], loss=7.6789
	step [57/195], loss=7.8390
	step [58/195], loss=7.4847
	step [59/195], loss=7.3939
	step [60/195], loss=6.9526
	step [61/195], loss=8.1620
	step [62/195], loss=7.4966
	step [63/195], loss=7.9842
	step [64/195], loss=7.4433
	step [65/195], loss=8.4396
	step [66/195], loss=8.3996
	step [67/195], loss=7.2988
	step [68/195], loss=8.7767
	step [69/195], loss=8.1277
	step [70/195], loss=7.0290
	step [71/195], loss=8.3971
	step [72/195], loss=8.2472
	step [73/195], loss=7.8408
	step [74/195], loss=8.6486
	step [75/195], loss=7.9919
	step [76/195], loss=7.2617
	step [77/195], loss=7.0228
	step [78/195], loss=7.5928
	step [79/195], loss=9.1265
	step [80/195], loss=8.1488
	step [81/195], loss=8.2483
	step [82/195], loss=7.2443
	step [83/195], loss=8.3229
	step [84/195], loss=7.4277
	step [85/195], loss=9.1291
	step [86/195], loss=6.7780
	step [87/195], loss=8.4793
	step [88/195], loss=6.9767
	step [89/195], loss=7.8579
	step [90/195], loss=7.9930
	step [91/195], loss=6.6890
	step [92/195], loss=6.9967
	step [93/195], loss=6.9646
	step [94/195], loss=7.0317
	step [95/195], loss=9.4175
	step [96/195], loss=7.0332
	step [97/195], loss=8.5458
	step [98/195], loss=6.6132
	step [99/195], loss=7.3390
	step [100/195], loss=6.8569
	step [101/195], loss=7.8863
	step [102/195], loss=8.0907
	step [103/195], loss=7.0468
	step [104/195], loss=8.2426
	step [105/195], loss=8.9607
	step [106/195], loss=7.6751
	step [107/195], loss=6.8425
	step [108/195], loss=7.9368
	step [109/195], loss=7.7449
	step [110/195], loss=9.6152
	step [111/195], loss=8.1556
	step [112/195], loss=7.1398
	step [113/195], loss=8.3546
	step [114/195], loss=7.4642
	step [115/195], loss=7.6231
	step [116/195], loss=6.2331
	step [117/195], loss=7.3660
	step [118/195], loss=7.3037
	step [119/195], loss=8.5156
	step [120/195], loss=7.4933
	step [121/195], loss=8.7590
	step [122/195], loss=8.0552
	step [123/195], loss=7.7662
	step [124/195], loss=7.5311
	step [125/195], loss=8.3178
	step [126/195], loss=7.3635
	step [127/195], loss=8.3821
	step [128/195], loss=6.8334
	step [129/195], loss=6.6131
	step [130/195], loss=8.5173
	step [131/195], loss=8.1632
	step [132/195], loss=6.8606
	step [133/195], loss=9.3894
	step [134/195], loss=9.6957
	step [135/195], loss=6.9947
	step [136/195], loss=6.7755
	step [137/195], loss=7.8854
	step [138/195], loss=6.8367
	step [139/195], loss=6.6235
	step [140/195], loss=8.3919
	step [141/195], loss=7.5958
	step [142/195], loss=6.4427
	step [143/195], loss=9.4258
	step [144/195], loss=7.7320
	step [145/195], loss=6.0045
	step [146/195], loss=7.3422
	step [147/195], loss=7.7074
	step [148/195], loss=6.7325
	step [149/195], loss=9.5816
	step [150/195], loss=6.9881
	step [151/195], loss=7.7168
	step [152/195], loss=8.1830
	step [153/195], loss=7.9964
	step [154/195], loss=7.3498
	step [155/195], loss=7.8696
	step [156/195], loss=7.5468
	step [157/195], loss=8.4648
	step [158/195], loss=7.5804
	step [159/195], loss=8.5092
	step [160/195], loss=8.4356
	step [161/195], loss=7.0141
	step [162/195], loss=8.9725
	step [163/195], loss=7.5479
	step [164/195], loss=6.5916
	step [165/195], loss=8.1862
	step [166/195], loss=7.8835
	step [167/195], loss=7.0784
	step [168/195], loss=7.5667
	step [169/195], loss=7.2884
	step [170/195], loss=8.1892
	step [171/195], loss=7.5527
	step [172/195], loss=8.6700
	step [173/195], loss=7.4245
	step [174/195], loss=7.8452
	step [175/195], loss=6.3715
	step [176/195], loss=7.7385
	step [177/195], loss=7.5486
	step [178/195], loss=6.5290
	step [179/195], loss=7.4653
	step [180/195], loss=8.1312
	step [181/195], loss=7.6352
	step [182/195], loss=7.3454
	step [183/195], loss=6.0747
	step [184/195], loss=9.1624
	step [185/195], loss=6.2418
	step [186/195], loss=8.1469
	step [187/195], loss=7.5221
	step [188/195], loss=7.1563
	step [189/195], loss=7.5416
	step [190/195], loss=7.5625
	step [191/195], loss=7.1363
	step [192/195], loss=7.5743
	step [193/195], loss=7.3598
	step [194/195], loss=8.5829
	step [195/195], loss=0.5123
	Evaluating
	loss=0.0349, precision=0.2095, recall=0.9971, f1=0.3462
Training epoch 22
	step [1/195], loss=6.5757
	step [2/195], loss=7.4648
	step [3/195], loss=7.2479
	step [4/195], loss=6.4100
	step [5/195], loss=7.3541
	step [6/195], loss=7.3337
	step [7/195], loss=8.2581
	step [8/195], loss=7.5647
	step [9/195], loss=6.1917
	step [10/195], loss=8.1464
	step [11/195], loss=7.0007
	step [12/195], loss=7.1382
	step [13/195], loss=7.4798
	step [14/195], loss=8.1225
	step [15/195], loss=8.7645
	step [16/195], loss=7.8494
	step [17/195], loss=8.1559
	step [18/195], loss=6.9406
	step [19/195], loss=7.8085
	step [20/195], loss=6.7560
	step [21/195], loss=8.1267
	step [22/195], loss=7.9391
	step [23/195], loss=5.9561
	step [24/195], loss=6.7257
	step [25/195], loss=6.4073
	step [26/195], loss=7.3766
	step [27/195], loss=7.3594
	step [28/195], loss=7.6359
	step [29/195], loss=7.7354
	step [30/195], loss=7.7375
	step [31/195], loss=9.0076
	step [32/195], loss=8.0848
	step [33/195], loss=8.1157
	step [34/195], loss=7.5140
	step [35/195], loss=7.0862
	step [36/195], loss=7.3688
	step [37/195], loss=7.7544
	step [38/195], loss=7.5238
	step [39/195], loss=7.1492
	step [40/195], loss=8.8250
	step [41/195], loss=6.9006
	step [42/195], loss=7.9394
	step [43/195], loss=8.1344
	step [44/195], loss=7.1314
	step [45/195], loss=7.2759
	step [46/195], loss=8.1758
	step [47/195], loss=7.8780
	step [48/195], loss=7.4887
	step [49/195], loss=7.9347
	step [50/195], loss=6.4423
	step [51/195], loss=7.5880
	step [52/195], loss=6.9264
	step [53/195], loss=7.1646
	step [54/195], loss=7.4318
	step [55/195], loss=6.9045
	step [56/195], loss=7.2447
	step [57/195], loss=7.9355
	step [58/195], loss=7.5649
	step [59/195], loss=7.0253
	step [60/195], loss=7.1001
	step [61/195], loss=7.3488
	step [62/195], loss=6.8740
	step [63/195], loss=7.4015
	step [64/195], loss=10.6363
	step [65/195], loss=8.1148
	step [66/195], loss=7.6225
	step [67/195], loss=7.9965
	step [68/195], loss=8.3460
	step [69/195], loss=7.4233
	step [70/195], loss=6.6323
	step [71/195], loss=8.2280
	step [72/195], loss=7.4444
	step [73/195], loss=6.9364
	step [74/195], loss=8.1438
	step [75/195], loss=8.3223
	step [76/195], loss=7.3726
	step [77/195], loss=7.9848
	step [78/195], loss=6.7666
	step [79/195], loss=6.8997
	step [80/195], loss=8.8238
	step [81/195], loss=6.1360
	step [82/195], loss=6.3381
	step [83/195], loss=7.4833
	step [84/195], loss=6.5973
	step [85/195], loss=7.2333
	step [86/195], loss=6.7727
	step [87/195], loss=8.8787
	step [88/195], loss=9.2351
	step [89/195], loss=7.5062
	step [90/195], loss=6.9230
	step [91/195], loss=9.3221
	step [92/195], loss=8.0231
	step [93/195], loss=6.5892
	step [94/195], loss=7.3633
	step [95/195], loss=7.0387
	step [96/195], loss=5.8105
	step [97/195], loss=8.1270
	step [98/195], loss=6.7449
	step [99/195], loss=9.2737
	step [100/195], loss=7.3640
	step [101/195], loss=8.0842
	step [102/195], loss=8.3787
	step [103/195], loss=7.6239
	step [104/195], loss=6.9302
	step [105/195], loss=6.7715
	step [106/195], loss=8.2201
	step [107/195], loss=7.3278
	step [108/195], loss=8.5090
	step [109/195], loss=6.7531
	step [110/195], loss=7.4437
	step [111/195], loss=7.0322
	step [112/195], loss=6.6830
	step [113/195], loss=8.1848
	step [114/195], loss=7.9200
	step [115/195], loss=7.3192
	step [116/195], loss=7.0700
	step [117/195], loss=6.8617
	step [118/195], loss=7.4859
	step [119/195], loss=7.9982
	step [120/195], loss=6.4688
	step [121/195], loss=6.7237
	step [122/195], loss=7.1666
	step [123/195], loss=8.2315
	step [124/195], loss=8.1364
	step [125/195], loss=7.7228
	step [126/195], loss=8.3654
	step [127/195], loss=8.7788
	step [128/195], loss=7.9655
	step [129/195], loss=7.6050
	step [130/195], loss=5.8847
	step [131/195], loss=8.5575
	step [132/195], loss=7.1394
	step [133/195], loss=7.5096
	step [134/195], loss=7.3617
	step [135/195], loss=7.9405
	step [136/195], loss=9.1629
	step [137/195], loss=7.5869
	step [138/195], loss=9.6613
	step [139/195], loss=6.7708
	step [140/195], loss=8.7746
	step [141/195], loss=8.1955
	step [142/195], loss=7.1877
	step [143/195], loss=6.6424
	step [144/195], loss=7.1818
	step [145/195], loss=6.3591
	step [146/195], loss=7.1023
	step [147/195], loss=8.5142
	step [148/195], loss=7.8249
	step [149/195], loss=7.5423
	step [150/195], loss=9.0828
	step [151/195], loss=7.8234
	step [152/195], loss=6.0671
	step [153/195], loss=6.8899
	step [154/195], loss=7.3638
	step [155/195], loss=7.6857
	step [156/195], loss=5.8772
	step [157/195], loss=7.1470
	step [158/195], loss=8.4551
	step [159/195], loss=7.6721
	step [160/195], loss=6.5841
	step [161/195], loss=8.4699
	step [162/195], loss=7.6621
	step [163/195], loss=7.4432
	step [164/195], loss=7.1000
	step [165/195], loss=7.4370
	step [166/195], loss=7.1579
	step [167/195], loss=6.5376
	step [168/195], loss=8.7777
	step [169/195], loss=8.1135
	step [170/195], loss=6.9938
	step [171/195], loss=8.8412
	step [172/195], loss=8.6211
	step [173/195], loss=7.2986
	step [174/195], loss=8.1058
	step [175/195], loss=7.4578
	step [176/195], loss=9.5648
	step [177/195], loss=7.1486
	step [178/195], loss=6.7154
	step [179/195], loss=6.5228
	step [180/195], loss=7.4189
	step [181/195], loss=7.2218
	step [182/195], loss=6.3369
	step [183/195], loss=6.9522
	step [184/195], loss=5.7376
	step [185/195], loss=5.8667
	step [186/195], loss=6.9865
	step [187/195], loss=7.1547
	step [188/195], loss=8.8206
	step [189/195], loss=7.4493
	step [190/195], loss=6.0204
	step [191/195], loss=7.4362
	step [192/195], loss=8.4413
	step [193/195], loss=7.9729
	step [194/195], loss=7.8668
	step [195/195], loss=0.4567
	Evaluating
	loss=0.0250, precision=0.2918, recall=0.9939, f1=0.4511
saving model as: 2_saved_model.pth
Training epoch 23
	step [1/195], loss=9.4539
	step [2/195], loss=7.6427
	step [3/195], loss=8.5403
	step [4/195], loss=7.5987
	step [5/195], loss=7.3739
	step [6/195], loss=7.3775
	step [7/195], loss=7.9472
	step [8/195], loss=8.4224
	step [9/195], loss=8.2418
	step [10/195], loss=5.8871
	step [11/195], loss=6.8238
	step [12/195], loss=6.7679
	step [13/195], loss=6.9235
	step [14/195], loss=8.3143
	step [15/195], loss=7.9292
	step [16/195], loss=8.1915
	step [17/195], loss=7.4603
	step [18/195], loss=5.9994
	step [19/195], loss=7.4847
	step [20/195], loss=7.8723
	step [21/195], loss=7.7462
	step [22/195], loss=6.4328
	step [23/195], loss=7.4609
	step [24/195], loss=6.4338
	step [25/195], loss=7.3088
	step [26/195], loss=6.8495
	step [27/195], loss=6.8468
	step [28/195], loss=7.5788
	step [29/195], loss=8.2760
	step [30/195], loss=7.5303
	step [31/195], loss=6.5892
	step [32/195], loss=7.3330
	step [33/195], loss=6.7381
	step [34/195], loss=8.8242
	step [35/195], loss=6.9040
	step [36/195], loss=7.1406
	step [37/195], loss=7.9344
	step [38/195], loss=7.3750
	step [39/195], loss=5.7850
	step [40/195], loss=6.7053
	step [41/195], loss=6.9773
	step [42/195], loss=5.4302
	step [43/195], loss=6.7772
	step [44/195], loss=7.3203
	step [45/195], loss=6.6823
	step [46/195], loss=6.9751
	step [47/195], loss=6.5782
	step [48/195], loss=7.4451
	step [49/195], loss=7.0087
	step [50/195], loss=7.8604
	step [51/195], loss=6.6739
	step [52/195], loss=8.3104
	step [53/195], loss=6.8744
	step [54/195], loss=7.1217
	step [55/195], loss=6.6871
	step [56/195], loss=7.2937
	step [57/195], loss=6.2903
	step [58/195], loss=7.5147
	step [59/195], loss=6.5015
	step [60/195], loss=6.2385
	step [61/195], loss=9.4060
	step [62/195], loss=6.4574
	step [63/195], loss=7.4689
	step [64/195], loss=6.7892
	step [65/195], loss=6.9449
	step [66/195], loss=7.3138
	step [67/195], loss=6.3766
	step [68/195], loss=8.1376
	step [69/195], loss=8.2578
	step [70/195], loss=6.7944
	step [71/195], loss=7.3064
	step [72/195], loss=7.4957
	step [73/195], loss=6.7353
	step [74/195], loss=6.1187
	step [75/195], loss=6.4087
	step [76/195], loss=8.2564
	step [77/195], loss=5.7843
	step [78/195], loss=7.4764
	step [79/195], loss=8.7530
	step [80/195], loss=7.1537
	step [81/195], loss=7.8009
	step [82/195], loss=6.9443
	step [83/195], loss=7.3721
	step [84/195], loss=5.9338
	step [85/195], loss=6.9046
	step [86/195], loss=7.1439
	step [87/195], loss=7.6185
	step [88/195], loss=7.4327
	step [89/195], loss=7.1616
	step [90/195], loss=7.2286
	step [91/195], loss=7.9892
	step [92/195], loss=7.9999
	step [93/195], loss=6.8151
	step [94/195], loss=6.1714
	step [95/195], loss=7.9185
	step [96/195], loss=7.2789
	step [97/195], loss=8.0520
	step [98/195], loss=7.6277
	step [99/195], loss=6.9234
	step [100/195], loss=7.1814
	step [101/195], loss=8.1239
	step [102/195], loss=8.2328
	step [103/195], loss=7.8070
	step [104/195], loss=6.7081
	step [105/195], loss=7.0334
	step [106/195], loss=7.6009
	step [107/195], loss=7.1001
	step [108/195], loss=7.7169
	step [109/195], loss=7.6322
	step [110/195], loss=7.4569
	step [111/195], loss=8.0014
	step [112/195], loss=7.0864
	step [113/195], loss=7.4290
	step [114/195], loss=7.9836
	step [115/195], loss=7.7718
	step [116/195], loss=6.7754
	step [117/195], loss=6.9112
	step [118/195], loss=6.9133
	step [119/195], loss=8.8376
	step [120/195], loss=7.7801
	step [121/195], loss=7.2413
	step [122/195], loss=8.2764
	step [123/195], loss=7.6222
	step [124/195], loss=8.2274
	step [125/195], loss=8.6836
	step [126/195], loss=7.0844
	step [127/195], loss=6.7926
	step [128/195], loss=7.5529
	step [129/195], loss=7.2962
	step [130/195], loss=6.8653
	step [131/195], loss=7.5949
	step [132/195], loss=7.6909
	step [133/195], loss=6.1565
	step [134/195], loss=8.4078
	step [135/195], loss=6.5887
	step [136/195], loss=6.8817
	step [137/195], loss=8.0914
	step [138/195], loss=7.8650
	step [139/195], loss=6.7247
	step [140/195], loss=9.5165
	step [141/195], loss=7.9745
	step [142/195], loss=6.8084
	step [143/195], loss=7.3695
	step [144/195], loss=6.7189
	step [145/195], loss=7.3904
	step [146/195], loss=8.0138
	step [147/195], loss=7.0960
	step [148/195], loss=8.4376
	step [149/195], loss=6.6382
	step [150/195], loss=7.4660
	step [151/195], loss=7.7740
	step [152/195], loss=7.3738
	step [153/195], loss=7.3314
	step [154/195], loss=7.9801
	step [155/195], loss=7.8509
	step [156/195], loss=6.9397
	step [157/195], loss=7.5181
	step [158/195], loss=8.0876
	step [159/195], loss=6.0410
	step [160/195], loss=6.3596
	step [161/195], loss=8.2823
	step [162/195], loss=6.5739
	step [163/195], loss=7.3159
	step [164/195], loss=6.9622
	step [165/195], loss=7.5175
	step [166/195], loss=7.9660
	step [167/195], loss=6.6593
	step [168/195], loss=6.7143
	step [169/195], loss=6.5211
	step [170/195], loss=7.8602
	step [171/195], loss=6.3334
	step [172/195], loss=7.0474
	step [173/195], loss=6.3159
	step [174/195], loss=6.8265
	step [175/195], loss=6.5771
	step [176/195], loss=7.8877
	step [177/195], loss=7.6209
	step [178/195], loss=5.9520
	step [179/195], loss=6.8473
	step [180/195], loss=6.5007
	step [181/195], loss=7.6498
	step [182/195], loss=7.4197
	step [183/195], loss=7.5980
	step [184/195], loss=7.3110
	step [185/195], loss=7.9672
	step [186/195], loss=7.1825
	step [187/195], loss=6.7310
	step [188/195], loss=7.0129
	step [189/195], loss=6.9766
	step [190/195], loss=6.0496
	step [191/195], loss=6.7075
	step [192/195], loss=7.1796
	step [193/195], loss=6.5086
	step [194/195], loss=7.1605
	step [195/195], loss=1.2756
	Evaluating
	loss=0.0318, precision=0.2185, recall=0.9967, f1=0.3585
Training epoch 24
	step [1/195], loss=8.2883
	step [2/195], loss=7.7034
	step [3/195], loss=7.9590
	step [4/195], loss=6.9330
	step [5/195], loss=7.7484
	step [6/195], loss=7.5241
	step [7/195], loss=6.0772
	step [8/195], loss=7.0920
	step [9/195], loss=6.6572
	step [10/195], loss=6.6573
	step [11/195], loss=6.0645
	step [12/195], loss=6.5312
	step [13/195], loss=7.4438
	step [14/195], loss=6.8649
	step [15/195], loss=7.3214
	step [16/195], loss=6.1368
	step [17/195], loss=6.5827
	step [18/195], loss=8.8230
	step [19/195], loss=7.2393
	step [20/195], loss=7.2905
	step [21/195], loss=6.3867
	step [22/195], loss=6.7773
	step [23/195], loss=6.9556
	step [24/195], loss=6.5230
	step [25/195], loss=7.8483
	step [26/195], loss=7.3570
	step [27/195], loss=7.4440
	step [28/195], loss=7.1594
	step [29/195], loss=5.9965
	step [30/195], loss=8.3450
	step [31/195], loss=6.9184
	step [32/195], loss=7.2817
	step [33/195], loss=7.4947
	step [34/195], loss=6.0076
	step [35/195], loss=7.9740
	step [36/195], loss=5.0301
	step [37/195], loss=9.0470
	step [38/195], loss=7.4435
	step [39/195], loss=5.5459
	step [40/195], loss=7.2685
	step [41/195], loss=7.3885
	step [42/195], loss=6.8044
	step [43/195], loss=6.6689
	step [44/195], loss=5.9384
	step [45/195], loss=5.9907
	step [46/195], loss=7.0536
	step [47/195], loss=7.5155
	step [48/195], loss=5.9743
	step [49/195], loss=5.5158
	step [50/195], loss=7.8383
	step [51/195], loss=7.1767
	step [52/195], loss=7.0631
	step [53/195], loss=7.1963
	step [54/195], loss=7.9161
	step [55/195], loss=6.6826
	step [56/195], loss=7.6777
	step [57/195], loss=7.5540
	step [58/195], loss=7.7257
	step [59/195], loss=7.3350
	step [60/195], loss=8.3016
	step [61/195], loss=6.4367
	step [62/195], loss=7.4877
	step [63/195], loss=6.8201
	step [64/195], loss=7.3074
	step [65/195], loss=7.4911
	step [66/195], loss=6.1095
	step [67/195], loss=7.4833
	step [68/195], loss=7.1419
	step [69/195], loss=6.3374
	step [70/195], loss=7.6343
	step [71/195], loss=6.6598
	step [72/195], loss=6.7247
	step [73/195], loss=5.6501
	step [74/195], loss=7.8419
	step [75/195], loss=6.2588
	step [76/195], loss=6.6017
	step [77/195], loss=6.7690
	step [78/195], loss=6.9108
	step [79/195], loss=7.1310
	step [80/195], loss=6.1796
	step [81/195], loss=7.4264
	step [82/195], loss=6.2150
	step [83/195], loss=8.1129
	step [84/195], loss=6.7614
	step [85/195], loss=6.1990
	step [86/195], loss=6.9810
	step [87/195], loss=6.7558
	step [88/195], loss=7.0224
	step [89/195], loss=8.6080
	step [90/195], loss=7.8276
	step [91/195], loss=6.3453
	step [92/195], loss=7.6483
	step [93/195], loss=6.7772
	step [94/195], loss=6.0602
	step [95/195], loss=6.5437
	step [96/195], loss=6.4744
	step [97/195], loss=7.1051
	step [98/195], loss=7.7152
	step [99/195], loss=7.7916
	step [100/195], loss=7.5371
	step [101/195], loss=6.8538
	step [102/195], loss=6.6618
	step [103/195], loss=6.9639
	step [104/195], loss=7.6052
	step [105/195], loss=7.0961
	step [106/195], loss=7.0945
	step [107/195], loss=6.4817
	step [108/195], loss=8.3749
	step [109/195], loss=8.8087
	step [110/195], loss=7.3647
	step [111/195], loss=5.6312
	step [112/195], loss=7.0911
	step [113/195], loss=6.9222
	step [114/195], loss=7.2346
	step [115/195], loss=7.0307
	step [116/195], loss=6.9213
	step [117/195], loss=7.2391
	step [118/195], loss=7.1846
	step [119/195], loss=6.5698
	step [120/195], loss=6.5011
	step [121/195], loss=7.9407
	step [122/195], loss=8.3524
	step [123/195], loss=7.5942
	step [124/195], loss=6.9026
	step [125/195], loss=7.9258
	step [126/195], loss=6.3894
	step [127/195], loss=7.1548
	step [128/195], loss=6.3211
	step [129/195], loss=6.7546
	step [130/195], loss=6.4101
	step [131/195], loss=5.5185
	step [132/195], loss=6.1728
	step [133/195], loss=7.9412
	step [134/195], loss=6.2735
	step [135/195], loss=6.1088
	step [136/195], loss=7.0619
	step [137/195], loss=7.3465
	step [138/195], loss=7.6021
	step [139/195], loss=5.7925
	step [140/195], loss=7.5398
	step [141/195], loss=6.8678
	step [142/195], loss=9.0518
	step [143/195], loss=6.9590
	step [144/195], loss=7.1978
	step [145/195], loss=7.2159
	step [146/195], loss=6.8506
	step [147/195], loss=8.8814
	step [148/195], loss=6.8714
	step [149/195], loss=6.7399
	step [150/195], loss=8.3289
	step [151/195], loss=6.8905
	step [152/195], loss=6.3690
	step [153/195], loss=7.2240
	step [154/195], loss=8.1138
	step [155/195], loss=7.1506
	step [156/195], loss=6.4024
	step [157/195], loss=6.0345
	step [158/195], loss=7.2442
	step [159/195], loss=8.0800
	step [160/195], loss=6.2035
	step [161/195], loss=7.9937
	step [162/195], loss=7.0486
	step [163/195], loss=7.6265
	step [164/195], loss=7.0787
	step [165/195], loss=6.6499
	step [166/195], loss=6.5999
	step [167/195], loss=7.0423
	step [168/195], loss=6.7340
	step [169/195], loss=7.0736
	step [170/195], loss=6.1973
	step [171/195], loss=7.1857
	step [172/195], loss=6.8564
	step [173/195], loss=7.0137
	step [174/195], loss=6.6313
	step [175/195], loss=6.9716
	step [176/195], loss=6.9305
	step [177/195], loss=7.5184
	step [178/195], loss=6.4955
	step [179/195], loss=8.8301
	step [180/195], loss=6.9113
	step [181/195], loss=8.0386
	step [182/195], loss=7.4718
	step [183/195], loss=5.9137
	step [184/195], loss=6.8884
	step [185/195], loss=6.5952
	step [186/195], loss=6.5531
	step [187/195], loss=8.6807
	step [188/195], loss=7.7155
	step [189/195], loss=7.2130
	step [190/195], loss=6.5968
	step [191/195], loss=6.8535
	step [192/195], loss=7.3779
	step [193/195], loss=5.7022
	step [194/195], loss=7.4389
	step [195/195], loss=0.5271
	Evaluating
	loss=0.0279, precision=0.2510, recall=0.9953, f1=0.4008
Training epoch 25
	step [1/195], loss=6.6787
	step [2/195], loss=8.1699
	step [3/195], loss=8.5054
	step [4/195], loss=7.1027
	step [5/195], loss=7.5108
	step [6/195], loss=7.8191
	step [7/195], loss=7.1612
	step [8/195], loss=6.9931
	step [9/195], loss=6.7710
	step [10/195], loss=7.3366
	step [11/195], loss=6.2768
	step [12/195], loss=5.1711
	step [13/195], loss=8.9625
	step [14/195], loss=6.9896
	step [15/195], loss=6.2985
	step [16/195], loss=5.9262
	step [17/195], loss=7.3024
	step [18/195], loss=7.6398
	step [19/195], loss=6.7722
	step [20/195], loss=6.8382
	step [21/195], loss=8.5751
	step [22/195], loss=6.2761
	step [23/195], loss=7.9000
	step [24/195], loss=6.3122
	step [25/195], loss=7.0918
	step [26/195], loss=6.4937
	step [27/195], loss=8.5686
	step [28/195], loss=6.4608
	step [29/195], loss=5.9967
	step [30/195], loss=7.2508
	step [31/195], loss=7.2239
	step [32/195], loss=5.6689
	step [33/195], loss=5.7326
	step [34/195], loss=7.1834
	step [35/195], loss=6.3998
	step [36/195], loss=7.3932
	step [37/195], loss=6.9412
	step [38/195], loss=6.2912
	step [39/195], loss=6.4455
	step [40/195], loss=7.1365
	step [41/195], loss=5.6187
	step [42/195], loss=5.3819
	step [43/195], loss=6.6770
	step [44/195], loss=5.6703
	step [45/195], loss=6.8835
	step [46/195], loss=6.2936
	step [47/195], loss=5.9080
	step [48/195], loss=6.8688
	step [49/195], loss=7.3937
	step [50/195], loss=7.0467
	step [51/195], loss=6.2204
	step [52/195], loss=7.0341
	step [53/195], loss=7.0585
	step [54/195], loss=8.3775
	step [55/195], loss=6.2133
	step [56/195], loss=6.7563
	step [57/195], loss=6.3782
	step [58/195], loss=6.8198
	step [59/195], loss=7.0497
	step [60/195], loss=6.7856
	step [61/195], loss=5.7768
	step [62/195], loss=6.0945
	step [63/195], loss=6.6868
	step [64/195], loss=6.3029
	step [65/195], loss=6.5651
	step [66/195], loss=6.8088
	step [67/195], loss=6.9848
	step [68/195], loss=6.0297
	step [69/195], loss=6.4616
	step [70/195], loss=7.2131
	step [71/195], loss=7.0450
	step [72/195], loss=6.8958
	step [73/195], loss=7.2007
	step [74/195], loss=6.7690
	step [75/195], loss=7.9393
	step [76/195], loss=6.9959
	step [77/195], loss=5.9112
	step [78/195], loss=7.7638
	step [79/195], loss=7.0590
	step [80/195], loss=7.0785
	step [81/195], loss=6.5923
	step [82/195], loss=6.0731
	step [83/195], loss=6.9609
	step [84/195], loss=7.9788
	step [85/195], loss=7.6946
	step [86/195], loss=6.4424
	step [87/195], loss=6.9953
	step [88/195], loss=6.2146
	step [89/195], loss=7.7523
	step [90/195], loss=6.0503
	step [91/195], loss=7.0021
	step [92/195], loss=5.9268
	step [93/195], loss=7.2752
	step [94/195], loss=6.4532
	step [95/195], loss=6.3399
	step [96/195], loss=7.0246
	step [97/195], loss=7.2954
	step [98/195], loss=7.7311
	step [99/195], loss=7.1795
	step [100/195], loss=7.1977
	step [101/195], loss=6.5134
	step [102/195], loss=6.4367
	step [103/195], loss=6.4891
	step [104/195], loss=7.8069
	step [105/195], loss=6.2412
	step [106/195], loss=6.4660
	step [107/195], loss=7.4220
	step [108/195], loss=7.0289
	step [109/195], loss=7.5893
	step [110/195], loss=5.9088
	step [111/195], loss=6.0993
	step [112/195], loss=7.1138
	step [113/195], loss=7.6267
	step [114/195], loss=7.1864
	step [115/195], loss=6.9677
	step [116/195], loss=7.9410
	step [117/195], loss=6.1109
	step [118/195], loss=6.1690
	step [119/195], loss=7.3909
	step [120/195], loss=6.8226
	step [121/195], loss=7.8046
	step [122/195], loss=6.3631
	step [123/195], loss=7.0527
	step [124/195], loss=5.9259
	step [125/195], loss=6.9752
	step [126/195], loss=7.2665
	step [127/195], loss=6.4343
	step [128/195], loss=6.2607
	step [129/195], loss=6.0161
	step [130/195], loss=6.6814
	step [131/195], loss=5.7010
	step [132/195], loss=6.1960
	step [133/195], loss=5.6110
	step [134/195], loss=6.0804
	step [135/195], loss=6.6588
	step [136/195], loss=7.3465
	step [137/195], loss=6.0381
	step [138/195], loss=6.8050
	step [139/195], loss=6.5410
	step [140/195], loss=7.4861
	step [141/195], loss=6.7890
	step [142/195], loss=7.8784
	step [143/195], loss=7.0766
	step [144/195], loss=5.7255
	step [145/195], loss=5.9063
	step [146/195], loss=7.4066
	step [147/195], loss=5.0590
	step [148/195], loss=6.2325
	step [149/195], loss=6.3671
	step [150/195], loss=6.8616
	step [151/195], loss=7.6161
	step [152/195], loss=8.1733
	step [153/195], loss=6.9264
	step [154/195], loss=6.1328
	step [155/195], loss=6.1293
	step [156/195], loss=6.9905
	step [157/195], loss=7.3027
	step [158/195], loss=6.1785
	step [159/195], loss=6.4711
	step [160/195], loss=6.3976
	step [161/195], loss=7.0164
	step [162/195], loss=7.0598
	step [163/195], loss=7.2306
	step [164/195], loss=6.5647
	step [165/195], loss=7.1980
	step [166/195], loss=7.5049
	step [167/195], loss=7.4110
	step [168/195], loss=5.7178
	step [169/195], loss=6.4557
	step [170/195], loss=6.3320
	step [171/195], loss=7.9603
	step [172/195], loss=7.3652
	step [173/195], loss=8.7313
	step [174/195], loss=7.9055
	step [175/195], loss=7.8935
	step [176/195], loss=6.5276
	step [177/195], loss=6.8572
	step [178/195], loss=6.9058
	step [179/195], loss=6.6921
	step [180/195], loss=7.2045
	step [181/195], loss=6.2302
	step [182/195], loss=7.0268
	step [183/195], loss=7.4534
	step [184/195], loss=6.9704
	step [185/195], loss=8.1505
	step [186/195], loss=6.9781
	step [187/195], loss=7.0781
	step [188/195], loss=6.9363
	step [189/195], loss=6.0436
	step [190/195], loss=5.9400
	step [191/195], loss=7.3422
	step [192/195], loss=7.4530
	step [193/195], loss=5.9944
	step [194/195], loss=6.0162
	step [195/195], loss=0.7514
	Evaluating
	loss=0.0238, precision=0.2793, recall=0.9943, f1=0.4361
Training epoch 26
	step [1/195], loss=6.2234
	step [2/195], loss=7.0704
	step [3/195], loss=6.7951
	step [4/195], loss=6.9027
	step [5/195], loss=7.5569
	step [6/195], loss=6.3086
	step [7/195], loss=6.6043
	step [8/195], loss=6.9546
	step [9/195], loss=7.8734
	step [10/195], loss=6.6155
	step [11/195], loss=6.5715
	step [12/195], loss=6.8192
	step [13/195], loss=6.7986
	step [14/195], loss=6.4667
	step [15/195], loss=5.8260
	step [16/195], loss=5.8654
	step [17/195], loss=6.5305
	step [18/195], loss=7.2214
	step [19/195], loss=7.1528
	step [20/195], loss=8.3625
	step [21/195], loss=6.1212
	step [22/195], loss=6.6885
	step [23/195], loss=5.7717
	step [24/195], loss=6.8483
	step [25/195], loss=6.6848
	step [26/195], loss=7.0298
	step [27/195], loss=7.6768
	step [28/195], loss=7.9051
	step [29/195], loss=7.5834
	step [30/195], loss=5.7535
	step [31/195], loss=6.6233
	step [32/195], loss=6.5883
	step [33/195], loss=6.1138
	step [34/195], loss=5.9729
	step [35/195], loss=6.0923
	step [36/195], loss=6.5258
	step [37/195], loss=5.1370
	step [38/195], loss=5.2547
	step [39/195], loss=6.6603
	step [40/195], loss=5.6853
	step [41/195], loss=6.4964
	step [42/195], loss=6.8306
	step [43/195], loss=5.7714
	step [44/195], loss=6.5497
	step [45/195], loss=6.1848
	step [46/195], loss=8.1601
	step [47/195], loss=6.0191
	step [48/195], loss=6.6482
	step [49/195], loss=6.6264
	step [50/195], loss=7.3778
	step [51/195], loss=6.9212
	step [52/195], loss=5.6252
	step [53/195], loss=6.3807
	step [54/195], loss=7.2562
	step [55/195], loss=6.7472
	step [56/195], loss=6.7742
	step [57/195], loss=5.8867
	step [58/195], loss=6.4512
	step [59/195], loss=6.8947
	step [60/195], loss=6.3097
	step [61/195], loss=6.7044
	step [62/195], loss=6.7636
	step [63/195], loss=5.9269
	step [64/195], loss=7.2186
	step [65/195], loss=6.7899
	step [66/195], loss=6.7845
	step [67/195], loss=6.5231
	step [68/195], loss=6.0739
	step [69/195], loss=6.4315
	step [70/195], loss=5.5676
	step [71/195], loss=6.6665
	step [72/195], loss=6.9797
	step [73/195], loss=6.9438
	step [74/195], loss=5.9302
	step [75/195], loss=7.2458
	step [76/195], loss=7.2565
	step [77/195], loss=7.6883
	step [78/195], loss=5.4838
	step [79/195], loss=7.0765
	step [80/195], loss=6.1348
	step [81/195], loss=6.6294
	step [82/195], loss=6.8350
	step [83/195], loss=5.6428
	step [84/195], loss=6.6826
	step [85/195], loss=6.0508
	step [86/195], loss=6.6823
	step [87/195], loss=6.5357
	step [88/195], loss=6.7064
	step [89/195], loss=7.1847
	step [90/195], loss=5.8224
	step [91/195], loss=6.4785
	step [92/195], loss=6.4223
	step [93/195], loss=7.1230
	step [94/195], loss=5.6646
	step [95/195], loss=7.7837
	step [96/195], loss=5.6223
	step [97/195], loss=6.7320
	step [98/195], loss=6.5575
	step [99/195], loss=6.9443
	step [100/195], loss=7.6448
	step [101/195], loss=6.4264
	step [102/195], loss=7.4938
	step [103/195], loss=7.1258
	step [104/195], loss=7.9230
	step [105/195], loss=7.8127
	step [106/195], loss=6.4562
	step [107/195], loss=7.1272
	step [108/195], loss=7.1877
	step [109/195], loss=6.7322
	step [110/195], loss=7.4243
	step [111/195], loss=6.5005
	step [112/195], loss=6.1847
	step [113/195], loss=6.6105
	step [114/195], loss=6.7357
	step [115/195], loss=7.2972
	step [116/195], loss=6.0947
	step [117/195], loss=6.3972
	step [118/195], loss=6.7132
	step [119/195], loss=7.2506
	step [120/195], loss=6.5306
	step [121/195], loss=7.0337
	step [122/195], loss=5.1065
	step [123/195], loss=8.1086
	step [124/195], loss=7.3343
	step [125/195], loss=8.1456
	step [126/195], loss=7.8446
	step [127/195], loss=6.2582
	step [128/195], loss=6.4500
	step [129/195], loss=7.4398
	step [130/195], loss=5.8445
	step [131/195], loss=7.1658
	step [132/195], loss=6.4158
	step [133/195], loss=5.8792
	step [134/195], loss=5.9251
	step [135/195], loss=6.2026
	step [136/195], loss=5.9157
	step [137/195], loss=6.5546
	step [138/195], loss=6.0554
	step [139/195], loss=6.6366
	step [140/195], loss=6.4224
	step [141/195], loss=7.2470
	step [142/195], loss=6.5927
	step [143/195], loss=6.2827
	step [144/195], loss=7.0438
	step [145/195], loss=7.4580
	step [146/195], loss=6.7093
	step [147/195], loss=7.7406
	step [148/195], loss=5.7479
	step [149/195], loss=6.6988
	step [150/195], loss=5.5985
	step [151/195], loss=5.6695
	step [152/195], loss=5.9173
	step [153/195], loss=8.5380
	step [154/195], loss=5.9585
	step [155/195], loss=6.1614
	step [156/195], loss=6.9953
	step [157/195], loss=6.4344
	step [158/195], loss=6.2075
	step [159/195], loss=6.0587
	step [160/195], loss=6.7304
	step [161/195], loss=6.4567
	step [162/195], loss=6.0583
	step [163/195], loss=6.4945
	step [164/195], loss=6.2802
	step [165/195], loss=7.7420
	step [166/195], loss=6.3425
	step [167/195], loss=6.9452
	step [168/195], loss=6.1602
	step [169/195], loss=6.1227
	step [170/195], loss=6.0196
	step [171/195], loss=6.3242
	step [172/195], loss=7.0687
	step [173/195], loss=5.7966
	step [174/195], loss=6.5007
	step [175/195], loss=6.7468
	step [176/195], loss=6.7962
	step [177/195], loss=6.9998
	step [178/195], loss=5.7428
	step [179/195], loss=9.0153
	step [180/195], loss=8.1183
	step [181/195], loss=6.5984
	step [182/195], loss=6.9387
	step [183/195], loss=5.5835
	step [184/195], loss=6.1944
	step [185/195], loss=6.7184
	step [186/195], loss=6.1980
	step [187/195], loss=5.9496
	step [188/195], loss=6.6652
	step [189/195], loss=7.7071
	step [190/195], loss=6.8203
	step [191/195], loss=6.4655
	step [192/195], loss=7.1515
	step [193/195], loss=7.4015
	step [194/195], loss=6.3519
	step [195/195], loss=0.2699
	Evaluating
	loss=0.0283, precision=0.2590, recall=0.9955, f1=0.4111
Training epoch 27
	step [1/195], loss=7.9217
	step [2/195], loss=5.6471
	step [3/195], loss=7.2223
	step [4/195], loss=6.8966
	step [5/195], loss=7.3792
	step [6/195], loss=5.6128
	step [7/195], loss=6.1791
	step [8/195], loss=6.6122
	step [9/195], loss=6.0208
	step [10/195], loss=6.4644
	step [11/195], loss=6.7033
	step [12/195], loss=7.0100
	step [13/195], loss=6.4156
	step [14/195], loss=6.2571
	step [15/195], loss=7.6454
	step [16/195], loss=7.2906
	step [17/195], loss=5.7892
	step [18/195], loss=6.2767
	step [19/195], loss=6.4281
	step [20/195], loss=6.3883
	step [21/195], loss=6.7383
	step [22/195], loss=7.1746
	step [23/195], loss=6.2188
	step [24/195], loss=6.0561
	step [25/195], loss=5.7164
	step [26/195], loss=6.9663
	step [27/195], loss=6.4456
	step [28/195], loss=5.7966
	step [29/195], loss=7.2427
	step [30/195], loss=6.3039
	step [31/195], loss=5.9168
	step [32/195], loss=5.9162
	step [33/195], loss=8.1502
	step [34/195], loss=6.7877
	step [35/195], loss=5.4332
	step [36/195], loss=7.1208
	step [37/195], loss=7.2114
	step [38/195], loss=7.0398
	step [39/195], loss=6.7707
	step [40/195], loss=5.9060
	step [41/195], loss=7.2244
	step [42/195], loss=8.0513
	step [43/195], loss=6.8576
	step [44/195], loss=6.3660
	step [45/195], loss=6.0359
	step [46/195], loss=5.9419
	step [47/195], loss=6.7457
	step [48/195], loss=6.4100
	step [49/195], loss=6.3726
	step [50/195], loss=6.4703
	step [51/195], loss=6.8391
	step [52/195], loss=5.7184
	step [53/195], loss=8.4098
	step [54/195], loss=6.1535
	step [55/195], loss=5.9762
	step [56/195], loss=7.3074
	step [57/195], loss=6.8745
	step [58/195], loss=6.1385
	step [59/195], loss=5.9184
	step [60/195], loss=6.8192
	step [61/195], loss=7.1001
	step [62/195], loss=7.0454
	step [63/195], loss=6.1738
	step [64/195], loss=6.2364
	step [65/195], loss=6.9887
	step [66/195], loss=7.1209
	step [67/195], loss=5.1159
	step [68/195], loss=6.2891
	step [69/195], loss=5.5300
	step [70/195], loss=7.3186
	step [71/195], loss=7.2414
	step [72/195], loss=7.1462
	step [73/195], loss=6.5212
	step [74/195], loss=6.2604
	step [75/195], loss=5.8525
	step [76/195], loss=5.8551
	step [77/195], loss=5.7363
	step [78/195], loss=5.5838
	step [79/195], loss=5.5137
	step [80/195], loss=5.7005
	step [81/195], loss=6.8361
	step [82/195], loss=5.4898
	step [83/195], loss=6.8592
	step [84/195], loss=5.3296
	step [85/195], loss=6.5511
	step [86/195], loss=5.9425
	step [87/195], loss=7.4061
	step [88/195], loss=6.4722
	step [89/195], loss=6.6193
	step [90/195], loss=6.9771
	step [91/195], loss=7.1141
	step [92/195], loss=6.7746
	step [93/195], loss=6.7012
	step [94/195], loss=6.1990
	step [95/195], loss=7.2840
	step [96/195], loss=6.0562
	step [97/195], loss=6.0579
	step [98/195], loss=5.7615
	step [99/195], loss=6.3778
	step [100/195], loss=6.3931
	step [101/195], loss=6.1571
	step [102/195], loss=6.1574
	step [103/195], loss=6.8102
	step [104/195], loss=5.7329
	step [105/195], loss=6.3207
	step [106/195], loss=6.4671
	step [107/195], loss=5.3009
	step [108/195], loss=6.8440
	step [109/195], loss=5.7871
	step [110/195], loss=7.1162
	step [111/195], loss=7.9798
	step [112/195], loss=6.5299
	step [113/195], loss=7.2004
	step [114/195], loss=6.6888
	step [115/195], loss=5.7797
	step [116/195], loss=6.7261
	step [117/195], loss=7.9699
	step [118/195], loss=6.6315
	step [119/195], loss=5.9645
	step [120/195], loss=8.0338
	step [121/195], loss=6.9502
	step [122/195], loss=7.5490
	step [123/195], loss=6.3373
	step [124/195], loss=6.3694
	step [125/195], loss=6.1033
	step [126/195], loss=6.3769
	step [127/195], loss=7.6271
	step [128/195], loss=5.9250
	step [129/195], loss=6.4316
	step [130/195], loss=5.4569
	step [131/195], loss=7.7864
	step [132/195], loss=5.1283
	step [133/195], loss=6.1256
	step [134/195], loss=6.2866
	step [135/195], loss=6.1478
	step [136/195], loss=6.2995
	step [137/195], loss=5.6513
	step [138/195], loss=6.4252
	step [139/195], loss=5.6895
	step [140/195], loss=5.9296
	step [141/195], loss=8.2584
	step [142/195], loss=6.4363
	step [143/195], loss=5.7438
	step [144/195], loss=5.7006
	step [145/195], loss=6.6331
	step [146/195], loss=6.8111
	step [147/195], loss=5.5113
	step [148/195], loss=7.6323
	step [149/195], loss=5.5399
	step [150/195], loss=6.9735
	step [151/195], loss=6.2571
	step [152/195], loss=7.5585
	step [153/195], loss=6.6117
	step [154/195], loss=7.2896
	step [155/195], loss=6.2463
	step [156/195], loss=6.0443
	step [157/195], loss=7.4610
	step [158/195], loss=6.2139
	step [159/195], loss=6.4185
	step [160/195], loss=5.5232
	step [161/195], loss=6.0025
	step [162/195], loss=5.3308
	step [163/195], loss=7.4512
	step [164/195], loss=6.3800
	step [165/195], loss=6.5798
	step [166/195], loss=6.5538
	step [167/195], loss=7.2574
	step [168/195], loss=6.1580
	step [169/195], loss=6.2184
	step [170/195], loss=5.6147
	step [171/195], loss=5.9340
	step [172/195], loss=6.5145
	step [173/195], loss=8.5646
	step [174/195], loss=6.6271
	step [175/195], loss=6.1738
	step [176/195], loss=6.1539
	step [177/195], loss=5.9423
	step [178/195], loss=5.4708
	step [179/195], loss=6.9944
	step [180/195], loss=6.5097
	step [181/195], loss=6.7223
	step [182/195], loss=5.9002
	step [183/195], loss=7.1720
	step [184/195], loss=7.2143
	step [185/195], loss=6.6428
	step [186/195], loss=5.4339
	step [187/195], loss=5.7180
	step [188/195], loss=6.3153
	step [189/195], loss=5.6072
	step [190/195], loss=6.4305
	step [191/195], loss=6.3582
	step [192/195], loss=6.6352
	step [193/195], loss=6.4022
	step [194/195], loss=6.6282
	step [195/195], loss=1.0485
	Evaluating
	loss=0.0253, precision=0.2575, recall=0.9956, f1=0.4092
Training epoch 28
	step [1/195], loss=5.8860
	step [2/195], loss=7.6212
	step [3/195], loss=6.6835
	step [4/195], loss=7.1628
	step [5/195], loss=7.5312
	step [6/195], loss=6.8126
	step [7/195], loss=6.3830
	step [8/195], loss=6.4909
	step [9/195], loss=5.7148
	step [10/195], loss=5.7143
	step [11/195], loss=5.6903
	step [12/195], loss=7.0483
	step [13/195], loss=7.1499
	step [14/195], loss=6.6791
	step [15/195], loss=6.5875
	step [16/195], loss=6.1251
	step [17/195], loss=6.0683
	step [18/195], loss=5.7156
	step [19/195], loss=7.2604
	step [20/195], loss=7.0063
	step [21/195], loss=6.9047
	step [22/195], loss=7.1812
	step [23/195], loss=6.5922
	step [24/195], loss=6.3745
	step [25/195], loss=5.4366
	step [26/195], loss=5.5014
	step [27/195], loss=5.9591
	step [28/195], loss=7.4217
	step [29/195], loss=6.2172
	step [30/195], loss=6.5572
	step [31/195], loss=6.0124
	step [32/195], loss=5.2038
	step [33/195], loss=6.5094
	step [34/195], loss=6.0294
	step [35/195], loss=5.5526
	step [36/195], loss=6.2880
	step [37/195], loss=5.9763
	step [38/195], loss=6.3856
	step [39/195], loss=6.1662
	step [40/195], loss=5.5958
	step [41/195], loss=6.9162
	step [42/195], loss=7.6731
	step [43/195], loss=6.6755
	step [44/195], loss=7.2914
	step [45/195], loss=5.4639
	step [46/195], loss=7.2759
	step [47/195], loss=7.0174
	step [48/195], loss=5.7635
	step [49/195], loss=6.2319
	step [50/195], loss=6.2203
	step [51/195], loss=6.0769
	step [52/195], loss=5.9763
	step [53/195], loss=5.3014
	step [54/195], loss=6.1077
	step [55/195], loss=5.4542
	step [56/195], loss=6.6762
	step [57/195], loss=6.0161
	step [58/195], loss=6.4134
	step [59/195], loss=7.2492
	step [60/195], loss=5.4221
	step [61/195], loss=6.2081
	step [62/195], loss=5.4383
	step [63/195], loss=7.1914
	step [64/195], loss=5.9155
	step [65/195], loss=4.9991
	step [66/195], loss=5.7555
	step [67/195], loss=6.3082
	step [68/195], loss=5.7304
	step [69/195], loss=6.6534
	step [70/195], loss=7.0157
	step [71/195], loss=6.5054
	step [72/195], loss=5.8833
	step [73/195], loss=5.9435
	step [74/195], loss=4.5922
	step [75/195], loss=5.4231
	step [76/195], loss=6.0672
	step [77/195], loss=6.5479
	step [78/195], loss=6.7164
	step [79/195], loss=6.4332
	step [80/195], loss=5.8348
	step [81/195], loss=5.7688
	step [82/195], loss=6.3847
	step [83/195], loss=7.4277
	step [84/195], loss=6.0573
	step [85/195], loss=6.2664
	step [86/195], loss=5.6661
	step [87/195], loss=5.9122
	step [88/195], loss=6.5363
	step [89/195], loss=5.6828
	step [90/195], loss=7.9742
	step [91/195], loss=6.1259
	step [92/195], loss=5.9612
	step [93/195], loss=7.0703
	step [94/195], loss=5.4875
	step [95/195], loss=8.4399
	step [96/195], loss=6.8423
	step [97/195], loss=6.0894
	step [98/195], loss=7.2645
	step [99/195], loss=5.4725
	step [100/195], loss=6.1021
	step [101/195], loss=6.1046
	step [102/195], loss=5.8118
	step [103/195], loss=6.0994
	step [104/195], loss=6.8360
	step [105/195], loss=6.6639
	step [106/195], loss=5.6010
	step [107/195], loss=6.6495
	step [108/195], loss=6.8331
	step [109/195], loss=7.1048
	step [110/195], loss=5.9517
	step [111/195], loss=5.9695
	step [112/195], loss=6.3601
	step [113/195], loss=7.1198
	step [114/195], loss=5.6078
	step [115/195], loss=7.0753
	step [116/195], loss=6.3718
	step [117/195], loss=5.7649
	step [118/195], loss=7.4441
	step [119/195], loss=6.0849
	step [120/195], loss=6.2600
	step [121/195], loss=6.9581
	step [122/195], loss=7.1081
	step [123/195], loss=5.2450
	step [124/195], loss=5.7594
	step [125/195], loss=5.8865
	step [126/195], loss=7.1560
	step [127/195], loss=6.5981
	step [128/195], loss=6.1709
	step [129/195], loss=6.1185
	step [130/195], loss=6.6483
	step [131/195], loss=6.2017
	step [132/195], loss=5.9707
	step [133/195], loss=6.3229
	step [134/195], loss=6.1778
	step [135/195], loss=6.6212
	step [136/195], loss=5.6704
	step [137/195], loss=5.2264
	step [138/195], loss=5.9044
	step [139/195], loss=6.2019
	step [140/195], loss=5.8316
	step [141/195], loss=6.9147
	step [142/195], loss=6.4119
	step [143/195], loss=6.5923
	step [144/195], loss=7.4673
	step [145/195], loss=6.4819
	step [146/195], loss=6.3490
	step [147/195], loss=6.6848
	step [148/195], loss=6.0374
	step [149/195], loss=5.3795
	step [150/195], loss=6.0018
	step [151/195], loss=7.7153
	step [152/195], loss=6.3772
	step [153/195], loss=7.0354
	step [154/195], loss=5.9591
	step [155/195], loss=5.4141
	step [156/195], loss=7.5361
	step [157/195], loss=7.1562
	step [158/195], loss=6.5466
	step [159/195], loss=5.1133
	step [160/195], loss=7.3720
	step [161/195], loss=4.7820
	step [162/195], loss=6.6541
	step [163/195], loss=6.9612
	step [164/195], loss=5.3901
	step [165/195], loss=6.5442
	step [166/195], loss=5.7800
	step [167/195], loss=7.1315
	step [168/195], loss=5.8312
	step [169/195], loss=5.9804
	step [170/195], loss=5.8561
	step [171/195], loss=5.8280
	step [172/195], loss=5.2766
	step [173/195], loss=6.4220
	step [174/195], loss=5.6151
	step [175/195], loss=6.4829
	step [176/195], loss=6.3792
	step [177/195], loss=6.7649
	step [178/195], loss=6.2223
	step [179/195], loss=6.5402
	step [180/195], loss=5.3134
	step [181/195], loss=6.7580
	step [182/195], loss=5.9076
	step [183/195], loss=7.8868
	step [184/195], loss=5.7384
	step [185/195], loss=7.6046
	step [186/195], loss=6.2072
	step [187/195], loss=6.4808
	step [188/195], loss=6.4906
	step [189/195], loss=7.0576
	step [190/195], loss=6.8927
	step [191/195], loss=5.9358
	step [192/195], loss=6.7596
	step [193/195], loss=5.5420
	step [194/195], loss=6.7489
	step [195/195], loss=0.5606
	Evaluating
	loss=0.0220, precision=0.2957, recall=0.9940, f1=0.4558
saving model as: 2_saved_model.pth
Training epoch 29
	step [1/195], loss=7.2945
	step [2/195], loss=4.6774
	step [3/195], loss=6.0553
	step [4/195], loss=6.0426
	step [5/195], loss=5.8493
	step [6/195], loss=5.6987
	step [7/195], loss=5.4038
	step [8/195], loss=6.3833
	step [9/195], loss=4.9647
	step [10/195], loss=6.5567
	step [11/195], loss=5.2817
	step [12/195], loss=5.3768
	step [13/195], loss=6.3360
	step [14/195], loss=6.8565
	step [15/195], loss=6.2764
	step [16/195], loss=6.1274
	step [17/195], loss=5.9298
	step [18/195], loss=6.3871
	step [19/195], loss=5.4592
	step [20/195], loss=5.7902
	step [21/195], loss=7.0655
	step [22/195], loss=6.1291
	step [23/195], loss=6.3470
	step [24/195], loss=6.8896
	step [25/195], loss=5.6856
	step [26/195], loss=5.3875
	step [27/195], loss=6.9259
	step [28/195], loss=5.2887
	step [29/195], loss=7.2852
	step [30/195], loss=6.2910
	step [31/195], loss=6.6398
	step [32/195], loss=7.2082
	step [33/195], loss=6.0785
	step [34/195], loss=6.6717
	step [35/195], loss=6.1360
	step [36/195], loss=5.6202
	step [37/195], loss=6.9511
	step [38/195], loss=6.0319
	step [39/195], loss=6.5598
	step [40/195], loss=6.0723
	step [41/195], loss=5.1044
	step [42/195], loss=6.8629
	step [43/195], loss=6.3525
	step [44/195], loss=5.4332
	step [45/195], loss=6.0361
	step [46/195], loss=6.6939
	step [47/195], loss=6.5381
	step [48/195], loss=5.8635
	step [49/195], loss=6.9720
	step [50/195], loss=6.3730
	step [51/195], loss=5.2476
	step [52/195], loss=6.4142
	step [53/195], loss=7.3243
	step [54/195], loss=6.3380
	step [55/195], loss=6.5956
	step [56/195], loss=6.0863
	step [57/195], loss=6.3819
	step [58/195], loss=6.6091
	step [59/195], loss=6.1952
	step [60/195], loss=6.3538
	step [61/195], loss=6.8439
	step [62/195], loss=6.8037
	step [63/195], loss=5.9561
	step [64/195], loss=6.3413
	step [65/195], loss=5.7762
	step [66/195], loss=5.2873
	step [67/195], loss=5.6231
	step [68/195], loss=6.4493
	step [69/195], loss=5.8948
	step [70/195], loss=5.9306
	step [71/195], loss=5.7756
	step [72/195], loss=6.1015
	step [73/195], loss=5.7881
	step [74/195], loss=5.6262
	step [75/195], loss=5.7834
	step [76/195], loss=4.9314
	step [77/195], loss=5.8996
	step [78/195], loss=5.8629
	step [79/195], loss=6.5957
	step [80/195], loss=6.0408
	step [81/195], loss=5.9892
	step [82/195], loss=6.2691
	step [83/195], loss=5.6413
	step [84/195], loss=6.8218
	step [85/195], loss=6.3941
	step [86/195], loss=6.4295
	step [87/195], loss=6.2078
	step [88/195], loss=7.5915
	step [89/195], loss=5.8690
	step [90/195], loss=5.7587
	step [91/195], loss=5.1456
	step [92/195], loss=6.8410
	step [93/195], loss=6.3339
	step [94/195], loss=5.5801
	step [95/195], loss=4.9440
	step [96/195], loss=6.7656
	step [97/195], loss=7.1491
	step [98/195], loss=6.7992
	step [99/195], loss=5.8679
	step [100/195], loss=6.1395
	step [101/195], loss=6.8551
	step [102/195], loss=5.7629
	step [103/195], loss=5.5409
	step [104/195], loss=5.4158
	step [105/195], loss=6.2924
	step [106/195], loss=5.7414
	step [107/195], loss=6.7228
	step [108/195], loss=6.8799
	step [109/195], loss=5.6173
	step [110/195], loss=5.5368
	step [111/195], loss=7.2439
	step [112/195], loss=6.2573
	step [113/195], loss=5.9767
	step [114/195], loss=4.8221
	step [115/195], loss=5.6273
	step [116/195], loss=5.8384
	step [117/195], loss=6.0872
	step [118/195], loss=6.4444
	step [119/195], loss=6.4976
	step [120/195], loss=7.2984
	step [121/195], loss=6.5163
	step [122/195], loss=5.6415
	step [123/195], loss=6.5727
	step [124/195], loss=7.3849
	step [125/195], loss=5.6346
	step [126/195], loss=6.4482
	step [127/195], loss=6.7710
	step [128/195], loss=5.4194
	step [129/195], loss=6.3794
	step [130/195], loss=5.8378
	step [131/195], loss=4.6333
	step [132/195], loss=5.7784
	step [133/195], loss=6.4857
	step [134/195], loss=5.8305
	step [135/195], loss=6.0382
	step [136/195], loss=6.5403
	step [137/195], loss=7.5220
	step [138/195], loss=6.1404
	step [139/195], loss=6.3024
	step [140/195], loss=5.5386
	step [141/195], loss=6.7364
	step [142/195], loss=6.0533
	step [143/195], loss=6.0134
	step [144/195], loss=5.9793
	step [145/195], loss=5.6433
	step [146/195], loss=5.4841
	step [147/195], loss=5.3832
	step [148/195], loss=7.0433
	step [149/195], loss=6.5921
	step [150/195], loss=6.0984
	step [151/195], loss=6.0265
	step [152/195], loss=6.4093
	step [153/195], loss=5.1864
	step [154/195], loss=6.8533
	step [155/195], loss=6.9337
	step [156/195], loss=6.0755
	step [157/195], loss=6.8286
	step [158/195], loss=5.6665
	step [159/195], loss=5.9285
	step [160/195], loss=5.6501
	step [161/195], loss=5.8619
	step [162/195], loss=5.7209
	step [163/195], loss=5.8638
	step [164/195], loss=5.9034
	step [165/195], loss=6.3992
	step [166/195], loss=5.8976
	step [167/195], loss=5.6162
	step [168/195], loss=5.4002
	step [169/195], loss=6.4923
	step [170/195], loss=5.3990
	step [171/195], loss=5.9982
	step [172/195], loss=5.3752
	step [173/195], loss=6.2425
	step [174/195], loss=5.1643
	step [175/195], loss=6.0181
	step [176/195], loss=5.9117
	step [177/195], loss=6.0113
	step [178/195], loss=5.5218
	step [179/195], loss=5.4448
	step [180/195], loss=6.6215
	step [181/195], loss=5.7596
	step [182/195], loss=6.5237
	step [183/195], loss=5.8769
	step [184/195], loss=6.1067
	step [185/195], loss=6.5281
	step [186/195], loss=6.1794
	step [187/195], loss=5.0538
	step [188/195], loss=5.9570
	step [189/195], loss=6.1041
	step [190/195], loss=6.2965
	step [191/195], loss=6.1438
	step [192/195], loss=7.4139
	step [193/195], loss=6.8112
	step [194/195], loss=6.7581
	step [195/195], loss=0.5271
	Evaluating
	loss=0.0307, precision=0.2160, recall=0.9966, f1=0.3551
Training epoch 30
	step [1/195], loss=5.4402
	step [2/195], loss=7.1651
	step [3/195], loss=6.5902
	step [4/195], loss=5.2148
	step [5/195], loss=6.1038
	step [6/195], loss=5.6935
	step [7/195], loss=5.8650
	step [8/195], loss=5.9276
	step [9/195], loss=5.9379
	step [10/195], loss=6.0168
	step [11/195], loss=6.1273
	step [12/195], loss=6.5131
	step [13/195], loss=6.3719
	step [14/195], loss=6.0575
	step [15/195], loss=5.5959
	step [16/195], loss=5.5706
	step [17/195], loss=5.7659
	step [18/195], loss=6.7503
	step [19/195], loss=6.2617
	step [20/195], loss=5.0621
	step [21/195], loss=5.6237
	step [22/195], loss=5.4996
	step [23/195], loss=5.7254
	step [24/195], loss=5.3538
	step [25/195], loss=7.0650
	step [26/195], loss=5.9307
	step [27/195], loss=4.8957
	step [28/195], loss=6.7182
	step [29/195], loss=6.0035
	step [30/195], loss=5.5119
	step [31/195], loss=6.4563
	step [32/195], loss=5.5867
	step [33/195], loss=6.0932
	step [34/195], loss=7.7810
	step [35/195], loss=6.5539
	step [36/195], loss=6.2147
	step [37/195], loss=6.4280
	step [38/195], loss=6.2247
	step [39/195], loss=6.8667
	step [40/195], loss=5.9184
	step [41/195], loss=7.3013
	step [42/195], loss=5.9165
	step [43/195], loss=6.8996
	step [44/195], loss=6.2065
	step [45/195], loss=6.3453
	step [46/195], loss=5.1619
	step [47/195], loss=6.2443
	step [48/195], loss=5.7150
	step [49/195], loss=6.2829
	step [50/195], loss=5.7827
	step [51/195], loss=5.7787
	step [52/195], loss=5.9612
	step [53/195], loss=6.3489
	step [54/195], loss=5.9434
	step [55/195], loss=5.9440
	step [56/195], loss=5.9828
	step [57/195], loss=5.4705
	step [58/195], loss=5.7737
	step [59/195], loss=5.5958
	step [60/195], loss=6.0519
	step [61/195], loss=6.0255
	step [62/195], loss=6.0337
	step [63/195], loss=5.8064
	step [64/195], loss=5.6863
	step [65/195], loss=6.3471
	step [66/195], loss=5.6182
	step [67/195], loss=5.9282
	step [68/195], loss=6.4314
	step [69/195], loss=5.9369
	step [70/195], loss=6.2295
	step [71/195], loss=6.3578
	step [72/195], loss=5.5655
	step [73/195], loss=6.0821
	step [74/195], loss=7.1436
	step [75/195], loss=5.8213
	step [76/195], loss=5.9469
	step [77/195], loss=6.5620
	step [78/195], loss=6.9502
	step [79/195], loss=6.5931
	step [80/195], loss=5.3580
	step [81/195], loss=5.9983
	step [82/195], loss=5.2306
	step [83/195], loss=6.3687
	step [84/195], loss=6.0741
	step [85/195], loss=5.2403
	step [86/195], loss=6.2972
	step [87/195], loss=6.0306
	step [88/195], loss=5.4854
	step [89/195], loss=5.1470
	step [90/195], loss=5.6794
	step [91/195], loss=6.1687
	step [92/195], loss=6.4695
	step [93/195], loss=6.3473
	step [94/195], loss=5.9236
	step [95/195], loss=6.2568
	step [96/195], loss=6.1094
	step [97/195], loss=5.5467
	step [98/195], loss=5.1482
	step [99/195], loss=5.7892
	step [100/195], loss=5.8260
	step [101/195], loss=4.9280
	step [102/195], loss=5.7089
	step [103/195], loss=5.4812
	step [104/195], loss=5.7460
	step [105/195], loss=5.2331
	step [106/195], loss=5.6810
	step [107/195], loss=5.5502
	step [108/195], loss=5.8916
	step [109/195], loss=5.9572
	step [110/195], loss=5.1904
	step [111/195], loss=5.2352
	step [112/195], loss=5.7078
	step [113/195], loss=5.4753
	step [114/195], loss=6.2137
	step [115/195], loss=4.9342
	step [116/195], loss=5.6462
	step [117/195], loss=6.9466
	step [118/195], loss=6.2573
	step [119/195], loss=5.5179
	step [120/195], loss=5.8331
	step [121/195], loss=5.3227
	step [122/195], loss=5.7337
	step [123/195], loss=6.7715
	step [124/195], loss=5.9391
	step [125/195], loss=5.6700
	step [126/195], loss=5.6022
	step [127/195], loss=6.2959
	step [128/195], loss=5.7163
	step [129/195], loss=6.0819
	step [130/195], loss=6.0357
	step [131/195], loss=6.6906
	step [132/195], loss=5.1793
	step [133/195], loss=7.3419
	step [134/195], loss=5.1132
	step [135/195], loss=6.2685
	step [136/195], loss=5.2727
	step [137/195], loss=5.1165
	step [138/195], loss=5.3404
	step [139/195], loss=5.5339
	step [140/195], loss=5.1025
	step [141/195], loss=5.8970
	step [142/195], loss=5.2840
	step [143/195], loss=5.3090
	step [144/195], loss=5.9673
	step [145/195], loss=6.0629
	step [146/195], loss=6.3935
	step [147/195], loss=6.1004
	step [148/195], loss=5.9728
	step [149/195], loss=5.3989
	step [150/195], loss=5.7498
	step [151/195], loss=6.9695
	step [152/195], loss=6.7529
	step [153/195], loss=6.6506
	step [154/195], loss=6.4882
	step [155/195], loss=6.0517
	step [156/195], loss=6.4809
	step [157/195], loss=5.9287
	step [158/195], loss=5.8129
	step [159/195], loss=6.0415
	step [160/195], loss=6.0851
	step [161/195], loss=5.4541
	step [162/195], loss=5.0767
	step [163/195], loss=5.8960
	step [164/195], loss=5.7144
	step [165/195], loss=5.7931
	step [166/195], loss=6.6405
	step [167/195], loss=5.4083
	step [168/195], loss=6.2797
	step [169/195], loss=5.5607
	step [170/195], loss=6.3254
	step [171/195], loss=6.2485
	step [172/195], loss=5.7228
	step [173/195], loss=6.5362
	step [174/195], loss=6.1527
	step [175/195], loss=6.0853
	step [176/195], loss=6.3360
	step [177/195], loss=6.0025
	step [178/195], loss=6.2612
	step [179/195], loss=6.0539
	step [180/195], loss=5.5663
	step [181/195], loss=5.5247
	step [182/195], loss=5.6590
	step [183/195], loss=6.3538
	step [184/195], loss=5.6367
	step [185/195], loss=5.8312
	step [186/195], loss=5.5122
	step [187/195], loss=5.8550
	step [188/195], loss=5.5454
	step [189/195], loss=6.1119
	step [190/195], loss=6.1103
	step [191/195], loss=6.1454
	step [192/195], loss=5.5389
	step [193/195], loss=5.3300
	step [194/195], loss=6.2970
	step [195/195], loss=0.4667
	Evaluating
	loss=0.0215, precision=0.2995, recall=0.9936, f1=0.4603
saving model as: 2_saved_model.pth
Training finished
best_f1: 0.46029707070832454
directing: Y rim_enhanced: False test_id 2
removed wrong scan: weights_Y_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_171_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_299_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_87_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_241_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_224_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_336_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_350_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_244_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_373_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_308_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_285_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_261_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_243_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_274_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_188_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_218_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_387_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_360_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_372_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_316_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_320_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_160_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_253_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_254_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_204_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_349_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_149_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_283_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_300_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_333_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_221_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_159_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_215_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_363_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_329_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_368_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_254_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_282_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_170_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_211_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_260_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_258_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_231_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_222_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_313_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_282_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_259_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_263_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_177_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_190_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_281_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_187_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_261_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_245_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_315_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_286_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_184_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_312_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_302_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_276_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_157_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_236_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_331_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_214_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_212_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_356_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_167_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_383_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_255_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_224_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_264_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_343_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_375_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_168_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_318_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_367_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_220_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_163_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_255_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_351_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_305_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_330_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_181_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_229_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_309_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_225_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_164_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_242_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_233_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_346_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_326_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_230_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_371_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_275_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_377_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_273_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_197_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_151_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_199_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_332_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_239_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_307_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_380_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_321_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_265_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_150_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_201_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_291_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_353_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_246_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_322_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_310_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_352_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_262_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_297_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_338_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_319_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_179_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_236_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_239_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_215_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_218_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_253_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_293_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_303_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_158_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_203_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_381_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_317_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_226_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_189_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_270_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_294_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_354_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_359_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_266_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_292_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_314_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_218_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_238_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_200_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_262_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_229_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_169_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_271_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_147_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_288_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_244_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_347_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_248_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_323_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_335_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_278_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_233_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_306_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_209_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_365_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_259_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_378_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_298_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_256_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_276_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_193_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_384_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_208_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_237_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_221_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_258_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_339_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_232_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_325_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_238_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_290_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_231_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_369_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_280_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_337_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_216_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_304_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_250_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_194_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_301_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_324_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_340_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_162_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_295_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_366_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_370_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_362_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_379_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_269_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_180_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_289_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_296_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_183_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_271_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_226_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_237_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_341_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_161_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_358_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_254_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_235_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_334_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_376_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_287_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_252_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_205_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_348_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_249_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_251_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_186_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_257_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_80_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_243_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_279_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_268_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_230_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_206_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_165_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_207_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_227_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_260_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_155_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_225_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_311_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_327_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_344_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_267_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_256_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_328_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_153_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_257_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_152_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_176_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_342_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_222_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_263_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_385_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_156_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_386_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_196_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_243_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_195_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_277_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_252_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_232_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_220_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_388_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_210_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_241_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_265_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_146_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_266_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_284_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_355_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_174_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_225_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_345_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_272_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_205_xwqg-B00034_2020-04-03.npy
# all image files: 20333 # all weight files in weight_dir: 15599 # image files with weight 15554
removed wrong scan: weights_Y_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_171_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_299_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_87_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_241_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_224_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_336_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_350_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_244_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_373_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_308_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_285_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_261_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_243_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_274_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_188_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_218_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_387_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_360_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_372_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_316_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_320_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_160_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_253_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_254_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_204_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_349_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_149_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_283_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_300_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_333_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_221_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_159_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_215_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_363_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_329_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_368_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_254_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_282_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_170_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_211_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_260_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_258_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_231_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_222_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_313_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_282_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_259_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_263_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_177_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_190_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_281_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_187_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_261_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_245_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_315_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_286_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_184_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_312_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_302_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_276_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_157_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_236_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_331_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_214_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_212_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_356_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_167_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_383_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_255_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_224_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_264_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_343_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_375_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_168_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_318_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_367_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_220_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_163_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_255_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_351_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_305_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_330_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_181_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_229_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_309_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_225_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_164_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_242_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_233_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_346_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_326_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_230_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_371_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_275_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_377_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_273_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_197_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_151_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_199_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_332_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_239_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_307_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_380_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_321_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_265_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_150_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_201_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_291_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_353_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_246_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_322_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_310_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_352_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_262_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_297_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_338_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_319_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_179_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_236_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_239_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_215_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_218_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_253_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_293_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_303_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_158_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_203_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_381_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_317_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_226_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_189_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_270_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_294_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_354_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_359_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_266_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_292_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_314_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_218_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_238_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_200_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_262_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_229_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_169_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_271_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_147_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_288_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_244_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_347_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_248_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_323_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_335_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_278_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_233_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_306_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_209_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_365_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_259_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_378_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_298_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_256_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_276_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_193_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_384_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_208_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_237_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_221_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_258_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_339_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_232_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_325_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_238_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_290_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_231_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_369_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_280_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_337_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_216_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_304_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_250_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_194_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_301_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_324_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_340_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_162_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_295_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_366_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_370_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_362_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_379_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_269_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_180_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_289_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_296_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_183_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_271_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_226_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_237_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_341_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_161_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_358_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_254_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_235_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_334_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_376_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_287_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_252_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_205_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_348_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_249_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_251_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_186_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_257_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_80_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_243_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_279_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_268_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_230_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_206_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_165_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_207_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_227_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_260_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_155_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_225_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_311_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_327_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_344_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_267_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_256_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_328_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_153_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_257_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_152_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_176_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_342_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_222_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_263_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_385_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_156_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_386_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_196_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_243_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_195_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_277_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_252_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_232_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_220_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_388_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_210_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_241_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_265_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_146_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_266_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_284_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_355_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_174_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_225_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_345_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_272_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_205_xwqg-B00034_2020-04-03.npy
# all image files: 20333 # all weight files in weight_dir: 4484 # image files with weight 4476
train: /ibex/scratch/projects/c2052/air_tube_seg/training_samples_enhanced/Y 15554
Using 4 GPUs
Going to train epochs [1-30]
Training epoch 1
	step [1/325], loss=215.2120
	step [2/325], loss=174.5713
	step [3/325], loss=162.8578
	step [4/325], loss=155.6189
	step [5/325], loss=152.0879
	step [6/325], loss=150.3929
	step [7/325], loss=149.3324
	step [8/325], loss=147.1267
	step [9/325], loss=144.2220
	step [10/325], loss=144.7635
	step [11/325], loss=143.2140
	step [12/325], loss=141.8137
	step [13/325], loss=140.1801
	step [14/325], loss=138.4343
	step [15/325], loss=135.9626
	step [16/325], loss=136.4284
	step [17/325], loss=132.6709
	step [18/325], loss=133.0351
	step [19/325], loss=131.2209
	step [20/325], loss=130.9556
	step [21/325], loss=129.1368
	step [22/325], loss=128.1506
	step [23/325], loss=126.9804
	step [24/325], loss=124.2673
	step [25/325], loss=123.7406
	step [26/325], loss=124.7680
	step [27/325], loss=122.9442
	step [28/325], loss=120.9464
	step [29/325], loss=119.3932
	step [30/325], loss=121.1402
	step [31/325], loss=118.5112
	step [32/325], loss=117.6121
	step [33/325], loss=116.6738
	step [34/325], loss=114.6828
	step [35/325], loss=114.1672
	step [36/325], loss=111.7827
	step [37/325], loss=111.3156
	step [38/325], loss=110.7189
	step [39/325], loss=111.0369
	step [40/325], loss=108.8834
	step [41/325], loss=110.8523
	step [42/325], loss=107.6703
	step [43/325], loss=108.5197
	step [44/325], loss=106.8985
	step [45/325], loss=106.7902
	step [46/325], loss=104.7000
	step [47/325], loss=103.3193
	step [48/325], loss=102.8012
	step [49/325], loss=103.2767
	step [50/325], loss=104.2199
	step [51/325], loss=102.1320
	step [52/325], loss=101.1005
	step [53/325], loss=101.0609
	step [54/325], loss=101.6073
	step [55/325], loss=101.1238
	step [56/325], loss=98.7141
	step [57/325], loss=99.2158
	step [58/325], loss=98.6766
	step [59/325], loss=99.6659
	step [60/325], loss=99.1940
	step [61/325], loss=97.4019
	step [62/325], loss=96.8931
	step [63/325], loss=97.9858
	step [64/325], loss=98.5490
	step [65/325], loss=97.9325
	step [66/325], loss=97.1550
	step [67/325], loss=97.4550
	step [68/325], loss=94.7702
	step [69/325], loss=96.0516
	step [70/325], loss=95.9515
	step [71/325], loss=93.8404
	step [72/325], loss=95.9947
	step [73/325], loss=95.0793
	step [74/325], loss=93.0465
	step [75/325], loss=91.8709
	step [76/325], loss=93.9186
	step [77/325], loss=92.7128
	step [78/325], loss=93.7522
	step [79/325], loss=92.0531
	step [80/325], loss=93.7866
	step [81/325], loss=91.3715
	step [82/325], loss=92.6781
	step [83/325], loss=90.8697
	step [84/325], loss=89.9441
	step [85/325], loss=90.6011
	step [86/325], loss=90.0140
	step [87/325], loss=89.3621
	step [88/325], loss=90.3488
	step [89/325], loss=90.5179
	step [90/325], loss=90.1543
	step [91/325], loss=90.3302
	step [92/325], loss=89.1202
	step [93/325], loss=87.2829
	step [94/325], loss=87.8099
	step [95/325], loss=89.1128
	step [96/325], loss=87.9199
	step [97/325], loss=87.4218
	step [98/325], loss=86.9483
	step [99/325], loss=87.6257
	step [100/325], loss=86.3119
	step [101/325], loss=85.3641
	step [102/325], loss=85.3088
	step [103/325], loss=87.4351
	step [104/325], loss=87.9538
	step [105/325], loss=87.4606
	step [106/325], loss=86.6591
	step [107/325], loss=86.8420
	step [108/325], loss=87.0658
	step [109/325], loss=84.9169
	step [110/325], loss=85.9798
	step [111/325], loss=83.6322
	step [112/325], loss=85.3070
	step [113/325], loss=84.2409
	step [114/325], loss=83.6012
	step [115/325], loss=85.0495
	step [116/325], loss=84.5699
	step [117/325], loss=83.3066
	step [118/325], loss=83.4631
	step [119/325], loss=85.5561
	step [120/325], loss=83.7024
	step [121/325], loss=85.1123
	step [122/325], loss=82.1917
	step [123/325], loss=83.5003
	step [124/325], loss=82.4226
	step [125/325], loss=82.7850
	step [126/325], loss=82.0758
	step [127/325], loss=83.4447
	step [128/325], loss=82.1622
	step [129/325], loss=82.9149
	step [130/325], loss=82.6598
	step [131/325], loss=82.5699
	step [132/325], loss=82.0280
	step [133/325], loss=81.4637
	step [134/325], loss=79.8834
	step [135/325], loss=81.5885
	step [136/325], loss=80.8815
	step [137/325], loss=81.6912
	step [138/325], loss=80.0897
	step [139/325], loss=81.2520
	step [140/325], loss=81.2845
	step [141/325], loss=80.6991
	step [142/325], loss=83.0313
	step [143/325], loss=80.5384
	step [144/325], loss=80.5666
	step [145/325], loss=81.2951
	step [146/325], loss=80.8878
	step [147/325], loss=79.8362
	step [148/325], loss=78.5466
	step [149/325], loss=81.4808
	step [150/325], loss=82.0765
	step [151/325], loss=79.6763
	step [152/325], loss=80.4892
	step [153/325], loss=79.2952
	step [154/325], loss=79.2372
	step [155/325], loss=79.1006
	step [156/325], loss=78.3192
	step [157/325], loss=79.3476
	step [158/325], loss=78.0893
	step [159/325], loss=77.0197
	step [160/325], loss=76.8972
	step [161/325], loss=77.5678
	step [162/325], loss=78.8365
	step [163/325], loss=76.9321
	step [164/325], loss=77.9112
	step [165/325], loss=78.2562
	step [166/325], loss=78.5474
	step [167/325], loss=77.7823
	step [168/325], loss=76.6073
	step [169/325], loss=77.8616
	step [170/325], loss=76.1140
	step [171/325], loss=77.0528
	step [172/325], loss=76.3290
	step [173/325], loss=77.1373
	step [174/325], loss=75.5633
	step [175/325], loss=76.0193
	step [176/325], loss=76.1984
	step [177/325], loss=77.1067
	step [178/325], loss=76.9292
	step [179/325], loss=75.9215
	step [180/325], loss=75.3519
	step [181/325], loss=74.5422
	step [182/325], loss=76.3454
	step [183/325], loss=76.1059
	step [184/325], loss=75.0621
	step [185/325], loss=75.9924
	step [186/325], loss=75.6348
	step [187/325], loss=76.3175
	step [188/325], loss=74.3973
	step [189/325], loss=73.1966
	step [190/325], loss=73.9589
	step [191/325], loss=74.8297
	step [192/325], loss=74.5751
	step [193/325], loss=73.7518
	step [194/325], loss=73.8892
	step [195/325], loss=72.6472
	step [196/325], loss=74.0193
	step [197/325], loss=75.3729
	step [198/325], loss=74.3108
	step [199/325], loss=72.6949
	step [200/325], loss=74.7639
	step [201/325], loss=72.7228
	step [202/325], loss=74.0430
	step [203/325], loss=72.6501
	step [204/325], loss=72.1415
	step [205/325], loss=71.4915
	step [206/325], loss=71.5864
	step [207/325], loss=71.6360
	step [208/325], loss=72.6152
	step [209/325], loss=71.1222
	step [210/325], loss=73.4432
	step [211/325], loss=72.9786
	step [212/325], loss=71.6463
	step [213/325], loss=72.7634
	step [214/325], loss=72.3207
	step [215/325], loss=72.0185
	step [216/325], loss=74.5092
	step [217/325], loss=71.1925
	step [218/325], loss=71.6928
	step [219/325], loss=70.6792
	step [220/325], loss=70.3294
	step [221/325], loss=71.0337
	step [222/325], loss=71.7276
	step [223/325], loss=70.9553
	step [224/325], loss=69.0694
	step [225/325], loss=70.7006
	step [226/325], loss=70.4162
	step [227/325], loss=72.1027
	step [228/325], loss=69.9787
	step [229/325], loss=70.6042
	step [230/325], loss=68.8799
	step [231/325], loss=69.8307
	step [232/325], loss=69.4213
	step [233/325], loss=71.6612
	step [234/325], loss=69.0052
	step [235/325], loss=68.7608
	step [236/325], loss=70.2336
	step [237/325], loss=68.4125
	step [238/325], loss=67.8049
	step [239/325], loss=69.2967
	step [240/325], loss=69.6563
	step [241/325], loss=69.3893
	step [242/325], loss=69.5981
	step [243/325], loss=69.3265
	step [244/325], loss=68.7287
	step [245/325], loss=68.0460
	step [246/325], loss=68.7025
	step [247/325], loss=66.9003
	step [248/325], loss=67.5420
	step [249/325], loss=68.8196
	step [250/325], loss=67.5638
	step [251/325], loss=67.3926
	step [252/325], loss=69.7507
	step [253/325], loss=67.4877
	step [254/325], loss=66.9231
	step [255/325], loss=69.0558
	step [256/325], loss=67.0878
	step [257/325], loss=68.7885
	step [258/325], loss=67.3108
	step [259/325], loss=68.7969
	step [260/325], loss=68.5476
	step [261/325], loss=68.0667
	step [262/325], loss=67.3208
	step [263/325], loss=66.1830
	step [264/325], loss=67.1728
	step [265/325], loss=66.9133
	step [266/325], loss=68.2718
	step [267/325], loss=65.9604
	step [268/325], loss=65.1722
	step [269/325], loss=66.4127
	step [270/325], loss=66.6419
	step [271/325], loss=66.8452
	step [272/325], loss=67.3724
	step [273/325], loss=66.0503
	step [274/325], loss=64.4319
	step [275/325], loss=66.4070
	step [276/325], loss=66.4985
	step [277/325], loss=65.6231
	step [278/325], loss=64.2960
	step [279/325], loss=65.7552
	step [280/325], loss=65.1840
	step [281/325], loss=65.2240
	step [282/325], loss=64.7968
	step [283/325], loss=65.1813
	step [284/325], loss=64.0277
	step [285/325], loss=64.5394
	step [286/325], loss=65.7744
	step [287/325], loss=62.2921
	step [288/325], loss=64.5380
	step [289/325], loss=64.1330
	step [290/325], loss=64.1824
	step [291/325], loss=64.4416
	step [292/325], loss=63.4301
	step [293/325], loss=63.4604
	step [294/325], loss=62.8632
	step [295/325], loss=64.7580
	step [296/325], loss=64.1758
	step [297/325], loss=63.8653
	step [298/325], loss=63.5721
	step [299/325], loss=63.1167
	step [300/325], loss=64.8910
	step [301/325], loss=63.4241
	step [302/325], loss=62.6328
	step [303/325], loss=62.5556
	step [304/325], loss=63.4924
	step [305/325], loss=63.3603
	step [306/325], loss=62.6988
	step [307/325], loss=62.9768
	step [308/325], loss=63.3646
	step [309/325], loss=61.6659
	step [310/325], loss=63.9694
	step [311/325], loss=62.9601
	step [312/325], loss=61.0810
	step [313/325], loss=62.9447
	step [314/325], loss=63.4518
	step [315/325], loss=61.4609
	step [316/325], loss=60.5823
	step [317/325], loss=61.5693
	step [318/325], loss=61.0444
	step [319/325], loss=61.3462
	step [320/325], loss=60.5805
	step [321/325], loss=61.8676
	step [322/325], loss=61.7057
	step [323/325], loss=61.1175
	step [324/325], loss=61.1684
	step [325/325], loss=2.8024
	Evaluating
	loss=0.3130, precision=0.1395, recall=0.9981, f1=0.2449
saving model as: 2_saved_model.pth
Training epoch 2
	step [1/325], loss=61.4998
	step [2/325], loss=61.7776
	step [3/325], loss=61.3051
	step [4/325], loss=60.1195
	step [5/325], loss=61.8770
	step [6/325], loss=60.4633
	step [7/325], loss=58.6274
	step [8/325], loss=58.9968
	step [9/325], loss=60.1945
	step [10/325], loss=60.1072
	step [11/325], loss=59.4458
	step [12/325], loss=60.3609
	step [13/325], loss=58.4347
	step [14/325], loss=59.9617
	step [15/325], loss=60.0655
	step [16/325], loss=61.1559
	step [17/325], loss=60.4997
	step [18/325], loss=59.8569
	step [19/325], loss=60.8268
	step [20/325], loss=60.6053
	step [21/325], loss=59.4776
	step [22/325], loss=59.2908
	step [23/325], loss=59.4226
	step [24/325], loss=59.7261
	step [25/325], loss=59.7156
	step [26/325], loss=59.2649
	step [27/325], loss=59.7702
	step [28/325], loss=58.8968
	step [29/325], loss=59.1601
	step [30/325], loss=58.5294
	step [31/325], loss=57.9243
	step [32/325], loss=57.4319
	step [33/325], loss=58.0982
	step [34/325], loss=58.8867
	step [35/325], loss=59.3701
	step [36/325], loss=57.5014
	step [37/325], loss=58.2797
	step [38/325], loss=57.9705
	step [39/325], loss=58.0543
	step [40/325], loss=57.5320
	step [41/325], loss=55.9520
	step [42/325], loss=57.8929
	step [43/325], loss=57.3267
	step [44/325], loss=57.1425
	step [45/325], loss=57.1980
	step [46/325], loss=57.5572
	step [47/325], loss=57.4276
	step [48/325], loss=58.1360
	step [49/325], loss=55.6712
	step [50/325], loss=56.6830
	step [51/325], loss=55.8682
	step [52/325], loss=56.0365
	step [53/325], loss=59.6503
	step [54/325], loss=55.7696
	step [55/325], loss=54.8941
	step [56/325], loss=56.4067
	step [57/325], loss=57.4231
	step [58/325], loss=55.4395
	step [59/325], loss=55.2198
	step [60/325], loss=56.4358
	step [61/325], loss=55.1986
	step [62/325], loss=55.2117
	step [63/325], loss=55.1742
	step [64/325], loss=56.8347
	step [65/325], loss=54.5442
	step [66/325], loss=56.3610
	step [67/325], loss=54.6203
	step [68/325], loss=56.1081
	step [69/325], loss=55.6447
	step [70/325], loss=55.4458
	step [71/325], loss=54.9153
	step [72/325], loss=56.0886
	step [73/325], loss=55.2021
	step [74/325], loss=53.9464
	step [75/325], loss=53.4763
	step [76/325], loss=54.3434
	step [77/325], loss=54.8422
	step [78/325], loss=54.9241
	step [79/325], loss=54.7595
	step [80/325], loss=55.7570
	step [81/325], loss=55.7747
	step [82/325], loss=54.1158
	step [83/325], loss=53.5670
	step [84/325], loss=54.5691
	step [85/325], loss=53.0823
	step [86/325], loss=52.4163
	step [87/325], loss=54.3353
	step [88/325], loss=53.9411
	step [89/325], loss=53.8205
	step [90/325], loss=54.4207
	step [91/325], loss=54.9869
	step [92/325], loss=53.7791
	step [93/325], loss=54.0353
	step [94/325], loss=52.1437
	step [95/325], loss=53.2074
	step [96/325], loss=52.8489
	step [97/325], loss=53.3721
	step [98/325], loss=52.7499
	step [99/325], loss=52.8794
	step [100/325], loss=52.8888
	step [101/325], loss=52.4813
	step [102/325], loss=50.8958
	step [103/325], loss=51.0737
	step [104/325], loss=51.7524
	step [105/325], loss=52.0445
	step [106/325], loss=52.6057
	step [107/325], loss=54.2254
	step [108/325], loss=53.7819
	step [109/325], loss=54.8317
	step [110/325], loss=53.4101
	step [111/325], loss=51.9850
	step [112/325], loss=50.6322
	step [113/325], loss=53.5884
	step [114/325], loss=50.2713
	step [115/325], loss=52.0311
	step [116/325], loss=51.9235
	step [117/325], loss=51.3614
	step [118/325], loss=52.5698
	step [119/325], loss=51.9896
	step [120/325], loss=50.7366
	step [121/325], loss=51.2651
	step [122/325], loss=50.3398
	step [123/325], loss=51.8273
	step [124/325], loss=51.8011
	step [125/325], loss=51.6289
	step [126/325], loss=51.6611
	step [127/325], loss=51.3217
	step [128/325], loss=51.7468
	step [129/325], loss=50.6288
	step [130/325], loss=49.6837
	step [131/325], loss=50.3934
	step [132/325], loss=50.5711
	step [133/325], loss=51.1206
	step [134/325], loss=51.2834
	step [135/325], loss=49.4930
	step [136/325], loss=50.6959
	step [137/325], loss=50.1423
	step [138/325], loss=50.5431
	step [139/325], loss=49.4619
	step [140/325], loss=48.8409
	step [141/325], loss=48.6745
	step [142/325], loss=51.5750
	step [143/325], loss=50.0636
	step [144/325], loss=50.9773
	step [145/325], loss=50.4958
	step [146/325], loss=50.3049
	step [147/325], loss=50.1033
	step [148/325], loss=50.7397
	step [149/325], loss=48.9319
	step [150/325], loss=48.3740
	step [151/325], loss=48.8756
	step [152/325], loss=48.6530
	step [153/325], loss=48.3028
	step [154/325], loss=51.0008
	step [155/325], loss=47.7517
	step [156/325], loss=50.7324
	step [157/325], loss=48.6112
	step [158/325], loss=48.1785
	step [159/325], loss=48.7312
	step [160/325], loss=48.1687
	step [161/325], loss=48.4409
	step [162/325], loss=47.1680
	step [163/325], loss=48.7525
	step [164/325], loss=49.1807
	step [165/325], loss=47.0527
	step [166/325], loss=48.6575
	step [167/325], loss=47.5986
	step [168/325], loss=46.9715
	step [169/325], loss=46.7835
	step [170/325], loss=49.4194
	step [171/325], loss=48.2014
	step [172/325], loss=46.9002
	step [173/325], loss=48.0874
	step [174/325], loss=49.1321
	step [175/325], loss=48.0203
	step [176/325], loss=48.0777
	step [177/325], loss=48.6929
	step [178/325], loss=46.3716
	step [179/325], loss=47.1118
	step [180/325], loss=47.2675
	step [181/325], loss=46.9019
	step [182/325], loss=47.5319
	step [183/325], loss=47.1299
	step [184/325], loss=46.3662
	step [185/325], loss=47.3773
	step [186/325], loss=48.6882
	step [187/325], loss=47.1216
	step [188/325], loss=46.6506
	step [189/325], loss=45.3327
	step [190/325], loss=45.1938
	step [191/325], loss=46.2005
	step [192/325], loss=43.9655
	step [193/325], loss=46.4474
	step [194/325], loss=47.6544
	step [195/325], loss=45.0874
	step [196/325], loss=46.3696
	step [197/325], loss=45.9806
	step [198/325], loss=45.4819
	step [199/325], loss=45.9068
	step [200/325], loss=45.2793
	step [201/325], loss=46.2158
	step [202/325], loss=44.7009
	step [203/325], loss=43.8017
	step [204/325], loss=46.5429
	step [205/325], loss=45.9038
	step [206/325], loss=44.8674
	step [207/325], loss=45.2894
	step [208/325], loss=44.6346
	step [209/325], loss=44.6036
	step [210/325], loss=43.4046
	step [211/325], loss=44.4684
	step [212/325], loss=43.8266
	step [213/325], loss=44.4479
	step [214/325], loss=43.2746
	step [215/325], loss=43.7924
	step [216/325], loss=43.3621
	step [217/325], loss=45.1902
	step [218/325], loss=43.9734
	step [219/325], loss=45.7788
	step [220/325], loss=45.8495
	step [221/325], loss=44.0136
	step [222/325], loss=43.8963
	step [223/325], loss=44.5757
	step [224/325], loss=43.3326
	step [225/325], loss=44.2288
	step [226/325], loss=44.2888
	step [227/325], loss=44.7162
	step [228/325], loss=44.0613
	step [229/325], loss=45.8612
	step [230/325], loss=44.1312
	step [231/325], loss=43.8339
	step [232/325], loss=45.2318
	step [233/325], loss=42.3473
	step [234/325], loss=45.1689
	step [235/325], loss=42.8693
	step [236/325], loss=43.8466
	step [237/325], loss=44.5252
	step [238/325], loss=44.2612
	step [239/325], loss=44.1499
	step [240/325], loss=43.4557
	step [241/325], loss=43.9906
	step [242/325], loss=44.0915
	step [243/325], loss=43.2014
	step [244/325], loss=42.9291
	step [245/325], loss=42.4434
	step [246/325], loss=42.1618
	step [247/325], loss=43.0055
	step [248/325], loss=42.8691
	step [249/325], loss=41.6967
	step [250/325], loss=42.7974
	step [251/325], loss=42.7210
	step [252/325], loss=42.2307
	step [253/325], loss=41.3331
	step [254/325], loss=42.1104
	step [255/325], loss=41.9224
	step [256/325], loss=41.5608
	step [257/325], loss=41.6416
	step [258/325], loss=42.4675
	step [259/325], loss=42.6998
	step [260/325], loss=41.8121
	step [261/325], loss=41.1779
	step [262/325], loss=39.8876
	step [263/325], loss=41.0929
	step [264/325], loss=43.7239
	step [265/325], loss=44.3710
	step [266/325], loss=41.8194
	step [267/325], loss=40.3983
	step [268/325], loss=41.2927
	step [269/325], loss=40.6272
	step [270/325], loss=41.2829
	step [271/325], loss=40.3204
	step [272/325], loss=40.2039
	step [273/325], loss=41.1972
	step [274/325], loss=41.0739
	step [275/325], loss=43.5576
	step [276/325], loss=40.8344
	step [277/325], loss=40.3684
	step [278/325], loss=39.3645
	step [279/325], loss=40.3717
	step [280/325], loss=39.8019
	step [281/325], loss=40.8167
	step [282/325], loss=40.3090
	step [283/325], loss=39.7426
	step [284/325], loss=39.6029
	step [285/325], loss=39.2442
	step [286/325], loss=39.7356
	step [287/325], loss=38.7026
	step [288/325], loss=39.5753
	step [289/325], loss=38.8895
	step [290/325], loss=38.5560
	step [291/325], loss=39.5111
	step [292/325], loss=40.9010
	step [293/325], loss=39.9522
	step [294/325], loss=38.6634
	step [295/325], loss=40.8013
	step [296/325], loss=39.5739
	step [297/325], loss=39.1674
	step [298/325], loss=39.4789
	step [299/325], loss=39.4694
	step [300/325], loss=37.8449
	step [301/325], loss=41.2128
	step [302/325], loss=38.3950
	step [303/325], loss=38.8420
	step [304/325], loss=38.5982
	step [305/325], loss=39.2047
	step [306/325], loss=40.3701
	step [307/325], loss=37.9385
	step [308/325], loss=38.9322
	step [309/325], loss=37.8767
	step [310/325], loss=39.3937
	step [311/325], loss=38.3969
	step [312/325], loss=37.2158
	step [313/325], loss=38.9353
	step [314/325], loss=38.1076
	step [315/325], loss=38.8309
	step [316/325], loss=39.5460
	step [317/325], loss=37.3955
	step [318/325], loss=37.2792
	step [319/325], loss=36.3071
	step [320/325], loss=38.9076
	step [321/325], loss=38.6621
	step [322/325], loss=37.3970
	step [323/325], loss=37.4064
	step [324/325], loss=36.6875
	step [325/325], loss=2.1492
	Evaluating
	loss=0.2014, precision=0.1482, recall=0.9979, f1=0.2581
saving model as: 2_saved_model.pth
Training epoch 3
	step [1/325], loss=37.0730
	step [2/325], loss=36.9452
	step [3/325], loss=37.3777
	step [4/325], loss=38.3490
	step [5/325], loss=38.5840
	step [6/325], loss=37.6293
	step [7/325], loss=36.9037
	step [8/325], loss=36.9508
	step [9/325], loss=35.9992
	step [10/325], loss=37.3747
	step [11/325], loss=39.0849
	step [12/325], loss=38.5032
	step [13/325], loss=37.5359
	step [14/325], loss=36.5402
	step [15/325], loss=38.4455
	step [16/325], loss=37.0475
	step [17/325], loss=36.8412
	step [18/325], loss=37.5610
	step [19/325], loss=36.0201
	step [20/325], loss=36.3476
	step [21/325], loss=37.4234
	step [22/325], loss=37.6324
	step [23/325], loss=35.7301
	step [24/325], loss=35.5897
	step [25/325], loss=36.3109
	step [26/325], loss=35.8486
	step [27/325], loss=34.9700
	step [28/325], loss=38.0035
	step [29/325], loss=35.4239
	step [30/325], loss=36.3102
	step [31/325], loss=35.6585
	step [32/325], loss=35.7814
	step [33/325], loss=36.6383
	step [34/325], loss=36.0595
	step [35/325], loss=36.8721
	step [36/325], loss=36.6105
	step [37/325], loss=38.4115
	step [38/325], loss=38.0295
	step [39/325], loss=35.8235
	step [40/325], loss=35.0738
	step [41/325], loss=35.6677
	step [42/325], loss=34.6997
	step [43/325], loss=36.3825
	step [44/325], loss=36.0183
	step [45/325], loss=34.7984
	step [46/325], loss=36.4031
	step [47/325], loss=34.7450
	step [48/325], loss=35.9002
	step [49/325], loss=34.2792
	step [50/325], loss=35.9159
	step [51/325], loss=36.4826
	step [52/325], loss=35.5751
	step [53/325], loss=34.4675
	step [54/325], loss=35.1162
	step [55/325], loss=34.4057
	step [56/325], loss=33.0696
	step [57/325], loss=34.6968
	step [58/325], loss=35.2270
	step [59/325], loss=34.3411
	step [60/325], loss=34.8159
	step [61/325], loss=34.4886
	step [62/325], loss=35.1423
	step [63/325], loss=34.3738
	step [64/325], loss=34.4053
	step [65/325], loss=34.0824
	step [66/325], loss=33.7238
	step [67/325], loss=35.0658
	step [68/325], loss=32.5640
	step [69/325], loss=35.4852
	step [70/325], loss=32.0805
	step [71/325], loss=34.4382
	step [72/325], loss=33.3220
	step [73/325], loss=34.4992
	step [74/325], loss=33.2333
	step [75/325], loss=33.7312
	step [76/325], loss=33.9811
	step [77/325], loss=33.2168
	step [78/325], loss=32.6675
	step [79/325], loss=33.6348
	step [80/325], loss=35.7936
	step [81/325], loss=33.8207
	step [82/325], loss=34.8805
	step [83/325], loss=33.2088
	step [84/325], loss=32.5544
	step [85/325], loss=32.9442
	step [86/325], loss=32.5161
	step [87/325], loss=34.7730
	step [88/325], loss=32.5503
	step [89/325], loss=31.6686
	step [90/325], loss=33.8562
	step [91/325], loss=33.7575
	step [92/325], loss=32.0888
	step [93/325], loss=32.6716
	step [94/325], loss=33.1244
	step [95/325], loss=32.5905
	step [96/325], loss=32.2063
	step [97/325], loss=31.5028
	step [98/325], loss=32.8387
	step [99/325], loss=33.5908
	step [100/325], loss=32.4361
	step [101/325], loss=32.7020
	step [102/325], loss=33.5729
	step [103/325], loss=31.6034
	step [104/325], loss=32.9059
	step [105/325], loss=32.4732
	step [106/325], loss=33.0800
	step [107/325], loss=32.0587
	step [108/325], loss=31.2763
	step [109/325], loss=35.0305
	step [110/325], loss=33.5757
	step [111/325], loss=32.0376
	step [112/325], loss=31.8567
	step [113/325], loss=32.3677
	step [114/325], loss=33.2356
	step [115/325], loss=30.9477
	step [116/325], loss=31.3178
	step [117/325], loss=30.7956
	step [118/325], loss=30.8465
	step [119/325], loss=30.6991
	step [120/325], loss=34.4197
	step [121/325], loss=31.2614
	step [122/325], loss=32.0422
	step [123/325], loss=31.5473
	step [124/325], loss=31.8340
	step [125/325], loss=32.3566
	step [126/325], loss=32.6039
	step [127/325], loss=31.5911
	step [128/325], loss=32.3989
	step [129/325], loss=31.4106
	step [130/325], loss=30.5941
	step [131/325], loss=30.9363
	step [132/325], loss=32.1458
	step [133/325], loss=30.6539
	step [134/325], loss=31.5621
	step [135/325], loss=32.0440
	step [136/325], loss=31.7505
	step [137/325], loss=30.5559
	step [138/325], loss=32.1802
	step [139/325], loss=31.3684
	step [140/325], loss=31.0897
	step [141/325], loss=30.2446
	step [142/325], loss=30.5381
	step [143/325], loss=33.0903
	step [144/325], loss=31.7761
	step [145/325], loss=30.4383
	step [146/325], loss=30.6282
	step [147/325], loss=29.7243
	step [148/325], loss=30.2327
	step [149/325], loss=31.3180
	step [150/325], loss=31.5040
	step [151/325], loss=29.7937
	step [152/325], loss=30.8336
	step [153/325], loss=29.7734
	step [154/325], loss=31.2643
	step [155/325], loss=30.1750
	step [156/325], loss=31.2336
	step [157/325], loss=30.3448
	step [158/325], loss=31.4517
	step [159/325], loss=30.4717
	step [160/325], loss=30.5246
	step [161/325], loss=30.5553
	step [162/325], loss=29.4920
	step [163/325], loss=29.1415
	step [164/325], loss=29.2550
	step [165/325], loss=29.2288
	step [166/325], loss=30.2248
	step [167/325], loss=30.3788
	step [168/325], loss=30.4636
	step [169/325], loss=28.7018
	step [170/325], loss=30.0529
	step [171/325], loss=30.6651
	step [172/325], loss=31.3141
	step [173/325], loss=29.3384
	step [174/325], loss=28.7398
	step [175/325], loss=30.4426
	step [176/325], loss=30.4016
	step [177/325], loss=29.4774
	step [178/325], loss=30.5207
	step [179/325], loss=30.5388
	step [180/325], loss=28.7861
	step [181/325], loss=29.1689
	step [182/325], loss=29.7515
	step [183/325], loss=28.1836
	step [184/325], loss=30.7256
	step [185/325], loss=29.9742
	step [186/325], loss=28.3657
	step [187/325], loss=28.9747
	step [188/325], loss=30.0315
	step [189/325], loss=27.6928
	step [190/325], loss=30.3334
	step [191/325], loss=28.3901
	step [192/325], loss=29.7457
	step [193/325], loss=29.9812
	step [194/325], loss=29.9572
	step [195/325], loss=27.5047
	step [196/325], loss=28.6876
	step [197/325], loss=27.7222
	step [198/325], loss=28.8298
	step [199/325], loss=28.3710
	step [200/325], loss=27.8275
	step [201/325], loss=28.9813
	step [202/325], loss=28.3395
	step [203/325], loss=27.8593
	step [204/325], loss=27.0826
	step [205/325], loss=28.6239
	step [206/325], loss=27.2111
	step [207/325], loss=28.8321
	step [208/325], loss=28.2510
	step [209/325], loss=26.9658
	step [210/325], loss=28.6713
	step [211/325], loss=27.2061
	step [212/325], loss=27.9710
	step [213/325], loss=29.4611
	step [214/325], loss=27.2495
	step [215/325], loss=27.9789
	step [216/325], loss=28.9484
	step [217/325], loss=27.2703
	step [218/325], loss=28.2612
	step [219/325], loss=28.7993
	step [220/325], loss=27.6864
	step [221/325], loss=28.0051
	step [222/325], loss=29.4933
	step [223/325], loss=27.2915
	step [224/325], loss=26.6773
	step [225/325], loss=27.7028
	step [226/325], loss=26.8170
	step [227/325], loss=27.4245
	step [228/325], loss=26.1761
	step [229/325], loss=27.1885
	step [230/325], loss=27.5284
	step [231/325], loss=27.5902
	step [232/325], loss=27.2586
	step [233/325], loss=26.8334
	step [234/325], loss=28.7272
	step [235/325], loss=26.7137
	step [236/325], loss=27.2331
	step [237/325], loss=27.3435
	step [238/325], loss=26.1110
	step [239/325], loss=27.6702
	step [240/325], loss=26.5233
	step [241/325], loss=28.5811
	step [242/325], loss=26.1813
	step [243/325], loss=25.8794
	step [244/325], loss=28.4667
	step [245/325], loss=26.6215
	step [246/325], loss=26.9875
	step [247/325], loss=25.5669
	step [248/325], loss=27.6525
	step [249/325], loss=25.2982
	step [250/325], loss=26.0592
	step [251/325], loss=26.8249
	step [252/325], loss=28.8711
	step [253/325], loss=27.1583
	step [254/325], loss=26.2513
	step [255/325], loss=25.3412
	step [256/325], loss=24.3491
	step [257/325], loss=26.9523
	step [258/325], loss=26.1093
	step [259/325], loss=26.3663
	step [260/325], loss=25.0495
	step [261/325], loss=26.6367
	step [262/325], loss=25.6225
	step [263/325], loss=25.7062
	step [264/325], loss=26.1227
	step [265/325], loss=26.1392
	step [266/325], loss=25.4714
	step [267/325], loss=25.3063
	step [268/325], loss=24.8382
	step [269/325], loss=26.5099
	step [270/325], loss=26.2062
	step [271/325], loss=27.3238
	step [272/325], loss=24.5845
	step [273/325], loss=25.2557
	step [274/325], loss=25.9602
	step [275/325], loss=27.4249
	step [276/325], loss=25.2427
	step [277/325], loss=26.9606
	step [278/325], loss=25.0618
	step [279/325], loss=24.7537
	step [280/325], loss=26.2443
	step [281/325], loss=24.9693
	step [282/325], loss=27.7396
	step [283/325], loss=26.1829
	step [284/325], loss=24.7712
	step [285/325], loss=24.1639
	step [286/325], loss=25.4913
	step [287/325], loss=25.4315
	step [288/325], loss=26.4784
	step [289/325], loss=25.2472
	step [290/325], loss=24.5623
	step [291/325], loss=28.1795
	step [292/325], loss=25.3220
	step [293/325], loss=26.2826
	step [294/325], loss=27.0114
	step [295/325], loss=23.8964
	step [296/325], loss=25.3087
	step [297/325], loss=25.5414
	step [298/325], loss=25.7693
	step [299/325], loss=25.1804
	step [300/325], loss=25.7806
	step [301/325], loss=24.2446
	step [302/325], loss=24.2489
	step [303/325], loss=23.7461
	step [304/325], loss=25.5532
	step [305/325], loss=25.0358
	step [306/325], loss=25.1041
	step [307/325], loss=23.4084
	step [308/325], loss=25.3665
	step [309/325], loss=25.3075
	step [310/325], loss=24.2333
	step [311/325], loss=23.9680
	step [312/325], loss=25.4320
	step [313/325], loss=25.0958
	step [314/325], loss=26.5054
	step [315/325], loss=26.1246
	step [316/325], loss=25.4873
	step [317/325], loss=24.4601
	step [318/325], loss=25.3137
	step [319/325], loss=23.1852
	step [320/325], loss=24.6565
	step [321/325], loss=26.4068
	step [322/325], loss=23.1686
	step [323/325], loss=24.5261
	step [324/325], loss=24.3613
	step [325/325], loss=1.4636
	Evaluating
	loss=0.1264, precision=0.1654, recall=0.9977, f1=0.2837
saving model as: 2_saved_model.pth
Training epoch 4
	step [1/325], loss=24.6361
	step [2/325], loss=24.2438
	step [3/325], loss=23.3892
	step [4/325], loss=24.0484
	step [5/325], loss=25.2435
	step [6/325], loss=23.6269
	step [7/325], loss=23.3546
	step [8/325], loss=24.8271
	step [9/325], loss=25.0275
	step [10/325], loss=23.4745
	step [11/325], loss=23.7893
	step [12/325], loss=26.0811
	step [13/325], loss=24.9489
	step [14/325], loss=24.4119
	step [15/325], loss=23.2111
	step [16/325], loss=24.1670
	step [17/325], loss=23.7059
	step [18/325], loss=26.5310
	step [19/325], loss=25.3876
	step [20/325], loss=23.1769
	step [21/325], loss=25.1005
	step [22/325], loss=23.5112
	step [23/325], loss=24.3106
	step [24/325], loss=25.3515
	step [25/325], loss=23.5810
	step [26/325], loss=23.5597
	step [27/325], loss=24.5340
	step [28/325], loss=25.3025
	step [29/325], loss=24.5019
	step [30/325], loss=22.5931
	step [31/325], loss=23.6527
	step [32/325], loss=25.0345
	step [33/325], loss=23.3355
	step [34/325], loss=24.6463
	step [35/325], loss=24.0377
	step [36/325], loss=24.2095
	step [37/325], loss=25.3730
	step [38/325], loss=23.4940
	step [39/325], loss=25.1039
	step [40/325], loss=23.0720
	step [41/325], loss=21.7988
	step [42/325], loss=22.0744
	step [43/325], loss=24.0942
	step [44/325], loss=23.4938
	step [45/325], loss=23.9025
	step [46/325], loss=22.9861
	step [47/325], loss=22.6324
	step [48/325], loss=23.6042
	step [49/325], loss=22.5114
	step [50/325], loss=24.0584
	step [51/325], loss=22.5143
	step [52/325], loss=24.3995
	step [53/325], loss=23.3217
	step [54/325], loss=24.7659
	step [55/325], loss=23.8871
	step [56/325], loss=23.3467
	step [57/325], loss=23.6360
	step [58/325], loss=25.6051
	step [59/325], loss=24.4849
	step [60/325], loss=23.4467
	step [61/325], loss=21.5319
	step [62/325], loss=22.2255
	step [63/325], loss=23.6415
	step [64/325], loss=22.1644
	step [65/325], loss=22.6162
	step [66/325], loss=22.2719
	step [67/325], loss=22.4564
	step [68/325], loss=22.7071
	step [69/325], loss=24.5167
	step [70/325], loss=23.2682
	step [71/325], loss=23.1558
	step [72/325], loss=22.4679
	step [73/325], loss=22.6932
	step [74/325], loss=23.1314
	step [75/325], loss=22.5890
	step [76/325], loss=21.4675
	step [77/325], loss=22.6150
	step [78/325], loss=21.3211
	step [79/325], loss=20.1936
	step [80/325], loss=22.3672
	step [81/325], loss=23.7876
	step [82/325], loss=22.2639
	step [83/325], loss=22.0097
	step [84/325], loss=22.3760
	step [85/325], loss=22.3043
	step [86/325], loss=23.1667
	step [87/325], loss=22.9981
	step [88/325], loss=20.8175
	step [89/325], loss=21.7402
	step [90/325], loss=21.7647
	step [91/325], loss=25.0007
	step [92/325], loss=23.4843
	step [93/325], loss=22.0059
	step [94/325], loss=21.2563
	step [95/325], loss=21.3487
	step [96/325], loss=22.3327
	step [97/325], loss=22.1521
	step [98/325], loss=21.9554
	step [99/325], loss=21.5174
	step [100/325], loss=21.9736
	step [101/325], loss=21.7478
	step [102/325], loss=23.4111
	step [103/325], loss=21.6664
	step [104/325], loss=22.5977
	step [105/325], loss=20.6825
	step [106/325], loss=20.4866
	step [107/325], loss=20.7050
	step [108/325], loss=22.0138
	step [109/325], loss=23.0632
	step [110/325], loss=21.8898
	step [111/325], loss=22.6860
	step [112/325], loss=21.6382
	step [113/325], loss=22.0538
	step [114/325], loss=21.5486
	step [115/325], loss=21.4417
	step [116/325], loss=22.6559
	step [117/325], loss=24.0624
	step [118/325], loss=21.5623
	step [119/325], loss=22.7349
	step [120/325], loss=20.9769
	step [121/325], loss=22.2093
	step [122/325], loss=21.5829
	step [123/325], loss=20.8004
	step [124/325], loss=21.0832
	step [125/325], loss=21.5213
	step [126/325], loss=21.1071
	step [127/325], loss=20.5053
	step [128/325], loss=22.2461
	step [129/325], loss=19.8171
	step [130/325], loss=22.5901
	step [131/325], loss=21.2088
	step [132/325], loss=22.6181
	step [133/325], loss=21.5342
	step [134/325], loss=20.5750
	step [135/325], loss=20.7613
	step [136/325], loss=21.3697
	step [137/325], loss=21.2887
	step [138/325], loss=22.3598
	step [139/325], loss=20.8555
	step [140/325], loss=19.8677
	step [141/325], loss=20.5118
	step [142/325], loss=22.4693
	step [143/325], loss=23.4030
	step [144/325], loss=22.5575
	step [145/325], loss=21.3463
	step [146/325], loss=21.7650
	step [147/325], loss=20.3462
	step [148/325], loss=21.2308
	step [149/325], loss=21.1164
	step [150/325], loss=22.1778
	step [151/325], loss=23.0907
	step [152/325], loss=20.1910
	step [153/325], loss=21.8999
	step [154/325], loss=20.7932
	step [155/325], loss=20.2558
	step [156/325], loss=20.6400
	step [157/325], loss=22.1145
	step [158/325], loss=20.4137
	step [159/325], loss=21.6776
	step [160/325], loss=19.4751
	step [161/325], loss=20.4527
	step [162/325], loss=20.6878
	step [163/325], loss=20.0787
	step [164/325], loss=21.0205
	step [165/325], loss=20.4192
	step [166/325], loss=19.6257
	step [167/325], loss=20.8778
	step [168/325], loss=18.7564
	step [169/325], loss=20.5245
	step [170/325], loss=19.5108
	step [171/325], loss=20.7243
	step [172/325], loss=19.8506
	step [173/325], loss=20.7116
	step [174/325], loss=21.1936
	step [175/325], loss=20.6658
	step [176/325], loss=20.4173
	step [177/325], loss=19.1736
	step [178/325], loss=18.2806
	step [179/325], loss=20.6460
	step [180/325], loss=20.6708
	step [181/325], loss=19.9672
	step [182/325], loss=19.3847
	step [183/325], loss=20.4991
	step [184/325], loss=19.6829
	step [185/325], loss=21.5112
	step [186/325], loss=18.7855
	step [187/325], loss=18.7365
	step [188/325], loss=19.9449
	step [189/325], loss=20.8270
	step [190/325], loss=19.6552
	step [191/325], loss=18.9898
	step [192/325], loss=20.3886
	step [193/325], loss=20.0053
	step [194/325], loss=21.4046
	step [195/325], loss=19.2051
	step [196/325], loss=19.9324
	step [197/325], loss=20.3819
	step [198/325], loss=20.9407
	step [199/325], loss=19.7258
	step [200/325], loss=20.6978
	step [201/325], loss=18.5423
	step [202/325], loss=20.8347
	step [203/325], loss=19.6288
	step [204/325], loss=19.4389
	step [205/325], loss=19.1823
	step [206/325], loss=19.5689
	step [207/325], loss=19.2524
	step [208/325], loss=19.9510
	step [209/325], loss=20.2923
	step [210/325], loss=18.7921
	step [211/325], loss=18.3087
	step [212/325], loss=18.5506
	step [213/325], loss=19.6237
	step [214/325], loss=19.6885
	step [215/325], loss=20.0727
	step [216/325], loss=18.2393
	step [217/325], loss=20.3814
	step [218/325], loss=18.5789
	step [219/325], loss=19.6008
	step [220/325], loss=19.2080
	step [221/325], loss=19.1490
	step [222/325], loss=21.8107
	step [223/325], loss=19.3918
	step [224/325], loss=20.6480
	step [225/325], loss=17.8260
	step [226/325], loss=18.7426
	step [227/325], loss=18.2982
	step [228/325], loss=17.9710
	step [229/325], loss=18.8022
	step [230/325], loss=17.9603
	step [231/325], loss=20.3575
	step [232/325], loss=18.8342
	step [233/325], loss=19.8697
	step [234/325], loss=19.8099
	step [235/325], loss=20.4089
	step [236/325], loss=19.2294
	step [237/325], loss=18.6803
	step [238/325], loss=19.5864
	step [239/325], loss=19.1108
	step [240/325], loss=18.7051
	step [241/325], loss=18.2273
	step [242/325], loss=19.2681
	step [243/325], loss=18.5215
	step [244/325], loss=18.9484
	step [245/325], loss=21.1259
	step [246/325], loss=20.5534
	step [247/325], loss=19.2299
	step [248/325], loss=18.8305
	step [249/325], loss=19.5045
	step [250/325], loss=18.9062
	step [251/325], loss=18.4384
	step [252/325], loss=18.8543
	step [253/325], loss=19.1909
	step [254/325], loss=21.3484
	step [255/325], loss=20.2146
	step [256/325], loss=19.6280
	step [257/325], loss=18.7198
	step [258/325], loss=19.3216
	step [259/325], loss=19.2747
	step [260/325], loss=19.5139
	step [261/325], loss=17.7326
	step [262/325], loss=18.7469
	step [263/325], loss=18.0099
	step [264/325], loss=19.1369
	step [265/325], loss=19.4205
	step [266/325], loss=19.3797
	step [267/325], loss=19.8527
	step [268/325], loss=17.7319
	step [269/325], loss=19.4687
	step [270/325], loss=18.6450
	step [271/325], loss=20.1566
	step [272/325], loss=17.5924
	step [273/325], loss=17.2640
	step [274/325], loss=19.0972
	step [275/325], loss=17.1762
	step [276/325], loss=17.4590
	step [277/325], loss=16.3734
	step [278/325], loss=18.2621
	step [279/325], loss=19.7459
	step [280/325], loss=18.1834
	step [281/325], loss=17.6042
	step [282/325], loss=17.9809
	step [283/325], loss=17.9188
	step [284/325], loss=18.8801
	step [285/325], loss=18.1107
	step [286/325], loss=17.3418
	step [287/325], loss=16.8567
	step [288/325], loss=17.6578
	step [289/325], loss=16.4838
	step [290/325], loss=16.3287
	step [291/325], loss=18.3896
	step [292/325], loss=18.2275
	step [293/325], loss=18.1428
	step [294/325], loss=16.9490
	step [295/325], loss=17.9276
	step [296/325], loss=18.2589
	step [297/325], loss=17.8333
	step [298/325], loss=17.7410
	step [299/325], loss=17.3164
	step [300/325], loss=19.4011
	step [301/325], loss=20.4501
	step [302/325], loss=18.0786
	step [303/325], loss=17.9742
	step [304/325], loss=18.9842
	step [305/325], loss=18.6629
	step [306/325], loss=17.4052
	step [307/325], loss=17.2562
	step [308/325], loss=19.1025
	step [309/325], loss=17.4688
	step [310/325], loss=18.2736
	step [311/325], loss=15.4641
	step [312/325], loss=17.3748
	step [313/325], loss=16.1885
	step [314/325], loss=17.1861
	step [315/325], loss=18.4923
	step [316/325], loss=16.5007
	step [317/325], loss=19.1288
	step [318/325], loss=17.1650
	step [319/325], loss=18.7693
	step [320/325], loss=18.9271
	step [321/325], loss=18.1315
	step [322/325], loss=17.6575
	step [323/325], loss=17.5475
	step [324/325], loss=18.7671
	step [325/325], loss=0.7932
	Evaluating
	loss=0.0850, precision=0.1418, recall=0.9982, f1=0.2483
Training epoch 5
	step [1/325], loss=16.7307
	step [2/325], loss=16.7276
	step [3/325], loss=16.7698
	step [4/325], loss=17.3377
	step [5/325], loss=17.2951
	step [6/325], loss=16.7819
	step [7/325], loss=17.5070
	step [8/325], loss=18.1323
	step [9/325], loss=16.8292
	step [10/325], loss=17.9144
	step [11/325], loss=16.5074
	step [12/325], loss=21.2643
	step [13/325], loss=18.2784
	step [14/325], loss=16.8166
	step [15/325], loss=16.9675
	step [16/325], loss=18.1108
	step [17/325], loss=17.1460
	step [18/325], loss=16.5719
	step [19/325], loss=17.4351
	step [20/325], loss=17.8809
	step [21/325], loss=17.5753
	step [22/325], loss=17.4422
	step [23/325], loss=16.2008
	step [24/325], loss=18.1325
	step [25/325], loss=16.8245
	step [26/325], loss=16.8497
	step [27/325], loss=16.2513
	step [28/325], loss=16.4220
	step [29/325], loss=15.6675
	step [30/325], loss=16.7929
	step [31/325], loss=16.2528
	step [32/325], loss=18.5741
	step [33/325], loss=15.9828
	step [34/325], loss=18.7657
	step [35/325], loss=16.5242
	step [36/325], loss=17.1421
	step [37/325], loss=17.0287
	step [38/325], loss=16.4268
	step [39/325], loss=15.8196
	step [40/325], loss=15.9072
	step [41/325], loss=16.3388
	step [42/325], loss=16.8588
	step [43/325], loss=15.3760
	step [44/325], loss=18.0993
	step [45/325], loss=15.7806
	step [46/325], loss=16.9192
	step [47/325], loss=16.7735
	step [48/325], loss=16.4801
	step [49/325], loss=16.6102
	step [50/325], loss=17.7630
	step [51/325], loss=19.4988
	step [52/325], loss=17.7408
	step [53/325], loss=17.6313
	step [54/325], loss=17.3074
	step [55/325], loss=16.5670
	step [56/325], loss=17.9519
	step [57/325], loss=16.3707
	step [58/325], loss=16.5978
	step [59/325], loss=17.2145
	step [60/325], loss=15.8003
	step [61/325], loss=16.4592
	step [62/325], loss=16.2350
	step [63/325], loss=19.7494
	step [64/325], loss=16.9714
	step [65/325], loss=16.4443
	step [66/325], loss=15.9021
	step [67/325], loss=16.3222
	step [68/325], loss=17.7950
	step [69/325], loss=17.4919
	step [70/325], loss=16.0538
	step [71/325], loss=17.0451
	step [72/325], loss=16.9377
	step [73/325], loss=15.9175
	step [74/325], loss=15.6403
	step [75/325], loss=17.0079
	step [76/325], loss=15.6502
	step [77/325], loss=15.3655
	step [78/325], loss=14.9856
	step [79/325], loss=15.6489
	step [80/325], loss=15.7002
	step [81/325], loss=16.0053
	step [82/325], loss=17.9948
	step [83/325], loss=16.2213
	step [84/325], loss=17.0534
	step [85/325], loss=18.3575
	step [86/325], loss=16.5434
	step [87/325], loss=15.2257
	step [88/325], loss=15.4861
	step [89/325], loss=16.9892
	step [90/325], loss=16.0793
	step [91/325], loss=17.1199
	step [92/325], loss=15.5668
	step [93/325], loss=16.7299
	step [94/325], loss=16.4030
	step [95/325], loss=18.2656
	step [96/325], loss=16.3350
	step [97/325], loss=16.0191
	step [98/325], loss=14.5461
	step [99/325], loss=16.8601
	step [100/325], loss=17.5028
	step [101/325], loss=16.8830
	step [102/325], loss=15.0869
	step [103/325], loss=16.1151
	step [104/325], loss=17.5953
	step [105/325], loss=15.5899
	step [106/325], loss=15.7002
	step [107/325], loss=15.3794
	step [108/325], loss=15.1832
	step [109/325], loss=16.3108
	step [110/325], loss=18.9842
	step [111/325], loss=15.3676
	step [112/325], loss=17.2889
	step [113/325], loss=17.6854
	step [114/325], loss=14.9896
	step [115/325], loss=17.2328
	step [116/325], loss=15.5794
	step [117/325], loss=15.7750
	step [118/325], loss=17.6176
	step [119/325], loss=16.7308
	step [120/325], loss=15.7116
	step [121/325], loss=16.2377
	step [122/325], loss=15.5349
	step [123/325], loss=15.1055
	step [124/325], loss=16.0612
	step [125/325], loss=14.7201
	step [126/325], loss=16.2486
	step [127/325], loss=17.4305
	step [128/325], loss=14.9953
	step [129/325], loss=16.9185
	step [130/325], loss=14.8906
	step [131/325], loss=16.7339
	step [132/325], loss=14.6250
	step [133/325], loss=17.7333
	step [134/325], loss=18.5769
	step [135/325], loss=16.6860
	step [136/325], loss=14.9624
	step [137/325], loss=15.6769
	step [138/325], loss=16.4884
	step [139/325], loss=16.5726
	step [140/325], loss=16.5686
	step [141/325], loss=16.4741
	step [142/325], loss=16.0455
	step [143/325], loss=14.9405
	step [144/325], loss=14.7940
	step [145/325], loss=15.1629
	step [146/325], loss=14.9242
	step [147/325], loss=17.1839
	step [148/325], loss=13.9406
	step [149/325], loss=16.3391
	step [150/325], loss=17.0112
	step [151/325], loss=15.5802
	step [152/325], loss=13.9783
	step [153/325], loss=16.3113
	step [154/325], loss=15.7275
	step [155/325], loss=14.8142
	step [156/325], loss=13.9774
	step [157/325], loss=14.8016
	step [158/325], loss=15.7104
	step [159/325], loss=16.1015
	step [160/325], loss=14.4124
	step [161/325], loss=19.4495
	step [162/325], loss=15.3109
	step [163/325], loss=16.4785
	step [164/325], loss=15.4986
	step [165/325], loss=16.7082
	step [166/325], loss=16.7788
	step [167/325], loss=15.4435
	step [168/325], loss=15.8007
	step [169/325], loss=14.3226
	step [170/325], loss=14.4766
	step [171/325], loss=15.2063
	step [172/325], loss=16.0030
	step [173/325], loss=16.6649
	step [174/325], loss=15.2665
	step [175/325], loss=15.5501
	step [176/325], loss=14.0789
	step [177/325], loss=14.7655
	step [178/325], loss=15.6073
	step [179/325], loss=14.3487
	step [180/325], loss=17.2266
	step [181/325], loss=13.6256
	step [182/325], loss=15.2332
	step [183/325], loss=14.3663
	step [184/325], loss=15.5206
	step [185/325], loss=13.5648
	step [186/325], loss=13.3534
	step [187/325], loss=16.1666
	step [188/325], loss=15.3990
	step [189/325], loss=15.4877
	step [190/325], loss=13.8672
	step [191/325], loss=15.7849
	step [192/325], loss=15.4110
	step [193/325], loss=13.2522
	step [194/325], loss=14.5098
	step [195/325], loss=15.5853
	step [196/325], loss=15.1484
	step [197/325], loss=15.1043
	step [198/325], loss=15.5050
	step [199/325], loss=15.8497
	step [200/325], loss=15.2749
	step [201/325], loss=17.1943
	step [202/325], loss=15.9852
	step [203/325], loss=13.9215
	step [204/325], loss=16.0063
	step [205/325], loss=15.3259
	step [206/325], loss=13.0089
	step [207/325], loss=15.2479
	step [208/325], loss=14.7609
	step [209/325], loss=15.3823
	step [210/325], loss=14.4511
	step [211/325], loss=16.0516
	step [212/325], loss=16.2866
	step [213/325], loss=16.7885
	step [214/325], loss=15.6374
	step [215/325], loss=15.2614
	step [216/325], loss=14.5150
	step [217/325], loss=14.3459
	step [218/325], loss=14.2836
	step [219/325], loss=16.3105
	step [220/325], loss=15.5621
	step [221/325], loss=13.5362
	step [222/325], loss=14.3972
	step [223/325], loss=15.4752
	step [224/325], loss=16.1305
	step [225/325], loss=14.3466
	step [226/325], loss=15.0980
	step [227/325], loss=14.6282
	step [228/325], loss=14.6621
	step [229/325], loss=13.6925
	step [230/325], loss=14.4351
	step [231/325], loss=14.4384
	step [232/325], loss=16.4088
	step [233/325], loss=14.6529
	step [234/325], loss=14.1062
	step [235/325], loss=13.4783
	step [236/325], loss=13.2605
	step [237/325], loss=16.5583
	step [238/325], loss=16.2002
	step [239/325], loss=17.1058
	step [240/325], loss=15.1919
	step [241/325], loss=14.8943
	step [242/325], loss=15.4521
	step [243/325], loss=15.3389
	step [244/325], loss=14.6790
	step [245/325], loss=13.9711
	step [246/325], loss=16.3421
	step [247/325], loss=14.7252
	step [248/325], loss=13.8211
	step [249/325], loss=13.8730
	step [250/325], loss=16.2935
	step [251/325], loss=16.2258
	step [252/325], loss=12.3307
	step [253/325], loss=14.1229
	step [254/325], loss=14.0927
	step [255/325], loss=14.5729
	step [256/325], loss=15.2210
	step [257/325], loss=14.6869
	step [258/325], loss=14.3481
	step [259/325], loss=13.8236
	step [260/325], loss=14.6439
	step [261/325], loss=14.8291
	step [262/325], loss=13.2879
	step [263/325], loss=14.3022
	step [264/325], loss=13.9224
	step [265/325], loss=13.8118
	step [266/325], loss=14.3556
	step [267/325], loss=13.5764
	step [268/325], loss=13.6514
	step [269/325], loss=14.3812
	step [270/325], loss=15.5655
	step [271/325], loss=15.4167
	step [272/325], loss=14.3875
	step [273/325], loss=15.8350
	step [274/325], loss=15.5828
	step [275/325], loss=13.6790
	step [276/325], loss=15.6372
	step [277/325], loss=13.9694
	step [278/325], loss=14.4496
	step [279/325], loss=14.9959
	step [280/325], loss=14.3238
	step [281/325], loss=14.3196
	step [282/325], loss=15.0961
	step [283/325], loss=14.0507
	step [284/325], loss=14.5652
	step [285/325], loss=14.2674
	step [286/325], loss=14.2446
	step [287/325], loss=15.2592
	step [288/325], loss=16.6964
	step [289/325], loss=15.3361
	step [290/325], loss=13.2848
	step [291/325], loss=13.4215
	step [292/325], loss=14.1001
	step [293/325], loss=12.8405
	step [294/325], loss=14.8742
	step [295/325], loss=12.3955
	step [296/325], loss=13.0298
	step [297/325], loss=14.9555
	step [298/325], loss=13.4071
	step [299/325], loss=13.0298
	step [300/325], loss=13.5221
	step [301/325], loss=14.8880
	step [302/325], loss=13.1492
	step [303/325], loss=14.1013
	step [304/325], loss=17.1981
	step [305/325], loss=14.0215
	step [306/325], loss=14.7515
	step [307/325], loss=13.4729
	step [308/325], loss=14.2659
	step [309/325], loss=14.2237
	step [310/325], loss=12.7315
	step [311/325], loss=13.4371
	step [312/325], loss=14.1024
	step [313/325], loss=14.8212
	step [314/325], loss=13.4376
	step [315/325], loss=14.1898
	step [316/325], loss=14.0741
	step [317/325], loss=15.4751
	step [318/325], loss=16.5441
	step [319/325], loss=12.1084
	step [320/325], loss=14.2772
	step [321/325], loss=13.6234
	step [322/325], loss=13.7461
	step [323/325], loss=13.5269
	step [324/325], loss=14.8544
	step [325/325], loss=0.5971
	Evaluating
	loss=0.0624, precision=0.1484, recall=0.9980, f1=0.2583
Training epoch 6
	step [1/325], loss=16.1682
	step [2/325], loss=15.1824
	step [3/325], loss=15.6458
	step [4/325], loss=14.2983
	step [5/325], loss=16.0059
	step [6/325], loss=14.2355
	step [7/325], loss=12.6591
	step [8/325], loss=12.9734
	step [9/325], loss=13.5943
	step [10/325], loss=13.0518
	step [11/325], loss=13.5505
	step [12/325], loss=14.1828
	step [13/325], loss=13.9770
	step [14/325], loss=13.9784
	step [15/325], loss=11.9518
	step [16/325], loss=13.2395
	step [17/325], loss=14.4969
	step [18/325], loss=13.7171
	step [19/325], loss=12.6963
	step [20/325], loss=12.6105
	step [21/325], loss=13.0747
	step [22/325], loss=12.8680
	step [23/325], loss=19.1093
	step [24/325], loss=17.0925
	step [25/325], loss=14.3551
	step [26/325], loss=13.3030
	step [27/325], loss=15.0690
	step [28/325], loss=13.3992
	step [29/325], loss=13.2326
	step [30/325], loss=14.0584
	step [31/325], loss=13.8391
	step [32/325], loss=13.0306
	step [33/325], loss=14.2588
	step [34/325], loss=13.4581
	step [35/325], loss=13.8434
	step [36/325], loss=13.4850
	step [37/325], loss=13.1767
	step [38/325], loss=14.9960
	step [39/325], loss=13.2889
	step [40/325], loss=14.5560
	step [41/325], loss=13.0124
	step [42/325], loss=14.2852
	step [43/325], loss=13.8552
	step [44/325], loss=13.8678
	step [45/325], loss=14.0857
	step [46/325], loss=13.4016
	step [47/325], loss=13.8866
	step [48/325], loss=14.4915
	step [49/325], loss=14.3884
	step [50/325], loss=12.4406
	step [51/325], loss=13.3088
	step [52/325], loss=12.6317
	step [53/325], loss=13.3019
	step [54/325], loss=12.9699
	step [55/325], loss=12.9458
	step [56/325], loss=13.2260
	step [57/325], loss=13.3420
	step [58/325], loss=12.6108
	step [59/325], loss=12.7436
	step [60/325], loss=13.1467
	step [61/325], loss=13.2728
	step [62/325], loss=14.5754
	step [63/325], loss=13.2069
	step [64/325], loss=12.9121
	step [65/325], loss=14.6498
	step [66/325], loss=11.7382
	step [67/325], loss=11.5347
	step [68/325], loss=13.3657
	step [69/325], loss=11.7060
	step [70/325], loss=12.7848
	step [71/325], loss=13.6310
	step [72/325], loss=12.0499
	step [73/325], loss=12.2271
	step [74/325], loss=11.4209
	step [75/325], loss=12.5655
	step [76/325], loss=13.9306
	step [77/325], loss=14.0793
	step [78/325], loss=11.8529
	step [79/325], loss=11.5972
	step [80/325], loss=12.5154
	step [81/325], loss=11.2644
	step [82/325], loss=13.6213
	step [83/325], loss=15.0947
	step [84/325], loss=16.1791
	step [85/325], loss=11.7720
	step [86/325], loss=12.2402
	step [87/325], loss=14.0530
	step [88/325], loss=13.3425
	step [89/325], loss=12.8320
	step [90/325], loss=13.5409
	step [91/325], loss=11.5563
	step [92/325], loss=13.3865
	step [93/325], loss=12.0169
	step [94/325], loss=12.9428
	step [95/325], loss=13.8354
	step [96/325], loss=12.5316
	step [97/325], loss=13.0627
	step [98/325], loss=12.8839
	step [99/325], loss=13.7957
	step [100/325], loss=12.2400
	step [101/325], loss=11.5070
	step [102/325], loss=13.0790
	step [103/325], loss=13.0501
	step [104/325], loss=13.4507
	step [105/325], loss=16.6070
	step [106/325], loss=13.6069
	step [107/325], loss=13.0946
	step [108/325], loss=12.2810
	step [109/325], loss=12.9324
	step [110/325], loss=12.9884
	step [111/325], loss=13.4639
	step [112/325], loss=14.4739
	step [113/325], loss=12.1036
	step [114/325], loss=12.9795
	step [115/325], loss=13.9853
	step [116/325], loss=10.8839
	step [117/325], loss=13.3365
	step [118/325], loss=13.5592
	step [119/325], loss=13.1299
	step [120/325], loss=11.5254
	step [121/325], loss=13.8276
	step [122/325], loss=12.3635
	step [123/325], loss=15.1189
	step [124/325], loss=12.1094
	step [125/325], loss=12.2225
	step [126/325], loss=12.3195
	step [127/325], loss=13.4022
	step [128/325], loss=12.2488
	step [129/325], loss=12.2595
	step [130/325], loss=13.6376
	step [131/325], loss=12.9640
	step [132/325], loss=11.9265
	step [133/325], loss=13.8504
	step [134/325], loss=12.2949
	step [135/325], loss=11.8970
	step [136/325], loss=12.0219
	step [137/325], loss=12.0465
	step [138/325], loss=13.0586
	step [139/325], loss=12.9774
	step [140/325], loss=12.1953
	step [141/325], loss=12.9808
	step [142/325], loss=12.2620
	step [143/325], loss=13.3289
	step [144/325], loss=12.8706
	step [145/325], loss=12.7980
	step [146/325], loss=14.0745
	step [147/325], loss=11.2755
	step [148/325], loss=11.9737
	step [149/325], loss=11.4583
	step [150/325], loss=12.5031
	step [151/325], loss=13.0153
	step [152/325], loss=10.7290
	step [153/325], loss=12.8843
	step [154/325], loss=12.8231
	step [155/325], loss=12.5554
	step [156/325], loss=11.8134
	step [157/325], loss=11.9931
	step [158/325], loss=14.2700
	step [159/325], loss=12.9946
	step [160/325], loss=10.1412
	step [161/325], loss=11.7409
	step [162/325], loss=13.8991
	step [163/325], loss=11.5379
	step [164/325], loss=12.3828
	step [165/325], loss=13.2555
	step [166/325], loss=14.9088
	step [167/325], loss=13.2265
	step [168/325], loss=12.4304
	step [169/325], loss=13.6476
	step [170/325], loss=11.6380
	step [171/325], loss=12.6786
	step [172/325], loss=11.5934
	step [173/325], loss=10.5413
	step [174/325], loss=13.2501
	step [175/325], loss=11.7324
	step [176/325], loss=11.9861
	step [177/325], loss=12.2567
	step [178/325], loss=11.2855
	step [179/325], loss=11.2500
	step [180/325], loss=12.5319
	step [181/325], loss=11.4739
	step [182/325], loss=12.2723
	step [183/325], loss=12.0876
	step [184/325], loss=12.4443
	step [185/325], loss=11.8924
	step [186/325], loss=11.4062
	step [187/325], loss=11.9776
	step [188/325], loss=11.1885
	step [189/325], loss=13.4576
	step [190/325], loss=11.9510
	step [191/325], loss=14.2381
	step [192/325], loss=11.8554
	step [193/325], loss=13.4619
	step [194/325], loss=11.9963
	step [195/325], loss=12.7112
	step [196/325], loss=12.3742
	step [197/325], loss=11.8260
	step [198/325], loss=12.1192
	step [199/325], loss=11.7984
	step [200/325], loss=13.2084
	step [201/325], loss=15.5656
	step [202/325], loss=10.9582
	step [203/325], loss=14.6236
	step [204/325], loss=11.5206
	step [205/325], loss=11.2633
	step [206/325], loss=11.9076
	step [207/325], loss=12.0879
	step [208/325], loss=13.0247
	step [209/325], loss=11.3508
	step [210/325], loss=12.6042
	step [211/325], loss=12.2836
	step [212/325], loss=12.3104
	step [213/325], loss=12.3552
	step [214/325], loss=13.2002
	step [215/325], loss=12.6358
	step [216/325], loss=12.5802
	step [217/325], loss=13.7992
	step [218/325], loss=11.9294
	step [219/325], loss=13.8014
	step [220/325], loss=13.6825
	step [221/325], loss=14.8600
	step [222/325], loss=11.7520
	step [223/325], loss=12.5993
	step [224/325], loss=12.0291
	step [225/325], loss=10.5172
	step [226/325], loss=12.7674
	step [227/325], loss=13.6373
	step [228/325], loss=11.4263
	step [229/325], loss=13.5544
	step [230/325], loss=11.1119
	step [231/325], loss=11.5301
	step [232/325], loss=11.5246
	step [233/325], loss=11.5813
	step [234/325], loss=10.8220
	step [235/325], loss=12.3672
	step [236/325], loss=11.7607
	step [237/325], loss=11.7654
	step [238/325], loss=10.8710
	step [239/325], loss=11.9220
	step [240/325], loss=10.8747
	step [241/325], loss=15.6715
	step [242/325], loss=11.1318
	step [243/325], loss=11.6472
	step [244/325], loss=10.9167
	step [245/325], loss=12.5414
	step [246/325], loss=11.5958
	step [247/325], loss=14.2297
	step [248/325], loss=11.8059
	step [249/325], loss=11.2861
	step [250/325], loss=11.3885
	step [251/325], loss=10.4360
	step [252/325], loss=10.9140
	step [253/325], loss=12.9620
	step [254/325], loss=12.1768
	step [255/325], loss=11.7416
	step [256/325], loss=12.9326
	step [257/325], loss=11.1526
	step [258/325], loss=12.4107
	step [259/325], loss=11.4949
	step [260/325], loss=12.4843
	step [261/325], loss=11.9482
	step [262/325], loss=11.5402
	step [263/325], loss=12.1963
	step [264/325], loss=10.0240
	step [265/325], loss=10.9955
	step [266/325], loss=12.6892
	step [267/325], loss=10.9086
	step [268/325], loss=11.3829
	step [269/325], loss=14.2475
	step [270/325], loss=11.1944
	step [271/325], loss=12.2436
	step [272/325], loss=14.2586
	step [273/325], loss=11.9626
	step [274/325], loss=11.0745
	step [275/325], loss=10.5755
	step [276/325], loss=12.7877
	step [277/325], loss=12.0038
	step [278/325], loss=13.8029
	step [279/325], loss=12.5073
	step [280/325], loss=11.7330
	step [281/325], loss=10.6610
	step [282/325], loss=12.0873
	step [283/325], loss=10.4292
	step [284/325], loss=12.0223
	step [285/325], loss=13.7040
	step [286/325], loss=12.2173
	step [287/325], loss=12.3720
	step [288/325], loss=11.6123
	step [289/325], loss=11.5914
	step [290/325], loss=11.9785
	step [291/325], loss=10.1706
	step [292/325], loss=11.9902
	step [293/325], loss=11.2853
	step [294/325], loss=14.2704
	step [295/325], loss=11.6991
	step [296/325], loss=10.8521
	step [297/325], loss=12.6889
	step [298/325], loss=11.0982
	step [299/325], loss=10.0072
	step [300/325], loss=10.5159
	step [301/325], loss=13.3066
	step [302/325], loss=10.8020
	step [303/325], loss=12.3370
	step [304/325], loss=11.6959
	step [305/325], loss=10.7688
	step [306/325], loss=11.4824
	step [307/325], loss=13.5252
	step [308/325], loss=11.2135
	step [309/325], loss=11.9994
	step [310/325], loss=10.0974
	step [311/325], loss=11.3755
	step [312/325], loss=10.5310
	step [313/325], loss=12.1327
	step [314/325], loss=9.4044
	step [315/325], loss=10.3901
	step [316/325], loss=11.8231
	step [317/325], loss=11.7154
	step [318/325], loss=13.0826
	step [319/325], loss=11.7376
	step [320/325], loss=10.2957
	step [321/325], loss=12.5654
	step [322/325], loss=10.7725
	step [323/325], loss=12.9822
	step [324/325], loss=11.6843
	step [325/325], loss=0.5334
	Evaluating
	loss=0.0525, precision=0.1502, recall=0.9980, f1=0.2610
Training epoch 7
	step [1/325], loss=10.8608
	step [2/325], loss=11.8991
	step [3/325], loss=12.0522
	step [4/325], loss=12.5604
	step [5/325], loss=10.8653
	step [6/325], loss=11.1257
	step [7/325], loss=11.7798
	step [8/325], loss=11.1477
	step [9/325], loss=12.7903
	step [10/325], loss=11.8836
	step [11/325], loss=11.6851
	step [12/325], loss=11.0809
	step [13/325], loss=10.9968
	step [14/325], loss=10.4981
	step [15/325], loss=11.3186
	step [16/325], loss=13.0441
	step [17/325], loss=12.5124
	step [18/325], loss=11.6327
	step [19/325], loss=11.6581
	step [20/325], loss=12.7720
	step [21/325], loss=12.5333
	step [22/325], loss=11.1334
	step [23/325], loss=11.0253
	step [24/325], loss=11.8282
	step [25/325], loss=10.4008
	step [26/325], loss=10.7757
	step [27/325], loss=10.9310
	step [28/325], loss=10.7544
	step [29/325], loss=13.0813
	step [30/325], loss=10.6797
	step [31/325], loss=11.3954
	step [32/325], loss=12.7048
	step [33/325], loss=11.4430
	step [34/325], loss=12.1848
	step [35/325], loss=12.6720
	step [36/325], loss=11.8754
	step [37/325], loss=12.4367
	step [38/325], loss=9.6882
	step [39/325], loss=11.8481
	step [40/325], loss=12.1646
	step [41/325], loss=11.5584
	step [42/325], loss=12.8392
	step [43/325], loss=10.7634
	step [44/325], loss=9.7199
	step [45/325], loss=10.7954
	step [46/325], loss=12.3321
	step [47/325], loss=10.2227
	step [48/325], loss=10.2117
	step [49/325], loss=12.3189
	step [50/325], loss=11.1548
	step [51/325], loss=11.2235
	step [52/325], loss=12.3243
	step [53/325], loss=10.7125
	step [54/325], loss=9.5705
	step [55/325], loss=9.9447
	step [56/325], loss=11.9208
	step [57/325], loss=11.7357
	step [58/325], loss=11.1324
	step [59/325], loss=10.4979
	step [60/325], loss=10.2925
	step [61/325], loss=11.3296
	step [62/325], loss=10.0769
	step [63/325], loss=11.8556
	step [64/325], loss=11.6413
	step [65/325], loss=10.7406
	step [66/325], loss=11.5611
	step [67/325], loss=10.0979
	step [68/325], loss=12.2229
	step [69/325], loss=10.1269
	step [70/325], loss=9.9456
	step [71/325], loss=9.4183
	step [72/325], loss=9.6359
	step [73/325], loss=10.5887
	step [74/325], loss=11.5444
	step [75/325], loss=11.1537
	step [76/325], loss=9.0063
	step [77/325], loss=10.4329
	step [78/325], loss=9.5171
	step [79/325], loss=11.3256
	step [80/325], loss=11.0891
	step [81/325], loss=12.1814
	step [82/325], loss=10.6562
	step [83/325], loss=10.7704
	step [84/325], loss=12.1671
	step [85/325], loss=9.7973
	step [86/325], loss=9.8108
	step [87/325], loss=10.7520
	step [88/325], loss=10.3297
	step [89/325], loss=10.7049
	step [90/325], loss=13.0941
	step [91/325], loss=10.1712
	step [92/325], loss=11.4811
	step [93/325], loss=11.0278
	step [94/325], loss=10.4211
	step [95/325], loss=13.0805
	step [96/325], loss=10.7681
	step [97/325], loss=10.1560
	step [98/325], loss=12.6025
	step [99/325], loss=10.4800
	step [100/325], loss=10.6214
	step [101/325], loss=11.6469
	step [102/325], loss=10.6461
	step [103/325], loss=13.3622
	step [104/325], loss=10.4764
	step [105/325], loss=9.8952
	step [106/325], loss=10.0342
	step [107/325], loss=9.2900
	step [108/325], loss=10.8040
	step [109/325], loss=10.2115
	step [110/325], loss=9.0951
	step [111/325], loss=10.4482
	step [112/325], loss=10.9353
	step [113/325], loss=11.4220
	step [114/325], loss=13.3109
	step [115/325], loss=10.6136
	step [116/325], loss=10.8628
	step [117/325], loss=10.7394
	step [118/325], loss=10.0733
	step [119/325], loss=11.0969
	step [120/325], loss=11.4671
	step [121/325], loss=10.2974
	step [122/325], loss=14.5318
	step [123/325], loss=10.4890
	step [124/325], loss=11.4427
	step [125/325], loss=10.8972
	step [126/325], loss=11.3051
	step [127/325], loss=11.6218
	step [128/325], loss=10.8893
	step [129/325], loss=11.2661
	step [130/325], loss=12.7988
	step [131/325], loss=9.7577
	step [132/325], loss=11.6139
	step [133/325], loss=11.7144
	step [134/325], loss=11.7813
	step [135/325], loss=12.5297
	step [136/325], loss=10.6822
	step [137/325], loss=10.5722
	step [138/325], loss=11.5806
	step [139/325], loss=12.8240
	step [140/325], loss=10.0997
	step [141/325], loss=10.4850
	step [142/325], loss=9.7856
	step [143/325], loss=8.8115
	step [144/325], loss=9.9125
	step [145/325], loss=14.8315
	step [146/325], loss=11.5750
	step [147/325], loss=10.3207
	step [148/325], loss=10.3149
	step [149/325], loss=9.4072
	step [150/325], loss=10.6919
	step [151/325], loss=11.5642
	step [152/325], loss=9.1851
	step [153/325], loss=10.3055
	step [154/325], loss=9.7037
	step [155/325], loss=11.3120
	step [156/325], loss=9.6633
	step [157/325], loss=11.8671
	step [158/325], loss=10.3837
	step [159/325], loss=11.4965
	step [160/325], loss=10.9034
	step [161/325], loss=10.2923
	step [162/325], loss=10.6386
	step [163/325], loss=10.3533
	step [164/325], loss=9.6661
	step [165/325], loss=12.9486
	step [166/325], loss=10.2269
	step [167/325], loss=12.0777
	step [168/325], loss=12.7011
	step [169/325], loss=9.7225
	step [170/325], loss=10.1900
	step [171/325], loss=9.1024
	step [172/325], loss=10.9744
	step [173/325], loss=9.7914
	step [174/325], loss=10.2867
	step [175/325], loss=10.8637
	step [176/325], loss=11.9569
	step [177/325], loss=11.4810
	step [178/325], loss=10.1629
	step [179/325], loss=11.0873
	step [180/325], loss=10.0639
	step [181/325], loss=10.0002
	step [182/325], loss=9.1193
	step [183/325], loss=12.1275
	step [184/325], loss=12.6703
	step [185/325], loss=12.8490
	step [186/325], loss=10.0462
	step [187/325], loss=10.0350
	step [188/325], loss=8.8273
	step [189/325], loss=9.0969
	step [190/325], loss=10.1918
	step [191/325], loss=10.9457
	step [192/325], loss=11.1550
	step [193/325], loss=9.7098
	step [194/325], loss=10.1015
	step [195/325], loss=11.4979
	step [196/325], loss=9.1180
	step [197/325], loss=9.0736
	step [198/325], loss=9.0516
	step [199/325], loss=8.8968
	step [200/325], loss=11.8822
	step [201/325], loss=9.9625
	step [202/325], loss=10.8086
	step [203/325], loss=12.1418
	step [204/325], loss=9.9708
	step [205/325], loss=10.2705
	step [206/325], loss=11.4931
	step [207/325], loss=9.2598
	step [208/325], loss=9.1071
	step [209/325], loss=10.0472
	step [210/325], loss=8.6578
	step [211/325], loss=11.7248
	step [212/325], loss=10.3932
	step [213/325], loss=10.8401
	step [214/325], loss=9.9519
	step [215/325], loss=11.6068
	step [216/325], loss=11.5391
	step [217/325], loss=8.4098
	step [218/325], loss=8.9838
	step [219/325], loss=9.0448
	step [220/325], loss=9.8848
	step [221/325], loss=10.7985
	step [222/325], loss=8.8981
	step [223/325], loss=11.1909
	step [224/325], loss=11.8998
	step [225/325], loss=11.0086
	step [226/325], loss=10.5510
	step [227/325], loss=10.1602
	step [228/325], loss=10.4895
	step [229/325], loss=10.4952
	step [230/325], loss=9.9335
	step [231/325], loss=8.9670
	step [232/325], loss=10.9533
	step [233/325], loss=9.2886
	step [234/325], loss=10.3684
	step [235/325], loss=11.6792
	step [236/325], loss=10.7330
	step [237/325], loss=8.9152
	step [238/325], loss=8.8854
	step [239/325], loss=11.1515
	step [240/325], loss=9.4935
	step [241/325], loss=10.6872
	step [242/325], loss=11.4177
	step [243/325], loss=10.9476
	step [244/325], loss=11.3934
	step [245/325], loss=9.6138
	step [246/325], loss=10.5716
	step [247/325], loss=10.8540
	step [248/325], loss=11.1319
	step [249/325], loss=9.4332
	step [250/325], loss=11.6122
	step [251/325], loss=11.1594
	step [252/325], loss=10.1663
	step [253/325], loss=10.3961
	step [254/325], loss=9.3199
	step [255/325], loss=11.7203
	step [256/325], loss=11.0856
	step [257/325], loss=10.2727
	step [258/325], loss=10.1678
	step [259/325], loss=9.5269
	step [260/325], loss=10.1256
	step [261/325], loss=10.2225
	step [262/325], loss=9.6421
	step [263/325], loss=10.6167
	step [264/325], loss=10.6610
	step [265/325], loss=12.8624
	step [266/325], loss=12.4061
	step [267/325], loss=10.5724
	step [268/325], loss=10.7800
	step [269/325], loss=10.6713
	step [270/325], loss=10.6357
	step [271/325], loss=10.0374
	step [272/325], loss=9.7872
	step [273/325], loss=10.5149
	step [274/325], loss=10.1972
	step [275/325], loss=10.9627
	step [276/325], loss=11.8532
	step [277/325], loss=8.9930
	step [278/325], loss=9.6249
	step [279/325], loss=9.6429
	step [280/325], loss=12.0282
	step [281/325], loss=11.5265
	step [282/325], loss=10.6886
	step [283/325], loss=8.9314
	step [284/325], loss=9.0910
	step [285/325], loss=9.8755
	step [286/325], loss=10.2309
	step [287/325], loss=9.3207
	step [288/325], loss=11.1656
	step [289/325], loss=10.0097
	step [290/325], loss=9.9671
	step [291/325], loss=8.9439
	step [292/325], loss=9.9905
	step [293/325], loss=10.1053
	step [294/325], loss=8.4595
	step [295/325], loss=11.8855
	step [296/325], loss=9.4653
	step [297/325], loss=10.0718
	step [298/325], loss=9.1849
	step [299/325], loss=10.7930
	step [300/325], loss=10.4493
	step [301/325], loss=10.0702
	step [302/325], loss=8.6138
	step [303/325], loss=11.1411
	step [304/325], loss=9.9692
	step [305/325], loss=10.0807
	step [306/325], loss=10.7580
	step [307/325], loss=9.3114
	step [308/325], loss=13.3125
	step [309/325], loss=10.7678
	step [310/325], loss=11.9559
	step [311/325], loss=8.5712
	step [312/325], loss=10.4467
	step [313/325], loss=9.0455
	step [314/325], loss=10.8863
	step [315/325], loss=9.2954
	step [316/325], loss=8.5296
	step [317/325], loss=9.1036
	step [318/325], loss=8.7620
	step [319/325], loss=10.1381
	step [320/325], loss=11.0709
	step [321/325], loss=9.1692
	step [322/325], loss=11.1080
	step [323/325], loss=11.6926
	step [324/325], loss=10.8936
	step [325/325], loss=0.3105
	Evaluating
	loss=0.0552, precision=0.1151, recall=0.9989, f1=0.2065
Training epoch 8
	step [1/325], loss=10.7412
	step [2/325], loss=10.6839
	step [3/325], loss=10.6303
	step [4/325], loss=8.8249
	step [5/325], loss=9.8063
	step [6/325], loss=9.2615
	step [7/325], loss=10.3384
	step [8/325], loss=10.6583
	step [9/325], loss=9.3688
	step [10/325], loss=10.0668
	step [11/325], loss=8.4441
	step [12/325], loss=11.1395
	step [13/325], loss=9.3978
	step [14/325], loss=9.1232
	step [15/325], loss=9.0308
	step [16/325], loss=10.9261
	step [17/325], loss=10.6323
	step [18/325], loss=8.8958
	step [19/325], loss=10.1339
	step [20/325], loss=10.1995
	step [21/325], loss=11.1239
	step [22/325], loss=8.9176
	step [23/325], loss=8.9743
	step [24/325], loss=10.7648
	step [25/325], loss=9.0913
	step [26/325], loss=9.4552
	step [27/325], loss=8.6599
	step [28/325], loss=9.4766
	step [29/325], loss=8.5408
	step [30/325], loss=9.8882
	step [31/325], loss=11.3072
	step [32/325], loss=9.6121
	step [33/325], loss=9.8648
	step [34/325], loss=8.4481
	step [35/325], loss=10.8037
	step [36/325], loss=10.2236
	step [37/325], loss=10.1079
	step [38/325], loss=11.3440
	step [39/325], loss=11.0470
	step [40/325], loss=9.7160
	step [41/325], loss=9.9689
	step [42/325], loss=10.0794
	step [43/325], loss=10.2537
	step [44/325], loss=10.7097
	step [45/325], loss=8.5277
	step [46/325], loss=9.4619
	step [47/325], loss=9.1930
	step [48/325], loss=11.6860
	step [49/325], loss=9.4326
	step [50/325], loss=9.8301
	step [51/325], loss=9.9541
	step [52/325], loss=9.3554
	step [53/325], loss=11.7598
	step [54/325], loss=10.7067
	step [55/325], loss=9.4486
	step [56/325], loss=11.1780
	step [57/325], loss=10.6252
	step [58/325], loss=11.7675
	step [59/325], loss=10.2368
	step [60/325], loss=9.7850
	step [61/325], loss=8.7055
	step [62/325], loss=10.6912
	step [63/325], loss=9.6399
	step [64/325], loss=9.2928
	step [65/325], loss=8.2328
	step [66/325], loss=9.8008
	step [67/325], loss=12.4508
	step [68/325], loss=9.8382
	step [69/325], loss=10.0159
	step [70/325], loss=8.9701
	step [71/325], loss=9.0991
	step [72/325], loss=10.3640
	step [73/325], loss=7.8537
	step [74/325], loss=10.0896
	step [75/325], loss=9.3798
	step [76/325], loss=8.3368
	step [77/325], loss=9.7051
	step [78/325], loss=9.6858
	step [79/325], loss=9.0879
	step [80/325], loss=10.4282
	step [81/325], loss=12.0347
	step [82/325], loss=9.3308
	step [83/325], loss=10.9904
	step [84/325], loss=10.0809
	step [85/325], loss=8.4817
	step [86/325], loss=8.0937
	step [87/325], loss=13.6625
	step [88/325], loss=9.9822
	step [89/325], loss=9.6153
	step [90/325], loss=10.5333
	step [91/325], loss=8.9834
	step [92/325], loss=9.5191
	step [93/325], loss=8.6280
	step [94/325], loss=8.7793
	step [95/325], loss=8.5812
	step [96/325], loss=10.2109
	step [97/325], loss=11.8430
	step [98/325], loss=10.0388
	step [99/325], loss=8.4297
	step [100/325], loss=7.6704
	step [101/325], loss=8.8248
	step [102/325], loss=9.8801
	step [103/325], loss=10.7168
	step [104/325], loss=8.9430
	step [105/325], loss=9.0266
	step [106/325], loss=9.7358
	step [107/325], loss=8.8264
	step [108/325], loss=9.6107
	step [109/325], loss=9.8672
	step [110/325], loss=10.2858
	step [111/325], loss=10.8016
	step [112/325], loss=8.9627
	step [113/325], loss=10.8624
	step [114/325], loss=12.8142
	step [115/325], loss=9.8097
	step [116/325], loss=9.8540
	step [117/325], loss=9.4582
	step [118/325], loss=9.1230
	step [119/325], loss=10.6024
	step [120/325], loss=10.0013
	step [121/325], loss=10.5124
	step [122/325], loss=8.7106
	step [123/325], loss=10.0654
	step [124/325], loss=9.3055
	step [125/325], loss=9.4702
	step [126/325], loss=9.0810
	step [127/325], loss=9.8015
	step [128/325], loss=9.2119
	step [129/325], loss=8.3126
	step [130/325], loss=10.5477
	step [131/325], loss=10.4664
	step [132/325], loss=9.2751
	step [133/325], loss=10.5933
	step [134/325], loss=9.5029
	step [135/325], loss=9.3170
	step [136/325], loss=9.9776
	step [137/325], loss=10.3357
	step [138/325], loss=9.6891
	step [139/325], loss=9.2434
	step [140/325], loss=9.4461
	step [141/325], loss=8.5185
	step [142/325], loss=10.0358
	step [143/325], loss=12.0394
	step [144/325], loss=9.8179
	step [145/325], loss=11.0932
	step [146/325], loss=10.5233
	step [147/325], loss=10.4308
	step [148/325], loss=8.7621
	step [149/325], loss=11.2215
	step [150/325], loss=8.5232
	step [151/325], loss=9.3571
	step [152/325], loss=10.6753
	step [153/325], loss=8.2093
	step [154/325], loss=11.1886
	step [155/325], loss=8.0148
	step [156/325], loss=8.4656
	step [157/325], loss=10.2675
	step [158/325], loss=8.2663
	step [159/325], loss=9.4788
	step [160/325], loss=8.5154
	step [161/325], loss=9.2701
	step [162/325], loss=9.0670
	step [163/325], loss=9.1855
	step [164/325], loss=8.5475
	step [165/325], loss=10.1346
	step [166/325], loss=9.5806
	step [167/325], loss=8.9661
	step [168/325], loss=9.0883
	step [169/325], loss=8.7803
	step [170/325], loss=9.2091
	step [171/325], loss=8.2013
	step [172/325], loss=11.7430
	step [173/325], loss=9.3183
	step [174/325], loss=8.4675
	step [175/325], loss=8.0324
	step [176/325], loss=10.7780
	step [177/325], loss=11.9236
	step [178/325], loss=7.7704
	step [179/325], loss=10.0712
	step [180/325], loss=10.0946
	step [181/325], loss=8.1231
	step [182/325], loss=7.0730
	step [183/325], loss=9.3410
	step [184/325], loss=10.8645
	step [185/325], loss=7.9566
	step [186/325], loss=8.7814
	step [187/325], loss=11.2483
	step [188/325], loss=8.8838
	step [189/325], loss=9.5773
	step [190/325], loss=9.0515
	step [191/325], loss=8.7682
	step [192/325], loss=8.9037
	step [193/325], loss=8.8436
	step [194/325], loss=10.0526
	step [195/325], loss=10.6790
	step [196/325], loss=9.0255
	step [197/325], loss=10.2452
	step [198/325], loss=8.1546
	step [199/325], loss=8.6685
	step [200/325], loss=9.2143
	step [201/325], loss=8.4704
	step [202/325], loss=8.0096
	step [203/325], loss=9.7846
	step [204/325], loss=8.9368
	step [205/325], loss=9.6051
	step [206/325], loss=9.5015
	step [207/325], loss=8.9858
	step [208/325], loss=9.0327
	step [209/325], loss=9.7914
	step [210/325], loss=10.0814
	step [211/325], loss=8.4632
	step [212/325], loss=6.8103
	step [213/325], loss=9.3034
	step [214/325], loss=9.1558
	step [215/325], loss=10.1583
	step [216/325], loss=9.3094
	step [217/325], loss=8.9155
	step [218/325], loss=9.5328
	step [219/325], loss=10.7769
	step [220/325], loss=8.3658
	step [221/325], loss=9.5257
	step [222/325], loss=9.2488
	step [223/325], loss=7.7916
	step [224/325], loss=8.7182
	step [225/325], loss=9.7479
	step [226/325], loss=9.8247
	step [227/325], loss=11.1714
	step [228/325], loss=8.9723
	step [229/325], loss=9.2681
	step [230/325], loss=7.8458
	step [231/325], loss=9.3246
	step [232/325], loss=8.5326
	step [233/325], loss=10.0362
	step [234/325], loss=9.8257
	step [235/325], loss=9.6970
	step [236/325], loss=7.6572
	step [237/325], loss=7.1918
	step [238/325], loss=8.0585
	step [239/325], loss=11.5587
	step [240/325], loss=7.9152
	step [241/325], loss=10.0556
	step [242/325], loss=9.4360
	step [243/325], loss=10.7093
	step [244/325], loss=9.7923
	step [245/325], loss=9.1592
	step [246/325], loss=9.4193
	step [247/325], loss=8.5728
	step [248/325], loss=9.8397
	step [249/325], loss=8.2950
	step [250/325], loss=9.5578
	step [251/325], loss=9.7102
	step [252/325], loss=9.9466
	step [253/325], loss=10.5493
	step [254/325], loss=8.8225
	step [255/325], loss=10.2694
	step [256/325], loss=10.6677
	step [257/325], loss=8.8975
	step [258/325], loss=8.2183
	step [259/325], loss=8.9656
	step [260/325], loss=9.1303
	step [261/325], loss=10.1349
	step [262/325], loss=8.0912
	step [263/325], loss=8.0572
	step [264/325], loss=8.4510
	step [265/325], loss=9.4855
	step [266/325], loss=8.9508
	step [267/325], loss=9.7980
	step [268/325], loss=9.9169
	step [269/325], loss=8.6639
	step [270/325], loss=8.7538
	step [271/325], loss=9.4654
	step [272/325], loss=10.0361
	step [273/325], loss=8.5521
	step [274/325], loss=9.0935
	step [275/325], loss=8.5788
	step [276/325], loss=9.9307
	step [277/325], loss=8.8926
	step [278/325], loss=8.7438
	step [279/325], loss=7.9697
	step [280/325], loss=10.2501
	step [281/325], loss=8.9471
	step [282/325], loss=8.4111
	step [283/325], loss=9.7944
	step [284/325], loss=9.0268
	step [285/325], loss=8.6307
	step [286/325], loss=9.2282
	step [287/325], loss=8.7073
	step [288/325], loss=8.8842
	step [289/325], loss=9.6223
	step [290/325], loss=8.0980
	step [291/325], loss=8.3123
	step [292/325], loss=7.6874
	step [293/325], loss=8.6278
	step [294/325], loss=9.2073
	step [295/325], loss=8.4638
	step [296/325], loss=11.8916
	step [297/325], loss=7.8555
	step [298/325], loss=8.5128
	step [299/325], loss=9.2599
	step [300/325], loss=10.2860
	step [301/325], loss=9.2541
	step [302/325], loss=8.8674
	step [303/325], loss=7.9608
	step [304/325], loss=13.9833
	step [305/325], loss=7.5877
	step [306/325], loss=10.5100
	step [307/325], loss=9.1549
	step [308/325], loss=8.4626
	step [309/325], loss=9.1685
	step [310/325], loss=8.5274
	step [311/325], loss=9.5729
	step [312/325], loss=9.4022
	step [313/325], loss=8.8573
	step [314/325], loss=9.1017
	step [315/325], loss=8.2666
	step [316/325], loss=10.0037
	step [317/325], loss=9.7368
	step [318/325], loss=9.0206
	step [319/325], loss=9.6345
	step [320/325], loss=8.8713
	step [321/325], loss=8.9900
	step [322/325], loss=10.0088
	step [323/325], loss=9.5415
	step [324/325], loss=9.8595
	step [325/325], loss=0.5614
	Evaluating
	loss=0.0369, precision=0.1765, recall=0.9978, f1=0.3000
saving model as: 2_saved_model.pth
Training epoch 9
	step [1/325], loss=8.3652
	step [2/325], loss=10.1385
	step [3/325], loss=8.9635
	step [4/325], loss=10.7100
	step [5/325], loss=8.6269
	step [6/325], loss=10.7822
	step [7/325], loss=8.9491
	step [8/325], loss=9.1661
	step [9/325], loss=8.4092
	step [10/325], loss=10.2898
	step [11/325], loss=8.3504
	step [12/325], loss=7.8356
	step [13/325], loss=9.9832
	step [14/325], loss=8.4709
	step [15/325], loss=7.4007
	step [16/325], loss=8.6031
	step [17/325], loss=8.0069
	step [18/325], loss=8.7005
	step [19/325], loss=8.1126
	step [20/325], loss=7.2665
	step [21/325], loss=7.6528
	step [22/325], loss=10.4808
	step [23/325], loss=8.1303
	step [24/325], loss=12.5602
	step [25/325], loss=8.9146
	step [26/325], loss=8.1502
	step [27/325], loss=8.5846
	step [28/325], loss=9.6640
	step [29/325], loss=7.0646
	step [30/325], loss=9.1820
	step [31/325], loss=9.7674
	step [32/325], loss=8.3710
	step [33/325], loss=7.1488
	step [34/325], loss=8.8321
	step [35/325], loss=9.7278
	step [36/325], loss=8.4028
	step [37/325], loss=8.4470
	step [38/325], loss=7.7663
	step [39/325], loss=8.1153
	step [40/325], loss=8.8957
	step [41/325], loss=7.5450
	step [42/325], loss=9.5631
	step [43/325], loss=8.4601
	step [44/325], loss=9.5455
	step [45/325], loss=8.9239
	step [46/325], loss=9.3991
	step [47/325], loss=8.8854
	step [48/325], loss=7.8530
	step [49/325], loss=8.8192
	step [50/325], loss=8.2433
	step [51/325], loss=7.6978
	step [52/325], loss=7.9565
	step [53/325], loss=8.3815
	step [54/325], loss=9.2963
	step [55/325], loss=8.2902
	step [56/325], loss=9.6396
	step [57/325], loss=9.6386
	step [58/325], loss=8.0767
	step [59/325], loss=8.7021
	step [60/325], loss=7.2887
	step [61/325], loss=7.7439
	step [62/325], loss=8.4535
	step [63/325], loss=8.0783
	step [64/325], loss=10.3464
	step [65/325], loss=9.0136
	step [66/325], loss=9.2222
	step [67/325], loss=7.9667
	step [68/325], loss=7.8422
	step [69/325], loss=7.2838
	step [70/325], loss=9.4142
	step [71/325], loss=9.3970
	step [72/325], loss=13.4969
	step [73/325], loss=8.8258
	step [74/325], loss=6.9122
	step [75/325], loss=7.5477
	step [76/325], loss=8.8082
	step [77/325], loss=8.1518
	step [78/325], loss=9.3405
	step [79/325], loss=8.7299
	step [80/325], loss=8.9599
	step [81/325], loss=7.5469
	step [82/325], loss=8.1434
	step [83/325], loss=8.0867
	step [84/325], loss=8.4774
	step [85/325], loss=9.9489
	step [86/325], loss=9.5627
	step [87/325], loss=9.0498
	step [88/325], loss=9.8627
	step [89/325], loss=8.1221
	step [90/325], loss=8.3705
	step [91/325], loss=9.7322
	step [92/325], loss=10.6884
	step [93/325], loss=9.5614
	step [94/325], loss=8.0455
	step [95/325], loss=8.8550
	step [96/325], loss=10.8641
	step [97/325], loss=7.9666
	step [98/325], loss=9.4877
	step [99/325], loss=8.4931
	step [100/325], loss=8.7865
	step [101/325], loss=8.1231
	step [102/325], loss=8.2350
	step [103/325], loss=10.3940
	step [104/325], loss=7.6404
	step [105/325], loss=8.6818
	step [106/325], loss=6.6736
	step [107/325], loss=9.5085
	step [108/325], loss=8.2817
	step [109/325], loss=9.1982
	step [110/325], loss=10.0534
	step [111/325], loss=7.1481
	step [112/325], loss=6.7564
	step [113/325], loss=7.7402
	step [114/325], loss=8.5242
	step [115/325], loss=7.7300
	step [116/325], loss=9.3674
	step [117/325], loss=8.0720
	step [118/325], loss=8.4847
	step [119/325], loss=7.8762
	step [120/325], loss=9.4996
	step [121/325], loss=7.9353
	step [122/325], loss=8.9269
	step [123/325], loss=8.5600
	step [124/325], loss=10.8612
	step [125/325], loss=7.6276
	step [126/325], loss=8.1736
	step [127/325], loss=7.5326
	step [128/325], loss=8.6749
	step [129/325], loss=7.6043
	step [130/325], loss=10.0146
	step [131/325], loss=7.8662
	step [132/325], loss=8.0408
	step [133/325], loss=7.9841
	step [134/325], loss=9.5618
	step [135/325], loss=9.0871
	step [136/325], loss=8.3856
	step [137/325], loss=7.2265
	step [138/325], loss=9.2986
	step [139/325], loss=6.9466
	step [140/325], loss=9.6735
	step [141/325], loss=9.0671
	step [142/325], loss=9.3444
	step [143/325], loss=8.1762
	step [144/325], loss=7.7690
	step [145/325], loss=7.9012
	step [146/325], loss=7.6038
	step [147/325], loss=8.2591
	step [148/325], loss=9.4378
	step [149/325], loss=8.0273
	step [150/325], loss=9.4830
	step [151/325], loss=8.8462
	step [152/325], loss=8.3492
	step [153/325], loss=8.2945
	step [154/325], loss=8.8350
	step [155/325], loss=8.6200
	step [156/325], loss=7.6205
	step [157/325], loss=9.4938
	step [158/325], loss=8.7959
	step [159/325], loss=8.9316
	step [160/325], loss=9.3761
	step [161/325], loss=9.8215
	step [162/325], loss=10.7619
	step [163/325], loss=9.2484
	step [164/325], loss=9.3399
	step [165/325], loss=13.9219
	step [166/325], loss=9.1897
	step [167/325], loss=10.2711
	step [168/325], loss=9.1061
	step [169/325], loss=7.8726
	step [170/325], loss=8.8283
	step [171/325], loss=8.1892
	step [172/325], loss=8.0113
	step [173/325], loss=11.6295
	step [174/325], loss=7.4902
	step [175/325], loss=9.9651
	step [176/325], loss=8.5561
	step [177/325], loss=7.1248
	step [178/325], loss=7.4468
	step [179/325], loss=11.5213
	step [180/325], loss=8.2028
	step [181/325], loss=7.2707
	step [182/325], loss=8.7960
	step [183/325], loss=8.9517
	step [184/325], loss=9.3937
	step [185/325], loss=8.3383
	step [186/325], loss=8.4008
	step [187/325], loss=8.6884
	step [188/325], loss=9.1397
	step [189/325], loss=10.7446
	step [190/325], loss=8.9152
	step [191/325], loss=10.0426
	step [192/325], loss=6.7354
	step [193/325], loss=8.1295
	step [194/325], loss=8.3427
	step [195/325], loss=8.1928
	step [196/325], loss=7.3051
	step [197/325], loss=8.3696
	step [198/325], loss=9.6345
	step [199/325], loss=10.0081
	step [200/325], loss=8.6975
	step [201/325], loss=6.7804
	step [202/325], loss=7.7179
	step [203/325], loss=7.0667
	step [204/325], loss=8.9167
	step [205/325], loss=7.7404
	step [206/325], loss=8.5825
	step [207/325], loss=8.0992
	step [208/325], loss=10.2824
	step [209/325], loss=8.5819
	step [210/325], loss=9.9958
	step [211/325], loss=9.4489
	step [212/325], loss=8.4637
	step [213/325], loss=10.0501
	step [214/325], loss=7.6590
	step [215/325], loss=8.6474
	step [216/325], loss=9.0367
	step [217/325], loss=8.6603
	step [218/325], loss=8.1777
	step [219/325], loss=8.4608
	step [220/325], loss=11.2620
	step [221/325], loss=9.3821
	step [222/325], loss=7.8513
	step [223/325], loss=8.7891
	step [224/325], loss=8.0514
	step [225/325], loss=8.7624
	step [226/325], loss=8.8571
	step [227/325], loss=9.3257
	step [228/325], loss=9.2709
	step [229/325], loss=9.5848
	step [230/325], loss=7.2357
	step [231/325], loss=10.3643
	step [232/325], loss=7.6726
	step [233/325], loss=8.4170
	step [234/325], loss=8.7754
	step [235/325], loss=7.4539
	step [236/325], loss=8.5705
	step [237/325], loss=6.9456
	step [238/325], loss=6.5680
	step [239/325], loss=9.7208
	step [240/325], loss=9.9974
	step [241/325], loss=9.0737
	step [242/325], loss=6.5961
	step [243/325], loss=9.4931
	step [244/325], loss=8.1427
	step [245/325], loss=8.8623
	step [246/325], loss=8.6098
	step [247/325], loss=7.0567
	step [248/325], loss=7.5959
	step [249/325], loss=7.5869
	step [250/325], loss=8.7788
	step [251/325], loss=7.8230
	step [252/325], loss=8.8245
	step [253/325], loss=7.8987
	step [254/325], loss=8.8465
	step [255/325], loss=7.8735
	step [256/325], loss=8.1734
	step [257/325], loss=9.9718
	step [258/325], loss=7.5687
	step [259/325], loss=7.5815
	step [260/325], loss=8.1256
	step [261/325], loss=11.2381
	step [262/325], loss=6.8083
	step [263/325], loss=6.7972
	step [264/325], loss=10.5998
	step [265/325], loss=8.7802
	step [266/325], loss=8.2403
	step [267/325], loss=8.9452
	step [268/325], loss=7.9272
	step [269/325], loss=7.7807
	step [270/325], loss=8.3155
	step [271/325], loss=7.8148
	step [272/325], loss=7.8793
	step [273/325], loss=9.1453
	step [274/325], loss=8.8792
	step [275/325], loss=10.0152
	step [276/325], loss=7.5588
	step [277/325], loss=7.8755
	step [278/325], loss=7.9985
	step [279/325], loss=7.5184
	step [280/325], loss=9.6895
	step [281/325], loss=7.2059
	step [282/325], loss=7.9157
	step [283/325], loss=8.0826
	step [284/325], loss=7.8497
	step [285/325], loss=8.2193
	step [286/325], loss=7.2486
	step [287/325], loss=6.6586
	step [288/325], loss=10.2839
	step [289/325], loss=8.8944
	step [290/325], loss=7.3357
	step [291/325], loss=8.5175
	step [292/325], loss=7.2692
	step [293/325], loss=8.4330
	step [294/325], loss=8.1191
	step [295/325], loss=8.4183
	step [296/325], loss=8.8743
	step [297/325], loss=7.4824
	step [298/325], loss=8.2601
	step [299/325], loss=9.2822
	step [300/325], loss=10.0189
	step [301/325], loss=8.0380
	step [302/325], loss=7.7630
	step [303/325], loss=8.3791
	step [304/325], loss=7.7006
	step [305/325], loss=6.8127
	step [306/325], loss=7.5512
	step [307/325], loss=7.0616
	step [308/325], loss=6.0183
	step [309/325], loss=9.2994
	step [310/325], loss=8.2792
	step [311/325], loss=8.4738
	step [312/325], loss=9.7243
	step [313/325], loss=9.0733
	step [314/325], loss=7.7140
	step [315/325], loss=9.2292
	step [316/325], loss=10.9823
	step [317/325], loss=10.1754
	step [318/325], loss=8.4139
	step [319/325], loss=8.4600
	step [320/325], loss=7.9656
	step [321/325], loss=8.3906
	step [322/325], loss=7.7562
	step [323/325], loss=8.4644
	step [324/325], loss=7.3482
	step [325/325], loss=0.6018
	Evaluating
	loss=0.0350, precision=0.1682, recall=0.9977, f1=0.2879
Training epoch 10
	step [1/325], loss=7.9464
	step [2/325], loss=8.2994
	step [3/325], loss=9.5706
	step [4/325], loss=8.2411
	step [5/325], loss=6.2656
	step [6/325], loss=7.8431
	step [7/325], loss=9.5888
	step [8/325], loss=8.8883
	step [9/325], loss=8.7192
	step [10/325], loss=9.4863
	step [11/325], loss=8.1250
	step [12/325], loss=8.2115
	step [13/325], loss=8.4378
	step [14/325], loss=7.6932
	step [15/325], loss=8.9486
	step [16/325], loss=8.0175
	step [17/325], loss=8.7002
	step [18/325], loss=8.2763
	step [19/325], loss=7.9332
	step [20/325], loss=7.3596
	step [21/325], loss=8.3227
	step [22/325], loss=8.1735
	step [23/325], loss=8.3117
	step [24/325], loss=7.9808
	step [25/325], loss=10.0017
	step [26/325], loss=6.7463
	step [27/325], loss=7.7420
	step [28/325], loss=10.3400
	step [29/325], loss=7.9210
	step [30/325], loss=8.7746
	step [31/325], loss=8.2307
	step [32/325], loss=7.6855
	step [33/325], loss=7.9972
	step [34/325], loss=7.5363
	step [35/325], loss=7.8223
	step [36/325], loss=8.9797
	step [37/325], loss=8.7667
	step [38/325], loss=6.5532
	step [39/325], loss=7.3657
	step [40/325], loss=7.1003
	step [41/325], loss=8.4929
	step [42/325], loss=8.7998
	step [43/325], loss=8.4680
	step [44/325], loss=8.2923
	step [45/325], loss=7.8551
	step [46/325], loss=7.0940
	step [47/325], loss=6.6611
	step [48/325], loss=8.7287
	step [49/325], loss=9.6783
	step [50/325], loss=6.9478
	step [51/325], loss=10.2694
	step [52/325], loss=7.2949
	step [53/325], loss=8.1762
	step [54/325], loss=8.5028
	step [55/325], loss=7.4972
	step [56/325], loss=7.4021
	step [57/325], loss=8.1508
	step [58/325], loss=6.0735
	step [59/325], loss=7.0407
	step [60/325], loss=7.3672
	step [61/325], loss=9.6795
	step [62/325], loss=7.7460
	step [63/325], loss=7.3185
	step [64/325], loss=9.6340
	step [65/325], loss=6.6821
	step [66/325], loss=7.1179
	step [67/325], loss=8.7497
	step [68/325], loss=7.0466
	step [69/325], loss=8.1000
	step [70/325], loss=8.1834
	step [71/325], loss=9.5451
	step [72/325], loss=9.1102
	step [73/325], loss=8.8012
	step [74/325], loss=7.2684
	step [75/325], loss=8.7979
	step [76/325], loss=6.6530
	step [77/325], loss=9.9398
	step [78/325], loss=8.0616
	step [79/325], loss=7.6809
	step [80/325], loss=8.8057
	step [81/325], loss=6.0872
	step [82/325], loss=8.8751
	step [83/325], loss=8.3790
	step [84/325], loss=8.2041
	step [85/325], loss=6.4289
	step [86/325], loss=8.8137
	step [87/325], loss=9.1738
	step [88/325], loss=7.5823
	step [89/325], loss=10.9314
	step [90/325], loss=8.4112
	step [91/325], loss=7.8395
	step [92/325], loss=7.9145
	step [93/325], loss=7.2405
	step [94/325], loss=8.0698
	step [95/325], loss=7.9682
	step [96/325], loss=7.7477
	step [97/325], loss=7.8072
	step [98/325], loss=6.8969
	step [99/325], loss=8.5523
	step [100/325], loss=9.4795
	step [101/325], loss=8.2069
	step [102/325], loss=6.3825
	step [103/325], loss=9.5098
	step [104/325], loss=7.9727
	step [105/325], loss=8.2588
	step [106/325], loss=8.8721
	step [107/325], loss=6.8034
	step [108/325], loss=6.5626
	step [109/325], loss=9.1590
	step [110/325], loss=8.2352
	step [111/325], loss=7.9969
	step [112/325], loss=8.3697
	step [113/325], loss=8.9422
	step [114/325], loss=7.2072
	step [115/325], loss=8.3049
	step [116/325], loss=10.5120
	step [117/325], loss=8.9295
	step [118/325], loss=7.7033
	step [119/325], loss=7.7507
	step [120/325], loss=6.9694
	step [121/325], loss=6.8335
	step [122/325], loss=7.9961
	step [123/325], loss=7.8116
	step [124/325], loss=8.6697
	step [125/325], loss=7.9952
	step [126/325], loss=7.4183
	step [127/325], loss=8.8697
	step [128/325], loss=8.4634
	step [129/325], loss=9.2185
	step [130/325], loss=9.8702
	step [131/325], loss=7.7046
	step [132/325], loss=8.1908
	step [133/325], loss=8.2125
	step [134/325], loss=6.9701
	step [135/325], loss=8.3504
	step [136/325], loss=7.2583
	step [137/325], loss=8.6456
	step [138/325], loss=8.5480
	step [139/325], loss=8.2790
	step [140/325], loss=6.4440
	step [141/325], loss=7.1007
	step [142/325], loss=8.2668
	step [143/325], loss=8.3807
	step [144/325], loss=7.9814
	step [145/325], loss=7.4681
	step [146/325], loss=7.9318
	step [147/325], loss=10.1880
	step [148/325], loss=8.1571
	step [149/325], loss=6.5246
	step [150/325], loss=7.0238
	step [151/325], loss=8.5979
	step [152/325], loss=7.3619
	step [153/325], loss=9.8254
	step [154/325], loss=7.7834
	step [155/325], loss=8.1985
	step [156/325], loss=8.8955
	step [157/325], loss=8.7169
	step [158/325], loss=7.1260
	step [159/325], loss=7.7638
	step [160/325], loss=7.9198
	step [161/325], loss=9.9522
	step [162/325], loss=9.4388
	step [163/325], loss=6.7864
	step [164/325], loss=8.0297
	step [165/325], loss=6.8352
	step [166/325], loss=7.3759
	step [167/325], loss=7.5506
	step [168/325], loss=7.5767
	step [169/325], loss=7.4227
	step [170/325], loss=7.0522
	step [171/325], loss=6.8699
	step [172/325], loss=8.1891
	step [173/325], loss=7.2856
	step [174/325], loss=6.6734
	step [175/325], loss=5.8889
	step [176/325], loss=6.9831
	step [177/325], loss=6.8668
	step [178/325], loss=6.6129
	step [179/325], loss=6.6487
	step [180/325], loss=7.6709
	step [181/325], loss=7.4156
	step [182/325], loss=8.7058
	step [183/325], loss=8.5873
	step [184/325], loss=7.5806
	step [185/325], loss=9.5408
	step [186/325], loss=8.2052
	step [187/325], loss=8.9558
	step [188/325], loss=5.9573
	step [189/325], loss=7.8132
	step [190/325], loss=6.8170
	step [191/325], loss=7.2379
	step [192/325], loss=6.0675
	step [193/325], loss=8.8212
	step [194/325], loss=8.0093
	step [195/325], loss=8.1852
	step [196/325], loss=8.5927
	step [197/325], loss=7.4524
	step [198/325], loss=8.0525
	step [199/325], loss=8.9833
	step [200/325], loss=6.8110
	step [201/325], loss=8.4856
	step [202/325], loss=7.2373
	step [203/325], loss=7.8972
	step [204/325], loss=8.6504
	step [205/325], loss=7.6314
	step [206/325], loss=7.2515
	step [207/325], loss=7.9799
	step [208/325], loss=7.8890
	step [209/325], loss=9.9043
	step [210/325], loss=7.0065
	step [211/325], loss=7.0162
	step [212/325], loss=6.6929
	step [213/325], loss=7.1033
	step [214/325], loss=6.6133
	step [215/325], loss=6.0074
	step [216/325], loss=8.6931
	step [217/325], loss=7.0440
	step [218/325], loss=7.3505
	step [219/325], loss=7.8826
	step [220/325], loss=8.0694
	step [221/325], loss=7.4806
	step [222/325], loss=7.4668
	step [223/325], loss=8.9470
	step [224/325], loss=6.7653
	step [225/325], loss=8.4884
	step [226/325], loss=7.7188
	step [227/325], loss=8.0659
	step [228/325], loss=6.9095
	step [229/325], loss=7.1697
	step [230/325], loss=9.0030
	step [231/325], loss=8.6479
	step [232/325], loss=7.8068
	step [233/325], loss=9.7146
	step [234/325], loss=7.1774
	step [235/325], loss=8.3704
	step [236/325], loss=6.8488
	step [237/325], loss=8.3802
	step [238/325], loss=6.5777
	step [239/325], loss=6.1036
	step [240/325], loss=9.3526
	step [241/325], loss=6.5783
	step [242/325], loss=10.2984
	step [243/325], loss=8.3878
	step [244/325], loss=7.4319
	step [245/325], loss=8.1586
	step [246/325], loss=9.3936
	step [247/325], loss=7.4528
	step [248/325], loss=5.6604
	step [249/325], loss=8.8271
	step [250/325], loss=7.6371
	step [251/325], loss=8.6496
	step [252/325], loss=7.1675
	step [253/325], loss=9.4692
	step [254/325], loss=9.7643
	step [255/325], loss=8.1407
	step [256/325], loss=8.2044
	step [257/325], loss=8.2656
	step [258/325], loss=9.9628
	step [259/325], loss=8.2828
	step [260/325], loss=9.9595
	step [261/325], loss=8.4845
	step [262/325], loss=8.4349
	step [263/325], loss=7.8650
	step [264/325], loss=8.6654
	step [265/325], loss=8.0485
	step [266/325], loss=7.3543
	step [267/325], loss=6.9412
	step [268/325], loss=7.8606
	step [269/325], loss=8.2186
	step [270/325], loss=8.9377
	step [271/325], loss=9.0776
	step [272/325], loss=6.5753
	step [273/325], loss=7.0149
	step [274/325], loss=5.9272
	step [275/325], loss=7.3497
	step [276/325], loss=7.1883
	step [277/325], loss=8.6366
	step [278/325], loss=7.0021
	step [279/325], loss=7.3279
	step [280/325], loss=7.0356
	step [281/325], loss=7.1873
	step [282/325], loss=7.4862
	step [283/325], loss=8.6905
	step [284/325], loss=10.2599
	step [285/325], loss=6.7817
	step [286/325], loss=6.7668
	step [287/325], loss=8.2354
	step [288/325], loss=7.1525
	step [289/325], loss=6.5458
	step [290/325], loss=7.5213
	step [291/325], loss=6.6290
	step [292/325], loss=7.5439
	step [293/325], loss=8.6614
	step [294/325], loss=7.3595
	step [295/325], loss=7.5599
	step [296/325], loss=6.8679
	step [297/325], loss=7.2965
	step [298/325], loss=7.4429
	step [299/325], loss=7.5205
	step [300/325], loss=7.9578
	step [301/325], loss=8.0544
	step [302/325], loss=6.2956
	step [303/325], loss=8.9856
	step [304/325], loss=7.4420
	step [305/325], loss=9.1255
	step [306/325], loss=7.4682
	step [307/325], loss=6.0041
	step [308/325], loss=7.1590
	step [309/325], loss=7.2755
	step [310/325], loss=6.7349
	step [311/325], loss=7.2494
	step [312/325], loss=7.6922
	step [313/325], loss=8.7748
	step [314/325], loss=6.6140
	step [315/325], loss=6.9949
	step [316/325], loss=7.5640
	step [317/325], loss=8.2983
	step [318/325], loss=8.7128
	step [319/325], loss=7.5590
	step [320/325], loss=8.3603
	step [321/325], loss=6.6028
	step [322/325], loss=8.3270
	step [323/325], loss=6.8111
	step [324/325], loss=9.2716
	step [325/325], loss=1.2854
	Evaluating
	loss=0.0326, precision=0.1533, recall=0.9982, f1=0.2657
Training epoch 11
	step [1/325], loss=7.7493
	step [2/325], loss=7.9119
	step [3/325], loss=7.0427
	step [4/325], loss=8.1981
	step [5/325], loss=6.4675
	step [6/325], loss=7.9403
	step [7/325], loss=6.1066
	step [8/325], loss=7.9920
	step [9/325], loss=8.3606
	step [10/325], loss=4.9033
	step [11/325], loss=6.4615
	step [12/325], loss=6.4435
	step [13/325], loss=8.4668
	step [14/325], loss=8.7767
	step [15/325], loss=8.2038
	step [16/325], loss=7.8491
	step [17/325], loss=8.9944
	step [18/325], loss=7.0267
	step [19/325], loss=6.5342
	step [20/325], loss=7.4334
	step [21/325], loss=7.0813
	step [22/325], loss=8.2313
	step [23/325], loss=8.8015
	step [24/325], loss=6.1057
	step [25/325], loss=6.1807
	step [26/325], loss=8.1819
	step [27/325], loss=10.2813
	step [28/325], loss=7.3962
	step [29/325], loss=7.6414
	step [30/325], loss=7.4968
	step [31/325], loss=7.4482
	step [32/325], loss=9.2955
	step [33/325], loss=7.5387
	step [34/325], loss=6.6104
	step [35/325], loss=8.4986
	step [36/325], loss=9.1225
	step [37/325], loss=7.4830
	step [38/325], loss=6.4944
	step [39/325], loss=7.9441
	step [40/325], loss=7.5961
	step [41/325], loss=7.5450
	step [42/325], loss=7.5820
	step [43/325], loss=6.9351
	step [44/325], loss=7.4259
	step [45/325], loss=7.4089
	step [46/325], loss=7.7110
	step [47/325], loss=8.1472
	step [48/325], loss=8.0125
	step [49/325], loss=7.4801
	step [50/325], loss=6.0473
	step [51/325], loss=8.6149
	step [52/325], loss=7.4748
	step [53/325], loss=7.9181
	step [54/325], loss=7.4301
	step [55/325], loss=9.1158
	step [56/325], loss=6.6943
	step [57/325], loss=6.2440
	step [58/325], loss=8.4398
	step [59/325], loss=7.3787
	step [60/325], loss=5.9727
	step [61/325], loss=7.0286
	step [62/325], loss=7.0021
	step [63/325], loss=8.1704
	step [64/325], loss=7.9726
	step [65/325], loss=8.7197
	step [66/325], loss=6.9352
	step [67/325], loss=6.1655
	step [68/325], loss=7.0929
	step [69/325], loss=8.0470
	step [70/325], loss=8.5871
	step [71/325], loss=6.2058
	step [72/325], loss=7.4271
	step [73/325], loss=6.8519
	step [74/325], loss=8.9384
	step [75/325], loss=7.0660
	step [76/325], loss=10.0035
	step [77/325], loss=6.0884
	step [78/325], loss=10.3232
	step [79/325], loss=7.3870
	step [80/325], loss=6.6479
	step [81/325], loss=7.1038
	step [82/325], loss=8.0944
	step [83/325], loss=6.3301
	step [84/325], loss=7.1278
	step [85/325], loss=6.7621
	step [86/325], loss=8.6936
	step [87/325], loss=6.8864
	step [88/325], loss=7.3338
	step [89/325], loss=7.4442
	step [90/325], loss=6.4723
	step [91/325], loss=8.0936
	step [92/325], loss=5.9745
	step [93/325], loss=7.1841
	step [94/325], loss=6.6226
	step [95/325], loss=7.4552
	step [96/325], loss=6.6058
	step [97/325], loss=5.8205
	step [98/325], loss=7.1481
	step [99/325], loss=7.7913
	step [100/325], loss=7.4798
	step [101/325], loss=7.5294
	step [102/325], loss=7.4802
	step [103/325], loss=6.1673
	step [104/325], loss=8.6065
	step [105/325], loss=7.9711
	step [106/325], loss=7.9432
	step [107/325], loss=7.6005
	step [108/325], loss=6.3291
	step [109/325], loss=8.1837
	step [110/325], loss=6.0861
	step [111/325], loss=6.9323
	step [112/325], loss=7.3915
	step [113/325], loss=8.5851
	step [114/325], loss=7.8323
	step [115/325], loss=7.5865
	step [116/325], loss=9.3925
	step [117/325], loss=7.3562
	step [118/325], loss=6.8337
	step [119/325], loss=7.4290
	step [120/325], loss=7.8765
	step [121/325], loss=6.2950
	step [122/325], loss=7.5353
	step [123/325], loss=7.8624
	step [124/325], loss=8.5792
	step [125/325], loss=7.3339
	step [126/325], loss=7.6334
	step [127/325], loss=7.7858
	step [128/325], loss=8.4774
	step [129/325], loss=8.3902
	step [130/325], loss=6.5644
	step [131/325], loss=6.5390
	step [132/325], loss=9.2144
	step [133/325], loss=7.3442
	step [134/325], loss=6.3288
	step [135/325], loss=7.3278
	step [136/325], loss=5.5008
	step [137/325], loss=7.6995
	step [138/325], loss=6.6284
	step [139/325], loss=8.8557
	step [140/325], loss=7.1102
	step [141/325], loss=6.1189
	step [142/325], loss=6.7594
	step [143/325], loss=8.7296
	step [144/325], loss=8.0734
	step [145/325], loss=7.2877
	step [146/325], loss=7.7567
	step [147/325], loss=7.7433
	step [148/325], loss=7.5026
	step [149/325], loss=7.2264
	step [150/325], loss=7.7038
	step [151/325], loss=9.0526
	step [152/325], loss=7.8414
	step [153/325], loss=8.4809
	step [154/325], loss=9.0347
	step [155/325], loss=9.5278
	step [156/325], loss=7.5590
	step [157/325], loss=6.1394
	step [158/325], loss=8.1313
	step [159/325], loss=7.8572
	step [160/325], loss=8.0795
	step [161/325], loss=7.2478
	step [162/325], loss=7.3478
	step [163/325], loss=6.5225
	step [164/325], loss=8.2855
	step [165/325], loss=7.5070
	step [166/325], loss=8.0256
	step [167/325], loss=7.8697
	step [168/325], loss=7.1276
	step [169/325], loss=6.6964
	step [170/325], loss=8.2091
	step [171/325], loss=7.2476
	step [172/325], loss=6.7894
	step [173/325], loss=8.9019
	step [174/325], loss=8.6279
	step [175/325], loss=6.6176
	step [176/325], loss=6.4565
	step [177/325], loss=6.9925
	step [178/325], loss=9.9167
	step [179/325], loss=6.5388
	step [180/325], loss=7.9374
	step [181/325], loss=6.8678
	step [182/325], loss=7.3644
	step [183/325], loss=8.1287
	step [184/325], loss=7.3760
	step [185/325], loss=7.3184
	step [186/325], loss=7.5769
	step [187/325], loss=6.6516
	step [188/325], loss=7.0389
	step [189/325], loss=8.2985
	step [190/325], loss=7.7265
	step [191/325], loss=8.3709
	step [192/325], loss=7.9784
	step [193/325], loss=7.5742
	step [194/325], loss=8.0991
	step [195/325], loss=9.0106
	step [196/325], loss=6.2070
	step [197/325], loss=6.5137
	step [198/325], loss=6.7180
	step [199/325], loss=7.7122
	step [200/325], loss=6.8061
	step [201/325], loss=6.6192
	step [202/325], loss=8.0423
	step [203/325], loss=7.2926
	step [204/325], loss=6.6339
	step [205/325], loss=6.7411
	step [206/325], loss=9.8631
	step [207/325], loss=6.4874
	step [208/325], loss=7.6061
	step [209/325], loss=6.8289
	step [210/325], loss=5.5064
	step [211/325], loss=6.8874
	step [212/325], loss=7.4577
	step [213/325], loss=8.1293
	step [214/325], loss=8.0609
	step [215/325], loss=6.3362
	step [216/325], loss=6.8831
	step [217/325], loss=10.0805
	step [218/325], loss=6.9501
	step [219/325], loss=6.6383
	step [220/325], loss=7.0367
	step [221/325], loss=7.6498
	step [222/325], loss=6.1976
	step [223/325], loss=8.0339
	step [224/325], loss=7.9875
	step [225/325], loss=9.1188
	step [226/325], loss=6.5311
	step [227/325], loss=6.6043
	step [228/325], loss=7.2363
	step [229/325], loss=6.5822
	step [230/325], loss=6.8821
	step [231/325], loss=6.4138
	step [232/325], loss=7.6941
	step [233/325], loss=5.7978
	step [234/325], loss=9.0785
	step [235/325], loss=8.3333
	step [236/325], loss=7.1511
	step [237/325], loss=7.1117
	step [238/325], loss=7.2151
	step [239/325], loss=6.7286
	step [240/325], loss=7.4742
	step [241/325], loss=8.1497
	step [242/325], loss=7.3453
	step [243/325], loss=8.4335
	step [244/325], loss=7.3716
	step [245/325], loss=7.5578
	step [246/325], loss=8.4607
	step [247/325], loss=7.1687
	step [248/325], loss=7.4706
	step [249/325], loss=6.8643
	step [250/325], loss=7.4308
	step [251/325], loss=5.6613
	step [252/325], loss=6.9843
	step [253/325], loss=9.5634
	step [254/325], loss=6.4732
	step [255/325], loss=6.8626
	step [256/325], loss=6.7224
	step [257/325], loss=8.4601
	step [258/325], loss=8.1303
	step [259/325], loss=7.0800
	step [260/325], loss=6.9444
	step [261/325], loss=7.7584
	step [262/325], loss=7.8452
	step [263/325], loss=6.5974
	step [264/325], loss=6.2141
	step [265/325], loss=8.1916
	step [266/325], loss=6.5215
	step [267/325], loss=7.7175
	step [268/325], loss=8.0635
	step [269/325], loss=7.0670
	step [270/325], loss=6.9505
	step [271/325], loss=6.9194
	step [272/325], loss=6.6470
	step [273/325], loss=6.0401
	step [274/325], loss=7.0423
	step [275/325], loss=8.5229
	step [276/325], loss=7.4376
	step [277/325], loss=7.3639
	step [278/325], loss=6.1501
	step [279/325], loss=7.1967
	step [280/325], loss=7.0500
	step [281/325], loss=9.4859
	step [282/325], loss=6.0809
	step [283/325], loss=7.2288
	step [284/325], loss=7.6130
	step [285/325], loss=6.3225
	step [286/325], loss=9.5032
	step [287/325], loss=10.0116
	step [288/325], loss=6.5796
	step [289/325], loss=7.0216
	step [290/325], loss=6.9885
	step [291/325], loss=7.0237
	step [292/325], loss=7.6032
	step [293/325], loss=7.3306
	step [294/325], loss=6.3693
	step [295/325], loss=8.0552
	step [296/325], loss=8.0273
	step [297/325], loss=6.7461
	step [298/325], loss=8.6715
	step [299/325], loss=6.4444
	step [300/325], loss=6.7165
	step [301/325], loss=6.7923
	step [302/325], loss=7.9957
	step [303/325], loss=7.0633
	step [304/325], loss=8.3639
	step [305/325], loss=7.1643
	step [306/325], loss=6.1018
	step [307/325], loss=6.0202
	step [308/325], loss=7.6814
	step [309/325], loss=8.5558
	step [310/325], loss=7.5074
	step [311/325], loss=8.0348
	step [312/325], loss=6.7887
	step [313/325], loss=6.2285
	step [314/325], loss=6.5527
	step [315/325], loss=5.8803
	step [316/325], loss=6.7301
	step [317/325], loss=8.3787
	step [318/325], loss=6.6666
	step [319/325], loss=8.9008
	step [320/325], loss=6.9196
	step [321/325], loss=5.5836
	step [322/325], loss=7.1245
	step [323/325], loss=7.3405
	step [324/325], loss=6.4082
	step [325/325], loss=0.2713
	Evaluating
	loss=0.0282, precision=0.1728, recall=0.9979, f1=0.2946
Training epoch 12
	step [1/325], loss=6.5688
	step [2/325], loss=7.9325
	step [3/325], loss=6.0702
	step [4/325], loss=7.4191
	step [5/325], loss=8.8847
	step [6/325], loss=8.2810
	step [7/325], loss=6.5999
	step [8/325], loss=7.1287
	step [9/325], loss=6.9213
	step [10/325], loss=5.9527
	step [11/325], loss=8.2360
	step [12/325], loss=5.3355
	step [13/325], loss=5.8796
	step [14/325], loss=6.8927
	step [15/325], loss=7.5525
	step [16/325], loss=7.3566
	step [17/325], loss=6.7427
	step [18/325], loss=5.9903
	step [19/325], loss=6.9476
	step [20/325], loss=7.0061
	step [21/325], loss=6.5990
	step [22/325], loss=7.6920
	step [23/325], loss=6.7227
	step [24/325], loss=6.2681
	step [25/325], loss=6.3020
	step [26/325], loss=7.1777
	step [27/325], loss=7.3277
	step [28/325], loss=7.0617
	step [29/325], loss=6.3835
	step [30/325], loss=7.0604
	step [31/325], loss=5.0039
	step [32/325], loss=6.5416
	step [33/325], loss=7.7516
	step [34/325], loss=6.9936
	step [35/325], loss=7.8743
	step [36/325], loss=6.2456
	step [37/325], loss=7.7495
	step [38/325], loss=7.3275
	step [39/325], loss=7.7032
	step [40/325], loss=6.3661
	step [41/325], loss=7.2469
	step [42/325], loss=9.3522
	step [43/325], loss=7.1695
	step [44/325], loss=6.2540
	step [45/325], loss=8.3612
	step [46/325], loss=7.7202
	step [47/325], loss=6.3249
	step [48/325], loss=8.1797
	step [49/325], loss=7.4715
	step [50/325], loss=6.6130
	step [51/325], loss=8.4166
	step [52/325], loss=7.4378
	step [53/325], loss=7.4646
	step [54/325], loss=7.8762
	step [55/325], loss=7.1329
	step [56/325], loss=5.2760
	step [57/325], loss=7.7143
	step [58/325], loss=6.1999
	step [59/325], loss=8.0587
	step [60/325], loss=6.5025
	step [61/325], loss=6.0834
	step [62/325], loss=6.2877
	step [63/325], loss=6.3212
	step [64/325], loss=6.3805
	step [65/325], loss=6.4358
	step [66/325], loss=6.8854
	step [67/325], loss=8.8162
	step [68/325], loss=5.6428
	step [69/325], loss=8.6577
	step [70/325], loss=7.2010
	step [71/325], loss=6.0805
	step [72/325], loss=8.6129
	step [73/325], loss=7.0808
	step [74/325], loss=6.5289
	step [75/325], loss=7.9394
	step [76/325], loss=8.0995
	step [77/325], loss=6.4440
	step [78/325], loss=8.0269
	step [79/325], loss=7.4055
	step [80/325], loss=7.1268
	step [81/325], loss=7.6645
	step [82/325], loss=9.0451
	step [83/325], loss=10.5323
	step [84/325], loss=7.3043
	step [85/325], loss=6.1957
	step [86/325], loss=9.0815
	step [87/325], loss=8.2573
	step [88/325], loss=8.7108
	step [89/325], loss=7.9317
	step [90/325], loss=7.1804
	step [91/325], loss=7.2576
	step [92/325], loss=5.5305
	step [93/325], loss=7.9149
	step [94/325], loss=5.3968
	step [95/325], loss=7.4808
	step [96/325], loss=6.8832
	step [97/325], loss=8.1369
	step [98/325], loss=8.9309
	step [99/325], loss=7.1137
	step [100/325], loss=7.1459
	step [101/325], loss=8.0370
	step [102/325], loss=7.1856
	step [103/325], loss=6.9025
	step [104/325], loss=6.6760
	step [105/325], loss=7.1303
	step [106/325], loss=6.2055
	step [107/325], loss=6.4660
	step [108/325], loss=6.9947
	step [109/325], loss=6.5473
	step [110/325], loss=8.1300
	step [111/325], loss=7.2310
	step [112/325], loss=6.5520
	step [113/325], loss=5.9325
	step [114/325], loss=7.7610
	step [115/325], loss=6.9338
	step [116/325], loss=6.6786
	step [117/325], loss=8.0497
	step [118/325], loss=6.2660
	step [119/325], loss=6.3707
	step [120/325], loss=5.8819
	step [121/325], loss=5.7036
	step [122/325], loss=6.6257
	step [123/325], loss=5.6969
	step [124/325], loss=6.8431
	step [125/325], loss=8.9975
	step [126/325], loss=7.0868
	step [127/325], loss=6.9108
	step [128/325], loss=6.0432
	step [129/325], loss=6.4657
	step [130/325], loss=6.6875
	step [131/325], loss=8.6267
	step [132/325], loss=6.3287
	step [133/325], loss=9.3784
	step [134/325], loss=7.5482
	step [135/325], loss=6.1073
	step [136/325], loss=5.9466
	step [137/325], loss=7.4554
	step [138/325], loss=7.6244
	step [139/325], loss=6.5445
	step [140/325], loss=6.6874
	step [141/325], loss=5.7564
	step [142/325], loss=9.0855
	step [143/325], loss=7.0712
	step [144/325], loss=5.7908
	step [145/325], loss=8.0261
	step [146/325], loss=6.1241
	step [147/325], loss=5.8180
	step [148/325], loss=7.1592
	step [149/325], loss=6.4291
	step [150/325], loss=7.1066
	step [151/325], loss=7.0954
	step [152/325], loss=6.3396
	step [153/325], loss=5.6081
	step [154/325], loss=7.0053
	step [155/325], loss=7.0022
	step [156/325], loss=6.3653
	step [157/325], loss=7.2173
	step [158/325], loss=5.3373
	step [159/325], loss=5.5665
	step [160/325], loss=8.3437
	step [161/325], loss=7.6923
	step [162/325], loss=6.8129
	step [163/325], loss=6.4420
	step [164/325], loss=5.6580
	step [165/325], loss=6.5984
	step [166/325], loss=6.2637
	step [167/325], loss=8.5605
	step [168/325], loss=7.8975
	step [169/325], loss=5.9515
	step [170/325], loss=6.8282
	step [171/325], loss=7.1785
	step [172/325], loss=6.3743
	step [173/325], loss=6.3610
	step [174/325], loss=7.1698
	step [175/325], loss=7.3307
	step [176/325], loss=5.9181
	step [177/325], loss=6.6010
	step [178/325], loss=6.1829
	step [179/325], loss=7.2633
	step [180/325], loss=7.5012
	step [181/325], loss=6.7428
	step [182/325], loss=7.1669
	step [183/325], loss=7.1004
	step [184/325], loss=7.0330
	step [185/325], loss=5.9363
	step [186/325], loss=5.5867
	step [187/325], loss=7.8573
	step [188/325], loss=6.6178
	step [189/325], loss=6.7981
	step [190/325], loss=6.2134
	step [191/325], loss=9.5745
	step [192/325], loss=7.3495
	step [193/325], loss=5.3265
	step [194/325], loss=6.7390
	step [195/325], loss=7.7654
	step [196/325], loss=5.3493
	step [197/325], loss=8.0897
	step [198/325], loss=6.7276
	step [199/325], loss=8.1252
	step [200/325], loss=7.2751
	step [201/325], loss=7.5057
	step [202/325], loss=7.2324
	step [203/325], loss=5.6794
	step [204/325], loss=6.3806
	step [205/325], loss=7.3381
	step [206/325], loss=6.4372
	step [207/325], loss=6.4401
	step [208/325], loss=5.5167
	step [209/325], loss=6.0388
	step [210/325], loss=5.9800
	step [211/325], loss=6.4437
	step [212/325], loss=6.0221
	step [213/325], loss=6.6791
	step [214/325], loss=6.3867
	step [215/325], loss=7.3753
	step [216/325], loss=6.3235
	step [217/325], loss=5.3448
	step [218/325], loss=6.6667
	step [219/325], loss=6.9665
	step [220/325], loss=7.1924
	step [221/325], loss=6.7407
	step [222/325], loss=6.0010
	step [223/325], loss=9.9354
	step [224/325], loss=6.2919
	step [225/325], loss=7.3781
	step [226/325], loss=5.4465
	step [227/325], loss=4.8417
	step [228/325], loss=7.3363
	step [229/325], loss=5.7943
	step [230/325], loss=6.8471
	step [231/325], loss=7.0342
	step [232/325], loss=7.1104
	step [233/325], loss=8.0756
	step [234/325], loss=6.5795
	step [235/325], loss=6.8817
	step [236/325], loss=8.1326
	step [237/325], loss=7.6763
	step [238/325], loss=6.6367
	step [239/325], loss=8.8393
	step [240/325], loss=6.0299
	step [241/325], loss=9.1545
	step [242/325], loss=6.0678
	step [243/325], loss=6.9102
	step [244/325], loss=7.3062
	step [245/325], loss=6.9515
	step [246/325], loss=6.7661
	step [247/325], loss=6.4021
	step [248/325], loss=6.8154
	step [249/325], loss=6.2688
	step [250/325], loss=7.0504
	step [251/325], loss=7.7333
	step [252/325], loss=10.3016
	step [253/325], loss=8.1565
	step [254/325], loss=6.5754
	step [255/325], loss=8.1651
	step [256/325], loss=6.0027
	step [257/325], loss=7.6767
	step [258/325], loss=7.4703
	step [259/325], loss=7.2688
	step [260/325], loss=5.4481
	step [261/325], loss=7.7989
	step [262/325], loss=6.0430
	step [263/325], loss=6.5583
	step [264/325], loss=6.2493
	step [265/325], loss=5.7281
	step [266/325], loss=6.1234
	step [267/325], loss=6.9040
	step [268/325], loss=7.3750
	step [269/325], loss=8.6873
	step [270/325], loss=7.5624
	step [271/325], loss=6.9217
	step [272/325], loss=7.2068
	step [273/325], loss=6.5575
	step [274/325], loss=7.2628
	step [275/325], loss=6.5953
	step [276/325], loss=7.1239
	step [277/325], loss=7.1486
	step [278/325], loss=6.8018
	step [279/325], loss=9.8426
	step [280/325], loss=7.8272
	step [281/325], loss=6.7072
	step [282/325], loss=6.2309
	step [283/325], loss=5.7444
	step [284/325], loss=6.2226
	step [285/325], loss=5.8832
	step [286/325], loss=5.4863
	step [287/325], loss=7.4064
	step [288/325], loss=10.3744
	step [289/325], loss=5.8992
	step [290/325], loss=7.2578
	step [291/325], loss=6.7381
	step [292/325], loss=7.4242
	step [293/325], loss=5.9431
	step [294/325], loss=7.9654
	step [295/325], loss=8.7682
	step [296/325], loss=8.2297
	step [297/325], loss=8.7113
	step [298/325], loss=7.7119
	step [299/325], loss=7.7538
	step [300/325], loss=5.7920
	step [301/325], loss=8.8908
	step [302/325], loss=8.2324
	step [303/325], loss=7.9558
	step [304/325], loss=8.9011
	step [305/325], loss=8.7136
	step [306/325], loss=6.5722
	step [307/325], loss=7.7797
	step [308/325], loss=8.8083
	step [309/325], loss=7.9152
	step [310/325], loss=7.1203
	step [311/325], loss=7.1933
	step [312/325], loss=7.3014
	step [313/325], loss=6.1135
	step [314/325], loss=6.9313
	step [315/325], loss=8.9688
	step [316/325], loss=6.4819
	step [317/325], loss=4.6637
	step [318/325], loss=5.5023
	step [319/325], loss=7.6494
	step [320/325], loss=9.2511
	step [321/325], loss=7.1267
	step [322/325], loss=5.5234
	step [323/325], loss=6.0390
	step [324/325], loss=6.1560
	step [325/325], loss=0.2097
	Evaluating
	loss=0.0228, precision=0.2163, recall=0.9963, f1=0.3554
saving model as: 2_saved_model.pth
Training epoch 13
	step [1/325], loss=7.6617
	step [2/325], loss=7.1657
	step [3/325], loss=7.7578
	step [4/325], loss=6.6488
	step [5/325], loss=7.3101
	step [6/325], loss=7.7903
	step [7/325], loss=7.2566
	step [8/325], loss=7.2594
	step [9/325], loss=8.0712
	step [10/325], loss=6.6197
	step [11/325], loss=5.5691
	step [12/325], loss=6.5496
	step [13/325], loss=6.0948
	step [14/325], loss=6.2929
	step [15/325], loss=7.5629
	step [16/325], loss=6.8962
	step [17/325], loss=6.0982
	step [18/325], loss=5.2659
	step [19/325], loss=7.3581
	step [20/325], loss=7.6346
	step [21/325], loss=7.7675
	step [22/325], loss=6.6684
	step [23/325], loss=6.3740
	step [24/325], loss=7.1237
	step [25/325], loss=7.0772
	step [26/325], loss=6.1714
	step [27/325], loss=4.9398
	step [28/325], loss=5.4601
	step [29/325], loss=7.2794
	step [30/325], loss=8.2898
	step [31/325], loss=7.3900
	step [32/325], loss=7.1163
	step [33/325], loss=6.2707
	step [34/325], loss=5.8309
	step [35/325], loss=5.9436
	step [36/325], loss=6.4179
	step [37/325], loss=6.3747
	step [38/325], loss=5.2211
	step [39/325], loss=7.3229
	step [40/325], loss=9.3391
	step [41/325], loss=5.6417
	step [42/325], loss=5.5267
	step [43/325], loss=7.4742
	step [44/325], loss=5.5911
	step [45/325], loss=6.3236
	step [46/325], loss=5.9749
	step [47/325], loss=6.8578
	step [48/325], loss=5.9140
	step [49/325], loss=9.5412
	step [50/325], loss=9.6179
	step [51/325], loss=7.3554
	step [52/325], loss=6.5357
	step [53/325], loss=5.7827
	step [54/325], loss=6.8305
	step [55/325], loss=7.3069
	step [56/325], loss=7.7013
	step [57/325], loss=6.8177
	step [58/325], loss=6.7759
	step [59/325], loss=8.0527
	step [60/325], loss=6.6345
	step [61/325], loss=6.0063
	step [62/325], loss=7.0959
	step [63/325], loss=6.1914
	step [64/325], loss=6.3525
	step [65/325], loss=5.5514
	step [66/325], loss=5.6440
	step [67/325], loss=5.5446
	step [68/325], loss=6.6151
	step [69/325], loss=8.4861
	step [70/325], loss=5.6397
	step [71/325], loss=7.5656
	step [72/325], loss=7.4079
	step [73/325], loss=6.9687
	step [74/325], loss=6.6682
	step [75/325], loss=6.1423
	step [76/325], loss=6.5496
	step [77/325], loss=6.1662
	step [78/325], loss=6.9929
	step [79/325], loss=6.4702
	step [80/325], loss=7.6571
	step [81/325], loss=7.6442
	step [82/325], loss=6.6026
	step [83/325], loss=5.5673
	step [84/325], loss=7.3416
	step [85/325], loss=5.5890
	step [86/325], loss=6.4656
	step [87/325], loss=7.2583
	step [88/325], loss=7.1805
	step [89/325], loss=6.6240
	step [90/325], loss=6.3473
	step [91/325], loss=5.3856
	step [92/325], loss=8.2107
	step [93/325], loss=6.6983
	step [94/325], loss=6.5312
	step [95/325], loss=8.8073
	step [96/325], loss=6.5344
	step [97/325], loss=5.9084
	step [98/325], loss=6.5613
	step [99/325], loss=6.4437
	step [100/325], loss=6.6505
	step [101/325], loss=8.8533
	step [102/325], loss=6.6154
	step [103/325], loss=7.5504
	step [104/325], loss=6.0252
	step [105/325], loss=5.8750
	step [106/325], loss=6.7303
	step [107/325], loss=6.7208
	step [108/325], loss=5.5081
	step [109/325], loss=5.5543
	step [110/325], loss=7.5542
	step [111/325], loss=6.8606
	step [112/325], loss=8.2130
	step [113/325], loss=6.2500
	step [114/325], loss=5.4661
	step [115/325], loss=5.8289
	step [116/325], loss=5.5577
	step [117/325], loss=5.5761
	step [118/325], loss=6.6456
	step [119/325], loss=8.0064
	step [120/325], loss=7.5361
	step [121/325], loss=6.2696
	step [122/325], loss=7.1919
	step [123/325], loss=5.7119
	step [124/325], loss=6.6128
	step [125/325], loss=6.5707
	step [126/325], loss=8.5147
	step [127/325], loss=5.0688
	step [128/325], loss=6.0903
	step [129/325], loss=6.7776
	step [130/325], loss=6.1958
	step [131/325], loss=6.2931
	step [132/325], loss=6.3351
	step [133/325], loss=7.3814
	step [134/325], loss=5.7691
	step [135/325], loss=5.4092
	step [136/325], loss=5.9197
	step [137/325], loss=7.1931
	step [138/325], loss=5.5997
	step [139/325], loss=5.6460
	step [140/325], loss=6.2026
	step [141/325], loss=6.9605
	step [142/325], loss=5.5634
	step [143/325], loss=5.6888
	step [144/325], loss=7.2557
	step [145/325], loss=5.6297
	step [146/325], loss=8.1944
	step [147/325], loss=6.5685
	step [148/325], loss=6.1757
	step [149/325], loss=5.6813
	step [150/325], loss=7.7923
	step [151/325], loss=6.0762
	step [152/325], loss=6.1620
	step [153/325], loss=5.6733
	step [154/325], loss=6.3050
	step [155/325], loss=5.9939
	step [156/325], loss=6.1006
	step [157/325], loss=7.6536
	step [158/325], loss=8.1256
	step [159/325], loss=7.6967
	step [160/325], loss=5.2556
	step [161/325], loss=5.2701
	step [162/325], loss=6.1749
	step [163/325], loss=5.6901
	step [164/325], loss=6.9427
	step [165/325], loss=5.1634
	step [166/325], loss=6.8605
	step [167/325], loss=6.4975
	step [168/325], loss=7.6783
	step [169/325], loss=8.0215
	step [170/325], loss=8.7786
	step [171/325], loss=7.3019
	step [172/325], loss=6.8555
	step [173/325], loss=7.1046
	step [174/325], loss=7.7179
	step [175/325], loss=7.0397
	step [176/325], loss=6.5642
	step [177/325], loss=5.8619
	step [178/325], loss=7.0797
	step [179/325], loss=5.2133
	step [180/325], loss=7.7648
	step [181/325], loss=6.4990
	step [182/325], loss=7.5950
	step [183/325], loss=5.8296
	step [184/325], loss=5.9754
	step [185/325], loss=7.1091
	step [186/325], loss=8.8190
	step [187/325], loss=7.9507
	step [188/325], loss=6.3201
	step [189/325], loss=8.2123
	step [190/325], loss=6.6697
	step [191/325], loss=5.8436
	step [192/325], loss=6.2720
	step [193/325], loss=7.2662
	step [194/325], loss=7.6077
	step [195/325], loss=6.3399
	step [196/325], loss=7.4279
	step [197/325], loss=8.5733
	step [198/325], loss=6.0294
	step [199/325], loss=7.4553
	step [200/325], loss=7.6527
	step [201/325], loss=7.0677
	step [202/325], loss=6.6636
	step [203/325], loss=6.2438
	step [204/325], loss=8.3031
	step [205/325], loss=6.9606
	step [206/325], loss=6.5069
	step [207/325], loss=5.3806
	step [208/325], loss=5.9215
	step [209/325], loss=7.5020
	step [210/325], loss=5.8183
	step [211/325], loss=8.5963
	step [212/325], loss=5.4843
	step [213/325], loss=5.8454
	step [214/325], loss=6.9425
	step [215/325], loss=6.3278
	step [216/325], loss=5.4394
	step [217/325], loss=8.4900
	step [218/325], loss=4.9656
	step [219/325], loss=7.6429
	step [220/325], loss=5.2247
	step [221/325], loss=5.5733
	step [222/325], loss=7.7728
	step [223/325], loss=7.2913
	step [224/325], loss=6.8101
	step [225/325], loss=7.5416
	step [226/325], loss=6.4029
	step [227/325], loss=7.6540
	step [228/325], loss=5.8285
	step [229/325], loss=7.4899
	step [230/325], loss=5.6262
	step [231/325], loss=6.3438
	step [232/325], loss=6.4009
	step [233/325], loss=6.1830
	step [234/325], loss=4.4549
	step [235/325], loss=6.1143
	step [236/325], loss=5.7162
	step [237/325], loss=6.8715
	step [238/325], loss=7.3563
	step [239/325], loss=6.8391
	step [240/325], loss=7.5665
	step [241/325], loss=4.2418
	step [242/325], loss=6.6051
	step [243/325], loss=6.0374
	step [244/325], loss=5.8058
	step [245/325], loss=6.6905
	step [246/325], loss=6.3220
	step [247/325], loss=6.8712
	step [248/325], loss=6.8904
	step [249/325], loss=6.5260
	step [250/325], loss=6.0950
	step [251/325], loss=6.8039
	step [252/325], loss=7.0052
	step [253/325], loss=6.7125
	step [254/325], loss=5.7369
	step [255/325], loss=4.9530
	step [256/325], loss=7.6294
	step [257/325], loss=7.6383
	step [258/325], loss=6.4698
	step [259/325], loss=5.3277
	step [260/325], loss=6.8275
	step [261/325], loss=7.0482
	step [262/325], loss=6.9098
	step [263/325], loss=6.8558
	step [264/325], loss=6.0970
	step [265/325], loss=7.9550
	step [266/325], loss=6.0568
	step [267/325], loss=5.5943
	step [268/325], loss=5.7760
	step [269/325], loss=6.9310
	step [270/325], loss=6.3082
	step [271/325], loss=7.2685
	step [272/325], loss=10.0178
	step [273/325], loss=10.0779
	step [274/325], loss=6.1719
	step [275/325], loss=5.6380
	step [276/325], loss=6.2319
	step [277/325], loss=5.9648
	step [278/325], loss=6.4216
	step [279/325], loss=6.4664
	step [280/325], loss=7.6098
	step [281/325], loss=8.3188
	step [282/325], loss=6.9325
	step [283/325], loss=7.2177
	step [284/325], loss=6.0145
	step [285/325], loss=5.8013
	step [286/325], loss=7.2199
	step [287/325], loss=6.3984
	step [288/325], loss=6.8231
	step [289/325], loss=5.9648
	step [290/325], loss=6.8512
	step [291/325], loss=6.9385
	step [292/325], loss=6.3715
	step [293/325], loss=4.6312
	step [294/325], loss=5.9353
	step [295/325], loss=5.5351
	step [296/325], loss=5.6104
	step [297/325], loss=8.0529
	step [298/325], loss=7.4986
	step [299/325], loss=7.7415
	step [300/325], loss=7.2430
	step [301/325], loss=5.3060
	step [302/325], loss=5.4358
	step [303/325], loss=6.4407
	step [304/325], loss=7.1366
	step [305/325], loss=7.7598
	step [306/325], loss=5.5442
	step [307/325], loss=8.5158
	step [308/325], loss=5.8335
	step [309/325], loss=5.8692
	step [310/325], loss=6.2948
	step [311/325], loss=5.4101
	step [312/325], loss=7.1019
	step [313/325], loss=6.7432
	step [314/325], loss=7.8599
	step [315/325], loss=7.6978
	step [316/325], loss=5.3542
	step [317/325], loss=6.0302
	step [318/325], loss=7.7187
	step [319/325], loss=6.5676
	step [320/325], loss=6.6685
	step [321/325], loss=5.4011
	step [322/325], loss=5.6824
	step [323/325], loss=4.6407
	step [324/325], loss=6.3356
	step [325/325], loss=0.7721
	Evaluating
	loss=0.0232, precision=0.2039, recall=0.9970, f1=0.3386
Training epoch 14
	step [1/325], loss=5.6811
	step [2/325], loss=7.1699
	step [3/325], loss=5.1189
	step [4/325], loss=5.7387
	step [5/325], loss=7.2263
	step [6/325], loss=6.7797
	step [7/325], loss=7.6818
	step [8/325], loss=5.3898
	step [9/325], loss=5.7912
	step [10/325], loss=9.5678
	step [11/325], loss=5.8266
	step [12/325], loss=5.8259
	step [13/325], loss=6.0117
	step [14/325], loss=5.1294
	step [15/325], loss=5.8428
	step [16/325], loss=5.3164
	step [17/325], loss=4.6995
	step [18/325], loss=7.7687
	step [19/325], loss=6.5218
	step [20/325], loss=6.1830
	step [21/325], loss=9.0414
	step [22/325], loss=5.6917
	step [23/325], loss=7.0976
	step [24/325], loss=5.6298
	step [25/325], loss=5.9525
	step [26/325], loss=7.1454
	step [27/325], loss=5.9635
	step [28/325], loss=7.2188
	step [29/325], loss=5.7977
	step [30/325], loss=5.2392
	step [31/325], loss=5.7606
	step [32/325], loss=5.1074
	step [33/325], loss=6.3307
	step [34/325], loss=6.6955
	step [35/325], loss=7.1271
	step [36/325], loss=6.9561
	step [37/325], loss=6.9485
	step [38/325], loss=6.2231
	step [39/325], loss=5.4766
	step [40/325], loss=6.2937
	step [41/325], loss=7.0489
	step [42/325], loss=6.8409
	step [43/325], loss=5.3630
	step [44/325], loss=5.9691
	step [45/325], loss=7.5011
	step [46/325], loss=4.5166
	step [47/325], loss=8.8114
	step [48/325], loss=5.5070
	step [49/325], loss=7.0257
	step [50/325], loss=5.6977
	step [51/325], loss=6.7848
	step [52/325], loss=7.5875
	step [53/325], loss=5.6697
	step [54/325], loss=6.2235
	step [55/325], loss=6.1869
	step [56/325], loss=5.2516
	step [57/325], loss=7.9783
	step [58/325], loss=6.0399
	step [59/325], loss=6.7194
	step [60/325], loss=8.0720
	step [61/325], loss=6.0948
	step [62/325], loss=5.9950
	step [63/325], loss=7.4534
	step [64/325], loss=6.2131
	step [65/325], loss=7.0557
	step [66/325], loss=6.8588
	step [67/325], loss=6.1564
	step [68/325], loss=6.5670
	step [69/325], loss=5.1310
	step [70/325], loss=5.7634
	step [71/325], loss=6.1254
	step [72/325], loss=5.9121
	step [73/325], loss=6.4293
	step [74/325], loss=5.4068
	step [75/325], loss=6.1704
	step [76/325], loss=7.9569
	step [77/325], loss=5.7203
	step [78/325], loss=5.6201
	step [79/325], loss=6.5669
	step [80/325], loss=7.4666
	step [81/325], loss=7.2841
	step [82/325], loss=6.5430
	step [83/325], loss=6.9466
	step [84/325], loss=6.3237
	step [85/325], loss=6.0456
	step [86/325], loss=6.1786
	step [87/325], loss=7.2891
	step [88/325], loss=5.9527
	step [89/325], loss=7.2199
	step [90/325], loss=5.5079
	step [91/325], loss=5.3977
	step [92/325], loss=6.5643
	step [93/325], loss=6.4956
	step [94/325], loss=7.0000
	step [95/325], loss=6.9685
	step [96/325], loss=6.7283
	step [97/325], loss=8.6751
	step [98/325], loss=6.0353
	step [99/325], loss=7.1760
	step [100/325], loss=6.5566
	step [101/325], loss=7.1875
	step [102/325], loss=5.1039
	step [103/325], loss=7.1474
	step [104/325], loss=7.1476
	step [105/325], loss=7.0625
	step [106/325], loss=7.2382
	step [107/325], loss=5.6872
	step [108/325], loss=6.0823
	step [109/325], loss=5.5460
	step [110/325], loss=5.9860
	step [111/325], loss=5.5447
	step [112/325], loss=7.1241
	step [113/325], loss=5.3773
	step [114/325], loss=6.2881
	step [115/325], loss=4.6661
	step [116/325], loss=5.5485
	step [117/325], loss=7.9974
	step [118/325], loss=7.9541
	step [119/325], loss=5.8454
	step [120/325], loss=5.1727
	step [121/325], loss=8.0661
	step [122/325], loss=6.6690
	step [123/325], loss=5.5754
	step [124/325], loss=6.3544
	step [125/325], loss=6.9773
	step [126/325], loss=7.2032
	step [127/325], loss=7.1401
	step [128/325], loss=6.9446
	step [129/325], loss=6.7198
	step [130/325], loss=6.7135
	step [131/325], loss=5.4196
	step [132/325], loss=6.0626
	step [133/325], loss=5.3978
	step [134/325], loss=5.5168
	step [135/325], loss=5.2431
	step [136/325], loss=5.4631
	step [137/325], loss=6.6802
	step [138/325], loss=4.8371
	step [139/325], loss=6.5819
	step [140/325], loss=7.2973
	step [141/325], loss=6.2489
	step [142/325], loss=5.8276
	step [143/325], loss=5.8865
	step [144/325], loss=5.7305
	step [145/325], loss=6.0542
	step [146/325], loss=6.8633
	step [147/325], loss=5.9758
	step [148/325], loss=5.3442
	step [149/325], loss=5.2367
	step [150/325], loss=5.9367
	step [151/325], loss=6.5158
	step [152/325], loss=9.9118
	step [153/325], loss=7.9746
	step [154/325], loss=5.7892
	step [155/325], loss=6.3365
	step [156/325], loss=6.2686
	step [157/325], loss=5.6577
	step [158/325], loss=5.7516
	step [159/325], loss=5.3821
	step [160/325], loss=5.5459
	step [161/325], loss=5.3739
	step [162/325], loss=5.4108
	step [163/325], loss=6.9656
	step [164/325], loss=5.1247
	step [165/325], loss=7.5252
	step [166/325], loss=7.4828
	step [167/325], loss=5.9833
	step [168/325], loss=8.3867
	step [169/325], loss=5.3518
	step [170/325], loss=4.9779
	step [171/325], loss=6.9276
	step [172/325], loss=6.3434
	step [173/325], loss=4.8602
	step [174/325], loss=6.6708
	step [175/325], loss=7.1915
	step [176/325], loss=6.1932
	step [177/325], loss=6.0001
	step [178/325], loss=6.2924
	step [179/325], loss=6.3158
	step [180/325], loss=5.2373
	step [181/325], loss=6.1170
	step [182/325], loss=7.6226
	step [183/325], loss=7.3068
	step [184/325], loss=5.1807
	step [185/325], loss=6.3342
	step [186/325], loss=6.7356
	step [187/325], loss=6.7264
	step [188/325], loss=7.4015
	step [189/325], loss=6.4038
	step [190/325], loss=5.1256
	step [191/325], loss=5.5810
	step [192/325], loss=6.7564
	step [193/325], loss=5.7594
	step [194/325], loss=6.4491
	step [195/325], loss=6.0770
	step [196/325], loss=4.8603
	step [197/325], loss=6.7475
	step [198/325], loss=6.9083
	step [199/325], loss=5.0562
	step [200/325], loss=5.2668
	step [201/325], loss=8.0454
	step [202/325], loss=7.1586
	step [203/325], loss=6.8160
	step [204/325], loss=7.3596
	step [205/325], loss=5.5324
	step [206/325], loss=4.9499
	step [207/325], loss=5.6165
	step [208/325], loss=5.7287
	step [209/325], loss=7.8842
	step [210/325], loss=6.0800
	step [211/325], loss=8.4121
	step [212/325], loss=6.4964
	step [213/325], loss=5.0903
	step [214/325], loss=8.2364
	step [215/325], loss=5.7134
	step [216/325], loss=6.4992
	step [217/325], loss=6.1420
	step [218/325], loss=6.4390
	step [219/325], loss=7.3280
	step [220/325], loss=6.3675
	step [221/325], loss=5.8551
	step [222/325], loss=7.3337
	step [223/325], loss=6.8044
	step [224/325], loss=7.6488
	step [225/325], loss=6.3699
	step [226/325], loss=7.2567
	step [227/325], loss=8.0222
	step [228/325], loss=6.7755
	step [229/325], loss=7.9427
	step [230/325], loss=6.0345
	step [231/325], loss=6.3516
	step [232/325], loss=8.1528
	step [233/325], loss=5.3995
	step [234/325], loss=6.1005
	step [235/325], loss=6.5170
	step [236/325], loss=5.9856
	step [237/325], loss=5.8447
	step [238/325], loss=5.9424
	step [239/325], loss=9.0271
	step [240/325], loss=7.3382
	step [241/325], loss=5.6795
	step [242/325], loss=5.9301
	step [243/325], loss=6.3952
	step [244/325], loss=5.9752
	step [245/325], loss=6.9164
	step [246/325], loss=5.4771
	step [247/325], loss=5.7460
	step [248/325], loss=6.2634
	step [249/325], loss=6.2536
	step [250/325], loss=6.2723
	step [251/325], loss=5.9815
	step [252/325], loss=6.8538
	step [253/325], loss=5.0738
	step [254/325], loss=5.2805
	step [255/325], loss=5.5894
	step [256/325], loss=6.5173
	step [257/325], loss=6.2327
	step [258/325], loss=5.4821
	step [259/325], loss=7.1531
	step [260/325], loss=5.8857
	step [261/325], loss=6.3373
	step [262/325], loss=5.1770
	step [263/325], loss=6.0136
	step [264/325], loss=6.6828
	step [265/325], loss=6.3206
	step [266/325], loss=6.1909
	step [267/325], loss=9.2598
	step [268/325], loss=6.2625
	step [269/325], loss=7.6755
	step [270/325], loss=4.9258
	step [271/325], loss=8.4070
	step [272/325], loss=6.4026
	step [273/325], loss=5.2045
	step [274/325], loss=5.2394
	step [275/325], loss=5.4281
	step [276/325], loss=5.8023
	step [277/325], loss=5.6476
	step [278/325], loss=5.9706
	step [279/325], loss=6.3414
	step [280/325], loss=6.2895
	step [281/325], loss=6.1234
	step [282/325], loss=7.3098
	step [283/325], loss=6.8487
	step [284/325], loss=6.2513
	step [285/325], loss=7.7846
	step [286/325], loss=10.0515
	step [287/325], loss=7.7811
	step [288/325], loss=4.9508
	step [289/325], loss=7.4574
	step [290/325], loss=5.9979
	step [291/325], loss=6.9646
	step [292/325], loss=5.2249
	step [293/325], loss=6.8568
	step [294/325], loss=5.2773
	step [295/325], loss=5.3052
	step [296/325], loss=6.2947
	step [297/325], loss=5.1683
	step [298/325], loss=8.0340
	step [299/325], loss=6.6757
	step [300/325], loss=5.5334
	step [301/325], loss=5.4775
	step [302/325], loss=6.5131
	step [303/325], loss=7.5761
	step [304/325], loss=6.3322
	step [305/325], loss=5.3029
	step [306/325], loss=6.7085
	step [307/325], loss=6.7846
	step [308/325], loss=6.8230
	step [309/325], loss=5.6514
	step [310/325], loss=6.6390
	step [311/325], loss=6.6717
	step [312/325], loss=5.6765
	step [313/325], loss=5.7911
	step [314/325], loss=5.8728
	step [315/325], loss=6.2481
	step [316/325], loss=6.4644
	step [317/325], loss=6.1971
	step [318/325], loss=6.3947
	step [319/325], loss=6.1504
	step [320/325], loss=5.6741
	step [321/325], loss=6.7438
	step [322/325], loss=7.3530
	step [323/325], loss=6.2827
	step [324/325], loss=7.5353
	step [325/325], loss=1.1706
	Evaluating
	loss=0.0261, precision=0.1695, recall=0.9975, f1=0.2898
Training epoch 15
	step [1/325], loss=6.3742
	step [2/325], loss=6.5063
	step [3/325], loss=5.2918
	step [4/325], loss=6.5296
	step [5/325], loss=7.0025
	step [6/325], loss=5.9290
	step [7/325], loss=6.8994
	step [8/325], loss=6.5806
	step [9/325], loss=4.7509
	step [10/325], loss=4.8904
	step [11/325], loss=7.5511
	step [12/325], loss=6.1831
	step [13/325], loss=5.8486
	step [14/325], loss=6.7074
	step [15/325], loss=5.9478
	step [16/325], loss=5.8905
	step [17/325], loss=7.7633
	step [18/325], loss=6.9301
	step [19/325], loss=6.3472
	step [20/325], loss=6.0377
	step [21/325], loss=5.4106
	step [22/325], loss=6.0699
	step [23/325], loss=5.4225
	step [24/325], loss=6.7426
	step [25/325], loss=4.4341
	step [26/325], loss=6.4118
	step [27/325], loss=6.4701
	step [28/325], loss=5.6860
	step [29/325], loss=4.0764
	step [30/325], loss=6.8703
	step [31/325], loss=5.0455
	step [32/325], loss=7.2967
	step [33/325], loss=5.3665
	step [34/325], loss=6.0708
	step [35/325], loss=6.5413
	step [36/325], loss=5.8753
	step [37/325], loss=5.3264
	step [38/325], loss=6.6364
	step [39/325], loss=5.7537
	step [40/325], loss=4.6149
	step [41/325], loss=7.5919
	step [42/325], loss=5.4697
	step [43/325], loss=5.6554
	step [44/325], loss=6.2952
	step [45/325], loss=5.8660
	step [46/325], loss=8.2888
	step [47/325], loss=5.3723
	step [48/325], loss=6.6433
	step [49/325], loss=4.5068
	step [50/325], loss=6.1743
	step [51/325], loss=6.1268
	step [52/325], loss=6.7545
	step [53/325], loss=6.7407
	step [54/325], loss=7.2240
	step [55/325], loss=5.8642
	step [56/325], loss=5.9666
	step [57/325], loss=5.6876
	step [58/325], loss=5.7086
	step [59/325], loss=6.4269
	step [60/325], loss=6.4175
	step [61/325], loss=5.8843
	step [62/325], loss=5.3281
	step [63/325], loss=7.2722
	step [64/325], loss=6.6750
	step [65/325], loss=6.0982
	step [66/325], loss=5.4373
	step [67/325], loss=6.5878
	step [68/325], loss=6.9676
	step [69/325], loss=7.0545
	step [70/325], loss=5.9151
	step [71/325], loss=7.2899
	step [72/325], loss=6.9200
	step [73/325], loss=5.8980
	step [74/325], loss=7.4066
	step [75/325], loss=6.5764
	step [76/325], loss=7.3754
	step [77/325], loss=6.6926
	step [78/325], loss=5.5980
	step [79/325], loss=6.2877
	step [80/325], loss=5.1015
	step [81/325], loss=6.4676
	step [82/325], loss=6.4715
	step [83/325], loss=5.4169
	step [84/325], loss=6.6348
	step [85/325], loss=5.7876
	step [86/325], loss=6.9428
	step [87/325], loss=7.0359
	step [88/325], loss=5.9989
	step [89/325], loss=6.1207
	step [90/325], loss=4.9211
	step [91/325], loss=5.5667
	step [92/325], loss=6.6062
	step [93/325], loss=5.7823
	step [94/325], loss=6.2967
	step [95/325], loss=4.7044
	step [96/325], loss=7.4254
	step [97/325], loss=5.8255
	step [98/325], loss=6.1840
	step [99/325], loss=5.9861
	step [100/325], loss=6.1406
	step [101/325], loss=6.1689
	step [102/325], loss=5.3659
	step [103/325], loss=4.7103
	step [104/325], loss=5.1932
	step [105/325], loss=6.1775
	step [106/325], loss=5.7079
	step [107/325], loss=7.6419
	step [108/325], loss=6.8105
	step [109/325], loss=5.2045
	step [110/325], loss=6.0233
	step [111/325], loss=6.7700
	step [112/325], loss=6.8058
	step [113/325], loss=6.3537
	step [114/325], loss=6.3420
	step [115/325], loss=7.1046
	step [116/325], loss=6.6913
	step [117/325], loss=7.9936
	step [118/325], loss=6.5777
	step [119/325], loss=7.0772
	step [120/325], loss=5.9728
	step [121/325], loss=4.8911
	step [122/325], loss=5.2873
	step [123/325], loss=6.3753
	step [124/325], loss=6.0355
	step [125/325], loss=5.4163
	step [126/325], loss=6.3389
	step [127/325], loss=6.6081
	step [128/325], loss=6.2157
	step [129/325], loss=5.4863
	step [130/325], loss=5.5140
	step [131/325], loss=4.6952
	step [132/325], loss=6.0713
	step [133/325], loss=8.3248
	step [134/325], loss=5.1233
	step [135/325], loss=5.5294
	step [136/325], loss=6.6825
	step [137/325], loss=5.3767
	step [138/325], loss=6.1101
	step [139/325], loss=5.3443
	step [140/325], loss=5.7873
	step [141/325], loss=6.2741
	step [142/325], loss=6.6050
	step [143/325], loss=6.3674
	step [144/325], loss=6.2304
	step [145/325], loss=6.3880
	step [146/325], loss=6.7981
	step [147/325], loss=5.4393
	step [148/325], loss=6.9093
	step [149/325], loss=6.7672
	step [150/325], loss=6.5793
	step [151/325], loss=5.2438
	step [152/325], loss=6.4158
	step [153/325], loss=6.5833
	step [154/325], loss=6.3101
	step [155/325], loss=6.1238
	step [156/325], loss=6.2711
	step [157/325], loss=6.9117
	step [158/325], loss=5.6610
	step [159/325], loss=6.2506
	step [160/325], loss=5.6326
	step [161/325], loss=5.5395
	step [162/325], loss=7.4105
	step [163/325], loss=6.1622
	step [164/325], loss=5.4030
	step [165/325], loss=6.2789
	step [166/325], loss=7.0586
	step [167/325], loss=6.1281
	step [168/325], loss=4.6473
	step [169/325], loss=4.5980
	step [170/325], loss=5.3051
	step [171/325], loss=5.5662
	step [172/325], loss=7.0885
	step [173/325], loss=8.4783
	step [174/325], loss=6.4561
	step [175/325], loss=5.8816
	step [176/325], loss=6.8283
	step [177/325], loss=7.3787
	step [178/325], loss=6.4723
	step [179/325], loss=6.7088
	step [180/325], loss=5.2092
	step [181/325], loss=5.2684
	step [182/325], loss=6.8791
	step [183/325], loss=5.8237
	step [184/325], loss=6.1973
	step [185/325], loss=5.7567
	step [186/325], loss=6.8202
	step [187/325], loss=5.6545
	step [188/325], loss=6.0108
	step [189/325], loss=4.1077
	step [190/325], loss=6.1633
	step [191/325], loss=6.0318
	step [192/325], loss=7.0098
	step [193/325], loss=5.7559
	step [194/325], loss=5.8159
	step [195/325], loss=5.2832
	step [196/325], loss=6.2349
	step [197/325], loss=7.4293
	step [198/325], loss=6.6269
	step [199/325], loss=5.2718
	step [200/325], loss=5.7203
	step [201/325], loss=5.7396
	step [202/325], loss=6.4533
	step [203/325], loss=5.2804
	step [204/325], loss=6.6593
	step [205/325], loss=5.1100
	step [206/325], loss=5.6995
	step [207/325], loss=6.0954
	step [208/325], loss=6.1354
	step [209/325], loss=6.4276
	step [210/325], loss=4.7582
	step [211/325], loss=5.7906
	step [212/325], loss=7.3089
	step [213/325], loss=6.2052
	step [214/325], loss=5.4936
	step [215/325], loss=5.5254
	step [216/325], loss=6.1766
	step [217/325], loss=6.7888
	step [218/325], loss=5.6891
	step [219/325], loss=5.4763
	step [220/325], loss=7.5742
	step [221/325], loss=6.0569
	step [222/325], loss=5.7029
	step [223/325], loss=6.7062
	step [224/325], loss=4.8926
	step [225/325], loss=5.1569
	step [226/325], loss=7.2661
	step [227/325], loss=5.8241
	step [228/325], loss=5.8550
	step [229/325], loss=6.1492
	step [230/325], loss=7.9462
	step [231/325], loss=6.3106
	step [232/325], loss=7.1367
	step [233/325], loss=6.4880
	step [234/325], loss=5.5025
	step [235/325], loss=5.1987
	step [236/325], loss=5.4426
	step [237/325], loss=7.0121
	step [238/325], loss=6.1750
	step [239/325], loss=4.9505
	step [240/325], loss=5.9270
	step [241/325], loss=7.0173
	step [242/325], loss=4.8099
	step [243/325], loss=7.1652
	step [244/325], loss=6.2571
	step [245/325], loss=5.5743
	step [246/325], loss=6.8044
	step [247/325], loss=5.5373
	step [248/325], loss=5.8901
	step [249/325], loss=5.3057
	step [250/325], loss=4.8183
	step [251/325], loss=6.2227
	step [252/325], loss=5.0001
	step [253/325], loss=7.3877
	step [254/325], loss=4.7986
	step [255/325], loss=7.3501
	step [256/325], loss=5.8250
	step [257/325], loss=7.0635
	step [258/325], loss=6.9316
	step [259/325], loss=5.7196
	step [260/325], loss=6.0476
	step [261/325], loss=6.2340
	step [262/325], loss=4.8820
	step [263/325], loss=7.2520
	step [264/325], loss=6.9178
	step [265/325], loss=7.5673
	step [266/325], loss=6.5512
	step [267/325], loss=6.0799
	step [268/325], loss=6.0447
	step [269/325], loss=7.2894
	step [270/325], loss=6.7037
	step [271/325], loss=6.5725
	step [272/325], loss=4.7463
	step [273/325], loss=5.3602
	step [274/325], loss=6.4329
	step [275/325], loss=6.5346
	step [276/325], loss=5.5415
	step [277/325], loss=7.2088
	step [278/325], loss=6.4575
	step [279/325], loss=5.1836
	step [280/325], loss=6.6935
	step [281/325], loss=5.4988
	step [282/325], loss=5.0351
	step [283/325], loss=6.4369
	step [284/325], loss=5.6268
	step [285/325], loss=6.2081
	step [286/325], loss=5.6224
	step [287/325], loss=5.2599
	step [288/325], loss=5.5481
	step [289/325], loss=5.1403
	step [290/325], loss=7.2300
	step [291/325], loss=5.1096
	step [292/325], loss=5.7647
	step [293/325], loss=5.4971
	step [294/325], loss=5.4207
	step [295/325], loss=5.7862
	step [296/325], loss=6.0087
	step [297/325], loss=6.7017
	step [298/325], loss=5.4204
	step [299/325], loss=8.1056
	step [300/325], loss=5.6841
	step [301/325], loss=5.6195
	step [302/325], loss=7.1584
	step [303/325], loss=5.5617
	step [304/325], loss=5.3333
	step [305/325], loss=6.6204
	step [306/325], loss=6.7101
	step [307/325], loss=5.1864
	step [308/325], loss=6.3741
	step [309/325], loss=6.1986
	step [310/325], loss=6.2984
	step [311/325], loss=8.1768
	step [312/325], loss=6.0994
	step [313/325], loss=6.3186
	step [314/325], loss=5.8324
	step [315/325], loss=4.7023
	step [316/325], loss=6.8427
	step [317/325], loss=5.2916
	step [318/325], loss=7.0891
	step [319/325], loss=5.0572
	step [320/325], loss=6.7436
	step [321/325], loss=3.9825
	step [322/325], loss=5.3534
	step [323/325], loss=6.5137
	step [324/325], loss=5.9572
	step [325/325], loss=0.3548
	Evaluating
	loss=0.0247, precision=0.1739, recall=0.9978, f1=0.2961
Training epoch 16
	step [1/325], loss=7.1490
	step [2/325], loss=5.9584
	step [3/325], loss=5.5046
	step [4/325], loss=6.3101
	step [5/325], loss=4.8359
	step [6/325], loss=6.5945
	step [7/325], loss=5.4024
	step [8/325], loss=5.5773
	step [9/325], loss=5.4359
	step [10/325], loss=6.9117
	step [11/325], loss=7.1008
	step [12/325], loss=6.1204
	step [13/325], loss=5.3338
	step [14/325], loss=6.7390
	step [15/325], loss=5.1999
	step [16/325], loss=6.6190
	step [17/325], loss=6.2314
	step [18/325], loss=5.6191
	step [19/325], loss=5.1484
	step [20/325], loss=4.8768
	step [21/325], loss=6.5362
	step [22/325], loss=7.2780
	step [23/325], loss=4.9126
	step [24/325], loss=6.6154
	step [25/325], loss=5.9463
	step [26/325], loss=6.6846
	step [27/325], loss=4.7688
	step [28/325], loss=6.6462
	step [29/325], loss=5.2446
	step [30/325], loss=5.7802
	step [31/325], loss=5.7195
	step [32/325], loss=5.2611
	step [33/325], loss=5.3992
	step [34/325], loss=5.3291
	step [35/325], loss=7.3279
	step [36/325], loss=6.2763
	step [37/325], loss=4.6891
	step [38/325], loss=6.4522
	step [39/325], loss=6.2958
	step [40/325], loss=5.9573
	step [41/325], loss=5.5891
	step [42/325], loss=5.1220
	step [43/325], loss=5.0962
	step [44/325], loss=5.2461
	step [45/325], loss=5.1998
	step [46/325], loss=5.5621
	step [47/325], loss=4.9644
	step [48/325], loss=5.4334
	step [49/325], loss=6.4905
	step [50/325], loss=5.8828
	step [51/325], loss=6.1973
	step [52/325], loss=5.3742
	step [53/325], loss=4.9961
	step [54/325], loss=5.1772
	step [55/325], loss=5.0388
	step [56/325], loss=6.8915
	step [57/325], loss=7.7866
	step [58/325], loss=6.8636
	step [59/325], loss=5.8847
	step [60/325], loss=6.9559
	step [61/325], loss=6.0713
	step [62/325], loss=5.2552
	step [63/325], loss=7.0817
	step [64/325], loss=6.3984
	step [65/325], loss=5.3962
	step [66/325], loss=9.6759
	step [67/325], loss=5.6155
	step [68/325], loss=6.4180
	step [69/325], loss=7.1975
	step [70/325], loss=4.6709
	step [71/325], loss=4.6854
	step [72/325], loss=4.6273
	step [73/325], loss=6.0744
	step [74/325], loss=4.7010
	step [75/325], loss=6.0195
	step [76/325], loss=5.4113
	step [77/325], loss=7.7382
	step [78/325], loss=6.1078
	step [79/325], loss=7.1079
	step [80/325], loss=3.9701
	step [81/325], loss=6.3449
	step [82/325], loss=5.2033
	step [83/325], loss=5.3672
	step [84/325], loss=5.1634
	step [85/325], loss=5.8855
	step [86/325], loss=6.9136
	step [87/325], loss=5.3412
	step [88/325], loss=6.7994
	step [89/325], loss=6.6280
	step [90/325], loss=4.7174
	step [91/325], loss=7.3515
	step [92/325], loss=5.3598
	step [93/325], loss=5.9050
	step [94/325], loss=4.9253
	step [95/325], loss=5.7757
	step [96/325], loss=6.7384
	step [97/325], loss=5.5525
	step [98/325], loss=5.1311
	step [99/325], loss=5.8121
	step [100/325], loss=5.8857
	step [101/325], loss=6.7500
	step [102/325], loss=6.0381
	step [103/325], loss=5.0170
	step [104/325], loss=5.6586
	step [105/325], loss=6.4561
	step [106/325], loss=6.2790
	step [107/325], loss=6.7620
	step [108/325], loss=6.3546
	step [109/325], loss=7.5052
	step [110/325], loss=8.5119
	step [111/325], loss=6.2248
	step [112/325], loss=5.0689
	step [113/325], loss=6.3497
	step [114/325], loss=5.8284
	step [115/325], loss=6.3050
	step [116/325], loss=6.0073
	step [117/325], loss=6.5524
	step [118/325], loss=6.7493
	step [119/325], loss=5.0318
	step [120/325], loss=6.4401
	step [121/325], loss=5.5409
	step [122/325], loss=5.3051
	step [123/325], loss=5.9881
	step [124/325], loss=5.4547
	step [125/325], loss=6.0368
	step [126/325], loss=5.0607
	step [127/325], loss=6.9461
	step [128/325], loss=5.3509
	step [129/325], loss=6.4563
	step [130/325], loss=4.9038
	step [131/325], loss=5.5136
	step [132/325], loss=5.3709
	step [133/325], loss=4.8982
	step [134/325], loss=4.7727
	step [135/325], loss=5.1034
	step [136/325], loss=5.4831
	step [137/325], loss=6.1633
	step [138/325], loss=5.4599
	step [139/325], loss=5.7583
	step [140/325], loss=5.3611
	step [141/325], loss=4.8798
	step [142/325], loss=7.3417
	step [143/325], loss=6.5049
	step [144/325], loss=5.1632
	step [145/325], loss=7.4978
	step [146/325], loss=5.2433
	step [147/325], loss=5.0642
	step [148/325], loss=6.6365
	step [149/325], loss=6.1230
	step [150/325], loss=5.5642
	step [151/325], loss=4.3313
	step [152/325], loss=6.0376
	step [153/325], loss=4.9766
	step [154/325], loss=6.3871
	step [155/325], loss=6.1239
	step [156/325], loss=5.1149
	step [157/325], loss=4.8979
	step [158/325], loss=6.0680
	step [159/325], loss=5.9011
	step [160/325], loss=9.0411
	step [161/325], loss=5.1382
	step [162/325], loss=4.5624
	step [163/325], loss=8.7417
	step [164/325], loss=5.6483
	step [165/325], loss=5.1227
	step [166/325], loss=6.1545
	step [167/325], loss=4.8309
	step [168/325], loss=6.5488
	step [169/325], loss=6.1412
	step [170/325], loss=7.0293
	step [171/325], loss=7.0398
	step [172/325], loss=6.6921
	step [173/325], loss=7.4097
	step [174/325], loss=7.1991
	step [175/325], loss=5.7657
	step [176/325], loss=6.0616
	step [177/325], loss=7.9944
	step [178/325], loss=5.4828
	step [179/325], loss=5.7659
	step [180/325], loss=5.4087
	step [181/325], loss=5.7731
	step [182/325], loss=5.0639
	step [183/325], loss=5.9957
	step [184/325], loss=6.4567
	step [185/325], loss=6.3513
	step [186/325], loss=6.3050
	step [187/325], loss=5.3967
	step [188/325], loss=5.3507
	step [189/325], loss=7.0825
	step [190/325], loss=6.8160
	step [191/325], loss=4.8007
	step [192/325], loss=5.6762
	step [193/325], loss=6.0996
	step [194/325], loss=5.8154
	step [195/325], loss=6.2907
	step [196/325], loss=5.0606
	step [197/325], loss=5.9139
	step [198/325], loss=6.7115
	step [199/325], loss=5.3579
	step [200/325], loss=7.7397
	step [201/325], loss=5.1218
	step [202/325], loss=5.0965
	step [203/325], loss=5.8357
	step [204/325], loss=5.4659
	step [205/325], loss=5.5693
	step [206/325], loss=6.2056
	step [207/325], loss=5.5662
	step [208/325], loss=5.0791
	step [209/325], loss=6.0232
	step [210/325], loss=6.4236
	step [211/325], loss=8.4670
	step [212/325], loss=4.8716
	step [213/325], loss=5.9394
	step [214/325], loss=5.3632
	step [215/325], loss=5.8911
	step [216/325], loss=5.0630
	step [217/325], loss=6.9473
	step [218/325], loss=5.7352
	step [219/325], loss=5.9359
	step [220/325], loss=6.4722
	step [221/325], loss=5.4251
	step [222/325], loss=4.9028
	step [223/325], loss=5.6551
	step [224/325], loss=6.0702
	step [225/325], loss=5.2403
	step [226/325], loss=6.4921
	step [227/325], loss=5.6543
	step [228/325], loss=4.9944
	step [229/325], loss=5.5678
	step [230/325], loss=3.8404
	step [231/325], loss=6.5613
	step [232/325], loss=6.8326
	step [233/325], loss=6.1049
	step [234/325], loss=6.4966
	step [235/325], loss=5.3924
	step [236/325], loss=5.7922
	step [237/325], loss=5.5284
	step [238/325], loss=5.7082
	step [239/325], loss=4.1128
	step [240/325], loss=6.0899
	step [241/325], loss=6.6762
	step [242/325], loss=4.6559
	step [243/325], loss=4.1550
	step [244/325], loss=7.2044
	step [245/325], loss=6.1560
	step [246/325], loss=5.3645
	step [247/325], loss=5.8166
	step [248/325], loss=5.8631
	step [249/325], loss=6.2183
	step [250/325], loss=5.1269
	step [251/325], loss=5.7177
	step [252/325], loss=5.6811
	step [253/325], loss=4.7851
	step [254/325], loss=4.0689
	step [255/325], loss=5.6413
	step [256/325], loss=7.5101
	step [257/325], loss=5.2331
	step [258/325], loss=4.5947
	step [259/325], loss=5.4648
	step [260/325], loss=5.5344
	step [261/325], loss=6.8296
	step [262/325], loss=6.7490
	step [263/325], loss=6.6668
	step [264/325], loss=7.9842
	step [265/325], loss=6.2573
	step [266/325], loss=6.5895
	step [267/325], loss=4.6869
	step [268/325], loss=5.1046
	step [269/325], loss=6.0943
	step [270/325], loss=5.2589
	step [271/325], loss=6.6948
	step [272/325], loss=7.1171
	step [273/325], loss=6.1039
	step [274/325], loss=7.9204
	step [275/325], loss=6.5281
	step [276/325], loss=4.7905
	step [277/325], loss=6.1609
	step [278/325], loss=6.7473
	step [279/325], loss=6.6184
	step [280/325], loss=6.0220
	step [281/325], loss=6.6204
	step [282/325], loss=5.7444
	step [283/325], loss=7.2368
	step [284/325], loss=5.5872
	step [285/325], loss=6.0139
	step [286/325], loss=6.1239
	step [287/325], loss=5.5238
	step [288/325], loss=4.8260
	step [289/325], loss=6.5922
	step [290/325], loss=7.4515
	step [291/325], loss=4.9993
	step [292/325], loss=5.2383
	step [293/325], loss=4.9024
	step [294/325], loss=6.1201
	step [295/325], loss=5.4209
	step [296/325], loss=6.9280
	step [297/325], loss=5.5273
	step [298/325], loss=5.8389
	step [299/325], loss=5.9664
	step [300/325], loss=5.3474
	step [301/325], loss=6.1601
	step [302/325], loss=4.7976
	step [303/325], loss=5.2485
	step [304/325], loss=5.2984
	step [305/325], loss=5.3174
	step [306/325], loss=4.8412
	step [307/325], loss=6.5789
	step [308/325], loss=5.7250
	step [309/325], loss=4.6666
	step [310/325], loss=6.6439
	step [311/325], loss=6.8415
	step [312/325], loss=5.4961
	step [313/325], loss=5.5722
	step [314/325], loss=6.5499
	step [315/325], loss=5.8474
	step [316/325], loss=7.2625
	step [317/325], loss=5.7372
	step [318/325], loss=5.0219
	step [319/325], loss=5.3268
	step [320/325], loss=7.5478
	step [321/325], loss=6.3263
	step [322/325], loss=4.7328
	step [323/325], loss=6.2344
	step [324/325], loss=5.0841
	step [325/325], loss=0.2275
	Evaluating
	loss=0.0209, precision=0.2170, recall=0.9969, f1=0.3564
saving model as: 2_saved_model.pth
Training epoch 17
	step [1/325], loss=4.7779
	step [2/325], loss=5.8205
	step [3/325], loss=7.6208
	step [4/325], loss=4.8308
	step [5/325], loss=5.5395
	step [6/325], loss=5.4570
	step [7/325], loss=5.1190
	step [8/325], loss=6.0125
	step [9/325], loss=5.8697
	step [10/325], loss=6.3372
	step [11/325], loss=5.0516
	step [12/325], loss=7.5843
	step [13/325], loss=5.7018
	step [14/325], loss=5.0145
	step [15/325], loss=6.2767
	step [16/325], loss=5.3403
	step [17/325], loss=7.3305
	step [18/325], loss=5.3826
	step [19/325], loss=4.7410
	step [20/325], loss=6.6485
	step [21/325], loss=5.8833
	step [22/325], loss=4.5363
	step [23/325], loss=5.4966
	step [24/325], loss=5.6484
	step [25/325], loss=5.2659
	step [26/325], loss=6.2502
	step [27/325], loss=5.3907
	step [28/325], loss=4.7081
	step [29/325], loss=4.8156
	step [30/325], loss=5.4688
	step [31/325], loss=6.1115
	step [32/325], loss=6.6050
	step [33/325], loss=5.5576
	step [34/325], loss=5.0145
	step [35/325], loss=4.5827
	step [36/325], loss=5.4515
	step [37/325], loss=5.2151
	step [38/325], loss=5.6880
	step [39/325], loss=5.3524
	step [40/325], loss=6.0071
	step [41/325], loss=5.2002
	step [42/325], loss=6.6000
	step [43/325], loss=5.4789
	step [44/325], loss=4.3892
	step [45/325], loss=7.1826
	step [46/325], loss=5.3936
	step [47/325], loss=6.3101
	step [48/325], loss=5.2521
	step [49/325], loss=6.1940
	step [50/325], loss=5.4288
	step [51/325], loss=4.6939
	step [52/325], loss=5.7263
	step [53/325], loss=5.9940
	step [54/325], loss=6.6395
	step [55/325], loss=5.2605
	step [56/325], loss=6.5828
	step [57/325], loss=4.9753
	step [58/325], loss=5.1919
	step [59/325], loss=6.2100
	step [60/325], loss=5.4759
	step [61/325], loss=4.8204
	step [62/325], loss=6.5090
	step [63/325], loss=5.5632
	step [64/325], loss=5.0182
	step [65/325], loss=6.0451
	step [66/325], loss=4.7729
	step [67/325], loss=4.8487
	step [68/325], loss=5.8138
	step [69/325], loss=4.9366
	step [70/325], loss=6.2918
	step [71/325], loss=6.5256
	step [72/325], loss=6.5271
	step [73/325], loss=6.9558
	step [74/325], loss=6.2443
	step [75/325], loss=6.1828
	step [76/325], loss=4.6322
	step [77/325], loss=6.3846
	step [78/325], loss=4.4484
	step [79/325], loss=4.4341
	step [80/325], loss=5.5536
	step [81/325], loss=4.5075
	step [82/325], loss=4.8297
	step [83/325], loss=4.8880
	step [84/325], loss=4.1627
	step [85/325], loss=5.5941
	step [86/325], loss=7.5521
	step [87/325], loss=5.3694
	step [88/325], loss=6.0142
	step [89/325], loss=5.0380
	step [90/325], loss=5.1218
	step [91/325], loss=5.1281
	step [92/325], loss=6.5009
	step [93/325], loss=4.3091
	step [94/325], loss=8.5406
	step [95/325], loss=5.5847
	step [96/325], loss=7.1855
	step [97/325], loss=6.2444
	step [98/325], loss=7.2320
	step [99/325], loss=6.4912
	step [100/325], loss=4.7208
	step [101/325], loss=6.3049
	step [102/325], loss=5.7829
	step [103/325], loss=5.9484
	step [104/325], loss=6.4751
	step [105/325], loss=5.8291
	step [106/325], loss=5.4860
	step [107/325], loss=5.7772
	step [108/325], loss=5.4255
	step [109/325], loss=6.6780
	step [110/325], loss=5.4555
	step [111/325], loss=5.6601
	step [112/325], loss=5.3221
	step [113/325], loss=5.1450
	step [114/325], loss=5.8228
	step [115/325], loss=5.0646
	step [116/325], loss=4.7924
	step [117/325], loss=6.3607
	step [118/325], loss=6.1432
	step [119/325], loss=6.6151
	step [120/325], loss=5.8119
	step [121/325], loss=6.2458
	step [122/325], loss=5.3582
	step [123/325], loss=5.3356
	step [124/325], loss=5.4430
	step [125/325], loss=8.6025
	step [126/325], loss=4.9771
	step [127/325], loss=4.7814
	step [128/325], loss=5.6179
	step [129/325], loss=6.5852
	step [130/325], loss=6.8418
	step [131/325], loss=5.3357
	step [132/325], loss=6.7413
	step [133/325], loss=6.2926
	step [134/325], loss=6.2318
	step [135/325], loss=7.0556
	step [136/325], loss=5.6170
	step [137/325], loss=5.8418
	step [138/325], loss=5.3778
	step [139/325], loss=5.7318
	step [140/325], loss=4.4274
	step [141/325], loss=5.1592
	step [142/325], loss=5.6568
	step [143/325], loss=5.4691
	step [144/325], loss=4.6842
	step [145/325], loss=4.7920
	step [146/325], loss=5.2665
	step [147/325], loss=4.6685
	step [148/325], loss=7.8071
	step [149/325], loss=5.8001
	step [150/325], loss=6.1589
	step [151/325], loss=4.8851
	step [152/325], loss=5.3283
	step [153/325], loss=6.1235
	step [154/325], loss=6.6708
	step [155/325], loss=6.2920
	step [156/325], loss=4.5596
	step [157/325], loss=5.1699
	step [158/325], loss=6.6389
	step [159/325], loss=5.4954
	step [160/325], loss=5.3369
	step [161/325], loss=5.7648
	step [162/325], loss=4.6620
	step [163/325], loss=6.2129
	step [164/325], loss=5.9056
	step [165/325], loss=6.4937
	step [166/325], loss=5.3571
	step [167/325], loss=5.9161
	step [168/325], loss=4.5260
	step [169/325], loss=5.5169
	step [170/325], loss=4.3689
	step [171/325], loss=6.5795
	step [172/325], loss=6.6824
	step [173/325], loss=6.9273
	step [174/325], loss=5.9209
	step [175/325], loss=6.1449
	step [176/325], loss=5.9516
	step [177/325], loss=6.2134
	step [178/325], loss=6.9077
	step [179/325], loss=5.0018
	step [180/325], loss=5.2847
	step [181/325], loss=8.0622
	step [182/325], loss=4.8459
	step [183/325], loss=5.8499
	step [184/325], loss=5.7599
	step [185/325], loss=6.2242
	step [186/325], loss=5.2833
	step [187/325], loss=5.2956
	step [188/325], loss=6.9052
	step [189/325], loss=5.3156
	step [190/325], loss=5.2052
	step [191/325], loss=5.1586
	step [192/325], loss=5.7477
	step [193/325], loss=5.5537
	step [194/325], loss=4.7643
	step [195/325], loss=6.0722
	step [196/325], loss=4.5695
	step [197/325], loss=5.5502
	step [198/325], loss=3.3288
	step [199/325], loss=5.2099
	step [200/325], loss=5.8747
	step [201/325], loss=6.7047
	step [202/325], loss=6.9912
	step [203/325], loss=5.8976
	step [204/325], loss=5.2476
	step [205/325], loss=4.3258
	step [206/325], loss=7.8774
	step [207/325], loss=4.7709
	step [208/325], loss=5.9117
	step [209/325], loss=6.8832
	step [210/325], loss=4.8384
	step [211/325], loss=6.9151
	step [212/325], loss=7.0518
	step [213/325], loss=5.0787
	step [214/325], loss=5.6406
	step [215/325], loss=5.3265
	step [216/325], loss=5.4992
	step [217/325], loss=6.4663
	step [218/325], loss=7.2343
	step [219/325], loss=5.4992
	step [220/325], loss=5.1816
	step [221/325], loss=7.4273
	step [222/325], loss=7.2280
	step [223/325], loss=5.6250
	step [224/325], loss=5.7512
	step [225/325], loss=5.8384
	step [226/325], loss=6.5637
	step [227/325], loss=5.3997
	step [228/325], loss=5.2976
	step [229/325], loss=5.4494
	step [230/325], loss=6.5386
	step [231/325], loss=5.5927
	step [232/325], loss=5.2631
	step [233/325], loss=5.3904
	step [234/325], loss=6.2103
	step [235/325], loss=5.8646
	step [236/325], loss=5.6184
	step [237/325], loss=4.9623
	step [238/325], loss=6.9401
	step [239/325], loss=5.8837
	step [240/325], loss=6.6318
	step [241/325], loss=5.5685
	step [242/325], loss=5.5203
	step [243/325], loss=5.1077
	step [244/325], loss=4.6136
	step [245/325], loss=6.1380
	step [246/325], loss=5.4051
	step [247/325], loss=5.2534
	step [248/325], loss=6.0173
	step [249/325], loss=6.3490
	step [250/325], loss=7.2855
	step [251/325], loss=5.3899
	step [252/325], loss=4.8493
	step [253/325], loss=5.3402
	step [254/325], loss=4.6429
	step [255/325], loss=5.2615
	step [256/325], loss=4.4923
	step [257/325], loss=5.1246
	step [258/325], loss=6.1958
	step [259/325], loss=4.3928
	step [260/325], loss=5.2894
	step [261/325], loss=5.3120
	step [262/325], loss=4.1966
	step [263/325], loss=6.5383
	step [264/325], loss=4.8881
	step [265/325], loss=6.2437
	step [266/325], loss=5.2039
	step [267/325], loss=4.8782
	step [268/325], loss=5.0796
	step [269/325], loss=5.4155
	step [270/325], loss=5.0778
	step [271/325], loss=5.2316
	step [272/325], loss=5.8398
	step [273/325], loss=5.0460
	step [274/325], loss=5.7835
	step [275/325], loss=7.2126
	step [276/325], loss=5.1893
	step [277/325], loss=6.0103
	step [278/325], loss=6.5892
	step [279/325], loss=5.5144
	step [280/325], loss=7.3517
	step [281/325], loss=5.3165
	step [282/325], loss=5.2069
	step [283/325], loss=5.2173
	step [284/325], loss=5.3817
	step [285/325], loss=5.0125
	step [286/325], loss=6.6244
	step [287/325], loss=6.7865
	step [288/325], loss=5.5997
	step [289/325], loss=5.0624
	step [290/325], loss=5.8519
	step [291/325], loss=5.3903
	step [292/325], loss=4.2386
	step [293/325], loss=6.2673
	step [294/325], loss=5.6458
	step [295/325], loss=5.9056
	step [296/325], loss=5.1541
	step [297/325], loss=5.4283
	step [298/325], loss=5.1401
	step [299/325], loss=5.2090
	step [300/325], loss=5.8529
	step [301/325], loss=6.3595
	step [302/325], loss=5.5881
	step [303/325], loss=5.0123
	step [304/325], loss=5.1968
	step [305/325], loss=4.9710
	step [306/325], loss=5.1466
	step [307/325], loss=5.9694
	step [308/325], loss=6.0327
	step [309/325], loss=5.4995
	step [310/325], loss=5.7620
	step [311/325], loss=5.5025
	step [312/325], loss=4.1981
	step [313/325], loss=7.0056
	step [314/325], loss=6.0516
	step [315/325], loss=6.9007
	step [316/325], loss=6.2476
	step [317/325], loss=6.1115
	step [318/325], loss=4.8212
	step [319/325], loss=4.2384
	step [320/325], loss=6.0901
	step [321/325], loss=6.3329
	step [322/325], loss=5.6114
	step [323/325], loss=7.3124
	step [324/325], loss=5.1007
	step [325/325], loss=0.0341
	Evaluating
	loss=0.0197, precision=0.2124, recall=0.9967, f1=0.3502
Training epoch 18
	step [1/325], loss=4.4563
	step [2/325], loss=5.4461
	step [3/325], loss=5.6596
	step [4/325], loss=4.9839
	step [5/325], loss=5.7788
	step [6/325], loss=4.4863
	step [7/325], loss=5.5470
	step [8/325], loss=4.7712
	step [9/325], loss=5.0756
	step [10/325], loss=6.0270
	step [11/325], loss=6.1123
	step [12/325], loss=5.7649
	step [13/325], loss=5.2298
	step [14/325], loss=5.3726
	step [15/325], loss=6.7035
	step [16/325], loss=6.7647
	step [17/325], loss=6.5862
	step [18/325], loss=6.1924
	step [19/325], loss=5.6720
	step [20/325], loss=7.1957
	step [21/325], loss=5.1843
	step [22/325], loss=5.1094
	step [23/325], loss=4.8423
	step [24/325], loss=5.5821
	step [25/325], loss=5.7695
	step [26/325], loss=4.8281
	step [27/325], loss=4.9352
	step [28/325], loss=4.9762
	step [29/325], loss=5.4150
	step [30/325], loss=6.7517
	step [31/325], loss=5.5783
	step [32/325], loss=5.2735
	step [33/325], loss=6.0946
	step [34/325], loss=4.2584
	step [35/325], loss=5.5116
	step [36/325], loss=6.0748
	step [37/325], loss=5.5518
	step [38/325], loss=6.0148
	step [39/325], loss=6.3917
	step [40/325], loss=4.4485
	step [41/325], loss=7.1790
	step [42/325], loss=4.7876
	step [43/325], loss=5.9454
	step [44/325], loss=5.9717
	step [45/325], loss=6.5345
	step [46/325], loss=5.8485
	step [47/325], loss=4.7133
	step [48/325], loss=5.1264
	step [49/325], loss=5.4643
	step [50/325], loss=5.4468
	step [51/325], loss=4.8306
	step [52/325], loss=5.4504
	step [53/325], loss=5.4317
	step [54/325], loss=4.0435
	step [55/325], loss=6.1662
	step [56/325], loss=6.1466
	step [57/325], loss=4.4309
	step [58/325], loss=5.6539
	step [59/325], loss=5.1889
	step [60/325], loss=5.8925
	step [61/325], loss=6.5548
	step [62/325], loss=6.8462
	step [63/325], loss=5.6363
	step [64/325], loss=5.4251
	step [65/325], loss=6.2984
	step [66/325], loss=5.3463
	step [67/325], loss=5.6842
	step [68/325], loss=6.2799
	step [69/325], loss=6.0995
	step [70/325], loss=5.0357
	step [71/325], loss=5.8601
	step [72/325], loss=4.2366
	step [73/325], loss=5.3499
	step [74/325], loss=5.0777
	step [75/325], loss=5.4508
	step [76/325], loss=5.6255
	step [77/325], loss=6.7014
	step [78/325], loss=5.6289
	step [79/325], loss=5.1777
	step [80/325], loss=5.9010
	step [81/325], loss=4.9720
	step [82/325], loss=5.8763
	step [83/325], loss=5.2165
	step [84/325], loss=5.9694
	step [85/325], loss=6.2784
	step [86/325], loss=6.2127
	step [87/325], loss=4.6580
	step [88/325], loss=6.1628
	step [89/325], loss=4.8609
	step [90/325], loss=5.0896
	step [91/325], loss=5.6210
	step [92/325], loss=5.1253
	step [93/325], loss=4.6975
	step [94/325], loss=6.1647
	step [95/325], loss=5.3922
	step [96/325], loss=4.0669
	step [97/325], loss=6.4427
	step [98/325], loss=5.4174
	step [99/325], loss=5.6893
	step [100/325], loss=6.2725
	step [101/325], loss=5.5382
	step [102/325], loss=6.0978
	step [103/325], loss=5.9282
	step [104/325], loss=5.6476
	step [105/325], loss=5.7173
	step [106/325], loss=5.2652
	step [107/325], loss=4.5890
	step [108/325], loss=3.8961
	step [109/325], loss=5.6350
	step [110/325], loss=5.7945
	step [111/325], loss=5.3211
	step [112/325], loss=4.7794
	step [113/325], loss=3.5587
	step [114/325], loss=5.5189
	step [115/325], loss=5.6450
	step [116/325], loss=5.8507
	step [117/325], loss=6.1658
	step [118/325], loss=5.6027
	step [119/325], loss=6.7747
	step [120/325], loss=5.5100
	step [121/325], loss=4.9136
	step [122/325], loss=5.5845
	step [123/325], loss=5.5443
	step [124/325], loss=5.2962
	step [125/325], loss=5.1487
	step [126/325], loss=6.1309
	step [127/325], loss=6.5067
	step [128/325], loss=4.3972
	step [129/325], loss=5.6218
	step [130/325], loss=5.0017
	step [131/325], loss=5.9133
	step [132/325], loss=4.2678
	step [133/325], loss=6.2691
	step [134/325], loss=5.4017
	step [135/325], loss=4.4109
	step [136/325], loss=5.6155
	step [137/325], loss=6.4248
	step [138/325], loss=3.6909
	step [139/325], loss=4.8815
	step [140/325], loss=4.4466
	step [141/325], loss=5.1566
	step [142/325], loss=7.3017
	step [143/325], loss=4.6500
	step [144/325], loss=5.0849
	step [145/325], loss=4.3786
	step [146/325], loss=4.5458
	step [147/325], loss=4.4599
	step [148/325], loss=5.8722
	step [149/325], loss=4.4398
	step [150/325], loss=4.4594
	step [151/325], loss=5.6933
	step [152/325], loss=5.4390
	step [153/325], loss=5.7925
	step [154/325], loss=5.4191
	step [155/325], loss=5.8396
	step [156/325], loss=4.8845
	step [157/325], loss=5.1657
	step [158/325], loss=4.4971
	step [159/325], loss=5.3114
	step [160/325], loss=5.9901
	step [161/325], loss=4.8966
	step [162/325], loss=4.9165
	step [163/325], loss=6.9791
	step [164/325], loss=6.2934
	step [165/325], loss=4.0736
	step [166/325], loss=5.8211
	step [167/325], loss=5.0316
	step [168/325], loss=4.9866
	step [169/325], loss=5.6243
	step [170/325], loss=5.7486
	step [171/325], loss=5.8421
	step [172/325], loss=6.2433
	step [173/325], loss=4.2930
	step [174/325], loss=3.5636
	step [175/325], loss=5.3752
	step [176/325], loss=5.6688
	step [177/325], loss=4.2250
	step [178/325], loss=6.7547
	step [179/325], loss=5.3182
	step [180/325], loss=6.6414
	step [181/325], loss=6.9076
	step [182/325], loss=6.1633
	step [183/325], loss=5.6887
	step [184/325], loss=6.1066
	step [185/325], loss=5.6777
	step [186/325], loss=4.5904
	step [187/325], loss=5.4498
	step [188/325], loss=6.3129
	step [189/325], loss=5.9538
	step [190/325], loss=6.1652
	step [191/325], loss=4.6586
	step [192/325], loss=6.5757
	step [193/325], loss=5.8246
	step [194/325], loss=6.3153
	step [195/325], loss=4.1246
	step [196/325], loss=7.3934
	step [197/325], loss=4.9679
	step [198/325], loss=5.4645
	step [199/325], loss=7.0428
	step [200/325], loss=6.8149
	step [201/325], loss=5.5545
	step [202/325], loss=4.2242
	step [203/325], loss=5.9792
	step [204/325], loss=4.3788
	step [205/325], loss=6.3533
	step [206/325], loss=7.3681
	step [207/325], loss=6.1252
	step [208/325], loss=5.1574
	step [209/325], loss=4.0038
	step [210/325], loss=5.9503
	step [211/325], loss=6.1590
	step [212/325], loss=4.1246
	step [213/325], loss=4.6374
	step [214/325], loss=5.1146
	step [215/325], loss=4.7999
	step [216/325], loss=5.7763
	step [217/325], loss=5.3199
	step [218/325], loss=5.0143
	step [219/325], loss=7.3740
	step [220/325], loss=6.4958
	step [221/325], loss=4.4729
	step [222/325], loss=6.3955
	step [223/325], loss=5.7769
	step [224/325], loss=5.5742
	step [225/325], loss=6.2636
	step [226/325], loss=5.9435
	step [227/325], loss=6.1847
	step [228/325], loss=5.6156
	step [229/325], loss=5.4270
	step [230/325], loss=6.8305
	step [231/325], loss=4.9241
	step [232/325], loss=5.0044
	step [233/325], loss=4.9916
	step [234/325], loss=6.4329
	step [235/325], loss=5.5554
	step [236/325], loss=4.6916
	step [237/325], loss=5.5512
	step [238/325], loss=5.6471
	step [239/325], loss=6.0790
	step [240/325], loss=5.2161
	step [241/325], loss=5.1177
	step [242/325], loss=5.8102
	step [243/325], loss=4.7220
	step [244/325], loss=4.3776
	step [245/325], loss=7.0227
	step [246/325], loss=4.7712
	step [247/325], loss=4.0687
	step [248/325], loss=4.6282
	step [249/325], loss=4.8080
	step [250/325], loss=4.9427
	step [251/325], loss=5.3352
	step [252/325], loss=4.6273
	step [253/325], loss=6.5893
	step [254/325], loss=5.0415
	step [255/325], loss=4.9012
	step [256/325], loss=5.1012
	step [257/325], loss=5.5601
	step [258/325], loss=5.9571
	step [259/325], loss=5.4549
	step [260/325], loss=5.5428
	step [261/325], loss=6.0501
	step [262/325], loss=5.1045
	step [263/325], loss=6.1941
	step [264/325], loss=7.1060
	step [265/325], loss=5.1867
	step [266/325], loss=4.6716
	step [267/325], loss=6.3785
	step [268/325], loss=4.7698
	step [269/325], loss=6.1618
	step [270/325], loss=5.7903
	step [271/325], loss=5.8387
	step [272/325], loss=4.5916
	step [273/325], loss=4.8659
	step [274/325], loss=4.8769
	step [275/325], loss=5.3195
	step [276/325], loss=4.6280
	step [277/325], loss=4.4895
	step [278/325], loss=4.7570
	step [279/325], loss=7.1281
	step [280/325], loss=4.7186
	step [281/325], loss=6.0106
	step [282/325], loss=5.4915
	step [283/325], loss=5.2992
	step [284/325], loss=5.6084
	step [285/325], loss=5.1125
	step [286/325], loss=6.0638
	step [287/325], loss=4.6064
	step [288/325], loss=5.2498
	step [289/325], loss=7.7958
	step [290/325], loss=4.7396
	step [291/325], loss=6.0406
	step [292/325], loss=4.3200
	step [293/325], loss=6.2423
	step [294/325], loss=7.6403
	step [295/325], loss=5.9057
	step [296/325], loss=5.8864
	step [297/325], loss=6.0547
	step [298/325], loss=5.0723
	step [299/325], loss=5.5898
	step [300/325], loss=5.3172
	step [301/325], loss=5.2092
	step [302/325], loss=5.3368
	step [303/325], loss=5.3148
	step [304/325], loss=5.2505
	step [305/325], loss=5.3920
	step [306/325], loss=5.7071
	step [307/325], loss=4.8042
	step [308/325], loss=3.5687
	step [309/325], loss=4.5874
	step [310/325], loss=5.8522
	step [311/325], loss=5.4622
	step [312/325], loss=5.7119
	step [313/325], loss=5.3773
	step [314/325], loss=6.2103
	step [315/325], loss=4.6427
	step [316/325], loss=6.5735
	step [317/325], loss=4.6682
	step [318/325], loss=5.0044
	step [319/325], loss=5.7729
	step [320/325], loss=4.0215
	step [321/325], loss=4.9130
	step [322/325], loss=5.9675
	step [323/325], loss=4.6349
	step [324/325], loss=6.5115
	step [325/325], loss=0.2578
	Evaluating
	loss=0.0200, precision=0.2126, recall=0.9970, f1=0.3505
Training epoch 19
	step [1/325], loss=6.1322
	step [2/325], loss=6.3091
	step [3/325], loss=5.5015
	step [4/325], loss=6.2030
	step [5/325], loss=5.6947
	step [6/325], loss=4.7379
	step [7/325], loss=4.2733
	step [8/325], loss=4.3204
	step [9/325], loss=6.1049
	step [10/325], loss=5.8544
	step [11/325], loss=5.6348
	step [12/325], loss=4.5179
	step [13/325], loss=5.8795
	step [14/325], loss=5.4309
	step [15/325], loss=5.2449
	step [16/325], loss=5.1219
	step [17/325], loss=5.5165
	step [18/325], loss=7.1743
	step [19/325], loss=4.8481
	step [20/325], loss=5.4595
	step [21/325], loss=5.9550
	step [22/325], loss=5.1330
	step [23/325], loss=4.4410
	step [24/325], loss=5.4216
	step [25/325], loss=5.3575
	step [26/325], loss=5.6735
	step [27/325], loss=5.7275
	step [28/325], loss=5.6437
	step [29/325], loss=3.9151
	step [30/325], loss=5.2272
	step [31/325], loss=6.1046
	step [32/325], loss=4.9676
	step [33/325], loss=5.8922
	step [34/325], loss=6.7815
	step [35/325], loss=6.0259
	step [36/325], loss=5.7720
	step [37/325], loss=5.4451
	step [38/325], loss=6.0096
	step [39/325], loss=5.8861
	step [40/325], loss=5.7396
	step [41/325], loss=5.6139
	step [42/325], loss=6.3065
	step [43/325], loss=6.0311
	step [44/325], loss=6.1264
	step [45/325], loss=4.4429
	step [46/325], loss=5.9901
	step [47/325], loss=5.0896
	step [48/325], loss=5.6725
	step [49/325], loss=5.3438
	step [50/325], loss=5.7818
	step [51/325], loss=5.3946
	step [52/325], loss=5.5858
	step [53/325], loss=3.9111
	step [54/325], loss=5.1646
	step [55/325], loss=5.4467
	step [56/325], loss=4.8194
	step [57/325], loss=4.4115
	step [58/325], loss=4.4613
	step [59/325], loss=6.1931
	step [60/325], loss=5.4280
	step [61/325], loss=6.3508
	step [62/325], loss=4.1340
	step [63/325], loss=5.6981
	step [64/325], loss=6.4970
	step [65/325], loss=4.6776
	step [66/325], loss=5.8745
	step [67/325], loss=7.0758
	step [68/325], loss=5.1893
	step [69/325], loss=4.9745
	step [70/325], loss=5.4588
	step [71/325], loss=6.4538
	step [72/325], loss=5.2487
	step [73/325], loss=5.8753
	step [74/325], loss=5.1955
	step [75/325], loss=4.1739
	step [76/325], loss=7.3894
	step [77/325], loss=4.3690
	step [78/325], loss=5.0408
	step [79/325], loss=4.5979
	step [80/325], loss=4.4506
	step [81/325], loss=5.3494
	step [82/325], loss=5.4375
	step [83/325], loss=5.4471
	step [84/325], loss=4.6894
	step [85/325], loss=5.1564
	step [86/325], loss=4.9723
	step [87/325], loss=6.1359
	step [88/325], loss=5.1272
	step [89/325], loss=5.0230
	step [90/325], loss=5.1461
	step [91/325], loss=5.4194
	step [92/325], loss=6.7597
	step [93/325], loss=5.3171
	step [94/325], loss=4.8950
	step [95/325], loss=5.2933
	step [96/325], loss=5.8407
	step [97/325], loss=6.2149
	step [98/325], loss=5.2685
	step [99/325], loss=4.5844
	step [100/325], loss=6.4664
	step [101/325], loss=4.0715
	step [102/325], loss=6.0354
	step [103/325], loss=5.4514
	step [104/325], loss=4.8638
	step [105/325], loss=5.1777
	step [106/325], loss=5.1842
	step [107/325], loss=5.9419
	step [108/325], loss=5.3951
	step [109/325], loss=5.3745
	step [110/325], loss=5.6985
	step [111/325], loss=5.2025
	step [112/325], loss=5.5815
	step [113/325], loss=4.1580
	step [114/325], loss=4.7123
	step [115/325], loss=5.3654
	step [116/325], loss=5.2750
	step [117/325], loss=4.9473
	step [118/325], loss=5.1796
	step [119/325], loss=6.7237
	step [120/325], loss=4.9059
	step [121/325], loss=5.4840
	step [122/325], loss=4.8200
	step [123/325], loss=4.9630
	step [124/325], loss=6.5338
	step [125/325], loss=4.9044
	step [126/325], loss=4.1617
	step [127/325], loss=6.1906
	step [128/325], loss=5.8092
	step [129/325], loss=6.2458
	step [130/325], loss=6.2163
	step [131/325], loss=6.0625
	step [132/325], loss=5.1145
	step [133/325], loss=5.2923
	step [134/325], loss=5.6359
	step [135/325], loss=6.1716
	step [136/325], loss=6.3351
	step [137/325], loss=4.7234
	step [138/325], loss=5.0362
	step [139/325], loss=6.7279
	step [140/325], loss=5.1784
	step [141/325], loss=5.8788
	step [142/325], loss=4.2977
	step [143/325], loss=5.9933
	step [144/325], loss=6.5170
	step [145/325], loss=5.5617
	step [146/325], loss=4.7817
	step [147/325], loss=6.3881
	step [148/325], loss=4.6285
	step [149/325], loss=5.5434
	step [150/325], loss=4.9838
	step [151/325], loss=5.4577
	step [152/325], loss=5.8254
	step [153/325], loss=4.5884
	step [154/325], loss=4.7697
	step [155/325], loss=5.0525
	step [156/325], loss=6.8384
	step [157/325], loss=5.5852
	step [158/325], loss=5.2468
	step [159/325], loss=5.0334
	step [160/325], loss=6.8134
	step [161/325], loss=5.4017
	step [162/325], loss=5.5612
	step [163/325], loss=5.6738
	step [164/325], loss=5.1645
	step [165/325], loss=5.8940
	step [166/325], loss=5.1228
	step [167/325], loss=4.9273
	step [168/325], loss=5.3428
	step [169/325], loss=5.4791
	step [170/325], loss=5.8710
	step [171/325], loss=4.4416
	step [172/325], loss=4.7931
	step [173/325], loss=5.1786
	step [174/325], loss=6.0859
	step [175/325], loss=5.9319
	step [176/325], loss=4.4899
	step [177/325], loss=5.2398
	step [178/325], loss=4.5078
	step [179/325], loss=4.4528
	step [180/325], loss=4.2501
	step [181/325], loss=4.7702
	step [182/325], loss=5.5630
	step [183/325], loss=3.8506
	step [184/325], loss=4.9328
	step [185/325], loss=4.4632
	step [186/325], loss=4.7228
	step [187/325], loss=4.6190
	step [188/325], loss=5.2843
	step [189/325], loss=4.5631
	step [190/325], loss=5.4979
	step [191/325], loss=5.4185
	step [192/325], loss=6.1737
	step [193/325], loss=4.8943
	step [194/325], loss=4.7790
	step [195/325], loss=4.9522
	step [196/325], loss=4.6666
	step [197/325], loss=4.9357
	step [198/325], loss=5.1319
	step [199/325], loss=5.0370
	step [200/325], loss=5.3388
	step [201/325], loss=4.9553
	step [202/325], loss=5.4442
	step [203/325], loss=5.0464
	step [204/325], loss=5.1430
	step [205/325], loss=5.2451
	step [206/325], loss=7.0579
	step [207/325], loss=3.8841
	step [208/325], loss=5.6509
	step [209/325], loss=6.0146
	step [210/325], loss=5.4184
	step [211/325], loss=5.8348
	step [212/325], loss=4.8991
	step [213/325], loss=5.1968
	step [214/325], loss=5.8419
	step [215/325], loss=4.7600
	step [216/325], loss=5.4515
	step [217/325], loss=5.0412
	step [218/325], loss=4.9451
	step [219/325], loss=3.8765
	step [220/325], loss=5.1103
	step [221/325], loss=4.5304
	step [222/325], loss=5.2624
	step [223/325], loss=5.6603
	step [224/325], loss=4.6617
	step [225/325], loss=3.5844
	step [226/325], loss=5.1361
	step [227/325], loss=4.8629
	step [228/325], loss=7.6563
	step [229/325], loss=4.7843
	step [230/325], loss=4.0007
	step [231/325], loss=6.2693
	step [232/325], loss=4.5196
	step [233/325], loss=4.7070
	step [234/325], loss=4.8087
	step [235/325], loss=5.5074
	step [236/325], loss=5.2516
	step [237/325], loss=5.2314
	step [238/325], loss=3.5261
	step [239/325], loss=4.6626
	step [240/325], loss=5.3687
	step [241/325], loss=5.5015
	step [242/325], loss=5.4260
	step [243/325], loss=5.4167
	step [244/325], loss=4.9625
	step [245/325], loss=5.5841
	step [246/325], loss=4.3861
	step [247/325], loss=4.7101
	step [248/325], loss=6.0033
	step [249/325], loss=5.6466
	step [250/325], loss=4.6157
	step [251/325], loss=5.7118
	step [252/325], loss=6.3206
	step [253/325], loss=4.6073
	step [254/325], loss=4.1647
	step [255/325], loss=4.9753
	step [256/325], loss=5.5582
	step [257/325], loss=5.0615
	step [258/325], loss=6.0644
	step [259/325], loss=4.4621
	step [260/325], loss=5.6028
	step [261/325], loss=6.2289
	step [262/325], loss=4.7629
	step [263/325], loss=4.3038
	step [264/325], loss=7.2438
	step [265/325], loss=5.6928
	step [266/325], loss=4.7607
	step [267/325], loss=4.0829
	step [268/325], loss=5.2442
	step [269/325], loss=5.8846
	step [270/325], loss=4.9143
	step [271/325], loss=5.5308
	step [272/325], loss=4.7056
	step [273/325], loss=5.2695
	step [274/325], loss=5.3114
	step [275/325], loss=4.8458
	step [276/325], loss=4.6524
	step [277/325], loss=4.7680
	step [278/325], loss=4.6860
	step [279/325], loss=5.5437
	step [280/325], loss=5.0460
	step [281/325], loss=5.7211
	step [282/325], loss=6.0133
	step [283/325], loss=5.5872
	step [284/325], loss=4.6597
	step [285/325], loss=5.2631
	step [286/325], loss=5.4387
	step [287/325], loss=6.0464
	step [288/325], loss=5.8830
	step [289/325], loss=5.5065
	step [290/325], loss=4.9877
	step [291/325], loss=4.5913
	step [292/325], loss=5.1181
	step [293/325], loss=5.4555
	step [294/325], loss=5.0510
	step [295/325], loss=4.9842
	step [296/325], loss=4.7291
	step [297/325], loss=5.8926
	step [298/325], loss=4.9153
	step [299/325], loss=5.3521
	step [300/325], loss=5.7043
	step [301/325], loss=5.1042
	step [302/325], loss=5.4483
	step [303/325], loss=5.1441
	step [304/325], loss=6.2216
	step [305/325], loss=4.2189
	step [306/325], loss=5.1421
	step [307/325], loss=4.8029
	step [308/325], loss=5.2145
	step [309/325], loss=5.8946
	step [310/325], loss=5.0287
	step [311/325], loss=5.3577
	step [312/325], loss=5.7289
	step [313/325], loss=5.7052
	step [314/325], loss=5.4682
	step [315/325], loss=4.6299
	step [316/325], loss=4.7466
	step [317/325], loss=5.4609
	step [318/325], loss=5.3219
	step [319/325], loss=4.4877
	step [320/325], loss=5.4168
	step [321/325], loss=6.0037
	step [322/325], loss=5.4337
	step [323/325], loss=6.0759
	step [324/325], loss=4.6499
	step [325/325], loss=0.1246
	Evaluating
	loss=0.0217, precision=0.1789, recall=0.9976, f1=0.3033
Training epoch 20
	step [1/325], loss=4.4253
	step [2/325], loss=5.1346
	step [3/325], loss=6.4532
	step [4/325], loss=5.3938
	step [5/325], loss=5.5121
	step [6/325], loss=3.6530
	step [7/325], loss=4.4858
	step [8/325], loss=5.3767
	step [9/325], loss=4.7404
	step [10/325], loss=6.2440
	step [11/325], loss=5.6418
	step [12/325], loss=3.9300
	step [13/325], loss=6.4303
	step [14/325], loss=5.2934
	step [15/325], loss=4.6841
	step [16/325], loss=5.2683
	step [17/325], loss=5.1402
	step [18/325], loss=4.6159
	step [19/325], loss=5.7120
	step [20/325], loss=4.7878
	step [21/325], loss=4.3820
	step [22/325], loss=5.2635
	step [23/325], loss=6.1498
	step [24/325], loss=6.4148
	step [25/325], loss=4.8955
	step [26/325], loss=4.1769
	step [27/325], loss=4.3638
	step [28/325], loss=5.6426
	step [29/325], loss=4.4423
	step [30/325], loss=4.7639
	step [31/325], loss=5.8037
	step [32/325], loss=5.5882
	step [33/325], loss=4.7899
	step [34/325], loss=5.8551
	step [35/325], loss=4.9234
	step [36/325], loss=4.6043
	step [37/325], loss=5.3551
	step [38/325], loss=4.3819
	step [39/325], loss=4.2699
	step [40/325], loss=4.8388
	step [41/325], loss=5.0164
	step [42/325], loss=5.0107
	step [43/325], loss=4.3367
	step [44/325], loss=4.2969
	step [45/325], loss=6.2199
	step [46/325], loss=5.2510
	step [47/325], loss=5.2074
	step [48/325], loss=4.3464
	step [49/325], loss=4.8948
	step [50/325], loss=5.6740
	step [51/325], loss=5.6679
	step [52/325], loss=5.0690
	step [53/325], loss=6.1587
	step [54/325], loss=4.8656
	step [55/325], loss=6.0591
	step [56/325], loss=5.0868
	step [57/325], loss=4.1362
	step [58/325], loss=5.7113
	step [59/325], loss=4.9295
	step [60/325], loss=5.3373
	step [61/325], loss=5.0295
	step [62/325], loss=4.6830
	step [63/325], loss=5.7228
	step [64/325], loss=5.4959
	step [65/325], loss=4.9799
	step [66/325], loss=5.2792
	step [67/325], loss=4.6280
	step [68/325], loss=3.8119
	step [69/325], loss=7.4867
	step [70/325], loss=6.4099
	step [71/325], loss=5.5299
	step [72/325], loss=5.5099
	step [73/325], loss=5.3565
	step [74/325], loss=4.3476
	step [75/325], loss=7.9044
	step [76/325], loss=5.5209
	step [77/325], loss=4.6549
	step [78/325], loss=6.1811
	step [79/325], loss=5.2093
	step [80/325], loss=4.9358
	step [81/325], loss=5.2169
	step [82/325], loss=4.6940
	step [83/325], loss=4.3001
	step [84/325], loss=5.1711
	step [85/325], loss=5.3909
	step [86/325], loss=6.8561
	step [87/325], loss=5.9888
	step [88/325], loss=6.7341
	step [89/325], loss=4.9241
	step [90/325], loss=4.4187
	step [91/325], loss=5.1825
	step [92/325], loss=5.2137
	step [93/325], loss=6.6595
	step [94/325], loss=6.0985
	step [95/325], loss=3.9707
	step [96/325], loss=6.1335
	step [97/325], loss=4.6072
	step [98/325], loss=4.6178
	step [99/325], loss=4.2365
	step [100/325], loss=5.8588
	step [101/325], loss=4.6039
	step [102/325], loss=3.7258
	step [103/325], loss=5.4345
	step [104/325], loss=5.9125
	step [105/325], loss=4.3556
	step [106/325], loss=5.0683
	step [107/325], loss=4.5357
	step [108/325], loss=4.5943
	step [109/325], loss=4.9017
	step [110/325], loss=5.5117
	step [111/325], loss=5.2010
	step [112/325], loss=4.9728
	step [113/325], loss=5.2279
	step [114/325], loss=4.6235
	step [115/325], loss=4.2123
	step [116/325], loss=4.0419
	step [117/325], loss=5.8758
	step [118/325], loss=5.9057
	step [119/325], loss=5.1206
	step [120/325], loss=5.8361
	step [121/325], loss=5.2340
	step [122/325], loss=4.9853
	step [123/325], loss=4.1519
	step [124/325], loss=5.9837
	step [125/325], loss=5.2826
	step [126/325], loss=5.0475
	step [127/325], loss=5.4280
	step [128/325], loss=4.8836
	step [129/325], loss=4.5925
	step [130/325], loss=5.6205
	step [131/325], loss=6.8385
	step [132/325], loss=5.6612
	step [133/325], loss=4.5944
	step [134/325], loss=5.0245
	step [135/325], loss=4.8919
	step [136/325], loss=6.8847
	step [137/325], loss=5.6853
	step [138/325], loss=4.4217
	step [139/325], loss=5.2449
	step [140/325], loss=5.9653
	step [141/325], loss=6.5251
	step [142/325], loss=5.4468
	step [143/325], loss=5.2813
	step [144/325], loss=5.1797
	step [145/325], loss=4.8774
	step [146/325], loss=5.0691
	step [147/325], loss=5.3593
	step [148/325], loss=5.7120
	step [149/325], loss=5.8125
	step [150/325], loss=5.8434
	step [151/325], loss=5.9719
	step [152/325], loss=6.1942
	step [153/325], loss=4.8098
	step [154/325], loss=5.0656
	step [155/325], loss=4.2160
	step [156/325], loss=5.4723
	step [157/325], loss=5.2078
	step [158/325], loss=7.0729
	step [159/325], loss=5.1603
	step [160/325], loss=5.3262
	step [161/325], loss=4.3502
	step [162/325], loss=5.5592
	step [163/325], loss=6.0845
	step [164/325], loss=5.6839
	step [165/325], loss=5.0355
	step [166/325], loss=5.0366
	step [167/325], loss=6.4237
	step [168/325], loss=5.2508
	step [169/325], loss=5.1838
	step [170/325], loss=5.0612
	step [171/325], loss=6.0374
	step [172/325], loss=5.3049
	step [173/325], loss=4.0355
	step [174/325], loss=4.8686
	step [175/325], loss=6.9165
	step [176/325], loss=4.3425
	step [177/325], loss=4.8072
	step [178/325], loss=3.9467
	step [179/325], loss=4.7249
	step [180/325], loss=4.5408
	step [181/325], loss=5.3380
	step [182/325], loss=4.5682
	step [183/325], loss=5.5077
	step [184/325], loss=4.6130
	step [185/325], loss=6.0438
	step [186/325], loss=5.0640
	step [187/325], loss=4.2575
	step [188/325], loss=4.0861
	step [189/325], loss=5.5758
	step [190/325], loss=5.1190
	step [191/325], loss=5.4767
	step [192/325], loss=4.0599
	step [193/325], loss=5.5257
	step [194/325], loss=6.2771
	step [195/325], loss=5.7395
	step [196/325], loss=5.0156
	step [197/325], loss=4.3075
	step [198/325], loss=4.9047
	step [199/325], loss=6.1848
	step [200/325], loss=5.6994
	step [201/325], loss=4.4659
	step [202/325], loss=5.4496
	step [203/325], loss=5.0073
	step [204/325], loss=5.0258
	step [205/325], loss=5.3786
	step [206/325], loss=4.2263
	step [207/325], loss=4.3777
	step [208/325], loss=4.7996
	step [209/325], loss=4.4497
	step [210/325], loss=4.9132
	step [211/325], loss=7.3597
	step [212/325], loss=5.4923
	step [213/325], loss=6.1217
	step [214/325], loss=4.7505
	step [215/325], loss=5.3914
	step [216/325], loss=5.9002
	step [217/325], loss=4.7025
	step [218/325], loss=4.5364
	step [219/325], loss=6.0956
	step [220/325], loss=4.5945
	step [221/325], loss=5.1279
	step [222/325], loss=4.6198
	step [223/325], loss=4.4092
	step [224/325], loss=4.7074
	step [225/325], loss=3.8645
	step [226/325], loss=5.9385
	step [227/325], loss=4.5379
	step [228/325], loss=4.6549
	step [229/325], loss=4.8428
	step [230/325], loss=4.5215
	step [231/325], loss=4.0370
	step [232/325], loss=4.3106
	step [233/325], loss=5.0545
	step [234/325], loss=5.4737
	step [235/325], loss=5.3147
	step [236/325], loss=6.8430
	step [237/325], loss=4.6992
	step [238/325], loss=5.4611
	step [239/325], loss=5.4041
	step [240/325], loss=6.6039
	step [241/325], loss=4.4702
	step [242/325], loss=4.2463
	step [243/325], loss=6.7815
	step [244/325], loss=5.1788
	step [245/325], loss=5.0074
	step [246/325], loss=5.9715
	step [247/325], loss=5.8365
	step [248/325], loss=4.7112
	step [249/325], loss=6.5209
	step [250/325], loss=5.5257
	step [251/325], loss=5.2836
	step [252/325], loss=4.2956
	step [253/325], loss=4.4866
	step [254/325], loss=5.5158
	step [255/325], loss=3.9856
	step [256/325], loss=5.9624
	step [257/325], loss=4.3745
	step [258/325], loss=4.7371
	step [259/325], loss=6.3980
	step [260/325], loss=5.8606
	step [261/325], loss=5.1049
	step [262/325], loss=4.4081
	step [263/325], loss=4.9733
	step [264/325], loss=4.7776
	step [265/325], loss=6.2321
	step [266/325], loss=5.0047
	step [267/325], loss=5.1757
	step [268/325], loss=4.8797
	step [269/325], loss=5.1166
	step [270/325], loss=5.4204
	step [271/325], loss=6.5588
	step [272/325], loss=6.0721
	step [273/325], loss=4.2364
	step [274/325], loss=5.4461
	step [275/325], loss=4.5182
	step [276/325], loss=3.7755
	step [277/325], loss=5.0198
	step [278/325], loss=4.8418
	step [279/325], loss=5.5919
	step [280/325], loss=5.5925
	step [281/325], loss=5.7125
	step [282/325], loss=6.5673
	step [283/325], loss=5.7156
	step [284/325], loss=4.8687
	step [285/325], loss=3.4224
	step [286/325], loss=4.6849
	step [287/325], loss=5.1719
	step [288/325], loss=5.0451
	step [289/325], loss=5.1884
	step [290/325], loss=5.2058
	step [291/325], loss=4.5212
	step [292/325], loss=4.2706
	step [293/325], loss=5.2165
	step [294/325], loss=4.9920
	step [295/325], loss=3.9003
	step [296/325], loss=6.1076
	step [297/325], loss=4.9971
	step [298/325], loss=5.3224
	step [299/325], loss=4.7589
	step [300/325], loss=4.9624
	step [301/325], loss=5.3361
	step [302/325], loss=7.3215
	step [303/325], loss=5.0229
	step [304/325], loss=5.9678
	step [305/325], loss=4.5016
	step [306/325], loss=5.5241
	step [307/325], loss=5.4398
	step [308/325], loss=4.5919
	step [309/325], loss=4.6075
	step [310/325], loss=5.4990
	step [311/325], loss=5.6577
	step [312/325], loss=4.1664
	step [313/325], loss=6.0765
	step [314/325], loss=4.5192
	step [315/325], loss=6.5785
	step [316/325], loss=4.2618
	step [317/325], loss=4.9656
	step [318/325], loss=4.3373
	step [319/325], loss=5.4732
	step [320/325], loss=6.1629
	step [321/325], loss=4.5744
	step [322/325], loss=4.1630
	step [323/325], loss=5.0363
	step [324/325], loss=4.4213
	step [325/325], loss=0.4092
	Evaluating
	loss=0.0177, precision=0.2298, recall=0.9965, f1=0.3735
saving model as: 2_saved_model.pth
Training epoch 21
	step [1/325], loss=5.0414
	step [2/325], loss=5.1863
	step [3/325], loss=4.5809
	step [4/325], loss=4.6404
	step [5/325], loss=6.0565
	step [6/325], loss=5.8918
	step [7/325], loss=5.6843
	step [8/325], loss=4.2128
	step [9/325], loss=4.4916
	step [10/325], loss=5.2142
	step [11/325], loss=5.6860
	step [12/325], loss=4.1201
	step [13/325], loss=5.2623
	step [14/325], loss=4.8994
	step [15/325], loss=5.5393
	step [16/325], loss=4.9439
	step [17/325], loss=6.2686
	step [18/325], loss=5.2274
	step [19/325], loss=5.0901
	step [20/325], loss=4.7014
	step [21/325], loss=5.9904
	step [22/325], loss=3.9312
	step [23/325], loss=5.0950
	step [24/325], loss=4.0877
	step [25/325], loss=4.6837
	step [26/325], loss=6.8595
	step [27/325], loss=5.6047
	step [28/325], loss=5.8890
	step [29/325], loss=4.5682
	step [30/325], loss=5.4164
	step [31/325], loss=4.9570
	step [32/325], loss=4.6109
	step [33/325], loss=5.6953
	step [34/325], loss=4.5755
	step [35/325], loss=5.9051
	step [36/325], loss=4.0220
	step [37/325], loss=4.9761
	step [38/325], loss=4.9430
	step [39/325], loss=5.2924
	step [40/325], loss=5.2851
	step [41/325], loss=5.3907
	step [42/325], loss=5.0845
	step [43/325], loss=4.2874
	step [44/325], loss=5.1137
	step [45/325], loss=4.8113
	step [46/325], loss=5.3483
	step [47/325], loss=4.9463
	step [48/325], loss=4.8533
	step [49/325], loss=4.9975
	step [50/325], loss=5.2703
	step [51/325], loss=4.4166
	step [52/325], loss=5.5864
	step [53/325], loss=5.5478
	step [54/325], loss=4.2874
	step [55/325], loss=4.1640
	step [56/325], loss=4.5108
	step [57/325], loss=5.2822
	step [58/325], loss=4.5717
	step [59/325], loss=5.0980
	step [60/325], loss=4.9743
	step [61/325], loss=5.2808
	step [62/325], loss=6.6652
	step [63/325], loss=4.7537
	step [64/325], loss=5.1800
	step [65/325], loss=4.8530
	step [66/325], loss=4.5351
	step [67/325], loss=4.9656
	step [68/325], loss=5.0639
	step [69/325], loss=4.9460
	step [70/325], loss=4.8582
	step [71/325], loss=5.7187
	step [72/325], loss=4.7951
	step [73/325], loss=4.6399
	step [74/325], loss=5.1115
	step [75/325], loss=5.7401
	step [76/325], loss=5.0189
	step [77/325], loss=4.0205
	step [78/325], loss=4.9845
	step [79/325], loss=4.5245
	step [80/325], loss=5.7938
	step [81/325], loss=5.0123
	step [82/325], loss=5.4566
	step [83/325], loss=5.0107
	step [84/325], loss=4.8897
	step [85/325], loss=5.6230
	step [86/325], loss=5.3114
	step [87/325], loss=7.7384
	step [88/325], loss=4.3832
	step [89/325], loss=4.8981
	step [90/325], loss=4.6215
	step [91/325], loss=4.2687
	step [92/325], loss=4.7815
	step [93/325], loss=4.9406
	step [94/325], loss=4.1816
	step [95/325], loss=5.4701
	step [96/325], loss=4.3487
	step [97/325], loss=5.5742
	step [98/325], loss=5.2142
	step [99/325], loss=6.1155
	step [100/325], loss=4.1580
	step [101/325], loss=4.9721
	step [102/325], loss=4.5653
	step [103/325], loss=4.8689
	step [104/325], loss=5.8882
	step [105/325], loss=4.7116
	step [106/325], loss=5.1666
	step [107/325], loss=5.1758
	step [108/325], loss=3.5680
	step [109/325], loss=5.5247
	step [110/325], loss=5.3730
	step [111/325], loss=5.2671
	step [112/325], loss=6.4301
	step [113/325], loss=4.4832
	step [114/325], loss=5.7639
	step [115/325], loss=4.9232
	step [116/325], loss=5.3479
	step [117/325], loss=4.2777
	step [118/325], loss=5.1439
	step [119/325], loss=4.5271
	step [120/325], loss=5.0729
	step [121/325], loss=5.7502
	step [122/325], loss=4.4648
	step [123/325], loss=5.2902
	step [124/325], loss=5.2270
	step [125/325], loss=4.4282
	step [126/325], loss=5.2606
	step [127/325], loss=5.2087
	step [128/325], loss=5.3424
	step [129/325], loss=4.7711
	step [130/325], loss=4.3083
	step [131/325], loss=4.3932
	step [132/325], loss=6.7693
	step [133/325], loss=5.8670
	step [134/325], loss=4.9041
	step [135/325], loss=5.1965
	step [136/325], loss=4.9618
	step [137/325], loss=4.7724
	step [138/325], loss=5.0069
	step [139/325], loss=4.5440
	step [140/325], loss=4.3426
	step [141/325], loss=4.9713
	step [142/325], loss=6.6660
	step [143/325], loss=5.2116
	step [144/325], loss=5.0635
	step [145/325], loss=5.5343
	step [146/325], loss=5.7596
	step [147/325], loss=7.2691
	step [148/325], loss=5.9767
	step [149/325], loss=4.7498
	step [150/325], loss=5.0981
	step [151/325], loss=6.2774
	step [152/325], loss=4.6941
	step [153/325], loss=5.2044
	step [154/325], loss=6.4279
	step [155/325], loss=6.4901
	step [156/325], loss=4.6746
	step [157/325], loss=5.1158
	step [158/325], loss=5.7010
	step [159/325], loss=5.8302
	step [160/325], loss=3.8498
	step [161/325], loss=4.7460
	step [162/325], loss=5.5235
	step [163/325], loss=5.4163
	step [164/325], loss=4.1932
	step [165/325], loss=3.9958
	step [166/325], loss=3.8251
	step [167/325], loss=5.5302
	step [168/325], loss=4.5538
	step [169/325], loss=5.7837
	step [170/325], loss=5.0440
	step [171/325], loss=5.5178
	step [172/325], loss=4.3499
	step [173/325], loss=4.5466
	step [174/325], loss=4.6082
	step [175/325], loss=5.3470
	step [176/325], loss=4.6989
	step [177/325], loss=4.9733
	step [178/325], loss=5.1110
	step [179/325], loss=3.8600
	step [180/325], loss=5.1177
	step [181/325], loss=4.4287
	step [182/325], loss=4.6792
	step [183/325], loss=4.9192
	step [184/325], loss=5.4646
	step [185/325], loss=5.2931
	step [186/325], loss=5.1379
	step [187/325], loss=5.7859
	step [188/325], loss=4.2459
	step [189/325], loss=5.8616
	step [190/325], loss=6.2137
	step [191/325], loss=5.2858
	step [192/325], loss=4.3548
	step [193/325], loss=4.4972
	step [194/325], loss=5.7994
	step [195/325], loss=4.4483
	step [196/325], loss=5.2274
	step [197/325], loss=4.5875
	step [198/325], loss=3.7372
	step [199/325], loss=5.5458
	step [200/325], loss=4.6889
	step [201/325], loss=4.6787
	step [202/325], loss=3.9723
	step [203/325], loss=4.2373
	step [204/325], loss=4.1924
	step [205/325], loss=6.4314
	step [206/325], loss=4.7703
	step [207/325], loss=5.6230
	step [208/325], loss=5.3082
	step [209/325], loss=4.3444
	step [210/325], loss=6.5855
	step [211/325], loss=4.8029
	step [212/325], loss=4.2118
	step [213/325], loss=5.7389
	step [214/325], loss=4.1313
	step [215/325], loss=5.6276
	step [216/325], loss=4.8960
	step [217/325], loss=5.8669
	step [218/325], loss=4.2475
	step [219/325], loss=3.7780
	step [220/325], loss=4.4262
	step [221/325], loss=6.6884
	step [222/325], loss=4.8164
	step [223/325], loss=4.5114
	step [224/325], loss=4.7341
	step [225/325], loss=5.5854
	step [226/325], loss=5.7871
	step [227/325], loss=4.9384
	step [228/325], loss=4.3460
	step [229/325], loss=4.7366
	step [230/325], loss=4.6972
	step [231/325], loss=4.8563
	step [232/325], loss=4.4533
	step [233/325], loss=5.0555
	step [234/325], loss=4.8763
	step [235/325], loss=4.7300
	step [236/325], loss=4.5329
	step [237/325], loss=4.4081
	step [238/325], loss=5.2747
	step [239/325], loss=4.3637
	step [240/325], loss=4.3425
	step [241/325], loss=4.6406
	step [242/325], loss=4.9572
	step [243/325], loss=5.0925
	step [244/325], loss=5.9970
	step [245/325], loss=5.4339
	step [246/325], loss=4.9739
	step [247/325], loss=6.5973
	step [248/325], loss=5.7647
	step [249/325], loss=4.9543
	step [250/325], loss=5.4947
	step [251/325], loss=5.5102
	step [252/325], loss=6.4460
	step [253/325], loss=4.8973
	step [254/325], loss=4.3157
	step [255/325], loss=4.4982
	step [256/325], loss=3.6948
	step [257/325], loss=4.2396
	step [258/325], loss=5.1844
	step [259/325], loss=5.3243
	step [260/325], loss=6.2311
	step [261/325], loss=6.1586
	step [262/325], loss=4.2542
	step [263/325], loss=6.9484
	step [264/325], loss=4.3557
	step [265/325], loss=5.7085
	step [266/325], loss=5.1355
	step [267/325], loss=4.5332
	step [268/325], loss=5.6686
	step [269/325], loss=4.9656
	step [270/325], loss=6.7072
	step [271/325], loss=4.4048
	step [272/325], loss=4.7783
	step [273/325], loss=4.9654
	step [274/325], loss=4.5238
	step [275/325], loss=6.0775
	step [276/325], loss=5.0242
	step [277/325], loss=4.2077
	step [278/325], loss=6.2170
	step [279/325], loss=5.9271
	step [280/325], loss=5.1240
	step [281/325], loss=5.4940
	step [282/325], loss=5.7658
	step [283/325], loss=4.6469
	step [284/325], loss=5.9815
	step [285/325], loss=5.3140
	step [286/325], loss=4.2956
	step [287/325], loss=5.2087
	step [288/325], loss=4.1015
	step [289/325], loss=4.3585
	step [290/325], loss=5.2581
	step [291/325], loss=4.5515
	step [292/325], loss=3.8032
	step [293/325], loss=6.5228
	step [294/325], loss=4.5817
	step [295/325], loss=4.8585
	step [296/325], loss=6.1363
	step [297/325], loss=4.5805
	step [298/325], loss=5.6054
	step [299/325], loss=5.7010
	step [300/325], loss=4.6195
	step [301/325], loss=5.9550
	step [302/325], loss=5.6095
	step [303/325], loss=5.9409
	step [304/325], loss=4.4180
	step [305/325], loss=4.9762
	step [306/325], loss=5.1323
	step [307/325], loss=4.0124
	step [308/325], loss=5.0939
	step [309/325], loss=4.2655
	step [310/325], loss=4.0637
	step [311/325], loss=4.4678
	step [312/325], loss=5.3294
	step [313/325], loss=4.5330
	step [314/325], loss=4.5449
	step [315/325], loss=4.3898
	step [316/325], loss=4.0339
	step [317/325], loss=4.7806
	step [318/325], loss=4.7676
	step [319/325], loss=4.1033
	step [320/325], loss=5.3819
	step [321/325], loss=5.0822
	step [322/325], loss=4.1331
	step [323/325], loss=6.1057
	step [324/325], loss=5.3813
	step [325/325], loss=0.3086
	Evaluating
	loss=0.0213, precision=0.1997, recall=0.9974, f1=0.3328
Training epoch 22
	step [1/325], loss=4.9055
	step [2/325], loss=5.1475
	step [3/325], loss=4.9464
	step [4/325], loss=4.6763
	step [5/325], loss=4.1208
	step [6/325], loss=5.3170
	step [7/325], loss=5.7487
	step [8/325], loss=4.2141
	step [9/325], loss=5.0740
	step [10/325], loss=4.0015
	step [11/325], loss=5.0100
	step [12/325], loss=4.6591
	step [13/325], loss=5.6692
	step [14/325], loss=6.4011
	step [15/325], loss=4.4643
	step [16/325], loss=5.2560
	step [17/325], loss=4.8679
	step [18/325], loss=5.5439
	step [19/325], loss=4.5841
	step [20/325], loss=4.4300
	step [21/325], loss=4.3850
	step [22/325], loss=5.6226
	step [23/325], loss=5.0159
	step [24/325], loss=4.0850
	step [25/325], loss=4.0434
	step [26/325], loss=4.7815
	step [27/325], loss=4.6588
	step [28/325], loss=3.6070
	step [29/325], loss=4.9977
	step [30/325], loss=4.5772
	step [31/325], loss=4.4836
	step [32/325], loss=6.2839
	step [33/325], loss=4.0422
	step [34/325], loss=4.2202
	step [35/325], loss=4.9695
	step [36/325], loss=5.5986
	step [37/325], loss=4.1619
	step [38/325], loss=4.5640
	step [39/325], loss=5.7260
	step [40/325], loss=5.7696
	step [41/325], loss=5.0019
	step [42/325], loss=5.4184
	step [43/325], loss=4.3835
	step [44/325], loss=4.8126
	step [45/325], loss=4.9157
	step [46/325], loss=4.5500
	step [47/325], loss=4.4859
	step [48/325], loss=5.0974
	step [49/325], loss=4.7850
	step [50/325], loss=3.9673
	step [51/325], loss=4.8755
	step [52/325], loss=4.6431
	step [53/325], loss=5.2899
	step [54/325], loss=5.4802
	step [55/325], loss=5.2336
	step [56/325], loss=4.5071
	step [57/325], loss=6.9309
	step [58/325], loss=5.1735
	step [59/325], loss=5.6069
	step [60/325], loss=5.1544
	step [61/325], loss=6.7490
	step [62/325], loss=6.1709
	step [63/325], loss=3.6360
	step [64/325], loss=6.5343
	step [65/325], loss=4.2635
	step [66/325], loss=4.9721
	step [67/325], loss=4.6272
	step [68/325], loss=6.7400
	step [69/325], loss=5.2379
	step [70/325], loss=4.9089
	step [71/325], loss=5.8793
	step [72/325], loss=4.7073
	step [73/325], loss=4.6628
	step [74/325], loss=5.1692
	step [75/325], loss=4.5058
	step [76/325], loss=4.7863
	step [77/325], loss=4.4172
	step [78/325], loss=3.6246
	step [79/325], loss=5.2942
	step [80/325], loss=4.1186
	step [81/325], loss=5.0182
	step [82/325], loss=4.5721
	step [83/325], loss=5.9198
	step [84/325], loss=4.0928
	step [85/325], loss=4.9972
	step [86/325], loss=4.4658
	step [87/325], loss=4.7264
	step [88/325], loss=4.8430
	step [89/325], loss=4.1692
	step [90/325], loss=6.4261
	step [91/325], loss=4.3715
	step [92/325], loss=5.1907
	step [93/325], loss=4.8815
	step [94/325], loss=4.4700
	step [95/325], loss=4.3770
	step [96/325], loss=4.3212
	step [97/325], loss=5.8760
	step [98/325], loss=5.5282
	step [99/325], loss=6.0285
	step [100/325], loss=5.0962
	step [101/325], loss=4.3631
	step [102/325], loss=4.3976
	step [103/325], loss=4.1494
	step [104/325], loss=5.3996
	step [105/325], loss=4.1512
	step [106/325], loss=4.0924
	step [107/325], loss=5.9561
	step [108/325], loss=4.7026
	step [109/325], loss=7.1985
	step [110/325], loss=4.5631
	step [111/325], loss=3.4205
	step [112/325], loss=5.8254
	step [113/325], loss=4.9830
	step [114/325], loss=5.5895
	step [115/325], loss=4.3160
	step [116/325], loss=4.8510
	step [117/325], loss=5.4451
	step [118/325], loss=4.6730
	step [119/325], loss=5.3311
	step [120/325], loss=5.7959
	step [121/325], loss=5.4948
	step [122/325], loss=5.4514
	step [123/325], loss=4.0347
	step [124/325], loss=4.9238
	step [125/325], loss=4.2120
	step [126/325], loss=5.2494
	step [127/325], loss=5.4239
	step [128/325], loss=5.6349
	step [129/325], loss=4.1773
	step [130/325], loss=4.9005
	step [131/325], loss=4.4259
	step [132/325], loss=4.5217
	step [133/325], loss=5.3642
	step [134/325], loss=5.3184
	step [135/325], loss=5.5899
	step [136/325], loss=4.5700
	step [137/325], loss=3.4589
	step [138/325], loss=5.2297
	step [139/325], loss=5.2468
	step [140/325], loss=4.3394
	step [141/325], loss=5.1620
	step [142/325], loss=5.0941
	step [143/325], loss=4.2332
	step [144/325], loss=4.3819
	step [145/325], loss=4.6280
	step [146/325], loss=5.3651
	step [147/325], loss=6.6594
	step [148/325], loss=4.3822
	step [149/325], loss=5.2897
	step [150/325], loss=5.2437
	step [151/325], loss=5.5694
	step [152/325], loss=4.1063
	step [153/325], loss=4.8958
	step [154/325], loss=4.5246
	step [155/325], loss=4.5976
	step [156/325], loss=4.4406
	step [157/325], loss=3.6195
	step [158/325], loss=4.4115
	step [159/325], loss=4.7035
	step [160/325], loss=4.3720
	step [161/325], loss=4.8468
	step [162/325], loss=7.5228
	step [163/325], loss=4.1575
	step [164/325], loss=5.4097
	step [165/325], loss=4.0197
	step [166/325], loss=5.1798
	step [167/325], loss=6.2509
	step [168/325], loss=5.1716
	step [169/325], loss=5.2626
	step [170/325], loss=3.3419
	step [171/325], loss=5.4116
	step [172/325], loss=3.8822
	step [173/325], loss=6.3397
	step [174/325], loss=5.6417
	step [175/325], loss=4.1691
	step [176/325], loss=4.4649
	step [177/325], loss=4.7004
	step [178/325], loss=4.4547
	step [179/325], loss=4.3586
	step [180/325], loss=4.6747
	step [181/325], loss=4.1172
	step [182/325], loss=4.5427
	step [183/325], loss=4.0412
	step [184/325], loss=5.4169
	step [185/325], loss=4.9970
	step [186/325], loss=5.1057
	step [187/325], loss=4.5365
	step [188/325], loss=3.8318
	step [189/325], loss=4.3839
	step [190/325], loss=4.9647
	step [191/325], loss=4.5201
	step [192/325], loss=6.0094
	step [193/325], loss=5.2645
	step [194/325], loss=5.3922
	step [195/325], loss=5.0726
	step [196/325], loss=5.1238
	step [197/325], loss=4.3365
	step [198/325], loss=5.8590
	step [199/325], loss=5.6277
	step [200/325], loss=4.1788
	step [201/325], loss=4.2880
	step [202/325], loss=6.9049
	step [203/325], loss=3.7753
	step [204/325], loss=5.6562
	step [205/325], loss=4.8119
	step [206/325], loss=4.5087
	step [207/325], loss=4.8837
	step [208/325], loss=4.7547
	step [209/325], loss=4.9066
	step [210/325], loss=4.7967
	step [211/325], loss=4.9732
	step [212/325], loss=4.6407
	step [213/325], loss=3.7641
	step [214/325], loss=4.9768
	step [215/325], loss=4.1553
	step [216/325], loss=5.1071
	step [217/325], loss=5.3235
	step [218/325], loss=5.0644
	step [219/325], loss=4.1456
	step [220/325], loss=4.7270
	step [221/325], loss=4.9638
	step [222/325], loss=5.1199
	step [223/325], loss=5.6509
	step [224/325], loss=6.1259
	step [225/325], loss=5.0630
	step [226/325], loss=5.9527
	step [227/325], loss=4.9599
	step [228/325], loss=3.7596
	step [229/325], loss=4.7747
	step [230/325], loss=4.6919
	step [231/325], loss=4.8027
	step [232/325], loss=6.1513
	step [233/325], loss=4.4458
	step [234/325], loss=4.2484
	step [235/325], loss=4.9964
	step [236/325], loss=4.6685
	step [237/325], loss=3.5336
	step [238/325], loss=4.4015
	step [239/325], loss=5.4780
	step [240/325], loss=4.6607
	step [241/325], loss=5.8986
	step [242/325], loss=5.7223
	step [243/325], loss=3.4907
	step [244/325], loss=5.0341
	step [245/325], loss=5.4121
	step [246/325], loss=4.4742
	step [247/325], loss=5.0254
	step [248/325], loss=4.7183
	step [249/325], loss=4.3234
	step [250/325], loss=4.9359
	step [251/325], loss=4.3905
	step [252/325], loss=4.5757
	step [253/325], loss=4.9700
	step [254/325], loss=4.9231
	step [255/325], loss=4.7207
	step [256/325], loss=5.7131
	step [257/325], loss=4.7291
	step [258/325], loss=4.5506
	step [259/325], loss=5.0886
	step [260/325], loss=5.3919
	step [261/325], loss=4.0126
	step [262/325], loss=3.8826
	step [263/325], loss=7.4040
	step [264/325], loss=4.9421
	step [265/325], loss=4.6002
	step [266/325], loss=4.3116
	step [267/325], loss=4.6290
	step [268/325], loss=4.5024
	step [269/325], loss=4.3270
	step [270/325], loss=4.1247
	step [271/325], loss=4.5501
	step [272/325], loss=5.9315
	step [273/325], loss=4.6641
	step [274/325], loss=3.4566
	step [275/325], loss=4.5093
	step [276/325], loss=4.5153
	step [277/325], loss=4.8016
	step [278/325], loss=3.6479
	step [279/325], loss=4.1694
	step [280/325], loss=5.9583
	step [281/325], loss=4.7593
	step [282/325], loss=5.6401
	step [283/325], loss=5.9563
	step [284/325], loss=5.2632
	step [285/325], loss=5.9948
	step [286/325], loss=4.8700
	step [287/325], loss=5.0671
	step [288/325], loss=6.0364
	step [289/325], loss=4.9625
	step [290/325], loss=5.1111
	step [291/325], loss=4.9134
	step [292/325], loss=4.4529
	step [293/325], loss=4.1550
	step [294/325], loss=4.5146
	step [295/325], loss=5.0147
	step [296/325], loss=5.2250
	step [297/325], loss=3.9502
	step [298/325], loss=6.3193
	step [299/325], loss=3.9162
	step [300/325], loss=4.4161
	step [301/325], loss=4.5735
	step [302/325], loss=4.5312
	step [303/325], loss=5.6344
	step [304/325], loss=3.9425
	step [305/325], loss=4.9986
	step [306/325], loss=5.5910
	step [307/325], loss=5.5172
	step [308/325], loss=4.8835
	step [309/325], loss=5.4902
	step [310/325], loss=4.0954
	step [311/325], loss=5.2529
	step [312/325], loss=4.5706
	step [313/325], loss=6.2686
	step [314/325], loss=4.0165
	step [315/325], loss=4.8703
	step [316/325], loss=5.2618
	step [317/325], loss=5.0333
	step [318/325], loss=4.7759
	step [319/325], loss=4.6857
	step [320/325], loss=4.4444
	step [321/325], loss=4.8893
	step [322/325], loss=4.8347
	step [323/325], loss=5.0875
	step [324/325], loss=4.0324
	step [325/325], loss=0.1450
	Evaluating
	loss=0.0169, precision=0.2292, recall=0.9966, f1=0.3726
Training epoch 23
	step [1/325], loss=4.1181
	step [2/325], loss=4.3216
	step [3/325], loss=4.5160
	step [4/325], loss=4.8526
	step [5/325], loss=5.0802
	step [6/325], loss=5.3503
	step [7/325], loss=4.8768
	step [8/325], loss=3.6667
	step [9/325], loss=5.7398
	step [10/325], loss=4.5715
	step [11/325], loss=3.9189
	step [12/325], loss=5.4725
	step [13/325], loss=4.1253
	step [14/325], loss=5.2764
	step [15/325], loss=4.6936
	step [16/325], loss=5.2075
	step [17/325], loss=4.8149
	step [18/325], loss=4.1079
	step [19/325], loss=3.3478
	step [20/325], loss=4.7831
	step [21/325], loss=4.6414
	step [22/325], loss=5.1635
	step [23/325], loss=5.2319
	step [24/325], loss=4.3825
	step [25/325], loss=4.5864
	step [26/325], loss=4.4700
	step [27/325], loss=4.3837
	step [28/325], loss=5.0089
	step [29/325], loss=5.2462
	step [30/325], loss=4.4967
	step [31/325], loss=4.7649
	step [32/325], loss=4.5129
	step [33/325], loss=4.3735
	step [34/325], loss=4.7458
	step [35/325], loss=3.9318
	step [36/325], loss=5.5095
	step [37/325], loss=5.5409
	step [38/325], loss=4.4897
	step [39/325], loss=5.4133
	step [40/325], loss=4.6252
	step [41/325], loss=4.3120
	step [42/325], loss=5.0247
	step [43/325], loss=3.8422
	step [44/325], loss=4.2818
	step [45/325], loss=5.1070
	step [46/325], loss=4.5348
	step [47/325], loss=4.5024
	step [48/325], loss=3.9734
	step [49/325], loss=3.9636
	step [50/325], loss=5.2418
	step [51/325], loss=4.9684
	step [52/325], loss=5.2596
	step [53/325], loss=4.5506
	step [54/325], loss=5.5506
	step [55/325], loss=4.2390
	step [56/325], loss=5.8293
	step [57/325], loss=5.1776
	step [58/325], loss=4.9095
	step [59/325], loss=5.1139
	step [60/325], loss=4.9310
	step [61/325], loss=4.5917
	step [62/325], loss=4.7146
	step [63/325], loss=3.1791
	step [64/325], loss=4.8073
	step [65/325], loss=4.8568
	step [66/325], loss=6.2760
	step [67/325], loss=5.7306
	step [68/325], loss=5.9492
	step [69/325], loss=5.8028
	step [70/325], loss=4.3709
	step [71/325], loss=6.6051
	step [72/325], loss=6.0106
	step [73/325], loss=5.5109
	step [74/325], loss=7.5508
	step [75/325], loss=4.2287
	step [76/325], loss=4.8150
	step [77/325], loss=5.9707
	step [78/325], loss=6.0318
	step [79/325], loss=7.2731
	step [80/325], loss=4.9268
	step [81/325], loss=5.9131
	step [82/325], loss=4.7693
	step [83/325], loss=4.8465
	step [84/325], loss=4.2346
	step [85/325], loss=4.1586
	step [86/325], loss=4.3356
	step [87/325], loss=4.3557
	step [88/325], loss=5.6273
	step [89/325], loss=4.1303
	step [90/325], loss=6.2177
	step [91/325], loss=4.5772
	step [92/325], loss=6.4886
	step [93/325], loss=4.9859
	step [94/325], loss=5.0842
	step [95/325], loss=6.6605
	step [96/325], loss=5.6730
	step [97/325], loss=4.2574
	step [98/325], loss=4.7642
	step [99/325], loss=6.0032
	step [100/325], loss=5.8463
	step [101/325], loss=6.4556
	step [102/325], loss=4.8476
	step [103/325], loss=5.3658
	step [104/325], loss=6.1663
	step [105/325], loss=5.5359
	step [106/325], loss=4.5467
	step [107/325], loss=4.4190
	step [108/325], loss=5.5621
	step [109/325], loss=6.5943
	step [110/325], loss=4.1764
	step [111/325], loss=3.5809
	step [112/325], loss=5.3558
	step [113/325], loss=4.2428
	step [114/325], loss=5.7876
	step [115/325], loss=5.8608
	step [116/325], loss=6.1942
	step [117/325], loss=5.8915
	step [118/325], loss=4.7463
	step [119/325], loss=5.5032
	step [120/325], loss=4.5138
	step [121/325], loss=4.6912
	step [122/325], loss=4.4933
	step [123/325], loss=4.7544
	step [124/325], loss=5.7102
	step [125/325], loss=4.6637
	step [126/325], loss=5.2178
	step [127/325], loss=5.5697
	step [128/325], loss=4.7497
	step [129/325], loss=5.1601
	step [130/325], loss=4.7232
	step [131/325], loss=4.3448
	step [132/325], loss=5.5689
	step [133/325], loss=4.2873
	step [134/325], loss=5.6624
	step [135/325], loss=4.9750
	step [136/325], loss=4.0143
	step [137/325], loss=5.4420
	step [138/325], loss=4.8701
	step [139/325], loss=4.8970
	step [140/325], loss=5.6138
	step [141/325], loss=4.9597
	step [142/325], loss=4.0707
	step [143/325], loss=4.6779
	step [144/325], loss=5.2860
	step [145/325], loss=5.5993
	step [146/325], loss=6.3706
	step [147/325], loss=5.0356
	step [148/325], loss=4.2501
	step [149/325], loss=6.3933
	step [150/325], loss=5.0614
	step [151/325], loss=6.7418
	step [152/325], loss=5.0334
	step [153/325], loss=5.2279
	step [154/325], loss=4.9343
	step [155/325], loss=4.7284
	step [156/325], loss=4.7416
	step [157/325], loss=4.1254
	step [158/325], loss=7.0557
	step [159/325], loss=4.4783
	step [160/325], loss=5.1747
	step [161/325], loss=5.3634
	step [162/325], loss=4.5035
	step [163/325], loss=3.9941
	step [164/325], loss=3.4938
	step [165/325], loss=4.7721
	step [166/325], loss=4.1376
	step [167/325], loss=4.3861
	step [168/325], loss=5.5842
	step [169/325], loss=4.2085
	step [170/325], loss=4.7440
	step [171/325], loss=4.5332
	step [172/325], loss=5.3401
	step [173/325], loss=5.5027
	step [174/325], loss=4.8975
	step [175/325], loss=6.2370
	step [176/325], loss=6.0489
	step [177/325], loss=5.3580
	step [178/325], loss=4.4621
	step [179/325], loss=4.9601
	step [180/325], loss=4.7697
	step [181/325], loss=5.6473
	step [182/325], loss=4.2487
	step [183/325], loss=5.2080
	step [184/325], loss=4.9278
	step [185/325], loss=4.1004
	step [186/325], loss=4.8839
	step [187/325], loss=5.2651
	step [188/325], loss=5.8442
	step [189/325], loss=5.2044
	step [190/325], loss=5.9505
	step [191/325], loss=5.7747
	step [192/325], loss=4.5172
	step [193/325], loss=4.7202
	step [194/325], loss=4.9936
	step [195/325], loss=4.3785
	step [196/325], loss=4.2695
	step [197/325], loss=4.7476
	step [198/325], loss=3.7157
	step [199/325], loss=3.9261
	step [200/325], loss=4.1976
	step [201/325], loss=4.3751
	step [202/325], loss=4.8193
	step [203/325], loss=5.4960
	step [204/325], loss=5.3349
	step [205/325], loss=3.4264
	step [206/325], loss=5.0105
	step [207/325], loss=4.0097
	step [208/325], loss=5.0017
	step [209/325], loss=3.5675
	step [210/325], loss=6.2256
	step [211/325], loss=4.7432
	step [212/325], loss=5.3543
	step [213/325], loss=4.9604
	step [214/325], loss=4.8939
	step [215/325], loss=5.0429
	step [216/325], loss=5.0764
	step [217/325], loss=4.7831
	step [218/325], loss=4.6363
	step [219/325], loss=6.7760
	step [220/325], loss=5.9950
	step [221/325], loss=4.4610
	step [222/325], loss=6.2814
	step [223/325], loss=4.7326
	step [224/325], loss=4.9123
	step [225/325], loss=4.3943
	step [226/325], loss=4.0853
	step [227/325], loss=4.1153
	step [228/325], loss=4.7160
	step [229/325], loss=5.0707
	step [230/325], loss=4.6837
	step [231/325], loss=3.3282
	step [232/325], loss=4.5174
	step [233/325], loss=4.5995
	step [234/325], loss=5.5134
	step [235/325], loss=4.4729
	step [236/325], loss=4.5724
	step [237/325], loss=5.3384
	step [238/325], loss=5.0606
	step [239/325], loss=5.0445
	step [240/325], loss=4.4402
	step [241/325], loss=6.7106
	step [242/325], loss=5.6428
	step [243/325], loss=5.7050
	step [244/325], loss=4.7885
	step [245/325], loss=4.2999
	step [246/325], loss=4.1486
	step [247/325], loss=4.1746
	step [248/325], loss=4.8849
	step [249/325], loss=3.9367
	step [250/325], loss=4.1102
	step [251/325], loss=4.8939
	step [252/325], loss=5.0892
	step [253/325], loss=5.2609
	step [254/325], loss=4.6396
	step [255/325], loss=4.2576
	step [256/325], loss=4.8436
	step [257/325], loss=5.1463
	step [258/325], loss=3.7378
	step [259/325], loss=5.9500
	step [260/325], loss=5.3017
	step [261/325], loss=5.6295
	step [262/325], loss=4.7051
	step [263/325], loss=4.7887
	step [264/325], loss=5.0736
	step [265/325], loss=3.7804
	step [266/325], loss=4.6253
	step [267/325], loss=4.8659
	step [268/325], loss=4.0236
	step [269/325], loss=4.0934
	step [270/325], loss=5.2538
	step [271/325], loss=4.5836
	step [272/325], loss=4.7351
	step [273/325], loss=3.9158
	step [274/325], loss=4.6437
	step [275/325], loss=4.4254
	step [276/325], loss=3.9539
	step [277/325], loss=3.8677
	step [278/325], loss=6.1020
	step [279/325], loss=4.2378
	step [280/325], loss=5.0237
	step [281/325], loss=5.0239
	step [282/325], loss=4.9952
	step [283/325], loss=4.5369
	step [284/325], loss=5.8660
	step [285/325], loss=5.5393
	step [286/325], loss=5.2949
	step [287/325], loss=4.3907
	step [288/325], loss=4.5826
	step [289/325], loss=4.7821
	step [290/325], loss=4.6391
	step [291/325], loss=3.5109
	step [292/325], loss=3.1229
	step [293/325], loss=4.1150
	step [294/325], loss=4.2232
	step [295/325], loss=5.5170
	step [296/325], loss=4.8551
	step [297/325], loss=5.2210
	step [298/325], loss=4.0595
	step [299/325], loss=5.2371
	step [300/325], loss=5.6158
	step [301/325], loss=4.2447
	step [302/325], loss=4.7032
	step [303/325], loss=4.7602
	step [304/325], loss=5.2409
	step [305/325], loss=5.0084
	step [306/325], loss=6.0228
	step [307/325], loss=3.9424
	step [308/325], loss=5.5318
	step [309/325], loss=5.0259
	step [310/325], loss=4.5731
	step [311/325], loss=5.2731
	step [312/325], loss=5.2420
	step [313/325], loss=5.6047
	step [314/325], loss=5.0144
	step [315/325], loss=3.8431
	step [316/325], loss=4.6475
	step [317/325], loss=4.3543
	step [318/325], loss=3.7749
	step [319/325], loss=4.1508
	step [320/325], loss=5.6691
	step [321/325], loss=5.3180
	step [322/325], loss=5.6673
	step [323/325], loss=4.6457
	step [324/325], loss=4.5622
	step [325/325], loss=0.4261
	Evaluating
	loss=0.0215, precision=0.1925, recall=0.9975, f1=0.3227
Training epoch 24
	step [1/325], loss=5.0402
	step [2/325], loss=5.1460
	step [3/325], loss=4.9617
	step [4/325], loss=5.0882
	step [5/325], loss=5.0097
	step [6/325], loss=4.2716
	step [7/325], loss=5.1561
	step [8/325], loss=4.2026
	step [9/325], loss=5.8248
	step [10/325], loss=4.4849
	step [11/325], loss=4.3625
	step [12/325], loss=4.1084
	step [13/325], loss=4.9652
	step [14/325], loss=3.9411
	step [15/325], loss=3.7076
	step [16/325], loss=5.3699
	step [17/325], loss=6.1956
	step [18/325], loss=4.8867
	step [19/325], loss=4.3588
	step [20/325], loss=5.3004
	step [21/325], loss=4.2142
	step [22/325], loss=5.3229
	step [23/325], loss=4.3473
	step [24/325], loss=4.5214
	step [25/325], loss=4.3351
	step [26/325], loss=3.8249
	step [27/325], loss=4.9648
	step [28/325], loss=4.9261
	step [29/325], loss=4.7217
	step [30/325], loss=6.0268
	step [31/325], loss=4.6409
	step [32/325], loss=4.8709
	step [33/325], loss=5.1214
	step [34/325], loss=3.7222
	step [35/325], loss=4.1662
	step [36/325], loss=3.2673
	step [37/325], loss=4.9089
	step [38/325], loss=5.7607
	step [39/325], loss=5.6564
	step [40/325], loss=5.1174
	step [41/325], loss=4.2709
	step [42/325], loss=5.0718
	step [43/325], loss=3.2483
	step [44/325], loss=4.5278
	step [45/325], loss=5.1972
	step [46/325], loss=3.8252
	step [47/325], loss=3.6781
	step [48/325], loss=4.5079
	step [49/325], loss=5.1101
	step [50/325], loss=5.0441
	step [51/325], loss=5.6732
	step [52/325], loss=4.1309
	step [53/325], loss=3.8283
	step [54/325], loss=5.2441
	step [55/325], loss=4.2262
	step [56/325], loss=5.0663
	step [57/325], loss=3.9651
	step [58/325], loss=4.3232
	step [59/325], loss=6.1552
	step [60/325], loss=4.6934
	step [61/325], loss=4.8172
	step [62/325], loss=5.0954
	step [63/325], loss=5.0066
	step [64/325], loss=4.2886
	step [65/325], loss=4.6595
	step [66/325], loss=5.1540
	step [67/325], loss=4.1821
	step [68/325], loss=4.8205
	step [69/325], loss=3.8119
	step [70/325], loss=5.7531
	step [71/325], loss=4.9220
	step [72/325], loss=4.1473
	step [73/325], loss=4.4657
	step [74/325], loss=4.5590
	step [75/325], loss=5.1134
	step [76/325], loss=4.8286
	step [77/325], loss=4.7145
	step [78/325], loss=4.7486
	step [79/325], loss=5.5963
	step [80/325], loss=4.1070
	step [81/325], loss=5.7512
	step [82/325], loss=4.7270
	step [83/325], loss=4.0295
	step [84/325], loss=4.7398
	step [85/325], loss=4.5858
	step [86/325], loss=4.6452
	step [87/325], loss=4.1155
	step [88/325], loss=3.5089
	step [89/325], loss=4.5354
	step [90/325], loss=5.1525
	step [91/325], loss=4.2185
	step [92/325], loss=5.0182
	step [93/325], loss=3.1981
	step [94/325], loss=3.8501
	step [95/325], loss=3.8895
	step [96/325], loss=4.7149
	step [97/325], loss=4.9660
	step [98/325], loss=4.8327
	step [99/325], loss=4.8040
	step [100/325], loss=4.1426
	step [101/325], loss=5.3967
	step [102/325], loss=4.4013
	step [103/325], loss=4.0557
	step [104/325], loss=4.1823
	step [105/325], loss=4.5227
	step [106/325], loss=5.1479
	step [107/325], loss=4.1868
	step [108/325], loss=5.2629
	step [109/325], loss=5.2485
	step [110/325], loss=4.8701
	step [111/325], loss=3.8921
	step [112/325], loss=3.8365
	step [113/325], loss=4.5968
	step [114/325], loss=4.0748
	step [115/325], loss=4.5134
	step [116/325], loss=4.9336
	step [117/325], loss=4.2341
	step [118/325], loss=3.3272
	step [119/325], loss=5.4984
	step [120/325], loss=4.5488
	step [121/325], loss=3.7386
	step [122/325], loss=4.5993
	step [123/325], loss=5.3380
	step [124/325], loss=4.7879
	step [125/325], loss=4.6760
	step [126/325], loss=4.6690
	step [127/325], loss=4.4505
	step [128/325], loss=3.3705
	step [129/325], loss=5.0813
	step [130/325], loss=5.2010
	step [131/325], loss=5.2795
	step [132/325], loss=4.2129
	step [133/325], loss=5.4457
	step [134/325], loss=5.3250
	step [135/325], loss=4.7232
	step [136/325], loss=4.7290
	step [137/325], loss=4.5582
	step [138/325], loss=4.9323
	step [139/325], loss=4.3938
	step [140/325], loss=4.3655
	step [141/325], loss=5.3607
	step [142/325], loss=3.4388
	step [143/325], loss=4.8479
	step [144/325], loss=5.5442
	step [145/325], loss=4.9046
	step [146/325], loss=4.4276
	step [147/325], loss=4.8128
	step [148/325], loss=6.0429
	step [149/325], loss=4.0535
	step [150/325], loss=4.1365
	step [151/325], loss=4.6486
	step [152/325], loss=5.3406
	step [153/325], loss=4.4501
	step [154/325], loss=4.4600
	step [155/325], loss=5.4022
	step [156/325], loss=4.5144
	step [157/325], loss=4.7161
	step [158/325], loss=4.4860
	step [159/325], loss=4.8440
	step [160/325], loss=4.4619
	step [161/325], loss=3.8796
	step [162/325], loss=4.5984
	step [163/325], loss=4.2915
	step [164/325], loss=4.7528
	step [165/325], loss=5.5652
	step [166/325], loss=5.7881
	step [167/325], loss=4.4703
	step [168/325], loss=5.0534
	step [169/325], loss=5.6160
	step [170/325], loss=4.7761
	step [171/325], loss=4.2782
	step [172/325], loss=6.2901
	step [173/325], loss=4.8488
	step [174/325], loss=4.0040
	step [175/325], loss=4.4809
	step [176/325], loss=4.3750
	step [177/325], loss=4.1788
	step [178/325], loss=6.0850
	step [179/325], loss=2.8433
	step [180/325], loss=4.7975
	step [181/325], loss=5.1300
	step [182/325], loss=6.2215
	step [183/325], loss=3.2156
	step [184/325], loss=3.8804
	step [185/325], loss=5.3979
	step [186/325], loss=5.1296
	step [187/325], loss=5.2733
	step [188/325], loss=5.5384
	step [189/325], loss=4.8842
	step [190/325], loss=4.6468
	step [191/325], loss=4.7134
	step [192/325], loss=5.4651
	step [193/325], loss=4.2512
	step [194/325], loss=5.3187
	step [195/325], loss=4.4794
	step [196/325], loss=4.8010
	step [197/325], loss=5.0026
	step [198/325], loss=6.1619
	step [199/325], loss=4.0961
	step [200/325], loss=4.8169
	step [201/325], loss=5.8912
	step [202/325], loss=4.2356
	step [203/325], loss=4.0805
	step [204/325], loss=5.9864
	step [205/325], loss=4.3615
	step [206/325], loss=4.9729
	step [207/325], loss=7.6560
	step [208/325], loss=3.9811
	step [209/325], loss=3.1075
	step [210/325], loss=4.7056
	step [211/325], loss=5.2675
	step [212/325], loss=3.1639
	step [213/325], loss=3.2725
	step [214/325], loss=5.5019
	step [215/325], loss=3.4196
	step [216/325], loss=5.7385
	step [217/325], loss=4.4869
	step [218/325], loss=4.7733
	step [219/325], loss=4.1877
	step [220/325], loss=4.9389
	step [221/325], loss=4.7726
	step [222/325], loss=4.5613
	step [223/325], loss=3.8834
	step [224/325], loss=3.5777
	step [225/325], loss=5.0908
	step [226/325], loss=4.6460
	step [227/325], loss=4.6561
	step [228/325], loss=5.0025
	step [229/325], loss=6.1642
	step [230/325], loss=3.7408
	step [231/325], loss=5.2691
	step [232/325], loss=3.2941
	step [233/325], loss=4.6606
	step [234/325], loss=5.1079
	step [235/325], loss=3.7736
	step [236/325], loss=4.7743
	step [237/325], loss=4.0508
	step [238/325], loss=3.7480
	step [239/325], loss=4.9771
	step [240/325], loss=4.9659
	step [241/325], loss=4.3734
	step [242/325], loss=4.5847
	step [243/325], loss=3.9413
	step [244/325], loss=3.8948
	step [245/325], loss=4.7081
	step [246/325], loss=4.3225
	step [247/325], loss=5.4552
	step [248/325], loss=4.3638
	step [249/325], loss=5.1012
	step [250/325], loss=3.7420
	step [251/325], loss=4.8984
	step [252/325], loss=4.4123
	step [253/325], loss=4.1097
	step [254/325], loss=3.9177
	step [255/325], loss=4.2275
	step [256/325], loss=6.0639
	step [257/325], loss=4.6679
	step [258/325], loss=5.0836
	step [259/325], loss=3.8794
	step [260/325], loss=5.7151
	step [261/325], loss=5.9673
	step [262/325], loss=4.7038
	step [263/325], loss=5.1047
	step [264/325], loss=4.5136
	step [265/325], loss=3.7173
	step [266/325], loss=4.5379
	step [267/325], loss=6.3197
	step [268/325], loss=3.2914
	step [269/325], loss=6.7563
	step [270/325], loss=5.0974
	step [271/325], loss=3.3790
	step [272/325], loss=4.5772
	step [273/325], loss=4.1375
	step [274/325], loss=5.1933
	step [275/325], loss=4.4392
	step [276/325], loss=6.1486
	step [277/325], loss=3.9458
	step [278/325], loss=3.6787
	step [279/325], loss=4.8781
	step [280/325], loss=5.0521
	step [281/325], loss=4.5344
	step [282/325], loss=4.1251
	step [283/325], loss=6.0583
	step [284/325], loss=4.4727
	step [285/325], loss=5.5862
	step [286/325], loss=4.7173
	step [287/325], loss=5.0363
	step [288/325], loss=4.3387
	step [289/325], loss=3.9541
	step [290/325], loss=4.8685
	step [291/325], loss=4.2445
	step [292/325], loss=4.3483
	step [293/325], loss=4.0273
	step [294/325], loss=5.2476
	step [295/325], loss=4.7314
	step [296/325], loss=4.9952
	step [297/325], loss=3.7361
	step [298/325], loss=5.5395
	step [299/325], loss=4.7842
	step [300/325], loss=5.0347
	step [301/325], loss=4.2766
	step [302/325], loss=4.2039
	step [303/325], loss=5.1916
	step [304/325], loss=5.0507
	step [305/325], loss=5.0451
	step [306/325], loss=3.6773
	step [307/325], loss=4.5175
	step [308/325], loss=5.8211
	step [309/325], loss=4.5879
	step [310/325], loss=5.2363
	step [311/325], loss=4.1112
	step [312/325], loss=4.2120
	step [313/325], loss=4.6627
	step [314/325], loss=4.4404
	step [315/325], loss=4.3880
	step [316/325], loss=4.4960
	step [317/325], loss=4.6752
	step [318/325], loss=5.9172
	step [319/325], loss=4.7664
	step [320/325], loss=3.8015
	step [321/325], loss=5.3518
	step [322/325], loss=5.1416
	step [323/325], loss=4.9096
	step [324/325], loss=4.3101
	step [325/325], loss=0.2677
	Evaluating
	loss=0.0190, precision=0.2116, recall=0.9970, f1=0.3491
Training epoch 25
	step [1/325], loss=3.7684
	step [2/325], loss=4.2764
	step [3/325], loss=5.7201
	step [4/325], loss=5.1542
	step [5/325], loss=5.9683
	step [6/325], loss=5.2097
	step [7/325], loss=5.0254
	step [8/325], loss=5.1987
	step [9/325], loss=4.9165
	step [10/325], loss=6.0445
	step [11/325], loss=4.1875
	step [12/325], loss=3.9391
	step [13/325], loss=4.1878
	step [14/325], loss=4.3744
	step [15/325], loss=4.1537
	step [16/325], loss=6.5839
	step [17/325], loss=3.4079
	step [18/325], loss=3.7366
	step [19/325], loss=5.0456
	step [20/325], loss=4.0347
	step [21/325], loss=5.5133
	step [22/325], loss=3.4377
	step [23/325], loss=4.6105
	step [24/325], loss=4.7351
	step [25/325], loss=5.0221
	step [26/325], loss=5.7852
	step [27/325], loss=5.5172
	step [28/325], loss=5.6252
	step [29/325], loss=5.1703
	step [30/325], loss=4.8141
	step [31/325], loss=4.6637
	step [32/325], loss=5.2945
	step [33/325], loss=4.5553
	step [34/325], loss=4.7805
	step [35/325], loss=4.7225
	step [36/325], loss=4.4340
	step [37/325], loss=4.9270
	step [38/325], loss=4.3556
	step [39/325], loss=4.6874
	step [40/325], loss=4.5515
	step [41/325], loss=3.9663
	step [42/325], loss=4.9613
	step [43/325], loss=3.6819
	step [44/325], loss=3.9932
	step [45/325], loss=5.3402
	step [46/325], loss=4.3977
	step [47/325], loss=4.7956
	step [48/325], loss=4.1878
	step [49/325], loss=4.1100
	step [50/325], loss=4.7902
	step [51/325], loss=4.5552
	step [52/325], loss=6.2098
	step [53/325], loss=4.5130
	step [54/325], loss=4.9933
	step [55/325], loss=4.5150
	step [56/325], loss=5.4335
	step [57/325], loss=3.8919
	step [58/325], loss=6.2084
	step [59/325], loss=5.0384
	step [60/325], loss=4.4431
	step [61/325], loss=4.7916
	step [62/325], loss=3.5808
	step [63/325], loss=4.5309
	step [64/325], loss=4.6200
	step [65/325], loss=3.5216
	step [66/325], loss=3.8968
	step [67/325], loss=3.9878
	step [68/325], loss=4.5431
	step [69/325], loss=5.9524
	step [70/325], loss=3.8086
	step [71/325], loss=3.8490
	step [72/325], loss=3.5610
	step [73/325], loss=3.3359
	step [74/325], loss=4.7536
	step [75/325], loss=3.8943
	step [76/325], loss=4.7598
	step [77/325], loss=3.3870
	step [78/325], loss=4.1464
	step [79/325], loss=4.5158
	step [80/325], loss=5.4683
	step [81/325], loss=5.1686
	step [82/325], loss=4.4613
	step [83/325], loss=5.5133
	step [84/325], loss=5.6782
	step [85/325], loss=4.3880
	step [86/325], loss=4.5608
	step [87/325], loss=5.3872
	step [88/325], loss=4.9983
	step [89/325], loss=4.3683
	step [90/325], loss=4.6421
	step [91/325], loss=4.5057
	step [92/325], loss=5.4018
	step [93/325], loss=4.7331
	step [94/325], loss=4.3802
	step [95/325], loss=3.4653
	step [96/325], loss=4.4348
	step [97/325], loss=4.4261
	step [98/325], loss=4.1869
	step [99/325], loss=4.6267
	step [100/325], loss=4.2526
	step [101/325], loss=4.5299
	step [102/325], loss=3.6345
	step [103/325], loss=3.5780
	step [104/325], loss=3.6597
	step [105/325], loss=4.4714
	step [106/325], loss=4.4392
	step [107/325], loss=5.7401
	step [108/325], loss=4.4712
	step [109/325], loss=4.2605
	step [110/325], loss=4.4314
	step [111/325], loss=4.5886
	step [112/325], loss=5.2470
	step [113/325], loss=3.7754
	step [114/325], loss=3.8250
	step [115/325], loss=5.7647
	step [116/325], loss=4.8053
	step [117/325], loss=3.4508
	step [118/325], loss=4.8080
	step [119/325], loss=4.0418
	step [120/325], loss=4.4221
	step [121/325], loss=4.1653
	step [122/325], loss=4.7138
	step [123/325], loss=3.6516
	step [124/325], loss=3.3331
	step [125/325], loss=4.0413
	step [126/325], loss=4.2939
	step [127/325], loss=4.5400
	step [128/325], loss=4.9578
	step [129/325], loss=5.2498
	step [130/325], loss=4.8150
	step [131/325], loss=4.2011
	step [132/325], loss=4.4760
	step [133/325], loss=5.4698
	step [134/325], loss=3.3278
	step [135/325], loss=4.5752
	step [136/325], loss=4.6855
	step [137/325], loss=4.3716
	step [138/325], loss=5.0236
	step [139/325], loss=4.3074
	step [140/325], loss=4.4098
	step [141/325], loss=4.1899
	step [142/325], loss=3.3612
	step [143/325], loss=4.4958
	step [144/325], loss=4.3044
	step [145/325], loss=4.2291
	step [146/325], loss=4.6612
	step [147/325], loss=3.7350
	step [148/325], loss=5.7638
	step [149/325], loss=3.8442
	step [150/325], loss=4.8465
	step [151/325], loss=5.1924
	step [152/325], loss=3.7281
	step [153/325], loss=3.8307
	step [154/325], loss=5.3180
	step [155/325], loss=4.3736
	step [156/325], loss=4.4147
	step [157/325], loss=4.7271
	step [158/325], loss=4.4845
	step [159/325], loss=4.2864
	step [160/325], loss=4.2969
	step [161/325], loss=4.6293
	step [162/325], loss=3.9927
	step [163/325], loss=3.6308
	step [164/325], loss=4.3242
	step [165/325], loss=3.6364
	step [166/325], loss=3.6997
	step [167/325], loss=4.1312
	step [168/325], loss=4.5507
	step [169/325], loss=5.2411
	step [170/325], loss=3.8272
	step [171/325], loss=3.9524
	step [172/325], loss=5.7643
	step [173/325], loss=4.4230
	step [174/325], loss=4.7516
	step [175/325], loss=4.8842
	step [176/325], loss=5.8629
	step [177/325], loss=5.2203
	step [178/325], loss=4.5231
	step [179/325], loss=3.8043
	step [180/325], loss=5.4553
	step [181/325], loss=4.9031
	step [182/325], loss=4.8084
	step [183/325], loss=5.3077
	step [184/325], loss=3.9078
	step [185/325], loss=5.1433
	step [186/325], loss=3.9303
	step [187/325], loss=6.1287
	step [188/325], loss=5.0929
	step [189/325], loss=5.7169
	step [190/325], loss=3.4071
	step [191/325], loss=3.4828
	step [192/325], loss=4.3726
	step [193/325], loss=4.7750
	step [194/325], loss=4.3857
	step [195/325], loss=4.2014
	step [196/325], loss=3.4795
	step [197/325], loss=4.0095
	step [198/325], loss=3.3691
	step [199/325], loss=5.1141
	step [200/325], loss=5.1820
	step [201/325], loss=5.0179
	step [202/325], loss=3.8456
	step [203/325], loss=5.4363
	step [204/325], loss=6.3810
	step [205/325], loss=5.1176
	step [206/325], loss=5.3066
	step [207/325], loss=4.6819
	step [208/325], loss=5.0900
	step [209/325], loss=3.7814
	step [210/325], loss=4.0900
	step [211/325], loss=4.7937
	step [212/325], loss=4.4407
	step [213/325], loss=4.2917
	step [214/325], loss=4.0661
	step [215/325], loss=4.9754
	step [216/325], loss=4.5978
	step [217/325], loss=5.3653
	step [218/325], loss=4.5008
	step [219/325], loss=4.9595
	step [220/325], loss=4.4505
	step [221/325], loss=3.7890
	step [222/325], loss=5.2111
	step [223/325], loss=3.5901
	step [224/325], loss=4.5810
	step [225/325], loss=5.4794
	step [226/325], loss=4.8371
	step [227/325], loss=4.6231
	step [228/325], loss=4.2562
	step [229/325], loss=5.7916
	step [230/325], loss=4.7545
	step [231/325], loss=4.8812
	step [232/325], loss=4.8792
	step [233/325], loss=5.5516
	step [234/325], loss=3.8540
	step [235/325], loss=3.6048
	step [236/325], loss=4.3190
	step [237/325], loss=3.9330
	step [238/325], loss=3.9274
	step [239/325], loss=5.1980
	step [240/325], loss=4.8317
	step [241/325], loss=3.4790
	step [242/325], loss=4.2803
	step [243/325], loss=4.3021
	step [244/325], loss=3.9653
	step [245/325], loss=4.1259
	step [246/325], loss=4.4501
	step [247/325], loss=3.5194
	step [248/325], loss=4.7055
	step [249/325], loss=5.5629
	step [250/325], loss=3.2387
	step [251/325], loss=5.8865
	step [252/325], loss=5.2828
	step [253/325], loss=4.9869
	step [254/325], loss=4.7166
	step [255/325], loss=4.8756
	step [256/325], loss=4.3769
	step [257/325], loss=4.8224
	step [258/325], loss=5.3381
	step [259/325], loss=3.3374
	step [260/325], loss=4.7461
	step [261/325], loss=4.7302
	step [262/325], loss=3.9390
	step [263/325], loss=4.4833
	step [264/325], loss=3.9679
	step [265/325], loss=4.5623
	step [266/325], loss=3.9089
	step [267/325], loss=5.3301
	step [268/325], loss=5.3378
	step [269/325], loss=3.6593
	step [270/325], loss=4.5646
	step [271/325], loss=4.0360
	step [272/325], loss=3.8584
	step [273/325], loss=4.7291
	step [274/325], loss=4.3525
	step [275/325], loss=4.2296
	step [276/325], loss=4.3980
	step [277/325], loss=3.6018
	step [278/325], loss=5.2562
	step [279/325], loss=3.5750
	step [280/325], loss=5.9309
	step [281/325], loss=3.8061
	step [282/325], loss=4.5682
	step [283/325], loss=4.8647
	step [284/325], loss=4.8702
	step [285/325], loss=4.2321
	step [286/325], loss=4.6740
	step [287/325], loss=4.1564
	step [288/325], loss=4.6367
	step [289/325], loss=3.6270
	step [290/325], loss=5.1104
	step [291/325], loss=6.1103
	step [292/325], loss=4.3049
	step [293/325], loss=4.4711
	step [294/325], loss=4.3418
	step [295/325], loss=5.0604
	step [296/325], loss=4.2304
	step [297/325], loss=4.3848
	step [298/325], loss=4.1391
	step [299/325], loss=4.7327
	step [300/325], loss=4.7366
	step [301/325], loss=3.8385
	step [302/325], loss=4.5741
	step [303/325], loss=5.0985
	step [304/325], loss=4.0792
	step [305/325], loss=6.2277
	step [306/325], loss=3.6082
	step [307/325], loss=4.9760
	step [308/325], loss=3.5110
	step [309/325], loss=4.5713
	step [310/325], loss=4.3798
	step [311/325], loss=3.8206
	step [312/325], loss=5.3346
	step [313/325], loss=3.6579
	step [314/325], loss=4.3917
	step [315/325], loss=3.2288
	step [316/325], loss=4.9361
	step [317/325], loss=4.5081
	step [318/325], loss=4.2420
	step [319/325], loss=4.5251
	step [320/325], loss=4.5913
	step [321/325], loss=4.9608
	step [322/325], loss=3.8242
	step [323/325], loss=4.1855
	step [324/325], loss=4.3461
	step [325/325], loss=0.2179
	Evaluating
	loss=0.0166, precision=0.2404, recall=0.9964, f1=0.3874
saving model as: 2_saved_model.pth
Training epoch 26
	step [1/325], loss=3.8973
	step [2/325], loss=4.3777
	step [3/325], loss=3.6334
	step [4/325], loss=3.5278
	step [5/325], loss=4.4464
	step [6/325], loss=5.0936
	step [7/325], loss=4.1192
	step [8/325], loss=3.9416
	step [9/325], loss=4.1127
	step [10/325], loss=5.3929
	step [11/325], loss=5.0010
	step [12/325], loss=3.2532
	step [13/325], loss=4.2428
	step [14/325], loss=4.0873
	step [15/325], loss=5.6671
	step [16/325], loss=4.3652
	step [17/325], loss=3.7551
	step [18/325], loss=4.6665
	step [19/325], loss=3.9751
	step [20/325], loss=4.0462
	step [21/325], loss=3.9861
	step [22/325], loss=3.9839
	step [23/325], loss=4.5996
	step [24/325], loss=3.0092
	step [25/325], loss=4.4852
	step [26/325], loss=4.7579
	step [27/325], loss=3.5211
	step [28/325], loss=5.2761
	step [29/325], loss=5.4585
	step [30/325], loss=3.6060
	step [31/325], loss=3.2477
	step [32/325], loss=4.3958
	step [33/325], loss=4.7840
	step [34/325], loss=3.5608
	step [35/325], loss=4.0443
	step [36/325], loss=4.8732
	step [37/325], loss=3.9292
	step [38/325], loss=4.1264
	step [39/325], loss=4.6924
	step [40/325], loss=4.9647
	step [41/325], loss=3.5760
	step [42/325], loss=4.5955
	step [43/325], loss=5.0025
	step [44/325], loss=5.0978
	step [45/325], loss=4.4083
	step [46/325], loss=4.3447
	step [47/325], loss=3.9032
	step [48/325], loss=5.0225
	step [49/325], loss=4.4848
	step [50/325], loss=4.8168
	step [51/325], loss=4.4256
	step [52/325], loss=4.3318
	step [53/325], loss=3.2651
	step [54/325], loss=4.8687
	step [55/325], loss=4.5065
	step [56/325], loss=4.2961
	step [57/325], loss=5.2945
	step [58/325], loss=4.9256
	step [59/325], loss=4.6437
	step [60/325], loss=4.8715
	step [61/325], loss=4.2855
	step [62/325], loss=5.2348
	step [63/325], loss=4.8212
	step [64/325], loss=4.1062
	step [65/325], loss=5.3361
	step [66/325], loss=4.2745
	step [67/325], loss=4.1654
	step [68/325], loss=4.2098
	step [69/325], loss=3.9500
	step [70/325], loss=4.0397
	step [71/325], loss=4.5330
	step [72/325], loss=4.3988
	step [73/325], loss=4.1758
	step [74/325], loss=6.0933
	step [75/325], loss=5.0963
	step [76/325], loss=4.1950
	step [77/325], loss=5.1015
	step [78/325], loss=5.2245
	step [79/325], loss=4.3296
	step [80/325], loss=4.8483
	step [81/325], loss=5.5024
	step [82/325], loss=5.0976
	step [83/325], loss=3.4939
	step [84/325], loss=4.6133
	step [85/325], loss=4.2293
	step [86/325], loss=5.3606
	step [87/325], loss=4.7886
	step [88/325], loss=4.5325
	step [89/325], loss=5.0958
	step [90/325], loss=4.5170
	step [91/325], loss=4.7705
	step [92/325], loss=4.0667
	step [93/325], loss=4.2953
	step [94/325], loss=3.9126
	step [95/325], loss=4.5393
	step [96/325], loss=4.2230
	step [97/325], loss=4.3414
	step [98/325], loss=4.8025
	step [99/325], loss=4.2960
	step [100/325], loss=4.8604
	step [101/325], loss=5.7602
	step [102/325], loss=6.0887
	step [103/325], loss=4.1088
	step [104/325], loss=3.8234
	step [105/325], loss=4.5261
	step [106/325], loss=3.1797
	step [107/325], loss=4.5795
	step [108/325], loss=3.6209
	step [109/325], loss=3.9372
	step [110/325], loss=4.0449
	step [111/325], loss=4.0590
	step [112/325], loss=3.4888
	step [113/325], loss=3.6931
	step [114/325], loss=4.3908
	step [115/325], loss=3.5633
	step [116/325], loss=4.5611
	step [117/325], loss=3.9236
	step [118/325], loss=3.7454
	step [119/325], loss=4.6694
	step [120/325], loss=4.2853
	step [121/325], loss=4.2838
	step [122/325], loss=4.0251
	step [123/325], loss=3.9858
	step [124/325], loss=4.9391
	step [125/325], loss=5.3579
	step [126/325], loss=4.9091
	step [127/325], loss=4.4570
	step [128/325], loss=6.6605
	step [129/325], loss=4.7523
	step [130/325], loss=4.2933
	step [131/325], loss=4.4510
	step [132/325], loss=4.4303
	step [133/325], loss=4.6489
	step [134/325], loss=5.1941
	step [135/325], loss=4.0235
	step [136/325], loss=3.7803
	step [137/325], loss=4.1497
	step [138/325], loss=4.5453
	step [139/325], loss=4.5095
	step [140/325], loss=4.9602
	step [141/325], loss=4.6759
	step [142/325], loss=2.5523
	step [143/325], loss=5.0778
	step [144/325], loss=5.5890
	step [145/325], loss=5.0893
	step [146/325], loss=4.1638
	step [147/325], loss=7.6090
	step [148/325], loss=4.9459
	step [149/325], loss=5.7447
	step [150/325], loss=5.4261
	step [151/325], loss=4.4557
	step [152/325], loss=5.2826
	step [153/325], loss=3.9504
	step [154/325], loss=4.6078
	step [155/325], loss=4.8469
	step [156/325], loss=4.0844
	step [157/325], loss=4.5204
	step [158/325], loss=4.4091
	step [159/325], loss=4.4030
	step [160/325], loss=4.8269
	step [161/325], loss=6.0987
	step [162/325], loss=4.7589
	step [163/325], loss=4.1521
	step [164/325], loss=3.2893
	step [165/325], loss=4.8476
	step [166/325], loss=5.2638
	step [167/325], loss=4.6329
	step [168/325], loss=3.8193
	step [169/325], loss=4.8124
	step [170/325], loss=3.0080
	step [171/325], loss=3.7555
	step [172/325], loss=3.6503
	step [173/325], loss=4.0367
	step [174/325], loss=3.5232
	step [175/325], loss=4.0645
	step [176/325], loss=4.9405
	step [177/325], loss=5.5098
	step [178/325], loss=4.7212
	step [179/325], loss=4.6233
	step [180/325], loss=3.8599
	step [181/325], loss=4.1406
	step [182/325], loss=5.0765
	step [183/325], loss=5.0626
	step [184/325], loss=4.3449
	step [185/325], loss=4.4297
	step [186/325], loss=5.1087
	step [187/325], loss=4.3927
	step [188/325], loss=4.8284
	step [189/325], loss=3.8204
	step [190/325], loss=4.6149
	step [191/325], loss=4.7338
	step [192/325], loss=4.8480
	step [193/325], loss=4.8612
	step [194/325], loss=5.2124
	step [195/325], loss=4.8539
	step [196/325], loss=5.0455
	step [197/325], loss=4.7545
	step [198/325], loss=4.3840
	step [199/325], loss=4.1986
	step [200/325], loss=4.7318
	step [201/325], loss=2.9559
	step [202/325], loss=3.4578
	step [203/325], loss=4.5257
	step [204/325], loss=3.8760
	step [205/325], loss=4.6890
	step [206/325], loss=6.9468
	step [207/325], loss=5.4739
	step [208/325], loss=3.6014
	step [209/325], loss=4.3439
	step [210/325], loss=5.2412
	step [211/325], loss=4.3693
	step [212/325], loss=4.9569
	step [213/325], loss=4.3518
	step [214/325], loss=4.6243
	step [215/325], loss=4.2264
	step [216/325], loss=4.2075
	step [217/325], loss=5.7449
	step [218/325], loss=4.6602
	step [219/325], loss=3.4776
	step [220/325], loss=3.9701
	step [221/325], loss=4.5289
	step [222/325], loss=3.9282
	step [223/325], loss=4.1157
	step [224/325], loss=3.8400
	step [225/325], loss=4.0844
	step [226/325], loss=3.1068
	step [227/325], loss=3.9632
	step [228/325], loss=4.0638
	step [229/325], loss=5.7385
	step [230/325], loss=4.6807
	step [231/325], loss=4.5120
	step [232/325], loss=4.8950
	step [233/325], loss=3.7298
	step [234/325], loss=3.8636
	step [235/325], loss=4.3709
	step [236/325], loss=4.4957
	step [237/325], loss=3.8351
	step [238/325], loss=4.4790
	step [239/325], loss=5.4030
	step [240/325], loss=5.2467
	step [241/325], loss=4.6872
	step [242/325], loss=4.3195
	step [243/325], loss=4.0145
	step [244/325], loss=5.1368
	step [245/325], loss=4.5454
	step [246/325], loss=3.5260
	step [247/325], loss=4.6746
	step [248/325], loss=4.2605
	step [249/325], loss=4.6094
	step [250/325], loss=4.1047
	step [251/325], loss=3.8998
	step [252/325], loss=3.2189
	step [253/325], loss=4.2995
	step [254/325], loss=4.0050
	step [255/325], loss=4.3664
	step [256/325], loss=4.4243
	step [257/325], loss=3.7725
	step [258/325], loss=4.7596
	step [259/325], loss=3.6552
	step [260/325], loss=4.4798
	step [261/325], loss=3.9748
	step [262/325], loss=5.2189
	step [263/325], loss=4.7590
	step [264/325], loss=5.0774
	step [265/325], loss=4.7280
	step [266/325], loss=6.1036
	step [267/325], loss=5.1348
	step [268/325], loss=4.6881
	step [269/325], loss=3.5837
	step [270/325], loss=4.5944
	step [271/325], loss=4.5003
	step [272/325], loss=4.5309
	step [273/325], loss=3.8416
	step [274/325], loss=4.8301
	step [275/325], loss=4.9254
	step [276/325], loss=4.1091
	step [277/325], loss=4.4691
	step [278/325], loss=4.4701
	step [279/325], loss=3.9283
	step [280/325], loss=4.1212
	step [281/325], loss=4.4568
	step [282/325], loss=3.7828
	step [283/325], loss=5.0090
	step [284/325], loss=4.2082
	step [285/325], loss=4.4715
	step [286/325], loss=5.2610
	step [287/325], loss=3.8914
	step [288/325], loss=4.2419
	step [289/325], loss=3.5889
	step [290/325], loss=3.3139
	step [291/325], loss=4.3378
	step [292/325], loss=4.8029
	step [293/325], loss=4.4583
	step [294/325], loss=3.6363
	step [295/325], loss=4.9417
	step [296/325], loss=5.1045
	step [297/325], loss=4.5522
	step [298/325], loss=4.8160
	step [299/325], loss=4.3340
	step [300/325], loss=5.0120
	step [301/325], loss=5.2997
	step [302/325], loss=4.5573
	step [303/325], loss=4.5256
	step [304/325], loss=4.9944
	step [305/325], loss=4.7586
	step [306/325], loss=4.8751
	step [307/325], loss=3.7307
	step [308/325], loss=3.8127
	step [309/325], loss=4.5816
	step [310/325], loss=4.5913
	step [311/325], loss=3.7176
	step [312/325], loss=6.2288
	step [313/325], loss=4.9334
	step [314/325], loss=4.4262
	step [315/325], loss=4.6542
	step [316/325], loss=4.8488
	step [317/325], loss=4.7469
	step [318/325], loss=4.1207
	step [319/325], loss=4.2830
	step [320/325], loss=4.4791
	step [321/325], loss=3.7077
	step [322/325], loss=4.0231
	step [323/325], loss=4.6209
	step [324/325], loss=4.1120
	step [325/325], loss=0.0716
	Evaluating
	loss=0.0165, precision=0.2387, recall=0.9965, f1=0.3851
Training epoch 27
	step [1/325], loss=3.6837
	step [2/325], loss=4.4996
	step [3/325], loss=4.7064
	step [4/325], loss=3.9644
	step [5/325], loss=4.6212
	step [6/325], loss=4.1214
	step [7/325], loss=4.7112
	step [8/325], loss=4.1900
	step [9/325], loss=4.7571
	step [10/325], loss=3.8957
	step [11/325], loss=4.3984
	step [12/325], loss=4.7350
	step [13/325], loss=4.8484
	step [14/325], loss=3.7914
	step [15/325], loss=3.7366
	step [16/325], loss=5.0400
	step [17/325], loss=3.8466
	step [18/325], loss=4.5290
	step [19/325], loss=4.1800
	step [20/325], loss=4.8602
	step [21/325], loss=5.0803
	step [22/325], loss=4.1709
	step [23/325], loss=3.3584
	step [24/325], loss=4.0508
	step [25/325], loss=3.6615
	step [26/325], loss=4.3383
	step [27/325], loss=4.4466
	step [28/325], loss=4.5106
	step [29/325], loss=5.0379
	step [30/325], loss=5.2338
	step [31/325], loss=3.8563
	step [32/325], loss=5.1325
	step [33/325], loss=3.6177
	step [34/325], loss=4.2910
	step [35/325], loss=4.9214
	step [36/325], loss=5.1422
	step [37/325], loss=4.2262
	step [38/325], loss=5.6005
	step [39/325], loss=4.0048
	step [40/325], loss=4.6110
	step [41/325], loss=4.2483
	step [42/325], loss=4.6109
	step [43/325], loss=5.0002
	step [44/325], loss=4.8456
	step [45/325], loss=4.8594
	step [46/325], loss=4.2559
	step [47/325], loss=4.1513
	step [48/325], loss=3.7370
	step [49/325], loss=4.0983
	step [50/325], loss=4.7794
	step [51/325], loss=3.8013
	step [52/325], loss=5.1215
	step [53/325], loss=4.5720
	step [54/325], loss=4.2171
	step [55/325], loss=5.2692
	step [56/325], loss=4.5574
	step [57/325], loss=3.8437
	step [58/325], loss=4.4632
	step [59/325], loss=3.7226
	step [60/325], loss=3.3855
	step [61/325], loss=3.5221
	step [62/325], loss=5.0949
	step [63/325], loss=4.7801
	step [64/325], loss=3.8275
	step [65/325], loss=4.5329
	step [66/325], loss=4.2118
	step [67/325], loss=3.7522
	step [68/325], loss=4.8747
	step [69/325], loss=3.9919
	step [70/325], loss=3.9361
	step [71/325], loss=4.2317
	step [72/325], loss=4.5623
	step [73/325], loss=4.6091
	step [74/325], loss=4.8734
	step [75/325], loss=4.2603
	step [76/325], loss=3.8267
	step [77/325], loss=5.2488
	step [78/325], loss=4.1832
	step [79/325], loss=4.7986
	step [80/325], loss=3.9872
	step [81/325], loss=4.3329
	step [82/325], loss=3.8917
	step [83/325], loss=3.3627
	step [84/325], loss=4.0346
	step [85/325], loss=4.0781
	step [86/325], loss=4.3174
	step [87/325], loss=4.8253
	step [88/325], loss=4.3034
	step [89/325], loss=3.7440
	step [90/325], loss=4.0679
	step [91/325], loss=4.3734
	step [92/325], loss=4.0987
	step [93/325], loss=3.7127
	step [94/325], loss=3.9439
	step [95/325], loss=4.9105
	step [96/325], loss=3.6224
	step [97/325], loss=3.2574
	step [98/325], loss=4.4664
	step [99/325], loss=3.8407
	step [100/325], loss=4.0660
	step [101/325], loss=4.6319
	step [102/325], loss=3.2814
	step [103/325], loss=4.8234
	step [104/325], loss=5.9620
	step [105/325], loss=3.8313
	step [106/325], loss=5.0195
	step [107/325], loss=3.5722
	step [108/325], loss=3.8248
	step [109/325], loss=4.6917
	step [110/325], loss=4.7203
	step [111/325], loss=4.3818
	step [112/325], loss=4.7118
	step [113/325], loss=4.4842
	step [114/325], loss=4.1110
	step [115/325], loss=4.3162
	step [116/325], loss=3.8197
	step [117/325], loss=3.5511
	step [118/325], loss=3.7325
	step [119/325], loss=4.5469
	step [120/325], loss=5.6570
	step [121/325], loss=5.4492
	step [122/325], loss=4.4851
	step [123/325], loss=5.1228
	step [124/325], loss=4.7611
	step [125/325], loss=4.8101
	step [126/325], loss=4.5237
	step [127/325], loss=3.7881
	step [128/325], loss=4.5904
	step [129/325], loss=3.8162
	step [130/325], loss=3.9909
	step [131/325], loss=4.8149
	step [132/325], loss=4.1411
	step [133/325], loss=3.9893
	step [134/325], loss=4.1072
	step [135/325], loss=5.8260
	step [136/325], loss=5.6208
	step [137/325], loss=4.3976
	step [138/325], loss=4.3560
	step [139/325], loss=4.1720
	step [140/325], loss=3.8043
	step [141/325], loss=4.3707
	step [142/325], loss=4.7920
	step [143/325], loss=3.5749
	step [144/325], loss=4.4137
	step [145/325], loss=3.8671
	step [146/325], loss=3.5213
	step [147/325], loss=4.2865
	step [148/325], loss=4.0039
	step [149/325], loss=3.7215
	step [150/325], loss=4.4682
	step [151/325], loss=4.8970
	step [152/325], loss=3.6484
	step [153/325], loss=4.6401
	step [154/325], loss=5.2896
	step [155/325], loss=4.2202
	step [156/325], loss=3.4335
	step [157/325], loss=3.9520
	step [158/325], loss=4.2815
	step [159/325], loss=4.2324
	step [160/325], loss=4.4728
	step [161/325], loss=3.9988
	step [162/325], loss=4.1013
	step [163/325], loss=4.1091
	step [164/325], loss=4.1219
	step [165/325], loss=6.4718
	step [166/325], loss=4.3994
	step [167/325], loss=4.4783
	step [168/325], loss=4.0468
	step [169/325], loss=5.2231
	step [170/325], loss=4.1851
	step [171/325], loss=4.6782
	step [172/325], loss=5.1223
	step [173/325], loss=4.5676
	step [174/325], loss=3.6608
	step [175/325], loss=3.8944
	step [176/325], loss=3.8053
	step [177/325], loss=3.8869
	step [178/325], loss=3.6184
	step [179/325], loss=3.6014
	step [180/325], loss=5.3690
	step [181/325], loss=3.0455
	step [182/325], loss=4.7624
	step [183/325], loss=5.9397
	step [184/325], loss=3.6573
	step [185/325], loss=3.9512
	step [186/325], loss=5.6135
	step [187/325], loss=5.0722
	step [188/325], loss=4.6246
	step [189/325], loss=3.5253
	step [190/325], loss=3.9448
	step [191/325], loss=3.6147
	step [192/325], loss=5.4296
	step [193/325], loss=4.3068
	step [194/325], loss=4.5570
	step [195/325], loss=4.1229
	step [196/325], loss=4.8605
	step [197/325], loss=3.6403
	step [198/325], loss=5.0131
	step [199/325], loss=5.2169
	step [200/325], loss=4.6435
	step [201/325], loss=4.0705
	step [202/325], loss=4.9234
	step [203/325], loss=3.5541
	step [204/325], loss=4.0084
	step [205/325], loss=3.5211
	step [206/325], loss=4.5424
	step [207/325], loss=4.4802
	step [208/325], loss=4.7810
	step [209/325], loss=4.4868
	step [210/325], loss=4.1655
	step [211/325], loss=4.5176
	step [212/325], loss=3.6663
	step [213/325], loss=3.6944
	step [214/325], loss=4.6522
	step [215/325], loss=4.4052
	step [216/325], loss=4.1274
	step [217/325], loss=3.6263
	step [218/325], loss=3.4936
	step [219/325], loss=5.4269
	step [220/325], loss=4.3718
	step [221/325], loss=3.8103
	step [222/325], loss=5.2544
	step [223/325], loss=4.3186
	step [224/325], loss=4.6441
	step [225/325], loss=5.0021
	step [226/325], loss=4.2431
	step [227/325], loss=4.4268
	step [228/325], loss=3.4399
	step [229/325], loss=4.0300
	step [230/325], loss=3.8577
	step [231/325], loss=4.3778
	step [232/325], loss=3.7944
	step [233/325], loss=4.5385
	step [234/325], loss=4.8667
	step [235/325], loss=4.1471
	step [236/325], loss=4.0950
	step [237/325], loss=4.8391
	step [238/325], loss=3.5531
	step [239/325], loss=4.5167
	step [240/325], loss=3.6041
	step [241/325], loss=4.7146
	step [242/325], loss=5.7780
	step [243/325], loss=3.9690
	step [244/325], loss=3.6885
	step [245/325], loss=4.7046
	step [246/325], loss=3.7763
	step [247/325], loss=4.4284
	step [248/325], loss=5.2159
	step [249/325], loss=5.1002
	step [250/325], loss=4.1321
	step [251/325], loss=5.5996
	step [252/325], loss=3.7537
	step [253/325], loss=4.8692
	step [254/325], loss=5.2915
	step [255/325], loss=4.1032
	step [256/325], loss=4.0847
	step [257/325], loss=5.2369
	step [258/325], loss=3.2403
	step [259/325], loss=4.0422
	step [260/325], loss=3.9962
	step [261/325], loss=3.7794
	step [262/325], loss=4.4413
	step [263/325], loss=3.6266
	step [264/325], loss=4.8811
	step [265/325], loss=4.9081
	step [266/325], loss=4.2755
	step [267/325], loss=4.8823
	step [268/325], loss=4.3548
	step [269/325], loss=4.3694
	step [270/325], loss=4.6960
	step [271/325], loss=3.9788
	step [272/325], loss=4.7592
	step [273/325], loss=3.7930
	step [274/325], loss=4.4251
	step [275/325], loss=4.9292
	step [276/325], loss=5.6200
	step [277/325], loss=4.9604
	step [278/325], loss=3.9892
	step [279/325], loss=3.8478
	step [280/325], loss=4.2926
	step [281/325], loss=4.8151
	step [282/325], loss=3.9453
	step [283/325], loss=4.2529
	step [284/325], loss=3.9881
	step [285/325], loss=4.5235
	step [286/325], loss=4.1234
	step [287/325], loss=4.7844
	step [288/325], loss=4.0159
	step [289/325], loss=3.3254
	step [290/325], loss=4.2835
	step [291/325], loss=4.5300
	step [292/325], loss=4.1581
	step [293/325], loss=4.0196
	step [294/325], loss=4.5466
	step [295/325], loss=3.3127
	step [296/325], loss=4.4637
	step [297/325], loss=2.8767
	step [298/325], loss=4.4398
	step [299/325], loss=4.4693
	step [300/325], loss=4.0206
	step [301/325], loss=3.8224
	step [302/325], loss=4.5603
	step [303/325], loss=4.2451
	step [304/325], loss=4.9768
	step [305/325], loss=4.2158
	step [306/325], loss=3.8924
	step [307/325], loss=3.6211
	step [308/325], loss=4.6033
	step [309/325], loss=5.1574
	step [310/325], loss=4.3442
	step [311/325], loss=4.8402
	step [312/325], loss=3.8127
	step [313/325], loss=4.9233
	step [314/325], loss=4.2748
	step [315/325], loss=5.3488
	step [316/325], loss=3.8126
	step [317/325], loss=4.5721
	step [318/325], loss=6.6109
	step [319/325], loss=5.2944
	step [320/325], loss=3.4088
	step [321/325], loss=4.5346
	step [322/325], loss=3.9204
	step [323/325], loss=4.4708
	step [324/325], loss=4.2808
	step [325/325], loss=0.2021
	Evaluating
	loss=0.0163, precision=0.2330, recall=0.9965, f1=0.3777
Training epoch 28
	step [1/325], loss=3.2083
	step [2/325], loss=4.1149
	step [3/325], loss=3.5946
	step [4/325], loss=4.2313
	step [5/325], loss=3.7409
	step [6/325], loss=3.9995
	step [7/325], loss=5.0415
	step [8/325], loss=4.4263
	step [9/325], loss=3.8829
	step [10/325], loss=4.2916
	step [11/325], loss=4.5403
	step [12/325], loss=4.4224
	step [13/325], loss=3.6458
	step [14/325], loss=4.1566
	step [15/325], loss=4.8045
	step [16/325], loss=4.2042
	step [17/325], loss=4.3312
	step [18/325], loss=4.2390
	step [19/325], loss=4.2412
	step [20/325], loss=4.2748
	step [21/325], loss=3.8435
	step [22/325], loss=4.4633
	step [23/325], loss=4.0184
	step [24/325], loss=3.0886
	step [25/325], loss=3.4799
	step [26/325], loss=4.2671
	step [27/325], loss=3.8668
	step [28/325], loss=3.6273
	step [29/325], loss=4.3508
	step [30/325], loss=3.7464
	step [31/325], loss=3.0411
	step [32/325], loss=3.7633
	step [33/325], loss=4.5249
	step [34/325], loss=4.6027
	step [35/325], loss=3.7392
	step [36/325], loss=4.0453
	step [37/325], loss=3.4104
	step [38/325], loss=3.5129
	step [39/325], loss=5.6764
	step [40/325], loss=3.7819
	step [41/325], loss=3.3405
	step [42/325], loss=3.8242
	step [43/325], loss=3.8179
	step [44/325], loss=3.7214
	step [45/325], loss=4.1133
	step [46/325], loss=3.4942
	step [47/325], loss=4.6329
	step [48/325], loss=4.1559
	step [49/325], loss=3.7070
	step [50/325], loss=4.1338
	step [51/325], loss=4.6865
	step [52/325], loss=3.6127
	step [53/325], loss=4.2305
	step [54/325], loss=4.7542
	step [55/325], loss=4.1515
	step [56/325], loss=3.6294
	step [57/325], loss=3.6299
	step [58/325], loss=3.6221
	step [59/325], loss=4.6441
	step [60/325], loss=3.9779
	step [61/325], loss=4.1410
	step [62/325], loss=4.5751
	step [63/325], loss=4.7284
	step [64/325], loss=3.7217
	step [65/325], loss=5.2669
	step [66/325], loss=4.5853
	step [67/325], loss=4.8183
	step [68/325], loss=3.3397
	step [69/325], loss=4.0800
	step [70/325], loss=3.4640
	step [71/325], loss=5.4641
	step [72/325], loss=3.5556
	step [73/325], loss=3.7841
	step [74/325], loss=4.4060
	step [75/325], loss=4.1465
	step [76/325], loss=5.5155
	step [77/325], loss=4.5151
	step [78/325], loss=4.1624
	step [79/325], loss=4.7472
	step [80/325], loss=4.0725
	step [81/325], loss=4.5062
	step [82/325], loss=4.6024
	step [83/325], loss=4.0489
	step [84/325], loss=4.2905
	step [85/325], loss=3.2873
	step [86/325], loss=3.6669
	step [87/325], loss=4.3543
	step [88/325], loss=4.3622
	step [89/325], loss=3.7615
	step [90/325], loss=3.0130
	step [91/325], loss=3.9665
	step [92/325], loss=4.1984
	step [93/325], loss=4.6874
	step [94/325], loss=3.5751
	step [95/325], loss=3.9327
	step [96/325], loss=3.7332
	step [97/325], loss=4.4182
	step [98/325], loss=5.0546
	step [99/325], loss=3.9076
	step [100/325], loss=4.5213
	step [101/325], loss=4.6580
	step [102/325], loss=4.0469
	step [103/325], loss=3.5415
	step [104/325], loss=4.4423
	step [105/325], loss=5.0194
	step [106/325], loss=3.1923
	step [107/325], loss=3.7594
	step [108/325], loss=3.6566
	step [109/325], loss=4.6556
	step [110/325], loss=3.8164
	step [111/325], loss=3.5997
	step [112/325], loss=4.2703
	step [113/325], loss=5.0722
	step [114/325], loss=3.7313
	step [115/325], loss=3.7817
	step [116/325], loss=5.0566
	step [117/325], loss=4.2020
	step [118/325], loss=6.6746
	step [119/325], loss=4.7969
	step [120/325], loss=4.2191
	step [121/325], loss=4.2295
	step [122/325], loss=4.4142
	step [123/325], loss=4.0349
	step [124/325], loss=4.1639
	step [125/325], loss=3.9335
	step [126/325], loss=4.7339
	step [127/325], loss=3.4071
	step [128/325], loss=5.3132
	step [129/325], loss=4.1781
	step [130/325], loss=4.6021
	step [131/325], loss=4.6083
	step [132/325], loss=4.0336
	step [133/325], loss=4.6898
	step [134/325], loss=4.1737
	step [135/325], loss=3.9991
	step [136/325], loss=4.2971
	step [137/325], loss=4.7076
	step [138/325], loss=5.2341
	step [139/325], loss=4.3333
	step [140/325], loss=4.4332
	step [141/325], loss=5.2789
	step [142/325], loss=4.4856
	step [143/325], loss=4.4811
	step [144/325], loss=4.9112
	step [145/325], loss=4.8449
	step [146/325], loss=3.7318
	step [147/325], loss=3.4617
	step [148/325], loss=4.0633
	step [149/325], loss=4.2621
	step [150/325], loss=4.2104
	step [151/325], loss=5.5864
	step [152/325], loss=3.1334
	step [153/325], loss=5.1488
	step [154/325], loss=4.5837
	step [155/325], loss=4.9333
	step [156/325], loss=4.0345
	step [157/325], loss=4.5450
	step [158/325], loss=3.8396
	step [159/325], loss=3.9354
	step [160/325], loss=5.0056
	step [161/325], loss=4.0394
	step [162/325], loss=4.2504
	step [163/325], loss=5.5328
	step [164/325], loss=4.5905
	step [165/325], loss=4.6524
	step [166/325], loss=2.8162
	step [167/325], loss=4.1712
	step [168/325], loss=4.0448
	step [169/325], loss=4.7483
	step [170/325], loss=3.7742
	step [171/325], loss=3.4368
	step [172/325], loss=5.0006
	step [173/325], loss=3.7555
	step [174/325], loss=4.9447
	step [175/325], loss=5.3454
	step [176/325], loss=4.8016
	step [177/325], loss=3.5530
	step [178/325], loss=4.4591
	step [179/325], loss=4.0954
	step [180/325], loss=5.6167
	step [181/325], loss=4.3240
	step [182/325], loss=4.2029
	step [183/325], loss=4.5985
	step [184/325], loss=4.5181
	step [185/325], loss=4.0111
	step [186/325], loss=4.4544
	step [187/325], loss=4.6070
	step [188/325], loss=4.8662
	step [189/325], loss=5.2201
	step [190/325], loss=3.8450
	step [191/325], loss=5.0422
	step [192/325], loss=3.6074
	step [193/325], loss=4.6434
	step [194/325], loss=3.6413
	step [195/325], loss=4.1286
	step [196/325], loss=4.2129
	step [197/325], loss=4.2228
	step [198/325], loss=4.1489
	step [199/325], loss=3.7740
	step [200/325], loss=4.2002
	step [201/325], loss=4.8394
	step [202/325], loss=4.7318
	step [203/325], loss=4.9988
	step [204/325], loss=3.2641
	step [205/325], loss=3.8856
	step [206/325], loss=5.5084
	step [207/325], loss=4.4375
	step [208/325], loss=4.0911
	step [209/325], loss=5.3589
	step [210/325], loss=4.1725
	step [211/325], loss=4.2896
	step [212/325], loss=4.7253
	step [213/325], loss=3.8666
	step [214/325], loss=3.7395
	step [215/325], loss=4.7010
	step [216/325], loss=3.6917
	step [217/325], loss=4.5971
	step [218/325], loss=6.1624
	step [219/325], loss=4.2671
	step [220/325], loss=4.8145
	step [221/325], loss=3.8822
	step [222/325], loss=4.0583
	step [223/325], loss=4.4357
	step [224/325], loss=4.6385
	step [225/325], loss=4.4566
	step [226/325], loss=4.6004
	step [227/325], loss=4.1149
	step [228/325], loss=4.2319
	step [229/325], loss=4.2479
	step [230/325], loss=3.4470
	step [231/325], loss=3.5309
	step [232/325], loss=3.5982
	step [233/325], loss=4.1770
	step [234/325], loss=5.3208
	step [235/325], loss=3.7955
	step [236/325], loss=3.7347
	step [237/325], loss=5.1266
	step [238/325], loss=3.8905
	step [239/325], loss=3.5674
	step [240/325], loss=3.8411
	step [241/325], loss=4.8285
	step [242/325], loss=5.1814
	step [243/325], loss=3.7674
	step [244/325], loss=5.5865
	step [245/325], loss=4.2151
	step [246/325], loss=3.7920
	step [247/325], loss=3.5501
	step [248/325], loss=4.8040
	step [249/325], loss=4.2794
	step [250/325], loss=4.0025
	step [251/325], loss=4.5077
	step [252/325], loss=3.8694
	step [253/325], loss=4.7193
	step [254/325], loss=4.2005
	step [255/325], loss=4.5097
	step [256/325], loss=4.4820
	step [257/325], loss=5.0906
	step [258/325], loss=3.4334
	step [259/325], loss=3.5619
	step [260/325], loss=5.2109
	step [261/325], loss=4.6744
	step [262/325], loss=3.4327
	step [263/325], loss=3.4621
	step [264/325], loss=3.5939
	step [265/325], loss=3.5339
	step [266/325], loss=4.2222
	step [267/325], loss=4.1995
	step [268/325], loss=4.7485
	step [269/325], loss=4.5084
	step [270/325], loss=4.0973
	step [271/325], loss=4.4639
	step [272/325], loss=4.6255
	step [273/325], loss=4.2377
	step [274/325], loss=4.3862
	step [275/325], loss=5.1622
	step [276/325], loss=4.1671
	step [277/325], loss=3.8927
	step [278/325], loss=3.0738
	step [279/325], loss=3.9426
	step [280/325], loss=4.3741
	step [281/325], loss=3.1306
	step [282/325], loss=6.1178
	step [283/325], loss=4.2236
	step [284/325], loss=4.1682
	step [285/325], loss=4.5608
	step [286/325], loss=4.4507
	step [287/325], loss=5.1589
	step [288/325], loss=4.5751
	step [289/325], loss=4.6126
	step [290/325], loss=4.4187
	step [291/325], loss=4.7515
	step [292/325], loss=5.0764
	step [293/325], loss=4.2940
	step [294/325], loss=4.0544
	step [295/325], loss=3.9765
	step [296/325], loss=4.8345
	step [297/325], loss=4.9819
	step [298/325], loss=4.8514
	step [299/325], loss=3.9924
	step [300/325], loss=5.2392
	step [301/325], loss=3.4458
	step [302/325], loss=4.0724
	step [303/325], loss=4.6119
	step [304/325], loss=4.2870
	step [305/325], loss=5.2418
	step [306/325], loss=4.3096
	step [307/325], loss=4.5514
	step [308/325], loss=6.1412
	step [309/325], loss=4.2857
	step [310/325], loss=3.8553
	step [311/325], loss=4.3054
	step [312/325], loss=4.0916
	step [313/325], loss=4.6272
	step [314/325], loss=4.2809
	step [315/325], loss=4.7772
	step [316/325], loss=4.5015
	step [317/325], loss=5.0037
	step [318/325], loss=3.4881
	step [319/325], loss=3.9348
	step [320/325], loss=4.0746
	step [321/325], loss=4.2825
	step [322/325], loss=4.8763
	step [323/325], loss=4.9558
	step [324/325], loss=4.1501
	step [325/325], loss=0.2534
	Evaluating
	loss=0.0171, precision=0.2216, recall=0.9967, f1=0.3626
Training epoch 29
	step [1/325], loss=4.3653
	step [2/325], loss=4.0631
	step [3/325], loss=4.2521
	step [4/325], loss=3.7868
	step [5/325], loss=4.5762
	step [6/325], loss=3.9801
	step [7/325], loss=4.1416
	step [8/325], loss=5.1793
	step [9/325], loss=4.6993
	step [10/325], loss=4.2216
	step [11/325], loss=5.0426
	step [12/325], loss=4.0636
	step [13/325], loss=3.8627
	step [14/325], loss=4.0558
	step [15/325], loss=4.6489
	step [16/325], loss=4.1131
	step [17/325], loss=4.3193
	step [18/325], loss=4.6760
	step [19/325], loss=4.6428
	step [20/325], loss=4.7717
	step [21/325], loss=3.9991
	step [22/325], loss=4.2995
	step [23/325], loss=4.1226
	step [24/325], loss=5.2087
	step [25/325], loss=4.6018
	step [26/325], loss=4.0761
	step [27/325], loss=3.4566
	step [28/325], loss=3.9260
	step [29/325], loss=3.7738
	step [30/325], loss=3.6235
	step [31/325], loss=4.0608
	step [32/325], loss=4.9717
	step [33/325], loss=4.8338
	step [34/325], loss=4.2511
	step [35/325], loss=4.0952
	step [36/325], loss=3.7719
	step [37/325], loss=3.7295
	step [38/325], loss=4.2156
	step [39/325], loss=4.3487
	step [40/325], loss=4.9120
	step [41/325], loss=4.0998
	step [42/325], loss=5.0855
	step [43/325], loss=4.8999
	step [44/325], loss=3.6535
	step [45/325], loss=3.8859
	step [46/325], loss=4.0242
	step [47/325], loss=4.9171
	step [48/325], loss=4.5062
	step [49/325], loss=3.6630
	step [50/325], loss=4.2825
	step [51/325], loss=4.0897
	step [52/325], loss=3.7572
	step [53/325], loss=4.7039
	step [54/325], loss=4.5039
	step [55/325], loss=4.4087
	step [56/325], loss=3.9463
	step [57/325], loss=4.0511
	step [58/325], loss=4.3917
	step [59/325], loss=3.8415
	step [60/325], loss=3.7368
	step [61/325], loss=4.6035
	step [62/325], loss=3.7577
	step [63/325], loss=5.0914
	step [64/325], loss=3.8505
	step [65/325], loss=3.8078
	step [66/325], loss=3.8178
	step [67/325], loss=4.3859
	step [68/325], loss=4.0699
	step [69/325], loss=4.1517
	step [70/325], loss=4.4285
	step [71/325], loss=3.5448
	step [72/325], loss=4.5703
	step [73/325], loss=4.3460
	step [74/325], loss=4.1588
	step [75/325], loss=3.5254
	step [76/325], loss=3.7904
	step [77/325], loss=4.1728
	step [78/325], loss=4.2482
	step [79/325], loss=3.7979
	step [80/325], loss=2.8479
	step [81/325], loss=3.5998
	step [82/325], loss=4.5851
	step [83/325], loss=3.4145
	step [84/325], loss=4.2106
	step [85/325], loss=4.7109
	step [86/325], loss=3.9978
	step [87/325], loss=3.7330
	step [88/325], loss=4.6910
	step [89/325], loss=3.7991
	step [90/325], loss=4.1497
	step [91/325], loss=3.9485
	step [92/325], loss=4.7420
	step [93/325], loss=4.4192
	step [94/325], loss=4.1026
	step [95/325], loss=3.8271
	step [96/325], loss=4.0025
	step [97/325], loss=4.4344
	step [98/325], loss=5.0209
	step [99/325], loss=4.6777
	step [100/325], loss=3.6332
	step [101/325], loss=4.7144
	step [102/325], loss=3.6302
	step [103/325], loss=3.9239
	step [104/325], loss=3.4242
	step [105/325], loss=4.0342
	step [106/325], loss=3.9029
	step [107/325], loss=2.9194
	step [108/325], loss=3.1104
	step [109/325], loss=4.4471
	step [110/325], loss=4.9228
	step [111/325], loss=4.3243
	step [112/325], loss=3.7570
	step [113/325], loss=4.2083
	step [114/325], loss=3.5481
	step [115/325], loss=3.6220
	step [116/325], loss=4.6247
	step [117/325], loss=3.3082
	step [118/325], loss=3.9065
	step [119/325], loss=5.2703
	step [120/325], loss=5.0073
	step [121/325], loss=3.2889
	step [122/325], loss=5.1935
	step [123/325], loss=3.3972
	step [124/325], loss=5.0614
	step [125/325], loss=3.7457
	step [126/325], loss=3.9471
	step [127/325], loss=5.1394
	step [128/325], loss=3.6203
	step [129/325], loss=3.7033
	step [130/325], loss=5.1857
	step [131/325], loss=4.8861
	step [132/325], loss=4.1548
	step [133/325], loss=3.5833
	step [134/325], loss=4.2390
	step [135/325], loss=4.2772
	step [136/325], loss=4.2652
	step [137/325], loss=4.1207
	step [138/325], loss=3.9261
	step [139/325], loss=4.5685
	step [140/325], loss=4.0023
	step [141/325], loss=4.3498
	step [142/325], loss=4.1667
	step [143/325], loss=4.6446
	step [144/325], loss=3.7877
	step [145/325], loss=3.2430
	step [146/325], loss=3.7384
	step [147/325], loss=3.9162
	step [148/325], loss=3.7057
	step [149/325], loss=4.2560
	step [150/325], loss=3.7526
	step [151/325], loss=4.3111
	step [152/325], loss=4.3011
	step [153/325], loss=3.8253
	step [154/325], loss=3.5870
	step [155/325], loss=3.4901
	step [156/325], loss=5.2119
	step [157/325], loss=4.2342
	step [158/325], loss=3.7370
	step [159/325], loss=3.3261
	step [160/325], loss=3.5382
	step [161/325], loss=3.9332
	step [162/325], loss=3.9463
	step [163/325], loss=4.2383
	step [164/325], loss=4.1962
	step [165/325], loss=5.5595
	step [166/325], loss=4.6006
	step [167/325], loss=3.9346
	step [168/325], loss=4.1103
	step [169/325], loss=3.4585
	step [170/325], loss=4.4091
	step [171/325], loss=4.8112
	step [172/325], loss=4.3328
	step [173/325], loss=4.1821
	step [174/325], loss=4.4130
	step [175/325], loss=4.2755
	step [176/325], loss=4.6785
	step [177/325], loss=3.7418
	step [178/325], loss=4.6210
	step [179/325], loss=4.3596
	step [180/325], loss=3.9686
	step [181/325], loss=5.1442
	step [182/325], loss=4.5864
	step [183/325], loss=3.9694
	step [184/325], loss=3.8143
	step [185/325], loss=4.8050
	step [186/325], loss=4.6841
	step [187/325], loss=4.2857
	step [188/325], loss=4.4081
	step [189/325], loss=4.9964
	step [190/325], loss=3.9301
	step [191/325], loss=4.1264
	step [192/325], loss=3.8012
	step [193/325], loss=4.4432
	step [194/325], loss=5.3621
	step [195/325], loss=3.7011
	step [196/325], loss=3.7625
	step [197/325], loss=3.9363
	step [198/325], loss=4.1972
	step [199/325], loss=3.3058
	step [200/325], loss=3.6723
	step [201/325], loss=3.5314
	step [202/325], loss=3.7308
	step [203/325], loss=3.6175
	step [204/325], loss=3.9683
	step [205/325], loss=4.6974
	step [206/325], loss=4.8290
	step [207/325], loss=5.4131
	step [208/325], loss=4.6510
	step [209/325], loss=3.8271
	step [210/325], loss=4.0826
	step [211/325], loss=3.9637
	step [212/325], loss=5.2868
	step [213/325], loss=4.8157
	step [214/325], loss=3.2853
	step [215/325], loss=4.9113
	step [216/325], loss=5.3085
	step [217/325], loss=3.0856
	step [218/325], loss=4.2000
	step [219/325], loss=3.4783
	step [220/325], loss=4.0951
	step [221/325], loss=4.1586
	step [222/325], loss=4.6919
	step [223/325], loss=4.9341
	step [224/325], loss=3.6291
	step [225/325], loss=4.1661
	step [226/325], loss=3.4556
	step [227/325], loss=4.3899
	step [228/325], loss=3.9959
	step [229/325], loss=3.2553
	step [230/325], loss=4.0327
	step [231/325], loss=4.7670
	step [232/325], loss=4.6228
	step [233/325], loss=5.1979
	step [234/325], loss=4.0883
	step [235/325], loss=3.8878
	step [236/325], loss=4.8166
	step [237/325], loss=3.9488
	step [238/325], loss=3.6692
	step [239/325], loss=4.4247
	step [240/325], loss=3.8118
	step [241/325], loss=4.7049
	step [242/325], loss=4.5406
	step [243/325], loss=4.1844
	step [244/325], loss=4.6750
	step [245/325], loss=4.7595
	step [246/325], loss=4.2645
	step [247/325], loss=3.6426
	step [248/325], loss=5.2258
	step [249/325], loss=4.8864
	step [250/325], loss=4.4244
	step [251/325], loss=4.3162
	step [252/325], loss=3.8492
	step [253/325], loss=5.1731
	step [254/325], loss=3.8305
	step [255/325], loss=4.7183
	step [256/325], loss=4.2814
	step [257/325], loss=3.5164
	step [258/325], loss=3.5079
	step [259/325], loss=5.2157
	step [260/325], loss=4.1490
	step [261/325], loss=4.0146
	step [262/325], loss=3.8050
	step [263/325], loss=3.2398
	step [264/325], loss=4.5316
	step [265/325], loss=3.4671
	step [266/325], loss=4.0427
	step [267/325], loss=4.0542
	step [268/325], loss=4.4434
	step [269/325], loss=3.8169
	step [270/325], loss=4.4882
	step [271/325], loss=3.6000
	step [272/325], loss=4.2640
	step [273/325], loss=4.1551
	step [274/325], loss=4.0328
	step [275/325], loss=3.5324
	step [276/325], loss=3.3174
	step [277/325], loss=3.8895
	step [278/325], loss=3.3244
	step [279/325], loss=4.6298
	step [280/325], loss=3.7494
	step [281/325], loss=4.7028
	step [282/325], loss=3.8410
	step [283/325], loss=4.7388
	step [284/325], loss=3.3501
	step [285/325], loss=4.3063
	step [286/325], loss=4.0698
	step [287/325], loss=3.9184
	step [288/325], loss=3.9030
	step [289/325], loss=4.5632
	step [290/325], loss=4.1061
	step [291/325], loss=3.7039
	step [292/325], loss=4.5606
	step [293/325], loss=5.0651
	step [294/325], loss=3.5181
	step [295/325], loss=3.9635
	step [296/325], loss=4.2147
	step [297/325], loss=3.9965
	step [298/325], loss=3.4164
	step [299/325], loss=4.5454
	step [300/325], loss=3.6869
	step [301/325], loss=3.2010
	step [302/325], loss=4.7881
	step [303/325], loss=4.6976
	step [304/325], loss=4.1541
	step [305/325], loss=4.3212
	step [306/325], loss=3.8536
	step [307/325], loss=4.8472
	step [308/325], loss=4.1807
	step [309/325], loss=4.4884
	step [310/325], loss=4.1091
	step [311/325], loss=3.6103
	step [312/325], loss=4.4709
	step [313/325], loss=4.0049
	step [314/325], loss=3.9541
	step [315/325], loss=4.1993
	step [316/325], loss=3.6798
	step [317/325], loss=4.9342
	step [318/325], loss=4.1409
	step [319/325], loss=4.8191
	step [320/325], loss=4.6394
	step [321/325], loss=4.2304
	step [322/325], loss=3.9185
	step [323/325], loss=4.1204
	step [324/325], loss=4.2573
	step [325/325], loss=0.0647
	Evaluating
	loss=0.0167, precision=0.2384, recall=0.9966, f1=0.3848
Training epoch 30
	step [1/325], loss=3.7586
	step [2/325], loss=4.8900
	step [3/325], loss=4.2670
	step [4/325], loss=3.1064
	step [5/325], loss=3.3843
	step [6/325], loss=4.0697
	step [7/325], loss=5.1641
	step [8/325], loss=3.3042
	step [9/325], loss=4.5005
	step [10/325], loss=4.2456
	step [11/325], loss=3.9300
	step [12/325], loss=2.8091
	step [13/325], loss=3.9846
	step [14/325], loss=3.9835
	step [15/325], loss=4.0193
	step [16/325], loss=3.0630
	step [17/325], loss=4.1174
	step [18/325], loss=3.8300
	step [19/325], loss=5.0050
	step [20/325], loss=3.4508
	step [21/325], loss=4.0867
	step [22/325], loss=4.6541
	step [23/325], loss=3.5609
	step [24/325], loss=5.0830
	step [25/325], loss=3.6251
	step [26/325], loss=4.7091
	step [27/325], loss=4.1609
	step [28/325], loss=3.8633
	step [29/325], loss=4.2042
	step [30/325], loss=4.4008
	step [31/325], loss=4.1602
	step [32/325], loss=3.6415
	step [33/325], loss=4.3572
	step [34/325], loss=3.9020
	step [35/325], loss=4.9736
	step [36/325], loss=4.6523
	step [37/325], loss=4.0177
	step [38/325], loss=4.8299
	step [39/325], loss=3.4852
	step [40/325], loss=4.8719
	step [41/325], loss=4.1254
	step [42/325], loss=3.7844
	step [43/325], loss=3.4157
	step [44/325], loss=3.6583
	step [45/325], loss=2.8171
	step [46/325], loss=3.4899
	step [47/325], loss=4.0130
	step [48/325], loss=3.1020
	step [49/325], loss=4.0406
	step [50/325], loss=4.3676
	step [51/325], loss=5.1505
	step [52/325], loss=3.3549
	step [53/325], loss=5.0813
	step [54/325], loss=4.6291
	step [55/325], loss=4.5404
	step [56/325], loss=4.5057
	step [57/325], loss=3.6361
	step [58/325], loss=3.6573
	step [59/325], loss=4.3626
	step [60/325], loss=4.9932
	step [61/325], loss=3.8925
	step [62/325], loss=3.7024
	step [63/325], loss=4.0032
	step [64/325], loss=4.5317
	step [65/325], loss=4.1024
	step [66/325], loss=4.5554
	step [67/325], loss=4.9632
	step [68/325], loss=3.8465
	step [69/325], loss=4.2895
	step [70/325], loss=4.7774
	step [71/325], loss=2.9445
	step [72/325], loss=3.5716
	step [73/325], loss=5.4226
	step [74/325], loss=5.0812
	step [75/325], loss=3.9519
	step [76/325], loss=4.9073
	step [77/325], loss=3.5990
	step [78/325], loss=4.4106
	step [79/325], loss=4.1634
	step [80/325], loss=4.0581
	step [81/325], loss=4.3095
	step [82/325], loss=3.6142
	step [83/325], loss=4.1452
	step [84/325], loss=3.9964
	step [85/325], loss=4.0350
	step [86/325], loss=3.7199
	step [87/325], loss=4.1949
	step [88/325], loss=3.8739
	step [89/325], loss=4.1555
	step [90/325], loss=4.1908
	step [91/325], loss=4.5335
	step [92/325], loss=3.3645
	step [93/325], loss=4.6195
	step [94/325], loss=3.8485
	step [95/325], loss=3.7401
	step [96/325], loss=5.3145
	step [97/325], loss=4.5547
	step [98/325], loss=4.0513
	step [99/325], loss=4.1226
	step [100/325], loss=3.6773
	step [101/325], loss=4.2926
	step [102/325], loss=3.8809
	step [103/325], loss=4.1068
	step [104/325], loss=3.9957
	step [105/325], loss=4.6542
	step [106/325], loss=4.2996
	step [107/325], loss=4.5277
	step [108/325], loss=4.3751
	step [109/325], loss=4.8522
	step [110/325], loss=4.7206
	step [111/325], loss=3.8914
	step [112/325], loss=4.9768
	step [113/325], loss=4.1818
	step [114/325], loss=4.2452
	step [115/325], loss=4.0294
	step [116/325], loss=5.0092
	step [117/325], loss=4.7272
	step [118/325], loss=5.0597
	step [119/325], loss=3.5260
	step [120/325], loss=3.6321
	step [121/325], loss=4.4938
	step [122/325], loss=3.3879
	step [123/325], loss=3.6337
	step [124/325], loss=4.4284
	step [125/325], loss=4.8017
	step [126/325], loss=4.2813
	step [127/325], loss=4.4688
	step [128/325], loss=4.8047
	step [129/325], loss=4.1260
	step [130/325], loss=5.2822
	step [131/325], loss=4.9105
	step [132/325], loss=3.7671
	step [133/325], loss=3.6461
	step [134/325], loss=4.2589
	step [135/325], loss=3.9574
	step [136/325], loss=2.7383
	step [137/325], loss=4.2170
	step [138/325], loss=4.9949
	step [139/325], loss=4.4931
	step [140/325], loss=3.2360
	step [141/325], loss=4.8316
	step [142/325], loss=4.6862
	step [143/325], loss=5.1591
	step [144/325], loss=3.9847
	step [145/325], loss=4.3114
	step [146/325], loss=4.0528
	step [147/325], loss=3.7557
	step [148/325], loss=4.1059
	step [149/325], loss=4.1055
	step [150/325], loss=4.3931
	step [151/325], loss=4.1416
	step [152/325], loss=4.6558
	step [153/325], loss=4.6409
	step [154/325], loss=4.2009
	step [155/325], loss=4.4098
	step [156/325], loss=3.3510
	step [157/325], loss=3.8957
	step [158/325], loss=3.0637
	step [159/325], loss=5.1067
	step [160/325], loss=3.6238
	step [161/325], loss=3.4619
	step [162/325], loss=4.8959
	step [163/325], loss=3.8838
	step [164/325], loss=4.1644
	step [165/325], loss=3.4550
	step [166/325], loss=4.7930
	step [167/325], loss=2.7981
	step [168/325], loss=4.0412
	step [169/325], loss=3.8346
	step [170/325], loss=2.9591
	step [171/325], loss=4.6522
	step [172/325], loss=4.3524
	step [173/325], loss=3.4976
	step [174/325], loss=4.2300
	step [175/325], loss=4.4052
	step [176/325], loss=3.9475
	step [177/325], loss=3.1760
	step [178/325], loss=3.3571
	step [179/325], loss=4.1796
	step [180/325], loss=3.9044
	step [181/325], loss=4.1520
	step [182/325], loss=3.3822
	step [183/325], loss=4.3486
	step [184/325], loss=3.9041
	step [185/325], loss=3.9279
	step [186/325], loss=5.1772
	step [187/325], loss=4.9236
	step [188/325], loss=3.8152
	step [189/325], loss=3.4146
	step [190/325], loss=3.7846
	step [191/325], loss=4.3876
	step [192/325], loss=4.0411
	step [193/325], loss=4.5489
	step [194/325], loss=5.3062
	step [195/325], loss=4.9442
	step [196/325], loss=4.9573
	step [197/325], loss=4.1413
	step [198/325], loss=3.2803
	step [199/325], loss=4.1460
	step [200/325], loss=4.0692
	step [201/325], loss=4.2505
	step [202/325], loss=3.8129
	step [203/325], loss=3.9868
	step [204/325], loss=4.2562
	step [205/325], loss=3.6132
	step [206/325], loss=4.4752
	step [207/325], loss=3.5300
	step [208/325], loss=4.7354
	step [209/325], loss=4.4083
	step [210/325], loss=3.7806
	step [211/325], loss=3.9717
	step [212/325], loss=4.4850
	step [213/325], loss=4.2462
	step [214/325], loss=4.5271
	step [215/325], loss=4.5848
	step [216/325], loss=4.4865
	step [217/325], loss=4.5487
	step [218/325], loss=3.7373
	step [219/325], loss=3.7106
	step [220/325], loss=3.9900
	step [221/325], loss=3.1167
	step [222/325], loss=4.5742
	step [223/325], loss=4.1123
	step [224/325], loss=5.1935
	step [225/325], loss=3.4296
	step [226/325], loss=3.9282
	step [227/325], loss=3.5854
	step [228/325], loss=4.7375
	step [229/325], loss=3.3373
	step [230/325], loss=4.6454
	step [231/325], loss=4.4929
	step [232/325], loss=3.4056
	step [233/325], loss=4.3583
	step [234/325], loss=2.8453
	step [235/325], loss=3.7597
	step [236/325], loss=4.3127
	step [237/325], loss=4.2604
	step [238/325], loss=3.5331
	step [239/325], loss=5.0616
	step [240/325], loss=5.3311
	step [241/325], loss=4.1615
	step [242/325], loss=5.0566
	step [243/325], loss=4.0094
	step [244/325], loss=4.9056
	step [245/325], loss=3.8807
	step [246/325], loss=4.1919
	step [247/325], loss=4.1601
	step [248/325], loss=5.9369
	step [249/325], loss=3.0624
	step [250/325], loss=4.7614
	step [251/325], loss=3.2257
	step [252/325], loss=3.3321
	step [253/325], loss=3.9457
	step [254/325], loss=3.6043
	step [255/325], loss=4.9204
	step [256/325], loss=3.7337
	step [257/325], loss=4.1176
	step [258/325], loss=3.6348
	step [259/325], loss=3.6490
	step [260/325], loss=4.7829
	step [261/325], loss=3.5886
	step [262/325], loss=3.5041
	step [263/325], loss=3.4729
	step [264/325], loss=4.3475
	step [265/325], loss=4.9379
	step [266/325], loss=3.6680
	step [267/325], loss=4.2876
	step [268/325], loss=3.9647
	step [269/325], loss=4.7111
	step [270/325], loss=4.7181
	step [271/325], loss=3.8006
	step [272/325], loss=3.8383
	step [273/325], loss=3.0949
	step [274/325], loss=3.1742
	step [275/325], loss=4.0420
	step [276/325], loss=4.0164
	step [277/325], loss=3.5260
	step [278/325], loss=4.6228
	step [279/325], loss=3.7808
	step [280/325], loss=3.9931
	step [281/325], loss=3.9319
	step [282/325], loss=4.2983
	step [283/325], loss=4.0553
	step [284/325], loss=4.0629
	step [285/325], loss=5.0926
	step [286/325], loss=3.3411
	step [287/325], loss=3.8499
	step [288/325], loss=4.5671
	step [289/325], loss=4.2759
	step [290/325], loss=4.9978
	step [291/325], loss=4.0821
	step [292/325], loss=5.0604
	step [293/325], loss=3.4927
	step [294/325], loss=4.7120
	step [295/325], loss=3.8031
	step [296/325], loss=4.3276
	step [297/325], loss=3.5201
	step [298/325], loss=4.1219
	step [299/325], loss=3.2778
	step [300/325], loss=4.5396
	step [301/325], loss=4.9987
	step [302/325], loss=4.1360
	step [303/325], loss=3.9729
	step [304/325], loss=3.5466
	step [305/325], loss=3.0585
	step [306/325], loss=4.1090
	step [307/325], loss=3.6724
	step [308/325], loss=3.6654
	step [309/325], loss=3.7569
	step [310/325], loss=3.2080
	step [311/325], loss=3.6585
	step [312/325], loss=5.1984
	step [313/325], loss=4.5060
	step [314/325], loss=4.6236
	step [315/325], loss=3.6548
	step [316/325], loss=4.4987
	step [317/325], loss=4.2842
	step [318/325], loss=3.7798
	step [319/325], loss=3.9499
	step [320/325], loss=3.0935
	step [321/325], loss=4.3878
	step [322/325], loss=3.2424
	step [323/325], loss=3.2210
	step [324/325], loss=3.2131
	step [325/325], loss=0.1415
	Evaluating
	loss=0.0135, precision=0.2743, recall=0.9954, f1=0.4301
saving model as: 2_saved_model.pth
Training finished
best_f1: 0.430071295636984
directing: Z rim_enhanced: False test_id 2
removed wrong scan: weights_Z_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_292_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_299_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_58_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_265_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_271_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_300_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_206_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_252_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_294_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_46_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_291_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_298_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_276_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_262_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_293_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_295_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_278_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_87_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_214_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_297_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_154_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_282_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_218_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_254_xwqg-B00034_2020-04-03.npy
# all image files: 15579 # all weight files in weight_dir: 12033 # image files with weight 12007
removed wrong scan: weights_Z_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_292_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_299_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_58_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_265_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_271_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_300_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_206_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_252_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_294_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_46_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_291_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_298_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_276_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_262_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_293_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_295_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_278_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_87_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_214_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_297_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_154_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_282_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_218_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_254_xwqg-B00034_2020-04-03.npy
# all image files: 15579 # all weight files in weight_dir: 3486 # image files with weight 3477
train: /ibex/scratch/projects/c2052/air_tube_seg/training_samples_enhanced/Z 12007
Using 4 GPUs
Going to train epochs [1-30]
Training epoch 1
	step [1/251], loss=149.5230
	step [2/251], loss=140.0800
	step [3/251], loss=130.2784
	step [4/251], loss=125.7475
	step [5/251], loss=121.5857
	step [6/251], loss=115.9326
	step [7/251], loss=115.4004
	step [8/251], loss=110.5944
	step [9/251], loss=107.0808
	step [10/251], loss=107.2375
	step [11/251], loss=104.2705
	step [12/251], loss=102.1749
	step [13/251], loss=100.1052
	step [14/251], loss=96.7787
	step [15/251], loss=96.5510
	step [16/251], loss=94.0552
	step [17/251], loss=91.5881
	step [18/251], loss=89.8751
	step [19/251], loss=88.4863
	step [20/251], loss=87.7824
	step [21/251], loss=84.6975
	step [22/251], loss=86.0043
	step [23/251], loss=83.3786
	step [24/251], loss=82.9398
	step [25/251], loss=82.5953
	step [26/251], loss=80.6605
	step [27/251], loss=80.4739
	step [28/251], loss=77.2691
	step [29/251], loss=78.3871
	step [30/251], loss=77.0061
	step [31/251], loss=79.9398
	step [32/251], loss=75.1391
	step [33/251], loss=75.2266
	step [34/251], loss=76.1745
	step [35/251], loss=76.1335
	step [36/251], loss=73.3655
	step [37/251], loss=72.1422
	step [38/251], loss=74.9837
	step [39/251], loss=71.5985
	step [40/251], loss=72.9307
	step [41/251], loss=69.8111
	step [42/251], loss=70.4522
	step [43/251], loss=70.1628
	step [44/251], loss=68.8996
	step [45/251], loss=66.9415
	step [46/251], loss=67.3472
	step [47/251], loss=68.9737
	step [48/251], loss=68.7870
	step [49/251], loss=66.0204
	step [50/251], loss=65.8941
	step [51/251], loss=67.0863
	step [52/251], loss=65.8971
	step [53/251], loss=65.2959
	step [54/251], loss=64.8428
	step [55/251], loss=66.5574
	step [56/251], loss=67.4705
	step [57/251], loss=64.2472
	step [58/251], loss=65.1760
	step [59/251], loss=61.9184
	step [60/251], loss=64.7218
	step [61/251], loss=63.3719
	step [62/251], loss=63.5949
	step [63/251], loss=63.4654
	step [64/251], loss=61.3106
	step [65/251], loss=61.8614
	step [66/251], loss=63.5602
	step [67/251], loss=60.9133
	step [68/251], loss=63.0616
	step [69/251], loss=62.4610
	step [70/251], loss=60.7482
	step [71/251], loss=63.5089
	step [72/251], loss=60.1492
	step [73/251], loss=62.3121
	step [74/251], loss=61.0199
	step [75/251], loss=60.0115
	step [76/251], loss=58.8260
	step [77/251], loss=58.7221
	step [78/251], loss=60.6882
	step [79/251], loss=60.2538
	step [80/251], loss=61.1502
	step [81/251], loss=60.1528
	step [82/251], loss=59.1199
	step [83/251], loss=59.7493
	step [84/251], loss=58.6203
	step [85/251], loss=61.9909
	step [86/251], loss=57.8757
	step [87/251], loss=57.0188
	step [88/251], loss=57.4750
	step [89/251], loss=59.1793
	step [90/251], loss=57.5782
	step [91/251], loss=57.3070
	step [92/251], loss=60.1958
	step [93/251], loss=58.0489
	step [94/251], loss=57.8868
	step [95/251], loss=57.3184
	step [96/251], loss=57.7380
	step [97/251], loss=56.7940
	step [98/251], loss=58.7945
	step [99/251], loss=57.7203
	step [100/251], loss=57.3588
	step [101/251], loss=55.9309
	step [102/251], loss=57.5740
	step [103/251], loss=57.0039
	step [104/251], loss=54.7712
	step [105/251], loss=57.0096
	step [106/251], loss=55.5109
	step [107/251], loss=55.4642
	step [108/251], loss=56.3940
	step [109/251], loss=55.0828
	step [110/251], loss=54.4174
	step [111/251], loss=56.8850
	step [112/251], loss=54.9415
	step [113/251], loss=55.6144
	step [114/251], loss=54.5942
	step [115/251], loss=53.7778
	step [116/251], loss=55.9979
	step [117/251], loss=53.6813
	step [118/251], loss=53.3209
	step [119/251], loss=56.9155
	step [120/251], loss=53.7729
	step [121/251], loss=55.3895
	step [122/251], loss=56.0083
	step [123/251], loss=56.2346
	step [124/251], loss=54.2815
	step [125/251], loss=52.5661
	step [126/251], loss=54.5067
	step [127/251], loss=54.3591
	step [128/251], loss=53.4176
	step [129/251], loss=52.4610
	step [130/251], loss=54.5433
	step [131/251], loss=53.8764
	step [132/251], loss=54.1196
	step [133/251], loss=54.2269
	step [134/251], loss=53.5818
	step [135/251], loss=54.7399
	step [136/251], loss=52.6939
	step [137/251], loss=52.5653
	step [138/251], loss=51.7928
	step [139/251], loss=53.9160
	step [140/251], loss=53.4333
	step [141/251], loss=52.5774
	step [142/251], loss=53.7743
	step [143/251], loss=51.9801
	step [144/251], loss=52.8838
	step [145/251], loss=52.0387
	step [146/251], loss=51.3271
	step [147/251], loss=51.2101
	step [148/251], loss=53.4205
	step [149/251], loss=53.5151
	step [150/251], loss=52.0987
	step [151/251], loss=51.1428
	step [152/251], loss=53.4608
	step [153/251], loss=52.6124
	step [154/251], loss=53.8014
	step [155/251], loss=49.2239
	step [156/251], loss=52.4569
	step [157/251], loss=52.8368
	step [158/251], loss=50.1006
	step [159/251], loss=49.9234
	step [160/251], loss=51.0770
	step [161/251], loss=53.1811
	step [162/251], loss=50.8192
	step [163/251], loss=50.0554
	step [164/251], loss=49.8050
	step [165/251], loss=50.9236
	step [166/251], loss=50.0477
	step [167/251], loss=50.0729
	step [168/251], loss=52.3925
	step [169/251], loss=49.3420
	step [170/251], loss=48.3233
	step [171/251], loss=50.1030
	step [172/251], loss=48.7388
	step [173/251], loss=52.2852
	step [174/251], loss=49.4631
	step [175/251], loss=49.3876
	step [176/251], loss=50.9672
	step [177/251], loss=50.2526
	step [178/251], loss=51.4375
	step [179/251], loss=51.2971
	step [180/251], loss=50.9287
	step [181/251], loss=49.0789
	step [182/251], loss=52.8697
	step [183/251], loss=49.0148
	step [184/251], loss=47.6722
	step [185/251], loss=49.3792
	step [186/251], loss=48.5629
	step [187/251], loss=48.8817
	step [188/251], loss=49.4398
	step [189/251], loss=48.3397
	step [190/251], loss=48.1646
	step [191/251], loss=49.3329
	step [192/251], loss=47.5462
	step [193/251], loss=48.2130
	step [194/251], loss=50.7243
	step [195/251], loss=47.8953
	step [196/251], loss=49.1424
	step [197/251], loss=48.1201
	step [198/251], loss=49.1245
	step [199/251], loss=46.5332
	step [200/251], loss=46.6249
	step [201/251], loss=48.5261
	step [202/251], loss=49.4278
	step [203/251], loss=46.9281
	step [204/251], loss=48.1780
	step [205/251], loss=47.5186
	step [206/251], loss=48.3260
	step [207/251], loss=49.7726
	step [208/251], loss=47.0931
	step [209/251], loss=48.6291
	step [210/251], loss=47.9373
	step [211/251], loss=49.3582
	step [212/251], loss=48.1673
	step [213/251], loss=48.5984
	step [214/251], loss=46.0692
	step [215/251], loss=46.2941
	step [216/251], loss=47.3541
	step [217/251], loss=46.4102
	step [218/251], loss=46.8685
	step [219/251], loss=48.4864
	step [220/251], loss=47.9724
	step [221/251], loss=45.4916
	step [222/251], loss=46.8008
	step [223/251], loss=46.4450
	step [224/251], loss=46.8835
	step [225/251], loss=45.3173
	step [226/251], loss=46.1165
	step [227/251], loss=45.0548
	step [228/251], loss=47.1062
	step [229/251], loss=48.5795
	step [230/251], loss=45.3609
	step [231/251], loss=45.2596
	step [232/251], loss=45.6515
	step [233/251], loss=45.5205
	step [234/251], loss=45.8727
	step [235/251], loss=46.0345
	step [236/251], loss=46.0181
	step [237/251], loss=47.4907
	step [238/251], loss=45.8823
	step [239/251], loss=45.8951
	step [240/251], loss=45.5703
	step [241/251], loss=46.1098
	step [242/251], loss=44.9179
	step [243/251], loss=44.8045
	step [244/251], loss=44.0233
	step [245/251], loss=46.6668
	step [246/251], loss=44.3505
	step [247/251], loss=45.8406
	step [248/251], loss=46.7807
	step [249/251], loss=44.6378
	step [250/251], loss=44.5922
	step [251/251], loss=6.5767
	Evaluating
	loss=0.2195, precision=0.1648, recall=0.9974, f1=0.2829
saving model as: 2_saved_model.pth
Training epoch 2
	step [1/251], loss=44.9165
	step [2/251], loss=44.2105
	step [3/251], loss=45.0098
	step [4/251], loss=44.8394
	step [5/251], loss=43.4723
	step [6/251], loss=44.6301
	step [7/251], loss=46.1628
	step [8/251], loss=44.7897
	step [9/251], loss=45.5821
	step [10/251], loss=43.2585
	step [11/251], loss=42.9207
	step [12/251], loss=44.6763
	step [13/251], loss=44.6752
	step [14/251], loss=42.3203
	step [15/251], loss=44.1761
	step [16/251], loss=46.3504
	step [17/251], loss=44.6768
	step [18/251], loss=43.3547
	step [19/251], loss=43.6698
	step [20/251], loss=43.5943
	step [21/251], loss=42.9722
	step [22/251], loss=42.9095
	step [23/251], loss=43.0512
	step [24/251], loss=43.4665
	step [25/251], loss=43.0227
	step [26/251], loss=42.5788
	step [27/251], loss=42.1206
	step [28/251], loss=42.6792
	step [29/251], loss=42.0493
	step [30/251], loss=41.0395
	step [31/251], loss=45.1528
	step [32/251], loss=43.6320
	step [33/251], loss=42.6695
	step [34/251], loss=42.5391
	step [35/251], loss=41.8875
	step [36/251], loss=41.8207
	step [37/251], loss=44.0342
	step [38/251], loss=41.1767
	step [39/251], loss=41.5775
	step [40/251], loss=41.1004
	step [41/251], loss=41.6041
	step [42/251], loss=41.5843
	step [43/251], loss=44.2965
	step [44/251], loss=42.1050
	step [45/251], loss=41.7547
	step [46/251], loss=41.8579
	step [47/251], loss=43.4190
	step [48/251], loss=41.8033
	step [49/251], loss=43.0549
	step [50/251], loss=43.0662
	step [51/251], loss=42.4020
	step [52/251], loss=42.8490
	step [53/251], loss=41.2060
	step [54/251], loss=42.1527
	step [55/251], loss=40.7190
	step [56/251], loss=41.4525
	step [57/251], loss=43.2831
	step [58/251], loss=42.9343
	step [59/251], loss=40.1006
	step [60/251], loss=40.9426
	step [61/251], loss=41.1602
	step [62/251], loss=39.3839
	step [63/251], loss=42.4227
	step [64/251], loss=39.7629
	step [65/251], loss=40.2665
	step [66/251], loss=41.2635
	step [67/251], loss=41.7690
	step [68/251], loss=40.1954
	step [69/251], loss=39.7877
	step [70/251], loss=40.9083
	step [71/251], loss=39.4572
	step [72/251], loss=39.9007
	step [73/251], loss=41.4410
	step [74/251], loss=40.8597
	step [75/251], loss=39.8982
	step [76/251], loss=41.2849
	step [77/251], loss=39.8041
	step [78/251], loss=39.4872
	step [79/251], loss=39.0628
	step [80/251], loss=40.0524
	step [81/251], loss=42.1713
	step [82/251], loss=39.3020
	step [83/251], loss=39.9031
	step [84/251], loss=38.2696
	step [85/251], loss=39.3283
	step [86/251], loss=39.2806
	step [87/251], loss=38.6747
	step [88/251], loss=39.4613
	step [89/251], loss=38.7221
	step [90/251], loss=40.6251
	step [91/251], loss=39.0559
	step [92/251], loss=38.6331
	step [93/251], loss=37.6224
	step [94/251], loss=39.2344
	step [95/251], loss=40.6986
	step [96/251], loss=37.6032
	step [97/251], loss=38.1936
	step [98/251], loss=38.4845
	step [99/251], loss=38.3773
	step [100/251], loss=38.7821
	step [101/251], loss=40.9100
	step [102/251], loss=42.3910
	step [103/251], loss=39.3922
	step [104/251], loss=38.1891
	step [105/251], loss=39.7602
	step [106/251], loss=38.4104
	step [107/251], loss=38.0838
	step [108/251], loss=39.1507
	step [109/251], loss=39.7447
	step [110/251], loss=39.7983
	step [111/251], loss=40.0379
	step [112/251], loss=38.7001
	step [113/251], loss=39.0828
	step [114/251], loss=37.9109
	step [115/251], loss=38.6754
	step [116/251], loss=39.0271
	step [117/251], loss=38.2078
	step [118/251], loss=37.7231
	step [119/251], loss=36.7558
	step [120/251], loss=37.9811
	step [121/251], loss=37.8533
	step [122/251], loss=37.2113
	step [123/251], loss=39.5420
	step [124/251], loss=37.5128
	step [125/251], loss=37.3331
	step [126/251], loss=37.2446
	step [127/251], loss=37.7738
	step [128/251], loss=38.2266
	step [129/251], loss=37.3989
	step [130/251], loss=37.8394
	step [131/251], loss=38.1742
	step [132/251], loss=37.5487
	step [133/251], loss=38.8597
	step [134/251], loss=36.7411
	step [135/251], loss=36.7876
	step [136/251], loss=37.4949
	step [137/251], loss=37.5780
	step [138/251], loss=36.9578
	step [139/251], loss=36.9419
	step [140/251], loss=35.8970
	step [141/251], loss=34.8792
	step [142/251], loss=35.4019
	step [143/251], loss=37.6319
	step [144/251], loss=36.6862
	step [145/251], loss=35.7715
	step [146/251], loss=35.2109
	step [147/251], loss=37.1231
	step [148/251], loss=37.6364
	step [149/251], loss=36.1862
	step [150/251], loss=38.9574
	step [151/251], loss=35.0880
	step [152/251], loss=37.8710
	step [153/251], loss=36.9400
	step [154/251], loss=37.2021
	step [155/251], loss=36.2978
	step [156/251], loss=36.5679
	step [157/251], loss=35.3054
	step [158/251], loss=35.7604
	step [159/251], loss=35.7873
	step [160/251], loss=34.0399
	step [161/251], loss=36.9400
	step [162/251], loss=34.8780
	step [163/251], loss=36.8843
	step [164/251], loss=34.3242
	step [165/251], loss=35.2147
	step [166/251], loss=35.7737
	step [167/251], loss=35.1363
	step [168/251], loss=35.1161
	step [169/251], loss=35.1066
	step [170/251], loss=35.6336
	step [171/251], loss=36.6592
	step [172/251], loss=36.1131
	step [173/251], loss=35.8105
	step [174/251], loss=34.1124
	step [175/251], loss=33.9496
	step [176/251], loss=35.4127
	step [177/251], loss=34.3469
	step [178/251], loss=37.0479
	step [179/251], loss=34.4303
	step [180/251], loss=35.1395
	step [181/251], loss=34.3221
	step [182/251], loss=37.8417
	step [183/251], loss=34.4638
	step [184/251], loss=34.1703
	step [185/251], loss=34.8030
	step [186/251], loss=35.4117
	step [187/251], loss=35.6170
	step [188/251], loss=34.2311
	step [189/251], loss=33.5199
	step [190/251], loss=36.9052
	step [191/251], loss=33.1078
	step [192/251], loss=34.4893
	step [193/251], loss=32.5376
	step [194/251], loss=34.7690
	step [195/251], loss=33.8990
	step [196/251], loss=33.9910
	step [197/251], loss=33.8859
	step [198/251], loss=32.8327
	step [199/251], loss=33.5391
	step [200/251], loss=34.2482
	step [201/251], loss=33.4596
	step [202/251], loss=33.8141
	step [203/251], loss=33.9155
	step [204/251], loss=31.6839
	step [205/251], loss=33.9969
	step [206/251], loss=33.1438
	step [207/251], loss=34.7065
	step [208/251], loss=33.8367
	step [209/251], loss=33.2106
	step [210/251], loss=34.4315
	step [211/251], loss=33.0092
	step [212/251], loss=33.4595
	step [213/251], loss=33.4680
	step [214/251], loss=31.9407
	step [215/251], loss=33.5132
	step [216/251], loss=34.2638
	step [217/251], loss=34.7429
	step [218/251], loss=35.2106
	step [219/251], loss=32.3365
	step [220/251], loss=33.5621
	step [221/251], loss=31.9977
	step [222/251], loss=32.8622
	step [223/251], loss=32.6012
	step [224/251], loss=32.6052
	step [225/251], loss=34.1497
	step [226/251], loss=34.7638
	step [227/251], loss=32.1463
	step [228/251], loss=35.1068
	step [229/251], loss=34.5905
	step [230/251], loss=32.5002
	step [231/251], loss=31.9532
	step [232/251], loss=33.1944
	step [233/251], loss=34.1751
	step [234/251], loss=32.9965
	step [235/251], loss=32.2158
	step [236/251], loss=34.6815
	step [237/251], loss=32.1213
	step [238/251], loss=31.7811
	step [239/251], loss=32.9155
	step [240/251], loss=31.5770
	step [241/251], loss=33.2033
	step [242/251], loss=31.0582
	step [243/251], loss=34.0064
	step [244/251], loss=32.5010
	step [245/251], loss=32.1711
	step [246/251], loss=30.5954
	step [247/251], loss=34.0671
	step [248/251], loss=32.0581
	step [249/251], loss=32.2610
	step [250/251], loss=32.0884
	step [251/251], loss=6.6817
	Evaluating
	loss=0.1540, precision=0.1638, recall=0.9976, f1=0.2814
Training epoch 3
	step [1/251], loss=34.4745
	step [2/251], loss=32.6543
	step [3/251], loss=32.2416
	step [4/251], loss=31.5126
	step [5/251], loss=30.8176
	step [6/251], loss=30.9073
	step [7/251], loss=31.1165
	step [8/251], loss=30.6829
	step [9/251], loss=31.1051
	step [10/251], loss=31.8578
	step [11/251], loss=33.6510
	step [12/251], loss=31.5529
	step [13/251], loss=31.4033
	step [14/251], loss=31.2466
	step [15/251], loss=31.8365
	step [16/251], loss=30.7359
	step [17/251], loss=32.4038
	step [18/251], loss=31.6626
	step [19/251], loss=31.0252
	step [20/251], loss=31.3829
	step [21/251], loss=31.3032
	step [22/251], loss=29.9706
	step [23/251], loss=30.8221
	step [24/251], loss=32.1863
	step [25/251], loss=31.1996
	step [26/251], loss=31.9291
	step [27/251], loss=29.7756
	step [28/251], loss=31.1701
	step [29/251], loss=32.8502
	step [30/251], loss=29.0022
	step [31/251], loss=29.3516
	step [32/251], loss=31.1866
	step [33/251], loss=30.9192
	step [34/251], loss=29.3219
	step [35/251], loss=28.6419
	step [36/251], loss=30.6595
	step [37/251], loss=31.5935
	step [38/251], loss=30.1413
	step [39/251], loss=30.4982
	step [40/251], loss=28.2680
	step [41/251], loss=29.1711
	step [42/251], loss=30.1842
	step [43/251], loss=31.8185
	step [44/251], loss=31.7174
	step [45/251], loss=30.8843
	step [46/251], loss=28.9471
	step [47/251], loss=30.6717
	step [48/251], loss=30.3898
	step [49/251], loss=30.6605
	step [50/251], loss=29.9233
	step [51/251], loss=29.3933
	step [52/251], loss=29.3462
	step [53/251], loss=30.2777
	step [54/251], loss=28.8415
	step [55/251], loss=29.2478
	step [56/251], loss=32.5510
	step [57/251], loss=29.3510
	step [58/251], loss=28.3141
	step [59/251], loss=32.1036
	step [60/251], loss=29.8357
	step [61/251], loss=28.4805
	step [62/251], loss=30.1382
	step [63/251], loss=29.0254
	step [64/251], loss=27.8077
	step [65/251], loss=28.6114
	step [66/251], loss=29.0749
	step [67/251], loss=29.9582
	step [68/251], loss=29.7696
	step [69/251], loss=28.5306
	step [70/251], loss=29.6801
	step [71/251], loss=30.0056
	step [72/251], loss=29.7838
	step [73/251], loss=29.0341
	step [74/251], loss=28.1616
	step [75/251], loss=27.7975
	step [76/251], loss=27.3795
	step [77/251], loss=30.7358
	step [78/251], loss=28.7073
	step [79/251], loss=27.7951
	step [80/251], loss=28.4084
	step [81/251], loss=29.5672
	step [82/251], loss=28.5497
	step [83/251], loss=27.1918
	step [84/251], loss=28.5941
	step [85/251], loss=29.9242
	step [86/251], loss=28.9556
	step [87/251], loss=29.7375
	step [88/251], loss=30.7844
	step [89/251], loss=28.7208
	step [90/251], loss=28.8156
	step [91/251], loss=30.6458
	step [92/251], loss=27.3765
	step [93/251], loss=27.9541
	step [94/251], loss=28.6736
	step [95/251], loss=29.2476
	step [96/251], loss=27.7189
	step [97/251], loss=28.3972
	step [98/251], loss=28.4455
	step [99/251], loss=28.7396
	step [100/251], loss=29.6379
	step [101/251], loss=26.2353
	step [102/251], loss=28.1258
	step [103/251], loss=28.7125
	step [104/251], loss=27.8240
	step [105/251], loss=28.8726
	step [106/251], loss=28.0039
	step [107/251], loss=26.7884
	step [108/251], loss=29.9109
	step [109/251], loss=27.0675
	step [110/251], loss=28.5706
	step [111/251], loss=27.7196
	step [112/251], loss=26.7507
	step [113/251], loss=28.2829
	step [114/251], loss=29.3706
	step [115/251], loss=27.9172
	step [116/251], loss=27.9035
	step [117/251], loss=26.7050
	step [118/251], loss=27.0855
	step [119/251], loss=27.0572
	step [120/251], loss=27.9406
	step [121/251], loss=27.8154
	step [122/251], loss=29.4538
	step [123/251], loss=26.5369
	step [124/251], loss=28.1961
	step [125/251], loss=28.3337
	step [126/251], loss=27.3573
	step [127/251], loss=27.4033
	step [128/251], loss=27.2546
	step [129/251], loss=26.6934
	step [130/251], loss=27.3452
	step [131/251], loss=28.1184
	step [132/251], loss=27.4253
	step [133/251], loss=27.2065
	step [134/251], loss=26.8180
	step [135/251], loss=27.8882
	step [136/251], loss=26.8390
	step [137/251], loss=28.1958
	step [138/251], loss=27.3501
	step [139/251], loss=26.5333
	step [140/251], loss=27.7095
	step [141/251], loss=27.6360
	step [142/251], loss=26.9993
	step [143/251], loss=27.9616
	step [144/251], loss=28.0159
	step [145/251], loss=25.9839
	step [146/251], loss=25.6550
	step [147/251], loss=26.2262
	step [148/251], loss=27.4748
	step [149/251], loss=27.8758
	step [150/251], loss=26.5356
	step [151/251], loss=26.5044
	step [152/251], loss=27.1047
	step [153/251], loss=26.0926
	step [154/251], loss=26.0984
	step [155/251], loss=25.2765
	step [156/251], loss=26.0086
	step [157/251], loss=25.7298
	step [158/251], loss=26.3905
	step [159/251], loss=25.5919
	step [160/251], loss=26.0524
	step [161/251], loss=26.5762
	step [162/251], loss=25.5600
	step [163/251], loss=25.8090
	step [164/251], loss=25.6730
	step [165/251], loss=25.9516
	step [166/251], loss=25.4882
	step [167/251], loss=24.2847
	step [168/251], loss=26.2852
	step [169/251], loss=27.7477
	step [170/251], loss=26.5534
	step [171/251], loss=26.3570
	step [172/251], loss=27.2363
	step [173/251], loss=24.3706
	step [174/251], loss=24.8623
	step [175/251], loss=26.7106
	step [176/251], loss=25.0062
	step [177/251], loss=27.0682
	step [178/251], loss=25.4536
	step [179/251], loss=25.9604
	step [180/251], loss=27.3212
	step [181/251], loss=27.2946
	step [182/251], loss=24.3976
	step [183/251], loss=25.5476
	step [184/251], loss=25.0101
	step [185/251], loss=26.7252
	step [186/251], loss=25.5441
	step [187/251], loss=25.6419
	step [188/251], loss=25.7796
	step [189/251], loss=25.7935
	step [190/251], loss=27.2279
	step [191/251], loss=24.1679
	step [192/251], loss=24.1966
	step [193/251], loss=25.8761
	step [194/251], loss=25.0053
	step [195/251], loss=26.5150
	step [196/251], loss=26.2722
	step [197/251], loss=24.0536
	step [198/251], loss=25.8048
	step [199/251], loss=26.9648
	step [200/251], loss=24.8315
	step [201/251], loss=27.8102
	step [202/251], loss=26.3868
	step [203/251], loss=24.4341
	step [204/251], loss=24.9946
	step [205/251], loss=24.1364
	step [206/251], loss=24.7456
	step [207/251], loss=25.6337
	step [208/251], loss=25.0781
	step [209/251], loss=25.0579
	step [210/251], loss=24.9058
	step [211/251], loss=25.0511
	step [212/251], loss=25.8194
	step [213/251], loss=23.7666
	step [214/251], loss=26.5910
	step [215/251], loss=24.4264
	step [216/251], loss=26.5480
	step [217/251], loss=24.0529
	step [218/251], loss=24.7361
	step [219/251], loss=24.9342
	step [220/251], loss=24.3067
	step [221/251], loss=24.1350
	step [222/251], loss=23.7923
	step [223/251], loss=24.6781
	step [224/251], loss=23.3693
	step [225/251], loss=24.6274
	step [226/251], loss=24.8570
	step [227/251], loss=25.4262
	step [228/251], loss=24.5075
	step [229/251], loss=24.1953
	step [230/251], loss=26.1916
	step [231/251], loss=24.2146
	step [232/251], loss=27.8175
	step [233/251], loss=23.9361
	step [234/251], loss=25.0333
	step [235/251], loss=24.5407
	step [236/251], loss=24.9579
	step [237/251], loss=24.4191
	step [238/251], loss=24.7476
	step [239/251], loss=24.5474
	step [240/251], loss=25.2412
	step [241/251], loss=23.5513
	step [242/251], loss=26.7103
	step [243/251], loss=24.0949
	step [244/251], loss=23.9079
	step [245/251], loss=24.9241
	step [246/251], loss=22.7360
	step [247/251], loss=25.7139
	step [248/251], loss=23.8057
	step [249/251], loss=23.8600
	step [250/251], loss=23.8986
	step [251/251], loss=4.7598
	Evaluating
	loss=0.1123, precision=0.1802, recall=0.9970, f1=0.3053
saving model as: 2_saved_model.pth
Training epoch 4
	step [1/251], loss=23.7270
	step [2/251], loss=24.4998
	step [3/251], loss=24.6923
	step [4/251], loss=25.0658
	step [5/251], loss=25.4409
	step [6/251], loss=23.2316
	step [7/251], loss=23.9058
	step [8/251], loss=23.8615
	step [9/251], loss=24.8794
	step [10/251], loss=22.4678
	step [11/251], loss=22.9498
	step [12/251], loss=23.8325
	step [13/251], loss=22.4936
	step [14/251], loss=24.0427
	step [15/251], loss=24.6277
	step [16/251], loss=23.8229
	step [17/251], loss=24.0576
	step [18/251], loss=25.0410
	step [19/251], loss=25.0691
	step [20/251], loss=23.9860
	step [21/251], loss=22.7896
	step [22/251], loss=23.7396
	step [23/251], loss=22.7501
	step [24/251], loss=21.7478
	step [25/251], loss=25.1773
	step [26/251], loss=24.2516
	step [27/251], loss=21.2534
	step [28/251], loss=23.8772
	step [29/251], loss=23.7083
	step [30/251], loss=25.8933
	step [31/251], loss=22.9492
	step [32/251], loss=25.0931
	step [33/251], loss=23.6049
	step [34/251], loss=22.2843
	step [35/251], loss=23.1252
	step [36/251], loss=23.6916
	step [37/251], loss=21.2898
	step [38/251], loss=22.8520
	step [39/251], loss=22.5090
	step [40/251], loss=22.1504
	step [41/251], loss=22.6025
	step [42/251], loss=24.2360
	step [43/251], loss=24.0077
	step [44/251], loss=21.3318
	step [45/251], loss=26.3412
	step [46/251], loss=22.7429
	step [47/251], loss=23.7621
	step [48/251], loss=24.0407
	step [49/251], loss=23.5380
	step [50/251], loss=26.0491
	step [51/251], loss=23.0119
	step [52/251], loss=21.7027
	step [53/251], loss=22.9050
	step [54/251], loss=23.2553
	step [55/251], loss=21.9211
	step [56/251], loss=21.6517
	step [57/251], loss=23.2512
	step [58/251], loss=23.7823
	step [59/251], loss=21.8778
	step [60/251], loss=22.6927
	step [61/251], loss=20.5537
	step [62/251], loss=21.4076
	step [63/251], loss=21.8925
	step [64/251], loss=22.4783
	step [65/251], loss=22.2508
	step [66/251], loss=22.8676
	step [67/251], loss=21.9054
	step [68/251], loss=22.9706
	step [69/251], loss=22.0871
	step [70/251], loss=23.4782
	step [71/251], loss=22.4829
	step [72/251], loss=23.0791
	step [73/251], loss=21.6578
	step [74/251], loss=23.5049
	step [75/251], loss=21.6937
	step [76/251], loss=24.2941
	step [77/251], loss=20.9702
	step [78/251], loss=22.4770
	step [79/251], loss=23.2390
	step [80/251], loss=23.2319
	step [81/251], loss=24.2445
	step [82/251], loss=21.3583
	step [83/251], loss=24.0640
	step [84/251], loss=21.5380
	step [85/251], loss=21.7349
	step [86/251], loss=22.2100
	step [87/251], loss=22.8806
	step [88/251], loss=23.4623
	step [89/251], loss=22.4597
	step [90/251], loss=22.7490
	step [91/251], loss=20.7809
	step [92/251], loss=20.7947
	step [93/251], loss=22.2794
	step [94/251], loss=22.2468
	step [95/251], loss=22.0498
	step [96/251], loss=21.9680
	step [97/251], loss=21.4433
	step [98/251], loss=24.1499
	step [99/251], loss=24.1961
	step [100/251], loss=21.3143
	step [101/251], loss=21.1280
	step [102/251], loss=20.3423
	step [103/251], loss=20.6397
	step [104/251], loss=23.2119
	step [105/251], loss=20.7759
	step [106/251], loss=21.3065
	step [107/251], loss=22.2409
	step [108/251], loss=22.1084
	step [109/251], loss=21.6725
	step [110/251], loss=20.2786
	step [111/251], loss=22.6275
	step [112/251], loss=20.8544
	step [113/251], loss=20.2281
	step [114/251], loss=22.8635
	step [115/251], loss=21.4964
	step [116/251], loss=21.4271
	step [117/251], loss=22.2446
	step [118/251], loss=20.4958
	step [119/251], loss=20.9066
	step [120/251], loss=23.9054
	step [121/251], loss=20.5682
	step [122/251], loss=21.2858
	step [123/251], loss=22.1358
	step [124/251], loss=19.9436
	step [125/251], loss=21.0455
	step [126/251], loss=22.3704
	step [127/251], loss=20.1318
	step [128/251], loss=21.7572
	step [129/251], loss=20.2919
	step [130/251], loss=21.0410
	step [131/251], loss=20.2734
	step [132/251], loss=19.8276
	step [133/251], loss=21.2833
	step [134/251], loss=21.1163
	step [135/251], loss=19.4031
	step [136/251], loss=20.3640
	step [137/251], loss=21.6088
	step [138/251], loss=20.7873
	step [139/251], loss=21.6701
	step [140/251], loss=21.1881
	step [141/251], loss=21.2044
	step [142/251], loss=23.4096
	step [143/251], loss=22.9961
	step [144/251], loss=19.6502
	step [145/251], loss=21.7070
	step [146/251], loss=22.1672
	step [147/251], loss=20.0030
	step [148/251], loss=21.2381
	step [149/251], loss=20.8970
	step [150/251], loss=19.7414
	step [151/251], loss=21.0353
	step [152/251], loss=21.9285
	step [153/251], loss=20.9590
	step [154/251], loss=20.2419
	step [155/251], loss=21.1750
	step [156/251], loss=20.2167
	step [157/251], loss=20.4914
	step [158/251], loss=21.5440
	step [159/251], loss=19.5013
	step [160/251], loss=20.6162
	step [161/251], loss=20.8331
	step [162/251], loss=20.4554
	step [163/251], loss=20.7936
	step [164/251], loss=20.7752
	step [165/251], loss=20.0795
	step [166/251], loss=20.1423
	step [167/251], loss=19.8833
	step [168/251], loss=20.3196
	step [169/251], loss=18.9575
	step [170/251], loss=20.7628
	step [171/251], loss=20.4699
	step [172/251], loss=20.8423
	step [173/251], loss=21.3804
	step [174/251], loss=20.0197
	step [175/251], loss=22.3998
	step [176/251], loss=18.9923
	step [177/251], loss=20.1446
	step [178/251], loss=21.4859
	step [179/251], loss=19.6685
	step [180/251], loss=19.8129
	step [181/251], loss=19.6978
	step [182/251], loss=19.5628
	step [183/251], loss=19.6549
	step [184/251], loss=19.4616
	step [185/251], loss=19.7161
	step [186/251], loss=19.5773
	step [187/251], loss=20.1259
	step [188/251], loss=20.5261
	step [189/251], loss=22.4267
	step [190/251], loss=19.5204
	step [191/251], loss=19.9148
	step [192/251], loss=19.2807
	step [193/251], loss=20.3169
	step [194/251], loss=19.8263
	step [195/251], loss=19.7881
	step [196/251], loss=22.0669
	step [197/251], loss=19.3687
	step [198/251], loss=19.9820
	step [199/251], loss=19.6600
	step [200/251], loss=19.0507
	step [201/251], loss=19.7442
	step [202/251], loss=19.2180
	step [203/251], loss=19.1800
	step [204/251], loss=19.6164
	step [205/251], loss=20.5929
	step [206/251], loss=20.6119
	step [207/251], loss=20.3535
	step [208/251], loss=21.8711
	step [209/251], loss=19.4511
	step [210/251], loss=18.6642
	step [211/251], loss=20.9720
	step [212/251], loss=19.1832
	step [213/251], loss=19.8506
	step [214/251], loss=17.7197
	step [215/251], loss=19.0189
	step [216/251], loss=21.1420
	step [217/251], loss=18.8319
	step [218/251], loss=21.0131
	step [219/251], loss=17.7256
	step [220/251], loss=20.8710
	step [221/251], loss=19.1609
	step [222/251], loss=19.4596
	step [223/251], loss=18.5964
	step [224/251], loss=20.6378
	step [225/251], loss=20.6978
	step [226/251], loss=19.6130
	step [227/251], loss=18.9217
	step [228/251], loss=20.5438
	step [229/251], loss=20.4332
	step [230/251], loss=18.9173
	step [231/251], loss=17.2995
	step [232/251], loss=18.6364
	step [233/251], loss=19.3641
	step [234/251], loss=18.1466
	step [235/251], loss=19.3478
	step [236/251], loss=20.3625
	step [237/251], loss=19.6205
	step [238/251], loss=19.9594
	step [239/251], loss=19.8409
	step [240/251], loss=19.6713
	step [241/251], loss=18.8366
	step [242/251], loss=18.1619
	step [243/251], loss=19.3197
	step [244/251], loss=20.3646
	step [245/251], loss=20.2513
	step [246/251], loss=18.1405
	step [247/251], loss=19.0593
	step [248/251], loss=19.0028
	step [249/251], loss=18.1270
	step [250/251], loss=18.1407
	step [251/251], loss=2.7331
	Evaluating
	loss=0.0829, precision=0.2082, recall=0.9964, f1=0.3444
saving model as: 2_saved_model.pth
Training epoch 5
	step [1/251], loss=18.0381
	step [2/251], loss=17.4072
	step [3/251], loss=19.8901
	step [4/251], loss=19.5989
	step [5/251], loss=18.7221
	step [6/251], loss=21.2438
	step [7/251], loss=19.9396
	step [8/251], loss=19.2126
	step [9/251], loss=19.9906
	step [10/251], loss=19.0144
	step [11/251], loss=18.3290
	step [12/251], loss=18.1792
	step [13/251], loss=20.1970
	step [14/251], loss=19.0893
	step [15/251], loss=20.1960
	step [16/251], loss=19.4320
	step [17/251], loss=17.8211
	step [18/251], loss=18.4961
	step [19/251], loss=17.6192
	step [20/251], loss=17.7171
	step [21/251], loss=17.5340
	step [22/251], loss=20.1473
	step [23/251], loss=16.9654
	step [24/251], loss=19.6928
	step [25/251], loss=21.7518
	step [26/251], loss=18.0266
	step [27/251], loss=18.9546
	step [28/251], loss=19.5552
	step [29/251], loss=17.8062
	step [30/251], loss=19.2170
	step [31/251], loss=19.9716
	step [32/251], loss=19.7264
	step [33/251], loss=17.1394
	step [34/251], loss=17.6207
	step [35/251], loss=18.0926
	step [36/251], loss=19.4460
	step [37/251], loss=20.0358
	step [38/251], loss=17.1044
	step [39/251], loss=19.8220
	step [40/251], loss=16.3826
	step [41/251], loss=19.5995
	step [42/251], loss=19.5944
	step [43/251], loss=19.7029
	step [44/251], loss=17.9157
	step [45/251], loss=20.4037
	step [46/251], loss=19.2615
	step [47/251], loss=17.7732
	step [48/251], loss=18.7907
	step [49/251], loss=17.6584
	step [50/251], loss=17.2001
	step [51/251], loss=18.8578
	step [52/251], loss=19.6967
	step [53/251], loss=18.2942
	step [54/251], loss=17.9525
	step [55/251], loss=19.6514
	step [56/251], loss=17.7074
	step [57/251], loss=19.0424
	step [58/251], loss=17.7594
	step [59/251], loss=19.4129
	step [60/251], loss=17.7406
	step [61/251], loss=17.7477
	step [62/251], loss=17.3863
	step [63/251], loss=17.8391
	step [64/251], loss=16.3686
	step [65/251], loss=17.1605
	step [66/251], loss=17.6777
	step [67/251], loss=17.2340
	step [68/251], loss=17.4455
	step [69/251], loss=16.9785
	step [70/251], loss=20.9524
	step [71/251], loss=18.5980
	step [72/251], loss=16.8920
	step [73/251], loss=18.6056
	step [74/251], loss=19.1668
	step [75/251], loss=16.9271
	step [76/251], loss=18.4287
	step [77/251], loss=17.1108
	step [78/251], loss=18.4944
	step [79/251], loss=18.6262
	step [80/251], loss=19.1159
	step [81/251], loss=18.1885
	step [82/251], loss=18.9114
	step [83/251], loss=17.1864
	step [84/251], loss=18.3007
	step [85/251], loss=17.7746
	step [86/251], loss=17.9951
	step [87/251], loss=18.1082
	step [88/251], loss=18.9964
	step [89/251], loss=17.8827
	step [90/251], loss=17.7156
	step [91/251], loss=16.8306
	step [92/251], loss=17.6803
	step [93/251], loss=17.2952
	step [94/251], loss=17.5747
	step [95/251], loss=16.9272
	step [96/251], loss=17.7911
	step [97/251], loss=17.0293
	step [98/251], loss=16.6322
	step [99/251], loss=15.7078
	step [100/251], loss=16.4011
	step [101/251], loss=19.9049
	step [102/251], loss=17.7146
	step [103/251], loss=17.2680
	step [104/251], loss=16.2903
	step [105/251], loss=17.2299
	step [106/251], loss=17.4723
	step [107/251], loss=17.9550
	step [108/251], loss=18.8241
	step [109/251], loss=19.9653
	step [110/251], loss=15.5305
	step [111/251], loss=15.1578
	step [112/251], loss=15.8856
	step [113/251], loss=17.2875
	step [114/251], loss=16.8992
	step [115/251], loss=17.0598
	step [116/251], loss=15.4350
	step [117/251], loss=16.7073
	step [118/251], loss=17.8803
	step [119/251], loss=17.1064
	step [120/251], loss=19.1189
	step [121/251], loss=19.1557
	step [122/251], loss=17.4328
	step [123/251], loss=16.1797
	step [124/251], loss=17.8475
	step [125/251], loss=16.3099
	step [126/251], loss=17.8698
	step [127/251], loss=15.9800
	step [128/251], loss=18.6134
	step [129/251], loss=17.7049
	step [130/251], loss=17.2378
	step [131/251], loss=16.9232
	step [132/251], loss=16.6675
	step [133/251], loss=16.1133
	step [134/251], loss=17.1346
	step [135/251], loss=17.8614
	step [136/251], loss=17.1146
	step [137/251], loss=16.6231
	step [138/251], loss=17.1820
	step [139/251], loss=16.0446
	step [140/251], loss=16.9908
	step [141/251], loss=19.0143
	step [142/251], loss=18.1203
	step [143/251], loss=16.5679
	step [144/251], loss=16.7968
	step [145/251], loss=17.0356
	step [146/251], loss=15.8563
	step [147/251], loss=18.1784
	step [148/251], loss=16.2314
	step [149/251], loss=19.0613
	step [150/251], loss=17.4550
	step [151/251], loss=17.1825
	step [152/251], loss=16.0007
	step [153/251], loss=17.2738
	step [154/251], loss=16.3638
	step [155/251], loss=19.5392
	step [156/251], loss=17.1740
	step [157/251], loss=16.9024
	step [158/251], loss=17.5406
	step [159/251], loss=20.5478
	step [160/251], loss=19.2956
	step [161/251], loss=18.1716
	step [162/251], loss=18.3328
	step [163/251], loss=14.9831
	step [164/251], loss=16.8874
	step [165/251], loss=16.5483
	step [166/251], loss=16.1592
	step [167/251], loss=17.3750
	step [168/251], loss=18.0973
	step [169/251], loss=17.8840
	step [170/251], loss=15.2480
	step [171/251], loss=17.4844
	step [172/251], loss=17.2256
	step [173/251], loss=15.9650
	step [174/251], loss=16.9604
	step [175/251], loss=17.6820
	step [176/251], loss=16.5170
	step [177/251], loss=17.7424
	step [178/251], loss=16.5228
	step [179/251], loss=16.6447
	step [180/251], loss=17.7421
	step [181/251], loss=14.9196
	step [182/251], loss=17.4974
	step [183/251], loss=16.7299
	step [184/251], loss=17.0515
	step [185/251], loss=15.9408
	step [186/251], loss=17.4946
	step [187/251], loss=17.4142
	step [188/251], loss=16.0547
	step [189/251], loss=16.1319
	step [190/251], loss=15.5663
	step [191/251], loss=15.6799
	step [192/251], loss=15.9963
	step [193/251], loss=15.1427
	step [194/251], loss=15.6972
	step [195/251], loss=16.4840
	step [196/251], loss=16.1841
	step [197/251], loss=16.5481
	step [198/251], loss=17.5124
	step [199/251], loss=16.0799
	step [200/251], loss=15.2630
	step [201/251], loss=15.1532
	step [202/251], loss=16.9677
	step [203/251], loss=15.8757
	step [204/251], loss=15.8682
	step [205/251], loss=15.9472
	step [206/251], loss=17.6370
	step [207/251], loss=15.9359
	step [208/251], loss=17.0759
	step [209/251], loss=15.1804
	step [210/251], loss=17.2696
	step [211/251], loss=15.5070
	step [212/251], loss=17.5500
	step [213/251], loss=16.8708
	step [214/251], loss=17.0072
	step [215/251], loss=15.6075
	step [216/251], loss=15.6636
	step [217/251], loss=16.0664
	step [218/251], loss=16.1258
	step [219/251], loss=14.6393
	step [220/251], loss=14.9306
	step [221/251], loss=17.4362
	step [222/251], loss=14.7552
	step [223/251], loss=14.7050
	step [224/251], loss=15.7347
	step [225/251], loss=14.9103
	step [226/251], loss=15.5329
	step [227/251], loss=16.4929
	step [228/251], loss=15.0894
	step [229/251], loss=15.8035
	step [230/251], loss=16.2279
	step [231/251], loss=17.4824
	step [232/251], loss=15.1978
	step [233/251], loss=15.5907
	step [234/251], loss=16.7254
	step [235/251], loss=14.8084
	step [236/251], loss=14.9618
	step [237/251], loss=15.6125
	step [238/251], loss=15.0208
	step [239/251], loss=16.2294
	step [240/251], loss=18.0617
	step [241/251], loss=15.9455
	step [242/251], loss=15.4806
	step [243/251], loss=14.6231
	step [244/251], loss=16.8134
	step [245/251], loss=16.3381
	step [246/251], loss=14.5558
	step [247/251], loss=16.0326
	step [248/251], loss=15.0090
	step [249/251], loss=14.2443
	step [250/251], loss=14.7106
	step [251/251], loss=1.8925
	Evaluating
	loss=0.0681, precision=0.2112, recall=0.9964, f1=0.3485
saving model as: 2_saved_model.pth
Training epoch 6
	step [1/251], loss=15.7007
	step [2/251], loss=15.7806
	step [3/251], loss=15.2244
	step [4/251], loss=16.6992
	step [5/251], loss=16.6258
	step [6/251], loss=14.0410
	step [7/251], loss=14.9888
	step [8/251], loss=16.9518
	step [9/251], loss=14.3073
	step [10/251], loss=15.7756
	step [11/251], loss=15.8365
	step [12/251], loss=14.9757
	step [13/251], loss=16.1736
	step [14/251], loss=16.0829
	step [15/251], loss=17.2098
	step [16/251], loss=14.5075
	step [17/251], loss=16.1230
	step [18/251], loss=14.9554
	step [19/251], loss=14.3546
	step [20/251], loss=14.5675
	step [21/251], loss=14.8637
	step [22/251], loss=14.7249
	step [23/251], loss=15.5038
	step [24/251], loss=17.8981
	step [25/251], loss=13.9882
	step [26/251], loss=15.9683
	step [27/251], loss=15.8453
	step [28/251], loss=15.8989
	step [29/251], loss=15.3606
	step [30/251], loss=15.6114
	step [31/251], loss=15.0509
	step [32/251], loss=14.7500
	step [33/251], loss=14.9409
	step [34/251], loss=15.5224
	step [35/251], loss=14.7248
	step [36/251], loss=15.2115
	step [37/251], loss=15.5035
	step [38/251], loss=16.2021
	step [39/251], loss=15.3164
	step [40/251], loss=14.6426
	step [41/251], loss=15.7048
	step [42/251], loss=15.1217
	step [43/251], loss=14.2413
	step [44/251], loss=14.0091
	step [45/251], loss=14.3684
	step [46/251], loss=16.9512
	step [47/251], loss=13.7515
	step [48/251], loss=15.1097
	step [49/251], loss=14.3574
	step [50/251], loss=17.2370
	step [51/251], loss=13.7028
	step [52/251], loss=15.0657
	step [53/251], loss=14.0809
	step [54/251], loss=13.9287
	step [55/251], loss=13.6087
	step [56/251], loss=14.0388
	step [57/251], loss=16.0222
	step [58/251], loss=14.4717
	step [59/251], loss=14.2825
	step [60/251], loss=13.7592
	step [61/251], loss=14.5695
	step [62/251], loss=15.5346
	step [63/251], loss=13.2797
	step [64/251], loss=17.0949
	step [65/251], loss=15.1742
	step [66/251], loss=16.2817
	step [67/251], loss=14.9448
	step [68/251], loss=13.6211
	step [69/251], loss=14.5614
	step [70/251], loss=13.9912
	step [71/251], loss=15.9692
	step [72/251], loss=16.2804
	step [73/251], loss=13.9965
	step [74/251], loss=18.1510
	step [75/251], loss=12.7195
	step [76/251], loss=13.5107
	step [77/251], loss=17.7103
	step [78/251], loss=13.8882
	step [79/251], loss=15.7803
	step [80/251], loss=15.0505
	step [81/251], loss=15.9588
	step [82/251], loss=13.5176
	step [83/251], loss=16.6945
	step [84/251], loss=14.7071
	step [85/251], loss=12.9662
	step [86/251], loss=16.2271
	step [87/251], loss=16.7924
	step [88/251], loss=14.7594
	step [89/251], loss=14.6384
	step [90/251], loss=14.4157
	step [91/251], loss=14.9179
	step [92/251], loss=13.9507
	step [93/251], loss=14.8634
	step [94/251], loss=14.1946
	step [95/251], loss=13.6409
	step [96/251], loss=14.1157
	step [97/251], loss=14.3548
	step [98/251], loss=15.1962
	step [99/251], loss=15.1035
	step [100/251], loss=15.1992
	step [101/251], loss=14.5453
	step [102/251], loss=12.3677
	step [103/251], loss=15.6162
	step [104/251], loss=15.5013
	step [105/251], loss=13.9890
	step [106/251], loss=14.6981
	step [107/251], loss=12.8482
	step [108/251], loss=14.5309
	step [109/251], loss=14.7606
	step [110/251], loss=15.3671
	step [111/251], loss=13.7546
	step [112/251], loss=14.0406
	step [113/251], loss=14.8581
	step [114/251], loss=17.3516
	step [115/251], loss=13.7591
	step [116/251], loss=14.6229
	step [117/251], loss=14.7871
	step [118/251], loss=14.4902
	step [119/251], loss=14.2692
	step [120/251], loss=15.1572
	step [121/251], loss=15.2767
	step [122/251], loss=14.7729
	step [123/251], loss=14.7633
	step [124/251], loss=13.5021
	step [125/251], loss=14.4624
	step [126/251], loss=15.7007
	step [127/251], loss=13.6675
	step [128/251], loss=15.0856
	step [129/251], loss=13.0904
	step [130/251], loss=13.7840
	step [131/251], loss=14.2238
	step [132/251], loss=13.6136
	step [133/251], loss=13.0540
	step [134/251], loss=16.0089
	step [135/251], loss=16.8798
	step [136/251], loss=15.3651
	step [137/251], loss=14.8286
	step [138/251], loss=15.4303
	step [139/251], loss=14.1405
	step [140/251], loss=14.8628
	step [141/251], loss=13.8293
	step [142/251], loss=15.7411
	step [143/251], loss=13.0899
	step [144/251], loss=14.9356
	step [145/251], loss=15.1608
	step [146/251], loss=13.6541
	step [147/251], loss=13.4470
	step [148/251], loss=15.4009
	step [149/251], loss=13.5190
	step [150/251], loss=15.4707
	step [151/251], loss=12.2667
	step [152/251], loss=16.3372
	step [153/251], loss=15.1300
	step [154/251], loss=13.5082
	step [155/251], loss=14.5496
	step [156/251], loss=15.1914
	step [157/251], loss=15.7019
	step [158/251], loss=15.4374
	step [159/251], loss=14.6430
	step [160/251], loss=14.0764
	step [161/251], loss=12.8484
	step [162/251], loss=13.8424
	step [163/251], loss=14.2574
	step [164/251], loss=13.3829
	step [165/251], loss=12.9765
	step [166/251], loss=15.5482
	step [167/251], loss=12.1723
	step [168/251], loss=14.0029
	step [169/251], loss=14.4808
	step [170/251], loss=13.3827
	step [171/251], loss=14.9321
	step [172/251], loss=14.1177
	step [173/251], loss=14.9770
	step [174/251], loss=14.6481
	step [175/251], loss=16.0673
	step [176/251], loss=13.9473
	step [177/251], loss=15.9038
	step [178/251], loss=13.2266
	step [179/251], loss=16.9917
	step [180/251], loss=12.9746
	step [181/251], loss=12.9795
	step [182/251], loss=15.0758
	step [183/251], loss=16.1603
	step [184/251], loss=13.1628
	step [185/251], loss=14.0405
	step [186/251], loss=13.5222
	step [187/251], loss=13.7342
	step [188/251], loss=14.3126
	step [189/251], loss=12.7136
	step [190/251], loss=13.6075
	step [191/251], loss=14.3153
	step [192/251], loss=14.0615
	step [193/251], loss=12.9474
	step [194/251], loss=14.5947
	step [195/251], loss=15.0205
	step [196/251], loss=12.7824
	step [197/251], loss=14.0168
	step [198/251], loss=12.8162
	step [199/251], loss=15.0211
	step [200/251], loss=12.9046
	step [201/251], loss=13.5447
	step [202/251], loss=12.8632
	step [203/251], loss=12.7617
	step [204/251], loss=14.4255
	step [205/251], loss=12.9092
	step [206/251], loss=14.0472
	step [207/251], loss=15.1479
	step [208/251], loss=15.5859
	step [209/251], loss=14.1638
	step [210/251], loss=13.2236
	step [211/251], loss=14.6238
	step [212/251], loss=13.6611
	step [213/251], loss=15.1724
	step [214/251], loss=14.0738
	step [215/251], loss=14.8417
	step [216/251], loss=15.5997
	step [217/251], loss=12.8702
	step [218/251], loss=13.7563
	step [219/251], loss=11.8332
	step [220/251], loss=12.7566
	step [221/251], loss=13.7889
	step [222/251], loss=12.8721
	step [223/251], loss=15.3177
	step [224/251], loss=13.4093
	step [225/251], loss=12.6948
	step [226/251], loss=11.6441
	step [227/251], loss=12.6770
	step [228/251], loss=13.2316
	step [229/251], loss=14.6019
	step [230/251], loss=15.8528
	step [231/251], loss=13.6370
	step [232/251], loss=13.7599
	step [233/251], loss=12.8915
	step [234/251], loss=12.8260
	step [235/251], loss=14.4123
	step [236/251], loss=15.3759
	step [237/251], loss=15.9690
	step [238/251], loss=16.1296
	step [239/251], loss=13.0248
	step [240/251], loss=15.5581
	step [241/251], loss=14.8699
	step [242/251], loss=12.5336
	step [243/251], loss=11.3843
	step [244/251], loss=15.2330
	step [245/251], loss=13.1654
	step [246/251], loss=13.5984
	step [247/251], loss=12.2969
	step [248/251], loss=11.3222
	step [249/251], loss=13.6348
	step [250/251], loss=13.3217
	step [251/251], loss=2.5363
	Evaluating
	loss=0.0602, precision=0.1779, recall=0.9974, f1=0.3019
Training epoch 7
	step [1/251], loss=12.5741
	step [2/251], loss=14.4658
	step [3/251], loss=13.0562
	step [4/251], loss=15.5757
	step [5/251], loss=13.3490
	step [6/251], loss=13.6826
	step [7/251], loss=14.0906
	step [8/251], loss=15.3189
	step [9/251], loss=12.8955
	step [10/251], loss=12.0692
	step [11/251], loss=11.9881
	step [12/251], loss=11.9258
	step [13/251], loss=12.3988
	step [14/251], loss=14.5092
	step [15/251], loss=13.2968
	step [16/251], loss=11.5352
	step [17/251], loss=11.0270
	step [18/251], loss=12.7127
	step [19/251], loss=14.7832
	step [20/251], loss=13.2865
	step [21/251], loss=13.8052
	step [22/251], loss=13.3602
	step [23/251], loss=12.6626
	step [24/251], loss=14.2394
	step [25/251], loss=14.1789
	step [26/251], loss=12.4923
	step [27/251], loss=12.0741
	step [28/251], loss=14.4873
	step [29/251], loss=12.6605
	step [30/251], loss=14.3182
	step [31/251], loss=12.4755
	step [32/251], loss=14.0511
	step [33/251], loss=13.8058
	step [34/251], loss=12.6479
	step [35/251], loss=15.1473
	step [36/251], loss=11.5453
	step [37/251], loss=13.4435
	step [38/251], loss=11.7551
	step [39/251], loss=14.3725
	step [40/251], loss=12.8232
	step [41/251], loss=12.4325
	step [42/251], loss=12.6328
	step [43/251], loss=12.9722
	step [44/251], loss=17.8061
	step [45/251], loss=12.6851
	step [46/251], loss=12.9266
	step [47/251], loss=12.3305
	step [48/251], loss=11.3924
	step [49/251], loss=14.0742
	step [50/251], loss=14.7097
	step [51/251], loss=12.6063
	step [52/251], loss=12.7743
	step [53/251], loss=13.3356
	step [54/251], loss=12.6417
	step [55/251], loss=12.5225
	step [56/251], loss=12.2953
	step [57/251], loss=13.5112
	step [58/251], loss=13.1040
	step [59/251], loss=12.9446
	step [60/251], loss=13.8219
	step [61/251], loss=13.1665
	step [62/251], loss=14.9049
	step [63/251], loss=13.0676
	step [64/251], loss=13.2284
	step [65/251], loss=12.7612
	step [66/251], loss=14.1257
	step [67/251], loss=14.0959
	step [68/251], loss=12.3172
	step [69/251], loss=12.1249
	step [70/251], loss=12.2194
	step [71/251], loss=13.1190
	step [72/251], loss=11.7325
	step [73/251], loss=13.9353
	step [74/251], loss=15.5594
	step [75/251], loss=12.6967
	step [76/251], loss=12.8053
	step [77/251], loss=11.8267
	step [78/251], loss=12.8332
	step [79/251], loss=13.9807
	step [80/251], loss=13.3133
	step [81/251], loss=13.9980
	step [82/251], loss=14.2205
	step [83/251], loss=12.5308
	step [84/251], loss=11.6171
	step [85/251], loss=12.5136
	step [86/251], loss=13.6390
	step [87/251], loss=11.7954
	step [88/251], loss=13.2438
	step [89/251], loss=14.6256
	step [90/251], loss=13.4281
	step [91/251], loss=13.1680
	step [92/251], loss=13.4914
	step [93/251], loss=12.9426
	step [94/251], loss=11.6548
	step [95/251], loss=11.8206
	step [96/251], loss=11.3322
	step [97/251], loss=15.4042
	step [98/251], loss=12.1731
	step [99/251], loss=12.1516
	step [100/251], loss=12.9195
	step [101/251], loss=11.9267
	step [102/251], loss=14.2172
	step [103/251], loss=12.4017
	step [104/251], loss=12.1569
	step [105/251], loss=11.6218
	step [106/251], loss=11.8514
	step [107/251], loss=14.4717
	step [108/251], loss=12.2698
	step [109/251], loss=11.0736
	step [110/251], loss=13.5076
	step [111/251], loss=11.6684
	step [112/251], loss=12.8499
	step [113/251], loss=10.6049
	step [114/251], loss=12.6784
	step [115/251], loss=12.0558
	step [116/251], loss=15.1063
	step [117/251], loss=11.7556
	step [118/251], loss=11.7021
	step [119/251], loss=14.0606
	step [120/251], loss=11.7191
	step [121/251], loss=11.9351
	step [122/251], loss=12.9718
	step [123/251], loss=12.0839
	step [124/251], loss=14.7285
	step [125/251], loss=10.9462
	step [126/251], loss=13.6130
	step [127/251], loss=14.5140
	step [128/251], loss=13.9477
	step [129/251], loss=10.8429
	step [130/251], loss=12.3372
	step [131/251], loss=13.2710
	step [132/251], loss=12.4369
	step [133/251], loss=11.4219
	step [134/251], loss=12.0231
	step [135/251], loss=10.9619
	step [136/251], loss=12.7165
	step [137/251], loss=13.5390
	step [138/251], loss=13.3582
	step [139/251], loss=13.1297
	step [140/251], loss=13.1963
	step [141/251], loss=11.9153
	step [142/251], loss=12.4717
	step [143/251], loss=11.6482
	step [144/251], loss=12.0288
	step [145/251], loss=12.9937
	step [146/251], loss=14.0260
	step [147/251], loss=12.6606
	step [148/251], loss=11.6377
	step [149/251], loss=12.3650
	step [150/251], loss=11.0504
	step [151/251], loss=13.6386
	step [152/251], loss=12.2201
	step [153/251], loss=12.3776
	step [154/251], loss=12.9389
	step [155/251], loss=10.7285
	step [156/251], loss=14.4000
	step [157/251], loss=11.8149
	step [158/251], loss=11.7445
	step [159/251], loss=11.6487
	step [160/251], loss=12.2495
	step [161/251], loss=11.4251
	step [162/251], loss=12.7400
	step [163/251], loss=13.7776
	step [164/251], loss=11.9566
	step [165/251], loss=11.4887
	step [166/251], loss=9.1842
	step [167/251], loss=11.5218
	step [168/251], loss=12.3641
	step [169/251], loss=12.6381
	step [170/251], loss=12.2627
	step [171/251], loss=12.8767
	step [172/251], loss=12.3058
	step [173/251], loss=12.4261
	step [174/251], loss=12.9227
	step [175/251], loss=12.6546
	step [176/251], loss=10.1183
	step [177/251], loss=11.0692
	step [178/251], loss=13.9883
	step [179/251], loss=10.0239
	step [180/251], loss=11.7612
	step [181/251], loss=11.2930
	step [182/251], loss=12.7453
	step [183/251], loss=11.5361
	step [184/251], loss=11.7165
	step [185/251], loss=12.4096
	step [186/251], loss=11.3703
	step [187/251], loss=10.6340
	step [188/251], loss=13.3402
	step [189/251], loss=10.0942
	step [190/251], loss=11.2565
	step [191/251], loss=12.9005
	step [192/251], loss=15.6293
	step [193/251], loss=11.5893
	step [194/251], loss=11.5563
	step [195/251], loss=11.7220
	step [196/251], loss=12.0056
	step [197/251], loss=13.1394
	step [198/251], loss=10.7601
	step [199/251], loss=10.5223
	step [200/251], loss=10.4744
	step [201/251], loss=11.2009
	step [202/251], loss=12.7662
	step [203/251], loss=16.0115
	step [204/251], loss=10.7624
	step [205/251], loss=11.7725
	step [206/251], loss=11.6365
	step [207/251], loss=10.0383
	step [208/251], loss=12.9999
	step [209/251], loss=10.1254
	step [210/251], loss=9.9725
	step [211/251], loss=10.5651
	step [212/251], loss=12.2910
	step [213/251], loss=11.0636
	step [214/251], loss=11.7541
	step [215/251], loss=11.1649
	step [216/251], loss=12.1214
	step [217/251], loss=9.5937
	step [218/251], loss=11.6215
	step [219/251], loss=11.4613
	step [220/251], loss=10.6145
	step [221/251], loss=13.6997
	step [222/251], loss=13.5991
	step [223/251], loss=10.0019
	step [224/251], loss=9.9697
	step [225/251], loss=14.3660
	step [226/251], loss=13.1815
	step [227/251], loss=13.0905
	step [228/251], loss=10.8763
	step [229/251], loss=11.2570
	step [230/251], loss=10.8141
	step [231/251], loss=12.2713
	step [232/251], loss=9.9733
	step [233/251], loss=11.4569
	step [234/251], loss=13.0282
	step [235/251], loss=14.4651
	step [236/251], loss=12.1680
	step [237/251], loss=10.7122
	step [238/251], loss=12.4021
	step [239/251], loss=11.7296
	step [240/251], loss=11.4784
	step [241/251], loss=12.1079
	step [242/251], loss=11.1036
	step [243/251], loss=12.1904
	step [244/251], loss=12.6569
	step [245/251], loss=11.9163
	step [246/251], loss=11.0052
	step [247/251], loss=14.2472
	step [248/251], loss=10.8751
	step [249/251], loss=10.0315
	step [250/251], loss=11.4555
	step [251/251], loss=2.2180
	Evaluating
	loss=0.0530, precision=0.1771, recall=0.9974, f1=0.3008
Training epoch 8
	step [1/251], loss=11.4800
	step [2/251], loss=12.1794
	step [3/251], loss=13.6940
	step [4/251], loss=12.6215
	step [5/251], loss=11.4890
	step [6/251], loss=12.0538
	step [7/251], loss=12.0132
	step [8/251], loss=11.0880
	step [9/251], loss=11.8202
	step [10/251], loss=11.7467
	step [11/251], loss=11.4268
	step [12/251], loss=10.0563
	step [13/251], loss=11.1529
	step [14/251], loss=11.4638
	step [15/251], loss=11.6136
	step [16/251], loss=12.2908
	step [17/251], loss=10.9156
	step [18/251], loss=10.9903
	step [19/251], loss=12.8029
	step [20/251], loss=13.5144
	step [21/251], loss=10.4227
	step [22/251], loss=11.3533
	step [23/251], loss=8.8922
	step [24/251], loss=12.9306
	step [25/251], loss=13.3036
	step [26/251], loss=11.5320
	step [27/251], loss=10.9131
	step [28/251], loss=12.5994
	step [29/251], loss=12.6764
	step [30/251], loss=12.1538
	step [31/251], loss=11.4648
	step [32/251], loss=13.6791
	step [33/251], loss=13.5705
	step [34/251], loss=12.0781
	step [35/251], loss=10.3045
	step [36/251], loss=10.9517
	step [37/251], loss=11.1224
	step [38/251], loss=11.2851
	step [39/251], loss=12.0530
	step [40/251], loss=10.4699
	step [41/251], loss=10.2991
	step [42/251], loss=9.7647
	step [43/251], loss=12.0005
	step [44/251], loss=12.1548
	step [45/251], loss=8.8689
	step [46/251], loss=12.8930
	step [47/251], loss=11.0977
	step [48/251], loss=11.2097
	step [49/251], loss=12.1860
	step [50/251], loss=12.3789
	step [51/251], loss=11.0536
	step [52/251], loss=12.6378
	step [53/251], loss=9.9499
	step [54/251], loss=10.4425
	step [55/251], loss=10.7343
	step [56/251], loss=10.6426
	step [57/251], loss=11.8467
	step [58/251], loss=9.3983
	step [59/251], loss=12.8349
	step [60/251], loss=11.3993
	step [61/251], loss=11.6424
	step [62/251], loss=11.4747
	step [63/251], loss=13.3005
	step [64/251], loss=10.9209
	step [65/251], loss=13.7151
	step [66/251], loss=11.3215
	step [67/251], loss=8.9733
	step [68/251], loss=12.0140
	step [69/251], loss=10.6916
	step [70/251], loss=10.6164
	step [71/251], loss=12.1114
	step [72/251], loss=10.4175
	step [73/251], loss=11.7450
	step [74/251], loss=9.9778
	step [75/251], loss=8.7483
	step [76/251], loss=11.9897
	step [77/251], loss=10.4838
	step [78/251], loss=11.3668
	step [79/251], loss=10.2281
	step [80/251], loss=11.5249
	step [81/251], loss=14.1311
	step [82/251], loss=12.0299
	step [83/251], loss=13.1816
	step [84/251], loss=12.4241
	step [85/251], loss=11.3048
	step [86/251], loss=10.9633
	step [87/251], loss=12.1213
	step [88/251], loss=13.0540
	step [89/251], loss=10.4364
	step [90/251], loss=10.6687
	step [91/251], loss=9.2328
	step [92/251], loss=11.7741
	step [93/251], loss=10.9753
	step [94/251], loss=9.3532
	step [95/251], loss=11.7004
	step [96/251], loss=12.2554
	step [97/251], loss=11.3560
	step [98/251], loss=12.4708
	step [99/251], loss=12.0756
	step [100/251], loss=10.7505
	step [101/251], loss=11.1441
	step [102/251], loss=12.1477
	step [103/251], loss=11.8650
	step [104/251], loss=11.0511
	step [105/251], loss=13.1525
	step [106/251], loss=10.5497
	step [107/251], loss=11.1089
	step [108/251], loss=12.0197
	step [109/251], loss=11.7165
	step [110/251], loss=9.5111
	step [111/251], loss=10.4483
	step [112/251], loss=11.0375
	step [113/251], loss=8.4341
	step [114/251], loss=9.7785
	step [115/251], loss=12.4233
	step [116/251], loss=11.2765
	step [117/251], loss=12.4874
	step [118/251], loss=12.3255
	step [119/251], loss=12.2598
	step [120/251], loss=10.2456
	step [121/251], loss=10.5141
	step [122/251], loss=11.1580
	step [123/251], loss=9.8070
	step [124/251], loss=10.9567
	step [125/251], loss=10.1725
	step [126/251], loss=10.2417
	step [127/251], loss=10.4644
	step [128/251], loss=11.7101
	step [129/251], loss=12.5455
	step [130/251], loss=11.1769
	step [131/251], loss=10.0158
	step [132/251], loss=8.9736
	step [133/251], loss=9.5755
	step [134/251], loss=11.3408
	step [135/251], loss=10.4116
	step [136/251], loss=11.2579
	step [137/251], loss=10.4079
	step [138/251], loss=10.6446
	step [139/251], loss=14.3285
	step [140/251], loss=11.2643
	step [141/251], loss=11.8114
	step [142/251], loss=9.0143
	step [143/251], loss=11.2148
	step [144/251], loss=11.2236
	step [145/251], loss=10.3837
	step [146/251], loss=10.9255
	step [147/251], loss=11.9993
	step [148/251], loss=11.2792
	step [149/251], loss=14.0105
	step [150/251], loss=13.6456
	step [151/251], loss=10.0701
	step [152/251], loss=11.4238
	step [153/251], loss=8.9384
	step [154/251], loss=8.7062
	step [155/251], loss=8.5426
	step [156/251], loss=10.4145
	step [157/251], loss=10.9000
	step [158/251], loss=9.9478
	step [159/251], loss=12.1833
	step [160/251], loss=12.4778
	step [161/251], loss=10.5549
	step [162/251], loss=9.7348
	step [163/251], loss=11.9025
	step [164/251], loss=11.0989
	step [165/251], loss=12.6323
	step [166/251], loss=10.4149
	step [167/251], loss=13.8724
	step [168/251], loss=11.7036
	step [169/251], loss=9.8617
	step [170/251], loss=10.2266
	step [171/251], loss=10.3727
	step [172/251], loss=9.2049
	step [173/251], loss=12.5161
	step [174/251], loss=8.5465
	step [175/251], loss=9.2379
	step [176/251], loss=10.3747
	step [177/251], loss=11.5937
	step [178/251], loss=9.6362
	step [179/251], loss=11.4248
	step [180/251], loss=11.3066
	step [181/251], loss=12.6980
	step [182/251], loss=12.1619
	step [183/251], loss=10.9120
	step [184/251], loss=11.0513
	step [185/251], loss=9.9421
	step [186/251], loss=8.9949
	step [187/251], loss=10.7410
	step [188/251], loss=11.2091
	step [189/251], loss=9.7473
	step [190/251], loss=11.0867
	step [191/251], loss=13.6612
	step [192/251], loss=10.3404
	step [193/251], loss=11.0701
	step [194/251], loss=11.6625
	step [195/251], loss=12.8913
	step [196/251], loss=11.7530
	step [197/251], loss=10.6525
	step [198/251], loss=8.6080
	step [199/251], loss=10.9688
	step [200/251], loss=10.9333
	step [201/251], loss=13.5321
	step [202/251], loss=9.4880
	step [203/251], loss=12.5354
	step [204/251], loss=11.2339
	step [205/251], loss=10.1719
	step [206/251], loss=12.0546
	step [207/251], loss=9.0179
	step [208/251], loss=12.1679
	step [209/251], loss=9.7120
	step [210/251], loss=11.8131
	step [211/251], loss=9.2272
	step [212/251], loss=13.2083
	step [213/251], loss=9.2338
	step [214/251], loss=11.1104
	step [215/251], loss=7.1459
	step [216/251], loss=10.2335
	step [217/251], loss=10.8114
	step [218/251], loss=12.2957
	step [219/251], loss=10.8726
	step [220/251], loss=9.1598
	step [221/251], loss=10.7187
	step [222/251], loss=12.6157
	step [223/251], loss=11.3292
	step [224/251], loss=9.7251
	step [225/251], loss=8.9589
	step [226/251], loss=9.9380
	step [227/251], loss=10.5795
	step [228/251], loss=10.6454
	step [229/251], loss=10.0434
	step [230/251], loss=11.2090
	step [231/251], loss=9.5044
	step [232/251], loss=9.6080
	step [233/251], loss=10.3191
	step [234/251], loss=9.3641
	step [235/251], loss=11.2343
	step [236/251], loss=11.3215
	step [237/251], loss=12.1415
	step [238/251], loss=9.1122
	step [239/251], loss=9.9345
	step [240/251], loss=9.3963
	step [241/251], loss=9.6497
	step [242/251], loss=12.3571
	step [243/251], loss=8.9435
	step [244/251], loss=11.5917
	step [245/251], loss=9.6198
	step [246/251], loss=9.8790
	step [247/251], loss=10.0851
	step [248/251], loss=12.6051
	step [249/251], loss=11.8143
	step [250/251], loss=11.4537
	step [251/251], loss=1.3762
	Evaluating
	loss=0.0451, precision=0.1704, recall=0.9976, f1=0.2911
Training epoch 9
	step [1/251], loss=10.0310
	step [2/251], loss=10.1546
	step [3/251], loss=9.8429
	step [4/251], loss=8.5145
	step [5/251], loss=12.7670
	step [6/251], loss=10.6500
	step [7/251], loss=10.2962
	step [8/251], loss=13.8227
	step [9/251], loss=10.9498
	step [10/251], loss=9.1081
	step [11/251], loss=10.7087
	step [12/251], loss=9.1753
	step [13/251], loss=11.4030
	step [14/251], loss=10.9101
	step [15/251], loss=10.9865
	step [16/251], loss=11.2372
	step [17/251], loss=11.2264
	step [18/251], loss=10.7161
	step [19/251], loss=11.7713
	step [20/251], loss=10.0860
	step [21/251], loss=12.7823
	step [22/251], loss=11.2454
	step [23/251], loss=8.7086
	step [24/251], loss=9.7595
	step [25/251], loss=8.7240
	step [26/251], loss=10.3566
	step [27/251], loss=9.5233
	step [28/251], loss=9.4328
	step [29/251], loss=10.9287
	step [30/251], loss=11.0346
	step [31/251], loss=10.9209
	step [32/251], loss=9.5470
	step [33/251], loss=9.9233
	step [34/251], loss=10.9121
	step [35/251], loss=8.7686
	step [36/251], loss=10.2123
	step [37/251], loss=8.5701
	step [38/251], loss=10.4906
	step [39/251], loss=10.0506
	step [40/251], loss=10.0053
	step [41/251], loss=12.0209
	step [42/251], loss=11.2694
	step [43/251], loss=10.4777
	step [44/251], loss=9.1641
	step [45/251], loss=10.6026
	step [46/251], loss=10.0508
	step [47/251], loss=9.3541
	step [48/251], loss=8.9638
	step [49/251], loss=8.8794
	step [50/251], loss=11.6802
	step [51/251], loss=11.0144
	step [52/251], loss=11.0726
	step [53/251], loss=10.7459
	step [54/251], loss=9.3047
	step [55/251], loss=10.8457
	step [56/251], loss=9.8851
	step [57/251], loss=8.8832
	step [58/251], loss=10.6021
	step [59/251], loss=10.9110
	step [60/251], loss=7.7144
	step [61/251], loss=7.9162
	step [62/251], loss=8.6877
	step [63/251], loss=10.6397
	step [64/251], loss=9.8626
	step [65/251], loss=9.9196
	step [66/251], loss=12.4870
	step [67/251], loss=9.9705
	step [68/251], loss=10.3626
	step [69/251], loss=12.1896
	step [70/251], loss=11.4235
	step [71/251], loss=11.4045
	step [72/251], loss=10.8158
	step [73/251], loss=8.7731
	step [74/251], loss=11.9238
	step [75/251], loss=10.1115
	step [76/251], loss=12.4720
	step [77/251], loss=9.8851
	step [78/251], loss=10.3476
	step [79/251], loss=10.2925
	step [80/251], loss=9.8361
	step [81/251], loss=11.9318
	step [82/251], loss=8.7689
	step [83/251], loss=9.0827
	step [84/251], loss=9.2891
	step [85/251], loss=10.9526
	step [86/251], loss=11.4632
	step [87/251], loss=13.3358
	step [88/251], loss=10.2212
	step [89/251], loss=7.6516
	step [90/251], loss=9.4815
	step [91/251], loss=8.3861
	step [92/251], loss=9.9357
	step [93/251], loss=9.6138
	step [94/251], loss=12.8965
	step [95/251], loss=10.4184
	step [96/251], loss=10.9361
	step [97/251], loss=9.0554
	step [98/251], loss=11.0738
	step [99/251], loss=8.9047
	step [100/251], loss=9.4714
	step [101/251], loss=10.2509
	step [102/251], loss=11.7610
	step [103/251], loss=10.7294
	step [104/251], loss=11.8126
	step [105/251], loss=8.3633
	step [106/251], loss=8.8625
	step [107/251], loss=10.4758
	step [108/251], loss=11.2216
	step [109/251], loss=10.5222
	step [110/251], loss=8.7520
	step [111/251], loss=10.9332
	step [112/251], loss=8.4167
	step [113/251], loss=8.3286
	step [114/251], loss=9.0890
	step [115/251], loss=10.0816
	step [116/251], loss=11.3717
	step [117/251], loss=10.6598
	step [118/251], loss=9.6433
	step [119/251], loss=8.9371
	step [120/251], loss=10.1798
	step [121/251], loss=9.8267
	step [122/251], loss=8.6271
	step [123/251], loss=10.9344
	step [124/251], loss=11.7793
	step [125/251], loss=9.6870
	step [126/251], loss=9.5385
	step [127/251], loss=10.9279
	step [128/251], loss=10.0405
	step [129/251], loss=11.5620
	step [130/251], loss=9.7273
	step [131/251], loss=10.8618
	step [132/251], loss=9.5150
	step [133/251], loss=9.2362
	step [134/251], loss=10.2383
	step [135/251], loss=10.0158
	step [136/251], loss=9.5864
	step [137/251], loss=11.2027
	step [138/251], loss=11.7392
	step [139/251], loss=11.0236
	step [140/251], loss=9.4250
	step [141/251], loss=13.5168
	step [142/251], loss=11.1587
	step [143/251], loss=11.9881
	step [144/251], loss=8.8183
	step [145/251], loss=10.9098
	step [146/251], loss=11.3785
	step [147/251], loss=9.9739
	step [148/251], loss=10.8587
	step [149/251], loss=7.7263
	step [150/251], loss=9.1667
	step [151/251], loss=9.6381
	step [152/251], loss=9.9771
	step [153/251], loss=9.4681
	step [154/251], loss=10.7379
	step [155/251], loss=10.6061
	step [156/251], loss=11.1193
	step [157/251], loss=9.2538
	step [158/251], loss=10.3663
	step [159/251], loss=10.7594
	step [160/251], loss=9.1418
	step [161/251], loss=10.9154
	step [162/251], loss=9.0548
	step [163/251], loss=8.2644
	step [164/251], loss=9.0089
	step [165/251], loss=9.7116
	step [166/251], loss=10.1177
	step [167/251], loss=8.8829
	step [168/251], loss=9.9798
	step [169/251], loss=12.2382
	step [170/251], loss=9.5937
	step [171/251], loss=9.4711
	step [172/251], loss=10.0361
	step [173/251], loss=10.0227
	step [174/251], loss=8.3301
	step [175/251], loss=10.9070
	step [176/251], loss=9.0014
	step [177/251], loss=10.5300
	step [178/251], loss=10.5660
	step [179/251], loss=9.2444
	step [180/251], loss=8.7186
	step [181/251], loss=11.2475
	step [182/251], loss=9.3801
	step [183/251], loss=11.9539
	step [184/251], loss=8.7030
	step [185/251], loss=12.4881
	step [186/251], loss=9.7707
	step [187/251], loss=10.4412
	step [188/251], loss=11.4052
	step [189/251], loss=10.6229
	step [190/251], loss=11.5847
	step [191/251], loss=9.1421
	step [192/251], loss=9.7697
	step [193/251], loss=9.3111
	step [194/251], loss=10.9853
	step [195/251], loss=9.0897
	step [196/251], loss=10.1896
	step [197/251], loss=10.1899
	step [198/251], loss=8.9690
	step [199/251], loss=9.5069
	step [200/251], loss=8.1694
	step [201/251], loss=10.2660
	step [202/251], loss=9.8305
	step [203/251], loss=8.9482
	step [204/251], loss=9.4254
	step [205/251], loss=8.6319
	step [206/251], loss=9.7567
	step [207/251], loss=8.2767
	step [208/251], loss=9.2979
	step [209/251], loss=9.2267
	step [210/251], loss=8.4610
	step [211/251], loss=10.8579
	step [212/251], loss=11.6594
	step [213/251], loss=11.3504
	step [214/251], loss=7.9029
	step [215/251], loss=11.2031
	step [216/251], loss=9.5823
	step [217/251], loss=10.3386
	step [218/251], loss=9.0318
	step [219/251], loss=9.8075
	step [220/251], loss=10.2123
	step [221/251], loss=10.7328
	step [222/251], loss=9.1436
	step [223/251], loss=9.2371
	step [224/251], loss=9.7589
	step [225/251], loss=9.8357
	step [226/251], loss=11.0054
	step [227/251], loss=10.1648
	step [228/251], loss=8.6122
	step [229/251], loss=10.0319
	step [230/251], loss=10.8475
	step [231/251], loss=10.5043
	step [232/251], loss=10.7482
	step [233/251], loss=10.1120
	step [234/251], loss=8.9198
	step [235/251], loss=8.7896
	step [236/251], loss=9.3919
	step [237/251], loss=11.8670
	step [238/251], loss=8.1783
	step [239/251], loss=12.8957
	step [240/251], loss=10.9417
	step [241/251], loss=9.3103
	step [242/251], loss=9.1796
	step [243/251], loss=10.3705
	step [244/251], loss=13.1504
	step [245/251], loss=9.8747
	step [246/251], loss=9.2588
	step [247/251], loss=9.5807
	step [248/251], loss=9.3954
	step [249/251], loss=10.1941
	step [250/251], loss=9.7398
	step [251/251], loss=1.5823
	Evaluating
	loss=0.0350, precision=0.2070, recall=0.9967, f1=0.3428
Training epoch 10
	step [1/251], loss=12.8297
	step [2/251], loss=10.3050
	step [3/251], loss=8.9448
	step [4/251], loss=10.4713
	step [5/251], loss=9.6362
	step [6/251], loss=9.3331
	step [7/251], loss=8.5993
	step [8/251], loss=9.4722
	step [9/251], loss=10.9182
	step [10/251], loss=9.7588
	step [11/251], loss=10.3235
	step [12/251], loss=7.6702
	step [13/251], loss=10.4100
	step [14/251], loss=8.5591
	step [15/251], loss=8.6101
	step [16/251], loss=9.8597
	step [17/251], loss=9.7167
	step [18/251], loss=8.6230
	step [19/251], loss=12.1603
	step [20/251], loss=10.4983
	step [21/251], loss=11.2886
	step [22/251], loss=9.9049
	step [23/251], loss=8.5473
	step [24/251], loss=7.9902
	step [25/251], loss=11.9506
	step [26/251], loss=9.2825
	step [27/251], loss=10.5420
	step [28/251], loss=9.6366
	step [29/251], loss=8.4176
	step [30/251], loss=9.5507
	step [31/251], loss=10.2691
	step [32/251], loss=10.2502
	step [33/251], loss=9.3939
	step [34/251], loss=10.5800
	step [35/251], loss=7.9764
	step [36/251], loss=8.8541
	step [37/251], loss=9.0550
	step [38/251], loss=9.0798
	step [39/251], loss=8.5888
	step [40/251], loss=11.0119
	step [41/251], loss=10.3671
	step [42/251], loss=10.8155
	step [43/251], loss=8.8063
	step [44/251], loss=9.4054
	step [45/251], loss=9.4580
	step [46/251], loss=8.6577
	step [47/251], loss=10.7862
	step [48/251], loss=10.0726
	step [49/251], loss=9.8961
	step [50/251], loss=8.5443
	step [51/251], loss=8.6651
	step [52/251], loss=7.9399
	step [53/251], loss=9.4547
	step [54/251], loss=9.0501
	step [55/251], loss=12.2371
	step [56/251], loss=8.4706
	step [57/251], loss=11.7149
	step [58/251], loss=7.7267
	step [59/251], loss=10.7124
	step [60/251], loss=9.8371
	step [61/251], loss=8.9337
	step [62/251], loss=10.9219
	step [63/251], loss=8.3078
	step [64/251], loss=9.5505
	step [65/251], loss=8.6536
	step [66/251], loss=9.8658
	step [67/251], loss=9.7811
	step [68/251], loss=9.5356
	step [69/251], loss=8.7293
	step [70/251], loss=9.3061
	step [71/251], loss=10.4761
	step [72/251], loss=10.4897
	step [73/251], loss=9.8286
	step [74/251], loss=8.4886
	step [75/251], loss=9.0982
	step [76/251], loss=9.5699
	step [77/251], loss=7.3846
	step [78/251], loss=9.1963
	step [79/251], loss=10.2229
	step [80/251], loss=8.6943
	step [81/251], loss=9.7609
	step [82/251], loss=10.2115
	step [83/251], loss=9.5713
	step [84/251], loss=11.2621
	step [85/251], loss=9.2459
	step [86/251], loss=9.8944
	step [87/251], loss=10.4309
	step [88/251], loss=8.0239
	step [89/251], loss=12.6527
	step [90/251], loss=8.9222
	step [91/251], loss=9.2695
	step [92/251], loss=9.3175
	step [93/251], loss=9.7427
	step [94/251], loss=8.3601
	step [95/251], loss=11.4442
	step [96/251], loss=8.0162
	step [97/251], loss=10.6538
	step [98/251], loss=10.4165
	step [99/251], loss=8.4928
	step [100/251], loss=9.4661
	step [101/251], loss=10.2329
	step [102/251], loss=9.4266
	step [103/251], loss=9.9192
	step [104/251], loss=11.4931
	step [105/251], loss=8.4654
	step [106/251], loss=8.9114
	step [107/251], loss=8.8283
	step [108/251], loss=9.4245
	step [109/251], loss=8.7017
	step [110/251], loss=8.9587
	step [111/251], loss=10.1577
	step [112/251], loss=10.5180
	step [113/251], loss=12.3382
	step [114/251], loss=10.6047
	step [115/251], loss=9.4741
	step [116/251], loss=10.6670
	step [117/251], loss=8.4626
	step [118/251], loss=10.1400
	step [119/251], loss=9.4543
	step [120/251], loss=8.8787
	step [121/251], loss=9.5168
	step [122/251], loss=10.9088
	step [123/251], loss=9.1682
	step [124/251], loss=9.7313
	step [125/251], loss=8.4281
	step [126/251], loss=8.3947
	step [127/251], loss=8.8881
	step [128/251], loss=8.4257
	step [129/251], loss=8.9625
	step [130/251], loss=8.7216
	step [131/251], loss=8.9843
	step [132/251], loss=10.1659
	step [133/251], loss=12.1625
	step [134/251], loss=9.8864
	step [135/251], loss=7.5867
	step [136/251], loss=8.9563
	step [137/251], loss=9.6402
	step [138/251], loss=9.7722
	step [139/251], loss=10.3894
	step [140/251], loss=8.0032
	step [141/251], loss=7.9367
	step [142/251], loss=8.9672
	step [143/251], loss=8.3674
	step [144/251], loss=7.8341
	step [145/251], loss=8.0559
	step [146/251], loss=9.1568
	step [147/251], loss=12.2580
	step [148/251], loss=7.5338
	step [149/251], loss=9.5943
	step [150/251], loss=8.9683
	step [151/251], loss=10.8471
	step [152/251], loss=10.8641
	step [153/251], loss=9.0133
	step [154/251], loss=10.0091
	step [155/251], loss=9.3315
	step [156/251], loss=9.1931
	step [157/251], loss=8.1578
	step [158/251], loss=9.6537
	step [159/251], loss=9.2713
	step [160/251], loss=9.5930
	step [161/251], loss=9.4546
	step [162/251], loss=10.9331
	step [163/251], loss=12.5048
	step [164/251], loss=8.2247
	step [165/251], loss=10.1204
	step [166/251], loss=8.2340
	step [167/251], loss=7.8057
	step [168/251], loss=8.8055
	step [169/251], loss=8.6329
	step [170/251], loss=8.3356
	step [171/251], loss=9.0973
	step [172/251], loss=8.6826
	step [173/251], loss=9.3117
	step [174/251], loss=8.7397
	step [175/251], loss=8.4424
	step [176/251], loss=8.9338
	step [177/251], loss=10.7652
	step [178/251], loss=9.3139
	step [179/251], loss=10.7033
	step [180/251], loss=9.6896
	step [181/251], loss=8.3907
	step [182/251], loss=11.1495
	step [183/251], loss=8.4898
	step [184/251], loss=9.5681
	step [185/251], loss=8.9204
	step [186/251], loss=9.6028
	step [187/251], loss=9.0262
	step [188/251], loss=10.3391
	step [189/251], loss=10.2178
	step [190/251], loss=6.9712
	step [191/251], loss=8.7421
	step [192/251], loss=9.7151
	step [193/251], loss=10.2087
	step [194/251], loss=8.5141
	step [195/251], loss=7.6217
	step [196/251], loss=9.0776
	step [197/251], loss=8.3539
	step [198/251], loss=9.4729
	step [199/251], loss=10.2403
	step [200/251], loss=7.1065
	step [201/251], loss=7.6253
	step [202/251], loss=9.6575
	step [203/251], loss=7.6159
	step [204/251], loss=7.2636
	step [205/251], loss=11.7173
	step [206/251], loss=7.8731
	step [207/251], loss=9.5310
	step [208/251], loss=8.8519
	step [209/251], loss=7.9091
	step [210/251], loss=7.9391
	step [211/251], loss=9.3838
	step [212/251], loss=8.5283
	step [213/251], loss=8.8801
	step [214/251], loss=10.9433
	step [215/251], loss=9.5227
	step [216/251], loss=8.8689
	step [217/251], loss=8.7987
	step [218/251], loss=9.5734
	step [219/251], loss=9.5532
	step [220/251], loss=10.2383
	step [221/251], loss=7.6813
	step [222/251], loss=9.7739
	step [223/251], loss=11.0006
	step [224/251], loss=13.3958
	step [225/251], loss=8.8952
	step [226/251], loss=8.5803
	step [227/251], loss=7.7761
	step [228/251], loss=10.4268
	step [229/251], loss=8.6090
	step [230/251], loss=10.1209
	step [231/251], loss=8.5850
	step [232/251], loss=8.7355
	step [233/251], loss=8.9050
	step [234/251], loss=9.1218
	step [235/251], loss=9.1687
	step [236/251], loss=9.4196
	step [237/251], loss=11.5472
	step [238/251], loss=8.5740
	step [239/251], loss=9.0866
	step [240/251], loss=8.9725
	step [241/251], loss=11.2753
	step [242/251], loss=8.0864
	step [243/251], loss=8.1820
	step [244/251], loss=10.5222
	step [245/251], loss=11.5346
	step [246/251], loss=10.3059
	step [247/251], loss=10.9773
	step [248/251], loss=9.1247
	step [249/251], loss=9.8444
	step [250/251], loss=8.6726
	step [251/251], loss=1.2414
	Evaluating
	loss=0.0323, precision=0.2047, recall=0.9965, f1=0.3397
Training epoch 11
	step [1/251], loss=11.4638
	step [2/251], loss=9.1344
	step [3/251], loss=10.3024
	step [4/251], loss=8.4804
	step [5/251], loss=8.2978
	step [6/251], loss=7.8715
	step [7/251], loss=10.0958
	step [8/251], loss=9.8462
	step [9/251], loss=8.5662
	step [10/251], loss=9.3249
	step [11/251], loss=7.2070
	step [12/251], loss=11.5913
	step [13/251], loss=10.3550
	step [14/251], loss=9.9415
	step [15/251], loss=8.6272
	step [16/251], loss=8.0707
	step [17/251], loss=7.6750
	step [18/251], loss=8.3880
	step [19/251], loss=10.0605
	step [20/251], loss=9.5294
	step [21/251], loss=7.8936
	step [22/251], loss=8.8376
	step [23/251], loss=8.8266
	step [24/251], loss=8.2645
	step [25/251], loss=9.1553
	step [26/251], loss=8.4587
	step [27/251], loss=9.5041
	step [28/251], loss=8.3022
	step [29/251], loss=8.2637
	step [30/251], loss=9.7406
	step [31/251], loss=8.3178
	step [32/251], loss=7.6144
	step [33/251], loss=10.2014
	step [34/251], loss=7.4991
	step [35/251], loss=10.9711
	step [36/251], loss=8.8265
	step [37/251], loss=9.0539
	step [38/251], loss=10.3330
	step [39/251], loss=10.1081
	step [40/251], loss=9.4661
	step [41/251], loss=7.9676
	step [42/251], loss=8.2210
	step [43/251], loss=7.1593
	step [44/251], loss=8.4592
	step [45/251], loss=9.3669
	step [46/251], loss=11.6451
	step [47/251], loss=9.4675
	step [48/251], loss=9.4032
	step [49/251], loss=9.5333
	step [50/251], loss=10.2633
	step [51/251], loss=10.6447
	step [52/251], loss=9.4631
	step [53/251], loss=10.1801
	step [54/251], loss=7.2914
	step [55/251], loss=10.7172
	step [56/251], loss=8.5030
	step [57/251], loss=9.5626
	step [58/251], loss=7.3043
	step [59/251], loss=9.3610
	step [60/251], loss=9.0017
	step [61/251], loss=8.3281
	step [62/251], loss=8.2348
	step [63/251], loss=10.3002
	step [64/251], loss=8.9944
	step [65/251], loss=8.8225
	step [66/251], loss=9.3056
	step [67/251], loss=8.3344
	step [68/251], loss=8.2670
	step [69/251], loss=8.4216
	step [70/251], loss=9.3324
	step [71/251], loss=9.4103
	step [72/251], loss=8.4945
	step [73/251], loss=8.6202
	step [74/251], loss=9.0945
	step [75/251], loss=10.2126
	step [76/251], loss=8.7683
	step [77/251], loss=10.6656
	step [78/251], loss=10.2086
	step [79/251], loss=9.6717
	step [80/251], loss=8.4144
	step [81/251], loss=8.0674
	step [82/251], loss=6.8176
	step [83/251], loss=8.0580
	step [84/251], loss=9.8045
	step [85/251], loss=9.3520
	step [86/251], loss=9.8179
	step [87/251], loss=8.5479
	step [88/251], loss=7.4821
	step [89/251], loss=8.3494
	step [90/251], loss=11.0120
	step [91/251], loss=9.3786
	step [92/251], loss=8.1151
	step [93/251], loss=7.3539
	step [94/251], loss=9.6221
	step [95/251], loss=8.6387
	step [96/251], loss=11.4569
	step [97/251], loss=8.4167
	step [98/251], loss=10.2402
	step [99/251], loss=9.7321
	step [100/251], loss=7.3501
	step [101/251], loss=8.5608
	step [102/251], loss=9.5467
	step [103/251], loss=8.4879
	step [104/251], loss=9.4324
	step [105/251], loss=10.3188
	step [106/251], loss=7.8395
	step [107/251], loss=7.9561
	step [108/251], loss=11.3863
	step [109/251], loss=5.9655
	step [110/251], loss=8.1978
	step [111/251], loss=10.6681
	step [112/251], loss=9.6463
	step [113/251], loss=9.5094
	step [114/251], loss=6.0518
	step [115/251], loss=9.0592
	step [116/251], loss=9.4268
	step [117/251], loss=7.0239
	step [118/251], loss=8.4257
	step [119/251], loss=6.9935
	step [120/251], loss=8.2934
	step [121/251], loss=7.9169
	step [122/251], loss=8.1572
	step [123/251], loss=8.9687
	step [124/251], loss=10.1524
	step [125/251], loss=9.0553
	step [126/251], loss=9.0030
	step [127/251], loss=9.1329
	step [128/251], loss=11.3107
	step [129/251], loss=8.3511
	step [130/251], loss=8.0606
	step [131/251], loss=7.8528
	step [132/251], loss=9.4708
	step [133/251], loss=8.5346
	step [134/251], loss=12.1416
	step [135/251], loss=9.7984
	step [136/251], loss=7.4022
	step [137/251], loss=8.6423
	step [138/251], loss=7.8956
	step [139/251], loss=8.9457
	step [140/251], loss=8.6294
	step [141/251], loss=8.9996
	step [142/251], loss=7.8661
	step [143/251], loss=11.1284
	step [144/251], loss=10.1140
	step [145/251], loss=9.5898
	step [146/251], loss=8.8278
	step [147/251], loss=9.4659
	step [148/251], loss=9.3079
	step [149/251], loss=9.4530
	step [150/251], loss=8.1231
	step [151/251], loss=8.3711
	step [152/251], loss=8.3942
	step [153/251], loss=10.1845
	step [154/251], loss=8.5239
	step [155/251], loss=10.4226
	step [156/251], loss=10.3927
	step [157/251], loss=8.5964
	step [158/251], loss=7.8404
	step [159/251], loss=7.9716
	step [160/251], loss=7.8531
	step [161/251], loss=8.5695
	step [162/251], loss=10.2308
	step [163/251], loss=7.3710
	step [164/251], loss=8.0274
	step [165/251], loss=9.1019
	step [166/251], loss=8.0651
	step [167/251], loss=9.3184
	step [168/251], loss=8.6915
	step [169/251], loss=8.5177
	step [170/251], loss=8.4881
	step [171/251], loss=11.9246
	step [172/251], loss=6.9776
	step [173/251], loss=8.7288
	step [174/251], loss=8.3371
	step [175/251], loss=10.2206
	step [176/251], loss=9.3373
	step [177/251], loss=8.6134
	step [178/251], loss=7.9931
	step [179/251], loss=8.4358
	step [180/251], loss=9.7285
	step [181/251], loss=8.1525
	step [182/251], loss=9.3046
	step [183/251], loss=7.0730
	step [184/251], loss=8.1244
	step [185/251], loss=7.7324
	step [186/251], loss=8.1385
	step [187/251], loss=7.9879
	step [188/251], loss=9.7180
	step [189/251], loss=7.5413
	step [190/251], loss=8.1613
	step [191/251], loss=9.1120
	step [192/251], loss=8.9094
	step [193/251], loss=10.0019
	step [194/251], loss=10.6901
	step [195/251], loss=10.5269
	step [196/251], loss=8.8928
	step [197/251], loss=7.9390
	step [198/251], loss=9.1211
	step [199/251], loss=12.0349
	step [200/251], loss=8.5956
	step [201/251], loss=9.0818
	step [202/251], loss=8.4645
	step [203/251], loss=8.6713
	step [204/251], loss=7.1888
	step [205/251], loss=8.8127
	step [206/251], loss=9.7596
	step [207/251], loss=8.1391
	step [208/251], loss=9.0801
	step [209/251], loss=9.9623
	step [210/251], loss=7.8231
	step [211/251], loss=8.3542
	step [212/251], loss=9.2980
	step [213/251], loss=7.6251
	step [214/251], loss=7.6335
	step [215/251], loss=8.5122
	step [216/251], loss=8.4341
	step [217/251], loss=8.3702
	step [218/251], loss=8.4080
	step [219/251], loss=10.0064
	step [220/251], loss=11.6168
	step [221/251], loss=9.0944
	step [222/251], loss=8.8460
	step [223/251], loss=8.6379
	step [224/251], loss=7.4951
	step [225/251], loss=9.6508
	step [226/251], loss=8.9621
	step [227/251], loss=6.8077
	step [228/251], loss=7.2883
	step [229/251], loss=7.6635
	step [230/251], loss=7.5527
	step [231/251], loss=7.9943
	step [232/251], loss=9.2304
	step [233/251], loss=11.0133
	step [234/251], loss=9.5437
	step [235/251], loss=9.5084
	step [236/251], loss=9.5292
	step [237/251], loss=9.2473
	step [238/251], loss=8.5726
	step [239/251], loss=9.8233
	step [240/251], loss=9.4388
	step [241/251], loss=8.3615
	step [242/251], loss=7.9666
	step [243/251], loss=8.7368
	step [244/251], loss=8.4759
	step [245/251], loss=9.4159
	step [246/251], loss=10.8400
	step [247/251], loss=8.0106
	step [248/251], loss=9.1655
	step [249/251], loss=8.3681
	step [250/251], loss=9.3108
	step [251/251], loss=1.4699
	Evaluating
	loss=0.0304, precision=0.2170, recall=0.9964, f1=0.3564
saving model as: 2_saved_model.pth
Training epoch 12
	step [1/251], loss=8.6955
	step [2/251], loss=9.4025
	step [3/251], loss=10.7130
	step [4/251], loss=8.5396
	step [5/251], loss=9.6247
	step [6/251], loss=7.2567
	step [7/251], loss=6.8220
	step [8/251], loss=7.3509
	step [9/251], loss=7.2658
	step [10/251], loss=8.5076
	step [11/251], loss=9.3564
	step [12/251], loss=9.4747
	step [13/251], loss=7.5703
	step [14/251], loss=11.9954
	step [15/251], loss=10.7918
	step [16/251], loss=8.3196
	step [17/251], loss=9.2374
	step [18/251], loss=8.5145
	step [19/251], loss=8.7225
	step [20/251], loss=7.6562
	step [21/251], loss=8.4410
	step [22/251], loss=9.1202
	step [23/251], loss=7.7138
	step [24/251], loss=8.8052
	step [25/251], loss=8.0409
	step [26/251], loss=7.8683
	step [27/251], loss=7.6747
	step [28/251], loss=7.9607
	step [29/251], loss=7.6753
	step [30/251], loss=7.4993
	step [31/251], loss=8.8646
	step [32/251], loss=7.2246
	step [33/251], loss=8.8319
	step [34/251], loss=8.5955
	step [35/251], loss=7.0126
	step [36/251], loss=8.9008
	step [37/251], loss=8.7302
	step [38/251], loss=9.9634
	step [39/251], loss=9.2107
	step [40/251], loss=8.2028
	step [41/251], loss=9.0616
	step [42/251], loss=7.1512
	step [43/251], loss=8.0709
	step [44/251], loss=7.5072
	step [45/251], loss=8.4101
	step [46/251], loss=7.7873
	step [47/251], loss=6.3861
	step [48/251], loss=8.0707
	step [49/251], loss=8.0390
	step [50/251], loss=8.7705
	step [51/251], loss=8.9332
	step [52/251], loss=8.1762
	step [53/251], loss=8.3441
	step [54/251], loss=10.2476
	step [55/251], loss=8.8369
	step [56/251], loss=9.4598
	step [57/251], loss=8.4018
	step [58/251], loss=9.3766
	step [59/251], loss=8.7311
	step [60/251], loss=8.4893
	step [61/251], loss=9.3821
	step [62/251], loss=8.6183
	step [63/251], loss=9.5423
	step [64/251], loss=9.9553
	step [65/251], loss=9.5763
	step [66/251], loss=8.1391
	step [67/251], loss=7.5889
	step [68/251], loss=8.9213
	step [69/251], loss=9.0737
	step [70/251], loss=8.4220
	step [71/251], loss=9.1693
	step [72/251], loss=8.7106
	step [73/251], loss=9.7116
	step [74/251], loss=7.1680
	step [75/251], loss=9.3482
	step [76/251], loss=7.3156
	step [77/251], loss=10.9546
	step [78/251], loss=10.0408
	step [79/251], loss=8.2629
	step [80/251], loss=7.1871
	step [81/251], loss=9.5701
	step [82/251], loss=9.4474
	step [83/251], loss=7.7847
	step [84/251], loss=8.3018
	step [85/251], loss=6.9574
	step [86/251], loss=8.0440
	step [87/251], loss=9.0331
	step [88/251], loss=10.1115
	step [89/251], loss=6.9946
	step [90/251], loss=9.1352
	step [91/251], loss=8.9490
	step [92/251], loss=10.4907
	step [93/251], loss=9.3840
	step [94/251], loss=6.8062
	step [95/251], loss=7.9577
	step [96/251], loss=7.8573
	step [97/251], loss=11.4836
	step [98/251], loss=12.1512
	step [99/251], loss=9.9737
	step [100/251], loss=6.8933
	step [101/251], loss=7.8869
	step [102/251], loss=7.7454
	step [103/251], loss=9.4772
	step [104/251], loss=8.4516
	step [105/251], loss=9.6381
	step [106/251], loss=7.5992
	step [107/251], loss=6.1093
	step [108/251], loss=9.6705
	step [109/251], loss=8.7571
	step [110/251], loss=6.9823
	step [111/251], loss=9.6274
	step [112/251], loss=10.4771
	step [113/251], loss=9.2525
	step [114/251], loss=9.6673
	step [115/251], loss=11.5289
	step [116/251], loss=7.8880
	step [117/251], loss=8.7528
	step [118/251], loss=8.2465
	step [119/251], loss=9.0614
	step [120/251], loss=8.5108
	step [121/251], loss=9.8002
	step [122/251], loss=8.8662
	step [123/251], loss=8.3124
	step [124/251], loss=8.7257
	step [125/251], loss=8.1405
	step [126/251], loss=9.6390
	step [127/251], loss=9.5248
	step [128/251], loss=11.1868
	step [129/251], loss=7.9116
	step [130/251], loss=7.2294
	step [131/251], loss=7.6815
	step [132/251], loss=8.3566
	step [133/251], loss=8.4331
	step [134/251], loss=9.1639
	step [135/251], loss=8.3999
	step [136/251], loss=9.1317
	step [137/251], loss=10.8944
	step [138/251], loss=7.7327
	step [139/251], loss=9.1889
	step [140/251], loss=7.8852
	step [141/251], loss=6.6046
	step [142/251], loss=7.6665
	step [143/251], loss=6.7719
	step [144/251], loss=8.2380
	step [145/251], loss=8.2093
	step [146/251], loss=8.7303
	step [147/251], loss=7.5325
	step [148/251], loss=8.4420
	step [149/251], loss=7.8297
	step [150/251], loss=6.8477
	step [151/251], loss=7.9757
	step [152/251], loss=8.6242
	step [153/251], loss=9.5786
	step [154/251], loss=8.5634
	step [155/251], loss=8.5785
	step [156/251], loss=9.1470
	step [157/251], loss=6.9702
	step [158/251], loss=7.8955
	step [159/251], loss=9.4449
	step [160/251], loss=8.6904
	step [161/251], loss=7.6964
	step [162/251], loss=8.9556
	step [163/251], loss=7.8659
	step [164/251], loss=9.1558
	step [165/251], loss=7.5467
	step [166/251], loss=8.6158
	step [167/251], loss=8.4542
	step [168/251], loss=7.9594
	step [169/251], loss=10.5601
	step [170/251], loss=8.5330
	step [171/251], loss=9.3726
	step [172/251], loss=7.4214
	step [173/251], loss=6.8579
	step [174/251], loss=8.1470
	step [175/251], loss=7.4546
	step [176/251], loss=6.5069
	step [177/251], loss=9.1447
	step [178/251], loss=9.3779
	step [179/251], loss=9.8320
	step [180/251], loss=7.9072
	step [181/251], loss=8.3035
	step [182/251], loss=7.5750
	step [183/251], loss=6.0962
	step [184/251], loss=7.2041
	step [185/251], loss=10.5127
	step [186/251], loss=8.4059
	step [187/251], loss=7.6969
	step [188/251], loss=10.0739
	step [189/251], loss=8.1570
	step [190/251], loss=8.3509
	step [191/251], loss=8.1864
	step [192/251], loss=8.8846
	step [193/251], loss=9.9401
	step [194/251], loss=7.3928
	step [195/251], loss=7.5365
	step [196/251], loss=8.0082
	step [197/251], loss=7.5357
	step [198/251], loss=10.1974
	step [199/251], loss=9.8950
	step [200/251], loss=8.8790
	step [201/251], loss=9.6669
	step [202/251], loss=6.1201
	step [203/251], loss=7.3855
	step [204/251], loss=10.0620
	step [205/251], loss=7.2220
	step [206/251], loss=9.0537
	step [207/251], loss=8.3347
	step [208/251], loss=10.6542
	step [209/251], loss=6.6951
	step [210/251], loss=7.6263
	step [211/251], loss=7.9006
	step [212/251], loss=8.9257
	step [213/251], loss=9.2291
	step [214/251], loss=9.2909
	step [215/251], loss=7.4231
	step [216/251], loss=8.2253
	step [217/251], loss=9.8684
	step [218/251], loss=7.3901
	step [219/251], loss=8.8778
	step [220/251], loss=7.9275
	step [221/251], loss=7.1402
	step [222/251], loss=9.2839
	step [223/251], loss=7.1918
	step [224/251], loss=6.6242
	step [225/251], loss=8.9504
	step [226/251], loss=10.8279
	step [227/251], loss=8.2328
	step [228/251], loss=7.2731
	step [229/251], loss=6.1181
	step [230/251], loss=8.0567
	step [231/251], loss=8.1996
	step [232/251], loss=7.6805
	step [233/251], loss=8.3666
	step [234/251], loss=8.4165
	step [235/251], loss=7.7365
	step [236/251], loss=10.0768
	step [237/251], loss=9.4715
	step [238/251], loss=9.3755
	step [239/251], loss=7.9973
	step [240/251], loss=7.6692
	step [241/251], loss=9.2670
	step [242/251], loss=7.8325
	step [243/251], loss=8.5509
	step [244/251], loss=8.7011
	step [245/251], loss=8.0744
	step [246/251], loss=7.6227
	step [247/251], loss=7.7852
	step [248/251], loss=8.6281
	step [249/251], loss=9.3424
	step [250/251], loss=8.6357
	step [251/251], loss=1.6443
	Evaluating
	loss=0.0354, precision=0.1742, recall=0.9976, f1=0.2966
Training epoch 13
	step [1/251], loss=8.4232
	step [2/251], loss=7.0193
	step [3/251], loss=10.0325
	step [4/251], loss=7.2806
	step [5/251], loss=7.4512
	step [6/251], loss=8.7849
	step [7/251], loss=7.6690
	step [8/251], loss=9.0694
	step [9/251], loss=8.2127
	step [10/251], loss=9.2973
	step [11/251], loss=8.2267
	step [12/251], loss=7.1328
	step [13/251], loss=9.8939
	step [14/251], loss=9.4744
	step [15/251], loss=10.7862
	step [16/251], loss=7.9622
	step [17/251], loss=11.0326
	step [18/251], loss=8.7481
	step [19/251], loss=10.0619
	step [20/251], loss=7.5028
	step [21/251], loss=10.6035
	step [22/251], loss=7.7934
	step [23/251], loss=7.9477
	step [24/251], loss=7.0783
	step [25/251], loss=6.9568
	step [26/251], loss=10.7750
	step [27/251], loss=7.5891
	step [28/251], loss=8.3411
	step [29/251], loss=9.3124
	step [30/251], loss=8.1878
	step [31/251], loss=7.0605
	step [32/251], loss=8.0283
	step [33/251], loss=7.8435
	step [34/251], loss=8.7420
	step [35/251], loss=7.9094
	step [36/251], loss=8.4068
	step [37/251], loss=7.7820
	step [38/251], loss=8.5917
	step [39/251], loss=7.5786
	step [40/251], loss=9.2509
	step [41/251], loss=7.8582
	step [42/251], loss=9.2910
	step [43/251], loss=10.3553
	step [44/251], loss=8.8928
	step [45/251], loss=8.0922
	step [46/251], loss=8.6641
	step [47/251], loss=7.9003
	step [48/251], loss=8.0257
	step [49/251], loss=7.2104
	step [50/251], loss=7.8810
	step [51/251], loss=8.9256
	step [52/251], loss=7.2560
	step [53/251], loss=7.1857
	step [54/251], loss=8.3292
	step [55/251], loss=7.4263
	step [56/251], loss=6.5046
	step [57/251], loss=7.9150
	step [58/251], loss=9.0420
	step [59/251], loss=7.3944
	step [60/251], loss=7.3751
	step [61/251], loss=7.9293
	step [62/251], loss=9.3448
	step [63/251], loss=7.4416
	step [64/251], loss=12.6189
	step [65/251], loss=8.0844
	step [66/251], loss=7.2990
	step [67/251], loss=7.1929
	step [68/251], loss=9.0185
	step [69/251], loss=8.8753
	step [70/251], loss=7.9020
	step [71/251], loss=7.9371
	step [72/251], loss=7.8727
	step [73/251], loss=7.7677
	step [74/251], loss=7.3129
	step [75/251], loss=7.9534
	step [76/251], loss=7.6092
	step [77/251], loss=7.9798
	step [78/251], loss=7.7555
	step [79/251], loss=8.8756
	step [80/251], loss=8.0959
	step [81/251], loss=8.9771
	step [82/251], loss=6.8029
	step [83/251], loss=10.1756
	step [84/251], loss=7.6834
	step [85/251], loss=8.2676
	step [86/251], loss=8.6494
	step [87/251], loss=7.2719
	step [88/251], loss=7.4328
	step [89/251], loss=10.6960
	step [90/251], loss=7.6839
	step [91/251], loss=6.9427
	step [92/251], loss=12.8782
	step [93/251], loss=7.4626
	step [94/251], loss=7.6238
	step [95/251], loss=8.1109
	step [96/251], loss=7.9714
	step [97/251], loss=8.3134
	step [98/251], loss=7.2907
	step [99/251], loss=8.6187
	step [100/251], loss=10.0678
	step [101/251], loss=7.7993
	step [102/251], loss=6.7757
	step [103/251], loss=7.9516
	step [104/251], loss=9.0809
	step [105/251], loss=10.3080
	step [106/251], loss=7.2002
	step [107/251], loss=11.3416
	step [108/251], loss=8.7876
	step [109/251], loss=7.7319
	step [110/251], loss=8.5890
	step [111/251], loss=9.3622
	step [112/251], loss=8.2164
	step [113/251], loss=6.8324
	step [114/251], loss=10.4545
	step [115/251], loss=9.5106
	step [116/251], loss=8.2919
	step [117/251], loss=7.8547
	step [118/251], loss=8.4854
	step [119/251], loss=8.9794
	step [120/251], loss=7.0766
	step [121/251], loss=6.6877
	step [122/251], loss=7.8046
	step [123/251], loss=8.8037
	step [124/251], loss=7.8981
	step [125/251], loss=8.0219
	step [126/251], loss=7.5646
	step [127/251], loss=8.1857
	step [128/251], loss=7.5628
	step [129/251], loss=8.3503
	step [130/251], loss=8.4259
	step [131/251], loss=8.5660
	step [132/251], loss=8.7646
	step [133/251], loss=8.1445
	step [134/251], loss=7.3140
	step [135/251], loss=9.1022
	step [136/251], loss=8.7422
	step [137/251], loss=8.1704
	step [138/251], loss=8.9535
	step [139/251], loss=8.7618
	step [140/251], loss=7.2214
	step [141/251], loss=8.4436
	step [142/251], loss=9.2465
	step [143/251], loss=8.6683
	step [144/251], loss=7.4732
	step [145/251], loss=7.3102
	step [146/251], loss=7.3190
	step [147/251], loss=8.2310
	step [148/251], loss=8.7771
	step [149/251], loss=6.2706
	step [150/251], loss=6.3601
	step [151/251], loss=9.0255
	step [152/251], loss=7.3111
	step [153/251], loss=8.4786
	step [154/251], loss=6.8486
	step [155/251], loss=9.0934
	step [156/251], loss=7.9294
	step [157/251], loss=8.6457
	step [158/251], loss=8.6108
	step [159/251], loss=6.5598
	step [160/251], loss=8.4381
	step [161/251], loss=7.1052
	step [162/251], loss=8.0431
	step [163/251], loss=7.9346
	step [164/251], loss=7.9649
	step [165/251], loss=8.0584
	step [166/251], loss=7.7868
	step [167/251], loss=7.9372
	step [168/251], loss=8.5699
	step [169/251], loss=7.7015
	step [170/251], loss=9.4312
	step [171/251], loss=7.2549
	step [172/251], loss=9.3081
	step [173/251], loss=7.3332
	step [174/251], loss=9.4582
	step [175/251], loss=9.7518
	step [176/251], loss=9.5999
	step [177/251], loss=7.5635
	step [178/251], loss=9.9081
	step [179/251], loss=6.1566
	step [180/251], loss=8.5397
	step [181/251], loss=7.4078
	step [182/251], loss=8.1573
	step [183/251], loss=7.1266
	step [184/251], loss=6.8937
	step [185/251], loss=7.3174
	step [186/251], loss=8.2331
	step [187/251], loss=6.7288
	step [188/251], loss=7.9997
	step [189/251], loss=10.3994
	step [190/251], loss=7.6848
	step [191/251], loss=9.8729
	step [192/251], loss=7.4077
	step [193/251], loss=10.6097
	step [194/251], loss=6.6710
	step [195/251], loss=9.3056
	step [196/251], loss=6.9099
	step [197/251], loss=8.2855
	step [198/251], loss=8.6332
	step [199/251], loss=6.9646
	step [200/251], loss=8.4941
	step [201/251], loss=11.2858
	step [202/251], loss=8.3387
	step [203/251], loss=7.6763
	step [204/251], loss=8.5779
	step [205/251], loss=7.4307
	step [206/251], loss=7.0230
	step [207/251], loss=7.5872
	step [208/251], loss=7.9534
	step [209/251], loss=7.9869
	step [210/251], loss=8.4404
	step [211/251], loss=8.4644
	step [212/251], loss=7.0734
	step [213/251], loss=8.4882
	step [214/251], loss=6.9427
	step [215/251], loss=8.0565
	step [216/251], loss=8.2461
	step [217/251], loss=9.2489
	step [218/251], loss=8.6802
	step [219/251], loss=7.5569
	step [220/251], loss=8.2813
	step [221/251], loss=6.5738
	step [222/251], loss=7.5454
	step [223/251], loss=5.3410
	step [224/251], loss=7.4337
	step [225/251], loss=7.1472
	step [226/251], loss=6.5259
	step [227/251], loss=7.1645
	step [228/251], loss=11.2193
	step [229/251], loss=6.5374
	step [230/251], loss=9.3005
	step [231/251], loss=9.1441
	step [232/251], loss=7.2366
	step [233/251], loss=9.5598
	step [234/251], loss=7.2755
	step [235/251], loss=8.7878
	step [236/251], loss=8.7178
	step [237/251], loss=8.4462
	step [238/251], loss=8.8580
	step [239/251], loss=8.4999
	step [240/251], loss=11.1622
	step [241/251], loss=8.0845
	step [242/251], loss=7.3834
	step [243/251], loss=6.9460
	step [244/251], loss=7.9306
	step [245/251], loss=8.7099
	step [246/251], loss=7.2906
	step [247/251], loss=6.9745
	step [248/251], loss=9.8192
	step [249/251], loss=8.1744
	step [250/251], loss=6.7593
	step [251/251], loss=1.2770
	Evaluating
	loss=0.0303, precision=0.1903, recall=0.9973, f1=0.3196
Training epoch 14
	step [1/251], loss=7.9058
	step [2/251], loss=9.4668
	step [3/251], loss=8.8232
	step [4/251], loss=7.4405
	step [5/251], loss=8.5562
	step [6/251], loss=8.0857
	step [7/251], loss=8.6112
	step [8/251], loss=8.8408
	step [9/251], loss=7.7707
	step [10/251], loss=7.1557
	step [11/251], loss=8.4152
	step [12/251], loss=9.0608
	step [13/251], loss=9.2516
	step [14/251], loss=9.6182
	step [15/251], loss=7.6613
	step [16/251], loss=10.5781
	step [17/251], loss=8.0812
	step [18/251], loss=7.4115
	step [19/251], loss=7.7100
	step [20/251], loss=7.5200
	step [21/251], loss=7.7963
	step [22/251], loss=6.0943
	step [23/251], loss=7.7705
	step [24/251], loss=7.5166
	step [25/251], loss=8.0872
	step [26/251], loss=7.7574
	step [27/251], loss=8.9078
	step [28/251], loss=7.7044
	step [29/251], loss=7.5154
	step [30/251], loss=8.3143
	step [31/251], loss=8.2753
	step [32/251], loss=8.3588
	step [33/251], loss=8.9765
	step [34/251], loss=9.9380
	step [35/251], loss=7.4549
	step [36/251], loss=8.9340
	step [37/251], loss=9.5663
	step [38/251], loss=8.1614
	step [39/251], loss=7.7547
	step [40/251], loss=7.6113
	step [41/251], loss=6.1433
	step [42/251], loss=8.4261
	step [43/251], loss=7.6788
	step [44/251], loss=8.9161
	step [45/251], loss=7.4710
	step [46/251], loss=9.5988
	step [47/251], loss=7.9248
	step [48/251], loss=6.8004
	step [49/251], loss=7.0478
	step [50/251], loss=7.7970
	step [51/251], loss=7.8499
	step [52/251], loss=7.9306
	step [53/251], loss=6.5224
	step [54/251], loss=7.3632
	step [55/251], loss=7.3754
	step [56/251], loss=8.2088
	step [57/251], loss=6.8882
	step [58/251], loss=9.5642
	step [59/251], loss=9.2594
	step [60/251], loss=7.8919
	step [61/251], loss=7.1603
	step [62/251], loss=8.3345
	step [63/251], loss=9.6318
	step [64/251], loss=9.0531
	step [65/251], loss=9.6514
	step [66/251], loss=8.1555
	step [67/251], loss=8.0824
	step [68/251], loss=5.7241
	step [69/251], loss=6.9786
	step [70/251], loss=8.7221
	step [71/251], loss=6.9661
	step [72/251], loss=10.8621
	step [73/251], loss=8.5212
	step [74/251], loss=8.0483
	step [75/251], loss=8.2407
	step [76/251], loss=9.5658
	step [77/251], loss=9.1631
	step [78/251], loss=9.5290
	step [79/251], loss=9.2974
	step [80/251], loss=9.2142
	step [81/251], loss=6.6831
	step [82/251], loss=7.0577
	step [83/251], loss=6.8450
	step [84/251], loss=9.2211
	step [85/251], loss=7.8164
	step [86/251], loss=6.9147
	step [87/251], loss=6.9522
	step [88/251], loss=8.8236
	step [89/251], loss=7.5428
	step [90/251], loss=8.1185
	step [91/251], loss=5.7489
	step [92/251], loss=7.6798
	step [93/251], loss=8.4294
	step [94/251], loss=6.5536
	step [95/251], loss=7.7655
	step [96/251], loss=5.2226
	step [97/251], loss=8.8878
	step [98/251], loss=9.0304
	step [99/251], loss=10.3850
	step [100/251], loss=8.7990
	step [101/251], loss=8.1476
	step [102/251], loss=8.5589
	step [103/251], loss=8.1285
	step [104/251], loss=9.0671
	step [105/251], loss=8.0291
	step [106/251], loss=7.1649
	step [107/251], loss=9.0183
	step [108/251], loss=7.5048
	step [109/251], loss=9.7116
	step [110/251], loss=6.6526
	step [111/251], loss=7.6280
	step [112/251], loss=7.5956
	step [113/251], loss=8.5081
	step [114/251], loss=9.5663
	step [115/251], loss=8.4008
	step [116/251], loss=7.6183
	step [117/251], loss=8.1171
	step [118/251], loss=8.7791
	step [119/251], loss=7.2463
	step [120/251], loss=9.1470
	step [121/251], loss=6.2255
	step [122/251], loss=7.0014
	step [123/251], loss=6.8455
	step [124/251], loss=7.7197
	step [125/251], loss=7.3335
	step [126/251], loss=6.6787
	step [127/251], loss=7.9475
	step [128/251], loss=8.2362
	step [129/251], loss=6.9903
	step [130/251], loss=7.3605
	step [131/251], loss=5.6767
	step [132/251], loss=6.7597
	step [133/251], loss=7.5433
	step [134/251], loss=8.5954
	step [135/251], loss=9.4402
	step [136/251], loss=8.7982
	step [137/251], loss=8.7758
	step [138/251], loss=5.7763
	step [139/251], loss=7.7532
	step [140/251], loss=7.3986
	step [141/251], loss=7.3120
	step [142/251], loss=6.1229
	step [143/251], loss=8.7853
	step [144/251], loss=8.9830
	step [145/251], loss=7.2264
	step [146/251], loss=6.3895
	step [147/251], loss=7.3990
	step [148/251], loss=8.3812
	step [149/251], loss=7.8488
	step [150/251], loss=9.4417
	step [151/251], loss=6.7258
	step [152/251], loss=7.8322
	step [153/251], loss=9.6368
	step [154/251], loss=8.9309
	step [155/251], loss=7.4412
	step [156/251], loss=5.9267
	step [157/251], loss=7.6597
	step [158/251], loss=8.3528
	step [159/251], loss=8.1575
	step [160/251], loss=7.8365
	step [161/251], loss=8.7386
	step [162/251], loss=8.0600
	step [163/251], loss=7.9866
	step [164/251], loss=8.0547
	step [165/251], loss=8.6438
	step [166/251], loss=7.0578
	step [167/251], loss=9.8447
	step [168/251], loss=7.5845
	step [169/251], loss=7.8104
	step [170/251], loss=8.4774
	step [171/251], loss=6.7980
	step [172/251], loss=7.6503
	step [173/251], loss=5.9193
	step [174/251], loss=6.3322
	step [175/251], loss=9.2422
	step [176/251], loss=6.9652
	step [177/251], loss=8.3582
	step [178/251], loss=10.0570
	step [179/251], loss=7.5840
	step [180/251], loss=6.5099
	step [181/251], loss=8.7010
	step [182/251], loss=7.2407
	step [183/251], loss=7.4986
	step [184/251], loss=8.1743
	step [185/251], loss=7.8286
	step [186/251], loss=9.7661
	step [187/251], loss=8.6169
	step [188/251], loss=8.4528
	step [189/251], loss=6.5972
	step [190/251], loss=7.7001
	step [191/251], loss=7.3160
	step [192/251], loss=5.8568
	step [193/251], loss=6.2765
	step [194/251], loss=7.9106
	step [195/251], loss=7.5388
	step [196/251], loss=6.4333
	step [197/251], loss=8.6146
	step [198/251], loss=7.3719
	step [199/251], loss=8.5408
	step [200/251], loss=9.6128
	step [201/251], loss=7.4805
	step [202/251], loss=6.0877
	step [203/251], loss=6.8217
	step [204/251], loss=7.9752
	step [205/251], loss=7.6545
	step [206/251], loss=6.9548
	step [207/251], loss=6.9708
	step [208/251], loss=7.8204
	step [209/251], loss=7.3684
	step [210/251], loss=6.8681
	step [211/251], loss=8.4975
	step [212/251], loss=7.8434
	step [213/251], loss=7.6624
	step [214/251], loss=9.4102
	step [215/251], loss=6.8232
	step [216/251], loss=6.7463
	step [217/251], loss=8.5582
	step [218/251], loss=8.0557
	step [219/251], loss=7.5232
	step [220/251], loss=8.6591
	step [221/251], loss=9.4770
	step [222/251], loss=6.9916
	step [223/251], loss=8.6585
	step [224/251], loss=9.9875
	step [225/251], loss=7.1585
	step [226/251], loss=6.8343
	step [227/251], loss=8.6304
	step [228/251], loss=8.1736
	step [229/251], loss=7.5241
	step [230/251], loss=9.7240
	step [231/251], loss=6.3338
	step [232/251], loss=8.6966
	step [233/251], loss=8.5605
	step [234/251], loss=8.2298
	step [235/251], loss=6.9027
	step [236/251], loss=9.7399
	step [237/251], loss=8.5395
	step [238/251], loss=6.9841
	step [239/251], loss=8.2277
	step [240/251], loss=8.6526
	step [241/251], loss=8.6234
	step [242/251], loss=7.5527
	step [243/251], loss=7.5471
	step [244/251], loss=8.7776
	step [245/251], loss=8.2263
	step [246/251], loss=6.7756
	step [247/251], loss=7.1052
	step [248/251], loss=8.1034
	step [249/251], loss=7.3542
	step [250/251], loss=7.6826
	step [251/251], loss=1.9291
	Evaluating
	loss=0.0290, precision=0.1985, recall=0.9971, f1=0.3311
Training epoch 15
	step [1/251], loss=6.3336
	step [2/251], loss=8.7563
	step [3/251], loss=6.5196
	step [4/251], loss=8.8175
	step [5/251], loss=6.5558
	step [6/251], loss=8.9006
	step [7/251], loss=7.3462
	step [8/251], loss=7.4160
	step [9/251], loss=5.5010
	step [10/251], loss=8.0411
	step [11/251], loss=7.4489
	step [12/251], loss=9.2486
	step [13/251], loss=5.9768
	step [14/251], loss=7.5616
	step [15/251], loss=7.0851
	step [16/251], loss=8.5199
	step [17/251], loss=8.5948
	step [18/251], loss=8.0356
	step [19/251], loss=7.5914
	step [20/251], loss=7.2447
	step [21/251], loss=9.8968
	step [22/251], loss=8.9427
	step [23/251], loss=7.9565
	step [24/251], loss=6.5069
	step [25/251], loss=8.7487
	step [26/251], loss=8.4755
	step [27/251], loss=7.4078
	step [28/251], loss=8.1843
	step [29/251], loss=7.8827
	step [30/251], loss=9.8867
	step [31/251], loss=8.5584
	step [32/251], loss=5.9463
	step [33/251], loss=7.4450
	step [34/251], loss=8.2207
	step [35/251], loss=9.5561
	step [36/251], loss=7.5013
	step [37/251], loss=7.7863
	step [38/251], loss=7.8038
	step [39/251], loss=7.1197
	step [40/251], loss=7.5203
	step [41/251], loss=9.2087
	step [42/251], loss=8.0207
	step [43/251], loss=6.6403
	step [44/251], loss=8.6703
	step [45/251], loss=7.1114
	step [46/251], loss=7.8686
	step [47/251], loss=7.8920
	step [48/251], loss=8.4720
	step [49/251], loss=9.1668
	step [50/251], loss=6.6553
	step [51/251], loss=7.3403
	step [52/251], loss=8.1310
	step [53/251], loss=8.6798
	step [54/251], loss=5.8018
	step [55/251], loss=7.4475
	step [56/251], loss=7.1560
	step [57/251], loss=8.4292
	step [58/251], loss=8.1860
	step [59/251], loss=6.8981
	step [60/251], loss=5.9968
	step [61/251], loss=7.5620
	step [62/251], loss=6.6260
	step [63/251], loss=7.3917
	step [64/251], loss=9.5230
	step [65/251], loss=7.6287
	step [66/251], loss=9.0544
	step [67/251], loss=8.5887
	step [68/251], loss=6.9956
	step [69/251], loss=7.4313
	step [70/251], loss=7.9312
	step [71/251], loss=7.4165
	step [72/251], loss=6.6295
	step [73/251], loss=7.0810
	step [74/251], loss=8.1248
	step [75/251], loss=6.2559
	step [76/251], loss=7.0625
	step [77/251], loss=6.6421
	step [78/251], loss=8.0867
	step [79/251], loss=8.2767
	step [80/251], loss=6.9731
	step [81/251], loss=9.3926
	step [82/251], loss=7.1785
	step [83/251], loss=7.1541
	step [84/251], loss=9.4237
	step [85/251], loss=5.4517
	step [86/251], loss=8.4085
	step [87/251], loss=5.8145
	step [88/251], loss=7.9815
	step [89/251], loss=7.3647
	step [90/251], loss=7.2035
	step [91/251], loss=9.2071
	step [92/251], loss=8.5793
	step [93/251], loss=7.1818
	step [94/251], loss=7.9168
	step [95/251], loss=8.5200
	step [96/251], loss=9.2604
	step [97/251], loss=8.6784
	step [98/251], loss=8.6268
	step [99/251], loss=7.0853
	step [100/251], loss=7.7257
	step [101/251], loss=8.0663
	step [102/251], loss=9.1571
	step [103/251], loss=9.1322
	step [104/251], loss=6.4682
	step [105/251], loss=6.4384
	step [106/251], loss=7.3391
	step [107/251], loss=9.4204
	step [108/251], loss=6.7082
	step [109/251], loss=8.8003
	step [110/251], loss=7.8135
	step [111/251], loss=6.2292
	step [112/251], loss=9.5155
	step [113/251], loss=9.5942
	step [114/251], loss=7.0978
	step [115/251], loss=7.4746
	step [116/251], loss=6.4615
	step [117/251], loss=10.1689
	step [118/251], loss=6.6972
	step [119/251], loss=7.5950
	step [120/251], loss=6.7838
	step [121/251], loss=6.2263
	step [122/251], loss=6.6690
	step [123/251], loss=7.1053
	step [124/251], loss=6.4193
	step [125/251], loss=7.3287
	step [126/251], loss=5.9396
	step [127/251], loss=9.5568
	step [128/251], loss=7.1795
	step [129/251], loss=7.3849
	step [130/251], loss=8.4689
	step [131/251], loss=9.3166
	step [132/251], loss=8.7736
	step [133/251], loss=6.4012
	step [134/251], loss=7.6770
	step [135/251], loss=9.1865
	step [136/251], loss=7.1884
	step [137/251], loss=9.9288
	step [138/251], loss=8.0835
	step [139/251], loss=6.8567
	step [140/251], loss=7.7467
	step [141/251], loss=7.3956
	step [142/251], loss=6.5998
	step [143/251], loss=7.0324
	step [144/251], loss=9.2215
	step [145/251], loss=8.9407
	step [146/251], loss=7.6090
	step [147/251], loss=9.7608
	step [148/251], loss=8.5095
	step [149/251], loss=8.1705
	step [150/251], loss=7.0078
	step [151/251], loss=9.9428
	step [152/251], loss=9.0731
	step [153/251], loss=6.7722
	step [154/251], loss=8.6953
	step [155/251], loss=7.1988
	step [156/251], loss=7.8273
	step [157/251], loss=6.8856
	step [158/251], loss=7.7172
	step [159/251], loss=8.3237
	step [160/251], loss=8.2556
	step [161/251], loss=7.1491
	step [162/251], loss=6.6104
	step [163/251], loss=7.3585
	step [164/251], loss=8.5013
	step [165/251], loss=7.3177
	step [166/251], loss=7.6697
	step [167/251], loss=7.9341
	step [168/251], loss=7.3100
	step [169/251], loss=8.5695
	step [170/251], loss=7.6883
	step [171/251], loss=6.6011
	step [172/251], loss=7.6178
	step [173/251], loss=7.1088
	step [174/251], loss=6.9461
	step [175/251], loss=7.5591
	step [176/251], loss=6.4776
	step [177/251], loss=7.7329
	step [178/251], loss=6.8869
	step [179/251], loss=6.0220
	step [180/251], loss=7.8290
	step [181/251], loss=8.2429
	step [182/251], loss=7.5004
	step [183/251], loss=7.5017
	step [184/251], loss=7.4548
	step [185/251], loss=6.3198
	step [186/251], loss=6.9017
	step [187/251], loss=6.3777
	step [188/251], loss=8.6935
	step [189/251], loss=7.8412
	step [190/251], loss=8.2410
	step [191/251], loss=7.0891
	step [192/251], loss=5.9882
	step [193/251], loss=9.3044
	step [194/251], loss=8.2555
	step [195/251], loss=6.5152
	step [196/251], loss=8.7651
	step [197/251], loss=8.4456
	step [198/251], loss=6.8746
	step [199/251], loss=8.2478
	step [200/251], loss=7.9684
	step [201/251], loss=7.1917
	step [202/251], loss=7.4348
	step [203/251], loss=7.0420
	step [204/251], loss=8.1464
	step [205/251], loss=6.6017
	step [206/251], loss=8.0406
	step [207/251], loss=7.9872
	step [208/251], loss=9.0481
	step [209/251], loss=8.5710
	step [210/251], loss=6.6836
	step [211/251], loss=6.2038
	step [212/251], loss=6.7567
	step [213/251], loss=11.0408
	step [214/251], loss=6.5416
	step [215/251], loss=7.2746
	step [216/251], loss=7.4587
	step [217/251], loss=7.6367
	step [218/251], loss=8.2235
	step [219/251], loss=7.0723
	step [220/251], loss=8.0948
	step [221/251], loss=7.2362
	step [222/251], loss=5.6679
	step [223/251], loss=10.0885
	step [224/251], loss=8.2831
	step [225/251], loss=7.8890
	step [226/251], loss=7.2640
	step [227/251], loss=7.1389
	step [228/251], loss=7.4154
	step [229/251], loss=9.9714
	step [230/251], loss=8.4481
	step [231/251], loss=7.4421
	step [232/251], loss=7.6757
	step [233/251], loss=8.2929
	step [234/251], loss=7.1030
	step [235/251], loss=9.1701
	step [236/251], loss=6.6996
	step [237/251], loss=6.4538
	step [238/251], loss=9.2852
	step [239/251], loss=7.5854
	step [240/251], loss=6.7293
	step [241/251], loss=7.9404
	step [242/251], loss=7.6542
	step [243/251], loss=8.0119
	step [244/251], loss=6.5043
	step [245/251], loss=7.0650
	step [246/251], loss=7.6473
	step [247/251], loss=7.9381
	step [248/251], loss=6.3653
	step [249/251], loss=7.1617
	step [250/251], loss=8.3293
	step [251/251], loss=1.0806
	Evaluating
	loss=0.0258, precision=0.2196, recall=0.9963, f1=0.3599
saving model as: 2_saved_model.pth
Training epoch 16
	step [1/251], loss=6.1745
	step [2/251], loss=6.1417
	step [3/251], loss=7.7773
	step [4/251], loss=7.1997
	step [5/251], loss=7.4664
	step [6/251], loss=5.9936
	step [7/251], loss=9.0583
	step [8/251], loss=6.9638
	step [9/251], loss=8.1669
	step [10/251], loss=6.5609
	step [11/251], loss=7.3293
	step [12/251], loss=6.6628
	step [13/251], loss=8.5819
	step [14/251], loss=6.7875
	step [15/251], loss=7.5936
	step [16/251], loss=8.9650
	step [17/251], loss=8.5562
	step [18/251], loss=7.3017
	step [19/251], loss=7.4803
	step [20/251], loss=8.5105
	step [21/251], loss=7.7705
	step [22/251], loss=7.7026
	step [23/251], loss=7.3132
	step [24/251], loss=7.4045
	step [25/251], loss=7.6439
	step [26/251], loss=8.9149
	step [27/251], loss=7.2451
	step [28/251], loss=8.6618
	step [29/251], loss=8.0982
	step [30/251], loss=7.0939
	step [31/251], loss=9.4603
	step [32/251], loss=9.1406
	step [33/251], loss=7.7712
	step [34/251], loss=8.7032
	step [35/251], loss=7.1445
	step [36/251], loss=8.2340
	step [37/251], loss=7.4701
	step [38/251], loss=7.5364
	step [39/251], loss=6.3997
	step [40/251], loss=8.8453
	step [41/251], loss=8.8483
	step [42/251], loss=7.3506
	step [43/251], loss=7.6099
	step [44/251], loss=7.3943
	step [45/251], loss=7.4594
	step [46/251], loss=7.6591
	step [47/251], loss=11.5774
	step [48/251], loss=6.4234
	step [49/251], loss=7.7535
	step [50/251], loss=8.4129
	step [51/251], loss=6.9118
	step [52/251], loss=7.7342
	step [53/251], loss=8.5215
	step [54/251], loss=7.7684
	step [55/251], loss=7.7679
	step [56/251], loss=8.2218
	step [57/251], loss=7.1492
	step [58/251], loss=8.0786
	step [59/251], loss=9.7994
	step [60/251], loss=8.4150
	step [61/251], loss=7.2011
	step [62/251], loss=5.9713
	step [63/251], loss=6.5584
	step [64/251], loss=8.9771
	step [65/251], loss=7.7081
	step [66/251], loss=8.2116
	step [67/251], loss=8.4849
	step [68/251], loss=7.5599
	step [69/251], loss=8.0417
	step [70/251], loss=7.3636
	step [71/251], loss=9.0098
	step [72/251], loss=7.8035
	step [73/251], loss=6.1099
	step [74/251], loss=8.9648
	step [75/251], loss=7.7520
	step [76/251], loss=6.8120
	step [77/251], loss=7.9932
	step [78/251], loss=7.3890
	step [79/251], loss=8.5029
	step [80/251], loss=9.2196
	step [81/251], loss=8.8904
	step [82/251], loss=7.1238
	step [83/251], loss=8.9678
	step [84/251], loss=6.2205
	step [85/251], loss=7.4907
	step [86/251], loss=7.3015
	step [87/251], loss=7.3010
	step [88/251], loss=6.9254
	step [89/251], loss=5.7464
	step [90/251], loss=7.1661
	step [91/251], loss=8.5837
	step [92/251], loss=8.6240
	step [93/251], loss=6.9105
	step [94/251], loss=6.3816
	step [95/251], loss=7.7588
	step [96/251], loss=6.1410
	step [97/251], loss=9.0960
	step [98/251], loss=8.3694
	step [99/251], loss=6.6799
	step [100/251], loss=9.0897
	step [101/251], loss=7.5636
	step [102/251], loss=8.3271
	step [103/251], loss=6.8484
	step [104/251], loss=6.4194
	step [105/251], loss=8.3691
	step [106/251], loss=7.6242
	step [107/251], loss=8.1853
	step [108/251], loss=5.5697
	step [109/251], loss=8.7117
	step [110/251], loss=6.9941
	step [111/251], loss=8.0675
	step [112/251], loss=6.1330
	step [113/251], loss=7.6708
	step [114/251], loss=7.9233
	step [115/251], loss=6.7202
	step [116/251], loss=6.4402
	step [117/251], loss=6.7012
	step [118/251], loss=7.2371
	step [119/251], loss=6.3067
	step [120/251], loss=7.1515
	step [121/251], loss=7.8957
	step [122/251], loss=5.9893
	step [123/251], loss=9.6384
	step [124/251], loss=6.3496
	step [125/251], loss=6.1019
	step [126/251], loss=8.8215
	step [127/251], loss=8.1610
	step [128/251], loss=10.6266
	step [129/251], loss=6.2149
	step [130/251], loss=8.2022
	step [131/251], loss=6.9784
	step [132/251], loss=6.9715
	step [133/251], loss=6.9945
	step [134/251], loss=7.2444
	step [135/251], loss=6.4555
	step [136/251], loss=8.3175
	step [137/251], loss=7.0438
	step [138/251], loss=8.9793
	step [139/251], loss=6.9753
	step [140/251], loss=6.6291
	step [141/251], loss=7.5036
	step [142/251], loss=7.0760
	step [143/251], loss=6.4889
	step [144/251], loss=6.6409
	step [145/251], loss=6.6446
	step [146/251], loss=8.6756
	step [147/251], loss=9.3308
	step [148/251], loss=6.3446
	step [149/251], loss=6.5085
	step [150/251], loss=8.1915
	step [151/251], loss=7.3274
	step [152/251], loss=6.6834
	step [153/251], loss=7.1351
	step [154/251], loss=6.1238
	step [155/251], loss=6.6590
	step [156/251], loss=9.2478
	step [157/251], loss=10.3143
	step [158/251], loss=7.8765
	step [159/251], loss=7.5396
	step [160/251], loss=7.2051
	step [161/251], loss=7.2101
	step [162/251], loss=7.2538
	step [163/251], loss=6.5954
	step [164/251], loss=7.4466
	step [165/251], loss=7.4250
	step [166/251], loss=7.6912
	step [167/251], loss=7.2548
	step [168/251], loss=7.2750
	step [169/251], loss=6.4036
	step [170/251], loss=6.3748
	step [171/251], loss=8.3112
	step [172/251], loss=7.0334
	step [173/251], loss=5.9050
	step [174/251], loss=8.6854
	step [175/251], loss=6.6356
	step [176/251], loss=7.4426
	step [177/251], loss=8.8694
	step [178/251], loss=6.8717
	step [179/251], loss=7.2181
	step [180/251], loss=8.0543
	step [181/251], loss=8.6449
	step [182/251], loss=7.2713
	step [183/251], loss=7.5435
	step [184/251], loss=6.2106
	step [185/251], loss=7.1996
	step [186/251], loss=6.5954
	step [187/251], loss=8.1639
	step [188/251], loss=6.3780
	step [189/251], loss=7.2808
	step [190/251], loss=7.4960
	step [191/251], loss=7.1902
	step [192/251], loss=7.3970
	step [193/251], loss=8.3495
	step [194/251], loss=8.0075
	step [195/251], loss=5.9252
	step [196/251], loss=7.4357
	step [197/251], loss=6.6932
	step [198/251], loss=8.4279
	step [199/251], loss=6.7952
	step [200/251], loss=8.7190
	step [201/251], loss=6.9828
	step [202/251], loss=8.4629
	step [203/251], loss=8.2137
	step [204/251], loss=7.6266
	step [205/251], loss=6.7866
	step [206/251], loss=8.8255
	step [207/251], loss=8.4294
	step [208/251], loss=7.1801
	step [209/251], loss=5.3087
	step [210/251], loss=6.8038
	step [211/251], loss=7.3600
	step [212/251], loss=6.9226
	step [213/251], loss=9.2365
	step [214/251], loss=8.7667
	step [215/251], loss=5.7625
	step [216/251], loss=8.6857
	step [217/251], loss=5.1720
	step [218/251], loss=7.9991
	step [219/251], loss=6.0904
	step [220/251], loss=5.5285
	step [221/251], loss=7.4087
	step [222/251], loss=5.8512
	step [223/251], loss=7.1344
	step [224/251], loss=7.8301
	step [225/251], loss=8.6186
	step [226/251], loss=7.1221
	step [227/251], loss=8.2957
	step [228/251], loss=7.8538
	step [229/251], loss=6.7837
	step [230/251], loss=7.5422
	step [231/251], loss=7.0850
	step [232/251], loss=7.3858
	step [233/251], loss=7.1078
	step [234/251], loss=6.5294
	step [235/251], loss=6.3286
	step [236/251], loss=7.3796
	step [237/251], loss=8.3379
	step [238/251], loss=7.4286
	step [239/251], loss=8.3253
	step [240/251], loss=6.7420
	step [241/251], loss=10.0933
	step [242/251], loss=7.7790
	step [243/251], loss=7.2900
	step [244/251], loss=6.2318
	step [245/251], loss=6.5968
	step [246/251], loss=6.9810
	step [247/251], loss=7.0250
	step [248/251], loss=8.2394
	step [249/251], loss=5.9646
	step [250/251], loss=6.8507
	step [251/251], loss=1.0614
	Evaluating
	loss=0.0273, precision=0.1949, recall=0.9972, f1=0.3261
Training epoch 17
	step [1/251], loss=7.0872
	step [2/251], loss=8.5514
	step [3/251], loss=5.8184
	step [4/251], loss=7.1524
	step [5/251], loss=7.6189
	step [6/251], loss=7.4272
	step [7/251], loss=7.7404
	step [8/251], loss=6.3712
	step [9/251], loss=7.5189
	step [10/251], loss=7.2434
	step [11/251], loss=8.6923
	step [12/251], loss=6.1255
	step [13/251], loss=7.4483
	step [14/251], loss=5.2559
	step [15/251], loss=5.9933
	step [16/251], loss=6.0088
	step [17/251], loss=8.1308
	step [18/251], loss=8.8983
	step [19/251], loss=6.7709
	step [20/251], loss=7.3880
	step [21/251], loss=6.0489
	step [22/251], loss=7.6781
	step [23/251], loss=6.7903
	step [24/251], loss=5.9806
	step [25/251], loss=6.1534
	step [26/251], loss=6.0133
	step [27/251], loss=8.5234
	step [28/251], loss=7.4018
	step [29/251], loss=6.3396
	step [30/251], loss=6.2064
	step [31/251], loss=7.0495
	step [32/251], loss=7.7981
	step [33/251], loss=8.0714
	step [34/251], loss=7.2356
	step [35/251], loss=9.9607
	step [36/251], loss=7.1402
	step [37/251], loss=8.3769
	step [38/251], loss=6.4787
	step [39/251], loss=8.1265
	step [40/251], loss=8.3392
	step [41/251], loss=7.7428
	step [42/251], loss=7.4279
	step [43/251], loss=8.4248
	step [44/251], loss=7.3111
	step [45/251], loss=7.1679
	step [46/251], loss=7.4245
	step [47/251], loss=8.1576
	step [48/251], loss=6.8496
	step [49/251], loss=8.9774
	step [50/251], loss=7.2040
	step [51/251], loss=8.4393
	step [52/251], loss=5.9850
	step [53/251], loss=7.0298
	step [54/251], loss=7.9172
	step [55/251], loss=7.9116
	step [56/251], loss=5.4543
	step [57/251], loss=5.5051
	step [58/251], loss=8.3146
	step [59/251], loss=8.9217
	step [60/251], loss=5.9698
	step [61/251], loss=7.2383
	step [62/251], loss=5.9056
	step [63/251], loss=6.5255
	step [64/251], loss=6.2283
	step [65/251], loss=7.7178
	step [66/251], loss=5.5692
	step [67/251], loss=8.5982
	step [68/251], loss=7.2626
	step [69/251], loss=9.3060
	step [70/251], loss=7.4318
	step [71/251], loss=6.6328
	step [72/251], loss=8.6178
	step [73/251], loss=9.2920
	step [74/251], loss=7.2569
	step [75/251], loss=6.5601
	step [76/251], loss=6.2936
	step [77/251], loss=9.1864
	step [78/251], loss=6.6266
	step [79/251], loss=7.0622
	step [80/251], loss=7.3074
	step [81/251], loss=8.4333
	step [82/251], loss=8.7007
	step [83/251], loss=8.5373
	step [84/251], loss=7.9084
	step [85/251], loss=5.4918
	step [86/251], loss=8.1969
	step [87/251], loss=6.3489
	step [88/251], loss=6.8964
	step [89/251], loss=8.0361
	step [90/251], loss=6.2846
	step [91/251], loss=8.5985
	step [92/251], loss=6.8686
	step [93/251], loss=7.7987
	step [94/251], loss=7.5338
	step [95/251], loss=7.3606
	step [96/251], loss=7.0271
	step [97/251], loss=6.7105
	step [98/251], loss=6.0122
	step [99/251], loss=7.2422
	step [100/251], loss=7.7861
	step [101/251], loss=7.7761
	step [102/251], loss=6.3727
	step [103/251], loss=8.5494
	step [104/251], loss=9.0663
	step [105/251], loss=9.6415
	step [106/251], loss=7.0383
	step [107/251], loss=7.0233
	step [108/251], loss=6.4396
	step [109/251], loss=8.2771
	step [110/251], loss=7.4887
	step [111/251], loss=7.7724
	step [112/251], loss=7.9256
	step [113/251], loss=6.2837
	step [114/251], loss=7.8303
	step [115/251], loss=7.4949
	step [116/251], loss=10.6734
	step [117/251], loss=6.6155
	step [118/251], loss=6.6691
	step [119/251], loss=7.9699
	step [120/251], loss=9.8222
	step [121/251], loss=7.7025
	step [122/251], loss=7.2195
	step [123/251], loss=7.0376
	step [124/251], loss=7.2789
	step [125/251], loss=5.7840
	step [126/251], loss=8.4587
	step [127/251], loss=6.7501
	step [128/251], loss=8.3077
	step [129/251], loss=8.2043
	step [130/251], loss=7.0193
	step [131/251], loss=7.2471
	step [132/251], loss=8.5345
	step [133/251], loss=6.7069
	step [134/251], loss=7.0913
	step [135/251], loss=7.5316
	step [136/251], loss=7.3594
	step [137/251], loss=6.8941
	step [138/251], loss=7.2034
	step [139/251], loss=7.4958
	step [140/251], loss=7.2602
	step [141/251], loss=7.6699
	step [142/251], loss=7.0572
	step [143/251], loss=8.2958
	step [144/251], loss=6.6750
	step [145/251], loss=6.9020
	step [146/251], loss=6.7258
	step [147/251], loss=5.1073
	step [148/251], loss=6.7415
	step [149/251], loss=5.7847
	step [150/251], loss=9.7896
	step [151/251], loss=6.1803
	step [152/251], loss=10.0644
	step [153/251], loss=9.7804
	step [154/251], loss=6.2178
	step [155/251], loss=6.5737
	step [156/251], loss=7.2141
	step [157/251], loss=6.5093
	step [158/251], loss=6.9885
	step [159/251], loss=7.2649
	step [160/251], loss=7.1917
	step [161/251], loss=6.3378
	step [162/251], loss=6.9538
	step [163/251], loss=7.9710
	step [164/251], loss=6.9940
	step [165/251], loss=7.6943
	step [166/251], loss=7.7885
	step [167/251], loss=8.1485
	step [168/251], loss=8.3056
	step [169/251], loss=8.1683
	step [170/251], loss=7.3506
	step [171/251], loss=8.3835
	step [172/251], loss=7.9350
	step [173/251], loss=6.9021
	step [174/251], loss=6.7140
	step [175/251], loss=7.1524
	step [176/251], loss=7.0800
	step [177/251], loss=7.9071
	step [178/251], loss=6.5065
	step [179/251], loss=8.3342
	step [180/251], loss=6.3745
	step [181/251], loss=7.1952
	step [182/251], loss=7.2085
	step [183/251], loss=5.7018
	step [184/251], loss=6.0701
	step [185/251], loss=8.5433
	step [186/251], loss=5.7780
	step [187/251], loss=7.8727
	step [188/251], loss=6.0402
	step [189/251], loss=7.8516
	step [190/251], loss=6.2312
	step [191/251], loss=7.9838
	step [192/251], loss=6.4261
	step [193/251], loss=7.4108
	step [194/251], loss=7.7037
	step [195/251], loss=8.4239
	step [196/251], loss=9.3114
	step [197/251], loss=5.1749
	step [198/251], loss=6.6675
	step [199/251], loss=7.1313
	step [200/251], loss=8.1332
	step [201/251], loss=6.7390
	step [202/251], loss=7.0382
	step [203/251], loss=7.8257
	step [204/251], loss=7.6091
	step [205/251], loss=5.6338
	step [206/251], loss=7.4212
	step [207/251], loss=7.5338
	step [208/251], loss=6.4743
	step [209/251], loss=7.9511
	step [210/251], loss=7.6129
	step [211/251], loss=8.1579
	step [212/251], loss=8.3234
	step [213/251], loss=8.4981
	step [214/251], loss=7.6989
	step [215/251], loss=8.5142
	step [216/251], loss=7.6536
	step [217/251], loss=8.9866
	step [218/251], loss=7.6490
	step [219/251], loss=7.3450
	step [220/251], loss=7.7627
	step [221/251], loss=6.7825
	step [222/251], loss=7.5480
	step [223/251], loss=8.6153
	step [224/251], loss=9.6795
	step [225/251], loss=10.0152
	step [226/251], loss=6.9806
	step [227/251], loss=9.1948
	step [228/251], loss=6.7169
	step [229/251], loss=7.4514
	step [230/251], loss=8.6734
	step [231/251], loss=7.4510
	step [232/251], loss=6.4954
	step [233/251], loss=7.8563
	step [234/251], loss=7.3371
	step [235/251], loss=7.7143
	step [236/251], loss=7.2564
	step [237/251], loss=6.3941
	step [238/251], loss=7.2766
	step [239/251], loss=5.8621
	step [240/251], loss=7.1522
	step [241/251], loss=7.5394
	step [242/251], loss=6.0549
	step [243/251], loss=5.5425
	step [244/251], loss=7.6040
	step [245/251], loss=6.3663
	step [246/251], loss=6.0765
	step [247/251], loss=8.7413
	step [248/251], loss=7.3483
	step [249/251], loss=5.9402
	step [250/251], loss=6.9745
	step [251/251], loss=1.7303
	Evaluating
	loss=0.0209, precision=0.2369, recall=0.9959, f1=0.3827
saving model as: 2_saved_model.pth
Training epoch 18
	step [1/251], loss=6.4903
	step [2/251], loss=7.6618
	step [3/251], loss=9.2745
	step [4/251], loss=7.2218
	step [5/251], loss=7.0895
	step [6/251], loss=7.7832
	step [7/251], loss=8.6260
	step [8/251], loss=7.0533
	step [9/251], loss=7.0879
	step [10/251], loss=5.3878
	step [11/251], loss=7.9011
	step [12/251], loss=8.4651
	step [13/251], loss=6.3029
	step [14/251], loss=7.6515
	step [15/251], loss=6.1744
	step [16/251], loss=8.1790
	step [17/251], loss=7.3649
	step [18/251], loss=10.5739
	step [19/251], loss=6.3319
	step [20/251], loss=6.7271
	step [21/251], loss=8.0855
	step [22/251], loss=7.9814
	step [23/251], loss=7.0196
	step [24/251], loss=6.4904
	step [25/251], loss=7.2395
	step [26/251], loss=7.0594
	step [27/251], loss=8.9299
	step [28/251], loss=6.9111
	step [29/251], loss=9.2937
	step [30/251], loss=8.4645
	step [31/251], loss=8.6451
	step [32/251], loss=6.6932
	step [33/251], loss=6.0894
	step [34/251], loss=8.3978
	step [35/251], loss=7.7629
	step [36/251], loss=6.2696
	step [37/251], loss=7.2709
	step [38/251], loss=7.6940
	step [39/251], loss=6.8145
	step [40/251], loss=8.1828
	step [41/251], loss=6.0776
	step [42/251], loss=6.5729
	step [43/251], loss=8.4986
	step [44/251], loss=7.5958
	step [45/251], loss=8.6931
	step [46/251], loss=5.9661
	step [47/251], loss=7.9258
	step [48/251], loss=8.1537
	step [49/251], loss=6.9585
	step [50/251], loss=7.0604
	step [51/251], loss=7.3820
	step [52/251], loss=5.7171
	step [53/251], loss=7.2887
	step [54/251], loss=5.5352
	step [55/251], loss=7.1448
	step [56/251], loss=5.4234
	step [57/251], loss=5.6887
	step [58/251], loss=5.4792
	step [59/251], loss=7.7774
	step [60/251], loss=6.9672
	step [61/251], loss=7.5321
	step [62/251], loss=7.6700
	step [63/251], loss=9.0097
	step [64/251], loss=9.1377
	step [65/251], loss=8.1488
	step [66/251], loss=8.4051
	step [67/251], loss=7.4966
	step [68/251], loss=6.6110
	step [69/251], loss=6.8003
	step [70/251], loss=8.1393
	step [71/251], loss=7.4003
	step [72/251], loss=8.7908
	step [73/251], loss=8.0583
	step [74/251], loss=5.2165
	step [75/251], loss=8.7123
	step [76/251], loss=7.4104
	step [77/251], loss=7.8596
	step [78/251], loss=7.1593
	step [79/251], loss=7.5346
	step [80/251], loss=7.8766
	step [81/251], loss=7.5782
	step [82/251], loss=8.8121
	step [83/251], loss=8.3818
	step [84/251], loss=8.7746
	step [85/251], loss=8.8324
	step [86/251], loss=6.3996
	step [87/251], loss=7.0022
	step [88/251], loss=7.5938
	step [89/251], loss=6.0449
	step [90/251], loss=7.1990
	step [91/251], loss=7.8124
	step [92/251], loss=6.5247
	step [93/251], loss=8.2209
	step [94/251], loss=7.7946
	step [95/251], loss=8.1186
	step [96/251], loss=6.8250
	step [97/251], loss=7.0723
	step [98/251], loss=6.5924
	step [99/251], loss=9.9756
	step [100/251], loss=6.1726
	step [101/251], loss=6.8945
	step [102/251], loss=6.5097
	step [103/251], loss=5.7195
	step [104/251], loss=7.6970
	step [105/251], loss=7.2195
	step [106/251], loss=8.4299
	step [107/251], loss=6.7495
	step [108/251], loss=7.7773
	step [109/251], loss=9.0239
	step [110/251], loss=6.9360
	step [111/251], loss=6.9071
	step [112/251], loss=6.8403
	step [113/251], loss=6.7845
	step [114/251], loss=8.0874
	step [115/251], loss=8.8214
	step [116/251], loss=6.1495
	step [117/251], loss=6.5310
	step [118/251], loss=5.8277
	step [119/251], loss=5.9498
	step [120/251], loss=5.5951
	step [121/251], loss=5.0856
	step [122/251], loss=6.2720
	step [123/251], loss=6.9449
	step [124/251], loss=6.2533
	step [125/251], loss=8.6527
	step [126/251], loss=8.0493
	step [127/251], loss=8.2207
	step [128/251], loss=7.8929
	step [129/251], loss=10.3251
	step [130/251], loss=7.5408
	step [131/251], loss=9.1498
	step [132/251], loss=7.3108
	step [133/251], loss=6.1368
	step [134/251], loss=5.9300
	step [135/251], loss=6.2625
	step [136/251], loss=9.1769
	step [137/251], loss=8.7455
	step [138/251], loss=7.4441
	step [139/251], loss=7.0443
	step [140/251], loss=7.7144
	step [141/251], loss=6.7574
	step [142/251], loss=7.9742
	step [143/251], loss=6.8376
	step [144/251], loss=6.9362
	step [145/251], loss=6.9449
	step [146/251], loss=7.6184
	step [147/251], loss=9.3544
	step [148/251], loss=7.1270
	step [149/251], loss=6.3643
	step [150/251], loss=9.8655
	step [151/251], loss=7.0259
	step [152/251], loss=7.8849
	step [153/251], loss=9.2974
	step [154/251], loss=7.4696
	step [155/251], loss=6.1061
	step [156/251], loss=6.5857
	step [157/251], loss=6.6547
	step [158/251], loss=7.0639
	step [159/251], loss=7.1266
	step [160/251], loss=6.4993
	step [161/251], loss=7.3485
	step [162/251], loss=8.2269
	step [163/251], loss=6.8671
	step [164/251], loss=5.7158
	step [165/251], loss=6.6586
	step [166/251], loss=6.1241
	step [167/251], loss=6.6615
	step [168/251], loss=7.4819
	step [169/251], loss=5.8852
	step [170/251], loss=6.6614
	step [171/251], loss=8.2654
	step [172/251], loss=7.6550
	step [173/251], loss=6.5343
	step [174/251], loss=7.0615
	step [175/251], loss=6.5779
	step [176/251], loss=7.4246
	step [177/251], loss=5.9957
	step [178/251], loss=6.2125
	step [179/251], loss=5.5156
	step [180/251], loss=5.8335
	step [181/251], loss=8.7623
	step [182/251], loss=8.0858
	step [183/251], loss=7.6002
	step [184/251], loss=6.3244
	step [185/251], loss=6.0446
	step [186/251], loss=8.9380
	step [187/251], loss=7.5368
	step [188/251], loss=6.0676
	step [189/251], loss=8.0504
	step [190/251], loss=6.5126
	step [191/251], loss=6.6955
	step [192/251], loss=5.4845
	step [193/251], loss=9.3148
	step [194/251], loss=8.1764
	step [195/251], loss=7.8683
	step [196/251], loss=5.6004
	step [197/251], loss=7.8282
	step [198/251], loss=7.1830
	step [199/251], loss=9.1699
	step [200/251], loss=5.7676
	step [201/251], loss=6.5094
	step [202/251], loss=6.4079
	step [203/251], loss=7.8117
	step [204/251], loss=6.2995
	step [205/251], loss=6.5325
	step [206/251], loss=7.8802
	step [207/251], loss=6.1776
	step [208/251], loss=7.0778
	step [209/251], loss=7.6560
	step [210/251], loss=7.5945
	step [211/251], loss=7.5543
	step [212/251], loss=8.3103
	step [213/251], loss=7.4862
	step [214/251], loss=6.4700
	step [215/251], loss=7.5625
	step [216/251], loss=6.0010
	step [217/251], loss=6.8829
	step [218/251], loss=6.6020
	step [219/251], loss=8.0214
	step [220/251], loss=6.6649
	step [221/251], loss=5.5175
	step [222/251], loss=6.3638
	step [223/251], loss=7.5252
	step [224/251], loss=7.0147
	step [225/251], loss=6.6310
	step [226/251], loss=6.9445
	step [227/251], loss=6.9863
	step [228/251], loss=6.3229
	step [229/251], loss=9.2996
	step [230/251], loss=7.4302
	step [231/251], loss=6.9619
	step [232/251], loss=6.0910
	step [233/251], loss=6.4493
	step [234/251], loss=5.5224
	step [235/251], loss=6.0438
	step [236/251], loss=8.0533
	step [237/251], loss=8.1472
	step [238/251], loss=6.1196
	step [239/251], loss=6.6507
	step [240/251], loss=6.8395
	step [241/251], loss=6.2149
	step [242/251], loss=6.3094
	step [243/251], loss=7.3954
	step [244/251], loss=6.3856
	step [245/251], loss=6.5159
	step [246/251], loss=6.0304
	step [247/251], loss=5.8676
	step [248/251], loss=7.5669
	step [249/251], loss=5.8119
	step [250/251], loss=7.7754
	step [251/251], loss=0.7445
	Evaluating
	loss=0.0227, precision=0.2154, recall=0.9967, f1=0.3542
Training epoch 19
	step [1/251], loss=7.7574
	step [2/251], loss=7.8621
	step [3/251], loss=5.7885
	step [4/251], loss=6.9005
	step [5/251], loss=7.9664
	step [6/251], loss=6.6696
	step [7/251], loss=5.6490
	step [8/251], loss=6.6430
	step [9/251], loss=6.3871
	step [10/251], loss=7.6686
	step [11/251], loss=5.6393
	step [12/251], loss=9.0301
	step [13/251], loss=6.2907
	step [14/251], loss=5.9683
	step [15/251], loss=8.2918
	step [16/251], loss=5.4393
	step [17/251], loss=6.8762
	step [18/251], loss=5.8913
	step [19/251], loss=5.5478
	step [20/251], loss=7.8926
	step [21/251], loss=7.6626
	step [22/251], loss=8.0831
	step [23/251], loss=8.4898
	step [24/251], loss=6.7789
	step [25/251], loss=5.9673
	step [26/251], loss=6.4330
	step [27/251], loss=6.1572
	step [28/251], loss=6.4514
	step [29/251], loss=9.7955
	step [30/251], loss=8.2111
	step [31/251], loss=6.4143
	step [32/251], loss=8.6197
	step [33/251], loss=7.3705
	step [34/251], loss=7.7131
	step [35/251], loss=6.0847
	step [36/251], loss=6.2821
	step [37/251], loss=8.7556
	step [38/251], loss=6.5490
	step [39/251], loss=7.9193
	step [40/251], loss=7.0473
	step [41/251], loss=5.6851
	step [42/251], loss=6.1951
	step [43/251], loss=7.6213
	step [44/251], loss=8.9955
	step [45/251], loss=6.4274
	step [46/251], loss=9.2395
	step [47/251], loss=6.6448
	step [48/251], loss=5.9463
	step [49/251], loss=6.3776
	step [50/251], loss=7.7253
	step [51/251], loss=7.0239
	step [52/251], loss=6.5721
	step [53/251], loss=8.2177
	step [54/251], loss=6.1088
	step [55/251], loss=6.2430
	step [56/251], loss=5.1659
	step [57/251], loss=7.6355
	step [58/251], loss=6.8567
	step [59/251], loss=7.2588
	step [60/251], loss=6.8689
	step [61/251], loss=6.7165
	step [62/251], loss=6.3131
	step [63/251], loss=6.2241
	step [64/251], loss=6.3563
	step [65/251], loss=6.9491
	step [66/251], loss=5.4863
	step [67/251], loss=6.8306
	step [68/251], loss=7.4510
	step [69/251], loss=6.3102
	step [70/251], loss=8.5835
	step [71/251], loss=5.7368
	step [72/251], loss=6.3922
	step [73/251], loss=6.7913
	step [74/251], loss=7.4606
	step [75/251], loss=6.4751
	step [76/251], loss=7.4246
	step [77/251], loss=8.0272
	step [78/251], loss=6.0280
	step [79/251], loss=8.0595
	step [80/251], loss=8.5459
	step [81/251], loss=5.8527
	step [82/251], loss=7.5519
	step [83/251], loss=6.4912
	step [84/251], loss=6.1440
	step [85/251], loss=6.5699
	step [86/251], loss=6.8483
	step [87/251], loss=5.8413
	step [88/251], loss=6.9468
	step [89/251], loss=7.3674
	step [90/251], loss=7.0142
	step [91/251], loss=6.0835
	step [92/251], loss=6.8280
	step [93/251], loss=6.0157
	step [94/251], loss=6.5260
	step [95/251], loss=8.7067
	step [96/251], loss=9.2012
	step [97/251], loss=8.2149
	step [98/251], loss=6.9061
	step [99/251], loss=7.2631
	step [100/251], loss=7.8801
	step [101/251], loss=9.4122
	step [102/251], loss=6.8574
	step [103/251], loss=7.9882
	step [104/251], loss=6.3905
	step [105/251], loss=8.3395
	step [106/251], loss=6.5796
	step [107/251], loss=7.6296
	step [108/251], loss=10.0418
	step [109/251], loss=6.9794
	step [110/251], loss=6.5460
	step [111/251], loss=6.4488
	step [112/251], loss=6.5965
	step [113/251], loss=8.6482
	step [114/251], loss=7.2247
	step [115/251], loss=6.8518
	step [116/251], loss=8.0103
	step [117/251], loss=9.0451
	step [118/251], loss=8.0056
	step [119/251], loss=7.9626
	step [120/251], loss=7.5876
	step [121/251], loss=7.4444
	step [122/251], loss=7.9491
	step [123/251], loss=5.9460
	step [124/251], loss=6.1052
	step [125/251], loss=6.3650
	step [126/251], loss=7.6268
	step [127/251], loss=10.8639
	step [128/251], loss=7.9214
	step [129/251], loss=7.7601
	step [130/251], loss=8.4110
	step [131/251], loss=7.3408
	step [132/251], loss=7.5789
	step [133/251], loss=8.4289
	step [134/251], loss=6.8668
	step [135/251], loss=5.4516
	step [136/251], loss=8.7415
	step [137/251], loss=6.8418
	step [138/251], loss=7.1282
	step [139/251], loss=6.6160
	step [140/251], loss=7.7415
	step [141/251], loss=5.8148
	step [142/251], loss=6.0227
	step [143/251], loss=6.1243
	step [144/251], loss=5.1821
	step [145/251], loss=9.2353
	step [146/251], loss=6.8344
	step [147/251], loss=6.5184
	step [148/251], loss=7.9052
	step [149/251], loss=8.1403
	step [150/251], loss=8.2654
	step [151/251], loss=6.1830
	step [152/251], loss=5.5732
	step [153/251], loss=6.5043
	step [154/251], loss=6.6912
	step [155/251], loss=6.2583
	step [156/251], loss=9.0263
	step [157/251], loss=6.0382
	step [158/251], loss=6.8057
	step [159/251], loss=6.8411
	step [160/251], loss=7.2717
	step [161/251], loss=5.9499
	step [162/251], loss=7.9116
	step [163/251], loss=6.4332
	step [164/251], loss=6.5309
	step [165/251], loss=4.7725
	step [166/251], loss=5.2167
	step [167/251], loss=7.0459
	step [168/251], loss=7.1722
	step [169/251], loss=7.1490
	step [170/251], loss=7.9917
	step [171/251], loss=9.5041
	step [172/251], loss=11.3129
	step [173/251], loss=8.8871
	step [174/251], loss=6.0531
	step [175/251], loss=7.3919
	step [176/251], loss=8.3821
	step [177/251], loss=7.1591
	step [178/251], loss=6.0661
	step [179/251], loss=6.6343
	step [180/251], loss=7.7413
	step [181/251], loss=6.3247
	step [182/251], loss=8.6304
	step [183/251], loss=5.9489
	step [184/251], loss=7.3772
	step [185/251], loss=6.2803
	step [186/251], loss=8.1617
	step [187/251], loss=5.8502
	step [188/251], loss=6.3203
	step [189/251], loss=6.1571
	step [190/251], loss=5.9583
	step [191/251], loss=7.0884
	step [192/251], loss=7.5671
	step [193/251], loss=8.9741
	step [194/251], loss=5.7769
	step [195/251], loss=5.9642
	step [196/251], loss=6.0434
	step [197/251], loss=5.3163
	step [198/251], loss=9.4128
	step [199/251], loss=6.1394
	step [200/251], loss=6.7044
	step [201/251], loss=6.4150
	step [202/251], loss=8.1685
	step [203/251], loss=7.1490
	step [204/251], loss=8.9655
	step [205/251], loss=7.4987
	step [206/251], loss=6.2676
	step [207/251], loss=6.0646
	step [208/251], loss=5.5274
	step [209/251], loss=7.6348
	step [210/251], loss=7.6568
	step [211/251], loss=7.1986
	step [212/251], loss=5.6701
	step [213/251], loss=5.8630
	step [214/251], loss=6.8061
	step [215/251], loss=6.8575
	step [216/251], loss=7.6014
	step [217/251], loss=6.8931
	step [218/251], loss=6.4986
	step [219/251], loss=5.5282
	step [220/251], loss=6.9756
	step [221/251], loss=7.1146
	step [222/251], loss=6.6992
	step [223/251], loss=8.7685
	step [224/251], loss=8.5771
	step [225/251], loss=6.5462
	step [226/251], loss=6.2501
	step [227/251], loss=7.3860
	step [228/251], loss=6.1556
	step [229/251], loss=7.2201
	step [230/251], loss=6.3237
	step [231/251], loss=7.2015
	step [232/251], loss=6.9480
	step [233/251], loss=8.4045
	step [234/251], loss=6.4526
	step [235/251], loss=8.3835
	step [236/251], loss=6.2878
	step [237/251], loss=6.6022
	step [238/251], loss=6.0792
	step [239/251], loss=7.1084
	step [240/251], loss=5.6039
	step [241/251], loss=7.8918
	step [242/251], loss=9.3612
	step [243/251], loss=8.8267
	step [244/251], loss=6.2586
	step [245/251], loss=7.5588
	step [246/251], loss=6.2541
	step [247/251], loss=6.9816
	step [248/251], loss=7.9657
	step [249/251], loss=7.0162
	step [250/251], loss=7.1827
	step [251/251], loss=0.5895
	Evaluating
	loss=0.0246, precision=0.2126, recall=0.9966, f1=0.3504
Training epoch 20
	step [1/251], loss=8.7687
	step [2/251], loss=6.8198
	step [3/251], loss=5.4639
	step [4/251], loss=7.2048
	step [5/251], loss=6.6642
	step [6/251], loss=6.3999
	step [7/251], loss=5.5078
	step [8/251], loss=7.6478
	step [9/251], loss=7.6181
	step [10/251], loss=5.9652
	step [11/251], loss=7.3138
	step [12/251], loss=7.4235
	step [13/251], loss=7.9332
	step [14/251], loss=7.2755
	step [15/251], loss=6.7010
	step [16/251], loss=7.2859
	step [17/251], loss=7.6936
	step [18/251], loss=8.4546
	step [19/251], loss=6.3147
	step [20/251], loss=6.3747
	step [21/251], loss=7.3309
	step [22/251], loss=7.0243
	step [23/251], loss=5.9429
	step [24/251], loss=5.9289
	step [25/251], loss=7.3559
	step [26/251], loss=7.0435
	step [27/251], loss=7.9586
	step [28/251], loss=6.5503
	step [29/251], loss=6.7016
	step [30/251], loss=8.7959
	step [31/251], loss=4.5652
	step [32/251], loss=8.4127
	step [33/251], loss=6.7619
	step [34/251], loss=7.7820
	step [35/251], loss=6.7434
	step [36/251], loss=8.5366
	step [37/251], loss=7.1772
	step [38/251], loss=7.1313
	step [39/251], loss=7.8533
	step [40/251], loss=6.8950
	step [41/251], loss=7.1636
	step [42/251], loss=7.5870
	step [43/251], loss=9.2702
	step [44/251], loss=6.4733
	step [45/251], loss=7.6281
	step [46/251], loss=6.0701
	step [47/251], loss=6.2272
	step [48/251], loss=5.8367
	step [49/251], loss=6.4278
	step [50/251], loss=8.9173
	step [51/251], loss=6.8227
	step [52/251], loss=6.3574
	step [53/251], loss=6.2052
	step [54/251], loss=6.3743
	step [55/251], loss=5.7742
	step [56/251], loss=7.4552
	step [57/251], loss=6.3948
	step [58/251], loss=6.7634
	step [59/251], loss=7.3380
	step [60/251], loss=6.8691
	step [61/251], loss=6.7108
	step [62/251], loss=6.6622
	step [63/251], loss=5.6091
	step [64/251], loss=6.5137
	step [65/251], loss=6.4188
	step [66/251], loss=5.9381
	step [67/251], loss=8.9529
	step [68/251], loss=7.5872
	step [69/251], loss=5.5441
	step [70/251], loss=6.1115
	step [71/251], loss=7.0799
	step [72/251], loss=8.8854
	step [73/251], loss=6.5532
	step [74/251], loss=6.8604
	step [75/251], loss=6.2459
	step [76/251], loss=7.4354
	step [77/251], loss=8.5426
	step [78/251], loss=7.0128
	step [79/251], loss=6.2934
	step [80/251], loss=6.3648
	step [81/251], loss=5.5728
	step [82/251], loss=7.6950
	step [83/251], loss=6.8455
	step [84/251], loss=6.3775
	step [85/251], loss=7.4678
	step [86/251], loss=6.8029
	step [87/251], loss=6.8554
	step [88/251], loss=8.1788
	step [89/251], loss=6.8185
	step [90/251], loss=6.7615
	step [91/251], loss=6.5748
	step [92/251], loss=6.0337
	step [93/251], loss=6.3922
	step [94/251], loss=8.6897
	step [95/251], loss=7.3443
	step [96/251], loss=6.0708
	step [97/251], loss=9.1204
	step [98/251], loss=7.7618
	step [99/251], loss=7.4163
	step [100/251], loss=7.4497
	step [101/251], loss=6.7172
	step [102/251], loss=7.5146
	step [103/251], loss=8.1983
	step [104/251], loss=6.2507
	step [105/251], loss=9.1492
	step [106/251], loss=8.3694
	step [107/251], loss=8.7990
	step [108/251], loss=7.5051
	step [109/251], loss=6.8195
	step [110/251], loss=6.4993
	step [111/251], loss=7.0681
	step [112/251], loss=5.1336
	step [113/251], loss=6.5701
	step [114/251], loss=6.5397
	step [115/251], loss=6.2577
	step [116/251], loss=6.4286
	step [117/251], loss=5.6214
	step [118/251], loss=7.2523
	step [119/251], loss=6.9918
	step [120/251], loss=6.5262
	step [121/251], loss=8.7565
	step [122/251], loss=6.1161
	step [123/251], loss=6.8036
	step [124/251], loss=7.9706
	step [125/251], loss=5.7841
	step [126/251], loss=7.2497
	step [127/251], loss=8.9384
	step [128/251], loss=9.1038
	step [129/251], loss=7.4132
	step [130/251], loss=7.8788
	step [131/251], loss=6.9897
	step [132/251], loss=7.8250
	step [133/251], loss=6.9638
	step [134/251], loss=8.1125
	step [135/251], loss=7.0471
	step [136/251], loss=7.4764
	step [137/251], loss=6.1996
	step [138/251], loss=8.3184
	step [139/251], loss=5.6667
	step [140/251], loss=6.5418
	step [141/251], loss=6.5531
	step [142/251], loss=6.6076
	step [143/251], loss=9.1221
	step [144/251], loss=7.9440
	step [145/251], loss=6.4540
	step [146/251], loss=7.4778
	step [147/251], loss=6.7163
	step [148/251], loss=6.9442
	step [149/251], loss=8.6033
	step [150/251], loss=8.7453
	step [151/251], loss=6.4392
	step [152/251], loss=6.2120
	step [153/251], loss=6.3392
	step [154/251], loss=6.0035
	step [155/251], loss=8.2504
	step [156/251], loss=10.4699
	step [157/251], loss=7.3697
	step [158/251], loss=6.6385
	step [159/251], loss=7.3550
	step [160/251], loss=6.5404
	step [161/251], loss=6.5278
	step [162/251], loss=6.9571
	step [163/251], loss=7.2370
	step [164/251], loss=5.6721
	step [165/251], loss=6.6671
	step [166/251], loss=7.8731
	step [167/251], loss=7.3587
	step [168/251], loss=5.0470
	step [169/251], loss=7.4259
	step [170/251], loss=6.2916
	step [171/251], loss=5.6396
	step [172/251], loss=5.9081
	step [173/251], loss=9.4502
	step [174/251], loss=6.6162
	step [175/251], loss=6.9600
	step [176/251], loss=7.0299
	step [177/251], loss=6.5929
	step [178/251], loss=8.0108
	step [179/251], loss=5.8527
	step [180/251], loss=6.2111
	step [181/251], loss=5.9753
	step [182/251], loss=6.7062
	step [183/251], loss=5.8037
	step [184/251], loss=6.9372
	step [185/251], loss=6.5483
	step [186/251], loss=5.9951
	step [187/251], loss=5.7584
	step [188/251], loss=5.2174
	step [189/251], loss=7.8450
	step [190/251], loss=7.4180
	step [191/251], loss=8.5413
	step [192/251], loss=6.3451
	step [193/251], loss=6.3394
	step [194/251], loss=8.1110
	step [195/251], loss=6.7890
	step [196/251], loss=5.4767
	step [197/251], loss=6.8479
	step [198/251], loss=7.6952
	step [199/251], loss=7.2156
	step [200/251], loss=7.3053
	step [201/251], loss=8.0987
	step [202/251], loss=6.0428
	step [203/251], loss=6.2230
	step [204/251], loss=6.2900
	step [205/251], loss=7.5554
	step [206/251], loss=6.3689
	step [207/251], loss=5.3183
	step [208/251], loss=6.6259
	step [209/251], loss=7.3223
	step [210/251], loss=5.5105
	step [211/251], loss=7.2130
	step [212/251], loss=7.3535
	step [213/251], loss=9.1397
	step [214/251], loss=8.4511
	step [215/251], loss=9.5631
	step [216/251], loss=5.9411
	step [217/251], loss=6.7260
	step [218/251], loss=6.8862
	step [219/251], loss=7.2458
	step [220/251], loss=8.3350
	step [221/251], loss=5.9336
	step [222/251], loss=6.6248
	step [223/251], loss=6.4100
	step [224/251], loss=7.4951
	step [225/251], loss=6.2071
	step [226/251], loss=8.5437
	step [227/251], loss=6.6414
	step [228/251], loss=7.0818
	step [229/251], loss=7.0223
	step [230/251], loss=9.5469
	step [231/251], loss=6.9246
	step [232/251], loss=7.9809
	step [233/251], loss=6.3901
	step [234/251], loss=7.3516
	step [235/251], loss=5.6402
	step [236/251], loss=6.1509
	step [237/251], loss=6.2603
	step [238/251], loss=8.1902
	step [239/251], loss=5.6533
	step [240/251], loss=6.5891
	step [241/251], loss=6.2140
	step [242/251], loss=7.4066
	step [243/251], loss=8.0798
	step [244/251], loss=6.3913
	step [245/251], loss=7.3201
	step [246/251], loss=6.3631
	step [247/251], loss=8.1128
	step [248/251], loss=7.6176
	step [249/251], loss=7.1852
	step [250/251], loss=7.4881
	step [251/251], loss=0.9628
	Evaluating
	loss=0.0278, precision=0.1953, recall=0.9971, f1=0.3266
Training epoch 21
	step [1/251], loss=6.5326
	step [2/251], loss=4.9122
	step [3/251], loss=6.1770
	step [4/251], loss=6.6072
	step [5/251], loss=7.2254
	step [6/251], loss=8.1594
	step [7/251], loss=6.5291
	step [8/251], loss=4.9741
	step [9/251], loss=6.7208
	step [10/251], loss=5.2061
	step [11/251], loss=6.3107
	step [12/251], loss=7.6968
	step [13/251], loss=9.9351
	step [14/251], loss=6.4148
	step [15/251], loss=6.1071
	step [16/251], loss=5.8346
	step [17/251], loss=7.3120
	step [18/251], loss=7.3797
	step [19/251], loss=7.5213
	step [20/251], loss=8.2465
	step [21/251], loss=7.9023
	step [22/251], loss=7.9064
	step [23/251], loss=7.7825
	step [24/251], loss=6.5631
	step [25/251], loss=6.5111
	step [26/251], loss=8.8988
	step [27/251], loss=5.8939
	step [28/251], loss=8.3702
	step [29/251], loss=7.1379
	step [30/251], loss=8.0067
	step [31/251], loss=6.6141
	step [32/251], loss=6.9941
	step [33/251], loss=7.0116
	step [34/251], loss=6.7767
	step [35/251], loss=7.5958
	step [36/251], loss=5.7945
	step [37/251], loss=5.2713
	step [38/251], loss=6.2840
	step [39/251], loss=6.9113
	step [40/251], loss=6.0298
	step [41/251], loss=7.0656
	step [42/251], loss=5.3851
	step [43/251], loss=6.4934
	step [44/251], loss=6.3259
	step [45/251], loss=6.9627
	step [46/251], loss=5.0692
	step [47/251], loss=5.3896
	step [48/251], loss=6.3106
	step [49/251], loss=7.4266
	step [50/251], loss=6.7359
	step [51/251], loss=5.6644
	step [52/251], loss=7.0250
	step [53/251], loss=7.8995
	step [54/251], loss=9.2131
	step [55/251], loss=6.3657
	step [56/251], loss=8.8233
	step [57/251], loss=6.3072
	step [58/251], loss=6.3145
	step [59/251], loss=5.7158
	step [60/251], loss=8.2040
	step [61/251], loss=7.1924
	step [62/251], loss=7.9770
	step [63/251], loss=6.4686
	step [64/251], loss=6.0290
	step [65/251], loss=6.8596
	step [66/251], loss=6.6789
	step [67/251], loss=5.7558
	step [68/251], loss=5.3332
	step [69/251], loss=7.0326
	step [70/251], loss=5.0227
	step [71/251], loss=6.6668
	step [72/251], loss=7.5851
	step [73/251], loss=6.8400
	step [74/251], loss=5.6558
	step [75/251], loss=5.5265
	step [76/251], loss=5.9420
	step [77/251], loss=7.8894
	step [78/251], loss=6.1136
	step [79/251], loss=6.8597
	step [80/251], loss=6.4871
	step [81/251], loss=8.4507
	step [82/251], loss=7.0171
	step [83/251], loss=7.8447
	step [84/251], loss=6.2241
	step [85/251], loss=6.1437
	step [86/251], loss=7.0748
	step [87/251], loss=9.3330
	step [88/251], loss=6.5370
	step [89/251], loss=6.9271
	step [90/251], loss=7.6429
	step [91/251], loss=5.7063
	step [92/251], loss=7.0726
	step [93/251], loss=7.4045
	step [94/251], loss=7.7381
	step [95/251], loss=7.2614
	step [96/251], loss=5.3426
	step [97/251], loss=7.9737
	step [98/251], loss=7.4660
	step [99/251], loss=6.5309
	step [100/251], loss=7.6734
	step [101/251], loss=7.9787
	step [102/251], loss=8.1131
	step [103/251], loss=7.4202
	step [104/251], loss=6.0273
	step [105/251], loss=5.7951
	step [106/251], loss=7.2100
	step [107/251], loss=6.3205
	step [108/251], loss=8.3666
	step [109/251], loss=6.2148
	step [110/251], loss=6.4497
	step [111/251], loss=5.9356
	step [112/251], loss=6.9148
	step [113/251], loss=6.9764
	step [114/251], loss=5.2555
	step [115/251], loss=7.5110
	step [116/251], loss=5.8214
	step [117/251], loss=5.0114
	step [118/251], loss=6.5296
	step [119/251], loss=7.7799
	step [120/251], loss=6.1500
	step [121/251], loss=6.0802
	step [122/251], loss=9.0638
	step [123/251], loss=8.0331
	step [124/251], loss=6.2289
	step [125/251], loss=8.2773
	step [126/251], loss=7.6072
	step [127/251], loss=7.9134
	step [128/251], loss=8.5389
	step [129/251], loss=6.5858
	step [130/251], loss=7.6700
	step [131/251], loss=7.2619
	step [132/251], loss=7.5858
	step [133/251], loss=5.6852
	step [134/251], loss=5.9908
	step [135/251], loss=7.7190
	step [136/251], loss=7.1275
	step [137/251], loss=7.3329
	step [138/251], loss=6.4134
	step [139/251], loss=7.9592
	step [140/251], loss=6.7705
	step [141/251], loss=6.8795
	step [142/251], loss=6.3715
	step [143/251], loss=5.5224
	step [144/251], loss=7.1945
	step [145/251], loss=5.4295
	step [146/251], loss=6.7444
	step [147/251], loss=5.4460
	step [148/251], loss=8.6245
	step [149/251], loss=7.2186
	step [150/251], loss=4.8941
	step [151/251], loss=7.5788
	step [152/251], loss=6.4826
	step [153/251], loss=6.8253
	step [154/251], loss=6.2506
	step [155/251], loss=6.0879
	step [156/251], loss=6.0303
	step [157/251], loss=8.6987
	step [158/251], loss=7.8910
	step [159/251], loss=7.0912
	step [160/251], loss=7.2871
	step [161/251], loss=7.6251
	step [162/251], loss=5.3690
	step [163/251], loss=6.7803
	step [164/251], loss=6.8870
	step [165/251], loss=6.2044
	step [166/251], loss=5.9197
	step [167/251], loss=8.0002
	step [168/251], loss=6.0945
	step [169/251], loss=6.4952
	step [170/251], loss=7.1529
	step [171/251], loss=6.8266
	step [172/251], loss=5.4580
	step [173/251], loss=6.3685
	step [174/251], loss=6.4770
	step [175/251], loss=7.0953
	step [176/251], loss=5.6834
	step [177/251], loss=6.3081
	step [178/251], loss=7.2286
	step [179/251], loss=8.3299
	step [180/251], loss=7.8728
	step [181/251], loss=7.6239
	step [182/251], loss=6.9713
	step [183/251], loss=7.3879
	step [184/251], loss=8.4275
	step [185/251], loss=5.9783
	step [186/251], loss=7.5734
	step [187/251], loss=5.3913
	step [188/251], loss=6.2888
	step [189/251], loss=6.1516
	step [190/251], loss=6.8411
	step [191/251], loss=7.0554
	step [192/251], loss=7.0317
	step [193/251], loss=6.8030
	step [194/251], loss=7.5193
	step [195/251], loss=5.8631
	step [196/251], loss=6.4687
	step [197/251], loss=7.4549
	step [198/251], loss=6.1310
	step [199/251], loss=5.4645
	step [200/251], loss=8.1109
	step [201/251], loss=7.7914
	step [202/251], loss=6.3645
	step [203/251], loss=10.0097
	step [204/251], loss=8.3668
	step [205/251], loss=8.3407
	step [206/251], loss=6.6712
	step [207/251], loss=6.9128
	step [208/251], loss=8.0237
	step [209/251], loss=6.2600
	step [210/251], loss=6.0114
	step [211/251], loss=7.0625
	step [212/251], loss=6.4450
	step [213/251], loss=6.8579
	step [214/251], loss=6.2111
	step [215/251], loss=8.7452
	step [216/251], loss=8.0420
	step [217/251], loss=7.7486
	step [218/251], loss=7.3565
	step [219/251], loss=5.9786
	step [220/251], loss=6.4880
	step [221/251], loss=6.1073
	step [222/251], loss=6.6507
	step [223/251], loss=7.8750
	step [224/251], loss=5.3649
	step [225/251], loss=5.2525
	step [226/251], loss=6.2191
	step [227/251], loss=5.7761
	step [228/251], loss=8.9512
	step [229/251], loss=6.5327
	step [230/251], loss=4.9902
	step [231/251], loss=5.8978
	step [232/251], loss=5.8501
	step [233/251], loss=6.0862
	step [234/251], loss=4.9544
	step [235/251], loss=5.6148
	step [236/251], loss=7.8658
	step [237/251], loss=4.9973
	step [238/251], loss=5.6348
	step [239/251], loss=6.8758
	step [240/251], loss=5.0748
	step [241/251], loss=7.1899
	step [242/251], loss=7.9606
	step [243/251], loss=7.7466
	step [244/251], loss=6.0471
	step [245/251], loss=8.1380
	step [246/251], loss=6.1992
	step [247/251], loss=7.8047
	step [248/251], loss=8.4292
	step [249/251], loss=8.4359
	step [250/251], loss=7.8883
	step [251/251], loss=0.7772
	Evaluating
	loss=0.0227, precision=0.2041, recall=0.9968, f1=0.3388
Training epoch 22
	step [1/251], loss=7.9369
	step [2/251], loss=7.1589
	step [3/251], loss=6.6301
	step [4/251], loss=7.7509
	step [5/251], loss=6.8378
	step [6/251], loss=8.7745
	step [7/251], loss=7.2592
	step [8/251], loss=5.1304
	step [9/251], loss=8.3569
	step [10/251], loss=7.0289
	step [11/251], loss=5.7851
	step [12/251], loss=5.8911
	step [13/251], loss=7.7070
	step [14/251], loss=7.0836
	step [15/251], loss=6.7393
	step [16/251], loss=7.0095
	step [17/251], loss=5.2485
	step [18/251], loss=6.2489
	step [19/251], loss=6.0979
	step [20/251], loss=6.7331
	step [21/251], loss=5.4322
	step [22/251], loss=7.7284
	step [23/251], loss=5.1942
	step [24/251], loss=8.5906
	step [25/251], loss=5.6791
	step [26/251], loss=4.4959
	step [27/251], loss=5.9034
	step [28/251], loss=6.4085
	step [29/251], loss=7.3636
	step [30/251], loss=5.8859
	step [31/251], loss=8.4678
	step [32/251], loss=6.1058
	step [33/251], loss=7.7209
	step [34/251], loss=6.4247
	step [35/251], loss=7.5738
	step [36/251], loss=5.6289
	step [37/251], loss=7.4371
	step [38/251], loss=5.1611
	step [39/251], loss=5.9732
	step [40/251], loss=6.8649
	step [41/251], loss=6.1294
	step [42/251], loss=7.7486
	step [43/251], loss=6.7382
	step [44/251], loss=6.9286
	step [45/251], loss=6.4831
	step [46/251], loss=7.4490
	step [47/251], loss=7.3444
	step [48/251], loss=6.5204
	step [49/251], loss=6.6158
	step [50/251], loss=5.7679
	step [51/251], loss=7.5825
	step [52/251], loss=8.6063
	step [53/251], loss=6.8674
	step [54/251], loss=7.1887
	step [55/251], loss=7.8089
	step [56/251], loss=5.4304
	step [57/251], loss=5.6496
	step [58/251], loss=7.3177
	step [59/251], loss=5.8609
	step [60/251], loss=7.6579
	step [61/251], loss=7.4364
	step [62/251], loss=5.2596
	step [63/251], loss=8.1835
	step [64/251], loss=6.6619
	step [65/251], loss=6.5327
	step [66/251], loss=5.7757
	step [67/251], loss=6.2842
	step [68/251], loss=9.3533
	step [69/251], loss=7.1012
	step [70/251], loss=7.2900
	step [71/251], loss=5.3155
	step [72/251], loss=6.9785
	step [73/251], loss=8.2456
	step [74/251], loss=5.8652
	step [75/251], loss=6.0126
	step [76/251], loss=4.6153
	step [77/251], loss=8.9695
	step [78/251], loss=5.7865
	step [79/251], loss=6.9545
	step [80/251], loss=7.0856
	step [81/251], loss=7.0103
	step [82/251], loss=7.3952
	step [83/251], loss=7.3681
	step [84/251], loss=8.5563
	step [85/251], loss=7.1141
	step [86/251], loss=6.0808
	step [87/251], loss=6.0821
	step [88/251], loss=6.6950
	step [89/251], loss=6.2862
	step [90/251], loss=7.2492
	step [91/251], loss=7.0854
	step [92/251], loss=8.0235
	step [93/251], loss=6.2070
	step [94/251], loss=8.3281
	step [95/251], loss=7.1699
	step [96/251], loss=6.4015
	step [97/251], loss=6.8568
	step [98/251], loss=6.9168
	step [99/251], loss=6.6113
	step [100/251], loss=7.4831
	step [101/251], loss=6.0233
	step [102/251], loss=7.1460
	step [103/251], loss=6.4112
	step [104/251], loss=6.2059
	step [105/251], loss=7.2121
	step [106/251], loss=6.2594
	step [107/251], loss=5.8972
	step [108/251], loss=6.3364
	step [109/251], loss=7.0464
	step [110/251], loss=6.0500
	step [111/251], loss=6.4533
	step [112/251], loss=7.8524
	step [113/251], loss=6.4102
	step [114/251], loss=5.8363
	step [115/251], loss=7.6306
	step [116/251], loss=6.1048
	step [117/251], loss=6.9908
	step [118/251], loss=7.4801
	step [119/251], loss=5.8612
	step [120/251], loss=7.8427
	step [121/251], loss=6.3301
	step [122/251], loss=6.7819
	step [123/251], loss=6.7338
	step [124/251], loss=5.6054
	step [125/251], loss=6.3230
	step [126/251], loss=6.1826
	step [127/251], loss=5.3277
	step [128/251], loss=6.9917
	step [129/251], loss=6.4756
	step [130/251], loss=7.4115
	step [131/251], loss=5.6494
	step [132/251], loss=8.5603
	step [133/251], loss=6.5592
	step [134/251], loss=7.5759
	step [135/251], loss=6.8036
	step [136/251], loss=6.5601
	step [137/251], loss=6.3474
	step [138/251], loss=7.7242
	step [139/251], loss=7.5993
	step [140/251], loss=7.8468
	step [141/251], loss=7.0079
	step [142/251], loss=6.1748
	step [143/251], loss=5.7921
	step [144/251], loss=7.5790
	step [145/251], loss=5.2576
	step [146/251], loss=5.5479
	step [147/251], loss=7.2941
	step [148/251], loss=9.0462
	step [149/251], loss=5.1408
	step [150/251], loss=7.0323
	step [151/251], loss=7.0577
	step [152/251], loss=6.7316
	step [153/251], loss=7.3578
	step [154/251], loss=6.6143
	step [155/251], loss=8.0697
	step [156/251], loss=6.3125
	step [157/251], loss=6.8782
	step [158/251], loss=6.7771
	step [159/251], loss=6.6339
	step [160/251], loss=6.9264
	step [161/251], loss=5.5614
	step [162/251], loss=6.7972
	step [163/251], loss=5.5157
	step [164/251], loss=5.6490
	step [165/251], loss=5.9837
	step [166/251], loss=6.6050
	step [167/251], loss=9.7516
	step [168/251], loss=6.3356
	step [169/251], loss=5.9962
	step [170/251], loss=6.3830
	step [171/251], loss=6.4655
	step [172/251], loss=6.5506
	step [173/251], loss=7.0379
	step [174/251], loss=6.7948
	step [175/251], loss=6.5373
	step [176/251], loss=5.5410
	step [177/251], loss=6.5915
	step [178/251], loss=7.7879
	step [179/251], loss=5.8343
	step [180/251], loss=5.6874
	step [181/251], loss=6.6806
	step [182/251], loss=7.5066
	step [183/251], loss=5.8843
	step [184/251], loss=5.6819
	step [185/251], loss=6.2903
	step [186/251], loss=6.1824
	step [187/251], loss=7.3952
	step [188/251], loss=7.1576
	step [189/251], loss=5.3584
	step [190/251], loss=6.2469
	step [191/251], loss=6.5814
	step [192/251], loss=6.6326
	step [193/251], loss=5.4814
	step [194/251], loss=5.5231
	step [195/251], loss=6.9253
	step [196/251], loss=5.3215
	step [197/251], loss=5.5013
	step [198/251], loss=5.4733
	step [199/251], loss=6.4747
	step [200/251], loss=7.8153
	step [201/251], loss=8.5869
	step [202/251], loss=6.2480
	step [203/251], loss=7.3707
	step [204/251], loss=6.1861
	step [205/251], loss=6.2088
	step [206/251], loss=6.3884
	step [207/251], loss=7.1436
	step [208/251], loss=6.0887
	step [209/251], loss=5.9552
	step [210/251], loss=7.3646
	step [211/251], loss=8.3658
	step [212/251], loss=7.7986
	step [213/251], loss=5.4757
	step [214/251], loss=5.8813
	step [215/251], loss=6.5977
	step [216/251], loss=7.3122
	step [217/251], loss=7.7651
	step [218/251], loss=7.6982
	step [219/251], loss=6.1270
	step [220/251], loss=7.1964
	step [221/251], loss=5.7134
	step [222/251], loss=6.1817
	step [223/251], loss=6.3535
	step [224/251], loss=7.1673
	step [225/251], loss=7.2897
	step [226/251], loss=6.1990
	step [227/251], loss=6.7836
	step [228/251], loss=6.6433
	step [229/251], loss=6.9409
	step [230/251], loss=7.4916
	step [231/251], loss=6.2163
	step [232/251], loss=7.0611
	step [233/251], loss=7.5792
	step [234/251], loss=7.8929
	step [235/251], loss=5.7207
	step [236/251], loss=5.9338
	step [237/251], loss=6.1669
	step [238/251], loss=4.7522
	step [239/251], loss=7.3348
	step [240/251], loss=5.3259
	step [241/251], loss=7.8040
	step [242/251], loss=8.0594
	step [243/251], loss=5.6963
	step [244/251], loss=6.1222
	step [245/251], loss=7.0735
	step [246/251], loss=6.6149
	step [247/251], loss=7.4471
	step [248/251], loss=5.4258
	step [249/251], loss=8.0580
	step [250/251], loss=9.5157
	step [251/251], loss=1.3924
	Evaluating
	loss=0.0234, precision=0.2120, recall=0.9968, f1=0.3496
Training epoch 23
	step [1/251], loss=5.7385
	step [2/251], loss=7.7848
	step [3/251], loss=5.4833
	step [4/251], loss=6.6769
	step [5/251], loss=6.6815
	step [6/251], loss=5.7826
	step [7/251], loss=6.0291
	step [8/251], loss=6.6705
	step [9/251], loss=6.0068
	step [10/251], loss=7.4080
	step [11/251], loss=5.8197
	step [12/251], loss=6.8510
	step [13/251], loss=5.8851
	step [14/251], loss=7.5465
	step [15/251], loss=5.3733
	step [16/251], loss=6.8565
	step [17/251], loss=6.5524
	step [18/251], loss=4.8187
	step [19/251], loss=8.0118
	step [20/251], loss=6.1748
	step [21/251], loss=8.3107
	step [22/251], loss=5.8967
	step [23/251], loss=5.7976
	step [24/251], loss=6.3174
	step [25/251], loss=6.5516
	step [26/251], loss=6.9034
	step [27/251], loss=5.8032
	step [28/251], loss=8.3727
	step [29/251], loss=5.6294
	step [30/251], loss=5.5100
	step [31/251], loss=6.9770
	step [32/251], loss=5.8711
	step [33/251], loss=7.2767
	step [34/251], loss=6.4716
	step [35/251], loss=5.5801
	step [36/251], loss=7.7464
	step [37/251], loss=5.9919
	step [38/251], loss=8.2565
	step [39/251], loss=7.2311
	step [40/251], loss=6.1814
	step [41/251], loss=6.0499
	step [42/251], loss=6.2539
	step [43/251], loss=6.3418
	step [44/251], loss=8.4604
	step [45/251], loss=5.4682
	step [46/251], loss=5.4020
	step [47/251], loss=7.7762
	step [48/251], loss=6.5973
	step [49/251], loss=5.8951
	step [50/251], loss=7.7819
	step [51/251], loss=7.8477
	step [52/251], loss=6.4234
	step [53/251], loss=5.8437
	step [54/251], loss=6.6528
	step [55/251], loss=6.4620
	step [56/251], loss=6.9859
	step [57/251], loss=6.6723
	step [58/251], loss=6.3377
	step [59/251], loss=5.6984
	step [60/251], loss=6.6289
	step [61/251], loss=6.2479
	step [62/251], loss=7.2343
	step [63/251], loss=5.2274
	step [64/251], loss=7.2900
	step [65/251], loss=6.2936
	step [66/251], loss=6.6377
	step [67/251], loss=5.8768
	step [68/251], loss=7.1860
	step [69/251], loss=5.0577
	step [70/251], loss=6.6214
	step [71/251], loss=4.9924
	step [72/251], loss=7.8433
	step [73/251], loss=5.9941
	step [74/251], loss=8.5866
	step [75/251], loss=5.8613
	step [76/251], loss=5.3981
	step [77/251], loss=6.4226
	step [78/251], loss=5.8452
	step [79/251], loss=6.1775
	step [80/251], loss=6.6617
	step [81/251], loss=7.3360
	step [82/251], loss=7.2987
	step [83/251], loss=6.0475
	step [84/251], loss=6.8755
	step [85/251], loss=6.6684
	step [86/251], loss=5.5089
	step [87/251], loss=6.5308
	step [88/251], loss=6.3971
	step [89/251], loss=6.0852
	step [90/251], loss=6.7328
	step [91/251], loss=6.5258
	step [92/251], loss=6.7007
	step [93/251], loss=5.8921
	step [94/251], loss=6.9656
	step [95/251], loss=6.5848
	step [96/251], loss=5.6990
	step [97/251], loss=5.4768
	step [98/251], loss=6.9038
	step [99/251], loss=4.9916
	step [100/251], loss=6.4056
	step [101/251], loss=7.0937
	step [102/251], loss=6.5330
	step [103/251], loss=7.9796
	step [104/251], loss=6.8193
	step [105/251], loss=5.6231
	step [106/251], loss=8.4134
	step [107/251], loss=7.3125
	step [108/251], loss=5.9006
	step [109/251], loss=6.2895
	step [110/251], loss=6.5750
	step [111/251], loss=6.2234
	step [112/251], loss=4.9651
	step [113/251], loss=6.7513
	step [114/251], loss=8.0584
	step [115/251], loss=7.1841
	step [116/251], loss=8.1212
	step [117/251], loss=8.9601
	step [118/251], loss=8.1118
	step [119/251], loss=6.4289
	step [120/251], loss=6.5759
	step [121/251], loss=6.6232
	step [122/251], loss=7.1682
	step [123/251], loss=6.8143
	step [124/251], loss=5.5647
	step [125/251], loss=5.1444
	step [126/251], loss=7.7648
	step [127/251], loss=5.9635
	step [128/251], loss=6.7542
	step [129/251], loss=6.9312
	step [130/251], loss=6.8431
	step [131/251], loss=8.3510
	step [132/251], loss=5.7701
	step [133/251], loss=6.8588
	step [134/251], loss=6.0633
	step [135/251], loss=5.5608
	step [136/251], loss=7.0207
	step [137/251], loss=7.3270
	step [138/251], loss=6.5129
	step [139/251], loss=6.2603
	step [140/251], loss=6.4852
	step [141/251], loss=5.5208
	step [142/251], loss=6.9606
	step [143/251], loss=5.8507
	step [144/251], loss=5.6477
	step [145/251], loss=6.9240
	step [146/251], loss=7.6790
	step [147/251], loss=7.5725
	step [148/251], loss=6.3416
	step [149/251], loss=6.9294
	step [150/251], loss=7.3040
	step [151/251], loss=6.4674
	step [152/251], loss=7.1288
	step [153/251], loss=5.4714
	step [154/251], loss=8.8214
	step [155/251], loss=6.0055
	step [156/251], loss=5.7287
	step [157/251], loss=5.6353
	step [158/251], loss=6.0586
	step [159/251], loss=6.7997
	step [160/251], loss=5.8835
	step [161/251], loss=7.1541
	step [162/251], loss=5.3468
	step [163/251], loss=7.6391
	step [164/251], loss=5.5817
	step [165/251], loss=6.4202
	step [166/251], loss=8.3276
	step [167/251], loss=7.5066
	step [168/251], loss=7.7562
	step [169/251], loss=6.4800
	step [170/251], loss=6.2147
	step [171/251], loss=7.2253
	step [172/251], loss=6.6242
	step [173/251], loss=7.2216
	step [174/251], loss=6.0337
	step [175/251], loss=7.3714
	step [176/251], loss=6.0378
	step [177/251], loss=7.5025
	step [178/251], loss=6.6754
	step [179/251], loss=6.2160
	step [180/251], loss=6.9207
	step [181/251], loss=8.2022
	step [182/251], loss=6.9314
	step [183/251], loss=7.6121
	step [184/251], loss=5.6756
	step [185/251], loss=6.1535
	step [186/251], loss=6.7236
	step [187/251], loss=6.4580
	step [188/251], loss=5.2638
	step [189/251], loss=7.1874
	step [190/251], loss=7.1639
	step [191/251], loss=5.6741
	step [192/251], loss=7.1933
	step [193/251], loss=7.4644
	step [194/251], loss=6.9086
	step [195/251], loss=9.5443
	step [196/251], loss=5.9375
	step [197/251], loss=6.8169
	step [198/251], loss=4.5974
	step [199/251], loss=7.3655
	step [200/251], loss=7.4389
	step [201/251], loss=6.0549
	step [202/251], loss=6.5086
	step [203/251], loss=6.2082
	step [204/251], loss=5.7183
	step [205/251], loss=6.9970
	step [206/251], loss=5.9801
	step [207/251], loss=8.9433
	step [208/251], loss=5.6220
	step [209/251], loss=6.1487
	step [210/251], loss=6.3086
	step [211/251], loss=5.5714
	step [212/251], loss=8.2487
	step [213/251], loss=6.7253
	step [214/251], loss=6.9817
	step [215/251], loss=5.4781
	step [216/251], loss=8.0222
	step [217/251], loss=5.4952
	step [218/251], loss=7.0801
	step [219/251], loss=5.6463
	step [220/251], loss=6.5971
	step [221/251], loss=6.7082
	step [222/251], loss=7.5503
	step [223/251], loss=8.2745
	step [224/251], loss=7.7558
	step [225/251], loss=7.5125
	step [226/251], loss=6.3316
	step [227/251], loss=6.2569
	step [228/251], loss=7.6791
	step [229/251], loss=6.3084
	step [230/251], loss=7.1348
	step [231/251], loss=7.2912
	step [232/251], loss=9.0592
	step [233/251], loss=8.7536
	step [234/251], loss=5.6668
	step [235/251], loss=5.5114
	step [236/251], loss=6.4536
	step [237/251], loss=7.4980
	step [238/251], loss=6.2494
	step [239/251], loss=8.4287
	step [240/251], loss=6.0611
	step [241/251], loss=6.7731
	step [242/251], loss=8.2592
	step [243/251], loss=7.2106
	step [244/251], loss=5.8782
	step [245/251], loss=5.8182
	step [246/251], loss=5.4179
	step [247/251], loss=7.7412
	step [248/251], loss=6.8275
	step [249/251], loss=8.0470
	step [250/251], loss=7.9092
	step [251/251], loss=1.3475
	Evaluating
	loss=0.0244, precision=0.1930, recall=0.9974, f1=0.3234
Training epoch 24
	step [1/251], loss=8.3416
	step [2/251], loss=5.8561
	step [3/251], loss=6.8215
	step [4/251], loss=8.4709
	step [5/251], loss=6.5413
	step [6/251], loss=6.7495
	step [7/251], loss=7.2305
	step [8/251], loss=7.6302
	step [9/251], loss=7.0262
	step [10/251], loss=6.3799
	step [11/251], loss=8.3113
	step [12/251], loss=5.0089
	step [13/251], loss=6.1431
	step [14/251], loss=5.3778
	step [15/251], loss=5.1385
	step [16/251], loss=6.6546
	step [17/251], loss=7.4996
	step [18/251], loss=5.9564
	step [19/251], loss=6.3039
	step [20/251], loss=8.4692
	step [21/251], loss=6.7949
	step [22/251], loss=7.0099
	step [23/251], loss=5.6321
	step [24/251], loss=6.0351
	step [25/251], loss=7.3096
	step [26/251], loss=6.3223
	step [27/251], loss=8.0463
	step [28/251], loss=6.5470
	step [29/251], loss=7.5933
	step [30/251], loss=8.4938
	step [31/251], loss=5.4331
	step [32/251], loss=5.3574
	step [33/251], loss=7.0326
	step [34/251], loss=6.2021
	step [35/251], loss=5.7826
	step [36/251], loss=6.0795
	step [37/251], loss=6.9100
	step [38/251], loss=6.5331
	step [39/251], loss=7.3556
	step [40/251], loss=7.5937
	step [41/251], loss=6.4531
	step [42/251], loss=6.9493
	step [43/251], loss=7.0369
	step [44/251], loss=8.6254
	step [45/251], loss=7.9930
	step [46/251], loss=5.5028
	step [47/251], loss=6.6068
	step [48/251], loss=7.1556
	step [49/251], loss=5.2947
	step [50/251], loss=6.6983
	step [51/251], loss=4.8883
	step [52/251], loss=6.3308
	step [53/251], loss=5.9578
	step [54/251], loss=5.6672
	step [55/251], loss=5.9589
	step [56/251], loss=4.4907
	step [57/251], loss=7.1400
	step [58/251], loss=7.4208
	step [59/251], loss=7.6481
	step [60/251], loss=6.4891
	step [61/251], loss=8.9130
	step [62/251], loss=5.2904
	step [63/251], loss=8.6464
	step [64/251], loss=6.4523
	step [65/251], loss=7.2152
	step [66/251], loss=7.6858
	step [67/251], loss=4.4091
	step [68/251], loss=6.0391
	step [69/251], loss=6.0965
	step [70/251], loss=6.5909
	step [71/251], loss=6.9340
	step [72/251], loss=6.8070
	step [73/251], loss=6.5927
	step [74/251], loss=4.9820
	step [75/251], loss=7.2517
	step [76/251], loss=8.7673
	step [77/251], loss=4.3316
	step [78/251], loss=5.8584
	step [79/251], loss=5.1117
	step [80/251], loss=6.9063
	step [81/251], loss=4.9760
	step [82/251], loss=8.3349
	step [83/251], loss=6.7071
	step [84/251], loss=5.7224
	step [85/251], loss=4.6288
	step [86/251], loss=5.1510
	step [87/251], loss=5.5489
	step [88/251], loss=8.3766
	step [89/251], loss=5.8927
	step [90/251], loss=5.3548
	step [91/251], loss=6.8877
	step [92/251], loss=7.0022
	step [93/251], loss=8.5100
	step [94/251], loss=6.9555
	step [95/251], loss=5.0670
	step [96/251], loss=6.4241
	step [97/251], loss=7.3754
	step [98/251], loss=5.5818
	step [99/251], loss=6.9689
	step [100/251], loss=6.7443
	step [101/251], loss=6.3010
	step [102/251], loss=6.8881
	step [103/251], loss=7.7810
	step [104/251], loss=6.5963
	step [105/251], loss=5.8021
	step [106/251], loss=5.8145
	step [107/251], loss=7.8135
	step [108/251], loss=6.1929
	step [109/251], loss=7.7839
	step [110/251], loss=6.1272
	step [111/251], loss=6.8092
	step [112/251], loss=5.4504
	step [113/251], loss=6.9569
	step [114/251], loss=5.5297
	step [115/251], loss=8.2294
	step [116/251], loss=6.3134
	step [117/251], loss=6.4937
	step [118/251], loss=6.8270
	step [119/251], loss=8.7194
	step [120/251], loss=6.5727
	step [121/251], loss=6.7611
	step [122/251], loss=5.6031
	step [123/251], loss=7.4025
	step [124/251], loss=4.9885
	step [125/251], loss=8.9016
	step [126/251], loss=7.0184
	step [127/251], loss=6.2548
	step [128/251], loss=6.6266
	step [129/251], loss=8.1723
	step [130/251], loss=6.7227
	step [131/251], loss=6.7271
	step [132/251], loss=5.9969
	step [133/251], loss=6.2545
	step [134/251], loss=6.1414
	step [135/251], loss=7.4585
	step [136/251], loss=5.6051
	step [137/251], loss=6.9019
	step [138/251], loss=7.5431
	step [139/251], loss=6.9485
	step [140/251], loss=7.5606
	step [141/251], loss=5.4605
	step [142/251], loss=7.4142
	step [143/251], loss=5.5263
	step [144/251], loss=5.6118
	step [145/251], loss=7.0971
	step [146/251], loss=5.3839
	step [147/251], loss=6.6851
	step [148/251], loss=7.2191
	step [149/251], loss=5.9736
	step [150/251], loss=5.2463
	step [151/251], loss=6.7714
	step [152/251], loss=5.9475
	step [153/251], loss=6.1822
	step [154/251], loss=6.4986
	step [155/251], loss=7.1829
	step [156/251], loss=8.9432
	step [157/251], loss=6.7760
	step [158/251], loss=5.3299
	step [159/251], loss=8.3234
	step [160/251], loss=6.2361
	step [161/251], loss=6.6872
	step [162/251], loss=6.6990
	step [163/251], loss=7.6946
	step [164/251], loss=6.5993
	step [165/251], loss=6.1432
	step [166/251], loss=6.4755
	step [167/251], loss=5.7981
	step [168/251], loss=6.1160
	step [169/251], loss=4.8400
	step [170/251], loss=4.6901
	step [171/251], loss=7.1529
	step [172/251], loss=5.1916
	step [173/251], loss=5.9417
	step [174/251], loss=6.7624
	step [175/251], loss=5.5197
	step [176/251], loss=5.9645
	step [177/251], loss=5.9005
	step [178/251], loss=5.7974
	step [179/251], loss=8.2375
	step [180/251], loss=7.5322
	step [181/251], loss=6.1232
	step [182/251], loss=6.3315
	step [183/251], loss=7.1461
	step [184/251], loss=6.4439
	step [185/251], loss=6.8489
	step [186/251], loss=6.4692
	step [187/251], loss=4.7002
	step [188/251], loss=6.2516
	step [189/251], loss=7.2093
	step [190/251], loss=5.6271
	step [191/251], loss=6.1519
	step [192/251], loss=6.1985
	step [193/251], loss=6.0516
	step [194/251], loss=5.7758
	step [195/251], loss=6.2953
	step [196/251], loss=5.5509
	step [197/251], loss=7.5131
	step [198/251], loss=6.8633
	step [199/251], loss=6.7121
	step [200/251], loss=5.9773
	step [201/251], loss=6.0294
	step [202/251], loss=7.3316
	step [203/251], loss=5.7948
	step [204/251], loss=7.4783
	step [205/251], loss=8.2238
	step [206/251], loss=4.8150
	step [207/251], loss=5.9272
	step [208/251], loss=7.1746
	step [209/251], loss=8.7649
	step [210/251], loss=7.3995
	step [211/251], loss=5.8102
	step [212/251], loss=5.1062
	step [213/251], loss=5.8660
	step [214/251], loss=6.6661
	step [215/251], loss=5.7130
	step [216/251], loss=7.3676
	step [217/251], loss=5.2077
	step [218/251], loss=6.3544
	step [219/251], loss=6.5770
	step [220/251], loss=6.5656
	step [221/251], loss=6.6836
	step [222/251], loss=7.5696
	step [223/251], loss=6.4555
	step [224/251], loss=4.3087
	step [225/251], loss=6.5967
	step [226/251], loss=6.1784
	step [227/251], loss=7.8172
	step [228/251], loss=4.9018
	step [229/251], loss=7.5482
	step [230/251], loss=5.5190
	step [231/251], loss=8.6867
	step [232/251], loss=6.6181
	step [233/251], loss=5.6644
	step [234/251], loss=6.3660
	step [235/251], loss=6.7138
	step [236/251], loss=5.8227
	step [237/251], loss=5.5109
	step [238/251], loss=6.5467
	step [239/251], loss=4.4147
	step [240/251], loss=6.1151
	step [241/251], loss=8.4082
	step [242/251], loss=5.9216
	step [243/251], loss=7.2239
	step [244/251], loss=7.0607
	step [245/251], loss=5.6684
	step [246/251], loss=6.3542
	step [247/251], loss=6.8076
	step [248/251], loss=6.4602
	step [249/251], loss=6.2246
	step [250/251], loss=7.6198
	step [251/251], loss=0.9129
	Evaluating
	loss=0.0266, precision=0.1864, recall=0.9975, f1=0.3142
Training epoch 25
	step [1/251], loss=5.0531
	step [2/251], loss=6.1659
	step [3/251], loss=4.6730
	step [4/251], loss=6.9938
	step [5/251], loss=8.4229
	step [6/251], loss=6.8231
	step [7/251], loss=6.3703
	step [8/251], loss=5.8097
	step [9/251], loss=5.0392
	step [10/251], loss=7.1629
	step [11/251], loss=5.7849
	step [12/251], loss=5.9942
	step [13/251], loss=5.8009
	step [14/251], loss=5.6415
	step [15/251], loss=7.9427
	step [16/251], loss=5.6939
	step [17/251], loss=7.6248
	step [18/251], loss=6.3532
	step [19/251], loss=5.5688
	step [20/251], loss=5.1192
	step [21/251], loss=6.4953
	step [22/251], loss=7.9690
	step [23/251], loss=7.1431
	step [24/251], loss=7.2049
	step [25/251], loss=6.7216
	step [26/251], loss=7.0918
	step [27/251], loss=8.0369
	step [28/251], loss=5.6965
	step [29/251], loss=5.1823
	step [30/251], loss=6.2767
	step [31/251], loss=7.7549
	step [32/251], loss=5.1723
	step [33/251], loss=7.3267
	step [34/251], loss=6.1727
	step [35/251], loss=4.2654
	step [36/251], loss=7.3690
	step [37/251], loss=5.0082
	step [38/251], loss=6.4627
	step [39/251], loss=6.8824
	step [40/251], loss=7.0898
	step [41/251], loss=5.4886
	step [42/251], loss=6.0877
	step [43/251], loss=6.4723
	step [44/251], loss=5.9501
	step [45/251], loss=6.7723
	step [46/251], loss=6.3841
	step [47/251], loss=7.1617
	step [48/251], loss=5.5447
	step [49/251], loss=5.6374
	step [50/251], loss=6.3250
	step [51/251], loss=7.2698
	step [52/251], loss=8.2974
	step [53/251], loss=6.6750
	step [54/251], loss=8.4478
	step [55/251], loss=7.0465
	step [56/251], loss=6.5365
	step [57/251], loss=7.6219
	step [58/251], loss=5.9318
	step [59/251], loss=6.9346
	step [60/251], loss=6.8794
	step [61/251], loss=6.2775
	step [62/251], loss=5.7333
	step [63/251], loss=6.4842
	step [64/251], loss=5.8387
	step [65/251], loss=6.3119
	step [66/251], loss=8.5216
	step [67/251], loss=6.1499
	step [68/251], loss=7.6075
	step [69/251], loss=6.0770
	step [70/251], loss=5.6064
	step [71/251], loss=6.4896
	step [72/251], loss=6.0921
	step [73/251], loss=7.0681
	step [74/251], loss=6.6101
	step [75/251], loss=5.8326
	step [76/251], loss=6.5038
	step [77/251], loss=6.2637
	step [78/251], loss=5.9112
	step [79/251], loss=6.3356
	step [80/251], loss=5.4908
	step [81/251], loss=8.3790
	step [82/251], loss=6.3591
	step [83/251], loss=6.4949
	step [84/251], loss=5.5825
	step [85/251], loss=6.6713
	step [86/251], loss=5.1325
	step [87/251], loss=5.4656
	step [88/251], loss=6.5667
	step [89/251], loss=7.9548
	step [90/251], loss=5.2183
	step [91/251], loss=7.3448
	step [92/251], loss=8.1061
	step [93/251], loss=6.1586
	step [94/251], loss=6.0828
	step [95/251], loss=6.8753
	step [96/251], loss=7.4475
	step [97/251], loss=5.9754
	step [98/251], loss=7.0024
	step [99/251], loss=6.3443
	step [100/251], loss=5.8922
	step [101/251], loss=6.8751
	step [102/251], loss=7.7224
	step [103/251], loss=7.4242
	step [104/251], loss=7.3420
	step [105/251], loss=6.2734
	step [106/251], loss=6.1779
	step [107/251], loss=5.9498
	step [108/251], loss=5.3926
	step [109/251], loss=6.6041
	step [110/251], loss=7.7709
	step [111/251], loss=5.8268
	step [112/251], loss=6.5171
	step [113/251], loss=5.3972
	step [114/251], loss=7.0176
	step [115/251], loss=6.9767
	step [116/251], loss=4.7771
	step [117/251], loss=7.2420
	step [118/251], loss=6.1711
	step [119/251], loss=6.9662
	step [120/251], loss=6.6706
	step [121/251], loss=6.3204
	step [122/251], loss=5.4030
	step [123/251], loss=6.7713
	step [124/251], loss=5.3709
	step [125/251], loss=10.7274
	step [126/251], loss=8.4927
	step [127/251], loss=5.9519
	step [128/251], loss=7.1512
	step [129/251], loss=7.2433
	step [130/251], loss=7.5080
	step [131/251], loss=7.6931
	step [132/251], loss=6.5682
	step [133/251], loss=6.2747
	step [134/251], loss=6.7444
	step [135/251], loss=5.1979
	step [136/251], loss=5.5019
	step [137/251], loss=9.4108
	step [138/251], loss=6.6548
	step [139/251], loss=6.2923
	step [140/251], loss=5.9049
	step [141/251], loss=6.0492
	step [142/251], loss=6.9460
	step [143/251], loss=5.7771
	step [144/251], loss=7.3432
	step [145/251], loss=6.2344
	step [146/251], loss=6.3397
	step [147/251], loss=6.9683
	step [148/251], loss=7.1426
	step [149/251], loss=6.7467
	step [150/251], loss=6.8662
	step [151/251], loss=5.2535
	step [152/251], loss=7.1794
	step [153/251], loss=7.3137
	step [154/251], loss=6.9259
	step [155/251], loss=7.7905
	step [156/251], loss=8.0291
	step [157/251], loss=5.9467
	step [158/251], loss=5.8958
	step [159/251], loss=6.8660
	step [160/251], loss=7.3304
	step [161/251], loss=6.1808
	step [162/251], loss=6.0279
	step [163/251], loss=5.4529
	step [164/251], loss=6.4614
	step [165/251], loss=6.4507
	step [166/251], loss=5.1230
	step [167/251], loss=7.5728
	step [168/251], loss=6.2489
	step [169/251], loss=6.9421
	step [170/251], loss=6.7146
	step [171/251], loss=7.2374
	step [172/251], loss=6.7776
	step [173/251], loss=6.3534
	step [174/251], loss=6.8845
	step [175/251], loss=5.3875
	step [176/251], loss=6.9391
	step [177/251], loss=7.2403
	step [178/251], loss=6.0881
	step [179/251], loss=7.6673
	step [180/251], loss=6.4621
	step [181/251], loss=5.0545
	step [182/251], loss=6.4430
	step [183/251], loss=4.9614
	step [184/251], loss=6.0511
	step [185/251], loss=5.5301
	step [186/251], loss=6.7862
	step [187/251], loss=7.8840
	step [188/251], loss=5.6724
	step [189/251], loss=6.2650
	step [190/251], loss=5.9571
	step [191/251], loss=6.2387
	step [192/251], loss=7.0376
	step [193/251], loss=5.9306
	step [194/251], loss=6.6679
	step [195/251], loss=5.8867
	step [196/251], loss=8.6627
	step [197/251], loss=8.1996
	step [198/251], loss=5.0303
	step [199/251], loss=5.3191
	step [200/251], loss=6.7975
	step [201/251], loss=6.7369
	step [202/251], loss=6.4220
	step [203/251], loss=5.9936
	step [204/251], loss=4.9746
	step [205/251], loss=5.1657
	step [206/251], loss=6.3904
	step [207/251], loss=7.1005
	step [208/251], loss=5.6510
	step [209/251], loss=5.7568
	step [210/251], loss=5.5885
	step [211/251], loss=5.1291
	step [212/251], loss=5.5281
	step [213/251], loss=7.3364
	step [214/251], loss=6.0388
	step [215/251], loss=5.3207
	step [216/251], loss=6.8270
	step [217/251], loss=5.9233
	step [218/251], loss=7.5436
	step [219/251], loss=6.0999
	step [220/251], loss=4.8986
	step [221/251], loss=6.2713
	step [222/251], loss=5.5325
	step [223/251], loss=7.4288
	step [224/251], loss=6.6744
	step [225/251], loss=5.0912
	step [226/251], loss=6.4855
	step [227/251], loss=6.2866
	step [228/251], loss=7.7468
	step [229/251], loss=7.4614
	step [230/251], loss=8.6607
	step [231/251], loss=8.1827
	step [232/251], loss=7.4435
	step [233/251], loss=6.5353
	step [234/251], loss=6.1802
	step [235/251], loss=7.1304
	step [236/251], loss=6.5412
	step [237/251], loss=5.9211
	step [238/251], loss=7.4533
	step [239/251], loss=6.5879
	step [240/251], loss=6.2212
	step [241/251], loss=6.9465
	step [242/251], loss=6.8421
	step [243/251], loss=6.3304
	step [244/251], loss=5.3288
	step [245/251], loss=5.8284
	step [246/251], loss=5.8293
	step [247/251], loss=5.8969
	step [248/251], loss=7.0174
	step [249/251], loss=6.6096
	step [250/251], loss=6.5102
	step [251/251], loss=0.8479
	Evaluating
	loss=0.0230, precision=0.2133, recall=0.9968, f1=0.3514
Training epoch 26
	step [1/251], loss=5.4110
	step [2/251], loss=6.3210
	step [3/251], loss=7.2237
	step [4/251], loss=8.7291
	step [5/251], loss=6.7446
	step [6/251], loss=7.9234
	step [7/251], loss=6.7371
	step [8/251], loss=6.1558
	step [9/251], loss=6.7735
	step [10/251], loss=8.0517
	step [11/251], loss=7.1362
	step [12/251], loss=5.2690
	step [13/251], loss=7.1882
	step [14/251], loss=5.2044
	step [15/251], loss=5.4969
	step [16/251], loss=5.7153
	step [17/251], loss=8.1888
	step [18/251], loss=6.4506
	step [19/251], loss=7.0172
	step [20/251], loss=6.9552
	step [21/251], loss=6.5783
	step [22/251], loss=5.7272
	step [23/251], loss=5.8428
	step [24/251], loss=7.1705
	step [25/251], loss=5.4848
	step [26/251], loss=8.5229
	step [27/251], loss=5.4918
	step [28/251], loss=7.0873
	step [29/251], loss=6.8941
	step [30/251], loss=5.6105
	step [31/251], loss=5.0716
	step [32/251], loss=6.3697
	step [33/251], loss=6.2166
	step [34/251], loss=6.1864
	step [35/251], loss=7.5119
	step [36/251], loss=6.9958
	step [37/251], loss=4.8556
	step [38/251], loss=6.3612
	step [39/251], loss=5.2563
	step [40/251], loss=4.8386
	step [41/251], loss=6.0219
	step [42/251], loss=6.2579
	step [43/251], loss=5.5724
	step [44/251], loss=5.4677
	step [45/251], loss=6.4716
	step [46/251], loss=6.9174
	step [47/251], loss=6.0549
	step [48/251], loss=6.3403
	step [49/251], loss=7.1273
	step [50/251], loss=6.0920
	step [51/251], loss=5.8234
	step [52/251], loss=5.8699
	step [53/251], loss=4.9454
	step [54/251], loss=6.5224
	step [55/251], loss=6.1152
	step [56/251], loss=5.3210
	step [57/251], loss=6.4239
	step [58/251], loss=5.1686
	step [59/251], loss=5.3558
	step [60/251], loss=4.9315
	step [61/251], loss=5.2279
	step [62/251], loss=6.9487
	step [63/251], loss=5.9003
	step [64/251], loss=6.4055
	step [65/251], loss=5.9181
	step [66/251], loss=7.9799
	step [67/251], loss=5.1104
	step [68/251], loss=6.3613
	step [69/251], loss=6.0897
	step [70/251], loss=6.9406
	step [71/251], loss=6.1790
	step [72/251], loss=5.8787
	step [73/251], loss=5.9779
	step [74/251], loss=7.1217
	step [75/251], loss=6.3391
	step [76/251], loss=7.6572
	step [77/251], loss=6.0710
	step [78/251], loss=6.1478
	step [79/251], loss=6.4435
	step [80/251], loss=6.5955
	step [81/251], loss=7.1777
	step [82/251], loss=6.7015
	step [83/251], loss=7.5661
	step [84/251], loss=6.4493
	step [85/251], loss=5.0471
	step [86/251], loss=6.1309
	step [87/251], loss=6.7293
	step [88/251], loss=6.5251
	step [89/251], loss=6.3797
	step [90/251], loss=5.2383
	step [91/251], loss=7.3600
	step [92/251], loss=5.5780
	step [93/251], loss=6.3790
	step [94/251], loss=6.3478
	step [95/251], loss=5.4746
	step [96/251], loss=7.6662
	step [97/251], loss=5.4926
	step [98/251], loss=6.3404
	step [99/251], loss=6.3406
	step [100/251], loss=5.1409
	step [101/251], loss=6.5613
	step [102/251], loss=7.1735
	step [103/251], loss=5.4236
	step [104/251], loss=5.6697
	step [105/251], loss=5.9404
	step [106/251], loss=5.9214
	step [107/251], loss=7.2110
	step [108/251], loss=6.9564
	step [109/251], loss=6.4789
	step [110/251], loss=7.1522
	step [111/251], loss=6.9266
	step [112/251], loss=5.2189
	step [113/251], loss=4.8851
	step [114/251], loss=5.6155
	step [115/251], loss=6.2718
	step [116/251], loss=6.7110
	step [117/251], loss=5.3039
	step [118/251], loss=6.5814
	step [119/251], loss=5.5297
	step [120/251], loss=5.3282
	step [121/251], loss=5.5723
	step [122/251], loss=7.2212
	step [123/251], loss=8.1711
	step [124/251], loss=6.5845
	step [125/251], loss=6.4691
	step [126/251], loss=6.0687
	step [127/251], loss=5.7625
	step [128/251], loss=6.8060
	step [129/251], loss=6.4641
	step [130/251], loss=7.5376
	step [131/251], loss=6.5317
	step [132/251], loss=7.5565
	step [133/251], loss=6.1231
	step [134/251], loss=7.2651
	step [135/251], loss=7.5853
	step [136/251], loss=5.0476
	step [137/251], loss=6.7663
	step [138/251], loss=6.4875
	step [139/251], loss=5.5760
	step [140/251], loss=5.7478
	step [141/251], loss=6.0065
	step [142/251], loss=5.5626
	step [143/251], loss=5.3192
	step [144/251], loss=7.1619
	step [145/251], loss=6.6818
	step [146/251], loss=5.7482
	step [147/251], loss=6.7003
	step [148/251], loss=7.0880
	step [149/251], loss=6.1679
	step [150/251], loss=7.9310
	step [151/251], loss=5.8169
	step [152/251], loss=6.4732
	step [153/251], loss=7.6299
	step [154/251], loss=5.8598
	step [155/251], loss=5.3172
	step [156/251], loss=6.0647
	step [157/251], loss=4.9313
	step [158/251], loss=6.1408
	step [159/251], loss=6.4115
	step [160/251], loss=6.2128
	step [161/251], loss=7.6054
	step [162/251], loss=7.1248
	step [163/251], loss=5.2705
	step [164/251], loss=6.1950
	step [165/251], loss=7.0919
	step [166/251], loss=5.8715
	step [167/251], loss=7.1923
	step [168/251], loss=6.1651
	step [169/251], loss=5.5802
	step [170/251], loss=4.6028
	step [171/251], loss=5.3787
	step [172/251], loss=7.6167
	step [173/251], loss=6.1547
	step [174/251], loss=5.2147
	step [175/251], loss=5.6786
	step [176/251], loss=7.3897
	step [177/251], loss=5.4656
	step [178/251], loss=6.3356
	step [179/251], loss=6.8310
	step [180/251], loss=6.0305
	step [181/251], loss=7.0300
	step [182/251], loss=6.0289
	step [183/251], loss=6.3690
	step [184/251], loss=5.8701
	step [185/251], loss=5.6562
	step [186/251], loss=7.5997
	step [187/251], loss=5.9196
	step [188/251], loss=7.0578
	step [189/251], loss=6.9967
	step [190/251], loss=5.8059
	step [191/251], loss=6.7325
	step [192/251], loss=6.8965
	step [193/251], loss=5.3618
	step [194/251], loss=6.2412
	step [195/251], loss=5.8430
	step [196/251], loss=5.7376
	step [197/251], loss=6.4769
	step [198/251], loss=6.7801
	step [199/251], loss=6.8324
	step [200/251], loss=7.9838
	step [201/251], loss=5.7004
	step [202/251], loss=6.5304
	step [203/251], loss=4.8191
	step [204/251], loss=6.1122
	step [205/251], loss=6.9770
	step [206/251], loss=4.5321
	step [207/251], loss=5.9615
	step [208/251], loss=5.6293
	step [209/251], loss=6.9160
	step [210/251], loss=6.9839
	step [211/251], loss=8.2966
	step [212/251], loss=6.2193
	step [213/251], loss=4.4481
	step [214/251], loss=6.8375
	step [215/251], loss=6.8761
	step [216/251], loss=5.9253
	step [217/251], loss=7.4942
	step [218/251], loss=6.1931
	step [219/251], loss=4.9567
	step [220/251], loss=6.8632
	step [221/251], loss=5.4537
	step [222/251], loss=7.8903
	step [223/251], loss=7.1094
	step [224/251], loss=6.0719
	step [225/251], loss=8.1854
	step [226/251], loss=6.0452
	step [227/251], loss=8.0150
	step [228/251], loss=5.6814
	step [229/251], loss=7.1547
	step [230/251], loss=8.1083
	step [231/251], loss=5.6346
	step [232/251], loss=8.7769
	step [233/251], loss=5.9621
	step [234/251], loss=7.5407
	step [235/251], loss=6.8306
	step [236/251], loss=6.4959
	step [237/251], loss=6.4939
	step [238/251], loss=7.3908
	step [239/251], loss=5.8852
	step [240/251], loss=6.7098
	step [241/251], loss=6.6098
	step [242/251], loss=6.9932
	step [243/251], loss=5.2630
	step [244/251], loss=5.3628
	step [245/251], loss=5.4177
	step [246/251], loss=7.6184
	step [247/251], loss=5.1909
	step [248/251], loss=8.7773
	step [249/251], loss=5.7869
	step [250/251], loss=7.6443
	step [251/251], loss=1.5613
	Evaluating
	loss=0.0224, precision=0.2351, recall=0.9961, f1=0.3805
Training epoch 27
	step [1/251], loss=6.2279
	step [2/251], loss=6.5533
	step [3/251], loss=7.7124
	step [4/251], loss=5.9568
	step [5/251], loss=6.2882
	step [6/251], loss=7.0914
	step [7/251], loss=5.5707
	step [8/251], loss=5.3877
	step [9/251], loss=5.8983
	step [10/251], loss=6.9394
	step [11/251], loss=5.4165
	step [12/251], loss=5.7419
	step [13/251], loss=5.8007
	step [14/251], loss=6.2657
	step [15/251], loss=7.1660
	step [16/251], loss=4.6200
	step [17/251], loss=5.1753
	step [18/251], loss=5.9070
	step [19/251], loss=4.8244
	step [20/251], loss=7.5536
	step [21/251], loss=6.3073
	step [22/251], loss=7.0100
	step [23/251], loss=6.7614
	step [24/251], loss=7.6382
	step [25/251], loss=6.1439
	step [26/251], loss=6.0126
	step [27/251], loss=5.9688
	step [28/251], loss=7.0029
	step [29/251], loss=6.4612
	step [30/251], loss=6.0486
	step [31/251], loss=6.7771
	step [32/251], loss=6.5491
	step [33/251], loss=7.8913
	step [34/251], loss=5.0303
	step [35/251], loss=4.8339
	step [36/251], loss=7.2622
	step [37/251], loss=5.9013
	step [38/251], loss=5.6727
	step [39/251], loss=6.6127
	step [40/251], loss=6.0947
	step [41/251], loss=7.2790
	step [42/251], loss=4.5499
	step [43/251], loss=6.4028
	step [44/251], loss=7.9444
	step [45/251], loss=5.6480
	step [46/251], loss=5.6475
	step [47/251], loss=6.2156
	step [48/251], loss=6.3513
	step [49/251], loss=5.5490
	step [50/251], loss=5.6264
	step [51/251], loss=5.6459
	step [52/251], loss=6.5862
	step [53/251], loss=6.1899
	step [54/251], loss=5.6491
	step [55/251], loss=6.3791
	step [56/251], loss=8.3341
	step [57/251], loss=6.7688
	step [58/251], loss=5.5507
	step [59/251], loss=6.6459
	step [60/251], loss=3.9436
	step [61/251], loss=6.4544
	step [62/251], loss=6.6099
	step [63/251], loss=6.2385
	step [64/251], loss=6.2424
	step [65/251], loss=5.4403
	step [66/251], loss=7.0777
	step [67/251], loss=7.2182
	step [68/251], loss=4.5536
	step [69/251], loss=6.4106
	step [70/251], loss=7.1675
	step [71/251], loss=6.5464
	step [72/251], loss=7.9863
	step [73/251], loss=7.2275
	step [74/251], loss=5.7380
	step [75/251], loss=8.0627
	step [76/251], loss=6.7275
	step [77/251], loss=5.4183
	step [78/251], loss=5.9507
	step [79/251], loss=5.6506
	step [80/251], loss=5.6989
	step [81/251], loss=6.0734
	step [82/251], loss=6.3924
	step [83/251], loss=6.2322
	step [84/251], loss=5.6787
	step [85/251], loss=6.7677
	step [86/251], loss=5.7675
	step [87/251], loss=7.2385
	step [88/251], loss=5.3029
	step [89/251], loss=6.9494
	step [90/251], loss=7.4210
	step [91/251], loss=6.3821
	step [92/251], loss=5.4056
	step [93/251], loss=5.8236
	step [94/251], loss=6.9207
	step [95/251], loss=6.0896
	step [96/251], loss=6.5810
	step [97/251], loss=6.1807
	step [98/251], loss=4.9745
	step [99/251], loss=7.3986
	step [100/251], loss=7.2651
	step [101/251], loss=6.7626
	step [102/251], loss=6.4018
	step [103/251], loss=6.2392
	step [104/251], loss=6.2271
	step [105/251], loss=6.4813
	step [106/251], loss=6.0863
	step [107/251], loss=6.3832
	step [108/251], loss=3.8453
	step [109/251], loss=6.7794
	step [110/251], loss=7.2852
	step [111/251], loss=6.3178
	step [112/251], loss=4.9788
	step [113/251], loss=5.3814
	step [114/251], loss=5.4716
	step [115/251], loss=5.7529
	step [116/251], loss=6.1893
	step [117/251], loss=6.4483
	step [118/251], loss=6.1279
	step [119/251], loss=6.5275
	step [120/251], loss=6.8871
	step [121/251], loss=7.3576
	step [122/251], loss=7.0396
	step [123/251], loss=6.2961
	step [124/251], loss=5.0022
	step [125/251], loss=6.6200
	step [126/251], loss=5.7858
	step [127/251], loss=4.6671
	step [128/251], loss=6.0274
	step [129/251], loss=5.6573
	step [130/251], loss=5.9144
	step [131/251], loss=5.7455
	step [132/251], loss=4.5344
	step [133/251], loss=6.3348
	step [134/251], loss=6.4874
	step [135/251], loss=7.2801
	step [136/251], loss=5.8297
	step [137/251], loss=7.5152
	step [138/251], loss=9.6578
	step [139/251], loss=7.4281
	step [140/251], loss=6.0209
	step [141/251], loss=6.3619
	step [142/251], loss=4.7158
	step [143/251], loss=5.8251
	step [144/251], loss=5.6654
	step [145/251], loss=5.7314
	step [146/251], loss=6.9741
	step [147/251], loss=7.6205
	step [148/251], loss=7.3093
	step [149/251], loss=7.3566
	step [150/251], loss=7.4819
	step [151/251], loss=8.5744
	step [152/251], loss=5.9768
	step [153/251], loss=6.6181
	step [154/251], loss=7.2680
	step [155/251], loss=5.2643
	step [156/251], loss=7.4959
	step [157/251], loss=5.1455
	step [158/251], loss=5.7543
	step [159/251], loss=4.5944
	step [160/251], loss=6.6647
	step [161/251], loss=6.6356
	step [162/251], loss=6.0471
	step [163/251], loss=6.0197
	step [164/251], loss=6.9177
	step [165/251], loss=5.1528
	step [166/251], loss=5.3023
	step [167/251], loss=6.3098
	step [168/251], loss=5.3132
	step [169/251], loss=5.4817
	step [170/251], loss=4.9218
	step [171/251], loss=5.9238
	step [172/251], loss=7.4637
	step [173/251], loss=4.8778
	step [174/251], loss=5.8930
	step [175/251], loss=6.1765
	step [176/251], loss=5.1265
	step [177/251], loss=5.1021
	step [178/251], loss=5.7748
	step [179/251], loss=5.6801
	step [180/251], loss=6.7010
	step [181/251], loss=7.7038
	step [182/251], loss=5.0641
	step [183/251], loss=7.4193
	step [184/251], loss=4.9196
	step [185/251], loss=6.8325
	step [186/251], loss=5.2242
	step [187/251], loss=7.1315
	step [188/251], loss=6.1932
	step [189/251], loss=6.7179
	step [190/251], loss=5.9363
	step [191/251], loss=5.6972
	step [192/251], loss=4.7384
	step [193/251], loss=6.1576
	step [194/251], loss=7.0734
	step [195/251], loss=6.5702
	step [196/251], loss=6.8490
	step [197/251], loss=7.9299
	step [198/251], loss=6.4538
	step [199/251], loss=8.3253
	step [200/251], loss=6.5238
	step [201/251], loss=6.1172
	step [202/251], loss=6.4536
	step [203/251], loss=8.0048
	step [204/251], loss=6.5249
	step [205/251], loss=6.0238
	step [206/251], loss=6.4441
	step [207/251], loss=5.7503
	step [208/251], loss=7.1918
	step [209/251], loss=6.3937
	step [210/251], loss=5.9087
	step [211/251], loss=6.1787
	step [212/251], loss=7.2284
	step [213/251], loss=5.8318
	step [214/251], loss=6.0346
	step [215/251], loss=6.9637
	step [216/251], loss=5.7059
	step [217/251], loss=5.7230
	step [218/251], loss=4.9994
	step [219/251], loss=5.9616
	step [220/251], loss=4.1102
	step [221/251], loss=6.5447
	step [222/251], loss=6.3240
	step [223/251], loss=6.3496
	step [224/251], loss=7.3232
	step [225/251], loss=6.7120
	step [226/251], loss=7.5895
	step [227/251], loss=5.2857
	step [228/251], loss=6.6629
	step [229/251], loss=6.7296
	step [230/251], loss=7.0019
	step [231/251], loss=5.5978
	step [232/251], loss=6.1945
	step [233/251], loss=6.9109
	step [234/251], loss=8.2064
	step [235/251], loss=9.1631
	step [236/251], loss=4.7397
	step [237/251], loss=6.3331
	step [238/251], loss=7.6066
	step [239/251], loss=4.9769
	step [240/251], loss=4.8930
	step [241/251], loss=4.8821
	step [242/251], loss=7.1981
	step [243/251], loss=5.4623
	step [244/251], loss=7.2710
	step [245/251], loss=6.9324
	step [246/251], loss=5.0540
	step [247/251], loss=8.9050
	step [248/251], loss=5.2351
	step [249/251], loss=6.0371
	step [250/251], loss=5.6664
	step [251/251], loss=0.6023
	Evaluating
	loss=0.0200, precision=0.2212, recall=0.9962, f1=0.3621
Training epoch 28
	step [1/251], loss=4.9895
	step [2/251], loss=6.0322
	step [3/251], loss=6.3082
	step [4/251], loss=5.8417
	step [5/251], loss=6.0997
	step [6/251], loss=5.9209
	step [7/251], loss=4.7402
	step [8/251], loss=5.7083
	step [9/251], loss=8.3637
	step [10/251], loss=5.7684
	step [11/251], loss=5.4312
	step [12/251], loss=7.8086
	step [13/251], loss=5.8207
	step [14/251], loss=6.3186
	step [15/251], loss=5.2066
	step [16/251], loss=5.2374
	step [17/251], loss=7.5525
	step [18/251], loss=6.2071
	step [19/251], loss=6.4621
	step [20/251], loss=7.6204
	step [21/251], loss=8.1254
	step [22/251], loss=5.7665
	step [23/251], loss=5.6050
	step [24/251], loss=5.0837
	step [25/251], loss=5.1996
	step [26/251], loss=5.4324
	step [27/251], loss=5.9274
	step [28/251], loss=7.3370
	step [29/251], loss=6.6501
	step [30/251], loss=7.0752
	step [31/251], loss=6.1300
	step [32/251], loss=7.4547
	step [33/251], loss=6.6129
	step [34/251], loss=5.5483
	step [35/251], loss=5.9092
	step [36/251], loss=6.5946
	step [37/251], loss=4.6997
	step [38/251], loss=6.8900
	step [39/251], loss=6.2080
	step [40/251], loss=6.1233
	step [41/251], loss=5.0050
	step [42/251], loss=8.1113
	step [43/251], loss=8.4254
	step [44/251], loss=6.8065
	step [45/251], loss=5.5610
	step [46/251], loss=5.7623
	step [47/251], loss=5.1216
	step [48/251], loss=5.3726
	step [49/251], loss=5.1036
	step [50/251], loss=6.0877
	step [51/251], loss=6.1058
	step [52/251], loss=5.4769
	step [53/251], loss=6.7857
	step [54/251], loss=8.2407
	step [55/251], loss=7.6419
	step [56/251], loss=5.4699
	step [57/251], loss=4.9300
	step [58/251], loss=6.6853
	step [59/251], loss=5.4778
	step [60/251], loss=5.6390
	step [61/251], loss=6.2927
	step [62/251], loss=6.2733
	step [63/251], loss=6.2289
	step [64/251], loss=5.1349
	step [65/251], loss=6.8853
	step [66/251], loss=5.8968
	step [67/251], loss=6.2949
	step [68/251], loss=4.4026
	step [69/251], loss=5.0675
	step [70/251], loss=5.5539
	step [71/251], loss=6.7610
	step [72/251], loss=5.3687
	step [73/251], loss=5.7512
	step [74/251], loss=6.0868
	step [75/251], loss=8.1924
	step [76/251], loss=6.6435
	step [77/251], loss=5.3556
	step [78/251], loss=4.6636
	step [79/251], loss=6.0338
	step [80/251], loss=5.7872
	step [81/251], loss=6.4598
	step [82/251], loss=4.8829
	step [83/251], loss=5.7466
	step [84/251], loss=6.7932
	step [85/251], loss=5.9836
	step [86/251], loss=5.8371
	step [87/251], loss=7.0208
	step [88/251], loss=5.6582
	step [89/251], loss=5.2560
	step [90/251], loss=4.8221
	step [91/251], loss=6.2249
	step [92/251], loss=6.7369
	step [93/251], loss=8.0143
	step [94/251], loss=4.9525
	step [95/251], loss=6.8541
	step [96/251], loss=5.5073
	step [97/251], loss=5.3489
	step [98/251], loss=6.6782
	step [99/251], loss=6.0814
	step [100/251], loss=8.4845
	step [101/251], loss=6.4285
	step [102/251], loss=5.5497
	step [103/251], loss=5.0491
	step [104/251], loss=5.9449
	step [105/251], loss=4.9957
	step [106/251], loss=6.7810
	step [107/251], loss=6.8551
	step [108/251], loss=6.8526
	step [109/251], loss=5.3729
	step [110/251], loss=4.9179
	step [111/251], loss=5.4501
	step [112/251], loss=7.9572
	step [113/251], loss=8.5655
	step [114/251], loss=7.1859
	step [115/251], loss=5.5708
	step [116/251], loss=6.4585
	step [117/251], loss=7.0068
	step [118/251], loss=7.3722
	step [119/251], loss=5.9376
	step [120/251], loss=7.2802
	step [121/251], loss=6.7446
	step [122/251], loss=5.7885
	step [123/251], loss=5.5086
	step [124/251], loss=7.6189
	step [125/251], loss=7.0539
	step [126/251], loss=6.1009
	step [127/251], loss=8.0266
	step [128/251], loss=5.7726
	step [129/251], loss=6.0783
	step [130/251], loss=5.9384
	step [131/251], loss=7.1634
	step [132/251], loss=7.2029
	step [133/251], loss=6.4219
	step [134/251], loss=6.9743
	step [135/251], loss=7.1037
	step [136/251], loss=6.4020
	step [137/251], loss=5.3892
	step [138/251], loss=6.0138
	step [139/251], loss=6.7779
	step [140/251], loss=7.0359
	step [141/251], loss=6.1690
	step [142/251], loss=7.2435
	step [143/251], loss=5.6367
	step [144/251], loss=5.9332
	step [145/251], loss=5.8699
	step [146/251], loss=6.7111
	step [147/251], loss=4.6760
	step [148/251], loss=4.4993
	step [149/251], loss=6.6130
	step [150/251], loss=6.5511
	step [151/251], loss=7.2645
	step [152/251], loss=6.2246
	step [153/251], loss=5.5656
	step [154/251], loss=6.6242
	step [155/251], loss=4.5975
	step [156/251], loss=6.5857
	step [157/251], loss=5.9358
	step [158/251], loss=7.6748
	step [159/251], loss=4.9396
	step [160/251], loss=4.7337
	step [161/251], loss=3.8931
	step [162/251], loss=5.4961
	step [163/251], loss=8.0716
	step [164/251], loss=7.2303
	step [165/251], loss=6.5242
	step [166/251], loss=6.4457
	step [167/251], loss=7.4034
	step [168/251], loss=6.7767
	step [169/251], loss=6.5366
	step [170/251], loss=6.5247
	step [171/251], loss=5.3377
	step [172/251], loss=6.6573
	step [173/251], loss=6.8389
	step [174/251], loss=4.7864
	step [175/251], loss=6.1344
	step [176/251], loss=6.7896
	step [177/251], loss=6.1021
	step [178/251], loss=5.8684
	step [179/251], loss=6.4214
	step [180/251], loss=4.6153
	step [181/251], loss=4.8090
	step [182/251], loss=5.8385
	step [183/251], loss=6.1618
	step [184/251], loss=7.2570
	step [185/251], loss=6.0725
	step [186/251], loss=5.2492
	step [187/251], loss=5.2615
	step [188/251], loss=5.9889
	step [189/251], loss=5.8516
	step [190/251], loss=4.6883
	step [191/251], loss=6.4312
	step [192/251], loss=4.5803
	step [193/251], loss=5.4462
	step [194/251], loss=6.1112
	step [195/251], loss=6.0999
	step [196/251], loss=6.5045
	step [197/251], loss=6.9376
	step [198/251], loss=5.1635
	step [199/251], loss=5.3846
	step [200/251], loss=4.9374
	step [201/251], loss=6.4746
	step [202/251], loss=6.2754
	step [203/251], loss=8.0581
	step [204/251], loss=4.9083
	step [205/251], loss=6.6126
	step [206/251], loss=5.5862
	step [207/251], loss=6.8360
	step [208/251], loss=6.5771
	step [209/251], loss=5.3393
	step [210/251], loss=6.2769
	step [211/251], loss=5.8613
	step [212/251], loss=5.4191
	step [213/251], loss=6.4153
	step [214/251], loss=6.5460
	step [215/251], loss=5.6969
	step [216/251], loss=6.1046
	step [217/251], loss=6.0169
	step [218/251], loss=6.4849
	step [219/251], loss=6.4419
	step [220/251], loss=6.1548
	step [221/251], loss=4.8421
	step [222/251], loss=7.2936
	step [223/251], loss=5.5109
	step [224/251], loss=7.3330
	step [225/251], loss=6.1482
	step [226/251], loss=5.9062
	step [227/251], loss=6.0498
	step [228/251], loss=4.9895
	step [229/251], loss=6.3168
	step [230/251], loss=5.8142
	step [231/251], loss=7.3118
	step [232/251], loss=5.9035
	step [233/251], loss=8.7795
	step [234/251], loss=6.0565
	step [235/251], loss=4.1996
	step [236/251], loss=5.8729
	step [237/251], loss=6.3040
	step [238/251], loss=7.8111
	step [239/251], loss=7.2009
	step [240/251], loss=6.1708
	step [241/251], loss=5.7415
	step [242/251], loss=6.2967
	step [243/251], loss=6.5856
	step [244/251], loss=7.1057
	step [245/251], loss=7.4628
	step [246/251], loss=6.0835
	step [247/251], loss=6.2399
	step [248/251], loss=5.9621
	step [249/251], loss=7.2009
	step [250/251], loss=5.8677
	step [251/251], loss=0.8147
	Evaluating
	loss=0.0211, precision=0.2263, recall=0.9966, f1=0.3689
Training epoch 29
	step [1/251], loss=7.1733
	step [2/251], loss=6.8247
	step [3/251], loss=4.8065
	step [4/251], loss=5.2678
	step [5/251], loss=5.2703
	step [6/251], loss=5.7397
	step [7/251], loss=6.2283
	step [8/251], loss=5.7829
	step [9/251], loss=5.5957
	step [10/251], loss=6.0506
	step [11/251], loss=5.7919
	step [12/251], loss=5.1057
	step [13/251], loss=6.6950
	step [14/251], loss=3.9010
	step [15/251], loss=5.3695
	step [16/251], loss=5.3869
	step [17/251], loss=5.4001
	step [18/251], loss=6.3592
	step [19/251], loss=6.5754
	step [20/251], loss=6.3980
	step [21/251], loss=5.0457
	step [22/251], loss=5.9224
	step [23/251], loss=6.4974
	step [24/251], loss=6.4208
	step [25/251], loss=5.0681
	step [26/251], loss=5.5771
	step [27/251], loss=6.1312
	step [28/251], loss=6.9693
	step [29/251], loss=5.1127
	step [30/251], loss=4.8469
	step [31/251], loss=7.2745
	step [32/251], loss=4.8129
	step [33/251], loss=5.6077
	step [34/251], loss=6.8966
	step [35/251], loss=5.4217
	step [36/251], loss=6.4142
	step [37/251], loss=5.1619
	step [38/251], loss=6.2084
	step [39/251], loss=5.3975
	step [40/251], loss=5.2848
	step [41/251], loss=6.6862
	step [42/251], loss=5.9677
	step [43/251], loss=6.2870
	step [44/251], loss=5.4466
	step [45/251], loss=5.6017
	step [46/251], loss=5.8008
	step [47/251], loss=6.3547
	step [48/251], loss=6.7279
	step [49/251], loss=5.0738
	step [50/251], loss=5.6345
	step [51/251], loss=7.1505
	step [52/251], loss=5.5496
	step [53/251], loss=5.1645
	step [54/251], loss=7.6095
	step [55/251], loss=6.1959
	step [56/251], loss=4.8447
	step [57/251], loss=5.5925
	step [58/251], loss=6.8345
	step [59/251], loss=6.3570
	step [60/251], loss=4.9139
	step [61/251], loss=5.7215
	step [62/251], loss=5.7215
	step [63/251], loss=5.3593
	step [64/251], loss=6.1647
	step [65/251], loss=5.6302
	step [66/251], loss=8.6416
	step [67/251], loss=5.9795
	step [68/251], loss=5.2453
	step [69/251], loss=6.0306
	step [70/251], loss=6.4536
	step [71/251], loss=5.5888
	step [72/251], loss=6.0595
	step [73/251], loss=6.3859
	step [74/251], loss=6.5214
	step [75/251], loss=7.2365
	step [76/251], loss=6.1565
	step [77/251], loss=5.4908
	step [78/251], loss=5.9448
	step [79/251], loss=5.6537
	step [80/251], loss=5.9943
	step [81/251], loss=5.3012
	step [82/251], loss=6.2428
	step [83/251], loss=5.5161
	step [84/251], loss=6.4839
	step [85/251], loss=6.7160
	step [86/251], loss=6.5224
	step [87/251], loss=5.7265
	step [88/251], loss=6.9133
	step [89/251], loss=6.2132
	step [90/251], loss=6.7090
	step [91/251], loss=5.5492
	step [92/251], loss=6.1300
	step [93/251], loss=6.8317
	step [94/251], loss=5.7394
	step [95/251], loss=7.2445
	step [96/251], loss=6.3465
	step [97/251], loss=4.9087
	step [98/251], loss=6.5092
	step [99/251], loss=5.7225
	step [100/251], loss=4.8970
	step [101/251], loss=5.2134
	step [102/251], loss=4.5206
	step [103/251], loss=4.2129
	step [104/251], loss=6.5412
	step [105/251], loss=5.3779
	step [106/251], loss=4.8677
	step [107/251], loss=7.2895
	step [108/251], loss=6.0357
	step [109/251], loss=5.1695
	step [110/251], loss=5.2286
	step [111/251], loss=6.5448
	step [112/251], loss=5.3446
	step [113/251], loss=5.4455
	step [114/251], loss=6.2703
	step [115/251], loss=6.3594
	step [116/251], loss=5.2719
	step [117/251], loss=5.8955
	step [118/251], loss=8.1899
	step [119/251], loss=5.2220
	step [120/251], loss=7.9480
	step [121/251], loss=6.0577
	step [122/251], loss=7.4116
	step [123/251], loss=6.6282
	step [124/251], loss=6.1320
	step [125/251], loss=6.1760
	step [126/251], loss=7.2866
	step [127/251], loss=4.9611
	step [128/251], loss=6.0247
	step [129/251], loss=5.5858
	step [130/251], loss=6.2966
	step [131/251], loss=4.7550
	step [132/251], loss=6.2816
	step [133/251], loss=5.5493
	step [134/251], loss=6.1257
	step [135/251], loss=5.3778
	step [136/251], loss=5.0187
	step [137/251], loss=7.3961
	step [138/251], loss=6.2733
	step [139/251], loss=6.0526
	step [140/251], loss=6.7844
	step [141/251], loss=5.8454
	step [142/251], loss=5.3405
	step [143/251], loss=5.3826
	step [144/251], loss=8.0353
	step [145/251], loss=5.8051
	step [146/251], loss=6.7298
	step [147/251], loss=5.8799
	step [148/251], loss=5.4731
	step [149/251], loss=6.4642
	step [150/251], loss=6.3773
	step [151/251], loss=5.2818
	step [152/251], loss=7.1595
	step [153/251], loss=5.8149
	step [154/251], loss=6.2076
	step [155/251], loss=6.0976
	step [156/251], loss=6.1647
	step [157/251], loss=5.7954
	step [158/251], loss=5.9760
	step [159/251], loss=6.1152
	step [160/251], loss=5.4801
	step [161/251], loss=5.1203
	step [162/251], loss=6.2661
	step [163/251], loss=5.8703
	step [164/251], loss=6.4861
	step [165/251], loss=6.0951
	step [166/251], loss=6.0765
	step [167/251], loss=8.3175
	step [168/251], loss=5.4334
	step [169/251], loss=6.5413
	step [170/251], loss=6.5721
	step [171/251], loss=5.1972
	step [172/251], loss=6.1982
	step [173/251], loss=6.1413
	step [174/251], loss=5.7488
	step [175/251], loss=6.9790
	step [176/251], loss=4.8907
	step [177/251], loss=5.2187
	step [178/251], loss=8.6755
	step [179/251], loss=7.2514
	step [180/251], loss=6.4477
	step [181/251], loss=7.8362
	step [182/251], loss=7.2905
	step [183/251], loss=5.2147
	step [184/251], loss=5.5311
	step [185/251], loss=4.9810
	step [186/251], loss=6.2066
	step [187/251], loss=5.6310
	step [188/251], loss=8.1383
	step [189/251], loss=6.6948
	step [190/251], loss=5.8368
	step [191/251], loss=7.3293
	step [192/251], loss=6.5782
	step [193/251], loss=6.1896
	step [194/251], loss=5.9722
	step [195/251], loss=6.1798
	step [196/251], loss=5.7019
	step [197/251], loss=7.0150
	step [198/251], loss=6.4911
	step [199/251], loss=6.7716
	step [200/251], loss=6.1985
	step [201/251], loss=5.8239
	step [202/251], loss=7.0340
	step [203/251], loss=6.4163
	step [204/251], loss=6.0807
	step [205/251], loss=6.8265
	step [206/251], loss=5.5927
	step [207/251], loss=7.5180
	step [208/251], loss=5.2321
	step [209/251], loss=5.0812
	step [210/251], loss=6.6017
	step [211/251], loss=5.5370
	step [212/251], loss=6.6777
	step [213/251], loss=6.7148
	step [214/251], loss=5.3126
	step [215/251], loss=5.9465
	step [216/251], loss=5.4630
	step [217/251], loss=5.7705
	step [218/251], loss=7.4541
	step [219/251], loss=7.0901
	step [220/251], loss=5.2949
	step [221/251], loss=6.3601
	step [222/251], loss=8.5339
	step [223/251], loss=6.5819
	step [224/251], loss=5.7568
	step [225/251], loss=6.9035
	step [226/251], loss=6.1168
	step [227/251], loss=6.4283
	step [228/251], loss=6.5273
	step [229/251], loss=5.8811
	step [230/251], loss=6.4688
	step [231/251], loss=6.7532
	step [232/251], loss=7.4708
	step [233/251], loss=5.8553
	step [234/251], loss=5.0481
	step [235/251], loss=6.9855
	step [236/251], loss=6.6641
	step [237/251], loss=5.6447
	step [238/251], loss=6.1551
	step [239/251], loss=5.9018
	step [240/251], loss=5.9089
	step [241/251], loss=5.2171
	step [242/251], loss=5.7579
	step [243/251], loss=5.2743
	step [244/251], loss=6.9551
	step [245/251], loss=6.2678
	step [246/251], loss=4.5539
	step [247/251], loss=6.0204
	step [248/251], loss=5.8338
	step [249/251], loss=9.3093
	step [250/251], loss=5.5588
	step [251/251], loss=1.7084
	Evaluating
	loss=0.0259, precision=0.1934, recall=0.9972, f1=0.3239
Training epoch 30
	step [1/251], loss=5.2074
	step [2/251], loss=5.4253
	step [3/251], loss=5.3356
	step [4/251], loss=6.3568
	step [5/251], loss=6.1849
	step [6/251], loss=6.1568
	step [7/251], loss=5.8149
	step [8/251], loss=7.9409
	step [9/251], loss=5.7412
	step [10/251], loss=5.3561
	step [11/251], loss=5.2874
	step [12/251], loss=6.2039
	step [13/251], loss=6.1759
	step [14/251], loss=5.2349
	step [15/251], loss=6.5184
	step [16/251], loss=6.5178
	step [17/251], loss=6.0314
	step [18/251], loss=6.7430
	step [19/251], loss=5.7709
	step [20/251], loss=6.1214
	step [21/251], loss=5.6581
	step [22/251], loss=9.5676
	step [23/251], loss=4.8549
	step [24/251], loss=6.5032
	step [25/251], loss=5.4030
	step [26/251], loss=5.1398
	step [27/251], loss=6.0689
	step [28/251], loss=6.5302
	step [29/251], loss=6.3883
	step [30/251], loss=6.3392
	step [31/251], loss=6.9193
	step [32/251], loss=6.5324
	step [33/251], loss=4.2170
	step [34/251], loss=5.4378
	step [35/251], loss=5.0113
	step [36/251], loss=6.2713
	step [37/251], loss=5.8306
	step [38/251], loss=4.8634
	step [39/251], loss=6.6025
	step [40/251], loss=6.1086
	step [41/251], loss=6.6599
	step [42/251], loss=6.4284
	step [43/251], loss=6.4331
	step [44/251], loss=6.1592
	step [45/251], loss=6.3017
	step [46/251], loss=7.2160
	step [47/251], loss=6.3146
	step [48/251], loss=5.7544
	step [49/251], loss=5.8260
	step [50/251], loss=5.4584
	step [51/251], loss=6.3685
	step [52/251], loss=6.5229
	step [53/251], loss=5.1907
	step [54/251], loss=8.2677
	step [55/251], loss=6.2495
	step [56/251], loss=5.2030
	step [57/251], loss=5.5847
	step [58/251], loss=6.6700
	step [59/251], loss=4.5084
	step [60/251], loss=5.2490
	step [61/251], loss=8.4538
	step [62/251], loss=5.8155
	step [63/251], loss=4.8868
	step [64/251], loss=7.4865
	step [65/251], loss=6.1350
	step [66/251], loss=6.4067
	step [67/251], loss=7.6136
	step [68/251], loss=6.1640
	step [69/251], loss=5.8746
	step [70/251], loss=5.9887
	step [71/251], loss=7.5574
	step [72/251], loss=6.1624
	step [73/251], loss=6.5477
	step [74/251], loss=8.1650
	step [75/251], loss=4.9997
	step [76/251], loss=5.5610
	step [77/251], loss=6.7757
	step [78/251], loss=4.7446
	step [79/251], loss=4.9653
	step [80/251], loss=5.5872
	step [81/251], loss=4.3625
	step [82/251], loss=6.4843
	step [83/251], loss=8.2188
	step [84/251], loss=7.3354
	step [85/251], loss=6.8816
	step [86/251], loss=5.6142
	step [87/251], loss=6.8055
	step [88/251], loss=5.5917
	step [89/251], loss=5.2259
	step [90/251], loss=5.6652
	step [91/251], loss=4.7620
	step [92/251], loss=5.9059
	step [93/251], loss=4.9060
	step [94/251], loss=5.8590
	step [95/251], loss=6.0400
	step [96/251], loss=7.3493
	step [97/251], loss=6.8368
	step [98/251], loss=3.9842
	step [99/251], loss=6.7516
	step [100/251], loss=5.7788
	step [101/251], loss=6.1398
	step [102/251], loss=5.8822
	step [103/251], loss=5.7264
	step [104/251], loss=6.4330
	step [105/251], loss=5.8106
	step [106/251], loss=8.7405
	step [107/251], loss=5.6544
	step [108/251], loss=4.3473
	step [109/251], loss=5.1339
	step [110/251], loss=6.1070
	step [111/251], loss=5.4895
	step [112/251], loss=5.6771
	step [113/251], loss=6.8382
	step [114/251], loss=5.6704
	step [115/251], loss=6.5748
	step [116/251], loss=6.8674
	step [117/251], loss=6.0849
	step [118/251], loss=5.2974
	step [119/251], loss=6.3437
	step [120/251], loss=6.2846
	step [121/251], loss=6.2861
	step [122/251], loss=5.7797
	step [123/251], loss=7.1280
	step [124/251], loss=5.2332
	step [125/251], loss=4.4951
	step [126/251], loss=5.8174
	step [127/251], loss=7.7757
	step [128/251], loss=5.6760
	step [129/251], loss=4.1590
	step [130/251], loss=5.0909
	step [131/251], loss=6.3897
	step [132/251], loss=4.6605
	step [133/251], loss=5.6195
	step [134/251], loss=5.5988
	step [135/251], loss=5.6926
	step [136/251], loss=6.6660
	step [137/251], loss=5.1775
	step [138/251], loss=5.6326
	step [139/251], loss=5.7061
	step [140/251], loss=6.0881
	step [141/251], loss=5.1698
	step [142/251], loss=6.5012
	step [143/251], loss=6.8042
	step [144/251], loss=7.4587
	step [145/251], loss=6.2235
	step [146/251], loss=5.6790
	step [147/251], loss=6.6125
	step [148/251], loss=7.6956
	step [149/251], loss=8.2654
	step [150/251], loss=5.8072
	step [151/251], loss=5.2908
	step [152/251], loss=6.2814
	step [153/251], loss=4.8770
	step [154/251], loss=6.5942
	step [155/251], loss=5.7496
	step [156/251], loss=6.7951
	step [157/251], loss=6.0810
	step [158/251], loss=6.3861
	step [159/251], loss=5.4825
	step [160/251], loss=6.7641
	step [161/251], loss=4.7758
	step [162/251], loss=5.0682
	step [163/251], loss=5.2901
	step [164/251], loss=5.1263
	step [165/251], loss=8.8662
	step [166/251], loss=4.9071
	step [167/251], loss=4.8465
	step [168/251], loss=5.5047
	step [169/251], loss=4.6026
	step [170/251], loss=6.6773
	step [171/251], loss=5.6093
	step [172/251], loss=4.9181
	step [173/251], loss=5.5019
	step [174/251], loss=6.8232
	step [175/251], loss=5.7209
	step [176/251], loss=5.4799
	step [177/251], loss=5.6373
	step [178/251], loss=5.5658
	step [179/251], loss=7.1202
	step [180/251], loss=6.2471
	step [181/251], loss=5.6398
	step [182/251], loss=7.0144
	step [183/251], loss=4.9631
	step [184/251], loss=5.5049
	step [185/251], loss=5.1396
	step [186/251], loss=6.6921
	step [187/251], loss=6.0543
	step [188/251], loss=5.9371
	step [189/251], loss=4.2435
	step [190/251], loss=4.8362
	step [191/251], loss=6.9899
	step [192/251], loss=4.4861
	step [193/251], loss=4.9662
	step [194/251], loss=8.6936
	step [195/251], loss=7.0177
	step [196/251], loss=5.4446
	step [197/251], loss=5.8795
	step [198/251], loss=5.9903
	step [199/251], loss=6.1437
	step [200/251], loss=5.6130
	step [201/251], loss=6.5741
	step [202/251], loss=6.5251
	step [203/251], loss=6.3192
	step [204/251], loss=4.8663
	step [205/251], loss=5.9318
	step [206/251], loss=5.9840
	step [207/251], loss=6.5217
	step [208/251], loss=4.3124
	step [209/251], loss=7.0390
	step [210/251], loss=5.6590
	step [211/251], loss=5.8178
	step [212/251], loss=6.2517
	step [213/251], loss=7.6792
	step [214/251], loss=6.4340
	step [215/251], loss=5.7700
	step [216/251], loss=6.0709
	step [217/251], loss=4.8917
	step [218/251], loss=6.1369
	step [219/251], loss=5.3889
	step [220/251], loss=5.3200
	step [221/251], loss=5.6002
	step [222/251], loss=5.2077
	step [223/251], loss=6.4251
	step [224/251], loss=5.6693
	step [225/251], loss=5.8065
	step [226/251], loss=6.7388
	step [227/251], loss=7.1545
	step [228/251], loss=6.6390
	step [229/251], loss=3.9567
	step [230/251], loss=7.3408
	step [231/251], loss=5.6716
	step [232/251], loss=6.2329
	step [233/251], loss=6.3770
	step [234/251], loss=6.4796
	step [235/251], loss=7.8452
	step [236/251], loss=4.8688
	step [237/251], loss=6.8529
	step [238/251], loss=5.8310
	step [239/251], loss=6.2873
	step [240/251], loss=5.1023
	step [241/251], loss=6.5619
	step [242/251], loss=5.8168
	step [243/251], loss=6.9148
	step [244/251], loss=5.5234
	step [245/251], loss=6.8056
	step [246/251], loss=5.9157
	step [247/251], loss=7.0512
	step [248/251], loss=5.6020
	step [249/251], loss=5.1885
	step [250/251], loss=7.7057
	step [251/251], loss=1.1800
	Evaluating
	loss=0.0191, precision=0.2312, recall=0.9964, f1=0.3753
Training finished
best_f1: 0.38268666329527423
directing: X rim_enhanced: False test_id 3
removed wrong scan: weights_X_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_218_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_46_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_49_xwqg-A00121_2019-05-15.npy
# all image files: 12135 # all weight files in weight_dir: 9360 # image files with weight 9310
removed wrong scan: weights_X_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_218_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_46_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_49_xwqg-A00121_2019-05-15.npy
# all image files: 12135 # all weight files in weight_dir: 2522 # image files with weight 2522
train: /ibex/scratch/projects/c2052/air_tube_seg/training_samples_enhanced/X 9310
Using 4 GPUs
Going to train epochs [1-30]
Training epoch 1
	step [1/194], loss=318.3099
	step [2/194], loss=201.8779
	step [3/194], loss=138.6955
	step [4/194], loss=123.9031
	step [5/194], loss=116.5673
	step [6/194], loss=110.6132
	step [7/194], loss=106.7395
	step [8/194], loss=106.7721
	step [9/194], loss=105.1269
	step [10/194], loss=104.1432
	step [11/194], loss=100.6779
	step [12/194], loss=100.6445
	step [13/194], loss=98.6661
	step [14/194], loss=97.5196
	step [15/194], loss=96.3991
	step [16/194], loss=96.5529
	step [17/194], loss=95.4327
	step [18/194], loss=94.9691
	step [19/194], loss=94.7983
	step [20/194], loss=92.9474
	step [21/194], loss=93.5692
	step [22/194], loss=93.0855
	step [23/194], loss=92.0888
	step [24/194], loss=90.8701
	step [25/194], loss=89.9606
	step [26/194], loss=90.2448
	step [27/194], loss=89.1507
	step [28/194], loss=88.2133
	step [29/194], loss=89.0894
	step [30/194], loss=87.1296
	step [31/194], loss=85.7516
	step [32/194], loss=85.0242
	step [33/194], loss=86.5210
	step [34/194], loss=84.7173
	step [35/194], loss=83.1542
	step [36/194], loss=83.9435
	step [37/194], loss=83.1806
	step [38/194], loss=81.5251
	step [39/194], loss=83.1601
	step [40/194], loss=80.2633
	step [41/194], loss=81.0089
	step [42/194], loss=80.0968
	step [43/194], loss=78.5544
	step [44/194], loss=78.9529
	step [45/194], loss=78.9735
	step [46/194], loss=78.9671
	step [47/194], loss=77.5718
	step [48/194], loss=76.3195
	step [49/194], loss=77.9869
	step [50/194], loss=77.3254
	step [51/194], loss=75.6307
	step [52/194], loss=76.1885
	step [53/194], loss=76.4146
	step [54/194], loss=74.0185
	step [55/194], loss=74.0346
	step [56/194], loss=74.9929
	step [57/194], loss=72.9385
	step [58/194], loss=74.4512
	step [59/194], loss=73.8645
	step [60/194], loss=73.9329
	step [61/194], loss=75.0921
	step [62/194], loss=71.8617
	step [63/194], loss=72.1216
	step [64/194], loss=73.6319
	step [65/194], loss=71.6712
	step [66/194], loss=70.6501
	step [67/194], loss=70.5183
	step [68/194], loss=70.0681
	step [69/194], loss=70.4710
	step [70/194], loss=70.5724
	step [71/194], loss=70.2193
	step [72/194], loss=71.7038
	step [73/194], loss=68.8508
	step [74/194], loss=68.6220
	step [75/194], loss=69.1920
	step [76/194], loss=69.2531
	step [77/194], loss=67.2803
	step [78/194], loss=68.3105
	step [79/194], loss=67.6528
	step [80/194], loss=68.3589
	step [81/194], loss=67.4672
	step [82/194], loss=68.2008
	step [83/194], loss=66.5675
	step [84/194], loss=66.0605
	step [85/194], loss=66.7264
	step [86/194], loss=64.9071
	step [87/194], loss=66.9216
	step [88/194], loss=65.2950
	step [89/194], loss=65.8240
	step [90/194], loss=63.7438
	step [91/194], loss=65.5550
	step [92/194], loss=66.5003
	step [93/194], loss=64.3498
	step [94/194], loss=64.2190
	step [95/194], loss=64.5614
	step [96/194], loss=63.1874
	step [97/194], loss=62.5078
	step [98/194], loss=63.0031
	step [99/194], loss=63.2714
	step [100/194], loss=61.3724
	step [101/194], loss=62.8243
	step [102/194], loss=64.1298
	step [103/194], loss=62.0174
	step [104/194], loss=63.0491
	step [105/194], loss=63.1201
	step [106/194], loss=61.2404
	step [107/194], loss=61.4582
	step [108/194], loss=62.5322
	step [109/194], loss=61.5231
	step [110/194], loss=61.1108
	step [111/194], loss=62.7063
	step [112/194], loss=61.6981
	step [113/194], loss=61.7380
	step [114/194], loss=60.9242
	step [115/194], loss=62.6166
	step [116/194], loss=60.2447
	step [117/194], loss=60.8862
	step [118/194], loss=60.7994
	step [119/194], loss=59.4169
	step [120/194], loss=60.2795
	step [121/194], loss=59.8813
	step [122/194], loss=61.3491
	step [123/194], loss=58.1547
	step [124/194], loss=58.3892
	step [125/194], loss=58.8669
	step [126/194], loss=59.5975
	step [127/194], loss=58.1372
	step [128/194], loss=57.9319
	step [129/194], loss=58.8838
	step [130/194], loss=58.4728
	step [131/194], loss=58.1611
	step [132/194], loss=57.9496
	step [133/194], loss=58.6621
	step [134/194], loss=57.1360
	step [135/194], loss=58.6925
	step [136/194], loss=57.6272
	step [137/194], loss=58.6677
	step [138/194], loss=56.4638
	step [139/194], loss=56.5142
	step [140/194], loss=57.8002
	step [141/194], loss=56.7281
	step [142/194], loss=57.5781
	step [143/194], loss=57.2097
	step [144/194], loss=56.3352
	step [145/194], loss=56.7643
	step [146/194], loss=56.7559
	step [147/194], loss=55.9494
	step [148/194], loss=55.9515
	step [149/194], loss=55.7224
	step [150/194], loss=57.1992
	step [151/194], loss=56.9903
	step [152/194], loss=54.4453
	step [153/194], loss=57.2852
	step [154/194], loss=56.2277
	step [155/194], loss=56.0940
	step [156/194], loss=56.9293
	step [157/194], loss=55.4550
	step [158/194], loss=54.9198
	step [159/194], loss=55.0455
	step [160/194], loss=54.0751
	step [161/194], loss=53.1393
	step [162/194], loss=55.0830
	step [163/194], loss=57.8352
	step [164/194], loss=56.4136
	step [165/194], loss=52.8239
	step [166/194], loss=54.9427
	step [167/194], loss=54.0991
	step [168/194], loss=54.8930
	step [169/194], loss=54.4466
	step [170/194], loss=54.1020
	step [171/194], loss=52.9510
	step [172/194], loss=52.7064
	step [173/194], loss=52.5018
	step [174/194], loss=53.8259
	step [175/194], loss=54.0123
	step [176/194], loss=52.6189
	step [177/194], loss=53.9435
	step [178/194], loss=52.0169
	step [179/194], loss=52.3103
	step [180/194], loss=53.5355
	step [181/194], loss=52.0433
	step [182/194], loss=53.7751
	step [183/194], loss=52.3362
	step [184/194], loss=53.1155
	step [185/194], loss=52.9860
	step [186/194], loss=50.9213
	step [187/194], loss=49.8353
	step [188/194], loss=50.9405
	step [189/194], loss=51.4149
	step [190/194], loss=51.8397
	step [191/194], loss=50.1292
	step [192/194], loss=51.9475
	step [193/194], loss=50.4626
	step [194/194], loss=48.0275
	Evaluating
	loss=0.2411, precision=0.2127, recall=0.9921, f1=0.3503
saving model as: 3_saved_model.pth
Training epoch 2
	step [1/194], loss=49.8945
	step [2/194], loss=50.4989
	step [3/194], loss=50.4169
	step [4/194], loss=50.5859
	step [5/194], loss=49.2770
	step [6/194], loss=50.5915
	step [7/194], loss=50.7827
	step [8/194], loss=48.9817
	step [9/194], loss=50.0571
	step [10/194], loss=51.3526
	step [11/194], loss=48.3855
	step [12/194], loss=50.3340
	step [13/194], loss=48.8591
	step [14/194], loss=49.2297
	step [15/194], loss=50.1306
	step [16/194], loss=48.2217
	step [17/194], loss=47.3853
	step [18/194], loss=48.4869
	step [19/194], loss=50.2321
	step [20/194], loss=50.3922
	step [21/194], loss=49.7166
	step [22/194], loss=48.4805
	step [23/194], loss=47.4170
	step [24/194], loss=49.1841
	step [25/194], loss=47.2731
	step [26/194], loss=48.9136
	step [27/194], loss=47.6748
	step [28/194], loss=49.1911
	step [29/194], loss=48.3382
	step [30/194], loss=47.0529
	step [31/194], loss=47.1145
	step [32/194], loss=47.0839
	step [33/194], loss=48.0803
	step [34/194], loss=46.7755
	step [35/194], loss=48.5143
	step [36/194], loss=46.2371
	step [37/194], loss=46.7512
	step [38/194], loss=47.4915
	step [39/194], loss=46.9467
	step [40/194], loss=46.8359
	step [41/194], loss=47.2764
	step [42/194], loss=48.8967
	step [43/194], loss=47.3139
	step [44/194], loss=47.3354
	step [45/194], loss=47.4594
	step [46/194], loss=46.9258
	step [47/194], loss=47.0196
	step [48/194], loss=45.2443
	step [49/194], loss=46.1322
	step [50/194], loss=44.5818
	step [51/194], loss=44.5957
	step [52/194], loss=45.0731
	step [53/194], loss=45.6533
	step [54/194], loss=45.6281
	step [55/194], loss=47.4385
	step [56/194], loss=45.5170
	step [57/194], loss=47.3986
	step [58/194], loss=44.9029
	step [59/194], loss=44.1758
	step [60/194], loss=44.6007
	step [61/194], loss=46.5981
	step [62/194], loss=46.6643
	step [63/194], loss=44.8760
	step [64/194], loss=45.4835
	step [65/194], loss=45.2482
	step [66/194], loss=45.0878
	step [67/194], loss=43.8769
	step [68/194], loss=44.1049
	step [69/194], loss=44.7669
	step [70/194], loss=45.1878
	step [71/194], loss=43.5310
	step [72/194], loss=45.7143
	step [73/194], loss=45.2711
	step [74/194], loss=44.0062
	step [75/194], loss=44.8548
	step [76/194], loss=43.6719
	step [77/194], loss=44.7121
	step [78/194], loss=43.7875
	step [79/194], loss=43.7278
	step [80/194], loss=43.3215
	step [81/194], loss=44.6663
	step [82/194], loss=42.9652
	step [83/194], loss=44.3661
	step [84/194], loss=41.5402
	step [85/194], loss=42.9988
	step [86/194], loss=43.5901
	step [87/194], loss=42.8729
	step [88/194], loss=43.4940
	step [89/194], loss=43.3141
	step [90/194], loss=41.1310
	step [91/194], loss=43.4179
	step [92/194], loss=42.9974
	step [93/194], loss=43.5043
	step [94/194], loss=41.8152
	step [95/194], loss=42.9373
	step [96/194], loss=43.9314
	step [97/194], loss=41.5602
	step [98/194], loss=43.3615
	step [99/194], loss=42.9565
	step [100/194], loss=43.7825
	step [101/194], loss=42.2826
	step [102/194], loss=43.9143
	step [103/194], loss=41.0763
	step [104/194], loss=41.8605
	step [105/194], loss=42.7604
	step [106/194], loss=42.1538
	step [107/194], loss=41.0549
	step [108/194], loss=40.4380
	step [109/194], loss=41.1374
	step [110/194], loss=40.9754
	step [111/194], loss=40.7307
	step [112/194], loss=39.2111
	step [113/194], loss=40.4144
	step [114/194], loss=42.7046
	step [115/194], loss=41.3151
	step [116/194], loss=42.1213
	step [117/194], loss=40.8243
	step [118/194], loss=40.8733
	step [119/194], loss=42.3218
	step [120/194], loss=42.0891
	step [121/194], loss=42.4209
	step [122/194], loss=40.5420
	step [123/194], loss=39.8628
	step [124/194], loss=39.6967
	step [125/194], loss=39.7176
	step [126/194], loss=40.6758
	step [127/194], loss=41.8331
	step [128/194], loss=41.2483
	step [129/194], loss=40.2889
	step [130/194], loss=39.7730
	step [131/194], loss=40.2054
	step [132/194], loss=40.4373
	step [133/194], loss=39.7701
	step [134/194], loss=40.0602
	step [135/194], loss=39.1488
	step [136/194], loss=39.3851
	step [137/194], loss=40.3916
	step [138/194], loss=39.8411
	step [139/194], loss=38.7508
	step [140/194], loss=38.9110
	step [141/194], loss=38.4595
	step [142/194], loss=39.4018
	step [143/194], loss=39.7915
	step [144/194], loss=38.7019
	step [145/194], loss=39.3792
	step [146/194], loss=38.8782
	step [147/194], loss=39.5453
	step [148/194], loss=39.0351
	step [149/194], loss=39.2566
	step [150/194], loss=38.6665
	step [151/194], loss=39.2263
	step [152/194], loss=37.4189
	step [153/194], loss=37.7474
	step [154/194], loss=40.0746
	step [155/194], loss=37.9650
	step [156/194], loss=39.0882
	step [157/194], loss=38.8949
	step [158/194], loss=37.3769
	step [159/194], loss=37.9574
	step [160/194], loss=38.5859
	step [161/194], loss=38.9930
	step [162/194], loss=37.3907
	step [163/194], loss=39.2342
	step [164/194], loss=37.2313
	step [165/194], loss=37.6414
	step [166/194], loss=37.9121
	step [167/194], loss=39.8311
	step [168/194], loss=37.7454
	step [169/194], loss=37.4878
	step [170/194], loss=36.5924
	step [171/194], loss=40.3619
	step [172/194], loss=37.5917
	step [173/194], loss=38.2602
	step [174/194], loss=38.0114
	step [175/194], loss=36.4605
	step [176/194], loss=37.5935
	step [177/194], loss=39.4540
	step [178/194], loss=36.0671
	step [179/194], loss=37.1131
	step [180/194], loss=36.7721
	step [181/194], loss=38.0390
	step [182/194], loss=37.5928
	step [183/194], loss=36.9814
	step [184/194], loss=36.5572
	step [185/194], loss=37.6200
	step [186/194], loss=36.2900
	step [187/194], loss=36.9887
	step [188/194], loss=36.4302
	step [189/194], loss=36.7952
	step [190/194], loss=36.6612
	step [191/194], loss=38.1111
	step [192/194], loss=37.1943
	step [193/194], loss=36.7138
	step [194/194], loss=36.0755
	Evaluating
	loss=0.1665, precision=0.1957, recall=0.9936, f1=0.3270
Training epoch 3
