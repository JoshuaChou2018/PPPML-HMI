Loading anaconda...
...Anaconda env loaded
directing: Y rim_enhanced: True test_id 0
removed wrong scan: weights_Y_335_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_383_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_262_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_311_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_260_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_225_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_320_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_192_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_319_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_226_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_346_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_271_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_277_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_290_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_269_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_379_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_375_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_378_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_366_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_309_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_200_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_327_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_193_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_174_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_253_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_220_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_265_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_360_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_303_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_232_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_243_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_240_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_237_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_256_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_254_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_254_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_231_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_359_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_172_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_340_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_263_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_388_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_307_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_278_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_196_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_272_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_314_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_350_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_361_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_194_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_323_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_201_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_299_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_259_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_276_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_167_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_185_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_215_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_268_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_264_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_384_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_261_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_280_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_287_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_385_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_216_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_349_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_348_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_371_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_357_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_234_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_288_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_324_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_229_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_184_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_148_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_168_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_370_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_286_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_302_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_265_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_153_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_274_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_255_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_217_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_355_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_294_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_159_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_338_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_236_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_152_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_292_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_222_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_376_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_183_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_258_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_282_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_278_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_284_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_300_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_163_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_197_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_203_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_243_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_224_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_238_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_374_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_218_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_259_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_282_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_387_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_353_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_386_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_296_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_341_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_206_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_220_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_239_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_231_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_202_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_358_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_246_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_315_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_275_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_169_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_316_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_221_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_188_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_208_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_273_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_251_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_224_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_333_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_263_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_173_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_243_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_241_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_326_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_230_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_236_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_345_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_206_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_258_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_187_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_87_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_154_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_253_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_337_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_232_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_147_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_377_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_362_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_233_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_250_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_297_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_354_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_262_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_225_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_365_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_356_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_242_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_245_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_230_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_318_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_181_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_305_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_255_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_298_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_301_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_293_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_229_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_332_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_257_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_295_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_239_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_260_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_228_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_317_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_364_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_262_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_162_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_310_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_160_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_237_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_254_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_257_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_291_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_279_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_215_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_372_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_218_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_210_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_158_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_156_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_339_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_190_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_373_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_205_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_329_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_380_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_271_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_235_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_244_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_312_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_186_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_308_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_347_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_241_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_270_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_261_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_222_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_189_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_289_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_146_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_252_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_344_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_331_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_313_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_328_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_265_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_283_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_248_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_381_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_211_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_214_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_176_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_221_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_150_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_343_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_249_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_322_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_351_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_234_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_195_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_330_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_334_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_306_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_213_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_325_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_342_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_247_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_352_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_225_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_226_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_256_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_242_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_368_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_382_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_157_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_238_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_267_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_367_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_321_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_233_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_281_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_161_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_336_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_170_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_304_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_44_xwqg-B00034_2020-04-03.npy
# all image files: 21521 # all weight files in weight_dir: 15948 # image files with weight 15948
removed wrong scan: weights_Y_335_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_383_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_262_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_311_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_260_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_225_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_320_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_192_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_319_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_226_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_346_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_271_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_277_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_290_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_269_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_379_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_375_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_378_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_366_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_309_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_200_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_327_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_193_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_174_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_253_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_220_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_265_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_360_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_303_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_232_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_243_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_240_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_237_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_256_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_254_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_254_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_231_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_359_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_172_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_340_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_263_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_388_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_307_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_278_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_196_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_272_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_314_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_350_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_361_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_194_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_323_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_201_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_299_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_259_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_276_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_167_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_185_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_215_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_268_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_264_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_384_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_261_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_280_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_287_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_385_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_216_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_349_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_348_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_371_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_357_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_234_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_288_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_324_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_229_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_184_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_148_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_168_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_370_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_286_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_302_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_265_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_153_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_274_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_255_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_217_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_355_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_294_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_159_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_338_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_236_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_152_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_292_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_222_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_376_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_183_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_258_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_282_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_278_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_284_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_300_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_163_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_197_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_203_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_243_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_224_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_238_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_374_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_218_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_259_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_282_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_387_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_353_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_386_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_296_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_341_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_206_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_220_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_239_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_231_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_202_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_358_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_246_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_315_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_275_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_169_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_316_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_221_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_188_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_208_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_273_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_251_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_224_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_333_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_263_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_173_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_243_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_241_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_326_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_230_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_236_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_345_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_206_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_258_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_187_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_87_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_154_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_253_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_337_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_232_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_147_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_377_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_362_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_233_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_250_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_297_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_354_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_262_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_225_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_365_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_356_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_242_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_245_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_230_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_318_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_181_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_305_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_255_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_298_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_301_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_293_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_229_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_332_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_257_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_295_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_239_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_260_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_228_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_317_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_364_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_262_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_162_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_310_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_160_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_237_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_254_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_257_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_291_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_279_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_215_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_372_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_218_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_210_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_158_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_156_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_339_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_190_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_373_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_205_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_329_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_380_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_271_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_235_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_244_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_312_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_186_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_308_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_347_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_241_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_270_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_261_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_222_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_189_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_289_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_146_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_252_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_344_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_331_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_313_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_328_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_265_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_283_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_248_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_381_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_211_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_214_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_176_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_221_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_150_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_343_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_249_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_322_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_351_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_234_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_195_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_330_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_334_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_306_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_213_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_325_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_342_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_247_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_352_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_225_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_226_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_256_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_242_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_368_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_382_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_157_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_238_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_267_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_367_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_321_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_233_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_281_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_161_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_336_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_170_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_304_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_44_xwqg-B00034_2020-04-03.npy
# all image files: 21521 # all weight files in weight_dir: 4124 # image files with weight 4124
train: /ibex/scratch/projects/c2052/air_tube_seg/training_samples/Y 15948
Using 4 GPUs
Going to train epochs [54-103]
Training epoch 54
	step [1/250], loss=74.5605
	step [2/250], loss=79.6898
	step [3/250], loss=109.6904
	step [4/250], loss=87.6660
	step [5/250], loss=85.8433
	step [6/250], loss=104.0204
	step [7/250], loss=108.4037
	step [8/250], loss=89.1472
	step [9/250], loss=80.8694
	step [10/250], loss=91.0364
	step [11/250], loss=87.8505
	step [12/250], loss=76.4443
	step [13/250], loss=80.4906
	step [14/250], loss=82.9188
	step [15/250], loss=83.1161
	step [16/250], loss=85.3120
	step [17/250], loss=80.0653
	step [18/250], loss=75.6641
	step [19/250], loss=86.1902
	step [20/250], loss=75.9225
	step [21/250], loss=66.8467
	step [22/250], loss=81.1245
	step [23/250], loss=74.0879
	step [24/250], loss=86.0174
	step [25/250], loss=100.3524
	step [26/250], loss=93.7288
	step [27/250], loss=97.1070
	step [28/250], loss=91.2124
	step [29/250], loss=70.9203
	step [30/250], loss=100.7240
	step [31/250], loss=73.3058
	step [32/250], loss=100.4522
	step [33/250], loss=87.0307
	step [34/250], loss=96.6091
	step [35/250], loss=80.7859
	step [36/250], loss=84.1671
	step [37/250], loss=86.2331
	step [38/250], loss=70.6018
	step [39/250], loss=88.2735
	step [40/250], loss=111.8335
	step [41/250], loss=84.3004
	step [42/250], loss=78.7325
	step [43/250], loss=83.6431
	step [44/250], loss=93.5414
	step [45/250], loss=96.0871
	step [46/250], loss=86.4238
	step [47/250], loss=76.9727
	step [48/250], loss=82.5781
	step [49/250], loss=82.6075
	step [50/250], loss=103.1055
	step [51/250], loss=88.5841
	step [52/250], loss=92.3731
	step [53/250], loss=70.8365
	step [54/250], loss=88.5031
	step [55/250], loss=94.9289
	step [56/250], loss=82.5134
	step [57/250], loss=83.2254
	step [58/250], loss=80.9478
	step [59/250], loss=82.5256
	step [60/250], loss=74.4134
	step [61/250], loss=83.9658
	step [62/250], loss=82.3846
	step [63/250], loss=81.3461
	step [64/250], loss=92.7805
	step [65/250], loss=86.8503
	step [66/250], loss=83.6058
	step [67/250], loss=69.1728
	step [68/250], loss=88.1909
	step [69/250], loss=92.0299
	step [70/250], loss=84.8006
	step [71/250], loss=88.7859
	step [72/250], loss=97.5802
	step [73/250], loss=72.3722
	step [74/250], loss=80.7273
	step [75/250], loss=69.5561
	step [76/250], loss=106.2810
	step [77/250], loss=79.9776
	step [78/250], loss=90.9834
	step [79/250], loss=91.6083
	step [80/250], loss=90.1837
	step [81/250], loss=93.6375
	step [82/250], loss=85.9522
	step [83/250], loss=69.2904
	step [84/250], loss=83.7431
	step [85/250], loss=87.6022
	step [86/250], loss=73.1367
	step [87/250], loss=70.8241
	step [88/250], loss=90.0179
	step [89/250], loss=83.8062
	step [90/250], loss=86.4698
	step [91/250], loss=71.9153
	step [92/250], loss=80.3960
	step [93/250], loss=97.8644
	step [94/250], loss=77.8986
	step [95/250], loss=86.2883
	step [96/250], loss=86.1234
	step [97/250], loss=89.2358
	step [98/250], loss=72.7484
	step [99/250], loss=96.6626
	step [100/250], loss=84.5254
	step [101/250], loss=88.3326
	step [102/250], loss=88.9453
	step [103/250], loss=84.0218
	step [104/250], loss=71.8278
	step [105/250], loss=100.2006
	step [106/250], loss=92.5536
	step [107/250], loss=86.6798
	step [108/250], loss=77.4100
	step [109/250], loss=71.6265
	step [110/250], loss=80.5295
	step [111/250], loss=93.2803
	step [112/250], loss=94.3275
	step [113/250], loss=100.9438
	step [114/250], loss=67.5883
	step [115/250], loss=87.1581
	step [116/250], loss=83.9545
	step [117/250], loss=83.3256
	step [118/250], loss=90.5900
	step [119/250], loss=77.1123
	step [120/250], loss=99.9991
	step [121/250], loss=103.6785
	step [122/250], loss=106.1077
	step [123/250], loss=69.2428
	step [124/250], loss=76.8551
	step [125/250], loss=88.3431
	step [126/250], loss=87.0108
	step [127/250], loss=97.2413
	step [128/250], loss=70.6533
	step [129/250], loss=63.4510
	step [130/250], loss=86.8132
	step [131/250], loss=88.6695
	step [132/250], loss=109.3248
	step [133/250], loss=96.1298
	step [134/250], loss=86.0068
	step [135/250], loss=84.7892
	step [136/250], loss=88.8086
	step [137/250], loss=78.3932
	step [138/250], loss=84.6586
	step [139/250], loss=72.4848
	step [140/250], loss=95.1444
	step [141/250], loss=92.9981
	step [142/250], loss=88.4990
	step [143/250], loss=89.7869
	step [144/250], loss=93.6637
	step [145/250], loss=89.5085
	step [146/250], loss=84.6469
	step [147/250], loss=99.2052
	step [148/250], loss=80.5704
	step [149/250], loss=90.3934
	step [150/250], loss=85.4697
	step [151/250], loss=82.1479
	step [152/250], loss=83.8212
	step [153/250], loss=85.2148
	step [154/250], loss=82.0813
	step [155/250], loss=92.4190
	step [156/250], loss=79.2468
	step [157/250], loss=75.1261
	step [158/250], loss=64.7170
	step [159/250], loss=84.9167
	step [160/250], loss=82.2369
	step [161/250], loss=78.7694
	step [162/250], loss=90.3602
	step [163/250], loss=65.0101
	step [164/250], loss=96.0335
	step [165/250], loss=82.5579
	step [166/250], loss=95.9610
	step [167/250], loss=83.6086
	step [168/250], loss=79.6176
	step [169/250], loss=79.7321
	step [170/250], loss=105.4274
	step [171/250], loss=80.5610
	step [172/250], loss=64.9415
	step [173/250], loss=80.9513
	step [174/250], loss=82.0330
	step [175/250], loss=88.4654
	step [176/250], loss=99.0565
	step [177/250], loss=86.0520
	step [178/250], loss=82.5330
	step [179/250], loss=88.4044
	step [180/250], loss=77.7412
	step [181/250], loss=92.6455
	step [182/250], loss=93.4284
	step [183/250], loss=82.1986
	step [184/250], loss=81.1624
	step [185/250], loss=81.8823
	step [186/250], loss=90.0895
	step [187/250], loss=82.7589
	step [188/250], loss=63.6852
	step [189/250], loss=68.2815
	step [190/250], loss=80.4184
	step [191/250], loss=87.2422
	step [192/250], loss=77.8274
	step [193/250], loss=94.0767
	step [194/250], loss=90.0021
	step [195/250], loss=85.6771
	step [196/250], loss=95.6749
	step [197/250], loss=71.5548
	step [198/250], loss=100.5255
	step [199/250], loss=92.7393
	step [200/250], loss=86.2680
	step [201/250], loss=81.9270
	step [202/250], loss=107.5392
	step [203/250], loss=82.6767
	step [204/250], loss=73.7386
	step [205/250], loss=102.9909
	step [206/250], loss=94.9430
	step [207/250], loss=79.8301
	step [208/250], loss=90.7775
	step [209/250], loss=90.9457
	step [210/250], loss=83.8733
	step [211/250], loss=91.2318
	step [212/250], loss=94.8099
	step [213/250], loss=98.4297
	step [214/250], loss=86.7524
	step [215/250], loss=86.9122
	step [216/250], loss=94.0686
	step [217/250], loss=79.0957
	step [218/250], loss=84.3436
	step [219/250], loss=77.3528
	step [220/250], loss=86.5660
	step [221/250], loss=106.7828
	step [222/250], loss=92.3056
	step [223/250], loss=96.1896
	step [224/250], loss=86.4925
	step [225/250], loss=80.4072
	step [226/250], loss=84.5315
	step [227/250], loss=85.8457
	step [228/250], loss=82.5777
	step [229/250], loss=83.9833
	step [230/250], loss=92.5284
	step [231/250], loss=83.3330
	step [232/250], loss=71.4949
	step [233/250], loss=90.2154
	step [234/250], loss=91.5640
	step [235/250], loss=89.2413
	step [236/250], loss=73.4812
	step [237/250], loss=85.2519
	step [238/250], loss=92.3155
	step [239/250], loss=87.5345
	step [240/250], loss=95.0234
	step [241/250], loss=90.4531
	step [242/250], loss=83.3411
	step [243/250], loss=71.4132
	step [244/250], loss=71.5584
	step [245/250], loss=71.2589
	step [246/250], loss=72.2227
	step [247/250], loss=101.6481
	step [248/250], loss=86.5047
	step [249/250], loss=80.9463
	step [250/250], loss=16.7518
	Evaluating
	loss=0.0112, precision=0.2162, recall=0.8852, f1=0.3475
saving model as: 0_saved_model.pth
Training epoch 55
	step [1/250], loss=96.6288
	step [2/250], loss=73.0160
	step [3/250], loss=87.9231
	step [4/250], loss=105.7037
	step [5/250], loss=88.9774
	step [6/250], loss=81.4779
	step [7/250], loss=86.4739
	step [8/250], loss=74.2961
	step [9/250], loss=70.9240
	step [10/250], loss=83.9304
	step [11/250], loss=84.8269
	step [12/250], loss=76.0573
	step [13/250], loss=97.7460
	step [14/250], loss=103.4507
	step [15/250], loss=94.1993
	step [16/250], loss=91.8063
	step [17/250], loss=87.7038
	step [18/250], loss=102.7747
	step [19/250], loss=85.2016
	step [20/250], loss=74.9015
	step [21/250], loss=87.1339
	step [22/250], loss=100.2069
	step [23/250], loss=93.3714
	step [24/250], loss=93.6999
	step [25/250], loss=87.2386
	step [26/250], loss=80.2871
	step [27/250], loss=72.4950
	step [28/250], loss=69.4126
	step [29/250], loss=73.6567
	step [30/250], loss=92.0587
	step [31/250], loss=102.6995
	step [32/250], loss=120.2588
	step [33/250], loss=98.0506
	step [34/250], loss=76.5992
	step [35/250], loss=87.7458
	step [36/250], loss=91.7268
	step [37/250], loss=66.8877
	step [38/250], loss=83.7796
	step [39/250], loss=105.5960
	step [40/250], loss=76.0638
	step [41/250], loss=89.9954
	step [42/250], loss=91.7295
	step [43/250], loss=80.8139
	step [44/250], loss=99.5504
	step [45/250], loss=71.2327
	step [46/250], loss=98.1570
	step [47/250], loss=76.8596
	step [48/250], loss=92.5792
	step [49/250], loss=84.1537
	step [50/250], loss=89.8511
	step [51/250], loss=98.1051
	step [52/250], loss=91.6548
	step [53/250], loss=79.5184
	step [54/250], loss=94.1585
	step [55/250], loss=65.2667
	step [56/250], loss=101.4085
	step [57/250], loss=80.5346
	step [58/250], loss=104.4158
	step [59/250], loss=91.5360
	step [60/250], loss=62.4458
	step [61/250], loss=97.7403
	step [62/250], loss=102.8602
	step [63/250], loss=94.6311
	step [64/250], loss=95.0113
	step [65/250], loss=73.1608
	step [66/250], loss=82.3662
	step [67/250], loss=105.7512
	step [68/250], loss=87.4261
	step [69/250], loss=109.7292
	step [70/250], loss=81.5727
	step [71/250], loss=96.7747
	step [72/250], loss=72.8507
	step [73/250], loss=93.6236
	step [74/250], loss=75.3858
	step [75/250], loss=100.5226
	step [76/250], loss=89.9677
	step [77/250], loss=81.3767
	step [78/250], loss=86.4556
	step [79/250], loss=78.1867
	step [80/250], loss=99.1794
	step [81/250], loss=91.3991
	step [82/250], loss=84.8849
	step [83/250], loss=74.5500
	step [84/250], loss=84.3585
	step [85/250], loss=104.3363
	step [86/250], loss=74.2653
	step [87/250], loss=86.9690
	step [88/250], loss=89.4809
	step [89/250], loss=83.5206
	step [90/250], loss=89.8314
	step [91/250], loss=73.2838
	step [92/250], loss=76.8667
	step [93/250], loss=106.4711
	step [94/250], loss=64.8509
	step [95/250], loss=75.6696
	step [96/250], loss=93.2696
	step [97/250], loss=78.4733
	step [98/250], loss=85.8742
	step [99/250], loss=86.4713
	step [100/250], loss=94.6257
	step [101/250], loss=102.6548
	step [102/250], loss=82.6836
	step [103/250], loss=78.7658
	step [104/250], loss=100.9716
	step [105/250], loss=91.7710
	step [106/250], loss=103.4974
	step [107/250], loss=73.0473
	step [108/250], loss=70.7005
	step [109/250], loss=86.6823
	step [110/250], loss=90.5848
	step [111/250], loss=87.1148
	step [112/250], loss=90.4582
	step [113/250], loss=74.9646
	step [114/250], loss=82.7234
	step [115/250], loss=73.1020
	step [116/250], loss=110.5542
	step [117/250], loss=68.6926
	step [118/250], loss=86.3380
	step [119/250], loss=68.5077
	step [120/250], loss=101.0631
	step [121/250], loss=91.5124
	step [122/250], loss=84.9208
	step [123/250], loss=91.0225
	step [124/250], loss=73.2225
	step [125/250], loss=71.2179
	step [126/250], loss=86.1095
	step [127/250], loss=93.8161
	step [128/250], loss=81.6619
	step [129/250], loss=81.7561
	step [130/250], loss=82.9882
	step [131/250], loss=93.5683
	step [132/250], loss=95.7282
	step [133/250], loss=88.0093
	step [134/250], loss=74.4408
	step [135/250], loss=81.0460
	step [136/250], loss=104.1689
	step [137/250], loss=90.3710
	step [138/250], loss=87.2684
	step [139/250], loss=93.3416
	step [140/250], loss=84.5156
	step [141/250], loss=80.5219
	step [142/250], loss=89.0724
	step [143/250], loss=75.8627
	step [144/250], loss=81.1816
	step [145/250], loss=93.1775
	step [146/250], loss=84.8252
	step [147/250], loss=94.0748
	step [148/250], loss=80.1097
	step [149/250], loss=76.8770
	step [150/250], loss=86.5722
	step [151/250], loss=98.9916
	step [152/250], loss=82.4308
	step [153/250], loss=86.8798
	step [154/250], loss=90.7154
	step [155/250], loss=73.7491
	step [156/250], loss=80.3316
	step [157/250], loss=89.7689
	step [158/250], loss=96.2491
	step [159/250], loss=92.2885
	step [160/250], loss=81.3217
	step [161/250], loss=94.7351
	step [162/250], loss=86.1699
	step [163/250], loss=89.8274
	step [164/250], loss=88.5735
	step [165/250], loss=81.3846
	step [166/250], loss=77.8718
	step [167/250], loss=77.2955
	step [168/250], loss=93.8425
	step [169/250], loss=86.1221
	step [170/250], loss=85.1803
	step [171/250], loss=87.1403
	step [172/250], loss=73.5774
	step [173/250], loss=85.5984
	step [174/250], loss=92.0735
	step [175/250], loss=85.9773
	step [176/250], loss=78.3481
	step [177/250], loss=88.6952
	step [178/250], loss=97.2879
	step [179/250], loss=86.3090
	step [180/250], loss=86.9107
	step [181/250], loss=84.4382
	step [182/250], loss=69.2176
	step [183/250], loss=84.0389
	step [184/250], loss=71.8189
	step [185/250], loss=86.3985
	step [186/250], loss=83.1846
	step [187/250], loss=75.8837
	step [188/250], loss=86.1765
	step [189/250], loss=89.7096
	step [190/250], loss=68.8273
	step [191/250], loss=80.6375
	step [192/250], loss=74.9426
	step [193/250], loss=80.8733
	step [194/250], loss=95.7516
	step [195/250], loss=81.2264
	step [196/250], loss=99.0203
	step [197/250], loss=87.2973
	step [198/250], loss=95.5453
	step [199/250], loss=72.6101
	step [200/250], loss=78.4888
	step [201/250], loss=76.9297
	step [202/250], loss=90.7432
	step [203/250], loss=78.5212
	step [204/250], loss=88.9559
	step [205/250], loss=83.4735
	step [206/250], loss=77.5395
	step [207/250], loss=99.1769
	step [208/250], loss=80.8987
	step [209/250], loss=80.2069
	step [210/250], loss=74.8773
	step [211/250], loss=73.4746
	step [212/250], loss=87.0948
	step [213/250], loss=82.3759
	step [214/250], loss=81.4109
	step [215/250], loss=81.2178
	step [216/250], loss=72.7554
	step [217/250], loss=69.0562
	step [218/250], loss=68.1249
	step [219/250], loss=85.4235
	step [220/250], loss=98.1217
	step [221/250], loss=73.2420
	step [222/250], loss=76.4266
	step [223/250], loss=89.3059
	step [224/250], loss=92.3830
	step [225/250], loss=90.3783
	step [226/250], loss=76.5954
	step [227/250], loss=96.4594
	step [228/250], loss=80.6074
	step [229/250], loss=99.2804
	step [230/250], loss=98.2085
	step [231/250], loss=79.5226
	step [232/250], loss=61.6741
	step [233/250], loss=71.5207
	step [234/250], loss=94.7396
	step [235/250], loss=89.9418
	step [236/250], loss=101.7087
	step [237/250], loss=81.7163
	step [238/250], loss=58.9094
	step [239/250], loss=68.1397
	step [240/250], loss=73.7432
	step [241/250], loss=69.7650
	step [242/250], loss=78.5967
	step [243/250], loss=84.6643
	step [244/250], loss=72.6317
	step [245/250], loss=92.1513
	step [246/250], loss=87.9243
	step [247/250], loss=87.1357
	step [248/250], loss=88.0803
	step [249/250], loss=83.8764
	step [250/250], loss=15.6426
	Evaluating
	loss=0.0103, precision=0.2246, recall=0.8852, f1=0.3583
saving model as: 0_saved_model.pth
Training epoch 56
	step [1/250], loss=89.8874
	step [2/250], loss=85.0535
	step [3/250], loss=83.2976
	step [4/250], loss=74.5253
	step [5/250], loss=87.1858
	step [6/250], loss=86.9405
	step [7/250], loss=112.4335
	step [8/250], loss=88.4408
	step [9/250], loss=77.5718
	step [10/250], loss=77.5473
	step [11/250], loss=76.1320
	step [12/250], loss=90.3379
	step [13/250], loss=80.7038
	step [14/250], loss=76.7802
	step [15/250], loss=96.3561
	step [16/250], loss=92.3995
	step [17/250], loss=85.2161
	step [18/250], loss=90.5686
	step [19/250], loss=95.6190
	step [20/250], loss=88.9401
	step [21/250], loss=85.5098
	step [22/250], loss=95.7140
	step [23/250], loss=86.0868
	step [24/250], loss=87.6477
	step [25/250], loss=80.9082
	step [26/250], loss=76.9455
	step [27/250], loss=80.2618
	step [28/250], loss=76.2527
	step [29/250], loss=91.6904
	step [30/250], loss=81.3615
	step [31/250], loss=86.6389
	step [32/250], loss=84.7585
	step [33/250], loss=92.1974
	step [34/250], loss=88.4001
	step [35/250], loss=98.1818
	step [36/250], loss=77.0070
	step [37/250], loss=95.0453
	step [38/250], loss=69.6226
	step [39/250], loss=83.5107
	step [40/250], loss=85.1782
	step [41/250], loss=86.1361
	step [42/250], loss=81.7658
	step [43/250], loss=98.9635
	step [44/250], loss=81.3510
	step [45/250], loss=72.3156
	step [46/250], loss=77.1033
	step [47/250], loss=79.9777
	step [48/250], loss=63.9728
	step [49/250], loss=96.4515
	step [50/250], loss=79.4192
	step [51/250], loss=94.8211
	step [52/250], loss=86.9565
	step [53/250], loss=89.5592
	step [54/250], loss=78.7778
	step [55/250], loss=104.5239
	step [56/250], loss=86.2388
	step [57/250], loss=91.5917
	step [58/250], loss=73.9925
	step [59/250], loss=64.1782
	step [60/250], loss=81.3354
	step [61/250], loss=63.5743
	step [62/250], loss=79.5492
	step [63/250], loss=68.2654
	step [64/250], loss=81.5071
	step [65/250], loss=75.8191
	step [66/250], loss=76.9946
	step [67/250], loss=80.2998
	step [68/250], loss=80.7447
	step [69/250], loss=73.4983
	step [70/250], loss=93.4949
	step [71/250], loss=75.8369
	step [72/250], loss=86.7781
	step [73/250], loss=78.1741
	step [74/250], loss=102.3913
	step [75/250], loss=79.3427
	step [76/250], loss=82.2010
	step [77/250], loss=78.5718
	step [78/250], loss=86.6329
	step [79/250], loss=88.6380
	step [80/250], loss=95.2003
	step [81/250], loss=58.8611
	step [82/250], loss=78.5728
	step [83/250], loss=90.0265
	step [84/250], loss=86.9241
	step [85/250], loss=88.8376
	step [86/250], loss=104.5425
	step [87/250], loss=78.5272
	step [88/250], loss=85.8030
	step [89/250], loss=80.8498
	step [90/250], loss=82.7289
	step [91/250], loss=76.6533
	step [92/250], loss=85.4961
	step [93/250], loss=94.3881
	step [94/250], loss=87.2517
	step [95/250], loss=79.1298
	step [96/250], loss=79.2056
	step [97/250], loss=84.3391
	step [98/250], loss=82.4438
	step [99/250], loss=72.0389
	step [100/250], loss=70.3705
	step [101/250], loss=76.4609
	step [102/250], loss=70.3472
	step [103/250], loss=79.3242
	step [104/250], loss=81.1768
	step [105/250], loss=78.7410
	step [106/250], loss=78.3056
	step [107/250], loss=95.5543
	step [108/250], loss=90.6895
	step [109/250], loss=88.3059
	step [110/250], loss=76.0641
	step [111/250], loss=63.1961
	step [112/250], loss=88.0101
	step [113/250], loss=84.4131
	step [114/250], loss=83.7855
	step [115/250], loss=81.3451
	step [116/250], loss=63.6195
	step [117/250], loss=78.1946
	step [118/250], loss=103.4041
	step [119/250], loss=79.8811
	step [120/250], loss=75.2501
	step [121/250], loss=73.6018
	step [122/250], loss=86.1553
	step [123/250], loss=88.3811
	step [124/250], loss=87.1419
	step [125/250], loss=98.8147
	step [126/250], loss=94.7206
	step [127/250], loss=79.6535
	step [128/250], loss=82.3964
	step [129/250], loss=85.0068
	step [130/250], loss=96.8682
	step [131/250], loss=79.5708
	step [132/250], loss=82.7260
	step [133/250], loss=94.3083
	step [134/250], loss=88.4533
	step [135/250], loss=73.5676
	step [136/250], loss=89.3894
	step [137/250], loss=89.5888
	step [138/250], loss=87.8688
	step [139/250], loss=77.7384
	step [140/250], loss=95.8421
	step [141/250], loss=92.7306
	step [142/250], loss=76.6830
	step [143/250], loss=76.5930
	step [144/250], loss=75.0378
	step [145/250], loss=76.4138
	step [146/250], loss=73.9313
	step [147/250], loss=88.3423
	step [148/250], loss=89.3315
	step [149/250], loss=102.8921
	step [150/250], loss=70.7472
	step [151/250], loss=85.3747
	step [152/250], loss=82.6611
	step [153/250], loss=84.9664
	step [154/250], loss=82.2029
	step [155/250], loss=90.5097
	step [156/250], loss=95.5346
	step [157/250], loss=94.8326
	step [158/250], loss=85.0364
	step [159/250], loss=88.4622
	step [160/250], loss=83.8916
	step [161/250], loss=92.5414
	step [162/250], loss=79.7445
	step [163/250], loss=86.9085
	step [164/250], loss=84.5257
	step [165/250], loss=101.0830
	step [166/250], loss=82.8461
	step [167/250], loss=77.2064
	step [168/250], loss=81.0169
	step [169/250], loss=80.4458
	step [170/250], loss=97.3814
	step [171/250], loss=81.0704
	step [172/250], loss=89.7954
	step [173/250], loss=69.5505
	step [174/250], loss=102.2887
	step [175/250], loss=80.4322
	step [176/250], loss=96.2533
	step [177/250], loss=93.5088
	step [178/250], loss=88.4103
	step [179/250], loss=86.7466
	step [180/250], loss=74.9144
	step [181/250], loss=83.1451
	step [182/250], loss=108.2870
	step [183/250], loss=83.9036
	step [184/250], loss=86.7770
	step [185/250], loss=79.2918
	step [186/250], loss=74.0883
	step [187/250], loss=84.6603
	step [188/250], loss=81.7932
	step [189/250], loss=73.1939
	step [190/250], loss=78.0696
	step [191/250], loss=86.6183
	step [192/250], loss=80.5746
	step [193/250], loss=85.2746
	step [194/250], loss=78.7417
	step [195/250], loss=107.7214
	step [196/250], loss=85.3894
	step [197/250], loss=87.5088
	step [198/250], loss=99.0340
	step [199/250], loss=82.5403
	step [200/250], loss=94.2916
	step [201/250], loss=91.0834
	step [202/250], loss=79.7300
	step [203/250], loss=89.2912
	step [204/250], loss=95.2441
	step [205/250], loss=80.9377
	step [206/250], loss=88.6973
	step [207/250], loss=96.1328
	step [208/250], loss=107.9869
	step [209/250], loss=99.4561
	step [210/250], loss=85.6108
	step [211/250], loss=101.5204
	step [212/250], loss=83.9812
	step [213/250], loss=79.8540
	step [214/250], loss=104.9587
	step [215/250], loss=82.7310
	step [216/250], loss=75.3175
	step [217/250], loss=78.1532
	step [218/250], loss=83.9286
	step [219/250], loss=85.2323
	step [220/250], loss=73.7664
	step [221/250], loss=75.4392
	step [222/250], loss=61.8483
	step [223/250], loss=97.5374
	step [224/250], loss=84.4720
	step [225/250], loss=95.1761
	step [226/250], loss=95.8947
	step [227/250], loss=75.6789
	step [228/250], loss=109.4753
	step [229/250], loss=75.7527
	step [230/250], loss=101.5546
	step [231/250], loss=99.3596
	step [232/250], loss=96.9400
	step [233/250], loss=92.8382
	step [234/250], loss=73.8533
	step [235/250], loss=81.1997
	step [236/250], loss=80.1441
	step [237/250], loss=76.2629
	step [238/250], loss=87.8115
	step [239/250], loss=80.7922
	step [240/250], loss=103.5942
	step [241/250], loss=82.6511
	step [242/250], loss=77.2964
	step [243/250], loss=96.4603
	step [244/250], loss=72.0096
	step [245/250], loss=88.5036
	step [246/250], loss=91.5736
	step [247/250], loss=80.9183
	step [248/250], loss=96.7970
	step [249/250], loss=94.1681
	step [250/250], loss=9.0499
	Evaluating
	loss=0.0103, precision=0.2333, recall=0.8770, f1=0.3685
saving model as: 0_saved_model.pth
Training epoch 57
	step [1/250], loss=72.0015
	step [2/250], loss=72.5040
	step [3/250], loss=94.1150
	step [4/250], loss=70.2575
	step [5/250], loss=83.0880
	step [6/250], loss=82.2391
	step [7/250], loss=73.3299
	step [8/250], loss=84.4244
	step [9/250], loss=100.5006
	step [10/250], loss=89.9673
	step [11/250], loss=84.6123
	step [12/250], loss=84.3645
	step [13/250], loss=72.2614
	step [14/250], loss=85.2249
	step [15/250], loss=94.2142
	step [16/250], loss=85.1281
	step [17/250], loss=82.6362
	step [18/250], loss=70.3279
	step [19/250], loss=89.8437
	step [20/250], loss=74.5967
	step [21/250], loss=83.9115
	step [22/250], loss=89.9471
	step [23/250], loss=97.5131
	step [24/250], loss=77.3330
	step [25/250], loss=76.1807
	step [26/250], loss=78.1225
	step [27/250], loss=75.7990
	step [28/250], loss=94.8652
	step [29/250], loss=81.7735
	step [30/250], loss=99.8707
	step [31/250], loss=85.1704
	step [32/250], loss=57.3047
	step [33/250], loss=94.6808
	step [34/250], loss=86.0424
	step [35/250], loss=76.6627
	step [36/250], loss=96.9048
	step [37/250], loss=80.1924
	step [38/250], loss=93.1507
	step [39/250], loss=82.0515
	step [40/250], loss=89.4350
	step [41/250], loss=80.5955
	step [42/250], loss=90.8604
	step [43/250], loss=81.0360
	step [44/250], loss=82.3310
	step [45/250], loss=86.0589
	step [46/250], loss=76.5164
	step [47/250], loss=87.8663
	step [48/250], loss=70.5948
	step [49/250], loss=74.3004
	step [50/250], loss=93.9046
	step [51/250], loss=85.7860
	step [52/250], loss=88.8992
	step [53/250], loss=94.7439
	step [54/250], loss=84.1324
	step [55/250], loss=92.7781
	step [56/250], loss=86.1844
	step [57/250], loss=75.1704
	step [58/250], loss=86.5022
	step [59/250], loss=110.0489
	step [60/250], loss=72.2654
	step [61/250], loss=80.0984
	step [62/250], loss=88.6429
	step [63/250], loss=95.2183
	step [64/250], loss=87.1092
	step [65/250], loss=83.1367
	step [66/250], loss=78.4208
	step [67/250], loss=81.0298
	step [68/250], loss=86.8651
	step [69/250], loss=69.0628
	step [70/250], loss=80.5890
	step [71/250], loss=91.9665
	step [72/250], loss=77.0823
	step [73/250], loss=89.1208
	step [74/250], loss=104.9650
	step [75/250], loss=87.4421
	step [76/250], loss=90.7758
	step [77/250], loss=94.2357
	step [78/250], loss=81.2644
	step [79/250], loss=90.3430
	step [80/250], loss=93.9922
	step [81/250], loss=66.2419
	step [82/250], loss=95.8037
	step [83/250], loss=85.8194
	step [84/250], loss=105.9302
	step [85/250], loss=83.1226
	step [86/250], loss=71.0691
	step [87/250], loss=78.8485
	step [88/250], loss=74.6335
	step [89/250], loss=88.3792
	step [90/250], loss=88.2702
	step [91/250], loss=81.5756
	step [92/250], loss=70.8814
	step [93/250], loss=91.0521
	step [94/250], loss=88.0883
	step [95/250], loss=78.5620
	step [96/250], loss=64.6419
	step [97/250], loss=104.4379
	step [98/250], loss=75.2731
	step [99/250], loss=100.6527
	step [100/250], loss=79.8335
	step [101/250], loss=64.8553
	step [102/250], loss=83.5456
	step [103/250], loss=85.5398
	step [104/250], loss=77.2624
	step [105/250], loss=77.5621
	step [106/250], loss=84.0745
	step [107/250], loss=83.7631
	step [108/250], loss=86.0700
	step [109/250], loss=82.2107
	step [110/250], loss=79.9495
	step [111/250], loss=92.0162
	step [112/250], loss=90.7176
	step [113/250], loss=82.1868
	step [114/250], loss=99.5797
	step [115/250], loss=87.2310
	step [116/250], loss=79.5919
	step [117/250], loss=81.1775
	step [118/250], loss=82.4162
	step [119/250], loss=81.5157
	step [120/250], loss=80.7175
	step [121/250], loss=82.2094
	step [122/250], loss=87.1471
	step [123/250], loss=88.8520
	step [124/250], loss=83.8395
	step [125/250], loss=92.8953
	step [126/250], loss=77.6097
	step [127/250], loss=73.5198
	step [128/250], loss=66.5761
	step [129/250], loss=89.3769
	step [130/250], loss=81.7562
	step [131/250], loss=87.8806
	step [132/250], loss=98.6671
	step [133/250], loss=71.4620
	step [134/250], loss=85.0263
	step [135/250], loss=81.6617
	step [136/250], loss=80.3493
	step [137/250], loss=88.6999
	step [138/250], loss=86.1431
	step [139/250], loss=78.0212
	step [140/250], loss=98.7029
	step [141/250], loss=94.8885
	step [142/250], loss=99.8833
	step [143/250], loss=85.5597
	step [144/250], loss=83.4346
	step [145/250], loss=100.8974
	step [146/250], loss=102.6863
	step [147/250], loss=67.6275
	step [148/250], loss=103.9327
	step [149/250], loss=73.8176
	step [150/250], loss=85.6503
	step [151/250], loss=85.2316
	step [152/250], loss=110.5154
	step [153/250], loss=73.4466
	step [154/250], loss=97.6369
	step [155/250], loss=87.0115
	step [156/250], loss=91.0265
	step [157/250], loss=93.3888
	step [158/250], loss=84.1024
	step [159/250], loss=93.5456
	step [160/250], loss=90.0690
	step [161/250], loss=80.0309
	step [162/250], loss=87.5479
	step [163/250], loss=80.8208
	step [164/250], loss=95.4010
	step [165/250], loss=93.0815
	step [166/250], loss=73.8882
	step [167/250], loss=86.8962
	step [168/250], loss=77.4639
	step [169/250], loss=101.3699
	step [170/250], loss=84.7472
	step [171/250], loss=88.7368
	step [172/250], loss=98.3745
	step [173/250], loss=86.1463
	step [174/250], loss=84.4395
	step [175/250], loss=80.7628
	step [176/250], loss=93.4126
	step [177/250], loss=75.5940
	step [178/250], loss=80.3195
	step [179/250], loss=95.8873
	step [180/250], loss=84.5382
	step [181/250], loss=83.3953
	step [182/250], loss=74.0640
	step [183/250], loss=70.2805
	step [184/250], loss=94.8223
	step [185/250], loss=82.4894
	step [186/250], loss=87.2719
	step [187/250], loss=106.1861
	step [188/250], loss=73.5929
	step [189/250], loss=77.1202
	step [190/250], loss=91.2775
	step [191/250], loss=90.3237
	step [192/250], loss=76.8416
	step [193/250], loss=81.3898
	step [194/250], loss=75.5415
	step [195/250], loss=65.0582
	step [196/250], loss=84.3878
	step [197/250], loss=75.3430
	step [198/250], loss=88.4495
	step [199/250], loss=94.2203
	step [200/250], loss=108.5564
	step [201/250], loss=76.3319
	step [202/250], loss=80.0484
	step [203/250], loss=86.5566
	step [204/250], loss=95.7863
	step [205/250], loss=95.9518
	step [206/250], loss=87.4459
	step [207/250], loss=81.2137
	step [208/250], loss=68.0270
	step [209/250], loss=79.7227
	step [210/250], loss=83.9504
	step [211/250], loss=84.9185
	step [212/250], loss=89.0158
	step [213/250], loss=99.4337
	step [214/250], loss=85.3357
	step [215/250], loss=87.6852
	step [216/250], loss=89.9029
	step [217/250], loss=87.5315
	step [218/250], loss=80.8343
	step [219/250], loss=80.7786
	step [220/250], loss=83.8328
	step [221/250], loss=91.3538
	step [222/250], loss=83.2046
	step [223/250], loss=70.1024
	step [224/250], loss=77.8017
	step [225/250], loss=90.0505
	step [226/250], loss=88.3462
	step [227/250], loss=78.6109
	step [228/250], loss=89.2992
	step [229/250], loss=84.2728
	step [230/250], loss=82.0428
	step [231/250], loss=104.8187
	step [232/250], loss=85.2715
	step [233/250], loss=91.0925
	step [234/250], loss=79.5331
	step [235/250], loss=79.3554
	step [236/250], loss=99.8406
	step [237/250], loss=93.1288
	step [238/250], loss=80.9225
	step [239/250], loss=89.7850
	step [240/250], loss=89.0236
	step [241/250], loss=92.9879
	step [242/250], loss=65.2106
	step [243/250], loss=74.9784
	step [244/250], loss=81.2119
	step [245/250], loss=98.2930
	step [246/250], loss=75.1863
	step [247/250], loss=99.1278
	step [248/250], loss=88.3213
	step [249/250], loss=73.8761
	step [250/250], loss=13.4208
	Evaluating
	loss=0.0095, precision=0.2689, recall=0.8776, f1=0.4117
saving model as: 0_saved_model.pth
Training epoch 58
	step [1/250], loss=89.3727
	step [2/250], loss=85.5842
	step [3/250], loss=95.3771
	step [4/250], loss=91.1783
	step [5/250], loss=89.0547
	step [6/250], loss=99.9428
	step [7/250], loss=79.8033
	step [8/250], loss=76.8074
	step [9/250], loss=87.5553
	step [10/250], loss=94.6442
	step [11/250], loss=86.0707
	step [12/250], loss=93.7772
	step [13/250], loss=97.7680
	step [14/250], loss=77.2787
	step [15/250], loss=93.2851
	step [16/250], loss=83.6230
	step [17/250], loss=79.8293
	step [18/250], loss=68.1504
	step [19/250], loss=87.3684
	step [20/250], loss=99.0694
	step [21/250], loss=88.9234
	step [22/250], loss=68.1212
	step [23/250], loss=92.2364
	step [24/250], loss=72.8252
	step [25/250], loss=97.8470
	step [26/250], loss=82.3674
	step [27/250], loss=69.9476
	step [28/250], loss=73.6644
	step [29/250], loss=73.0938
	step [30/250], loss=86.7490
	step [31/250], loss=94.4641
	step [32/250], loss=82.5855
	step [33/250], loss=82.8119
	step [34/250], loss=84.0601
	step [35/250], loss=92.7108
	step [36/250], loss=83.5993
	step [37/250], loss=88.9277
	step [38/250], loss=84.1233
	step [39/250], loss=106.6772
	step [40/250], loss=88.0450
	step [41/250], loss=67.9186
	step [42/250], loss=94.4238
	step [43/250], loss=87.6203
	step [44/250], loss=69.4995
	step [45/250], loss=103.3426
	step [46/250], loss=82.3519
	step [47/250], loss=85.9526
	step [48/250], loss=77.8268
	step [49/250], loss=71.7280
	step [50/250], loss=67.2417
	step [51/250], loss=81.0942
	step [52/250], loss=100.4260
	step [53/250], loss=78.4288
	step [54/250], loss=88.8594
	step [55/250], loss=82.7462
	step [56/250], loss=70.0536
	step [57/250], loss=97.7508
	step [58/250], loss=96.2315
	step [59/250], loss=77.4703
	step [60/250], loss=78.7883
	step [61/250], loss=96.2771
	step [62/250], loss=82.3027
	step [63/250], loss=75.4440
	step [64/250], loss=94.1192
	step [65/250], loss=78.3798
	step [66/250], loss=78.9657
	step [67/250], loss=89.7718
	step [68/250], loss=77.2141
	step [69/250], loss=84.9866
	step [70/250], loss=83.1465
	step [71/250], loss=84.5549
	step [72/250], loss=81.4706
	step [73/250], loss=80.9991
	step [74/250], loss=108.2216
	step [75/250], loss=75.7860
	step [76/250], loss=95.8847
	step [77/250], loss=95.0204
	step [78/250], loss=75.7855
	step [79/250], loss=84.7164
	step [80/250], loss=73.1327
	step [81/250], loss=79.2866
	step [82/250], loss=73.7039
	step [83/250], loss=115.6888
	step [84/250], loss=88.0048
	step [85/250], loss=91.4504
	step [86/250], loss=104.7383
	step [87/250], loss=74.7597
	step [88/250], loss=85.2645
	step [89/250], loss=97.1660
	step [90/250], loss=87.4729
	step [91/250], loss=95.9164
	step [92/250], loss=82.8226
	step [93/250], loss=72.7327
	step [94/250], loss=92.3446
	step [95/250], loss=99.0835
	step [96/250], loss=90.0427
	step [97/250], loss=85.3612
	step [98/250], loss=72.4596
	step [99/250], loss=87.2484
	step [100/250], loss=89.1638
	step [101/250], loss=65.5766
	step [102/250], loss=73.7846
	step [103/250], loss=87.2984
	step [104/250], loss=92.5045
	step [105/250], loss=72.8838
	step [106/250], loss=80.8696
	step [107/250], loss=70.9492
	step [108/250], loss=80.9442
	step [109/250], loss=81.0975
	step [110/250], loss=76.0748
	step [111/250], loss=86.6457
	step [112/250], loss=90.6907
	step [113/250], loss=92.1786
	step [114/250], loss=94.1953
	step [115/250], loss=67.4725
	step [116/250], loss=80.7920
	step [117/250], loss=78.9586
	step [118/250], loss=71.2026
	step [119/250], loss=68.8560
	step [120/250], loss=84.5376
	step [121/250], loss=101.3260
	step [122/250], loss=65.8210
	step [123/250], loss=101.6799
	step [124/250], loss=68.6867
	step [125/250], loss=88.8647
	step [126/250], loss=93.7106
	step [127/250], loss=82.0249
	step [128/250], loss=65.5373
	step [129/250], loss=77.5599
	step [130/250], loss=70.9900
	step [131/250], loss=80.2250
	step [132/250], loss=93.2529
	step [133/250], loss=76.4470
	step [134/250], loss=58.3415
	step [135/250], loss=76.2853
	step [136/250], loss=74.3895
	step [137/250], loss=88.8488
	step [138/250], loss=65.2525
	step [139/250], loss=94.3580
	step [140/250], loss=83.0335
	step [141/250], loss=70.9551
	step [142/250], loss=97.5758
	step [143/250], loss=92.6046
	step [144/250], loss=96.0811
	step [145/250], loss=83.4870
	step [146/250], loss=86.9360
	step [147/250], loss=83.7599
	step [148/250], loss=90.6105
	step [149/250], loss=103.7648
	step [150/250], loss=85.8773
	step [151/250], loss=78.8223
	step [152/250], loss=73.6292
	step [153/250], loss=84.9102
	step [154/250], loss=79.1013
	step [155/250], loss=78.1550
	step [156/250], loss=79.7656
	step [157/250], loss=82.1011
	step [158/250], loss=85.9716
	step [159/250], loss=94.5547
	step [160/250], loss=79.1825
	step [161/250], loss=93.0263
	step [162/250], loss=85.5533
	step [163/250], loss=62.7425
	step [164/250], loss=96.6665
	step [165/250], loss=92.4406
	step [166/250], loss=85.6762
	step [167/250], loss=79.8685
	step [168/250], loss=112.1240
	step [169/250], loss=60.0218
	step [170/250], loss=90.4971
	step [171/250], loss=90.4970
	step [172/250], loss=81.3134
	step [173/250], loss=67.7817
	step [174/250], loss=89.7118
	step [175/250], loss=101.2634
	step [176/250], loss=74.9726
	step [177/250], loss=93.1399
	step [178/250], loss=80.9682
	step [179/250], loss=84.0819
	step [180/250], loss=77.6251
	step [181/250], loss=100.5275
	step [182/250], loss=59.7013
	step [183/250], loss=74.7864
	step [184/250], loss=89.5841
	step [185/250], loss=87.2988
	step [186/250], loss=91.6387
	step [187/250], loss=66.0675
	step [188/250], loss=99.3565
	step [189/250], loss=90.3829
	step [190/250], loss=95.0853
	step [191/250], loss=74.2916
	step [192/250], loss=72.8444
	step [193/250], loss=88.3017
	step [194/250], loss=75.8180
	step [195/250], loss=79.1133
	step [196/250], loss=86.1445
	step [197/250], loss=89.6923
	step [198/250], loss=92.7843
	step [199/250], loss=107.5557
	step [200/250], loss=82.1214
	step [201/250], loss=65.8766
	step [202/250], loss=98.1852
	step [203/250], loss=87.7743
	step [204/250], loss=89.4717
	step [205/250], loss=97.8027
	step [206/250], loss=87.0670
	step [207/250], loss=84.8162
	step [208/250], loss=95.8090
	step [209/250], loss=89.2700
	step [210/250], loss=73.1843
	step [211/250], loss=89.7961
	step [212/250], loss=77.2503
	step [213/250], loss=82.5053
	step [214/250], loss=64.1456
	step [215/250], loss=81.9778
	step [216/250], loss=79.6772
	step [217/250], loss=66.7986
	step [218/250], loss=84.4147
	step [219/250], loss=109.2543
	step [220/250], loss=79.8178
	step [221/250], loss=101.4738
	step [222/250], loss=101.6627
	step [223/250], loss=80.7196
	step [224/250], loss=80.3619
	step [225/250], loss=61.9874
	step [226/250], loss=79.1618
	step [227/250], loss=82.9843
	step [228/250], loss=75.4863
	step [229/250], loss=103.9939
	step [230/250], loss=89.7628
	step [231/250], loss=84.2091
	step [232/250], loss=79.8080
	step [233/250], loss=75.6516
	step [234/250], loss=57.5535
	step [235/250], loss=73.0630
	step [236/250], loss=72.8595
	step [237/250], loss=89.7999
	step [238/250], loss=91.4982
	step [239/250], loss=76.5120
	step [240/250], loss=93.4129
	step [241/250], loss=74.2501
	step [242/250], loss=79.1210
	step [243/250], loss=94.4697
	step [244/250], loss=94.9858
	step [245/250], loss=79.7041
	step [246/250], loss=93.8580
	step [247/250], loss=93.4246
	step [248/250], loss=68.5700
	step [249/250], loss=86.3110
	step [250/250], loss=12.5605
	Evaluating
	loss=0.0122, precision=0.1938, recall=0.8826, f1=0.3178
Training epoch 59
	step [1/250], loss=91.4372
	step [2/250], loss=86.2057
	step [3/250], loss=92.7923
	step [4/250], loss=74.0938
	step [5/250], loss=90.7480
	step [6/250], loss=76.9742
	step [7/250], loss=101.0549
	step [8/250], loss=90.2068
	step [9/250], loss=79.1653
	step [10/250], loss=82.0550
	step [11/250], loss=96.2116
	step [12/250], loss=76.1607
	step [13/250], loss=85.2910
	step [14/250], loss=75.0071
	step [15/250], loss=73.3578
	step [16/250], loss=84.7623
	step [17/250], loss=67.5274
	step [18/250], loss=96.9959
	step [19/250], loss=86.5982
	step [20/250], loss=77.0346
	step [21/250], loss=79.2506
	step [22/250], loss=81.6770
	step [23/250], loss=91.9090
	step [24/250], loss=87.8246
	step [25/250], loss=90.3613
	step [26/250], loss=75.1291
	step [27/250], loss=95.0519
	step [28/250], loss=92.6651
	step [29/250], loss=93.6087
	step [30/250], loss=73.8053
	step [31/250], loss=81.9801
	step [32/250], loss=86.6830
	step [33/250], loss=95.9140
	step [34/250], loss=117.0458
	step [35/250], loss=81.3601
	step [36/250], loss=79.7713
	step [37/250], loss=74.9165
	step [38/250], loss=88.3734
	step [39/250], loss=96.1485
	step [40/250], loss=100.6311
	step [41/250], loss=82.6483
	step [42/250], loss=97.2910
	step [43/250], loss=99.5502
	step [44/250], loss=76.2263
	step [45/250], loss=77.5366
	step [46/250], loss=91.5389
	step [47/250], loss=88.0124
	step [48/250], loss=79.1644
	step [49/250], loss=77.7974
	step [50/250], loss=78.4790
	step [51/250], loss=90.3061
	step [52/250], loss=76.2640
	step [53/250], loss=90.3144
	step [54/250], loss=88.0864
	step [55/250], loss=80.0338
	step [56/250], loss=77.7145
	step [57/250], loss=96.3622
	step [58/250], loss=72.5258
	step [59/250], loss=85.9023
	step [60/250], loss=73.6079
	step [61/250], loss=95.6086
	step [62/250], loss=80.8413
	step [63/250], loss=77.2666
	step [64/250], loss=86.2958
	step [65/250], loss=75.3607
	step [66/250], loss=73.7205
	step [67/250], loss=79.9740
	step [68/250], loss=81.2221
	step [69/250], loss=105.8332
	step [70/250], loss=100.6729
	step [71/250], loss=74.7405
	step [72/250], loss=92.2177
	step [73/250], loss=88.5976
	step [74/250], loss=85.5437
	step [75/250], loss=61.5744
	step [76/250], loss=78.6314
	step [77/250], loss=80.8615
	step [78/250], loss=88.1091
	step [79/250], loss=86.8205
	step [80/250], loss=100.0944
	step [81/250], loss=78.5792
	step [82/250], loss=75.8971
	step [83/250], loss=85.3247
	step [84/250], loss=82.2327
	step [85/250], loss=100.9021
	step [86/250], loss=83.7337
	step [87/250], loss=76.2364
	step [88/250], loss=69.2651
	step [89/250], loss=87.1274
	step [90/250], loss=72.1390
	step [91/250], loss=70.7166
	step [92/250], loss=80.8160
	step [93/250], loss=98.2666
	step [94/250], loss=87.5954
	step [95/250], loss=88.4624
	step [96/250], loss=72.0012
	step [97/250], loss=95.5613
	step [98/250], loss=87.9339
	step [99/250], loss=83.3616
	step [100/250], loss=105.2667
	step [101/250], loss=73.5677
	step [102/250], loss=83.9936
	step [103/250], loss=80.8413
	step [104/250], loss=80.2074
	step [105/250], loss=69.3421
	step [106/250], loss=75.2896
	step [107/250], loss=87.9485
	step [108/250], loss=69.1161
	step [109/250], loss=81.5432
	step [110/250], loss=86.7861
	step [111/250], loss=99.6366
	step [112/250], loss=88.9010
	step [113/250], loss=68.2350
	step [114/250], loss=74.4439
	step [115/250], loss=90.5952
	step [116/250], loss=92.1983
	step [117/250], loss=85.7887
	step [118/250], loss=88.8883
	step [119/250], loss=97.2838
	step [120/250], loss=83.6945
	step [121/250], loss=73.4431
	step [122/250], loss=84.0176
	step [123/250], loss=85.6506
	step [124/250], loss=88.5201
	step [125/250], loss=76.4355
	step [126/250], loss=70.9970
	step [127/250], loss=82.4639
	step [128/250], loss=77.8778
	step [129/250], loss=80.2709
	step [130/250], loss=62.6818
	step [131/250], loss=90.0440
	step [132/250], loss=83.1533
	step [133/250], loss=71.6267
	step [134/250], loss=88.0284
	step [135/250], loss=73.8834
	step [136/250], loss=79.5007
	step [137/250], loss=74.7660
	step [138/250], loss=82.0216
	step [139/250], loss=76.3270
	step [140/250], loss=81.1722
	step [141/250], loss=99.9989
	step [142/250], loss=90.9600
	step [143/250], loss=65.4660
	step [144/250], loss=93.9409
	step [145/250], loss=86.3673
	step [146/250], loss=97.7067
	step [147/250], loss=90.2484
	step [148/250], loss=83.9130
	step [149/250], loss=73.9807
	step [150/250], loss=78.8034
	step [151/250], loss=113.3146
	step [152/250], loss=78.9318
	step [153/250], loss=76.9321
	step [154/250], loss=89.3406
	step [155/250], loss=71.0838
	step [156/250], loss=72.9107
	step [157/250], loss=84.0653
	step [158/250], loss=68.4909
	step [159/250], loss=94.9521
	step [160/250], loss=96.5441
	step [161/250], loss=91.4874
	step [162/250], loss=77.7090
	step [163/250], loss=79.7503
	step [164/250], loss=74.5975
	step [165/250], loss=72.6796
	step [166/250], loss=94.5728
	step [167/250], loss=80.8468
	step [168/250], loss=97.1216
	step [169/250], loss=83.4185
	step [170/250], loss=78.8078
	step [171/250], loss=88.1170
	step [172/250], loss=69.9986
	step [173/250], loss=86.0363
	step [174/250], loss=95.5917
	step [175/250], loss=116.2108
	step [176/250], loss=68.6793
	step [177/250], loss=64.5995
	step [178/250], loss=82.8126
	step [179/250], loss=81.9499
	step [180/250], loss=80.6652
	step [181/250], loss=82.6745
	step [182/250], loss=73.9507
	step [183/250], loss=72.5683
	step [184/250], loss=93.8377
	step [185/250], loss=72.6942
	step [186/250], loss=96.9741
	step [187/250], loss=79.4691
	step [188/250], loss=75.5216
	step [189/250], loss=89.2756
	step [190/250], loss=84.1787
	step [191/250], loss=90.7616
	step [192/250], loss=82.0670
	step [193/250], loss=107.7867
	step [194/250], loss=83.6526
	step [195/250], loss=97.5076
	step [196/250], loss=100.0953
	step [197/250], loss=84.8217
	step [198/250], loss=88.5801
	step [199/250], loss=86.0828
	step [200/250], loss=62.2662
	step [201/250], loss=80.8118
	step [202/250], loss=92.2685
	step [203/250], loss=69.6642
	step [204/250], loss=77.6119
	step [205/250], loss=78.1522
	step [206/250], loss=86.1821
	step [207/250], loss=70.1842
	step [208/250], loss=87.9389
	step [209/250], loss=92.5207
	step [210/250], loss=88.4432
	step [211/250], loss=82.6611
	step [212/250], loss=85.2263
	step [213/250], loss=79.2125
	step [214/250], loss=83.9442
	step [215/250], loss=95.4468
	step [216/250], loss=99.8031
	step [217/250], loss=71.9697
	step [218/250], loss=82.6362
	step [219/250], loss=77.9192
	step [220/250], loss=71.7954
	step [221/250], loss=83.8156
	step [222/250], loss=92.2198
	step [223/250], loss=90.1208
	step [224/250], loss=74.9879
	step [225/250], loss=82.5224
	step [226/250], loss=81.2212
	step [227/250], loss=73.5450
	step [228/250], loss=106.5050
	step [229/250], loss=76.9059
	step [230/250], loss=67.5032
	step [231/250], loss=74.9369
	step [232/250], loss=83.8090
	step [233/250], loss=73.2459
	step [234/250], loss=78.0367
	step [235/250], loss=80.6381
	step [236/250], loss=79.1207
	step [237/250], loss=74.8273
	step [238/250], loss=82.0244
	step [239/250], loss=101.2114
	step [240/250], loss=72.3826
	step [241/250], loss=85.1003
	step [242/250], loss=82.8533
	step [243/250], loss=75.7656
	step [244/250], loss=88.9221
	step [245/250], loss=92.0621
	step [246/250], loss=93.3207
	step [247/250], loss=87.6326
	step [248/250], loss=98.8514
	step [249/250], loss=90.1226
	step [250/250], loss=19.8723
	Evaluating
	loss=0.0103, precision=0.2393, recall=0.8821, f1=0.3765
Training epoch 60
	step [1/250], loss=80.1386
	step [2/250], loss=83.4336
	step [3/250], loss=82.9127
	step [4/250], loss=99.9084
	step [5/250], loss=83.0409
	step [6/250], loss=78.6793
	step [7/250], loss=92.8525
	step [8/250], loss=94.5495
	step [9/250], loss=87.0798
	step [10/250], loss=75.5915
	step [11/250], loss=64.7386
	step [12/250], loss=78.3663
	step [13/250], loss=75.9615
	step [14/250], loss=74.6935
	step [15/250], loss=81.5248
	step [16/250], loss=77.0248
	step [17/250], loss=85.4804
	step [18/250], loss=100.2200
	step [19/250], loss=108.6913
	step [20/250], loss=90.3726
	step [21/250], loss=87.0411
	step [22/250], loss=87.8271
	step [23/250], loss=76.7166
	step [24/250], loss=81.3341
	step [25/250], loss=82.4345
	step [26/250], loss=91.1332
	step [27/250], loss=73.8369
	step [28/250], loss=70.9916
	step [29/250], loss=68.9805
	step [30/250], loss=74.9279
	step [31/250], loss=92.1505
	step [32/250], loss=81.7711
	step [33/250], loss=100.4231
	step [34/250], loss=80.7186
	step [35/250], loss=83.4270
	step [36/250], loss=91.9499
	step [37/250], loss=84.6652
	step [38/250], loss=81.3751
	step [39/250], loss=78.9102
	step [40/250], loss=90.0840
	step [41/250], loss=79.7006
	step [42/250], loss=77.1884
	step [43/250], loss=89.1648
	step [44/250], loss=87.6646
	step [45/250], loss=92.8408
	step [46/250], loss=76.5498
	step [47/250], loss=79.6007
	step [48/250], loss=89.1234
	step [49/250], loss=86.8533
	step [50/250], loss=72.7484
	step [51/250], loss=101.1214
	step [52/250], loss=101.5278
	step [53/250], loss=76.7186
	step [54/250], loss=73.8986
	step [55/250], loss=64.4755
	step [56/250], loss=91.4925
	step [57/250], loss=91.7234
	step [58/250], loss=78.0554
	step [59/250], loss=93.3393
	step [60/250], loss=67.9967
	step [61/250], loss=70.5384
	step [62/250], loss=68.7896
	step [63/250], loss=74.1884
	step [64/250], loss=86.7765
	step [65/250], loss=80.6059
	step [66/250], loss=73.1992
	step [67/250], loss=84.9818
	step [68/250], loss=73.8678
	step [69/250], loss=82.5030
	step [70/250], loss=84.7465
	step [71/250], loss=56.7320
	step [72/250], loss=80.0388
	step [73/250], loss=81.2233
	step [74/250], loss=105.6188
	step [75/250], loss=79.4543
	step [76/250], loss=81.7520
	step [77/250], loss=70.0968
	step [78/250], loss=83.7042
	step [79/250], loss=99.0256
	step [80/250], loss=79.6064
	step [81/250], loss=75.2418
	step [82/250], loss=76.4074
	step [83/250], loss=85.5925
	step [84/250], loss=71.8153
	step [85/250], loss=84.4008
	step [86/250], loss=93.0003
	step [87/250], loss=75.6128
	step [88/250], loss=88.5468
	step [89/250], loss=77.8723
	step [90/250], loss=76.7424
	step [91/250], loss=65.0655
	step [92/250], loss=77.4405
	step [93/250], loss=82.1076
	step [94/250], loss=84.7354
	step [95/250], loss=88.3422
	step [96/250], loss=97.3551
	step [97/250], loss=91.7943
	step [98/250], loss=81.2086
	step [99/250], loss=79.0149
	step [100/250], loss=94.8982
	step [101/250], loss=69.8020
	step [102/250], loss=82.3198
	step [103/250], loss=90.4628
	step [104/250], loss=73.8162
	step [105/250], loss=91.0963
	step [106/250], loss=94.4533
	step [107/250], loss=97.9411
	step [108/250], loss=81.7890
	step [109/250], loss=62.8286
	step [110/250], loss=78.6040
	step [111/250], loss=90.7120
	step [112/250], loss=74.9465
	step [113/250], loss=92.9366
	step [114/250], loss=70.8571
	step [115/250], loss=83.6017
	step [116/250], loss=63.3517
	step [117/250], loss=93.1478
	step [118/250], loss=80.7797
	step [119/250], loss=77.8485
	step [120/250], loss=101.2327
	step [121/250], loss=72.5101
	step [122/250], loss=79.0237
	step [123/250], loss=79.9884
	step [124/250], loss=85.9362
	step [125/250], loss=108.8370
	step [126/250], loss=106.3557
	step [127/250], loss=75.1092
	step [128/250], loss=96.9670
	step [129/250], loss=97.9259
	step [130/250], loss=70.8011
	step [131/250], loss=102.4952
	step [132/250], loss=73.7507
	step [133/250], loss=93.2529
	step [134/250], loss=78.5732
	step [135/250], loss=75.3360
	step [136/250], loss=79.5493
	step [137/250], loss=86.2494
	step [138/250], loss=89.9364
	step [139/250], loss=70.5627
	step [140/250], loss=70.3938
	step [141/250], loss=95.8971
	step [142/250], loss=73.2945
	step [143/250], loss=94.0058
	step [144/250], loss=71.9890
	step [145/250], loss=76.7729
	step [146/250], loss=72.5614
	step [147/250], loss=90.7413
	step [148/250], loss=80.9230
	step [149/250], loss=90.8671
	step [150/250], loss=80.9509
	step [151/250], loss=76.4007
	step [152/250], loss=89.0835
	step [153/250], loss=91.3269
	step [154/250], loss=71.4892
	step [155/250], loss=84.4230
	step [156/250], loss=116.6883
	step [157/250], loss=78.2174
	step [158/250], loss=85.7203
	step [159/250], loss=84.6886
	step [160/250], loss=95.8143
	step [161/250], loss=72.7391
	step [162/250], loss=81.4707
	step [163/250], loss=65.9319
	step [164/250], loss=70.6120
	step [165/250], loss=98.1843
	step [166/250], loss=88.0744
	step [167/250], loss=71.8097
	step [168/250], loss=86.3731
	step [169/250], loss=93.3099
	step [170/250], loss=83.9839
	step [171/250], loss=95.5604
	step [172/250], loss=74.9867
	step [173/250], loss=82.1405
	step [174/250], loss=96.0502
	step [175/250], loss=89.5510
	step [176/250], loss=94.4074
	step [177/250], loss=86.3509
	step [178/250], loss=82.6584
	step [179/250], loss=72.5443
	step [180/250], loss=84.2667
	step [181/250], loss=83.8084
	step [182/250], loss=85.7466
	step [183/250], loss=89.2218
	step [184/250], loss=75.1014
	step [185/250], loss=103.3928
	step [186/250], loss=86.3568
	step [187/250], loss=70.7376
	step [188/250], loss=106.7431
	step [189/250], loss=97.1304
	step [190/250], loss=87.1727
	step [191/250], loss=99.0948
	step [192/250], loss=89.9518
	step [193/250], loss=78.2269
	step [194/250], loss=74.7435
	step [195/250], loss=72.7140
	step [196/250], loss=79.6338
	step [197/250], loss=81.9297
	step [198/250], loss=89.5265
	step [199/250], loss=92.1213
	step [200/250], loss=81.4260
	step [201/250], loss=78.1468
	step [202/250], loss=86.1143
	step [203/250], loss=95.3744
	step [204/250], loss=65.3678
	step [205/250], loss=92.0072
	step [206/250], loss=75.0546
	step [207/250], loss=83.1351
	step [208/250], loss=72.2737
	step [209/250], loss=82.0586
	step [210/250], loss=85.8763
	step [211/250], loss=85.9983
	step [212/250], loss=86.7261
	step [213/250], loss=86.0291
	step [214/250], loss=87.4450
	step [215/250], loss=73.2679
	step [216/250], loss=103.0936
	step [217/250], loss=87.7952
	step [218/250], loss=91.7781
	step [219/250], loss=87.7995
	step [220/250], loss=93.6950
	step [221/250], loss=77.2264
	step [222/250], loss=88.4417
	step [223/250], loss=95.7818
	step [224/250], loss=89.7614
	step [225/250], loss=85.9124
	step [226/250], loss=68.5029
	step [227/250], loss=84.8333
	step [228/250], loss=89.3041
	step [229/250], loss=72.6678
	step [230/250], loss=85.7441
	step [231/250], loss=82.6553
	step [232/250], loss=68.5048
	step [233/250], loss=87.6565
	step [234/250], loss=69.2485
	step [235/250], loss=84.0509
	step [236/250], loss=82.3923
	step [237/250], loss=79.4700
	step [238/250], loss=90.0227
	step [239/250], loss=74.4100
	step [240/250], loss=78.1993
	step [241/250], loss=65.0371
	step [242/250], loss=93.9359
	step [243/250], loss=85.6716
	step [244/250], loss=80.5175
	step [245/250], loss=93.6734
	step [246/250], loss=68.4338
	step [247/250], loss=90.6683
	step [248/250], loss=80.9510
	step [249/250], loss=72.4624
	step [250/250], loss=15.8729
	Evaluating
	loss=0.0087, precision=0.2834, recall=0.8594, f1=0.4262
saving model as: 0_saved_model.pth
Training epoch 61
	step [1/250], loss=73.3298
	step [2/250], loss=78.8353
	step [3/250], loss=95.7130
	step [4/250], loss=76.9194
	step [5/250], loss=85.5478
	step [6/250], loss=79.3125
	step [7/250], loss=82.3629
	step [8/250], loss=75.8669
	step [9/250], loss=71.7173
	step [10/250], loss=93.8362
	step [11/250], loss=76.8162
	step [12/250], loss=82.8130
	step [13/250], loss=90.9035
	step [14/250], loss=66.5642
	step [15/250], loss=80.8094
	step [16/250], loss=79.3261
	step [17/250], loss=73.5382
	step [18/250], loss=80.6534
	step [19/250], loss=98.4778
	step [20/250], loss=57.7315
	step [21/250], loss=76.8940
	step [22/250], loss=69.1904
	step [23/250], loss=75.2540
	step [24/250], loss=71.7578
	step [25/250], loss=95.7973
	step [26/250], loss=97.4418
	step [27/250], loss=79.0320
	step [28/250], loss=75.7514
	step [29/250], loss=88.5994
	step [30/250], loss=92.4479
	step [31/250], loss=83.3330
	step [32/250], loss=85.4051
	step [33/250], loss=84.9525
	step [34/250], loss=66.9756
	step [35/250], loss=69.2548
	step [36/250], loss=95.3165
	step [37/250], loss=87.0956
	step [38/250], loss=84.0946
	step [39/250], loss=88.3967
	step [40/250], loss=79.4102
	step [41/250], loss=79.0711
	step [42/250], loss=95.7634
	step [43/250], loss=73.5422
	step [44/250], loss=74.5448
	step [45/250], loss=84.2238
	step [46/250], loss=104.0701
	step [47/250], loss=70.6918
	step [48/250], loss=76.4188
	step [49/250], loss=77.0440
	step [50/250], loss=81.9260
	step [51/250], loss=74.4305
	step [52/250], loss=80.6182
	step [53/250], loss=108.6788
	step [54/250], loss=93.6188
	step [55/250], loss=78.2938
	step [56/250], loss=77.2946
	step [57/250], loss=93.4867
	step [58/250], loss=80.8988
	step [59/250], loss=77.5465
	step [60/250], loss=83.2622
	step [61/250], loss=91.2162
	step [62/250], loss=100.8164
	step [63/250], loss=90.9512
	step [64/250], loss=84.2383
	step [65/250], loss=78.3356
	step [66/250], loss=69.8017
	step [67/250], loss=80.1158
	step [68/250], loss=106.7280
	step [69/250], loss=78.4659
	step [70/250], loss=80.6970
	step [71/250], loss=97.7913
	step [72/250], loss=83.6916
	step [73/250], loss=77.6449
	step [74/250], loss=89.1908
	step [75/250], loss=95.3975
	step [76/250], loss=96.5086
	step [77/250], loss=85.7380
	step [78/250], loss=84.4732
	step [79/250], loss=80.7563
	step [80/250], loss=86.8677
	step [81/250], loss=90.2947
	step [82/250], loss=72.7940
	step [83/250], loss=100.7069
	step [84/250], loss=84.8962
	step [85/250], loss=78.9136
	step [86/250], loss=82.4201
	step [87/250], loss=100.2244
	step [88/250], loss=73.9650
	step [89/250], loss=85.0375
	step [90/250], loss=76.4884
	step [91/250], loss=98.9055
	step [92/250], loss=91.4722
	step [93/250], loss=92.3238
	step [94/250], loss=104.0347
	step [95/250], loss=82.3535
	step [96/250], loss=81.2453
	step [97/250], loss=91.3824
	step [98/250], loss=80.3347
	step [99/250], loss=87.2460
	step [100/250], loss=89.9913
	step [101/250], loss=66.6519
	step [102/250], loss=85.5057
	step [103/250], loss=74.0488
	step [104/250], loss=79.6479
	step [105/250], loss=90.1497
	step [106/250], loss=93.2119
	step [107/250], loss=64.1745
	step [108/250], loss=69.1977
	step [109/250], loss=79.3405
	step [110/250], loss=107.8271
	step [111/250], loss=78.9649
	step [112/250], loss=85.1992
	step [113/250], loss=79.5776
	step [114/250], loss=78.3381
	step [115/250], loss=88.2561
	step [116/250], loss=77.3666
	step [117/250], loss=83.7923
	step [118/250], loss=89.6479
	step [119/250], loss=85.5163
	step [120/250], loss=80.5433
	step [121/250], loss=86.8286
	step [122/250], loss=81.6433
	step [123/250], loss=77.0351
	step [124/250], loss=78.8395
	step [125/250], loss=100.8211
	step [126/250], loss=81.4638
	step [127/250], loss=98.0385
	step [128/250], loss=79.2754
	step [129/250], loss=76.6146
	step [130/250], loss=92.0311
	step [131/250], loss=98.8847
	step [132/250], loss=98.4784
	step [133/250], loss=108.8241
	step [134/250], loss=76.5187
	step [135/250], loss=99.3876
	step [136/250], loss=80.1730
	step [137/250], loss=73.6101
	step [138/250], loss=68.8454
	step [139/250], loss=77.3628
	step [140/250], loss=65.0038
	step [141/250], loss=82.5904
	step [142/250], loss=87.1023
	step [143/250], loss=79.6764
	step [144/250], loss=72.3086
	step [145/250], loss=76.5394
	step [146/250], loss=74.2218
	step [147/250], loss=77.8130
	step [148/250], loss=80.3458
	step [149/250], loss=76.5030
	step [150/250], loss=79.1557
	step [151/250], loss=76.3507
	step [152/250], loss=76.7172
	step [153/250], loss=77.4006
	step [154/250], loss=104.3506
	step [155/250], loss=68.3842
	step [156/250], loss=87.0813
	step [157/250], loss=79.4726
	step [158/250], loss=77.9874
	step [159/250], loss=85.3639
	step [160/250], loss=102.9683
	step [161/250], loss=89.3804
	step [162/250], loss=51.1922
	step [163/250], loss=88.3739
	step [164/250], loss=93.4169
	step [165/250], loss=69.4089
	step [166/250], loss=80.6675
	step [167/250], loss=72.8482
	step [168/250], loss=77.7800
	step [169/250], loss=87.6757
	step [170/250], loss=75.1187
	step [171/250], loss=94.0763
	step [172/250], loss=88.5837
	step [173/250], loss=68.6934
	step [174/250], loss=82.4173
	step [175/250], loss=82.4870
	step [176/250], loss=88.3330
	step [177/250], loss=74.0008
	step [178/250], loss=67.3220
	step [179/250], loss=76.3299
	step [180/250], loss=67.9138
	step [181/250], loss=83.3615
	step [182/250], loss=86.0857
	step [183/250], loss=87.4024
	step [184/250], loss=77.3938
	step [185/250], loss=104.7097
	step [186/250], loss=94.5964
	step [187/250], loss=85.9619
	step [188/250], loss=68.5484
	step [189/250], loss=85.7785
	step [190/250], loss=96.7317
	step [191/250], loss=95.6589
	step [192/250], loss=84.7098
	step [193/250], loss=90.3386
	step [194/250], loss=92.2653
	step [195/250], loss=84.1240
	step [196/250], loss=77.8788
	step [197/250], loss=84.3120
	step [198/250], loss=93.5270
	step [199/250], loss=83.8179
	step [200/250], loss=89.9016
	step [201/250], loss=69.1870
	step [202/250], loss=76.1514
	step [203/250], loss=69.7860
	step [204/250], loss=89.0716
	step [205/250], loss=86.6774
	step [206/250], loss=87.2590
	step [207/250], loss=98.2728
	step [208/250], loss=89.7702
	step [209/250], loss=80.7087
	step [210/250], loss=73.5686
	step [211/250], loss=77.7336
	step [212/250], loss=86.5357
	step [213/250], loss=84.8208
	step [214/250], loss=91.6401
	step [215/250], loss=89.6685
	step [216/250], loss=69.7402
	step [217/250], loss=80.2850
	step [218/250], loss=79.4029
	step [219/250], loss=73.7415
	step [220/250], loss=84.8986
	step [221/250], loss=87.2683
	step [222/250], loss=73.5771
	step [223/250], loss=102.8004
	step [224/250], loss=93.1766
	step [225/250], loss=78.6053
	step [226/250], loss=82.0906
	step [227/250], loss=68.6392
	step [228/250], loss=73.6980
	step [229/250], loss=83.0312
	step [230/250], loss=83.2767
	step [231/250], loss=76.8178
	step [232/250], loss=83.2604
	step [233/250], loss=72.9959
	step [234/250], loss=86.1550
	step [235/250], loss=86.0873
	step [236/250], loss=91.2683
	step [237/250], loss=77.5142
	step [238/250], loss=82.7051
	step [239/250], loss=87.8556
	step [240/250], loss=80.2724
	step [241/250], loss=89.1784
	step [242/250], loss=79.2397
	step [243/250], loss=79.1481
	step [244/250], loss=77.0090
	step [245/250], loss=88.6529
	step [246/250], loss=98.8622
	step [247/250], loss=86.3220
	step [248/250], loss=75.4406
	step [249/250], loss=75.8065
	step [250/250], loss=7.9825
	Evaluating
	loss=0.0088, precision=0.2797, recall=0.8674, f1=0.4230
Training epoch 62
	step [1/250], loss=68.5724
	step [2/250], loss=82.5290
	step [3/250], loss=84.8577
	step [4/250], loss=82.9353
	step [5/250], loss=113.3378
	step [6/250], loss=69.0769
	step [7/250], loss=81.7313
	step [8/250], loss=90.1210
	step [9/250], loss=89.0391
	step [10/250], loss=84.1225
	step [11/250], loss=76.7000
	step [12/250], loss=94.2643
	step [13/250], loss=71.0220
	step [14/250], loss=100.3743
	step [15/250], loss=90.0616
	step [16/250], loss=82.1611
	step [17/250], loss=66.6967
	step [18/250], loss=82.2682
	step [19/250], loss=65.9947
	step [20/250], loss=82.2918
	step [21/250], loss=84.4884
	step [22/250], loss=88.1782
	step [23/250], loss=82.1473
	step [24/250], loss=76.5167
	step [25/250], loss=72.8103
	step [26/250], loss=89.6479
	step [27/250], loss=93.8645
	step [28/250], loss=93.7765
	step [29/250], loss=82.4224
	step [30/250], loss=89.9786
	step [31/250], loss=72.3996
	step [32/250], loss=74.9525
	step [33/250], loss=81.3107
	step [34/250], loss=73.6891
	step [35/250], loss=79.3278
	step [36/250], loss=83.1486
	step [37/250], loss=75.4125
	step [38/250], loss=79.3659
	step [39/250], loss=86.2977
	step [40/250], loss=81.7108
	step [41/250], loss=93.4597
	step [42/250], loss=77.5811
	step [43/250], loss=84.7153
	step [44/250], loss=86.1061
	step [45/250], loss=76.6994
	step [46/250], loss=93.5189
	step [47/250], loss=89.5573
	step [48/250], loss=87.4458
	step [49/250], loss=91.4207
	step [50/250], loss=70.8155
	step [51/250], loss=88.7054
	step [52/250], loss=77.6091
	step [53/250], loss=88.4396
	step [54/250], loss=93.9392
	step [55/250], loss=87.8570
	step [56/250], loss=85.7521
	step [57/250], loss=93.4088
	step [58/250], loss=76.0792
	step [59/250], loss=86.5431
	step [60/250], loss=77.0905
	step [61/250], loss=98.0572
	step [62/250], loss=80.9218
	step [63/250], loss=75.8333
	step [64/250], loss=58.3388
	step [65/250], loss=57.1461
	step [66/250], loss=79.5385
	step [67/250], loss=82.4670
	step [68/250], loss=96.7774
	step [69/250], loss=81.7546
	step [70/250], loss=87.9550
	step [71/250], loss=84.8077
	step [72/250], loss=75.5718
	step [73/250], loss=86.0367
	step [74/250], loss=74.7681
	step [75/250], loss=80.5614
	step [76/250], loss=87.5674
	step [77/250], loss=77.0441
	step [78/250], loss=87.0862
	step [79/250], loss=70.4496
	step [80/250], loss=80.6496
	step [81/250], loss=86.8732
	step [82/250], loss=82.7936
	step [83/250], loss=73.1631
	step [84/250], loss=82.7710
	step [85/250], loss=72.6751
	step [86/250], loss=81.2790
	step [87/250], loss=79.8676
	step [88/250], loss=96.4803
	step [89/250], loss=83.3776
	step [90/250], loss=92.2084
	step [91/250], loss=76.5394
	step [92/250], loss=86.0896
	step [93/250], loss=76.0452
	step [94/250], loss=99.8114
	step [95/250], loss=80.3513
	step [96/250], loss=87.0999
	step [97/250], loss=79.1490
	step [98/250], loss=98.1377
	step [99/250], loss=87.7287
	step [100/250], loss=91.1736
	step [101/250], loss=80.4282
	step [102/250], loss=92.2330
	step [103/250], loss=82.6898
	step [104/250], loss=87.3694
	step [105/250], loss=78.2025
	step [106/250], loss=97.8136
	step [107/250], loss=66.4688
	step [108/250], loss=85.5324
	step [109/250], loss=90.0905
	step [110/250], loss=85.8407
	step [111/250], loss=77.7971
	step [112/250], loss=97.4061
	step [113/250], loss=72.6339
	step [114/250], loss=76.9695
	step [115/250], loss=76.2422
	step [116/250], loss=89.3529
	step [117/250], loss=82.3485
	step [118/250], loss=93.0795
	step [119/250], loss=78.5047
	step [120/250], loss=85.2537
	step [121/250], loss=88.9066
	step [122/250], loss=78.5900
	step [123/250], loss=92.0117
	step [124/250], loss=78.1398
	step [125/250], loss=84.3638
	step [126/250], loss=84.8220
	step [127/250], loss=68.5997
	step [128/250], loss=92.7394
	step [129/250], loss=99.5036
	step [130/250], loss=83.0561
	step [131/250], loss=85.4684
	step [132/250], loss=80.4159
	step [133/250], loss=86.4767
	step [134/250], loss=78.2357
	step [135/250], loss=80.6644
	step [136/250], loss=77.8477
	step [137/250], loss=83.9929
	step [138/250], loss=76.5363
	step [139/250], loss=80.5977
	step [140/250], loss=88.5666
	step [141/250], loss=75.8215
	step [142/250], loss=81.3747
	step [143/250], loss=75.7645
	step [144/250], loss=89.2163
	step [145/250], loss=86.3981
	step [146/250], loss=69.1977
	step [147/250], loss=84.4174
	step [148/250], loss=92.9028
	step [149/250], loss=66.0129
	step [150/250], loss=77.7921
	step [151/250], loss=70.0819
	step [152/250], loss=77.6580
	step [153/250], loss=84.5320
	step [154/250], loss=101.3172
	step [155/250], loss=83.8459
	step [156/250], loss=75.1811
	step [157/250], loss=77.3998
	step [158/250], loss=91.8692
	step [159/250], loss=71.3106
	step [160/250], loss=62.0238
	step [161/250], loss=82.3125
	step [162/250], loss=77.8977
	step [163/250], loss=72.8567
	step [164/250], loss=79.0246
	step [165/250], loss=74.1972
	step [166/250], loss=90.0261
	step [167/250], loss=72.7102
	step [168/250], loss=89.5530
	step [169/250], loss=97.4837
	step [170/250], loss=72.9147
	step [171/250], loss=77.1767
	step [172/250], loss=75.9438
	step [173/250], loss=83.6392
	step [174/250], loss=77.8761
	step [175/250], loss=78.7684
	step [176/250], loss=102.5612
	step [177/250], loss=90.3664
	step [178/250], loss=81.3508
	step [179/250], loss=67.3032
	step [180/250], loss=69.5464
	step [181/250], loss=92.8678
	step [182/250], loss=89.5862
	step [183/250], loss=96.0401
	step [184/250], loss=71.7645
	step [185/250], loss=99.6105
	step [186/250], loss=87.9264
	step [187/250], loss=79.6941
	step [188/250], loss=91.7869
	step [189/250], loss=79.3376
	step [190/250], loss=71.4119
	step [191/250], loss=89.0956
	step [192/250], loss=66.5518
	step [193/250], loss=105.5982
	step [194/250], loss=88.9912
	step [195/250], loss=92.3084
	step [196/250], loss=112.4445
	step [197/250], loss=67.5319
	step [198/250], loss=105.3323
	step [199/250], loss=73.0334
	step [200/250], loss=90.3972
	step [201/250], loss=90.9138
	step [202/250], loss=89.5460
	step [203/250], loss=91.6950
	step [204/250], loss=81.7118
	step [205/250], loss=93.4091
	step [206/250], loss=93.6670
	step [207/250], loss=74.9447
	step [208/250], loss=77.9411
	step [209/250], loss=68.5103
	step [210/250], loss=90.5180
	step [211/250], loss=67.6990
	step [212/250], loss=72.9542
	step [213/250], loss=76.4329
	step [214/250], loss=97.5841
	step [215/250], loss=81.1241
	step [216/250], loss=84.5561
	step [217/250], loss=76.6701
	step [218/250], loss=71.5221
	step [219/250], loss=91.2791
	step [220/250], loss=101.5294
	step [221/250], loss=78.8891
	step [222/250], loss=87.0439
	step [223/250], loss=82.8068
	step [224/250], loss=80.4556
	step [225/250], loss=60.5255
	step [226/250], loss=86.4329
	step [227/250], loss=81.0030
	step [228/250], loss=81.2102
	step [229/250], loss=80.6870
	step [230/250], loss=82.7780
	step [231/250], loss=58.0286
	step [232/250], loss=69.9092
	step [233/250], loss=66.2574
	step [234/250], loss=85.4662
	step [235/250], loss=84.4466
	step [236/250], loss=72.1932
	step [237/250], loss=90.4932
	step [238/250], loss=72.1586
	step [239/250], loss=87.5469
	step [240/250], loss=65.1618
	step [241/250], loss=73.2890
	step [242/250], loss=95.3448
	step [243/250], loss=76.9320
	step [244/250], loss=72.8512
	step [245/250], loss=92.9041
	step [246/250], loss=90.5109
	step [247/250], loss=88.6889
	step [248/250], loss=94.0542
	step [249/250], loss=60.8106
	step [250/250], loss=24.4497
	Evaluating
	loss=0.0087, precision=0.2758, recall=0.8726, f1=0.4192
Training epoch 63
	step [1/250], loss=88.4285
	step [2/250], loss=79.2769
	step [3/250], loss=74.3002
	step [4/250], loss=75.8404
	step [5/250], loss=93.6472
	step [6/250], loss=83.5208
	step [7/250], loss=85.6253
	step [8/250], loss=90.4555
	step [9/250], loss=78.4294
	step [10/250], loss=75.7767
	step [11/250], loss=83.6743
	step [12/250], loss=64.0069
	step [13/250], loss=79.4785
	step [14/250], loss=86.9828
	step [15/250], loss=95.8907
	step [16/250], loss=88.8284
	step [17/250], loss=88.2417
	step [18/250], loss=77.2486
	step [19/250], loss=83.4618
	step [20/250], loss=71.8731
	step [21/250], loss=73.4568
	step [22/250], loss=90.0039
	step [23/250], loss=86.7677
	step [24/250], loss=100.3518
	step [25/250], loss=86.7465
	step [26/250], loss=67.7889
	step [27/250], loss=74.0365
	step [28/250], loss=72.2743
	step [29/250], loss=73.9865
	step [30/250], loss=77.3729
	step [31/250], loss=70.4188
	step [32/250], loss=87.2711
	step [33/250], loss=113.3407
	step [34/250], loss=81.3125
	step [35/250], loss=84.0370
	step [36/250], loss=75.5257
	step [37/250], loss=93.1275
	step [38/250], loss=70.8301
	step [39/250], loss=70.3422
	step [40/250], loss=75.0650
	step [41/250], loss=100.8156
	step [42/250], loss=80.9855
	step [43/250], loss=88.2758
	step [44/250], loss=86.8422
	step [45/250], loss=73.9090
	step [46/250], loss=80.2747
	step [47/250], loss=84.7198
	step [48/250], loss=71.6327
	step [49/250], loss=88.8005
	step [50/250], loss=83.3245
	step [51/250], loss=83.1048
	step [52/250], loss=93.3775
	step [53/250], loss=83.2219
	step [54/250], loss=85.6571
	step [55/250], loss=70.3847
	step [56/250], loss=83.4017
	step [57/250], loss=74.7409
	step [58/250], loss=65.1293
	step [59/250], loss=73.5429
	step [60/250], loss=78.3564
	step [61/250], loss=86.1799
	step [62/250], loss=75.1880
	step [63/250], loss=78.2584
	step [64/250], loss=97.4616
	step [65/250], loss=72.1192
	step [66/250], loss=86.8494
	step [67/250], loss=73.3819
	step [68/250], loss=84.2364
	step [69/250], loss=89.4933
	step [70/250], loss=79.7195
	step [71/250], loss=87.3073
	step [72/250], loss=84.1497
	step [73/250], loss=81.1036
	step [74/250], loss=82.7646
	step [75/250], loss=69.3283
	step [76/250], loss=98.9275
	step [77/250], loss=71.3006
	step [78/250], loss=82.3971
	step [79/250], loss=78.7671
	step [80/250], loss=83.0322
	step [81/250], loss=86.9607
	step [82/250], loss=83.1070
	step [83/250], loss=87.5767
	step [84/250], loss=88.5237
	step [85/250], loss=96.4102
	step [86/250], loss=76.9414
	step [87/250], loss=104.1177
	step [88/250], loss=71.5774
	step [89/250], loss=87.1589
	step [90/250], loss=78.9982
	step [91/250], loss=93.2618
	step [92/250], loss=73.3529
	step [93/250], loss=68.4862
	step [94/250], loss=74.6103
	step [95/250], loss=81.8795
	step [96/250], loss=81.8751
	step [97/250], loss=82.2959
	step [98/250], loss=81.6769
	step [99/250], loss=85.6608
	step [100/250], loss=80.0359
	step [101/250], loss=76.5870
	step [102/250], loss=80.6566
	step [103/250], loss=85.1788
	step [104/250], loss=84.2654
	step [105/250], loss=69.2425
	step [106/250], loss=87.9012
	step [107/250], loss=64.3855
	step [108/250], loss=89.7256
	step [109/250], loss=87.6753
	step [110/250], loss=85.7148
	step [111/250], loss=92.8938
	step [112/250], loss=80.2763
	step [113/250], loss=81.8332
	step [114/250], loss=81.7558
	step [115/250], loss=88.9459
	step [116/250], loss=77.3887
	step [117/250], loss=81.4146
	step [118/250], loss=70.7631
	step [119/250], loss=89.5578
	step [120/250], loss=91.7758
	step [121/250], loss=66.3541
	step [122/250], loss=91.7118
	step [123/250], loss=78.1875
	step [124/250], loss=70.7397
	step [125/250], loss=85.1792
	step [126/250], loss=71.3919
	step [127/250], loss=67.7534
	step [128/250], loss=80.4006
	step [129/250], loss=112.5204
	step [130/250], loss=78.8202
	step [131/250], loss=85.4207
	step [132/250], loss=68.5013
	step [133/250], loss=107.6524
	step [134/250], loss=77.2839
	step [135/250], loss=93.0779
	step [136/250], loss=69.9849
	step [137/250], loss=72.4887
	step [138/250], loss=71.9603
	step [139/250], loss=99.7898
	step [140/250], loss=90.8981
	step [141/250], loss=72.5231
	step [142/250], loss=94.1232
	step [143/250], loss=82.4538
	step [144/250], loss=72.5566
	step [145/250], loss=89.1061
	step [146/250], loss=73.2511
	step [147/250], loss=88.8464
	step [148/250], loss=78.0927
	step [149/250], loss=81.9153
	step [150/250], loss=88.7038
	step [151/250], loss=89.0455
	step [152/250], loss=79.2054
	step [153/250], loss=97.1842
	step [154/250], loss=77.6295
	step [155/250], loss=89.4800
	step [156/250], loss=94.1897
	step [157/250], loss=72.2577
	step [158/250], loss=92.0294
	step [159/250], loss=98.8508
	step [160/250], loss=75.4778
	step [161/250], loss=79.1281
	step [162/250], loss=66.1497
	step [163/250], loss=66.6311
	step [164/250], loss=83.6310
	step [165/250], loss=70.0586
	step [166/250], loss=75.4945
	step [167/250], loss=88.7396
	step [168/250], loss=81.1381
	step [169/250], loss=94.9659
	step [170/250], loss=93.4924
	step [171/250], loss=86.5279
	step [172/250], loss=72.8056
	step [173/250], loss=74.9416
	step [174/250], loss=84.0921
	step [175/250], loss=69.3359
	step [176/250], loss=79.3897
	step [177/250], loss=95.3530
	step [178/250], loss=94.3281
	step [179/250], loss=105.9958
	step [180/250], loss=95.6133
	step [181/250], loss=73.2239
	step [182/250], loss=81.8953
	step [183/250], loss=79.3347
	step [184/250], loss=94.9048
	step [185/250], loss=80.3245
	step [186/250], loss=85.1083
	step [187/250], loss=66.8579
	step [188/250], loss=84.4453
	step [189/250], loss=81.4967
	step [190/250], loss=97.3744
	step [191/250], loss=72.5281
	step [192/250], loss=81.3103
	step [193/250], loss=72.2815
	step [194/250], loss=82.9689
	step [195/250], loss=84.9692
	step [196/250], loss=82.0277
	step [197/250], loss=82.1401
	step [198/250], loss=88.4177
	step [199/250], loss=87.5815
	step [200/250], loss=71.9203
	step [201/250], loss=88.0243
	step [202/250], loss=77.5291
	step [203/250], loss=78.8752
	step [204/250], loss=87.6340
	step [205/250], loss=75.3941
	step [206/250], loss=87.7287
	step [207/250], loss=90.0607
	step [208/250], loss=71.0867
	step [209/250], loss=90.2564
	step [210/250], loss=82.2368
	step [211/250], loss=75.7724
	step [212/250], loss=93.3200
	step [213/250], loss=91.6425
	step [214/250], loss=94.1571
	step [215/250], loss=72.6919
	step [216/250], loss=75.7141
	step [217/250], loss=96.8233
	step [218/250], loss=78.4124
	step [219/250], loss=73.7432
	step [220/250], loss=87.5129
	step [221/250], loss=89.7798
	step [222/250], loss=87.1429
	step [223/250], loss=74.3004
	step [224/250], loss=94.1050
	step [225/250], loss=79.2814
	step [226/250], loss=79.5766
	step [227/250], loss=77.2869
	step [228/250], loss=82.9369
	step [229/250], loss=79.1681
	step [230/250], loss=87.5980
	step [231/250], loss=72.6527
	step [232/250], loss=91.9709
	step [233/250], loss=98.6755
	step [234/250], loss=88.8277
	step [235/250], loss=67.2139
	step [236/250], loss=73.5341
	step [237/250], loss=84.3018
	step [238/250], loss=79.1110
	step [239/250], loss=89.3856
	step [240/250], loss=69.7815
	step [241/250], loss=83.5335
	step [242/250], loss=60.5669
	step [243/250], loss=89.7095
	step [244/250], loss=80.2044
	step [245/250], loss=89.9997
	step [246/250], loss=86.3528
	step [247/250], loss=84.4735
	step [248/250], loss=86.0709
	step [249/250], loss=76.3522
	step [250/250], loss=11.4388
	Evaluating
	loss=0.0108, precision=0.2074, recall=0.8781, f1=0.3355
Training epoch 64
	step [1/250], loss=81.0368
	step [2/250], loss=81.7600
	step [3/250], loss=76.8855
	step [4/250], loss=92.2739
	step [5/250], loss=71.6643
	step [6/250], loss=78.1164
	step [7/250], loss=86.9311
	step [8/250], loss=82.6919
	step [9/250], loss=74.8462
	step [10/250], loss=57.6130
	step [11/250], loss=77.7387
	step [12/250], loss=93.1881
	step [13/250], loss=95.9363
	step [14/250], loss=72.0469
	step [15/250], loss=67.8959
	step [16/250], loss=91.4276
	step [17/250], loss=79.5388
	step [18/250], loss=71.3170
	step [19/250], loss=80.9533
	step [20/250], loss=78.8554
	step [21/250], loss=87.8508
	step [22/250], loss=73.2348
	step [23/250], loss=97.8441
	step [24/250], loss=81.3997
	step [25/250], loss=74.4776
	step [26/250], loss=78.7894
	step [27/250], loss=78.1594
	step [28/250], loss=79.4828
	step [29/250], loss=91.5097
	step [30/250], loss=84.7177
	step [31/250], loss=90.9300
	step [32/250], loss=85.7467
	step [33/250], loss=82.8430
	step [34/250], loss=79.6553
	step [35/250], loss=78.7372
	step [36/250], loss=79.7888
	step [37/250], loss=85.5195
	step [38/250], loss=74.5792
	step [39/250], loss=89.6218
	step [40/250], loss=83.8608
	step [41/250], loss=90.8058
	step [42/250], loss=68.3730
	step [43/250], loss=63.4595
	step [44/250], loss=95.4327
	step [45/250], loss=85.7244
	step [46/250], loss=78.9595
	step [47/250], loss=80.6177
	step [48/250], loss=82.4712
	step [49/250], loss=70.2600
	step [50/250], loss=102.6751
	step [51/250], loss=73.8637
	step [52/250], loss=79.8133
	step [53/250], loss=66.6345
	step [54/250], loss=74.9879
	step [55/250], loss=86.6633
	step [56/250], loss=77.2665
	step [57/250], loss=72.2178
	step [58/250], loss=79.3975
	step [59/250], loss=81.6633
	step [60/250], loss=90.0718
	step [61/250], loss=97.3548
	step [62/250], loss=71.1096
	step [63/250], loss=80.2016
	step [64/250], loss=73.8952
	step [65/250], loss=79.5044
	step [66/250], loss=78.7537
	step [67/250], loss=77.4519
	step [68/250], loss=91.7163
	step [69/250], loss=73.1296
	step [70/250], loss=86.5932
	step [71/250], loss=95.4710
	step [72/250], loss=78.4099
	step [73/250], loss=104.1636
	step [74/250], loss=76.1250
	step [75/250], loss=83.6807
	step [76/250], loss=76.2995
	step [77/250], loss=89.8516
	step [78/250], loss=61.7775
	step [79/250], loss=94.7966
	step [80/250], loss=95.2232
	step [81/250], loss=76.2897
	step [82/250], loss=86.9419
	step [83/250], loss=88.9576
	step [84/250], loss=77.3435
	step [85/250], loss=76.6068
	step [86/250], loss=77.9057
	step [87/250], loss=78.4605
	step [88/250], loss=80.2706
	step [89/250], loss=79.5739
	step [90/250], loss=75.8744
	step [91/250], loss=71.0490
	step [92/250], loss=87.3642
	step [93/250], loss=77.8164
	step [94/250], loss=57.0330
	step [95/250], loss=75.0154
	step [96/250], loss=71.2148
	step [97/250], loss=77.6174
	step [98/250], loss=77.8790
	step [99/250], loss=68.3438
	step [100/250], loss=73.1677
	step [101/250], loss=72.6097
	step [102/250], loss=88.2004
	step [103/250], loss=87.7111
	step [104/250], loss=80.2828
	step [105/250], loss=86.0995
	step [106/250], loss=75.9876
	step [107/250], loss=90.4567
	step [108/250], loss=81.4479
	step [109/250], loss=70.2815
	step [110/250], loss=72.3698
	step [111/250], loss=94.0065
	step [112/250], loss=87.5077
	step [113/250], loss=96.0681
	step [114/250], loss=77.4118
	step [115/250], loss=85.7359
	step [116/250], loss=71.1869
	step [117/250], loss=93.2628
	step [118/250], loss=89.6008
	step [119/250], loss=61.7613
	step [120/250], loss=85.3183
	step [121/250], loss=74.3216
	step [122/250], loss=85.1195
	step [123/250], loss=66.5783
	step [124/250], loss=92.3152
	step [125/250], loss=80.7899
	step [126/250], loss=94.2433
	step [127/250], loss=89.9326
	step [128/250], loss=79.3480
	step [129/250], loss=91.4332
	step [130/250], loss=109.4393
	step [131/250], loss=77.4391
	step [132/250], loss=65.5219
	step [133/250], loss=77.7742
	step [134/250], loss=82.6983
	step [135/250], loss=75.8063
	step [136/250], loss=104.6309
	step [137/250], loss=88.7768
	step [138/250], loss=93.1665
	step [139/250], loss=68.2437
	step [140/250], loss=70.7563
	step [141/250], loss=76.9250
	step [142/250], loss=92.9044
	step [143/250], loss=115.9283
	step [144/250], loss=72.4470
	step [145/250], loss=82.1385
	step [146/250], loss=82.3848
	step [147/250], loss=82.6401
	step [148/250], loss=96.6489
	step [149/250], loss=77.9457
	step [150/250], loss=94.0902
	step [151/250], loss=73.1725
	step [152/250], loss=95.0860
	step [153/250], loss=93.0344
	step [154/250], loss=68.8558
	step [155/250], loss=82.9110
	step [156/250], loss=99.3712
	step [157/250], loss=87.4005
	step [158/250], loss=82.0073
	step [159/250], loss=92.8903
	step [160/250], loss=79.1184
	step [161/250], loss=80.6439
	step [162/250], loss=76.5631
	step [163/250], loss=68.5484
	step [164/250], loss=85.9568
	step [165/250], loss=79.5030
	step [166/250], loss=76.6571
	step [167/250], loss=96.7768
	step [168/250], loss=69.7508
	step [169/250], loss=107.2083
	step [170/250], loss=85.7945
	step [171/250], loss=77.5121
	step [172/250], loss=79.9758
	step [173/250], loss=105.5244
	step [174/250], loss=89.9198
	step [175/250], loss=81.8198
	step [176/250], loss=78.1486
	step [177/250], loss=82.3339
	step [178/250], loss=75.6240
	step [179/250], loss=82.2320
	step [180/250], loss=86.8506
	step [181/250], loss=82.1798
	step [182/250], loss=105.2155
	step [183/250], loss=76.9107
	step [184/250], loss=73.3169
	step [185/250], loss=96.7752
	step [186/250], loss=79.7841
	step [187/250], loss=77.6766
	step [188/250], loss=84.5699
	step [189/250], loss=87.4248
	step [190/250], loss=72.5050
	step [191/250], loss=80.3583
	step [192/250], loss=74.1130
	step [193/250], loss=87.3184
	step [194/250], loss=85.9398
	step [195/250], loss=96.7007
	step [196/250], loss=82.1396
	step [197/250], loss=89.3756
	step [198/250], loss=100.6784
	step [199/250], loss=83.1165
	step [200/250], loss=94.6323
	step [201/250], loss=80.1080
	step [202/250], loss=79.9788
	step [203/250], loss=72.4021
	step [204/250], loss=81.1265
	step [205/250], loss=94.0567
	step [206/250], loss=72.7041
	step [207/250], loss=76.6095
	step [208/250], loss=79.7094
	step [209/250], loss=88.7172
	step [210/250], loss=82.5161
	step [211/250], loss=73.4914
	step [212/250], loss=81.4504
	step [213/250], loss=100.0037
	step [214/250], loss=93.2086
	step [215/250], loss=92.0592
	step [216/250], loss=82.7551
	step [217/250], loss=70.5281
	step [218/250], loss=90.2615
	step [219/250], loss=71.6960
	step [220/250], loss=71.3875
	step [221/250], loss=75.8347
	step [222/250], loss=82.4073
	step [223/250], loss=78.6354
	step [224/250], loss=77.8326
	step [225/250], loss=85.0822
	step [226/250], loss=69.7636
	step [227/250], loss=72.3718
	step [228/250], loss=90.0681
	step [229/250], loss=82.3378
	step [230/250], loss=76.4288
	step [231/250], loss=82.9082
	step [232/250], loss=78.1987
	step [233/250], loss=70.7928
	step [234/250], loss=91.2574
	step [235/250], loss=87.7631
	step [236/250], loss=74.8438
	step [237/250], loss=77.6009
	step [238/250], loss=80.8722
	step [239/250], loss=73.5988
	step [240/250], loss=82.0610
	step [241/250], loss=69.2938
	step [242/250], loss=81.4837
	step [243/250], loss=74.1132
	step [244/250], loss=81.4838
	step [245/250], loss=66.8472
	step [246/250], loss=88.3576
	step [247/250], loss=75.6540
	step [248/250], loss=93.1368
	step [249/250], loss=85.4576
	step [250/250], loss=12.9958
	Evaluating
	loss=0.0092, precision=0.2690, recall=0.8743, f1=0.4115
Training epoch 65
	step [1/250], loss=78.4589
	step [2/250], loss=80.2559
	step [3/250], loss=93.0335
	step [4/250], loss=81.1919
	step [5/250], loss=88.5348
	step [6/250], loss=81.2877
	step [7/250], loss=90.5638
	step [8/250], loss=103.8426
	step [9/250], loss=75.4130
	step [10/250], loss=89.4284
	step [11/250], loss=79.7294
	step [12/250], loss=60.6540
	step [13/250], loss=81.3002
	step [14/250], loss=76.6529
	step [15/250], loss=82.4129
	step [16/250], loss=79.6018
	step [17/250], loss=93.8110
	step [18/250], loss=84.9340
	step [19/250], loss=85.3067
	step [20/250], loss=73.7108
	step [21/250], loss=81.1788
	step [22/250], loss=79.7946
	step [23/250], loss=70.0039
	step [24/250], loss=84.2214
	step [25/250], loss=77.7339
	step [26/250], loss=80.0204
	step [27/250], loss=78.7255
	step [28/250], loss=70.4695
	step [29/250], loss=90.7528
	step [30/250], loss=60.7107
	step [31/250], loss=66.1932
	step [32/250], loss=70.1278
	step [33/250], loss=88.0363
	step [34/250], loss=78.7126
	step [35/250], loss=80.0856
	step [36/250], loss=77.4169
	step [37/250], loss=98.9640
	step [38/250], loss=77.2840
	step [39/250], loss=66.2753
	step [40/250], loss=72.6199
	step [41/250], loss=86.1905
	step [42/250], loss=79.7679
	step [43/250], loss=78.5267
	step [44/250], loss=79.5622
	step [45/250], loss=86.9178
	step [46/250], loss=88.1720
	step [47/250], loss=92.9152
	step [48/250], loss=90.6966
	step [49/250], loss=92.9978
	step [50/250], loss=88.1652
	step [51/250], loss=95.5857
	step [52/250], loss=82.8729
	step [53/250], loss=73.9132
	step [54/250], loss=104.5665
	step [55/250], loss=86.0075
	step [56/250], loss=72.8673
	step [57/250], loss=64.8886
	step [58/250], loss=92.8788
	step [59/250], loss=65.2145
	step [60/250], loss=89.4192
	step [61/250], loss=81.4237
	step [62/250], loss=69.8110
	step [63/250], loss=75.8848
	step [64/250], loss=88.5306
	step [65/250], loss=80.5120
	step [66/250], loss=90.0665
	step [67/250], loss=83.2857
	step [68/250], loss=83.3074
	step [69/250], loss=79.6123
	step [70/250], loss=86.3574
	step [71/250], loss=83.2628
	step [72/250], loss=77.9707
	step [73/250], loss=75.4513
	step [74/250], loss=86.9076
	step [75/250], loss=71.8524
	step [76/250], loss=71.3169
	step [77/250], loss=62.4720
	step [78/250], loss=72.5593
	step [79/250], loss=75.8070
	step [80/250], loss=92.2905
	step [81/250], loss=87.1734
	step [82/250], loss=72.1379
	step [83/250], loss=69.5185
	step [84/250], loss=82.1381
	step [85/250], loss=81.3936
	step [86/250], loss=69.4339
	step [87/250], loss=74.5527
	step [88/250], loss=87.0538
	step [89/250], loss=81.7712
	step [90/250], loss=76.2020
	step [91/250], loss=87.1933
	step [92/250], loss=83.9138
	step [93/250], loss=94.8006
	step [94/250], loss=94.1754
	step [95/250], loss=74.6023
	step [96/250], loss=99.6367
	step [97/250], loss=92.3622
	step [98/250], loss=72.3425
	step [99/250], loss=99.1595
	step [100/250], loss=91.6975
	step [101/250], loss=76.6002
	step [102/250], loss=99.4171
	step [103/250], loss=80.7560
	step [104/250], loss=68.0117
	step [105/250], loss=77.3366
	step [106/250], loss=71.7268
	step [107/250], loss=81.9883
	step [108/250], loss=80.3584
	step [109/250], loss=85.1775
	step [110/250], loss=61.4906
	step [111/250], loss=84.6382
	step [112/250], loss=97.7776
	step [113/250], loss=86.7584
	step [114/250], loss=78.0985
	step [115/250], loss=79.1832
	step [116/250], loss=82.5841
	step [117/250], loss=83.8782
	step [118/250], loss=78.5780
	step [119/250], loss=79.0206
	step [120/250], loss=72.5203
	step [121/250], loss=79.0376
	step [122/250], loss=92.3413
	step [123/250], loss=84.9117
	step [124/250], loss=78.3419
	step [125/250], loss=77.4535
	step [126/250], loss=87.2537
	step [127/250], loss=90.6198
	step [128/250], loss=67.0573
	step [129/250], loss=77.2711
	step [130/250], loss=82.5553
	step [131/250], loss=65.1710
	step [132/250], loss=77.8120
	step [133/250], loss=111.0380
	step [134/250], loss=83.7201
	step [135/250], loss=91.1446
	step [136/250], loss=91.9370
	step [137/250], loss=107.8280
	step [138/250], loss=76.1099
	step [139/250], loss=96.9948
	step [140/250], loss=89.4174
	step [141/250], loss=71.5274
	step [142/250], loss=83.9163
	step [143/250], loss=94.3783
	step [144/250], loss=62.5633
	step [145/250], loss=99.8284
	step [146/250], loss=78.4024
	step [147/250], loss=66.8932
	step [148/250], loss=83.6539
	step [149/250], loss=96.1202
	step [150/250], loss=70.7438
	step [151/250], loss=76.0030
	step [152/250], loss=70.6668
	step [153/250], loss=91.5835
	step [154/250], loss=78.2074
	step [155/250], loss=75.3676
	step [156/250], loss=91.6512
	step [157/250], loss=94.6638
	step [158/250], loss=75.9337
	step [159/250], loss=83.6024
	step [160/250], loss=96.0117
	step [161/250], loss=73.9496
	step [162/250], loss=83.9129
	step [163/250], loss=95.7776
	step [164/250], loss=80.2156
	step [165/250], loss=84.3716
	step [166/250], loss=83.9018
	step [167/250], loss=77.2559
	step [168/250], loss=76.9216
	step [169/250], loss=81.9464
	step [170/250], loss=79.4557
	step [171/250], loss=77.5368
	step [172/250], loss=78.4502
	step [173/250], loss=72.2800
	step [174/250], loss=82.1305
	step [175/250], loss=67.0604
	step [176/250], loss=85.2106
	step [177/250], loss=83.3346
	step [178/250], loss=84.2486
	step [179/250], loss=83.1026
	step [180/250], loss=75.1054
	step [181/250], loss=81.0580
	step [182/250], loss=82.7219
	step [183/250], loss=76.7050
	step [184/250], loss=82.4590
	step [185/250], loss=66.5403
	step [186/250], loss=76.1796
	step [187/250], loss=88.1350
	step [188/250], loss=73.0640
	step [189/250], loss=97.4248
	step [190/250], loss=80.0761
	step [191/250], loss=74.5931
	step [192/250], loss=81.4547
	step [193/250], loss=78.0913
	step [194/250], loss=63.6013
	step [195/250], loss=78.6425
	step [196/250], loss=92.4593
	step [197/250], loss=87.8783
	step [198/250], loss=88.7435
	step [199/250], loss=75.6984
	step [200/250], loss=78.0040
	step [201/250], loss=67.9548
	step [202/250], loss=88.6094
	step [203/250], loss=63.5010
	step [204/250], loss=67.3434
	step [205/250], loss=76.9539
	step [206/250], loss=88.2612
	step [207/250], loss=107.8528
	step [208/250], loss=69.6470
	step [209/250], loss=92.6942
	step [210/250], loss=75.5025
	step [211/250], loss=76.8285
	step [212/250], loss=84.1799
	step [213/250], loss=88.4925
	step [214/250], loss=77.6422
	step [215/250], loss=69.3894
	step [216/250], loss=80.0808
	step [217/250], loss=81.7939
	step [218/250], loss=84.2581
	step [219/250], loss=83.3219
	step [220/250], loss=76.0031
	step [221/250], loss=84.0952
	step [222/250], loss=87.1201
	step [223/250], loss=76.3436
	step [224/250], loss=87.7323
	step [225/250], loss=86.6815
	step [226/250], loss=93.6723
	step [227/250], loss=105.7613
	step [228/250], loss=97.5696
	step [229/250], loss=74.0614
	step [230/250], loss=78.1193
	step [231/250], loss=81.3812
	step [232/250], loss=73.2560
	step [233/250], loss=73.3595
	step [234/250], loss=78.7192
	step [235/250], loss=78.9151
	step [236/250], loss=85.4918
	step [237/250], loss=90.7009
	step [238/250], loss=88.3414
	step [239/250], loss=89.8008
	step [240/250], loss=72.1268
	step [241/250], loss=81.3980
	step [242/250], loss=74.4292
	step [243/250], loss=84.0549
	step [244/250], loss=70.2501
	step [245/250], loss=82.5085
	step [246/250], loss=81.1742
	step [247/250], loss=94.1248
	step [248/250], loss=87.0431
	step [249/250], loss=95.0233
	step [250/250], loss=17.7111
	Evaluating
	loss=0.0109, precision=0.2008, recall=0.8620, f1=0.3257
Training epoch 66
	step [1/250], loss=84.5782
	step [2/250], loss=78.6877
	step [3/250], loss=90.6014
	step [4/250], loss=75.3589
	step [5/250], loss=93.3049
	step [6/250], loss=78.9192
	step [7/250], loss=103.3566
	step [8/250], loss=63.2679
	step [9/250], loss=81.2205
	step [10/250], loss=72.2164
	step [11/250], loss=77.2381
	step [12/250], loss=86.3518
	step [13/250], loss=89.1917
	step [14/250], loss=94.1913
	step [15/250], loss=100.1829
	step [16/250], loss=69.4081
	step [17/250], loss=91.0706
	step [18/250], loss=73.1215
	step [19/250], loss=71.2686
	step [20/250], loss=78.4175
	step [21/250], loss=81.2368
	step [22/250], loss=87.4369
	step [23/250], loss=84.9624
	step [24/250], loss=100.5927
	step [25/250], loss=69.5387
	step [26/250], loss=85.5920
	step [27/250], loss=98.9996
	step [28/250], loss=68.2642
	step [29/250], loss=73.1140
	step [30/250], loss=70.5484
	step [31/250], loss=88.0276
	step [32/250], loss=78.8750
	step [33/250], loss=77.0482
	step [34/250], loss=79.8860
	step [35/250], loss=83.8657
	step [36/250], loss=63.8499
	step [37/250], loss=82.1539
	step [38/250], loss=84.1079
	step [39/250], loss=64.6948
	step [40/250], loss=66.9720
	step [41/250], loss=88.9950
	step [42/250], loss=72.9214
	step [43/250], loss=87.7373
	step [44/250], loss=78.0709
	step [45/250], loss=69.9858
	step [46/250], loss=84.8114
	step [47/250], loss=80.6053
	step [48/250], loss=80.3690
	step [49/250], loss=85.7160
	step [50/250], loss=114.3849
	step [51/250], loss=82.8512
	step [52/250], loss=82.7842
	step [53/250], loss=93.7658
	step [54/250], loss=75.7713
	step [55/250], loss=92.4961
	step [56/250], loss=80.7893
	step [57/250], loss=86.4867
	step [58/250], loss=102.3735
	step [59/250], loss=99.2853
	step [60/250], loss=90.9329
	step [61/250], loss=101.0299
	step [62/250], loss=88.1474
	step [63/250], loss=92.2498
	step [64/250], loss=77.8539
	step [65/250], loss=87.8314
	step [66/250], loss=81.6646
	step [67/250], loss=72.8538
	step [68/250], loss=83.4005
	step [69/250], loss=75.1219
	step [70/250], loss=98.0335
	step [71/250], loss=83.9777
	step [72/250], loss=68.1461
	step [73/250], loss=78.8044
	step [74/250], loss=78.5114
	step [75/250], loss=83.2332
	step [76/250], loss=74.3994
	step [77/250], loss=80.8111
	step [78/250], loss=78.2209
	step [79/250], loss=77.0565
	step [80/250], loss=58.9457
	step [81/250], loss=89.2024
	step [82/250], loss=85.2319
	step [83/250], loss=82.5084
	step [84/250], loss=83.5629
	step [85/250], loss=78.0209
	step [86/250], loss=98.6638
	step [87/250], loss=76.8154
	step [88/250], loss=74.5488
	step [89/250], loss=98.5616
	step [90/250], loss=86.5537
	step [91/250], loss=78.9652
	step [92/250], loss=75.6921
	step [93/250], loss=94.5280
	step [94/250], loss=69.2623
	step [95/250], loss=86.6440
	step [96/250], loss=90.3610
	step [97/250], loss=76.6129
	step [98/250], loss=82.5701
	step [99/250], loss=89.3883
	step [100/250], loss=91.8139
	step [101/250], loss=70.8005
	step [102/250], loss=63.7265
	step [103/250], loss=64.8660
	step [104/250], loss=82.7009
	step [105/250], loss=102.0964
	step [106/250], loss=75.8716
	step [107/250], loss=71.2308
	step [108/250], loss=80.7063
	step [109/250], loss=70.1746
	step [110/250], loss=93.4071
	step [111/250], loss=82.1233
	step [112/250], loss=73.3174
	step [113/250], loss=78.7766
	step [114/250], loss=90.4510
	step [115/250], loss=79.5890
	step [116/250], loss=88.5150
	step [117/250], loss=75.2763
	step [118/250], loss=84.5411
	step [119/250], loss=76.6096
	step [120/250], loss=80.4558
	step [121/250], loss=114.4589
	step [122/250], loss=77.1282
	step [123/250], loss=94.6996
	step [124/250], loss=71.7111
	step [125/250], loss=95.8781
	step [126/250], loss=84.3632
	step [127/250], loss=73.9669
	step [128/250], loss=73.1573
	step [129/250], loss=84.1502
	step [130/250], loss=97.0538
	step [131/250], loss=78.5598
	step [132/250], loss=79.6821
	step [133/250], loss=102.0397
	step [134/250], loss=86.1101
	step [135/250], loss=74.5032
	step [136/250], loss=99.3350
	step [137/250], loss=81.5771
	step [138/250], loss=75.8386
	step [139/250], loss=91.8848
	step [140/250], loss=68.4651
	step [141/250], loss=100.8479
	step [142/250], loss=95.3354
	step [143/250], loss=83.8968
	step [144/250], loss=75.2064
	step [145/250], loss=68.5943
	step [146/250], loss=77.2094
	step [147/250], loss=72.9111
	step [148/250], loss=89.5218
	step [149/250], loss=73.9305
	step [150/250], loss=62.8711
	step [151/250], loss=65.9273
	step [152/250], loss=72.0633
	step [153/250], loss=78.4964
	step [154/250], loss=71.2505
	step [155/250], loss=73.2802
	step [156/250], loss=73.7655
	step [157/250], loss=71.9321
	step [158/250], loss=79.2092
	step [159/250], loss=76.8279
	step [160/250], loss=90.4930
	step [161/250], loss=79.1230
	step [162/250], loss=77.8902
	step [163/250], loss=79.4109
	step [164/250], loss=69.3242
	step [165/250], loss=87.7810
	step [166/250], loss=77.6392
	step [167/250], loss=74.0116
	step [168/250], loss=86.8373
	step [169/250], loss=64.5365
	step [170/250], loss=92.5627
	step [171/250], loss=91.7185
	step [172/250], loss=80.4220
	step [173/250], loss=97.5957
	step [174/250], loss=85.4867
	step [175/250], loss=90.0204
	step [176/250], loss=93.7509
	step [177/250], loss=86.5927
	step [178/250], loss=73.0679
	step [179/250], loss=86.4223
	step [180/250], loss=78.3526
	step [181/250], loss=69.4622
	step [182/250], loss=75.1391
	step [183/250], loss=96.4347
	step [184/250], loss=95.0570
	step [185/250], loss=80.4858
	step [186/250], loss=80.4338
	step [187/250], loss=85.9698
	step [188/250], loss=61.5803
	step [189/250], loss=74.0807
	step [190/250], loss=92.2206
	step [191/250], loss=87.1756
	step [192/250], loss=87.1821
	step [193/250], loss=71.3962
	step [194/250], loss=98.0141
	step [195/250], loss=79.6541
	step [196/250], loss=76.2926
	step [197/250], loss=79.1584
	step [198/250], loss=66.0508
	step [199/250], loss=81.2030
	step [200/250], loss=78.2590
	step [201/250], loss=83.3668
	step [202/250], loss=95.2971
	step [203/250], loss=93.7050
	step [204/250], loss=63.9443
	step [205/250], loss=85.2867
	step [206/250], loss=76.1556
	step [207/250], loss=70.6137
	step [208/250], loss=83.2054
	step [209/250], loss=80.9091
	step [210/250], loss=73.9641
	step [211/250], loss=90.2933
	step [212/250], loss=77.8576
	step [213/250], loss=84.4821
	step [214/250], loss=72.1633
	step [215/250], loss=80.6538
	step [216/250], loss=89.1332
	step [217/250], loss=88.3156
	step [218/250], loss=71.6922
	step [219/250], loss=79.5794
	step [220/250], loss=63.4880
	step [221/250], loss=85.4735
	step [222/250], loss=65.6864
	step [223/250], loss=83.0636
	step [224/250], loss=70.9833
	step [225/250], loss=90.4453
	step [226/250], loss=83.0367
	step [227/250], loss=65.8060
	step [228/250], loss=80.7463
	step [229/250], loss=67.3216
	step [230/250], loss=74.8414
	step [231/250], loss=84.2576
	step [232/250], loss=73.4422
	step [233/250], loss=99.3745
	step [234/250], loss=79.3088
	step [235/250], loss=93.1879
	step [236/250], loss=81.8096
	step [237/250], loss=79.3663
	step [238/250], loss=92.2572
	step [239/250], loss=96.2589
	step [240/250], loss=78.1614
	step [241/250], loss=72.2364
	step [242/250], loss=85.2317
	step [243/250], loss=65.9251
	step [244/250], loss=79.9305
	step [245/250], loss=82.3119
	step [246/250], loss=81.6461
	step [247/250], loss=80.2326
	step [248/250], loss=84.5480
	step [249/250], loss=91.5529
	step [250/250], loss=13.9857
	Evaluating
	loss=0.0089, precision=0.2737, recall=0.8713, f1=0.4166
Training epoch 67
	step [1/250], loss=82.5043
	step [2/250], loss=66.4329
	step [3/250], loss=89.2780
	step [4/250], loss=98.1330
	step [5/250], loss=78.3646
	step [6/250], loss=94.8591
	step [7/250], loss=83.6321
	step [8/250], loss=90.2439
	step [9/250], loss=83.0949
	step [10/250], loss=78.2330
	step [11/250], loss=84.9052
	step [12/250], loss=100.6167
	step [13/250], loss=76.5215
	step [14/250], loss=105.9623
	step [15/250], loss=85.5623
	step [16/250], loss=81.9357
	step [17/250], loss=79.0356
	step [18/250], loss=65.2128
	step [19/250], loss=76.0377
	step [20/250], loss=67.4859
	step [21/250], loss=82.0530
	step [22/250], loss=97.4120
	step [23/250], loss=69.9297
	step [24/250], loss=85.4499
	step [25/250], loss=68.3402
	step [26/250], loss=86.8448
	step [27/250], loss=75.3611
	step [28/250], loss=89.7886
	step [29/250], loss=66.5542
	step [30/250], loss=79.1908
	step [31/250], loss=87.1551
	step [32/250], loss=80.6838
	step [33/250], loss=82.7807
	step [34/250], loss=79.9250
	step [35/250], loss=78.9871
	step [36/250], loss=83.0899
	step [37/250], loss=106.8853
	step [38/250], loss=81.7332
	step [39/250], loss=88.9327
	step [40/250], loss=86.8419
	step [41/250], loss=86.6397
	step [42/250], loss=79.5021
	step [43/250], loss=86.3870
	step [44/250], loss=70.4974
	step [45/250], loss=99.4237
	step [46/250], loss=72.0560
	step [47/250], loss=62.8897
	step [48/250], loss=82.7735
	step [49/250], loss=85.4502
	step [50/250], loss=69.2717
	step [51/250], loss=80.2702
	step [52/250], loss=87.2900
	step [53/250], loss=65.7544
	step [54/250], loss=79.0335
	step [55/250], loss=87.2234
	step [56/250], loss=81.8350
	step [57/250], loss=91.0320
	step [58/250], loss=82.1728
	step [59/250], loss=73.1156
	step [60/250], loss=89.9501
	step [61/250], loss=83.9745
	step [62/250], loss=73.7754
	step [63/250], loss=83.5368
	step [64/250], loss=76.8683
	step [65/250], loss=78.0907
	step [66/250], loss=101.9914
	step [67/250], loss=71.4443
	step [68/250], loss=78.4795
	step [69/250], loss=67.5452
	step [70/250], loss=90.7832
	step [71/250], loss=74.2767
	step [72/250], loss=71.6187
	step [73/250], loss=78.2362
	step [74/250], loss=71.8848
	step [75/250], loss=79.9131
	step [76/250], loss=96.7670
	step [77/250], loss=84.7135
	step [78/250], loss=83.0648
	step [79/250], loss=92.4169
	step [80/250], loss=83.5405
	step [81/250], loss=98.0464
	step [82/250], loss=72.6791
	step [83/250], loss=88.3122
	step [84/250], loss=77.4489
	step [85/250], loss=70.8350
	step [86/250], loss=64.1714
	step [87/250], loss=103.2209
	step [88/250], loss=75.8827
	step [89/250], loss=82.0078
	step [90/250], loss=71.2837
	step [91/250], loss=88.4879
	step [92/250], loss=96.9031
	step [93/250], loss=87.9402
	step [94/250], loss=83.7708
	step [95/250], loss=78.9995
	step [96/250], loss=93.7213
	step [97/250], loss=88.9215
	step [98/250], loss=67.2958
	step [99/250], loss=92.7654
	step [100/250], loss=74.2308
	step [101/250], loss=76.9866
	step [102/250], loss=95.8261
	step [103/250], loss=76.4257
	step [104/250], loss=83.2562
	step [105/250], loss=67.2984
	step [106/250], loss=67.9093
	step [107/250], loss=81.9608
	step [108/250], loss=70.1902
	step [109/250], loss=70.6241
	step [110/250], loss=81.3008
	step [111/250], loss=81.6880
	step [112/250], loss=65.6992
	step [113/250], loss=75.2458
	step [114/250], loss=85.7069
	step [115/250], loss=83.0426
	step [116/250], loss=74.6616
	step [117/250], loss=74.5777
	step [118/250], loss=88.2579
	step [119/250], loss=80.1715
	step [120/250], loss=87.6082
	step [121/250], loss=102.9175
	step [122/250], loss=92.1692
	step [123/250], loss=102.9280
	step [124/250], loss=93.9834
	step [125/250], loss=63.6993
	step [126/250], loss=91.4411
	step [127/250], loss=83.1340
	step [128/250], loss=75.6692
	step [129/250], loss=83.7769
	step [130/250], loss=71.6540
	step [131/250], loss=78.7512
	step [132/250], loss=82.6687
	step [133/250], loss=94.9297
	step [134/250], loss=80.1987
	step [135/250], loss=91.2383
	step [136/250], loss=103.3269
	step [137/250], loss=74.0558
	step [138/250], loss=92.6302
	step [139/250], loss=84.8424
	step [140/250], loss=89.2893
	step [141/250], loss=85.0829
	step [142/250], loss=75.0469
	step [143/250], loss=88.9105
	step [144/250], loss=72.7926
	step [145/250], loss=86.2276
	step [146/250], loss=105.1481
	step [147/250], loss=79.7741
	step [148/250], loss=75.3241
	step [149/250], loss=74.4500
	step [150/250], loss=76.8992
	step [151/250], loss=90.3914
	step [152/250], loss=76.4999
	step [153/250], loss=76.7181
	step [154/250], loss=78.9841
	step [155/250], loss=81.9148
	step [156/250], loss=70.0226
	step [157/250], loss=81.0053
	step [158/250], loss=68.6172
	step [159/250], loss=85.5757
	step [160/250], loss=86.6464
	step [161/250], loss=73.7478
	step [162/250], loss=90.1791
	step [163/250], loss=66.3377
	step [164/250], loss=79.3173
	step [165/250], loss=77.4324
	step [166/250], loss=81.8982
	step [167/250], loss=90.5227
	step [168/250], loss=76.1926
	step [169/250], loss=93.1573
	step [170/250], loss=67.2283
	step [171/250], loss=70.1012
	step [172/250], loss=91.0497
	step [173/250], loss=74.5911
	step [174/250], loss=100.7916
	step [175/250], loss=78.9540
	step [176/250], loss=89.2286
	step [177/250], loss=80.2307
	step [178/250], loss=81.5013
	step [179/250], loss=92.4239
	step [180/250], loss=68.4206
	step [181/250], loss=92.2477
	step [182/250], loss=84.1765
	step [183/250], loss=72.4038
	step [184/250], loss=75.6334
	step [185/250], loss=72.5704
	step [186/250], loss=77.9934
	step [187/250], loss=74.5604
	step [188/250], loss=85.4869
	step [189/250], loss=74.8808
	step [190/250], loss=75.5156
	step [191/250], loss=68.4414
	step [192/250], loss=75.7786
	step [193/250], loss=72.8382
	step [194/250], loss=88.3747
	step [195/250], loss=83.1483
	step [196/250], loss=67.5467
	step [197/250], loss=75.2765
	step [198/250], loss=95.7523
	step [199/250], loss=77.9405
	step [200/250], loss=81.0886
	step [201/250], loss=84.1344
	step [202/250], loss=85.7052
	step [203/250], loss=69.7424
	step [204/250], loss=58.8864
	step [205/250], loss=85.2685
	step [206/250], loss=88.8723
	step [207/250], loss=71.6111
	step [208/250], loss=71.4365
	step [209/250], loss=81.4226
	step [210/250], loss=71.1136
	step [211/250], loss=88.9233
	step [212/250], loss=95.1171
	step [213/250], loss=72.4860
	step [214/250], loss=84.2215
	step [215/250], loss=69.8065
	step [216/250], loss=67.7611
	step [217/250], loss=89.7698
	step [218/250], loss=79.4246
	step [219/250], loss=65.9438
	step [220/250], loss=84.7131
	step [221/250], loss=89.2351
	step [222/250], loss=76.8068
	step [223/250], loss=73.3477
	step [224/250], loss=91.2548
	step [225/250], loss=82.8661
	step [226/250], loss=79.0853
	step [227/250], loss=85.3406
	step [228/250], loss=66.1911
	step [229/250], loss=67.8986
	step [230/250], loss=87.6663
	step [231/250], loss=95.5430
	step [232/250], loss=79.8898
	step [233/250], loss=70.3447
	step [234/250], loss=74.4817
	step [235/250], loss=67.5062
	step [236/250], loss=73.5161
	step [237/250], loss=70.4465
	step [238/250], loss=80.0845
	step [239/250], loss=59.3628
	step [240/250], loss=73.7743
	step [241/250], loss=82.2989
	step [242/250], loss=84.5865
	step [243/250], loss=85.6632
	step [244/250], loss=62.7915
	step [245/250], loss=78.7545
	step [246/250], loss=90.2081
	step [247/250], loss=73.4660
	step [248/250], loss=77.7292
	step [249/250], loss=86.3585
	step [250/250], loss=16.7647
	Evaluating
	loss=0.0088, precision=0.2791, recall=0.8742, f1=0.4231
Training epoch 68
	step [1/250], loss=96.7540
	step [2/250], loss=78.1943
	step [3/250], loss=75.6181
	step [4/250], loss=73.0118
	step [5/250], loss=74.2194
	step [6/250], loss=81.7422
	step [7/250], loss=74.8036
	step [8/250], loss=93.6042
	step [9/250], loss=84.1294
	step [10/250], loss=99.6846
	step [11/250], loss=78.2745
	step [12/250], loss=82.4089
	step [13/250], loss=91.9887
	step [14/250], loss=80.7728
	step [15/250], loss=80.5739
	step [16/250], loss=86.3019
	step [17/250], loss=81.4960
	step [18/250], loss=101.5515
	step [19/250], loss=71.0674
	step [20/250], loss=82.7413
	step [21/250], loss=64.7209
	step [22/250], loss=74.3237
	step [23/250], loss=88.7569
	step [24/250], loss=75.0715
	step [25/250], loss=98.2638
	step [26/250], loss=76.9995
	step [27/250], loss=80.3368
	step [28/250], loss=93.6182
	step [29/250], loss=75.9280
	step [30/250], loss=80.9798
	step [31/250], loss=70.4779
	step [32/250], loss=92.2213
	step [33/250], loss=58.3395
	step [34/250], loss=84.3197
	step [35/250], loss=73.6736
	step [36/250], loss=87.6037
	step [37/250], loss=83.1344
	step [38/250], loss=81.7634
	step [39/250], loss=102.6565
	step [40/250], loss=77.5428
	step [41/250], loss=67.6427
	step [42/250], loss=75.3171
	step [43/250], loss=84.7671
	step [44/250], loss=65.0524
	step [45/250], loss=75.2253
	step [46/250], loss=87.5530
	step [47/250], loss=71.6680
	step [48/250], loss=80.7667
	step [49/250], loss=77.8721
	step [50/250], loss=90.2765
	step [51/250], loss=77.3968
	step [52/250], loss=70.6363
	step [53/250], loss=74.3501
	step [54/250], loss=85.5851
	step [55/250], loss=73.5131
	step [56/250], loss=88.2533
	step [57/250], loss=84.2559
	step [58/250], loss=84.2506
	step [59/250], loss=78.9838
	step [60/250], loss=66.6319
	step [61/250], loss=58.8259
	step [62/250], loss=75.3445
	step [63/250], loss=81.9477
	step [64/250], loss=94.9654
	step [65/250], loss=81.5262
	step [66/250], loss=87.4059
	step [67/250], loss=71.9897
	step [68/250], loss=69.1999
	step [69/250], loss=70.4421
	step [70/250], loss=85.9445
	step [71/250], loss=73.9946
	step [72/250], loss=93.3612
	step [73/250], loss=84.9018
	step [74/250], loss=65.4553
	step [75/250], loss=83.2824
	step [76/250], loss=78.7139
	step [77/250], loss=86.0171
	step [78/250], loss=70.6355
	step [79/250], loss=77.2169
	step [80/250], loss=75.0278
	step [81/250], loss=80.6187
	step [82/250], loss=81.6673
	step [83/250], loss=86.2600
	step [84/250], loss=81.7527
	step [85/250], loss=59.4003
	step [86/250], loss=68.2049
	step [87/250], loss=75.9048
	step [88/250], loss=76.0976
	step [89/250], loss=89.9194
	step [90/250], loss=96.3494
	step [91/250], loss=69.8742
	step [92/250], loss=94.3924
	step [93/250], loss=99.0645
	step [94/250], loss=96.5593
	step [95/250], loss=60.5688
	step [96/250], loss=71.3732
	step [97/250], loss=78.4023
	step [98/250], loss=87.0469
	step [99/250], loss=87.7248
	step [100/250], loss=80.5714
	step [101/250], loss=67.0385
	step [102/250], loss=83.3360
	step [103/250], loss=83.0016
	step [104/250], loss=95.6900
	step [105/250], loss=73.6262
	step [106/250], loss=94.0811
	step [107/250], loss=74.1519
	step [108/250], loss=71.3345
	step [109/250], loss=89.0987
	step [110/250], loss=76.9956
	step [111/250], loss=81.5446
	step [112/250], loss=73.2143
	step [113/250], loss=84.7175
	step [114/250], loss=67.6588
	step [115/250], loss=85.4476
	step [116/250], loss=79.0170
	step [117/250], loss=89.9982
	step [118/250], loss=75.1974
	step [119/250], loss=70.2027
	step [120/250], loss=89.9220
	step [121/250], loss=71.7332
	step [122/250], loss=65.4819
	step [123/250], loss=76.8220
	step [124/250], loss=91.9137
	step [125/250], loss=91.4673
	step [126/250], loss=77.3534
	step [127/250], loss=74.9937
	step [128/250], loss=85.7997
	step [129/250], loss=97.2432
	step [130/250], loss=90.5148
	step [131/250], loss=57.6095
	step [132/250], loss=83.8600
	step [133/250], loss=77.4178
	step [134/250], loss=84.2384
	step [135/250], loss=87.9535
	step [136/250], loss=76.8210
	step [137/250], loss=76.0049
	step [138/250], loss=85.4545
	step [139/250], loss=81.2685
	step [140/250], loss=73.5698
	step [141/250], loss=86.4474
	step [142/250], loss=73.7007
	step [143/250], loss=79.4320
	step [144/250], loss=80.3233
	step [145/250], loss=71.9554
	step [146/250], loss=78.8934
	step [147/250], loss=103.9833
	step [148/250], loss=74.1001
	step [149/250], loss=81.2310
	step [150/250], loss=73.4967
	step [151/250], loss=76.5560
	step [152/250], loss=65.8309
	step [153/250], loss=78.3803
	step [154/250], loss=74.0439
	step [155/250], loss=73.6350
	step [156/250], loss=81.6994
	step [157/250], loss=80.0898
	step [158/250], loss=79.4711
	step [159/250], loss=81.8450
	step [160/250], loss=84.4011
	step [161/250], loss=62.7917
	step [162/250], loss=63.8269
	step [163/250], loss=86.7742
	step [164/250], loss=82.2546
	step [165/250], loss=84.7832
	step [166/250], loss=77.9528
	step [167/250], loss=77.2979
	step [168/250], loss=82.4204
	step [169/250], loss=82.1705
	step [170/250], loss=74.1122
	step [171/250], loss=77.6351
	step [172/250], loss=87.4142
	step [173/250], loss=76.2109
	step [174/250], loss=84.6045
	step [175/250], loss=85.1844
	step [176/250], loss=93.2083
	step [177/250], loss=96.4792
	step [178/250], loss=91.7762
	step [179/250], loss=58.8117
	step [180/250], loss=79.2153
	step [181/250], loss=82.5233
	step [182/250], loss=89.1310
	step [183/250], loss=77.1667
	step [184/250], loss=87.2566
	step [185/250], loss=86.8198
	step [186/250], loss=67.5130
	step [187/250], loss=93.4198
	step [188/250], loss=75.1073
	step [189/250], loss=66.5017
	step [190/250], loss=88.8515
	step [191/250], loss=70.1523
	step [192/250], loss=83.0368
	step [193/250], loss=91.2537
	step [194/250], loss=109.8364
	step [195/250], loss=94.4388
	step [196/250], loss=66.5379
	step [197/250], loss=80.2372
	step [198/250], loss=72.8747
	step [199/250], loss=90.6788
	step [200/250], loss=85.9831
	step [201/250], loss=81.8523
	step [202/250], loss=96.2659
	step [203/250], loss=70.6921
	step [204/250], loss=86.0012
	step [205/250], loss=68.1801
	step [206/250], loss=71.4532
	step [207/250], loss=90.2749
	step [208/250], loss=89.0673
	step [209/250], loss=66.1062
	step [210/250], loss=77.7704
	step [211/250], loss=79.4624
	step [212/250], loss=77.7388
	step [213/250], loss=76.4060
	step [214/250], loss=108.2017
	step [215/250], loss=94.9720
	step [216/250], loss=76.5752
	step [217/250], loss=67.7178
	step [218/250], loss=92.2282
	step [219/250], loss=82.8869
	step [220/250], loss=74.5596
	step [221/250], loss=85.1487
	step [222/250], loss=67.2155
	step [223/250], loss=105.2972
	step [224/250], loss=67.1290
	step [225/250], loss=77.7033
	step [226/250], loss=89.7439
	step [227/250], loss=75.0370
	step [228/250], loss=72.4643
	step [229/250], loss=74.6588
	step [230/250], loss=87.0821
	step [231/250], loss=90.3979
	step [232/250], loss=77.7236
	step [233/250], loss=80.1749
	step [234/250], loss=98.8431
	step [235/250], loss=79.6076
	step [236/250], loss=80.5760
	step [237/250], loss=83.5367
	step [238/250], loss=71.5767
	step [239/250], loss=77.0916
	step [240/250], loss=87.4735
	step [241/250], loss=71.7107
	step [242/250], loss=92.4416
	step [243/250], loss=89.1341
	step [244/250], loss=78.3702
	step [245/250], loss=66.2804
	step [246/250], loss=73.3708
	step [247/250], loss=80.4610
	step [248/250], loss=103.2707
	step [249/250], loss=73.2719
	step [250/250], loss=13.0776
	Evaluating
	loss=0.0082, precision=0.2934, recall=0.8613, f1=0.4377
saving model as: 0_saved_model.pth
Training epoch 69
	step [1/250], loss=86.8234
	step [2/250], loss=93.7659
	step [3/250], loss=70.8412
	step [4/250], loss=93.2191
	step [5/250], loss=91.3297
	step [6/250], loss=75.8072
	step [7/250], loss=85.1514
	step [8/250], loss=81.2626
	step [9/250], loss=81.1634
	step [10/250], loss=85.7739
	step [11/250], loss=80.6407
	step [12/250], loss=87.2315
	step [13/250], loss=85.5424
	step [14/250], loss=82.1943
	step [15/250], loss=84.5188
	step [16/250], loss=86.3335
	step [17/250], loss=97.9830
	step [18/250], loss=71.1917
	step [19/250], loss=73.3358
	step [20/250], loss=87.7055
	step [21/250], loss=81.6871
	step [22/250], loss=76.8957
	step [23/250], loss=69.1311
	step [24/250], loss=77.5005
	step [25/250], loss=71.5004
	step [26/250], loss=78.4140
	step [27/250], loss=88.5908
	step [28/250], loss=69.7331
	step [29/250], loss=74.7511
	step [30/250], loss=88.5274
	step [31/250], loss=81.3946
	step [32/250], loss=90.7135
	step [33/250], loss=83.9367
	step [34/250], loss=76.0529
	step [35/250], loss=73.9664
	step [36/250], loss=88.1345
	step [37/250], loss=93.9191
	step [38/250], loss=71.2671
	step [39/250], loss=67.1235
	step [40/250], loss=77.1556
	step [41/250], loss=79.4171
	step [42/250], loss=68.1831
	step [43/250], loss=76.2496
	step [44/250], loss=79.6540
	step [45/250], loss=78.7103
	step [46/250], loss=88.4200
	step [47/250], loss=70.7483
	step [48/250], loss=93.0269
	step [49/250], loss=83.1414
	step [50/250], loss=76.7526
	step [51/250], loss=78.2284
	step [52/250], loss=86.5752
	step [53/250], loss=67.4717
	step [54/250], loss=76.8860
	step [55/250], loss=94.6427
	step [56/250], loss=78.0780
	step [57/250], loss=89.9283
	step [58/250], loss=108.8577
	step [59/250], loss=81.4501
	step [60/250], loss=89.3011
	step [61/250], loss=79.6513
	step [62/250], loss=61.4124
	step [63/250], loss=82.4809
	step [64/250], loss=83.9850
	step [65/250], loss=70.7047
	step [66/250], loss=79.4243
	step [67/250], loss=72.6752
	step [68/250], loss=76.5283
	step [69/250], loss=98.3243
	step [70/250], loss=79.9912
	step [71/250], loss=95.2789
	step [72/250], loss=63.0662
	step [73/250], loss=69.5619
	step [74/250], loss=87.3962
	step [75/250], loss=75.0489
	step [76/250], loss=75.8269
	step [77/250], loss=67.3796
	step [78/250], loss=83.0285
	step [79/250], loss=70.9724
	step [80/250], loss=77.3372
	step [81/250], loss=73.1988
	step [82/250], loss=73.1175
	step [83/250], loss=76.7639
	step [84/250], loss=84.5473
	step [85/250], loss=84.2643
	step [86/250], loss=74.5093
	step [87/250], loss=76.8761
	step [88/250], loss=74.2161
	step [89/250], loss=79.3193
	step [90/250], loss=70.8920
	step [91/250], loss=82.5524
	step [92/250], loss=81.9526
	step [93/250], loss=87.9093
	step [94/250], loss=87.1276
	step [95/250], loss=72.1276
	step [96/250], loss=99.5518
	step [97/250], loss=86.4918
	step [98/250], loss=73.3787
	step [99/250], loss=77.6595
	step [100/250], loss=78.0046
	step [101/250], loss=71.2692
	step [102/250], loss=83.7568
	step [103/250], loss=71.6971
	step [104/250], loss=92.7897
	step [105/250], loss=74.3277
	step [106/250], loss=77.0519
	step [107/250], loss=72.8865
	step [108/250], loss=83.0802
	step [109/250], loss=89.6594
	step [110/250], loss=83.6535
	step [111/250], loss=74.3816
	step [112/250], loss=88.2069
	step [113/250], loss=65.5248
	step [114/250], loss=79.6329
	step [115/250], loss=71.9079
	step [116/250], loss=89.6935
	step [117/250], loss=74.8512
	step [118/250], loss=85.8152
	step [119/250], loss=86.4682
	step [120/250], loss=85.7470
	step [121/250], loss=79.5312
	step [122/250], loss=78.9579
	step [123/250], loss=75.2763
	step [124/250], loss=76.8250
	step [125/250], loss=87.3266
	step [126/250], loss=74.3267
	step [127/250], loss=72.7354
	step [128/250], loss=89.4155
	step [129/250], loss=75.5078
	step [130/250], loss=81.1213
	step [131/250], loss=82.4358
	step [132/250], loss=73.1304
	step [133/250], loss=75.5401
	step [134/250], loss=89.0521
	step [135/250], loss=79.8254
	step [136/250], loss=79.2123
	step [137/250], loss=79.8160
	step [138/250], loss=86.8668
	step [139/250], loss=95.1683
	step [140/250], loss=83.5574
	step [141/250], loss=68.6849
	step [142/250], loss=73.1462
	step [143/250], loss=70.9499
	step [144/250], loss=74.4319
	step [145/250], loss=95.1086
	step [146/250], loss=95.1360
	step [147/250], loss=90.7977
	step [148/250], loss=75.4515
	step [149/250], loss=94.8012
	step [150/250], loss=72.4509
	step [151/250], loss=89.6458
	step [152/250], loss=76.8213
	step [153/250], loss=68.6432
	step [154/250], loss=108.0611
	step [155/250], loss=90.6507
	step [156/250], loss=85.5946
	step [157/250], loss=84.1678
	step [158/250], loss=79.5085
	step [159/250], loss=80.2559
	step [160/250], loss=71.7332
	step [161/250], loss=75.5436
	step [162/250], loss=65.8704
	step [163/250], loss=85.4552
	step [164/250], loss=77.0440
	step [165/250], loss=80.8523
	step [166/250], loss=79.5353
	step [167/250], loss=78.9813
	step [168/250], loss=93.8452
	step [169/250], loss=84.3435
	step [170/250], loss=66.0226
	step [171/250], loss=102.3799
	step [172/250], loss=84.8948
	step [173/250], loss=66.8644
	step [174/250], loss=71.9961
	step [175/250], loss=66.0529
	step [176/250], loss=70.3006
	step [177/250], loss=73.7500
	step [178/250], loss=83.7056
	step [179/250], loss=92.5402
	step [180/250], loss=67.9671
	step [181/250], loss=79.5545
	step [182/250], loss=82.2284
	step [183/250], loss=81.5834
	step [184/250], loss=115.4386
	step [185/250], loss=78.0061
	step [186/250], loss=67.9279
	step [187/250], loss=72.2655
	step [188/250], loss=73.9079
	step [189/250], loss=87.1797
	step [190/250], loss=78.0659
	step [191/250], loss=57.9983
	step [192/250], loss=68.8616
	step [193/250], loss=62.0247
	step [194/250], loss=75.8062
	step [195/250], loss=80.5875
	step [196/250], loss=97.0155
	step [197/250], loss=74.6562
	step [198/250], loss=79.3382
	step [199/250], loss=71.9353
	step [200/250], loss=75.8275
	step [201/250], loss=90.6716
	step [202/250], loss=82.9453
	step [203/250], loss=76.2160
	step [204/250], loss=93.0592
	step [205/250], loss=83.2227
	step [206/250], loss=63.2392
	step [207/250], loss=83.4650
	step [208/250], loss=86.3138
	step [209/250], loss=79.6748
	step [210/250], loss=79.7486
	step [211/250], loss=66.8588
	step [212/250], loss=96.0797
	step [213/250], loss=86.7225
	step [214/250], loss=71.3567
	step [215/250], loss=93.6415
	step [216/250], loss=79.4527
	step [217/250], loss=83.4443
	step [218/250], loss=74.8368
	step [219/250], loss=91.7282
	step [220/250], loss=66.5639
	step [221/250], loss=76.2811
	step [222/250], loss=90.9592
	step [223/250], loss=68.7859
	step [224/250], loss=86.7163
	step [225/250], loss=74.5626
	step [226/250], loss=75.0680
	step [227/250], loss=83.5940
	step [228/250], loss=80.1457
	step [229/250], loss=78.3998
	step [230/250], loss=91.2426
	step [231/250], loss=72.7064
	step [232/250], loss=88.1945
	step [233/250], loss=88.1726
	step [234/250], loss=74.2930
	step [235/250], loss=102.1215
	step [236/250], loss=73.6168
	step [237/250], loss=71.5326
	step [238/250], loss=63.0432
	step [239/250], loss=65.7842
	step [240/250], loss=92.2429
	step [241/250], loss=85.4722
	step [242/250], loss=94.3289
	step [243/250], loss=78.3620
	step [244/250], loss=61.6178
	step [245/250], loss=96.9607
	step [246/250], loss=77.6913
	step [247/250], loss=78.5090
	step [248/250], loss=70.6040
	step [249/250], loss=76.5409
	step [250/250], loss=9.8299
	Evaluating
	loss=0.0076, precision=0.3160, recall=0.8625, f1=0.4625
saving model as: 0_saved_model.pth
Training epoch 70
	step [1/250], loss=102.8954
	step [2/250], loss=80.0435
	step [3/250], loss=84.1165
	step [4/250], loss=76.0208
	step [5/250], loss=78.7430
	step [6/250], loss=71.2310
	step [7/250], loss=73.5593
	step [8/250], loss=68.2993
	step [9/250], loss=94.0029
	step [10/250], loss=83.2035
	step [11/250], loss=97.2420
	step [12/250], loss=105.6653
	step [13/250], loss=70.6975
	step [14/250], loss=73.5139
	step [15/250], loss=73.2033
	step [16/250], loss=63.4027
	step [17/250], loss=88.5433
	step [18/250], loss=92.0245
	step [19/250], loss=72.2752
	step [20/250], loss=71.2526
	step [21/250], loss=97.8363
	step [22/250], loss=92.5130
	step [23/250], loss=62.7737
	step [24/250], loss=87.5216
	step [25/250], loss=70.7910
	step [26/250], loss=91.5785
	step [27/250], loss=79.5510
	step [28/250], loss=70.5241
	step [29/250], loss=68.2931
	step [30/250], loss=79.5721
	step [31/250], loss=92.4375
	step [32/250], loss=88.6911
	step [33/250], loss=75.9123
	step [34/250], loss=105.0030
	step [35/250], loss=82.4165
	step [36/250], loss=72.2112
	step [37/250], loss=94.4109
	step [38/250], loss=80.4501
	step [39/250], loss=95.9293
	step [40/250], loss=75.8017
	step [41/250], loss=74.5880
	step [42/250], loss=73.7091
	step [43/250], loss=79.3540
	step [44/250], loss=85.8129
	step [45/250], loss=80.7675
	step [46/250], loss=99.3776
	step [47/250], loss=74.3189
	step [48/250], loss=82.6803
	step [49/250], loss=67.5119
	step [50/250], loss=79.4400
	step [51/250], loss=71.2780
	step [52/250], loss=79.2003
	step [53/250], loss=76.2888
	step [54/250], loss=70.0878
	step [55/250], loss=80.1462
	step [56/250], loss=74.5175
	step [57/250], loss=78.0885
	step [58/250], loss=77.4841
	step [59/250], loss=76.7395
	step [60/250], loss=93.5040
	step [61/250], loss=83.3482
	step [62/250], loss=83.6266
	step [63/250], loss=87.9628
	step [64/250], loss=75.0298
	step [65/250], loss=71.5027
	step [66/250], loss=85.2364
	step [67/250], loss=66.0773
	step [68/250], loss=83.5842
	step [69/250], loss=97.6921
	step [70/250], loss=79.4585
	step [71/250], loss=101.6112
	step [72/250], loss=97.8729
	step [73/250], loss=69.2483
	step [74/250], loss=82.7677
	step [75/250], loss=89.5140
	step [76/250], loss=68.0102
	step [77/250], loss=63.5340
	step [78/250], loss=68.1911
	step [79/250], loss=65.4335
	step [80/250], loss=66.5154
	step [81/250], loss=100.7937
	step [82/250], loss=80.5251
	step [83/250], loss=73.1225
	step [84/250], loss=81.6125
	step [85/250], loss=81.7416
	step [86/250], loss=79.4040
	step [87/250], loss=86.1064
	step [88/250], loss=76.1391
	step [89/250], loss=68.9086
	step [90/250], loss=94.9970
	step [91/250], loss=67.1848
	step [92/250], loss=76.2135
	step [93/250], loss=88.6404
	step [94/250], loss=80.0335
	step [95/250], loss=89.8326
	step [96/250], loss=85.7454
	step [97/250], loss=83.7362
	step [98/250], loss=84.8338
	step [99/250], loss=77.3675
	step [100/250], loss=98.8726
	step [101/250], loss=76.8186
	step [102/250], loss=78.7137
	step [103/250], loss=72.0382
	step [104/250], loss=86.5645
	step [105/250], loss=74.8153
	step [106/250], loss=74.7347
	step [107/250], loss=75.2946
	step [108/250], loss=98.4082
	step [109/250], loss=80.9617
	step [110/250], loss=83.5116
	step [111/250], loss=72.8839
	step [112/250], loss=90.9004
	step [113/250], loss=87.0451
	step [114/250], loss=86.8190
	step [115/250], loss=77.2616
	step [116/250], loss=75.2160
	step [117/250], loss=74.9779
	step [118/250], loss=66.3383
	step [119/250], loss=86.0647
	step [120/250], loss=72.5070
	step [121/250], loss=54.9716
	step [122/250], loss=89.1615
	step [123/250], loss=76.3297
	step [124/250], loss=79.3899
	step [125/250], loss=69.3741
	step [126/250], loss=66.2211
	step [127/250], loss=106.9225
	step [128/250], loss=93.7408
	step [129/250], loss=86.4556
	step [130/250], loss=71.0753
	step [131/250], loss=69.1074
	step [132/250], loss=82.6489
	step [133/250], loss=68.6850
	step [134/250], loss=94.4722
	step [135/250], loss=75.9109
	step [136/250], loss=99.9290
	step [137/250], loss=83.4125
	step [138/250], loss=85.0854
	step [139/250], loss=72.2136
	step [140/250], loss=64.4054
	step [141/250], loss=76.0559
	step [142/250], loss=74.7157
	step [143/250], loss=88.2920
	step [144/250], loss=70.9399
	step [145/250], loss=72.3683
	step [146/250], loss=86.3082
	step [147/250], loss=79.6759
	step [148/250], loss=75.7315
	step [149/250], loss=81.9162
	step [150/250], loss=95.6331
	step [151/250], loss=81.1608
	step [152/250], loss=63.7317
	step [153/250], loss=68.7308
	step [154/250], loss=74.9836
	step [155/250], loss=76.0316
	step [156/250], loss=83.1504
	step [157/250], loss=73.4659
	step [158/250], loss=89.9613
	step [159/250], loss=100.2380
	step [160/250], loss=68.8827
	step [161/250], loss=96.8861
	step [162/250], loss=84.1659
	step [163/250], loss=85.0733
	step [164/250], loss=63.2550
	step [165/250], loss=81.1594
	step [166/250], loss=60.6076
	step [167/250], loss=73.7778
	step [168/250], loss=82.0924
	step [169/250], loss=77.1007
	step [170/250], loss=76.4338
	step [171/250], loss=83.0762
	step [172/250], loss=85.5332
	step [173/250], loss=88.3714
	step [174/250], loss=87.9552
	step [175/250], loss=73.9482
	step [176/250], loss=80.7403
	step [177/250], loss=73.1598
	step [178/250], loss=89.1583
	step [179/250], loss=76.8391
	step [180/250], loss=81.3615
	step [181/250], loss=81.2533
	step [182/250], loss=72.6016
	step [183/250], loss=83.6087
	step [184/250], loss=83.0081
	step [185/250], loss=85.0092
	step [186/250], loss=74.7955
	step [187/250], loss=90.4734
	step [188/250], loss=73.4034
	step [189/250], loss=82.4116
	step [190/250], loss=83.6955
	step [191/250], loss=75.7471
	step [192/250], loss=75.6085
	step [193/250], loss=66.1326
	step [194/250], loss=83.5265
	step [195/250], loss=65.7046
	step [196/250], loss=97.4725
	step [197/250], loss=67.8623
	step [198/250], loss=84.6223
	step [199/250], loss=68.2779
	step [200/250], loss=83.9518
	step [201/250], loss=88.0471
	step [202/250], loss=92.7849
	step [203/250], loss=106.8493
	step [204/250], loss=69.4025
	step [205/250], loss=79.6066
	step [206/250], loss=63.6372
	step [207/250], loss=77.5701
	step [208/250], loss=76.6444
	step [209/250], loss=85.7842
	step [210/250], loss=71.4173
	step [211/250], loss=74.6883
	step [212/250], loss=86.1384
	step [213/250], loss=69.8404
	step [214/250], loss=77.1695
	step [215/250], loss=91.3527
	step [216/250], loss=76.2185
	step [217/250], loss=68.5965
	step [218/250], loss=81.0464
	step [219/250], loss=67.3015
	step [220/250], loss=59.0676
	step [221/250], loss=70.2270
	step [222/250], loss=80.5743
	step [223/250], loss=70.0843
	step [224/250], loss=91.0847
	step [225/250], loss=96.4395
	step [226/250], loss=72.4258
	step [227/250], loss=97.6352
	step [228/250], loss=77.5714
	step [229/250], loss=68.1890
	step [230/250], loss=73.4594
	step [231/250], loss=71.9187
	step [232/250], loss=87.9439
	step [233/250], loss=90.6310
	step [234/250], loss=66.7584
	step [235/250], loss=87.3506
	step [236/250], loss=79.2375
	step [237/250], loss=71.8618
	step [238/250], loss=71.6122
	step [239/250], loss=77.8131
	step [240/250], loss=79.4844
	step [241/250], loss=71.4840
	step [242/250], loss=90.2752
	step [243/250], loss=83.6205
	step [244/250], loss=70.9511
	step [245/250], loss=71.4341
	step [246/250], loss=81.5594
	step [247/250], loss=76.4596
	step [248/250], loss=87.5610
	step [249/250], loss=101.3950
	step [250/250], loss=12.2101
	Evaluating
	loss=0.0096, precision=0.2409, recall=0.8681, f1=0.3771
Training epoch 71
	step [1/250], loss=60.7571
	step [2/250], loss=76.5113
	step [3/250], loss=72.1448
	step [4/250], loss=72.2698
	step [5/250], loss=73.4536
	step [6/250], loss=69.5432
	step [7/250], loss=63.7050
	step [8/250], loss=72.9080
	step [9/250], loss=76.2229
	step [10/250], loss=70.2809
	step [11/250], loss=78.0813
	step [12/250], loss=84.8374
	step [13/250], loss=85.8671
	step [14/250], loss=66.5345
	step [15/250], loss=90.0488
	step [16/250], loss=98.5728
	step [17/250], loss=84.0689
	step [18/250], loss=83.1368
	step [19/250], loss=77.5206
	step [20/250], loss=73.2009
	step [21/250], loss=86.5864
	step [22/250], loss=89.8511
	step [23/250], loss=62.5720
	step [24/250], loss=89.9613
	step [25/250], loss=83.6423
	step [26/250], loss=81.8373
	step [27/250], loss=86.8994
	step [28/250], loss=66.2792
	step [29/250], loss=75.7378
	step [30/250], loss=74.7743
	step [31/250], loss=78.1626
	step [32/250], loss=61.0495
	step [33/250], loss=80.2614
	step [34/250], loss=69.1046
	step [35/250], loss=66.5078
	step [36/250], loss=77.0145
	step [37/250], loss=68.3939
	step [38/250], loss=83.4598
	step [39/250], loss=84.7633
	step [40/250], loss=77.8094
	step [41/250], loss=68.5943
	step [42/250], loss=76.5308
	step [43/250], loss=67.8049
	step [44/250], loss=77.7928
	step [45/250], loss=75.8261
	step [46/250], loss=85.6304
	step [47/250], loss=84.5925
	step [48/250], loss=70.3454
	step [49/250], loss=68.2535
	step [50/250], loss=86.5610
	step [51/250], loss=85.2537
	step [52/250], loss=73.0394
	step [53/250], loss=74.9328
	step [54/250], loss=75.2055
	step [55/250], loss=102.0422
	step [56/250], loss=82.4273
	step [57/250], loss=78.1144
	step [58/250], loss=71.6268
	step [59/250], loss=80.6244
	step [60/250], loss=85.2149
	step [61/250], loss=66.7315
	step [62/250], loss=84.2568
	step [63/250], loss=85.0230
	step [64/250], loss=77.1781
	step [65/250], loss=72.6583
	step [66/250], loss=75.7985
	step [67/250], loss=89.9137
	step [68/250], loss=96.5752
	step [69/250], loss=85.3350
	step [70/250], loss=77.7217
	step [71/250], loss=70.8807
	step [72/250], loss=77.1377
	step [73/250], loss=87.3798
	step [74/250], loss=77.8895
	step [75/250], loss=80.2440
	step [76/250], loss=99.0077
	step [77/250], loss=71.8524
	step [78/250], loss=82.9159
	step [79/250], loss=87.1621
	step [80/250], loss=69.2749
	step [81/250], loss=78.5276
	step [82/250], loss=83.1176
	step [83/250], loss=85.2887
	step [84/250], loss=81.4544
	step [85/250], loss=98.5012
	step [86/250], loss=74.3070
	step [87/250], loss=76.5076
	step [88/250], loss=83.0844
	step [89/250], loss=78.5650
	step [90/250], loss=63.8368
	step [91/250], loss=89.1170
	step [92/250], loss=77.1180
	step [93/250], loss=85.9446
	step [94/250], loss=88.9826
	step [95/250], loss=67.0182
	step [96/250], loss=77.3707
	step [97/250], loss=80.1955
	step [98/250], loss=81.6100
	step [99/250], loss=79.5502
	step [100/250], loss=73.4081
	step [101/250], loss=70.1997
	step [102/250], loss=75.0364
	step [103/250], loss=71.8007
	step [104/250], loss=84.5762
	step [105/250], loss=69.9169
	step [106/250], loss=94.5732
	step [107/250], loss=87.4759
	step [108/250], loss=87.3609
	step [109/250], loss=81.8626
	step [110/250], loss=81.9623
	step [111/250], loss=82.7569
	step [112/250], loss=91.8085
	step [113/250], loss=64.4370
	step [114/250], loss=78.6603
	step [115/250], loss=81.4128
	step [116/250], loss=77.3633
	step [117/250], loss=73.0074
	step [118/250], loss=83.2831
	step [119/250], loss=91.6117
	step [120/250], loss=82.5764
	step [121/250], loss=88.4443
	step [122/250], loss=95.0575
	step [123/250], loss=84.1046
	step [124/250], loss=79.3124
	step [125/250], loss=67.2649
	step [126/250], loss=96.1703
	step [127/250], loss=90.4616
	step [128/250], loss=70.5197
	step [129/250], loss=63.5000
	step [130/250], loss=78.0379
	step [131/250], loss=95.0303
	step [132/250], loss=81.6965
	step [133/250], loss=83.0866
	step [134/250], loss=71.0234
	step [135/250], loss=80.2240
	step [136/250], loss=67.1916
	step [137/250], loss=77.6453
	step [138/250], loss=69.2500
	step [139/250], loss=86.1513
	step [140/250], loss=65.6277
	step [141/250], loss=93.5435
	step [142/250], loss=70.9882
	step [143/250], loss=92.6682
	step [144/250], loss=72.0929
	step [145/250], loss=73.9699
	step [146/250], loss=72.4275
	step [147/250], loss=99.8638
	step [148/250], loss=80.2215
	step [149/250], loss=81.7920
	step [150/250], loss=74.1879
	step [151/250], loss=86.9660
	step [152/250], loss=79.5014
	step [153/250], loss=75.2387
	step [154/250], loss=73.5459
	step [155/250], loss=97.3733
	step [156/250], loss=85.5794
	step [157/250], loss=70.5988
	step [158/250], loss=77.4660
	step [159/250], loss=76.4113
	step [160/250], loss=84.9063
	step [161/250], loss=89.4394
	step [162/250], loss=75.4785
	step [163/250], loss=84.9066
	step [164/250], loss=73.8632
	step [165/250], loss=94.1357
	step [166/250], loss=64.3288
	step [167/250], loss=86.4583
	step [168/250], loss=74.0473
	step [169/250], loss=77.2603
	step [170/250], loss=77.4767
	step [171/250], loss=88.5441
	step [172/250], loss=87.3751
	step [173/250], loss=78.2569
	step [174/250], loss=63.3959
	step [175/250], loss=74.3925
	step [176/250], loss=86.4209
	step [177/250], loss=82.7343
	step [178/250], loss=90.1804
	step [179/250], loss=77.5199
	step [180/250], loss=79.9994
	step [181/250], loss=69.9831
	step [182/250], loss=60.5957
	step [183/250], loss=75.1389
	step [184/250], loss=72.4874
	step [185/250], loss=69.5367
	step [186/250], loss=64.5165
	step [187/250], loss=86.4495
	step [188/250], loss=78.5400
	step [189/250], loss=88.5970
	step [190/250], loss=77.6781
	step [191/250], loss=80.6304
	step [192/250], loss=94.7283
	step [193/250], loss=97.6206
	step [194/250], loss=81.0790
	step [195/250], loss=76.0827
	step [196/250], loss=74.8344
	step [197/250], loss=84.2589
	step [198/250], loss=87.1610
	step [199/250], loss=72.3488
	step [200/250], loss=82.0008
	step [201/250], loss=82.2634
	step [202/250], loss=86.7501
	step [203/250], loss=80.5444
	step [204/250], loss=81.4878
	step [205/250], loss=85.9347
	step [206/250], loss=80.1430
	step [207/250], loss=79.6174
	step [208/250], loss=110.1844
	step [209/250], loss=79.8728
	step [210/250], loss=94.8134
	step [211/250], loss=72.6398
	step [212/250], loss=64.9156
	step [213/250], loss=88.2842
	step [214/250], loss=77.3660
	step [215/250], loss=78.1932
	step [216/250], loss=85.8286
	step [217/250], loss=83.7864
	step [218/250], loss=72.0072
	step [219/250], loss=76.5795
	step [220/250], loss=91.0433
	step [221/250], loss=80.3312
	step [222/250], loss=68.4222
	step [223/250], loss=70.3499
	step [224/250], loss=79.8466
	step [225/250], loss=72.8280
	step [226/250], loss=90.3711
	step [227/250], loss=79.2400
	step [228/250], loss=77.4325
	step [229/250], loss=94.1872
	step [230/250], loss=66.8630
	step [231/250], loss=73.3426
	step [232/250], loss=98.0099
	step [233/250], loss=74.7909
	step [234/250], loss=74.8815
	step [235/250], loss=82.6295
	step [236/250], loss=79.6406
	step [237/250], loss=77.1631
	step [238/250], loss=68.2885
	step [239/250], loss=84.3650
	step [240/250], loss=76.9866
	step [241/250], loss=78.4310
	step [242/250], loss=88.1653
	step [243/250], loss=83.7804
	step [244/250], loss=80.0407
	step [245/250], loss=86.7467
	step [246/250], loss=77.6656
	step [247/250], loss=79.2650
	step [248/250], loss=80.5979
	step [249/250], loss=89.6813
	step [250/250], loss=14.2622
	Evaluating
	loss=0.0083, precision=0.2835, recall=0.8587, f1=0.4263
Training epoch 72
	step [1/250], loss=64.4251
	step [2/250], loss=73.2347
	step [3/250], loss=85.5602
	step [4/250], loss=73.2186
	step [5/250], loss=82.2458
	step [6/250], loss=62.4624
	step [7/250], loss=83.1759
	step [8/250], loss=90.5496
	step [9/250], loss=74.0330
	step [10/250], loss=81.8789
	step [11/250], loss=68.5831
	step [12/250], loss=85.1649
	step [13/250], loss=59.8847
	step [14/250], loss=76.6332
	step [15/250], loss=52.4518
	step [16/250], loss=76.8815
	step [17/250], loss=88.0144
	step [18/250], loss=83.3739
	step [19/250], loss=70.3931
	step [20/250], loss=88.2860
	step [21/250], loss=78.4515
	step [22/250], loss=89.0272
	step [23/250], loss=59.8521
	step [24/250], loss=110.9240
	step [25/250], loss=67.4604
	step [26/250], loss=73.7525
	step [27/250], loss=76.2006
	step [28/250], loss=87.2376
	step [29/250], loss=78.1876
	step [30/250], loss=73.0146
	step [31/250], loss=85.5799
	step [32/250], loss=73.1870
	step [33/250], loss=67.0317
	step [34/250], loss=93.8776
	step [35/250], loss=75.4367
	step [36/250], loss=84.5121
	step [37/250], loss=66.5597
	step [38/250], loss=78.0934
	step [39/250], loss=86.5327
	step [40/250], loss=78.1391
	step [41/250], loss=96.1451
	step [42/250], loss=59.6215
	step [43/250], loss=70.8118
	step [44/250], loss=80.7099
	step [45/250], loss=74.9589
	step [46/250], loss=73.8886
	step [47/250], loss=95.3424
	step [48/250], loss=59.3214
	step [49/250], loss=76.2995
	step [50/250], loss=77.5402
	step [51/250], loss=84.9179
	step [52/250], loss=98.7385
	step [53/250], loss=94.2837
	step [54/250], loss=79.6605
	step [55/250], loss=102.1655
	step [56/250], loss=77.9599
	step [57/250], loss=80.1306
	step [58/250], loss=99.8031
	step [59/250], loss=84.5557
	step [60/250], loss=81.5234
	step [61/250], loss=70.6651
	step [62/250], loss=86.2049
	step [63/250], loss=65.3888
	step [64/250], loss=84.4168
	step [65/250], loss=69.3376
	step [66/250], loss=79.6751
	step [67/250], loss=95.3761
	step [68/250], loss=78.1885
	step [69/250], loss=70.5070
	step [70/250], loss=87.1947
	step [71/250], loss=95.7230
	step [72/250], loss=74.9683
	step [73/250], loss=71.2054
	step [74/250], loss=88.9644
	step [75/250], loss=74.0637
	step [76/250], loss=57.8949
	step [77/250], loss=77.2245
	step [78/250], loss=98.8941
	step [79/250], loss=102.7670
	step [80/250], loss=75.2096
	step [81/250], loss=74.1088
	step [82/250], loss=94.4048
	step [83/250], loss=74.2941
	step [84/250], loss=92.1614
	step [85/250], loss=87.2118
	step [86/250], loss=80.6889
	step [87/250], loss=88.0695
	step [88/250], loss=87.0455
	step [89/250], loss=91.2639
	step [90/250], loss=73.5740
	step [91/250], loss=79.1637
	step [92/250], loss=66.4321
	step [93/250], loss=78.9144
	step [94/250], loss=91.0938
	step [95/250], loss=71.6227
	step [96/250], loss=79.2871
	step [97/250], loss=73.9083
	step [98/250], loss=82.3755
	step [99/250], loss=73.4976
	step [100/250], loss=67.9758
	step [101/250], loss=72.1460
	step [102/250], loss=85.2086
	step [103/250], loss=62.7803
	step [104/250], loss=75.6886
	step [105/250], loss=77.5916
	step [106/250], loss=79.6912
	step [107/250], loss=82.6539
	step [108/250], loss=85.2261
	step [109/250], loss=84.2999
	step [110/250], loss=65.9369
	step [111/250], loss=73.6933
	step [112/250], loss=78.1433
	step [113/250], loss=84.5441
	step [114/250], loss=77.3617
	step [115/250], loss=84.1832
	step [116/250], loss=70.8585
	step [117/250], loss=70.3403
	step [118/250], loss=96.2165
	step [119/250], loss=87.3525
	step [120/250], loss=71.9885
	step [121/250], loss=76.4367
	step [122/250], loss=83.1241
	step [123/250], loss=78.8887
	step [124/250], loss=85.3944
	step [125/250], loss=72.6235
	step [126/250], loss=81.5155
	step [127/250], loss=80.1030
	step [128/250], loss=82.2928
	step [129/250], loss=68.7771
	step [130/250], loss=71.0851
	step [131/250], loss=92.5666
	step [132/250], loss=58.7335
	step [133/250], loss=66.7916
	step [134/250], loss=76.2932
	step [135/250], loss=80.9746
	step [136/250], loss=85.3699
	step [137/250], loss=69.3899
	step [138/250], loss=73.1544
	step [139/250], loss=62.7494
	step [140/250], loss=62.5666
	step [141/250], loss=82.6664
	step [142/250], loss=85.8926
	step [143/250], loss=81.4916
	step [144/250], loss=64.0180
	step [145/250], loss=84.0813
	step [146/250], loss=87.0346
	step [147/250], loss=78.2191
	step [148/250], loss=68.8300
	step [149/250], loss=69.4747
	step [150/250], loss=88.4050
	step [151/250], loss=99.2057
	step [152/250], loss=88.5888
	step [153/250], loss=73.9716
	step [154/250], loss=78.8072
	step [155/250], loss=66.7125
	step [156/250], loss=82.1317
	step [157/250], loss=93.3865
	step [158/250], loss=92.7897
	step [159/250], loss=70.7483
	step [160/250], loss=69.8257
	step [161/250], loss=90.0091
	step [162/250], loss=89.0668
	step [163/250], loss=100.0773
	step [164/250], loss=87.2186
	step [165/250], loss=80.4924
	step [166/250], loss=81.6261
	step [167/250], loss=82.2258
	step [168/250], loss=76.4399
	step [169/250], loss=89.2376
	step [170/250], loss=68.7392
	step [171/250], loss=86.5626
	step [172/250], loss=85.7352
	step [173/250], loss=72.6737
	step [174/250], loss=84.9039
	step [175/250], loss=77.7579
	step [176/250], loss=75.5180
	step [177/250], loss=69.8669
	step [178/250], loss=79.3110
	step [179/250], loss=61.2035
	step [180/250], loss=76.6954
	step [181/250], loss=67.9894
	step [182/250], loss=79.6671
	step [183/250], loss=82.1418
	step [184/250], loss=73.0166
	step [185/250], loss=91.3663
	step [186/250], loss=75.3522
	step [187/250], loss=73.5963
	step [188/250], loss=76.7626
	step [189/250], loss=76.8729
	step [190/250], loss=84.9650
	step [191/250], loss=84.1537
	step [192/250], loss=74.8896
	step [193/250], loss=88.9791
	step [194/250], loss=77.6455
	step [195/250], loss=64.1782
	step [196/250], loss=92.9916
	step [197/250], loss=68.3424
	step [198/250], loss=73.1442
	step [199/250], loss=114.2994
	step [200/250], loss=103.9760
	step [201/250], loss=89.2551
	step [202/250], loss=58.0151
	step [203/250], loss=78.0171
	step [204/250], loss=81.9502
	step [205/250], loss=86.7473
	step [206/250], loss=89.5775
	step [207/250], loss=78.1748
	step [208/250], loss=82.7313
	step [209/250], loss=99.8417
	step [210/250], loss=80.1528
	step [211/250], loss=75.1699
	step [212/250], loss=83.7799
	step [213/250], loss=76.9523
	step [214/250], loss=79.6378
	step [215/250], loss=94.1143
	step [216/250], loss=76.1853
	step [217/250], loss=83.4545
	step [218/250], loss=77.8968
	step [219/250], loss=72.1218
	step [220/250], loss=71.8350
	step [221/250], loss=103.0842
	step [222/250], loss=91.2202
	step [223/250], loss=90.7569
	step [224/250], loss=75.0835
	step [225/250], loss=74.9054
	step [226/250], loss=85.8310
	step [227/250], loss=75.9022
	step [228/250], loss=104.6482
	step [229/250], loss=75.0207
	step [230/250], loss=74.5118
	step [231/250], loss=89.8551
	step [232/250], loss=89.0855
	step [233/250], loss=69.4254
	step [234/250], loss=84.9998
	step [235/250], loss=90.5395
	step [236/250], loss=80.8750
	step [237/250], loss=79.1740
	step [238/250], loss=89.4276
	step [239/250], loss=77.7173
	step [240/250], loss=83.7221
	step [241/250], loss=72.8935
	step [242/250], loss=81.5833
	step [243/250], loss=70.6814
	step [244/250], loss=76.6156
	step [245/250], loss=88.0876
	step [246/250], loss=68.5623
	step [247/250], loss=59.8934
	step [248/250], loss=60.0678
	step [249/250], loss=74.7427
	step [250/250], loss=15.0012
	Evaluating
	loss=0.0095, precision=0.2489, recall=0.8706, f1=0.3872
Training epoch 73
	step [1/250], loss=73.5646
	step [2/250], loss=91.5657
	step [3/250], loss=74.7566
	step [4/250], loss=83.6602
	step [5/250], loss=68.7887
	step [6/250], loss=78.8808
	step [7/250], loss=84.9890
	step [8/250], loss=72.1470
	step [9/250], loss=80.9632
	step [10/250], loss=96.2513
	step [11/250], loss=76.9552
	step [12/250], loss=88.1069
	step [13/250], loss=65.5773
	step [14/250], loss=74.5520
	step [15/250], loss=75.6110
	step [16/250], loss=76.2843
	step [17/250], loss=99.2877
	step [18/250], loss=91.2871
	step [19/250], loss=87.2033
	step [20/250], loss=84.1794
	step [21/250], loss=74.3798
	step [22/250], loss=71.7324
	step [23/250], loss=74.4389
	step [24/250], loss=83.8593
	step [25/250], loss=83.4642
	step [26/250], loss=70.9572
	step [27/250], loss=75.8224
	step [28/250], loss=65.9724
	step [29/250], loss=69.6831
	step [30/250], loss=85.7235
	step [31/250], loss=79.7897
	step [32/250], loss=92.4984
	step [33/250], loss=75.2735
	step [34/250], loss=106.4763
	step [35/250], loss=83.8277
	step [36/250], loss=90.0075
	step [37/250], loss=67.9795
	step [38/250], loss=72.1216
	step [39/250], loss=86.1756
	step [40/250], loss=96.5273
	step [41/250], loss=81.5472
	step [42/250], loss=80.8926
	step [43/250], loss=69.4974
	step [44/250], loss=90.5342
	step [45/250], loss=79.5597
	step [46/250], loss=85.2691
	step [47/250], loss=86.7968
	step [48/250], loss=89.8121
	step [49/250], loss=92.0430
	step [50/250], loss=58.7938
	step [51/250], loss=96.8351
	step [52/250], loss=87.7153
	step [53/250], loss=78.3351
	step [54/250], loss=56.2601
	step [55/250], loss=89.4795
	step [56/250], loss=72.6358
	step [57/250], loss=73.9829
	step [58/250], loss=86.7285
	step [59/250], loss=97.9630
	step [60/250], loss=64.0742
	step [61/250], loss=86.2576
	step [62/250], loss=75.5214
	step [63/250], loss=71.2524
	step [64/250], loss=83.3541
	step [65/250], loss=89.5612
	step [66/250], loss=78.2379
	step [67/250], loss=84.3490
	step [68/250], loss=79.2608
	step [69/250], loss=89.4392
	step [70/250], loss=59.9304
	step [71/250], loss=79.8927
	step [72/250], loss=85.7101
	step [73/250], loss=74.3821
	step [74/250], loss=89.0724
	step [75/250], loss=75.6820
	step [76/250], loss=72.4244
	step [77/250], loss=69.0872
	step [78/250], loss=108.4913
	step [79/250], loss=90.7128
	step [80/250], loss=73.9796
	step [81/250], loss=69.6981
	step [82/250], loss=71.7807
	step [83/250], loss=93.5506
	step [84/250], loss=79.2511
	step [85/250], loss=67.1858
	step [86/250], loss=85.8867
	step [87/250], loss=82.3565
	step [88/250], loss=69.7944
	step [89/250], loss=67.5177
	step [90/250], loss=78.7492
	step [91/250], loss=90.3886
	step [92/250], loss=70.3912
	step [93/250], loss=67.3444
	step [94/250], loss=75.2067
	step [95/250], loss=67.2671
	step [96/250], loss=87.8900
	step [97/250], loss=75.1319
	step [98/250], loss=82.2304
	step [99/250], loss=75.0098
	step [100/250], loss=86.8162
	step [101/250], loss=65.9425
	step [102/250], loss=70.4781
	step [103/250], loss=91.2877
	step [104/250], loss=82.7370
	step [105/250], loss=76.2288
	step [106/250], loss=77.0663
	step [107/250], loss=77.3551
	step [108/250], loss=87.5699
	step [109/250], loss=92.7195
	step [110/250], loss=91.4856
	step [111/250], loss=77.8316
	step [112/250], loss=93.4127
	step [113/250], loss=76.0081
	step [114/250], loss=83.8734
	step [115/250], loss=77.8042
	step [116/250], loss=88.4862
	step [117/250], loss=82.2690
	step [118/250], loss=75.4861
	step [119/250], loss=68.6627
	step [120/250], loss=74.4989
	step [121/250], loss=76.5879
	step [122/250], loss=68.9540
	step [123/250], loss=84.6887
	step [124/250], loss=82.9551
	step [125/250], loss=70.7846
	step [126/250], loss=70.6588
	step [127/250], loss=60.9135
	step [128/250], loss=89.9322
	step [129/250], loss=72.8682
	step [130/250], loss=86.4843
	step [131/250], loss=74.5054
	step [132/250], loss=91.1376
	step [133/250], loss=85.5057
	step [134/250], loss=78.7462
	step [135/250], loss=83.4720
	step [136/250], loss=62.6258
	step [137/250], loss=80.0905
	step [138/250], loss=95.1967
	step [139/250], loss=62.5388
	step [140/250], loss=88.5734
	step [141/250], loss=66.4621
	step [142/250], loss=85.4104
	step [143/250], loss=80.2751
	step [144/250], loss=76.7495
	step [145/250], loss=83.6658
	step [146/250], loss=80.1155
	step [147/250], loss=78.8333
	step [148/250], loss=94.2730
	step [149/250], loss=76.9777
	step [150/250], loss=92.7424
	step [151/250], loss=80.3160
	step [152/250], loss=83.6672
	step [153/250], loss=72.1158
	step [154/250], loss=76.8114
	step [155/250], loss=81.0987
	step [156/250], loss=60.0424
	step [157/250], loss=87.6511
	step [158/250], loss=60.8765
	step [159/250], loss=83.2648
	step [160/250], loss=69.8784
	step [161/250], loss=61.1795
	step [162/250], loss=76.2764
	step [163/250], loss=80.6158
	step [164/250], loss=88.3555
	step [165/250], loss=83.2417
	step [166/250], loss=81.0050
	step [167/250], loss=97.9276
	step [168/250], loss=83.0865
	step [169/250], loss=81.9170
	step [170/250], loss=65.8919
	step [171/250], loss=71.9993
	step [172/250], loss=76.7904
	step [173/250], loss=82.0355
	step [174/250], loss=104.0123
	step [175/250], loss=85.6165
	step [176/250], loss=67.8628
	step [177/250], loss=82.7580
	step [178/250], loss=81.8960
	step [179/250], loss=81.1602
	step [180/250], loss=77.0616
	step [181/250], loss=77.6890
	step [182/250], loss=92.0304
	step [183/250], loss=71.5936
	step [184/250], loss=78.0724
	step [185/250], loss=85.2619
	step [186/250], loss=82.1857
	step [187/250], loss=71.3323
	step [188/250], loss=52.9620
	step [189/250], loss=72.6684
	step [190/250], loss=90.7424
	step [191/250], loss=74.0844
	step [192/250], loss=81.9398
	step [193/250], loss=78.0474
	step [194/250], loss=75.7940
	step [195/250], loss=74.9482
	step [196/250], loss=73.5812
	step [197/250], loss=82.3983
	step [198/250], loss=81.0148
	step [199/250], loss=79.7597
	step [200/250], loss=63.3454
	step [201/250], loss=76.6895
	step [202/250], loss=77.2755
	step [203/250], loss=84.4074
	step [204/250], loss=73.9334
	step [205/250], loss=84.0661
	step [206/250], loss=72.9058
	step [207/250], loss=89.4779
	step [208/250], loss=84.1812
	step [209/250], loss=77.1435
	step [210/250], loss=77.2257
	step [211/250], loss=82.1599
	step [212/250], loss=79.2674
	step [213/250], loss=69.1409
	step [214/250], loss=76.0531
	step [215/250], loss=78.8225
	step [216/250], loss=83.7758
	step [217/250], loss=86.8316
	step [218/250], loss=73.0325
	step [219/250], loss=70.8943
	step [220/250], loss=71.6954
	step [221/250], loss=65.9927
	step [222/250], loss=98.3767
	step [223/250], loss=83.4577
	step [224/250], loss=72.6758
	step [225/250], loss=75.3541
	step [226/250], loss=77.6616
	step [227/250], loss=88.4325
	step [228/250], loss=67.9383
	step [229/250], loss=96.4915
	step [230/250], loss=84.7719
	step [231/250], loss=60.7364
	step [232/250], loss=65.0087
	step [233/250], loss=69.1138
	step [234/250], loss=70.0200
	step [235/250], loss=72.8446
	step [236/250], loss=67.4517
	step [237/250], loss=95.9464
	step [238/250], loss=98.3858
	step [239/250], loss=54.8032
	step [240/250], loss=88.7855
	step [241/250], loss=89.7616
	step [242/250], loss=67.6511
	step [243/250], loss=91.4964
	step [244/250], loss=83.2479
	step [245/250], loss=74.6563
	step [246/250], loss=81.2851
	step [247/250], loss=93.8891
	step [248/250], loss=78.9477
	step [249/250], loss=76.5762
	step [250/250], loss=20.5985
	Evaluating
	loss=0.0084, precision=0.2829, recall=0.8584, f1=0.4255
Training epoch 74
	step [1/250], loss=85.8351
	step [2/250], loss=73.4352
	step [3/250], loss=92.3218
	step [4/250], loss=72.8414
	step [5/250], loss=82.9700
	step [6/250], loss=89.8383
	step [7/250], loss=73.4463
	step [8/250], loss=79.5039
	step [9/250], loss=64.7782
	step [10/250], loss=83.5773
	step [11/250], loss=72.7345
	step [12/250], loss=68.4066
	step [13/250], loss=60.6321
	step [14/250], loss=75.7044
	step [15/250], loss=76.1681
	step [16/250], loss=74.0367
	step [17/250], loss=82.4179
	step [18/250], loss=81.1147
	step [19/250], loss=72.4330
	step [20/250], loss=76.9697
	step [21/250], loss=78.5573
	step [22/250], loss=82.4661
	step [23/250], loss=90.9428
	step [24/250], loss=84.4095
	step [25/250], loss=79.8623
	step [26/250], loss=70.8342
	step [27/250], loss=80.1189
	step [28/250], loss=80.9242
	step [29/250], loss=66.3428
	step [30/250], loss=99.1786
	step [31/250], loss=75.3629
	step [32/250], loss=81.6125
	step [33/250], loss=74.8750
	step [34/250], loss=61.6957
	step [35/250], loss=78.0484
	step [36/250], loss=99.0558
	step [37/250], loss=81.3931
	step [38/250], loss=70.0135
	step [39/250], loss=71.8538
	step [40/250], loss=98.8291
	step [41/250], loss=75.7290
	step [42/250], loss=75.0015
	step [43/250], loss=70.8261
	step [44/250], loss=68.5227
	step [45/250], loss=80.8558
	step [46/250], loss=62.0765
	step [47/250], loss=73.4051
	step [48/250], loss=84.0743
	step [49/250], loss=78.0129
	step [50/250], loss=79.8353
	step [51/250], loss=83.6758
	step [52/250], loss=89.5495
	step [53/250], loss=71.6992
	step [54/250], loss=87.5508
	step [55/250], loss=71.7765
	step [56/250], loss=77.1871
	step [57/250], loss=87.5421
	step [58/250], loss=78.4345
	step [59/250], loss=85.5922
	step [60/250], loss=81.6322
	step [61/250], loss=98.2588
	step [62/250], loss=86.4188
	step [63/250], loss=74.5781
	step [64/250], loss=77.2277
	step [65/250], loss=72.5630
	step [66/250], loss=70.5442
	step [67/250], loss=80.7113
	step [68/250], loss=75.5952
	step [69/250], loss=74.3173
	step [70/250], loss=79.9583
	step [71/250], loss=92.5470
	step [72/250], loss=101.8583
	step [73/250], loss=78.2507
	step [74/250], loss=79.8210
	step [75/250], loss=71.3227
	step [76/250], loss=79.6337
	step [77/250], loss=84.6214
	step [78/250], loss=94.9259
	step [79/250], loss=91.8219
	step [80/250], loss=81.5820
	step [81/250], loss=76.5073
	step [82/250], loss=76.3745
	step [83/250], loss=68.5063
	step [84/250], loss=77.4611
	step [85/250], loss=85.5320
	step [86/250], loss=83.3396
	step [87/250], loss=88.8788
	step [88/250], loss=74.9017
	step [89/250], loss=75.5873
	step [90/250], loss=73.3468
	step [91/250], loss=81.2392
	step [92/250], loss=92.4663
	step [93/250], loss=71.8591
	step [94/250], loss=70.6220
	step [95/250], loss=71.1382
	step [96/250], loss=69.8853
	step [97/250], loss=77.2379
	step [98/250], loss=91.0418
	step [99/250], loss=75.8790
	step [100/250], loss=79.9463
	step [101/250], loss=81.6114
	step [102/250], loss=72.6139
	step [103/250], loss=72.7858
	step [104/250], loss=87.0294
	step [105/250], loss=70.8735
	step [106/250], loss=75.0466
	step [107/250], loss=71.2404
	step [108/250], loss=63.9866
	step [109/250], loss=74.8220
	step [110/250], loss=106.7458
	step [111/250], loss=86.6946
	step [112/250], loss=71.4279
	step [113/250], loss=103.1009
	step [114/250], loss=76.6825
	step [115/250], loss=73.6435
	step [116/250], loss=60.4669
	step [117/250], loss=79.4136
	step [118/250], loss=78.2263
	step [119/250], loss=65.8455
	step [120/250], loss=86.7737
	step [121/250], loss=73.6790
	step [122/250], loss=66.9691
	step [123/250], loss=81.1929
	step [124/250], loss=83.0294
	step [125/250], loss=74.3843
	step [126/250], loss=74.5741
	step [127/250], loss=85.8528
	step [128/250], loss=103.0694
	step [129/250], loss=83.0916
	step [130/250], loss=87.8960
	step [131/250], loss=71.9075
	step [132/250], loss=92.3714
	step [133/250], loss=91.3488
	step [134/250], loss=69.9295
	step [135/250], loss=79.1872
	step [136/250], loss=76.6892
	step [137/250], loss=86.5448
	step [138/250], loss=65.1183
	step [139/250], loss=84.8517
	step [140/250], loss=97.5005
	step [141/250], loss=68.4870
	step [142/250], loss=83.0305
	step [143/250], loss=84.5867
	step [144/250], loss=78.1227
	step [145/250], loss=89.4037
	step [146/250], loss=73.7101
	step [147/250], loss=98.8292
	step [148/250], loss=82.8711
	step [149/250], loss=84.0456
	step [150/250], loss=69.6784
	step [151/250], loss=104.7259
	step [152/250], loss=71.2906
	step [153/250], loss=68.3775
	step [154/250], loss=79.0312
	step [155/250], loss=78.3701
	step [156/250], loss=83.6806
	step [157/250], loss=78.8478
	step [158/250], loss=82.9220
	step [159/250], loss=82.4465
	step [160/250], loss=71.0539
	step [161/250], loss=74.8665
	step [162/250], loss=84.8660
	step [163/250], loss=80.2888
	step [164/250], loss=83.1893
	step [165/250], loss=60.3767
	step [166/250], loss=70.5999
	step [167/250], loss=65.2913
	step [168/250], loss=75.3656
	step [169/250], loss=67.8241
	step [170/250], loss=83.4051
	step [171/250], loss=79.3568
	step [172/250], loss=87.0510
	step [173/250], loss=67.9886
	step [174/250], loss=103.2131
	step [175/250], loss=79.1398
	step [176/250], loss=69.3713
	step [177/250], loss=73.4274
	step [178/250], loss=82.2383
	step [179/250], loss=65.9629
	step [180/250], loss=73.8497
	step [181/250], loss=80.8559
	step [182/250], loss=75.5484
	step [183/250], loss=68.1062
	step [184/250], loss=79.2952
	step [185/250], loss=74.8849
	step [186/250], loss=76.5843
	step [187/250], loss=78.2198
	step [188/250], loss=82.6978
	step [189/250], loss=81.5894
	step [190/250], loss=79.5707
	step [191/250], loss=88.0169
	step [192/250], loss=85.9989
	step [193/250], loss=77.9701
	step [194/250], loss=91.0002
	step [195/250], loss=72.4360
	step [196/250], loss=84.9042
	step [197/250], loss=66.2143
	step [198/250], loss=81.6483
	step [199/250], loss=70.9391
	step [200/250], loss=88.4327
	step [201/250], loss=70.2580
	step [202/250], loss=72.3941
	step [203/250], loss=90.6716
	step [204/250], loss=77.7409
	step [205/250], loss=74.1478
	step [206/250], loss=66.1346
	step [207/250], loss=66.6469
	step [208/250], loss=85.0410
	step [209/250], loss=78.6166
	step [210/250], loss=85.4927
	step [211/250], loss=87.5461
	step [212/250], loss=65.7924
	step [213/250], loss=80.0616
	step [214/250], loss=76.6599
	step [215/250], loss=78.4834
	step [216/250], loss=77.8286
	step [217/250], loss=79.1639
	step [218/250], loss=82.8361
	step [219/250], loss=80.0659
	step [220/250], loss=70.2445
	step [221/250], loss=73.3893
	step [222/250], loss=63.2740
	step [223/250], loss=76.1698
	step [224/250], loss=82.3482
	step [225/250], loss=88.3107
	step [226/250], loss=71.7324
	step [227/250], loss=73.7070
	step [228/250], loss=89.7861
	step [229/250], loss=65.2216
	step [230/250], loss=71.9302
	step [231/250], loss=76.9525
	step [232/250], loss=73.2350
	step [233/250], loss=68.1064
	step [234/250], loss=64.3944
	step [235/250], loss=92.6024
	step [236/250], loss=70.5734
	step [237/250], loss=83.0951
	step [238/250], loss=61.9878
	step [239/250], loss=93.3360
	step [240/250], loss=81.8275
	step [241/250], loss=76.3172
	step [242/250], loss=88.2556
	step [243/250], loss=70.5360
	step [244/250], loss=69.8020
	step [245/250], loss=90.4195
	step [246/250], loss=78.5564
	step [247/250], loss=73.0359
	step [248/250], loss=72.6186
	step [249/250], loss=107.8139
	step [250/250], loss=17.8168
	Evaluating
	loss=0.0079, precision=0.3034, recall=0.8685, f1=0.4497
Training epoch 75
	step [1/250], loss=82.8385
	step [2/250], loss=88.2682
	step [3/250], loss=73.1093
	step [4/250], loss=96.3571
	step [5/250], loss=72.5652
	step [6/250], loss=79.8195
	step [7/250], loss=63.2379
	step [8/250], loss=78.0899
	step [9/250], loss=66.9372
	step [10/250], loss=67.2355
	step [11/250], loss=69.1725
	step [12/250], loss=77.2625
	step [13/250], loss=78.3488
	step [14/250], loss=77.3558
	step [15/250], loss=90.5988
	step [16/250], loss=89.7740
	step [17/250], loss=85.4604
	step [18/250], loss=73.8251
	step [19/250], loss=66.9934
	step [20/250], loss=72.2750
	step [21/250], loss=88.3083
	step [22/250], loss=75.5131
	step [23/250], loss=80.7391
	step [24/250], loss=83.1779
	step [25/250], loss=87.8870
	step [26/250], loss=72.0730
	step [27/250], loss=70.6502
	step [28/250], loss=79.2223
	step [29/250], loss=88.1159
	step [30/250], loss=63.2067
	step [31/250], loss=91.3571
	step [32/250], loss=82.0160
	step [33/250], loss=75.4882
	step [34/250], loss=78.5221
	step [35/250], loss=75.3602
	step [36/250], loss=73.9502
	step [37/250], loss=74.8425
	step [38/250], loss=77.4881
	step [39/250], loss=72.5137
	step [40/250], loss=72.3056
	step [41/250], loss=68.7686
	step [42/250], loss=62.0560
	step [43/250], loss=94.1468
	step [44/250], loss=66.5635
	step [45/250], loss=79.0913
	step [46/250], loss=72.3722
	step [47/250], loss=84.1824
	step [48/250], loss=73.7659
	step [49/250], loss=64.7953
	step [50/250], loss=81.6522
	step [51/250], loss=77.4108
	step [52/250], loss=69.1658
	step [53/250], loss=94.9231
	step [54/250], loss=73.0482
	step [55/250], loss=89.0674
	step [56/250], loss=85.5982
	step [57/250], loss=82.5199
	step [58/250], loss=82.0816
	step [59/250], loss=75.0287
	step [60/250], loss=72.7766
	step [61/250], loss=87.6102
	step [62/250], loss=75.8759
	step [63/250], loss=91.8882
	step [64/250], loss=70.9313
	step [65/250], loss=92.5724
	step [66/250], loss=101.0224
	step [67/250], loss=74.9705
	step [68/250], loss=90.7065
	step [69/250], loss=71.5203
	step [70/250], loss=82.7881
	step [71/250], loss=73.1685
	step [72/250], loss=73.7743
	step [73/250], loss=77.3423
	step [74/250], loss=99.7649
	step [75/250], loss=85.8094
	step [76/250], loss=70.2341
	step [77/250], loss=73.1287
	step [78/250], loss=68.3031
	step [79/250], loss=73.8795
	step [80/250], loss=75.3437
	step [81/250], loss=83.4368
	step [82/250], loss=84.7632
	step [83/250], loss=89.6168
	step [84/250], loss=86.2013
	step [85/250], loss=86.9245
	step [86/250], loss=83.1148
	step [87/250], loss=65.8439
	step [88/250], loss=82.7582
	step [89/250], loss=73.1300
	step [90/250], loss=81.8521
	step [91/250], loss=85.2031
	step [92/250], loss=83.9331
	step [93/250], loss=73.5940
	step [94/250], loss=85.1984
	step [95/250], loss=73.0249
	step [96/250], loss=84.6063
	step [97/250], loss=78.9717
	step [98/250], loss=71.5509
	step [99/250], loss=68.5451
	step [100/250], loss=63.3621
	step [101/250], loss=71.4065
	step [102/250], loss=80.9742
	step [103/250], loss=77.9449
	step [104/250], loss=90.6310
	step [105/250], loss=74.4045
	step [106/250], loss=85.1903
	step [107/250], loss=85.5334
	step [108/250], loss=66.3854
	step [109/250], loss=64.9819
	step [110/250], loss=77.7301
	step [111/250], loss=80.9541
	step [112/250], loss=73.9097
	step [113/250], loss=73.2140
	step [114/250], loss=88.1636
	step [115/250], loss=72.8745
	step [116/250], loss=78.9313
	step [117/250], loss=75.7120
	step [118/250], loss=89.0147
	step [119/250], loss=76.0571
	step [120/250], loss=77.5680
	step [121/250], loss=88.7737
	step [122/250], loss=86.5934
	step [123/250], loss=79.0360
	step [124/250], loss=78.2199
	step [125/250], loss=64.4448
	step [126/250], loss=81.7321
	step [127/250], loss=82.9202
	step [128/250], loss=89.9692
	step [129/250], loss=69.5439
	step [130/250], loss=71.5480
	step [131/250], loss=75.4798
	step [132/250], loss=88.4579
	step [133/250], loss=80.2270
	step [134/250], loss=83.5549
	step [135/250], loss=80.4259
	step [136/250], loss=71.1652
	step [137/250], loss=58.5189
	step [138/250], loss=88.9628
	step [139/250], loss=65.7241
	step [140/250], loss=80.7325
	step [141/250], loss=90.8328
	step [142/250], loss=76.6501
	step [143/250], loss=71.0240
	step [144/250], loss=75.9052
	step [145/250], loss=78.3087
	step [146/250], loss=80.6194
	step [147/250], loss=76.8975
	step [148/250], loss=70.6781
	step [149/250], loss=98.0568
	step [150/250], loss=65.1752
	step [151/250], loss=82.7276
	step [152/250], loss=83.6567
	step [153/250], loss=90.0433
	step [154/250], loss=73.3074
	step [155/250], loss=86.0063
	step [156/250], loss=88.2486
	step [157/250], loss=77.6343
	step [158/250], loss=89.5008
	step [159/250], loss=55.5810
	step [160/250], loss=66.6909
	step [161/250], loss=73.8051
	step [162/250], loss=75.7492
	step [163/250], loss=96.0501
	step [164/250], loss=86.7261
	step [165/250], loss=76.2590
	step [166/250], loss=86.4488
	step [167/250], loss=83.3660
	step [168/250], loss=72.8759
	step [169/250], loss=76.6760
	step [170/250], loss=84.7896
	step [171/250], loss=84.0305
	step [172/250], loss=78.9424
	step [173/250], loss=79.8753
	step [174/250], loss=84.6557
	step [175/250], loss=81.0383
	step [176/250], loss=70.0610
	step [177/250], loss=75.0434
	step [178/250], loss=71.9075
	step [179/250], loss=64.4502
	step [180/250], loss=81.7846
	step [181/250], loss=73.1786
	step [182/250], loss=81.7849
	step [183/250], loss=78.6114
	step [184/250], loss=87.4546
	step [185/250], loss=83.2651
	step [186/250], loss=70.0538
	step [187/250], loss=85.0901
	step [188/250], loss=84.5179
	step [189/250], loss=78.2443
	step [190/250], loss=62.6246
	step [191/250], loss=87.9025
	step [192/250], loss=66.9141
	step [193/250], loss=71.1445
	step [194/250], loss=97.2091
	step [195/250], loss=75.8823
	step [196/250], loss=76.2672
	step [197/250], loss=71.8426
	step [198/250], loss=78.0721
	step [199/250], loss=84.7825
	step [200/250], loss=77.5021
	step [201/250], loss=75.0003
	step [202/250], loss=74.2268
	step [203/250], loss=77.4813
	step [204/250], loss=78.9045
	step [205/250], loss=75.2716
	step [206/250], loss=68.9118
	step [207/250], loss=62.8474
	step [208/250], loss=71.4531
	step [209/250], loss=61.8594
	step [210/250], loss=91.9419
	step [211/250], loss=87.8047
	step [212/250], loss=70.7788
	step [213/250], loss=58.9941
	step [214/250], loss=87.5454
	step [215/250], loss=78.1301
	step [216/250], loss=68.9081
	step [217/250], loss=71.1107
	step [218/250], loss=91.2545
	step [219/250], loss=63.2495
	step [220/250], loss=75.8390
	step [221/250], loss=99.8168
	step [222/250], loss=76.8284
	step [223/250], loss=80.9906
	step [224/250], loss=74.8115
	step [225/250], loss=89.2939
	step [226/250], loss=74.5353
	step [227/250], loss=82.9728
	step [228/250], loss=84.7353
	step [229/250], loss=89.7560
	step [230/250], loss=78.5986
	step [231/250], loss=79.4381
	step [232/250], loss=84.3243
	step [233/250], loss=80.5173
	step [234/250], loss=62.9832
	step [235/250], loss=71.7176
	step [236/250], loss=77.3123
	step [237/250], loss=82.4660
	step [238/250], loss=79.2264
	step [239/250], loss=77.7200
	step [240/250], loss=72.9459
	step [241/250], loss=72.4442
	step [242/250], loss=78.5179
	step [243/250], loss=73.2103
	step [244/250], loss=78.8983
	step [245/250], loss=71.6812
	step [246/250], loss=59.8513
	step [247/250], loss=78.1759
	step [248/250], loss=69.9689
	step [249/250], loss=99.2552
	step [250/250], loss=19.1353
	Evaluating
	loss=0.0093, precision=0.2477, recall=0.8671, f1=0.3853
Training epoch 76
	step [1/250], loss=84.7141
	step [2/250], loss=78.7554
	step [3/250], loss=93.2094
	step [4/250], loss=88.1921
	step [5/250], loss=65.8467
	step [6/250], loss=74.5194
	step [7/250], loss=70.8972
	step [8/250], loss=78.9446
	step [9/250], loss=67.8245
	step [10/250], loss=81.5864
	step [11/250], loss=75.5445
	step [12/250], loss=86.8355
	step [13/250], loss=84.7746
	step [14/250], loss=83.9970
	step [15/250], loss=77.8497
	step [16/250], loss=80.0454
	step [17/250], loss=89.1740
	step [18/250], loss=74.1021
	step [19/250], loss=64.7227
	step [20/250], loss=80.9378
	step [21/250], loss=67.7140
	step [22/250], loss=84.2781
	step [23/250], loss=95.3739
	step [24/250], loss=83.3008
	step [25/250], loss=69.2089
	step [26/250], loss=73.2345
	step [27/250], loss=77.4183
	step [28/250], loss=83.8365
	step [29/250], loss=53.7114
	step [30/250], loss=81.3960
	step [31/250], loss=89.2082
	step [32/250], loss=83.1969
	step [33/250], loss=84.6815
	step [34/250], loss=81.8853
	step [35/250], loss=66.1159
	step [36/250], loss=78.3399
	step [37/250], loss=68.9650
	step [38/250], loss=62.3278
	step [39/250], loss=71.9301
	step [40/250], loss=73.7308
	step [41/250], loss=72.1046
	step [42/250], loss=71.9092
	step [43/250], loss=86.6960
	step [44/250], loss=72.1701
	step [45/250], loss=77.7441
	step [46/250], loss=81.9570
	step [47/250], loss=81.4682
	step [48/250], loss=70.4673
	step [49/250], loss=70.2157
	step [50/250], loss=83.9430
	step [51/250], loss=72.2056
	step [52/250], loss=70.0409
	step [53/250], loss=84.0766
	step [54/250], loss=72.5301
	step [55/250], loss=68.3764
	step [56/250], loss=71.1547
	step [57/250], loss=90.7723
	step [58/250], loss=72.0040
	step [59/250], loss=83.5641
	step [60/250], loss=84.2791
	step [61/250], loss=78.1682
	step [62/250], loss=104.7053
	step [63/250], loss=77.1820
	step [64/250], loss=73.9391
	step [65/250], loss=69.6842
	step [66/250], loss=85.3226
	step [67/250], loss=80.1608
	step [68/250], loss=84.9163
	step [69/250], loss=84.8017
	step [70/250], loss=73.3906
	step [71/250], loss=79.6533
	step [72/250], loss=75.3336
	step [73/250], loss=66.8177
	step [74/250], loss=73.0170
	step [75/250], loss=78.1150
	step [76/250], loss=62.9363
	step [77/250], loss=99.5170
	step [78/250], loss=81.1802
	step [79/250], loss=86.4300
	step [80/250], loss=71.7030
	step [81/250], loss=82.0957
	step [82/250], loss=92.2917
	step [83/250], loss=80.4580
	step [84/250], loss=80.3876
	step [85/250], loss=86.7442
	step [86/250], loss=89.5257
	step [87/250], loss=93.1292
	step [88/250], loss=64.5709
	step [89/250], loss=83.5186
	step [90/250], loss=62.9478
	step [91/250], loss=78.5181
	step [92/250], loss=81.6415
	step [93/250], loss=66.1117
	step [94/250], loss=63.2814
	step [95/250], loss=75.8010
	step [96/250], loss=72.4465
	step [97/250], loss=94.0610
	step [98/250], loss=77.4036
	step [99/250], loss=86.0216
	step [100/250], loss=110.5724
	step [101/250], loss=73.1846
	step [102/250], loss=79.4950
	step [103/250], loss=83.9892
	step [104/250], loss=61.4689
	step [105/250], loss=83.9762
	step [106/250], loss=82.7762
	step [107/250], loss=82.1798
	step [108/250], loss=81.2261
	step [109/250], loss=79.9099
	step [110/250], loss=81.6556
	step [111/250], loss=79.1254
	step [112/250], loss=78.8571
	step [113/250], loss=71.9411
	step [114/250], loss=75.0463
	step [115/250], loss=83.8445
	step [116/250], loss=82.1259
	step [117/250], loss=63.8033
	step [118/250], loss=80.1294
	step [119/250], loss=74.6103
	step [120/250], loss=68.3148
	step [121/250], loss=61.1501
	step [122/250], loss=75.9801
	step [123/250], loss=65.1505
	step [124/250], loss=94.8090
	step [125/250], loss=76.7322
	step [126/250], loss=83.6823
	step [127/250], loss=88.2076
	step [128/250], loss=73.1083
	step [129/250], loss=85.5230
	step [130/250], loss=52.1053
	step [131/250], loss=81.6046
	step [132/250], loss=83.0369
	step [133/250], loss=60.8341
	step [134/250], loss=73.7405
	step [135/250], loss=77.6326
	step [136/250], loss=74.2708
	step [137/250], loss=70.9774
	step [138/250], loss=82.2694
	step [139/250], loss=79.4559
	step [140/250], loss=76.2222
	step [141/250], loss=82.4738
	step [142/250], loss=72.6993
	step [143/250], loss=93.2656
	step [144/250], loss=75.1101
	step [145/250], loss=75.9335
	step [146/250], loss=78.8280
	step [147/250], loss=75.7011
	step [148/250], loss=74.4849
	step [149/250], loss=84.9970
	step [150/250], loss=62.0806
	step [151/250], loss=68.2258
	step [152/250], loss=69.4708
	step [153/250], loss=85.6771
	step [154/250], loss=78.0849
	step [155/250], loss=82.1305
	step [156/250], loss=77.5421
	step [157/250], loss=77.7479
	step [158/250], loss=81.6099
	step [159/250], loss=89.8414
	step [160/250], loss=67.9830
	step [161/250], loss=66.5738
	step [162/250], loss=69.2186
	step [163/250], loss=73.1803
	step [164/250], loss=92.9131
	step [165/250], loss=84.9998
	step [166/250], loss=77.1782
	step [167/250], loss=88.0049
	step [168/250], loss=96.4221
	step [169/250], loss=81.9873
	step [170/250], loss=75.1917
	step [171/250], loss=64.6202
	step [172/250], loss=76.9079
	step [173/250], loss=86.0045
	step [174/250], loss=78.1743
	step [175/250], loss=81.8394
	step [176/250], loss=78.0091
	step [177/250], loss=73.1259
	step [178/250], loss=78.0347
	step [179/250], loss=58.7958
	step [180/250], loss=80.3117
	step [181/250], loss=73.2468
	step [182/250], loss=66.7373
	step [183/250], loss=76.5995
	step [184/250], loss=85.0945
	step [185/250], loss=78.1988
	step [186/250], loss=68.8454
	step [187/250], loss=69.9501
	step [188/250], loss=81.4395
	step [189/250], loss=97.7850
	step [190/250], loss=71.8451
	step [191/250], loss=87.6689
	step [192/250], loss=73.6134
	step [193/250], loss=89.5294
	step [194/250], loss=75.3624
	step [195/250], loss=78.8825
	step [196/250], loss=84.5600
	step [197/250], loss=87.1334
	step [198/250], loss=70.8628
	step [199/250], loss=85.9213
	step [200/250], loss=70.5100
	step [201/250], loss=73.4155
	step [202/250], loss=63.9438
	step [203/250], loss=74.4938
	step [204/250], loss=87.9268
	step [205/250], loss=102.4585
	step [206/250], loss=66.6695
	step [207/250], loss=67.5826
	step [208/250], loss=83.4719
	step [209/250], loss=73.9695
	step [210/250], loss=73.6389
	step [211/250], loss=56.7117
	step [212/250], loss=78.1475
	step [213/250], loss=99.6724
	step [214/250], loss=75.6618
	step [215/250], loss=98.1205
	step [216/250], loss=71.5959
	step [217/250], loss=61.9374
	step [218/250], loss=77.3105
	step [219/250], loss=63.4394
	step [220/250], loss=90.3791
	step [221/250], loss=85.9705
	step [222/250], loss=86.7426
	step [223/250], loss=73.3448
	step [224/250], loss=71.8781
	step [225/250], loss=91.4094
	step [226/250], loss=96.3428
	step [227/250], loss=76.9962
	step [228/250], loss=85.1358
	step [229/250], loss=74.5395
	step [230/250], loss=80.0106
	step [231/250], loss=77.3115
	step [232/250], loss=81.0669
	step [233/250], loss=82.7615
	step [234/250], loss=71.0413
	step [235/250], loss=71.9016
	step [236/250], loss=77.2658
	step [237/250], loss=88.9638
	step [238/250], loss=66.5795
	step [239/250], loss=75.1190
	step [240/250], loss=92.4310
	step [241/250], loss=71.1990
	step [242/250], loss=65.9427
	step [243/250], loss=86.5663
	step [244/250], loss=79.0259
	step [245/250], loss=81.8005
	step [246/250], loss=72.6815
	step [247/250], loss=72.1432
	step [248/250], loss=75.8851
	step [249/250], loss=87.9332
	step [250/250], loss=14.2544
	Evaluating
	loss=0.0077, precision=0.2950, recall=0.8542, f1=0.4385
Training epoch 77
	step [1/250], loss=60.2340
	step [2/250], loss=85.4908
	step [3/250], loss=74.3002
	step [4/250], loss=85.5412
	step [5/250], loss=66.0418
	step [6/250], loss=76.1609
	step [7/250], loss=77.3528
	step [8/250], loss=78.7034
	step [9/250], loss=81.4633
	step [10/250], loss=78.3232
	step [11/250], loss=74.6686
	step [12/250], loss=85.0219
	step [13/250], loss=78.5905
	step [14/250], loss=82.5421
	step [15/250], loss=78.7845
	step [16/250], loss=71.0791
	step [17/250], loss=87.2220
	step [18/250], loss=85.3894
	step [19/250], loss=94.3149
	step [20/250], loss=89.5138
	step [21/250], loss=73.2773
	step [22/250], loss=80.3764
	step [23/250], loss=53.5292
	step [24/250], loss=74.4051
	step [25/250], loss=76.8209
	step [26/250], loss=91.0829
	step [27/250], loss=96.7386
	step [28/250], loss=75.3546
	step [29/250], loss=76.6455
	step [30/250], loss=76.5818
	step [31/250], loss=71.7145
	step [32/250], loss=59.9695
	step [33/250], loss=79.1291
	step [34/250], loss=89.6297
	step [35/250], loss=66.2496
	step [36/250], loss=81.0590
	step [37/250], loss=58.9777
	step [38/250], loss=84.5773
	step [39/250], loss=88.7046
	step [40/250], loss=80.5515
	step [41/250], loss=82.3414
	step [42/250], loss=73.0996
	step [43/250], loss=68.6613
	step [44/250], loss=85.2130
	step [45/250], loss=88.1631
	step [46/250], loss=71.5456
	step [47/250], loss=82.0925
	step [48/250], loss=79.7855
	step [49/250], loss=79.9998
	step [50/250], loss=83.2823
	step [51/250], loss=74.8070
	step [52/250], loss=60.8462
	step [53/250], loss=78.3866
	step [54/250], loss=89.9023
	step [55/250], loss=90.7684
	step [56/250], loss=70.5243
	step [57/250], loss=74.5305
	step [58/250], loss=76.5394
	step [59/250], loss=67.4173
	step [60/250], loss=55.8747
	step [61/250], loss=83.6905
	step [62/250], loss=89.1886
	step [63/250], loss=71.6913
	step [64/250], loss=79.4693
	step [65/250], loss=72.8726
	step [66/250], loss=90.2335
	step [67/250], loss=78.0110
	step [68/250], loss=72.1418
	step [69/250], loss=71.4847
	step [70/250], loss=72.0963
	step [71/250], loss=90.7449
	step [72/250], loss=70.3855
	step [73/250], loss=76.8267
	step [74/250], loss=79.9627
	step [75/250], loss=80.1359
	step [76/250], loss=57.6876
	step [77/250], loss=75.7750
	step [78/250], loss=73.9699
	step [79/250], loss=76.5463
	step [80/250], loss=70.5626
	step [81/250], loss=67.0567
	step [82/250], loss=68.3564
	step [83/250], loss=84.8288
	step [84/250], loss=61.8025
	step [85/250], loss=86.2785
	step [86/250], loss=74.4560
	step [87/250], loss=82.6029
	step [88/250], loss=80.9878
	step [89/250], loss=66.7266
	step [90/250], loss=79.5010
	step [91/250], loss=73.4125
	step [92/250], loss=72.2268
	step [93/250], loss=74.1501
	step [94/250], loss=91.1498
	step [95/250], loss=74.6996
	step [96/250], loss=65.6208
	step [97/250], loss=76.9940
	step [98/250], loss=81.3874
	step [99/250], loss=79.8863
	step [100/250], loss=74.8548
	step [101/250], loss=64.8558
	step [102/250], loss=68.3804
	step [103/250], loss=88.3890
	step [104/250], loss=77.4739
	step [105/250], loss=70.0492
	step [106/250], loss=78.7380
	step [107/250], loss=81.9786
	step [108/250], loss=80.5540
	step [109/250], loss=77.1026
	step [110/250], loss=72.6508
	step [111/250], loss=82.5277
	step [112/250], loss=81.2866
	step [113/250], loss=85.1610
	step [114/250], loss=85.6073
	step [115/250], loss=79.4710
	step [116/250], loss=98.9783
	step [117/250], loss=97.9106
	step [118/250], loss=79.6405
	step [119/250], loss=77.5714
	step [120/250], loss=72.4577
	step [121/250], loss=81.4767
	step [122/250], loss=87.0542
	step [123/250], loss=75.1392
	step [124/250], loss=72.6385
	step [125/250], loss=83.8814
	step [126/250], loss=76.3143
	step [127/250], loss=78.3947
	step [128/250], loss=75.9875
	step [129/250], loss=61.0272
	step [130/250], loss=71.7325
	step [131/250], loss=87.4996
	step [132/250], loss=72.2519
	step [133/250], loss=86.0336
	step [134/250], loss=72.4223
	step [135/250], loss=67.6341
	step [136/250], loss=66.2626
	step [137/250], loss=74.1243
	step [138/250], loss=71.9466
	step [139/250], loss=81.8250
	step [140/250], loss=71.4636
	step [141/250], loss=70.9329
	step [142/250], loss=72.4176
	step [143/250], loss=78.8273
	step [144/250], loss=82.9317
	step [145/250], loss=69.8733
	step [146/250], loss=78.4439
	step [147/250], loss=72.1191
	step [148/250], loss=85.1541
	step [149/250], loss=77.2421
	step [150/250], loss=72.8767
	step [151/250], loss=79.7746
	step [152/250], loss=84.8200
	step [153/250], loss=80.7264
	step [154/250], loss=75.8501
	step [155/250], loss=92.0809
	step [156/250], loss=91.2254
	step [157/250], loss=70.7340
	step [158/250], loss=87.0643
	step [159/250], loss=78.7891
	step [160/250], loss=76.4293
	step [161/250], loss=75.7845
	step [162/250], loss=86.4438
	step [163/250], loss=79.6918
	step [164/250], loss=112.1024
	step [165/250], loss=79.9067
	step [166/250], loss=66.0223
	step [167/250], loss=67.8921
	step [168/250], loss=94.6822
	step [169/250], loss=78.9581
	step [170/250], loss=77.9127
	step [171/250], loss=75.0493
	step [172/250], loss=85.5039
	step [173/250], loss=78.4872
	step [174/250], loss=95.9919
	step [175/250], loss=77.2317
	step [176/250], loss=73.2391
	step [177/250], loss=65.3699
	step [178/250], loss=69.9890
	step [179/250], loss=80.3597
	step [180/250], loss=89.0802
	step [181/250], loss=101.0667
	step [182/250], loss=71.1603
	step [183/250], loss=74.7312
	step [184/250], loss=70.8888
	step [185/250], loss=88.5683
	step [186/250], loss=79.4038
	step [187/250], loss=92.7963
	step [188/250], loss=75.9085
	step [189/250], loss=83.1334
	step [190/250], loss=91.8234
	step [191/250], loss=80.0998
	step [192/250], loss=79.8810
	step [193/250], loss=55.8274
	step [194/250], loss=67.8529
	step [195/250], loss=91.5649
	step [196/250], loss=75.9569
	step [197/250], loss=69.8265
	step [198/250], loss=63.4848
	step [199/250], loss=72.0230
	step [200/250], loss=98.9282
	step [201/250], loss=85.9635
	step [202/250], loss=70.0530
	step [203/250], loss=78.1280
	step [204/250], loss=83.4265
	step [205/250], loss=66.5299
	step [206/250], loss=80.6700
	step [207/250], loss=83.0650
	step [208/250], loss=78.6444
	step [209/250], loss=62.8507
	step [210/250], loss=78.4887
	step [211/250], loss=62.2739
	step [212/250], loss=70.3358
	step [213/250], loss=56.7137
	step [214/250], loss=76.3763
	step [215/250], loss=62.2436
	step [216/250], loss=87.0268
	step [217/250], loss=76.5151
	step [218/250], loss=82.9711
	step [219/250], loss=91.8257
	step [220/250], loss=83.5011
	step [221/250], loss=91.1927
	step [222/250], loss=101.5570
	step [223/250], loss=71.3253
	step [224/250], loss=75.4535
	step [225/250], loss=71.4685
	step [226/250], loss=78.4008
	step [227/250], loss=90.9272
	step [228/250], loss=81.8749
	step [229/250], loss=72.0195
	step [230/250], loss=74.3293
	step [231/250], loss=78.3626
	step [232/250], loss=73.0342
	step [233/250], loss=80.4909
	step [234/250], loss=82.2215
	step [235/250], loss=79.6828
	step [236/250], loss=80.0076
	step [237/250], loss=54.2578
	step [238/250], loss=68.9091
	step [239/250], loss=78.3031
	step [240/250], loss=82.4728
	step [241/250], loss=74.6131
	step [242/250], loss=87.5368
	step [243/250], loss=72.8948
	step [244/250], loss=67.9400
	step [245/250], loss=71.0059
	step [246/250], loss=57.3256
	step [247/250], loss=79.6008
	step [248/250], loss=80.7901
	step [249/250], loss=87.9528
	step [250/250], loss=9.3436
	Evaluating
	loss=0.0087, precision=0.2586, recall=0.8584, f1=0.3974
Training epoch 78
	step [1/250], loss=83.2685
	step [2/250], loss=68.9439
	step [3/250], loss=85.3138
	step [4/250], loss=72.9649
	step [5/250], loss=92.0967
	step [6/250], loss=82.7679
	step [7/250], loss=82.5744
	step [8/250], loss=82.6879
	step [9/250], loss=78.5807
	step [10/250], loss=85.9205
	step [11/250], loss=96.6553
	step [12/250], loss=82.6618
	step [13/250], loss=58.6580
	step [14/250], loss=77.0955
	step [15/250], loss=74.2208
	step [16/250], loss=85.6949
	step [17/250], loss=80.1857
	step [18/250], loss=64.8332
	step [19/250], loss=80.2977
	step [20/250], loss=88.5695
	step [21/250], loss=63.2198
	step [22/250], loss=76.0953
	step [23/250], loss=75.6012
	step [24/250], loss=71.6555
	step [25/250], loss=59.8727
	step [26/250], loss=87.8150
	step [27/250], loss=68.9675
	step [28/250], loss=70.8457
	step [29/250], loss=70.8573
	step [30/250], loss=79.6342
	step [31/250], loss=68.5269
	step [32/250], loss=74.4541
	step [33/250], loss=70.8293
	step [34/250], loss=71.7408
	step [35/250], loss=82.9869
	step [36/250], loss=90.8961
	step [37/250], loss=75.9688
	step [38/250], loss=83.9115
	step [39/250], loss=69.2161
	step [40/250], loss=65.6573
	step [41/250], loss=77.1373
	step [42/250], loss=77.2982
	step [43/250], loss=64.4192
	step [44/250], loss=70.4515
	step [45/250], loss=80.7980
	step [46/250], loss=73.0609
	step [47/250], loss=66.2330
	step [48/250], loss=86.4875
	step [49/250], loss=77.1491
	step [50/250], loss=74.4792
	step [51/250], loss=69.0947
	step [52/250], loss=92.2826
	step [53/250], loss=70.6119
	step [54/250], loss=81.7224
	step [55/250], loss=86.8306
	step [56/250], loss=71.2102
	step [57/250], loss=75.1383
	step [58/250], loss=76.8680
	step [59/250], loss=80.1208
	step [60/250], loss=72.6553
	step [61/250], loss=71.8290
	step [62/250], loss=61.5103
	step [63/250], loss=71.5768
	step [64/250], loss=66.3750
	step [65/250], loss=82.1778
	step [66/250], loss=64.1169
	step [67/250], loss=74.7705
	step [68/250], loss=79.7479
	step [69/250], loss=75.9642
	step [70/250], loss=70.3268
	step [71/250], loss=67.5360
	step [72/250], loss=88.8778
	step [73/250], loss=65.7414
	step [74/250], loss=68.7287
	step [75/250], loss=76.9524
	step [76/250], loss=96.0804
	step [77/250], loss=68.1923
	step [78/250], loss=66.5826
	step [79/250], loss=93.4206
	step [80/250], loss=80.4938
	step [81/250], loss=87.3313
	step [82/250], loss=59.9712
	step [83/250], loss=71.7990
	step [84/250], loss=81.3636
	step [85/250], loss=84.7565
	step [86/250], loss=94.9519
	step [87/250], loss=70.5565
	step [88/250], loss=86.3140
	step [89/250], loss=85.8564
	step [90/250], loss=64.8918
	step [91/250], loss=60.4972
	step [92/250], loss=67.3541
	step [93/250], loss=63.1291
	step [94/250], loss=65.1197
	step [95/250], loss=79.0780
	step [96/250], loss=77.6887
	step [97/250], loss=88.0325
	step [98/250], loss=83.8416
	step [99/250], loss=87.5081
	step [100/250], loss=85.8894
	step [101/250], loss=81.1458
	step [102/250], loss=72.1078
	step [103/250], loss=73.1700
	step [104/250], loss=74.7979
	step [105/250], loss=73.0411
	step [106/250], loss=61.2108
	step [107/250], loss=81.0078
	step [108/250], loss=74.7170
	step [109/250], loss=75.3852
	step [110/250], loss=86.7944
	step [111/250], loss=80.9401
	step [112/250], loss=78.9372
	step [113/250], loss=75.9324
	step [114/250], loss=82.2529
	step [115/250], loss=76.3101
	step [116/250], loss=87.6344
	step [117/250], loss=93.0385
	step [118/250], loss=73.8943
	step [119/250], loss=72.7545
	step [120/250], loss=83.9646
	step [121/250], loss=71.8328
	step [122/250], loss=76.1053
	step [123/250], loss=83.4717
	step [124/250], loss=67.9303
	step [125/250], loss=68.2371
	step [126/250], loss=78.9208
	step [127/250], loss=82.8487
	step [128/250], loss=73.4757
	step [129/250], loss=64.3070
	step [130/250], loss=79.0992
	step [131/250], loss=78.7164
	step [132/250], loss=89.5059
	step [133/250], loss=88.9853
	step [134/250], loss=84.6576
	step [135/250], loss=86.2763
	step [136/250], loss=98.2824
	step [137/250], loss=76.7671
	step [138/250], loss=68.5632
	step [139/250], loss=75.7498
	step [140/250], loss=66.1895
	step [141/250], loss=67.1483
	step [142/250], loss=70.0214
	step [143/250], loss=73.4648
	step [144/250], loss=90.1704
	step [145/250], loss=77.5319
	step [146/250], loss=77.9472
	step [147/250], loss=76.1685
	step [148/250], loss=78.0986
	step [149/250], loss=85.0817
	step [150/250], loss=66.6767
	step [151/250], loss=68.6595
	step [152/250], loss=74.2817
	step [153/250], loss=91.2303
	step [154/250], loss=72.4768
	step [155/250], loss=67.5308
	step [156/250], loss=70.8225
	step [157/250], loss=85.1108
	step [158/250], loss=87.0125
	step [159/250], loss=85.9476
	step [160/250], loss=79.1740
	step [161/250], loss=67.1403
	step [162/250], loss=72.1869
	step [163/250], loss=76.1955
	step [164/250], loss=97.3696
	step [165/250], loss=80.6652
	step [166/250], loss=74.3481
	step [167/250], loss=74.8429
	step [168/250], loss=91.5440
	step [169/250], loss=84.2009
	step [170/250], loss=91.7868
	step [171/250], loss=69.2208
	step [172/250], loss=63.1182
	step [173/250], loss=75.9864
	step [174/250], loss=85.9605
	step [175/250], loss=88.5371
	step [176/250], loss=70.1181
	step [177/250], loss=73.3438
	step [178/250], loss=82.1364
	step [179/250], loss=77.4968
	step [180/250], loss=94.6317
	step [181/250], loss=78.9485
	step [182/250], loss=76.6382
	step [183/250], loss=77.1918
	step [184/250], loss=70.4349
	step [185/250], loss=65.3784
	step [186/250], loss=100.0126
	step [187/250], loss=61.8235
	step [188/250], loss=79.3634
	step [189/250], loss=71.6411
	step [190/250], loss=78.8144
	step [191/250], loss=58.5185
	step [192/250], loss=91.7923
	step [193/250], loss=84.8651
	step [194/250], loss=87.1321
	step [195/250], loss=59.2522
	step [196/250], loss=63.9887
	step [197/250], loss=77.2393
	step [198/250], loss=79.7687
	step [199/250], loss=72.6118
	step [200/250], loss=87.4861
	step [201/250], loss=71.4804
	step [202/250], loss=82.5772
	step [203/250], loss=67.4727
	step [204/250], loss=79.3070
	step [205/250], loss=77.7666
	step [206/250], loss=86.9817
	step [207/250], loss=78.4737
	step [208/250], loss=73.7724
	step [209/250], loss=73.3870
	step [210/250], loss=78.6535
	step [211/250], loss=63.9976
	step [212/250], loss=83.4494
	step [213/250], loss=65.8709
	step [214/250], loss=91.6166
	step [215/250], loss=73.4003
	step [216/250], loss=95.9474
	step [217/250], loss=62.2598
	step [218/250], loss=87.2171
	step [219/250], loss=67.5982
	step [220/250], loss=74.0970
	step [221/250], loss=85.6908
	step [222/250], loss=77.5632
	step [223/250], loss=66.5494
	step [224/250], loss=81.2034
	step [225/250], loss=78.6752
	step [226/250], loss=75.2244
	step [227/250], loss=82.8299
	step [228/250], loss=77.0794
	step [229/250], loss=75.8531
	step [230/250], loss=67.6018
	step [231/250], loss=89.5121
	step [232/250], loss=98.7161
	step [233/250], loss=63.5907
	step [234/250], loss=80.6847
	step [235/250], loss=86.6113
	step [236/250], loss=79.3452
	step [237/250], loss=76.3039
	step [238/250], loss=77.0324
	step [239/250], loss=76.9479
	step [240/250], loss=70.2655
	step [241/250], loss=100.7012
	step [242/250], loss=77.9995
	step [243/250], loss=87.4769
	step [244/250], loss=85.2373
	step [245/250], loss=89.2013
	step [246/250], loss=89.7941
	step [247/250], loss=60.5621
	step [248/250], loss=67.2579
	step [249/250], loss=101.8763
	step [250/250], loss=19.4130
	Evaluating
	loss=0.0090, precision=0.2469, recall=0.8657, f1=0.3842
Training epoch 79
	step [1/250], loss=81.4770
	step [2/250], loss=86.2544
	step [3/250], loss=69.2314
	step [4/250], loss=65.5594
	step [5/250], loss=83.2288
	step [6/250], loss=84.9163
	step [7/250], loss=73.1342
	step [8/250], loss=56.6037
	step [9/250], loss=66.3884
	step [10/250], loss=102.9610
	step [11/250], loss=79.1790
	step [12/250], loss=83.4656
	step [13/250], loss=61.1827
	step [14/250], loss=87.4191
	step [15/250], loss=74.1561
	step [16/250], loss=67.3386
	step [17/250], loss=77.4185
	step [18/250], loss=73.5849
	step [19/250], loss=83.4795
	step [20/250], loss=75.7620
	step [21/250], loss=82.1065
	step [22/250], loss=84.0367
	step [23/250], loss=91.5215
	step [24/250], loss=71.5930
	step [25/250], loss=70.8933
	step [26/250], loss=60.8868
	step [27/250], loss=91.1020
	step [28/250], loss=78.4530
	step [29/250], loss=74.0136
	step [30/250], loss=73.2888
	step [31/250], loss=64.7799
	step [32/250], loss=56.3475
	step [33/250], loss=62.3382
	step [34/250], loss=94.4110
	step [35/250], loss=63.6829
	step [36/250], loss=97.9484
	step [37/250], loss=84.9531
	step [38/250], loss=82.5193
	step [39/250], loss=76.3517
	step [40/250], loss=73.1689
	step [41/250], loss=76.5810
	step [42/250], loss=84.0150
	step [43/250], loss=78.9433
	step [44/250], loss=73.7642
	step [45/250], loss=75.9383
	step [46/250], loss=71.8761
	step [47/250], loss=80.8480
	step [48/250], loss=85.7289
	step [49/250], loss=73.8801
	step [50/250], loss=81.8199
	step [51/250], loss=84.9728
	step [52/250], loss=86.5372
	step [53/250], loss=82.5720
	step [54/250], loss=87.3149
	step [55/250], loss=75.7651
	step [56/250], loss=89.9859
	step [57/250], loss=82.5753
	step [58/250], loss=78.1906
	step [59/250], loss=82.4013
	step [60/250], loss=70.8449
	step [61/250], loss=95.4064
	step [62/250], loss=77.8245
	step [63/250], loss=76.3241
	step [64/250], loss=71.9231
	step [65/250], loss=70.5914
	step [66/250], loss=86.3099
	step [67/250], loss=95.1107
	step [68/250], loss=83.1653
	step [69/250], loss=67.3917
	step [70/250], loss=77.2617
	step [71/250], loss=80.4830
	step [72/250], loss=81.1939
	step [73/250], loss=60.9399
	step [74/250], loss=71.8525
	step [75/250], loss=73.1327
	step [76/250], loss=94.0925
	step [77/250], loss=72.4999
	step [78/250], loss=85.9678
	step [79/250], loss=80.0432
	step [80/250], loss=77.7506
	step [81/250], loss=72.9383
	step [82/250], loss=74.2578
	step [83/250], loss=84.3981
	step [84/250], loss=78.8981
	step [85/250], loss=82.9281
	step [86/250], loss=68.5310
	step [87/250], loss=76.0413
	step [88/250], loss=87.3899
	step [89/250], loss=79.5957
	step [90/250], loss=76.4668
	step [91/250], loss=74.2970
	step [92/250], loss=81.8791
	step [93/250], loss=81.5591
	step [94/250], loss=54.5279
	step [95/250], loss=79.0283
	step [96/250], loss=79.4788
	step [97/250], loss=68.5572
	step [98/250], loss=77.2922
	step [99/250], loss=88.0339
	step [100/250], loss=79.4980
	step [101/250], loss=73.1454
	step [102/250], loss=64.4056
	step [103/250], loss=72.7403
	step [104/250], loss=64.3772
	step [105/250], loss=83.0827
	step [106/250], loss=83.6843
	step [107/250], loss=73.9053
	step [108/250], loss=72.5842
	step [109/250], loss=86.9982
	step [110/250], loss=79.9251
	step [111/250], loss=62.1516
	step [112/250], loss=81.5785
	step [113/250], loss=68.6324
	step [114/250], loss=68.1879
	step [115/250], loss=96.8245
	step [116/250], loss=71.7185
	step [117/250], loss=69.4550
	step [118/250], loss=80.3438
	step [119/250], loss=76.2844
	step [120/250], loss=93.2572
	step [121/250], loss=60.9839
	step [122/250], loss=70.8913
	step [123/250], loss=70.4973
	step [124/250], loss=62.0788
	step [125/250], loss=85.4062
	step [126/250], loss=69.6584
	step [127/250], loss=87.7055
	step [128/250], loss=81.7164
	step [129/250], loss=69.4786
	step [130/250], loss=86.2113
	step [131/250], loss=96.9443
	step [132/250], loss=77.0260
	step [133/250], loss=71.2124
	step [134/250], loss=67.9579
	step [135/250], loss=99.8258
	step [136/250], loss=71.9283
	step [137/250], loss=77.4987
	step [138/250], loss=78.2361
	step [139/250], loss=81.0738
	step [140/250], loss=76.0984
	step [141/250], loss=74.5935
	step [142/250], loss=83.8297
	step [143/250], loss=60.2777
	step [144/250], loss=77.9188
	step [145/250], loss=70.5532
	step [146/250], loss=80.6029
	step [147/250], loss=80.0308
	step [148/250], loss=85.8231
	step [149/250], loss=66.4814
	step [150/250], loss=80.4244
	step [151/250], loss=74.5258
	step [152/250], loss=68.1479
	step [153/250], loss=68.2162
	step [154/250], loss=77.0133
	step [155/250], loss=72.2835
	step [156/250], loss=76.4569
	step [157/250], loss=81.8620
	step [158/250], loss=68.8717
	step [159/250], loss=84.8605
	step [160/250], loss=90.3114
	step [161/250], loss=66.3435
	step [162/250], loss=74.9733
	step [163/250], loss=57.5608
	step [164/250], loss=95.6010
	step [165/250], loss=75.5278
	step [166/250], loss=79.6102
	step [167/250], loss=85.8024
	step [168/250], loss=80.8021
	step [169/250], loss=81.8704
	step [170/250], loss=72.1287
	step [171/250], loss=71.3190
	step [172/250], loss=81.0997
	step [173/250], loss=63.7928
	step [174/250], loss=72.9586
	step [175/250], loss=57.1768
	step [176/250], loss=80.9268
	step [177/250], loss=74.3603
	step [178/250], loss=77.3625
	step [179/250], loss=79.0393
	step [180/250], loss=76.6692
	step [181/250], loss=93.5168
	step [182/250], loss=78.4358
	step [183/250], loss=71.0403
	step [184/250], loss=89.4995
	step [185/250], loss=93.9352
	step [186/250], loss=70.0728
	step [187/250], loss=70.0655
	step [188/250], loss=79.5200
	step [189/250], loss=63.2401
	step [190/250], loss=63.8098
	step [191/250], loss=85.2535
	step [192/250], loss=71.3210
	step [193/250], loss=83.6137
	step [194/250], loss=76.0646
	step [195/250], loss=81.2960
	step [196/250], loss=92.6271
	step [197/250], loss=82.7775
	step [198/250], loss=90.3822
	step [199/250], loss=65.5447
	step [200/250], loss=80.4261
	step [201/250], loss=72.1703
	step [202/250], loss=72.6251
	step [203/250], loss=79.7237
	step [204/250], loss=82.8077
	step [205/250], loss=78.4689
	step [206/250], loss=72.5455
	step [207/250], loss=80.5198
	step [208/250], loss=92.1177
	step [209/250], loss=79.6028
	step [210/250], loss=71.7409
	step [211/250], loss=93.1120
	step [212/250], loss=60.7987
	step [213/250], loss=65.0896
	step [214/250], loss=75.5314
	step [215/250], loss=68.2081
	step [216/250], loss=79.5235
	step [217/250], loss=85.9817
	step [218/250], loss=75.0464
	step [219/250], loss=90.4314
	step [220/250], loss=68.4790
	step [221/250], loss=72.0554
	step [222/250], loss=67.6303
	step [223/250], loss=78.1280
	step [224/250], loss=86.4538
	step [225/250], loss=70.8552
	step [226/250], loss=75.1999
	step [227/250], loss=73.3250
	step [228/250], loss=82.0472
	step [229/250], loss=89.3064
	step [230/250], loss=70.3375
	step [231/250], loss=79.9748
	step [232/250], loss=81.4055
	step [233/250], loss=80.3025
	step [234/250], loss=81.3253
	step [235/250], loss=73.0074
	step [236/250], loss=87.3333
	step [237/250], loss=76.8004
	step [238/250], loss=84.4615
	step [239/250], loss=93.1536
	step [240/250], loss=62.0217
	step [241/250], loss=70.2500
	step [242/250], loss=80.5414
	step [243/250], loss=76.6744
	step [244/250], loss=63.9805
	step [245/250], loss=72.2786
	step [246/250], loss=74.7132
	step [247/250], loss=87.3679
	step [248/250], loss=81.4658
	step [249/250], loss=72.2273
	step [250/250], loss=11.0788
	Evaluating
	loss=0.0106, precision=0.2074, recall=0.8827, f1=0.3359
Training epoch 80
	step [1/250], loss=68.6629
	step [2/250], loss=85.2967
	step [3/250], loss=87.0532
	step [4/250], loss=64.9160
	step [5/250], loss=71.9777
	step [6/250], loss=69.3364
	step [7/250], loss=87.5111
	step [8/250], loss=84.5113
	step [9/250], loss=77.5251
	step [10/250], loss=85.2672
	step [11/250], loss=67.8477
	step [12/250], loss=80.9914
	step [13/250], loss=74.0009
	step [14/250], loss=75.2779
	step [15/250], loss=80.6883
	step [16/250], loss=76.0650
	step [17/250], loss=70.5174
	step [18/250], loss=80.4433
	step [19/250], loss=84.9810
	step [20/250], loss=84.4349
	step [21/250], loss=72.8445
	step [22/250], loss=77.9839
	step [23/250], loss=83.5983
	step [24/250], loss=75.8582
	step [25/250], loss=72.2535
	step [26/250], loss=75.6136
	step [27/250], loss=82.8500
	step [28/250], loss=84.9948
	step [29/250], loss=67.5252
	step [30/250], loss=60.0868
	step [31/250], loss=79.4630
	step [32/250], loss=83.4273
	step [33/250], loss=79.7150
	step [34/250], loss=82.7367
	step [35/250], loss=90.0574
	step [36/250], loss=68.8577
	step [37/250], loss=72.3911
	step [38/250], loss=90.0638
	step [39/250], loss=76.5928
	step [40/250], loss=90.8481
	step [41/250], loss=66.4480
	step [42/250], loss=71.6113
	step [43/250], loss=77.7420
	step [44/250], loss=87.6420
	step [45/250], loss=73.5842
	step [46/250], loss=76.7750
	step [47/250], loss=74.4377
	step [48/250], loss=73.3799
	step [49/250], loss=73.5784
	step [50/250], loss=75.0459
	step [51/250], loss=68.6831
	step [52/250], loss=85.6866
	step [53/250], loss=92.3485
	step [54/250], loss=67.3403
	step [55/250], loss=69.5996
	step [56/250], loss=75.2376
	step [57/250], loss=67.7576
	step [58/250], loss=79.6748
	step [59/250], loss=61.2620
	step [60/250], loss=78.8877
	step [61/250], loss=73.5241
	step [62/250], loss=63.8608
	step [63/250], loss=77.8573
	step [64/250], loss=80.9247
	step [65/250], loss=67.2216
	step [66/250], loss=78.0301
	step [67/250], loss=72.9124
	step [68/250], loss=68.5649
	step [69/250], loss=69.0644
	step [70/250], loss=75.8471
	step [71/250], loss=57.9507
	step [72/250], loss=84.3599
	step [73/250], loss=87.4920
	step [74/250], loss=92.9591
	step [75/250], loss=95.2890
	step [76/250], loss=60.9669
	step [77/250], loss=67.8827
	step [78/250], loss=107.2224
	step [79/250], loss=87.0702
	step [80/250], loss=80.6203
	step [81/250], loss=74.5230
	step [82/250], loss=73.3795
	step [83/250], loss=79.6907
	step [84/250], loss=68.1201
	step [85/250], loss=78.3848
	step [86/250], loss=82.0245
	step [87/250], loss=71.3969
	step [88/250], loss=68.3220
	step [89/250], loss=72.5949
	step [90/250], loss=70.8020
	step [91/250], loss=88.7589
	step [92/250], loss=81.9685
	step [93/250], loss=71.4712
	step [94/250], loss=69.8524
	step [95/250], loss=68.2050
	step [96/250], loss=75.5976
	step [97/250], loss=65.1540
	step [98/250], loss=70.5039
	step [99/250], loss=79.8382
	step [100/250], loss=92.7209
	step [101/250], loss=70.1464
	step [102/250], loss=70.7967
	step [103/250], loss=80.5056
	step [104/250], loss=78.3311
	step [105/250], loss=62.9679
	step [106/250], loss=65.2333
	step [107/250], loss=80.3938
	step [108/250], loss=88.1690
	step [109/250], loss=61.4852
	step [110/250], loss=71.5972
	step [111/250], loss=60.6599
	step [112/250], loss=102.5663
	step [113/250], loss=79.6411
	step [114/250], loss=80.1595
	step [115/250], loss=77.0062
	step [116/250], loss=72.1244
	step [117/250], loss=78.0464
	step [118/250], loss=72.5955
	step [119/250], loss=85.5676
	step [120/250], loss=76.9607
	step [121/250], loss=82.9323
	step [122/250], loss=87.2579
	step [123/250], loss=78.2346
	step [124/250], loss=68.4250
	step [125/250], loss=73.4250
	step [126/250], loss=77.2873
	step [127/250], loss=77.7252
	step [128/250], loss=78.6041
	step [129/250], loss=69.7059
	step [130/250], loss=90.7219
	step [131/250], loss=91.7458
	step [132/250], loss=76.4471
	step [133/250], loss=73.1999
	step [134/250], loss=88.9253
	step [135/250], loss=69.7686
	step [136/250], loss=89.9702
	step [137/250], loss=74.6316
	step [138/250], loss=85.0657
	step [139/250], loss=73.4505
	step [140/250], loss=84.6865
	step [141/250], loss=67.3090
	step [142/250], loss=89.4929
	step [143/250], loss=77.1256
	step [144/250], loss=75.7080
	step [145/250], loss=80.8502
	step [146/250], loss=93.8303
	step [147/250], loss=64.5928
	step [148/250], loss=51.0022
	step [149/250], loss=81.5945
	step [150/250], loss=73.8548
	step [151/250], loss=73.2868
	step [152/250], loss=72.8362
	step [153/250], loss=79.3638
	step [154/250], loss=73.6163
	step [155/250], loss=77.3071
	step [156/250], loss=76.3796
	step [157/250], loss=77.9845
	step [158/250], loss=81.3186
	step [159/250], loss=71.2124
	step [160/250], loss=82.9848
	step [161/250], loss=67.4747
	step [162/250], loss=74.1909
	step [163/250], loss=58.9841
	step [164/250], loss=88.3954
	step [165/250], loss=82.8355
	step [166/250], loss=80.5736
	step [167/250], loss=85.8595
	step [168/250], loss=74.3660
	step [169/250], loss=87.6061
	step [170/250], loss=70.3494
	step [171/250], loss=78.1569
	step [172/250], loss=71.9528
	step [173/250], loss=82.2122
	step [174/250], loss=83.8126
	step [175/250], loss=90.7880
	step [176/250], loss=82.8693
	step [177/250], loss=75.8676
	step [178/250], loss=77.4459
	step [179/250], loss=71.9832
	step [180/250], loss=80.2180
	step [181/250], loss=78.6783
	step [182/250], loss=76.2888
	step [183/250], loss=72.7584
	step [184/250], loss=86.2401
	step [185/250], loss=88.5248
	step [186/250], loss=89.8999
	step [187/250], loss=70.4384
	step [188/250], loss=80.9818
	step [189/250], loss=74.8586
	step [190/250], loss=87.1623
	step [191/250], loss=59.7171
	step [192/250], loss=85.3934
	step [193/250], loss=80.9224
	step [194/250], loss=81.6984
	step [195/250], loss=66.2435
	step [196/250], loss=75.8393
	step [197/250], loss=76.0968
	step [198/250], loss=78.5701
	step [199/250], loss=88.3711
	step [200/250], loss=79.2393
	step [201/250], loss=78.2038
	step [202/250], loss=89.8170
	step [203/250], loss=72.8815
	step [204/250], loss=77.0039
	step [205/250], loss=72.0460
	step [206/250], loss=66.7938
	step [207/250], loss=69.7309
	step [208/250], loss=75.3318
	step [209/250], loss=78.6727
	step [210/250], loss=69.4409
	step [211/250], loss=67.7892
	step [212/250], loss=79.5018
	step [213/250], loss=87.4012
	step [214/250], loss=76.1868
	step [215/250], loss=71.6483
	step [216/250], loss=86.8640
	step [217/250], loss=79.5414
	step [218/250], loss=60.9367
	step [219/250], loss=85.7096
	step [220/250], loss=88.4502
	step [221/250], loss=65.2511
	step [222/250], loss=73.8462
	step [223/250], loss=85.3137
	step [224/250], loss=74.0607
	step [225/250], loss=80.2450
	step [226/250], loss=84.0070
	step [227/250], loss=83.0626
	step [228/250], loss=89.3309
	step [229/250], loss=73.6098
	step [230/250], loss=69.0920
	step [231/250], loss=80.2241
	step [232/250], loss=64.1542
	step [233/250], loss=90.6323
	step [234/250], loss=101.4262
	step [235/250], loss=66.6179
	step [236/250], loss=92.2681
	step [237/250], loss=67.2973
	step [238/250], loss=61.9763
	step [239/250], loss=74.8887
	step [240/250], loss=70.7381
	step [241/250], loss=63.6181
	step [242/250], loss=78.4447
	step [243/250], loss=76.8038
	step [244/250], loss=69.0719
	step [245/250], loss=72.4551
	step [246/250], loss=83.1302
	step [247/250], loss=86.5923
	step [248/250], loss=107.1611
	step [249/250], loss=56.8554
	step [250/250], loss=12.5244
	Evaluating
	loss=0.0083, precision=0.2814, recall=0.8741, f1=0.4258
Training epoch 81
	step [1/250], loss=79.9476
	step [2/250], loss=77.7590
	step [3/250], loss=79.0811
	step [4/250], loss=71.8826
	step [5/250], loss=77.9063
	step [6/250], loss=79.5525
	step [7/250], loss=63.3065
	step [8/250], loss=66.6050
	step [9/250], loss=72.8212
	step [10/250], loss=64.1027
	step [11/250], loss=60.0658
	step [12/250], loss=81.3789
	step [13/250], loss=75.6788
	step [14/250], loss=78.4320
	step [15/250], loss=72.1506
	step [16/250], loss=75.9111
	step [17/250], loss=92.9783
	step [18/250], loss=81.8751
	step [19/250], loss=76.0369
	step [20/250], loss=68.7920
	step [21/250], loss=74.8483
	step [22/250], loss=70.8705
	step [23/250], loss=79.2775
	step [24/250], loss=60.5572
	step [25/250], loss=82.2840
	step [26/250], loss=74.0551
	step [27/250], loss=80.9964
	step [28/250], loss=67.6629
	step [29/250], loss=77.8884
	step [30/250], loss=81.5844
	step [31/250], loss=87.0530
	step [32/250], loss=94.6135
	step [33/250], loss=85.7491
	step [34/250], loss=67.9187
	step [35/250], loss=71.7235
	step [36/250], loss=69.5280
	step [37/250], loss=82.8122
	step [38/250], loss=82.5657
	step [39/250], loss=67.8772
	step [40/250], loss=86.0184
	step [41/250], loss=105.7333
	step [42/250], loss=79.8321
	step [43/250], loss=75.6190
	step [44/250], loss=94.2930
	step [45/250], loss=64.3579
	step [46/250], loss=71.5970
	step [47/250], loss=73.3399
	step [48/250], loss=74.6679
	step [49/250], loss=75.0513
	step [50/250], loss=59.9268
	step [51/250], loss=74.1659
	step [52/250], loss=73.0215
	step [53/250], loss=92.6977
	step [54/250], loss=76.2257
	step [55/250], loss=84.0003
	step [56/250], loss=80.4896
	step [57/250], loss=67.2779
	step [58/250], loss=70.7549
	step [59/250], loss=76.5920
	step [60/250], loss=84.7045
	step [61/250], loss=80.3890
	step [62/250], loss=65.6258
	step [63/250], loss=68.8455
	step [64/250], loss=72.6241
	step [65/250], loss=68.7797
	step [66/250], loss=85.2708
	step [67/250], loss=85.8264
	step [68/250], loss=79.9474
	step [69/250], loss=78.1621
	step [70/250], loss=85.3786
	step [71/250], loss=72.7462
	step [72/250], loss=67.5457
	step [73/250], loss=76.0370
	step [74/250], loss=82.5843
	step [75/250], loss=83.0712
	step [76/250], loss=66.9266
	step [77/250], loss=89.2859
	step [78/250], loss=82.9963
	step [79/250], loss=73.0273
	step [80/250], loss=81.5315
	step [81/250], loss=79.0836
	step [82/250], loss=71.8148
	step [83/250], loss=85.6349
	step [84/250], loss=76.9747
	step [85/250], loss=84.6504
	step [86/250], loss=68.9728
	step [87/250], loss=71.8188
	step [88/250], loss=80.5791
	step [89/250], loss=80.4804
	step [90/250], loss=78.2681
	step [91/250], loss=60.2746
	step [92/250], loss=84.2232
	step [93/250], loss=88.3028
	step [94/250], loss=73.1984
	step [95/250], loss=70.3141
	step [96/250], loss=74.1589
	step [97/250], loss=73.8906
	step [98/250], loss=81.9474
	step [99/250], loss=78.4502
	step [100/250], loss=67.0477
	step [101/250], loss=73.2211
	step [102/250], loss=74.3504
	step [103/250], loss=69.5164
	step [104/250], loss=76.6669
	step [105/250], loss=85.1682
	step [106/250], loss=78.1432
	step [107/250], loss=71.3272
	step [108/250], loss=79.6444
	step [109/250], loss=74.8688
	step [110/250], loss=66.0253
	step [111/250], loss=57.2284
	step [112/250], loss=82.5082
	step [113/250], loss=79.8012
	step [114/250], loss=78.2893
	step [115/250], loss=78.4063
	step [116/250], loss=68.9380
	step [117/250], loss=76.0615
	step [118/250], loss=85.5374
	step [119/250], loss=91.2942
	step [120/250], loss=65.3225
	step [121/250], loss=76.1040
	step [122/250], loss=65.3410
	step [123/250], loss=77.2314
	step [124/250], loss=72.9293
	step [125/250], loss=73.9870
	step [126/250], loss=69.6389
	step [127/250], loss=68.6207
	step [128/250], loss=85.4280
	step [129/250], loss=72.5340
	step [130/250], loss=73.0887
	step [131/250], loss=84.2349
	step [132/250], loss=75.1613
	step [133/250], loss=69.8893
	step [134/250], loss=73.2399
	step [135/250], loss=81.0255
	step [136/250], loss=76.1962
	step [137/250], loss=92.2577
	step [138/250], loss=89.8839
	step [139/250], loss=80.6726
	step [140/250], loss=94.2389
	step [141/250], loss=71.3750
	step [142/250], loss=69.1608
	step [143/250], loss=94.9294
	step [144/250], loss=83.1247
	step [145/250], loss=83.9405
	step [146/250], loss=101.9377
	step [147/250], loss=86.2908
	step [148/250], loss=92.9111
	step [149/250], loss=73.8142
	step [150/250], loss=58.9243
	step [151/250], loss=74.4230
	step [152/250], loss=74.1111
	step [153/250], loss=62.9083
	step [154/250], loss=72.5325
	step [155/250], loss=68.1837
	step [156/250], loss=72.3520
	step [157/250], loss=97.3241
	step [158/250], loss=78.6431
	step [159/250], loss=67.6078
	step [160/250], loss=89.0980
	step [161/250], loss=83.9560
	step [162/250], loss=85.0717
	step [163/250], loss=74.6893
	step [164/250], loss=78.8851
	step [165/250], loss=84.3447
	step [166/250], loss=85.8382
	step [167/250], loss=72.4819
	step [168/250], loss=77.1682
	step [169/250], loss=69.4711
	step [170/250], loss=61.0031
	step [171/250], loss=71.0178
	step [172/250], loss=88.7946
	step [173/250], loss=76.5842
	step [174/250], loss=73.7878
	step [175/250], loss=80.9546
	step [176/250], loss=78.4301
	step [177/250], loss=109.9171
	step [178/250], loss=106.9912
	step [179/250], loss=69.1233
	step [180/250], loss=82.8446
	step [181/250], loss=74.5469
	step [182/250], loss=75.4578
	step [183/250], loss=86.1776
	step [184/250], loss=62.8342
	step [185/250], loss=70.4811
	step [186/250], loss=72.1807
	step [187/250], loss=72.5987
	step [188/250], loss=95.5209
	step [189/250], loss=73.9491
	step [190/250], loss=69.3233
	step [191/250], loss=70.7374
	step [192/250], loss=88.6852
	step [193/250], loss=68.6461
	step [194/250], loss=69.4372
	step [195/250], loss=69.3181
	step [196/250], loss=75.1898
	step [197/250], loss=81.7888
	step [198/250], loss=80.0947
	step [199/250], loss=71.0110
	step [200/250], loss=88.3303
	step [201/250], loss=70.5702
	step [202/250], loss=57.4779
	step [203/250], loss=66.4199
	step [204/250], loss=75.0995
	step [205/250], loss=91.7030
	step [206/250], loss=75.1179
	step [207/250], loss=67.7406
	step [208/250], loss=69.4236
	step [209/250], loss=88.6898
	step [210/250], loss=60.8318
	step [211/250], loss=64.6579
	step [212/250], loss=72.4982
	step [213/250], loss=81.8365
	step [214/250], loss=73.2755
	step [215/250], loss=72.1861
	step [216/250], loss=61.0158
	step [217/250], loss=80.3854
	step [218/250], loss=77.8830
	step [219/250], loss=76.3137
	step [220/250], loss=78.8031
	step [221/250], loss=82.7179
	step [222/250], loss=81.3561
	step [223/250], loss=85.0158
	step [224/250], loss=67.1006
	step [225/250], loss=66.1727
	step [226/250], loss=99.7651
	step [227/250], loss=85.1945
	step [228/250], loss=62.7113
	step [229/250], loss=91.3227
	step [230/250], loss=67.1328
	step [231/250], loss=81.1326
	step [232/250], loss=58.1848
	step [233/250], loss=85.6026
	step [234/250], loss=80.0587
	step [235/250], loss=77.7168
	step [236/250], loss=84.7327
	step [237/250], loss=92.0431
	step [238/250], loss=87.1760
	step [239/250], loss=74.4441
	step [240/250], loss=80.5090
	step [241/250], loss=72.9799
	step [242/250], loss=59.4858
	step [243/250], loss=78.9862
	step [244/250], loss=75.6638
	step [245/250], loss=69.0179
	step [246/250], loss=69.9804
	step [247/250], loss=67.7872
	step [248/250], loss=83.8134
	step [249/250], loss=67.5197
	step [250/250], loss=16.3866
	Evaluating
	loss=0.0077, precision=0.2941, recall=0.8492, f1=0.4368
Training epoch 82
	step [1/250], loss=85.1942
	step [2/250], loss=85.1980
	step [3/250], loss=65.5244
	step [4/250], loss=76.7432
	step [5/250], loss=71.0105
	step [6/250], loss=60.2928
	step [7/250], loss=79.4514
	step [8/250], loss=78.5915
	step [9/250], loss=75.9644
	step [10/250], loss=85.5146
	step [11/250], loss=72.6533
	step [12/250], loss=67.7299
	step [13/250], loss=96.1472
	step [14/250], loss=82.2983
	step [15/250], loss=74.3109
	step [16/250], loss=74.2994
	step [17/250], loss=77.8747
	step [18/250], loss=76.0274
	step [19/250], loss=64.8187
	step [20/250], loss=82.0165
	step [21/250], loss=69.1776
	step [22/250], loss=66.4534
	step [23/250], loss=75.0614
	step [24/250], loss=82.5162
	step [25/250], loss=81.7192
	step [26/250], loss=62.4968
	step [27/250], loss=65.8075
	step [28/250], loss=71.2926
	step [29/250], loss=72.8372
	step [30/250], loss=77.5406
	step [31/250], loss=88.1871
	step [32/250], loss=78.4005
	step [33/250], loss=72.6899
	step [34/250], loss=84.3878
	step [35/250], loss=77.1135
	step [36/250], loss=71.0780
	step [37/250], loss=90.2980
	step [38/250], loss=69.7505
	step [39/250], loss=81.5443
	step [40/250], loss=64.2454
	step [41/250], loss=80.2425
	step [42/250], loss=86.0956
	step [43/250], loss=76.9161
	step [44/250], loss=82.8637
	step [45/250], loss=62.8870
	step [46/250], loss=75.5811
	step [47/250], loss=68.5438
	step [48/250], loss=66.2545
	step [49/250], loss=75.9279
	step [50/250], loss=85.4880
	step [51/250], loss=72.9017
	step [52/250], loss=76.1554
	step [53/250], loss=78.4541
	step [54/250], loss=71.1216
	step [55/250], loss=80.4080
	step [56/250], loss=80.5631
	step [57/250], loss=60.0115
	step [58/250], loss=65.8673
	step [59/250], loss=81.0785
	step [60/250], loss=69.4989
	step [61/250], loss=91.0318
	step [62/250], loss=90.9072
	step [63/250], loss=78.2960
	step [64/250], loss=72.8922
	step [65/250], loss=72.2789
	step [66/250], loss=77.7031
	step [67/250], loss=78.6935
	step [68/250], loss=80.8098
	step [69/250], loss=80.4007
	step [70/250], loss=89.1979
	step [71/250], loss=80.9854
	step [72/250], loss=73.9335
	step [73/250], loss=90.7349
	step [74/250], loss=88.3076
	step [75/250], loss=75.6858
	step [76/250], loss=65.7676
	step [77/250], loss=80.8184
	step [78/250], loss=80.9230
	step [79/250], loss=68.8504
	step [80/250], loss=87.3640
	step [81/250], loss=72.3323
	step [82/250], loss=72.5727
	step [83/250], loss=64.2370
	step [84/250], loss=69.9340
	step [85/250], loss=91.9596
	step [86/250], loss=76.2419
	step [87/250], loss=91.0786
	step [88/250], loss=72.0597
	step [89/250], loss=81.9492
	step [90/250], loss=75.5091
	step [91/250], loss=87.1394
	step [92/250], loss=71.5591
	step [93/250], loss=73.4564
	step [94/250], loss=71.5755
	step [95/250], loss=74.4795
	step [96/250], loss=67.4354
	step [97/250], loss=64.6573
	step [98/250], loss=63.5008
	step [99/250], loss=85.5466
	step [100/250], loss=68.2930
	step [101/250], loss=84.0502
	step [102/250], loss=72.7664
	step [103/250], loss=88.6590
	step [104/250], loss=81.9720
	step [105/250], loss=68.4755
	step [106/250], loss=73.0119
	step [107/250], loss=97.3684
	step [108/250], loss=73.4990
	step [109/250], loss=63.0579
	step [110/250], loss=84.8568
	step [111/250], loss=72.2265
	step [112/250], loss=78.3377
	step [113/250], loss=72.3048
	step [114/250], loss=77.8737
	step [115/250], loss=73.5535
	step [116/250], loss=83.0786
	step [117/250], loss=89.8163
	step [118/250], loss=81.6394
	step [119/250], loss=88.3442
	step [120/250], loss=70.0830
	step [121/250], loss=66.2222
	step [122/250], loss=86.1376
	step [123/250], loss=82.9723
	step [124/250], loss=78.9258
	step [125/250], loss=79.2849
	step [126/250], loss=84.2886
	step [127/250], loss=78.3720
	step [128/250], loss=82.7842
	step [129/250], loss=70.4519
	step [130/250], loss=80.2206
	step [131/250], loss=70.6237
	step [132/250], loss=77.0881
	step [133/250], loss=72.4972
	step [134/250], loss=79.8153
	step [135/250], loss=66.1058
	step [136/250], loss=95.3559
	step [137/250], loss=90.3795
	step [138/250], loss=79.7279
	step [139/250], loss=76.8913
	step [140/250], loss=72.1969
	step [141/250], loss=66.8814
	step [142/250], loss=73.2744
	step [143/250], loss=67.0819
	step [144/250], loss=80.2659
	step [145/250], loss=83.2879
	step [146/250], loss=73.4522
	step [147/250], loss=71.4162
	step [148/250], loss=62.4495
	step [149/250], loss=74.1609
	step [150/250], loss=96.7429
	step [151/250], loss=69.9192
	step [152/250], loss=65.8284
	step [153/250], loss=81.3561
	step [154/250], loss=64.2508
	step [155/250], loss=90.5737
	step [156/250], loss=62.2020
	step [157/250], loss=79.2376
	step [158/250], loss=86.5642
	step [159/250], loss=79.9591
	step [160/250], loss=78.9503
	step [161/250], loss=71.5488
	step [162/250], loss=67.9209
	step [163/250], loss=71.3411
	step [164/250], loss=68.8192
	step [165/250], loss=84.4492
	step [166/250], loss=71.2314
	step [167/250], loss=88.8902
	step [168/250], loss=65.3543
	step [169/250], loss=67.2815
	step [170/250], loss=52.4727
	step [171/250], loss=68.8377
	step [172/250], loss=80.2844
	step [173/250], loss=83.4491
	step [174/250], loss=79.3241
	step [175/250], loss=93.2651
	step [176/250], loss=72.2246
	step [177/250], loss=78.8180
	step [178/250], loss=79.0348
	step [179/250], loss=90.7680
	step [180/250], loss=65.4958
	step [181/250], loss=93.6444
	step [182/250], loss=70.6025
	step [183/250], loss=74.8638
	step [184/250], loss=77.5619
	step [185/250], loss=73.2751
	step [186/250], loss=86.5833
	step [187/250], loss=74.6633
	step [188/250], loss=75.4474
	step [189/250], loss=69.4500
	step [190/250], loss=66.0308
	step [191/250], loss=83.7695
	step [192/250], loss=76.9866
	step [193/250], loss=81.7026
	step [194/250], loss=76.3187
	step [195/250], loss=75.4048
	step [196/250], loss=102.4591
	step [197/250], loss=82.0789
	step [198/250], loss=69.5406
	step [199/250], loss=78.1446
	step [200/250], loss=75.9597
	step [201/250], loss=74.4649
	step [202/250], loss=69.7378
	step [203/250], loss=74.8240
	step [204/250], loss=85.2956
	step [205/250], loss=75.4605
	step [206/250], loss=77.6299
	step [207/250], loss=82.0100
	step [208/250], loss=83.2203
	step [209/250], loss=74.6622
	step [210/250], loss=79.5989
	step [211/250], loss=96.1746
	step [212/250], loss=63.1694
	step [213/250], loss=71.2969
	step [214/250], loss=78.7064
	step [215/250], loss=64.9189
	step [216/250], loss=81.9530
	step [217/250], loss=74.0170
	step [218/250], loss=82.9342
	step [219/250], loss=79.0724
	step [220/250], loss=84.1225
	step [221/250], loss=66.0712
	step [222/250], loss=81.8678
	step [223/250], loss=59.3349
	step [224/250], loss=74.8804
	step [225/250], loss=68.6192
	step [226/250], loss=71.0631
	step [227/250], loss=65.3987
	step [228/250], loss=77.4295
	step [229/250], loss=79.6988
	step [230/250], loss=72.7998
	step [231/250], loss=69.7634
	step [232/250], loss=81.2595
	step [233/250], loss=71.7175
	step [234/250], loss=79.1108
	step [235/250], loss=76.1480
	step [236/250], loss=77.8592
	step [237/250], loss=67.0366
	step [238/250], loss=70.9892
	step [239/250], loss=77.3980
	step [240/250], loss=85.3280
	step [241/250], loss=75.2728
	step [242/250], loss=63.5403
	step [243/250], loss=65.6709
	step [244/250], loss=70.1427
	step [245/250], loss=91.3854
	step [246/250], loss=66.7506
	step [247/250], loss=80.9828
	step [248/250], loss=71.7345
	step [249/250], loss=69.9561
	step [250/250], loss=16.0090
	Evaluating
	loss=0.0077, precision=0.2862, recall=0.8662, f1=0.4302
Training epoch 83
	step [1/250], loss=92.8463
	step [2/250], loss=77.1963
	step [3/250], loss=85.2200
	step [4/250], loss=71.5025
	step [5/250], loss=81.5085
	step [6/250], loss=74.0585
	step [7/250], loss=73.6436
	step [8/250], loss=68.8746
	step [9/250], loss=84.1344
	step [10/250], loss=73.2504
	step [11/250], loss=63.5128
	step [12/250], loss=76.6073
	step [13/250], loss=70.9827
	step [14/250], loss=67.2358
	step [15/250], loss=78.6682
	step [16/250], loss=77.0491
	step [17/250], loss=74.4931
	step [18/250], loss=65.9533
	step [19/250], loss=63.2416
	step [20/250], loss=65.7348
	step [21/250], loss=81.7823
	step [22/250], loss=91.1405
	step [23/250], loss=73.8298
	step [24/250], loss=72.3266
	step [25/250], loss=87.2473
	step [26/250], loss=68.7845
	step [27/250], loss=95.6583
	step [28/250], loss=62.3435
	step [29/250], loss=84.0851
	step [30/250], loss=77.5485
	step [31/250], loss=85.1811
	step [32/250], loss=74.0473
	step [33/250], loss=82.4618
	step [34/250], loss=72.1320
	step [35/250], loss=68.6219
	step [36/250], loss=81.3438
	step [37/250], loss=77.9872
	step [38/250], loss=75.7759
	step [39/250], loss=80.7536
	step [40/250], loss=66.6776
	step [41/250], loss=59.2287
	step [42/250], loss=69.8067
	step [43/250], loss=91.2309
	step [44/250], loss=78.3898
	step [45/250], loss=81.2574
	step [46/250], loss=68.8190
	step [47/250], loss=65.7288
	step [48/250], loss=91.7715
	step [49/250], loss=68.3958
	step [50/250], loss=72.4442
	step [51/250], loss=80.7782
	step [52/250], loss=107.7533
	step [53/250], loss=78.8068
	step [54/250], loss=84.2001
	step [55/250], loss=61.8239
	step [56/250], loss=67.0351
	step [57/250], loss=80.9224
	step [58/250], loss=72.3478
	step [59/250], loss=75.3712
	step [60/250], loss=82.2674
	step [61/250], loss=70.3127
	step [62/250], loss=72.6100
	step [63/250], loss=75.4781
	step [64/250], loss=79.3746
	step [65/250], loss=86.3638
	step [66/250], loss=74.9155
	step [67/250], loss=63.4258
	step [68/250], loss=74.0595
	step [69/250], loss=78.7210
	step [70/250], loss=70.6213
	step [71/250], loss=69.9523
	step [72/250], loss=71.5678
	step [73/250], loss=72.8661
	step [74/250], loss=80.7574
	step [75/250], loss=67.8360
	step [76/250], loss=76.9000
	step [77/250], loss=67.7149
	step [78/250], loss=74.6723
	step [79/250], loss=78.2290
	step [80/250], loss=70.3473
	step [81/250], loss=79.8131
	step [82/250], loss=76.4435
	step [83/250], loss=75.9960
	step [84/250], loss=91.2746
	step [85/250], loss=68.6334
	step [86/250], loss=73.2361
	step [87/250], loss=82.3682
	step [88/250], loss=78.7423
	step [89/250], loss=69.6691
	step [90/250], loss=71.1956
	step [91/250], loss=70.1852
	step [92/250], loss=74.7907
	step [93/250], loss=65.8863
	step [94/250], loss=78.8596
	step [95/250], loss=90.1929
	step [96/250], loss=67.3323
	step [97/250], loss=88.9616
	step [98/250], loss=69.9051
	step [99/250], loss=83.3864
	step [100/250], loss=74.2943
	step [101/250], loss=71.2257
	step [102/250], loss=68.5163
	step [103/250], loss=71.7707
	step [104/250], loss=75.3898
	step [105/250], loss=67.9913
	step [106/250], loss=75.3364
	step [107/250], loss=60.0163
	step [108/250], loss=76.4808
	step [109/250], loss=78.5740
	step [110/250], loss=89.0352
	step [111/250], loss=85.0035
	step [112/250], loss=81.5300
	step [113/250], loss=86.1736
	step [114/250], loss=74.9185
	step [115/250], loss=68.9758
	step [116/250], loss=76.6794
	step [117/250], loss=72.4493
	step [118/250], loss=55.0028
	step [119/250], loss=64.1005
	step [120/250], loss=78.2729
	step [121/250], loss=64.7677
	step [122/250], loss=74.4331
	step [123/250], loss=82.4447
	step [124/250], loss=64.9880
	step [125/250], loss=62.6836
	step [126/250], loss=94.9730
	step [127/250], loss=70.1642
	step [128/250], loss=88.7079
	step [129/250], loss=87.0487
	step [130/250], loss=80.4448
	step [131/250], loss=67.3727
	step [132/250], loss=74.0076
	step [133/250], loss=67.9186
	step [134/250], loss=70.7158
	step [135/250], loss=94.2398
	step [136/250], loss=62.4840
	step [137/250], loss=71.2038
	step [138/250], loss=92.2805
	step [139/250], loss=68.5743
	step [140/250], loss=73.4984
	step [141/250], loss=69.1717
	step [142/250], loss=75.1410
	step [143/250], loss=79.5596
	step [144/250], loss=72.3727
	step [145/250], loss=78.6820
	step [146/250], loss=78.5548
	step [147/250], loss=85.0710
	step [148/250], loss=80.2626
	step [149/250], loss=75.5156
	step [150/250], loss=70.2820
	step [151/250], loss=74.2464
	step [152/250], loss=73.2742
	step [153/250], loss=73.4630
	step [154/250], loss=89.2036
	step [155/250], loss=78.6566
	step [156/250], loss=70.1350
	step [157/250], loss=65.3930
	step [158/250], loss=63.9623
	step [159/250], loss=87.5650
	step [160/250], loss=72.7546
	step [161/250], loss=78.6655
	step [162/250], loss=68.7218
	step [163/250], loss=80.9709
	step [164/250], loss=83.8274
	step [165/250], loss=79.7523
	step [166/250], loss=67.4359
	step [167/250], loss=85.0311
	step [168/250], loss=83.3616
	step [169/250], loss=87.9186
	step [170/250], loss=77.3258
	step [171/250], loss=69.5585
	step [172/250], loss=67.6526
	step [173/250], loss=96.9295
	step [174/250], loss=87.7998
	step [175/250], loss=66.1725
	step [176/250], loss=72.6426
	step [177/250], loss=63.4273
	step [178/250], loss=82.5541
	step [179/250], loss=89.8822
	step [180/250], loss=73.9135
	step [181/250], loss=71.4971
	step [182/250], loss=78.5088
	step [183/250], loss=81.0783
	step [184/250], loss=63.1477
	step [185/250], loss=69.1557
	step [186/250], loss=61.4544
	step [187/250], loss=66.7455
	step [188/250], loss=66.8736
	step [189/250], loss=81.2639
	step [190/250], loss=89.9070
	step [191/250], loss=81.8848
	step [192/250], loss=68.0092
	step [193/250], loss=67.4153
	step [194/250], loss=68.5639
	step [195/250], loss=71.5152
	step [196/250], loss=80.1935
	step [197/250], loss=73.2255
	step [198/250], loss=70.1247
	step [199/250], loss=82.3357
	step [200/250], loss=66.4816
	step [201/250], loss=80.3571
	step [202/250], loss=69.6759
	step [203/250], loss=68.1646
	step [204/250], loss=74.9773
	step [205/250], loss=81.2652
	step [206/250], loss=81.5645
	step [207/250], loss=74.8864
	step [208/250], loss=93.0372
	step [209/250], loss=80.4828
	step [210/250], loss=61.2415
	step [211/250], loss=83.4968
	step [212/250], loss=87.6666
	step [213/250], loss=81.6719
	step [214/250], loss=107.8559
	step [215/250], loss=85.6057
	step [216/250], loss=79.1770
	step [217/250], loss=71.1470
	step [218/250], loss=62.9570
	step [219/250], loss=84.1810
	step [220/250], loss=78.8983
	step [221/250], loss=65.2315
	step [222/250], loss=74.5487
	step [223/250], loss=70.3862
	step [224/250], loss=78.6912
	step [225/250], loss=85.7045
	step [226/250], loss=63.8753
	step [227/250], loss=68.4164
	step [228/250], loss=90.2597
	step [229/250], loss=66.6053
	step [230/250], loss=70.0034
	step [231/250], loss=83.5914
	step [232/250], loss=85.7734
	step [233/250], loss=73.1475
	step [234/250], loss=60.9088
	step [235/250], loss=83.6851
	step [236/250], loss=79.0137
	step [237/250], loss=67.8299
	step [238/250], loss=71.1841
	step [239/250], loss=77.6854
	step [240/250], loss=84.6055
	step [241/250], loss=65.1350
	step [242/250], loss=95.2753
	step [243/250], loss=80.0175
	step [244/250], loss=80.8643
	step [245/250], loss=74.4822
	step [246/250], loss=84.2768
	step [247/250], loss=74.2398
	step [248/250], loss=86.3083
	step [249/250], loss=80.3887
	step [250/250], loss=18.8617
	Evaluating
	loss=0.0081, precision=0.2708, recall=0.8707, f1=0.4131
Training epoch 84
	step [1/250], loss=81.7004
	step [2/250], loss=76.0946
	step [3/250], loss=83.7988
	step [4/250], loss=67.9541
	step [5/250], loss=92.2122
	step [6/250], loss=68.0085
	step [7/250], loss=65.7708
	step [8/250], loss=77.2825
	step [9/250], loss=84.1065
	step [10/250], loss=64.3828
	step [11/250], loss=93.9502
	step [12/250], loss=83.9372
	step [13/250], loss=85.4588
	step [14/250], loss=77.2958
	step [15/250], loss=89.4723
	step [16/250], loss=80.7780
	step [17/250], loss=74.7360
	step [18/250], loss=64.8577
	step [19/250], loss=74.2037
	step [20/250], loss=73.7268
	step [21/250], loss=63.4849
	step [22/250], loss=74.3669
	step [23/250], loss=77.7018
	step [24/250], loss=77.6985
	step [25/250], loss=75.0439
	step [26/250], loss=90.0920
	step [27/250], loss=78.8778
	step [28/250], loss=86.0093
	step [29/250], loss=91.3868
	step [30/250], loss=81.4863
	step [31/250], loss=63.4450
	step [32/250], loss=87.7548
	step [33/250], loss=68.9113
	step [34/250], loss=84.1382
	step [35/250], loss=74.3443
	step [36/250], loss=81.2030
	step [37/250], loss=68.8354
	step [38/250], loss=78.1638
	step [39/250], loss=77.4469
	step [40/250], loss=80.8620
	step [41/250], loss=66.7749
	step [42/250], loss=89.9465
	step [43/250], loss=65.3601
	step [44/250], loss=86.1231
	step [45/250], loss=78.1942
	step [46/250], loss=65.0737
	step [47/250], loss=77.0131
	step [48/250], loss=73.5616
	step [49/250], loss=74.4159
	step [50/250], loss=81.3711
	step [51/250], loss=82.1424
	step [52/250], loss=72.2261
	step [53/250], loss=67.3300
	step [54/250], loss=66.2601
	step [55/250], loss=78.4789
	step [56/250], loss=87.4611
	step [57/250], loss=71.8762
	step [58/250], loss=83.6435
	step [59/250], loss=74.4440
	step [60/250], loss=93.8132
	step [61/250], loss=86.8933
	step [62/250], loss=76.7299
	step [63/250], loss=64.1103
	step [64/250], loss=82.4158
	step [65/250], loss=69.8269
	step [66/250], loss=68.6294
	step [67/250], loss=81.6786
	step [68/250], loss=84.1516
	step [69/250], loss=83.1596
	step [70/250], loss=84.3994
	step [71/250], loss=69.9869
	step [72/250], loss=69.4958
	step [73/250], loss=68.4971
	step [74/250], loss=68.9632
	step [75/250], loss=66.5036
	step [76/250], loss=79.2033
	step [77/250], loss=78.2231
	step [78/250], loss=75.5324
	step [79/250], loss=82.2803
	step [80/250], loss=83.3727
	step [81/250], loss=72.3992
	step [82/250], loss=76.2055
	step [83/250], loss=87.8355
	step [84/250], loss=67.4599
	step [85/250], loss=75.8311
	step [86/250], loss=81.4196
	step [87/250], loss=84.0114
	step [88/250], loss=69.6616
	step [89/250], loss=72.0737
	step [90/250], loss=60.5373
	step [91/250], loss=84.1993
	step [92/250], loss=81.0916
	step [93/250], loss=76.5121
	step [94/250], loss=77.4440
	step [95/250], loss=68.8457
	step [96/250], loss=74.1335
	step [97/250], loss=78.1519
	step [98/250], loss=76.8145
	step [99/250], loss=75.7032
	step [100/250], loss=80.3278
	step [101/250], loss=82.5945
	step [102/250], loss=64.5838
	step [103/250], loss=80.0729
	step [104/250], loss=70.7482
	step [105/250], loss=86.9495
	step [106/250], loss=82.9528
	step [107/250], loss=68.9220
	step [108/250], loss=82.8633
	step [109/250], loss=69.8260
	step [110/250], loss=75.7872
	step [111/250], loss=67.3947
	step [112/250], loss=76.1021
	step [113/250], loss=74.8345
	step [114/250], loss=76.8217
	step [115/250], loss=75.8973
	step [116/250], loss=65.1589
	step [117/250], loss=67.0771
	step [118/250], loss=73.8493
	step [119/250], loss=75.2086
	step [120/250], loss=76.2423
	step [121/250], loss=68.4480
	step [122/250], loss=81.0419
	step [123/250], loss=69.5816
	step [124/250], loss=68.2997
	step [125/250], loss=79.1866
	step [126/250], loss=67.9722
	step [127/250], loss=69.2757
	step [128/250], loss=85.4268
	step [129/250], loss=67.5045
	step [130/250], loss=68.4956
	step [131/250], loss=69.5171
	step [132/250], loss=86.4387
	step [133/250], loss=74.4694
	step [134/250], loss=80.2665
	step [135/250], loss=65.1906
	step [136/250], loss=67.3904
	step [137/250], loss=73.6747
	step [138/250], loss=76.0748
	step [139/250], loss=92.4476
	step [140/250], loss=87.2671
	step [141/250], loss=78.7182
	step [142/250], loss=67.3189
	step [143/250], loss=111.2899
	step [144/250], loss=99.0294
	step [145/250], loss=78.9274
	step [146/250], loss=59.8593
	step [147/250], loss=84.0027
	step [148/250], loss=67.8531
	step [149/250], loss=83.1659
	step [150/250], loss=70.6715
	step [151/250], loss=69.0088
	step [152/250], loss=80.3786
	step [153/250], loss=63.9822
	step [154/250], loss=78.7787
	step [155/250], loss=75.4845
	step [156/250], loss=77.0729
	step [157/250], loss=72.3182
	step [158/250], loss=67.2004
	step [159/250], loss=60.4739
	step [160/250], loss=84.5410
	step [161/250], loss=92.4865
	step [162/250], loss=79.0171
	step [163/250], loss=64.8702
	step [164/250], loss=78.4958
	step [165/250], loss=62.2231
	step [166/250], loss=74.2080
	step [167/250], loss=61.5398
	step [168/250], loss=95.8914
	step [169/250], loss=67.8815
	step [170/250], loss=66.9904
	step [171/250], loss=71.5934
	step [172/250], loss=84.1936
	step [173/250], loss=71.7141
	step [174/250], loss=73.2822
	step [175/250], loss=70.7767
	step [176/250], loss=81.5069
	step [177/250], loss=85.9199
	step [178/250], loss=71.6049
	step [179/250], loss=80.6233
	step [180/250], loss=73.4346
	step [181/250], loss=65.8128
	step [182/250], loss=63.7616
	step [183/250], loss=65.8786
	step [184/250], loss=77.2085
	step [185/250], loss=91.0905
	step [186/250], loss=65.3799
	step [187/250], loss=68.7605
	step [188/250], loss=82.7617
	step [189/250], loss=74.8318
	step [190/250], loss=74.5934
	step [191/250], loss=85.8542
	step [192/250], loss=86.6031
	step [193/250], loss=71.1022
	step [194/250], loss=70.5905
	step [195/250], loss=57.3101
	step [196/250], loss=83.4018
	step [197/250], loss=71.5489
	step [198/250], loss=79.0407
	step [199/250], loss=71.8982
	step [200/250], loss=87.1622
	step [201/250], loss=65.7773
	step [202/250], loss=89.3553
	step [203/250], loss=91.5399
	step [204/250], loss=80.6401
	step [205/250], loss=66.7608
	step [206/250], loss=84.4992
	step [207/250], loss=78.3410
	step [208/250], loss=74.0531
	step [209/250], loss=72.6603
	step [210/250], loss=71.9521
	step [211/250], loss=79.8071
	step [212/250], loss=63.1217
	step [213/250], loss=76.2025
	step [214/250], loss=71.0793
	step [215/250], loss=65.8399
	step [216/250], loss=69.0332
	step [217/250], loss=63.4704
	step [218/250], loss=80.8800
	step [219/250], loss=82.7046
	step [220/250], loss=78.0969
	step [221/250], loss=70.0451
	step [222/250], loss=81.4277
	step [223/250], loss=71.4511
	step [224/250], loss=65.2944
	step [225/250], loss=61.3733
	step [226/250], loss=82.7635
	step [227/250], loss=69.5620
	step [228/250], loss=78.0702
	step [229/250], loss=81.8078
	step [230/250], loss=75.5144
	step [231/250], loss=80.3257
	step [232/250], loss=60.2585
	step [233/250], loss=82.4694
	step [234/250], loss=69.9808
	step [235/250], loss=82.5672
	step [236/250], loss=63.0017
	step [237/250], loss=70.4130
	step [238/250], loss=94.0772
	step [239/250], loss=92.9625
	step [240/250], loss=74.6810
	step [241/250], loss=59.0708
	step [242/250], loss=75.4803
	step [243/250], loss=70.9148
	step [244/250], loss=72.6427
	step [245/250], loss=82.7917
	step [246/250], loss=94.4193
	step [247/250], loss=76.5204
	step [248/250], loss=87.7053
	step [249/250], loss=64.9275
	step [250/250], loss=7.9177
	Evaluating
	loss=0.0068, precision=0.3256, recall=0.8577, f1=0.4720
saving model as: 0_saved_model.pth
Training epoch 85
	step [1/250], loss=69.5061
	step [2/250], loss=71.5722
	step [3/250], loss=66.0187
	step [4/250], loss=77.0790
	step [5/250], loss=93.7491
	step [6/250], loss=67.5374
	step [7/250], loss=88.2935
	step [8/250], loss=70.5702
	step [9/250], loss=67.5582
	step [10/250], loss=73.0810
	step [11/250], loss=81.3090
	step [12/250], loss=70.3833
	step [13/250], loss=62.5322
	step [14/250], loss=65.0718
	step [15/250], loss=81.2220
	step [16/250], loss=71.1701
	step [17/250], loss=68.8545
	step [18/250], loss=77.6493
	step [19/250], loss=69.4849
	step [20/250], loss=71.5270
	step [21/250], loss=68.4840
	step [22/250], loss=69.7426
	step [23/250], loss=68.1272
	step [24/250], loss=76.8192
	step [25/250], loss=88.4406
	step [26/250], loss=67.6227
	step [27/250], loss=86.0039
	step [28/250], loss=77.9711
	step [29/250], loss=68.7581
	step [30/250], loss=78.4891
	step [31/250], loss=77.1235
	step [32/250], loss=92.0112
	step [33/250], loss=104.2212
	step [34/250], loss=87.0764
	step [35/250], loss=96.9857
	step [36/250], loss=72.6809
	step [37/250], loss=73.5247
	step [38/250], loss=80.9858
	step [39/250], loss=72.9827
	step [40/250], loss=77.9053
	step [41/250], loss=82.8376
	step [42/250], loss=74.9366
	step [43/250], loss=80.9541
	step [44/250], loss=85.6043
	step [45/250], loss=72.6387
	step [46/250], loss=76.6600
	step [47/250], loss=70.9773
	step [48/250], loss=70.3319
	step [49/250], loss=86.5927
	step [50/250], loss=84.0869
	step [51/250], loss=91.8863
	step [52/250], loss=74.6601
	step [53/250], loss=70.3260
	step [54/250], loss=55.5471
	step [55/250], loss=75.3261
	step [56/250], loss=68.7988
	step [57/250], loss=78.4899
	step [58/250], loss=69.3589
	step [59/250], loss=67.0995
	step [60/250], loss=67.0735
	step [61/250], loss=68.5081
	step [62/250], loss=70.9553
	step [63/250], loss=84.1152
	step [64/250], loss=73.6236
	step [65/250], loss=78.6271
	step [66/250], loss=78.1853
	step [67/250], loss=83.7807
	step [68/250], loss=82.7404
	step [69/250], loss=55.7440
	step [70/250], loss=91.3432
	step [71/250], loss=68.9858
	step [72/250], loss=80.1383
	step [73/250], loss=75.5253
	step [74/250], loss=71.7734
	step [75/250], loss=77.8601
	step [76/250], loss=87.7403
	step [77/250], loss=72.9220
	step [78/250], loss=66.0080
	step [79/250], loss=63.5146
	step [80/250], loss=63.3027
	step [81/250], loss=69.3011
	step [82/250], loss=85.0850
	step [83/250], loss=76.5033
	step [84/250], loss=61.8359
	step [85/250], loss=69.5272
	step [86/250], loss=60.8151
	step [87/250], loss=65.1667
	step [88/250], loss=65.2559
	step [89/250], loss=69.5301
	step [90/250], loss=76.0199
	step [91/250], loss=96.9309
	step [92/250], loss=69.3401
	step [93/250], loss=76.7734
	step [94/250], loss=88.5649
	step [95/250], loss=66.6988
	step [96/250], loss=79.3176
	step [97/250], loss=67.2216
	step [98/250], loss=74.1093
	step [99/250], loss=61.5458
	step [100/250], loss=71.2960
	step [101/250], loss=73.1236
	step [102/250], loss=86.5646
	step [103/250], loss=90.8574
	step [104/250], loss=66.6527
	step [105/250], loss=73.4817
	step [106/250], loss=75.5501
	step [107/250], loss=74.0684
	step [108/250], loss=71.8859
	step [109/250], loss=70.9694
	step [110/250], loss=75.5645
	step [111/250], loss=81.0843
	step [112/250], loss=84.9823
	step [113/250], loss=88.3852
	step [114/250], loss=70.9264
	step [115/250], loss=72.3822
	step [116/250], loss=73.4814
	step [117/250], loss=64.6541
	step [118/250], loss=76.8789
	step [119/250], loss=72.8078
	step [120/250], loss=73.4933
	step [121/250], loss=69.7175
	step [122/250], loss=75.7487
	step [123/250], loss=85.9264
	step [124/250], loss=96.7851
	step [125/250], loss=73.3709
	step [126/250], loss=91.9369
	step [127/250], loss=79.9619
	step [128/250], loss=76.7609
	step [129/250], loss=53.6742
	step [130/250], loss=74.1486
	step [131/250], loss=84.4552
	step [132/250], loss=77.6765
	step [133/250], loss=87.8315
	step [134/250], loss=65.1806
	step [135/250], loss=76.0060
	step [136/250], loss=65.1062
	step [137/250], loss=62.3715
	step [138/250], loss=75.3233
	step [139/250], loss=80.6185
	step [140/250], loss=78.8502
	step [141/250], loss=70.5939
	step [142/250], loss=73.7937
	step [143/250], loss=71.1135
	step [144/250], loss=78.2356
	step [145/250], loss=86.9634
	step [146/250], loss=70.1065
	step [147/250], loss=72.0335
	step [148/250], loss=73.9935
	step [149/250], loss=82.1230
	step [150/250], loss=79.7866
	step [151/250], loss=80.5052
	step [152/250], loss=65.5237
	step [153/250], loss=63.8471
	step [154/250], loss=71.9077
	step [155/250], loss=72.7463
	step [156/250], loss=69.1829
	step [157/250], loss=77.8621
	step [158/250], loss=85.0431
	step [159/250], loss=57.0102
	step [160/250], loss=79.9768
	step [161/250], loss=76.5640
	step [162/250], loss=78.3316
	step [163/250], loss=74.8855
	step [164/250], loss=90.7995
	step [165/250], loss=71.5499
	step [166/250], loss=84.3885
	step [167/250], loss=99.5813
	step [168/250], loss=75.8235
	step [169/250], loss=92.0837
	step [170/250], loss=99.2915
	step [171/250], loss=95.7723
	step [172/250], loss=71.4055
	step [173/250], loss=93.7996
	step [174/250], loss=65.7533
	step [175/250], loss=71.8281
	step [176/250], loss=68.3260
	step [177/250], loss=84.1718
	step [178/250], loss=68.6160
	step [179/250], loss=71.5842
	step [180/250], loss=68.1410
	step [181/250], loss=73.8174
	step [182/250], loss=79.1012
	step [183/250], loss=98.8091
	step [184/250], loss=74.1270
	step [185/250], loss=83.0085
	step [186/250], loss=57.6460
	step [187/250], loss=85.0074
	step [188/250], loss=76.4027
	step [189/250], loss=80.1718
	step [190/250], loss=68.4018
	step [191/250], loss=82.0395
	step [192/250], loss=67.9722
	step [193/250], loss=81.2033
	step [194/250], loss=66.2393
	step [195/250], loss=81.1328
	step [196/250], loss=84.2187
	step [197/250], loss=80.4269
	step [198/250], loss=94.8118
	step [199/250], loss=71.1848
	step [200/250], loss=80.7183
	step [201/250], loss=77.1427
	step [202/250], loss=86.5021
	step [203/250], loss=66.0131
	step [204/250], loss=80.0007
	step [205/250], loss=74.7243
	step [206/250], loss=77.4017
	step [207/250], loss=90.5471
	step [208/250], loss=74.6710
	step [209/250], loss=83.8348
	step [210/250], loss=78.8222
	step [211/250], loss=76.2707
	step [212/250], loss=75.6400
	step [213/250], loss=75.3240
	step [214/250], loss=85.0851
	step [215/250], loss=67.9951
	step [216/250], loss=76.5883
	step [217/250], loss=75.7524
	step [218/250], loss=69.4104
	step [219/250], loss=63.6264
	step [220/250], loss=73.1020
	step [221/250], loss=70.3287
	step [222/250], loss=66.7346
	step [223/250], loss=87.4612
	step [224/250], loss=74.4205
	step [225/250], loss=69.7311
	step [226/250], loss=72.3431
	step [227/250], loss=58.4939
	step [228/250], loss=59.0746
	step [229/250], loss=67.2128
	step [230/250], loss=79.4730
	step [231/250], loss=85.1121
	step [232/250], loss=87.6652
	step [233/250], loss=77.9866
	step [234/250], loss=81.0187
	step [235/250], loss=63.9869
	step [236/250], loss=72.9700
	step [237/250], loss=68.3788
	step [238/250], loss=60.3275
	step [239/250], loss=88.8481
	step [240/250], loss=61.8528
	step [241/250], loss=80.9950
	step [242/250], loss=71.4146
	step [243/250], loss=73.5807
	step [244/250], loss=75.6160
	step [245/250], loss=71.1356
	step [246/250], loss=68.4386
	step [247/250], loss=73.1268
	step [248/250], loss=68.7676
	step [249/250], loss=68.1055
	step [250/250], loss=13.4943
	Evaluating
	loss=0.0081, precision=0.2678, recall=0.8606, f1=0.4085
Training epoch 86
	step [1/250], loss=63.0988
	step [2/250], loss=86.8258
	step [3/250], loss=75.2118
	step [4/250], loss=75.7607
	step [5/250], loss=64.1132
	step [6/250], loss=91.1443
	step [7/250], loss=68.3640
	step [8/250], loss=81.0936
	step [9/250], loss=76.4760
	step [10/250], loss=69.4786
	step [11/250], loss=86.0339
	step [12/250], loss=60.4336
	step [13/250], loss=66.5894
	step [14/250], loss=84.9756
	step [15/250], loss=85.0177
	step [16/250], loss=71.3330
	step [17/250], loss=108.0381
	step [18/250], loss=61.5545
	step [19/250], loss=67.2677
	step [20/250], loss=65.6231
	step [21/250], loss=90.0724
	step [22/250], loss=77.5829
	step [23/250], loss=85.2529
	step [24/250], loss=74.4742
	step [25/250], loss=74.3775
	step [26/250], loss=84.4185
	step [27/250], loss=79.1965
	step [28/250], loss=79.5287
	step [29/250], loss=87.3879
	step [30/250], loss=67.2875
	step [31/250], loss=70.6783
	step [32/250], loss=64.5902
	step [33/250], loss=100.4550
	step [34/250], loss=68.4187
	step [35/250], loss=81.3955
	step [36/250], loss=64.5454
	step [37/250], loss=65.9028
	step [38/250], loss=74.6948
	step [39/250], loss=88.0770
	step [40/250], loss=73.0033
	step [41/250], loss=70.8159
	step [42/250], loss=67.3119
	step [43/250], loss=61.2352
	step [44/250], loss=86.9192
	step [45/250], loss=71.6925
	step [46/250], loss=77.8561
	step [47/250], loss=66.0206
	step [48/250], loss=80.8973
	step [49/250], loss=87.4201
	step [50/250], loss=89.1169
	step [51/250], loss=74.4680
	step [52/250], loss=73.4402
	step [53/250], loss=87.0864
	step [54/250], loss=65.9338
	step [55/250], loss=61.0665
	step [56/250], loss=80.6835
	step [57/250], loss=67.9586
	step [58/250], loss=73.9773
	step [59/250], loss=77.9115
	step [60/250], loss=76.0118
	step [61/250], loss=59.5049
	step [62/250], loss=77.7269
	step [63/250], loss=63.5650
	step [64/250], loss=75.6340
	step [65/250], loss=87.0847
	step [66/250], loss=75.7628
	step [67/250], loss=59.2188
	step [68/250], loss=70.9113
	step [69/250], loss=72.1909
	step [70/250], loss=75.9690
	step [71/250], loss=86.0706
	step [72/250], loss=62.2348
	step [73/250], loss=91.9823
	step [74/250], loss=70.6146
	step [75/250], loss=78.7722
	step [76/250], loss=51.5584
	step [77/250], loss=68.6900
	step [78/250], loss=72.3981
	step [79/250], loss=74.9156
	step [80/250], loss=82.8383
	step [81/250], loss=69.2460
	step [82/250], loss=80.1775
	step [83/250], loss=71.1229
	step [84/250], loss=84.6876
	step [85/250], loss=95.6655
	step [86/250], loss=66.3945
	step [87/250], loss=66.2705
	step [88/250], loss=82.4207
	step [89/250], loss=73.5791
	step [90/250], loss=75.3509
	step [91/250], loss=81.6794
	step [92/250], loss=65.0614
	step [93/250], loss=73.2860
	step [94/250], loss=69.5842
	step [95/250], loss=73.9169
	step [96/250], loss=67.8418
	step [97/250], loss=84.0323
	step [98/250], loss=68.0546
	step [99/250], loss=67.0927
	step [100/250], loss=74.0645
	step [101/250], loss=78.6672
	step [102/250], loss=74.6012
	step [103/250], loss=79.8817
	step [104/250], loss=80.9802
	step [105/250], loss=86.3783
	step [106/250], loss=68.6667
	step [107/250], loss=71.3655
	step [108/250], loss=86.7852
	step [109/250], loss=64.1963
	step [110/250], loss=80.9897
	step [111/250], loss=77.2739
	step [112/250], loss=77.8702
	step [113/250], loss=79.7321
	step [114/250], loss=68.8981
	step [115/250], loss=62.1965
	step [116/250], loss=79.2032
	step [117/250], loss=83.0658
	step [118/250], loss=98.4278
	step [119/250], loss=75.7391
	step [120/250], loss=83.0599
	step [121/250], loss=73.0169
	step [122/250], loss=84.9147
	step [123/250], loss=65.1915
	step [124/250], loss=83.5032
	step [125/250], loss=84.0243
	step [126/250], loss=76.6483
	step [127/250], loss=74.5048
	step [128/250], loss=73.4886
	step [129/250], loss=67.3747
	step [130/250], loss=83.0045
	step [131/250], loss=65.7828
	step [132/250], loss=59.9861
	step [133/250], loss=65.0477
	step [134/250], loss=84.7499
	step [135/250], loss=84.3383
	step [136/250], loss=95.0024
	step [137/250], loss=88.4224
	step [138/250], loss=76.3575
	step [139/250], loss=72.0900
	step [140/250], loss=64.2583
	step [141/250], loss=62.7263
	step [142/250], loss=66.8484
	step [143/250], loss=84.3591
	step [144/250], loss=79.6013
	step [145/250], loss=62.0578
	step [146/250], loss=70.7799
	step [147/250], loss=79.8502
	step [148/250], loss=77.0412
	step [149/250], loss=67.1873
	step [150/250], loss=82.9219
	step [151/250], loss=87.9718
	step [152/250], loss=58.9445
	step [153/250], loss=74.3063
	step [154/250], loss=82.9748
	step [155/250], loss=64.2866
	step [156/250], loss=74.1311
	step [157/250], loss=87.5047
	step [158/250], loss=76.6637
	step [159/250], loss=77.7948
	step [160/250], loss=72.8582
	step [161/250], loss=78.1563
	step [162/250], loss=76.4261
	step [163/250], loss=79.9095
	step [164/250], loss=61.4556
	step [165/250], loss=90.5070
	step [166/250], loss=81.5156
	step [167/250], loss=81.7671
	step [168/250], loss=65.7769
	step [169/250], loss=72.5417
	step [170/250], loss=72.5197
	step [171/250], loss=77.8505
	step [172/250], loss=75.4594
	step [173/250], loss=90.9938
	step [174/250], loss=56.0551
	step [175/250], loss=88.1200
	step [176/250], loss=85.5882
	step [177/250], loss=61.3416
	step [178/250], loss=68.6714
	step [179/250], loss=81.7824
	step [180/250], loss=72.3695
	step [181/250], loss=83.2611
	step [182/250], loss=81.3134
	step [183/250], loss=75.9792
	step [184/250], loss=82.6376
	step [185/250], loss=66.8272
	step [186/250], loss=84.4018
	step [187/250], loss=64.8748
	step [188/250], loss=73.1880
	step [189/250], loss=75.3401
	step [190/250], loss=79.6294
	step [191/250], loss=74.5704
	step [192/250], loss=81.9076
	step [193/250], loss=71.9798
	step [194/250], loss=80.5989
	step [195/250], loss=75.3949
	step [196/250], loss=69.3022
	step [197/250], loss=66.4868
	step [198/250], loss=80.6074
	step [199/250], loss=67.5839
	step [200/250], loss=76.0475
	step [201/250], loss=78.9180
	step [202/250], loss=68.3314
	step [203/250], loss=77.3815
	step [204/250], loss=72.1695
	step [205/250], loss=64.2700
	step [206/250], loss=68.5876
	step [207/250], loss=71.7122
	step [208/250], loss=69.9873
	step [209/250], loss=83.4404
	step [210/250], loss=73.7713
	step [211/250], loss=68.4340
	step [212/250], loss=53.3302
	step [213/250], loss=71.8887
	step [214/250], loss=76.7148
	step [215/250], loss=94.8685
	step [216/250], loss=75.6994
	step [217/250], loss=69.9047
	step [218/250], loss=82.8530
	step [219/250], loss=81.1378
	step [220/250], loss=84.1532
	step [221/250], loss=57.4689
	step [222/250], loss=84.4127
	step [223/250], loss=83.8520
	step [224/250], loss=65.5975
	step [225/250], loss=84.4796
	step [226/250], loss=73.9508
	step [227/250], loss=73.3105
	step [228/250], loss=83.6604
	step [229/250], loss=82.9551
	step [230/250], loss=68.3452
	step [231/250], loss=66.1331
	step [232/250], loss=80.7231
	step [233/250], loss=73.0710
	step [234/250], loss=69.5346
	step [235/250], loss=74.0782
	step [236/250], loss=65.9796
	step [237/250], loss=86.4593
	step [238/250], loss=78.5757
	step [239/250], loss=71.2683
	step [240/250], loss=71.9497
	step [241/250], loss=70.9431
	step [242/250], loss=90.2609
	step [243/250], loss=73.5363
	step [244/250], loss=80.8108
	step [245/250], loss=75.6301
	step [246/250], loss=60.3459
	step [247/250], loss=84.4797
	step [248/250], loss=72.7562
	step [249/250], loss=59.5378
	step [250/250], loss=14.1047
	Evaluating
	loss=0.0086, precision=0.2482, recall=0.8648, f1=0.3857
Training epoch 87
	step [1/250], loss=88.0424
	step [2/250], loss=87.6154
	step [3/250], loss=76.4042
	step [4/250], loss=77.2864
	step [5/250], loss=68.0714
	step [6/250], loss=70.4570
	step [7/250], loss=73.3591
	step [8/250], loss=91.4103
	step [9/250], loss=69.7866
	step [10/250], loss=89.9641
	step [11/250], loss=82.9776
	step [12/250], loss=83.8615
	step [13/250], loss=62.8625
	step [14/250], loss=77.0430
	step [15/250], loss=64.5729
	step [16/250], loss=74.3814
	step [17/250], loss=79.9856
	step [18/250], loss=75.3430
	step [19/250], loss=61.4527
	step [20/250], loss=64.2655
	step [21/250], loss=70.2975
	step [22/250], loss=70.8978
	step [23/250], loss=72.8437
	step [24/250], loss=79.2180
	step [25/250], loss=75.0999
	step [26/250], loss=76.8549
	step [27/250], loss=81.3916
	step [28/250], loss=72.0144
	step [29/250], loss=77.3690
	step [30/250], loss=85.4325
	step [31/250], loss=73.5184
	step [32/250], loss=69.8900
	step [33/250], loss=60.7870
	step [34/250], loss=82.2482
	step [35/250], loss=82.3829
	step [36/250], loss=50.9435
	step [37/250], loss=68.3183
	step [38/250], loss=87.2731
	step [39/250], loss=74.7260
	step [40/250], loss=81.9269
	step [41/250], loss=72.8263
	step [42/250], loss=67.0923
	step [43/250], loss=62.6721
	step [44/250], loss=66.8555
	step [45/250], loss=82.6079
	step [46/250], loss=64.2694
	step [47/250], loss=81.0643
	step [48/250], loss=87.0217
	step [49/250], loss=63.7663
	step [50/250], loss=63.7626
	step [51/250], loss=70.8713
	step [52/250], loss=61.2507
	step [53/250], loss=89.9633
	step [54/250], loss=73.8016
	step [55/250], loss=64.1445
	step [56/250], loss=69.3489
	step [57/250], loss=85.7967
	step [58/250], loss=70.6564
	step [59/250], loss=78.3516
	step [60/250], loss=72.5793
	step [61/250], loss=81.8105
	step [62/250], loss=66.6293
	step [63/250], loss=86.8833
	step [64/250], loss=76.3300
	step [65/250], loss=82.1176
	step [66/250], loss=59.9605
	step [67/250], loss=87.2590
	step [68/250], loss=73.4450
	step [69/250], loss=81.3548
	step [70/250], loss=63.4655
	step [71/250], loss=82.3450
	step [72/250], loss=66.5064
	step [73/250], loss=72.3483
	step [74/250], loss=71.2094
	step [75/250], loss=78.3620
	step [76/250], loss=72.6106
	step [77/250], loss=71.7018
	step [78/250], loss=84.6867
	step [79/250], loss=86.3222
	step [80/250], loss=71.8326
	step [81/250], loss=84.8894
	step [82/250], loss=82.0833
	step [83/250], loss=67.2732
	step [84/250], loss=63.4313
	step [85/250], loss=96.5665
	step [86/250], loss=77.7847
	step [87/250], loss=85.9122
	step [88/250], loss=69.0314
	step [89/250], loss=79.8979
	step [90/250], loss=73.8680
	step [91/250], loss=71.6548
	step [92/250], loss=82.7999
	step [93/250], loss=77.3517
	step [94/250], loss=83.2977
	step [95/250], loss=79.9793
	step [96/250], loss=59.4723
	step [97/250], loss=70.4099
	step [98/250], loss=69.9050
	step [99/250], loss=82.5005
	step [100/250], loss=68.2799
	step [101/250], loss=80.1558
	step [102/250], loss=78.3289
	step [103/250], loss=84.3919
	step [104/250], loss=57.5588
	step [105/250], loss=66.6362
	step [106/250], loss=79.6872
	step [107/250], loss=90.7844
	step [108/250], loss=67.1413
	step [109/250], loss=58.9677
	step [110/250], loss=88.9786
	step [111/250], loss=86.6467
	step [112/250], loss=55.6442
	step [113/250], loss=71.3892
	step [114/250], loss=79.9725
	step [115/250], loss=56.5669
	step [116/250], loss=72.9759
	step [117/250], loss=82.0930
	step [118/250], loss=67.9840
	step [119/250], loss=67.1632
	step [120/250], loss=59.9688
	step [121/250], loss=67.0805
	step [122/250], loss=69.2828
	step [123/250], loss=75.7238
	step [124/250], loss=87.3388
	step [125/250], loss=70.6968
	step [126/250], loss=76.9357
	step [127/250], loss=73.2477
	step [128/250], loss=71.6774
	step [129/250], loss=74.4922
	step [130/250], loss=79.6399
	step [131/250], loss=88.8611
	step [132/250], loss=85.1737
	step [133/250], loss=78.3297
	step [134/250], loss=71.6297
	step [135/250], loss=78.2455
	step [136/250], loss=77.3708
	step [137/250], loss=67.3060
	step [138/250], loss=78.9909
	step [139/250], loss=70.7363
	step [140/250], loss=68.3730
	step [141/250], loss=83.5270
	step [142/250], loss=79.9375
	step [143/250], loss=91.7882
	step [144/250], loss=56.5624
	step [145/250], loss=74.2574
	step [146/250], loss=80.4657
	step [147/250], loss=70.6252
	step [148/250], loss=74.0099
	step [149/250], loss=64.1007
	step [150/250], loss=73.6137
	step [151/250], loss=79.4403
	step [152/250], loss=89.4503
	step [153/250], loss=77.4506
	step [154/250], loss=76.3739
	step [155/250], loss=74.5511
	step [156/250], loss=59.9096
	step [157/250], loss=67.7328
	step [158/250], loss=84.5482
	step [159/250], loss=71.7025
	step [160/250], loss=87.8856
	step [161/250], loss=67.7890
	step [162/250], loss=88.4742
	step [163/250], loss=81.8939
	step [164/250], loss=65.2869
	step [165/250], loss=69.2978
	step [166/250], loss=94.1299
	step [167/250], loss=72.7074
	step [168/250], loss=96.4430
	step [169/250], loss=74.9551
	step [170/250], loss=79.0824
	step [171/250], loss=70.1022
	step [172/250], loss=69.1981
	step [173/250], loss=72.5990
	step [174/250], loss=82.0736
	step [175/250], loss=71.3340
	step [176/250], loss=65.5779
	step [177/250], loss=76.2815
	step [178/250], loss=86.1302
	step [179/250], loss=76.9845
	step [180/250], loss=65.7310
	step [181/250], loss=58.6997
	step [182/250], loss=70.2090
	step [183/250], loss=79.1053
	step [184/250], loss=72.6386
	step [185/250], loss=74.3823
	step [186/250], loss=71.5047
	step [187/250], loss=68.0408
	step [188/250], loss=74.6132
	step [189/250], loss=70.4426
	step [190/250], loss=86.4203
	step [191/250], loss=98.1292
	step [192/250], loss=66.8782
	step [193/250], loss=84.4847
	step [194/250], loss=78.7935
	step [195/250], loss=69.0977
	step [196/250], loss=76.5394
	step [197/250], loss=94.8612
	step [198/250], loss=71.8983
	step [199/250], loss=81.7112
	step [200/250], loss=73.2562
	step [201/250], loss=66.0181
	step [202/250], loss=83.7459
	step [203/250], loss=71.3884
	step [204/250], loss=73.7359
	step [205/250], loss=60.2813
	step [206/250], loss=75.0903
	step [207/250], loss=70.3811
	step [208/250], loss=74.1788
	step [209/250], loss=62.8907
	step [210/250], loss=62.3826
	step [211/250], loss=64.7394
	step [212/250], loss=73.1722
	step [213/250], loss=70.9964
	step [214/250], loss=90.0982
	step [215/250], loss=77.0745
	step [216/250], loss=69.4919
	step [217/250], loss=62.3911
	step [218/250], loss=77.0403
	step [219/250], loss=74.8524
	step [220/250], loss=70.7948
	step [221/250], loss=93.6181
	step [222/250], loss=74.5703
	step [223/250], loss=93.6520
	step [224/250], loss=77.4090
	step [225/250], loss=75.7630
	step [226/250], loss=80.6795
	step [227/250], loss=77.2543
	step [228/250], loss=72.1323
	step [229/250], loss=72.5946
	step [230/250], loss=77.5677
	step [231/250], loss=77.8528
	step [232/250], loss=75.0766
	step [233/250], loss=89.0895
	step [234/250], loss=66.6787
	step [235/250], loss=73.6636
	step [236/250], loss=75.2796
	step [237/250], loss=74.7986
	step [238/250], loss=87.9834
	step [239/250], loss=79.6805
	step [240/250], loss=72.3331
	step [241/250], loss=76.1630
	step [242/250], loss=67.3165
	step [243/250], loss=81.2923
	step [244/250], loss=68.8710
	step [245/250], loss=72.5105
	step [246/250], loss=99.9153
	step [247/250], loss=71.5512
	step [248/250], loss=65.9247
	step [249/250], loss=73.1195
	step [250/250], loss=18.5035
	Evaluating
	loss=0.0075, precision=0.3043, recall=0.8569, f1=0.4491
Training epoch 88
	step [1/250], loss=77.4507
	step [2/250], loss=87.4490
	step [3/250], loss=73.0041
	step [4/250], loss=79.1033
	step [5/250], loss=79.3202
	step [6/250], loss=77.1769
	step [7/250], loss=80.1634
	step [8/250], loss=63.4070
	step [9/250], loss=68.3660
	step [10/250], loss=86.7890
	step [11/250], loss=70.0135
	step [12/250], loss=74.6103
	step [13/250], loss=74.7892
	step [14/250], loss=66.4526
	step [15/250], loss=74.4918
	step [16/250], loss=76.2408
	step [17/250], loss=59.6964
	step [18/250], loss=64.9601
	step [19/250], loss=80.6844
	step [20/250], loss=82.0909
	step [21/250], loss=66.7591
	step [22/250], loss=76.5560
	step [23/250], loss=74.7278
	step [24/250], loss=89.5356
	step [25/250], loss=98.4025
	step [26/250], loss=77.5162
	step [27/250], loss=72.0404
	step [28/250], loss=74.2093
	step [29/250], loss=85.8417
	step [30/250], loss=63.6311
	step [31/250], loss=71.0369
	step [32/250], loss=72.4313
	step [33/250], loss=76.3408
	step [34/250], loss=55.2969
	step [35/250], loss=64.6233
	step [36/250], loss=69.5576
	step [37/250], loss=64.6121
	step [38/250], loss=78.7297
	step [39/250], loss=74.3664
	step [40/250], loss=77.0798
	step [41/250], loss=68.2851
	step [42/250], loss=73.0708
	step [43/250], loss=77.5735
	step [44/250], loss=80.0848
	step [45/250], loss=75.1633
	step [46/250], loss=80.3012
	step [47/250], loss=89.1355
	step [48/250], loss=72.6476
	step [49/250], loss=78.4628
	step [50/250], loss=69.0900
	step [51/250], loss=62.0494
	step [52/250], loss=83.5078
	step [53/250], loss=79.0376
	step [54/250], loss=81.2675
	step [55/250], loss=83.6232
	step [56/250], loss=69.1306
	step [57/250], loss=88.6141
	step [58/250], loss=69.4947
	step [59/250], loss=77.4143
	step [60/250], loss=66.1529
	step [61/250], loss=84.4504
	step [62/250], loss=58.9491
	step [63/250], loss=69.1485
	step [64/250], loss=76.3021
	step [65/250], loss=74.2498
	step [66/250], loss=74.7061
	step [67/250], loss=72.3042
	step [68/250], loss=66.9838
	step [69/250], loss=80.7125
	step [70/250], loss=76.9515
	step [71/250], loss=79.9136
	step [72/250], loss=72.8911
	step [73/250], loss=70.6496
	step [74/250], loss=67.0036
	step [75/250], loss=84.8987
	step [76/250], loss=78.6884
	step [77/250], loss=67.7542
	step [78/250], loss=77.0604
	step [79/250], loss=79.8633
	step [80/250], loss=84.3272
	step [81/250], loss=68.6693
	step [82/250], loss=76.2290
	step [83/250], loss=72.0523
	step [84/250], loss=91.9668
	step [85/250], loss=78.0572
	step [86/250], loss=81.4396
	step [87/250], loss=80.2430
	step [88/250], loss=64.5314
	step [89/250], loss=83.9413
	step [90/250], loss=62.9025
	step [91/250], loss=70.2769
	step [92/250], loss=80.8744
	step [93/250], loss=67.1498
	step [94/250], loss=63.8143
	step [95/250], loss=54.2015
	step [96/250], loss=72.9717
	step [97/250], loss=78.7437
	step [98/250], loss=67.7146
	step [99/250], loss=74.1644
	step [100/250], loss=75.6308
	step [101/250], loss=66.5419
	step [102/250], loss=94.5638
	step [103/250], loss=69.5362
	step [104/250], loss=91.8251
	step [105/250], loss=79.1435
	step [106/250], loss=69.2420
	step [107/250], loss=72.9747
	step [108/250], loss=81.7737
	step [109/250], loss=72.7792
	step [110/250], loss=85.0148
	step [111/250], loss=72.1192
	step [112/250], loss=71.1595
	step [113/250], loss=81.6590
	step [114/250], loss=54.9836
	step [115/250], loss=95.8663
	step [116/250], loss=66.9591
	step [117/250], loss=69.5351
	step [118/250], loss=72.2130
	step [119/250], loss=66.9076
	step [120/250], loss=66.4965
	step [121/250], loss=68.6841
	step [122/250], loss=93.0092
	step [123/250], loss=69.7240
	step [124/250], loss=71.8579
	step [125/250], loss=70.9207
	step [126/250], loss=81.9031
	step [127/250], loss=77.0215
	step [128/250], loss=82.1609
	step [129/250], loss=74.1264
	step [130/250], loss=73.0744
	step [131/250], loss=70.1993
	step [132/250], loss=79.5355
	step [133/250], loss=86.3994
	step [134/250], loss=77.6603
	step [135/250], loss=80.6158
	step [136/250], loss=79.1755
	step [137/250], loss=64.1241
	step [138/250], loss=65.1644
	step [139/250], loss=89.2440
	step [140/250], loss=80.3752
	step [141/250], loss=83.3945
	step [142/250], loss=75.8420
	step [143/250], loss=77.5303
	step [144/250], loss=68.1157
	step [145/250], loss=78.8176
	step [146/250], loss=69.7230
	step [147/250], loss=78.8588
	step [148/250], loss=61.7024
	step [149/250], loss=77.0316
	step [150/250], loss=73.2852
	step [151/250], loss=71.0560
	step [152/250], loss=74.1061
	step [153/250], loss=74.0762
	step [154/250], loss=94.3205
	step [155/250], loss=93.4695
	step [156/250], loss=70.3501
	step [157/250], loss=70.7373
	step [158/250], loss=78.6527
	step [159/250], loss=76.9800
	step [160/250], loss=85.7251
	step [161/250], loss=66.9020
	step [162/250], loss=74.2724
	step [163/250], loss=74.3670
	step [164/250], loss=66.8217
	step [165/250], loss=82.5665
	step [166/250], loss=88.3219
	step [167/250], loss=89.8920
	step [168/250], loss=75.1271
	step [169/250], loss=66.5201
	step [170/250], loss=60.9893
	step [171/250], loss=79.2081
	step [172/250], loss=70.9794
	step [173/250], loss=77.9832
	step [174/250], loss=75.0369
	step [175/250], loss=63.7209
	step [176/250], loss=80.3318
	step [177/250], loss=65.2789
	step [178/250], loss=74.6757
	step [179/250], loss=79.8445
	step [180/250], loss=89.8872
	step [181/250], loss=70.6903
	step [182/250], loss=58.1771
	step [183/250], loss=65.4208
	step [184/250], loss=68.2121
	step [185/250], loss=76.4594
	step [186/250], loss=79.6261
	step [187/250], loss=77.3127
	step [188/250], loss=69.0425
	step [189/250], loss=57.0101
	step [190/250], loss=87.0955
	step [191/250], loss=61.5987
	step [192/250], loss=63.5115
	step [193/250], loss=61.5165
	step [194/250], loss=94.9309
	step [195/250], loss=101.4479
	step [196/250], loss=71.6418
	step [197/250], loss=64.7435
	step [198/250], loss=87.2276
	step [199/250], loss=66.6865
	step [200/250], loss=58.5456
	step [201/250], loss=85.8868
	step [202/250], loss=78.1284
	step [203/250], loss=74.4923
	step [204/250], loss=76.6828
	step [205/250], loss=72.9937
	step [206/250], loss=60.7562
	step [207/250], loss=65.6299
	step [208/250], loss=71.5043
	step [209/250], loss=75.0643
	step [210/250], loss=83.8317
	step [211/250], loss=70.1539
	step [212/250], loss=77.8434
	step [213/250], loss=90.4495
	step [214/250], loss=76.3706
	step [215/250], loss=66.5968
	step [216/250], loss=72.4959
	step [217/250], loss=72.1629
	step [218/250], loss=79.2649
	step [219/250], loss=92.7773
	step [220/250], loss=75.1781
	step [221/250], loss=70.8808
	step [222/250], loss=88.6417
	step [223/250], loss=62.1637
	step [224/250], loss=69.8098
	step [225/250], loss=81.4307
	step [226/250], loss=60.8297
	step [227/250], loss=71.2562
	step [228/250], loss=71.0577
	step [229/250], loss=68.1151
	step [230/250], loss=64.3008
	step [231/250], loss=87.4830
	step [232/250], loss=87.5224
	step [233/250], loss=80.1258
	step [234/250], loss=62.1748
	step [235/250], loss=87.8708
	step [236/250], loss=80.1699
	step [237/250], loss=70.4652
	step [238/250], loss=75.3965
	step [239/250], loss=93.1988
	step [240/250], loss=73.0721
	step [241/250], loss=75.1556
	step [242/250], loss=65.9672
	step [243/250], loss=74.4175
	step [244/250], loss=71.4270
	step [245/250], loss=64.2328
	step [246/250], loss=75.2934
	step [247/250], loss=85.9390
	step [248/250], loss=63.4524
	step [249/250], loss=70.4374
	step [250/250], loss=14.0043
	Evaluating
	loss=0.0074, precision=0.3052, recall=0.8591, f1=0.4504
Training epoch 89
	step [1/250], loss=82.3383
	step [2/250], loss=65.7205
	step [3/250], loss=77.0333
	step [4/250], loss=85.6739
	step [5/250], loss=69.0048
	step [6/250], loss=57.9479
	step [7/250], loss=81.4682
	step [8/250], loss=81.9622
	step [9/250], loss=81.1153
	step [10/250], loss=84.1114
	step [11/250], loss=63.7614
	step [12/250], loss=53.9278
	step [13/250], loss=88.6545
	step [14/250], loss=73.0104
	step [15/250], loss=79.6517
	step [16/250], loss=103.2017
	step [17/250], loss=77.7918
	step [18/250], loss=78.0633
	step [19/250], loss=73.5304
	step [20/250], loss=61.1907
	step [21/250], loss=75.7869
	step [22/250], loss=90.2593
	step [23/250], loss=77.2411
	step [24/250], loss=61.1938
	step [25/250], loss=57.6118
	step [26/250], loss=70.9476
	step [27/250], loss=74.9868
	step [28/250], loss=78.6406
	step [29/250], loss=74.4452
	step [30/250], loss=74.5762
	step [31/250], loss=73.1257
	step [32/250], loss=68.4392
	step [33/250], loss=83.8203
	step [34/250], loss=68.4607
	step [35/250], loss=85.8432
	step [36/250], loss=65.6552
	step [37/250], loss=85.5616
	step [38/250], loss=63.9967
	step [39/250], loss=75.3728
	step [40/250], loss=75.3123
	step [41/250], loss=87.3087
	step [42/250], loss=76.1607
	step [43/250], loss=81.7250
	step [44/250], loss=90.6701
	step [45/250], loss=83.7677
	step [46/250], loss=78.8372
	step [47/250], loss=80.0857
	step [48/250], loss=73.0003
	step [49/250], loss=65.0197
	step [50/250], loss=75.8682
	step [51/250], loss=64.9965
	step [52/250], loss=71.9064
	step [53/250], loss=74.7920
	step [54/250], loss=73.3952
	step [55/250], loss=66.2749
	step [56/250], loss=77.4384
	step [57/250], loss=79.0623
	step [58/250], loss=75.0534
	step [59/250], loss=62.8885
	step [60/250], loss=84.6711
	step [61/250], loss=68.6218
	step [62/250], loss=88.7790
	step [63/250], loss=71.9756
	step [64/250], loss=69.3678
	step [65/250], loss=80.0186
	step [66/250], loss=71.2518
	step [67/250], loss=103.1505
	step [68/250], loss=65.3495
	step [69/250], loss=70.0871
	step [70/250], loss=73.1442
	step [71/250], loss=61.8319
	step [72/250], loss=62.6277
	step [73/250], loss=85.7722
	step [74/250], loss=75.0288
	step [75/250], loss=75.6894
	step [76/250], loss=78.7183
	step [77/250], loss=67.5137
	step [78/250], loss=67.7514
	step [79/250], loss=77.6717
	step [80/250], loss=81.8981
	step [81/250], loss=78.9165
	step [82/250], loss=91.0308
	step [83/250], loss=67.2154
	step [84/250], loss=74.8800
	step [85/250], loss=60.0687
	step [86/250], loss=78.1947
	step [87/250], loss=77.4695
	step [88/250], loss=71.4988
	step [89/250], loss=68.8918
	step [90/250], loss=77.4272
	step [91/250], loss=67.5465
	step [92/250], loss=86.9846
	step [93/250], loss=60.5742
	step [94/250], loss=60.9195
	step [95/250], loss=92.1951
	step [96/250], loss=74.2376
	step [97/250], loss=63.1020
	step [98/250], loss=60.9067
	step [99/250], loss=87.6113
	step [100/250], loss=68.1898
	step [101/250], loss=88.2871
	step [102/250], loss=65.2486
	step [103/250], loss=63.9289
	step [104/250], loss=72.4396
	step [105/250], loss=82.4882
	step [106/250], loss=79.7622
	step [107/250], loss=74.1142
	step [108/250], loss=75.2430
	step [109/250], loss=75.7753
	step [110/250], loss=66.0364
	step [111/250], loss=73.5827
	step [112/250], loss=73.9795
	step [113/250], loss=83.8546
	step [114/250], loss=69.3466
	step [115/250], loss=68.8110
	step [116/250], loss=83.7807
	step [117/250], loss=91.8176
	step [118/250], loss=91.1952
	step [119/250], loss=64.9031
	step [120/250], loss=71.1541
	step [121/250], loss=66.2067
	step [122/250], loss=70.0558
	step [123/250], loss=90.0534
	step [124/250], loss=82.9847
	step [125/250], loss=71.2371
	step [126/250], loss=87.1308
	step [127/250], loss=63.2821
	step [128/250], loss=73.8400
	step [129/250], loss=87.6320
	step [130/250], loss=80.5023
	step [131/250], loss=64.9163
	step [132/250], loss=68.7905
	step [133/250], loss=78.3507
	step [134/250], loss=78.8791
	step [135/250], loss=78.3350
	step [136/250], loss=67.0674
	step [137/250], loss=78.2818
	step [138/250], loss=72.1673
	step [139/250], loss=66.1480
	step [140/250], loss=71.7848
	step [141/250], loss=76.3708
	step [142/250], loss=80.9269
	step [143/250], loss=84.1370
	step [144/250], loss=74.9327
	step [145/250], loss=78.2206
	step [146/250], loss=77.5023
	step [147/250], loss=70.5307
	step [148/250], loss=67.5643
	step [149/250], loss=74.5611
	step [150/250], loss=63.8666
	step [151/250], loss=69.9564
	step [152/250], loss=80.1321
	step [153/250], loss=84.0796
	step [154/250], loss=64.8626
	step [155/250], loss=57.9344
	step [156/250], loss=76.9099
	step [157/250], loss=81.3995
	step [158/250], loss=84.9781
	step [159/250], loss=74.8193
	step [160/250], loss=79.8547
	step [161/250], loss=86.3461
	step [162/250], loss=82.9129
	step [163/250], loss=55.1009
	step [164/250], loss=84.3219
	step [165/250], loss=84.7103
	step [166/250], loss=57.8480
	step [167/250], loss=83.4273
	step [168/250], loss=74.6176
	step [169/250], loss=74.2679
	step [170/250], loss=75.9940
	step [171/250], loss=85.9821
	step [172/250], loss=74.0163
	step [173/250], loss=81.6252
	step [174/250], loss=74.9233
	step [175/250], loss=75.3579
	step [176/250], loss=80.9740
	step [177/250], loss=71.7321
	step [178/250], loss=93.2699
	step [179/250], loss=71.6467
	step [180/250], loss=78.9059
	step [181/250], loss=80.0553
	step [182/250], loss=63.3729
	step [183/250], loss=65.9137
	step [184/250], loss=84.0334
	step [185/250], loss=57.6582
	step [186/250], loss=77.0094
	step [187/250], loss=71.8018
	step [188/250], loss=83.1538
	step [189/250], loss=68.6137
	step [190/250], loss=77.4328
	step [191/250], loss=73.2553
	step [192/250], loss=77.3343
	step [193/250], loss=68.3075
	step [194/250], loss=60.0038
	step [195/250], loss=73.1062
	step [196/250], loss=77.2157
	step [197/250], loss=71.4769
	step [198/250], loss=72.1002
	step [199/250], loss=72.3084
	step [200/250], loss=61.2122
	step [201/250], loss=72.9829
	step [202/250], loss=73.9534
	step [203/250], loss=70.3543
	step [204/250], loss=60.5484
	step [205/250], loss=60.5060
	step [206/250], loss=68.7709
	step [207/250], loss=83.9485
	step [208/250], loss=52.5035
	step [209/250], loss=73.7352
	step [210/250], loss=89.6835
	step [211/250], loss=64.1386
	step [212/250], loss=69.5040
	step [213/250], loss=79.1303
	step [214/250], loss=62.3710
	step [215/250], loss=78.7791
	step [216/250], loss=85.2794
	step [217/250], loss=63.7047
	step [218/250], loss=81.1853
	step [219/250], loss=76.5642
	step [220/250], loss=64.7054
	step [221/250], loss=64.0084
	step [222/250], loss=96.0263
	step [223/250], loss=53.8826
	step [224/250], loss=64.4684
	step [225/250], loss=74.6735
	step [226/250], loss=82.6621
	step [227/250], loss=54.9221
	step [228/250], loss=81.0035
	step [229/250], loss=77.4682
	step [230/250], loss=70.4013
	step [231/250], loss=63.4875
	step [232/250], loss=86.1557
	step [233/250], loss=81.8795
	step [234/250], loss=70.5818
	step [235/250], loss=79.4008
	step [236/250], loss=73.6366
	step [237/250], loss=79.9768
	step [238/250], loss=62.5604
	step [239/250], loss=93.5770
	step [240/250], loss=70.3251
	step [241/250], loss=84.2634
	step [242/250], loss=75.4113
	step [243/250], loss=76.1685
	step [244/250], loss=76.3374
	step [245/250], loss=94.0829
	step [246/250], loss=81.5353
	step [247/250], loss=67.6066
	step [248/250], loss=70.1321
	step [249/250], loss=79.8512
	step [250/250], loss=12.2141
	Evaluating
	loss=0.0073, precision=0.3095, recall=0.8680, f1=0.4563
Training epoch 90
	step [1/250], loss=67.5302
	step [2/250], loss=70.5025
	step [3/250], loss=73.3094
	step [4/250], loss=70.7144
	step [5/250], loss=71.2083
	step [6/250], loss=62.5003
	step [7/250], loss=67.3229
	step [8/250], loss=74.0468
	step [9/250], loss=72.1428
	step [10/250], loss=82.8810
	step [11/250], loss=68.5973
	step [12/250], loss=85.6552
	step [13/250], loss=67.3244
	step [14/250], loss=65.1046
	step [15/250], loss=79.5785
	step [16/250], loss=80.9349
	step [17/250], loss=84.6232
	step [18/250], loss=86.1131
	step [19/250], loss=72.5935
	step [20/250], loss=62.5668
	step [21/250], loss=72.4317
	step [22/250], loss=60.0367
	step [23/250], loss=71.9595
	step [24/250], loss=71.7128
	step [25/250], loss=70.9667
	step [26/250], loss=82.1591
	step [27/250], loss=61.8149
	step [28/250], loss=106.6241
	step [29/250], loss=71.0999
	step [30/250], loss=93.1037
	step [31/250], loss=75.3410
	step [32/250], loss=81.5712
	step [33/250], loss=75.7570
	step [34/250], loss=64.9765
	step [35/250], loss=89.6624
	step [36/250], loss=75.8204
	step [37/250], loss=70.5602
	step [38/250], loss=74.7860
	step [39/250], loss=78.3317
	step [40/250], loss=62.6986
	step [41/250], loss=74.1764
	step [42/250], loss=71.7577
	step [43/250], loss=67.1191
	step [44/250], loss=67.3533
	step [45/250], loss=75.9231
	step [46/250], loss=82.2399
	step [47/250], loss=69.6148
	step [48/250], loss=76.0498
	step [49/250], loss=84.9460
	step [50/250], loss=70.9954
	step [51/250], loss=88.9792
	step [52/250], loss=67.0771
	step [53/250], loss=87.2193
	step [54/250], loss=80.2858
	step [55/250], loss=76.0121
	step [56/250], loss=77.8128
	step [57/250], loss=64.2976
	step [58/250], loss=83.6774
	step [59/250], loss=82.8380
	step [60/250], loss=67.8820
	step [61/250], loss=73.4641
	step [62/250], loss=82.4105
	step [63/250], loss=72.2135
	step [64/250], loss=80.9405
	step [65/250], loss=58.1562
	step [66/250], loss=78.1521
	step [67/250], loss=64.1169
	step [68/250], loss=77.1428
	step [69/250], loss=76.3808
	step [70/250], loss=92.8137
	step [71/250], loss=60.3989
	step [72/250], loss=75.6365
	step [73/250], loss=79.9708
	step [74/250], loss=84.0969
	step [75/250], loss=70.9824
	step [76/250], loss=66.7714
	step [77/250], loss=66.5127
	step [78/250], loss=70.6674
	step [79/250], loss=72.7709
	step [80/250], loss=63.4538
	step [81/250], loss=90.2872
	step [82/250], loss=74.4718
	step [83/250], loss=81.5875
	step [84/250], loss=78.3528
	step [85/250], loss=75.4197
	step [86/250], loss=56.6746
	step [87/250], loss=76.9429
	step [88/250], loss=67.2795
	step [89/250], loss=82.9401
	step [90/250], loss=78.1646
	step [91/250], loss=75.3057
	step [92/250], loss=67.2997
	step [93/250], loss=80.8699
	step [94/250], loss=74.0012
	step [95/250], loss=66.3259
	step [96/250], loss=81.0182
	step [97/250], loss=89.0707
	step [98/250], loss=59.5524
	step [99/250], loss=75.4253
	step [100/250], loss=74.0916
	step [101/250], loss=90.8480
	step [102/250], loss=64.8800
	step [103/250], loss=80.7510
	step [104/250], loss=79.5888
	step [105/250], loss=70.1390
	step [106/250], loss=72.2537
	step [107/250], loss=65.3961
	step [108/250], loss=74.6511
	step [109/250], loss=77.4611
	step [110/250], loss=73.8909
	step [111/250], loss=76.3817
	step [112/250], loss=71.8239
	step [113/250], loss=72.5494
	step [114/250], loss=79.3027
	step [115/250], loss=75.2664
	step [116/250], loss=75.0114
	step [117/250], loss=71.9753
	step [118/250], loss=64.5492
	step [119/250], loss=62.0701
	step [120/250], loss=65.2719
	step [121/250], loss=69.2228
	step [122/250], loss=70.2695
	step [123/250], loss=69.6431
	step [124/250], loss=77.6350
	step [125/250], loss=69.2781
	step [126/250], loss=69.8770
	step [127/250], loss=67.7531
	step [128/250], loss=81.3005
	step [129/250], loss=79.1926
	step [130/250], loss=76.8506
	step [131/250], loss=78.8657
	step [132/250], loss=85.5079
	step [133/250], loss=75.8542
	step [134/250], loss=74.3612
	step [135/250], loss=83.7816
	step [136/250], loss=71.2398
	step [137/250], loss=64.6538
	step [138/250], loss=77.9526
	step [139/250], loss=77.7042
	step [140/250], loss=76.8893
	step [141/250], loss=59.8134
	step [142/250], loss=87.1167
	step [143/250], loss=73.5064
	step [144/250], loss=73.6473
	step [145/250], loss=66.5544
	step [146/250], loss=59.4210
	step [147/250], loss=67.7578
	step [148/250], loss=68.4785
	step [149/250], loss=69.4192
	step [150/250], loss=83.4127
	step [151/250], loss=80.1365
	step [152/250], loss=69.5593
	step [153/250], loss=74.1989
	step [154/250], loss=68.8678
	step [155/250], loss=69.8884
	step [156/250], loss=67.7052
	step [157/250], loss=72.8454
	step [158/250], loss=82.7598
	step [159/250], loss=73.3539
	step [160/250], loss=83.1725
	step [161/250], loss=65.1940
	step [162/250], loss=79.6613
	step [163/250], loss=71.4318
	step [164/250], loss=82.8740
	step [165/250], loss=76.3861
	step [166/250], loss=69.3018
	step [167/250], loss=75.6492
	step [168/250], loss=81.4684
	step [169/250], loss=61.7963
	step [170/250], loss=74.4682
	step [171/250], loss=84.8453
	step [172/250], loss=66.9715
	step [173/250], loss=72.7360
	step [174/250], loss=83.7130
	step [175/250], loss=62.3809
	step [176/250], loss=72.4679
	step [177/250], loss=73.3273
	step [178/250], loss=83.8836
	step [179/250], loss=79.8701
	step [180/250], loss=71.2731
	step [181/250], loss=73.8560
	step [182/250], loss=75.9171
	step [183/250], loss=67.3082
	step [184/250], loss=80.5258
	step [185/250], loss=72.8914
	step [186/250], loss=81.9181
	step [187/250], loss=91.3201
	step [188/250], loss=74.7356
	step [189/250], loss=77.7736
	step [190/250], loss=74.9141
	step [191/250], loss=77.8725
	step [192/250], loss=65.4441
	step [193/250], loss=73.2847
	step [194/250], loss=65.3880
	step [195/250], loss=74.9424
	step [196/250], loss=85.8873
	step [197/250], loss=82.3092
	step [198/250], loss=67.6246
	step [199/250], loss=69.2014
	step [200/250], loss=77.1690
	step [201/250], loss=72.2983
	step [202/250], loss=71.3108
	step [203/250], loss=93.1148
	step [204/250], loss=68.7691
	step [205/250], loss=82.7679
	step [206/250], loss=71.4850
	step [207/250], loss=74.2532
	step [208/250], loss=81.5390
	step [209/250], loss=76.3891
	step [210/250], loss=88.9689
	step [211/250], loss=78.2160
	step [212/250], loss=64.7166
	step [213/250], loss=76.1775
	step [214/250], loss=67.6732
	step [215/250], loss=84.8819
	step [216/250], loss=68.0075
	step [217/250], loss=77.6009
	step [218/250], loss=62.3050
	step [219/250], loss=68.7084
	step [220/250], loss=80.9908
	step [221/250], loss=62.4410
	step [222/250], loss=67.0731
	step [223/250], loss=82.5924
	step [224/250], loss=56.2159
	step [225/250], loss=61.6677
	step [226/250], loss=75.1797
	step [227/250], loss=73.7168
	step [228/250], loss=87.0595
	step [229/250], loss=70.7848
	step [230/250], loss=64.2686
	step [231/250], loss=68.1809
	step [232/250], loss=78.8017
	step [233/250], loss=67.6475
	step [234/250], loss=84.3723
	step [235/250], loss=71.9170
	step [236/250], loss=70.5921
	step [237/250], loss=82.5283
	step [238/250], loss=71.8531
	step [239/250], loss=79.0416
	step [240/250], loss=87.4683
	step [241/250], loss=73.9746
	step [242/250], loss=89.6497
	step [243/250], loss=73.3761
	step [244/250], loss=78.0676
	step [245/250], loss=74.8579
	step [246/250], loss=71.2850
	step [247/250], loss=71.6011
	step [248/250], loss=72.6989
	step [249/250], loss=74.4455
	step [250/250], loss=16.5175
	Evaluating
	loss=0.0077, precision=0.2920, recall=0.8625, f1=0.4363
Training epoch 91
	step [1/250], loss=67.1775
	step [2/250], loss=70.9737
	step [3/250], loss=54.5175
	step [4/250], loss=68.6893
	step [5/250], loss=81.0880
	step [6/250], loss=70.5076
	step [7/250], loss=57.4521
	step [8/250], loss=84.8103
	step [9/250], loss=82.3013
	step [10/250], loss=73.8156
	step [11/250], loss=82.9123
	step [12/250], loss=79.4186
	step [13/250], loss=76.6296
	step [14/250], loss=64.2300
	step [15/250], loss=87.9284
	step [16/250], loss=50.4144
	step [17/250], loss=85.4172
	step [18/250], loss=69.3043
	step [19/250], loss=87.5252
	step [20/250], loss=70.5560
	step [21/250], loss=73.2314
	step [22/250], loss=71.2630
	step [23/250], loss=78.3327
	step [24/250], loss=78.1283
	step [25/250], loss=66.4412
	step [26/250], loss=81.5884
	step [27/250], loss=76.6579
	step [28/250], loss=86.2199
	step [29/250], loss=50.8880
	step [30/250], loss=82.5027
	step [31/250], loss=71.9541
	step [32/250], loss=65.7733
	step [33/250], loss=68.2303
	step [34/250], loss=71.5557
	step [35/250], loss=75.3067
	step [36/250], loss=74.2955
	step [37/250], loss=60.3420
	step [38/250], loss=59.8590
	step [39/250], loss=86.9624
	step [40/250], loss=65.0801
	step [41/250], loss=73.6322
	step [42/250], loss=62.8608
	step [43/250], loss=70.5223
	step [44/250], loss=77.3966
	step [45/250], loss=85.3768
	step [46/250], loss=77.5656
	step [47/250], loss=80.5815
	step [48/250], loss=64.5233
	step [49/250], loss=67.0607
	step [50/250], loss=71.1909
	step [51/250], loss=74.3896
	step [52/250], loss=92.2109
	step [53/250], loss=80.2009
	step [54/250], loss=78.4557
	step [55/250], loss=65.1025
	step [56/250], loss=69.8905
	step [57/250], loss=88.0805
	step [58/250], loss=75.7930
	step [59/250], loss=65.8911
	step [60/250], loss=70.5925
	step [61/250], loss=80.1458
	step [62/250], loss=56.5654
	step [63/250], loss=76.9649
	step [64/250], loss=75.1837
	step [65/250], loss=68.9941
	step [66/250], loss=94.4392
	step [67/250], loss=73.7144
	step [68/250], loss=81.3712
	step [69/250], loss=74.9895
	step [70/250], loss=70.4756
	step [71/250], loss=66.7262
	step [72/250], loss=79.6713
	step [73/250], loss=76.7149
	step [74/250], loss=74.2992
	step [75/250], loss=92.7023
	step [76/250], loss=64.1858
	step [77/250], loss=91.7753
	step [78/250], loss=75.9581
	step [79/250], loss=78.9627
	step [80/250], loss=73.6151
	step [81/250], loss=80.3858
	step [82/250], loss=79.6076
	step [83/250], loss=67.4670
	step [84/250], loss=58.3371
	step [85/250], loss=75.3102
	step [86/250], loss=79.8324
	step [87/250], loss=79.7724
	step [88/250], loss=71.0217
	step [89/250], loss=72.7708
	step [90/250], loss=87.1432
	step [91/250], loss=66.9879
	step [92/250], loss=74.4731
	step [93/250], loss=73.3888
	step [94/250], loss=80.7931
	step [95/250], loss=78.1972
	step [96/250], loss=90.8217
	step [97/250], loss=82.7283
	step [98/250], loss=74.5739
	step [99/250], loss=65.1890
	step [100/250], loss=92.8121
	step [101/250], loss=72.2080
	step [102/250], loss=88.7108
	step [103/250], loss=84.5065
	step [104/250], loss=60.7448
	step [105/250], loss=71.6931
	step [106/250], loss=79.0509
	step [107/250], loss=85.1133
	step [108/250], loss=74.3693
	step [109/250], loss=84.3111
	step [110/250], loss=85.4195
	step [111/250], loss=76.9345
	step [112/250], loss=69.6667
	step [113/250], loss=80.1331
	step [114/250], loss=87.3850
	step [115/250], loss=79.5492
	step [116/250], loss=70.0971
	step [117/250], loss=91.2530
	step [118/250], loss=90.4841
	step [119/250], loss=64.6905
	step [120/250], loss=62.0295
	step [121/250], loss=70.1687
	step [122/250], loss=75.4418
	step [123/250], loss=76.2773
	step [124/250], loss=79.8877
	step [125/250], loss=79.0020
	step [126/250], loss=77.0745
	step [127/250], loss=72.9994
	step [128/250], loss=72.2975
	step [129/250], loss=82.4851
	step [130/250], loss=63.1245
	step [131/250], loss=77.0622
	step [132/250], loss=82.7586
	step [133/250], loss=65.7350
	step [134/250], loss=69.7006
	step [135/250], loss=58.0163
	step [136/250], loss=87.9385
	step [137/250], loss=63.1888
	step [138/250], loss=72.0528
	step [139/250], loss=74.6754
	step [140/250], loss=61.7542
	step [141/250], loss=66.3014
	step [142/250], loss=79.8013
	step [143/250], loss=73.9576
	step [144/250], loss=69.4835
	step [145/250], loss=84.7657
	step [146/250], loss=71.7960
	step [147/250], loss=67.8068
	step [148/250], loss=76.8877
	step [149/250], loss=69.3170
	step [150/250], loss=60.8875
	step [151/250], loss=71.9628
	step [152/250], loss=77.1131
	step [153/250], loss=70.0978
	step [154/250], loss=60.3383
	step [155/250], loss=63.9636
	step [156/250], loss=84.3598
	step [157/250], loss=85.9021
	step [158/250], loss=87.8883
	step [159/250], loss=57.4308
	step [160/250], loss=78.8957
	step [161/250], loss=81.7464
	step [162/250], loss=89.8436
	step [163/250], loss=73.3081
	step [164/250], loss=67.0682
	step [165/250], loss=75.9406
	step [166/250], loss=77.3048
	step [167/250], loss=69.9059
	step [168/250], loss=60.7403
	step [169/250], loss=66.3186
	step [170/250], loss=76.9661
	step [171/250], loss=76.9777
	step [172/250], loss=68.1713
	step [173/250], loss=62.6618
	step [174/250], loss=82.6326
	step [175/250], loss=56.6634
	step [176/250], loss=64.8706
	step [177/250], loss=59.7383
	step [178/250], loss=80.1676
	step [179/250], loss=72.4399
	step [180/250], loss=70.7530
	step [181/250], loss=77.8255
	step [182/250], loss=62.4051
	step [183/250], loss=72.1448
	step [184/250], loss=69.7805
	step [185/250], loss=68.4145
	step [186/250], loss=97.7110
	step [187/250], loss=72.0010
	step [188/250], loss=70.8414
	step [189/250], loss=63.0070
	step [190/250], loss=69.5344
	step [191/250], loss=76.8045
	step [192/250], loss=69.9892
	step [193/250], loss=73.6416
	step [194/250], loss=86.4334
	step [195/250], loss=74.5334
	step [196/250], loss=67.1550
	step [197/250], loss=71.6105
	step [198/250], loss=79.2482
	step [199/250], loss=75.7735
	step [200/250], loss=75.8947
	step [201/250], loss=69.2721
	step [202/250], loss=69.0120
	step [203/250], loss=80.9988
	step [204/250], loss=68.2524
	step [205/250], loss=75.7313
	step [206/250], loss=67.6573
	step [207/250], loss=65.9549
	step [208/250], loss=65.0366
	step [209/250], loss=72.4957
	step [210/250], loss=72.7166
	step [211/250], loss=76.4636
	step [212/250], loss=70.6343
	step [213/250], loss=61.5128
	step [214/250], loss=65.9963
	step [215/250], loss=59.0529
	step [216/250], loss=79.4313
	step [217/250], loss=71.4670
	step [218/250], loss=81.6163
	step [219/250], loss=85.4274
	step [220/250], loss=69.0636
	step [221/250], loss=78.8591
	step [222/250], loss=84.0715
	step [223/250], loss=87.2061
	step [224/250], loss=86.9677
	step [225/250], loss=84.0184
	step [226/250], loss=75.7466
	step [227/250], loss=74.2999
	step [228/250], loss=78.1110
	step [229/250], loss=57.4688
	step [230/250], loss=65.5751
	step [231/250], loss=74.4695
	step [232/250], loss=96.0890
	step [233/250], loss=69.6224
	step [234/250], loss=87.6596
	step [235/250], loss=93.8310
	step [236/250], loss=73.5756
	step [237/250], loss=68.7668
	step [238/250], loss=67.3498
	step [239/250], loss=75.9843
	step [240/250], loss=72.9251
	step [241/250], loss=68.9399
	step [242/250], loss=76.1934
	step [243/250], loss=72.7746
	step [244/250], loss=93.3982
	step [245/250], loss=69.1436
	step [246/250], loss=59.6327
	step [247/250], loss=73.4155
	step [248/250], loss=90.8367
	step [249/250], loss=72.7311
	step [250/250], loss=12.9803
	Evaluating
	loss=0.0080, precision=0.2838, recall=0.8563, f1=0.4263
Training epoch 92
	step [1/250], loss=66.0686
	step [2/250], loss=79.1495
	step [3/250], loss=74.1597
	step [4/250], loss=75.1170
	step [5/250], loss=66.2383
	step [6/250], loss=67.0288
	step [7/250], loss=83.6255
	step [8/250], loss=74.4652
	step [9/250], loss=88.0381
	step [10/250], loss=72.6835
	step [11/250], loss=75.9767
	step [12/250], loss=81.5798
	step [13/250], loss=72.6266
	step [14/250], loss=66.9967
	step [15/250], loss=79.6455
	step [16/250], loss=64.4787
	step [17/250], loss=68.2301
	step [18/250], loss=62.8398
	step [19/250], loss=57.1270
	step [20/250], loss=73.4827
	step [21/250], loss=76.2800
	step [22/250], loss=73.0855
	step [23/250], loss=72.3025
	step [24/250], loss=82.5477
	step [25/250], loss=74.4010
	step [26/250], loss=71.1313
	step [27/250], loss=79.8845
	step [28/250], loss=80.5885
	step [29/250], loss=66.8933
	step [30/250], loss=70.5598
	step [31/250], loss=78.7003
	step [32/250], loss=85.5335
	step [33/250], loss=60.8133
	step [34/250], loss=65.8347
	step [35/250], loss=68.4837
	step [36/250], loss=85.3459
	step [37/250], loss=73.0321
	step [38/250], loss=58.4989
	step [39/250], loss=73.3058
	step [40/250], loss=79.0487
	step [41/250], loss=63.8978
	step [42/250], loss=69.9953
	step [43/250], loss=72.9384
	step [44/250], loss=64.3166
	step [45/250], loss=66.5858
	step [46/250], loss=80.2456
	step [47/250], loss=80.1859
	step [48/250], loss=74.4373
	step [49/250], loss=79.4582
	step [50/250], loss=77.2419
	step [51/250], loss=69.9267
	step [52/250], loss=70.4786
	step [53/250], loss=76.5237
	step [54/250], loss=72.6351
	step [55/250], loss=72.3066
	step [56/250], loss=82.9509
	step [57/250], loss=72.2413
	step [58/250], loss=67.0407
	step [59/250], loss=76.7318
	step [60/250], loss=61.8814
	step [61/250], loss=70.4383
	step [62/250], loss=77.6311
	step [63/250], loss=68.2388
	step [64/250], loss=86.3545
	step [65/250], loss=71.2490
	step [66/250], loss=83.7529
	step [67/250], loss=75.4794
	step [68/250], loss=66.9835
	step [69/250], loss=75.6987
	step [70/250], loss=76.5032
	step [71/250], loss=72.0273
	step [72/250], loss=82.0597
	step [73/250], loss=72.4565
	step [74/250], loss=92.4231
	step [75/250], loss=73.3131
	step [76/250], loss=68.7647
	step [77/250], loss=66.2248
	step [78/250], loss=73.5493
	step [79/250], loss=73.4423
	step [80/250], loss=77.8629
	step [81/250], loss=74.1569
	step [82/250], loss=71.6669
	step [83/250], loss=74.0048
	step [84/250], loss=85.0209
	step [85/250], loss=78.0530
	step [86/250], loss=70.7998
	step [87/250], loss=75.3303
	step [88/250], loss=75.6212
	step [89/250], loss=64.4537
	step [90/250], loss=70.5106
	step [91/250], loss=75.6136
	step [92/250], loss=77.3643
	step [93/250], loss=71.5100
	step [94/250], loss=53.7164
	step [95/250], loss=78.5413
	step [96/250], loss=72.9194
	step [97/250], loss=84.4848
	step [98/250], loss=88.4267
	step [99/250], loss=77.5026
	step [100/250], loss=80.8561
	step [101/250], loss=81.2846
	step [102/250], loss=78.0648
	step [103/250], loss=67.9625
	step [104/250], loss=84.7882
	step [105/250], loss=69.6141
	step [106/250], loss=71.0260
	step [107/250], loss=58.8371
	step [108/250], loss=69.7491
	step [109/250], loss=71.8753
	step [110/250], loss=68.0189
	step [111/250], loss=74.0854
	step [112/250], loss=78.3015
	step [113/250], loss=70.5799
	step [114/250], loss=61.1478
	step [115/250], loss=69.7691
	step [116/250], loss=78.4301
	step [117/250], loss=77.3842
	step [118/250], loss=68.2382
	step [119/250], loss=66.1958
	step [120/250], loss=71.4436
	step [121/250], loss=62.2859
	step [122/250], loss=81.2460
	step [123/250], loss=91.4824
	step [124/250], loss=68.2702
	step [125/250], loss=84.6003
	step [126/250], loss=72.0173
	step [127/250], loss=79.2046
	step [128/250], loss=74.8633
	step [129/250], loss=73.9826
	step [130/250], loss=80.7433
	step [131/250], loss=65.8688
	step [132/250], loss=75.5054
	step [133/250], loss=71.9358
	step [134/250], loss=63.1333
	step [135/250], loss=78.5953
	step [136/250], loss=77.7888
	step [137/250], loss=89.6507
	step [138/250], loss=71.9570
	step [139/250], loss=69.2562
	step [140/250], loss=64.7420
	step [141/250], loss=86.1232
	step [142/250], loss=64.0932
	step [143/250], loss=90.7074
	step [144/250], loss=76.1256
	step [145/250], loss=74.9708
	step [146/250], loss=71.3464
	step [147/250], loss=72.2984
	step [148/250], loss=72.6456
	step [149/250], loss=81.0491
	step [150/250], loss=72.6833
	step [151/250], loss=67.7730
	step [152/250], loss=78.6322
	step [153/250], loss=59.9564
	step [154/250], loss=83.4198
	step [155/250], loss=75.8075
	step [156/250], loss=72.8522
	step [157/250], loss=75.2616
	step [158/250], loss=67.8525
	step [159/250], loss=76.6527
	step [160/250], loss=78.3808
	step [161/250], loss=59.3132
	step [162/250], loss=91.7842
	step [163/250], loss=71.3953
	step [164/250], loss=70.8700
	step [165/250], loss=66.3554
	step [166/250], loss=56.9025
	step [167/250], loss=85.9722
	step [168/250], loss=60.4282
	step [169/250], loss=62.9427
	step [170/250], loss=79.6593
	step [171/250], loss=68.9256
	step [172/250], loss=77.9702
	step [173/250], loss=86.6083
	step [174/250], loss=66.4261
	step [175/250], loss=73.4404
	step [176/250], loss=65.0247
	step [177/250], loss=70.9476
	step [178/250], loss=57.5026
	step [179/250], loss=77.9235
	step [180/250], loss=68.8022
	step [181/250], loss=59.1592
	step [182/250], loss=66.3650
	step [183/250], loss=84.4863
	step [184/250], loss=75.1363
	step [185/250], loss=70.7909
	step [186/250], loss=74.2298
	step [187/250], loss=78.3663
	step [188/250], loss=78.3892
	step [189/250], loss=75.1509
	step [190/250], loss=66.7527
	step [191/250], loss=66.1900
	step [192/250], loss=83.0057
	step [193/250], loss=71.1015
	step [194/250], loss=76.7222
	step [195/250], loss=82.7954
	step [196/250], loss=69.3243
	step [197/250], loss=85.7494
	step [198/250], loss=82.2896
	step [199/250], loss=70.4901
	step [200/250], loss=82.0918
	step [201/250], loss=65.4214
	step [202/250], loss=85.2738
	step [203/250], loss=78.5668
	step [204/250], loss=84.4597
	step [205/250], loss=70.5001
	step [206/250], loss=57.8345
	step [207/250], loss=74.2413
	step [208/250], loss=67.6368
	step [209/250], loss=69.2193
	step [210/250], loss=69.6541
	step [211/250], loss=82.6114
	step [212/250], loss=81.7556
	step [213/250], loss=76.1472
	step [214/250], loss=69.0179
	step [215/250], loss=81.0534
	step [216/250], loss=65.3372
	step [217/250], loss=79.4011
	step [218/250], loss=69.7242
	step [219/250], loss=90.7439
	step [220/250], loss=57.8468
	step [221/250], loss=60.3173
	step [222/250], loss=58.1157
	step [223/250], loss=78.3830
	step [224/250], loss=68.9081
	step [225/250], loss=75.4194
	step [226/250], loss=77.0849
	step [227/250], loss=74.3691
	step [228/250], loss=87.7990
	step [229/250], loss=71.4182
	step [230/250], loss=56.4340
	step [231/250], loss=73.6520
	step [232/250], loss=80.6938
	step [233/250], loss=69.6551
	step [234/250], loss=89.4287
	step [235/250], loss=79.6183
	step [236/250], loss=83.1957
	step [237/250], loss=72.0287
	step [238/250], loss=84.0677
	step [239/250], loss=70.8664
	step [240/250], loss=73.7956
	step [241/250], loss=74.8749
	step [242/250], loss=81.2700
	step [243/250], loss=76.9038
	step [244/250], loss=98.2158
	step [245/250], loss=64.2414
	step [246/250], loss=89.2785
	step [247/250], loss=81.7031
	step [248/250], loss=66.5946
	step [249/250], loss=68.6777
	step [250/250], loss=14.4158
	Evaluating
	loss=0.0081, precision=0.2803, recall=0.8647, f1=0.4233
Training epoch 93
	step [1/250], loss=76.6801
	step [2/250], loss=66.6826
	step [3/250], loss=48.2864
	step [4/250], loss=64.8451
	step [5/250], loss=61.1808
	step [6/250], loss=78.1205
	step [7/250], loss=86.5268
	step [8/250], loss=53.7295
	step [9/250], loss=77.8331
	step [10/250], loss=77.1974
	step [11/250], loss=66.2617
	step [12/250], loss=69.6751
	step [13/250], loss=78.6606
	step [14/250], loss=73.5320
	step [15/250], loss=70.0915
	step [16/250], loss=69.3615
	step [17/250], loss=80.8328
	step [18/250], loss=78.1152
	step [19/250], loss=68.4509
	step [20/250], loss=79.2755
	step [21/250], loss=70.4634
	step [22/250], loss=76.9902
	step [23/250], loss=82.0176
	step [24/250], loss=74.3370
	step [25/250], loss=65.9510
	step [26/250], loss=80.3129
	step [27/250], loss=84.3115
	step [28/250], loss=63.1981
	step [29/250], loss=83.4763
	step [30/250], loss=67.3087
	step [31/250], loss=73.2344
	step [32/250], loss=74.9647
	step [33/250], loss=73.6196
	step [34/250], loss=79.5132
	step [35/250], loss=83.9014
	step [36/250], loss=83.2684
	step [37/250], loss=88.1609
	step [38/250], loss=64.7798
	step [39/250], loss=82.9636
	step [40/250], loss=79.4064
	step [41/250], loss=68.2596
	step [42/250], loss=90.3395
	step [43/250], loss=74.9403
	step [44/250], loss=70.4059
	step [45/250], loss=67.5327
	step [46/250], loss=75.3393
	step [47/250], loss=75.9181
	step [48/250], loss=82.6280
	step [49/250], loss=74.6098
	step [50/250], loss=58.9898
	step [51/250], loss=62.0832
	step [52/250], loss=64.4519
	step [53/250], loss=66.3309
	step [54/250], loss=84.7696
	step [55/250], loss=71.0298
	step [56/250], loss=70.6589
	step [57/250], loss=80.0115
	step [58/250], loss=84.9867
	step [59/250], loss=71.8605
	step [60/250], loss=78.8588
	step [61/250], loss=70.6039
	step [62/250], loss=85.2294
	step [63/250], loss=73.9529
	step [64/250], loss=74.6402
	step [65/250], loss=63.6003
	step [66/250], loss=83.0757
	step [67/250], loss=76.2041
	step [68/250], loss=76.1468
	step [69/250], loss=61.5634
	step [70/250], loss=65.8501
	step [71/250], loss=67.1652
	step [72/250], loss=77.7994
	step [73/250], loss=83.1877
	step [74/250], loss=78.1658
	step [75/250], loss=68.3782
	step [76/250], loss=71.5692
	step [77/250], loss=89.8839
	step [78/250], loss=60.0772
	step [79/250], loss=77.1722
	step [80/250], loss=81.5520
	step [81/250], loss=78.4134
	step [82/250], loss=70.3100
	step [83/250], loss=72.0507
	step [84/250], loss=71.6476
	step [85/250], loss=82.6257
	step [86/250], loss=70.1347
	step [87/250], loss=61.5445
	step [88/250], loss=68.5008
	step [89/250], loss=66.5493
	step [90/250], loss=67.2350
	step [91/250], loss=83.5580
	step [92/250], loss=71.7639
	step [93/250], loss=72.1069
	step [94/250], loss=62.7649
	step [95/250], loss=73.3263
	step [96/250], loss=77.9313
	step [97/250], loss=84.2487
	step [98/250], loss=78.3483
	step [99/250], loss=68.3371
	step [100/250], loss=86.1002
	step [101/250], loss=80.5378
	step [102/250], loss=91.9630
	step [103/250], loss=86.0658
	step [104/250], loss=86.1359
	step [105/250], loss=72.6074
	step [106/250], loss=51.3994
	step [107/250], loss=77.5004
	step [108/250], loss=72.6594
	step [109/250], loss=71.9237
	step [110/250], loss=88.0212
	step [111/250], loss=77.2368
	step [112/250], loss=75.2812
	step [113/250], loss=100.6657
	step [114/250], loss=68.2869
	step [115/250], loss=82.8802
	step [116/250], loss=76.6446
	step [117/250], loss=70.5874
	step [118/250], loss=76.3134
	step [119/250], loss=76.9653
	step [120/250], loss=61.8398
	step [121/250], loss=83.5441
	step [122/250], loss=73.6732
	step [123/250], loss=81.8049
	step [124/250], loss=77.4230
	step [125/250], loss=77.8733
	step [126/250], loss=74.2403
	step [127/250], loss=53.4372
	step [128/250], loss=79.2963
	step [129/250], loss=74.9796
	step [130/250], loss=77.1940
	step [131/250], loss=60.3826
	step [132/250], loss=84.4247
	step [133/250], loss=54.0538
	step [134/250], loss=79.4790
	step [135/250], loss=75.7800
	step [136/250], loss=75.4829
	step [137/250], loss=68.6176
	step [138/250], loss=57.5827
	step [139/250], loss=91.6385
	step [140/250], loss=68.1458
	step [141/250], loss=67.5292
	step [142/250], loss=65.2567
	step [143/250], loss=57.9798
	step [144/250], loss=90.0828
	step [145/250], loss=55.3805
	step [146/250], loss=82.1116
	step [147/250], loss=68.7152
	step [148/250], loss=50.9115
	step [149/250], loss=68.1040
	step [150/250], loss=64.7794
	step [151/250], loss=73.6107
	step [152/250], loss=62.5665
	step [153/250], loss=84.2045
	step [154/250], loss=76.2904
	step [155/250], loss=71.0262
	step [156/250], loss=65.0394
	step [157/250], loss=79.8403
	step [158/250], loss=69.0451
	step [159/250], loss=89.0892
	step [160/250], loss=71.2892
	step [161/250], loss=69.4915
	step [162/250], loss=60.1013
	step [163/250], loss=77.0528
	step [164/250], loss=82.0356
	step [165/250], loss=61.8503
	step [166/250], loss=66.7050
	step [167/250], loss=66.5032
	step [168/250], loss=67.6290
	step [169/250], loss=78.8219
	step [170/250], loss=70.6839
	step [171/250], loss=63.2442
	step [172/250], loss=80.1549
	step [173/250], loss=81.2305
	step [174/250], loss=79.2160
	step [175/250], loss=84.2267
	step [176/250], loss=92.5832
	step [177/250], loss=63.1808
	step [178/250], loss=90.3411
	step [179/250], loss=72.5263
	step [180/250], loss=67.4060
	step [181/250], loss=65.1960
	step [182/250], loss=75.6538
	step [183/250], loss=86.4303
	step [184/250], loss=62.8847
	step [185/250], loss=75.0199
	step [186/250], loss=70.9645
	step [187/250], loss=70.8226
	step [188/250], loss=72.0698
	step [189/250], loss=73.3618
	step [190/250], loss=75.0952
	step [191/250], loss=76.9907
	step [192/250], loss=67.0346
	step [193/250], loss=69.8139
	step [194/250], loss=79.8535
	step [195/250], loss=69.7663
	step [196/250], loss=76.9440
	step [197/250], loss=63.4902
	step [198/250], loss=69.1592
	step [199/250], loss=79.3702
	step [200/250], loss=69.2444
	step [201/250], loss=61.2481
	step [202/250], loss=79.4430
	step [203/250], loss=66.4014
	step [204/250], loss=74.5627
	step [205/250], loss=78.7885
	step [206/250], loss=63.8626
	step [207/250], loss=77.9613
	step [208/250], loss=87.0656
	step [209/250], loss=73.4714
	step [210/250], loss=60.6009
	step [211/250], loss=78.8897
	step [212/250], loss=60.7078
	step [213/250], loss=66.1791
	step [214/250], loss=90.4880
	step [215/250], loss=73.5298
	step [216/250], loss=90.4066
	step [217/250], loss=78.3783
	step [218/250], loss=68.0495
	step [219/250], loss=61.4620
	step [220/250], loss=68.9760
	step [221/250], loss=71.3705
	step [222/250], loss=71.7940
	step [223/250], loss=65.3702
	step [224/250], loss=74.3543
	step [225/250], loss=84.8671
	step [226/250], loss=57.1046
	step [227/250], loss=68.6966
	step [228/250], loss=71.8182
	step [229/250], loss=83.0591
	step [230/250], loss=72.9247
	step [231/250], loss=74.2352
	step [232/250], loss=74.2538
	step [233/250], loss=73.4765
	step [234/250], loss=78.7542
	step [235/250], loss=73.8701
	step [236/250], loss=76.0189
	step [237/250], loss=60.4182
	step [238/250], loss=66.2721
	step [239/250], loss=88.8974
	step [240/250], loss=70.4394
	step [241/250], loss=92.6443
	step [242/250], loss=75.9348
	step [243/250], loss=76.5992
	step [244/250], loss=75.0721
	step [245/250], loss=71.6123
	step [246/250], loss=79.4375
	step [247/250], loss=70.0588
	step [248/250], loss=67.7007
	step [249/250], loss=80.7287
	step [250/250], loss=17.9687
	Evaluating
	loss=0.0071, precision=0.3155, recall=0.8529, f1=0.4606
Training epoch 94
	step [1/250], loss=81.4589
	step [2/250], loss=73.0690
	step [3/250], loss=72.1145
	step [4/250], loss=91.8913
	step [5/250], loss=81.8369
	step [6/250], loss=71.4145
	step [7/250], loss=74.3642
	step [8/250], loss=73.4215
	step [9/250], loss=68.2816
	step [10/250], loss=69.7314
	step [11/250], loss=86.4490
	step [12/250], loss=81.1328
	step [13/250], loss=89.5452
	step [14/250], loss=72.9850
	step [15/250], loss=67.9695
	step [16/250], loss=84.0466
	step [17/250], loss=76.8737
	step [18/250], loss=84.9994
	step [19/250], loss=77.4829
	step [20/250], loss=67.2642
	step [21/250], loss=66.9043
	step [22/250], loss=61.6168
	step [23/250], loss=76.3598
	step [24/250], loss=70.1493
	step [25/250], loss=69.0281
	step [26/250], loss=65.6055
	step [27/250], loss=79.8375
	step [28/250], loss=65.0073
	step [29/250], loss=62.7898
	step [30/250], loss=74.7496
	step [31/250], loss=75.5757
	step [32/250], loss=53.9848
	step [33/250], loss=77.1839
	step [34/250], loss=75.2956
	step [35/250], loss=74.1645
	step [36/250], loss=74.8470
	step [37/250], loss=68.3226
	step [38/250], loss=79.7084
	step [39/250], loss=77.4661
	step [40/250], loss=67.7799
	step [41/250], loss=86.4398
	step [42/250], loss=67.3859
	step [43/250], loss=67.9151
	step [44/250], loss=70.6956
	step [45/250], loss=66.2058
	step [46/250], loss=80.5572
	step [47/250], loss=71.1388
	step [48/250], loss=68.0030
	step [49/250], loss=63.2779
	step [50/250], loss=88.6392
	step [51/250], loss=71.2946
	step [52/250], loss=69.2163
	step [53/250], loss=85.3189
	step [54/250], loss=68.5272
	step [55/250], loss=75.8619
	step [56/250], loss=72.3011
	step [57/250], loss=88.8672
	step [58/250], loss=73.4445
	step [59/250], loss=65.5349
	step [60/250], loss=78.4493
	step [61/250], loss=69.5259
	step [62/250], loss=75.1929
	step [63/250], loss=74.5033
	step [64/250], loss=67.2954
	step [65/250], loss=68.0651
	step [66/250], loss=69.1562
	step [67/250], loss=81.5307
	step [68/250], loss=71.8922
	step [69/250], loss=73.7289
	step [70/250], loss=79.7480
	step [71/250], loss=82.7330
	step [72/250], loss=70.0647
	step [73/250], loss=80.7614
	step [74/250], loss=88.6918
	step [75/250], loss=63.6864
	step [76/250], loss=93.0520
	step [77/250], loss=88.0447
	step [78/250], loss=79.9899
	step [79/250], loss=58.9826
	step [80/250], loss=64.9891
	step [81/250], loss=79.2200
	step [82/250], loss=72.5838
	step [83/250], loss=85.2744
	step [84/250], loss=73.1265
	step [85/250], loss=83.8960
	step [86/250], loss=77.0785
	step [87/250], loss=70.6467
	step [88/250], loss=72.7152
	step [89/250], loss=74.7769
	step [90/250], loss=90.3050
	step [91/250], loss=75.0839
	step [92/250], loss=67.0117
	step [93/250], loss=71.7022
	step [94/250], loss=68.3153
	step [95/250], loss=67.3510
	step [96/250], loss=78.7623
	step [97/250], loss=67.3671
	step [98/250], loss=95.1599
	step [99/250], loss=58.5544
	step [100/250], loss=68.5892
	step [101/250], loss=70.7991
	step [102/250], loss=84.6472
	step [103/250], loss=75.5048
	step [104/250], loss=48.9186
	step [105/250], loss=81.6212
	step [106/250], loss=83.3427
	step [107/250], loss=77.2708
	step [108/250], loss=78.6291
	step [109/250], loss=65.8785
	step [110/250], loss=63.8134
	step [111/250], loss=71.0504
	step [112/250], loss=74.1722
	step [113/250], loss=83.0996
	step [114/250], loss=84.3590
	step [115/250], loss=67.3005
	step [116/250], loss=54.8173
	step [117/250], loss=72.7504
	step [118/250], loss=74.2365
	step [119/250], loss=88.0094
	step [120/250], loss=61.6302
	step [121/250], loss=71.9657
	step [122/250], loss=86.4133
	step [123/250], loss=76.7359
	step [124/250], loss=80.8291
	step [125/250], loss=63.7163
	step [126/250], loss=69.3746
	step [127/250], loss=58.0466
	step [128/250], loss=79.0066
	step [129/250], loss=71.4014
	step [130/250], loss=71.6347
	step [131/250], loss=77.3558
	step [132/250], loss=102.1185
	step [133/250], loss=53.3095
	step [134/250], loss=72.7954
	step [135/250], loss=76.8365
	step [136/250], loss=82.0291
	step [137/250], loss=74.0221
	step [138/250], loss=67.5494
	step [139/250], loss=69.1380
	step [140/250], loss=82.4665
	step [141/250], loss=92.4091
	step [142/250], loss=78.7272
	step [143/250], loss=72.2937
	step [144/250], loss=69.7929
	step [145/250], loss=75.7936
	step [146/250], loss=73.7722
	step [147/250], loss=66.0414
	step [148/250], loss=81.2225
	step [149/250], loss=70.5756
	step [150/250], loss=60.3708
	step [151/250], loss=73.8664
	step [152/250], loss=67.6663
	step [153/250], loss=66.6444
	step [154/250], loss=86.3570
	step [155/250], loss=65.4139
	step [156/250], loss=62.4896
	step [157/250], loss=73.8163
	step [158/250], loss=77.8832
	step [159/250], loss=70.8204
	step [160/250], loss=60.8835
	step [161/250], loss=63.1094
	step [162/250], loss=68.2650
	step [163/250], loss=68.6938
	step [164/250], loss=86.9946
	step [165/250], loss=73.4852
	step [166/250], loss=78.8795
	step [167/250], loss=68.2639
	step [168/250], loss=65.2754
	step [169/250], loss=54.4087
	step [170/250], loss=61.7109
	step [171/250], loss=67.8422
	step [172/250], loss=86.6213
	step [173/250], loss=59.4284
	step [174/250], loss=78.7204
	step [175/250], loss=80.7211
	step [176/250], loss=68.7197
	step [177/250], loss=84.4532
	step [178/250], loss=60.6024
	step [179/250], loss=70.7368
	step [180/250], loss=65.7659
	step [181/250], loss=53.5001
	step [182/250], loss=56.3757
	step [183/250], loss=99.1832
	step [184/250], loss=82.1589
	step [185/250], loss=67.4231
	step [186/250], loss=71.9156
	step [187/250], loss=78.7669
	step [188/250], loss=77.4535
	step [189/250], loss=70.3039
	step [190/250], loss=75.3141
	step [191/250], loss=62.9761
	step [192/250], loss=81.9808
	step [193/250], loss=64.9749
	step [194/250], loss=61.3234
	step [195/250], loss=86.8435
	step [196/250], loss=85.3018
	step [197/250], loss=73.4555
	step [198/250], loss=69.6610
	step [199/250], loss=64.0676
	step [200/250], loss=73.5522
	step [201/250], loss=71.1458
	step [202/250], loss=71.1024
	step [203/250], loss=64.2010
	step [204/250], loss=83.7396
	step [205/250], loss=75.1114
	step [206/250], loss=57.3390
	step [207/250], loss=87.3317
	step [208/250], loss=80.7393
	step [209/250], loss=81.3581
	step [210/250], loss=83.9727
	step [211/250], loss=79.2485
	step [212/250], loss=68.2789
	step [213/250], loss=53.2072
	step [214/250], loss=68.1759
	step [215/250], loss=64.7684
	step [216/250], loss=76.5085
	step [217/250], loss=73.6280
	step [218/250], loss=67.9702
	step [219/250], loss=69.5527
	step [220/250], loss=71.8707
	step [221/250], loss=82.5730
	step [222/250], loss=77.7399
	step [223/250], loss=77.2837
	step [224/250], loss=54.6661
	step [225/250], loss=68.3564
	step [226/250], loss=75.1773
	step [227/250], loss=69.1151
	step [228/250], loss=67.2442
	step [229/250], loss=70.3175
	step [230/250], loss=75.6745
	step [231/250], loss=61.9125
	step [232/250], loss=73.5892
	step [233/250], loss=79.7653
	step [234/250], loss=76.4856
	step [235/250], loss=86.5946
	step [236/250], loss=81.4664
	step [237/250], loss=69.1595
	step [238/250], loss=72.6526
	step [239/250], loss=68.6112
	step [240/250], loss=76.9300
	step [241/250], loss=82.6222
	step [242/250], loss=80.9579
	step [243/250], loss=78.5152
	step [244/250], loss=63.3504
	step [245/250], loss=68.7308
	step [246/250], loss=74.1319
	step [247/250], loss=76.7904
	step [248/250], loss=79.7520
	step [249/250], loss=74.9610
	step [250/250], loss=16.8040
	Evaluating
	loss=0.0069, precision=0.3256, recall=0.8626, f1=0.4727
saving model as: 0_saved_model.pth
Training epoch 95
	step [1/250], loss=62.1017
	step [2/250], loss=87.1220
	step [3/250], loss=54.6232
	step [4/250], loss=65.5513
	step [5/250], loss=64.3792
	step [6/250], loss=99.7824
	step [7/250], loss=78.5506
	step [8/250], loss=68.4253
	step [9/250], loss=83.0315
	step [10/250], loss=67.5375
	step [11/250], loss=62.9270
	step [12/250], loss=79.2322
	step [13/250], loss=73.7000
	step [14/250], loss=84.4576
	step [15/250], loss=64.9292
	step [16/250], loss=95.0041
	step [17/250], loss=71.3410
	step [18/250], loss=60.1875
	step [19/250], loss=68.1181
	step [20/250], loss=82.8696
	step [21/250], loss=66.8617
	step [22/250], loss=73.0128
	step [23/250], loss=70.3345
	step [24/250], loss=79.6819
	step [25/250], loss=66.4321
	step [26/250], loss=73.1982
	step [27/250], loss=68.0979
	step [28/250], loss=66.8026
	step [29/250], loss=72.1969
	step [30/250], loss=61.6704
	step [31/250], loss=65.8655
	step [32/250], loss=98.2052
	step [33/250], loss=70.2780
	step [34/250], loss=79.6838
	step [35/250], loss=61.7068
	step [36/250], loss=71.2292
	step [37/250], loss=60.1336
	step [38/250], loss=79.3629
	step [39/250], loss=68.3700
	step [40/250], loss=64.5482
	step [41/250], loss=74.0009
	step [42/250], loss=85.3038
	step [43/250], loss=84.8678
	step [44/250], loss=72.9993
	step [45/250], loss=81.2631
	step [46/250], loss=90.8133
	step [47/250], loss=94.6054
	step [48/250], loss=62.1136
	step [49/250], loss=74.1242
	step [50/250], loss=82.6259
	step [51/250], loss=75.4329
	step [52/250], loss=73.6388
	step [53/250], loss=64.9454
	step [54/250], loss=62.5912
	step [55/250], loss=73.7216
	step [56/250], loss=74.2694
	step [57/250], loss=99.5250
	step [58/250], loss=66.7224
	step [59/250], loss=78.3761
	step [60/250], loss=72.3078
	step [61/250], loss=84.6332
	step [62/250], loss=62.8509
	step [63/250], loss=61.7723
	step [64/250], loss=68.8735
	step [65/250], loss=75.5726
	step [66/250], loss=72.7310
	step [67/250], loss=74.8972
	step [68/250], loss=81.7309
	step [69/250], loss=81.3432
	step [70/250], loss=88.0265
	step [71/250], loss=76.5278
	step [72/250], loss=75.0361
	step [73/250], loss=79.6833
	step [74/250], loss=78.5264
	step [75/250], loss=61.9746
	step [76/250], loss=75.0987
	step [77/250], loss=86.4743
	step [78/250], loss=78.7303
	step [79/250], loss=69.5787
	step [80/250], loss=75.7934
	step [81/250], loss=60.0779
	step [82/250], loss=71.3155
	step [83/250], loss=78.6957
	step [84/250], loss=66.9667
	step [85/250], loss=70.9291
	step [86/250], loss=60.5556
	step [87/250], loss=73.3204
	step [88/250], loss=55.5622
	step [89/250], loss=90.0663
	step [90/250], loss=90.3132
	step [91/250], loss=70.7617
	step [92/250], loss=47.0037
	step [93/250], loss=77.4124
	step [94/250], loss=71.7615
	step [95/250], loss=83.2601
	step [96/250], loss=63.7423
	step [97/250], loss=82.3532
	step [98/250], loss=64.3679
	step [99/250], loss=69.9737
	step [100/250], loss=74.4640
	step [101/250], loss=63.6207
	step [102/250], loss=66.0252
	step [103/250], loss=72.4117
	step [104/250], loss=70.5643
	step [105/250], loss=80.8894
	step [106/250], loss=77.7869
	step [107/250], loss=81.4594
	step [108/250], loss=81.6992
	step [109/250], loss=68.7619
	step [110/250], loss=69.6847
	step [111/250], loss=75.6935
	step [112/250], loss=68.0311
	step [113/250], loss=83.6258
	step [114/250], loss=84.9078
	step [115/250], loss=70.4452
	step [116/250], loss=79.1026
	step [117/250], loss=74.8598
	step [118/250], loss=83.2596
	step [119/250], loss=72.0303
	step [120/250], loss=70.4323
	step [121/250], loss=76.3578
	step [122/250], loss=66.6389
	step [123/250], loss=73.3754
	step [124/250], loss=63.0372
	step [125/250], loss=81.6174
	step [126/250], loss=64.6384
	step [127/250], loss=70.2296
	step [128/250], loss=64.4155
	step [129/250], loss=64.2107
	step [130/250], loss=80.0234
	step [131/250], loss=80.2645
	step [132/250], loss=65.6992
	step [133/250], loss=66.6940
	step [134/250], loss=69.8526
	step [135/250], loss=94.8410
	step [136/250], loss=59.8516
	step [137/250], loss=78.7204
	step [138/250], loss=67.7804
	step [139/250], loss=76.7762
	step [140/250], loss=64.1461
	step [141/250], loss=72.5780
	step [142/250], loss=88.9646
	step [143/250], loss=79.0982
	step [144/250], loss=81.4714
	step [145/250], loss=81.0184
	step [146/250], loss=86.0628
	step [147/250], loss=69.6929
	step [148/250], loss=66.6819
	step [149/250], loss=82.7419
	step [150/250], loss=93.7839
	step [151/250], loss=74.1076
	step [152/250], loss=69.9100
	step [153/250], loss=67.8833
	step [154/250], loss=71.6598
	step [155/250], loss=65.7276
	step [156/250], loss=82.2887
	step [157/250], loss=73.0142
	step [158/250], loss=68.9078
	step [159/250], loss=60.5021
	step [160/250], loss=74.9348
	step [161/250], loss=78.3599
	step [162/250], loss=52.4004
	step [163/250], loss=88.1373
	step [164/250], loss=76.8743
	step [165/250], loss=79.8040
	step [166/250], loss=70.7432
	step [167/250], loss=70.6251
	step [168/250], loss=75.1545
	step [169/250], loss=69.9591
	step [170/250], loss=94.9540
	step [171/250], loss=80.7299
	step [172/250], loss=72.7769
	step [173/250], loss=87.0726
	step [174/250], loss=59.5972
	step [175/250], loss=79.8921
	step [176/250], loss=93.2337
	step [177/250], loss=75.6661
	step [178/250], loss=65.1800
	step [179/250], loss=69.2012
	step [180/250], loss=74.3766
	step [181/250], loss=59.5562
	step [182/250], loss=66.9713
	step [183/250], loss=66.5289
	step [184/250], loss=77.4669
	step [185/250], loss=74.9181
	step [186/250], loss=71.9866
	step [187/250], loss=64.3949
	step [188/250], loss=80.4567
	step [189/250], loss=55.8405
	step [190/250], loss=59.2677
	step [191/250], loss=89.8653
	step [192/250], loss=65.4328
	step [193/250], loss=83.2773
	step [194/250], loss=75.2759
	step [195/250], loss=56.8524
	step [196/250], loss=63.6982
	step [197/250], loss=70.8411
	step [198/250], loss=78.6414
	step [199/250], loss=66.5708
	step [200/250], loss=69.5890
	step [201/250], loss=69.8781
	step [202/250], loss=75.6006
	step [203/250], loss=79.7984
	step [204/250], loss=81.2421
	step [205/250], loss=93.5238
	step [206/250], loss=60.3639
	step [207/250], loss=73.5801
	step [208/250], loss=71.3851
	step [209/250], loss=57.2353
	step [210/250], loss=69.4006
	step [211/250], loss=73.5533
	step [212/250], loss=64.8059
	step [213/250], loss=55.6953
	step [214/250], loss=65.1660
	step [215/250], loss=72.8492
	step [216/250], loss=78.3358
	step [217/250], loss=62.7348
	step [218/250], loss=73.3788
	step [219/250], loss=76.2676
	step [220/250], loss=61.1920
	step [221/250], loss=73.3642
	step [222/250], loss=87.0329
	step [223/250], loss=54.7309
	step [224/250], loss=78.7600
	step [225/250], loss=64.6443
	step [226/250], loss=83.4058
	step [227/250], loss=63.5055
	step [228/250], loss=80.8090
	step [229/250], loss=82.8726
	step [230/250], loss=85.7561
	step [231/250], loss=81.0998
	step [232/250], loss=53.2006
	step [233/250], loss=65.8165
	step [234/250], loss=69.1252
	step [235/250], loss=77.7699
	step [236/250], loss=78.5058
	step [237/250], loss=63.2828
	step [238/250], loss=75.8899
	step [239/250], loss=77.4299
	step [240/250], loss=76.1922
	step [241/250], loss=69.9731
	step [242/250], loss=74.3869
	step [243/250], loss=67.6920
	step [244/250], loss=80.5098
	step [245/250], loss=71.0225
	step [246/250], loss=81.3972
	step [247/250], loss=55.4386
	step [248/250], loss=55.6174
	step [249/250], loss=74.7986
	step [250/250], loss=8.1362
	Evaluating
	loss=0.0094, precision=0.2204, recall=0.8619, f1=0.3511
Training epoch 96
	step [1/250], loss=85.5906
	step [2/250], loss=72.3774
	step [3/250], loss=74.0469
	step [4/250], loss=60.7232
	step [5/250], loss=74.6358
	step [6/250], loss=75.0770
	step [7/250], loss=73.1421
	step [8/250], loss=84.1048
	step [9/250], loss=73.3954
	step [10/250], loss=63.9650
	step [11/250], loss=76.7059
	step [12/250], loss=66.4746
	step [13/250], loss=83.4975
	step [14/250], loss=82.7803
	step [15/250], loss=57.8231
	step [16/250], loss=69.4331
	step [17/250], loss=86.7847
	step [18/250], loss=88.1726
	step [19/250], loss=61.7967
	step [20/250], loss=57.2624
	step [21/250], loss=73.8730
	step [22/250], loss=75.3816
	step [23/250], loss=86.0060
	step [24/250], loss=62.9211
	step [25/250], loss=57.4556
	step [26/250], loss=74.4111
	step [27/250], loss=80.8178
	step [28/250], loss=58.5081
	step [29/250], loss=70.6278
	step [30/250], loss=69.9003
	step [31/250], loss=86.0085
	step [32/250], loss=75.2404
	step [33/250], loss=84.5206
	step [34/250], loss=62.4876
	step [35/250], loss=68.9506
	step [36/250], loss=76.1605
	step [37/250], loss=72.7457
	step [38/250], loss=70.2253
	step [39/250], loss=81.6132
	step [40/250], loss=68.4548
	step [41/250], loss=62.2046
	step [42/250], loss=69.3014
	step [43/250], loss=69.6679
	step [44/250], loss=68.3151
	step [45/250], loss=73.9078
	step [46/250], loss=72.5320
	step [47/250], loss=82.6931
	step [48/250], loss=64.1214
	step [49/250], loss=64.3426
	step [50/250], loss=82.9489
	step [51/250], loss=72.0230
	step [52/250], loss=87.2590
	step [53/250], loss=89.2045
	step [54/250], loss=87.0859
	step [55/250], loss=72.0290
	step [56/250], loss=79.8675
	step [57/250], loss=89.4570
	step [58/250], loss=71.0667
	step [59/250], loss=80.4366
	step [60/250], loss=75.7318
	step [61/250], loss=73.4287
	step [62/250], loss=60.5516
	step [63/250], loss=72.3533
	step [64/250], loss=79.6113
	step [65/250], loss=63.8847
	step [66/250], loss=71.1930
	step [67/250], loss=65.8873
	step [68/250], loss=71.6795
	step [69/250], loss=83.3804
	step [70/250], loss=75.6587
	step [71/250], loss=67.4852
	step [72/250], loss=68.1435
	step [73/250], loss=78.8204
	step [74/250], loss=76.8737
	step [75/250], loss=76.7835
	step [76/250], loss=78.5801
	step [77/250], loss=68.4587
	step [78/250], loss=72.8289
	step [79/250], loss=70.1139
	step [80/250], loss=76.4261
	step [81/250], loss=81.8489
	step [82/250], loss=64.3246
	step [83/250], loss=59.2422
	step [84/250], loss=71.8983
	step [85/250], loss=79.7922
	step [86/250], loss=81.4807
	step [87/250], loss=76.6236
	step [88/250], loss=76.6041
	step [89/250], loss=63.7377
	step [90/250], loss=79.1493
	step [91/250], loss=77.2723
	step [92/250], loss=55.6291
	step [93/250], loss=79.4248
	step [94/250], loss=87.0256
	step [95/250], loss=86.9819
	step [96/250], loss=80.6118
	step [97/250], loss=75.9457
	step [98/250], loss=77.0766
	step [99/250], loss=76.8611
	step [100/250], loss=76.9850
	step [101/250], loss=69.0386
	step [102/250], loss=93.3047
	step [103/250], loss=84.4146
	step [104/250], loss=74.2350
	step [105/250], loss=85.9848
	step [106/250], loss=80.7560
	step [107/250], loss=79.3514
	step [108/250], loss=63.8131
	step [109/250], loss=84.6146
	step [110/250], loss=66.2543
	step [111/250], loss=64.8857
	step [112/250], loss=90.7272
	step [113/250], loss=63.8526
	step [114/250], loss=65.3704
	step [115/250], loss=57.2390
	step [116/250], loss=87.9631
	step [117/250], loss=70.6413
	step [118/250], loss=74.2616
	step [119/250], loss=65.7669
	step [120/250], loss=69.2413
	step [121/250], loss=76.7852
	step [122/250], loss=59.4586
	step [123/250], loss=59.3565
	step [124/250], loss=67.5675
	step [125/250], loss=63.5109
	step [126/250], loss=77.5940
	step [127/250], loss=77.9813
	step [128/250], loss=83.2755
	step [129/250], loss=76.0304
	step [130/250], loss=76.9250
	step [131/250], loss=79.9493
	step [132/250], loss=66.4716
	step [133/250], loss=80.3513
	step [134/250], loss=68.3350
	step [135/250], loss=71.9284
	step [136/250], loss=67.1022
	step [137/250], loss=63.8935
	step [138/250], loss=84.2091
	step [139/250], loss=76.3959
	step [140/250], loss=66.7946
	step [141/250], loss=66.3463
	step [142/250], loss=69.4217
	step [143/250], loss=73.3597
	step [144/250], loss=78.6201
	step [145/250], loss=72.4057
	step [146/250], loss=69.1646
	step [147/250], loss=77.5317
	step [148/250], loss=83.2274
	step [149/250], loss=83.0839
	step [150/250], loss=62.3177
	step [151/250], loss=66.7975
	step [152/250], loss=77.8880
	step [153/250], loss=74.7954
	step [154/250], loss=63.2822
	step [155/250], loss=76.7692
	step [156/250], loss=60.0884
	step [157/250], loss=61.1086
	step [158/250], loss=74.6626
	step [159/250], loss=60.2345
	step [160/250], loss=61.8790
	step [161/250], loss=75.7590
	step [162/250], loss=67.8275
	step [163/250], loss=70.8402
	step [164/250], loss=67.2114
	step [165/250], loss=76.3266
	step [166/250], loss=82.7007
	step [167/250], loss=88.3026
	step [168/250], loss=68.0711
	step [169/250], loss=87.7751
	step [170/250], loss=78.2727
	step [171/250], loss=70.6218
	step [172/250], loss=98.4494
	step [173/250], loss=80.8328
	step [174/250], loss=76.7734
	step [175/250], loss=78.6126
	step [176/250], loss=67.0523
	step [177/250], loss=75.3459
	step [178/250], loss=82.8288
	step [179/250], loss=62.0518
	step [180/250], loss=68.6358
	step [181/250], loss=74.3800
	step [182/250], loss=64.5284
	step [183/250], loss=67.8558
	step [184/250], loss=64.0127
	step [185/250], loss=59.5401
	step [186/250], loss=75.0251
	step [187/250], loss=77.1101
	step [188/250], loss=60.7174
	step [189/250], loss=53.1699
	step [190/250], loss=77.1909
	step [191/250], loss=76.0134
	step [192/250], loss=76.4930
	step [193/250], loss=76.3708
	step [194/250], loss=64.9571
	step [195/250], loss=65.5785
	step [196/250], loss=71.0818
	step [197/250], loss=78.6196
	step [198/250], loss=54.8363
	step [199/250], loss=83.0854
	step [200/250], loss=75.3373
	step [201/250], loss=80.1625
	step [202/250], loss=84.3842
	step [203/250], loss=75.3830
	step [204/250], loss=71.8157
	step [205/250], loss=82.7610
	step [206/250], loss=70.6817
	step [207/250], loss=66.2432
	step [208/250], loss=64.3335
	step [209/250], loss=55.9632
	step [210/250], loss=72.6260
	step [211/250], loss=61.0563
	step [212/250], loss=75.5284
	step [213/250], loss=69.4128
	step [214/250], loss=57.4956
	step [215/250], loss=69.7146
	step [216/250], loss=68.7915
	step [217/250], loss=76.1812
	step [218/250], loss=79.7171
	step [219/250], loss=78.0647
	step [220/250], loss=77.1328
	step [221/250], loss=81.7179
	step [222/250], loss=77.7571
	step [223/250], loss=82.7792
	step [224/250], loss=67.9830
	step [225/250], loss=63.3219
	step [226/250], loss=75.9212
	step [227/250], loss=75.5339
	step [228/250], loss=72.0841
	step [229/250], loss=64.2976
	step [230/250], loss=65.8509
	step [231/250], loss=86.2329
	step [232/250], loss=73.7796
	step [233/250], loss=77.7587
	step [234/250], loss=64.2519
	step [235/250], loss=88.9983
	step [236/250], loss=58.7288
	step [237/250], loss=81.2573
	step [238/250], loss=76.2127
	step [239/250], loss=78.8164
	step [240/250], loss=69.7680
	step [241/250], loss=76.4523
	step [242/250], loss=78.4243
	step [243/250], loss=77.4467
	step [244/250], loss=72.0383
	step [245/250], loss=71.8834
	step [246/250], loss=68.6914
	step [247/250], loss=88.7420
	step [248/250], loss=73.7809
	step [249/250], loss=64.3435
	step [250/250], loss=12.3923
	Evaluating
	loss=0.0073, precision=0.2847, recall=0.8586, f1=0.4276
Training epoch 97
	step [1/250], loss=71.3581
	step [2/250], loss=70.1778
	step [3/250], loss=79.7626
	step [4/250], loss=78.3312
	step [5/250], loss=65.8934
	step [6/250], loss=65.8347
	step [7/250], loss=68.3383
	step [8/250], loss=69.3255
	step [9/250], loss=72.5585
	step [10/250], loss=75.3556
	step [11/250], loss=68.1029
	step [12/250], loss=73.6903
	step [13/250], loss=82.9008
	step [14/250], loss=63.7019
	step [15/250], loss=64.4722
	step [16/250], loss=53.7508
	step [17/250], loss=70.0334
	step [18/250], loss=92.1742
	step [19/250], loss=73.8335
	step [20/250], loss=80.9008
	step [21/250], loss=72.6398
	step [22/250], loss=71.1669
	step [23/250], loss=61.2647
	step [24/250], loss=90.0862
	step [25/250], loss=61.8625
	step [26/250], loss=75.2412
	step [27/250], loss=76.4377
	step [28/250], loss=83.7802
	step [29/250], loss=78.8611
	step [30/250], loss=67.6565
	step [31/250], loss=49.9027
	step [32/250], loss=76.5767
	step [33/250], loss=77.4310
	step [34/250], loss=65.7741
	step [35/250], loss=65.5321
	step [36/250], loss=62.6820
	step [37/250], loss=67.5225
	step [38/250], loss=70.7428
	step [39/250], loss=69.4764
	step [40/250], loss=75.7645
	step [41/250], loss=73.2883
	step [42/250], loss=80.8504
	step [43/250], loss=67.7838
	step [44/250], loss=66.1477
	step [45/250], loss=85.4259
	step [46/250], loss=71.6060
	step [47/250], loss=71.7135
	step [48/250], loss=66.8074
	step [49/250], loss=90.2228
	step [50/250], loss=77.7871
	step [51/250], loss=70.0730
	step [52/250], loss=66.7848
	step [53/250], loss=71.5201
	step [54/250], loss=70.1582
	step [55/250], loss=63.5574
	step [56/250], loss=58.1944
	step [57/250], loss=71.1219
	step [58/250], loss=77.0346
	step [59/250], loss=91.0528
	step [60/250], loss=78.5223
	step [61/250], loss=83.1657
	step [62/250], loss=79.7984
	step [63/250], loss=79.1332
	step [64/250], loss=65.2398
	step [65/250], loss=68.2362
	step [66/250], loss=56.1009
	step [67/250], loss=73.6951
	step [68/250], loss=86.6423
	step [69/250], loss=75.2257
	step [70/250], loss=93.0258
	step [71/250], loss=72.0905
	step [72/250], loss=80.9531
	step [73/250], loss=68.1215
	step [74/250], loss=74.9678
	step [75/250], loss=77.1126
	step [76/250], loss=91.6456
	step [77/250], loss=71.8928
	step [78/250], loss=77.0122
	step [79/250], loss=98.4502
	step [80/250], loss=70.9031
	step [81/250], loss=64.6650
	step [82/250], loss=62.9434
	step [83/250], loss=71.7390
	step [84/250], loss=76.1988
	step [85/250], loss=89.5733
	step [86/250], loss=77.5009
	step [87/250], loss=83.5680
	step [88/250], loss=73.9710
	step [89/250], loss=69.9595
	step [90/250], loss=61.3287
	step [91/250], loss=75.9253
	step [92/250], loss=73.0989
	step [93/250], loss=82.9930
	step [94/250], loss=69.7558
	step [95/250], loss=67.6224
	step [96/250], loss=72.2390
	step [97/250], loss=62.1036
	step [98/250], loss=59.9874
	step [99/250], loss=63.4684
	step [100/250], loss=65.5564
	step [101/250], loss=75.5191
	step [102/250], loss=72.2545
	step [103/250], loss=90.9047
	step [104/250], loss=91.1040
	step [105/250], loss=74.8764
	step [106/250], loss=61.7316
	step [107/250], loss=69.3687
	step [108/250], loss=66.4853
	step [109/250], loss=68.0295
	step [110/250], loss=74.1092
	step [111/250], loss=61.0516
	step [112/250], loss=60.5719
	step [113/250], loss=69.6919
	step [114/250], loss=64.8508
	step [115/250], loss=76.1199
	step [116/250], loss=62.5733
	step [117/250], loss=55.1594
	step [118/250], loss=87.8455
	step [119/250], loss=79.6700
	step [120/250], loss=64.8136
	step [121/250], loss=59.4531
	step [122/250], loss=60.7770
	step [123/250], loss=65.7180
	step [124/250], loss=58.7809
	step [125/250], loss=70.8931
	step [126/250], loss=75.1477
	step [127/250], loss=71.7533
	step [128/250], loss=89.2949
	step [129/250], loss=68.0105
	step [130/250], loss=68.4305
	step [131/250], loss=71.9549
	step [132/250], loss=71.6956
	step [133/250], loss=74.5883
	step [134/250], loss=88.4857
	step [135/250], loss=69.7847
	step [136/250], loss=65.5811
	step [137/250], loss=77.6942
	step [138/250], loss=60.3467
	step [139/250], loss=74.0668
	step [140/250], loss=68.2595
	step [141/250], loss=67.7939
	step [142/250], loss=69.5193
	step [143/250], loss=81.3682
	step [144/250], loss=82.1306
	step [145/250], loss=74.7355
	step [146/250], loss=57.6268
	step [147/250], loss=85.3305
	step [148/250], loss=80.3580
	step [149/250], loss=78.8543
	step [150/250], loss=61.7980
	step [151/250], loss=73.5226
	step [152/250], loss=62.3972
	step [153/250], loss=71.1386
	step [154/250], loss=63.1356
	step [155/250], loss=82.5900
	step [156/250], loss=64.1657
	step [157/250], loss=86.6094
	step [158/250], loss=67.9920
	step [159/250], loss=75.3454
	step [160/250], loss=72.5756
	step [161/250], loss=68.9244
	step [162/250], loss=72.7987
	step [163/250], loss=66.1024
	step [164/250], loss=68.9165
	step [165/250], loss=92.5812
	step [166/250], loss=71.1279
	step [167/250], loss=63.3705
	step [168/250], loss=69.8024
	step [169/250], loss=76.7582
	step [170/250], loss=73.4906
	step [171/250], loss=91.8147
	step [172/250], loss=81.7189
	step [173/250], loss=70.6348
	step [174/250], loss=84.7328
	step [175/250], loss=67.5518
	step [176/250], loss=59.6650
	step [177/250], loss=64.2619
	step [178/250], loss=64.3891
	step [179/250], loss=96.8701
	step [180/250], loss=77.8005
	step [181/250], loss=88.7603
	step [182/250], loss=63.7497
	step [183/250], loss=81.3564
	step [184/250], loss=79.3041
	step [185/250], loss=61.0317
	step [186/250], loss=71.0053
	step [187/250], loss=74.6687
	step [188/250], loss=80.4256
	step [189/250], loss=82.0463
	step [190/250], loss=68.7599
	step [191/250], loss=84.9287
	step [192/250], loss=88.5176
	step [193/250], loss=85.7710
	step [194/250], loss=61.8297
	step [195/250], loss=71.0697
	step [196/250], loss=84.0262
	step [197/250], loss=73.3982
	step [198/250], loss=58.8728
	step [199/250], loss=76.5284
	step [200/250], loss=78.6550
	step [201/250], loss=83.3312
	step [202/250], loss=63.7719
	step [203/250], loss=83.3896
	step [204/250], loss=72.9093
	step [205/250], loss=83.2442
	step [206/250], loss=70.1828
	step [207/250], loss=78.1356
	step [208/250], loss=60.7776
	step [209/250], loss=61.0354
	step [210/250], loss=67.5503
	step [211/250], loss=73.9060
	step [212/250], loss=73.0656
	step [213/250], loss=72.4591
	step [214/250], loss=64.7169
	step [215/250], loss=81.9874
	step [216/250], loss=86.5473
	step [217/250], loss=76.9044
	step [218/250], loss=78.0010
	step [219/250], loss=76.4192
	step [220/250], loss=63.6817
	step [221/250], loss=73.3306
	step [222/250], loss=66.1478
	step [223/250], loss=65.6285
	step [224/250], loss=76.8464
	step [225/250], loss=63.0632
	step [226/250], loss=70.0379
	step [227/250], loss=67.6624
	step [228/250], loss=68.9260
	step [229/250], loss=70.6860
	step [230/250], loss=85.2027
	step [231/250], loss=78.4492
	step [232/250], loss=81.9915
	step [233/250], loss=63.6304
	step [234/250], loss=66.1592
	step [235/250], loss=64.4074
	step [236/250], loss=82.8079
	step [237/250], loss=74.9545
	step [238/250], loss=61.9614
	step [239/250], loss=73.5597
	step [240/250], loss=67.3988
	step [241/250], loss=65.2886
	step [242/250], loss=64.5161
	step [243/250], loss=66.7807
	step [244/250], loss=77.3261
	step [245/250], loss=70.6584
	step [246/250], loss=77.4437
	step [247/250], loss=66.8486
	step [248/250], loss=89.8653
	step [249/250], loss=59.7493
	step [250/250], loss=12.3569
	Evaluating
	loss=0.0060, precision=0.3508, recall=0.8437, f1=0.4956
saving model as: 0_saved_model.pth
Training epoch 98
	step [1/250], loss=82.0817
	step [2/250], loss=81.1790
	step [3/250], loss=78.5587
	step [4/250], loss=83.1571
	step [5/250], loss=82.1819
	step [6/250], loss=74.6622
	step [7/250], loss=76.1910
	step [8/250], loss=78.9831
	step [9/250], loss=51.9067
	step [10/250], loss=71.8673
	step [11/250], loss=74.9290
	step [12/250], loss=71.9907
	step [13/250], loss=67.5769
	step [14/250], loss=77.8191
	step [15/250], loss=82.7971
	step [16/250], loss=74.3529
	step [17/250], loss=55.0841
	step [18/250], loss=67.0837
	step [19/250], loss=66.9487
	step [20/250], loss=73.4230
	step [21/250], loss=70.7885
	step [22/250], loss=75.4258
	step [23/250], loss=87.1861
	step [24/250], loss=82.9647
	step [25/250], loss=83.5699
	step [26/250], loss=77.7793
	step [27/250], loss=80.7438
	step [28/250], loss=71.2739
	step [29/250], loss=66.1405
	step [30/250], loss=67.6298
	step [31/250], loss=66.2319
	step [32/250], loss=79.7917
	step [33/250], loss=79.1321
	step [34/250], loss=57.2832
	step [35/250], loss=71.8411
	step [36/250], loss=67.3096
	step [37/250], loss=74.2018
	step [38/250], loss=66.3397
	step [39/250], loss=89.8450
	step [40/250], loss=78.4431
	step [41/250], loss=67.3099
	step [42/250], loss=79.6814
	step [43/250], loss=63.8294
	step [44/250], loss=86.3391
	step [45/250], loss=77.2778
	step [46/250], loss=80.4395
	step [47/250], loss=92.8080
	step [48/250], loss=96.7791
	step [49/250], loss=83.6126
	step [50/250], loss=61.3782
	step [51/250], loss=61.4634
	step [52/250], loss=72.3244
	step [53/250], loss=78.2754
	step [54/250], loss=91.1747
	step [55/250], loss=67.2134
	step [56/250], loss=64.3830
	step [57/250], loss=77.5134
	step [58/250], loss=80.6390
	step [59/250], loss=82.7892
	step [60/250], loss=82.3695
	step [61/250], loss=79.0302
	step [62/250], loss=77.5100
	step [63/250], loss=57.8278
	step [64/250], loss=71.1973
	step [65/250], loss=69.8252
	step [66/250], loss=81.1657
	step [67/250], loss=71.0624
	step [68/250], loss=74.2606
	step [69/250], loss=65.9882
	step [70/250], loss=82.1078
	step [71/250], loss=71.9666
	step [72/250], loss=67.3377
	step [73/250], loss=57.5359
	step [74/250], loss=86.2340
	step [75/250], loss=67.6085
	step [76/250], loss=73.2241
	step [77/250], loss=60.3984
	step [78/250], loss=74.4608
	step [79/250], loss=65.2529
	step [80/250], loss=74.7422
	step [81/250], loss=82.0146
	step [82/250], loss=76.8190
	step [83/250], loss=73.9306
	step [84/250], loss=68.9448
	step [85/250], loss=69.9174
	step [86/250], loss=70.6113
	step [87/250], loss=71.1350
	step [88/250], loss=75.3348
	step [89/250], loss=87.7702
	step [90/250], loss=57.3879
	step [91/250], loss=76.4634
	step [92/250], loss=81.8534
	step [93/250], loss=69.7668
	step [94/250], loss=74.4734
	step [95/250], loss=76.4496
	step [96/250], loss=81.9572
	step [97/250], loss=87.0459
	step [98/250], loss=77.7199
	step [99/250], loss=71.7456
	step [100/250], loss=72.5372
	step [101/250], loss=69.4211
	step [102/250], loss=76.3987
	step [103/250], loss=67.9517
	step [104/250], loss=65.8210
	step [105/250], loss=85.9503
	step [106/250], loss=78.6457
	step [107/250], loss=68.3870
	step [108/250], loss=64.2110
	step [109/250], loss=79.2376
	step [110/250], loss=75.3285
	step [111/250], loss=66.4350
	step [112/250], loss=75.8032
	step [113/250], loss=87.9712
	step [114/250], loss=66.6101
	step [115/250], loss=66.3379
	step [116/250], loss=82.2393
	step [117/250], loss=66.9137
	step [118/250], loss=68.5446
	step [119/250], loss=73.7197
	step [120/250], loss=82.3017
	step [121/250], loss=72.9441
	step [122/250], loss=75.8374
	step [123/250], loss=64.1190
	step [124/250], loss=85.3227
	step [125/250], loss=83.0218
	step [126/250], loss=79.0974
	step [127/250], loss=65.1756
	step [128/250], loss=69.5189
	step [129/250], loss=63.7886
	step [130/250], loss=69.6233
	step [131/250], loss=87.5708
	step [132/250], loss=71.8987
	step [133/250], loss=79.7582
	step [134/250], loss=79.9736
	step [135/250], loss=61.6438
	step [136/250], loss=83.8727
	step [137/250], loss=67.6986
	step [138/250], loss=69.0865
	step [139/250], loss=65.7395
	step [140/250], loss=72.3822
	step [141/250], loss=72.4158
	step [142/250], loss=74.3090
	step [143/250], loss=80.5826
	step [144/250], loss=81.4123
	step [145/250], loss=84.8956
	step [146/250], loss=72.0863
	step [147/250], loss=77.7454
	step [148/250], loss=72.1659
	step [149/250], loss=68.4514
	step [150/250], loss=71.6003
	step [151/250], loss=53.3986
	step [152/250], loss=60.8121
	step [153/250], loss=76.1788
	step [154/250], loss=74.3045
	step [155/250], loss=80.0457
	step [156/250], loss=66.0161
	step [157/250], loss=64.6084
	step [158/250], loss=62.8029
	step [159/250], loss=79.4721
	step [160/250], loss=70.4220
	step [161/250], loss=68.1114
	step [162/250], loss=76.8399
	step [163/250], loss=69.7196
	step [164/250], loss=73.4330
	step [165/250], loss=79.5947
	step [166/250], loss=91.7290
	step [167/250], loss=87.1023
	step [168/250], loss=73.2522
	step [169/250], loss=68.3819
	step [170/250], loss=72.0517
	step [171/250], loss=84.8695
	step [172/250], loss=58.3405
	step [173/250], loss=76.2686
	step [174/250], loss=58.3318
	step [175/250], loss=64.7243
	step [176/250], loss=70.3354
	step [177/250], loss=70.1694
	step [178/250], loss=70.7355
	step [179/250], loss=70.2673
	step [180/250], loss=62.7881
	step [181/250], loss=97.5267
	step [182/250], loss=74.6424
	step [183/250], loss=77.5557
	step [184/250], loss=77.7687
	step [185/250], loss=82.1349
	step [186/250], loss=75.4318
	step [187/250], loss=69.6891
	step [188/250], loss=78.3201
	step [189/250], loss=66.9623
	step [190/250], loss=69.1750
	step [191/250], loss=76.2668
	step [192/250], loss=72.9703
	step [193/250], loss=56.9153
	step [194/250], loss=74.0188
	step [195/250], loss=80.8062
	step [196/250], loss=72.0736
	step [197/250], loss=63.9806
	step [198/250], loss=87.4093
	step [199/250], loss=56.1784
	step [200/250], loss=62.4131
	step [201/250], loss=68.3801
	step [202/250], loss=76.8030
	step [203/250], loss=57.4842
	step [204/250], loss=73.5181
	step [205/250], loss=80.6933
	step [206/250], loss=68.9076
	step [207/250], loss=74.6230
	step [208/250], loss=81.1511
	step [209/250], loss=79.4772
	step [210/250], loss=55.4197
	step [211/250], loss=62.5664
	step [212/250], loss=53.4297
	step [213/250], loss=80.2899
	step [214/250], loss=88.8496
	step [215/250], loss=89.9534
	step [216/250], loss=63.9019
	step [217/250], loss=79.7334
	step [218/250], loss=68.7657
	step [219/250], loss=69.5037
	step [220/250], loss=71.7553
	step [221/250], loss=72.1532
	step [222/250], loss=68.5865
	step [223/250], loss=64.9199
	step [224/250], loss=69.4809
	step [225/250], loss=64.2335
	step [226/250], loss=79.9357
	step [227/250], loss=61.8346
	step [228/250], loss=83.7251
	step [229/250], loss=66.1710
	step [230/250], loss=56.2081
	step [231/250], loss=67.8293
	step [232/250], loss=77.2368
	step [233/250], loss=66.4352
	step [234/250], loss=56.8136
	step [235/250], loss=65.4313
	step [236/250], loss=78.2171
	step [237/250], loss=74.8413
	step [238/250], loss=73.5485
	step [239/250], loss=85.2318
	step [240/250], loss=64.2241
	step [241/250], loss=70.3385
	step [242/250], loss=82.3435
	step [243/250], loss=82.4575
	step [244/250], loss=64.8135
	step [245/250], loss=62.3872
	step [246/250], loss=83.3244
	step [247/250], loss=55.2277
	step [248/250], loss=72.1496
	step [249/250], loss=57.0242
	step [250/250], loss=11.3528
	Evaluating
	loss=0.0067, precision=0.3285, recall=0.8509, f1=0.4740
Training epoch 99
	step [1/250], loss=88.0517
	step [2/250], loss=68.4983
	step [3/250], loss=77.1105
	step [4/250], loss=74.7661
	step [5/250], loss=75.1300
	step [6/250], loss=59.7449
	step [7/250], loss=80.1209
	step [8/250], loss=72.6920
	step [9/250], loss=86.3807
	step [10/250], loss=70.7271
	step [11/250], loss=68.9111
	step [12/250], loss=68.2551
	step [13/250], loss=76.3819
	step [14/250], loss=67.9353
	step [15/250], loss=68.9600
	step [16/250], loss=75.7146
	step [17/250], loss=69.4416
	step [18/250], loss=81.7752
	step [19/250], loss=75.0722
	step [20/250], loss=62.3780
	step [21/250], loss=54.9872
	step [22/250], loss=71.7300
	step [23/250], loss=79.6321
	step [24/250], loss=56.6101
	step [25/250], loss=62.8698
	step [26/250], loss=71.5813
	step [27/250], loss=70.4659
	step [28/250], loss=75.2680
	step [29/250], loss=75.5547
	step [30/250], loss=75.7073
	step [31/250], loss=72.3551
	step [32/250], loss=65.1391
	step [33/250], loss=63.1555
	step [34/250], loss=77.9681
	step [35/250], loss=79.5144
	step [36/250], loss=91.9403
	step [37/250], loss=79.2113
	step [38/250], loss=77.6086
	step [39/250], loss=74.8560
	step [40/250], loss=61.6616
	step [41/250], loss=82.2477
	step [42/250], loss=72.3886
	step [43/250], loss=70.7677
	step [44/250], loss=67.7713
	step [45/250], loss=87.8177
	step [46/250], loss=75.7448
	step [47/250], loss=83.3087
	step [48/250], loss=80.9252
	step [49/250], loss=75.1313
	step [50/250], loss=68.0842
	step [51/250], loss=77.2291
	step [52/250], loss=68.2465
	step [53/250], loss=81.2871
	step [54/250], loss=65.8734
	step [55/250], loss=72.9264
	step [56/250], loss=84.3028
	step [57/250], loss=59.4949
	step [58/250], loss=70.4951
	step [59/250], loss=61.2271
	step [60/250], loss=64.3693
	step [61/250], loss=71.9972
	step [62/250], loss=81.3609
	step [63/250], loss=78.5901
	step [64/250], loss=65.2158
	step [65/250], loss=64.9686
	step [66/250], loss=81.1656
	step [67/250], loss=75.1371
	step [68/250], loss=74.0895
	step [69/250], loss=78.2546
	step [70/250], loss=61.6900
	step [71/250], loss=77.6684
	step [72/250], loss=74.9752
	step [73/250], loss=82.1246
	step [74/250], loss=79.5320
	step [75/250], loss=66.5922
	step [76/250], loss=77.6421
	step [77/250], loss=88.1404
	step [78/250], loss=75.4106
	step [79/250], loss=79.1561
	step [80/250], loss=82.2549
	step [81/250], loss=73.6366
	step [82/250], loss=67.9763
	step [83/250], loss=60.1364
	step [84/250], loss=57.3579
	step [85/250], loss=63.7143
	step [86/250], loss=60.5663
	step [87/250], loss=69.3595
	step [88/250], loss=67.8442
	step [89/250], loss=84.7478
	step [90/250], loss=64.7049
	step [91/250], loss=73.6085
	step [92/250], loss=80.2976
	step [93/250], loss=86.0798
	step [94/250], loss=61.1841
	step [95/250], loss=61.2331
	step [96/250], loss=63.5863
	step [97/250], loss=81.2941
	step [98/250], loss=70.8106
	step [99/250], loss=66.0481
	step [100/250], loss=78.9243
	step [101/250], loss=73.6758
	step [102/250], loss=88.8077
	step [103/250], loss=81.4013
	step [104/250], loss=86.9191
	step [105/250], loss=69.4002
	step [106/250], loss=74.1479
	step [107/250], loss=72.5982
	step [108/250], loss=95.0009
	step [109/250], loss=81.4258
	step [110/250], loss=69.2806
	step [111/250], loss=65.0279
	step [112/250], loss=72.8743
	step [113/250], loss=74.8953
	step [114/250], loss=70.4051
	step [115/250], loss=75.5851
	step [116/250], loss=92.0772
	step [117/250], loss=100.8620
	step [118/250], loss=80.1653
	step [119/250], loss=63.7908
	step [120/250], loss=75.1641
	step [121/250], loss=72.6393
	step [122/250], loss=64.8420
	step [123/250], loss=76.8998
	step [124/250], loss=65.0872
	step [125/250], loss=60.5743
	step [126/250], loss=71.9576
	step [127/250], loss=64.8567
	step [128/250], loss=87.5171
	step [129/250], loss=84.4707
	step [130/250], loss=73.0563
	step [131/250], loss=61.6026
	step [132/250], loss=78.8467
	step [133/250], loss=79.1015
	step [134/250], loss=70.2787
	step [135/250], loss=64.0222
	step [136/250], loss=72.6301
	step [137/250], loss=60.3748
	step [138/250], loss=83.5481
	step [139/250], loss=57.6192
	step [140/250], loss=91.5741
	step [141/250], loss=85.4458
	step [142/250], loss=55.0524
	step [143/250], loss=68.0936
	step [144/250], loss=50.8084
	step [145/250], loss=83.8635
	step [146/250], loss=64.3303
	step [147/250], loss=68.6745
	step [148/250], loss=65.7435
	step [149/250], loss=63.9852
	step [150/250], loss=77.4548
	step [151/250], loss=79.0115
	step [152/250], loss=70.5733
	step [153/250], loss=82.6621
	step [154/250], loss=78.5563
	step [155/250], loss=66.1499
	step [156/250], loss=65.7048
	step [157/250], loss=60.1383
	step [158/250], loss=76.0596
	step [159/250], loss=69.6538
	step [160/250], loss=62.7527
	step [161/250], loss=81.0057
	step [162/250], loss=65.1724
	step [163/250], loss=65.2982
	step [164/250], loss=87.6656
	step [165/250], loss=67.0105
	step [166/250], loss=60.9248
	step [167/250], loss=64.1804
	step [168/250], loss=81.9554
	step [169/250], loss=63.0264
	step [170/250], loss=59.8081
	step [171/250], loss=71.3760
	step [172/250], loss=69.4815
	step [173/250], loss=64.0928
	step [174/250], loss=56.7838
	step [175/250], loss=71.0948
	step [176/250], loss=84.8311
	step [177/250], loss=83.6500
	step [178/250], loss=75.7773
	step [179/250], loss=67.8002
	step [180/250], loss=68.1647
	step [181/250], loss=65.1444
	step [182/250], loss=62.9259
	step [183/250], loss=79.9901
	step [184/250], loss=76.0363
	step [185/250], loss=82.9462
	step [186/250], loss=75.5152
	step [187/250], loss=71.9876
	step [188/250], loss=62.5902
	step [189/250], loss=82.4667
	step [190/250], loss=65.0493
	step [191/250], loss=66.0858
	step [192/250], loss=74.7249
	step [193/250], loss=69.8836
	step [194/250], loss=80.7946
	step [195/250], loss=66.8908
	step [196/250], loss=82.0541
	step [197/250], loss=75.7497
	step [198/250], loss=79.9252
	step [199/250], loss=82.6178
	step [200/250], loss=73.5744
	step [201/250], loss=67.8499
	step [202/250], loss=77.5932
	step [203/250], loss=85.1054
	step [204/250], loss=68.1364
	step [205/250], loss=65.4939
	step [206/250], loss=65.6927
	step [207/250], loss=57.5073
	step [208/250], loss=75.3574
	step [209/250], loss=65.7981
	step [210/250], loss=75.2726
	step [211/250], loss=70.7042
	step [212/250], loss=76.9447
	step [213/250], loss=75.0311
	step [214/250], loss=64.0401
	step [215/250], loss=63.3739
	step [216/250], loss=81.6994
	step [217/250], loss=78.8632
	step [218/250], loss=50.8393
	step [219/250], loss=73.5612
	step [220/250], loss=66.5972
	step [221/250], loss=84.4193
	step [222/250], loss=63.0079
	step [223/250], loss=74.7727
	step [224/250], loss=65.7527
	step [225/250], loss=69.4691
	step [226/250], loss=73.4586
	step [227/250], loss=80.5856
	step [228/250], loss=76.7873
	step [229/250], loss=71.3577
	step [230/250], loss=58.3719
	step [231/250], loss=79.0420
	step [232/250], loss=90.2386
	step [233/250], loss=73.7635
	step [234/250], loss=67.6490
	step [235/250], loss=75.6859
	step [236/250], loss=50.8985
	step [237/250], loss=51.5062
	step [238/250], loss=73.5041
	step [239/250], loss=67.1519
	step [240/250], loss=61.2906
	step [241/250], loss=63.9424
	step [242/250], loss=83.2791
	step [243/250], loss=62.3950
	step [244/250], loss=70.0568
	step [245/250], loss=80.8320
	step [246/250], loss=73.6369
	step [247/250], loss=71.6603
	step [248/250], loss=75.1894
	step [249/250], loss=85.3241
	step [250/250], loss=17.0419
	Evaluating
	loss=0.0074, precision=0.2940, recall=0.8493, f1=0.4368
Training epoch 100
	step [1/250], loss=68.3856
	step [2/250], loss=84.9398
	step [3/250], loss=64.0709
	step [4/250], loss=61.4426
	step [5/250], loss=72.4420
	step [6/250], loss=70.4946
	step [7/250], loss=66.0404
	step [8/250], loss=71.8477
	step [9/250], loss=69.2093
	step [10/250], loss=60.1268
	step [11/250], loss=72.3642
	step [12/250], loss=89.6080
	step [13/250], loss=66.9534
	step [14/250], loss=57.6182
	step [15/250], loss=82.6162
	step [16/250], loss=57.6610
	step [17/250], loss=79.0378
	step [18/250], loss=70.2631
	step [19/250], loss=60.4795
	step [20/250], loss=64.4760
	step [21/250], loss=64.9877
	step [22/250], loss=77.6145
	step [23/250], loss=70.2114
	step [24/250], loss=69.1111
	step [25/250], loss=61.7199
	step [26/250], loss=71.3672
	step [27/250], loss=72.4579
	step [28/250], loss=81.2776
	step [29/250], loss=88.7979
	step [30/250], loss=69.4439
	step [31/250], loss=58.1914
	step [32/250], loss=65.2428
	step [33/250], loss=71.9884
	step [34/250], loss=67.4841
	step [35/250], loss=49.3456
	step [36/250], loss=75.0265
	step [37/250], loss=62.1768
	step [38/250], loss=76.6556
	step [39/250], loss=89.4672
	step [40/250], loss=77.7335
	step [41/250], loss=85.1107
	step [42/250], loss=81.9373
	step [43/250], loss=75.6956
	step [44/250], loss=59.9523
	step [45/250], loss=77.1929
	step [46/250], loss=77.9932
	step [47/250], loss=77.4920
	step [48/250], loss=80.5615
	step [49/250], loss=79.7439
	step [50/250], loss=77.5033
	step [51/250], loss=65.3550
	step [52/250], loss=76.0310
	step [53/250], loss=58.7067
	step [54/250], loss=76.5177
	step [55/250], loss=81.7801
	step [56/250], loss=75.9622
	step [57/250], loss=75.7942
	step [58/250], loss=65.5254
	step [59/250], loss=71.5323
	step [60/250], loss=70.8074
	step [61/250], loss=66.2185
	step [62/250], loss=59.9777
	step [63/250], loss=52.0409
	step [64/250], loss=74.9557
	step [65/250], loss=81.3291
	step [66/250], loss=75.7322
	step [67/250], loss=73.8054
	step [68/250], loss=62.9515
	step [69/250], loss=68.5003
	step [70/250], loss=67.1146
	step [71/250], loss=65.1151
	step [72/250], loss=54.3708
	step [73/250], loss=60.0943
	step [74/250], loss=78.6777
	step [75/250], loss=58.7519
	step [76/250], loss=64.1197
	step [77/250], loss=77.0265
	step [78/250], loss=91.6314
	step [79/250], loss=72.7879
	step [80/250], loss=63.7371
	step [81/250], loss=57.9693
	step [82/250], loss=75.3602
	step [83/250], loss=82.2153
	step [84/250], loss=55.3636
	step [85/250], loss=66.3082
	step [86/250], loss=75.4380
	step [87/250], loss=87.9479
	step [88/250], loss=74.2957
	step [89/250], loss=66.6894
	step [90/250], loss=94.1877
	step [91/250], loss=76.1200
	step [92/250], loss=84.6609
	step [93/250], loss=80.4534
	step [94/250], loss=66.5043
	step [95/250], loss=74.2201
	step [96/250], loss=64.7729
	step [97/250], loss=80.7957
	step [98/250], loss=70.9441
	step [99/250], loss=79.0557
	step [100/250], loss=62.1342
	step [101/250], loss=76.6449
	step [102/250], loss=90.9875
	step [103/250], loss=77.5195
	step [104/250], loss=66.2468
	step [105/250], loss=70.3878
	step [106/250], loss=62.4274
	step [107/250], loss=75.4943
	step [108/250], loss=72.9596
	step [109/250], loss=75.5013
	step [110/250], loss=60.5335
	step [111/250], loss=76.8136
	step [112/250], loss=77.0152
	step [113/250], loss=69.2243
	step [114/250], loss=65.9960
	step [115/250], loss=74.7995
	step [116/250], loss=75.6870
	step [117/250], loss=62.8176
	step [118/250], loss=60.3346
	step [119/250], loss=65.0661
	step [120/250], loss=56.7855
	step [121/250], loss=71.8076
	step [122/250], loss=68.7414
	step [123/250], loss=61.5178
	step [124/250], loss=92.4325
	step [125/250], loss=79.4555
	step [126/250], loss=67.5385
	step [127/250], loss=81.9995
	step [128/250], loss=66.2257
	step [129/250], loss=86.7345
	step [130/250], loss=83.6957
	step [131/250], loss=66.5123
	step [132/250], loss=64.3115
	step [133/250], loss=70.4589
	step [134/250], loss=69.1505
	step [135/250], loss=81.4180
	step [136/250], loss=78.1669
	step [137/250], loss=75.0756
	step [138/250], loss=69.5401
	step [139/250], loss=69.9612
	step [140/250], loss=59.1436
	step [141/250], loss=81.2456
	step [142/250], loss=52.6895
	step [143/250], loss=63.7278
	step [144/250], loss=80.2069
	step [145/250], loss=73.9777
	step [146/250], loss=72.2227
	step [147/250], loss=69.6235
	step [148/250], loss=56.8451
	step [149/250], loss=80.0263
	step [150/250], loss=77.3537
	step [151/250], loss=62.9827
	step [152/250], loss=68.4705
	step [153/250], loss=77.2082
	step [154/250], loss=72.3253
	step [155/250], loss=79.4671
	step [156/250], loss=68.2790
	step [157/250], loss=77.2717
	step [158/250], loss=68.7231
	step [159/250], loss=67.0973
	step [160/250], loss=60.0502
	step [161/250], loss=79.7786
	step [162/250], loss=66.3613
	step [163/250], loss=75.4490
	step [164/250], loss=76.9208
	step [165/250], loss=78.7867
	step [166/250], loss=74.2242
	step [167/250], loss=68.5929
	step [168/250], loss=85.3875
	step [169/250], loss=64.7327
	step [170/250], loss=79.3951
	step [171/250], loss=86.5666
	step [172/250], loss=65.3192
	step [173/250], loss=81.4802
	step [174/250], loss=83.0836
	step [175/250], loss=90.0443
	step [176/250], loss=75.3723
	step [177/250], loss=67.8889
	step [178/250], loss=69.9158
	step [179/250], loss=73.4603
	step [180/250], loss=67.8219
	step [181/250], loss=79.1616
	step [182/250], loss=72.6573
	step [183/250], loss=82.3460
	step [184/250], loss=62.0275
	step [185/250], loss=78.6060
	step [186/250], loss=72.2202
	step [187/250], loss=67.6894
	step [188/250], loss=65.3698
	step [189/250], loss=68.3908
	step [190/250], loss=78.3833
	step [191/250], loss=59.6811
	step [192/250], loss=77.9260
	step [193/250], loss=76.8194
	step [194/250], loss=74.3182
	step [195/250], loss=71.7611
	step [196/250], loss=68.5341
	step [197/250], loss=71.7983
	step [198/250], loss=73.9294
	step [199/250], loss=73.9489
	step [200/250], loss=87.3194
	step [201/250], loss=74.9292
	step [202/250], loss=63.0011
	step [203/250], loss=79.5341
	step [204/250], loss=70.9691
	step [205/250], loss=72.9118
	step [206/250], loss=80.2768
	step [207/250], loss=73.2387
	step [208/250], loss=61.1641
	step [209/250], loss=65.4393
	step [210/250], loss=65.9823
	step [211/250], loss=77.3050
	step [212/250], loss=71.3004
	step [213/250], loss=73.7161
	step [214/250], loss=73.3139
	step [215/250], loss=82.0515
	step [216/250], loss=62.6144
	step [217/250], loss=61.0086
	step [218/250], loss=83.8552
	step [219/250], loss=66.5734
	step [220/250], loss=70.6156
	step [221/250], loss=69.2041
	step [222/250], loss=81.5954
	step [223/250], loss=65.8088
	step [224/250], loss=92.1137
	step [225/250], loss=70.1422
	step [226/250], loss=77.5855
	step [227/250], loss=89.2318
	step [228/250], loss=58.3004
	step [229/250], loss=73.7454
	step [230/250], loss=70.9453
	step [231/250], loss=66.1963
	step [232/250], loss=61.4526
	step [233/250], loss=74.6254
	step [234/250], loss=87.9281
	step [235/250], loss=80.5581
	step [236/250], loss=66.8777
	step [237/250], loss=71.2867
	step [238/250], loss=86.4099
	step [239/250], loss=73.8398
	step [240/250], loss=77.3517
	step [241/250], loss=80.5572
	step [242/250], loss=71.7497
	step [243/250], loss=58.2954
	step [244/250], loss=72.5573
	step [245/250], loss=71.9002
	step [246/250], loss=58.3666
	step [247/250], loss=69.5077
	step [248/250], loss=74.5205
	step [249/250], loss=72.7343
	step [250/250], loss=15.3143
	Evaluating
	loss=0.0067, precision=0.3182, recall=0.8557, f1=0.4639
Training epoch 101
	step [1/250], loss=71.3360
	step [2/250], loss=74.6174
	step [3/250], loss=76.4412
	step [4/250], loss=60.5169
	step [5/250], loss=77.1661
	step [6/250], loss=75.4031
	step [7/250], loss=70.8788
	step [8/250], loss=77.9424
	step [9/250], loss=68.5766
	step [10/250], loss=78.0894
	step [11/250], loss=73.0851
	step [12/250], loss=71.7194
	step [13/250], loss=74.8759
	step [14/250], loss=67.5439
	step [15/250], loss=70.9009
	step [16/250], loss=75.9376
	step [17/250], loss=66.8667
	step [18/250], loss=83.3301
	step [19/250], loss=88.9815
	step [20/250], loss=74.5509
	step [21/250], loss=67.4407
	step [22/250], loss=64.9431
	step [23/250], loss=69.3756
	step [24/250], loss=72.8438
	step [25/250], loss=76.8749
	step [26/250], loss=74.8219
	step [27/250], loss=58.0130
	step [28/250], loss=80.7856
	step [29/250], loss=67.4997
	step [30/250], loss=59.8491
	step [31/250], loss=79.9344
	step [32/250], loss=84.9032
	step [33/250], loss=72.5399
	step [34/250], loss=66.4068
	step [35/250], loss=67.8360
	step [36/250], loss=71.2249
	step [37/250], loss=67.5724
	step [38/250], loss=87.8593
	step [39/250], loss=89.4475
	step [40/250], loss=65.8579
	step [41/250], loss=78.6202
	step [42/250], loss=52.9508
	step [43/250], loss=67.1868
	step [44/250], loss=75.2539
	step [45/250], loss=65.7554
	step [46/250], loss=65.7955
	step [47/250], loss=73.8458
	step [48/250], loss=87.3375
	step [49/250], loss=85.2587
	step [50/250], loss=76.0136
	step [51/250], loss=64.7152
	step [52/250], loss=74.8790
	step [53/250], loss=64.1511
	step [54/250], loss=78.4159
	step [55/250], loss=75.5486
	step [56/250], loss=71.7554
	step [57/250], loss=73.0877
	step [58/250], loss=79.6255
	step [59/250], loss=71.9269
	step [60/250], loss=64.5314
	step [61/250], loss=69.3390
	step [62/250], loss=55.2608
	step [63/250], loss=73.9638
	step [64/250], loss=78.2903
	step [65/250], loss=78.5395
	step [66/250], loss=79.2836
	step [67/250], loss=66.4059
	step [68/250], loss=69.9695
	step [69/250], loss=78.6151
	step [70/250], loss=77.2507
	step [71/250], loss=64.5999
	step [72/250], loss=62.7557
	step [73/250], loss=74.1941
	step [74/250], loss=67.0792
	step [75/250], loss=79.2391
	step [76/250], loss=54.3449
	step [77/250], loss=76.1044
	step [78/250], loss=60.2191
	step [79/250], loss=79.5627
	step [80/250], loss=79.6250
	step [81/250], loss=68.2609
	step [82/250], loss=71.4028
	step [83/250], loss=72.0589
	step [84/250], loss=74.5203
	step [85/250], loss=77.9604
	step [86/250], loss=76.9629
	step [87/250], loss=78.2766
	step [88/250], loss=62.3371
	step [89/250], loss=72.9333
	step [90/250], loss=53.7999
	step [91/250], loss=70.3800
	step [92/250], loss=76.1578
	step [93/250], loss=77.9236
	step [94/250], loss=69.4526
	step [95/250], loss=64.3453
	step [96/250], loss=87.9212
	step [97/250], loss=74.7574
	step [98/250], loss=76.6568
	step [99/250], loss=80.1666
	step [100/250], loss=73.7980
	step [101/250], loss=66.9568
	step [102/250], loss=72.6257
	step [103/250], loss=80.9164
	step [104/250], loss=72.7645
	step [105/250], loss=70.0383
	step [106/250], loss=70.1975
	step [107/250], loss=71.2771
	step [108/250], loss=56.6267
	step [109/250], loss=65.0622
	step [110/250], loss=73.8014
	step [111/250], loss=80.3733
	step [112/250], loss=82.7419
	step [113/250], loss=68.5958
	step [114/250], loss=79.1475
	step [115/250], loss=80.8632
	step [116/250], loss=66.4351
	step [117/250], loss=79.3539
	step [118/250], loss=72.8638
	step [119/250], loss=67.5468
	step [120/250], loss=76.4189
	step [121/250], loss=75.6207
	step [122/250], loss=89.7100
	step [123/250], loss=70.2381
	step [124/250], loss=67.6094
	step [125/250], loss=66.2892
	step [126/250], loss=77.9973
	step [127/250], loss=75.5662
	step [128/250], loss=64.1032
	step [129/250], loss=69.1714
	step [130/250], loss=70.6143
	step [131/250], loss=82.5514
	step [132/250], loss=82.3599
	step [133/250], loss=58.5329
	step [134/250], loss=64.0342
	step [135/250], loss=81.0418
	step [136/250], loss=72.8892
	step [137/250], loss=71.0616
	step [138/250], loss=80.1778
	step [139/250], loss=68.3925
	step [140/250], loss=82.3862
	step [141/250], loss=66.5777
	step [142/250], loss=55.1330
	step [143/250], loss=71.9975
	step [144/250], loss=73.6882
	step [145/250], loss=73.4649
	step [146/250], loss=63.1291
	step [147/250], loss=63.0241
	step [148/250], loss=75.3306
	step [149/250], loss=71.0804
	step [150/250], loss=77.4803
	step [151/250], loss=88.2247
	step [152/250], loss=65.5454
	step [153/250], loss=79.9685
	step [154/250], loss=89.8964
	step [155/250], loss=76.7048
	step [156/250], loss=63.3673
	step [157/250], loss=84.7809
	step [158/250], loss=74.0213
	step [159/250], loss=76.1218
	step [160/250], loss=67.8317
	step [161/250], loss=77.5498
	step [162/250], loss=72.6043
	step [163/250], loss=74.8811
	step [164/250], loss=82.2995
	step [165/250], loss=65.1127
	step [166/250], loss=77.7804
	step [167/250], loss=76.1642
	step [168/250], loss=75.1769
	step [169/250], loss=71.2177
	step [170/250], loss=65.8016
	step [171/250], loss=75.0495
	step [172/250], loss=69.4905
	step [173/250], loss=67.6364
	step [174/250], loss=74.7442
	step [175/250], loss=62.1433
	step [176/250], loss=73.4842
	step [177/250], loss=53.9528
	step [178/250], loss=60.1847
	step [179/250], loss=73.8936
	step [180/250], loss=67.6246
	step [181/250], loss=64.0985
	step [182/250], loss=66.8376
	step [183/250], loss=66.8924
	step [184/250], loss=64.4157
	step [185/250], loss=73.2293
	step [186/250], loss=70.0743
	step [187/250], loss=78.0070
	step [188/250], loss=71.0092
	step [189/250], loss=59.8651
	step [190/250], loss=71.0831
	step [191/250], loss=72.0601
	step [192/250], loss=55.8837
	step [193/250], loss=64.0058
	step [194/250], loss=70.7910
	step [195/250], loss=80.0523
	step [196/250], loss=77.8234
	step [197/250], loss=75.4023
	step [198/250], loss=60.0180
	step [199/250], loss=75.4386
	step [200/250], loss=68.9509
	step [201/250], loss=72.1961
	step [202/250], loss=56.9716
	step [203/250], loss=76.7488
	step [204/250], loss=85.1254
	step [205/250], loss=75.0091
	step [206/250], loss=60.7662
	step [207/250], loss=73.1860
	step [208/250], loss=54.3904
	step [209/250], loss=58.9361
	step [210/250], loss=58.7928
	step [211/250], loss=76.4300
	step [212/250], loss=63.4208
	step [213/250], loss=66.1623
	step [214/250], loss=82.4182
	step [215/250], loss=83.9115
	step [216/250], loss=58.6759
	step [217/250], loss=78.2984
	step [218/250], loss=74.8542
	step [219/250], loss=71.3597
	step [220/250], loss=74.2672
	step [221/250], loss=72.7741
	step [222/250], loss=72.7457
	step [223/250], loss=87.9932
	step [224/250], loss=67.4789
	step [225/250], loss=60.2072
	step [226/250], loss=74.6932
	step [227/250], loss=54.4581
	step [228/250], loss=63.9916
	step [229/250], loss=76.7519
	step [230/250], loss=73.2981
	step [231/250], loss=68.9751
	step [232/250], loss=85.0023
	step [233/250], loss=87.9988
	step [234/250], loss=65.2420
	step [235/250], loss=74.2988
	step [236/250], loss=68.6539
	step [237/250], loss=56.0192
	step [238/250], loss=67.8119
	step [239/250], loss=68.2367
	step [240/250], loss=75.1490
	step [241/250], loss=68.5524
	step [242/250], loss=82.1922
	step [243/250], loss=74.3763
	step [244/250], loss=54.2666
	step [245/250], loss=71.1441
	step [246/250], loss=71.4915
	step [247/250], loss=72.9399
	step [248/250], loss=83.4607
	step [249/250], loss=85.4041
	step [250/250], loss=11.4541
	Evaluating
	loss=0.0080, precision=0.2776, recall=0.8463, f1=0.4180
Training epoch 102
	step [1/250], loss=58.3430
	step [2/250], loss=53.8969
	step [3/250], loss=67.2704
	step [4/250], loss=79.1377
	step [5/250], loss=49.7326
	step [6/250], loss=61.6234
	step [7/250], loss=69.1252
	step [8/250], loss=75.2732
	step [9/250], loss=70.8460
	step [10/250], loss=79.0889
	step [11/250], loss=79.4672
	step [12/250], loss=64.5914
	step [13/250], loss=76.5727
	step [14/250], loss=68.4585
	step [15/250], loss=74.9919
	step [16/250], loss=76.1290
	step [17/250], loss=74.5330
	step [18/250], loss=86.9852
	step [19/250], loss=70.0178
	step [20/250], loss=58.3541
	step [21/250], loss=80.4665
	step [22/250], loss=70.3365
	step [23/250], loss=69.2832
	step [24/250], loss=71.5720
	step [25/250], loss=74.6634
	step [26/250], loss=69.0972
	step [27/250], loss=75.4531
	step [28/250], loss=69.0898
	step [29/250], loss=83.8955
	step [30/250], loss=61.1316
	step [31/250], loss=77.6878
	step [32/250], loss=74.1243
	step [33/250], loss=57.7154
	step [34/250], loss=58.2777
	step [35/250], loss=62.6184
	step [36/250], loss=75.8739
	step [37/250], loss=70.3998
	step [38/250], loss=70.9339
	step [39/250], loss=59.7935
	step [40/250], loss=63.1014
	step [41/250], loss=77.6825
	step [42/250], loss=70.4554
	step [43/250], loss=65.2801
	step [44/250], loss=58.0889
	step [45/250], loss=73.1534
	step [46/250], loss=80.6163
	step [47/250], loss=79.5070
	step [48/250], loss=69.9302
	step [49/250], loss=91.9336
	step [50/250], loss=82.7606
	step [51/250], loss=78.9192
	step [52/250], loss=81.1273
	step [53/250], loss=81.6195
	step [54/250], loss=74.1456
	step [55/250], loss=69.0560
	step [56/250], loss=72.2880
	step [57/250], loss=76.4232
	step [58/250], loss=73.0649
	step [59/250], loss=77.5295
	step [60/250], loss=68.7501
	step [61/250], loss=70.7929
	step [62/250], loss=83.9678
	step [63/250], loss=70.2494
	step [64/250], loss=72.4194
	step [65/250], loss=71.1520
	step [66/250], loss=73.8758
	step [67/250], loss=90.9312
	step [68/250], loss=66.7411
	step [69/250], loss=51.1179
	step [70/250], loss=59.4623
	step [71/250], loss=51.7065
	step [72/250], loss=66.6152
	step [73/250], loss=67.0693
	step [74/250], loss=64.4526
	step [75/250], loss=71.9259
	step [76/250], loss=70.6719
	step [77/250], loss=60.5010
	step [78/250], loss=69.4936
	step [79/250], loss=97.7375
	step [80/250], loss=57.4842
	step [81/250], loss=65.9442
	step [82/250], loss=77.3940
	step [83/250], loss=68.4685
	step [84/250], loss=68.5531
	step [85/250], loss=60.8797
	step [86/250], loss=67.9770
	step [87/250], loss=79.9753
	step [88/250], loss=70.3445
	step [89/250], loss=67.2738
	step [90/250], loss=79.9780
	step [91/250], loss=76.4712
	step [92/250], loss=60.8603
	step [93/250], loss=63.9783
	step [94/250], loss=75.7312
	step [95/250], loss=70.6259
	step [96/250], loss=71.1704
	step [97/250], loss=68.6867
	step [98/250], loss=80.5274
	step [99/250], loss=61.8013
	step [100/250], loss=76.4077
	step [101/250], loss=66.5036
	step [102/250], loss=69.6295
	step [103/250], loss=75.0732
	step [104/250], loss=78.9264
	step [105/250], loss=56.2713
	step [106/250], loss=75.5668
	step [107/250], loss=68.3273
	step [108/250], loss=73.4422
	step [109/250], loss=84.4260
	step [110/250], loss=61.0236
	step [111/250], loss=67.7402
	step [112/250], loss=74.0214
	step [113/250], loss=69.5849
	step [114/250], loss=65.5004
	step [115/250], loss=68.7360
	step [116/250], loss=85.3905
	step [117/250], loss=73.6436
	step [118/250], loss=66.1011
	step [119/250], loss=80.0421
	step [120/250], loss=77.6812
	step [121/250], loss=78.1371
	step [122/250], loss=65.9774
	step [123/250], loss=82.0159
	step [124/250], loss=66.5918
	step [125/250], loss=84.2523
	step [126/250], loss=57.7656
	step [127/250], loss=91.6159
	step [128/250], loss=71.9836
	step [129/250], loss=78.3761
	step [130/250], loss=82.0524
	step [131/250], loss=60.0360
	step [132/250], loss=66.0294
	step [133/250], loss=78.0182
	step [134/250], loss=59.1332
	step [135/250], loss=69.8988
	step [136/250], loss=85.9770
	step [137/250], loss=63.4591
	step [138/250], loss=80.8772
	step [139/250], loss=55.6000
	step [140/250], loss=74.2155
	step [141/250], loss=70.0867
	step [142/250], loss=81.3985
	step [143/250], loss=72.1159
	step [144/250], loss=79.9618
	step [145/250], loss=64.9453
	step [146/250], loss=65.4218
	step [147/250], loss=68.9715
	step [148/250], loss=80.0941
	step [149/250], loss=61.5217
	step [150/250], loss=83.2744
	step [151/250], loss=70.6074
	step [152/250], loss=66.3398
	step [153/250], loss=68.3324
	step [154/250], loss=75.4678
	step [155/250], loss=78.8878
	step [156/250], loss=56.5436
	step [157/250], loss=67.1249
	step [158/250], loss=85.0120
	step [159/250], loss=73.7744
	step [160/250], loss=56.1308
	step [161/250], loss=67.7150
	step [162/250], loss=65.1484
	step [163/250], loss=68.8158
	step [164/250], loss=61.5447
	step [165/250], loss=81.2151
	step [166/250], loss=63.7682
	step [167/250], loss=61.2291
	step [168/250], loss=74.5513
	step [169/250], loss=68.0586
	step [170/250], loss=71.0785
	step [171/250], loss=75.3479
	step [172/250], loss=79.7763
	step [173/250], loss=72.4852
	step [174/250], loss=79.3214
	step [175/250], loss=67.6057
	step [176/250], loss=79.4798
	step [177/250], loss=64.8261
	step [178/250], loss=81.8472
	step [179/250], loss=70.8675
	step [180/250], loss=63.8410
	step [181/250], loss=70.5826
	step [182/250], loss=67.8220
	step [183/250], loss=85.0478
	step [184/250], loss=68.4705
	step [185/250], loss=73.1373
	step [186/250], loss=69.3914
	step [187/250], loss=73.1597
	step [188/250], loss=77.0463
	step [189/250], loss=59.2647
	step [190/250], loss=58.9272
	step [191/250], loss=66.1848
	step [192/250], loss=72.9119
	step [193/250], loss=77.5160
	step [194/250], loss=80.0125
	step [195/250], loss=73.8227
	step [196/250], loss=77.0826
	step [197/250], loss=77.5445
	step [198/250], loss=72.4315
	step [199/250], loss=71.1514
	step [200/250], loss=75.0103
	step [201/250], loss=67.1586
	step [202/250], loss=76.0046
	step [203/250], loss=72.9441
	step [204/250], loss=67.4493
	step [205/250], loss=77.2103
	step [206/250], loss=62.7025
	step [207/250], loss=74.2928
	step [208/250], loss=62.2223
	step [209/250], loss=73.9321
	step [210/250], loss=79.0891
	step [211/250], loss=71.5449
	step [212/250], loss=90.4858
	step [213/250], loss=70.7593
	step [214/250], loss=85.9998
	step [215/250], loss=70.4576
	step [216/250], loss=72.7990
	step [217/250], loss=71.6135
	step [218/250], loss=68.1628
	step [219/250], loss=76.9252
	step [220/250], loss=72.2329
	step [221/250], loss=71.5865
	step [222/250], loss=77.3275
	step [223/250], loss=73.2652
	step [224/250], loss=68.2411
	step [225/250], loss=63.5653
	step [226/250], loss=67.3121
	step [227/250], loss=66.4454
	step [228/250], loss=63.1221
	step [229/250], loss=84.0795
	step [230/250], loss=90.0497
	step [231/250], loss=67.5703
	step [232/250], loss=62.8234
	step [233/250], loss=79.4449
	step [234/250], loss=59.9785
	step [235/250], loss=81.3850
	step [236/250], loss=75.5255
	step [237/250], loss=71.4961
	step [238/250], loss=73.5242
	step [239/250], loss=86.4175
	step [240/250], loss=60.5205
	step [241/250], loss=80.0218
	step [242/250], loss=68.1250
	step [243/250], loss=67.7976
	step [244/250], loss=87.7829
	step [245/250], loss=66.9741
	step [246/250], loss=82.3119
	step [247/250], loss=75.5885
	step [248/250], loss=74.7516
	step [249/250], loss=79.0750
	step [250/250], loss=13.4970
	Evaluating
	loss=0.0070, precision=0.3152, recall=0.8459, f1=0.4593
Training epoch 103
	step [1/250], loss=78.7192
	step [2/250], loss=74.0491
	step [3/250], loss=73.8582
	step [4/250], loss=69.0404
	step [5/250], loss=59.7114
	step [6/250], loss=85.4829
	step [7/250], loss=78.8677
	step [8/250], loss=72.1990
	step [9/250], loss=65.5149
	step [10/250], loss=68.9328
	step [11/250], loss=67.9761
	step [12/250], loss=70.1719
	step [13/250], loss=76.6979
	step [14/250], loss=63.7813
	step [15/250], loss=85.5822
	step [16/250], loss=74.3703
	step [17/250], loss=85.0002
	step [18/250], loss=78.4809
	step [19/250], loss=58.7615
	step [20/250], loss=83.9047
	step [21/250], loss=81.3250
	step [22/250], loss=79.6264
	step [23/250], loss=61.8687
	step [24/250], loss=66.5124
	step [25/250], loss=69.5561
	step [26/250], loss=68.0976
	step [27/250], loss=63.6650
	step [28/250], loss=92.0113
	step [29/250], loss=77.0545
	step [30/250], loss=63.3338
	step [31/250], loss=76.3328
	step [32/250], loss=94.2589
	step [33/250], loss=86.2542
	step [34/250], loss=82.9080
	step [35/250], loss=65.9243
	step [36/250], loss=61.2417
	step [37/250], loss=81.9134
	step [38/250], loss=74.7823
	step [39/250], loss=78.3487
	step [40/250], loss=58.0627
	step [41/250], loss=65.1685
	step [42/250], loss=66.0185
	step [43/250], loss=59.5943
	step [44/250], loss=73.3079
	step [45/250], loss=69.5711
	step [46/250], loss=79.7696
	step [47/250], loss=62.7611
	step [48/250], loss=72.7797
	step [49/250], loss=68.6877
	step [50/250], loss=71.1077
	step [51/250], loss=77.3836
	step [52/250], loss=62.8947
	step [53/250], loss=59.1255
	step [54/250], loss=75.0260
	step [55/250], loss=78.5997
	step [56/250], loss=85.0873
	step [57/250], loss=66.7494
	step [58/250], loss=81.6717
	step [59/250], loss=81.1844
	step [60/250], loss=75.4115
	step [61/250], loss=79.9062
	step [62/250], loss=72.9444
	step [63/250], loss=61.9631
	step [64/250], loss=65.3570
	step [65/250], loss=65.4017
	step [66/250], loss=83.4622
	step [67/250], loss=69.7803
	step [68/250], loss=77.0210
	step [69/250], loss=68.4421
	step [70/250], loss=64.1160
	step [71/250], loss=86.4484
	step [72/250], loss=67.7589
	step [73/250], loss=69.1517
	step [74/250], loss=55.2126
	step [75/250], loss=75.2920
	step [76/250], loss=60.0115
	step [77/250], loss=89.6252
	step [78/250], loss=72.4492
	step [79/250], loss=76.5447
	step [80/250], loss=64.0764
	step [81/250], loss=59.9525
	step [82/250], loss=69.3718
	step [83/250], loss=82.2797
	step [84/250], loss=63.8403
	step [85/250], loss=72.0703
	step [86/250], loss=63.1771
	step [87/250], loss=63.9691
	step [88/250], loss=69.6997
	step [89/250], loss=84.0600
	step [90/250], loss=73.8670
	step [91/250], loss=74.1014
	step [92/250], loss=72.4538
	step [93/250], loss=75.6645
	step [94/250], loss=65.0353
	step [95/250], loss=59.5345
	step [96/250], loss=63.8923
	step [97/250], loss=67.5050
	step [98/250], loss=64.7953
	step [99/250], loss=68.2769
	step [100/250], loss=79.7990
	step [101/250], loss=77.4809
	step [102/250], loss=81.1090
	step [103/250], loss=82.4918
	step [104/250], loss=74.0856
	step [105/250], loss=65.9424
	step [106/250], loss=78.3000
	step [107/250], loss=65.0663
	step [108/250], loss=62.8785
	step [109/250], loss=71.1759
	step [110/250], loss=78.7752
	step [111/250], loss=69.3105
	step [112/250], loss=74.1149
	step [113/250], loss=70.6637
	step [114/250], loss=80.9770
	step [115/250], loss=64.5629
	step [116/250], loss=58.3566
	step [117/250], loss=73.4273
	step [118/250], loss=63.7386
	step [119/250], loss=63.4899
	step [120/250], loss=59.6392
	step [121/250], loss=72.7501
	step [122/250], loss=91.3125
	step [123/250], loss=72.8156
	step [124/250], loss=72.8480
	step [125/250], loss=67.6145
	step [126/250], loss=78.2980
	step [127/250], loss=74.3111
	step [128/250], loss=74.2445
	step [129/250], loss=84.1298
	step [130/250], loss=81.3085
	step [131/250], loss=70.3201
	step [132/250], loss=51.4016
	step [133/250], loss=68.3137
	step [134/250], loss=69.0391
	step [135/250], loss=65.5690
	step [136/250], loss=60.7415
	step [137/250], loss=79.6718
	step [138/250], loss=82.9071
	step [139/250], loss=60.0235
	step [140/250], loss=90.5918
	step [141/250], loss=74.4442
	step [142/250], loss=78.0738
	step [143/250], loss=65.8107
	step [144/250], loss=73.5476
	step [145/250], loss=74.4049
	step [146/250], loss=74.7474
	step [147/250], loss=89.8325
	step [148/250], loss=57.5430
	step [149/250], loss=65.3558
	step [150/250], loss=90.0212
	step [151/250], loss=68.9199
	step [152/250], loss=79.9138
	step [153/250], loss=54.8222
	step [154/250], loss=71.4333
	step [155/250], loss=88.0031
	step [156/250], loss=80.9839
	step [157/250], loss=73.1051
	step [158/250], loss=70.2642
	step [159/250], loss=60.8793
	step [160/250], loss=64.3664
	step [161/250], loss=71.7760
	step [162/250], loss=61.4726
	step [163/250], loss=81.4914
	step [164/250], loss=67.0597
	step [165/250], loss=61.5921
	step [166/250], loss=64.9763
	step [167/250], loss=69.4081
	step [168/250], loss=88.6256
	step [169/250], loss=67.5413
	step [170/250], loss=61.5767
	step [171/250], loss=54.9217
	step [172/250], loss=79.9152
	step [173/250], loss=70.9925
	step [174/250], loss=64.1627
	step [175/250], loss=88.8511
	step [176/250], loss=85.0279
	step [177/250], loss=77.2487
	step [178/250], loss=76.1324
	step [179/250], loss=64.6862
	step [180/250], loss=62.7619
	step [181/250], loss=81.0141
	step [182/250], loss=71.1921
	step [183/250], loss=77.1925
	step [184/250], loss=72.5295
	step [185/250], loss=81.6117
	step [186/250], loss=63.3018
	step [187/250], loss=77.7379
	step [188/250], loss=69.6409
	step [189/250], loss=75.6395
	step [190/250], loss=73.1836
	step [191/250], loss=67.0993
	step [192/250], loss=73.5928
	step [193/250], loss=71.1044
	step [194/250], loss=78.3279
	step [195/250], loss=82.1048
	step [196/250], loss=71.7031
	step [197/250], loss=75.0441
	step [198/250], loss=69.9851
	step [199/250], loss=69.5536
	step [200/250], loss=53.9418
	step [201/250], loss=70.4885
	step [202/250], loss=71.2033
	step [203/250], loss=63.1108
	step [204/250], loss=79.8406
	step [205/250], loss=73.5864
	step [206/250], loss=57.3348
	step [207/250], loss=76.8998
	step [208/250], loss=74.8030
	step [209/250], loss=60.8360
	step [210/250], loss=70.7196
	step [211/250], loss=62.4502
	step [212/250], loss=80.4206
	step [213/250], loss=60.0009
	step [214/250], loss=67.4220
	step [215/250], loss=64.2259
	step [216/250], loss=66.8977
	step [217/250], loss=65.3040
	step [218/250], loss=67.3535
	step [219/250], loss=78.0149
	step [220/250], loss=77.1023
	step [221/250], loss=71.4668
	step [222/250], loss=62.8923
	step [223/250], loss=55.9271
	step [224/250], loss=69.9589
	step [225/250], loss=83.0217
	step [226/250], loss=67.3456
	step [227/250], loss=71.2375
	step [228/250], loss=63.5334
	step [229/250], loss=87.3686
	step [230/250], loss=73.9066
	step [231/250], loss=76.6030
	step [232/250], loss=70.4830
	step [233/250], loss=65.1149
	step [234/250], loss=71.2747
	step [235/250], loss=69.8793
	step [236/250], loss=61.6831
	step [237/250], loss=85.0695
	step [238/250], loss=70.7178
	step [239/250], loss=57.1081
	step [240/250], loss=65.5744
	step [241/250], loss=74.1336
	step [242/250], loss=66.1373
	step [243/250], loss=68.3192
	step [244/250], loss=74.9164
	step [245/250], loss=63.1280
	step [246/250], loss=75.2897
	step [247/250], loss=58.3563
	step [248/250], loss=64.8372
	step [249/250], loss=69.8541
	step [250/250], loss=16.0251
	Evaluating
	loss=0.0075, precision=0.2734, recall=0.8578, f1=0.4147
Training finished
best_f1: 0.4955786687465099
directing: Y rim_enhanced: True test_id 1
removed wrong scan: weights_Y_335_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_383_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_262_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_311_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_260_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_225_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_320_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_192_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_319_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_226_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_346_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_271_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_277_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_290_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_269_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_379_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_375_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_378_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_366_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_309_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_200_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_327_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_193_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_174_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_253_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_220_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_265_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_360_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_303_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_232_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_243_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_240_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_237_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_256_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_254_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_254_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_231_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_359_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_172_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_340_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_263_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_388_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_307_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_278_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_196_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_272_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_314_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_350_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_361_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_194_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_323_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_201_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_299_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_259_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_276_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_167_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_185_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_215_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_268_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_264_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_384_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_261_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_280_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_287_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_385_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_216_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_349_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_348_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_371_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_357_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_234_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_288_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_324_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_229_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_184_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_148_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_168_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_370_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_286_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_302_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_265_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_153_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_274_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_255_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_217_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_355_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_294_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_159_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_338_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_236_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_152_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_292_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_222_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_376_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_183_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_258_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_282_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_278_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_284_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_300_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_163_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_197_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_203_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_243_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_224_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_238_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_374_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_218_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_259_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_282_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_387_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_353_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_386_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_296_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_341_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_206_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_220_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_239_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_231_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_202_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_358_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_246_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_315_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_275_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_169_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_316_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_221_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_188_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_208_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_273_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_251_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_224_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_333_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_263_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_173_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_243_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_241_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_326_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_230_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_236_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_345_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_206_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_258_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_187_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_87_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_154_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_253_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_337_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_232_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_147_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_377_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_362_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_233_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_250_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_297_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_354_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_262_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_225_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_365_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_356_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_242_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_245_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_230_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_318_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_181_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_305_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_255_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_298_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_301_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_293_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_229_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_332_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_257_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_295_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_239_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_260_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_228_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_317_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_364_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_262_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_162_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_310_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_160_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_237_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_254_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_257_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_291_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_279_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_215_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_372_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_218_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_210_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_158_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_156_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_339_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_190_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_373_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_205_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_329_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_380_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_271_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_235_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_244_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_312_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_186_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_308_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_347_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_241_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_270_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_261_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_222_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_189_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_289_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_146_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_252_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_344_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_331_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_313_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_328_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_265_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_283_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_248_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_381_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_211_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_214_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_176_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_221_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_150_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_343_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_249_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_322_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_351_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_234_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_195_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_330_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_334_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_306_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_213_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_325_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_342_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_247_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_352_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_225_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_226_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_256_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_242_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_368_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_382_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_157_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_238_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_267_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_367_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_321_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_233_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_281_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_161_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_336_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_170_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_304_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_44_xwqg-B00034_2020-04-03.npy
# all image files: 21521 # all weight files in weight_dir: 15610 # image files with weight 15610
removed wrong scan: weights_Y_335_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_383_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_262_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_311_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_260_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_225_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_320_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_192_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_319_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_226_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_346_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_271_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_277_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_290_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_269_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_379_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_375_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_378_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_366_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_309_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_200_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_327_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_193_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_174_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_253_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_220_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_265_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_360_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_303_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_232_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_243_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_240_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_237_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_256_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_254_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_254_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_231_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_359_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_172_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_340_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_263_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_388_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_307_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_278_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_196_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_272_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_314_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_350_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_361_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_194_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_323_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_201_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_299_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_259_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_276_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_167_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_185_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_215_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_268_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_264_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_384_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_261_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_280_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_287_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_385_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_216_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_349_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_348_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_371_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_357_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_234_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_288_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_324_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_229_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_184_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_148_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_168_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_370_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_286_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_302_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_265_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_153_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_274_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_255_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_217_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_355_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_294_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_159_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_338_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_236_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_152_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_292_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_222_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_376_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_183_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_258_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_282_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_278_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_284_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_300_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_163_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_197_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_203_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_243_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_224_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_238_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_374_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_218_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_259_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_282_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_387_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_353_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_386_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_296_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_341_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_206_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_220_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_239_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_231_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_202_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_358_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_246_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_315_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_275_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_169_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_316_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_221_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_188_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_208_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_273_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_251_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_224_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_333_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_263_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_173_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_243_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_241_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_326_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_230_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_236_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_345_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_206_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_258_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_187_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_87_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_154_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_253_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_337_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_232_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_147_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_377_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_362_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_233_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_250_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_297_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_354_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_262_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_225_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_365_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_356_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_242_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_245_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_230_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_318_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_181_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_305_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_255_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_298_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_301_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_293_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_229_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_332_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_257_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_295_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_239_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_260_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_228_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_317_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_364_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_262_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_162_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_310_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_160_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_237_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_254_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_257_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_291_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_279_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_215_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_372_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_218_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_210_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_158_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_156_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_339_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_190_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_373_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_205_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_329_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_380_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_271_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_235_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_244_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_312_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_186_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_308_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_347_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_241_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_270_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_261_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_222_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_189_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_289_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_146_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_252_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_344_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_331_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_313_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_328_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_265_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_283_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_248_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_381_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_211_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_214_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_176_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_221_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_150_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_343_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_249_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_322_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_351_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_234_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_195_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_330_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_334_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_306_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_213_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_325_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_342_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_247_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_352_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_225_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_226_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_256_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_242_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_368_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_382_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_157_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_238_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_267_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_367_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_321_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_233_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_281_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_161_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_336_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_170_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_304_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_44_xwqg-B00034_2020-04-03.npy
# all image files: 21521 # all weight files in weight_dir: 4462 # image files with weight 4462
train: /ibex/scratch/projects/c2052/air_tube_seg/training_samples/Y 15610
Using 4 GPUs
Going to train epochs [56-105]
Training epoch 56
	step [1/244], loss=86.5406
	step [2/244], loss=77.9688
	step [3/244], loss=81.1168
	step [4/244], loss=90.7386
	step [5/244], loss=80.6573
	step [6/244], loss=89.7378
	step [7/244], loss=85.4568
	step [8/244], loss=78.9578
	step [9/244], loss=83.0465
	step [10/244], loss=77.4829
	step [11/244], loss=86.9688
	step [12/244], loss=78.1611
	step [13/244], loss=68.0677
	step [14/244], loss=79.9985
	step [15/244], loss=78.2708
	step [16/244], loss=68.7062
	step [17/244], loss=75.6903
	step [18/244], loss=94.0151
	step [19/244], loss=74.2209
	step [20/244], loss=88.4736
	step [21/244], loss=76.3967
	step [22/244], loss=109.3139
	step [23/244], loss=76.1674
	step [24/244], loss=73.3377
	step [25/244], loss=78.8697
	step [26/244], loss=72.3850
	step [27/244], loss=89.7667
	step [28/244], loss=56.5852
	step [29/244], loss=80.8303
	step [30/244], loss=89.9023
	step [31/244], loss=67.4232
	step [32/244], loss=101.9934
	step [33/244], loss=80.8476
	step [34/244], loss=73.1628
	step [35/244], loss=76.5371
	step [36/244], loss=68.6598
	step [37/244], loss=77.7665
	step [38/244], loss=80.6542
	step [39/244], loss=83.1820
	step [40/244], loss=80.7423
	step [41/244], loss=68.5574
	step [42/244], loss=91.5455
	step [43/244], loss=77.8576
	step [44/244], loss=81.0286
	step [45/244], loss=86.6623
	step [46/244], loss=75.9971
	step [47/244], loss=77.1358
	step [48/244], loss=86.3772
	step [49/244], loss=75.4424
	step [50/244], loss=91.3065
	step [51/244], loss=82.2013
	step [52/244], loss=80.9669
	step [53/244], loss=75.1262
	step [54/244], loss=83.6133
	step [55/244], loss=84.3103
	step [56/244], loss=102.2275
	step [57/244], loss=79.2772
	step [58/244], loss=100.1962
	step [59/244], loss=82.0442
	step [60/244], loss=76.1726
	step [61/244], loss=92.6033
	step [62/244], loss=83.8934
	step [63/244], loss=72.6314
	step [64/244], loss=89.5609
	step [65/244], loss=76.1830
	step [66/244], loss=90.1673
	step [67/244], loss=77.9312
	step [68/244], loss=59.2987
	step [69/244], loss=79.2583
	step [70/244], loss=74.9691
	step [71/244], loss=90.5980
	step [72/244], loss=82.0234
	step [73/244], loss=75.2816
	step [74/244], loss=94.3611
	step [75/244], loss=65.0713
	step [76/244], loss=86.6633
	step [77/244], loss=76.2286
	step [78/244], loss=82.8194
	step [79/244], loss=95.7744
	step [80/244], loss=81.2076
	step [81/244], loss=83.8841
	step [82/244], loss=73.6760
	step [83/244], loss=81.2773
	step [84/244], loss=77.3858
	step [85/244], loss=80.1777
	step [86/244], loss=89.2801
	step [87/244], loss=87.3486
	step [88/244], loss=84.9819
	step [89/244], loss=91.8932
	step [90/244], loss=72.5638
	step [91/244], loss=89.9919
	step [92/244], loss=95.0024
	step [93/244], loss=86.4893
	step [94/244], loss=78.7608
	step [95/244], loss=81.5135
	step [96/244], loss=76.5905
	step [97/244], loss=77.7495
	step [98/244], loss=78.1801
	step [99/244], loss=77.8585
	step [100/244], loss=70.6674
	step [101/244], loss=74.0121
	step [102/244], loss=76.3845
	step [103/244], loss=77.1419
	step [104/244], loss=79.2823
	step [105/244], loss=78.0387
	step [106/244], loss=81.3951
	step [107/244], loss=88.1911
	step [108/244], loss=72.4519
	step [109/244], loss=68.8493
	step [110/244], loss=71.5750
	step [111/244], loss=82.3809
	step [112/244], loss=78.6684
	step [113/244], loss=83.8955
	step [114/244], loss=76.8319
	step [115/244], loss=85.2271
	step [116/244], loss=90.2698
	step [117/244], loss=88.5579
	step [118/244], loss=90.5130
	step [119/244], loss=94.8220
	step [120/244], loss=80.7877
	step [121/244], loss=82.2821
	step [122/244], loss=86.0705
	step [123/244], loss=85.3597
	step [124/244], loss=79.5448
	step [125/244], loss=72.7063
	step [126/244], loss=79.0973
	step [127/244], loss=83.8306
	step [128/244], loss=93.1117
	step [129/244], loss=84.9941
	step [130/244], loss=79.8416
	step [131/244], loss=74.6292
	step [132/244], loss=69.4115
	step [133/244], loss=79.6686
	step [134/244], loss=78.0669
	step [135/244], loss=87.8555
	step [136/244], loss=92.8225
	step [137/244], loss=75.6182
	step [138/244], loss=81.8540
	step [139/244], loss=71.8862
	step [140/244], loss=75.9542
	step [141/244], loss=80.7574
	step [142/244], loss=74.9239
	step [143/244], loss=76.9913
	step [144/244], loss=81.0098
	step [145/244], loss=60.7988
	step [146/244], loss=80.4340
	step [147/244], loss=68.5039
	step [148/244], loss=71.5035
	step [149/244], loss=70.9749
	step [150/244], loss=88.6864
	step [151/244], loss=70.6396
	step [152/244], loss=92.1838
	step [153/244], loss=82.4754
	step [154/244], loss=76.5203
	step [155/244], loss=77.1240
	step [156/244], loss=76.1017
	step [157/244], loss=75.2640
	step [158/244], loss=79.0864
	step [159/244], loss=79.5733
	step [160/244], loss=86.9858
	step [161/244], loss=88.5596
	step [162/244], loss=83.5962
	step [163/244], loss=81.4924
	step [164/244], loss=81.0848
	step [165/244], loss=89.8821
	step [166/244], loss=110.4363
	step [167/244], loss=80.6406
	step [168/244], loss=58.5237
	step [169/244], loss=90.1964
	step [170/244], loss=60.2659
	step [171/244], loss=93.3344
	step [172/244], loss=86.9320
	step [173/244], loss=79.7311
	step [174/244], loss=92.7807
	step [175/244], loss=88.8103
	step [176/244], loss=91.4605
	step [177/244], loss=64.1705
	step [178/244], loss=94.4118
	step [179/244], loss=79.1627
	step [180/244], loss=96.8181
	step [181/244], loss=89.3168
	step [182/244], loss=73.0413
	step [183/244], loss=73.1924
	step [184/244], loss=101.7066
	step [185/244], loss=80.9032
	step [186/244], loss=84.6513
	step [187/244], loss=70.0797
	step [188/244], loss=75.0268
	step [189/244], loss=62.5587
	step [190/244], loss=97.1349
	step [191/244], loss=88.9002
	step [192/244], loss=91.0646
	step [193/244], loss=71.1182
	step [194/244], loss=73.8295
	step [195/244], loss=72.3714
	step [196/244], loss=81.0288
	step [197/244], loss=65.7698
	step [198/244], loss=64.7383
	step [199/244], loss=71.7283
	step [200/244], loss=79.4068
	step [201/244], loss=84.0095
	step [202/244], loss=96.8457
	step [203/244], loss=75.9414
	step [204/244], loss=77.7448
	step [205/244], loss=72.8121
	step [206/244], loss=87.6511
	step [207/244], loss=96.1914
	step [208/244], loss=66.4080
	step [209/244], loss=86.3727
	step [210/244], loss=83.0217
	step [211/244], loss=83.7947
	step [212/244], loss=74.6795
	step [213/244], loss=88.5497
	step [214/244], loss=73.8913
	step [215/244], loss=67.7230
	step [216/244], loss=85.9651
	step [217/244], loss=79.9392
	step [218/244], loss=77.3806
	step [219/244], loss=70.6336
	step [220/244], loss=94.3993
	step [221/244], loss=71.8356
	step [222/244], loss=94.8042
	step [223/244], loss=65.5553
	step [224/244], loss=69.7886
	step [225/244], loss=69.4723
	step [226/244], loss=78.8217
	step [227/244], loss=93.4298
	step [228/244], loss=81.8210
	step [229/244], loss=63.3072
	step [230/244], loss=71.3163
	step [231/244], loss=65.7317
	step [232/244], loss=77.8142
	step [233/244], loss=71.8175
	step [234/244], loss=85.4231
	step [235/244], loss=73.1130
	step [236/244], loss=81.0529
	step [237/244], loss=63.5035
	step [238/244], loss=79.3794
	step [239/244], loss=83.8276
	step [240/244], loss=75.4807
	step [241/244], loss=64.9213
	step [242/244], loss=99.6283
	step [243/244], loss=80.5605
	step [244/244], loss=76.5408
	Evaluating
	loss=0.0102, precision=0.2980, recall=0.8503, f1=0.4413
saving model as: 1_saved_model.pth
Training epoch 57
	step [1/244], loss=59.1579
	step [2/244], loss=85.1004
	step [3/244], loss=79.8658
	step [4/244], loss=97.0839
	step [5/244], loss=85.1352
	step [6/244], loss=75.0938
	step [7/244], loss=79.9714
	step [8/244], loss=62.6527
	step [9/244], loss=80.2764
	step [10/244], loss=77.8615
	step [11/244], loss=64.2177
	step [12/244], loss=76.9027
	step [13/244], loss=82.2365
	step [14/244], loss=84.1233
	step [15/244], loss=69.8846
	step [16/244], loss=61.4523
	step [17/244], loss=77.7001
	step [18/244], loss=58.0014
	step [19/244], loss=83.6793
	step [20/244], loss=67.8585
	step [21/244], loss=69.5453
	step [22/244], loss=79.9827
	step [23/244], loss=71.9799
	step [24/244], loss=81.0472
	step [25/244], loss=67.3069
	step [26/244], loss=71.5867
	step [27/244], loss=80.5933
	step [28/244], loss=72.7976
	step [29/244], loss=86.7061
	step [30/244], loss=81.9628
	step [31/244], loss=76.5609
	step [32/244], loss=69.7965
	step [33/244], loss=79.7093
	step [34/244], loss=79.8508
	step [35/244], loss=64.1893
	step [36/244], loss=70.0112
	step [37/244], loss=77.3373
	step [38/244], loss=89.0578
	step [39/244], loss=104.8024
	step [40/244], loss=74.5041
	step [41/244], loss=90.0281
	step [42/244], loss=86.5351
	step [43/244], loss=93.1536
	step [44/244], loss=84.0697
	step [45/244], loss=70.8717
	step [46/244], loss=78.6907
	step [47/244], loss=63.8238
	step [48/244], loss=81.6044
	step [49/244], loss=76.4495
	step [50/244], loss=72.2811
	step [51/244], loss=85.5300
	step [52/244], loss=84.9936
	step [53/244], loss=88.3081
	step [54/244], loss=84.0602
	step [55/244], loss=78.5057
	step [56/244], loss=74.3122
	step [57/244], loss=85.8232
	step [58/244], loss=73.9992
	step [59/244], loss=79.4149
	step [60/244], loss=85.5639
	step [61/244], loss=70.5309
	step [62/244], loss=74.4937
	step [63/244], loss=80.2940
	step [64/244], loss=85.5485
	step [65/244], loss=88.5277
	step [66/244], loss=84.9023
	step [67/244], loss=69.3328
	step [68/244], loss=87.6829
	step [69/244], loss=72.4793
	step [70/244], loss=76.6416
	step [71/244], loss=67.0384
	step [72/244], loss=77.2412
	step [73/244], loss=92.6813
	step [74/244], loss=84.6032
	step [75/244], loss=99.0335
	step [76/244], loss=74.9141
	step [77/244], loss=82.6883
	step [78/244], loss=87.1234
	step [79/244], loss=91.2415
	step [80/244], loss=74.3133
	step [81/244], loss=74.4993
	step [82/244], loss=97.7577
	step [83/244], loss=95.5013
	step [84/244], loss=86.1030
	step [85/244], loss=78.1562
	step [86/244], loss=91.1365
	step [87/244], loss=77.5974
	step [88/244], loss=86.4620
	step [89/244], loss=83.3238
	step [90/244], loss=74.7979
	step [91/244], loss=89.8909
	step [92/244], loss=72.3318
	step [93/244], loss=92.4225
	step [94/244], loss=75.6379
	step [95/244], loss=67.6287
	step [96/244], loss=95.1674
	step [97/244], loss=100.8337
	step [98/244], loss=88.8792
	step [99/244], loss=87.4216
	step [100/244], loss=98.3034
	step [101/244], loss=59.5213
	step [102/244], loss=77.8237
	step [103/244], loss=67.6469
	step [104/244], loss=68.5441
	step [105/244], loss=84.3696
	step [106/244], loss=82.1075
	step [107/244], loss=105.5060
	step [108/244], loss=61.2681
	step [109/244], loss=82.6682
	step [110/244], loss=88.0359
	step [111/244], loss=92.0232
	step [112/244], loss=76.6310
	step [113/244], loss=76.5047
	step [114/244], loss=96.8227
	step [115/244], loss=67.9460
	step [116/244], loss=88.0843
	step [117/244], loss=89.6816
	step [118/244], loss=80.8567
	step [119/244], loss=67.2535
	step [120/244], loss=82.6698
	step [121/244], loss=77.3979
	step [122/244], loss=93.7176
	step [123/244], loss=83.8905
	step [124/244], loss=80.7878
	step [125/244], loss=81.2151
	step [126/244], loss=69.8085
	step [127/244], loss=80.8093
	step [128/244], loss=88.9791
	step [129/244], loss=80.4931
	step [130/244], loss=76.4924
	step [131/244], loss=91.5821
	step [132/244], loss=88.4377
	step [133/244], loss=79.2289
	step [134/244], loss=78.9389
	step [135/244], loss=64.7495
	step [136/244], loss=74.3200
	step [137/244], loss=86.6704
	step [138/244], loss=94.5869
	step [139/244], loss=79.8000
	step [140/244], loss=90.0092
	step [141/244], loss=105.0275
	step [142/244], loss=82.7552
	step [143/244], loss=98.0237
	step [144/244], loss=71.8553
	step [145/244], loss=83.8550
	step [146/244], loss=92.6295
	step [147/244], loss=74.7982
	step [148/244], loss=75.3794
	step [149/244], loss=74.9011
	step [150/244], loss=69.9620
	step [151/244], loss=86.3607
	step [152/244], loss=74.1656
	step [153/244], loss=81.1020
	step [154/244], loss=90.6847
	step [155/244], loss=84.1794
	step [156/244], loss=82.2382
	step [157/244], loss=79.8654
	step [158/244], loss=81.5233
	step [159/244], loss=72.4339
	step [160/244], loss=66.9350
	step [161/244], loss=72.5208
	step [162/244], loss=99.2814
	step [163/244], loss=79.3082
	step [164/244], loss=87.1671
	step [165/244], loss=93.6777
	step [166/244], loss=79.4280
	step [167/244], loss=83.1418
	step [168/244], loss=77.4058
	step [169/244], loss=76.7695
	step [170/244], loss=75.7174
	step [171/244], loss=75.0261
	step [172/244], loss=84.6747
	step [173/244], loss=90.4844
	step [174/244], loss=79.4241
	step [175/244], loss=98.6957
	step [176/244], loss=81.7862
	step [177/244], loss=73.8011
	step [178/244], loss=65.4296
	step [179/244], loss=71.8134
	step [180/244], loss=91.1930
	step [181/244], loss=90.6093
	step [182/244], loss=77.8644
	step [183/244], loss=69.9754
	step [184/244], loss=95.0338
	step [185/244], loss=71.8210
	step [186/244], loss=60.7167
	step [187/244], loss=76.6218
	step [188/244], loss=74.4256
	step [189/244], loss=73.9469
	step [190/244], loss=63.6655
	step [191/244], loss=77.4613
	step [192/244], loss=66.6353
	step [193/244], loss=97.2569
	step [194/244], loss=76.9220
	step [195/244], loss=75.0488
	step [196/244], loss=78.1129
	step [197/244], loss=96.3917
	step [198/244], loss=78.4390
	step [199/244], loss=71.9598
	step [200/244], loss=85.1389
	step [201/244], loss=89.4863
	step [202/244], loss=93.0506
	step [203/244], loss=89.5316
	step [204/244], loss=83.8609
	step [205/244], loss=86.5230
	step [206/244], loss=72.6895
	step [207/244], loss=74.6927
	step [208/244], loss=80.4183
	step [209/244], loss=75.4268
	step [210/244], loss=63.8431
	step [211/244], loss=100.8628
	step [212/244], loss=92.9049
	step [213/244], loss=80.2093
	step [214/244], loss=82.3810
	step [215/244], loss=74.8368
	step [216/244], loss=68.4777
	step [217/244], loss=78.6475
	step [218/244], loss=73.8060
	step [219/244], loss=71.8736
	step [220/244], loss=74.0817
	step [221/244], loss=90.9974
	step [222/244], loss=67.3196
	step [223/244], loss=82.3161
	step [224/244], loss=82.0163
	step [225/244], loss=72.1137
	step [226/244], loss=89.7304
	step [227/244], loss=95.7758
	step [228/244], loss=55.7521
	step [229/244], loss=75.4773
	step [230/244], loss=72.0138
	step [231/244], loss=77.7018
	step [232/244], loss=79.4658
	step [233/244], loss=70.2743
	step [234/244], loss=72.2746
	step [235/244], loss=79.9052
	step [236/244], loss=81.3055
	step [237/244], loss=55.8807
	step [238/244], loss=99.2767
	step [239/244], loss=80.2981
	step [240/244], loss=68.3351
	step [241/244], loss=75.3909
	step [242/244], loss=73.2186
	step [243/244], loss=77.0452
	step [244/244], loss=89.6694
	Evaluating
	loss=0.0110, precision=0.2746, recall=0.8641, f1=0.4167
Training epoch 58
	step [1/244], loss=92.2167
	step [2/244], loss=78.0154
	step [3/244], loss=76.8681
	step [4/244], loss=108.0913
	step [5/244], loss=81.4436
	step [6/244], loss=75.8775
	step [7/244], loss=79.6611
	step [8/244], loss=66.0369
	step [9/244], loss=87.2096
	step [10/244], loss=91.5376
	step [11/244], loss=96.0531
	step [12/244], loss=89.2925
	step [13/244], loss=84.9024
	step [14/244], loss=77.8667
	step [15/244], loss=107.8713
	step [16/244], loss=78.2514
	step [17/244], loss=96.0614
	step [18/244], loss=64.7010
	step [19/244], loss=66.2108
	step [20/244], loss=71.4965
	step [21/244], loss=70.1573
	step [22/244], loss=81.9151
	step [23/244], loss=100.8352
	step [24/244], loss=71.5613
	step [25/244], loss=79.1207
	step [26/244], loss=99.1622
	step [27/244], loss=76.7016
	step [28/244], loss=94.8882
	step [29/244], loss=75.9241
	step [30/244], loss=89.9325
	step [31/244], loss=65.7535
	step [32/244], loss=84.7157
	step [33/244], loss=77.8959
	step [34/244], loss=79.6526
	step [35/244], loss=76.6162
	step [36/244], loss=81.3759
	step [37/244], loss=89.5588
	step [38/244], loss=89.2023
	step [39/244], loss=92.5301
	step [40/244], loss=74.9490
	step [41/244], loss=82.2750
	step [42/244], loss=81.1115
	step [43/244], loss=74.7785
	step [44/244], loss=66.0349
	step [45/244], loss=91.5899
	step [46/244], loss=71.4355
	step [47/244], loss=68.0313
	step [48/244], loss=65.7402
	step [49/244], loss=70.0769
	step [50/244], loss=70.8042
	step [51/244], loss=63.7297
	step [52/244], loss=87.5114
	step [53/244], loss=87.6575
	step [54/244], loss=78.0022
	step [55/244], loss=78.2448
	step [56/244], loss=95.9358
	step [57/244], loss=86.4668
	step [58/244], loss=79.5625
	step [59/244], loss=85.1018
	step [60/244], loss=79.6752
	step [61/244], loss=77.8579
	step [62/244], loss=90.4077
	step [63/244], loss=87.3144
	step [64/244], loss=69.6142
	step [65/244], loss=76.2092
	step [66/244], loss=73.8881
	step [67/244], loss=67.7510
	step [68/244], loss=86.4417
	step [69/244], loss=71.9349
	step [70/244], loss=72.8793
	step [71/244], loss=86.2926
	step [72/244], loss=74.6862
	step [73/244], loss=75.9451
	step [74/244], loss=77.8075
	step [75/244], loss=76.5725
	step [76/244], loss=80.0525
	step [77/244], loss=84.5116
	step [78/244], loss=80.0313
	step [79/244], loss=74.0378
	step [80/244], loss=87.4075
	step [81/244], loss=73.1291
	step [82/244], loss=73.6565
	step [83/244], loss=71.5378
	step [84/244], loss=94.8784
	step [85/244], loss=91.3040
	step [86/244], loss=67.8494
	step [87/244], loss=98.2955
	step [88/244], loss=76.3149
	step [89/244], loss=69.6254
	step [90/244], loss=69.0310
	step [91/244], loss=86.0442
	step [92/244], loss=88.3326
	step [93/244], loss=78.6286
	step [94/244], loss=74.1962
	step [95/244], loss=87.5358
	step [96/244], loss=77.5605
	step [97/244], loss=78.0079
	step [98/244], loss=85.4766
	step [99/244], loss=74.6938
	step [100/244], loss=93.1424
	step [101/244], loss=72.4635
	step [102/244], loss=85.1415
	step [103/244], loss=66.5512
	step [104/244], loss=72.1389
	step [105/244], loss=73.0370
	step [106/244], loss=65.3115
	step [107/244], loss=73.8472
	step [108/244], loss=83.5660
	step [109/244], loss=70.6914
	step [110/244], loss=74.2423
	step [111/244], loss=74.5356
	step [112/244], loss=65.6138
	step [113/244], loss=98.6352
	step [114/244], loss=88.0206
	step [115/244], loss=86.0802
	step [116/244], loss=97.2508
	step [117/244], loss=76.8736
	step [118/244], loss=74.2883
	step [119/244], loss=72.9094
	step [120/244], loss=77.5929
	step [121/244], loss=81.7098
	step [122/244], loss=65.7631
	step [123/244], loss=76.4977
	step [124/244], loss=58.1725
	step [125/244], loss=72.9601
	step [126/244], loss=68.6731
	step [127/244], loss=71.6176
	step [128/244], loss=71.3898
	step [129/244], loss=100.3025
	step [130/244], loss=91.2872
	step [131/244], loss=78.6072
	step [132/244], loss=73.8803
	step [133/244], loss=83.8516
	step [134/244], loss=62.4074
	step [135/244], loss=81.0471
	step [136/244], loss=88.7646
	step [137/244], loss=94.5807
	step [138/244], loss=69.6036
	step [139/244], loss=65.2827
	step [140/244], loss=62.3544
	step [141/244], loss=78.2124
	step [142/244], loss=77.9641
	step [143/244], loss=70.5169
	step [144/244], loss=72.7705
	step [145/244], loss=66.6471
	step [146/244], loss=98.9387
	step [147/244], loss=91.0530
	step [148/244], loss=79.4815
	step [149/244], loss=88.5141
	step [150/244], loss=80.5035
	step [151/244], loss=86.3427
	step [152/244], loss=84.6282
	step [153/244], loss=90.7803
	step [154/244], loss=93.9145
	step [155/244], loss=95.3123
	step [156/244], loss=87.6971
	step [157/244], loss=91.8112
	step [158/244], loss=78.7201
	step [159/244], loss=77.4459
	step [160/244], loss=87.3198
	step [161/244], loss=72.8488
	step [162/244], loss=72.4043
	step [163/244], loss=98.2918
	step [164/244], loss=85.3015
	step [165/244], loss=68.7135
	step [166/244], loss=64.5625
	step [167/244], loss=66.6026
	step [168/244], loss=63.1945
	step [169/244], loss=104.5661
	step [170/244], loss=67.8436
	step [171/244], loss=83.7294
	step [172/244], loss=69.7616
	step [173/244], loss=73.9739
	step [174/244], loss=69.5339
	step [175/244], loss=69.8437
	step [176/244], loss=82.5632
	step [177/244], loss=64.1314
	step [178/244], loss=80.3981
	step [179/244], loss=87.6068
	step [180/244], loss=78.1334
	step [181/244], loss=83.6380
	step [182/244], loss=62.3003
	step [183/244], loss=82.2404
	step [184/244], loss=67.6871
	step [185/244], loss=96.8992
	step [186/244], loss=87.4875
	step [187/244], loss=82.6452
	step [188/244], loss=84.9649
	step [189/244], loss=83.2154
	step [190/244], loss=64.0935
	step [191/244], loss=85.3475
	step [192/244], loss=87.5323
	step [193/244], loss=76.3495
	step [194/244], loss=74.1804
	step [195/244], loss=81.2467
	step [196/244], loss=88.9458
	step [197/244], loss=66.4762
	step [198/244], loss=66.8495
	step [199/244], loss=81.5317
	step [200/244], loss=72.7023
	step [201/244], loss=80.9687
	step [202/244], loss=66.6098
	step [203/244], loss=73.8603
	step [204/244], loss=83.3489
	step [205/244], loss=77.9829
	step [206/244], loss=68.3199
	step [207/244], loss=72.7511
	step [208/244], loss=87.8195
	step [209/244], loss=65.9958
	step [210/244], loss=72.0453
	step [211/244], loss=78.8946
	step [212/244], loss=67.8929
	step [213/244], loss=84.7005
	step [214/244], loss=76.7184
	step [215/244], loss=94.1333
	step [216/244], loss=81.6813
	step [217/244], loss=78.8794
	step [218/244], loss=73.4378
	step [219/244], loss=95.0596
	step [220/244], loss=91.2761
	step [221/244], loss=94.4093
	step [222/244], loss=72.4055
	step [223/244], loss=79.5504
	step [224/244], loss=74.1404
	step [225/244], loss=76.2274
	step [226/244], loss=65.2946
	step [227/244], loss=82.5928
	step [228/244], loss=77.2203
	step [229/244], loss=74.5131
	step [230/244], loss=92.6136
	step [231/244], loss=86.1659
	step [232/244], loss=75.9554
	step [233/244], loss=64.0653
	step [234/244], loss=76.5523
	step [235/244], loss=81.1290
	step [236/244], loss=79.1974
	step [237/244], loss=88.6307
	step [238/244], loss=97.2202
	step [239/244], loss=81.4086
	step [240/244], loss=68.8920
	step [241/244], loss=87.0430
	step [242/244], loss=84.5185
	step [243/244], loss=65.4528
	step [244/244], loss=85.6167
	Evaluating
	loss=0.0092, precision=0.3245, recall=0.8607, f1=0.4714
saving model as: 1_saved_model.pth
Training epoch 59
	step [1/244], loss=82.1118
	step [2/244], loss=81.7377
	step [3/244], loss=91.9973
	step [4/244], loss=80.5677
	step [5/244], loss=78.7181
	step [6/244], loss=73.3978
	step [7/244], loss=69.2364
	step [8/244], loss=82.9028
	step [9/244], loss=79.0884
	step [10/244], loss=85.3710
	step [11/244], loss=66.7358
	step [12/244], loss=77.8607
	step [13/244], loss=87.8102
	step [14/244], loss=71.9979
	step [15/244], loss=73.6608
	step [16/244], loss=77.4146
	step [17/244], loss=84.1722
	step [18/244], loss=73.6652
	step [19/244], loss=85.7370
	step [20/244], loss=81.9630
	step [21/244], loss=83.9353
	step [22/244], loss=90.2517
	step [23/244], loss=79.6509
	step [24/244], loss=86.7769
	step [25/244], loss=72.3274
	step [26/244], loss=85.3447
	step [27/244], loss=74.8238
	step [28/244], loss=74.4634
	step [29/244], loss=73.8954
	step [30/244], loss=72.2224
	step [31/244], loss=87.4762
	step [32/244], loss=76.7654
	step [33/244], loss=75.0758
	step [34/244], loss=68.0457
	step [35/244], loss=91.3765
	step [36/244], loss=82.7921
	step [37/244], loss=72.8293
	step [38/244], loss=73.2112
	step [39/244], loss=71.2459
	step [40/244], loss=83.6417
	step [41/244], loss=73.1512
	step [42/244], loss=101.9286
	step [43/244], loss=93.2134
	step [44/244], loss=82.8602
	step [45/244], loss=66.7324
	step [46/244], loss=72.1967
	step [47/244], loss=70.4081
	step [48/244], loss=62.9601
	step [49/244], loss=69.9071
	step [50/244], loss=79.3537
	step [51/244], loss=75.0472
	step [52/244], loss=90.9918
	step [53/244], loss=54.9635
	step [54/244], loss=75.4368
	step [55/244], loss=92.9180
	step [56/244], loss=62.9282
	step [57/244], loss=82.0304
	step [58/244], loss=81.3442
	step [59/244], loss=92.1163
	step [60/244], loss=80.9752
	step [61/244], loss=81.8995
	step [62/244], loss=79.0030
	step [63/244], loss=74.9849
	step [64/244], loss=76.2817
	step [65/244], loss=75.6621
	step [66/244], loss=70.9322
	step [67/244], loss=102.2421
	step [68/244], loss=80.9095
	step [69/244], loss=93.2805
	step [70/244], loss=78.9774
	step [71/244], loss=81.0269
	step [72/244], loss=80.2280
	step [73/244], loss=77.7232
	step [74/244], loss=75.4772
	step [75/244], loss=82.7236
	step [76/244], loss=89.6902
	step [77/244], loss=75.1793
	step [78/244], loss=91.3323
	step [79/244], loss=76.8682
	step [80/244], loss=87.0863
	step [81/244], loss=73.6352
	step [82/244], loss=68.9156
	step [83/244], loss=74.1611
	step [84/244], loss=90.9130
	step [85/244], loss=74.0070
	step [86/244], loss=64.3459
	step [87/244], loss=83.4351
	step [88/244], loss=72.1028
	step [89/244], loss=78.4212
	step [90/244], loss=78.3602
	step [91/244], loss=77.0784
	step [92/244], loss=76.3978
	step [93/244], loss=89.8839
	step [94/244], loss=82.1201
	step [95/244], loss=78.9353
	step [96/244], loss=93.8840
	step [97/244], loss=72.8234
	step [98/244], loss=82.9105
	step [99/244], loss=79.8159
	step [100/244], loss=72.5345
	step [101/244], loss=85.5809
	step [102/244], loss=85.2580
	step [103/244], loss=79.2609
	step [104/244], loss=72.6820
	step [105/244], loss=75.5109
	step [106/244], loss=77.2567
	step [107/244], loss=81.6136
	step [108/244], loss=79.9400
	step [109/244], loss=75.4433
	step [110/244], loss=73.9774
	step [111/244], loss=95.1346
	step [112/244], loss=74.5463
	step [113/244], loss=83.4991
	step [114/244], loss=85.0714
	step [115/244], loss=76.5977
	step [116/244], loss=89.0237
	step [117/244], loss=66.2014
	step [118/244], loss=94.6713
	step [119/244], loss=74.4864
	step [120/244], loss=83.8904
	step [121/244], loss=78.5904
	step [122/244], loss=73.9323
	step [123/244], loss=67.1378
	step [124/244], loss=76.9053
	step [125/244], loss=79.7769
	step [126/244], loss=90.3731
	step [127/244], loss=72.0641
	step [128/244], loss=78.2428
	step [129/244], loss=79.9061
	step [130/244], loss=81.9757
	step [131/244], loss=95.9522
	step [132/244], loss=57.2095
	step [133/244], loss=66.2978
	step [134/244], loss=67.9611
	step [135/244], loss=82.3217
	step [136/244], loss=75.9133
	step [137/244], loss=76.6055
	step [138/244], loss=78.9576
	step [139/244], loss=82.3902
	step [140/244], loss=75.4530
	step [141/244], loss=93.2291
	step [142/244], loss=82.7661
	step [143/244], loss=87.9553
	step [144/244], loss=78.0844
	step [145/244], loss=79.9580
	step [146/244], loss=77.0444
	step [147/244], loss=74.6173
	step [148/244], loss=77.3717
	step [149/244], loss=99.1492
	step [150/244], loss=84.0590
	step [151/244], loss=97.4278
	step [152/244], loss=73.1515
	step [153/244], loss=81.2469
	step [154/244], loss=81.7881
	step [155/244], loss=78.0773
	step [156/244], loss=67.3043
	step [157/244], loss=76.2616
	step [158/244], loss=65.9952
	step [159/244], loss=73.4465
	step [160/244], loss=83.2744
	step [161/244], loss=76.0006
	step [162/244], loss=65.4095
	step [163/244], loss=95.8815
	step [164/244], loss=88.2066
	step [165/244], loss=73.8523
	step [166/244], loss=71.0361
	step [167/244], loss=75.1148
	step [168/244], loss=79.4067
	step [169/244], loss=69.9957
	step [170/244], loss=78.6211
	step [171/244], loss=68.8773
	step [172/244], loss=81.2216
	step [173/244], loss=80.1560
	step [174/244], loss=84.4969
	step [175/244], loss=77.7221
	step [176/244], loss=72.8307
	step [177/244], loss=79.3892
	step [178/244], loss=76.6587
	step [179/244], loss=89.1286
	step [180/244], loss=71.3125
	step [181/244], loss=85.6234
	step [182/244], loss=65.1019
	step [183/244], loss=88.0170
	step [184/244], loss=78.7445
	step [185/244], loss=91.0453
	step [186/244], loss=90.0576
	step [187/244], loss=74.5863
	step [188/244], loss=82.0005
	step [189/244], loss=76.4582
	step [190/244], loss=86.2807
	step [191/244], loss=78.0666
	step [192/244], loss=66.9945
	step [193/244], loss=75.5309
	step [194/244], loss=78.3816
	step [195/244], loss=72.3585
	step [196/244], loss=81.0449
	step [197/244], loss=79.2636
	step [198/244], loss=84.6007
	step [199/244], loss=73.1640
	step [200/244], loss=76.8686
	step [201/244], loss=73.5961
	step [202/244], loss=69.2145
	step [203/244], loss=71.8691
	step [204/244], loss=79.0878
	step [205/244], loss=89.4865
	step [206/244], loss=85.5203
	step [207/244], loss=72.7946
	step [208/244], loss=70.3617
	step [209/244], loss=76.2742
	step [210/244], loss=78.3540
	step [211/244], loss=81.9490
	step [212/244], loss=89.3868
	step [213/244], loss=86.1423
	step [214/244], loss=81.1590
	step [215/244], loss=76.7999
	step [216/244], loss=81.6824
	step [217/244], loss=89.8305
	step [218/244], loss=90.7549
	step [219/244], loss=75.6314
	step [220/244], loss=77.8748
	step [221/244], loss=76.8882
	step [222/244], loss=80.5195
	step [223/244], loss=92.4647
	step [224/244], loss=87.1998
	step [225/244], loss=81.4804
	step [226/244], loss=90.9088
	step [227/244], loss=75.2031
	step [228/244], loss=70.2638
	step [229/244], loss=81.3802
	step [230/244], loss=75.3567
	step [231/244], loss=82.6605
	step [232/244], loss=81.7126
	step [233/244], loss=65.7818
	step [234/244], loss=75.4884
	step [235/244], loss=94.5576
	step [236/244], loss=68.7750
	step [237/244], loss=74.1036
	step [238/244], loss=98.5422
	step [239/244], loss=88.8530
	step [240/244], loss=87.7641
	step [241/244], loss=82.6320
	step [242/244], loss=74.2464
	step [243/244], loss=65.0411
	step [244/244], loss=79.3303
	Evaluating
	loss=0.0096, precision=0.3085, recall=0.8591, f1=0.4539
Training epoch 60
	step [1/244], loss=77.1451
	step [2/244], loss=77.1365
	step [3/244], loss=66.2876
	step [4/244], loss=75.7039
	step [5/244], loss=73.2975
	step [6/244], loss=67.2074
	step [7/244], loss=81.9816
	step [8/244], loss=79.5514
	step [9/244], loss=69.1976
	step [10/244], loss=75.0583
	step [11/244], loss=88.2777
	step [12/244], loss=79.1333
	step [13/244], loss=72.1385
	step [14/244], loss=80.1662
	step [15/244], loss=82.5709
	step [16/244], loss=61.7061
	step [17/244], loss=78.6959
	step [18/244], loss=86.4594
	step [19/244], loss=77.1083
	step [20/244], loss=84.5718
	step [21/244], loss=85.8290
	step [22/244], loss=69.9221
	step [23/244], loss=76.8508
	step [24/244], loss=85.1140
	step [25/244], loss=68.5898
	step [26/244], loss=79.8155
	step [27/244], loss=89.7535
	step [28/244], loss=63.2779
	step [29/244], loss=100.5281
	step [30/244], loss=76.5804
	step [31/244], loss=77.4343
	step [32/244], loss=67.0691
	step [33/244], loss=80.0202
	step [34/244], loss=75.9609
	step [35/244], loss=70.1648
	step [36/244], loss=83.6716
	step [37/244], loss=83.8020
	step [38/244], loss=90.1265
	step [39/244], loss=83.3495
	step [40/244], loss=88.5974
	step [41/244], loss=81.8217
	step [42/244], loss=83.8394
	step [43/244], loss=73.4786
	step [44/244], loss=79.5637
	step [45/244], loss=73.4445
	step [46/244], loss=84.0825
	step [47/244], loss=63.5348
	step [48/244], loss=69.6636
	step [49/244], loss=90.7535
	step [50/244], loss=72.7638
	step [51/244], loss=72.7171
	step [52/244], loss=75.7788
	step [53/244], loss=72.1245
	step [54/244], loss=103.0561
	step [55/244], loss=96.7728
	step [56/244], loss=82.1373
	step [57/244], loss=79.2680
	step [58/244], loss=74.0865
	step [59/244], loss=70.8576
	step [60/244], loss=72.4965
	step [61/244], loss=85.5587
	step [62/244], loss=84.1696
	step [63/244], loss=94.0189
	step [64/244], loss=61.7996
	step [65/244], loss=55.1693
	step [66/244], loss=78.0890
	step [67/244], loss=76.6633
	step [68/244], loss=81.9944
	step [69/244], loss=94.1999
	step [70/244], loss=68.2126
	step [71/244], loss=90.3872
	step [72/244], loss=66.4110
	step [73/244], loss=83.8543
	step [74/244], loss=79.1605
	step [75/244], loss=88.7942
	step [76/244], loss=80.5524
	step [77/244], loss=96.5104
	step [78/244], loss=70.7483
	step [79/244], loss=91.1765
	step [80/244], loss=73.2392
	step [81/244], loss=93.5824
	step [82/244], loss=81.3564
	step [83/244], loss=96.2205
	step [84/244], loss=85.1270
	step [85/244], loss=72.4120
	step [86/244], loss=59.6452
	step [87/244], loss=79.0965
	step [88/244], loss=85.0171
	step [89/244], loss=58.5956
	step [90/244], loss=72.3229
	step [91/244], loss=85.0392
	step [92/244], loss=79.1744
	step [93/244], loss=57.3263
	step [94/244], loss=85.6637
	step [95/244], loss=60.6396
	step [96/244], loss=95.0968
	step [97/244], loss=94.1295
	step [98/244], loss=67.4388
	step [99/244], loss=90.5324
	step [100/244], loss=82.3527
	step [101/244], loss=88.8358
	step [102/244], loss=83.5383
	step [103/244], loss=85.9938
	step [104/244], loss=72.3260
	step [105/244], loss=86.9484
	step [106/244], loss=78.7492
	step [107/244], loss=88.1823
	step [108/244], loss=59.7491
	step [109/244], loss=95.5700
	step [110/244], loss=82.9625
	step [111/244], loss=90.1377
	step [112/244], loss=78.9319
	step [113/244], loss=72.0550
	step [114/244], loss=102.8568
	step [115/244], loss=74.2387
	step [116/244], loss=68.0616
	step [117/244], loss=94.9451
	step [118/244], loss=80.6821
	step [119/244], loss=76.2601
	step [120/244], loss=85.9527
	step [121/244], loss=70.8716
	step [122/244], loss=96.0917
	step [123/244], loss=65.3737
	step [124/244], loss=68.8758
	step [125/244], loss=80.1692
	step [126/244], loss=69.8062
	step [127/244], loss=73.4739
	step [128/244], loss=72.8171
	step [129/244], loss=67.6099
	step [130/244], loss=83.9223
	step [131/244], loss=72.2544
	step [132/244], loss=64.4159
	step [133/244], loss=62.8123
	step [134/244], loss=86.6744
	step [135/244], loss=69.0697
	step [136/244], loss=77.7445
	step [137/244], loss=87.4610
	step [138/244], loss=88.0226
	step [139/244], loss=80.7298
	step [140/244], loss=75.6305
	step [141/244], loss=97.6157
	step [142/244], loss=88.8602
	step [143/244], loss=84.8815
	step [144/244], loss=85.8820
	step [145/244], loss=75.4768
	step [146/244], loss=62.6479
	step [147/244], loss=71.8503
	step [148/244], loss=82.3998
	step [149/244], loss=73.5095
	step [150/244], loss=93.9806
	step [151/244], loss=80.8241
	step [152/244], loss=74.5486
	step [153/244], loss=89.6820
	step [154/244], loss=75.5819
	step [155/244], loss=80.9159
	step [156/244], loss=63.7376
	step [157/244], loss=75.6070
	step [158/244], loss=74.1611
	step [159/244], loss=66.9866
	step [160/244], loss=72.0053
	step [161/244], loss=98.0768
	step [162/244], loss=89.7185
	step [163/244], loss=71.3908
	step [164/244], loss=76.8499
	step [165/244], loss=86.5269
	step [166/244], loss=76.4949
	step [167/244], loss=64.2978
	step [168/244], loss=78.2238
	step [169/244], loss=90.5244
	step [170/244], loss=96.7173
	step [171/244], loss=65.4242
	step [172/244], loss=75.1352
	step [173/244], loss=67.1721
	step [174/244], loss=95.2617
	step [175/244], loss=88.4188
	step [176/244], loss=69.8983
	step [177/244], loss=86.6651
	step [178/244], loss=87.2979
	step [179/244], loss=71.3126
	step [180/244], loss=68.5032
	step [181/244], loss=66.5819
	step [182/244], loss=73.4672
	step [183/244], loss=88.7293
	step [184/244], loss=77.4499
	step [185/244], loss=98.8401
	step [186/244], loss=76.4828
	step [187/244], loss=87.4708
	step [188/244], loss=74.0847
	step [189/244], loss=74.9968
	step [190/244], loss=71.0340
	step [191/244], loss=91.4140
	step [192/244], loss=65.7219
	step [193/244], loss=61.4671
	step [194/244], loss=63.4131
	step [195/244], loss=94.4893
	step [196/244], loss=85.8746
	step [197/244], loss=69.7668
	step [198/244], loss=82.2196
	step [199/244], loss=71.4849
	step [200/244], loss=84.5038
	step [201/244], loss=72.6665
	step [202/244], loss=95.2566
	step [203/244], loss=73.6205
	step [204/244], loss=81.3416
	step [205/244], loss=99.7599
	step [206/244], loss=63.3722
	step [207/244], loss=68.7466
	step [208/244], loss=75.7012
	step [209/244], loss=64.9622
	step [210/244], loss=82.1625
	step [211/244], loss=74.4878
	step [212/244], loss=89.7452
	step [213/244], loss=88.9745
	step [214/244], loss=90.4329
	step [215/244], loss=68.2350
	step [216/244], loss=88.3361
	step [217/244], loss=82.7780
	step [218/244], loss=67.0233
	step [219/244], loss=77.4003
	step [220/244], loss=66.5774
	step [221/244], loss=78.3541
	step [222/244], loss=78.1233
	step [223/244], loss=82.7874
	step [224/244], loss=71.5993
	step [225/244], loss=75.8456
	step [226/244], loss=83.1429
	step [227/244], loss=90.2312
	step [228/244], loss=89.2174
	step [229/244], loss=83.5935
	step [230/244], loss=74.4082
	step [231/244], loss=78.9171
	step [232/244], loss=82.3063
	step [233/244], loss=78.6132
	step [234/244], loss=76.6721
	step [235/244], loss=68.4298
	step [236/244], loss=96.9398
	step [237/244], loss=86.6509
	step [238/244], loss=63.1678
	step [239/244], loss=78.5418
	step [240/244], loss=81.4688
	step [241/244], loss=93.0039
	step [242/244], loss=71.4821
	step [243/244], loss=96.4104
	step [244/244], loss=69.9222
	Evaluating
	loss=0.0089, precision=0.3355, recall=0.8584, f1=0.4825
saving model as: 1_saved_model.pth
Training epoch 61
	step [1/244], loss=94.3245
	step [2/244], loss=59.5447
	step [3/244], loss=85.5592
	step [4/244], loss=73.3966
	step [5/244], loss=88.1665
	step [6/244], loss=88.0770
	step [7/244], loss=67.0484
	step [8/244], loss=74.2767
	step [9/244], loss=72.8843
	step [10/244], loss=77.3914
	step [11/244], loss=66.6271
	step [12/244], loss=80.5849
	step [13/244], loss=66.3967
	step [14/244], loss=90.2825
	step [15/244], loss=77.0256
	step [16/244], loss=76.9008
	step [17/244], loss=81.8710
	step [18/244], loss=86.7436
	step [19/244], loss=109.8626
	step [20/244], loss=78.3348
	step [21/244], loss=75.8234
	step [22/244], loss=86.0971
	step [23/244], loss=81.5785
	step [24/244], loss=74.4005
	step [25/244], loss=69.2201
	step [26/244], loss=79.7320
	step [27/244], loss=84.4722
	step [28/244], loss=61.4025
	step [29/244], loss=89.5862
	step [30/244], loss=86.9709
	step [31/244], loss=72.2775
	step [32/244], loss=70.6923
	step [33/244], loss=77.9379
	step [34/244], loss=70.5992
	step [35/244], loss=68.8957
	step [36/244], loss=66.0695
	step [37/244], loss=75.1289
	step [38/244], loss=80.4407
	step [39/244], loss=72.6329
	step [40/244], loss=78.7472
	step [41/244], loss=88.9495
	step [42/244], loss=65.9527
	step [43/244], loss=72.2468
	step [44/244], loss=68.6769
	step [45/244], loss=95.0392
	step [46/244], loss=68.3392
	step [47/244], loss=84.3566
	step [48/244], loss=69.3632
	step [49/244], loss=90.3720
	step [50/244], loss=63.8062
	step [51/244], loss=84.4496
	step [52/244], loss=71.4311
	step [53/244], loss=75.2567
	step [54/244], loss=73.5729
	step [55/244], loss=78.7659
	step [56/244], loss=58.9376
	step [57/244], loss=70.7923
	step [58/244], loss=78.1122
	step [59/244], loss=86.2405
	step [60/244], loss=81.9397
	step [61/244], loss=70.0766
	step [62/244], loss=72.4687
	step [63/244], loss=85.8769
	step [64/244], loss=80.2658
	step [65/244], loss=87.4138
	step [66/244], loss=94.3885
	step [67/244], loss=79.0370
	step [68/244], loss=70.4195
	step [69/244], loss=73.4731
	step [70/244], loss=82.5347
	step [71/244], loss=75.9051
	step [72/244], loss=100.8005
	step [73/244], loss=91.4332
	step [74/244], loss=62.4053
	step [75/244], loss=76.5972
	step [76/244], loss=73.3293
	step [77/244], loss=79.3503
	step [78/244], loss=83.0556
	step [79/244], loss=78.0050
	step [80/244], loss=71.7971
	step [81/244], loss=82.6614
	step [82/244], loss=63.6628
	step [83/244], loss=68.3049
	step [84/244], loss=82.2399
	step [85/244], loss=79.5508
	step [86/244], loss=80.6983
	step [87/244], loss=69.3859
	step [88/244], loss=82.9430
	step [89/244], loss=66.8163
	step [90/244], loss=70.6847
	step [91/244], loss=68.6840
	step [92/244], loss=76.7610
	step [93/244], loss=87.0837
	step [94/244], loss=89.8010
	step [95/244], loss=80.8955
	step [96/244], loss=80.3100
	step [97/244], loss=88.3258
	step [98/244], loss=79.4225
	step [99/244], loss=93.2950
	step [100/244], loss=87.8398
	step [101/244], loss=92.4431
	step [102/244], loss=81.6105
	step [103/244], loss=75.1839
	step [104/244], loss=89.9769
	step [105/244], loss=77.2503
	step [106/244], loss=90.8705
	step [107/244], loss=82.3423
	step [108/244], loss=91.5641
	step [109/244], loss=70.3233
	step [110/244], loss=95.5576
	step [111/244], loss=69.0773
	step [112/244], loss=85.4268
	step [113/244], loss=89.0693
	step [114/244], loss=88.7667
	step [115/244], loss=89.4201
	step [116/244], loss=102.2888
	step [117/244], loss=80.3852
	step [118/244], loss=81.1330
	step [119/244], loss=75.4601
	step [120/244], loss=82.3590
	step [121/244], loss=63.5675
	step [122/244], loss=93.0073
	step [123/244], loss=82.9182
	step [124/244], loss=81.1561
	step [125/244], loss=87.5897
	step [126/244], loss=86.0376
	step [127/244], loss=83.8295
	step [128/244], loss=66.9405
	step [129/244], loss=85.0065
	step [130/244], loss=80.8008
	step [131/244], loss=81.7828
	step [132/244], loss=84.4580
	step [133/244], loss=66.0734
	step [134/244], loss=74.0123
	step [135/244], loss=100.3225
	step [136/244], loss=72.0582
	step [137/244], loss=64.6466
	step [138/244], loss=62.8978
	step [139/244], loss=76.6203
	step [140/244], loss=79.2199
	step [141/244], loss=76.7610
	step [142/244], loss=72.8545
	step [143/244], loss=96.7662
	step [144/244], loss=71.9947
	step [145/244], loss=72.8836
	step [146/244], loss=84.8476
	step [147/244], loss=63.6881
	step [148/244], loss=84.1592
	step [149/244], loss=76.0823
	step [150/244], loss=77.1792
	step [151/244], loss=74.6345
	step [152/244], loss=76.5853
	step [153/244], loss=95.6034
	step [154/244], loss=96.6668
	step [155/244], loss=79.7918
	step [156/244], loss=87.4170
	step [157/244], loss=68.7842
	step [158/244], loss=86.6529
	step [159/244], loss=66.8859
	step [160/244], loss=74.9608
	step [161/244], loss=76.1323
	step [162/244], loss=89.6299
	step [163/244], loss=73.6074
	step [164/244], loss=83.6072
	step [165/244], loss=74.4221
	step [166/244], loss=64.0929
	step [167/244], loss=72.8379
	step [168/244], loss=89.7351
	step [169/244], loss=87.4960
	step [170/244], loss=80.9194
	step [171/244], loss=65.5479
	step [172/244], loss=93.6636
	step [173/244], loss=84.0565
	step [174/244], loss=78.4712
	step [175/244], loss=76.0663
	step [176/244], loss=73.7236
	step [177/244], loss=85.1515
	step [178/244], loss=54.1296
	step [179/244], loss=74.5364
	step [180/244], loss=72.5524
	step [181/244], loss=89.5188
	step [182/244], loss=72.4942
	step [183/244], loss=89.0497
	step [184/244], loss=80.3251
	step [185/244], loss=88.1479
	step [186/244], loss=75.9609
	step [187/244], loss=81.9597
	step [188/244], loss=88.7394
	step [189/244], loss=82.7365
	step [190/244], loss=81.3257
	step [191/244], loss=65.3423
	step [192/244], loss=73.4391
	step [193/244], loss=74.0671
	step [194/244], loss=72.0871
	step [195/244], loss=73.9104
	step [196/244], loss=60.3857
	step [197/244], loss=80.6826
	step [198/244], loss=93.9929
	step [199/244], loss=71.9779
	step [200/244], loss=84.9900
	step [201/244], loss=75.9971
	step [202/244], loss=73.6082
	step [203/244], loss=79.6194
	step [204/244], loss=84.1711
	step [205/244], loss=78.3978
	step [206/244], loss=73.5831
	step [207/244], loss=83.0186
	step [208/244], loss=79.0101
	step [209/244], loss=72.5460
	step [210/244], loss=66.3466
	step [211/244], loss=81.3979
	step [212/244], loss=69.8362
	step [213/244], loss=68.8814
	step [214/244], loss=96.4087
	step [215/244], loss=79.3019
	step [216/244], loss=88.2878
	step [217/244], loss=75.0092
	step [218/244], loss=68.8871
	step [219/244], loss=71.5813
	step [220/244], loss=78.0578
	step [221/244], loss=83.0647
	step [222/244], loss=87.0849
	step [223/244], loss=73.6965
	step [224/244], loss=62.9155
	step [225/244], loss=87.9355
	step [226/244], loss=83.3528
	step [227/244], loss=76.2008
	step [228/244], loss=67.9424
	step [229/244], loss=70.8531
	step [230/244], loss=100.4302
	step [231/244], loss=73.5354
	step [232/244], loss=88.1808
	step [233/244], loss=92.0284
	step [234/244], loss=74.1524
	step [235/244], loss=80.3208
	step [236/244], loss=65.1273
	step [237/244], loss=91.9982
	step [238/244], loss=76.8470
	step [239/244], loss=78.6274
	step [240/244], loss=78.5383
	step [241/244], loss=87.6958
	step [242/244], loss=56.9535
	step [243/244], loss=76.8895
	step [244/244], loss=58.8335
	Evaluating
	loss=0.0089, precision=0.3245, recall=0.8645, f1=0.4719
Training epoch 62
	step [1/244], loss=86.8041
	step [2/244], loss=80.5172
	step [3/244], loss=79.1297
	step [4/244], loss=81.6258
	step [5/244], loss=80.5611
	step [6/244], loss=84.4346
	step [7/244], loss=88.6785
	step [8/244], loss=66.3714
	step [9/244], loss=66.7904
	step [10/244], loss=81.0065
	step [11/244], loss=87.5813
	step [12/244], loss=80.2066
	step [13/244], loss=93.3814
	step [14/244], loss=80.4050
	step [15/244], loss=72.9357
	step [16/244], loss=76.5728
	step [17/244], loss=95.2283
	step [18/244], loss=77.1635
	step [19/244], loss=85.0582
	step [20/244], loss=78.8510
	step [21/244], loss=73.8469
	step [22/244], loss=79.2313
	step [23/244], loss=81.7558
	step [24/244], loss=84.0561
	step [25/244], loss=55.5088
	step [26/244], loss=75.5333
	step [27/244], loss=64.2308
	step [28/244], loss=65.4756
	step [29/244], loss=67.5352
	step [30/244], loss=84.7403
	step [31/244], loss=80.2942
	step [32/244], loss=65.6227
	step [33/244], loss=85.2954
	step [34/244], loss=74.5670
	step [35/244], loss=79.6772
	step [36/244], loss=75.9242
	step [37/244], loss=74.7510
	step [38/244], loss=74.3484
	step [39/244], loss=83.4830
	step [40/244], loss=83.5539
	step [41/244], loss=79.9813
	step [42/244], loss=78.2328
	step [43/244], loss=74.7856
	step [44/244], loss=64.0014
	step [45/244], loss=63.5378
	step [46/244], loss=86.7451
	step [47/244], loss=84.8143
	step [48/244], loss=70.0699
	step [49/244], loss=83.2402
	step [50/244], loss=67.2557
	step [51/244], loss=87.1080
	step [52/244], loss=61.7530
	step [53/244], loss=75.6255
	step [54/244], loss=72.7947
	step [55/244], loss=88.7214
	step [56/244], loss=82.6883
	step [57/244], loss=72.6188
	step [58/244], loss=74.4964
	step [59/244], loss=79.7646
	step [60/244], loss=90.8688
	step [61/244], loss=78.5525
	step [62/244], loss=74.5022
	step [63/244], loss=75.3950
	step [64/244], loss=78.9709
	step [65/244], loss=71.9387
	step [66/244], loss=87.5433
	step [67/244], loss=83.4647
	step [68/244], loss=86.3518
	step [69/244], loss=73.7751
	step [70/244], loss=81.6266
	step [71/244], loss=80.8236
	step [72/244], loss=74.8713
	step [73/244], loss=82.8939
	step [74/244], loss=60.4909
	step [75/244], loss=87.8233
	step [76/244], loss=77.5576
	step [77/244], loss=66.8648
	step [78/244], loss=72.5159
	step [79/244], loss=83.6442
	step [80/244], loss=91.0334
	step [81/244], loss=69.9510
	step [82/244], loss=73.2703
	step [83/244], loss=69.4090
	step [84/244], loss=88.3726
	step [85/244], loss=80.2087
	step [86/244], loss=70.1198
	step [87/244], loss=73.3596
	step [88/244], loss=77.9920
	step [89/244], loss=82.3184
	step [90/244], loss=78.7671
	step [91/244], loss=78.5758
	step [92/244], loss=76.9466
	step [93/244], loss=71.9260
	step [94/244], loss=94.0627
	step [95/244], loss=76.3213
	step [96/244], loss=72.2959
	step [97/244], loss=73.0849
	step [98/244], loss=81.9066
	step [99/244], loss=91.4527
	step [100/244], loss=86.8132
	step [101/244], loss=66.5909
	step [102/244], loss=81.6246
	step [103/244], loss=75.4142
	step [104/244], loss=71.3338
	step [105/244], loss=85.5169
	step [106/244], loss=60.5061
	step [107/244], loss=73.6616
	step [108/244], loss=74.9573
	step [109/244], loss=94.6447
	step [110/244], loss=72.5674
	step [111/244], loss=80.2946
	step [112/244], loss=77.8075
	step [113/244], loss=75.2787
	step [114/244], loss=86.3592
	step [115/244], loss=86.5958
	step [116/244], loss=91.5941
	step [117/244], loss=81.8561
	step [118/244], loss=67.9035
	step [119/244], loss=83.9900
	step [120/244], loss=80.9354
	step [121/244], loss=84.8107
	step [122/244], loss=78.3568
	step [123/244], loss=77.3156
	step [124/244], loss=75.5272
	step [125/244], loss=86.1489
	step [126/244], loss=78.4177
	step [127/244], loss=76.0105
	step [128/244], loss=77.9165
	step [129/244], loss=60.9835
	step [130/244], loss=87.8734
	step [131/244], loss=79.6125
	step [132/244], loss=76.6557
	step [133/244], loss=100.5954
	step [134/244], loss=66.3831
	step [135/244], loss=68.5054
	step [136/244], loss=71.1469
	step [137/244], loss=71.7727
	step [138/244], loss=64.2614
	step [139/244], loss=104.4147
	step [140/244], loss=81.9348
	step [141/244], loss=84.1378
	step [142/244], loss=83.1132
	step [143/244], loss=79.0774
	step [144/244], loss=76.3161
	step [145/244], loss=62.7646
	step [146/244], loss=73.9940
	step [147/244], loss=85.2077
	step [148/244], loss=87.5540
	step [149/244], loss=62.4116
	step [150/244], loss=80.0993
	step [151/244], loss=89.6380
	step [152/244], loss=76.9629
	step [153/244], loss=71.1961
	step [154/244], loss=85.7295
	step [155/244], loss=76.2829
	step [156/244], loss=77.6445
	step [157/244], loss=84.6855
	step [158/244], loss=93.3448
	step [159/244], loss=70.2708
	step [160/244], loss=76.1657
	step [161/244], loss=80.5212
	step [162/244], loss=62.3192
	step [163/244], loss=62.1887
	step [164/244], loss=73.0164
	step [165/244], loss=81.3397
	step [166/244], loss=76.6687
	step [167/244], loss=71.2752
	step [168/244], loss=76.8823
	step [169/244], loss=79.7343
	step [170/244], loss=72.5912
	step [171/244], loss=74.3257
	step [172/244], loss=76.3541
	step [173/244], loss=82.9026
	step [174/244], loss=63.9867
	step [175/244], loss=73.8679
	step [176/244], loss=80.8726
	step [177/244], loss=71.7648
	step [178/244], loss=64.7544
	step [179/244], loss=73.9668
	step [180/244], loss=84.6918
	step [181/244], loss=79.6149
	step [182/244], loss=81.7617
	step [183/244], loss=91.8160
	step [184/244], loss=86.2217
	step [185/244], loss=101.4895
	step [186/244], loss=85.0243
	step [187/244], loss=72.2816
	step [188/244], loss=69.9229
	step [189/244], loss=75.9292
	step [190/244], loss=80.0548
	step [191/244], loss=89.5704
	step [192/244], loss=88.6279
	step [193/244], loss=78.0052
	step [194/244], loss=77.2671
	step [195/244], loss=73.9657
	step [196/244], loss=77.0095
	step [197/244], loss=71.7788
	step [198/244], loss=88.1962
	step [199/244], loss=76.6973
	step [200/244], loss=71.3759
	step [201/244], loss=93.4090
	step [202/244], loss=69.5876
	step [203/244], loss=67.6856
	step [204/244], loss=78.7453
	step [205/244], loss=68.7845
	step [206/244], loss=84.5658
	step [207/244], loss=96.1244
	step [208/244], loss=76.1554
	step [209/244], loss=73.8143
	step [210/244], loss=69.1282
	step [211/244], loss=61.3050
	step [212/244], loss=84.9837
	step [213/244], loss=82.6279
	step [214/244], loss=76.6667
	step [215/244], loss=85.1750
	step [216/244], loss=100.0452
	step [217/244], loss=80.2896
	step [218/244], loss=75.1888
	step [219/244], loss=75.5133
	step [220/244], loss=96.7644
	step [221/244], loss=82.0163
	step [222/244], loss=84.4375
	step [223/244], loss=81.6912
	step [224/244], loss=74.6772
	step [225/244], loss=65.4608
	step [226/244], loss=76.2949
	step [227/244], loss=84.4212
	step [228/244], loss=80.5375
	step [229/244], loss=89.9398
	step [230/244], loss=85.8698
	step [231/244], loss=85.8934
	step [232/244], loss=87.2147
	step [233/244], loss=74.0674
	step [234/244], loss=72.9909
	step [235/244], loss=84.1081
	step [236/244], loss=77.5778
	step [237/244], loss=74.5909
	step [238/244], loss=58.5019
	step [239/244], loss=86.1218
	step [240/244], loss=86.6880
	step [241/244], loss=77.7278
	step [242/244], loss=71.6493
	step [243/244], loss=67.5364
	step [244/244], loss=75.8248
	Evaluating
	loss=0.0091, precision=0.3061, recall=0.8703, f1=0.4530
Training epoch 63
	step [1/244], loss=87.1102
	step [2/244], loss=61.6419
	step [3/244], loss=71.3530
	step [4/244], loss=80.3316
	step [5/244], loss=80.5465
	step [6/244], loss=85.7866
	step [7/244], loss=80.6021
	step [8/244], loss=77.1762
	step [9/244], loss=64.9489
	step [10/244], loss=71.1627
	step [11/244], loss=92.4263
	step [12/244], loss=83.0734
	step [13/244], loss=98.0761
	step [14/244], loss=83.0299
	step [15/244], loss=75.7356
	step [16/244], loss=78.6455
	step [17/244], loss=108.6680
	step [18/244], loss=68.6456
	step [19/244], loss=89.9470
	step [20/244], loss=86.4802
	step [21/244], loss=87.6906
	step [22/244], loss=78.7994
	step [23/244], loss=70.3961
	step [24/244], loss=65.5418
	step [25/244], loss=72.4461
	step [26/244], loss=70.8647
	step [27/244], loss=72.1657
	step [28/244], loss=72.3154
	step [29/244], loss=94.3544
	step [30/244], loss=92.0067
	step [31/244], loss=73.4076
	step [32/244], loss=86.7266
	step [33/244], loss=67.0017
	step [34/244], loss=62.7655
	step [35/244], loss=80.3822
	step [36/244], loss=75.8505
	step [37/244], loss=60.3341
	step [38/244], loss=82.8603
	step [39/244], loss=65.1907
	step [40/244], loss=74.6539
	step [41/244], loss=80.3578
	step [42/244], loss=84.9734
	step [43/244], loss=83.3052
	step [44/244], loss=87.1211
	step [45/244], loss=62.7224
	step [46/244], loss=83.9944
	step [47/244], loss=71.5763
	step [48/244], loss=87.6682
	step [49/244], loss=80.0005
	step [50/244], loss=86.8748
	step [51/244], loss=85.5471
	step [52/244], loss=76.5586
	step [53/244], loss=90.2309
	step [54/244], loss=90.7813
	step [55/244], loss=76.3494
	step [56/244], loss=79.8094
	step [57/244], loss=76.5456
	step [58/244], loss=75.6132
	step [59/244], loss=75.2342
	step [60/244], loss=76.0931
	step [61/244], loss=83.9330
	step [62/244], loss=77.1701
	step [63/244], loss=85.9771
	step [64/244], loss=79.5349
	step [65/244], loss=80.7863
	step [66/244], loss=71.8155
	step [67/244], loss=62.1692
	step [68/244], loss=107.7970
	step [69/244], loss=83.9378
	step [70/244], loss=79.7049
	step [71/244], loss=93.4032
	step [72/244], loss=78.5960
	step [73/244], loss=87.8932
	step [74/244], loss=96.6617
	step [75/244], loss=70.6062
	step [76/244], loss=64.5296
	step [77/244], loss=79.3366
	step [78/244], loss=77.0624
	step [79/244], loss=93.1282
	step [80/244], loss=92.6212
	step [81/244], loss=69.6901
	step [82/244], loss=74.1204
	step [83/244], loss=66.0633
	step [84/244], loss=67.1669
	step [85/244], loss=83.9230
	step [86/244], loss=84.9940
	step [87/244], loss=80.7968
	step [88/244], loss=80.4520
	step [89/244], loss=78.7225
	step [90/244], loss=74.6613
	step [91/244], loss=77.5884
	step [92/244], loss=57.1009
	step [93/244], loss=81.6282
	step [94/244], loss=75.4809
	step [95/244], loss=69.8291
	step [96/244], loss=66.1899
	step [97/244], loss=86.8759
	step [98/244], loss=70.0646
	step [99/244], loss=93.1685
	step [100/244], loss=64.1812
	step [101/244], loss=68.6277
	step [102/244], loss=81.3734
	step [103/244], loss=72.3227
	step [104/244], loss=70.5436
	step [105/244], loss=86.4670
	step [106/244], loss=74.5674
	step [107/244], loss=96.6612
	step [108/244], loss=71.0463
	step [109/244], loss=75.4856
	step [110/244], loss=82.1010
	step [111/244], loss=96.2222
	step [112/244], loss=66.5282
	step [113/244], loss=59.4552
	step [114/244], loss=85.1067
	step [115/244], loss=82.0053
	step [116/244], loss=60.9053
	step [117/244], loss=68.5315
	step [118/244], loss=80.4991
	step [119/244], loss=64.3439
	step [120/244], loss=67.6063
	step [121/244], loss=69.1096
	step [122/244], loss=76.9271
	step [123/244], loss=80.2483
	step [124/244], loss=81.6604
	step [125/244], loss=83.2323
	step [126/244], loss=58.8934
	step [127/244], loss=70.6395
	step [128/244], loss=83.2782
	step [129/244], loss=58.3546
	step [130/244], loss=68.8568
	step [131/244], loss=78.9889
	step [132/244], loss=72.1727
	step [133/244], loss=72.7004
	step [134/244], loss=76.8624
	step [135/244], loss=68.0351
	step [136/244], loss=73.6292
	step [137/244], loss=87.4457
	step [138/244], loss=80.5669
	step [139/244], loss=85.9877
	step [140/244], loss=73.8782
	step [141/244], loss=82.1237
	step [142/244], loss=92.0157
	step [143/244], loss=73.7566
	step [144/244], loss=87.2013
	step [145/244], loss=83.1366
	step [146/244], loss=80.6995
	step [147/244], loss=71.1790
	step [148/244], loss=65.5246
	step [149/244], loss=66.5939
	step [150/244], loss=75.2961
	step [151/244], loss=102.3230
	step [152/244], loss=83.3116
	step [153/244], loss=86.5659
	step [154/244], loss=83.6359
	step [155/244], loss=68.7340
	step [156/244], loss=99.0844
	step [157/244], loss=69.3625
	step [158/244], loss=68.4428
	step [159/244], loss=80.8904
	step [160/244], loss=80.8714
	step [161/244], loss=60.6946
	step [162/244], loss=70.0160
	step [163/244], loss=82.4019
	step [164/244], loss=76.9899
	step [165/244], loss=61.8821
	step [166/244], loss=68.4893
	step [167/244], loss=102.3484
	step [168/244], loss=67.4609
	step [169/244], loss=65.3327
	step [170/244], loss=73.9080
	step [171/244], loss=64.2253
	step [172/244], loss=90.5994
	step [173/244], loss=75.4262
	step [174/244], loss=75.7501
	step [175/244], loss=79.8585
	step [176/244], loss=78.2081
	step [177/244], loss=90.8026
	step [178/244], loss=79.4184
	step [179/244], loss=65.0886
	step [180/244], loss=75.9181
	step [181/244], loss=79.2486
	step [182/244], loss=87.9055
	step [183/244], loss=75.7696
	step [184/244], loss=94.9102
	step [185/244], loss=84.2073
	step [186/244], loss=87.6462
	step [187/244], loss=73.3297
	step [188/244], loss=88.6317
	step [189/244], loss=77.5676
	step [190/244], loss=83.2489
	step [191/244], loss=77.1982
	step [192/244], loss=80.0346
	step [193/244], loss=76.1941
	step [194/244], loss=71.5472
	step [195/244], loss=69.0733
	step [196/244], loss=84.6044
	step [197/244], loss=91.6218
	step [198/244], loss=69.1240
	step [199/244], loss=83.7264
	step [200/244], loss=79.4223
	step [201/244], loss=84.5628
	step [202/244], loss=76.1997
	step [203/244], loss=84.1713
	step [204/244], loss=69.5382
	step [205/244], loss=80.8993
	step [206/244], loss=70.9744
	step [207/244], loss=75.2502
	step [208/244], loss=72.7288
	step [209/244], loss=77.2353
	step [210/244], loss=79.8253
	step [211/244], loss=78.6908
	step [212/244], loss=77.5842
	step [213/244], loss=56.7010
	step [214/244], loss=72.1765
	step [215/244], loss=82.6803
	step [216/244], loss=74.5288
	step [217/244], loss=79.0712
	step [218/244], loss=78.8894
	step [219/244], loss=59.7732
	step [220/244], loss=73.9468
	step [221/244], loss=70.3872
	step [222/244], loss=78.5439
	step [223/244], loss=76.1239
	step [224/244], loss=79.6632
	step [225/244], loss=72.4477
	step [226/244], loss=65.3501
	step [227/244], loss=74.2037
	step [228/244], loss=79.4626
	step [229/244], loss=75.1527
	step [230/244], loss=64.4634
	step [231/244], loss=75.4605
	step [232/244], loss=91.0820
	step [233/244], loss=65.7175
	step [234/244], loss=96.2580
	step [235/244], loss=67.8106
	step [236/244], loss=71.3899
	step [237/244], loss=100.7040
	step [238/244], loss=75.8366
	step [239/244], loss=80.2875
	step [240/244], loss=71.2355
	step [241/244], loss=84.5086
	step [242/244], loss=92.8183
	step [243/244], loss=74.9777
	step [244/244], loss=81.0939
	Evaluating
	loss=0.0087, precision=0.3295, recall=0.8495, f1=0.4749
Training epoch 64
	step [1/244], loss=85.7717
	step [2/244], loss=72.2326
	step [3/244], loss=83.8800
	step [4/244], loss=70.1011
	step [5/244], loss=64.2386
	step [6/244], loss=69.4423
	step [7/244], loss=77.1902
	step [8/244], loss=65.7979
	step [9/244], loss=67.9721
	step [10/244], loss=98.6332
	step [11/244], loss=64.0585
	step [12/244], loss=82.2881
	step [13/244], loss=83.6529
	step [14/244], loss=91.7983
	step [15/244], loss=79.5599
	step [16/244], loss=86.1939
	step [17/244], loss=66.0684
	step [18/244], loss=64.1068
	step [19/244], loss=86.6052
	step [20/244], loss=73.0226
	step [21/244], loss=61.8411
	step [22/244], loss=66.5173
	step [23/244], loss=87.8764
	step [24/244], loss=79.5130
	step [25/244], loss=76.9364
	step [26/244], loss=88.2197
	step [27/244], loss=70.2832
	step [28/244], loss=74.9865
	step [29/244], loss=70.4700
	step [30/244], loss=74.2037
	step [31/244], loss=75.9524
	step [32/244], loss=78.3941
	step [33/244], loss=87.9689
	step [34/244], loss=87.2054
	step [35/244], loss=69.8637
	step [36/244], loss=76.0530
	step [37/244], loss=85.6832
	step [38/244], loss=74.8565
	step [39/244], loss=81.3988
	step [40/244], loss=60.2939
	step [41/244], loss=83.3171
	step [42/244], loss=81.4052
	step [43/244], loss=92.8763
	step [44/244], loss=76.2428
	step [45/244], loss=90.6465
	step [46/244], loss=78.6054
	step [47/244], loss=71.6631
	step [48/244], loss=84.6240
	step [49/244], loss=69.2473
	step [50/244], loss=72.0265
	step [51/244], loss=87.5870
	step [52/244], loss=84.3902
	step [53/244], loss=72.7015
	step [54/244], loss=85.6940
	step [55/244], loss=76.0322
	step [56/244], loss=79.6308
	step [57/244], loss=80.7757
	step [58/244], loss=81.3113
	step [59/244], loss=83.6179
	step [60/244], loss=79.5358
	step [61/244], loss=69.1858
	step [62/244], loss=88.3432
	step [63/244], loss=74.7635
	step [64/244], loss=73.2998
	step [65/244], loss=63.1420
	step [66/244], loss=87.1745
	step [67/244], loss=83.4790
	step [68/244], loss=79.0893
	step [69/244], loss=86.9482
	step [70/244], loss=87.3371
	step [71/244], loss=89.2968
	step [72/244], loss=78.4688
	step [73/244], loss=72.6756
	step [74/244], loss=64.1728
	step [75/244], loss=77.7378
	step [76/244], loss=78.7279
	step [77/244], loss=79.8329
	step [78/244], loss=93.1590
	step [79/244], loss=75.5373
	step [80/244], loss=78.8007
	step [81/244], loss=73.2325
	step [82/244], loss=84.7122
	step [83/244], loss=91.9267
	step [84/244], loss=87.1300
	step [85/244], loss=85.5383
	step [86/244], loss=70.1172
	step [87/244], loss=81.0257
	step [88/244], loss=64.8973
	step [89/244], loss=78.7197
	step [90/244], loss=81.2048
	step [91/244], loss=88.9031
	step [92/244], loss=61.0669
	step [93/244], loss=99.1619
	step [94/244], loss=71.3263
	step [95/244], loss=86.5871
	step [96/244], loss=77.3897
	step [97/244], loss=70.5980
	step [98/244], loss=66.4203
	step [99/244], loss=67.1750
	step [100/244], loss=85.4088
	step [101/244], loss=72.9340
	step [102/244], loss=78.6254
	step [103/244], loss=65.1568
	step [104/244], loss=69.2189
	step [105/244], loss=74.1062
	step [106/244], loss=84.4781
	step [107/244], loss=70.5204
	step [108/244], loss=68.1552
	step [109/244], loss=74.5896
	step [110/244], loss=73.1235
	step [111/244], loss=83.6996
	step [112/244], loss=70.4619
	step [113/244], loss=66.0847
	step [114/244], loss=65.0367
	step [115/244], loss=87.7322
	step [116/244], loss=59.8711
	step [117/244], loss=76.3406
	step [118/244], loss=70.6361
	step [119/244], loss=77.9031
	step [120/244], loss=63.3456
	step [121/244], loss=86.3779
	step [122/244], loss=76.8012
	step [123/244], loss=81.2952
	step [124/244], loss=73.3718
	step [125/244], loss=82.9734
	step [126/244], loss=86.8868
	step [127/244], loss=74.0953
	step [128/244], loss=91.3873
	step [129/244], loss=85.6550
	step [130/244], loss=70.5193
	step [131/244], loss=81.8887
	step [132/244], loss=68.8708
	step [133/244], loss=78.0728
	step [134/244], loss=73.3479
	step [135/244], loss=76.8635
	step [136/244], loss=63.7480
	step [137/244], loss=76.1484
	step [138/244], loss=64.1396
	step [139/244], loss=83.4045
	step [140/244], loss=101.1272
	step [141/244], loss=73.9498
	step [142/244], loss=83.9591
	step [143/244], loss=76.3790
	step [144/244], loss=88.1888
	step [145/244], loss=63.3511
	step [146/244], loss=83.4234
	step [147/244], loss=80.3474
	step [148/244], loss=79.3049
	step [149/244], loss=87.0682
	step [150/244], loss=86.7327
	step [151/244], loss=77.2612
	step [152/244], loss=77.8956
	step [153/244], loss=73.0033
	step [154/244], loss=79.5382
	step [155/244], loss=82.3377
	step [156/244], loss=63.7885
	step [157/244], loss=74.9015
	step [158/244], loss=82.7957
	step [159/244], loss=85.0342
	step [160/244], loss=77.0661
	step [161/244], loss=94.2429
	step [162/244], loss=67.9890
	step [163/244], loss=74.3859
	step [164/244], loss=80.4078
	step [165/244], loss=73.9583
	step [166/244], loss=90.2353
	step [167/244], loss=83.4966
	step [168/244], loss=61.9935
	step [169/244], loss=72.7796
	step [170/244], loss=76.2421
	step [171/244], loss=68.4417
	step [172/244], loss=93.6073
	step [173/244], loss=80.2801
	step [174/244], loss=76.0119
	step [175/244], loss=59.9300
	step [176/244], loss=88.6533
	step [177/244], loss=95.4809
	step [178/244], loss=85.8365
	step [179/244], loss=93.2198
	step [180/244], loss=79.8522
	step [181/244], loss=80.4587
	step [182/244], loss=69.3059
	step [183/244], loss=83.0311
	step [184/244], loss=66.5686
	step [185/244], loss=65.3745
	step [186/244], loss=75.6851
	step [187/244], loss=70.9116
	step [188/244], loss=70.1856
	step [189/244], loss=71.1776
	step [190/244], loss=106.2632
	step [191/244], loss=86.5972
	step [192/244], loss=66.5972
	step [193/244], loss=91.6411
	step [194/244], loss=69.7676
	step [195/244], loss=74.9614
	step [196/244], loss=56.7277
	step [197/244], loss=83.4669
	step [198/244], loss=94.4230
	step [199/244], loss=71.3886
	step [200/244], loss=77.8657
	step [201/244], loss=71.1637
	step [202/244], loss=73.1337
	step [203/244], loss=70.6076
	step [204/244], loss=68.4933
	step [205/244], loss=86.7217
	step [206/244], loss=76.5031
	step [207/244], loss=78.3796
	step [208/244], loss=87.5296
	step [209/244], loss=70.8187
	step [210/244], loss=88.6611
	step [211/244], loss=91.1426
	step [212/244], loss=60.8384
	step [213/244], loss=88.5389
	step [214/244], loss=84.6362
	step [215/244], loss=97.1781
	step [216/244], loss=70.5216
	step [217/244], loss=73.6129
	step [218/244], loss=78.6047
	step [219/244], loss=58.2798
	step [220/244], loss=77.1960
	step [221/244], loss=73.2135
	step [222/244], loss=75.8788
	step [223/244], loss=85.6846
	step [224/244], loss=88.2146
	step [225/244], loss=68.5031
	step [226/244], loss=89.7240
	step [227/244], loss=71.3754
	step [228/244], loss=66.0347
	step [229/244], loss=67.0224
	step [230/244], loss=82.8451
	step [231/244], loss=78.2259
	step [232/244], loss=73.6515
	step [233/244], loss=73.7477
	step [234/244], loss=85.0233
	step [235/244], loss=72.2756
	step [236/244], loss=86.5219
	step [237/244], loss=63.8024
	step [238/244], loss=81.1892
	step [239/244], loss=70.6479
	step [240/244], loss=64.4404
	step [241/244], loss=83.5110
	step [242/244], loss=78.5686
	step [243/244], loss=83.7682
	step [244/244], loss=79.0401
	Evaluating
	loss=0.0092, precision=0.3205, recall=0.8620, f1=0.4673
Training epoch 65
	step [1/244], loss=69.8754
	step [2/244], loss=91.7521
	step [3/244], loss=74.6479
	step [4/244], loss=74.6800
	step [5/244], loss=63.8732
	step [6/244], loss=69.9423
	step [7/244], loss=84.6380
	step [8/244], loss=80.8670
	step [9/244], loss=76.0563
	step [10/244], loss=74.3414
	step [11/244], loss=97.7195
	step [12/244], loss=78.9593
	step [13/244], loss=66.2491
	step [14/244], loss=91.6420
	step [15/244], loss=76.9192
	step [16/244], loss=61.5678
	step [17/244], loss=87.9492
	step [18/244], loss=71.3495
	step [19/244], loss=61.4944
	step [20/244], loss=74.1134
	step [21/244], loss=82.9491
	step [22/244], loss=55.7818
	step [23/244], loss=76.9597
	step [24/244], loss=71.6337
	step [25/244], loss=69.9961
	step [26/244], loss=77.8005
	step [27/244], loss=71.6669
	step [28/244], loss=82.9620
	step [29/244], loss=87.4311
	step [30/244], loss=65.4096
	step [31/244], loss=63.6589
	step [32/244], loss=68.4790
	step [33/244], loss=77.2055
	step [34/244], loss=71.4724
	step [35/244], loss=98.8351
	step [36/244], loss=93.7086
	step [37/244], loss=71.4262
	step [38/244], loss=79.3711
	step [39/244], loss=84.7816
	step [40/244], loss=80.6397
	step [41/244], loss=66.5479
	step [42/244], loss=76.3449
	step [43/244], loss=76.3690
	step [44/244], loss=83.7919
	step [45/244], loss=75.3228
	step [46/244], loss=86.4369
	step [47/244], loss=75.0398
	step [48/244], loss=82.2183
	step [49/244], loss=91.7097
	step [50/244], loss=76.5866
	step [51/244], loss=76.4714
	step [52/244], loss=68.6203
	step [53/244], loss=76.6571
	step [54/244], loss=79.0940
	step [55/244], loss=77.4687
	step [56/244], loss=83.9291
	step [57/244], loss=80.3412
	step [58/244], loss=85.9110
	step [59/244], loss=67.6491
	step [60/244], loss=65.2411
	step [61/244], loss=78.6785
	step [62/244], loss=78.4116
	step [63/244], loss=81.8618
	step [64/244], loss=83.7995
	step [65/244], loss=84.7203
	step [66/244], loss=78.0442
	step [67/244], loss=76.0850
	step [68/244], loss=68.6470
	step [69/244], loss=82.5619
	step [70/244], loss=64.4913
	step [71/244], loss=74.2819
	step [72/244], loss=76.8170
	step [73/244], loss=72.3719
	step [74/244], loss=62.9371
	step [75/244], loss=67.4720
	step [76/244], loss=74.9666
	step [77/244], loss=90.6541
	step [78/244], loss=72.4498
	step [79/244], loss=85.8007
	step [80/244], loss=80.6183
	step [81/244], loss=64.1325
	step [82/244], loss=75.6474
	step [83/244], loss=84.3770
	step [84/244], loss=89.9476
	step [85/244], loss=72.8550
	step [86/244], loss=85.8402
	step [87/244], loss=81.4044
	step [88/244], loss=74.8765
	step [89/244], loss=70.4855
	step [90/244], loss=64.8682
	step [91/244], loss=82.6990
	step [92/244], loss=72.0787
	step [93/244], loss=62.5783
	step [94/244], loss=83.9982
	step [95/244], loss=109.6151
	step [96/244], loss=63.6972
	step [97/244], loss=72.7071
	step [98/244], loss=74.5634
	step [99/244], loss=84.0831
	step [100/244], loss=81.8427
	step [101/244], loss=83.4372
	step [102/244], loss=84.4270
	step [103/244], loss=81.2678
	step [104/244], loss=81.4519
	step [105/244], loss=78.5292
	step [106/244], loss=75.2650
	step [107/244], loss=94.5401
	step [108/244], loss=83.1367
	step [109/244], loss=66.5264
	step [110/244], loss=105.7437
	step [111/244], loss=70.6983
	step [112/244], loss=79.9894
	step [113/244], loss=65.2962
	step [114/244], loss=76.9851
	step [115/244], loss=73.6303
	step [116/244], loss=78.1901
	step [117/244], loss=79.7052
	step [118/244], loss=83.9883
	step [119/244], loss=79.3598
	step [120/244], loss=78.3983
	step [121/244], loss=81.9731
	step [122/244], loss=67.8401
	step [123/244], loss=90.4276
	step [124/244], loss=84.0516
	step [125/244], loss=82.0672
	step [126/244], loss=75.2747
	step [127/244], loss=87.1247
	step [128/244], loss=61.7425
	step [129/244], loss=69.6757
	step [130/244], loss=64.2643
	step [131/244], loss=79.7992
	step [132/244], loss=70.9627
	step [133/244], loss=73.0045
	step [134/244], loss=85.9333
	step [135/244], loss=68.1085
	step [136/244], loss=69.0177
	step [137/244], loss=62.9705
	step [138/244], loss=82.8602
	step [139/244], loss=87.6609
	step [140/244], loss=92.8334
	step [141/244], loss=82.2109
	step [142/244], loss=76.3646
	step [143/244], loss=78.9849
	step [144/244], loss=90.5467
	step [145/244], loss=68.6794
	step [146/244], loss=92.7622
	step [147/244], loss=70.5330
	step [148/244], loss=78.6842
	step [149/244], loss=75.7771
	step [150/244], loss=81.4586
	step [151/244], loss=70.3474
	step [152/244], loss=73.1705
	step [153/244], loss=69.1699
	step [154/244], loss=59.1682
	step [155/244], loss=95.3050
	step [156/244], loss=80.6025
	step [157/244], loss=68.6274
	step [158/244], loss=81.1524
	step [159/244], loss=72.4403
	step [160/244], loss=64.3494
	step [161/244], loss=63.1029
	step [162/244], loss=79.1045
	step [163/244], loss=104.6843
	step [164/244], loss=92.7844
	step [165/244], loss=89.3884
	step [166/244], loss=76.1342
	step [167/244], loss=74.9691
	step [168/244], loss=84.8241
	step [169/244], loss=69.2669
	step [170/244], loss=79.6367
	step [171/244], loss=79.2710
	step [172/244], loss=84.4001
	step [173/244], loss=78.0393
	step [174/244], loss=74.6337
	step [175/244], loss=109.7071
	step [176/244], loss=70.7536
	step [177/244], loss=70.2349
	step [178/244], loss=73.7792
	step [179/244], loss=92.0355
	step [180/244], loss=75.4849
	step [181/244], loss=80.8998
	step [182/244], loss=58.8898
	step [183/244], loss=77.8331
	step [184/244], loss=79.5552
	step [185/244], loss=69.8802
	step [186/244], loss=82.7738
	step [187/244], loss=87.8503
	step [188/244], loss=69.0844
	step [189/244], loss=66.1089
	step [190/244], loss=85.6068
	step [191/244], loss=72.5810
	step [192/244], loss=65.7477
	step [193/244], loss=86.8201
	step [194/244], loss=85.6414
	step [195/244], loss=63.9743
	step [196/244], loss=63.8592
	step [197/244], loss=78.5456
	step [198/244], loss=90.8991
	step [199/244], loss=79.7292
	step [200/244], loss=90.2171
	step [201/244], loss=77.3031
	step [202/244], loss=78.2206
	step [203/244], loss=65.0190
	step [204/244], loss=73.3333
	step [205/244], loss=66.3385
	step [206/244], loss=90.7756
	step [207/244], loss=88.4769
	step [208/244], loss=90.0303
	step [209/244], loss=82.1447
	step [210/244], loss=72.6692
	step [211/244], loss=63.1635
	step [212/244], loss=71.1002
	step [213/244], loss=71.4020
	step [214/244], loss=67.0973
	step [215/244], loss=56.2662
	step [216/244], loss=78.6720
	step [217/244], loss=96.6814
	step [218/244], loss=69.3633
	step [219/244], loss=85.5234
	step [220/244], loss=80.5032
	step [221/244], loss=75.4672
	step [222/244], loss=74.8896
	step [223/244], loss=71.7369
	step [224/244], loss=81.0713
	step [225/244], loss=77.8086
	step [226/244], loss=94.3945
	step [227/244], loss=62.5770
	step [228/244], loss=71.2128
	step [229/244], loss=77.1814
	step [230/244], loss=77.3024
	step [231/244], loss=73.5804
	step [232/244], loss=77.8203
	step [233/244], loss=79.9380
	step [234/244], loss=71.6035
	step [235/244], loss=69.7946
	step [236/244], loss=93.0504
	step [237/244], loss=75.4956
	step [238/244], loss=64.2649
	step [239/244], loss=71.3321
	step [240/244], loss=67.1667
	step [241/244], loss=70.3539
	step [242/244], loss=82.0626
	step [243/244], loss=76.1755
	step [244/244], loss=82.8191
	Evaluating
	loss=0.0085, precision=0.3367, recall=0.8516, f1=0.4826
saving model as: 1_saved_model.pth
Training epoch 66
	step [1/244], loss=69.2207
	step [2/244], loss=90.1622
	step [3/244], loss=89.3185
	step [4/244], loss=83.4867
	step [5/244], loss=75.7290
	step [6/244], loss=75.5927
	step [7/244], loss=83.7273
	step [8/244], loss=83.4912
	step [9/244], loss=63.3753
	step [10/244], loss=74.9102
	step [11/244], loss=78.1629
	step [12/244], loss=88.3447
	step [13/244], loss=68.0896
	step [14/244], loss=83.9640
	step [15/244], loss=74.5533
	step [16/244], loss=81.1448
	step [17/244], loss=69.5889
	step [18/244], loss=60.2293
	step [19/244], loss=79.5285
	step [20/244], loss=91.5243
	step [21/244], loss=82.1707
	step [22/244], loss=81.7791
	step [23/244], loss=71.4095
	step [24/244], loss=101.4376
	step [25/244], loss=79.2748
	step [26/244], loss=82.2070
	step [27/244], loss=83.1802
	step [28/244], loss=94.6548
	step [29/244], loss=70.8453
	step [30/244], loss=81.5631
	step [31/244], loss=85.9039
	step [32/244], loss=75.9555
	step [33/244], loss=68.4413
	step [34/244], loss=65.3945
	step [35/244], loss=66.8609
	step [36/244], loss=68.3999
	step [37/244], loss=75.2656
	step [38/244], loss=71.8543
	step [39/244], loss=73.4865
	step [40/244], loss=74.1386
	step [41/244], loss=83.6637
	step [42/244], loss=61.8323
	step [43/244], loss=82.9761
	step [44/244], loss=71.2076
	step [45/244], loss=102.2255
	step [46/244], loss=83.9165
	step [47/244], loss=84.0279
	step [48/244], loss=74.0744
	step [49/244], loss=68.7839
	step [50/244], loss=75.9322
	step [51/244], loss=86.0361
	step [52/244], loss=87.2824
	step [53/244], loss=76.5346
	step [54/244], loss=64.0016
	step [55/244], loss=78.6694
	step [56/244], loss=86.6452
	step [57/244], loss=73.3835
	step [58/244], loss=67.6249
	step [59/244], loss=81.5951
	step [60/244], loss=83.8718
	step [61/244], loss=72.3354
	step [62/244], loss=67.5175
	step [63/244], loss=89.1251
	step [64/244], loss=81.7353
	step [65/244], loss=88.6539
	step [66/244], loss=76.2442
	step [67/244], loss=75.6886
	step [68/244], loss=89.6722
	step [69/244], loss=79.7046
	step [70/244], loss=83.1513
	step [71/244], loss=72.3204
	step [72/244], loss=85.8435
	step [73/244], loss=72.3503
	step [74/244], loss=66.3860
	step [75/244], loss=62.9441
	step [76/244], loss=69.8146
	step [77/244], loss=70.3750
	step [78/244], loss=59.9673
	step [79/244], loss=90.2200
	step [80/244], loss=87.1984
	step [81/244], loss=82.1751
	step [82/244], loss=62.8088
	step [83/244], loss=73.1959
	step [84/244], loss=88.9256
	step [85/244], loss=61.3628
	step [86/244], loss=59.7717
	step [87/244], loss=60.4901
	step [88/244], loss=61.9464
	step [89/244], loss=62.3489
	step [90/244], loss=70.5382
	step [91/244], loss=70.8461
	step [92/244], loss=72.9896
	step [93/244], loss=56.3221
	step [94/244], loss=64.3837
	step [95/244], loss=78.3539
	step [96/244], loss=82.1434
	step [97/244], loss=71.7748
	step [98/244], loss=87.4538
	step [99/244], loss=93.4325
	step [100/244], loss=79.3569
	step [101/244], loss=75.5306
	step [102/244], loss=77.1479
	step [103/244], loss=71.0754
	step [104/244], loss=65.1486
	step [105/244], loss=78.9978
	step [106/244], loss=72.7910
	step [107/244], loss=86.1617
	step [108/244], loss=70.6370
	step [109/244], loss=68.9016
	step [110/244], loss=84.9249
	step [111/244], loss=95.9554
	step [112/244], loss=78.5623
	step [113/244], loss=66.8477
	step [114/244], loss=81.0784
	step [115/244], loss=60.5890
	step [116/244], loss=72.6071
	step [117/244], loss=83.0060
	step [118/244], loss=76.7127
	step [119/244], loss=73.7198
	step [120/244], loss=62.4446
	step [121/244], loss=69.0362
	step [122/244], loss=91.5246
	step [123/244], loss=75.3683
	step [124/244], loss=73.5875
	step [125/244], loss=83.2096
	step [126/244], loss=82.5322
	step [127/244], loss=76.5388
	step [128/244], loss=82.1658
	step [129/244], loss=76.5762
	step [130/244], loss=80.6779
	step [131/244], loss=68.9019
	step [132/244], loss=91.6536
	step [133/244], loss=77.3201
	step [134/244], loss=88.8574
	step [135/244], loss=74.7297
	step [136/244], loss=73.2918
	step [137/244], loss=83.5733
	step [138/244], loss=87.7598
	step [139/244], loss=61.2601
	step [140/244], loss=70.5949
	step [141/244], loss=58.5786
	step [142/244], loss=84.9798
	step [143/244], loss=68.8375
	step [144/244], loss=84.0568
	step [145/244], loss=64.9245
	step [146/244], loss=79.4019
	step [147/244], loss=91.5321
	step [148/244], loss=77.1328
	step [149/244], loss=86.9618
	step [150/244], loss=66.1319
	step [151/244], loss=76.7297
	step [152/244], loss=78.1057
	step [153/244], loss=71.4375
	step [154/244], loss=81.9849
	step [155/244], loss=78.9252
	step [156/244], loss=72.3616
	step [157/244], loss=74.8595
	step [158/244], loss=83.4195
	step [159/244], loss=84.8971
	step [160/244], loss=68.8338
	step [161/244], loss=85.4193
	step [162/244], loss=71.1237
	step [163/244], loss=58.3547
	step [164/244], loss=91.9565
	step [165/244], loss=79.7357
	step [166/244], loss=71.5598
	step [167/244], loss=78.5120
	step [168/244], loss=77.8857
	step [169/244], loss=83.8549
	step [170/244], loss=86.9688
	step [171/244], loss=90.7850
	step [172/244], loss=66.0991
	step [173/244], loss=86.6080
	step [174/244], loss=90.4978
	step [175/244], loss=80.4638
	step [176/244], loss=90.9170
	step [177/244], loss=74.1990
	step [178/244], loss=71.0100
	step [179/244], loss=74.6522
	step [180/244], loss=71.2649
	step [181/244], loss=87.4316
	step [182/244], loss=59.8190
	step [183/244], loss=85.5046
	step [184/244], loss=69.3947
	step [185/244], loss=84.2738
	step [186/244], loss=91.1379
	step [187/244], loss=74.3536
	step [188/244], loss=82.4001
	step [189/244], loss=93.4975
	step [190/244], loss=71.8556
	step [191/244], loss=80.1608
	step [192/244], loss=71.5815
	step [193/244], loss=85.0889
	step [194/244], loss=66.2944
	step [195/244], loss=68.2533
	step [196/244], loss=59.8609
	step [197/244], loss=87.0075
	step [198/244], loss=81.3477
	step [199/244], loss=93.9490
	step [200/244], loss=75.9167
	step [201/244], loss=67.0287
	step [202/244], loss=77.7933
	step [203/244], loss=73.2709
	step [204/244], loss=68.2657
	step [205/244], loss=80.5531
	step [206/244], loss=83.3903
	step [207/244], loss=83.0600
	step [208/244], loss=70.9739
	step [209/244], loss=68.7428
	step [210/244], loss=76.5508
	step [211/244], loss=68.7589
	step [212/244], loss=70.5409
	step [213/244], loss=75.0380
	step [214/244], loss=73.0688
	step [215/244], loss=100.5057
	step [216/244], loss=78.9076
	step [217/244], loss=83.8061
	step [218/244], loss=77.2965
	step [219/244], loss=89.3090
	step [220/244], loss=61.0541
	step [221/244], loss=77.6943
	step [222/244], loss=71.1018
	step [223/244], loss=82.0360
	step [224/244], loss=64.3420
	step [225/244], loss=71.4823
	step [226/244], loss=75.3688
	step [227/244], loss=79.1174
	step [228/244], loss=68.6527
	step [229/244], loss=91.2910
	step [230/244], loss=99.1538
	step [231/244], loss=68.0550
	step [232/244], loss=81.6915
	step [233/244], loss=73.8793
	step [234/244], loss=78.5906
	step [235/244], loss=75.5273
	step [236/244], loss=61.6658
	step [237/244], loss=75.7683
	step [238/244], loss=82.5158
	step [239/244], loss=78.7524
	step [240/244], loss=72.9890
	step [241/244], loss=80.9310
	step [242/244], loss=68.6339
	step [243/244], loss=77.6895
	step [244/244], loss=60.5596
	Evaluating
	loss=0.0083, precision=0.3381, recall=0.8557, f1=0.4847
saving model as: 1_saved_model.pth
Training epoch 67
	step [1/244], loss=83.1559
	step [2/244], loss=54.8692
	step [3/244], loss=72.1628
	step [4/244], loss=71.8346
	step [5/244], loss=88.2686
	step [6/244], loss=83.3967
	step [7/244], loss=73.5701
	step [8/244], loss=76.5360
	step [9/244], loss=95.1841
	step [10/244], loss=85.0723
	step [11/244], loss=86.8589
	step [12/244], loss=97.7642
	step [13/244], loss=75.3362
	step [14/244], loss=86.4565
	step [15/244], loss=71.5515
	step [16/244], loss=68.3916
	step [17/244], loss=86.1599
	step [18/244], loss=64.0758
	step [19/244], loss=70.3820
	step [20/244], loss=81.7463
	step [21/244], loss=74.5541
	step [22/244], loss=69.1893
	step [23/244], loss=72.6112
	step [24/244], loss=58.3214
	step [25/244], loss=67.2057
	step [26/244], loss=77.6353
	step [27/244], loss=70.3284
	step [28/244], loss=82.7549
	step [29/244], loss=80.1480
	step [30/244], loss=86.3941
	step [31/244], loss=90.0227
	step [32/244], loss=68.4827
	step [33/244], loss=76.3902
	step [34/244], loss=81.7818
	step [35/244], loss=63.5792
	step [36/244], loss=85.7060
	step [37/244], loss=81.3139
	step [38/244], loss=89.3580
	step [39/244], loss=68.8027
	step [40/244], loss=66.3698
	step [41/244], loss=78.0631
	step [42/244], loss=85.1852
	step [43/244], loss=83.7124
	step [44/244], loss=62.6661
	step [45/244], loss=74.7671
	step [46/244], loss=67.3777
	step [47/244], loss=62.4293
	step [48/244], loss=74.1211
	step [49/244], loss=68.6860
	step [50/244], loss=100.9489
	step [51/244], loss=72.8396
	step [52/244], loss=57.6937
	step [53/244], loss=70.9844
	step [54/244], loss=87.8584
	step [55/244], loss=81.9426
	step [56/244], loss=76.2537
	step [57/244], loss=87.5234
	step [58/244], loss=77.2266
	step [59/244], loss=83.3147
	step [60/244], loss=75.1753
	step [61/244], loss=64.9901
	step [62/244], loss=90.7471
	step [63/244], loss=95.0333
	step [64/244], loss=64.8872
	step [65/244], loss=82.0198
	step [66/244], loss=97.1774
	step [67/244], loss=70.7611
	step [68/244], loss=56.8541
	step [69/244], loss=72.4609
	step [70/244], loss=67.9888
	step [71/244], loss=96.9439
	step [72/244], loss=91.8275
	step [73/244], loss=58.2685
	step [74/244], loss=91.8856
	step [75/244], loss=83.2646
	step [76/244], loss=84.0201
	step [77/244], loss=82.1278
	step [78/244], loss=68.4714
	step [79/244], loss=59.5423
	step [80/244], loss=79.1850
	step [81/244], loss=77.7507
	step [82/244], loss=80.5585
	step [83/244], loss=67.9268
	step [84/244], loss=71.6918
	step [85/244], loss=70.7087
	step [86/244], loss=80.9860
	step [87/244], loss=73.4358
	step [88/244], loss=72.3961
	step [89/244], loss=90.8027
	step [90/244], loss=76.3711
	step [91/244], loss=80.3750
	step [92/244], loss=77.1974
	step [93/244], loss=77.3047
	step [94/244], loss=78.3649
	step [95/244], loss=95.5075
	step [96/244], loss=83.5997
	step [97/244], loss=69.2868
	step [98/244], loss=98.2869
	step [99/244], loss=67.7042
	step [100/244], loss=74.0830
	step [101/244], loss=73.1437
	step [102/244], loss=79.4012
	step [103/244], loss=60.2232
	step [104/244], loss=82.2138
	step [105/244], loss=95.5657
	step [106/244], loss=78.2516
	step [107/244], loss=84.7325
	step [108/244], loss=86.6719
	step [109/244], loss=71.2498
	step [110/244], loss=78.1304
	step [111/244], loss=75.8989
	step [112/244], loss=72.6274
	step [113/244], loss=73.2399
	step [114/244], loss=74.7674
	step [115/244], loss=88.6953
	step [116/244], loss=69.0425
	step [117/244], loss=78.3431
	step [118/244], loss=84.9595
	step [119/244], loss=75.6835
	step [120/244], loss=78.3584
	step [121/244], loss=68.0745
	step [122/244], loss=75.1694
	step [123/244], loss=84.2681
	step [124/244], loss=71.3680
	step [125/244], loss=67.3165
	step [126/244], loss=78.9838
	step [127/244], loss=68.0786
	step [128/244], loss=74.2562
	step [129/244], loss=75.9637
	step [130/244], loss=87.6861
	step [131/244], loss=59.6594
	step [132/244], loss=83.8321
	step [133/244], loss=80.0980
	step [134/244], loss=73.8803
	step [135/244], loss=70.6452
	step [136/244], loss=92.7147
	step [137/244], loss=71.7783
	step [138/244], loss=81.9065
	step [139/244], loss=75.7853
	step [140/244], loss=70.3551
	step [141/244], loss=66.3448
	step [142/244], loss=65.0653
	step [143/244], loss=69.5041
	step [144/244], loss=69.5985
	step [145/244], loss=73.1261
	step [146/244], loss=71.7493
	step [147/244], loss=61.2574
	step [148/244], loss=83.6703
	step [149/244], loss=89.9265
	step [150/244], loss=93.3594
	step [151/244], loss=70.8741
	step [152/244], loss=73.9057
	step [153/244], loss=76.4965
	step [154/244], loss=68.0093
	step [155/244], loss=72.6059
	step [156/244], loss=79.3865
	step [157/244], loss=88.7698
	step [158/244], loss=87.1038
	step [159/244], loss=64.2013
	step [160/244], loss=86.2102
	step [161/244], loss=70.6184
	step [162/244], loss=79.2730
	step [163/244], loss=81.5648
	step [164/244], loss=86.2963
	step [165/244], loss=69.5311
	step [166/244], loss=84.5391
	step [167/244], loss=65.4686
	step [168/244], loss=74.5523
	step [169/244], loss=78.2408
	step [170/244], loss=81.2160
	step [171/244], loss=82.2288
	step [172/244], loss=86.2802
	step [173/244], loss=72.3359
	step [174/244], loss=80.2354
	step [175/244], loss=60.5513
	step [176/244], loss=75.1816
	step [177/244], loss=67.5947
	step [178/244], loss=65.3921
	step [179/244], loss=59.2777
	step [180/244], loss=89.4187
	step [181/244], loss=63.0794
	step [182/244], loss=68.3732
	step [183/244], loss=77.4709
	step [184/244], loss=81.9933
	step [185/244], loss=85.0046
	step [186/244], loss=78.9512
	step [187/244], loss=69.8974
	step [188/244], loss=68.7849
	step [189/244], loss=62.0773
	step [190/244], loss=100.8277
	step [191/244], loss=83.1503
	step [192/244], loss=79.7920
	step [193/244], loss=71.9211
	step [194/244], loss=71.4549
	step [195/244], loss=82.9618
	step [196/244], loss=76.9818
	step [197/244], loss=74.6924
	step [198/244], loss=68.4813
	step [199/244], loss=80.7743
	step [200/244], loss=62.2411
	step [201/244], loss=88.7672
	step [202/244], loss=74.6150
	step [203/244], loss=65.0144
	step [204/244], loss=78.7758
	step [205/244], loss=67.8207
	step [206/244], loss=66.6964
	step [207/244], loss=68.8860
	step [208/244], loss=64.6365
	step [209/244], loss=77.6369
	step [210/244], loss=78.0249
	step [211/244], loss=79.9492
	step [212/244], loss=77.2257
	step [213/244], loss=74.1509
	step [214/244], loss=76.9928
	step [215/244], loss=86.6536
	step [216/244], loss=73.7883
	step [217/244], loss=84.1812
	step [218/244], loss=73.0985
	step [219/244], loss=69.7128
	step [220/244], loss=75.0503
	step [221/244], loss=77.9236
	step [222/244], loss=80.0516
	step [223/244], loss=86.8587
	step [224/244], loss=79.1334
	step [225/244], loss=80.9203
	step [226/244], loss=71.9149
	step [227/244], loss=63.8802
	step [228/244], loss=72.9993
	step [229/244], loss=61.7984
	step [230/244], loss=84.9806
	step [231/244], loss=76.5471
	step [232/244], loss=77.9375
	step [233/244], loss=68.9389
	step [234/244], loss=87.4128
	step [235/244], loss=75.6079
	step [236/244], loss=93.9160
	step [237/244], loss=75.5898
	step [238/244], loss=83.4143
	step [239/244], loss=77.7631
	step [240/244], loss=63.7217
	step [241/244], loss=76.5091
	step [242/244], loss=91.4672
	step [243/244], loss=85.7964
	step [244/244], loss=74.3512
	Evaluating
	loss=0.0096, precision=0.3040, recall=0.8661, f1=0.4501
Training epoch 68
	step [1/244], loss=76.5682
	step [2/244], loss=83.8582
	step [3/244], loss=86.8163
	step [4/244], loss=86.9880
	step [5/244], loss=77.8760
	step [6/244], loss=78.9512
	step [7/244], loss=83.1490
	step [8/244], loss=87.7748
	step [9/244], loss=82.5623
	step [10/244], loss=83.3621
	step [11/244], loss=85.4268
	step [12/244], loss=82.7131
	step [13/244], loss=79.0033
	step [14/244], loss=76.8994
	step [15/244], loss=77.2057
	step [16/244], loss=77.3244
	step [17/244], loss=86.6934
	step [18/244], loss=82.3996
	step [19/244], loss=61.3930
	step [20/244], loss=74.8378
	step [21/244], loss=90.9980
	step [22/244], loss=80.7483
	step [23/244], loss=73.0115
	step [24/244], loss=67.4621
	step [25/244], loss=70.3532
	step [26/244], loss=85.7482
	step [27/244], loss=64.8847
	step [28/244], loss=75.3599
	step [29/244], loss=67.8626
	step [30/244], loss=85.8087
	step [31/244], loss=77.7502
	step [32/244], loss=61.3812
	step [33/244], loss=73.5582
	step [34/244], loss=76.2059
	step [35/244], loss=77.9898
	step [36/244], loss=70.3279
	step [37/244], loss=76.1793
	step [38/244], loss=83.4118
	step [39/244], loss=88.1065
	step [40/244], loss=76.7283
	step [41/244], loss=71.8179
	step [42/244], loss=84.2476
	step [43/244], loss=81.2151
	step [44/244], loss=79.6315
	step [45/244], loss=59.2254
	step [46/244], loss=77.5284
	step [47/244], loss=86.1852
	step [48/244], loss=67.5503
	step [49/244], loss=82.6835
	step [50/244], loss=81.6553
	step [51/244], loss=59.1124
	step [52/244], loss=65.4626
	step [53/244], loss=70.3914
	step [54/244], loss=64.7897
	step [55/244], loss=64.2058
	step [56/244], loss=72.5620
	step [57/244], loss=69.8618
	step [58/244], loss=79.5388
	step [59/244], loss=67.5838
	step [60/244], loss=77.9292
	step [61/244], loss=82.0360
	step [62/244], loss=71.7671
	step [63/244], loss=91.8514
	step [64/244], loss=67.1778
	step [65/244], loss=75.4942
	step [66/244], loss=65.2911
	step [67/244], loss=65.7444
	step [68/244], loss=69.0497
	step [69/244], loss=79.5870
	step [70/244], loss=89.6706
	step [71/244], loss=61.7037
	step [72/244], loss=70.3915
	step [73/244], loss=64.3387
	step [74/244], loss=63.4299
	step [75/244], loss=88.6620
	step [76/244], loss=76.8768
	step [77/244], loss=61.3340
	step [78/244], loss=63.9400
	step [79/244], loss=84.0724
	step [80/244], loss=77.5887
	step [81/244], loss=63.9290
	step [82/244], loss=81.7720
	step [83/244], loss=67.7969
	step [84/244], loss=83.6901
	step [85/244], loss=84.9235
	step [86/244], loss=87.2760
	step [87/244], loss=65.2410
	step [88/244], loss=87.5555
	step [89/244], loss=77.4519
	step [90/244], loss=78.8650
	step [91/244], loss=83.0680
	step [92/244], loss=77.5596
	step [93/244], loss=70.7327
	step [94/244], loss=87.4787
	step [95/244], loss=86.5750
	step [96/244], loss=69.1011
	step [97/244], loss=88.3675
	step [98/244], loss=86.7747
	step [99/244], loss=71.3135
	step [100/244], loss=86.1168
	step [101/244], loss=82.6342
	step [102/244], loss=69.1794
	step [103/244], loss=67.9495
	step [104/244], loss=76.2022
	step [105/244], loss=56.6391
	step [106/244], loss=77.7701
	step [107/244], loss=74.9292
	step [108/244], loss=90.2473
	step [109/244], loss=83.1705
	step [110/244], loss=78.2573
	step [111/244], loss=81.4052
	step [112/244], loss=85.4825
	step [113/244], loss=70.5554
	step [114/244], loss=78.9767
	step [115/244], loss=74.5887
	step [116/244], loss=71.7945
	step [117/244], loss=74.9286
	step [118/244], loss=74.2334
	step [119/244], loss=78.0025
	step [120/244], loss=75.4506
	step [121/244], loss=85.3461
	step [122/244], loss=75.3489
	step [123/244], loss=85.3265
	step [124/244], loss=71.4279
	step [125/244], loss=73.7965
	step [126/244], loss=69.7084
	step [127/244], loss=69.6602
	step [128/244], loss=93.4033
	step [129/244], loss=76.5214
	step [130/244], loss=76.0080
	step [131/244], loss=84.2689
	step [132/244], loss=89.8098
	step [133/244], loss=64.2845
	step [134/244], loss=96.5327
	step [135/244], loss=75.7808
	step [136/244], loss=81.0755
	step [137/244], loss=74.2284
	step [138/244], loss=83.0150
	step [139/244], loss=94.5980
	step [140/244], loss=88.9372
	step [141/244], loss=96.9963
	step [142/244], loss=69.4271
	step [143/244], loss=74.0271
	step [144/244], loss=68.4523
	step [145/244], loss=75.7942
	step [146/244], loss=74.5991
	step [147/244], loss=74.3914
	step [148/244], loss=68.5520
	step [149/244], loss=76.6287
	step [150/244], loss=82.6219
	step [151/244], loss=77.0051
	step [152/244], loss=73.5278
	step [153/244], loss=95.6133
	step [154/244], loss=93.1974
	step [155/244], loss=70.3548
	step [156/244], loss=81.3029
	step [157/244], loss=85.6513
	step [158/244], loss=74.5199
	step [159/244], loss=67.8933
	step [160/244], loss=64.6924
	step [161/244], loss=69.4626
	step [162/244], loss=62.0392
	step [163/244], loss=73.1351
	step [164/244], loss=85.5365
	step [165/244], loss=77.0433
	step [166/244], loss=72.5560
	step [167/244], loss=68.5478
	step [168/244], loss=75.6323
	step [169/244], loss=80.3401
	step [170/244], loss=76.2618
	step [171/244], loss=67.3723
	step [172/244], loss=80.3402
	step [173/244], loss=76.3246
	step [174/244], loss=78.0866
	step [175/244], loss=76.8379
	step [176/244], loss=78.2395
	step [177/244], loss=83.3632
	step [178/244], loss=71.0343
	step [179/244], loss=77.5347
	step [180/244], loss=72.0493
	step [181/244], loss=75.2688
	step [182/244], loss=78.7785
	step [183/244], loss=69.1278
	step [184/244], loss=72.8061
	step [185/244], loss=56.8288
	step [186/244], loss=65.3417
	step [187/244], loss=76.4467
	step [188/244], loss=87.6192
	step [189/244], loss=78.8808
	step [190/244], loss=61.8738
	step [191/244], loss=77.3324
	step [192/244], loss=68.4282
	step [193/244], loss=85.8618
	step [194/244], loss=68.5507
	step [195/244], loss=67.3090
	step [196/244], loss=81.6746
	step [197/244], loss=91.5790
	step [198/244], loss=69.8325
	step [199/244], loss=87.9157
	step [200/244], loss=66.9078
	step [201/244], loss=76.5355
	step [202/244], loss=77.1019
	step [203/244], loss=74.9273
	step [204/244], loss=62.5398
	step [205/244], loss=80.4576
	step [206/244], loss=77.4668
	step [207/244], loss=72.5067
	step [208/244], loss=80.4739
	step [209/244], loss=69.1112
	step [210/244], loss=63.6443
	step [211/244], loss=69.9888
	step [212/244], loss=75.6088
	step [213/244], loss=75.8252
	step [214/244], loss=87.9232
	step [215/244], loss=68.7845
	step [216/244], loss=82.2128
	step [217/244], loss=99.7992
	step [218/244], loss=66.8679
	step [219/244], loss=66.9949
	step [220/244], loss=98.3928
	step [221/244], loss=79.5669
	step [222/244], loss=79.5395
	step [223/244], loss=74.1326
	step [224/244], loss=68.7813
	step [225/244], loss=82.2616
	step [226/244], loss=80.1962
	step [227/244], loss=74.7078
	step [228/244], loss=68.6867
	step [229/244], loss=90.9628
	step [230/244], loss=74.7621
	step [231/244], loss=82.0282
	step [232/244], loss=58.0482
	step [233/244], loss=71.8191
	step [234/244], loss=69.6983
	step [235/244], loss=58.3858
	step [236/244], loss=76.7111
	step [237/244], loss=79.8752
	step [238/244], loss=78.6493
	step [239/244], loss=82.1541
	step [240/244], loss=82.0140
	step [241/244], loss=69.4336
	step [242/244], loss=76.4011
	step [243/244], loss=86.4542
	step [244/244], loss=67.8372
	Evaluating
	loss=0.0081, precision=0.3484, recall=0.8537, f1=0.4949
saving model as: 1_saved_model.pth
Training epoch 69
	step [1/244], loss=80.1033
	step [2/244], loss=76.0820
	step [3/244], loss=83.9795
	step [4/244], loss=82.1857
	step [5/244], loss=83.7275
	step [6/244], loss=71.1229
	step [7/244], loss=76.5251
	step [8/244], loss=65.1877
	step [9/244], loss=85.7440
	step [10/244], loss=87.7808
	step [11/244], loss=63.7634
	step [12/244], loss=68.6688
	step [13/244], loss=80.8631
	step [14/244], loss=76.5709
	step [15/244], loss=78.7806
	step [16/244], loss=73.9658
	step [17/244], loss=88.7112
	step [18/244], loss=87.6087
	step [19/244], loss=70.5717
	step [20/244], loss=64.6511
	step [21/244], loss=89.8140
	step [22/244], loss=73.1953
	step [23/244], loss=75.3581
	step [24/244], loss=71.5293
	step [25/244], loss=76.1181
	step [26/244], loss=84.0593
	step [27/244], loss=74.1006
	step [28/244], loss=65.9372
	step [29/244], loss=82.2730
	step [30/244], loss=79.5040
	step [31/244], loss=71.5012
	step [32/244], loss=78.8078
	step [33/244], loss=79.8996
	step [34/244], loss=69.8006
	step [35/244], loss=78.3782
	step [36/244], loss=83.7280
	step [37/244], loss=72.8090
	step [38/244], loss=67.2993
	step [39/244], loss=74.7620
	step [40/244], loss=56.6737
	step [41/244], loss=73.2525
	step [42/244], loss=91.0889
	step [43/244], loss=81.0867
	step [44/244], loss=84.0534
	step [45/244], loss=83.0302
	step [46/244], loss=78.6886
	step [47/244], loss=62.0628
	step [48/244], loss=71.9400
	step [49/244], loss=75.7666
	step [50/244], loss=93.6535
	step [51/244], loss=80.6969
	step [52/244], loss=66.3892
	step [53/244], loss=85.2700
	step [54/244], loss=84.8839
	step [55/244], loss=84.2334
	step [56/244], loss=69.8986
	step [57/244], loss=64.2042
	step [58/244], loss=82.7039
	step [59/244], loss=86.6648
	step [60/244], loss=73.7570
	step [61/244], loss=64.0331
	step [62/244], loss=79.3413
	step [63/244], loss=72.5087
	step [64/244], loss=82.6194
	step [65/244], loss=55.1422
	step [66/244], loss=73.5355
	step [67/244], loss=80.6457
	step [68/244], loss=73.9039
	step [69/244], loss=78.3852
	step [70/244], loss=76.1617
	step [71/244], loss=67.2170
	step [72/244], loss=75.4974
	step [73/244], loss=77.3116
	step [74/244], loss=83.4189
	step [75/244], loss=82.8053
	step [76/244], loss=70.7417
	step [77/244], loss=71.9089
	step [78/244], loss=81.1279
	step [79/244], loss=77.8567
	step [80/244], loss=58.8125
	step [81/244], loss=63.6177
	step [82/244], loss=84.9984
	step [83/244], loss=74.6514
	step [84/244], loss=71.2013
	step [85/244], loss=85.1671
	step [86/244], loss=78.5536
	step [87/244], loss=68.7393
	step [88/244], loss=72.9345
	step [89/244], loss=93.7825
	step [90/244], loss=82.5423
	step [91/244], loss=69.7376
	step [92/244], loss=67.7308
	step [93/244], loss=68.8456
	step [94/244], loss=74.7406
	step [95/244], loss=77.0740
	step [96/244], loss=71.3711
	step [97/244], loss=69.4760
	step [98/244], loss=77.1879
	step [99/244], loss=74.2088
	step [100/244], loss=83.0766
	step [101/244], loss=93.9941
	step [102/244], loss=72.0488
	step [103/244], loss=69.4928
	step [104/244], loss=72.8293
	step [105/244], loss=69.7282
	step [106/244], loss=87.9201
	step [107/244], loss=98.8251
	step [108/244], loss=63.5228
	step [109/244], loss=64.9204
	step [110/244], loss=77.0388
	step [111/244], loss=59.1320
	step [112/244], loss=65.0628
	step [113/244], loss=85.0883
	step [114/244], loss=75.3867
	step [115/244], loss=77.5153
	step [116/244], loss=63.2430
	step [117/244], loss=86.4037
	step [118/244], loss=78.3414
	step [119/244], loss=60.4188
	step [120/244], loss=78.3754
	step [121/244], loss=79.0122
	step [122/244], loss=78.0378
	step [123/244], loss=69.9342
	step [124/244], loss=58.9304
	step [125/244], loss=74.6245
	step [126/244], loss=70.5512
	step [127/244], loss=88.6337
	step [128/244], loss=62.9219
	step [129/244], loss=69.9146
	step [130/244], loss=76.0217
	step [131/244], loss=68.8295
	step [132/244], loss=73.7899
	step [133/244], loss=64.8993
	step [134/244], loss=91.5883
	step [135/244], loss=74.4940
	step [136/244], loss=85.9354
	step [137/244], loss=75.2846
	step [138/244], loss=83.6339
	step [139/244], loss=68.8450
	step [140/244], loss=63.3863
	step [141/244], loss=71.4001
	step [142/244], loss=76.9537
	step [143/244], loss=69.9347
	step [144/244], loss=75.8143
	step [145/244], loss=72.1664
	step [146/244], loss=71.7143
	step [147/244], loss=80.0047
	step [148/244], loss=81.4632
	step [149/244], loss=71.0564
	step [150/244], loss=81.7007
	step [151/244], loss=68.7098
	step [152/244], loss=83.5843
	step [153/244], loss=80.0251
	step [154/244], loss=73.1129
	step [155/244], loss=85.2691
	step [156/244], loss=76.6296
	step [157/244], loss=71.8777
	step [158/244], loss=86.3290
	step [159/244], loss=80.5568
	step [160/244], loss=86.8106
	step [161/244], loss=67.7899
	step [162/244], loss=62.4124
	step [163/244], loss=73.2682
	step [164/244], loss=68.8882
	step [165/244], loss=77.6905
	step [166/244], loss=72.5892
	step [167/244], loss=79.5240
	step [168/244], loss=79.3086
	step [169/244], loss=71.8412
	step [170/244], loss=61.1200
	step [171/244], loss=65.1881
	step [172/244], loss=72.9665
	step [173/244], loss=66.2606
	step [174/244], loss=76.6228
	step [175/244], loss=69.1562
	step [176/244], loss=57.2920
	step [177/244], loss=88.0729
	step [178/244], loss=63.1705
	step [179/244], loss=64.3171
	step [180/244], loss=78.3175
	step [181/244], loss=88.5163
	step [182/244], loss=75.7077
	step [183/244], loss=83.7279
	step [184/244], loss=82.9911
	step [185/244], loss=85.2085
	step [186/244], loss=86.0819
	step [187/244], loss=83.9737
	step [188/244], loss=66.8264
	step [189/244], loss=96.2504
	step [190/244], loss=94.0194
	step [191/244], loss=83.2616
	step [192/244], loss=89.3794
	step [193/244], loss=78.6814
	step [194/244], loss=63.8187
	step [195/244], loss=68.7478
	step [196/244], loss=78.0909
	step [197/244], loss=87.7461
	step [198/244], loss=77.3720
	step [199/244], loss=77.8458
	step [200/244], loss=69.5353
	step [201/244], loss=54.7555
	step [202/244], loss=84.2304
	step [203/244], loss=82.7321
	step [204/244], loss=67.6550
	step [205/244], loss=70.0664
	step [206/244], loss=77.3075
	step [207/244], loss=96.8266
	step [208/244], loss=71.9919
	step [209/244], loss=79.2283
	step [210/244], loss=63.6087
	step [211/244], loss=85.6470
	step [212/244], loss=92.1632
	step [213/244], loss=88.6812
	step [214/244], loss=68.3990
	step [215/244], loss=86.2014
	step [216/244], loss=71.4276
	step [217/244], loss=85.9588
	step [218/244], loss=92.7936
	step [219/244], loss=83.7456
	step [220/244], loss=67.7477
	step [221/244], loss=66.2013
	step [222/244], loss=71.3351
	step [223/244], loss=73.3024
	step [224/244], loss=69.0776
	step [225/244], loss=78.3101
	step [226/244], loss=89.0277
	step [227/244], loss=79.5260
	step [228/244], loss=91.8550
	step [229/244], loss=85.3155
	step [230/244], loss=96.8392
	step [231/244], loss=79.8164
	step [232/244], loss=65.6610
	step [233/244], loss=74.8550
	step [234/244], loss=80.5774
	step [235/244], loss=78.9116
	step [236/244], loss=83.6571
	step [237/244], loss=85.6309
	step [238/244], loss=84.7147
	step [239/244], loss=70.3652
	step [240/244], loss=61.6107
	step [241/244], loss=74.5699
	step [242/244], loss=77.8599
	step [243/244], loss=64.6549
	step [244/244], loss=63.7066
	Evaluating
	loss=0.0087, precision=0.3325, recall=0.8675, f1=0.4807
Training epoch 70
	step [1/244], loss=87.0506
	step [2/244], loss=86.2833
	step [3/244], loss=79.8791
	step [4/244], loss=95.3916
	step [5/244], loss=95.5006
	step [6/244], loss=78.3299
	step [7/244], loss=93.2926
	step [8/244], loss=74.4648
	step [9/244], loss=86.2154
	step [10/244], loss=91.8216
	step [11/244], loss=87.4144
	step [12/244], loss=66.9581
	step [13/244], loss=75.6878
	step [14/244], loss=66.9875
	step [15/244], loss=69.3718
	step [16/244], loss=79.8516
	step [17/244], loss=79.3906
	step [18/244], loss=78.8323
	step [19/244], loss=81.7882
	step [20/244], loss=69.3399
	step [21/244], loss=66.1506
	step [22/244], loss=79.3630
	step [23/244], loss=55.8746
	step [24/244], loss=70.1973
	step [25/244], loss=78.4985
	step [26/244], loss=88.5213
	step [27/244], loss=64.5485
	step [28/244], loss=60.0331
	step [29/244], loss=77.2857
	step [30/244], loss=74.5590
	step [31/244], loss=82.2444
	step [32/244], loss=65.4884
	step [33/244], loss=81.0555
	step [34/244], loss=68.0838
	step [35/244], loss=57.0474
	step [36/244], loss=80.0695
	step [37/244], loss=71.8411
	step [38/244], loss=78.7959
	step [39/244], loss=85.9947
	step [40/244], loss=82.3721
	step [41/244], loss=61.2530
	step [42/244], loss=82.1117
	step [43/244], loss=84.4713
	step [44/244], loss=62.1832
	step [45/244], loss=72.6738
	step [46/244], loss=81.9690
	step [47/244], loss=70.1223
	step [48/244], loss=83.1259
	step [49/244], loss=69.2241
	step [50/244], loss=84.2133
	step [51/244], loss=75.1532
	step [52/244], loss=76.2099
	step [53/244], loss=66.3723
	step [54/244], loss=71.3930
	step [55/244], loss=64.2096
	step [56/244], loss=98.4464
	step [57/244], loss=73.2675
	step [58/244], loss=73.0320
	step [59/244], loss=85.4890
	step [60/244], loss=78.3649
	step [61/244], loss=67.1285
	step [62/244], loss=65.8683
	step [63/244], loss=94.1151
	step [64/244], loss=79.8413
	step [65/244], loss=80.5953
	step [66/244], loss=83.8773
	step [67/244], loss=86.4586
	step [68/244], loss=82.6949
	step [69/244], loss=88.6056
	step [70/244], loss=85.1125
	step [71/244], loss=78.0196
	step [72/244], loss=81.3987
	step [73/244], loss=73.0138
	step [74/244], loss=84.2262
	step [75/244], loss=64.2880
	step [76/244], loss=86.0141
	step [77/244], loss=57.7815
	step [78/244], loss=72.3306
	step [79/244], loss=76.1480
	step [80/244], loss=83.8712
	step [81/244], loss=68.3431
	step [82/244], loss=74.0171
	step [83/244], loss=76.9515
	step [84/244], loss=70.1684
	step [85/244], loss=72.9572
	step [86/244], loss=80.0582
	step [87/244], loss=75.1302
	step [88/244], loss=74.1350
	step [89/244], loss=74.0220
	step [90/244], loss=77.7229
	step [91/244], loss=78.2117
	step [92/244], loss=70.0786
	step [93/244], loss=66.9389
	step [94/244], loss=70.5065
	step [95/244], loss=56.7665
	step [96/244], loss=72.3117
	step [97/244], loss=77.0348
	step [98/244], loss=81.7410
	step [99/244], loss=78.7432
	step [100/244], loss=87.7995
	step [101/244], loss=85.5504
	step [102/244], loss=85.7367
	step [103/244], loss=80.8910
	step [104/244], loss=79.9712
	step [105/244], loss=80.6447
	step [106/244], loss=75.1489
	step [107/244], loss=72.2955
	step [108/244], loss=72.1405
	step [109/244], loss=92.1368
	step [110/244], loss=83.5733
	step [111/244], loss=76.6080
	step [112/244], loss=73.4775
	step [113/244], loss=66.7559
	step [114/244], loss=75.1384
	step [115/244], loss=76.4298
	step [116/244], loss=69.3995
	step [117/244], loss=74.8935
	step [118/244], loss=77.6322
	step [119/244], loss=75.4051
	step [120/244], loss=63.7389
	step [121/244], loss=85.9742
	step [122/244], loss=76.4479
	step [123/244], loss=72.5337
	step [124/244], loss=72.4625
	step [125/244], loss=81.5220
	step [126/244], loss=77.0908
	step [127/244], loss=74.4371
	step [128/244], loss=72.5827
	step [129/244], loss=66.9121
	step [130/244], loss=69.2472
	step [131/244], loss=91.7911
	step [132/244], loss=73.2702
	step [133/244], loss=78.0842
	step [134/244], loss=65.0899
	step [135/244], loss=76.7677
	step [136/244], loss=77.7223
	step [137/244], loss=64.7160
	step [138/244], loss=74.7215
	step [139/244], loss=66.0529
	step [140/244], loss=74.9746
	step [141/244], loss=73.1580
	step [142/244], loss=67.1190
	step [143/244], loss=77.0735
	step [144/244], loss=83.1425
	step [145/244], loss=77.0006
	step [146/244], loss=70.8809
	step [147/244], loss=99.6347
	step [148/244], loss=84.4606
	step [149/244], loss=63.6365
	step [150/244], loss=92.6015
	step [151/244], loss=69.0217
	step [152/244], loss=68.5392
	step [153/244], loss=59.2049
	step [154/244], loss=67.9393
	step [155/244], loss=79.8746
	step [156/244], loss=65.0341
	step [157/244], loss=81.6940
	step [158/244], loss=83.0954
	step [159/244], loss=78.2290
	step [160/244], loss=77.3318
	step [161/244], loss=55.9062
	step [162/244], loss=67.2397
	step [163/244], loss=72.3328
	step [164/244], loss=72.0415
	step [165/244], loss=86.4305
	step [166/244], loss=73.5039
	step [167/244], loss=72.6494
	step [168/244], loss=70.6671
	step [169/244], loss=77.9468
	step [170/244], loss=78.7028
	step [171/244], loss=84.7935
	step [172/244], loss=72.8360
	step [173/244], loss=62.5089
	step [174/244], loss=84.1621
	step [175/244], loss=80.4723
	step [176/244], loss=69.0675
	step [177/244], loss=79.2762
	step [178/244], loss=77.5137
	step [179/244], loss=88.7374
	step [180/244], loss=65.4759
	step [181/244], loss=79.2119
	step [182/244], loss=78.6886
	step [183/244], loss=74.1609
	step [184/244], loss=70.9357
	step [185/244], loss=82.5083
	step [186/244], loss=69.5965
	step [187/244], loss=71.4431
	step [188/244], loss=83.9564
	step [189/244], loss=71.0080
	step [190/244], loss=73.4074
	step [191/244], loss=77.3498
	step [192/244], loss=72.1117
	step [193/244], loss=60.5086
	step [194/244], loss=86.4688
	step [195/244], loss=82.2656
	step [196/244], loss=85.5570
	step [197/244], loss=56.4891
	step [198/244], loss=70.7257
	step [199/244], loss=68.8806
	step [200/244], loss=70.9672
	step [201/244], loss=76.3408
	step [202/244], loss=94.9749
	step [203/244], loss=65.0107
	step [204/244], loss=67.7942
	step [205/244], loss=66.1365
	step [206/244], loss=80.4470
	step [207/244], loss=97.7924
	step [208/244], loss=67.1239
	step [209/244], loss=75.1119
	step [210/244], loss=86.4607
	step [211/244], loss=89.1846
	step [212/244], loss=83.8757
	step [213/244], loss=77.9643
	step [214/244], loss=82.8685
	step [215/244], loss=80.3761
	step [216/244], loss=88.9511
	step [217/244], loss=83.0520
	step [218/244], loss=77.4212
	step [219/244], loss=84.7523
	step [220/244], loss=78.2000
	step [221/244], loss=71.4845
	step [222/244], loss=77.2722
	step [223/244], loss=75.3909
	step [224/244], loss=71.2175
	step [225/244], loss=67.4951
	step [226/244], loss=69.0762
	step [227/244], loss=69.0261
	step [228/244], loss=76.2891
	step [229/244], loss=78.3144
	step [230/244], loss=66.4658
	step [231/244], loss=84.5639
	step [232/244], loss=73.1812
	step [233/244], loss=77.1825
	step [234/244], loss=77.3558
	step [235/244], loss=72.0256
	step [236/244], loss=61.8304
	step [237/244], loss=63.4457
	step [238/244], loss=81.9081
	step [239/244], loss=66.0018
	step [240/244], loss=72.0109
	step [241/244], loss=91.3849
	step [242/244], loss=65.7271
	step [243/244], loss=74.2039
	step [244/244], loss=82.7623
	Evaluating
	loss=0.0090, precision=0.3160, recall=0.8570, f1=0.4617
Training epoch 71
	step [1/244], loss=74.9098
	step [2/244], loss=60.7068
	step [3/244], loss=78.9495
	step [4/244], loss=62.6444
	step [5/244], loss=77.1088
	step [6/244], loss=75.4854
	step [7/244], loss=99.7384
	step [8/244], loss=92.2691
	step [9/244], loss=74.6667
	step [10/244], loss=69.1992
	step [11/244], loss=88.9111
	step [12/244], loss=82.6918
	step [13/244], loss=59.7978
	step [14/244], loss=54.3704
	step [15/244], loss=90.2422
	step [16/244], loss=81.8697
	step [17/244], loss=49.5700
	step [18/244], loss=68.5098
	step [19/244], loss=73.9295
	step [20/244], loss=65.9326
	step [21/244], loss=71.2145
	step [22/244], loss=81.9576
	step [23/244], loss=75.9437
	step [24/244], loss=86.5936
	step [25/244], loss=83.9842
	step [26/244], loss=72.5636
	step [27/244], loss=87.9912
	step [28/244], loss=77.8244
	step [29/244], loss=75.5948
	step [30/244], loss=89.1563
	step [31/244], loss=73.8007
	step [32/244], loss=80.3488
	step [33/244], loss=62.9558
	step [34/244], loss=72.8194
	step [35/244], loss=69.0924
	step [36/244], loss=75.3720
	step [37/244], loss=64.6637
	step [38/244], loss=78.7081
	step [39/244], loss=71.2795
	step [40/244], loss=72.4293
	step [41/244], loss=81.1251
	step [42/244], loss=56.3594
	step [43/244], loss=76.1088
	step [44/244], loss=68.4043
	step [45/244], loss=85.3995
	step [46/244], loss=69.6921
	step [47/244], loss=85.0521
	step [48/244], loss=69.8569
	step [49/244], loss=90.8594
	step [50/244], loss=82.4681
	step [51/244], loss=76.2236
	step [52/244], loss=94.5428
	step [53/244], loss=79.2072
	step [54/244], loss=57.3038
	step [55/244], loss=94.9361
	step [56/244], loss=61.5971
	step [57/244], loss=86.8079
	step [58/244], loss=80.7210
	step [59/244], loss=67.4889
	step [60/244], loss=67.9304
	step [61/244], loss=89.3008
	step [62/244], loss=79.9213
	step [63/244], loss=63.7972
	step [64/244], loss=77.8385
	step [65/244], loss=71.0113
	step [66/244], loss=67.1338
	step [67/244], loss=72.4726
	step [68/244], loss=83.9667
	step [69/244], loss=79.7913
	step [70/244], loss=75.1789
	step [71/244], loss=80.1039
	step [72/244], loss=81.6446
	step [73/244], loss=60.0130
	step [74/244], loss=57.8554
	step [75/244], loss=95.1993
	step [76/244], loss=65.5413
	step [77/244], loss=80.0567
	step [78/244], loss=74.8176
	step [79/244], loss=60.6418
	step [80/244], loss=71.1495
	step [81/244], loss=69.8663
	step [82/244], loss=76.2150
	step [83/244], loss=69.0997
	step [84/244], loss=82.0088
	step [85/244], loss=77.2908
	step [86/244], loss=82.8010
	step [87/244], loss=71.1770
	step [88/244], loss=73.9692
	step [89/244], loss=93.3624
	step [90/244], loss=73.7753
	step [91/244], loss=76.2038
	step [92/244], loss=78.4630
	step [93/244], loss=77.9557
	step [94/244], loss=74.5683
	step [95/244], loss=78.9264
	step [96/244], loss=69.1471
	step [97/244], loss=68.3948
	step [98/244], loss=85.4480
	step [99/244], loss=61.8489
	step [100/244], loss=72.8615
	step [101/244], loss=75.4074
	step [102/244], loss=94.5387
	step [103/244], loss=78.4796
	step [104/244], loss=65.3183
	step [105/244], loss=73.8194
	step [106/244], loss=82.1456
	step [107/244], loss=75.4486
	step [108/244], loss=66.3908
	step [109/244], loss=68.0187
	step [110/244], loss=86.4741
	step [111/244], loss=76.9111
	step [112/244], loss=94.5751
	step [113/244], loss=64.4473
	step [114/244], loss=69.5396
	step [115/244], loss=77.1321
	step [116/244], loss=68.9170
	step [117/244], loss=77.3828
	step [118/244], loss=83.5158
	step [119/244], loss=68.9296
	step [120/244], loss=73.2921
	step [121/244], loss=84.0362
	step [122/244], loss=73.4627
	step [123/244], loss=92.4238
	step [124/244], loss=87.9505
	step [125/244], loss=61.7736
	step [126/244], loss=75.0692
	step [127/244], loss=87.9535
	step [128/244], loss=73.8037
	step [129/244], loss=72.5088
	step [130/244], loss=75.2262
	step [131/244], loss=74.9002
	step [132/244], loss=75.9629
	step [133/244], loss=77.9939
	step [134/244], loss=70.1108
	step [135/244], loss=78.2552
	step [136/244], loss=78.8934
	step [137/244], loss=82.5858
	step [138/244], loss=69.4226
	step [139/244], loss=64.0751
	step [140/244], loss=79.3580
	step [141/244], loss=93.8329
	step [142/244], loss=65.7480
	step [143/244], loss=104.7483
	step [144/244], loss=90.8058
	step [145/244], loss=70.2267
	step [146/244], loss=58.2853
	step [147/244], loss=70.1469
	step [148/244], loss=65.6465
	step [149/244], loss=80.9644
	step [150/244], loss=76.6138
	step [151/244], loss=74.1824
	step [152/244], loss=67.9269
	step [153/244], loss=59.5951
	step [154/244], loss=81.1979
	step [155/244], loss=78.1931
	step [156/244], loss=82.5816
	step [157/244], loss=77.7411
	step [158/244], loss=65.9900
	step [159/244], loss=82.7715
	step [160/244], loss=79.6858
	step [161/244], loss=77.0136
	step [162/244], loss=74.9389
	step [163/244], loss=70.8091
	step [164/244], loss=79.3179
	step [165/244], loss=75.1230
	step [166/244], loss=77.7291
	step [167/244], loss=71.9021
	step [168/244], loss=75.9329
	step [169/244], loss=73.3591
	step [170/244], loss=73.4548
	step [171/244], loss=75.2265
	step [172/244], loss=80.4372
	step [173/244], loss=67.5947
	step [174/244], loss=74.3583
	step [175/244], loss=88.0867
	step [176/244], loss=62.4948
	step [177/244], loss=66.5006
	step [178/244], loss=73.3577
	step [179/244], loss=70.8560
	step [180/244], loss=94.4983
	step [181/244], loss=69.4400
	step [182/244], loss=86.1299
	step [183/244], loss=81.2571
	step [184/244], loss=86.6879
	step [185/244], loss=87.2111
	step [186/244], loss=79.5599
	step [187/244], loss=71.6090
	step [188/244], loss=56.7473
	step [189/244], loss=69.0843
	step [190/244], loss=91.3917
	step [191/244], loss=70.1679
	step [192/244], loss=70.8751
	step [193/244], loss=98.0363
	step [194/244], loss=77.3957
	step [195/244], loss=80.0054
	step [196/244], loss=74.3679
	step [197/244], loss=80.3472
	step [198/244], loss=88.3813
	step [199/244], loss=68.1363
	step [200/244], loss=80.5926
	step [201/244], loss=75.4252
	step [202/244], loss=69.2489
	step [203/244], loss=89.5345
	step [204/244], loss=77.0743
	step [205/244], loss=53.6697
	step [206/244], loss=80.0077
	step [207/244], loss=70.4737
	step [208/244], loss=68.2767
	step [209/244], loss=64.6885
	step [210/244], loss=70.7157
	step [211/244], loss=70.4450
	step [212/244], loss=61.1523
	step [213/244], loss=58.0058
	step [214/244], loss=96.4106
	step [215/244], loss=92.4288
	step [216/244], loss=68.2191
	step [217/244], loss=85.4205
	step [218/244], loss=66.7711
	step [219/244], loss=90.6129
	step [220/244], loss=70.2644
	step [221/244], loss=71.0110
	step [222/244], loss=89.0150
	step [223/244], loss=77.2263
	step [224/244], loss=77.5704
	step [225/244], loss=71.7474
	step [226/244], loss=80.3361
	step [227/244], loss=79.8995
	step [228/244], loss=67.9379
	step [229/244], loss=71.9082
	step [230/244], loss=77.1268
	step [231/244], loss=78.6614
	step [232/244], loss=77.9701
	step [233/244], loss=53.6607
	step [234/244], loss=79.3935
	step [235/244], loss=71.6917
	step [236/244], loss=79.7917
	step [237/244], loss=69.5275
	step [238/244], loss=68.8230
	step [239/244], loss=59.2711
	step [240/244], loss=88.3529
	step [241/244], loss=64.2495
	step [242/244], loss=81.7715
	step [243/244], loss=89.4246
	step [244/244], loss=61.0817
	Evaluating
	loss=0.0082, precision=0.3374, recall=0.8607, f1=0.4848
Training epoch 72
	step [1/244], loss=77.7640
	step [2/244], loss=88.2262
	step [3/244], loss=76.2068
	step [4/244], loss=67.9058
	step [5/244], loss=82.4268
	step [6/244], loss=65.3296
	step [7/244], loss=80.3108
	step [8/244], loss=63.8415
	step [9/244], loss=68.3897
	step [10/244], loss=73.1371
	step [11/244], loss=68.5026
	step [12/244], loss=81.8438
	step [13/244], loss=66.2249
	step [14/244], loss=67.2676
	step [15/244], loss=68.4233
	step [16/244], loss=77.2124
	step [17/244], loss=64.9404
	step [18/244], loss=83.1971
	step [19/244], loss=72.3456
	step [20/244], loss=58.5801
	step [21/244], loss=70.1862
	step [22/244], loss=78.4889
	step [23/244], loss=70.0356
	step [24/244], loss=66.4321
	step [25/244], loss=73.8858
	step [26/244], loss=104.1280
	step [27/244], loss=88.9289
	step [28/244], loss=84.5110
	step [29/244], loss=59.3856
	step [30/244], loss=65.2698
	step [31/244], loss=80.8260
	step [32/244], loss=90.8197
	step [33/244], loss=62.6022
	step [34/244], loss=68.4489
	step [35/244], loss=87.8099
	step [36/244], loss=72.7275
	step [37/244], loss=62.2519
	step [38/244], loss=88.8293
	step [39/244], loss=57.3590
	step [40/244], loss=85.0024
	step [41/244], loss=78.0177
	step [42/244], loss=74.5745
	step [43/244], loss=90.0959
	step [44/244], loss=84.0542
	step [45/244], loss=70.3176
	step [46/244], loss=66.3269
	step [47/244], loss=69.0474
	step [48/244], loss=80.5145
	step [49/244], loss=78.7521
	step [50/244], loss=89.3570
	step [51/244], loss=75.4387
	step [52/244], loss=84.9158
	step [53/244], loss=65.4640
	step [54/244], loss=63.9063
	step [55/244], loss=77.1858
	step [56/244], loss=79.4777
	step [57/244], loss=93.5099
	step [58/244], loss=77.4534
	step [59/244], loss=63.1985
	step [60/244], loss=84.4074
	step [61/244], loss=83.2557
	step [62/244], loss=72.6160
	step [63/244], loss=84.3938
	step [64/244], loss=63.8550
	step [65/244], loss=68.8318
	step [66/244], loss=69.3189
	step [67/244], loss=83.6330
	step [68/244], loss=80.1221
	step [69/244], loss=85.6071
	step [70/244], loss=71.5763
	step [71/244], loss=70.1977
	step [72/244], loss=71.4986
	step [73/244], loss=78.3558
	step [74/244], loss=79.0375
	step [75/244], loss=69.7743
	step [76/244], loss=59.9183
	step [77/244], loss=86.4514
	step [78/244], loss=65.6715
	step [79/244], loss=78.9601
	step [80/244], loss=74.1712
	step [81/244], loss=81.8394
	step [82/244], loss=91.9453
	step [83/244], loss=76.3375
	step [84/244], loss=77.7693
	step [85/244], loss=75.0252
	step [86/244], loss=70.0618
	step [87/244], loss=85.0793
	step [88/244], loss=76.4719
	step [89/244], loss=90.6163
	step [90/244], loss=78.7924
	step [91/244], loss=78.8563
	step [92/244], loss=74.7790
	step [93/244], loss=53.5688
	step [94/244], loss=63.4223
	step [95/244], loss=63.7768
	step [96/244], loss=77.8356
	step [97/244], loss=103.8364
	step [98/244], loss=85.7812
	step [99/244], loss=78.9664
	step [100/244], loss=68.0019
	step [101/244], loss=66.1550
	step [102/244], loss=76.0792
	step [103/244], loss=81.7492
	step [104/244], loss=69.8756
	step [105/244], loss=76.9997
	step [106/244], loss=86.7482
	step [107/244], loss=58.4124
	step [108/244], loss=75.5060
	step [109/244], loss=67.8792
	step [110/244], loss=74.8363
	step [111/244], loss=70.2459
	step [112/244], loss=83.8857
	step [113/244], loss=80.7714
	step [114/244], loss=61.7210
	step [115/244], loss=83.4557
	step [116/244], loss=90.3277
	step [117/244], loss=72.4809
	step [118/244], loss=56.3871
	step [119/244], loss=87.6857
	step [120/244], loss=76.2115
	step [121/244], loss=63.8729
	step [122/244], loss=70.5521
	step [123/244], loss=77.1966
	step [124/244], loss=69.1980
	step [125/244], loss=61.5597
	step [126/244], loss=56.9105
	step [127/244], loss=82.5606
	step [128/244], loss=78.4073
	step [129/244], loss=83.6477
	step [130/244], loss=69.1590
	step [131/244], loss=72.2776
	step [132/244], loss=68.6177
	step [133/244], loss=74.8743
	step [134/244], loss=84.6291
	step [135/244], loss=83.8513
	step [136/244], loss=61.7291
	step [137/244], loss=76.5078
	step [138/244], loss=84.2373
	step [139/244], loss=83.1104
	step [140/244], loss=76.6357
	step [141/244], loss=68.1936
	step [142/244], loss=61.8718
	step [143/244], loss=73.3579
	step [144/244], loss=76.8303
	step [145/244], loss=69.7196
	step [146/244], loss=77.7972
	step [147/244], loss=66.5867
	step [148/244], loss=76.0820
	step [149/244], loss=75.2359
	step [150/244], loss=87.7492
	step [151/244], loss=62.8084
	step [152/244], loss=74.8442
	step [153/244], loss=73.2366
	step [154/244], loss=69.7128
	step [155/244], loss=71.7286
	step [156/244], loss=72.3707
	step [157/244], loss=76.7805
	step [158/244], loss=97.4962
	step [159/244], loss=59.3709
	step [160/244], loss=72.8718
	step [161/244], loss=78.1768
	step [162/244], loss=77.7452
	step [163/244], loss=77.9950
	step [164/244], loss=77.1451
	step [165/244], loss=67.5796
	step [166/244], loss=80.3665
	step [167/244], loss=71.4945
	step [168/244], loss=83.0995
	step [169/244], loss=74.4247
	step [170/244], loss=76.6924
	step [171/244], loss=69.6543
	step [172/244], loss=60.0011
	step [173/244], loss=64.5046
	step [174/244], loss=72.5979
	step [175/244], loss=78.6887
	step [176/244], loss=92.2212
	step [177/244], loss=78.4526
	step [178/244], loss=85.2111
	step [179/244], loss=66.2336
	step [180/244], loss=71.4758
	step [181/244], loss=66.3141
	step [182/244], loss=83.1048
	step [183/244], loss=67.6370
	step [184/244], loss=65.9899
	step [185/244], loss=83.5126
	step [186/244], loss=84.7614
	step [187/244], loss=66.3868
	step [188/244], loss=71.7613
	step [189/244], loss=80.0314
	step [190/244], loss=68.2564
	step [191/244], loss=81.2741
	step [192/244], loss=76.3134
	step [193/244], loss=88.9445
	step [194/244], loss=67.9003
	step [195/244], loss=82.0754
	step [196/244], loss=74.0107
	step [197/244], loss=73.7525
	step [198/244], loss=61.9108
	step [199/244], loss=75.4281
	step [200/244], loss=71.4080
	step [201/244], loss=79.4493
	step [202/244], loss=98.6062
	step [203/244], loss=87.4308
	step [204/244], loss=79.1121
	step [205/244], loss=68.6620
	step [206/244], loss=82.4311
	step [207/244], loss=78.5924
	step [208/244], loss=83.5833
	step [209/244], loss=66.2363
	step [210/244], loss=79.8858
	step [211/244], loss=79.1977
	step [212/244], loss=57.6112
	step [213/244], loss=74.4898
	step [214/244], loss=81.4052
	step [215/244], loss=66.8741
	step [216/244], loss=82.1462
	step [217/244], loss=70.9553
	step [218/244], loss=78.3780
	step [219/244], loss=69.6182
	step [220/244], loss=62.3608
	step [221/244], loss=72.4661
	step [222/244], loss=61.7850
	step [223/244], loss=60.7720
	step [224/244], loss=76.1377
	step [225/244], loss=77.8164
	step [226/244], loss=63.0025
	step [227/244], loss=70.7098
	step [228/244], loss=90.9789
	step [229/244], loss=86.6877
	step [230/244], loss=74.4058
	step [231/244], loss=73.5358
	step [232/244], loss=82.0534
	step [233/244], loss=73.3359
	step [234/244], loss=71.4520
	step [235/244], loss=76.1681
	step [236/244], loss=65.0678
	step [237/244], loss=96.2358
	step [238/244], loss=80.0472
	step [239/244], loss=89.7902
	step [240/244], loss=89.3922
	step [241/244], loss=82.7355
	step [242/244], loss=82.9350
	step [243/244], loss=88.9957
	step [244/244], loss=72.4916
	Evaluating
	loss=0.0082, precision=0.3439, recall=0.8545, f1=0.4904
Training epoch 73
	step [1/244], loss=69.1765
	step [2/244], loss=80.1913
	step [3/244], loss=67.2263
	step [4/244], loss=92.4125
	step [5/244], loss=77.6305
	step [6/244], loss=84.4622
	step [7/244], loss=88.3243
	step [8/244], loss=66.0299
	step [9/244], loss=59.1717
	step [10/244], loss=83.4906
	step [11/244], loss=84.3185
	step [12/244], loss=71.4442
	step [13/244], loss=78.3441
	step [14/244], loss=84.8499
	step [15/244], loss=77.4440
	step [16/244], loss=64.3712
	step [17/244], loss=64.1569
	step [18/244], loss=77.3968
	step [19/244], loss=81.9676
	step [20/244], loss=55.7933
	step [21/244], loss=68.5964
	step [22/244], loss=85.1901
	step [23/244], loss=81.7543
	step [24/244], loss=70.7651
	step [25/244], loss=64.8935
	step [26/244], loss=69.0637
	step [27/244], loss=76.5844
	step [28/244], loss=77.8687
	step [29/244], loss=72.1008
	step [30/244], loss=74.4596
	step [31/244], loss=72.2323
	step [32/244], loss=67.8295
	step [33/244], loss=65.6094
	step [34/244], loss=85.6950
	step [35/244], loss=67.7026
	step [36/244], loss=82.0039
	step [37/244], loss=70.3603
	step [38/244], loss=79.0235
	step [39/244], loss=69.5889
	step [40/244], loss=85.8902
	step [41/244], loss=71.2621
	step [42/244], loss=65.1108
	step [43/244], loss=68.1431
	step [44/244], loss=81.2299
	step [45/244], loss=82.2809
	step [46/244], loss=77.0951
	step [47/244], loss=56.9161
	step [48/244], loss=63.9630
	step [49/244], loss=81.9176
	step [50/244], loss=86.7858
	step [51/244], loss=68.1296
	step [52/244], loss=67.3869
	step [53/244], loss=71.2294
	step [54/244], loss=70.8654
	step [55/244], loss=67.6188
	step [56/244], loss=68.7510
	step [57/244], loss=76.5402
	step [58/244], loss=65.8636
	step [59/244], loss=65.7387
	step [60/244], loss=69.9795
	step [61/244], loss=73.1778
	step [62/244], loss=77.3871
	step [63/244], loss=63.6686
	step [64/244], loss=94.0315
	step [65/244], loss=72.9005
	step [66/244], loss=67.7244
	step [67/244], loss=77.7028
	step [68/244], loss=72.5827
	step [69/244], loss=84.9116
	step [70/244], loss=97.6699
	step [71/244], loss=66.3869
	step [72/244], loss=77.6497
	step [73/244], loss=73.0796
	step [74/244], loss=81.4617
	step [75/244], loss=68.2850
	step [76/244], loss=63.4986
	step [77/244], loss=71.7890
	step [78/244], loss=73.9246
	step [79/244], loss=80.7822
	step [80/244], loss=82.8590
	step [81/244], loss=71.8082
	step [82/244], loss=73.6938
	step [83/244], loss=92.1796
	step [84/244], loss=81.4350
	step [85/244], loss=70.1297
	step [86/244], loss=90.2377
	step [87/244], loss=83.4244
	step [88/244], loss=68.8182
	step [89/244], loss=85.0149
	step [90/244], loss=69.9366
	step [91/244], loss=83.7867
	step [92/244], loss=68.7611
	step [93/244], loss=70.7944
	step [94/244], loss=80.8238
	step [95/244], loss=77.4836
	step [96/244], loss=75.8201
	step [97/244], loss=68.2929
	step [98/244], loss=70.4788
	step [99/244], loss=75.6540
	step [100/244], loss=51.2899
	step [101/244], loss=73.2189
	step [102/244], loss=81.6078
	step [103/244], loss=78.4907
	step [104/244], loss=75.6810
	step [105/244], loss=78.0695
	step [106/244], loss=70.2211
	step [107/244], loss=76.2468
	step [108/244], loss=84.1890
	step [109/244], loss=60.6093
	step [110/244], loss=88.0057
	step [111/244], loss=60.3546
	step [112/244], loss=71.1192
	step [113/244], loss=75.4662
	step [114/244], loss=71.8161
	step [115/244], loss=75.0195
	step [116/244], loss=64.8876
	step [117/244], loss=69.4918
	step [118/244], loss=59.7090
	step [119/244], loss=78.4993
	step [120/244], loss=72.4901
	step [121/244], loss=74.7760
	step [122/244], loss=80.6394
	step [123/244], loss=97.4056
	step [124/244], loss=61.5497
	step [125/244], loss=68.1792
	step [126/244], loss=75.7866
	step [127/244], loss=79.4961
	step [128/244], loss=78.4665
	step [129/244], loss=74.0173
	step [130/244], loss=86.6914
	step [131/244], loss=81.1521
	step [132/244], loss=79.1407
	step [133/244], loss=63.0762
	step [134/244], loss=85.0374
	step [135/244], loss=82.6029
	step [136/244], loss=78.7123
	step [137/244], loss=68.6610
	step [138/244], loss=69.0681
	step [139/244], loss=83.1238
	step [140/244], loss=60.1935
	step [141/244], loss=80.0915
	step [142/244], loss=80.2237
	step [143/244], loss=68.6897
	step [144/244], loss=81.2232
	step [145/244], loss=81.5624
	step [146/244], loss=93.2068
	step [147/244], loss=86.5812
	step [148/244], loss=71.5933
	step [149/244], loss=63.4903
	step [150/244], loss=72.8568
	step [151/244], loss=72.1837
	step [152/244], loss=77.2123
	step [153/244], loss=69.5589
	step [154/244], loss=91.9363
	step [155/244], loss=72.1924
	step [156/244], loss=73.9859
	step [157/244], loss=65.3320
	step [158/244], loss=65.4561
	step [159/244], loss=64.7661
	step [160/244], loss=60.0957
	step [161/244], loss=63.4577
	step [162/244], loss=77.3535
	step [163/244], loss=76.7540
	step [164/244], loss=75.0798
	step [165/244], loss=79.4752
	step [166/244], loss=64.6777
	step [167/244], loss=50.4987
	step [168/244], loss=92.6079
	step [169/244], loss=97.4016
	step [170/244], loss=77.0471
	step [171/244], loss=69.5632
	step [172/244], loss=75.7975
	step [173/244], loss=66.6848
	step [174/244], loss=83.3534
	step [175/244], loss=78.7569
	step [176/244], loss=75.0166
	step [177/244], loss=81.2837
	step [178/244], loss=85.3360
	step [179/244], loss=65.8122
	step [180/244], loss=74.3229
	step [181/244], loss=77.5345
	step [182/244], loss=87.2306
	step [183/244], loss=80.0142
	step [184/244], loss=69.1940
	step [185/244], loss=65.1936
	step [186/244], loss=74.0531
	step [187/244], loss=87.5277
	step [188/244], loss=69.4867
	step [189/244], loss=90.4151
	step [190/244], loss=77.0469
	step [191/244], loss=71.5296
	step [192/244], loss=67.6212
	step [193/244], loss=57.3807
	step [194/244], loss=71.1167
	step [195/244], loss=78.9335
	step [196/244], loss=67.5272
	step [197/244], loss=87.7215
	step [198/244], loss=83.5248
	step [199/244], loss=69.5370
	step [200/244], loss=70.3757
	step [201/244], loss=75.3172
	step [202/244], loss=88.2536
	step [203/244], loss=68.5457
	step [204/244], loss=72.0375
	step [205/244], loss=77.1527
	step [206/244], loss=57.9169
	step [207/244], loss=72.9444
	step [208/244], loss=81.4417
	step [209/244], loss=72.5405
	step [210/244], loss=79.6946
	step [211/244], loss=81.3703
	step [212/244], loss=75.4651
	step [213/244], loss=63.5148
	step [214/244], loss=75.3165
	step [215/244], loss=62.2865
	step [216/244], loss=53.2664
	step [217/244], loss=81.4696
	step [218/244], loss=71.7837
	step [219/244], loss=77.1899
	step [220/244], loss=84.3566
	step [221/244], loss=67.7622
	step [222/244], loss=75.4984
	step [223/244], loss=75.2184
	step [224/244], loss=90.6487
	step [225/244], loss=69.8404
	step [226/244], loss=80.6958
	step [227/244], loss=90.8351
	step [228/244], loss=71.5650
	step [229/244], loss=64.1674
	step [230/244], loss=68.5186
	step [231/244], loss=91.6234
	step [232/244], loss=71.6238
	step [233/244], loss=68.7391
	step [234/244], loss=77.8806
	step [235/244], loss=69.6358
	step [236/244], loss=76.2301
	step [237/244], loss=96.8692
	step [238/244], loss=72.8912
	step [239/244], loss=78.2694
	step [240/244], loss=78.3910
	step [241/244], loss=77.0162
	step [242/244], loss=90.7851
	step [243/244], loss=78.0276
	step [244/244], loss=72.1077
	Evaluating
	loss=0.0085, precision=0.3315, recall=0.8545, f1=0.4777
Training epoch 74
	step [1/244], loss=78.6769
	step [2/244], loss=79.6193
	step [3/244], loss=69.5198
	step [4/244], loss=67.5063
	step [5/244], loss=80.4472
	step [6/244], loss=93.6422
	step [7/244], loss=88.4522
	step [8/244], loss=82.1057
	step [9/244], loss=75.3340
	step [10/244], loss=80.5589
	step [11/244], loss=60.4038
	step [12/244], loss=83.8305
	step [13/244], loss=86.0927
	step [14/244], loss=81.2655
	step [15/244], loss=68.7464
	step [16/244], loss=79.7837
	step [17/244], loss=76.6405
	step [18/244], loss=80.6599
	step [19/244], loss=66.1716
	step [20/244], loss=72.3594
	step [21/244], loss=103.5685
	step [22/244], loss=76.9599
	step [23/244], loss=82.9789
	step [24/244], loss=77.7869
	step [25/244], loss=76.0218
	step [26/244], loss=90.8383
	step [27/244], loss=80.4825
	step [28/244], loss=73.1272
	step [29/244], loss=67.6574
	step [30/244], loss=70.1363
	step [31/244], loss=74.9581
	step [32/244], loss=72.4444
	step [33/244], loss=71.5998
	step [34/244], loss=77.9894
	step [35/244], loss=77.5422
	step [36/244], loss=83.3664
	step [37/244], loss=67.0266
	step [38/244], loss=66.8702
	step [39/244], loss=54.5904
	step [40/244], loss=73.3595
	step [41/244], loss=77.1786
	step [42/244], loss=71.3028
	step [43/244], loss=99.1070
	step [44/244], loss=77.2007
	step [45/244], loss=75.3669
	step [46/244], loss=91.0383
	step [47/244], loss=84.2366
	step [48/244], loss=73.2669
	step [49/244], loss=69.9400
	step [50/244], loss=67.8021
	step [51/244], loss=86.4993
	step [52/244], loss=79.5769
	step [53/244], loss=75.6152
	step [54/244], loss=61.7839
	step [55/244], loss=76.7798
	step [56/244], loss=64.1344
	step [57/244], loss=75.6180
	step [58/244], loss=71.4896
	step [59/244], loss=76.8709
	step [60/244], loss=63.8603
	step [61/244], loss=68.1206
	step [62/244], loss=69.2022
	step [63/244], loss=66.1048
	step [64/244], loss=75.9907
	step [65/244], loss=70.1980
	step [66/244], loss=72.5596
	step [67/244], loss=65.3400
	step [68/244], loss=69.1338
	step [69/244], loss=85.3260
	step [70/244], loss=79.5065
	step [71/244], loss=59.8031
	step [72/244], loss=78.1382
	step [73/244], loss=76.4288
	step [74/244], loss=57.5788
	step [75/244], loss=88.1079
	step [76/244], loss=63.5598
	step [77/244], loss=77.0354
	step [78/244], loss=75.2803
	step [79/244], loss=73.4061
	step [80/244], loss=89.3652
	step [81/244], loss=84.0100
	step [82/244], loss=62.2724
	step [83/244], loss=72.9798
	step [84/244], loss=75.9160
	step [85/244], loss=82.9077
	step [86/244], loss=71.2112
	step [87/244], loss=80.4161
	step [88/244], loss=71.3195
	step [89/244], loss=72.7560
	step [90/244], loss=59.0997
	step [91/244], loss=88.6386
	step [92/244], loss=79.4467
	step [93/244], loss=91.2012
	step [94/244], loss=80.4327
	step [95/244], loss=79.3266
	step [96/244], loss=71.8212
	step [97/244], loss=62.3556
	step [98/244], loss=68.9247
	step [99/244], loss=72.8062
	step [100/244], loss=72.2319
	step [101/244], loss=69.5530
	step [102/244], loss=65.5883
	step [103/244], loss=69.0368
	step [104/244], loss=56.2424
	step [105/244], loss=82.1016
	step [106/244], loss=71.1431
	step [107/244], loss=87.4798
	step [108/244], loss=71.6492
	step [109/244], loss=75.8051
	step [110/244], loss=62.5951
	step [111/244], loss=77.6992
	step [112/244], loss=68.9795
	step [113/244], loss=89.4091
	step [114/244], loss=91.3757
	step [115/244], loss=61.0690
	step [116/244], loss=57.5876
	step [117/244], loss=74.4456
	step [118/244], loss=82.9842
	step [119/244], loss=80.3018
	step [120/244], loss=72.8870
	step [121/244], loss=74.9791
	step [122/244], loss=70.5008
	step [123/244], loss=63.7666
	step [124/244], loss=64.4309
	step [125/244], loss=91.5959
	step [126/244], loss=64.8527
	step [127/244], loss=83.4033
	step [128/244], loss=80.4322
	step [129/244], loss=83.4902
	step [130/244], loss=76.8922
	step [131/244], loss=65.8548
	step [132/244], loss=82.2822
	step [133/244], loss=73.5628
	step [134/244], loss=77.7582
	step [135/244], loss=84.7899
	step [136/244], loss=73.7688
	step [137/244], loss=69.5646
	step [138/244], loss=82.8612
	step [139/244], loss=87.7371
	step [140/244], loss=82.0166
	step [141/244], loss=71.9112
	step [142/244], loss=75.3984
	step [143/244], loss=65.3556
	step [144/244], loss=77.5008
	step [145/244], loss=69.3025
	step [146/244], loss=68.7487
	step [147/244], loss=85.6393
	step [148/244], loss=83.1168
	step [149/244], loss=83.4488
	step [150/244], loss=67.8286
	step [151/244], loss=80.9046
	step [152/244], loss=75.6322
	step [153/244], loss=74.7716
	step [154/244], loss=81.7311
	step [155/244], loss=73.9488
	step [156/244], loss=70.3034
	step [157/244], loss=65.2258
	step [158/244], loss=90.5711
	step [159/244], loss=80.9326
	step [160/244], loss=72.3760
	step [161/244], loss=66.2814
	step [162/244], loss=70.8755
	step [163/244], loss=66.7683
	step [164/244], loss=69.3421
	step [165/244], loss=82.6679
	step [166/244], loss=62.5708
	step [167/244], loss=68.4122
	step [168/244], loss=63.4504
	step [169/244], loss=82.6083
	step [170/244], loss=59.1915
	step [171/244], loss=81.5125
	step [172/244], loss=70.2760
	step [173/244], loss=78.5796
	step [174/244], loss=74.5241
	step [175/244], loss=79.6006
	step [176/244], loss=67.4668
	step [177/244], loss=65.3051
	step [178/244], loss=72.4621
	step [179/244], loss=60.4376
	step [180/244], loss=62.8299
	step [181/244], loss=85.5275
	step [182/244], loss=67.3087
	step [183/244], loss=81.0056
	step [184/244], loss=84.9818
	step [185/244], loss=66.2143
	step [186/244], loss=72.2974
	step [187/244], loss=75.5971
	step [188/244], loss=62.2480
	step [189/244], loss=68.2440
	step [190/244], loss=75.5418
	step [191/244], loss=76.5968
	step [192/244], loss=69.8492
	step [193/244], loss=76.4524
	step [194/244], loss=95.4844
	step [195/244], loss=71.3099
	step [196/244], loss=70.6199
	step [197/244], loss=62.4151
	step [198/244], loss=81.4319
	step [199/244], loss=69.3550
	step [200/244], loss=99.8582
	step [201/244], loss=76.8174
	step [202/244], loss=66.1944
	step [203/244], loss=70.9722
	step [204/244], loss=71.6187
	step [205/244], loss=60.7073
	step [206/244], loss=76.6415
	step [207/244], loss=83.3651
	step [208/244], loss=60.1048
	step [209/244], loss=78.6227
	step [210/244], loss=78.1422
	step [211/244], loss=71.8564
	step [212/244], loss=79.7804
	step [213/244], loss=76.9958
	step [214/244], loss=90.5310
	step [215/244], loss=82.0323
	step [216/244], loss=65.5480
	step [217/244], loss=58.5109
	step [218/244], loss=75.4879
	step [219/244], loss=65.9396
	step [220/244], loss=81.3933
	step [221/244], loss=62.9913
	step [222/244], loss=83.7814
	step [223/244], loss=64.1128
	step [224/244], loss=73.6316
	step [225/244], loss=67.6624
	step [226/244], loss=71.2546
	step [227/244], loss=81.6941
	step [228/244], loss=65.2783
	step [229/244], loss=82.0888
	step [230/244], loss=89.6241
	step [231/244], loss=73.2235
	step [232/244], loss=78.1019
	step [233/244], loss=67.5260
	step [234/244], loss=85.0708
	step [235/244], loss=73.7334
	step [236/244], loss=68.0610
	step [237/244], loss=58.1714
	step [238/244], loss=69.5105
	step [239/244], loss=73.8501
	step [240/244], loss=67.0336
	step [241/244], loss=71.4056
	step [242/244], loss=73.7984
	step [243/244], loss=85.4628
	step [244/244], loss=78.4931
	Evaluating
	loss=0.0090, precision=0.3117, recall=0.8555, f1=0.4569
Training epoch 75
	step [1/244], loss=77.8796
	step [2/244], loss=68.0430
	step [3/244], loss=63.6569
	step [4/244], loss=76.2423
	step [5/244], loss=59.8992
	step [6/244], loss=68.5768
	step [7/244], loss=60.9792
	step [8/244], loss=71.3779
	step [9/244], loss=67.2982
	step [10/244], loss=75.1953
	step [11/244], loss=66.6486
	step [12/244], loss=90.6892
	step [13/244], loss=84.2742
	step [14/244], loss=69.6659
	step [15/244], loss=74.5021
	step [16/244], loss=77.1210
	step [17/244], loss=75.6347
	step [18/244], loss=74.0296
	step [19/244], loss=82.3335
	step [20/244], loss=67.1117
	step [21/244], loss=68.8132
	step [22/244], loss=88.1763
	step [23/244], loss=68.2340
	step [24/244], loss=78.3027
	step [25/244], loss=79.4909
	step [26/244], loss=87.1809
	step [27/244], loss=58.3393
	step [28/244], loss=74.4022
	step [29/244], loss=78.7126
	step [30/244], loss=85.6838
	step [31/244], loss=82.7944
	step [32/244], loss=81.0257
	step [33/244], loss=87.8505
	step [34/244], loss=81.8429
	step [35/244], loss=67.6386
	step [36/244], loss=89.4951
	step [37/244], loss=81.7918
	step [38/244], loss=77.8672
	step [39/244], loss=71.7819
	step [40/244], loss=73.1323
	step [41/244], loss=84.7565
	step [42/244], loss=65.5937
	step [43/244], loss=65.2724
	step [44/244], loss=79.3911
	step [45/244], loss=61.6125
	step [46/244], loss=69.6817
	step [47/244], loss=82.2208
	step [48/244], loss=64.2583
	step [49/244], loss=64.3661
	step [50/244], loss=76.6299
	step [51/244], loss=75.4991
	step [52/244], loss=76.3503
	step [53/244], loss=71.9684
	step [54/244], loss=77.2698
	step [55/244], loss=76.8412
	step [56/244], loss=78.1945
	step [57/244], loss=73.2285
	step [58/244], loss=93.2023
	step [59/244], loss=75.5246
	step [60/244], loss=64.3088
	step [61/244], loss=93.2318
	step [62/244], loss=66.8987
	step [63/244], loss=73.3085
	step [64/244], loss=63.0227
	step [65/244], loss=78.6280
	step [66/244], loss=64.5889
	step [67/244], loss=64.9389
	step [68/244], loss=72.6060
	step [69/244], loss=81.0093
	step [70/244], loss=91.3639
	step [71/244], loss=61.1093
	step [72/244], loss=82.9971
	step [73/244], loss=61.0615
	step [74/244], loss=77.8053
	step [75/244], loss=63.6578
	step [76/244], loss=76.5672
	step [77/244], loss=68.6126
	step [78/244], loss=77.8763
	step [79/244], loss=69.9784
	step [80/244], loss=75.7502
	step [81/244], loss=82.5876
	step [82/244], loss=68.4019
	step [83/244], loss=71.1621
	step [84/244], loss=76.3523
	step [85/244], loss=63.6161
	step [86/244], loss=87.9971
	step [87/244], loss=63.3279
	step [88/244], loss=64.6807
	step [89/244], loss=91.8046
	step [90/244], loss=75.1503
	step [91/244], loss=63.9423
	step [92/244], loss=70.1900
	step [93/244], loss=58.6937
	step [94/244], loss=93.4041
	step [95/244], loss=66.5136
	step [96/244], loss=92.8454
	step [97/244], loss=65.3907
	step [98/244], loss=75.1539
	step [99/244], loss=69.3829
	step [100/244], loss=75.2486
	step [101/244], loss=83.7935
	step [102/244], loss=71.5711
	step [103/244], loss=62.7082
	step [104/244], loss=71.9884
	step [105/244], loss=67.3904
	step [106/244], loss=71.3941
	step [107/244], loss=89.1380
	step [108/244], loss=88.7391
	step [109/244], loss=64.8371
	step [110/244], loss=69.4708
	step [111/244], loss=72.6197
	step [112/244], loss=70.0054
	step [113/244], loss=72.8142
	step [114/244], loss=79.7783
	step [115/244], loss=80.1760
	step [116/244], loss=75.0529
	step [117/244], loss=75.0659
	step [118/244], loss=69.8528
	step [119/244], loss=79.3379
	step [120/244], loss=62.8080
	step [121/244], loss=67.8258
	step [122/244], loss=86.9996
	step [123/244], loss=87.2576
	step [124/244], loss=93.1915
	step [125/244], loss=57.8463
	step [126/244], loss=68.5911
	step [127/244], loss=83.0374
	step [128/244], loss=94.5519
	step [129/244], loss=87.2111
	step [130/244], loss=74.4520
	step [131/244], loss=78.9314
	step [132/244], loss=77.3429
	step [133/244], loss=85.3108
	step [134/244], loss=73.2238
	step [135/244], loss=92.0864
	step [136/244], loss=70.5604
	step [137/244], loss=73.0147
	step [138/244], loss=61.0190
	step [139/244], loss=86.8591
	step [140/244], loss=77.7780
	step [141/244], loss=62.6475
	step [142/244], loss=88.1617
	step [143/244], loss=95.3545
	step [144/244], loss=71.2197
	step [145/244], loss=75.6757
	step [146/244], loss=93.5260
	step [147/244], loss=72.7583
	step [148/244], loss=71.8784
	step [149/244], loss=65.3765
	step [150/244], loss=77.5266
	step [151/244], loss=58.2860
	step [152/244], loss=74.6820
	step [153/244], loss=86.9275
	step [154/244], loss=66.6820
	step [155/244], loss=77.9596
	step [156/244], loss=73.1475
	step [157/244], loss=66.6696
	step [158/244], loss=60.2235
	step [159/244], loss=74.8249
	step [160/244], loss=68.3246
	step [161/244], loss=76.5180
	step [162/244], loss=83.0078
	step [163/244], loss=74.3121
	step [164/244], loss=77.3273
	step [165/244], loss=62.8865
	step [166/244], loss=69.7103
	step [167/244], loss=70.8740
	step [168/244], loss=69.8376
	step [169/244], loss=74.4711
	step [170/244], loss=78.3311
	step [171/244], loss=58.8366
	step [172/244], loss=77.6930
	step [173/244], loss=83.3746
	step [174/244], loss=62.8272
	step [175/244], loss=67.4417
	step [176/244], loss=73.8418
	step [177/244], loss=78.5659
	step [178/244], loss=81.8711
	step [179/244], loss=74.3915
	step [180/244], loss=81.5979
	step [181/244], loss=86.1835
	step [182/244], loss=75.2074
	step [183/244], loss=52.5413
	step [184/244], loss=74.3865
	step [185/244], loss=73.6700
	step [186/244], loss=76.9519
	step [187/244], loss=63.0567
	step [188/244], loss=82.7234
	step [189/244], loss=65.1998
	step [190/244], loss=75.6894
	step [191/244], loss=72.3984
	step [192/244], loss=68.6204
	step [193/244], loss=74.6089
	step [194/244], loss=82.8085
	step [195/244], loss=79.2238
	step [196/244], loss=71.0053
	step [197/244], loss=60.8860
	step [198/244], loss=61.2164
	step [199/244], loss=73.6533
	step [200/244], loss=70.1679
	step [201/244], loss=77.2921
	step [202/244], loss=74.6583
	step [203/244], loss=73.2190
	step [204/244], loss=76.5228
	step [205/244], loss=79.1317
	step [206/244], loss=76.7111
	step [207/244], loss=72.3246
	step [208/244], loss=73.3008
	step [209/244], loss=82.2284
	step [210/244], loss=92.5461
	step [211/244], loss=71.1784
	step [212/244], loss=70.1987
	step [213/244], loss=69.7335
	step [214/244], loss=70.5876
	step [215/244], loss=73.6032
	step [216/244], loss=64.0176
	step [217/244], loss=70.2680
	step [218/244], loss=77.7158
	step [219/244], loss=64.4949
	step [220/244], loss=77.4360
	step [221/244], loss=83.5509
	step [222/244], loss=64.4912
	step [223/244], loss=73.9069
	step [224/244], loss=85.6263
	step [225/244], loss=57.6505
	step [226/244], loss=72.9242
	step [227/244], loss=78.0264
	step [228/244], loss=79.6627
	step [229/244], loss=83.3500
	step [230/244], loss=66.7394
	step [231/244], loss=80.1167
	step [232/244], loss=59.9470
	step [233/244], loss=79.1824
	step [234/244], loss=67.5112
	step [235/244], loss=72.7979
	step [236/244], loss=62.2110
	step [237/244], loss=78.1476
	step [238/244], loss=79.9030
	step [239/244], loss=62.6852
	step [240/244], loss=93.9415
	step [241/244], loss=74.8835
	step [242/244], loss=73.0473
	step [243/244], loss=83.1888
	step [244/244], loss=66.9971
	Evaluating
	loss=0.0088, precision=0.3164, recall=0.8601, f1=0.4626
Training epoch 76
	step [1/244], loss=90.0071
	step [2/244], loss=82.5541
	step [3/244], loss=56.1289
	step [4/244], loss=78.7290
	step [5/244], loss=75.2403
	step [6/244], loss=63.7945
	step [7/244], loss=79.9324
	step [8/244], loss=61.5075
	step [9/244], loss=94.6207
	step [10/244], loss=83.2544
	step [11/244], loss=78.1266
	step [12/244], loss=74.0825
	step [13/244], loss=84.1886
	step [14/244], loss=87.1076
	step [15/244], loss=83.0485
	step [16/244], loss=67.1376
	step [17/244], loss=75.0245
	step [18/244], loss=65.9274
	step [19/244], loss=66.2713
	step [20/244], loss=78.9133
	step [21/244], loss=79.9084
	step [22/244], loss=65.0313
	step [23/244], loss=72.0084
	step [24/244], loss=66.3875
	step [25/244], loss=75.1339
	step [26/244], loss=82.8872
	step [27/244], loss=90.5235
	step [28/244], loss=73.2393
	step [29/244], loss=74.8690
	step [30/244], loss=86.3702
	step [31/244], loss=67.5523
	step [32/244], loss=64.9483
	step [33/244], loss=65.3390
	step [34/244], loss=87.3302
	step [35/244], loss=77.0720
	step [36/244], loss=67.8697
	step [37/244], loss=68.4259
	step [38/244], loss=92.0567
	step [39/244], loss=79.6043
	step [40/244], loss=62.3885
	step [41/244], loss=73.3633
	step [42/244], loss=74.9173
	step [43/244], loss=70.2510
	step [44/244], loss=66.2034
	step [45/244], loss=76.9785
	step [46/244], loss=67.0234
	step [47/244], loss=80.1027
	step [48/244], loss=69.9319
	step [49/244], loss=70.2873
	step [50/244], loss=82.7353
	step [51/244], loss=55.2250
	step [52/244], loss=77.2650
	step [53/244], loss=75.0971
	step [54/244], loss=85.4272
	step [55/244], loss=80.1563
	step [56/244], loss=69.1184
	step [57/244], loss=66.2455
	step [58/244], loss=63.7112
	step [59/244], loss=66.7415
	step [60/244], loss=63.5572
	step [61/244], loss=61.9648
	step [62/244], loss=81.6001
	step [63/244], loss=81.2426
	step [64/244], loss=85.6233
	step [65/244], loss=72.4811
	step [66/244], loss=73.8615
	step [67/244], loss=75.8462
	step [68/244], loss=67.0726
	step [69/244], loss=85.2829
	step [70/244], loss=74.9927
	step [71/244], loss=72.1850
	step [72/244], loss=68.4399
	step [73/244], loss=76.4203
	step [74/244], loss=79.6693
	step [75/244], loss=75.4489
	step [76/244], loss=80.0353
	step [77/244], loss=71.2826
	step [78/244], loss=64.4728
	step [79/244], loss=85.8094
	step [80/244], loss=62.8315
	step [81/244], loss=73.4321
	step [82/244], loss=67.5110
	step [83/244], loss=67.2187
	step [84/244], loss=66.1994
	step [85/244], loss=66.0187
	step [86/244], loss=70.8425
	step [87/244], loss=81.3687
	step [88/244], loss=67.6142
	step [89/244], loss=67.1865
	step [90/244], loss=56.7617
	step [91/244], loss=94.9208
	step [92/244], loss=70.3811
	step [93/244], loss=71.1324
	step [94/244], loss=77.1244
	step [95/244], loss=74.2088
	step [96/244], loss=81.1562
	step [97/244], loss=72.4473
	step [98/244], loss=73.6178
	step [99/244], loss=61.5723
	step [100/244], loss=65.7245
	step [101/244], loss=86.6964
	step [102/244], loss=76.7892
	step [103/244], loss=73.9226
	step [104/244], loss=75.5427
	step [105/244], loss=56.0865
	step [106/244], loss=66.1584
	step [107/244], loss=84.6005
	step [108/244], loss=75.8532
	step [109/244], loss=74.9926
	step [110/244], loss=68.2107
	step [111/244], loss=72.7181
	step [112/244], loss=76.7050
	step [113/244], loss=73.2626
	step [114/244], loss=67.2816
	step [115/244], loss=74.6768
	step [116/244], loss=79.5835
	step [117/244], loss=60.8926
	step [118/244], loss=62.5078
	step [119/244], loss=63.5736
	step [120/244], loss=72.3901
	step [121/244], loss=73.8501
	step [122/244], loss=79.6983
	step [123/244], loss=92.7667
	step [124/244], loss=87.5983
	step [125/244], loss=68.5724
	step [126/244], loss=64.1626
	step [127/244], loss=77.8009
	step [128/244], loss=60.9583
	step [129/244], loss=68.0199
	step [130/244], loss=79.4182
	step [131/244], loss=77.8288
	step [132/244], loss=73.9478
	step [133/244], loss=82.3940
	step [134/244], loss=54.8520
	step [135/244], loss=62.8131
	step [136/244], loss=82.1600
	step [137/244], loss=81.7933
	step [138/244], loss=70.0520
	step [139/244], loss=80.0495
	step [140/244], loss=88.7730
	step [141/244], loss=76.4702
	step [142/244], loss=93.4966
	step [143/244], loss=86.7334
	step [144/244], loss=79.1712
	step [145/244], loss=79.9443
	step [146/244], loss=74.4097
	step [147/244], loss=74.5949
	step [148/244], loss=68.6611
	step [149/244], loss=75.5731
	step [150/244], loss=70.0571
	step [151/244], loss=67.4136
	step [152/244], loss=79.6241
	step [153/244], loss=58.6447
	step [154/244], loss=64.1602
	step [155/244], loss=81.1686
	step [156/244], loss=58.7076
	step [157/244], loss=78.4438
	step [158/244], loss=70.9196
	step [159/244], loss=64.8200
	step [160/244], loss=86.4037
	step [161/244], loss=63.7694
	step [162/244], loss=68.0461
	step [163/244], loss=58.4582
	step [164/244], loss=53.9285
	step [165/244], loss=88.6779
	step [166/244], loss=61.6879
	step [167/244], loss=72.6161
	step [168/244], loss=82.2120
	step [169/244], loss=75.2807
	step [170/244], loss=61.4996
	step [171/244], loss=72.9063
	step [172/244], loss=70.2743
	step [173/244], loss=77.6175
	step [174/244], loss=68.7814
	step [175/244], loss=71.9350
	step [176/244], loss=72.5234
	step [177/244], loss=67.3839
	step [178/244], loss=92.3116
	step [179/244], loss=84.8093
	step [180/244], loss=65.6630
	step [181/244], loss=78.8077
	step [182/244], loss=71.2664
	step [183/244], loss=71.2603
	step [184/244], loss=84.7841
	step [185/244], loss=78.5959
	step [186/244], loss=76.2391
	step [187/244], loss=101.7631
	step [188/244], loss=66.7264
	step [189/244], loss=82.6643
	step [190/244], loss=63.6284
	step [191/244], loss=75.2430
	step [192/244], loss=47.8713
	step [193/244], loss=71.5476
	step [194/244], loss=67.7581
	step [195/244], loss=73.2637
	step [196/244], loss=69.8823
	step [197/244], loss=69.3748
	step [198/244], loss=70.9693
	step [199/244], loss=72.1010
	step [200/244], loss=78.6469
	step [201/244], loss=74.5226
	step [202/244], loss=72.0216
	step [203/244], loss=72.4323
	step [204/244], loss=65.1317
	step [205/244], loss=66.4851
	step [206/244], loss=73.1067
	step [207/244], loss=72.0935
	step [208/244], loss=93.9604
	step [209/244], loss=80.8021
	step [210/244], loss=81.9103
	step [211/244], loss=88.7883
	step [212/244], loss=74.0819
	step [213/244], loss=73.3250
	step [214/244], loss=83.3196
	step [215/244], loss=79.0357
	step [216/244], loss=72.7980
	step [217/244], loss=76.6707
	step [218/244], loss=64.7544
	step [219/244], loss=66.4199
	step [220/244], loss=81.7655
	step [221/244], loss=63.7560
	step [222/244], loss=111.3319
	step [223/244], loss=80.2681
	step [224/244], loss=72.1263
	step [225/244], loss=64.1669
	step [226/244], loss=58.4597
	step [227/244], loss=78.2017
	step [228/244], loss=78.1030
	step [229/244], loss=78.2406
	step [230/244], loss=77.5188
	step [231/244], loss=73.2845
	step [232/244], loss=80.4020
	step [233/244], loss=84.4691
	step [234/244], loss=79.5601
	step [235/244], loss=85.8342
	step [236/244], loss=79.8722
	step [237/244], loss=65.5978
	step [238/244], loss=85.3933
	step [239/244], loss=88.0029
	step [240/244], loss=75.6658
	step [241/244], loss=74.7015
	step [242/244], loss=79.3156
	step [243/244], loss=91.6113
	step [244/244], loss=58.6697
	Evaluating
	loss=0.0092, precision=0.2934, recall=0.8570, f1=0.4371
Training epoch 77
	step [1/244], loss=71.9505
	step [2/244], loss=52.3668
	step [3/244], loss=68.1649
	step [4/244], loss=62.9124
	step [5/244], loss=56.7798
	step [6/244], loss=85.7111
	step [7/244], loss=64.2282
	step [8/244], loss=87.6782
	step [9/244], loss=80.8800
	step [10/244], loss=60.5087
	step [11/244], loss=62.6333
	step [12/244], loss=63.7141
	step [13/244], loss=67.6091
	step [14/244], loss=67.0157
	step [15/244], loss=64.3867
	step [16/244], loss=67.8798
	step [17/244], loss=89.6480
	step [18/244], loss=76.3026
	step [19/244], loss=55.1765
	step [20/244], loss=76.1898
	step [21/244], loss=68.3584
	step [22/244], loss=70.2092
	step [23/244], loss=85.3972
	step [24/244], loss=77.9634
	step [25/244], loss=66.0842
	step [26/244], loss=74.2586
	step [27/244], loss=88.2915
	step [28/244], loss=63.0243
	step [29/244], loss=68.6164
	step [30/244], loss=84.7743
	step [31/244], loss=80.3833
	step [32/244], loss=79.0614
	step [33/244], loss=69.2410
	step [34/244], loss=69.9405
	step [35/244], loss=68.1607
	step [36/244], loss=73.3133
	step [37/244], loss=69.6010
	step [38/244], loss=71.6091
	step [39/244], loss=70.6995
	step [40/244], loss=66.0408
	step [41/244], loss=72.9838
	step [42/244], loss=77.7085
	step [43/244], loss=68.7232
	step [44/244], loss=65.1665
	step [45/244], loss=57.9069
	step [46/244], loss=89.9545
	step [47/244], loss=63.9339
	step [48/244], loss=68.2745
	step [49/244], loss=81.5330
	step [50/244], loss=79.3407
	step [51/244], loss=74.5410
	step [52/244], loss=62.1169
	step [53/244], loss=75.3824
	step [54/244], loss=79.8457
	step [55/244], loss=63.5396
	step [56/244], loss=72.2365
	step [57/244], loss=68.5371
	step [58/244], loss=65.7950
	step [59/244], loss=71.3593
	step [60/244], loss=77.7401
	step [61/244], loss=67.1257
	step [62/244], loss=74.2543
	step [63/244], loss=60.8434
	step [64/244], loss=65.8943
	step [65/244], loss=57.1014
	step [66/244], loss=62.8349
	step [67/244], loss=81.3279
	step [68/244], loss=71.7954
	step [69/244], loss=78.3659
	step [70/244], loss=67.2430
	step [71/244], loss=64.2069
	step [72/244], loss=86.7178
	step [73/244], loss=63.5328
	step [74/244], loss=74.3401
	step [75/244], loss=56.8541
	step [76/244], loss=87.7711
	step [77/244], loss=69.4224
	step [78/244], loss=86.3160
	step [79/244], loss=76.4361
	step [80/244], loss=73.3583
	step [81/244], loss=67.3297
	step [82/244], loss=92.4093
	step [83/244], loss=77.8314
	step [84/244], loss=66.8294
	step [85/244], loss=73.9948
	step [86/244], loss=64.3942
	step [87/244], loss=74.1863
	step [88/244], loss=60.4221
	step [89/244], loss=58.1048
	step [90/244], loss=81.1947
	step [91/244], loss=77.7642
	step [92/244], loss=67.0076
	step [93/244], loss=67.1328
	step [94/244], loss=61.8466
	step [95/244], loss=81.1785
	step [96/244], loss=64.9047
	step [97/244], loss=63.4744
	step [98/244], loss=82.7116
	step [99/244], loss=60.7553
	step [100/244], loss=75.8384
	step [101/244], loss=79.3976
	step [102/244], loss=67.2896
	step [103/244], loss=81.9115
	step [104/244], loss=67.0801
	step [105/244], loss=69.9194
	step [106/244], loss=59.4818
	step [107/244], loss=70.9694
	step [108/244], loss=70.5842
	step [109/244], loss=80.3618
	step [110/244], loss=79.7100
	step [111/244], loss=63.0227
	step [112/244], loss=64.1390
	step [113/244], loss=75.7091
	step [114/244], loss=65.5573
	step [115/244], loss=100.1095
	step [116/244], loss=89.9036
	step [117/244], loss=79.2279
	step [118/244], loss=70.5058
	step [119/244], loss=76.0025
	step [120/244], loss=61.2209
	step [121/244], loss=72.2603
	step [122/244], loss=78.4033
	step [123/244], loss=74.0909
	step [124/244], loss=79.7970
	step [125/244], loss=69.1404
	step [126/244], loss=76.2424
	step [127/244], loss=65.3959
	step [128/244], loss=65.6184
	step [129/244], loss=79.5917
	step [130/244], loss=76.9550
	step [131/244], loss=71.6437
	step [132/244], loss=69.4886
	step [133/244], loss=68.8116
	step [134/244], loss=69.5768
	step [135/244], loss=55.0212
	step [136/244], loss=79.2508
	step [137/244], loss=88.1659
	step [138/244], loss=68.9806
	step [139/244], loss=80.9985
	step [140/244], loss=84.3509
	step [141/244], loss=75.6383
	step [142/244], loss=85.5430
	step [143/244], loss=79.1461
	step [144/244], loss=69.2080
	step [145/244], loss=68.6064
	step [146/244], loss=78.9543
	step [147/244], loss=81.2878
	step [148/244], loss=64.5606
	step [149/244], loss=89.5876
	step [150/244], loss=80.5288
	step [151/244], loss=92.4431
	step [152/244], loss=78.2800
	step [153/244], loss=94.8831
	step [154/244], loss=79.1029
	step [155/244], loss=76.4000
	step [156/244], loss=78.1338
	step [157/244], loss=81.4300
	step [158/244], loss=67.5080
	step [159/244], loss=65.2028
	step [160/244], loss=68.0555
	step [161/244], loss=87.2676
	step [162/244], loss=70.3341
	step [163/244], loss=71.6957
	step [164/244], loss=57.8905
	step [165/244], loss=81.2765
	step [166/244], loss=99.8260
	step [167/244], loss=77.8074
	step [168/244], loss=89.1851
	step [169/244], loss=73.9531
	step [170/244], loss=76.4231
	step [171/244], loss=75.7943
	step [172/244], loss=60.0425
	step [173/244], loss=87.2355
	step [174/244], loss=75.4691
	step [175/244], loss=74.8426
	step [176/244], loss=79.5720
	step [177/244], loss=79.1989
	step [178/244], loss=76.3315
	step [179/244], loss=86.9047
	step [180/244], loss=76.1924
	step [181/244], loss=93.8293
	step [182/244], loss=78.7550
	step [183/244], loss=75.9564
	step [184/244], loss=73.0018
	step [185/244], loss=65.6542
	step [186/244], loss=71.9104
	step [187/244], loss=79.1009
	step [188/244], loss=77.7660
	step [189/244], loss=66.8963
	step [190/244], loss=78.1191
	step [191/244], loss=66.2748
	step [192/244], loss=76.9864
	step [193/244], loss=83.5345
	step [194/244], loss=88.7858
	step [195/244], loss=76.9394
	step [196/244], loss=91.0873
	step [197/244], loss=68.2655
	step [198/244], loss=90.8636
	step [199/244], loss=72.4205
	step [200/244], loss=72.7181
	step [201/244], loss=75.1624
	step [202/244], loss=70.1733
	step [203/244], loss=72.7199
	step [204/244], loss=85.5695
	step [205/244], loss=67.4175
	step [206/244], loss=83.1681
	step [207/244], loss=83.8058
	step [208/244], loss=77.4794
	step [209/244], loss=76.0252
	step [210/244], loss=85.2938
	step [211/244], loss=77.7739
	step [212/244], loss=61.4226
	step [213/244], loss=80.3017
	step [214/244], loss=71.1845
	step [215/244], loss=89.3956
	step [216/244], loss=66.6998
	step [217/244], loss=81.1269
	step [218/244], loss=70.2258
	step [219/244], loss=75.2698
	step [220/244], loss=74.6849
	step [221/244], loss=78.2172
	step [222/244], loss=85.9649
	step [223/244], loss=70.7796
	step [224/244], loss=81.2141
	step [225/244], loss=82.2259
	step [226/244], loss=66.2931
	step [227/244], loss=80.1429
	step [228/244], loss=70.1445
	step [229/244], loss=70.6404
	step [230/244], loss=87.3320
	step [231/244], loss=66.8188
	step [232/244], loss=70.6780
	step [233/244], loss=63.2261
	step [234/244], loss=72.0941
	step [235/244], loss=73.5717
	step [236/244], loss=81.8348
	step [237/244], loss=76.3127
	step [238/244], loss=74.4202
	step [239/244], loss=75.9017
	step [240/244], loss=86.0201
	step [241/244], loss=66.8978
	step [242/244], loss=79.9038
	step [243/244], loss=63.2815
	step [244/244], loss=61.8000
	Evaluating
	loss=0.0090, precision=0.3129, recall=0.8665, f1=0.4598
Training epoch 78
	step [1/244], loss=79.8529
	step [2/244], loss=69.4275
	step [3/244], loss=70.0038
	step [4/244], loss=78.7663
	step [5/244], loss=74.8458
	step [6/244], loss=75.4403
	step [7/244], loss=71.3845
	step [8/244], loss=79.7150
	step [9/244], loss=89.2277
	step [10/244], loss=77.2016
	step [11/244], loss=70.9743
	step [12/244], loss=57.8864
	step [13/244], loss=61.0474
	step [14/244], loss=79.8260
	step [15/244], loss=69.9473
	step [16/244], loss=71.5258
	step [17/244], loss=78.4105
	step [18/244], loss=61.3077
	step [19/244], loss=78.0995
	step [20/244], loss=69.9611
	step [21/244], loss=65.8140
	step [22/244], loss=66.2842
	step [23/244], loss=72.2376
	step [24/244], loss=78.8619
	step [25/244], loss=68.1990
	step [26/244], loss=75.7722
	step [27/244], loss=70.6608
	step [28/244], loss=76.3486
	step [29/244], loss=69.6615
	step [30/244], loss=68.7638
	step [31/244], loss=64.0767
	step [32/244], loss=79.6108
	step [33/244], loss=72.1386
	step [34/244], loss=74.4076
	step [35/244], loss=76.2972
	step [36/244], loss=75.6538
	step [37/244], loss=64.8979
	step [38/244], loss=78.9205
	step [39/244], loss=77.8903
	step [40/244], loss=68.7027
	step [41/244], loss=82.9940
	step [42/244], loss=96.1409
	step [43/244], loss=70.1531
	step [44/244], loss=66.3198
	step [45/244], loss=59.8060
	step [46/244], loss=74.7446
	step [47/244], loss=77.8293
	step [48/244], loss=84.3904
	step [49/244], loss=80.6264
	step [50/244], loss=84.0142
	step [51/244], loss=82.4872
	step [52/244], loss=76.4252
	step [53/244], loss=56.9727
	step [54/244], loss=76.0490
	step [55/244], loss=68.7645
	step [56/244], loss=60.5393
	step [57/244], loss=60.2862
	step [58/244], loss=83.7168
	step [59/244], loss=67.9151
	step [60/244], loss=77.7487
	step [61/244], loss=71.8624
	step [62/244], loss=69.2028
	step [63/244], loss=86.9355
	step [64/244], loss=64.4263
	step [65/244], loss=66.9733
	step [66/244], loss=71.7739
	step [67/244], loss=73.4094
	step [68/244], loss=80.5907
	step [69/244], loss=63.9750
	step [70/244], loss=74.0413
	step [71/244], loss=70.6276
	step [72/244], loss=60.1587
	step [73/244], loss=97.5827
	step [74/244], loss=74.9305
	step [75/244], loss=72.7688
	step [76/244], loss=70.1119
	step [77/244], loss=72.1522
	step [78/244], loss=65.0599
	step [79/244], loss=84.0331
	step [80/244], loss=70.8532
	step [81/244], loss=78.6673
	step [82/244], loss=60.7146
	step [83/244], loss=73.6126
	step [84/244], loss=74.9997
	step [85/244], loss=78.3634
	step [86/244], loss=65.8569
	step [87/244], loss=77.8619
	step [88/244], loss=64.9907
	step [89/244], loss=81.8159
	step [90/244], loss=93.1494
	step [91/244], loss=81.9157
	step [92/244], loss=69.8268
	step [93/244], loss=81.7153
	step [94/244], loss=72.1286
	step [95/244], loss=83.7496
	step [96/244], loss=62.7892
	step [97/244], loss=80.7383
	step [98/244], loss=72.7186
	step [99/244], loss=85.2322
	step [100/244], loss=86.9633
	step [101/244], loss=84.7596
	step [102/244], loss=72.9814
	step [103/244], loss=78.8828
	step [104/244], loss=64.8178
	step [105/244], loss=60.6075
	step [106/244], loss=73.6266
	step [107/244], loss=70.9155
	step [108/244], loss=80.5152
	step [109/244], loss=84.6900
	step [110/244], loss=95.4440
	step [111/244], loss=76.6959
	step [112/244], loss=58.2098
	step [113/244], loss=73.2021
	step [114/244], loss=64.2457
	step [115/244], loss=76.4331
	step [116/244], loss=65.7874
	step [117/244], loss=67.8096
	step [118/244], loss=68.6641
	step [119/244], loss=63.8365
	step [120/244], loss=101.4134
	step [121/244], loss=56.8349
	step [122/244], loss=80.3214
	step [123/244], loss=69.5033
	step [124/244], loss=77.3631
	step [125/244], loss=75.3835
	step [126/244], loss=57.2279
	step [127/244], loss=73.7151
	step [128/244], loss=74.3042
	step [129/244], loss=78.6061
	step [130/244], loss=88.7704
	step [131/244], loss=74.6016
	step [132/244], loss=77.5254
	step [133/244], loss=80.3772
	step [134/244], loss=51.5664
	step [135/244], loss=56.7029
	step [136/244], loss=66.4923
	step [137/244], loss=74.0517
	step [138/244], loss=75.0481
	step [139/244], loss=80.3118
	step [140/244], loss=76.0350
	step [141/244], loss=58.5106
	step [142/244], loss=69.4354
	step [143/244], loss=67.6889
	step [144/244], loss=69.0488
	step [145/244], loss=73.9329
	step [146/244], loss=68.0718
	step [147/244], loss=75.2277
	step [148/244], loss=90.6835
	step [149/244], loss=62.6342
	step [150/244], loss=66.5042
	step [151/244], loss=69.1893
	step [152/244], loss=55.5643
	step [153/244], loss=85.8105
	step [154/244], loss=71.3915
	step [155/244], loss=64.4308
	step [156/244], loss=74.9487
	step [157/244], loss=74.8873
	step [158/244], loss=87.4947
	step [159/244], loss=74.3997
	step [160/244], loss=67.6702
	step [161/244], loss=74.7598
	step [162/244], loss=74.4611
	step [163/244], loss=72.8366
	step [164/244], loss=61.0348
	step [165/244], loss=83.2328
	step [166/244], loss=72.8849
	step [167/244], loss=71.0235
	step [168/244], loss=61.1451
	step [169/244], loss=86.4140
	step [170/244], loss=73.8685
	step [171/244], loss=83.6842
	step [172/244], loss=74.4131
	step [173/244], loss=75.7149
	step [174/244], loss=63.4806
	step [175/244], loss=68.3565
	step [176/244], loss=78.7657
	step [177/244], loss=81.4669
	step [178/244], loss=70.6101
	step [179/244], loss=55.4346
	step [180/244], loss=73.2627
	step [181/244], loss=76.3179
	step [182/244], loss=63.5507
	step [183/244], loss=83.3826
	step [184/244], loss=80.1149
	step [185/244], loss=76.6355
	step [186/244], loss=70.6925
	step [187/244], loss=71.6531
	step [188/244], loss=59.6179
	step [189/244], loss=90.8697
	step [190/244], loss=68.8121
	step [191/244], loss=73.3501
	step [192/244], loss=67.0333
	step [193/244], loss=72.7044
	step [194/244], loss=79.6784
	step [195/244], loss=76.6221
	step [196/244], loss=76.0006
	step [197/244], loss=66.6927
	step [198/244], loss=72.8367
	step [199/244], loss=64.9378
	step [200/244], loss=76.8911
	step [201/244], loss=92.0184
	step [202/244], loss=70.4504
	step [203/244], loss=76.4429
	step [204/244], loss=77.2448
	step [205/244], loss=64.3498
	step [206/244], loss=72.9095
	step [207/244], loss=79.8677
	step [208/244], loss=70.2928
	step [209/244], loss=70.3951
	step [210/244], loss=69.2775
	step [211/244], loss=71.7129
	step [212/244], loss=93.0674
	step [213/244], loss=81.0267
	step [214/244], loss=71.4969
	step [215/244], loss=78.3018
	step [216/244], loss=83.6983
	step [217/244], loss=54.9165
	step [218/244], loss=70.3809
	step [219/244], loss=86.7929
	step [220/244], loss=73.3533
	step [221/244], loss=82.8311
	step [222/244], loss=69.7080
	step [223/244], loss=81.0565
	step [224/244], loss=69.0213
	step [225/244], loss=63.2691
	step [226/244], loss=81.9668
	step [227/244], loss=67.4120
	step [228/244], loss=85.5523
	step [229/244], loss=71.1684
	step [230/244], loss=68.0549
	step [231/244], loss=77.6573
	step [232/244], loss=73.9068
	step [233/244], loss=76.8204
	step [234/244], loss=63.1413
	step [235/244], loss=89.0523
	step [236/244], loss=77.6548
	step [237/244], loss=76.4146
	step [238/244], loss=74.3244
	step [239/244], loss=78.0466
	step [240/244], loss=81.5003
	step [241/244], loss=82.5317
	step [242/244], loss=66.4926
	step [243/244], loss=71.6691
	step [244/244], loss=67.9021
	Evaluating
	loss=0.0076, precision=0.3595, recall=0.8533, f1=0.5059
saving model as: 1_saved_model.pth
Training epoch 79
	step [1/244], loss=65.9076
	step [2/244], loss=77.3598
	step [3/244], loss=70.7711
	step [4/244], loss=91.6518
	step [5/244], loss=80.7361
	step [6/244], loss=77.3808
	step [7/244], loss=61.5311
	step [8/244], loss=59.9680
	step [9/244], loss=59.7144
	step [10/244], loss=66.4725
	step [11/244], loss=87.0730
	step [12/244], loss=65.2443
	step [13/244], loss=77.2926
	step [14/244], loss=70.7309
	step [15/244], loss=81.8076
	step [16/244], loss=79.1059
	step [17/244], loss=72.1321
	step [18/244], loss=70.4244
	step [19/244], loss=67.3140
	step [20/244], loss=83.1503
	step [21/244], loss=86.7253
	step [22/244], loss=86.4408
	step [23/244], loss=69.4843
	step [24/244], loss=79.1802
	step [25/244], loss=57.5223
	step [26/244], loss=63.5787
	step [27/244], loss=69.3314
	step [28/244], loss=88.7621
	step [29/244], loss=62.2852
	step [30/244], loss=63.5843
	step [31/244], loss=74.1349
	step [32/244], loss=64.3475
	step [33/244], loss=61.2456
	step [34/244], loss=74.4618
	step [35/244], loss=66.1812
	step [36/244], loss=71.2056
	step [37/244], loss=62.0252
	step [38/244], loss=77.9453
	step [39/244], loss=77.8440
	step [40/244], loss=66.0828
	step [41/244], loss=76.4966
	step [42/244], loss=64.6283
	step [43/244], loss=69.7595
	step [44/244], loss=80.7694
	step [45/244], loss=60.1277
	step [46/244], loss=68.4555
	step [47/244], loss=69.7878
	step [48/244], loss=83.6078
	step [49/244], loss=55.9447
	step [50/244], loss=71.5344
	step [51/244], loss=69.0732
	step [52/244], loss=80.5172
	step [53/244], loss=91.1517
	step [54/244], loss=90.5279
	step [55/244], loss=68.1919
	step [56/244], loss=65.7240
	step [57/244], loss=84.5982
	step [58/244], loss=66.8105
	step [59/244], loss=61.0346
	step [60/244], loss=74.1320
	step [61/244], loss=76.7575
	step [62/244], loss=69.9704
	step [63/244], loss=75.8366
	step [64/244], loss=78.2228
	step [65/244], loss=68.5182
	step [66/244], loss=75.9744
	step [67/244], loss=69.2226
	step [68/244], loss=75.2141
	step [69/244], loss=68.1714
	step [70/244], loss=63.6358
	step [71/244], loss=70.2228
	step [72/244], loss=78.1994
	step [73/244], loss=79.2146
	step [74/244], loss=99.1730
	step [75/244], loss=67.8343
	step [76/244], loss=77.1707
	step [77/244], loss=58.4611
	step [78/244], loss=75.7850
	step [79/244], loss=107.3162
	step [80/244], loss=69.8876
	step [81/244], loss=73.3861
	step [82/244], loss=71.5122
	step [83/244], loss=81.8250
	step [84/244], loss=75.2201
	step [85/244], loss=80.9857
	step [86/244], loss=65.8352
	step [87/244], loss=102.2568
	step [88/244], loss=65.2886
	step [89/244], loss=78.3636
	step [90/244], loss=79.7215
	step [91/244], loss=75.9127
	step [92/244], loss=80.5654
	step [93/244], loss=68.6870
	step [94/244], loss=71.5152
	step [95/244], loss=86.8425
	step [96/244], loss=67.3900
	step [97/244], loss=62.4847
	step [98/244], loss=74.4775
	step [99/244], loss=66.8682
	step [100/244], loss=75.8581
	step [101/244], loss=64.2505
	step [102/244], loss=79.7230
	step [103/244], loss=73.4212
	step [104/244], loss=86.9610
	step [105/244], loss=80.9130
	step [106/244], loss=65.5736
	step [107/244], loss=62.2078
	step [108/244], loss=80.9275
	step [109/244], loss=63.2643
	step [110/244], loss=74.7438
	step [111/244], loss=68.2164
	step [112/244], loss=79.0456
	step [113/244], loss=73.8188
	step [114/244], loss=71.1189
	step [115/244], loss=76.2101
	step [116/244], loss=85.2465
	step [117/244], loss=63.4006
	step [118/244], loss=67.6193
	step [119/244], loss=58.2063
	step [120/244], loss=70.9604
	step [121/244], loss=88.8965
	step [122/244], loss=76.8020
	step [123/244], loss=66.8549
	step [124/244], loss=74.2826
	step [125/244], loss=75.7883
	step [126/244], loss=78.5322
	step [127/244], loss=82.9545
	step [128/244], loss=76.9220
	step [129/244], loss=66.0008
	step [130/244], loss=63.0477
	step [131/244], loss=82.8166
	step [132/244], loss=74.5808
	step [133/244], loss=74.5670
	step [134/244], loss=70.7610
	step [135/244], loss=71.4944
	step [136/244], loss=83.8611
	step [137/244], loss=87.9068
	step [138/244], loss=75.2864
	step [139/244], loss=71.4222
	step [140/244], loss=80.4287
	step [141/244], loss=63.4073
	step [142/244], loss=85.6322
	step [143/244], loss=88.5094
	step [144/244], loss=82.6688
	step [145/244], loss=65.1775
	step [146/244], loss=72.6096
	step [147/244], loss=78.2789
	step [148/244], loss=82.9939
	step [149/244], loss=58.3457
	step [150/244], loss=64.7377
	step [151/244], loss=79.0982
	step [152/244], loss=76.6270
	step [153/244], loss=66.7328
	step [154/244], loss=77.5986
	step [155/244], loss=77.3656
	step [156/244], loss=77.6520
	step [157/244], loss=75.8153
	step [158/244], loss=70.8631
	step [159/244], loss=90.5742
	step [160/244], loss=63.0771
	step [161/244], loss=72.2983
	step [162/244], loss=61.7135
	step [163/244], loss=62.8296
	step [164/244], loss=71.6216
	step [165/244], loss=86.7331
	step [166/244], loss=68.1507
	step [167/244], loss=69.9939
	step [168/244], loss=73.5451
	step [169/244], loss=64.8148
	step [170/244], loss=53.1557
	step [171/244], loss=61.7716
	step [172/244], loss=87.6948
	step [173/244], loss=80.4049
	step [174/244], loss=70.9256
	step [175/244], loss=77.3627
	step [176/244], loss=74.1286
	step [177/244], loss=79.6308
	step [178/244], loss=78.5708
	step [179/244], loss=65.9218
	step [180/244], loss=81.8249
	step [181/244], loss=68.4095
	step [182/244], loss=83.3980
	step [183/244], loss=68.3991
	step [184/244], loss=66.4739
	step [185/244], loss=74.0984
	step [186/244], loss=57.6305
	step [187/244], loss=74.7564
	step [188/244], loss=65.6516
	step [189/244], loss=74.6923
	step [190/244], loss=79.2219
	step [191/244], loss=83.0885
	step [192/244], loss=79.8211
	step [193/244], loss=63.7503
	step [194/244], loss=68.8223
	step [195/244], loss=78.3254
	step [196/244], loss=66.4420
	step [197/244], loss=67.8508
	step [198/244], loss=54.9780
	step [199/244], loss=82.8712
	step [200/244], loss=79.7180
	step [201/244], loss=71.6577
	step [202/244], loss=71.0368
	step [203/244], loss=83.0976
	step [204/244], loss=77.1300
	step [205/244], loss=69.1474
	step [206/244], loss=72.2154
	step [207/244], loss=76.3237
	step [208/244], loss=67.9860
	step [209/244], loss=73.2647
	step [210/244], loss=78.6439
	step [211/244], loss=66.5681
	step [212/244], loss=71.7811
	step [213/244], loss=81.1971
	step [214/244], loss=75.5165
	step [215/244], loss=71.9905
	step [216/244], loss=61.0692
	step [217/244], loss=73.1130
	step [218/244], loss=82.5686
	step [219/244], loss=63.1678
	step [220/244], loss=72.4042
	step [221/244], loss=68.0367
	step [222/244], loss=85.9610
	step [223/244], loss=79.5666
	step [224/244], loss=76.9965
	step [225/244], loss=61.3921
	step [226/244], loss=61.2799
	step [227/244], loss=66.9156
	step [228/244], loss=80.7806
	step [229/244], loss=55.6326
	step [230/244], loss=65.3402
	step [231/244], loss=69.9532
	step [232/244], loss=93.6929
	step [233/244], loss=62.2853
	step [234/244], loss=82.0263
	step [235/244], loss=71.5495
	step [236/244], loss=89.3022
	step [237/244], loss=81.1536
	step [238/244], loss=73.2960
	step [239/244], loss=78.3628
	step [240/244], loss=66.8724
	step [241/244], loss=68.4687
	step [242/244], loss=79.1509
	step [243/244], loss=60.7951
	step [244/244], loss=66.4060
	Evaluating
	loss=0.0075, precision=0.3510, recall=0.8508, f1=0.4969
Training epoch 80
	step [1/244], loss=77.8901
	step [2/244], loss=81.7262
	step [3/244], loss=91.2526
	step [4/244], loss=80.5986
	step [5/244], loss=71.4430
	step [6/244], loss=93.8787
	step [7/244], loss=92.1607
	step [8/244], loss=73.0071
	step [9/244], loss=61.6266
	step [10/244], loss=71.9134
	step [11/244], loss=81.0394
	step [12/244], loss=61.0274
	step [13/244], loss=73.3024
	step [14/244], loss=79.2927
	step [15/244], loss=72.6897
	step [16/244], loss=70.6968
	step [17/244], loss=76.4479
	step [18/244], loss=57.6307
	step [19/244], loss=73.5632
	step [20/244], loss=76.1049
	step [21/244], loss=59.7791
	step [22/244], loss=76.9193
	step [23/244], loss=66.1774
	step [24/244], loss=72.4533
	step [25/244], loss=90.5891
	step [26/244], loss=62.4674
	step [27/244], loss=80.8454
	step [28/244], loss=83.0935
	step [29/244], loss=71.2398
	step [30/244], loss=86.7590
	step [31/244], loss=74.9690
	step [32/244], loss=66.1696
	step [33/244], loss=74.3856
	step [34/244], loss=71.1776
	step [35/244], loss=79.4665
	step [36/244], loss=86.2212
	step [37/244], loss=78.6541
	step [38/244], loss=79.7766
	step [39/244], loss=64.5260
	step [40/244], loss=75.0926
	step [41/244], loss=74.7853
	step [42/244], loss=81.8344
	step [43/244], loss=77.1613
	step [44/244], loss=87.8144
	step [45/244], loss=68.5350
	step [46/244], loss=67.9933
	step [47/244], loss=71.2902
	step [48/244], loss=70.3915
	step [49/244], loss=65.5462
	step [50/244], loss=58.8635
	step [51/244], loss=68.8842
	step [52/244], loss=60.5130
	step [53/244], loss=67.6669
	step [54/244], loss=73.8445
	step [55/244], loss=66.3279
	step [56/244], loss=66.4347
	step [57/244], loss=64.5496
	step [58/244], loss=70.0913
	step [59/244], loss=71.6138
	step [60/244], loss=64.2950
	step [61/244], loss=69.4797
	step [62/244], loss=74.0766
	step [63/244], loss=69.9563
	step [64/244], loss=73.7700
	step [65/244], loss=57.3098
	step [66/244], loss=79.1936
	step [67/244], loss=73.2469
	step [68/244], loss=73.1726
	step [69/244], loss=66.2849
	step [70/244], loss=74.8628
	step [71/244], loss=68.8982
	step [72/244], loss=89.4258
	step [73/244], loss=73.9851
	step [74/244], loss=77.4636
	step [75/244], loss=74.1694
	step [76/244], loss=74.4363
	step [77/244], loss=65.1792
	step [78/244], loss=69.2679
	step [79/244], loss=73.8670
	step [80/244], loss=63.3720
	step [81/244], loss=67.1363
	step [82/244], loss=58.5693
	step [83/244], loss=63.0327
	step [84/244], loss=71.7253
	step [85/244], loss=82.2688
	step [86/244], loss=72.8437
	step [87/244], loss=75.5681
	step [88/244], loss=77.3617
	step [89/244], loss=68.0628
	step [90/244], loss=76.7859
	step [91/244], loss=60.7362
	step [92/244], loss=63.4510
	step [93/244], loss=58.9023
	step [94/244], loss=64.7815
	step [95/244], loss=66.7029
	step [96/244], loss=75.1844
	step [97/244], loss=62.3409
	step [98/244], loss=51.3519
	step [99/244], loss=62.9092
	step [100/244], loss=77.9363
	step [101/244], loss=72.1331
	step [102/244], loss=71.3278
	step [103/244], loss=68.8393
	step [104/244], loss=66.6276
	step [105/244], loss=63.5277
	step [106/244], loss=70.9100
	step [107/244], loss=93.7216
	step [108/244], loss=56.3708
	step [109/244], loss=59.9534
	step [110/244], loss=64.1884
	step [111/244], loss=67.9848
	step [112/244], loss=69.7081
	step [113/244], loss=81.5353
	step [114/244], loss=81.5459
	step [115/244], loss=70.4182
	step [116/244], loss=74.5713
	step [117/244], loss=73.8381
	step [118/244], loss=64.7793
	step [119/244], loss=77.9329
	step [120/244], loss=89.3194
	step [121/244], loss=79.8432
	step [122/244], loss=82.1007
	step [123/244], loss=74.3707
	step [124/244], loss=76.0690
	step [125/244], loss=58.0018
	step [126/244], loss=72.2526
	step [127/244], loss=87.4861
	step [128/244], loss=78.6224
	step [129/244], loss=77.1315
	step [130/244], loss=80.7511
	step [131/244], loss=74.7324
	step [132/244], loss=82.7540
	step [133/244], loss=60.4112
	step [134/244], loss=101.9800
	step [135/244], loss=64.0153
	step [136/244], loss=74.6550
	step [137/244], loss=68.5138
	step [138/244], loss=73.5068
	step [139/244], loss=66.2675
	step [140/244], loss=73.4756
	step [141/244], loss=80.8935
	step [142/244], loss=63.8109
	step [143/244], loss=76.4437
	step [144/244], loss=54.4549
	step [145/244], loss=67.7754
	step [146/244], loss=77.8153
	step [147/244], loss=70.2805
	step [148/244], loss=65.5081
	step [149/244], loss=57.5211
	step [150/244], loss=61.0413
	step [151/244], loss=81.3250
	step [152/244], loss=57.9966
	step [153/244], loss=70.9221
	step [154/244], loss=66.4328
	step [155/244], loss=70.5460
	step [156/244], loss=91.8510
	step [157/244], loss=65.4613
	step [158/244], loss=62.3838
	step [159/244], loss=69.8726
	step [160/244], loss=82.1500
	step [161/244], loss=73.7974
	step [162/244], loss=107.2350
	step [163/244], loss=63.0084
	step [164/244], loss=89.9651
	step [165/244], loss=68.9119
	step [166/244], loss=78.5396
	step [167/244], loss=73.3575
	step [168/244], loss=70.6522
	step [169/244], loss=89.3483
	step [170/244], loss=71.9648
	step [171/244], loss=79.0432
	step [172/244], loss=68.9182
	step [173/244], loss=77.6513
	step [174/244], loss=67.6968
	step [175/244], loss=79.6345
	step [176/244], loss=68.2173
	step [177/244], loss=67.0347
	step [178/244], loss=65.6541
	step [179/244], loss=74.2569
	step [180/244], loss=81.7472
	step [181/244], loss=80.4335
	step [182/244], loss=88.0483
	step [183/244], loss=75.3911
	step [184/244], loss=65.2709
	step [185/244], loss=75.3102
	step [186/244], loss=76.5663
	step [187/244], loss=72.5372
	step [188/244], loss=72.8050
	step [189/244], loss=74.3577
	step [190/244], loss=82.6311
	step [191/244], loss=75.3515
	step [192/244], loss=67.9082
	step [193/244], loss=62.6865
	step [194/244], loss=67.5696
	step [195/244], loss=68.9294
	step [196/244], loss=83.7685
	step [197/244], loss=71.9386
	step [198/244], loss=90.1439
	step [199/244], loss=72.0844
	step [200/244], loss=79.0903
	step [201/244], loss=73.4700
	step [202/244], loss=66.0774
	step [203/244], loss=94.3261
	step [204/244], loss=75.4167
	step [205/244], loss=68.4149
	step [206/244], loss=61.1790
	step [207/244], loss=97.7715
	step [208/244], loss=87.9943
	step [209/244], loss=82.9268
	step [210/244], loss=85.2489
	step [211/244], loss=68.4804
	step [212/244], loss=58.7159
	step [213/244], loss=59.7190
	step [214/244], loss=78.3359
	step [215/244], loss=79.7214
	step [216/244], loss=69.5679
	step [217/244], loss=65.1417
	step [218/244], loss=75.0447
	step [219/244], loss=63.0807
	step [220/244], loss=71.2846
	step [221/244], loss=56.9352
	step [222/244], loss=90.3528
	step [223/244], loss=73.2245
	step [224/244], loss=66.8239
	step [225/244], loss=81.5331
	step [226/244], loss=76.5012
	step [227/244], loss=82.9467
	step [228/244], loss=47.9966
	step [229/244], loss=83.9919
	step [230/244], loss=69.4624
	step [231/244], loss=73.6591
	step [232/244], loss=71.5613
	step [233/244], loss=76.0973
	step [234/244], loss=76.2606
	step [235/244], loss=55.2521
	step [236/244], loss=65.7028
	step [237/244], loss=72.3902
	step [238/244], loss=85.2351
	step [239/244], loss=86.0281
	step [240/244], loss=91.2008
	step [241/244], loss=72.2581
	step [242/244], loss=87.2290
	step [243/244], loss=92.5529
	step [244/244], loss=90.9064
	Evaluating
	loss=0.0078, precision=0.3423, recall=0.8545, f1=0.4888
Training epoch 81
	step [1/244], loss=72.1831
	step [2/244], loss=89.2452
	step [3/244], loss=77.0074
	step [4/244], loss=77.0944
	step [5/244], loss=75.6985
	step [6/244], loss=69.4127
	step [7/244], loss=66.0258
	step [8/244], loss=77.1693
	step [9/244], loss=75.3684
	step [10/244], loss=66.0177
	step [11/244], loss=76.1162
	step [12/244], loss=64.2864
	step [13/244], loss=66.7692
	step [14/244], loss=57.7795
	step [15/244], loss=86.4552
	step [16/244], loss=88.3775
	step [17/244], loss=69.7182
	step [18/244], loss=65.0121
	step [19/244], loss=67.4212
	step [20/244], loss=83.8458
	step [21/244], loss=76.2493
	step [22/244], loss=69.1133
	step [23/244], loss=61.3487
	step [24/244], loss=74.9673
	step [25/244], loss=68.9058
	step [26/244], loss=66.6361
	step [27/244], loss=76.5394
	step [28/244], loss=56.4229
	step [29/244], loss=64.3925
	step [30/244], loss=62.3171
	step [31/244], loss=57.2564
	step [32/244], loss=75.7415
	step [33/244], loss=82.2651
	step [34/244], loss=64.6485
	step [35/244], loss=73.6262
	step [36/244], loss=70.3574
	step [37/244], loss=67.2031
	step [38/244], loss=77.8809
	step [39/244], loss=57.8660
	step [40/244], loss=66.2430
	step [41/244], loss=65.7256
	step [42/244], loss=72.9592
	step [43/244], loss=60.8774
	step [44/244], loss=65.1522
	step [45/244], loss=76.6445
	step [46/244], loss=68.8176
	step [47/244], loss=71.0804
	step [48/244], loss=70.2924
	step [49/244], loss=72.8952
	step [50/244], loss=60.1761
	step [51/244], loss=77.8898
	step [52/244], loss=64.1107
	step [53/244], loss=60.7613
	step [54/244], loss=66.8376
	step [55/244], loss=82.4784
	step [56/244], loss=82.7778
	step [57/244], loss=82.5441
	step [58/244], loss=70.4662
	step [59/244], loss=76.2859
	step [60/244], loss=70.2080
	step [61/244], loss=77.1554
	step [62/244], loss=74.8136
	step [63/244], loss=77.3410
	step [64/244], loss=85.0442
	step [65/244], loss=77.5017
	step [66/244], loss=82.8616
	step [67/244], loss=80.6661
	step [68/244], loss=71.8371
	step [69/244], loss=62.0561
	step [70/244], loss=67.3814
	step [71/244], loss=76.0295
	step [72/244], loss=62.6919
	step [73/244], loss=67.7799
	step [74/244], loss=77.6631
	step [75/244], loss=72.8811
	step [76/244], loss=62.8290
	step [77/244], loss=72.0532
	step [78/244], loss=83.6038
	step [79/244], loss=72.8148
	step [80/244], loss=72.3212
	step [81/244], loss=62.5118
	step [82/244], loss=81.6926
	step [83/244], loss=89.7068
	step [84/244], loss=74.1240
	step [85/244], loss=76.5031
	step [86/244], loss=77.9758
	step [87/244], loss=90.3167
	step [88/244], loss=67.8264
	step [89/244], loss=69.8440
	step [90/244], loss=51.4899
	step [91/244], loss=63.4119
	step [92/244], loss=66.1122
	step [93/244], loss=71.9293
	step [94/244], loss=78.8272
	step [95/244], loss=66.2060
	step [96/244], loss=75.8408
	step [97/244], loss=76.0949
	step [98/244], loss=62.7785
	step [99/244], loss=68.9948
	step [100/244], loss=55.3530
	step [101/244], loss=65.8379
	step [102/244], loss=79.9942
	step [103/244], loss=85.3875
	step [104/244], loss=76.9443
	step [105/244], loss=83.5249
	step [106/244], loss=66.1083
	step [107/244], loss=73.0441
	step [108/244], loss=75.9075
	step [109/244], loss=78.7269
	step [110/244], loss=74.5704
	step [111/244], loss=55.2143
	step [112/244], loss=64.9746
	step [113/244], loss=67.5011
	step [114/244], loss=67.0481
	step [115/244], loss=63.0101
	step [116/244], loss=69.2878
	step [117/244], loss=66.8294
	step [118/244], loss=74.4062
	step [119/244], loss=80.6722
	step [120/244], loss=94.6897
	step [121/244], loss=81.7573
	step [122/244], loss=64.9209
	step [123/244], loss=82.4344
	step [124/244], loss=88.5278
	step [125/244], loss=81.4643
	step [126/244], loss=92.0665
	step [127/244], loss=72.2996
	step [128/244], loss=83.0206
	step [129/244], loss=61.5897
	step [130/244], loss=80.1932
	step [131/244], loss=73.5282
	step [132/244], loss=67.2895
	step [133/244], loss=64.2690
	step [134/244], loss=75.9026
	step [135/244], loss=67.5706
	step [136/244], loss=60.3712
	step [137/244], loss=68.1344
	step [138/244], loss=62.7079
	step [139/244], loss=86.2575
	step [140/244], loss=82.7031
	step [141/244], loss=68.1393
	step [142/244], loss=80.8345
	step [143/244], loss=68.4007
	step [144/244], loss=70.0206
	step [145/244], loss=72.1458
	step [146/244], loss=67.6547
	step [147/244], loss=64.6530
	step [148/244], loss=64.7023
	step [149/244], loss=54.8635
	step [150/244], loss=75.2927
	step [151/244], loss=64.3930
	step [152/244], loss=82.5491
	step [153/244], loss=80.5166
	step [154/244], loss=85.5209
	step [155/244], loss=68.0124
	step [156/244], loss=65.8410
	step [157/244], loss=76.9360
	step [158/244], loss=72.7231
	step [159/244], loss=60.7953
	step [160/244], loss=88.4356
	step [161/244], loss=53.1031
	step [162/244], loss=89.3353
	step [163/244], loss=46.2108
	step [164/244], loss=73.7534
	step [165/244], loss=87.5931
	step [166/244], loss=76.1901
	step [167/244], loss=81.6270
	step [168/244], loss=75.7084
	step [169/244], loss=75.4946
	step [170/244], loss=70.8845
	step [171/244], loss=69.6803
	step [172/244], loss=70.3856
	step [173/244], loss=69.6674
	step [174/244], loss=81.0387
	step [175/244], loss=73.8009
	step [176/244], loss=69.8293
	step [177/244], loss=70.9455
	step [178/244], loss=75.2374
	step [179/244], loss=70.2325
	step [180/244], loss=65.1011
	step [181/244], loss=58.8415
	step [182/244], loss=78.9683
	step [183/244], loss=83.5980
	step [184/244], loss=62.7758
	step [185/244], loss=80.0091
	step [186/244], loss=78.0677
	step [187/244], loss=74.7392
	step [188/244], loss=74.4797
	step [189/244], loss=56.6075
	step [190/244], loss=86.7814
	step [191/244], loss=81.8460
	step [192/244], loss=62.8535
	step [193/244], loss=61.7096
	step [194/244], loss=62.8156
	step [195/244], loss=70.7950
	step [196/244], loss=87.0801
	step [197/244], loss=73.6452
	step [198/244], loss=88.0239
	step [199/244], loss=64.3889
	step [200/244], loss=74.2607
	step [201/244], loss=87.0538
	step [202/244], loss=68.0780
	step [203/244], loss=58.4244
	step [204/244], loss=72.0632
	step [205/244], loss=76.8192
	step [206/244], loss=79.1177
	step [207/244], loss=65.5735
	step [208/244], loss=84.1790
	step [209/244], loss=79.3972
	step [210/244], loss=83.1985
	step [211/244], loss=78.4894
	step [212/244], loss=83.9462
	step [213/244], loss=90.8513
	step [214/244], loss=57.8869
	step [215/244], loss=80.0516
	step [216/244], loss=80.4511
	step [217/244], loss=78.1368
	step [218/244], loss=73.7209
	step [219/244], loss=81.6696
	step [220/244], loss=63.7516
	step [221/244], loss=73.4082
	step [222/244], loss=78.6736
	step [223/244], loss=80.8793
	step [224/244], loss=73.8688
	step [225/244], loss=63.8506
	step [226/244], loss=79.9537
	step [227/244], loss=88.1027
	step [228/244], loss=66.8168
	step [229/244], loss=77.4948
	step [230/244], loss=72.4922
	step [231/244], loss=78.0855
	step [232/244], loss=63.7269
	step [233/244], loss=72.0611
	step [234/244], loss=68.4155
	step [235/244], loss=69.1313
	step [236/244], loss=66.3834
	step [237/244], loss=81.8252
	step [238/244], loss=75.6622
	step [239/244], loss=76.6823
	step [240/244], loss=72.5112
	step [241/244], loss=74.5758
	step [242/244], loss=74.2196
	step [243/244], loss=77.9498
	step [244/244], loss=67.6141
	Evaluating
	loss=0.0077, precision=0.3489, recall=0.8608, f1=0.4965
Training epoch 82
	step [1/244], loss=76.8175
	step [2/244], loss=76.3187
	step [3/244], loss=78.8255
	step [4/244], loss=86.1299
	step [5/244], loss=81.7460
	step [6/244], loss=78.2177
	step [7/244], loss=82.4123
	step [8/244], loss=67.3979
	step [9/244], loss=64.3564
	step [10/244], loss=60.7706
	step [11/244], loss=59.2120
	step [12/244], loss=92.3194
	step [13/244], loss=76.9124
	step [14/244], loss=71.7717
	step [15/244], loss=58.3805
	step [16/244], loss=70.5685
	step [17/244], loss=72.5746
	step [18/244], loss=78.0879
	step [19/244], loss=60.6457
	step [20/244], loss=75.5038
	step [21/244], loss=78.6186
	step [22/244], loss=71.7467
	step [23/244], loss=67.4599
	step [24/244], loss=65.6627
	step [25/244], loss=73.6183
	step [26/244], loss=65.3212
	step [27/244], loss=72.3628
	step [28/244], loss=79.2938
	step [29/244], loss=76.6498
	step [30/244], loss=69.1878
	step [31/244], loss=59.9917
	step [32/244], loss=72.7989
	step [33/244], loss=71.9004
	step [34/244], loss=74.2759
	step [35/244], loss=76.1584
	step [36/244], loss=68.5384
	step [37/244], loss=76.2291
	step [38/244], loss=82.7197
	step [39/244], loss=63.7685
	step [40/244], loss=69.9151
	step [41/244], loss=68.7570
	step [42/244], loss=71.9777
	step [43/244], loss=82.7771
	step [44/244], loss=63.5584
	step [45/244], loss=61.1190
	step [46/244], loss=75.9249
	step [47/244], loss=70.7677
	step [48/244], loss=78.3806
	step [49/244], loss=67.8851
	step [50/244], loss=79.4673
	step [51/244], loss=74.1937
	step [52/244], loss=96.8139
	step [53/244], loss=73.8886
	step [54/244], loss=59.8143
	step [55/244], loss=59.1876
	step [56/244], loss=57.7421
	step [57/244], loss=67.1464
	step [58/244], loss=64.1368
	step [59/244], loss=74.2251
	step [60/244], loss=78.5401
	step [61/244], loss=69.2665
	step [62/244], loss=73.4753
	step [63/244], loss=71.4636
	step [64/244], loss=64.0988
	step [65/244], loss=89.3661
	step [66/244], loss=83.2476
	step [67/244], loss=69.6407
	step [68/244], loss=70.1025
	step [69/244], loss=56.7769
	step [70/244], loss=80.2912
	step [71/244], loss=85.8674
	step [72/244], loss=71.9737
	step [73/244], loss=79.6824
	step [74/244], loss=74.6037
	step [75/244], loss=71.3626
	step [76/244], loss=76.8879
	step [77/244], loss=81.2911
	step [78/244], loss=88.9908
	step [79/244], loss=80.3764
	step [80/244], loss=51.3546
	step [81/244], loss=82.0920
	step [82/244], loss=72.2938
	step [83/244], loss=76.3586
	step [84/244], loss=61.3694
	step [85/244], loss=60.5849
	step [86/244], loss=77.3539
	step [87/244], loss=80.5611
	step [88/244], loss=67.7710
	step [89/244], loss=70.9650
	step [90/244], loss=71.6800
	step [91/244], loss=76.0770
	step [92/244], loss=67.1309
	step [93/244], loss=70.4843
	step [94/244], loss=78.5205
	step [95/244], loss=67.8805
	step [96/244], loss=96.1442
	step [97/244], loss=75.2767
	step [98/244], loss=75.1476
	step [99/244], loss=66.9037
	step [100/244], loss=74.7988
	step [101/244], loss=71.4251
	step [102/244], loss=87.3210
	step [103/244], loss=62.8958
	step [104/244], loss=63.2024
	step [105/244], loss=72.1590
	step [106/244], loss=92.5715
	step [107/244], loss=62.9018
	step [108/244], loss=80.0122
	step [109/244], loss=61.2342
	step [110/244], loss=73.0720
	step [111/244], loss=74.4757
	step [112/244], loss=77.6193
	step [113/244], loss=63.4956
	step [114/244], loss=64.6883
	step [115/244], loss=87.1145
	step [116/244], loss=65.0318
	step [117/244], loss=78.3338
	step [118/244], loss=64.5933
	step [119/244], loss=74.2225
	step [120/244], loss=85.2297
	step [121/244], loss=85.5888
	step [122/244], loss=65.4578
	step [123/244], loss=77.4403
	step [124/244], loss=68.0159
	step [125/244], loss=70.3063
	step [126/244], loss=78.1729
	step [127/244], loss=66.7764
	step [128/244], loss=78.5457
	step [129/244], loss=73.0801
	step [130/244], loss=65.0970
	step [131/244], loss=82.1802
	step [132/244], loss=79.3231
	step [133/244], loss=77.0364
	step [134/244], loss=61.4426
	step [135/244], loss=83.7667
	step [136/244], loss=64.5463
	step [137/244], loss=74.2865
	step [138/244], loss=62.0780
	step [139/244], loss=89.5761
	step [140/244], loss=66.4690
	step [141/244], loss=70.9382
	step [142/244], loss=89.7231
	step [143/244], loss=76.5432
	step [144/244], loss=77.5800
	step [145/244], loss=62.9502
	step [146/244], loss=72.8487
	step [147/244], loss=54.4972
	step [148/244], loss=64.4923
	step [149/244], loss=62.1838
	step [150/244], loss=72.5702
	step [151/244], loss=72.4931
	step [152/244], loss=83.4009
	step [153/244], loss=62.9930
	step [154/244], loss=68.5358
	step [155/244], loss=84.2739
	step [156/244], loss=66.9697
	step [157/244], loss=71.3952
	step [158/244], loss=71.2604
	step [159/244], loss=60.9422
	step [160/244], loss=63.0519
	step [161/244], loss=57.6754
	step [162/244], loss=73.5935
	step [163/244], loss=58.4380
	step [164/244], loss=86.6374
	step [165/244], loss=88.2819
	step [166/244], loss=59.3572
	step [167/244], loss=66.9140
	step [168/244], loss=66.0735
	step [169/244], loss=59.6591
	step [170/244], loss=83.5721
	step [171/244], loss=64.4283
	step [172/244], loss=70.9997
	step [173/244], loss=75.1666
	step [174/244], loss=74.1661
	step [175/244], loss=64.9855
	step [176/244], loss=74.3072
	step [177/244], loss=66.6361
	step [178/244], loss=82.9131
	step [179/244], loss=86.8840
	step [180/244], loss=65.2399
	step [181/244], loss=62.6003
	step [182/244], loss=72.8612
	step [183/244], loss=76.1093
	step [184/244], loss=75.2410
	step [185/244], loss=73.4359
	step [186/244], loss=76.9217
	step [187/244], loss=61.8279
	step [188/244], loss=69.1117
	step [189/244], loss=73.2583
	step [190/244], loss=68.8770
	step [191/244], loss=57.3560
	step [192/244], loss=84.5347
	step [193/244], loss=69.0787
	step [194/244], loss=91.8148
	step [195/244], loss=60.5179
	step [196/244], loss=74.5796
	step [197/244], loss=72.0837
	step [198/244], loss=74.5302
	step [199/244], loss=72.1253
	step [200/244], loss=73.5695
	step [201/244], loss=55.0279
	step [202/244], loss=92.8419
	step [203/244], loss=86.8696
	step [204/244], loss=77.1850
	step [205/244], loss=64.1571
	step [206/244], loss=82.1882
	step [207/244], loss=75.8548
	step [208/244], loss=66.8799
	step [209/244], loss=60.8060
	step [210/244], loss=68.5859
	step [211/244], loss=73.6734
	step [212/244], loss=77.5801
	step [213/244], loss=75.1447
	step [214/244], loss=81.2495
	step [215/244], loss=94.9061
	step [216/244], loss=83.6421
	step [217/244], loss=61.5681
	step [218/244], loss=93.3856
	step [219/244], loss=79.1920
	step [220/244], loss=68.2825
	step [221/244], loss=70.5311
	step [222/244], loss=70.7857
	step [223/244], loss=85.6736
	step [224/244], loss=64.9150
	step [225/244], loss=58.3133
	step [226/244], loss=64.9845
	step [227/244], loss=72.4406
	step [228/244], loss=85.1894
	step [229/244], loss=78.8652
	step [230/244], loss=59.7753
	step [231/244], loss=69.4248
	step [232/244], loss=73.7758
	step [233/244], loss=79.4450
	step [234/244], loss=75.3652
	step [235/244], loss=78.6349
	step [236/244], loss=53.1672
	step [237/244], loss=79.2276
	step [238/244], loss=58.0566
	step [239/244], loss=71.1488
	step [240/244], loss=79.0891
	step [241/244], loss=76.6642
	step [242/244], loss=69.6767
	step [243/244], loss=53.8545
	step [244/244], loss=78.3001
	Evaluating
	loss=0.0072, precision=0.3829, recall=0.8496, f1=0.5279
saving model as: 1_saved_model.pth
Training epoch 83
	step [1/244], loss=51.5593
	step [2/244], loss=58.2208
	step [3/244], loss=73.8204
	step [4/244], loss=81.5761
	step [5/244], loss=70.9058
	step [6/244], loss=64.5812
	step [7/244], loss=74.1728
	step [8/244], loss=63.6383
	step [9/244], loss=70.0617
	step [10/244], loss=78.9877
	step [11/244], loss=84.4396
	step [12/244], loss=68.0425
	step [13/244], loss=81.1997
	step [14/244], loss=72.9208
	step [15/244], loss=88.4188
	step [16/244], loss=65.2834
	step [17/244], loss=64.4058
	step [18/244], loss=71.6992
	step [19/244], loss=79.3661
	step [20/244], loss=73.9658
	step [21/244], loss=69.2264
	step [22/244], loss=63.0277
	step [23/244], loss=57.1342
	step [24/244], loss=84.2928
	step [25/244], loss=65.8806
	step [26/244], loss=79.5704
	step [27/244], loss=57.1660
	step [28/244], loss=89.6823
	step [29/244], loss=83.8587
	step [30/244], loss=80.4568
	step [31/244], loss=69.7255
	step [32/244], loss=72.3136
	step [33/244], loss=69.3401
	step [34/244], loss=66.3004
	step [35/244], loss=65.7353
	step [36/244], loss=62.4577
	step [37/244], loss=56.7316
	step [38/244], loss=76.8751
	step [39/244], loss=79.3221
	step [40/244], loss=77.0246
	step [41/244], loss=81.2237
	step [42/244], loss=55.4324
	step [43/244], loss=75.4109
	step [44/244], loss=82.5834
	step [45/244], loss=75.4761
	step [46/244], loss=68.3448
	step [47/244], loss=86.0184
	step [48/244], loss=75.8782
	step [49/244], loss=71.2576
	step [50/244], loss=77.8483
	step [51/244], loss=59.1132
	step [52/244], loss=78.6949
	step [53/244], loss=67.9745
	step [54/244], loss=59.6779
	step [55/244], loss=55.9719
	step [56/244], loss=58.5871
	step [57/244], loss=56.5117
	step [58/244], loss=85.2587
	step [59/244], loss=73.8227
	step [60/244], loss=56.5928
	step [61/244], loss=81.6909
	step [62/244], loss=57.2499
	step [63/244], loss=80.4180
	step [64/244], loss=65.3680
	step [65/244], loss=89.2389
	step [66/244], loss=70.7056
	step [67/244], loss=72.6569
	step [68/244], loss=61.7243
	step [69/244], loss=67.9918
	step [70/244], loss=89.8052
	step [71/244], loss=83.4164
	step [72/244], loss=70.4202
	step [73/244], loss=80.2091
	step [74/244], loss=75.9624
	step [75/244], loss=70.0210
	step [76/244], loss=64.2229
	step [77/244], loss=76.3250
	step [78/244], loss=65.9148
	step [79/244], loss=68.9559
	step [80/244], loss=59.4438
	step [81/244], loss=75.7270
	step [82/244], loss=64.7786
	step [83/244], loss=65.3202
	step [84/244], loss=74.4888
	step [85/244], loss=80.0624
	step [86/244], loss=68.5893
	step [87/244], loss=77.6599
	step [88/244], loss=73.8294
	step [89/244], loss=69.7264
	step [90/244], loss=71.7935
	step [91/244], loss=68.0497
	step [92/244], loss=78.5674
	step [93/244], loss=76.0831
	step [94/244], loss=72.1206
	step [95/244], loss=77.2805
	step [96/244], loss=82.8861
	step [97/244], loss=68.2210
	step [98/244], loss=76.8574
	step [99/244], loss=72.7477
	step [100/244], loss=56.5250
	step [101/244], loss=65.6770
	step [102/244], loss=67.0533
	step [103/244], loss=76.6989
	step [104/244], loss=83.5522
	step [105/244], loss=76.6182
	step [106/244], loss=80.4045
	step [107/244], loss=63.7972
	step [108/244], loss=88.9705
	step [109/244], loss=70.4873
	step [110/244], loss=78.5355
	step [111/244], loss=70.1968
	step [112/244], loss=62.5636
	step [113/244], loss=86.4544
	step [114/244], loss=77.7680
	step [115/244], loss=80.4221
	step [116/244], loss=66.0616
	step [117/244], loss=71.6108
	step [118/244], loss=67.0238
	step [119/244], loss=70.3792
	step [120/244], loss=67.5190
	step [121/244], loss=92.3814
	step [122/244], loss=69.1613
	step [123/244], loss=66.8660
	step [124/244], loss=86.8352
	step [125/244], loss=66.9442
	step [126/244], loss=68.8738
	step [127/244], loss=66.0345
	step [128/244], loss=63.8925
	step [129/244], loss=66.1721
	step [130/244], loss=72.6532
	step [131/244], loss=60.5462
	step [132/244], loss=68.2459
	step [133/244], loss=70.1556
	step [134/244], loss=60.9946
	step [135/244], loss=79.1785
	step [136/244], loss=72.7689
	step [137/244], loss=63.5876
	step [138/244], loss=79.9837
	step [139/244], loss=62.8012
	step [140/244], loss=83.1656
	step [141/244], loss=76.4267
	step [142/244], loss=61.4138
	step [143/244], loss=68.3817
	step [144/244], loss=85.1217
	step [145/244], loss=63.2412
	step [146/244], loss=70.4357
	step [147/244], loss=66.1701
	step [148/244], loss=63.7318
	step [149/244], loss=85.1313
	step [150/244], loss=74.9440
	step [151/244], loss=76.1126
	step [152/244], loss=74.8608
	step [153/244], loss=73.2159
	step [154/244], loss=80.4753
	step [155/244], loss=84.7387
	step [156/244], loss=67.8156
	step [157/244], loss=64.0996
	step [158/244], loss=64.8153
	step [159/244], loss=56.7397
	step [160/244], loss=58.3984
	step [161/244], loss=61.3545
	step [162/244], loss=73.5531
	step [163/244], loss=79.0571
	step [164/244], loss=81.6943
	step [165/244], loss=73.8620
	step [166/244], loss=65.3037
	step [167/244], loss=68.8319
	step [168/244], loss=74.5657
	step [169/244], loss=76.5947
	step [170/244], loss=64.1427
	step [171/244], loss=61.8518
	step [172/244], loss=73.9607
	step [173/244], loss=88.7963
	step [174/244], loss=67.6151
	step [175/244], loss=69.7360
	step [176/244], loss=69.9691
	step [177/244], loss=92.5863
	step [178/244], loss=72.0247
	step [179/244], loss=69.8261
	step [180/244], loss=75.8019
	step [181/244], loss=74.1608
	step [182/244], loss=104.5991
	step [183/244], loss=71.9659
	step [184/244], loss=71.4408
	step [185/244], loss=83.1765
	step [186/244], loss=73.6680
	step [187/244], loss=60.4585
	step [188/244], loss=66.2292
	step [189/244], loss=64.0380
	step [190/244], loss=69.9680
	step [191/244], loss=74.0929
	step [192/244], loss=79.2788
	step [193/244], loss=84.0038
	step [194/244], loss=73.8796
	step [195/244], loss=81.7229
	step [196/244], loss=76.4536
	step [197/244], loss=68.4967
	step [198/244], loss=86.8467
	step [199/244], loss=78.4663
	step [200/244], loss=80.0385
	step [201/244], loss=81.5691
	step [202/244], loss=77.8655
	step [203/244], loss=70.5611
	step [204/244], loss=76.3925
	step [205/244], loss=63.0130
	step [206/244], loss=65.8615
	step [207/244], loss=84.6372
	step [208/244], loss=61.7428
	step [209/244], loss=63.1997
	step [210/244], loss=69.8187
	step [211/244], loss=69.5249
	step [212/244], loss=68.3306
	step [213/244], loss=75.0477
	step [214/244], loss=78.0082
	step [215/244], loss=61.1904
	step [216/244], loss=81.4457
	step [217/244], loss=67.1913
	step [218/244], loss=75.5525
	step [219/244], loss=70.9024
	step [220/244], loss=75.0902
	step [221/244], loss=80.8280
	step [222/244], loss=79.0801
	step [223/244], loss=59.9241
	step [224/244], loss=80.6684
	step [225/244], loss=73.2728
	step [226/244], loss=85.4010
	step [227/244], loss=78.7611
	step [228/244], loss=75.8664
	step [229/244], loss=70.0938
	step [230/244], loss=86.3162
	step [231/244], loss=59.2729
	step [232/244], loss=79.0043
	step [233/244], loss=82.8989
	step [234/244], loss=68.9412
	step [235/244], loss=75.6909
	step [236/244], loss=53.0343
	step [237/244], loss=75.1553
	step [238/244], loss=86.7760
	step [239/244], loss=85.2554
	step [240/244], loss=56.2281
	step [241/244], loss=87.0391
	step [242/244], loss=77.0517
	step [243/244], loss=74.3024
	step [244/244], loss=54.9155
	Evaluating
	loss=0.0081, precision=0.3398, recall=0.8515, f1=0.4858
Training epoch 84
	step [1/244], loss=74.0384
	step [2/244], loss=66.8380
	step [3/244], loss=68.1293
	step [4/244], loss=68.9994
	step [5/244], loss=69.8045
	step [6/244], loss=61.0128
	step [7/244], loss=70.8326
	step [8/244], loss=68.3363
	step [9/244], loss=73.7791
	step [10/244], loss=72.0163
	step [11/244], loss=72.6835
	step [12/244], loss=66.2180
	step [13/244], loss=65.7023
	step [14/244], loss=84.8546
	step [15/244], loss=85.9840
	step [16/244], loss=67.7274
	step [17/244], loss=76.8499
	step [18/244], loss=70.1974
	step [19/244], loss=67.2066
	step [20/244], loss=69.7429
	step [21/244], loss=76.8761
	step [22/244], loss=64.5444
	step [23/244], loss=83.5718
	step [24/244], loss=63.9057
	step [25/244], loss=72.4740
	step [26/244], loss=70.0760
	step [27/244], loss=71.6806
	step [28/244], loss=65.5425
	step [29/244], loss=68.1850
	step [30/244], loss=74.4560
	step [31/244], loss=75.1382
	step [32/244], loss=79.9008
	step [33/244], loss=61.4874
	step [34/244], loss=70.3849
	step [35/244], loss=83.1333
	step [36/244], loss=64.2042
	step [37/244], loss=74.6041
	step [38/244], loss=83.1424
	step [39/244], loss=67.4736
	step [40/244], loss=77.8748
	step [41/244], loss=62.3732
	step [42/244], loss=78.3805
	step [43/244], loss=79.8933
	step [44/244], loss=68.7468
	step [45/244], loss=75.9296
	step [46/244], loss=76.1209
	step [47/244], loss=78.9906
	step [48/244], loss=74.1236
	step [49/244], loss=70.4967
	step [50/244], loss=64.5151
	step [51/244], loss=83.7516
	step [52/244], loss=70.1333
	step [53/244], loss=66.2362
	step [54/244], loss=78.6772
	step [55/244], loss=70.4528
	step [56/244], loss=72.7120
	step [57/244], loss=72.9141
	step [58/244], loss=71.0616
	step [59/244], loss=73.3329
	step [60/244], loss=80.3234
	step [61/244], loss=81.1640
	step [62/244], loss=70.9884
	step [63/244], loss=86.6758
	step [64/244], loss=87.0278
	step [65/244], loss=71.0699
	step [66/244], loss=75.0247
	step [67/244], loss=73.2792
	step [68/244], loss=71.6510
	step [69/244], loss=95.2264
	step [70/244], loss=69.8785
	step [71/244], loss=76.0260
	step [72/244], loss=79.4769
	step [73/244], loss=57.1788
	step [74/244], loss=72.9662
	step [75/244], loss=57.6452
	step [76/244], loss=80.4683
	step [77/244], loss=64.8175
	step [78/244], loss=73.8857
	step [79/244], loss=78.5957
	step [80/244], loss=73.7430
	step [81/244], loss=79.2072
	step [82/244], loss=68.1047
	step [83/244], loss=70.3424
	step [84/244], loss=69.3569
	step [85/244], loss=70.0953
	step [86/244], loss=68.4537
	step [87/244], loss=75.2639
	step [88/244], loss=72.7731
	step [89/244], loss=84.3254
	step [90/244], loss=74.7682
	step [91/244], loss=83.9346
	step [92/244], loss=52.9778
	step [93/244], loss=82.4184
	step [94/244], loss=82.6748
	step [95/244], loss=63.3706
	step [96/244], loss=75.1037
	step [97/244], loss=72.0171
	step [98/244], loss=78.2461
	step [99/244], loss=68.9974
	step [100/244], loss=69.5459
	step [101/244], loss=63.8950
	step [102/244], loss=76.8296
	step [103/244], loss=68.6434
	step [104/244], loss=63.9529
	step [105/244], loss=64.1095
	step [106/244], loss=69.3774
	step [107/244], loss=76.2665
	step [108/244], loss=56.9978
	step [109/244], loss=74.2276
	step [110/244], loss=94.7151
	step [111/244], loss=80.8903
	step [112/244], loss=79.1683
	step [113/244], loss=86.5372
	step [114/244], loss=70.1371
	step [115/244], loss=62.8933
	step [116/244], loss=61.8163
	step [117/244], loss=76.1063
	step [118/244], loss=70.6927
	step [119/244], loss=59.4041
	step [120/244], loss=74.7029
	step [121/244], loss=64.3834
	step [122/244], loss=69.3741
	step [123/244], loss=78.6537
	step [124/244], loss=72.4084
	step [125/244], loss=62.4021
	step [126/244], loss=81.6957
	step [127/244], loss=64.0475
	step [128/244], loss=56.8661
	step [129/244], loss=66.6437
	step [130/244], loss=75.0242
	step [131/244], loss=60.2899
	step [132/244], loss=80.6835
	step [133/244], loss=79.8935
	step [134/244], loss=70.3778
	step [135/244], loss=53.4284
	step [136/244], loss=72.7866
	step [137/244], loss=66.7385
	step [138/244], loss=77.8181
	step [139/244], loss=77.7194
	step [140/244], loss=78.8119
	step [141/244], loss=79.8475
	step [142/244], loss=79.0318
	step [143/244], loss=87.1264
	step [144/244], loss=67.8904
	step [145/244], loss=61.9049
	step [146/244], loss=82.9805
	step [147/244], loss=66.6147
	step [148/244], loss=75.3031
	step [149/244], loss=61.5662
	step [150/244], loss=66.1274
	step [151/244], loss=69.9988
	step [152/244], loss=85.5002
	step [153/244], loss=79.9814
	step [154/244], loss=85.4182
	step [155/244], loss=60.0593
	step [156/244], loss=55.4173
	step [157/244], loss=61.5161
	step [158/244], loss=67.3656
	step [159/244], loss=67.8547
	step [160/244], loss=62.3500
	step [161/244], loss=80.8371
	step [162/244], loss=72.2980
	step [163/244], loss=68.7687
	step [164/244], loss=69.0247
	step [165/244], loss=75.5553
	step [166/244], loss=81.6501
	step [167/244], loss=77.3143
	step [168/244], loss=65.1244
	step [169/244], loss=82.4212
	step [170/244], loss=70.3242
	step [171/244], loss=75.6140
	step [172/244], loss=71.0272
	step [173/244], loss=65.7895
	step [174/244], loss=73.9997
	step [175/244], loss=77.0820
	step [176/244], loss=65.0659
	step [177/244], loss=66.7788
	step [178/244], loss=68.4837
	step [179/244], loss=87.1690
	step [180/244], loss=85.5593
	step [181/244], loss=75.0193
	step [182/244], loss=77.4514
	step [183/244], loss=71.1595
	step [184/244], loss=68.4771
	step [185/244], loss=75.4944
	step [186/244], loss=79.9530
	step [187/244], loss=79.4403
	step [188/244], loss=78.8944
	step [189/244], loss=62.1238
	step [190/244], loss=89.7534
	step [191/244], loss=77.8512
	step [192/244], loss=86.5140
	step [193/244], loss=72.6169
	step [194/244], loss=69.9547
	step [195/244], loss=56.4292
	step [196/244], loss=72.2396
	step [197/244], loss=71.5763
	step [198/244], loss=64.9677
	step [199/244], loss=75.5062
	step [200/244], loss=60.6313
	step [201/244], loss=62.4447
	step [202/244], loss=69.5391
	step [203/244], loss=72.5451
	step [204/244], loss=69.1091
	step [205/244], loss=66.8511
	step [206/244], loss=67.1453
	step [207/244], loss=82.0958
	step [208/244], loss=70.4636
	step [209/244], loss=61.3842
	step [210/244], loss=67.8670
	step [211/244], loss=69.7586
	step [212/244], loss=57.1660
	step [213/244], loss=79.1251
	step [214/244], loss=61.0684
	step [215/244], loss=59.4695
	step [216/244], loss=58.7303
	step [217/244], loss=88.5734
	step [218/244], loss=70.0274
	step [219/244], loss=71.1037
	step [220/244], loss=64.4724
	step [221/244], loss=72.8531
	step [222/244], loss=77.8141
	step [223/244], loss=71.1163
	step [224/244], loss=77.6070
	step [225/244], loss=68.7071
	step [226/244], loss=70.9262
	step [227/244], loss=71.1785
	step [228/244], loss=67.8824
	step [229/244], loss=76.4274
	step [230/244], loss=66.8989
	step [231/244], loss=66.9592
	step [232/244], loss=60.8086
	step [233/244], loss=62.1647
	step [234/244], loss=70.7343
	step [235/244], loss=67.0419
	step [236/244], loss=79.3800
	step [237/244], loss=75.0218
	step [238/244], loss=65.0028
	step [239/244], loss=74.9990
	step [240/244], loss=72.3390
	step [241/244], loss=92.2046
	step [242/244], loss=68.4399
	step [243/244], loss=70.8492
	step [244/244], loss=73.7345
	Evaluating
	loss=0.0076, precision=0.3550, recall=0.8457, f1=0.5001
Training epoch 85
	step [1/244], loss=70.7681
	step [2/244], loss=68.3772
	step [3/244], loss=75.6281
	step [4/244], loss=73.9439
	step [5/244], loss=84.9388
	step [6/244], loss=60.8339
	step [7/244], loss=68.3878
	step [8/244], loss=63.5980
	step [9/244], loss=72.7829
	step [10/244], loss=63.5593
	step [11/244], loss=74.4720
	step [12/244], loss=71.0009
	step [13/244], loss=77.2155
	step [14/244], loss=67.0347
	step [15/244], loss=89.5059
	step [16/244], loss=83.0861
	step [17/244], loss=72.5662
	step [18/244], loss=68.2503
	step [19/244], loss=77.4603
	step [20/244], loss=70.8725
	step [21/244], loss=64.6059
	step [22/244], loss=87.5023
	step [23/244], loss=62.0099
	step [24/244], loss=61.7452
	step [25/244], loss=74.4461
	step [26/244], loss=85.7768
	step [27/244], loss=68.2907
	step [28/244], loss=71.3342
	step [29/244], loss=84.7622
	step [30/244], loss=78.9969
	step [31/244], loss=74.2704
	step [32/244], loss=65.5506
	step [33/244], loss=71.5185
	step [34/244], loss=79.1429
	step [35/244], loss=61.0013
	step [36/244], loss=89.8338
	step [37/244], loss=79.8815
	step [38/244], loss=65.1285
	step [39/244], loss=64.4552
	step [40/244], loss=70.7356
	step [41/244], loss=77.7458
	step [42/244], loss=65.7490
	step [43/244], loss=87.2201
	step [44/244], loss=67.3437
	step [45/244], loss=75.9130
	step [46/244], loss=79.0954
	step [47/244], loss=65.3005
	step [48/244], loss=71.0779
	step [49/244], loss=73.0932
	step [50/244], loss=86.4357
	step [51/244], loss=66.0210
	step [52/244], loss=73.0328
	step [53/244], loss=72.8793
	step [54/244], loss=79.3832
	step [55/244], loss=66.5452
	step [56/244], loss=63.8813
	step [57/244], loss=70.2453
	step [58/244], loss=81.5189
	step [59/244], loss=80.6768
	step [60/244], loss=70.3626
	step [61/244], loss=84.6631
	step [62/244], loss=80.3582
	step [63/244], loss=77.7248
	step [64/244], loss=76.9516
	step [65/244], loss=71.0393
	step [66/244], loss=85.7783
	step [67/244], loss=63.9344
	step [68/244], loss=78.7961
	step [69/244], loss=76.1142
	step [70/244], loss=73.1717
	step [71/244], loss=71.3315
	step [72/244], loss=61.6586
	step [73/244], loss=86.2921
	step [74/244], loss=79.1807
	step [75/244], loss=59.8680
	step [76/244], loss=84.1025
	step [77/244], loss=68.3658
	step [78/244], loss=63.7030
	step [79/244], loss=68.1173
	step [80/244], loss=77.4613
	step [81/244], loss=57.3000
	step [82/244], loss=81.2189
	step [83/244], loss=76.8355
	step [84/244], loss=69.1426
	step [85/244], loss=72.4163
	step [86/244], loss=86.4451
	step [87/244], loss=89.8281
	step [88/244], loss=64.8322
	step [89/244], loss=73.9160
	step [90/244], loss=76.1006
	step [91/244], loss=66.8955
	step [92/244], loss=60.5302
	step [93/244], loss=71.7519
	step [94/244], loss=79.5070
	step [95/244], loss=81.8754
	step [96/244], loss=84.0551
	step [97/244], loss=66.8097
	step [98/244], loss=67.8093
	step [99/244], loss=76.1439
	step [100/244], loss=71.2834
	step [101/244], loss=75.0575
	step [102/244], loss=75.1036
	step [103/244], loss=72.0269
	step [104/244], loss=63.2739
	step [105/244], loss=64.6370
	step [106/244], loss=70.5558
	step [107/244], loss=82.9823
	step [108/244], loss=68.8516
	step [109/244], loss=84.7219
	step [110/244], loss=64.5946
	step [111/244], loss=65.7113
	step [112/244], loss=81.5359
	step [113/244], loss=57.9145
	step [114/244], loss=73.4319
	step [115/244], loss=71.4801
	step [116/244], loss=61.0297
	step [117/244], loss=74.8801
	step [118/244], loss=55.1312
	step [119/244], loss=78.6276
	step [120/244], loss=72.8982
	step [121/244], loss=78.7505
	step [122/244], loss=68.4913
	step [123/244], loss=95.9723
	step [124/244], loss=66.4358
	step [125/244], loss=65.8987
	step [126/244], loss=66.7722
	step [127/244], loss=73.3583
	step [128/244], loss=69.9166
	step [129/244], loss=78.2291
	step [130/244], loss=72.2977
	step [131/244], loss=69.7833
	step [132/244], loss=61.7349
	step [133/244], loss=61.1301
	step [134/244], loss=70.7863
	step [135/244], loss=79.4309
	step [136/244], loss=70.3368
	step [137/244], loss=70.5797
	step [138/244], loss=86.0510
	step [139/244], loss=65.0071
	step [140/244], loss=65.1375
	step [141/244], loss=65.6718
	step [142/244], loss=56.1365
	step [143/244], loss=75.1055
	step [144/244], loss=75.2571
	step [145/244], loss=65.7802
	step [146/244], loss=76.6888
	step [147/244], loss=62.7228
	step [148/244], loss=65.9910
	step [149/244], loss=67.5655
	step [150/244], loss=88.0085
	step [151/244], loss=81.6248
	step [152/244], loss=68.4954
	step [153/244], loss=70.2288
	step [154/244], loss=75.0788
	step [155/244], loss=48.1783
	step [156/244], loss=61.7594
	step [157/244], loss=71.0551
	step [158/244], loss=72.3820
	step [159/244], loss=59.4682
	step [160/244], loss=81.7276
	step [161/244], loss=79.0015
	step [162/244], loss=76.5671
	step [163/244], loss=59.4301
	step [164/244], loss=64.7311
	step [165/244], loss=68.9398
	step [166/244], loss=72.5760
	step [167/244], loss=82.1508
	step [168/244], loss=88.1787
	step [169/244], loss=89.9096
	step [170/244], loss=80.5475
	step [171/244], loss=60.6370
	step [172/244], loss=72.7583
	step [173/244], loss=55.1445
	step [174/244], loss=71.3555
	step [175/244], loss=61.5287
	step [176/244], loss=78.0558
	step [177/244], loss=81.6873
	step [178/244], loss=81.2783
	step [179/244], loss=62.6728
	step [180/244], loss=70.6214
	step [181/244], loss=76.3838
	step [182/244], loss=68.1071
	step [183/244], loss=75.8135
	step [184/244], loss=68.4193
	step [185/244], loss=67.3277
	step [186/244], loss=81.5085
	step [187/244], loss=73.1457
	step [188/244], loss=63.9248
	step [189/244], loss=68.1104
	step [190/244], loss=90.8524
	step [191/244], loss=81.4763
	step [192/244], loss=70.2002
	step [193/244], loss=56.3430
	step [194/244], loss=59.4900
	step [195/244], loss=67.9454
	step [196/244], loss=63.7412
	step [197/244], loss=63.7422
	step [198/244], loss=76.4200
	step [199/244], loss=75.2494
	step [200/244], loss=70.0823
	step [201/244], loss=71.9339
	step [202/244], loss=81.3348
	step [203/244], loss=73.7417
	step [204/244], loss=77.5967
	step [205/244], loss=61.3179
	step [206/244], loss=77.4785
	step [207/244], loss=74.3606
	step [208/244], loss=66.0342
	step [209/244], loss=62.6102
	step [210/244], loss=72.7417
	step [211/244], loss=75.3343
	step [212/244], loss=77.8808
	step [213/244], loss=72.3563
	step [214/244], loss=74.1588
	step [215/244], loss=77.3439
	step [216/244], loss=56.3971
	step [217/244], loss=76.7285
	step [218/244], loss=72.2621
	step [219/244], loss=66.6651
	step [220/244], loss=67.0224
	step [221/244], loss=62.4471
	step [222/244], loss=64.3609
	step [223/244], loss=68.2288
	step [224/244], loss=66.9501
	step [225/244], loss=78.7613
	step [226/244], loss=75.8609
	step [227/244], loss=71.4121
	step [228/244], loss=82.4588
	step [229/244], loss=80.9338
	step [230/244], loss=58.4363
	step [231/244], loss=71.7721
	step [232/244], loss=79.6835
	step [233/244], loss=62.9129
	step [234/244], loss=90.8448
	step [235/244], loss=72.5740
	step [236/244], loss=71.1142
	step [237/244], loss=62.1984
	step [238/244], loss=64.1654
	step [239/244], loss=66.8895
	step [240/244], loss=53.8109
	step [241/244], loss=65.5235
	step [242/244], loss=64.3666
	step [243/244], loss=68.7909
	step [244/244], loss=70.3905
	Evaluating
	loss=0.0075, precision=0.3536, recall=0.8487, f1=0.4992
Training epoch 86
	step [1/244], loss=77.8701
	step [2/244], loss=75.6221
	step [3/244], loss=76.9350
	step [4/244], loss=84.7009
	step [5/244], loss=68.2810
	step [6/244], loss=55.9866
	step [7/244], loss=74.3565
	step [8/244], loss=66.8404
	step [9/244], loss=63.0332
	step [10/244], loss=84.4383
	step [11/244], loss=76.8761
	step [12/244], loss=64.9144
	step [13/244], loss=83.7722
	step [14/244], loss=58.0692
	step [15/244], loss=69.6243
	step [16/244], loss=72.4429
	step [17/244], loss=58.8033
	step [18/244], loss=78.9670
	step [19/244], loss=74.0388
	step [20/244], loss=66.2245
	step [21/244], loss=76.1010
	step [22/244], loss=66.3994
	step [23/244], loss=71.5249
	step [24/244], loss=67.8134
	step [25/244], loss=69.7714
	step [26/244], loss=71.6397
	step [27/244], loss=60.5000
	step [28/244], loss=69.3943
	step [29/244], loss=68.3299
	step [30/244], loss=65.6462
	step [31/244], loss=79.0719
	step [32/244], loss=88.4911
	step [33/244], loss=72.9646
	step [34/244], loss=68.1647
	step [35/244], loss=84.0842
	step [36/244], loss=68.9433
	step [37/244], loss=72.4803
	step [38/244], loss=74.0082
	step [39/244], loss=67.9528
	step [40/244], loss=61.1997
	step [41/244], loss=77.8179
	step [42/244], loss=63.6666
	step [43/244], loss=71.4634
	step [44/244], loss=89.4129
	step [45/244], loss=91.6930
	step [46/244], loss=78.1871
	step [47/244], loss=77.2034
	step [48/244], loss=68.7374
	step [49/244], loss=67.6432
	step [50/244], loss=65.4517
	step [51/244], loss=84.8466
	step [52/244], loss=77.0302
	step [53/244], loss=82.5573
	step [54/244], loss=75.7400
	step [55/244], loss=63.2718
	step [56/244], loss=82.2930
	step [57/244], loss=62.6134
	step [58/244], loss=69.5046
	step [59/244], loss=68.1761
	step [60/244], loss=76.5756
	step [61/244], loss=56.6991
	step [62/244], loss=61.3959
	step [63/244], loss=78.9727
	step [64/244], loss=72.7205
	step [65/244], loss=71.8719
	step [66/244], loss=60.5110
	step [67/244], loss=66.4972
	step [68/244], loss=61.8826
	step [69/244], loss=68.3780
	step [70/244], loss=67.6432
	step [71/244], loss=66.8390
	step [72/244], loss=71.9179
	step [73/244], loss=64.5906
	step [74/244], loss=81.1682
	step [75/244], loss=64.1096
	step [76/244], loss=79.1233
	step [77/244], loss=71.6857
	step [78/244], loss=67.4686
	step [79/244], loss=70.6148
	step [80/244], loss=77.4946
	step [81/244], loss=79.7810
	step [82/244], loss=71.7794
	step [83/244], loss=73.6720
	step [84/244], loss=55.4829
	step [85/244], loss=72.0554
	step [86/244], loss=72.4050
	step [87/244], loss=65.7787
	step [88/244], loss=80.9473
	step [89/244], loss=73.4580
	step [90/244], loss=78.8101
	step [91/244], loss=74.9123
	step [92/244], loss=80.3110
	step [93/244], loss=60.8796
	step [94/244], loss=77.4846
	step [95/244], loss=61.1273
	step [96/244], loss=68.2307
	step [97/244], loss=77.5734
	step [98/244], loss=84.1701
	step [99/244], loss=56.9249
	step [100/244], loss=63.5563
	step [101/244], loss=65.6215
	step [102/244], loss=72.0799
	step [103/244], loss=62.8557
	step [104/244], loss=69.3844
	step [105/244], loss=70.4525
	step [106/244], loss=61.4265
	step [107/244], loss=68.0384
	step [108/244], loss=87.3299
	step [109/244], loss=72.9989
	step [110/244], loss=63.3099
	step [111/244], loss=81.4491
	step [112/244], loss=100.0464
	step [113/244], loss=67.4998
	step [114/244], loss=73.8897
	step [115/244], loss=70.9080
	step [116/244], loss=74.6058
	step [117/244], loss=78.8322
	step [118/244], loss=74.5688
	step [119/244], loss=76.5633
	step [120/244], loss=100.5436
	step [121/244], loss=67.7567
	step [122/244], loss=68.1870
	step [123/244], loss=74.7400
	step [124/244], loss=73.1181
	step [125/244], loss=72.6487
	step [126/244], loss=71.3459
	step [127/244], loss=64.2472
	step [128/244], loss=67.6373
	step [129/244], loss=78.3990
	step [130/244], loss=64.6218
	step [131/244], loss=71.2194
	step [132/244], loss=66.9186
	step [133/244], loss=90.5212
	step [134/244], loss=59.1193
	step [135/244], loss=80.0305
	step [136/244], loss=72.7895
	step [137/244], loss=75.6854
	step [138/244], loss=83.2739
	step [139/244], loss=65.5670
	step [140/244], loss=70.5647
	step [141/244], loss=76.5471
	step [142/244], loss=59.4393
	step [143/244], loss=59.4530
	step [144/244], loss=75.1797
	step [145/244], loss=65.4808
	step [146/244], loss=80.1302
	step [147/244], loss=80.0808
	step [148/244], loss=68.1592
	step [149/244], loss=64.1830
	step [150/244], loss=66.4434
	step [151/244], loss=59.5622
	step [152/244], loss=86.3063
	step [153/244], loss=73.8633
	step [154/244], loss=71.4191
	step [155/244], loss=74.5526
	step [156/244], loss=69.6499
	step [157/244], loss=67.7061
	step [158/244], loss=68.7189
	step [159/244], loss=67.0351
	step [160/244], loss=68.8738
	step [161/244], loss=61.9793
	step [162/244], loss=72.8754
	step [163/244], loss=62.8479
	step [164/244], loss=77.3390
	step [165/244], loss=76.4050
	step [166/244], loss=60.3536
	step [167/244], loss=86.4371
	step [168/244], loss=80.5885
	step [169/244], loss=58.2492
	step [170/244], loss=56.8748
	step [171/244], loss=80.9526
	step [172/244], loss=68.5416
	step [173/244], loss=64.1525
	step [174/244], loss=63.5053
	step [175/244], loss=71.3522
	step [176/244], loss=71.4898
	step [177/244], loss=69.6196
	step [178/244], loss=83.6892
	step [179/244], loss=77.7848
	step [180/244], loss=66.6028
	step [181/244], loss=72.5124
	step [182/244], loss=72.5033
	step [183/244], loss=78.2726
	step [184/244], loss=79.9819
	step [185/244], loss=62.7330
	step [186/244], loss=71.5667
	step [187/244], loss=66.1122
	step [188/244], loss=78.2173
	step [189/244], loss=69.9121
	step [190/244], loss=80.3697
	step [191/244], loss=77.5307
	step [192/244], loss=56.1952
	step [193/244], loss=81.2532
	step [194/244], loss=82.6857
	step [195/244], loss=75.2373
	step [196/244], loss=69.0187
	step [197/244], loss=58.3616
	step [198/244], loss=67.7558
	step [199/244], loss=84.2242
	step [200/244], loss=83.4963
	step [201/244], loss=76.1786
	step [202/244], loss=64.3454
	step [203/244], loss=55.5578
	step [204/244], loss=81.6980
	step [205/244], loss=67.0353
	step [206/244], loss=54.1892
	step [207/244], loss=71.6009
	step [208/244], loss=79.6260
	step [209/244], loss=69.4097
	step [210/244], loss=67.5471
	step [211/244], loss=61.2576
	step [212/244], loss=69.3212
	step [213/244], loss=82.0217
	step [214/244], loss=60.5141
	step [215/244], loss=67.1825
	step [216/244], loss=72.4869
	step [217/244], loss=72.5519
	step [218/244], loss=67.6840
	step [219/244], loss=78.4548
	step [220/244], loss=67.0675
	step [221/244], loss=77.4357
	step [222/244], loss=69.0147
	step [223/244], loss=70.6729
	step [224/244], loss=78.9107
	step [225/244], loss=75.4229
	step [226/244], loss=53.9619
	step [227/244], loss=78.4730
	step [228/244], loss=72.5635
	step [229/244], loss=71.2325
	step [230/244], loss=61.3187
	step [231/244], loss=60.8843
	step [232/244], loss=71.0151
	step [233/244], loss=69.0147
	step [234/244], loss=58.2095
	step [235/244], loss=90.1428
	step [236/244], loss=56.4522
	step [237/244], loss=59.9104
	step [238/244], loss=71.7505
	step [239/244], loss=94.7164
	step [240/244], loss=57.9300
	step [241/244], loss=92.0439
	step [242/244], loss=80.0690
	step [243/244], loss=76.4271
	step [244/244], loss=63.1208
	Evaluating
	loss=0.0071, precision=0.3731, recall=0.8459, f1=0.5178
Training epoch 87
	step [1/244], loss=54.7946
	step [2/244], loss=63.0647
	step [3/244], loss=70.0805
	step [4/244], loss=67.8657
	step [5/244], loss=81.7060
	step [6/244], loss=72.3092
	step [7/244], loss=89.7365
	step [8/244], loss=74.3263
	step [9/244], loss=80.3154
	step [10/244], loss=75.3479
	step [11/244], loss=88.7086
	step [12/244], loss=72.7697
	step [13/244], loss=68.3982
	step [14/244], loss=56.3961
	step [15/244], loss=71.5549
	step [16/244], loss=65.4157
	step [17/244], loss=79.2442
	step [18/244], loss=63.5248
	step [19/244], loss=71.8071
	step [20/244], loss=77.2262
	step [21/244], loss=91.9571
	step [22/244], loss=58.4604
	step [23/244], loss=72.6289
	step [24/244], loss=63.7572
	step [25/244], loss=76.2446
	step [26/244], loss=67.8783
	step [27/244], loss=51.3582
	step [28/244], loss=61.8665
	step [29/244], loss=56.8392
	step [30/244], loss=66.1135
	step [31/244], loss=75.9991
	step [32/244], loss=61.3577
	step [33/244], loss=68.5157
	step [34/244], loss=71.4534
	step [35/244], loss=59.3115
	step [36/244], loss=71.0637
	step [37/244], loss=73.3498
	step [38/244], loss=58.3556
	step [39/244], loss=71.5665
	step [40/244], loss=63.3648
	step [41/244], loss=67.0609
	step [42/244], loss=75.9952
	step [43/244], loss=78.0732
	step [44/244], loss=74.1701
	step [45/244], loss=69.1529
	step [46/244], loss=57.3008
	step [47/244], loss=66.9105
	step [48/244], loss=63.8219
	step [49/244], loss=71.3469
	step [50/244], loss=85.1263
	step [51/244], loss=82.4509
	step [52/244], loss=66.3949
	step [53/244], loss=75.6574
	step [54/244], loss=69.2644
	step [55/244], loss=82.9633
	step [56/244], loss=91.9445
	step [57/244], loss=94.4142
	step [58/244], loss=68.9851
	step [59/244], loss=82.7822
	step [60/244], loss=74.0825
	step [61/244], loss=59.8418
	step [62/244], loss=56.2809
	step [63/244], loss=78.8102
	step [64/244], loss=56.2387
	step [65/244], loss=67.6241
	step [66/244], loss=82.8167
	step [67/244], loss=79.0215
	step [68/244], loss=70.4314
	step [69/244], loss=67.0140
	step [70/244], loss=73.2659
	step [71/244], loss=54.0799
	step [72/244], loss=69.2080
	step [73/244], loss=82.5714
	step [74/244], loss=64.6812
	step [75/244], loss=51.7016
	step [76/244], loss=85.3053
	step [77/244], loss=72.3976
	step [78/244], loss=66.0099
	step [79/244], loss=61.9840
	step [80/244], loss=75.7926
	step [81/244], loss=64.3224
	step [82/244], loss=76.9961
	step [83/244], loss=63.7109
	step [84/244], loss=66.3103
	step [85/244], loss=72.7525
	step [86/244], loss=76.4203
	step [87/244], loss=75.1159
	step [88/244], loss=81.4912
	step [89/244], loss=73.1918
	step [90/244], loss=84.5573
	step [91/244], loss=70.1904
	step [92/244], loss=71.4030
	step [93/244], loss=65.5957
	step [94/244], loss=63.7418
	step [95/244], loss=72.2469
	step [96/244], loss=82.5805
	step [97/244], loss=68.7953
	step [98/244], loss=82.3136
	step [99/244], loss=66.4680
	step [100/244], loss=60.9507
	step [101/244], loss=66.1887
	step [102/244], loss=68.6489
	step [103/244], loss=64.3419
	step [104/244], loss=76.5228
	step [105/244], loss=72.7919
	step [106/244], loss=90.0680
	step [107/244], loss=69.1937
	step [108/244], loss=81.1416
	step [109/244], loss=78.0790
	step [110/244], loss=85.1017
	step [111/244], loss=84.8420
	step [112/244], loss=66.9009
	step [113/244], loss=97.5838
	step [114/244], loss=72.5451
	step [115/244], loss=75.8514
	step [116/244], loss=62.4390
	step [117/244], loss=65.2393
	step [118/244], loss=65.7498
	step [119/244], loss=61.3087
	step [120/244], loss=68.3170
	step [121/244], loss=59.3236
	step [122/244], loss=73.8424
	step [123/244], loss=77.0426
	step [124/244], loss=53.4802
	step [125/244], loss=80.1927
	step [126/244], loss=73.3300
	step [127/244], loss=78.9993
	step [128/244], loss=66.6602
	step [129/244], loss=75.5877
	step [130/244], loss=81.4467
	step [131/244], loss=75.4911
	step [132/244], loss=59.2402
	step [133/244], loss=63.1197
	step [134/244], loss=77.8400
	step [135/244], loss=70.9945
	step [136/244], loss=61.0986
	step [137/244], loss=69.7899
	step [138/244], loss=75.1635
	step [139/244], loss=69.3628
	step [140/244], loss=67.6078
	step [141/244], loss=68.3425
	step [142/244], loss=73.6009
	step [143/244], loss=67.8498
	step [144/244], loss=70.1146
	step [145/244], loss=71.5807
	step [146/244], loss=65.7884
	step [147/244], loss=61.4075
	step [148/244], loss=66.8271
	step [149/244], loss=69.0796
	step [150/244], loss=67.2850
	step [151/244], loss=79.8559
	step [152/244], loss=76.0773
	step [153/244], loss=61.8312
	step [154/244], loss=67.8556
	step [155/244], loss=57.5809
	step [156/244], loss=77.9623
	step [157/244], loss=73.3837
	step [158/244], loss=86.3579
	step [159/244], loss=65.1835
	step [160/244], loss=61.5925
	step [161/244], loss=57.4123
	step [162/244], loss=61.1678
	step [163/244], loss=82.8659
	step [164/244], loss=61.0247
	step [165/244], loss=68.1822
	step [166/244], loss=83.0012
	step [167/244], loss=65.7336
	step [168/244], loss=63.0250
	step [169/244], loss=78.4999
	step [170/244], loss=78.5388
	step [171/244], loss=75.3844
	step [172/244], loss=65.7747
	step [173/244], loss=57.3914
	step [174/244], loss=68.3427
	step [175/244], loss=62.4027
	step [176/244], loss=83.4314
	step [177/244], loss=72.9430
	step [178/244], loss=76.5590
	step [179/244], loss=64.8559
	step [180/244], loss=66.5470
	step [181/244], loss=59.6663
	step [182/244], loss=77.9837
	step [183/244], loss=79.8389
	step [184/244], loss=96.3426
	step [185/244], loss=77.2396
	step [186/244], loss=74.8204
	step [187/244], loss=78.5627
	step [188/244], loss=85.9456
	step [189/244], loss=61.9920
	step [190/244], loss=73.5008
	step [191/244], loss=67.7372
	step [192/244], loss=77.6854
	step [193/244], loss=63.7877
	step [194/244], loss=69.6900
	step [195/244], loss=77.5005
	step [196/244], loss=84.2509
	step [197/244], loss=73.5114
	step [198/244], loss=81.3089
	step [199/244], loss=76.2937
	step [200/244], loss=60.5183
	step [201/244], loss=64.4121
	step [202/244], loss=72.3067
	step [203/244], loss=57.0245
	step [204/244], loss=64.4403
	step [205/244], loss=63.9810
	step [206/244], loss=77.6021
	step [207/244], loss=72.5569
	step [208/244], loss=63.9831
	step [209/244], loss=74.7313
	step [210/244], loss=66.7386
	step [211/244], loss=87.7499
	step [212/244], loss=71.6676
	step [213/244], loss=53.4070
	step [214/244], loss=67.1768
	step [215/244], loss=68.7201
	step [216/244], loss=70.9272
	step [217/244], loss=71.4599
	step [218/244], loss=68.1911
	step [219/244], loss=63.5978
	step [220/244], loss=78.8155
	step [221/244], loss=70.6462
	step [222/244], loss=73.4492
	step [223/244], loss=80.6775
	step [224/244], loss=66.5871
	step [225/244], loss=75.5915
	step [226/244], loss=85.0470
	step [227/244], loss=92.5309
	step [228/244], loss=84.8327
	step [229/244], loss=63.0612
	step [230/244], loss=77.2256
	step [231/244], loss=79.5987
	step [232/244], loss=62.1963
	step [233/244], loss=75.1079
	step [234/244], loss=68.9934
	step [235/244], loss=58.8065
	step [236/244], loss=65.3933
	step [237/244], loss=76.9880
	step [238/244], loss=63.6899
	step [239/244], loss=80.4116
	step [240/244], loss=78.1934
	step [241/244], loss=72.8362
	step [242/244], loss=79.8917
	step [243/244], loss=82.0531
	step [244/244], loss=74.0455
	Evaluating
	loss=0.0074, precision=0.3581, recall=0.8546, f1=0.5047
Training epoch 88
	step [1/244], loss=80.5303
	step [2/244], loss=86.7570
	step [3/244], loss=70.0669
	step [4/244], loss=63.3646
	step [5/244], loss=77.7815
	step [6/244], loss=76.3441
	step [7/244], loss=69.9931
	step [8/244], loss=77.6570
	step [9/244], loss=59.4294
	step [10/244], loss=74.4805
	step [11/244], loss=80.0194
	step [12/244], loss=75.4829
	step [13/244], loss=54.4822
	step [14/244], loss=73.8790
	step [15/244], loss=73.4494
	step [16/244], loss=54.2069
	step [17/244], loss=80.6672
	step [18/244], loss=77.0401
	step [19/244], loss=72.2130
	step [20/244], loss=61.6693
	step [21/244], loss=75.0265
	step [22/244], loss=65.6210
	step [23/244], loss=52.8499
	step [24/244], loss=62.8781
	step [25/244], loss=69.5605
	step [26/244], loss=65.6240
	step [27/244], loss=61.0668
	step [28/244], loss=92.1988
	step [29/244], loss=72.9470
	step [30/244], loss=60.8715
	step [31/244], loss=85.7263
	step [32/244], loss=52.7716
	step [33/244], loss=67.7998
	step [34/244], loss=64.1094
	step [35/244], loss=79.3436
	step [36/244], loss=58.9094
	step [37/244], loss=74.5277
	step [38/244], loss=67.9557
	step [39/244], loss=66.2933
	step [40/244], loss=65.3171
	step [41/244], loss=79.2125
	step [42/244], loss=75.9023
	step [43/244], loss=65.6631
	step [44/244], loss=67.2157
	step [45/244], loss=81.9287
	step [46/244], loss=68.2348
	step [47/244], loss=65.4036
	step [48/244], loss=79.5730
	step [49/244], loss=71.5514
	step [50/244], loss=82.9183
	step [51/244], loss=69.9922
	step [52/244], loss=73.4414
	step [53/244], loss=67.8891
	step [54/244], loss=63.2322
	step [55/244], loss=69.4602
	step [56/244], loss=77.1015
	step [57/244], loss=62.4844
	step [58/244], loss=74.4606
	step [59/244], loss=57.2908
	step [60/244], loss=62.5948
	step [61/244], loss=72.4490
	step [62/244], loss=69.8262
	step [63/244], loss=79.1145
	step [64/244], loss=67.1472
	step [65/244], loss=73.4351
	step [66/244], loss=74.1064
	step [67/244], loss=83.1697
	step [68/244], loss=74.7533
	step [69/244], loss=78.3021
	step [70/244], loss=58.5888
	step [71/244], loss=78.8557
	step [72/244], loss=62.2811
	step [73/244], loss=72.7449
	step [74/244], loss=75.4739
	step [75/244], loss=62.9370
	step [76/244], loss=58.6322
	step [77/244], loss=65.8918
	step [78/244], loss=62.1965
	step [79/244], loss=74.2000
	step [80/244], loss=66.7151
	step [81/244], loss=71.0433
	step [82/244], loss=61.5434
	step [83/244], loss=64.4450
	step [84/244], loss=80.2890
	step [85/244], loss=70.4790
	step [86/244], loss=74.3553
	step [87/244], loss=72.7307
	step [88/244], loss=90.6061
	step [89/244], loss=82.1238
	step [90/244], loss=61.6395
	step [91/244], loss=63.7335
	step [92/244], loss=82.3098
	step [93/244], loss=65.4441
	step [94/244], loss=71.7271
	step [95/244], loss=60.4124
	step [96/244], loss=72.6842
	step [97/244], loss=77.3216
	step [98/244], loss=77.4142
	step [99/244], loss=74.2517
	step [100/244], loss=70.7186
	step [101/244], loss=79.5206
	step [102/244], loss=82.1084
	step [103/244], loss=68.3302
	step [104/244], loss=61.8537
	step [105/244], loss=73.0303
	step [106/244], loss=61.1804
	step [107/244], loss=61.2739
	step [108/244], loss=82.8382
	step [109/244], loss=67.3043
	step [110/244], loss=75.1568
	step [111/244], loss=59.1943
	step [112/244], loss=72.5617
	step [113/244], loss=56.4417
	step [114/244], loss=86.4352
	step [115/244], loss=61.2878
	step [116/244], loss=72.6336
	step [117/244], loss=67.1698
	step [118/244], loss=71.0972
	step [119/244], loss=61.0905
	step [120/244], loss=63.9272
	step [121/244], loss=61.8945
	step [122/244], loss=58.0827
	step [123/244], loss=69.6808
	step [124/244], loss=72.5562
	step [125/244], loss=73.5585
	step [126/244], loss=81.7024
	step [127/244], loss=65.0128
	step [128/244], loss=64.5781
	step [129/244], loss=67.6922
	step [130/244], loss=62.7823
	step [131/244], loss=62.6736
	step [132/244], loss=72.4875
	step [133/244], loss=69.4195
	step [134/244], loss=74.4995
	step [135/244], loss=76.9489
	step [136/244], loss=73.7393
	step [137/244], loss=72.1294
	step [138/244], loss=62.8743
	step [139/244], loss=72.1546
	step [140/244], loss=70.5506
	step [141/244], loss=71.3181
	step [142/244], loss=82.4969
	step [143/244], loss=69.2409
	step [144/244], loss=81.1602
	step [145/244], loss=77.7173
	step [146/244], loss=73.3839
	step [147/244], loss=61.2106
	step [148/244], loss=78.8670
	step [149/244], loss=66.2708
	step [150/244], loss=77.0323
	step [151/244], loss=71.5627
	step [152/244], loss=79.1536
	step [153/244], loss=76.8335
	step [154/244], loss=66.2455
	step [155/244], loss=81.7958
	step [156/244], loss=74.4064
	step [157/244], loss=60.8321
	step [158/244], loss=84.2445
	step [159/244], loss=77.5691
	step [160/244], loss=77.6411
	step [161/244], loss=67.3617
	step [162/244], loss=64.3865
	step [163/244], loss=95.1683
	step [164/244], loss=54.9982
	step [165/244], loss=85.2076
	step [166/244], loss=67.6984
	step [167/244], loss=62.6011
	step [168/244], loss=58.4965
	step [169/244], loss=77.2525
	step [170/244], loss=76.1618
	step [171/244], loss=83.9540
	step [172/244], loss=74.3512
	step [173/244], loss=50.7270
	step [174/244], loss=78.2690
	step [175/244], loss=61.4904
	step [176/244], loss=78.4586
	step [177/244], loss=63.8000
	step [178/244], loss=83.4539
	step [179/244], loss=62.2334
	step [180/244], loss=85.7589
	step [181/244], loss=68.1783
	step [182/244], loss=67.1357
	step [183/244], loss=67.9202
	step [184/244], loss=64.7990
	step [185/244], loss=65.1276
	step [186/244], loss=69.6703
	step [187/244], loss=88.1678
	step [188/244], loss=79.5317
	step [189/244], loss=69.0520
	step [190/244], loss=66.1089
	step [191/244], loss=67.5759
	step [192/244], loss=66.9334
	step [193/244], loss=65.7654
	step [194/244], loss=66.0657
	step [195/244], loss=63.9036
	step [196/244], loss=65.2967
	step [197/244], loss=74.7498
	step [198/244], loss=71.5694
	step [199/244], loss=84.0093
	step [200/244], loss=62.5397
	step [201/244], loss=69.6816
	step [202/244], loss=72.0503
	step [203/244], loss=76.9326
	step [204/244], loss=68.3845
	step [205/244], loss=72.4680
	step [206/244], loss=70.8660
	step [207/244], loss=75.0124
	step [208/244], loss=72.7761
	step [209/244], loss=65.5175
	step [210/244], loss=85.0029
	step [211/244], loss=71.4967
	step [212/244], loss=69.9883
	step [213/244], loss=68.9265
	step [214/244], loss=73.9305
	step [215/244], loss=65.3221
	step [216/244], loss=47.9334
	step [217/244], loss=74.3859
	step [218/244], loss=71.2022
	step [219/244], loss=62.0266
	step [220/244], loss=65.1716
	step [221/244], loss=106.7288
	step [222/244], loss=70.4818
	step [223/244], loss=56.7177
	step [224/244], loss=70.3544
	step [225/244], loss=72.0779
	step [226/244], loss=89.4279
	step [227/244], loss=73.3297
	step [228/244], loss=86.1798
	step [229/244], loss=82.9916
	step [230/244], loss=68.7046
	step [231/244], loss=66.2130
	step [232/244], loss=78.9712
	step [233/244], loss=70.7084
	step [234/244], loss=59.4067
	step [235/244], loss=59.3551
	step [236/244], loss=72.4710
	step [237/244], loss=74.3235
	step [238/244], loss=66.7476
	step [239/244], loss=79.2800
	step [240/244], loss=70.0588
	step [241/244], loss=74.1612
	step [242/244], loss=85.2563
	step [243/244], loss=73.1633
	step [244/244], loss=66.7398
	Evaluating
	loss=0.0086, precision=0.3158, recall=0.8548, f1=0.4612
Training epoch 89
	step [1/244], loss=77.0004
	step [2/244], loss=82.7782
	step [3/244], loss=72.7688
	step [4/244], loss=83.2835
	step [5/244], loss=82.5298
	step [6/244], loss=74.6906
	step [7/244], loss=74.6645
	step [8/244], loss=67.7484
	step [9/244], loss=82.8031
	step [10/244], loss=60.3443
	step [11/244], loss=72.4423
	step [12/244], loss=64.7267
	step [13/244], loss=61.3528
	step [14/244], loss=67.0443
	step [15/244], loss=68.9892
	step [16/244], loss=70.3704
	step [17/244], loss=70.6261
	step [18/244], loss=84.4027
	step [19/244], loss=62.0262
	step [20/244], loss=67.6015
	step [21/244], loss=80.2280
	step [22/244], loss=65.4703
	step [23/244], loss=81.1528
	step [24/244], loss=73.4973
	step [25/244], loss=80.5802
	step [26/244], loss=94.1898
	step [27/244], loss=85.4264
	step [28/244], loss=55.6353
	step [29/244], loss=72.5874
	step [30/244], loss=69.1842
	step [31/244], loss=69.7201
	step [32/244], loss=75.5596
	step [33/244], loss=66.3354
	step [34/244], loss=76.9519
	step [35/244], loss=72.9920
	step [36/244], loss=60.5856
	step [37/244], loss=74.7619
	step [38/244], loss=70.6895
	step [39/244], loss=58.0860
	step [40/244], loss=79.0814
	step [41/244], loss=73.6646
	step [42/244], loss=62.4857
	step [43/244], loss=80.8855
	step [44/244], loss=64.5230
	step [45/244], loss=71.5168
	step [46/244], loss=73.7944
	step [47/244], loss=62.2799
	step [48/244], loss=77.3158
	step [49/244], loss=64.1541
	step [50/244], loss=66.8303
	step [51/244], loss=65.8043
	step [52/244], loss=80.7747
	step [53/244], loss=60.2497
	step [54/244], loss=66.4233
	step [55/244], loss=77.5791
	step [56/244], loss=79.1555
	step [57/244], loss=74.8731
	step [58/244], loss=63.7813
	step [59/244], loss=79.2619
	step [60/244], loss=70.6011
	step [61/244], loss=74.9546
	step [62/244], loss=72.7832
	step [63/244], loss=75.3507
	step [64/244], loss=71.0115
	step [65/244], loss=75.6765
	step [66/244], loss=79.0061
	step [67/244], loss=74.1149
	step [68/244], loss=72.3970
	step [69/244], loss=77.8309
	step [70/244], loss=76.2931
	step [71/244], loss=71.7982
	step [72/244], loss=72.5105
	step [73/244], loss=64.9517
	step [74/244], loss=73.5419
	step [75/244], loss=71.0958
	step [76/244], loss=79.2074
	step [77/244], loss=57.4476
	step [78/244], loss=58.2540
	step [79/244], loss=65.9804
	step [80/244], loss=69.3556
	step [81/244], loss=65.8643
	step [82/244], loss=64.3947
	step [83/244], loss=70.0732
	step [84/244], loss=48.4896
	step [85/244], loss=55.2092
	step [86/244], loss=72.1785
	step [87/244], loss=67.7055
	step [88/244], loss=75.9226
	step [89/244], loss=75.6169
	step [90/244], loss=69.4453
	step [91/244], loss=69.7159
	step [92/244], loss=68.8221
	step [93/244], loss=82.1579
	step [94/244], loss=66.8300
	step [95/244], loss=76.8856
	step [96/244], loss=65.5107
	step [97/244], loss=73.5694
	step [98/244], loss=69.6713
	step [99/244], loss=67.6284
	step [100/244], loss=65.4948
	step [101/244], loss=76.3076
	step [102/244], loss=69.4715
	step [103/244], loss=58.7426
	step [104/244], loss=71.7357
	step [105/244], loss=78.0559
	step [106/244], loss=86.3292
	step [107/244], loss=95.6921
	step [108/244], loss=67.1687
	step [109/244], loss=76.3638
	step [110/244], loss=74.9961
	step [111/244], loss=68.9201
	step [112/244], loss=72.9639
	step [113/244], loss=69.9938
	step [114/244], loss=70.2432
	step [115/244], loss=66.0668
	step [116/244], loss=66.6578
	step [117/244], loss=58.3777
	step [118/244], loss=75.4735
	step [119/244], loss=72.7058
	step [120/244], loss=72.9361
	step [121/244], loss=85.9827
	step [122/244], loss=71.5630
	step [123/244], loss=64.8621
	step [124/244], loss=63.2902
	step [125/244], loss=77.0950
	step [126/244], loss=68.2731
	step [127/244], loss=60.3030
	step [128/244], loss=60.6936
	step [129/244], loss=65.7298
	step [130/244], loss=59.9382
	step [131/244], loss=78.1915
	step [132/244], loss=69.2719
	step [133/244], loss=67.6581
	step [134/244], loss=62.2100
	step [135/244], loss=66.3974
	step [136/244], loss=69.1352
	step [137/244], loss=78.6689
	step [138/244], loss=57.4177
	step [139/244], loss=73.4835
	step [140/244], loss=73.8235
	step [141/244], loss=64.1033
	step [142/244], loss=69.9871
	step [143/244], loss=73.8512
	step [144/244], loss=64.9029
	step [145/244], loss=56.6206
	step [146/244], loss=67.5414
	step [147/244], loss=73.0114
	step [148/244], loss=74.2526
	step [149/244], loss=62.3500
	step [150/244], loss=73.7222
	step [151/244], loss=81.3402
	step [152/244], loss=78.0490
	step [153/244], loss=70.3158
	step [154/244], loss=84.9844
	step [155/244], loss=66.1223
	step [156/244], loss=72.9793
	step [157/244], loss=80.7266
	step [158/244], loss=59.7704
	step [159/244], loss=68.3655
	step [160/244], loss=85.1828
	step [161/244], loss=79.6306
	step [162/244], loss=75.1844
	step [163/244], loss=97.0085
	step [164/244], loss=74.2474
	step [165/244], loss=66.5762
	step [166/244], loss=81.7110
	step [167/244], loss=73.4434
	step [168/244], loss=63.9086
	step [169/244], loss=51.3053
	step [170/244], loss=63.9547
	step [171/244], loss=67.2822
	step [172/244], loss=73.2209
	step [173/244], loss=76.9462
	step [174/244], loss=73.6567
	step [175/244], loss=77.5551
	step [176/244], loss=60.6742
	step [177/244], loss=78.8809
	step [178/244], loss=70.8911
	step [179/244], loss=75.0834
	step [180/244], loss=76.9339
	step [181/244], loss=71.6523
	step [182/244], loss=62.8634
	step [183/244], loss=62.3258
	step [184/244], loss=75.2474
	step [185/244], loss=78.9414
	step [186/244], loss=64.1047
	step [187/244], loss=64.0877
	step [188/244], loss=79.5571
	step [189/244], loss=68.8803
	step [190/244], loss=79.5793
	step [191/244], loss=62.7484
	step [192/244], loss=57.0129
	step [193/244], loss=67.3523
	step [194/244], loss=76.3084
	step [195/244], loss=65.1963
	step [196/244], loss=77.6816
	step [197/244], loss=75.6151
	step [198/244], loss=75.2076
	step [199/244], loss=69.2333
	step [200/244], loss=72.9189
	step [201/244], loss=74.5157
	step [202/244], loss=51.1418
	step [203/244], loss=68.2334
	step [204/244], loss=78.0488
	step [205/244], loss=77.8274
	step [206/244], loss=72.7838
	step [207/244], loss=84.7526
	step [208/244], loss=68.7914
	step [209/244], loss=64.2597
	step [210/244], loss=65.3158
	step [211/244], loss=83.2591
	step [212/244], loss=66.5108
	step [213/244], loss=73.5782
	step [214/244], loss=63.8965
	step [215/244], loss=56.2507
	step [216/244], loss=49.3321
	step [217/244], loss=70.7773
	step [218/244], loss=76.9879
	step [219/244], loss=58.3149
	step [220/244], loss=65.3274
	step [221/244], loss=58.9812
	step [222/244], loss=79.5913
	step [223/244], loss=73.3010
	step [224/244], loss=56.8181
	step [225/244], loss=76.9932
	step [226/244], loss=71.8175
	step [227/244], loss=81.9590
	step [228/244], loss=61.5353
	step [229/244], loss=71.2246
	step [230/244], loss=61.1706
	step [231/244], loss=69.6885
	step [232/244], loss=61.2527
	step [233/244], loss=74.1766
	step [234/244], loss=86.5837
	step [235/244], loss=74.0235
	step [236/244], loss=66.8540
	step [237/244], loss=76.6104
	step [238/244], loss=76.6517
	step [239/244], loss=77.7871
	step [240/244], loss=65.5685
	step [241/244], loss=66.2305
	step [242/244], loss=77.5979
	step [243/244], loss=71.1916
	step [244/244], loss=73.3348
	Evaluating
	loss=0.0067, precision=0.3856, recall=0.8524, f1=0.5310
saving model as: 1_saved_model.pth
Training epoch 90
	step [1/244], loss=83.4709
	step [2/244], loss=66.3737
	step [3/244], loss=59.0002
	step [4/244], loss=55.5556
	step [5/244], loss=75.8458
	step [6/244], loss=93.4895
	step [7/244], loss=66.6833
	step [8/244], loss=63.0156
	step [9/244], loss=66.5685
	step [10/244], loss=67.8672
	step [11/244], loss=58.6065
	step [12/244], loss=62.8501
	step [13/244], loss=67.1924
	step [14/244], loss=84.1761
	step [15/244], loss=67.0534
	step [16/244], loss=71.6110
	step [17/244], loss=70.3557
	step [18/244], loss=72.2161
	step [19/244], loss=86.0719
	step [20/244], loss=64.7164
	step [21/244], loss=57.3474
	step [22/244], loss=74.4566
	step [23/244], loss=76.5016
	step [24/244], loss=77.4391
	step [25/244], loss=63.7029
	step [26/244], loss=76.4817
	step [27/244], loss=70.5889
	step [28/244], loss=73.8851
	step [29/244], loss=62.6829
	step [30/244], loss=74.3879
	step [31/244], loss=82.9558
	step [32/244], loss=85.3301
	step [33/244], loss=67.0007
	step [34/244], loss=70.3971
	step [35/244], loss=73.5781
	step [36/244], loss=60.5969
	step [37/244], loss=67.1712
	step [38/244], loss=63.1574
	step [39/244], loss=74.1781
	step [40/244], loss=63.0270
	step [41/244], loss=62.1465
	step [42/244], loss=59.7477
	step [43/244], loss=60.4690
	step [44/244], loss=63.2321
	step [45/244], loss=71.0938
	step [46/244], loss=58.1226
	step [47/244], loss=78.2159
	step [48/244], loss=66.7913
	step [49/244], loss=66.2468
	step [50/244], loss=62.4388
	step [51/244], loss=71.4502
	step [52/244], loss=65.6227
	step [53/244], loss=85.3668
	step [54/244], loss=62.7738
	step [55/244], loss=70.4737
	step [56/244], loss=65.6488
	step [57/244], loss=73.5747
	step [58/244], loss=61.9244
	step [59/244], loss=70.1494
	step [60/244], loss=83.7488
	step [61/244], loss=67.1303
	step [62/244], loss=74.2053
	step [63/244], loss=79.3444
	step [64/244], loss=80.2724
	step [65/244], loss=65.9228
	step [66/244], loss=64.4253
	step [67/244], loss=61.6757
	step [68/244], loss=59.8501
	step [69/244], loss=63.0812
	step [70/244], loss=66.3818
	step [71/244], loss=62.5477
	step [72/244], loss=83.9471
	step [73/244], loss=63.4336
	step [74/244], loss=50.5002
	step [75/244], loss=50.1258
	step [76/244], loss=67.8693
	step [77/244], loss=47.8144
	step [78/244], loss=62.3726
	step [79/244], loss=65.3749
	step [80/244], loss=83.0884
	step [81/244], loss=72.7359
	step [82/244], loss=81.5513
	step [83/244], loss=71.6314
	step [84/244], loss=68.4934
	step [85/244], loss=87.1352
	step [86/244], loss=67.0081
	step [87/244], loss=51.3579
	step [88/244], loss=84.0184
	step [89/244], loss=75.2814
	step [90/244], loss=66.4160
	step [91/244], loss=88.5161
	step [92/244], loss=70.0972
	step [93/244], loss=65.9019
	step [94/244], loss=78.7552
	step [95/244], loss=89.2182
	step [96/244], loss=70.6504
	step [97/244], loss=64.9192
	step [98/244], loss=74.9669
	step [99/244], loss=72.4014
	step [100/244], loss=59.6786
	step [101/244], loss=71.6486
	step [102/244], loss=60.0389
	step [103/244], loss=65.9350
	step [104/244], loss=73.5032
	step [105/244], loss=65.3524
	step [106/244], loss=78.3168
	step [107/244], loss=62.1278
	step [108/244], loss=60.1355
	step [109/244], loss=72.2318
	step [110/244], loss=68.7864
	step [111/244], loss=72.6166
	step [112/244], loss=72.6922
	step [113/244], loss=69.0906
	step [114/244], loss=70.2950
	step [115/244], loss=82.9525
	step [116/244], loss=66.0038
	step [117/244], loss=70.1874
	step [118/244], loss=85.7193
	step [119/244], loss=66.7305
	step [120/244], loss=66.5991
	step [121/244], loss=87.8907
	step [122/244], loss=83.5462
	step [123/244], loss=78.5868
	step [124/244], loss=75.8696
	step [125/244], loss=64.2802
	step [126/244], loss=69.7171
	step [127/244], loss=86.7282
	step [128/244], loss=83.2331
	step [129/244], loss=73.6789
	step [130/244], loss=64.8732
	step [131/244], loss=60.9749
	step [132/244], loss=72.2463
	step [133/244], loss=68.3551
	step [134/244], loss=67.8777
	step [135/244], loss=80.0056
	step [136/244], loss=66.9095
	step [137/244], loss=90.6054
	step [138/244], loss=81.8430
	step [139/244], loss=62.6247
	step [140/244], loss=74.4807
	step [141/244], loss=66.3274
	step [142/244], loss=75.1770
	step [143/244], loss=69.5247
	step [144/244], loss=72.2470
	step [145/244], loss=73.7177
	step [146/244], loss=75.0024
	step [147/244], loss=80.1914
	step [148/244], loss=61.2193
	step [149/244], loss=64.8768
	step [150/244], loss=68.8415
	step [151/244], loss=73.7353
	step [152/244], loss=66.3792
	step [153/244], loss=71.8490
	step [154/244], loss=64.0343
	step [155/244], loss=62.0447
	step [156/244], loss=74.7690
	step [157/244], loss=75.5574
	step [158/244], loss=68.4959
	step [159/244], loss=63.0279
	step [160/244], loss=60.6228
	step [161/244], loss=72.9211
	step [162/244], loss=76.5005
	step [163/244], loss=77.4940
	step [164/244], loss=71.3574
	step [165/244], loss=66.9569
	step [166/244], loss=74.1637
	step [167/244], loss=71.7084
	step [168/244], loss=78.9895
	step [169/244], loss=69.2803
	step [170/244], loss=79.6174
	step [171/244], loss=60.8498
	step [172/244], loss=65.1057
	step [173/244], loss=75.4284
	step [174/244], loss=82.2517
	step [175/244], loss=71.1999
	step [176/244], loss=70.3805
	step [177/244], loss=67.3458
	step [178/244], loss=79.1209
	step [179/244], loss=58.7582
	step [180/244], loss=84.1271
	step [181/244], loss=79.6789
	step [182/244], loss=71.7086
	step [183/244], loss=76.0528
	step [184/244], loss=74.3936
	step [185/244], loss=73.0838
	step [186/244], loss=58.0676
	step [187/244], loss=63.5569
	step [188/244], loss=84.0630
	step [189/244], loss=67.9014
	step [190/244], loss=64.9989
	step [191/244], loss=73.7864
	step [192/244], loss=73.9848
	step [193/244], loss=65.9241
	step [194/244], loss=66.4931
	step [195/244], loss=64.5913
	step [196/244], loss=75.3444
	step [197/244], loss=59.5254
	step [198/244], loss=58.5070
	step [199/244], loss=70.1023
	step [200/244], loss=81.0643
	step [201/244], loss=79.7723
	step [202/244], loss=57.0404
	step [203/244], loss=59.8299
	step [204/244], loss=79.9293
	step [205/244], loss=66.2251
	step [206/244], loss=88.9615
	step [207/244], loss=63.9275
	step [208/244], loss=66.9132
	step [209/244], loss=61.5673
	step [210/244], loss=63.6085
	step [211/244], loss=77.8946
	step [212/244], loss=77.2329
	step [213/244], loss=79.3453
	step [214/244], loss=71.8146
	step [215/244], loss=95.9536
	step [216/244], loss=69.8659
	step [217/244], loss=63.9832
	step [218/244], loss=75.9185
	step [219/244], loss=69.3637
	step [220/244], loss=78.1223
	step [221/244], loss=75.6030
	step [222/244], loss=90.8001
	step [223/244], loss=68.8661
	step [224/244], loss=63.4091
	step [225/244], loss=75.6789
	step [226/244], loss=68.6436
	step [227/244], loss=70.7790
	step [228/244], loss=58.6159
	step [229/244], loss=70.9835
	step [230/244], loss=66.5862
	step [231/244], loss=91.2769
	step [232/244], loss=69.6874
	step [233/244], loss=83.7545
	step [234/244], loss=62.3656
	step [235/244], loss=60.7574
	step [236/244], loss=80.8830
	step [237/244], loss=74.7360
	step [238/244], loss=70.8933
	step [239/244], loss=68.2044
	step [240/244], loss=70.5571
	step [241/244], loss=67.0000
	step [242/244], loss=78.5196
	step [243/244], loss=74.0497
	step [244/244], loss=65.8685
	Evaluating
	loss=0.0082, precision=0.3309, recall=0.8498, f1=0.4764
Training epoch 91
	step [1/244], loss=62.1095
	step [2/244], loss=56.2950
	step [3/244], loss=59.2266
	step [4/244], loss=76.1102
	step [5/244], loss=80.1307
	step [6/244], loss=66.7723
	step [7/244], loss=73.7973
	step [8/244], loss=64.5457
	step [9/244], loss=64.3491
	step [10/244], loss=69.4697
	step [11/244], loss=65.8534
	step [12/244], loss=69.7663
	step [13/244], loss=75.2236
	step [14/244], loss=66.7366
	step [15/244], loss=93.6015
	step [16/244], loss=61.0192
	step [17/244], loss=64.5225
	step [18/244], loss=72.9256
	step [19/244], loss=64.7069
	step [20/244], loss=81.1359
	step [21/244], loss=65.8749
	step [22/244], loss=62.4869
	step [23/244], loss=69.6331
	step [24/244], loss=67.5124
	step [25/244], loss=58.7083
	step [26/244], loss=67.3943
	step [27/244], loss=81.4271
	step [28/244], loss=70.5650
	step [29/244], loss=69.2977
	step [30/244], loss=77.0471
	step [31/244], loss=77.9856
	step [32/244], loss=98.1300
	step [33/244], loss=74.0474
	step [34/244], loss=67.0123
	step [35/244], loss=65.8369
	step [36/244], loss=62.2711
	step [37/244], loss=64.1525
	step [38/244], loss=78.6571
	step [39/244], loss=81.0958
	step [40/244], loss=75.8354
	step [41/244], loss=73.9519
	step [42/244], loss=69.4231
	step [43/244], loss=67.9931
	step [44/244], loss=63.4175
	step [45/244], loss=69.9312
	step [46/244], loss=65.1418
	step [47/244], loss=74.2621
	step [48/244], loss=77.6024
	step [49/244], loss=84.4385
	step [50/244], loss=71.4703
	step [51/244], loss=80.1853
	step [52/244], loss=73.3902
	step [53/244], loss=80.8654
	step [54/244], loss=60.1900
	step [55/244], loss=90.3906
	step [56/244], loss=79.5236
	step [57/244], loss=56.7736
	step [58/244], loss=67.5850
	step [59/244], loss=60.1987
	step [60/244], loss=75.8340
	step [61/244], loss=56.8636
	step [62/244], loss=73.2652
	step [63/244], loss=53.3574
	step [64/244], loss=59.3547
	step [65/244], loss=83.0425
	step [66/244], loss=72.9691
	step [67/244], loss=69.6019
	step [68/244], loss=72.8395
	step [69/244], loss=72.3410
	step [70/244], loss=73.0734
	step [71/244], loss=61.6370
	step [72/244], loss=71.5095
	step [73/244], loss=61.7438
	step [74/244], loss=72.0385
	step [75/244], loss=78.7710
	step [76/244], loss=82.9737
	step [77/244], loss=79.0709
	step [78/244], loss=81.0016
	step [79/244], loss=82.1470
	step [80/244], loss=70.4233
	step [81/244], loss=81.6159
	step [82/244], loss=60.7755
	step [83/244], loss=70.7250
	step [84/244], loss=79.2490
	step [85/244], loss=84.1743
	step [86/244], loss=80.1311
	step [87/244], loss=62.1466
	step [88/244], loss=78.1909
	step [89/244], loss=59.1201
	step [90/244], loss=69.2664
	step [91/244], loss=79.0820
	step [92/244], loss=66.5451
	step [93/244], loss=66.0521
	step [94/244], loss=60.9033
	step [95/244], loss=65.8486
	step [96/244], loss=70.9717
	step [97/244], loss=70.1379
	step [98/244], loss=56.8512
	step [99/244], loss=79.9373
	step [100/244], loss=68.6834
	step [101/244], loss=73.4811
	step [102/244], loss=72.7502
	step [103/244], loss=69.4452
	step [104/244], loss=52.8984
	step [105/244], loss=64.3732
	step [106/244], loss=79.1601
	step [107/244], loss=62.7663
	step [108/244], loss=77.6505
	step [109/244], loss=65.1910
	step [110/244], loss=68.8017
	step [111/244], loss=66.1008
	step [112/244], loss=52.3405
	step [113/244], loss=73.5500
	step [114/244], loss=53.0538
	step [115/244], loss=80.3264
	step [116/244], loss=71.3742
	step [117/244], loss=86.8874
	step [118/244], loss=79.2793
	step [119/244], loss=65.2995
	step [120/244], loss=65.2958
	step [121/244], loss=88.5906
	step [122/244], loss=91.2270
	step [123/244], loss=56.3854
	step [124/244], loss=76.3788
	step [125/244], loss=54.3547
	step [126/244], loss=72.5643
	step [127/244], loss=75.4120
	step [128/244], loss=82.1578
	step [129/244], loss=67.8617
	step [130/244], loss=75.8134
	step [131/244], loss=71.3563
	step [132/244], loss=71.3024
	step [133/244], loss=64.5926
	step [134/244], loss=72.5218
	step [135/244], loss=65.3735
	step [136/244], loss=66.8910
	step [137/244], loss=63.7236
	step [138/244], loss=67.7641
	step [139/244], loss=69.4432
	step [140/244], loss=63.6996
	step [141/244], loss=67.6585
	step [142/244], loss=56.7064
	step [143/244], loss=60.7888
	step [144/244], loss=63.5966
	step [145/244], loss=56.8051
	step [146/244], loss=67.9165
	step [147/244], loss=83.0500
	step [148/244], loss=71.7609
	step [149/244], loss=84.5614
	step [150/244], loss=76.9086
	step [151/244], loss=66.3869
	step [152/244], loss=65.0420
	step [153/244], loss=66.1526
	step [154/244], loss=75.7025
	step [155/244], loss=67.4131
	step [156/244], loss=75.3371
	step [157/244], loss=76.1929
	step [158/244], loss=86.4250
	step [159/244], loss=87.4222
	step [160/244], loss=73.5636
	step [161/244], loss=69.5838
	step [162/244], loss=78.8320
	step [163/244], loss=64.1757
	step [164/244], loss=75.2777
	step [165/244], loss=70.8477
	step [166/244], loss=73.8330
	step [167/244], loss=86.3822
	step [168/244], loss=61.2657
	step [169/244], loss=86.4592
	step [170/244], loss=51.8141
	step [171/244], loss=64.6408
	step [172/244], loss=68.9793
	step [173/244], loss=83.4563
	step [174/244], loss=70.7337
	step [175/244], loss=60.1011
	step [176/244], loss=61.9035
	step [177/244], loss=60.4376
	step [178/244], loss=66.1592
	step [179/244], loss=70.3227
	step [180/244], loss=57.5436
	step [181/244], loss=74.2852
	step [182/244], loss=84.2312
	step [183/244], loss=71.6437
	step [184/244], loss=68.1510
	step [185/244], loss=61.4158
	step [186/244], loss=64.4758
	step [187/244], loss=71.6855
	step [188/244], loss=85.3026
	step [189/244], loss=67.7400
	step [190/244], loss=76.9723
	step [191/244], loss=71.9157
	step [192/244], loss=78.6838
	step [193/244], loss=71.7459
	step [194/244], loss=68.5272
	step [195/244], loss=72.8399
	step [196/244], loss=76.7255
	step [197/244], loss=70.3362
	step [198/244], loss=70.0423
	step [199/244], loss=60.3214
	step [200/244], loss=82.9563
	step [201/244], loss=69.1083
	step [202/244], loss=55.1446
	step [203/244], loss=71.9145
	step [204/244], loss=71.3533
	step [205/244], loss=70.9841
	step [206/244], loss=65.7377
	step [207/244], loss=74.4296
	step [208/244], loss=68.4766
	step [209/244], loss=72.6383
	step [210/244], loss=69.5125
	step [211/244], loss=64.6297
	step [212/244], loss=70.2289
	step [213/244], loss=68.7990
	step [214/244], loss=73.4025
	step [215/244], loss=67.3942
	step [216/244], loss=66.2262
	step [217/244], loss=74.4668
	step [218/244], loss=62.7775
	step [219/244], loss=61.9107
	step [220/244], loss=62.4636
	step [221/244], loss=65.0562
	step [222/244], loss=68.8371
	step [223/244], loss=73.3417
	step [224/244], loss=68.6162
	step [225/244], loss=81.0653
	step [226/244], loss=57.8589
	step [227/244], loss=77.9312
	step [228/244], loss=64.7142
	step [229/244], loss=69.8419
	step [230/244], loss=86.5432
	step [231/244], loss=75.2234
	step [232/244], loss=73.2096
	step [233/244], loss=74.6396
	step [234/244], loss=72.4409
	step [235/244], loss=61.5960
	step [236/244], loss=72.2008
	step [237/244], loss=68.2883
	step [238/244], loss=62.9598
	step [239/244], loss=73.9317
	step [240/244], loss=78.4697
	step [241/244], loss=66.8058
	step [242/244], loss=70.7980
	step [243/244], loss=71.1309
	step [244/244], loss=62.6127
	Evaluating
	loss=0.0070, precision=0.3741, recall=0.8471, f1=0.5190
Training epoch 92
	step [1/244], loss=64.9306
	step [2/244], loss=66.5899
	step [3/244], loss=54.7905
	step [4/244], loss=88.8384
	step [5/244], loss=77.3653
	step [6/244], loss=82.1684
	step [7/244], loss=85.3229
	step [8/244], loss=70.8553
	step [9/244], loss=61.6163
	step [10/244], loss=58.0351
	step [11/244], loss=74.9441
	step [12/244], loss=84.6058
	step [13/244], loss=77.0124
	step [14/244], loss=62.1721
	step [15/244], loss=66.3840
	step [16/244], loss=78.9268
	step [17/244], loss=60.0792
	step [18/244], loss=83.5042
	step [19/244], loss=70.1113
	step [20/244], loss=82.2732
	step [21/244], loss=54.4808
	step [22/244], loss=69.8749
	step [23/244], loss=66.4831
	step [24/244], loss=70.6585
	step [25/244], loss=65.4141
	step [26/244], loss=58.7760
	step [27/244], loss=72.5162
	step [28/244], loss=69.8983
	step [29/244], loss=69.9958
	step [30/244], loss=70.3907
	step [31/244], loss=75.9911
	step [32/244], loss=63.1971
	step [33/244], loss=82.9289
	step [34/244], loss=76.9435
	step [35/244], loss=83.7438
	step [36/244], loss=73.1641
	step [37/244], loss=65.2407
	step [38/244], loss=87.8161
	step [39/244], loss=70.8317
	step [40/244], loss=72.2968
	step [41/244], loss=79.1739
	step [42/244], loss=62.6793
	step [43/244], loss=59.3086
	step [44/244], loss=75.2672
	step [45/244], loss=65.8282
	step [46/244], loss=78.9011
	step [47/244], loss=62.4576
	step [48/244], loss=78.6990
	step [49/244], loss=91.4230
	step [50/244], loss=67.1242
	step [51/244], loss=67.5229
	step [52/244], loss=62.2096
	step [53/244], loss=74.2197
	step [54/244], loss=54.9993
	step [55/244], loss=77.0340
	step [56/244], loss=83.4979
	step [57/244], loss=62.3605
	step [58/244], loss=61.7012
	step [59/244], loss=59.9597
	step [60/244], loss=66.4894
	step [61/244], loss=71.1135
	step [62/244], loss=66.9772
	step [63/244], loss=66.7339
	step [64/244], loss=66.7303
	step [65/244], loss=78.4382
	step [66/244], loss=78.9333
	step [67/244], loss=50.8905
	step [68/244], loss=74.4995
	step [69/244], loss=53.5583
	step [70/244], loss=67.1974
	step [71/244], loss=66.0171
	step [72/244], loss=68.4268
	step [73/244], loss=80.2901
	step [74/244], loss=67.0926
	step [75/244], loss=83.6625
	step [76/244], loss=74.7294
	step [77/244], loss=75.8088
	step [78/244], loss=66.0282
	step [79/244], loss=69.0438
	step [80/244], loss=58.1349
	step [81/244], loss=73.1973
	step [82/244], loss=70.6463
	step [83/244], loss=65.0246
	step [84/244], loss=67.3730
	step [85/244], loss=56.8560
	step [86/244], loss=54.5127
	step [87/244], loss=92.8812
	step [88/244], loss=78.9989
	step [89/244], loss=64.1344
	step [90/244], loss=53.3568
	step [91/244], loss=68.6997
	step [92/244], loss=66.5237
	step [93/244], loss=86.8164
	step [94/244], loss=55.4941
	step [95/244], loss=66.4524
	step [96/244], loss=77.5497
	step [97/244], loss=77.7326
	step [98/244], loss=61.8832
	step [99/244], loss=63.6667
	step [100/244], loss=76.4370
	step [101/244], loss=79.1509
	step [102/244], loss=76.5421
	step [103/244], loss=69.2844
	step [104/244], loss=80.1476
	step [105/244], loss=76.3041
	step [106/244], loss=75.6905
	step [107/244], loss=66.2681
	step [108/244], loss=80.9671
	step [109/244], loss=66.5616
	step [110/244], loss=59.4678
	step [111/244], loss=61.3771
	step [112/244], loss=60.1882
	step [113/244], loss=59.7840
	step [114/244], loss=73.1179
	step [115/244], loss=76.7354
	step [116/244], loss=67.6539
	step [117/244], loss=56.0154
	step [118/244], loss=55.9756
	step [119/244], loss=64.3857
	step [120/244], loss=69.4035
	step [121/244], loss=91.2180
	step [122/244], loss=78.3597
	step [123/244], loss=73.2607
	step [124/244], loss=65.7463
	step [125/244], loss=67.6488
	step [126/244], loss=75.0671
	step [127/244], loss=69.2289
	step [128/244], loss=74.7743
	step [129/244], loss=67.0599
	step [130/244], loss=64.7148
	step [131/244], loss=74.4370
	step [132/244], loss=72.2002
	step [133/244], loss=73.2772
	step [134/244], loss=69.6787
	step [135/244], loss=66.6912
	step [136/244], loss=74.5608
	step [137/244], loss=58.9878
	step [138/244], loss=62.6644
	step [139/244], loss=66.3995
	step [140/244], loss=63.0294
	step [141/244], loss=55.9403
	step [142/244], loss=63.7036
	step [143/244], loss=83.5727
	step [144/244], loss=69.6895
	step [145/244], loss=66.6926
	step [146/244], loss=67.7498
	step [147/244], loss=75.2773
	step [148/244], loss=85.8366
	step [149/244], loss=70.0400
	step [150/244], loss=78.6161
	step [151/244], loss=48.7081
	step [152/244], loss=69.9504
	step [153/244], loss=80.5271
	step [154/244], loss=78.6888
	step [155/244], loss=80.7706
	step [156/244], loss=73.6566
	step [157/244], loss=78.7973
	step [158/244], loss=60.3137
	step [159/244], loss=80.6626
	step [160/244], loss=59.5454
	step [161/244], loss=50.4861
	step [162/244], loss=83.8689
	step [163/244], loss=62.3284
	step [164/244], loss=74.7240
	step [165/244], loss=71.7145
	step [166/244], loss=56.7287
	step [167/244], loss=63.1462
	step [168/244], loss=72.1995
	step [169/244], loss=74.3776
	step [170/244], loss=59.7112
	step [171/244], loss=68.4626
	step [172/244], loss=78.4656
	step [173/244], loss=73.2322
	step [174/244], loss=71.5395
	step [175/244], loss=82.3938
	step [176/244], loss=80.6719
	step [177/244], loss=89.3878
	step [178/244], loss=67.1286
	step [179/244], loss=62.4643
	step [180/244], loss=70.7490
	step [181/244], loss=72.7005
	step [182/244], loss=68.9197
	step [183/244], loss=76.4201
	step [184/244], loss=65.5731
	step [185/244], loss=74.0384
	step [186/244], loss=74.4615
	step [187/244], loss=71.5686
	step [188/244], loss=75.6986
	step [189/244], loss=84.4662
	step [190/244], loss=69.4782
	step [191/244], loss=68.0407
	step [192/244], loss=57.2384
	step [193/244], loss=60.3008
	step [194/244], loss=61.2368
	step [195/244], loss=78.2255
	step [196/244], loss=67.0060
	step [197/244], loss=56.4893
	step [198/244], loss=75.9609
	step [199/244], loss=66.4962
	step [200/244], loss=66.2415
	step [201/244], loss=61.3698
	step [202/244], loss=61.2340
	step [203/244], loss=69.5004
	step [204/244], loss=73.8916
	step [205/244], loss=75.6710
	step [206/244], loss=67.6584
	step [207/244], loss=73.6126
	step [208/244], loss=70.9980
	step [209/244], loss=68.1539
	step [210/244], loss=71.6940
	step [211/244], loss=70.9624
	step [212/244], loss=58.3824
	step [213/244], loss=61.8508
	step [214/244], loss=78.6521
	step [215/244], loss=85.3401
	step [216/244], loss=71.5505
	step [217/244], loss=73.5903
	step [218/244], loss=71.0036
	step [219/244], loss=75.0656
	step [220/244], loss=71.9633
	step [221/244], loss=68.9578
	step [222/244], loss=71.3898
	step [223/244], loss=62.7165
	step [224/244], loss=70.8944
	step [225/244], loss=67.8915
	step [226/244], loss=63.5417
	step [227/244], loss=74.9854
	step [228/244], loss=74.7775
	step [229/244], loss=69.4707
	step [230/244], loss=66.6470
	step [231/244], loss=65.0737
	step [232/244], loss=63.0267
	step [233/244], loss=56.8934
	step [234/244], loss=91.9922
	step [235/244], loss=83.4222
	step [236/244], loss=89.2377
	step [237/244], loss=61.3716
	step [238/244], loss=65.7723
	step [239/244], loss=72.4832
	step [240/244], loss=62.0872
	step [241/244], loss=72.9450
	step [242/244], loss=62.4850
	step [243/244], loss=82.3122
	step [244/244], loss=71.7666
	Evaluating
	loss=0.0070, precision=0.3734, recall=0.8437, f1=0.5177
Training epoch 93
	step [1/244], loss=62.0552
	step [2/244], loss=60.3171
	step [3/244], loss=65.0659
	step [4/244], loss=80.6289
	step [5/244], loss=56.0317
	step [6/244], loss=78.7831
	step [7/244], loss=81.2859
	step [8/244], loss=68.9041
	step [9/244], loss=68.2900
	step [10/244], loss=78.8469
	step [11/244], loss=59.0048
	step [12/244], loss=74.9500
	step [13/244], loss=64.4083
	step [14/244], loss=83.8209
	step [15/244], loss=67.9436
	step [16/244], loss=67.3225
	step [17/244], loss=80.2968
	step [18/244], loss=61.5336
	step [19/244], loss=57.9468
	step [20/244], loss=86.4895
	step [21/244], loss=73.1064
	step [22/244], loss=65.2597
	step [23/244], loss=67.9287
	step [24/244], loss=77.5281
	step [25/244], loss=72.3882
	step [26/244], loss=84.4207
	step [27/244], loss=53.2161
	step [28/244], loss=64.0596
	step [29/244], loss=73.3133
	step [30/244], loss=76.5195
	step [31/244], loss=66.6567
	step [32/244], loss=79.5057
	step [33/244], loss=81.1574
	step [34/244], loss=61.0172
	step [35/244], loss=63.6539
	step [36/244], loss=56.2226
	step [37/244], loss=69.4764
	step [38/244], loss=71.4042
	step [39/244], loss=63.3360
	step [40/244], loss=66.3367
	step [41/244], loss=68.8606
	step [42/244], loss=63.9586
	step [43/244], loss=59.5110
	step [44/244], loss=71.7402
	step [45/244], loss=69.5701
	step [46/244], loss=63.4950
	step [47/244], loss=65.2595
	step [48/244], loss=57.2534
	step [49/244], loss=58.5409
	step [50/244], loss=59.8882
	step [51/244], loss=60.2140
	step [52/244], loss=77.0553
	step [53/244], loss=78.3008
	step [54/244], loss=93.9825
	step [55/244], loss=51.6574
	step [56/244], loss=58.9795
	step [57/244], loss=64.8893
	step [58/244], loss=71.3879
	step [59/244], loss=72.2579
	step [60/244], loss=74.9179
	step [61/244], loss=74.1077
	step [62/244], loss=71.6395
	step [63/244], loss=79.3635
	step [64/244], loss=67.8794
	step [65/244], loss=71.9375
	step [66/244], loss=76.0025
	step [67/244], loss=67.0961
	step [68/244], loss=68.7872
	step [69/244], loss=68.0861
	step [70/244], loss=69.7201
	step [71/244], loss=68.3989
	step [72/244], loss=63.3119
	step [73/244], loss=71.1383
	step [74/244], loss=63.3259
	step [75/244], loss=82.0734
	step [76/244], loss=76.4963
	step [77/244], loss=58.0185
	step [78/244], loss=63.7959
	step [79/244], loss=78.0839
	step [80/244], loss=59.7608
	step [81/244], loss=60.6508
	step [82/244], loss=85.9347
	step [83/244], loss=73.7169
	step [84/244], loss=58.9385
	step [85/244], loss=73.7606
	step [86/244], loss=63.6414
	step [87/244], loss=67.1564
	step [88/244], loss=63.3668
	step [89/244], loss=78.2071
	step [90/244], loss=72.8515
	step [91/244], loss=78.5103
	step [92/244], loss=64.5176
	step [93/244], loss=85.2588
	step [94/244], loss=71.9376
	step [95/244], loss=63.5209
	step [96/244], loss=71.6637
	step [97/244], loss=76.2493
	step [98/244], loss=77.2572
	step [99/244], loss=60.6874
	step [100/244], loss=75.3068
	step [101/244], loss=67.5006
	step [102/244], loss=61.4537
	step [103/244], loss=72.1492
	step [104/244], loss=72.0887
	step [105/244], loss=73.6376
	step [106/244], loss=60.3392
	step [107/244], loss=74.2691
	step [108/244], loss=77.6683
	step [109/244], loss=77.2626
	step [110/244], loss=70.7966
	step [111/244], loss=91.7281
	step [112/244], loss=70.0624
	step [113/244], loss=82.7883
	step [114/244], loss=64.1745
	step [115/244], loss=56.5345
	step [116/244], loss=81.7337
	step [117/244], loss=73.9892
	step [118/244], loss=55.9067
	step [119/244], loss=66.6066
	step [120/244], loss=73.3890
	step [121/244], loss=76.1255
	step [122/244], loss=69.3169
	step [123/244], loss=68.7212
	step [124/244], loss=78.0572
	step [125/244], loss=77.1862
	step [126/244], loss=60.0199
	step [127/244], loss=60.9202
	step [128/244], loss=74.8582
	step [129/244], loss=59.8574
	step [130/244], loss=70.9338
	step [131/244], loss=70.3152
	step [132/244], loss=70.4917
	step [133/244], loss=79.5211
	step [134/244], loss=63.1944
	step [135/244], loss=57.6705
	step [136/244], loss=75.0340
	step [137/244], loss=63.1572
	step [138/244], loss=84.5301
	step [139/244], loss=71.6944
	step [140/244], loss=83.7919
	step [141/244], loss=64.8999
	step [142/244], loss=69.8553
	step [143/244], loss=67.6598
	step [144/244], loss=78.7536
	step [145/244], loss=63.8276
	step [146/244], loss=77.6501
	step [147/244], loss=80.9854
	step [148/244], loss=55.1895
	step [149/244], loss=62.3996
	step [150/244], loss=66.8687
	step [151/244], loss=66.5163
	step [152/244], loss=66.2834
	step [153/244], loss=70.8035
	step [154/244], loss=60.0536
	step [155/244], loss=57.9317
	step [156/244], loss=65.6215
	step [157/244], loss=60.4002
	step [158/244], loss=67.6768
	step [159/244], loss=66.2256
	step [160/244], loss=61.2870
	step [161/244], loss=61.8285
	step [162/244], loss=68.1287
	step [163/244], loss=88.2956
	step [164/244], loss=68.7572
	step [165/244], loss=79.3663
	step [166/244], loss=77.1600
	step [167/244], loss=74.3807
	step [168/244], loss=86.4399
	step [169/244], loss=73.7493
	step [170/244], loss=64.8172
	step [171/244], loss=66.0526
	step [172/244], loss=82.2644
	step [173/244], loss=59.1249
	step [174/244], loss=69.2028
	step [175/244], loss=69.0835
	step [176/244], loss=70.8629
	step [177/244], loss=66.6166
	step [178/244], loss=57.2813
	step [179/244], loss=58.9630
	step [180/244], loss=64.9986
	step [181/244], loss=60.1414
	step [182/244], loss=60.8156
	step [183/244], loss=68.7810
	step [184/244], loss=79.7244
	step [185/244], loss=54.1252
	step [186/244], loss=83.1803
	step [187/244], loss=68.6699
	step [188/244], loss=52.9993
	step [189/244], loss=71.1567
	step [190/244], loss=51.6811
	step [191/244], loss=65.2132
	step [192/244], loss=89.9340
	step [193/244], loss=70.8560
	step [194/244], loss=60.8028
	step [195/244], loss=70.6863
	step [196/244], loss=67.7813
	step [197/244], loss=66.7114
	step [198/244], loss=70.2840
	step [199/244], loss=74.5924
	step [200/244], loss=67.2961
	step [201/244], loss=73.2634
	step [202/244], loss=88.3706
	step [203/244], loss=75.4731
	step [204/244], loss=68.1371
	step [205/244], loss=82.2906
	step [206/244], loss=67.8222
	step [207/244], loss=75.8251
	step [208/244], loss=71.4578
	step [209/244], loss=65.6859
	step [210/244], loss=75.8715
	step [211/244], loss=60.8082
	step [212/244], loss=62.9951
	step [213/244], loss=89.1135
	step [214/244], loss=81.4431
	step [215/244], loss=79.7249
	step [216/244], loss=70.5025
	step [217/244], loss=75.1711
	step [218/244], loss=75.0887
	step [219/244], loss=74.6082
	step [220/244], loss=66.3844
	step [221/244], loss=84.1571
	step [222/244], loss=66.4446
	step [223/244], loss=66.7758
	step [224/244], loss=75.3785
	step [225/244], loss=52.8754
	step [226/244], loss=89.4772
	step [227/244], loss=82.4424
	step [228/244], loss=70.4953
	step [229/244], loss=74.7682
	step [230/244], loss=61.4377
	step [231/244], loss=66.5793
	step [232/244], loss=75.8942
	step [233/244], loss=64.8381
	step [234/244], loss=71.8075
	step [235/244], loss=76.8204
	step [236/244], loss=79.3463
	step [237/244], loss=82.3514
	step [238/244], loss=72.9103
	step [239/244], loss=60.3998
	step [240/244], loss=70.1630
	step [241/244], loss=82.0373
	step [242/244], loss=72.5218
	step [243/244], loss=73.5837
	step [244/244], loss=72.9427
	Evaluating
	loss=0.0073, precision=0.3604, recall=0.8491, f1=0.5060
Training epoch 94
	step [1/244], loss=83.7530
	step [2/244], loss=74.9136
	step [3/244], loss=55.4266
	step [4/244], loss=60.9429
	step [5/244], loss=74.9804
	step [6/244], loss=84.2039
	step [7/244], loss=72.9027
	step [8/244], loss=69.4327
	step [9/244], loss=73.1025
	step [10/244], loss=76.5375
	step [11/244], loss=58.0437
	step [12/244], loss=80.1740
	step [13/244], loss=80.7943
	step [14/244], loss=61.6666
	step [15/244], loss=70.4587
	step [16/244], loss=64.1270
	step [17/244], loss=58.4910
	step [18/244], loss=74.8895
	step [19/244], loss=103.3538
	step [20/244], loss=66.4502
	step [21/244], loss=58.8581
	step [22/244], loss=78.9668
	step [23/244], loss=60.5971
	step [24/244], loss=77.1300
	step [25/244], loss=70.0876
	step [26/244], loss=78.1214
	step [27/244], loss=72.1601
	step [28/244], loss=73.8124
	step [29/244], loss=76.5459
	step [30/244], loss=74.4078
	step [31/244], loss=66.7579
	step [32/244], loss=77.8266
	step [33/244], loss=61.9036
	step [34/244], loss=62.9711
	step [35/244], loss=69.5748
	step [36/244], loss=62.2654
	step [37/244], loss=63.9143
	step [38/244], loss=74.6340
	step [39/244], loss=78.9501
	step [40/244], loss=76.9742
	step [41/244], loss=52.1023
	step [42/244], loss=64.6477
	step [43/244], loss=72.2607
	step [44/244], loss=72.6157
	step [45/244], loss=75.7662
	step [46/244], loss=69.5052
	step [47/244], loss=62.1403
	step [48/244], loss=75.6930
	step [49/244], loss=73.0688
	step [50/244], loss=77.1971
	step [51/244], loss=73.1512
	step [52/244], loss=60.2138
	step [53/244], loss=63.3203
	step [54/244], loss=60.7109
	step [55/244], loss=79.1043
	step [56/244], loss=78.3377
	step [57/244], loss=79.9105
	step [58/244], loss=78.9802
	step [59/244], loss=59.4968
	step [60/244], loss=84.5444
	step [61/244], loss=69.6103
	step [62/244], loss=65.1536
	step [63/244], loss=59.3152
	step [64/244], loss=56.8178
	step [65/244], loss=66.3084
	step [66/244], loss=78.3382
	step [67/244], loss=71.9538
	step [68/244], loss=70.2805
	step [69/244], loss=71.7911
	step [70/244], loss=76.1853
	step [71/244], loss=75.8336
	step [72/244], loss=79.9993
	step [73/244], loss=74.8187
	step [74/244], loss=59.1170
	step [75/244], loss=75.2205
	step [76/244], loss=67.1847
	step [77/244], loss=75.1425
	step [78/244], loss=66.2038
	step [79/244], loss=63.0195
	step [80/244], loss=79.5439
	step [81/244], loss=66.9071
	step [82/244], loss=62.3582
	step [83/244], loss=78.8638
	step [84/244], loss=72.2193
	step [85/244], loss=80.4091
	step [86/244], loss=76.5740
	step [87/244], loss=60.0271
	step [88/244], loss=63.7655
	step [89/244], loss=86.9008
	step [90/244], loss=74.5003
	step [91/244], loss=74.7740
	step [92/244], loss=66.7475
	step [93/244], loss=64.7274
	step [94/244], loss=76.3893
	step [95/244], loss=87.9329
	step [96/244], loss=65.7924
	step [97/244], loss=69.1414
	step [98/244], loss=69.5868
	step [99/244], loss=70.4046
	step [100/244], loss=63.4250
	step [101/244], loss=74.3933
	step [102/244], loss=78.9746
	step [103/244], loss=58.3521
	step [104/244], loss=81.0211
	step [105/244], loss=64.4281
	step [106/244], loss=62.0202
	step [107/244], loss=60.9642
	step [108/244], loss=89.6674
	step [109/244], loss=50.1763
	step [110/244], loss=78.3367
	step [111/244], loss=74.3153
	step [112/244], loss=68.5535
	step [113/244], loss=71.8255
	step [114/244], loss=65.5759
	step [115/244], loss=68.0569
	step [116/244], loss=74.9305
	step [117/244], loss=70.1327
	step [118/244], loss=60.7388
	step [119/244], loss=71.0382
	step [120/244], loss=76.9580
	step [121/244], loss=72.8421
	step [122/244], loss=62.5402
	step [123/244], loss=66.8057
	step [124/244], loss=57.0172
	step [125/244], loss=60.5611
	step [126/244], loss=67.6673
	step [127/244], loss=74.7841
	step [128/244], loss=66.9518
	step [129/244], loss=68.3303
	step [130/244], loss=64.1731
	step [131/244], loss=57.4724
	step [132/244], loss=70.3359
	step [133/244], loss=60.1358
	step [134/244], loss=76.1547
	step [135/244], loss=59.5436
	step [136/244], loss=64.2617
	step [137/244], loss=84.5253
	step [138/244], loss=56.8918
	step [139/244], loss=57.8784
	step [140/244], loss=73.1428
	step [141/244], loss=67.8801
	step [142/244], loss=70.3095
	step [143/244], loss=55.5104
	step [144/244], loss=77.3131
	step [145/244], loss=54.4741
	step [146/244], loss=89.4248
	step [147/244], loss=61.3790
	step [148/244], loss=69.4829
	step [149/244], loss=81.6464
	step [150/244], loss=60.6964
	step [151/244], loss=74.2238
	step [152/244], loss=77.2942
	step [153/244], loss=64.8111
	step [154/244], loss=69.1344
	step [155/244], loss=67.2718
	step [156/244], loss=68.9323
	step [157/244], loss=64.6486
	step [158/244], loss=69.5023
	step [159/244], loss=63.7337
	step [160/244], loss=79.8512
	step [161/244], loss=66.9643
	step [162/244], loss=71.9902
	step [163/244], loss=75.5803
	step [164/244], loss=62.5067
	step [165/244], loss=66.9375
	step [166/244], loss=62.0227
	step [167/244], loss=76.2874
	step [168/244], loss=72.1234
	step [169/244], loss=57.4793
	step [170/244], loss=74.1792
	step [171/244], loss=74.6966
	step [172/244], loss=80.0702
	step [173/244], loss=57.7193
	step [174/244], loss=76.4760
	step [175/244], loss=65.3111
	step [176/244], loss=62.8444
	step [177/244], loss=62.2556
	step [178/244], loss=59.2816
	step [179/244], loss=67.2115
	step [180/244], loss=72.1580
	step [181/244], loss=73.9829
	step [182/244], loss=60.2353
	step [183/244], loss=79.9140
	step [184/244], loss=67.8783
	step [185/244], loss=56.7899
	step [186/244], loss=55.2782
	step [187/244], loss=68.4232
	step [188/244], loss=66.8046
	step [189/244], loss=60.9491
	step [190/244], loss=73.9508
	step [191/244], loss=64.8395
	step [192/244], loss=65.7396
	step [193/244], loss=74.4191
	step [194/244], loss=79.4780
	step [195/244], loss=75.1061
	step [196/244], loss=80.5952
	step [197/244], loss=56.9419
	step [198/244], loss=71.5470
	step [199/244], loss=60.0985
	step [200/244], loss=78.0952
	step [201/244], loss=63.6164
	step [202/244], loss=66.4441
	step [203/244], loss=64.1635
	step [204/244], loss=79.7363
	step [205/244], loss=80.8708
	step [206/244], loss=61.1868
	step [207/244], loss=62.9758
	step [208/244], loss=58.9210
	step [209/244], loss=67.7206
	step [210/244], loss=65.9392
	step [211/244], loss=63.1758
	step [212/244], loss=63.3751
	step [213/244], loss=58.1181
	step [214/244], loss=77.5444
	step [215/244], loss=51.0808
	step [216/244], loss=68.7097
	step [217/244], loss=84.5454
	step [218/244], loss=78.7471
	step [219/244], loss=67.8483
	step [220/244], loss=68.9171
	step [221/244], loss=77.8867
	step [222/244], loss=72.1435
	step [223/244], loss=85.9816
	step [224/244], loss=75.9157
	step [225/244], loss=82.7463
	step [226/244], loss=83.2136
	step [227/244], loss=71.7776
	step [228/244], loss=73.8141
	step [229/244], loss=89.0924
	step [230/244], loss=71.7062
	step [231/244], loss=75.8299
	step [232/244], loss=55.6508
	step [233/244], loss=64.3070
	step [234/244], loss=80.4774
	step [235/244], loss=63.9863
	step [236/244], loss=69.4016
	step [237/244], loss=59.9330
	step [238/244], loss=75.6347
	step [239/244], loss=63.3904
	step [240/244], loss=57.9222
	step [241/244], loss=68.4518
	step [242/244], loss=69.4253
	step [243/244], loss=73.5491
	step [244/244], loss=62.4487
	Evaluating
	loss=0.0069, precision=0.3776, recall=0.8474, f1=0.5224
Training epoch 95
	step [1/244], loss=73.8736
	step [2/244], loss=62.4818
	step [3/244], loss=55.8167
	step [4/244], loss=69.4242
	step [5/244], loss=69.7179
	step [6/244], loss=86.2050
	step [7/244], loss=47.2187
	step [8/244], loss=63.5289
	step [9/244], loss=60.8207
	step [10/244], loss=75.2660
	step [11/244], loss=53.9695
	step [12/244], loss=75.0506
	step [13/244], loss=67.8758
	step [14/244], loss=60.2107
	step [15/244], loss=62.8507
	step [16/244], loss=76.0223
	step [17/244], loss=74.5463
	step [18/244], loss=84.4369
	step [19/244], loss=73.2132
	step [20/244], loss=60.9350
	step [21/244], loss=69.7832
	step [22/244], loss=57.9526
	step [23/244], loss=58.7589
	step [24/244], loss=53.5460
	step [25/244], loss=77.5019
	step [26/244], loss=72.4098
	step [27/244], loss=64.3857
	step [28/244], loss=66.9801
	step [29/244], loss=77.9870
	step [30/244], loss=72.1766
	step [31/244], loss=77.7798
	step [32/244], loss=91.4417
	step [33/244], loss=59.3836
	step [34/244], loss=58.4435
	step [35/244], loss=82.5350
	step [36/244], loss=75.0989
	step [37/244], loss=56.8641
	step [38/244], loss=61.6133
	step [39/244], loss=67.2837
	step [40/244], loss=71.3331
	step [41/244], loss=71.9590
	step [42/244], loss=83.1579
	step [43/244], loss=61.4740
	step [44/244], loss=55.8189
	step [45/244], loss=75.0535
	step [46/244], loss=53.9705
	step [47/244], loss=63.6343
	step [48/244], loss=66.7402
	step [49/244], loss=66.7298
	step [50/244], loss=59.8363
	step [51/244], loss=76.1959
	step [52/244], loss=51.9528
	step [53/244], loss=64.1763
	step [54/244], loss=65.9355
	step [55/244], loss=64.9462
	step [56/244], loss=88.5427
	step [57/244], loss=67.9086
	step [58/244], loss=77.8387
	step [59/244], loss=66.2966
	step [60/244], loss=72.4964
	step [61/244], loss=63.4823
	step [62/244], loss=79.6853
	step [63/244], loss=61.7364
	step [64/244], loss=74.7104
	step [65/244], loss=79.7244
	step [66/244], loss=72.0886
	step [67/244], loss=80.2358
	step [68/244], loss=67.9496
	step [69/244], loss=83.3469
	step [70/244], loss=55.3415
	step [71/244], loss=81.6157
	step [72/244], loss=51.2949
	step [73/244], loss=63.4362
	step [74/244], loss=84.9912
	step [75/244], loss=78.0538
	step [76/244], loss=76.5874
	step [77/244], loss=93.4709
	step [78/244], loss=66.3436
	step [79/244], loss=68.0586
	step [80/244], loss=67.8851
	step [81/244], loss=70.4592
	step [82/244], loss=61.4502
	step [83/244], loss=61.1942
	step [84/244], loss=76.4225
	step [85/244], loss=74.6632
	step [86/244], loss=62.9265
	step [87/244], loss=93.2277
	step [88/244], loss=63.2116
	step [89/244], loss=62.8114
	step [90/244], loss=58.7664
	step [91/244], loss=70.7758
	step [92/244], loss=74.7354
	step [93/244], loss=78.9550
	step [94/244], loss=74.3834
	step [95/244], loss=77.5852
	step [96/244], loss=68.9102
	step [97/244], loss=67.4857
	step [98/244], loss=89.6756
	step [99/244], loss=71.7445
	step [100/244], loss=71.9028
	step [101/244], loss=78.9398
	step [102/244], loss=72.3057
	step [103/244], loss=77.5440
	step [104/244], loss=74.6041
	step [105/244], loss=68.5245
	step [106/244], loss=75.1590
	step [107/244], loss=63.0704
	step [108/244], loss=71.1350
	step [109/244], loss=74.4846
	step [110/244], loss=64.1061
	step [111/244], loss=66.6004
	step [112/244], loss=70.4817
	step [113/244], loss=64.2579
	step [114/244], loss=74.8293
	step [115/244], loss=83.7738
	step [116/244], loss=69.1536
	step [117/244], loss=60.2698
	step [118/244], loss=66.7984
	step [119/244], loss=66.5559
	step [120/244], loss=76.3708
	step [121/244], loss=78.8853
	step [122/244], loss=62.8265
	step [123/244], loss=69.5030
	step [124/244], loss=64.0954
	step [125/244], loss=68.2875
	step [126/244], loss=81.8899
	step [127/244], loss=66.2286
	step [128/244], loss=72.4660
	step [129/244], loss=80.6794
	step [130/244], loss=61.1899
	step [131/244], loss=73.2118
	step [132/244], loss=60.8130
	step [133/244], loss=76.9666
	step [134/244], loss=64.6557
	step [135/244], loss=60.8515
	step [136/244], loss=94.1217
	step [137/244], loss=83.2629
	step [138/244], loss=74.6193
	step [139/244], loss=67.6673
	step [140/244], loss=68.4042
	step [141/244], loss=64.9715
	step [142/244], loss=66.4703
	step [143/244], loss=71.0267
	step [144/244], loss=62.7680
	step [145/244], loss=63.4275
	step [146/244], loss=67.2361
	step [147/244], loss=75.9933
	step [148/244], loss=66.7801
	step [149/244], loss=60.9075
	step [150/244], loss=72.7316
	step [151/244], loss=81.4645
	step [152/244], loss=67.7690
	step [153/244], loss=73.1108
	step [154/244], loss=58.1950
	step [155/244], loss=69.5983
	step [156/244], loss=65.1608
	step [157/244], loss=73.9815
	step [158/244], loss=79.0836
	step [159/244], loss=63.5581
	step [160/244], loss=74.9564
	step [161/244], loss=71.6000
	step [162/244], loss=63.9293
	step [163/244], loss=64.7730
	step [164/244], loss=84.7827
	step [165/244], loss=77.3095
	step [166/244], loss=62.7232
	step [167/244], loss=60.8357
	step [168/244], loss=75.8195
	step [169/244], loss=65.5328
	step [170/244], loss=70.6728
	step [171/244], loss=68.3990
	step [172/244], loss=76.9186
	step [173/244], loss=68.5145
	step [174/244], loss=62.7714
	step [175/244], loss=57.5658
	step [176/244], loss=63.5603
	step [177/244], loss=64.9777
	step [178/244], loss=64.4990
	step [179/244], loss=71.8125
	step [180/244], loss=81.5330
	step [181/244], loss=67.6994
	step [182/244], loss=67.8287
	step [183/244], loss=77.8525
	step [184/244], loss=55.0565
	step [185/244], loss=60.5203
	step [186/244], loss=62.0091
	step [187/244], loss=56.1879
	step [188/244], loss=68.3881
	step [189/244], loss=58.1742
	step [190/244], loss=62.2214
	step [191/244], loss=84.6815
	step [192/244], loss=61.2697
	step [193/244], loss=69.4568
	step [194/244], loss=73.7234
	step [195/244], loss=77.5872
	step [196/244], loss=69.0166
	step [197/244], loss=67.9626
	step [198/244], loss=92.2516
	step [199/244], loss=83.7596
	step [200/244], loss=67.2370
	step [201/244], loss=71.8783
	step [202/244], loss=87.6398
	step [203/244], loss=66.5797
	step [204/244], loss=65.7606
	step [205/244], loss=54.9614
	step [206/244], loss=66.8346
	step [207/244], loss=72.5081
	step [208/244], loss=74.4003
	step [209/244], loss=75.9962
	step [210/244], loss=75.7018
	step [211/244], loss=87.2612
	step [212/244], loss=67.8227
	step [213/244], loss=75.6805
	step [214/244], loss=74.9107
	step [215/244], loss=63.0736
	step [216/244], loss=64.6573
	step [217/244], loss=78.4907
	step [218/244], loss=71.6382
	step [219/244], loss=56.9462
	step [220/244], loss=70.3042
	step [221/244], loss=66.2175
	step [222/244], loss=69.4474
	step [223/244], loss=56.5688
	step [224/244], loss=76.6380
	step [225/244], loss=80.7546
	step [226/244], loss=66.9273
	step [227/244], loss=58.4630
	step [228/244], loss=56.5564
	step [229/244], loss=94.2286
	step [230/244], loss=70.1366
	step [231/244], loss=63.4002
	step [232/244], loss=49.5938
	step [233/244], loss=73.0238
	step [234/244], loss=67.4750
	step [235/244], loss=61.7023
	step [236/244], loss=61.8281
	step [237/244], loss=79.2749
	step [238/244], loss=70.0958
	step [239/244], loss=70.6452
	step [240/244], loss=55.8451
	step [241/244], loss=71.6328
	step [242/244], loss=61.0485
	step [243/244], loss=80.8816
	step [244/244], loss=69.2385
	Evaluating
	loss=0.0066, precision=0.3932, recall=0.8479, f1=0.5372
saving model as: 1_saved_model.pth
Training epoch 96
	step [1/244], loss=61.9502
	step [2/244], loss=68.1632
	step [3/244], loss=61.3608
	step [4/244], loss=57.9301
	step [5/244], loss=66.2489
	step [6/244], loss=59.5883
	step [7/244], loss=63.3802
	step [8/244], loss=74.4357
	step [9/244], loss=61.1109
	step [10/244], loss=63.8336
	step [11/244], loss=55.0618
	step [12/244], loss=74.7454
	step [13/244], loss=82.5613
	step [14/244], loss=76.7316
	step [15/244], loss=63.0080
	step [16/244], loss=75.6031
	step [17/244], loss=74.5740
	step [18/244], loss=65.4629
	step [19/244], loss=70.9470
	step [20/244], loss=67.8637
	step [21/244], loss=76.2973
	step [22/244], loss=73.0305
	step [23/244], loss=69.0447
	step [24/244], loss=66.3527
	step [25/244], loss=73.9897
	step [26/244], loss=61.7347
	step [27/244], loss=71.7306
	step [28/244], loss=71.2483
	step [29/244], loss=79.8269
	step [30/244], loss=83.8381
	step [31/244], loss=81.0501
	step [32/244], loss=74.6447
	step [33/244], loss=67.8523
	step [34/244], loss=59.0999
	step [35/244], loss=61.1539
	step [36/244], loss=73.2890
	step [37/244], loss=61.0062
	step [38/244], loss=59.1787
	step [39/244], loss=68.4519
	step [40/244], loss=69.9082
	step [41/244], loss=79.0453
	step [42/244], loss=56.9474
	step [43/244], loss=83.0329
	step [44/244], loss=98.3429
	step [45/244], loss=68.4321
	step [46/244], loss=57.1653
	step [47/244], loss=88.2321
	step [48/244], loss=53.7656
	step [49/244], loss=67.2516
	step [50/244], loss=59.0844
	step [51/244], loss=63.7506
	step [52/244], loss=76.7444
	step [53/244], loss=71.8844
	step [54/244], loss=61.4529
	step [55/244], loss=83.8492
	step [56/244], loss=74.4152
	step [57/244], loss=75.5439
	step [58/244], loss=58.4277
	step [59/244], loss=77.0399
	step [60/244], loss=66.6886
	step [61/244], loss=72.5385
	step [62/244], loss=62.9400
	step [63/244], loss=71.4925
	step [64/244], loss=70.0093
	step [65/244], loss=58.4321
	step [66/244], loss=73.6364
	step [67/244], loss=76.5481
	step [68/244], loss=59.3041
	step [69/244], loss=66.8258
	step [70/244], loss=72.1981
	step [71/244], loss=83.3322
	step [72/244], loss=76.9818
	step [73/244], loss=70.4223
	step [74/244], loss=72.5615
	step [75/244], loss=45.0589
	step [76/244], loss=71.0920
	step [77/244], loss=70.2362
	step [78/244], loss=68.8425
	step [79/244], loss=69.3934
	step [80/244], loss=78.2632
	step [81/244], loss=67.8439
	step [82/244], loss=89.5618
	step [83/244], loss=66.9588
	step [84/244], loss=70.4801
	step [85/244], loss=65.4858
	step [86/244], loss=68.1673
	step [87/244], loss=65.5531
	step [88/244], loss=75.2978
	step [89/244], loss=67.2473
	step [90/244], loss=60.9581
	step [91/244], loss=77.0855
	step [92/244], loss=83.6574
	step [93/244], loss=68.3140
	step [94/244], loss=62.1290
	step [95/244], loss=65.0582
	step [96/244], loss=56.3536
	step [97/244], loss=64.4594
	step [98/244], loss=71.5011
	step [99/244], loss=87.1682
	step [100/244], loss=66.9258
	step [101/244], loss=82.8722
	step [102/244], loss=65.3398
	step [103/244], loss=75.0648
	step [104/244], loss=63.8959
	step [105/244], loss=62.0409
	step [106/244], loss=61.9410
	step [107/244], loss=67.0561
	step [108/244], loss=77.8971
	step [109/244], loss=73.3921
	step [110/244], loss=65.1942
	step [111/244], loss=75.3331
	step [112/244], loss=84.4056
	step [113/244], loss=58.7210
	step [114/244], loss=76.2896
	step [115/244], loss=72.4144
	step [116/244], loss=67.4287
	step [117/244], loss=57.7947
	step [118/244], loss=54.8087
	step [119/244], loss=61.6934
	step [120/244], loss=72.7010
	step [121/244], loss=69.9444
	step [122/244], loss=70.7889
	step [123/244], loss=65.7285
	step [124/244], loss=82.9036
	step [125/244], loss=63.1231
	step [126/244], loss=69.4185
	step [127/244], loss=73.5917
	step [128/244], loss=68.0757
	step [129/244], loss=61.2158
	step [130/244], loss=66.5735
	step [131/244], loss=56.9860
	step [132/244], loss=71.3208
	step [133/244], loss=88.4397
	step [134/244], loss=70.6517
	step [135/244], loss=62.8322
	step [136/244], loss=61.8532
	step [137/244], loss=79.7479
	step [138/244], loss=70.9853
	step [139/244], loss=72.0561
	step [140/244], loss=68.2060
	step [141/244], loss=65.5447
	step [142/244], loss=70.5476
	step [143/244], loss=81.9775
	step [144/244], loss=54.0323
	step [145/244], loss=71.6908
	step [146/244], loss=79.6820
	step [147/244], loss=70.8374
	step [148/244], loss=54.6799
	step [149/244], loss=79.7362
	step [150/244], loss=70.6726
	step [151/244], loss=61.8848
	step [152/244], loss=70.2854
	step [153/244], loss=63.1511
	step [154/244], loss=72.5192
	step [155/244], loss=82.3324
	step [156/244], loss=66.4329
	step [157/244], loss=72.5903
	step [158/244], loss=72.6243
	step [159/244], loss=67.1413
	step [160/244], loss=72.3263
	step [161/244], loss=58.2742
	step [162/244], loss=76.3266
	step [163/244], loss=75.0101
	step [164/244], loss=82.6430
	step [165/244], loss=73.6139
	step [166/244], loss=73.2573
	step [167/244], loss=66.2561
	step [168/244], loss=66.6133
	step [169/244], loss=70.1750
	step [170/244], loss=72.3308
	step [171/244], loss=69.8333
	step [172/244], loss=64.1638
	step [173/244], loss=64.8833
	step [174/244], loss=64.7189
	step [175/244], loss=82.9558
	step [176/244], loss=68.3049
	step [177/244], loss=62.3405
	step [178/244], loss=72.7510
	step [179/244], loss=60.2732
	step [180/244], loss=68.9474
	step [181/244], loss=61.6554
	step [182/244], loss=72.3428
	step [183/244], loss=67.5468
	step [184/244], loss=61.1909
	step [185/244], loss=70.1731
	step [186/244], loss=74.4807
	step [187/244], loss=71.0057
	step [188/244], loss=56.5737
	step [189/244], loss=64.8123
	step [190/244], loss=68.6306
	step [191/244], loss=62.7616
	step [192/244], loss=88.5641
	step [193/244], loss=82.7331
	step [194/244], loss=64.1531
	step [195/244], loss=69.1042
	step [196/244], loss=77.3393
	step [197/244], loss=62.5133
	step [198/244], loss=60.9001
	step [199/244], loss=74.9659
	step [200/244], loss=61.1091
	step [201/244], loss=48.1965
	step [202/244], loss=74.7461
	step [203/244], loss=66.3132
	step [204/244], loss=72.7462
	step [205/244], loss=63.2493
	step [206/244], loss=75.7952
	step [207/244], loss=68.9548
	step [208/244], loss=61.0605
	step [209/244], loss=70.4190
	step [210/244], loss=60.8796
	step [211/244], loss=59.1011
	step [212/244], loss=60.8730
	step [213/244], loss=53.2030
	step [214/244], loss=76.0292
	step [215/244], loss=69.7984
	step [216/244], loss=88.3424
	step [217/244], loss=73.4625
	step [218/244], loss=72.7798
	step [219/244], loss=73.5859
	step [220/244], loss=61.4110
	step [221/244], loss=60.0881
	step [222/244], loss=78.8127
	step [223/244], loss=75.4727
	step [224/244], loss=68.6685
	step [225/244], loss=59.8708
	step [226/244], loss=62.1565
	step [227/244], loss=70.9562
	step [228/244], loss=78.2112
	step [229/244], loss=78.9755
	step [230/244], loss=84.0978
	step [231/244], loss=71.1060
	step [232/244], loss=61.9425
	step [233/244], loss=73.4167
	step [234/244], loss=67.1007
	step [235/244], loss=85.4574
	step [236/244], loss=64.9036
	step [237/244], loss=70.9165
	step [238/244], loss=73.9443
	step [239/244], loss=66.3162
	step [240/244], loss=61.2259
	step [241/244], loss=90.5767
	step [242/244], loss=65.5949
	step [243/244], loss=64.4339
	step [244/244], loss=73.7742
	Evaluating
	loss=0.0079, precision=0.3291, recall=0.8479, f1=0.4741
Training epoch 97
	step [1/244], loss=75.2731
	step [2/244], loss=82.9720
	step [3/244], loss=72.1896
	step [4/244], loss=70.6952
	step [5/244], loss=64.2664
	step [6/244], loss=75.1716
	step [7/244], loss=74.8262
	step [8/244], loss=70.2888
	step [9/244], loss=71.1369
	step [10/244], loss=55.9872
	step [11/244], loss=74.2555
	step [12/244], loss=71.8201
	step [13/244], loss=67.1426
	step [14/244], loss=61.4864
	step [15/244], loss=77.8295
	step [16/244], loss=73.8104
	step [17/244], loss=85.4660
	step [18/244], loss=56.0910
	step [19/244], loss=62.1768
	step [20/244], loss=71.1441
	step [21/244], loss=68.9645
	step [22/244], loss=69.3986
	step [23/244], loss=53.3947
	step [24/244], loss=83.7233
	step [25/244], loss=64.2930
	step [26/244], loss=98.6175
	step [27/244], loss=67.0706
	step [28/244], loss=72.1871
	step [29/244], loss=81.2971
	step [30/244], loss=77.3540
	step [31/244], loss=66.6305
	step [32/244], loss=68.6182
	step [33/244], loss=69.8869
	step [34/244], loss=66.5901
	step [35/244], loss=71.2221
	step [36/244], loss=69.6855
	step [37/244], loss=67.3384
	step [38/244], loss=80.6730
	step [39/244], loss=65.3963
	step [40/244], loss=79.3500
	step [41/244], loss=70.5613
	step [42/244], loss=71.7799
	step [43/244], loss=65.7729
	step [44/244], loss=69.3368
	step [45/244], loss=63.4625
	step [46/244], loss=68.5208
	step [47/244], loss=72.2502
	step [48/244], loss=78.8646
	step [49/244], loss=85.1880
	step [50/244], loss=77.4476
	step [51/244], loss=65.8914
	step [52/244], loss=58.6141
	step [53/244], loss=63.3626
	step [54/244], loss=81.3101
	step [55/244], loss=63.2144
	step [56/244], loss=50.6936
	step [57/244], loss=70.7969
	step [58/244], loss=85.2122
	step [59/244], loss=58.3513
	step [60/244], loss=49.1660
	step [61/244], loss=62.3020
	step [62/244], loss=78.8158
	step [63/244], loss=67.5041
	step [64/244], loss=82.7620
	step [65/244], loss=68.0611
	step [66/244], loss=72.3240
	step [67/244], loss=72.7656
	step [68/244], loss=60.5437
	step [69/244], loss=62.1894
	step [70/244], loss=64.9684
	step [71/244], loss=66.9013
	step [72/244], loss=75.6811
	step [73/244], loss=63.3900
	step [74/244], loss=61.2471
	step [75/244], loss=74.2690
	step [76/244], loss=67.1412
	step [77/244], loss=66.5714
	step [78/244], loss=64.5292
	step [79/244], loss=57.7707
	step [80/244], loss=80.1108
	step [81/244], loss=69.4786
	step [82/244], loss=70.1530
	step [83/244], loss=65.9632
	step [84/244], loss=54.6946
	step [85/244], loss=71.7029
	step [86/244], loss=58.4004
	step [87/244], loss=74.8147
	step [88/244], loss=81.1588
	step [89/244], loss=75.2407
	step [90/244], loss=76.0257
	step [91/244], loss=70.4523
	step [92/244], loss=52.5695
	step [93/244], loss=66.9935
	step [94/244], loss=62.7758
	step [95/244], loss=78.0675
	step [96/244], loss=82.8729
	step [97/244], loss=69.2645
	step [98/244], loss=68.6378
	step [99/244], loss=47.7042
	step [100/244], loss=78.4135
	step [101/244], loss=58.3586
	step [102/244], loss=67.5130
	step [103/244], loss=68.8728
	step [104/244], loss=68.1071
	step [105/244], loss=59.0100
	step [106/244], loss=80.3071
	step [107/244], loss=56.7211
	step [108/244], loss=70.3105
	step [109/244], loss=63.0113
	step [110/244], loss=67.8047
	step [111/244], loss=72.1664
	step [112/244], loss=71.1938
	step [113/244], loss=53.3129
	step [114/244], loss=80.1395
	step [115/244], loss=68.7447
	step [116/244], loss=53.9350
	step [117/244], loss=65.7371
	step [118/244], loss=61.1428
	step [119/244], loss=67.6544
	step [120/244], loss=88.0593
	step [121/244], loss=92.8285
	step [122/244], loss=74.6687
	step [123/244], loss=78.5643
	step [124/244], loss=79.6609
	step [125/244], loss=69.6770
	step [126/244], loss=66.6307
	step [127/244], loss=67.9152
	step [128/244], loss=95.1503
	step [129/244], loss=72.8366
	step [130/244], loss=60.2567
	step [131/244], loss=77.2725
	step [132/244], loss=69.7282
	step [133/244], loss=75.7693
	step [134/244], loss=73.3560
	step [135/244], loss=50.3554
	step [136/244], loss=74.6053
	step [137/244], loss=53.7864
	step [138/244], loss=67.9487
	step [139/244], loss=71.0254
	step [140/244], loss=67.3776
	step [141/244], loss=68.9927
	step [142/244], loss=69.7518
	step [143/244], loss=79.4126
	step [144/244], loss=71.3958
	step [145/244], loss=77.1200
	step [146/244], loss=56.2460
	step [147/244], loss=70.6271
	step [148/244], loss=83.8760
	step [149/244], loss=62.7390
	step [150/244], loss=61.3057
	step [151/244], loss=61.2550
	step [152/244], loss=79.0655
	step [153/244], loss=54.5826
	step [154/244], loss=73.2194
	step [155/244], loss=76.2461
	step [156/244], loss=63.8475
	step [157/244], loss=65.9520
	step [158/244], loss=60.7870
	step [159/244], loss=69.9892
	step [160/244], loss=86.5294
	step [161/244], loss=70.8138
	step [162/244], loss=71.2918
	step [163/244], loss=60.9543
	step [164/244], loss=63.6630
	step [165/244], loss=74.2870
	step [166/244], loss=80.2273
	step [167/244], loss=70.1771
	step [168/244], loss=66.9674
	step [169/244], loss=62.8033
	step [170/244], loss=72.7566
	step [171/244], loss=75.0524
	step [172/244], loss=84.2846
	step [173/244], loss=85.9224
	step [174/244], loss=49.8667
	step [175/244], loss=72.7364
	step [176/244], loss=70.4419
	step [177/244], loss=69.4380
	step [178/244], loss=59.2665
	step [179/244], loss=65.7620
	step [180/244], loss=59.6432
	step [181/244], loss=73.1861
	step [182/244], loss=76.3942
	step [183/244], loss=72.5775
	step [184/244], loss=81.0512
	step [185/244], loss=73.3809
	step [186/244], loss=62.9028
	step [187/244], loss=55.8501
	step [188/244], loss=65.0378
	step [189/244], loss=62.8404
	step [190/244], loss=77.8569
	step [191/244], loss=61.1841
	step [192/244], loss=78.2172
	step [193/244], loss=59.5947
	step [194/244], loss=61.4081
	step [195/244], loss=66.4346
	step [196/244], loss=73.0173
	step [197/244], loss=65.3549
	step [198/244], loss=66.0546
	step [199/244], loss=62.1686
	step [200/244], loss=64.2286
	step [201/244], loss=72.9049
	step [202/244], loss=66.4271
	step [203/244], loss=58.3160
	step [204/244], loss=77.4523
	step [205/244], loss=60.8288
	step [206/244], loss=74.4660
	step [207/244], loss=51.0774
	step [208/244], loss=70.5267
	step [209/244], loss=75.0391
	step [210/244], loss=66.1436
	step [211/244], loss=71.0121
	step [212/244], loss=70.9634
	step [213/244], loss=67.3864
	step [214/244], loss=72.0686
	step [215/244], loss=76.1372
	step [216/244], loss=67.9262
	step [217/244], loss=63.2451
	step [218/244], loss=69.1970
	step [219/244], loss=60.0705
	step [220/244], loss=68.3684
	step [221/244], loss=70.2286
	step [222/244], loss=89.3801
	step [223/244], loss=75.4647
	step [224/244], loss=61.5776
	step [225/244], loss=55.9399
	step [226/244], loss=72.4160
	step [227/244], loss=78.2542
	step [228/244], loss=69.7557
	step [229/244], loss=66.5184
	step [230/244], loss=66.8904
	step [231/244], loss=65.2684
	step [232/244], loss=74.5344
	step [233/244], loss=63.0723
	step [234/244], loss=68.8446
	step [235/244], loss=76.8060
	step [236/244], loss=60.8370
	step [237/244], loss=72.6319
	step [238/244], loss=79.3835
	step [239/244], loss=70.6714
	step [240/244], loss=63.9285
	step [241/244], loss=63.6799
	step [242/244], loss=61.1421
	step [243/244], loss=78.5598
	step [244/244], loss=66.0191
	Evaluating
	loss=0.0078, precision=0.3383, recall=0.8483, f1=0.4837
Training epoch 98
	step [1/244], loss=65.8438
	step [2/244], loss=68.5426
	step [3/244], loss=71.9049
	step [4/244], loss=68.5365
	step [5/244], loss=65.3097
	step [6/244], loss=66.6054
	step [7/244], loss=66.6002
	step [8/244], loss=64.3565
	step [9/244], loss=73.5485
	step [10/244], loss=71.1910
	step [11/244], loss=68.3931
	step [12/244], loss=66.2363
	step [13/244], loss=67.6322
	step [14/244], loss=69.7088
	step [15/244], loss=71.7117
	step [16/244], loss=77.0796
	step [17/244], loss=77.3323
	step [18/244], loss=76.2162
	step [19/244], loss=84.1877
	step [20/244], loss=67.0998
	step [21/244], loss=82.7395
	step [22/244], loss=72.5034
	step [23/244], loss=70.9729
	step [24/244], loss=75.0691
	step [25/244], loss=74.6173
	step [26/244], loss=76.3224
	step [27/244], loss=67.5845
	step [28/244], loss=84.3689
	step [29/244], loss=66.2388
	step [30/244], loss=60.8705
	step [31/244], loss=64.2982
	step [32/244], loss=62.6490
	step [33/244], loss=69.5508
	step [34/244], loss=67.6930
	step [35/244], loss=74.4223
	step [36/244], loss=68.5410
	step [37/244], loss=59.0157
	step [38/244], loss=59.6185
	step [39/244], loss=74.6266
	step [40/244], loss=62.7276
	step [41/244], loss=66.1839
	step [42/244], loss=46.9723
	step [43/244], loss=84.2519
	step [44/244], loss=67.9205
	step [45/244], loss=84.4975
	step [46/244], loss=68.7278
	step [47/244], loss=67.5852
	step [48/244], loss=58.9770
	step [49/244], loss=68.9956
	step [50/244], loss=70.9994
	step [51/244], loss=53.4285
	step [52/244], loss=74.2182
	step [53/244], loss=61.8348
	step [54/244], loss=52.6958
	step [55/244], loss=66.8651
	step [56/244], loss=63.2274
	step [57/244], loss=68.6015
	step [58/244], loss=70.0134
	step [59/244], loss=94.1419
	step [60/244], loss=75.5112
	step [61/244], loss=72.5401
	step [62/244], loss=69.0482
	step [63/244], loss=76.4673
	step [64/244], loss=68.5836
	step [65/244], loss=63.9584
	step [66/244], loss=67.4766
	step [67/244], loss=63.2811
	step [68/244], loss=58.0518
	step [69/244], loss=54.6988
	step [70/244], loss=64.7825
	step [71/244], loss=65.1860
	step [72/244], loss=80.9704
	step [73/244], loss=72.0105
	step [74/244], loss=61.7755
	step [75/244], loss=67.5174
	step [76/244], loss=71.0068
	step [77/244], loss=63.9690
	step [78/244], loss=65.7248
	step [79/244], loss=67.7133
	step [80/244], loss=83.9425
	step [81/244], loss=81.2589
	step [82/244], loss=55.4783
	step [83/244], loss=73.2251
	step [84/244], loss=70.2628
	step [85/244], loss=60.3172
	step [86/244], loss=71.1578
	step [87/244], loss=67.9758
	step [88/244], loss=72.2311
	step [89/244], loss=64.0651
	step [90/244], loss=75.4817
	step [91/244], loss=69.1584
	step [92/244], loss=71.6328
	step [93/244], loss=82.3931
	step [94/244], loss=72.0305
	step [95/244], loss=51.1284
	step [96/244], loss=53.5954
	step [97/244], loss=54.8345
	step [98/244], loss=64.7408
	step [99/244], loss=71.0434
	step [100/244], loss=56.6695
	step [101/244], loss=63.1571
	step [102/244], loss=59.9546
	step [103/244], loss=68.5125
	step [104/244], loss=69.4754
	step [105/244], loss=78.1466
	step [106/244], loss=63.9166
	step [107/244], loss=74.4186
	step [108/244], loss=77.3770
	step [109/244], loss=75.3602
	step [110/244], loss=74.7361
	step [111/244], loss=74.1390
	step [112/244], loss=65.5768
	step [113/244], loss=69.9709
	step [114/244], loss=81.0938
	step [115/244], loss=74.6666
	step [116/244], loss=59.0862
	step [117/244], loss=70.2796
	step [118/244], loss=56.1166
	step [119/244], loss=65.8393
	step [120/244], loss=67.5692
	step [121/244], loss=62.8843
	step [122/244], loss=62.8192
	step [123/244], loss=69.4089
	step [124/244], loss=71.6903
	step [125/244], loss=72.2984
	step [126/244], loss=71.1409
	step [127/244], loss=71.9327
	step [128/244], loss=76.7197
	step [129/244], loss=60.7735
	step [130/244], loss=78.3368
	step [131/244], loss=85.1040
	step [132/244], loss=54.9691
	step [133/244], loss=85.4556
	step [134/244], loss=73.1490
	step [135/244], loss=85.4733
	step [136/244], loss=69.7470
	step [137/244], loss=77.1542
	step [138/244], loss=84.4814
	step [139/244], loss=65.6091
	step [140/244], loss=81.8688
	step [141/244], loss=68.4830
	step [142/244], loss=56.1456
	step [143/244], loss=69.7543
	step [144/244], loss=73.1630
	step [145/244], loss=65.3133
	step [146/244], loss=64.0430
	step [147/244], loss=68.5143
	step [148/244], loss=88.3339
	step [149/244], loss=69.8618
	step [150/244], loss=71.4427
	step [151/244], loss=79.5802
	step [152/244], loss=66.3604
	step [153/244], loss=68.9383
	step [154/244], loss=62.5299
	step [155/244], loss=71.7889
	step [156/244], loss=61.6580
	step [157/244], loss=72.4367
	step [158/244], loss=60.2938
	step [159/244], loss=66.7423
	step [160/244], loss=77.6094
	step [161/244], loss=65.1878
	step [162/244], loss=74.8937
	step [163/244], loss=64.4629
	step [164/244], loss=73.5103
	step [165/244], loss=93.9773
	step [166/244], loss=61.4043
	step [167/244], loss=59.7840
	step [168/244], loss=59.8824
	step [169/244], loss=71.1892
	step [170/244], loss=66.0521
	step [171/244], loss=63.5962
	step [172/244], loss=64.6779
	step [173/244], loss=73.7666
	step [174/244], loss=78.5342
	step [175/244], loss=71.4748
	step [176/244], loss=59.4163
	step [177/244], loss=73.1813
	step [178/244], loss=70.7052
	step [179/244], loss=64.5423
	step [180/244], loss=78.5009
	step [181/244], loss=78.9573
	step [182/244], loss=63.0678
	step [183/244], loss=70.1438
	step [184/244], loss=84.1924
	step [185/244], loss=56.0044
	step [186/244], loss=71.0278
	step [187/244], loss=73.9955
	step [188/244], loss=65.2586
	step [189/244], loss=67.8935
	step [190/244], loss=71.4415
	step [191/244], loss=61.8262
	step [192/244], loss=67.4597
	step [193/244], loss=61.9904
	step [194/244], loss=71.4881
	step [195/244], loss=69.9446
	step [196/244], loss=79.0306
	step [197/244], loss=59.1646
	step [198/244], loss=70.5209
	step [199/244], loss=66.5451
	step [200/244], loss=71.3985
	step [201/244], loss=65.9214
	step [202/244], loss=59.3151
	step [203/244], loss=66.8558
	step [204/244], loss=66.0745
	step [205/244], loss=71.2489
	step [206/244], loss=78.3768
	step [207/244], loss=62.7971
	step [208/244], loss=62.1190
	step [209/244], loss=78.6914
	step [210/244], loss=65.9318
	step [211/244], loss=69.0074
	step [212/244], loss=79.2130
	step [213/244], loss=66.7332
	step [214/244], loss=69.2765
	step [215/244], loss=65.1296
	step [216/244], loss=73.3977
	step [217/244], loss=71.6476
	step [218/244], loss=70.1900
	step [219/244], loss=66.5422
	step [220/244], loss=67.6482
	step [221/244], loss=71.7462
	step [222/244], loss=53.3152
	step [223/244], loss=70.6953
	step [224/244], loss=84.4966
	step [225/244], loss=69.7886
	step [226/244], loss=62.8117
	step [227/244], loss=63.8209
	step [228/244], loss=62.4037
	step [229/244], loss=70.7435
	step [230/244], loss=70.1430
	step [231/244], loss=68.6614
	step [232/244], loss=71.6939
	step [233/244], loss=58.0827
	step [234/244], loss=75.5571
	step [235/244], loss=66.1169
	step [236/244], loss=78.8646
	step [237/244], loss=69.1941
	step [238/244], loss=78.0772
	step [239/244], loss=56.7467
	step [240/244], loss=80.1683
	step [241/244], loss=63.3182
	step [242/244], loss=58.6473
	step [243/244], loss=53.0305
	step [244/244], loss=80.4825
	Evaluating
	loss=0.0072, precision=0.3588, recall=0.8515, f1=0.5049
Training epoch 99
	step [1/244], loss=66.1432
	step [2/244], loss=64.5559
	step [3/244], loss=57.3881
	step [4/244], loss=70.3453
	step [5/244], loss=66.0015
	step [6/244], loss=79.9410
	step [7/244], loss=75.0327
	step [8/244], loss=80.4189
	step [9/244], loss=66.9207
	step [10/244], loss=79.9416
	step [11/244], loss=80.7304
	step [12/244], loss=72.7744
	step [13/244], loss=80.3246
	step [14/244], loss=74.2517
	step [15/244], loss=67.2106
	step [16/244], loss=73.4563
	step [17/244], loss=64.0993
	step [18/244], loss=57.5979
	step [19/244], loss=77.0432
	step [20/244], loss=45.1966
	step [21/244], loss=67.2979
	step [22/244], loss=60.5440
	step [23/244], loss=54.7287
	step [24/244], loss=74.1804
	step [25/244], loss=79.6139
	step [26/244], loss=79.5898
	step [27/244], loss=65.4753
	step [28/244], loss=67.0839
	step [29/244], loss=68.0563
	step [30/244], loss=69.2719
	step [31/244], loss=65.8149
	step [32/244], loss=66.5619
	step [33/244], loss=77.2288
	step [34/244], loss=88.4119
	step [35/244], loss=65.4115
	step [36/244], loss=55.3809
	step [37/244], loss=53.3290
	step [38/244], loss=79.8968
	step [39/244], loss=65.7095
	step [40/244], loss=62.8906
	step [41/244], loss=60.6648
	step [42/244], loss=71.2980
	step [43/244], loss=78.8914
	step [44/244], loss=62.9454
	step [45/244], loss=66.7343
	step [46/244], loss=69.5856
	step [47/244], loss=58.2811
	step [48/244], loss=60.2738
	step [49/244], loss=78.7267
	step [50/244], loss=60.2883
	step [51/244], loss=63.5741
	step [52/244], loss=74.3927
	step [53/244], loss=74.6031
	step [54/244], loss=64.8348
	step [55/244], loss=78.3150
	step [56/244], loss=63.9252
	step [57/244], loss=75.1939
	step [58/244], loss=63.7352
	step [59/244], loss=78.6519
	step [60/244], loss=65.7933
	step [61/244], loss=63.1665
	step [62/244], loss=63.2593
	step [63/244], loss=82.5091
	step [64/244], loss=83.1179
	step [65/244], loss=74.6481
	step [66/244], loss=61.7633
	step [67/244], loss=60.1534
	step [68/244], loss=60.5511
	step [69/244], loss=80.0481
	step [70/244], loss=68.5095
	step [71/244], loss=73.9720
	step [72/244], loss=63.0017
	step [73/244], loss=54.3328
	step [74/244], loss=78.2408
	step [75/244], loss=77.3192
	step [76/244], loss=82.3686
	step [77/244], loss=78.5599
	step [78/244], loss=86.7376
	step [79/244], loss=71.9884
	step [80/244], loss=62.4801
	step [81/244], loss=68.9145
	step [82/244], loss=68.3209
	step [83/244], loss=61.2137
	step [84/244], loss=78.3194
	step [85/244], loss=71.5087
	step [86/244], loss=83.5195
	step [87/244], loss=64.2714
	step [88/244], loss=57.3374
	step [89/244], loss=70.7071
	step [90/244], loss=71.7313
	step [91/244], loss=77.9311
	step [92/244], loss=68.6307
	step [93/244], loss=54.9170
	step [94/244], loss=67.0102
	step [95/244], loss=56.2731
	step [96/244], loss=54.8014
	step [97/244], loss=73.3165
	step [98/244], loss=59.5259
	step [99/244], loss=70.2174
	step [100/244], loss=64.8875
	step [101/244], loss=70.2906
	step [102/244], loss=68.7575
	step [103/244], loss=65.5781
	step [104/244], loss=83.1092
	step [105/244], loss=75.5673
	step [106/244], loss=56.9045
	step [107/244], loss=67.5885
	step [108/244], loss=82.2034
	step [109/244], loss=58.6811
	step [110/244], loss=73.9444
	step [111/244], loss=58.8766
	step [112/244], loss=68.6833
	step [113/244], loss=58.2841
	step [114/244], loss=70.1077
	step [115/244], loss=70.5051
	step [116/244], loss=60.3303
	step [117/244], loss=59.1129
	step [118/244], loss=50.0131
	step [119/244], loss=69.2750
	step [120/244], loss=61.7141
	step [121/244], loss=67.6027
	step [122/244], loss=72.8020
	step [123/244], loss=63.5913
	step [124/244], loss=91.3501
	step [125/244], loss=65.5133
	step [126/244], loss=66.4720
	step [127/244], loss=52.9519
	step [128/244], loss=61.5363
	step [129/244], loss=69.7355
	step [130/244], loss=74.0889
	step [131/244], loss=61.1392
	step [132/244], loss=71.1544
	step [133/244], loss=64.6245
	step [134/244], loss=66.0824
	step [135/244], loss=66.0705
	step [136/244], loss=82.0420
	step [137/244], loss=76.6264
	step [138/244], loss=63.8935
	step [139/244], loss=75.3726
	step [140/244], loss=67.9750
	step [141/244], loss=70.5681
	step [142/244], loss=63.4955
	step [143/244], loss=76.3100
	step [144/244], loss=65.8548
	step [145/244], loss=71.7638
	step [146/244], loss=59.5648
	step [147/244], loss=67.5092
	step [148/244], loss=66.0564
	step [149/244], loss=68.9123
	step [150/244], loss=65.1281
	step [151/244], loss=74.2354
	step [152/244], loss=60.8174
	step [153/244], loss=70.6864
	step [154/244], loss=68.3315
	step [155/244], loss=72.0117
	step [156/244], loss=72.3338
	step [157/244], loss=70.0297
	step [158/244], loss=78.0907
	step [159/244], loss=60.7259
	step [160/244], loss=63.7416
	step [161/244], loss=45.7781
	step [162/244], loss=68.4241
	step [163/244], loss=72.0911
	step [164/244], loss=72.1493
	step [165/244], loss=82.4618
	step [166/244], loss=66.1464
	step [167/244], loss=64.0981
	step [168/244], loss=66.2107
	step [169/244], loss=76.8572
	step [170/244], loss=56.6711
	step [171/244], loss=69.0251
	step [172/244], loss=67.0883
	step [173/244], loss=60.4482
	step [174/244], loss=65.9997
	step [175/244], loss=79.9766
	step [176/244], loss=61.5733
	step [177/244], loss=63.9882
	step [178/244], loss=70.1659
	step [179/244], loss=70.5005
	step [180/244], loss=67.3290
	step [181/244], loss=69.6419
	step [182/244], loss=91.4636
	step [183/244], loss=64.2873
	step [184/244], loss=77.8984
	step [185/244], loss=62.4402
	step [186/244], loss=68.6616
	step [187/244], loss=70.0773
	step [188/244], loss=81.5532
	step [189/244], loss=53.4153
	step [190/244], loss=84.2791
	step [191/244], loss=94.7430
	step [192/244], loss=64.5303
	step [193/244], loss=79.6495
	step [194/244], loss=71.9515
	step [195/244], loss=63.3493
	step [196/244], loss=67.9738
	step [197/244], loss=63.7277
	step [198/244], loss=86.2819
	step [199/244], loss=67.5258
	step [200/244], loss=68.0508
	step [201/244], loss=65.8852
	step [202/244], loss=55.4179
	step [203/244], loss=74.6687
	step [204/244], loss=81.3305
	step [205/244], loss=71.2248
	step [206/244], loss=60.1964
	step [207/244], loss=56.9754
	step [208/244], loss=71.7328
	step [209/244], loss=65.2455
	step [210/244], loss=63.8454
	step [211/244], loss=62.9843
	step [212/244], loss=77.1268
	step [213/244], loss=77.7353
	step [214/244], loss=64.2081
	step [215/244], loss=59.8628
	step [216/244], loss=60.8362
	step [217/244], loss=77.0619
	step [218/244], loss=72.4251
	step [219/244], loss=76.3794
	step [220/244], loss=75.1279
	step [221/244], loss=76.4134
	step [222/244], loss=62.6100
	step [223/244], loss=66.0825
	step [224/244], loss=66.2882
	step [225/244], loss=72.1769
	step [226/244], loss=69.4544
	step [227/244], loss=57.7408
	step [228/244], loss=75.2689
	step [229/244], loss=54.8810
	step [230/244], loss=81.9915
	step [231/244], loss=63.2792
	step [232/244], loss=65.0755
	step [233/244], loss=89.7442
	step [234/244], loss=64.8267
	step [235/244], loss=63.2304
	step [236/244], loss=71.5424
	step [237/244], loss=70.8221
	step [238/244], loss=61.1980
	step [239/244], loss=71.2671
	step [240/244], loss=66.5687
	step [241/244], loss=76.6547
	step [242/244], loss=58.1573
	step [243/244], loss=66.8414
	step [244/244], loss=61.3858
	Evaluating
	loss=0.0066, precision=0.3942, recall=0.8509, f1=0.5388
saving model as: 1_saved_model.pth
Training epoch 100
	step [1/244], loss=78.0195
	step [2/244], loss=64.7398
	step [3/244], loss=79.4546
	step [4/244], loss=62.1054
	step [5/244], loss=65.8712
	step [6/244], loss=61.8870
	step [7/244], loss=70.6130
	step [8/244], loss=56.9776
	step [9/244], loss=57.1676
	step [10/244], loss=64.4748
	step [11/244], loss=70.4312
	step [12/244], loss=73.5992
	step [13/244], loss=72.2918
	step [14/244], loss=65.6331
	step [15/244], loss=78.1857
	step [16/244], loss=68.3754
	step [17/244], loss=76.4994
	step [18/244], loss=72.6532
	step [19/244], loss=76.2361
	step [20/244], loss=65.0352
	step [21/244], loss=68.3652
	step [22/244], loss=94.3581
	step [23/244], loss=59.4086
	step [24/244], loss=62.6555
	step [25/244], loss=57.7650
	step [26/244], loss=67.6444
	step [27/244], loss=56.2361
	step [28/244], loss=66.8068
	step [29/244], loss=78.0115
	step [30/244], loss=69.6633
	step [31/244], loss=57.9746
	step [32/244], loss=67.6563
	step [33/244], loss=76.7908
	step [34/244], loss=59.4138
	step [35/244], loss=67.1118
	step [36/244], loss=60.6182
	step [37/244], loss=60.8939
	step [38/244], loss=72.0745
	step [39/244], loss=75.9573
	step [40/244], loss=71.9360
	step [41/244], loss=76.0744
	step [42/244], loss=64.7750
	step [43/244], loss=66.1822
	step [44/244], loss=67.6817
	step [45/244], loss=61.6310
	step [46/244], loss=71.9249
	step [47/244], loss=62.6323
	step [48/244], loss=72.7017
	step [49/244], loss=67.9059
	step [50/244], loss=75.4572
	step [51/244], loss=70.0454
	step [52/244], loss=78.6297
	step [53/244], loss=60.6352
	step [54/244], loss=65.7564
	step [55/244], loss=67.8977
	step [56/244], loss=81.4396
	step [57/244], loss=77.7142
	step [58/244], loss=77.4806
	step [59/244], loss=62.9452
	step [60/244], loss=82.9186
	step [61/244], loss=63.0979
	step [62/244], loss=68.3522
	step [63/244], loss=71.6241
	step [64/244], loss=63.3536
	step [65/244], loss=69.4274
	step [66/244], loss=76.2521
	step [67/244], loss=71.6125
	step [68/244], loss=57.7801
	step [69/244], loss=62.2265
	step [70/244], loss=63.6349
	step [71/244], loss=80.3796
	step [72/244], loss=79.0959
	step [73/244], loss=58.0714
	step [74/244], loss=93.2184
	step [75/244], loss=57.1675
	step [76/244], loss=80.1529
	step [77/244], loss=65.4232
	step [78/244], loss=73.7061
	step [79/244], loss=79.5854
	step [80/244], loss=62.3634
	step [81/244], loss=77.0140
	step [82/244], loss=62.4409
	step [83/244], loss=74.1831
	step [84/244], loss=78.6545
	step [85/244], loss=65.3806
	step [86/244], loss=64.9832
	step [87/244], loss=65.5111
	step [88/244], loss=65.9777
	step [89/244], loss=66.1190
	step [90/244], loss=72.1403
	step [91/244], loss=61.8172
	step [92/244], loss=72.5356
	step [93/244], loss=65.2220
	step [94/244], loss=70.6599
	step [95/244], loss=73.3657
	step [96/244], loss=58.6446
	step [97/244], loss=77.4660
	step [98/244], loss=76.5743
	step [99/244], loss=69.9266
	step [100/244], loss=67.3062
	step [101/244], loss=62.4573
	step [102/244], loss=85.3866
	step [103/244], loss=80.7778
	step [104/244], loss=55.8888
	step [105/244], loss=49.8732
	step [106/244], loss=58.9822
	step [107/244], loss=80.1020
	step [108/244], loss=72.6314
	step [109/244], loss=61.3861
	step [110/244], loss=69.8337
	step [111/244], loss=62.9353
	step [112/244], loss=58.1138
	step [113/244], loss=48.9298
	step [114/244], loss=60.5839
	step [115/244], loss=88.5090
	step [116/244], loss=74.8334
	step [117/244], loss=66.4732
	step [118/244], loss=50.0156
	step [119/244], loss=54.7976
	step [120/244], loss=71.0204
	step [121/244], loss=66.5531
	step [122/244], loss=79.1822
	step [123/244], loss=62.3795
	step [124/244], loss=73.5639
	step [125/244], loss=74.1870
	step [126/244], loss=63.8019
	step [127/244], loss=80.7542
	step [128/244], loss=56.4141
	step [129/244], loss=72.8268
	step [130/244], loss=69.9793
	step [131/244], loss=66.3115
	step [132/244], loss=66.0902
	step [133/244], loss=65.5518
	step [134/244], loss=69.8975
	step [135/244], loss=70.5936
	step [136/244], loss=70.0022
	step [137/244], loss=59.9321
	step [138/244], loss=59.2322
	step [139/244], loss=57.1379
	step [140/244], loss=68.2427
	step [141/244], loss=66.4818
	step [142/244], loss=51.3114
	step [143/244], loss=56.9335
	step [144/244], loss=71.7803
	step [145/244], loss=67.1938
	step [146/244], loss=64.6235
	step [147/244], loss=83.0780
	step [148/244], loss=68.5047
	step [149/244], loss=83.6932
	step [150/244], loss=73.1547
	step [151/244], loss=67.2221
	step [152/244], loss=74.8480
	step [153/244], loss=75.8476
	step [154/244], loss=74.4809
	step [155/244], loss=84.8661
	step [156/244], loss=82.4594
	step [157/244], loss=55.4439
	step [158/244], loss=73.8054
	step [159/244], loss=63.0226
	step [160/244], loss=78.5835
	step [161/244], loss=56.4042
	step [162/244], loss=64.7855
	step [163/244], loss=52.3777
	step [164/244], loss=68.2486
	step [165/244], loss=54.8129
	step [166/244], loss=72.2164
	step [167/244], loss=82.8060
	step [168/244], loss=64.4160
	step [169/244], loss=64.1938
	step [170/244], loss=69.6820
	step [171/244], loss=64.9273
	step [172/244], loss=75.9520
	step [173/244], loss=79.2927
	step [174/244], loss=58.0921
	step [175/244], loss=65.2591
	step [176/244], loss=74.4848
	step [177/244], loss=70.6325
	step [178/244], loss=73.7287
	step [179/244], loss=55.6459
	step [180/244], loss=67.4202
	step [181/244], loss=71.1850
	step [182/244], loss=63.8940
	step [183/244], loss=67.9112
	step [184/244], loss=80.3125
	step [185/244], loss=73.7917
	step [186/244], loss=59.9145
	step [187/244], loss=69.7654
	step [188/244], loss=60.2523
	step [189/244], loss=64.3421
	step [190/244], loss=57.3672
	step [191/244], loss=72.2890
	step [192/244], loss=67.1246
	step [193/244], loss=67.9089
	step [194/244], loss=69.7458
	step [195/244], loss=71.0624
	step [196/244], loss=53.0678
	step [197/244], loss=61.4639
	step [198/244], loss=67.2679
	step [199/244], loss=87.0365
	step [200/244], loss=69.0669
	step [201/244], loss=78.3327
	step [202/244], loss=69.5222
	step [203/244], loss=72.3251
	step [204/244], loss=66.5501
	step [205/244], loss=74.9672
	step [206/244], loss=57.5675
	step [207/244], loss=75.4993
	step [208/244], loss=69.8305
	step [209/244], loss=57.3961
	step [210/244], loss=65.6426
	step [211/244], loss=64.7654
	step [212/244], loss=66.5632
	step [213/244], loss=78.5860
	step [214/244], loss=74.4000
	step [215/244], loss=81.4260
	step [216/244], loss=66.7870
	step [217/244], loss=61.1737
	step [218/244], loss=59.2232
	step [219/244], loss=66.9964
	step [220/244], loss=70.8475
	step [221/244], loss=58.4731
	step [222/244], loss=64.9113
	step [223/244], loss=74.5814
	step [224/244], loss=71.3615
	step [225/244], loss=79.6529
	step [226/244], loss=76.2065
	step [227/244], loss=64.9460
	step [228/244], loss=73.8010
	step [229/244], loss=83.0764
	step [230/244], loss=67.6202
	step [231/244], loss=77.1573
	step [232/244], loss=57.1486
	step [233/244], loss=73.1191
	step [234/244], loss=67.8428
	step [235/244], loss=56.5590
	step [236/244], loss=77.4447
	step [237/244], loss=61.2318
	step [238/244], loss=56.4072
	step [239/244], loss=77.3589
	step [240/244], loss=71.2563
	step [241/244], loss=62.4584
	step [242/244], loss=68.0736
	step [243/244], loss=63.1152
	step [244/244], loss=69.4240
	Evaluating
	loss=0.0070, precision=0.3646, recall=0.8497, f1=0.5102
Training epoch 101
	step [1/244], loss=62.8905
	step [2/244], loss=72.2218
	step [3/244], loss=49.5638
	step [4/244], loss=60.7145
	step [5/244], loss=75.5742
	step [6/244], loss=70.4534
	step [7/244], loss=63.8230
	step [8/244], loss=65.5534
	step [9/244], loss=91.9407
	step [10/244], loss=57.0363
	step [11/244], loss=60.2165
	step [12/244], loss=69.6940
	step [13/244], loss=60.9755
	step [14/244], loss=66.9424
	step [15/244], loss=57.5451
	step [16/244], loss=69.1156
	step [17/244], loss=58.8185
	step [18/244], loss=79.2062
	step [19/244], loss=56.4708
	step [20/244], loss=78.5924
	step [21/244], loss=52.5704
	step [22/244], loss=95.0844
	step [23/244], loss=64.4870
	step [24/244], loss=73.5471
	step [25/244], loss=72.6408
	step [26/244], loss=56.8889
	step [27/244], loss=63.6748
	step [28/244], loss=62.9993
	step [29/244], loss=68.8075
	step [30/244], loss=60.7471
	step [31/244], loss=63.0220
	step [32/244], loss=76.2970
	step [33/244], loss=70.5490
	step [34/244], loss=76.2316
	step [35/244], loss=78.0508
	step [36/244], loss=67.9652
	step [37/244], loss=77.3876
	step [38/244], loss=75.6651
	step [39/244], loss=71.6781
	step [40/244], loss=65.7229
	step [41/244], loss=63.9358
	step [42/244], loss=58.2619
	step [43/244], loss=59.8768
	step [44/244], loss=81.4418
	step [45/244], loss=78.1926
	step [46/244], loss=95.6268
	step [47/244], loss=69.0074
	step [48/244], loss=73.0324
	step [49/244], loss=82.1118
	step [50/244], loss=62.5969
	step [51/244], loss=69.3623
	step [52/244], loss=73.3572
	step [53/244], loss=68.0695
	step [54/244], loss=60.1079
	step [55/244], loss=79.2021
	step [56/244], loss=67.9218
	step [57/244], loss=65.8241
	step [58/244], loss=80.2778
	step [59/244], loss=57.6564
	step [60/244], loss=68.3097
	step [61/244], loss=89.8254
	step [62/244], loss=56.6471
	step [63/244], loss=69.7746
	step [64/244], loss=66.2387
	step [65/244], loss=61.3385
	step [66/244], loss=63.4463
	step [67/244], loss=80.6369
	step [68/244], loss=78.7152
	step [69/244], loss=74.4572
	step [70/244], loss=73.5803
	step [71/244], loss=65.2623
	step [72/244], loss=63.5079
	step [73/244], loss=63.6939
	step [74/244], loss=69.8801
	step [75/244], loss=67.4426
	step [76/244], loss=60.6524
	step [77/244], loss=67.2112
	step [78/244], loss=82.1044
	step [79/244], loss=70.6519
	step [80/244], loss=62.7389
	step [81/244], loss=69.4136
	step [82/244], loss=74.1389
	step [83/244], loss=75.0298
	step [84/244], loss=73.0034
	step [85/244], loss=86.6804
	step [86/244], loss=66.7699
	step [87/244], loss=73.7097
	step [88/244], loss=65.2843
	step [89/244], loss=52.9565
	step [90/244], loss=63.7505
	step [91/244], loss=65.1940
	step [92/244], loss=61.2690
	step [93/244], loss=65.8032
	step [94/244], loss=62.4908
	step [95/244], loss=63.5989
	step [96/244], loss=66.4136
	step [97/244], loss=73.2104
	step [98/244], loss=52.6949
	step [99/244], loss=66.9624
	step [100/244], loss=74.6713
	step [101/244], loss=64.1110
	step [102/244], loss=67.6293
	step [103/244], loss=58.5629
	step [104/244], loss=72.7172
	step [105/244], loss=68.0586
	step [106/244], loss=69.6814
	step [107/244], loss=61.9013
	step [108/244], loss=73.0668
	step [109/244], loss=71.2744
	step [110/244], loss=64.2414
	step [111/244], loss=67.3636
	step [112/244], loss=63.8233
	step [113/244], loss=60.0201
	step [114/244], loss=87.3150
	step [115/244], loss=58.9900
	step [116/244], loss=63.9400
	step [117/244], loss=77.9854
	step [118/244], loss=62.8340
	step [119/244], loss=56.8319
	step [120/244], loss=74.4302
	step [121/244], loss=67.2523
	step [122/244], loss=67.6111
	step [123/244], loss=73.7216
	step [124/244], loss=64.4624
	step [125/244], loss=73.2770
	step [126/244], loss=72.1432
	step [127/244], loss=61.1699
	step [128/244], loss=64.3394
	step [129/244], loss=72.0547
	step [130/244], loss=65.5589
	step [131/244], loss=71.0114
	step [132/244], loss=54.6830
	step [133/244], loss=74.1078
	step [134/244], loss=73.7948
	step [135/244], loss=69.1436
	step [136/244], loss=51.2905
	step [137/244], loss=85.2387
	step [138/244], loss=52.8701
	step [139/244], loss=52.3777
	step [140/244], loss=71.0190
	step [141/244], loss=62.8400
	step [142/244], loss=72.1898
	step [143/244], loss=73.3650
	step [144/244], loss=73.0651
	step [145/244], loss=78.6678
	step [146/244], loss=75.6599
	step [147/244], loss=92.2204
	step [148/244], loss=79.6908
	step [149/244], loss=61.6732
	step [150/244], loss=70.5426
	step [151/244], loss=81.6946
	step [152/244], loss=75.0702
	step [153/244], loss=70.4951
	step [154/244], loss=50.2310
	step [155/244], loss=79.0754
	step [156/244], loss=73.3627
	step [157/244], loss=67.6004
	step [158/244], loss=79.0787
	step [159/244], loss=78.1736
	step [160/244], loss=47.5829
	step [161/244], loss=53.6135
	step [162/244], loss=67.4073
	step [163/244], loss=66.3875
	step [164/244], loss=65.4156
	step [165/244], loss=63.7427
	step [166/244], loss=79.0043
	step [167/244], loss=74.2221
	step [168/244], loss=64.8842
	step [169/244], loss=62.4357
	step [170/244], loss=68.2190
	step [171/244], loss=78.8300
	step [172/244], loss=73.0135
	step [173/244], loss=69.6225
	step [174/244], loss=61.2557
	step [175/244], loss=71.1270
	step [176/244], loss=62.4531
	step [177/244], loss=67.0134
	step [178/244], loss=68.9055
	step [179/244], loss=60.3422
	step [180/244], loss=72.3665
	step [181/244], loss=71.1283
	step [182/244], loss=78.5323
	step [183/244], loss=51.7283
	step [184/244], loss=70.4344
	step [185/244], loss=74.0264
	step [186/244], loss=67.2951
	step [187/244], loss=63.7256
	step [188/244], loss=74.5343
	step [189/244], loss=64.0264
	step [190/244], loss=72.7627
	step [191/244], loss=71.8456
	step [192/244], loss=78.4371
	step [193/244], loss=87.1997
	step [194/244], loss=63.3837
	step [195/244], loss=81.9487
	step [196/244], loss=70.2046
	step [197/244], loss=65.8387
	step [198/244], loss=61.6140
	step [199/244], loss=73.1439
	step [200/244], loss=74.4493
	step [201/244], loss=64.0390
	step [202/244], loss=70.4366
	step [203/244], loss=54.6021
	step [204/244], loss=67.1640
	step [205/244], loss=75.0254
	step [206/244], loss=76.8189
	step [207/244], loss=91.1206
	step [208/244], loss=70.0829
	step [209/244], loss=73.2764
	step [210/244], loss=81.8478
	step [211/244], loss=63.8841
	step [212/244], loss=73.4153
	step [213/244], loss=70.1282
	step [214/244], loss=53.9836
	step [215/244], loss=77.2619
	step [216/244], loss=68.8333
	step [217/244], loss=70.7086
	step [218/244], loss=62.2171
	step [219/244], loss=72.1275
	step [220/244], loss=62.3734
	step [221/244], loss=67.7243
	step [222/244], loss=69.2005
	step [223/244], loss=59.8633
	step [224/244], loss=60.1668
	step [225/244], loss=69.4295
	step [226/244], loss=54.2634
	step [227/244], loss=71.9740
	step [228/244], loss=84.9837
	step [229/244], loss=61.8095
	step [230/244], loss=62.9555
	step [231/244], loss=74.5117
	step [232/244], loss=68.8061
	step [233/244], loss=66.7786
	step [234/244], loss=72.1013
	step [235/244], loss=71.5988
	step [236/244], loss=72.8809
	step [237/244], loss=69.1605
	step [238/244], loss=73.7951
	step [239/244], loss=70.8965
	step [240/244], loss=63.8109
	step [241/244], loss=71.3471
	step [242/244], loss=65.9602
	step [243/244], loss=69.5694
	step [244/244], loss=65.0263
	Evaluating
	loss=0.0062, precision=0.4154, recall=0.8418, f1=0.5562
saving model as: 1_saved_model.pth
Training epoch 102
	step [1/244], loss=60.7933
	step [2/244], loss=63.3468
	step [3/244], loss=69.0359
	step [4/244], loss=56.7434
	step [5/244], loss=60.3419
	step [6/244], loss=63.9825
	step [7/244], loss=52.8740
	step [8/244], loss=60.0866
	step [9/244], loss=72.1647
	step [10/244], loss=66.8227
	step [11/244], loss=69.3423
	step [12/244], loss=68.6223
	step [13/244], loss=65.2117
	step [14/244], loss=71.1791
	step [15/244], loss=65.6989
	step [16/244], loss=71.9820
	step [17/244], loss=76.3958
	step [18/244], loss=77.3785
	step [19/244], loss=67.4235
	step [20/244], loss=62.1602
	step [21/244], loss=78.0126
	step [22/244], loss=57.2107
	step [23/244], loss=71.3708
	step [24/244], loss=66.5608
	step [25/244], loss=79.2663
	step [26/244], loss=72.6000
	step [27/244], loss=69.5326
	step [28/244], loss=68.6409
	step [29/244], loss=63.3999
	step [30/244], loss=67.6356
	step [31/244], loss=65.4210
	step [32/244], loss=79.0728
	step [33/244], loss=79.6544
	step [34/244], loss=60.5848
	step [35/244], loss=66.3345
	step [36/244], loss=77.1239
	step [37/244], loss=74.4151
	step [38/244], loss=59.8010
	step [39/244], loss=66.9329
	step [40/244], loss=68.6312
	step [41/244], loss=71.0093
	step [42/244], loss=75.1240
	step [43/244], loss=75.7992
	step [44/244], loss=62.1786
	step [45/244], loss=65.5683
	step [46/244], loss=65.1637
	step [47/244], loss=66.4233
	step [48/244], loss=75.4393
	step [49/244], loss=72.3304
	step [50/244], loss=59.1158
	step [51/244], loss=58.3304
	step [52/244], loss=67.9034
	step [53/244], loss=75.0392
	step [54/244], loss=73.2934
	step [55/244], loss=91.3861
	step [56/244], loss=74.5117
	step [57/244], loss=66.5238
	step [58/244], loss=75.5004
	step [59/244], loss=66.5588
	step [60/244], loss=66.1286
	step [61/244], loss=66.4289
	step [62/244], loss=56.4308
	step [63/244], loss=66.1906
	step [64/244], loss=73.9435
	step [65/244], loss=65.3038
	step [66/244], loss=66.0283
	step [67/244], loss=68.3833
	step [68/244], loss=70.3342
	step [69/244], loss=74.8803
	step [70/244], loss=71.7818
	step [71/244], loss=55.7889
	step [72/244], loss=72.2453
	step [73/244], loss=60.9391
	step [74/244], loss=59.2753
	step [75/244], loss=75.0776
	step [76/244], loss=71.5938
	step [77/244], loss=69.5667
	step [78/244], loss=62.6020
	step [79/244], loss=61.7911
	step [80/244], loss=81.2975
	step [81/244], loss=62.5011
	step [82/244], loss=65.3784
	step [83/244], loss=83.6607
	step [84/244], loss=73.6481
	step [85/244], loss=76.3284
	step [86/244], loss=73.4019
	step [87/244], loss=71.8649
	step [88/244], loss=83.2843
	step [89/244], loss=74.5869
	step [90/244], loss=68.5244
	step [91/244], loss=76.5578
	step [92/244], loss=61.4557
	step [93/244], loss=70.1126
	step [94/244], loss=66.9788
	step [95/244], loss=79.6109
	step [96/244], loss=72.8659
	step [97/244], loss=61.5480
	step [98/244], loss=63.7162
	step [99/244], loss=71.4277
	step [100/244], loss=68.6580
	step [101/244], loss=69.2898
	step [102/244], loss=62.4413
	step [103/244], loss=66.5014
	step [104/244], loss=69.6018
	step [105/244], loss=69.9723
	step [106/244], loss=68.7259
	step [107/244], loss=61.9852
	step [108/244], loss=61.2532
	step [109/244], loss=78.3763
	step [110/244], loss=63.3280
	step [111/244], loss=80.5899
	step [112/244], loss=63.0992
	step [113/244], loss=78.3830
	step [114/244], loss=66.2091
	step [115/244], loss=61.4591
	step [116/244], loss=64.1842
	step [117/244], loss=70.7691
	step [118/244], loss=61.8345
	step [119/244], loss=65.4138
	step [120/244], loss=60.9021
	step [121/244], loss=71.0384
	step [122/244], loss=77.6056
	step [123/244], loss=79.5357
	step [124/244], loss=68.1375
	step [125/244], loss=72.2923
	step [126/244], loss=77.5267
	step [127/244], loss=69.5293
	step [128/244], loss=56.5414
	step [129/244], loss=83.7991
	step [130/244], loss=77.7222
	step [131/244], loss=55.5259
	step [132/244], loss=62.9853
	step [133/244], loss=61.6259
	step [134/244], loss=69.0497
	step [135/244], loss=60.1270
	step [136/244], loss=62.1352
	step [137/244], loss=67.7479
	step [138/244], loss=65.0290
	step [139/244], loss=63.4976
	step [140/244], loss=59.4886
	step [141/244], loss=75.3263
	step [142/244], loss=62.7278
	step [143/244], loss=63.2770
	step [144/244], loss=77.1126
	step [145/244], loss=70.9103
	step [146/244], loss=65.5276
	step [147/244], loss=72.7802
	step [148/244], loss=63.8366
	step [149/244], loss=60.6772
	step [150/244], loss=88.1169
	step [151/244], loss=56.2451
	step [152/244], loss=69.4205
	step [153/244], loss=77.9800
	step [154/244], loss=66.6304
	step [155/244], loss=72.2480
	step [156/244], loss=74.1813
	step [157/244], loss=74.4298
	step [158/244], loss=62.3527
	step [159/244], loss=80.6926
	step [160/244], loss=61.2669
	step [161/244], loss=66.5436
	step [162/244], loss=69.6297
	step [163/244], loss=50.6846
	step [164/244], loss=80.2206
	step [165/244], loss=51.6700
	step [166/244], loss=67.9325
	step [167/244], loss=62.5598
	step [168/244], loss=76.2053
	step [169/244], loss=60.5675
	step [170/244], loss=73.8800
	step [171/244], loss=79.1709
	step [172/244], loss=72.1654
	step [173/244], loss=77.4619
	step [174/244], loss=74.0362
	step [175/244], loss=52.2452
	step [176/244], loss=68.5402
	step [177/244], loss=68.4826
	step [178/244], loss=76.4165
	step [179/244], loss=76.9657
	step [180/244], loss=61.0407
	step [181/244], loss=64.3936
	step [182/244], loss=65.5522
	step [183/244], loss=58.2071
	step [184/244], loss=58.9367
	step [185/244], loss=72.9644
	step [186/244], loss=87.7704
	step [187/244], loss=65.9727
	step [188/244], loss=74.9758
	step [189/244], loss=70.4664
	step [190/244], loss=67.5231
	step [191/244], loss=75.1890
	step [192/244], loss=56.8102
	step [193/244], loss=66.8684
	step [194/244], loss=79.2918
	step [195/244], loss=69.9483
	step [196/244], loss=66.6426
	step [197/244], loss=68.4053
	step [198/244], loss=68.1341
	step [199/244], loss=61.2323
	step [200/244], loss=77.1057
	step [201/244], loss=75.1370
	step [202/244], loss=62.8969
	step [203/244], loss=73.3272
	step [204/244], loss=67.6456
	step [205/244], loss=68.0392
	step [206/244], loss=67.1009
	step [207/244], loss=71.2157
	step [208/244], loss=65.2466
	step [209/244], loss=72.5267
	step [210/244], loss=57.8613
	step [211/244], loss=77.5708
	step [212/244], loss=69.3287
	step [213/244], loss=66.2912
	step [214/244], loss=75.8947
	step [215/244], loss=67.4104
	step [216/244], loss=74.8271
	step [217/244], loss=83.6900
	step [218/244], loss=57.2325
	step [219/244], loss=64.6410
	step [220/244], loss=73.4538
	step [221/244], loss=70.9747
	step [222/244], loss=62.9592
	step [223/244], loss=72.7211
	step [224/244], loss=51.8868
	step [225/244], loss=82.9762
	step [226/244], loss=58.8491
	step [227/244], loss=71.5549
	step [228/244], loss=64.8836
	step [229/244], loss=62.4501
	step [230/244], loss=56.0831
	step [231/244], loss=64.2497
	step [232/244], loss=72.0104
	step [233/244], loss=56.6008
	step [234/244], loss=59.2827
	step [235/244], loss=77.1431
	step [236/244], loss=71.4219
	step [237/244], loss=63.4680
	step [238/244], loss=60.4093
	step [239/244], loss=59.7303
	step [240/244], loss=77.4196
	step [241/244], loss=58.3745
	step [242/244], loss=70.5451
	step [243/244], loss=79.1433
	step [244/244], loss=65.0396
	Evaluating
	loss=0.0069, precision=0.3788, recall=0.8451, f1=0.5231
Training epoch 103
	step [1/244], loss=73.3150
	step [2/244], loss=75.4066
	step [3/244], loss=70.9664
	step [4/244], loss=59.8165
	step [5/244], loss=72.4328
	step [6/244], loss=57.8973
	step [7/244], loss=70.8057
	step [8/244], loss=81.1965
	step [9/244], loss=66.7027
	step [10/244], loss=73.1999
	step [11/244], loss=88.2093
	step [12/244], loss=61.9128
	step [13/244], loss=67.5344
	step [14/244], loss=67.2419
	step [15/244], loss=60.5056
	step [16/244], loss=65.1840
	step [17/244], loss=76.2325
	step [18/244], loss=64.7268
	step [19/244], loss=67.3635
	step [20/244], loss=67.5438
	step [21/244], loss=53.0359
	step [22/244], loss=74.3753
	step [23/244], loss=63.1260
	step [24/244], loss=67.2299
	step [25/244], loss=70.1990
	step [26/244], loss=72.0168
	step [27/244], loss=49.4023
	step [28/244], loss=74.4078
	step [29/244], loss=77.4210
	step [30/244], loss=70.7617
	step [31/244], loss=58.9162
	step [32/244], loss=78.7284
	step [33/244], loss=65.7958
	step [34/244], loss=67.0694
	step [35/244], loss=62.2369
	step [36/244], loss=74.4081
	step [37/244], loss=74.3916
	step [38/244], loss=73.8971
	step [39/244], loss=66.5866
	step [40/244], loss=67.2003
	step [41/244], loss=76.1559
	step [42/244], loss=67.7191
	step [43/244], loss=56.7297
	step [44/244], loss=67.9412
	step [45/244], loss=67.7334
	step [46/244], loss=68.1544
	step [47/244], loss=67.2399
	step [48/244], loss=72.1555
	step [49/244], loss=61.6655
	step [50/244], loss=78.9524
	step [51/244], loss=67.8848
	step [52/244], loss=63.7394
	step [53/244], loss=59.1625
	step [54/244], loss=65.6917
	step [55/244], loss=70.0614
	step [56/244], loss=54.4664
	step [57/244], loss=77.7336
	step [58/244], loss=70.1752
	step [59/244], loss=61.6937
	step [60/244], loss=82.9697
	step [61/244], loss=70.9007
	step [62/244], loss=68.6136
	step [63/244], loss=59.5734
	step [64/244], loss=81.1475
	step [65/244], loss=73.0328
	step [66/244], loss=66.2653
	step [67/244], loss=74.0103
	step [68/244], loss=64.8553
	step [69/244], loss=58.7618
	step [70/244], loss=68.7166
	step [71/244], loss=72.5983
	step [72/244], loss=67.4735
	step [73/244], loss=62.4940
	step [74/244], loss=67.9050
	step [75/244], loss=72.5031
	step [76/244], loss=65.6516
	step [77/244], loss=67.1494
	step [78/244], loss=63.9730
	step [79/244], loss=69.2239
	step [80/244], loss=56.8908
	step [81/244], loss=79.0882
	step [82/244], loss=61.8053
	step [83/244], loss=56.2155
	step [84/244], loss=59.7446
	step [85/244], loss=60.6368
	step [86/244], loss=58.2920
	step [87/244], loss=74.0421
	step [88/244], loss=69.7110
	step [89/244], loss=57.5581
	step [90/244], loss=72.9160
	step [91/244], loss=72.5347
	step [92/244], loss=66.4410
	step [93/244], loss=55.2190
	step [94/244], loss=74.2877
	step [95/244], loss=66.4353
	step [96/244], loss=76.7002
	step [97/244], loss=75.3008
	step [98/244], loss=67.5250
	step [99/244], loss=55.1451
	step [100/244], loss=61.3681
	step [101/244], loss=61.1164
	step [102/244], loss=56.4455
	step [103/244], loss=70.8234
	step [104/244], loss=74.2555
	step [105/244], loss=65.3919
	step [106/244], loss=77.6645
	step [107/244], loss=73.4193
	step [108/244], loss=71.7647
	step [109/244], loss=56.5203
	step [110/244], loss=67.3804
	step [111/244], loss=72.0741
	step [112/244], loss=64.4241
	step [113/244], loss=67.5579
	step [114/244], loss=63.4938
	step [115/244], loss=67.3769
	step [116/244], loss=67.7577
	step [117/244], loss=66.5257
	step [118/244], loss=65.1306
	step [119/244], loss=81.4469
	step [120/244], loss=76.2628
	step [121/244], loss=55.0018
	step [122/244], loss=78.9455
	step [123/244], loss=71.0783
	step [124/244], loss=74.9686
	step [125/244], loss=72.7651
	step [126/244], loss=68.3122
	step [127/244], loss=71.6669
	step [128/244], loss=65.2300
	step [129/244], loss=93.2419
	step [130/244], loss=65.7861
	step [131/244], loss=84.4068
	step [132/244], loss=78.3004
	step [133/244], loss=79.3902
	step [134/244], loss=56.0102
	step [135/244], loss=63.2162
	step [136/244], loss=64.3488
	step [137/244], loss=83.4323
	step [138/244], loss=67.9733
	step [139/244], loss=74.5502
	step [140/244], loss=58.2535
	step [141/244], loss=60.3833
	step [142/244], loss=64.2264
	step [143/244], loss=55.8294
	step [144/244], loss=90.5383
	step [145/244], loss=63.1357
	step [146/244], loss=63.2702
	step [147/244], loss=80.0473
	step [148/244], loss=69.0818
	step [149/244], loss=69.9878
	step [150/244], loss=75.8239
	step [151/244], loss=63.8260
	step [152/244], loss=55.2013
	step [153/244], loss=74.4565
	step [154/244], loss=73.4313
	step [155/244], loss=78.0562
	step [156/244], loss=77.6939
	step [157/244], loss=70.2507
	step [158/244], loss=72.7372
	step [159/244], loss=61.7602
	step [160/244], loss=65.9147
	step [161/244], loss=65.4212
	step [162/244], loss=81.8120
	step [163/244], loss=67.2000
	step [164/244], loss=64.5190
	step [165/244], loss=57.7178
	step [166/244], loss=69.3026
	step [167/244], loss=91.5071
	step [168/244], loss=76.2983
	step [169/244], loss=73.8709
	step [170/244], loss=70.7961
	step [171/244], loss=69.2177
	step [172/244], loss=56.0457
	step [173/244], loss=66.6358
	step [174/244], loss=60.0755
	step [175/244], loss=57.1194
	step [176/244], loss=61.7642
	step [177/244], loss=65.9628
	step [178/244], loss=71.3557
	step [179/244], loss=61.6955
	step [180/244], loss=69.4434
	step [181/244], loss=57.1791
	step [182/244], loss=55.4561
	step [183/244], loss=77.6635
	step [184/244], loss=59.5978
	step [185/244], loss=63.3683
	step [186/244], loss=51.9228
	step [187/244], loss=71.1308
	step [188/244], loss=64.8226
	step [189/244], loss=76.0655
	step [190/244], loss=72.1688
	step [191/244], loss=67.0647
	step [192/244], loss=99.1021
	step [193/244], loss=61.9220
	step [194/244], loss=63.9556
	step [195/244], loss=70.0366
	step [196/244], loss=64.5676
	step [197/244], loss=61.7696
	step [198/244], loss=57.5228
	step [199/244], loss=53.6439
	step [200/244], loss=62.3616
	step [201/244], loss=63.9183
	step [202/244], loss=59.8427
	step [203/244], loss=58.1094
	step [204/244], loss=69.7579
	step [205/244], loss=63.5311
	step [206/244], loss=79.7670
	step [207/244], loss=60.8221
	step [208/244], loss=65.5741
	step [209/244], loss=76.4074
	step [210/244], loss=67.8004
	step [211/244], loss=59.2455
	step [212/244], loss=69.5615
	step [213/244], loss=63.8490
	step [214/244], loss=70.9691
	step [215/244], loss=59.0918
	step [216/244], loss=86.9968
	step [217/244], loss=82.1826
	step [218/244], loss=75.8822
	step [219/244], loss=60.0388
	step [220/244], loss=69.9267
	step [221/244], loss=61.5966
	step [222/244], loss=67.5439
	step [223/244], loss=74.0908
	step [224/244], loss=82.9065
	step [225/244], loss=59.2874
	step [226/244], loss=55.0787
	step [227/244], loss=59.1354
	step [228/244], loss=65.2606
	step [229/244], loss=72.3486
	step [230/244], loss=73.9770
	step [231/244], loss=58.5027
	step [232/244], loss=62.1241
	step [233/244], loss=69.0947
	step [234/244], loss=72.5576
	step [235/244], loss=72.2034
	step [236/244], loss=59.4880
	step [237/244], loss=68.9580
	step [238/244], loss=81.3676
	step [239/244], loss=62.3574
	step [240/244], loss=66.9648
	step [241/244], loss=62.6281
	step [242/244], loss=77.4587
	step [243/244], loss=65.7251
	step [244/244], loss=61.8109
	Evaluating
	loss=0.0066, precision=0.3905, recall=0.8503, f1=0.5352
Training epoch 104
	step [1/244], loss=76.2712
	step [2/244], loss=68.3310
	step [3/244], loss=60.5613
	step [4/244], loss=57.8692
	step [5/244], loss=66.6669
	step [6/244], loss=56.7535
	step [7/244], loss=65.4887
	step [8/244], loss=68.4636
	step [9/244], loss=94.7074
	step [10/244], loss=67.4373
	step [11/244], loss=82.7152
	step [12/244], loss=69.3191
	step [13/244], loss=49.8653
	step [14/244], loss=68.6831
	step [15/244], loss=67.3087
	step [16/244], loss=66.7369
	step [17/244], loss=54.7971
	step [18/244], loss=65.0524
	step [19/244], loss=76.5310
	step [20/244], loss=62.2696
	step [21/244], loss=65.1303
	step [22/244], loss=69.2084
	step [23/244], loss=62.0146
	step [24/244], loss=74.5684
	step [25/244], loss=72.8409
	step [26/244], loss=66.7013
	step [27/244], loss=55.0038
	step [28/244], loss=83.0555
	step [29/244], loss=52.8381
	step [30/244], loss=69.7917
	step [31/244], loss=71.1668
	step [32/244], loss=62.1105
	step [33/244], loss=63.0909
	step [34/244], loss=65.0427
	step [35/244], loss=64.3681
	step [36/244], loss=61.9317
	step [37/244], loss=86.6018
	step [38/244], loss=77.6472
	step [39/244], loss=60.5920
	step [40/244], loss=57.1365
	step [41/244], loss=71.1999
	step [42/244], loss=77.4185
	step [43/244], loss=69.2598
	step [44/244], loss=69.5751
	step [45/244], loss=75.5063
	step [46/244], loss=63.5170
	step [47/244], loss=78.9921
	step [48/244], loss=67.4177
	step [49/244], loss=62.4434
	step [50/244], loss=89.0482
	step [51/244], loss=54.7745
	step [52/244], loss=71.8674
	step [53/244], loss=71.0783
	step [54/244], loss=75.1006
	step [55/244], loss=55.8904
	step [56/244], loss=73.6150
	step [57/244], loss=75.9383
	step [58/244], loss=67.4832
	step [59/244], loss=72.9290
	step [60/244], loss=62.7183
	step [61/244], loss=49.5197
	step [62/244], loss=52.7643
	step [63/244], loss=76.7403
	step [64/244], loss=77.5502
	step [65/244], loss=71.3417
	step [66/244], loss=63.3323
	step [67/244], loss=59.5234
	step [68/244], loss=72.0475
	step [69/244], loss=68.1265
	step [70/244], loss=67.4217
	step [71/244], loss=47.4067
	step [72/244], loss=69.5035
	step [73/244], loss=74.5085
	step [74/244], loss=70.3195
	step [75/244], loss=59.9498
	step [76/244], loss=61.0208
	step [77/244], loss=71.3155
	step [78/244], loss=66.4305
	step [79/244], loss=64.5936
	step [80/244], loss=66.5334
	step [81/244], loss=50.5437
	step [82/244], loss=71.1390
	step [83/244], loss=71.2835
	step [84/244], loss=67.5669
	step [85/244], loss=74.1952
	step [86/244], loss=58.3229
	step [87/244], loss=57.8317
	step [88/244], loss=64.3179
	step [89/244], loss=66.2999
	step [90/244], loss=58.1164
	step [91/244], loss=52.4508
	step [92/244], loss=64.5773
	step [93/244], loss=68.0176
	step [94/244], loss=68.9799
	step [95/244], loss=78.0816
	step [96/244], loss=63.6011
	step [97/244], loss=77.1281
	step [98/244], loss=77.5753
	step [99/244], loss=68.9069
	step [100/244], loss=76.3158
	step [101/244], loss=65.6135
	step [102/244], loss=76.1698
	step [103/244], loss=65.5777
	step [104/244], loss=66.6674
	step [105/244], loss=72.5892
	step [106/244], loss=65.9356
	step [107/244], loss=67.6263
	step [108/244], loss=76.5843
	step [109/244], loss=56.4311
	step [110/244], loss=68.6355
	step [111/244], loss=74.3673
	step [112/244], loss=73.4786
	step [113/244], loss=59.5669
	step [114/244], loss=78.7693
	step [115/244], loss=71.9433
	step [116/244], loss=72.3060
	step [117/244], loss=70.3463
	step [118/244], loss=63.8540
	step [119/244], loss=63.0435
	step [120/244], loss=78.5028
	step [121/244], loss=65.4531
	step [122/244], loss=55.5126
	step [123/244], loss=79.4507
	step [124/244], loss=85.4764
	step [125/244], loss=76.5183
	step [126/244], loss=72.7753
	step [127/244], loss=65.7663
	step [128/244], loss=71.8890
	step [129/244], loss=62.2592
	step [130/244], loss=70.6557
	step [131/244], loss=83.1344
	step [132/244], loss=69.1940
	step [133/244], loss=77.5181
	step [134/244], loss=83.6447
	step [135/244], loss=57.1450
	step [136/244], loss=72.7709
	step [137/244], loss=72.2728
	step [138/244], loss=71.0584
	step [139/244], loss=64.0372
	step [140/244], loss=71.1782
	step [141/244], loss=67.3419
	step [142/244], loss=84.8892
	step [143/244], loss=63.3471
	step [144/244], loss=59.5795
	step [145/244], loss=57.7884
	step [146/244], loss=69.1986
	step [147/244], loss=87.2761
	step [148/244], loss=73.1634
	step [149/244], loss=63.8586
	step [150/244], loss=76.6961
	step [151/244], loss=77.0372
	step [152/244], loss=66.2733
	step [153/244], loss=57.5056
	step [154/244], loss=69.2399
	step [155/244], loss=71.3882
	step [156/244], loss=62.7871
	step [157/244], loss=63.7878
	step [158/244], loss=81.6543
	step [159/244], loss=63.7342
	step [160/244], loss=66.6072
	step [161/244], loss=68.8162
	step [162/244], loss=68.2336
	step [163/244], loss=57.6421
	step [164/244], loss=69.2965
	step [165/244], loss=60.8494
	step [166/244], loss=58.7505
	step [167/244], loss=68.5212
	step [168/244], loss=59.0678
	step [169/244], loss=80.0829
	step [170/244], loss=63.8204
	step [171/244], loss=70.4081
	step [172/244], loss=58.9981
	step [173/244], loss=69.0561
	step [174/244], loss=61.9113
	step [175/244], loss=77.2322
	step [176/244], loss=69.0390
	step [177/244], loss=66.0303
	step [178/244], loss=63.6027
	step [179/244], loss=57.8529
	step [180/244], loss=82.7859
	step [181/244], loss=70.2679
	step [182/244], loss=70.9450
	step [183/244], loss=75.4012
	step [184/244], loss=61.6370
	step [185/244], loss=67.6198
	step [186/244], loss=69.5634
	step [187/244], loss=84.4258
	step [188/244], loss=63.1136
	step [189/244], loss=69.6954
	step [190/244], loss=61.1244
	step [191/244], loss=77.6166
	step [192/244], loss=59.7254
	step [193/244], loss=67.2835
	step [194/244], loss=66.8408
	step [195/244], loss=55.7842
	step [196/244], loss=70.9206
	step [197/244], loss=60.0318
	step [198/244], loss=57.9316
	step [199/244], loss=68.2588
	step [200/244], loss=63.5931
	step [201/244], loss=67.2729
	step [202/244], loss=61.8812
	step [203/244], loss=59.1658
	step [204/244], loss=53.2155
	step [205/244], loss=59.7076
	step [206/244], loss=69.6220
	step [207/244], loss=59.6421
	step [208/244], loss=62.0550
	step [209/244], loss=71.8708
	step [210/244], loss=69.3676
	step [211/244], loss=79.7084
	step [212/244], loss=88.5848
	step [213/244], loss=79.5793
	step [214/244], loss=72.9630
	step [215/244], loss=55.4423
	step [216/244], loss=71.9330
	step [217/244], loss=68.7297
	step [218/244], loss=65.3818
	step [219/244], loss=56.0724
	step [220/244], loss=72.2173
	step [221/244], loss=59.0311
	step [222/244], loss=75.2654
	step [223/244], loss=62.3029
	step [224/244], loss=71.0296
	step [225/244], loss=72.8327
	step [226/244], loss=70.8159
	step [227/244], loss=63.9845
	step [228/244], loss=83.7051
	step [229/244], loss=69.2184
	step [230/244], loss=58.7962
	step [231/244], loss=63.9356
	step [232/244], loss=64.9989
	step [233/244], loss=57.4211
	step [234/244], loss=81.8333
	step [235/244], loss=61.4720
	step [236/244], loss=72.6302
	step [237/244], loss=67.9760
	step [238/244], loss=67.5135
	step [239/244], loss=65.5242
	step [240/244], loss=75.0700
	step [241/244], loss=74.4990
	step [242/244], loss=74.1961
	step [243/244], loss=65.3163
	step [244/244], loss=42.6158
	Evaluating
	loss=0.0072, precision=0.3642, recall=0.8539, f1=0.5107
Training epoch 105
	step [1/244], loss=69.2725
	step [2/244], loss=59.7967
	step [3/244], loss=66.0185
	step [4/244], loss=66.6760
	step [5/244], loss=64.8965
	step [6/244], loss=60.6517
	step [7/244], loss=61.2103
	step [8/244], loss=66.3106
	step [9/244], loss=57.7341
	step [10/244], loss=69.5707
	step [11/244], loss=60.4799
	step [12/244], loss=68.4627
	step [13/244], loss=61.6076
	step [14/244], loss=58.8673
	step [15/244], loss=70.2583
	step [16/244], loss=63.3022
	step [17/244], loss=82.2507
	step [18/244], loss=68.4386
	step [19/244], loss=70.0706
	step [20/244], loss=59.3988
	step [21/244], loss=69.6165
	step [22/244], loss=55.9873
	step [23/244], loss=74.2008
	step [24/244], loss=68.9416
	step [25/244], loss=73.5642
	step [26/244], loss=83.1058
	step [27/244], loss=70.4325
	step [28/244], loss=66.7307
	step [29/244], loss=57.8864
	step [30/244], loss=66.2886
	step [31/244], loss=60.0926
	step [32/244], loss=81.9413
	step [33/244], loss=68.9766
	step [34/244], loss=69.3243
	step [35/244], loss=65.5804
	step [36/244], loss=51.2603
	step [37/244], loss=61.6376
	step [38/244], loss=64.9047
	step [39/244], loss=78.3707
	step [40/244], loss=60.7812
	step [41/244], loss=65.5133
	step [42/244], loss=76.4977
	step [43/244], loss=54.7340
	step [44/244], loss=85.3634
	step [45/244], loss=95.6492
	step [46/244], loss=77.2624
	step [47/244], loss=64.2165
	step [48/244], loss=59.3341
	step [49/244], loss=86.0569
	step [50/244], loss=76.5083
	step [51/244], loss=68.4130
	step [52/244], loss=58.0324
	step [53/244], loss=71.1674
	step [54/244], loss=89.2761
	step [55/244], loss=76.0639
	step [56/244], loss=69.3890
	step [57/244], loss=75.1375
	step [58/244], loss=59.8611
	step [59/244], loss=66.5423
	step [60/244], loss=73.9003
	step [61/244], loss=71.0656
	step [62/244], loss=58.8967
	step [63/244], loss=58.2409
	step [64/244], loss=61.1200
	step [65/244], loss=65.2346
	step [66/244], loss=70.2673
	step [67/244], loss=62.4841
	step [68/244], loss=68.9448
	step [69/244], loss=61.2605
	step [70/244], loss=84.3369
	step [71/244], loss=56.6913
	step [72/244], loss=67.1491
	step [73/244], loss=66.3644
	step [74/244], loss=66.6269
	step [75/244], loss=89.0052
	step [76/244], loss=55.1084
	step [77/244], loss=60.4091
	step [78/244], loss=70.7791
	step [79/244], loss=79.8760
	step [80/244], loss=64.1877
	step [81/244], loss=67.0715
	step [82/244], loss=54.8029
	step [83/244], loss=79.2690
	step [84/244], loss=54.6838
	step [85/244], loss=70.5546
	step [86/244], loss=78.8270
	step [87/244], loss=75.5751
	step [88/244], loss=69.7633
	step [89/244], loss=61.7450
	step [90/244], loss=85.2614
	step [91/244], loss=65.2604
	step [92/244], loss=53.7107
	step [93/244], loss=62.8434
	step [94/244], loss=73.6111
	step [95/244], loss=66.9695
	step [96/244], loss=76.5871
	step [97/244], loss=65.0097
	step [98/244], loss=70.3627
	step [99/244], loss=60.7648
	step [100/244], loss=64.2823
	step [101/244], loss=64.7170
	step [102/244], loss=71.6896
	step [103/244], loss=74.0006
	step [104/244], loss=73.4160
	step [105/244], loss=64.2723
	step [106/244], loss=61.1147
	step [107/244], loss=69.6792
	step [108/244], loss=75.2400
	step [109/244], loss=73.5843
	step [110/244], loss=66.7215
	step [111/244], loss=57.8740
	step [112/244], loss=59.4646
	step [113/244], loss=60.3510
	step [114/244], loss=69.2610
	step [115/244], loss=68.2616
	step [116/244], loss=60.3678
	step [117/244], loss=76.7716
	step [118/244], loss=61.8236
	step [119/244], loss=67.0939
	step [120/244], loss=64.5621
	step [121/244], loss=66.2654
	step [122/244], loss=68.7313
	step [123/244], loss=84.0004
	step [124/244], loss=59.2463
	step [125/244], loss=80.6501
	step [126/244], loss=72.8366
	step [127/244], loss=75.5043
	step [128/244], loss=64.3846
	step [129/244], loss=69.7478
	step [130/244], loss=62.2557
	step [131/244], loss=67.6491
	step [132/244], loss=60.6244
	step [133/244], loss=62.1887
	step [134/244], loss=59.4609
	step [135/244], loss=68.7272
	step [136/244], loss=55.6111
	step [137/244], loss=68.8650
	step [138/244], loss=63.8957
	step [139/244], loss=73.7331
	step [140/244], loss=62.7700
	step [141/244], loss=54.8343
	step [142/244], loss=62.1033
	step [143/244], loss=72.8736
	step [144/244], loss=70.9118
	step [145/244], loss=67.7879
	step [146/244], loss=69.1011
	step [147/244], loss=66.6041
	step [148/244], loss=48.0425
	step [149/244], loss=63.7103
	step [150/244], loss=66.7743
	step [151/244], loss=57.5434
	step [152/244], loss=76.7348
	step [153/244], loss=64.7057
	step [154/244], loss=53.8266
	step [155/244], loss=62.2274
	step [156/244], loss=68.3769
	step [157/244], loss=67.1570
	step [158/244], loss=57.8127
	step [159/244], loss=54.9167
	step [160/244], loss=70.6585
	step [161/244], loss=57.7858
	step [162/244], loss=69.3105
	step [163/244], loss=71.7865
	step [164/244], loss=68.8388
	step [165/244], loss=54.0265
	step [166/244], loss=75.0981
	step [167/244], loss=60.9835
	step [168/244], loss=72.5388
	step [169/244], loss=55.3955
	step [170/244], loss=65.0546
	step [171/244], loss=75.6635
	step [172/244], loss=71.8669
	step [173/244], loss=86.0617
	step [174/244], loss=79.9895
	step [175/244], loss=69.2046
	step [176/244], loss=73.2120
	step [177/244], loss=67.9177
	step [178/244], loss=73.5756
	step [179/244], loss=56.6831
	step [180/244], loss=73.0644
	step [181/244], loss=62.6782
	step [182/244], loss=63.8101
	step [183/244], loss=71.2346
	step [184/244], loss=51.1964
	step [185/244], loss=67.5951
	step [186/244], loss=67.6113
	step [187/244], loss=74.0518
	step [188/244], loss=52.0816
	step [189/244], loss=70.1890
	step [190/244], loss=71.7086
	step [191/244], loss=57.8209
	step [192/244], loss=73.3832
	step [193/244], loss=61.9715
	step [194/244], loss=61.3422
	step [195/244], loss=68.1420
	step [196/244], loss=76.2320
	step [197/244], loss=74.4477
	step [198/244], loss=76.1693
	step [199/244], loss=58.3837
	step [200/244], loss=61.6627
	step [201/244], loss=53.3653
	step [202/244], loss=70.9954
	step [203/244], loss=60.1066
	step [204/244], loss=62.9402
	step [205/244], loss=85.1055
	step [206/244], loss=72.0854
	step [207/244], loss=72.8876
	step [208/244], loss=70.2096
	step [209/244], loss=74.4073
	step [210/244], loss=64.2536
	step [211/244], loss=66.0943
	step [212/244], loss=60.9401
	step [213/244], loss=75.7981
	step [214/244], loss=66.0701
	step [215/244], loss=61.5672
	step [216/244], loss=68.1383
	step [217/244], loss=51.5053
	step [218/244], loss=69.8795
	step [219/244], loss=65.1914
	step [220/244], loss=80.1235
	step [221/244], loss=76.7487
	step [222/244], loss=55.0597
	step [223/244], loss=77.9931
	step [224/244], loss=65.4825
	step [225/244], loss=80.1008
	step [226/244], loss=58.4076
	step [227/244], loss=68.9429
	step [228/244], loss=58.9045
	step [229/244], loss=87.1062
	step [230/244], loss=67.5610
	step [231/244], loss=59.4165
	step [232/244], loss=77.8480
	step [233/244], loss=82.2174
	step [234/244], loss=82.5074
	step [235/244], loss=60.4442
	step [236/244], loss=65.0801
	step [237/244], loss=74.4971
	step [238/244], loss=61.4414
	step [239/244], loss=70.3419
	step [240/244], loss=80.4734
	step [241/244], loss=65.3974
	step [242/244], loss=73.5236
	step [243/244], loss=107.9986
	step [244/244], loss=73.8683
	Evaluating
	loss=0.0073, precision=0.3596, recall=0.8493, f1=0.5053
Training finished
best_f1: 0.5562487761898333
directing: Y rim_enhanced: True test_id 2
removed wrong scan: weights_Y_335_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_383_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_262_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_311_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_260_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_225_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_320_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_192_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_319_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_226_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_346_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_271_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_277_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_290_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_269_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_379_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_375_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_378_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_366_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_309_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_200_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_327_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_193_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_174_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_253_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_220_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_265_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_360_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_303_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_232_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_243_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_240_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_237_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_256_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_254_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_254_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_231_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_359_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_172_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_340_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_263_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_388_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_307_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_278_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_196_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_272_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_314_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_350_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_361_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_194_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_323_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_201_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_299_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_259_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_276_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_167_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_185_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_215_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_268_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_264_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_384_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_261_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_280_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_287_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_385_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_216_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_349_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_348_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_371_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_357_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_234_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_288_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_324_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_229_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_184_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_148_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_168_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_370_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_286_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_302_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_265_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_153_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_274_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_255_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_217_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_355_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_294_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_159_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_338_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_236_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_152_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_292_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_222_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_376_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_183_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_258_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_282_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_278_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_284_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_300_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_163_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_197_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_203_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_243_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_224_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_238_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_374_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_218_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_259_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_282_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_387_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_353_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_386_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_296_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_341_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_206_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_220_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_239_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_231_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_202_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_358_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_246_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_315_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_275_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_169_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_316_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_221_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_188_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_208_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_273_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_251_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_224_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_333_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_263_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_173_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_243_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_241_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_326_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_230_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_236_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_345_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_206_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_258_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_187_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_87_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_154_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_253_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_337_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_232_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_147_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_377_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_362_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_233_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_250_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_297_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_354_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_262_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_225_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_365_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_356_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_242_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_245_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_230_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_318_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_181_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_305_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_255_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_298_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_301_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_293_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_229_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_332_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_257_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_295_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_239_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_260_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_228_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_317_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_364_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_262_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_162_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_310_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_160_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_237_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_254_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_257_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_291_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_279_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_215_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_372_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_218_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_210_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_158_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_156_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_339_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_190_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_373_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_205_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_329_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_380_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_271_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_235_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_244_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_312_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_186_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_308_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_347_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_241_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_270_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_261_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_222_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_189_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_289_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_146_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_252_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_344_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_331_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_313_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_328_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_265_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_283_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_248_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_381_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_211_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_214_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_176_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_221_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_150_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_343_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_249_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_322_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_351_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_234_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_195_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_330_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_334_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_306_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_213_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_325_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_342_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_247_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_352_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_225_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_226_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_256_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_242_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_368_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_382_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_157_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_238_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_267_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_367_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_321_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_233_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_281_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_161_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_336_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_170_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_304_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_44_xwqg-B00034_2020-04-03.npy
# all image files: 21521 # all weight files in weight_dir: 15586 # image files with weight 15586
removed wrong scan: weights_Y_335_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_383_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_262_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_311_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_260_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_225_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_320_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_192_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_319_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_226_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_346_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_271_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_277_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_290_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_269_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_379_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_375_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_378_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_366_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_309_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_200_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_327_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_193_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_174_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_253_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_220_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_265_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_360_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_303_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_232_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_243_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_240_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_237_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_256_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_254_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_254_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_231_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_359_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_172_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_340_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_263_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_388_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_307_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_278_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_196_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_272_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_314_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_350_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_361_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_194_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_323_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_201_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_299_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_259_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_276_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_167_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_185_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_215_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_268_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_264_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_384_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_261_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_280_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_287_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_385_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_216_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_349_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_348_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_371_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_357_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_234_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_288_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_324_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_229_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_184_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_148_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_168_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_370_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_286_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_302_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_265_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_153_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_274_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_255_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_217_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_355_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_294_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_159_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_338_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_236_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_152_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_292_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_222_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_376_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_183_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_258_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_282_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_278_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_284_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_300_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_163_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_197_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_203_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_243_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_224_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_238_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_374_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_218_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_259_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_282_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_387_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_353_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_386_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_296_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_341_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_206_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_220_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_239_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_231_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_202_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_358_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_246_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_315_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_275_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_169_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_316_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_221_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_188_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_208_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_273_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_251_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_224_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_333_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_263_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_173_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_243_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_241_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_326_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_230_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_236_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_345_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_206_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_258_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_187_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_87_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_154_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_253_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_337_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_232_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_147_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_377_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_362_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_233_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_250_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_297_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_354_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_262_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_225_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_365_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_356_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_242_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_245_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_230_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_318_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_181_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_305_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_255_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_298_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_301_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_293_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_229_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_332_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_257_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_295_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_239_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_260_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_228_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_317_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_364_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_262_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_162_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_310_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_160_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_237_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_254_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_257_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_291_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_279_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_215_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_372_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_218_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_210_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_158_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_156_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_339_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_190_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_373_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_205_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_329_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_380_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_271_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_235_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_244_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_312_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_186_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_308_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_347_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_241_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_270_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_261_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_222_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_189_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_289_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_146_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_252_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_344_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_331_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_313_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_328_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_265_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_283_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_248_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_381_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_211_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_214_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_176_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_221_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_150_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_343_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_249_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_322_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_351_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_234_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_195_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_330_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_334_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_306_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_213_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_325_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_342_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_247_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_352_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_225_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_226_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_256_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_242_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_368_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_382_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_157_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_238_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_267_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_367_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_321_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_233_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_281_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_161_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_336_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_170_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_304_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_44_xwqg-B00034_2020-04-03.npy
# all image files: 21521 # all weight files in weight_dir: 4486 # image files with weight 4486
train: /ibex/scratch/projects/c2052/air_tube_seg/training_samples/Y 15586
Using 4 GPUs
Going to train epochs [61-110]
Training epoch 61
	step [1/244], loss=82.8650
	step [2/244], loss=57.3406
	step [3/244], loss=78.2601
	step [4/244], loss=62.1513
	step [5/244], loss=105.7672
	step [6/244], loss=85.6542
	step [7/244], loss=89.5565
	step [8/244], loss=99.3318
	step [9/244], loss=73.9169
	step [10/244], loss=90.3221
	step [11/244], loss=76.4192
	step [12/244], loss=100.2284
	step [13/244], loss=72.4001
	step [14/244], loss=83.6706
	step [15/244], loss=72.6995
	step [16/244], loss=85.1953
	step [17/244], loss=75.3908
	step [18/244], loss=86.6269
	step [19/244], loss=77.9570
	step [20/244], loss=93.0033
	step [21/244], loss=87.4397
	step [22/244], loss=85.6340
	step [23/244], loss=81.7994
	step [24/244], loss=72.1425
	step [25/244], loss=84.3949
	step [26/244], loss=100.7222
	step [27/244], loss=92.1343
	step [28/244], loss=89.3975
	step [29/244], loss=87.2162
	step [30/244], loss=96.0971
	step [31/244], loss=79.4147
	step [32/244], loss=98.4802
	step [33/244], loss=73.7506
	step [34/244], loss=82.0383
	step [35/244], loss=75.1733
	step [36/244], loss=93.4087
	step [37/244], loss=85.3817
	step [38/244], loss=86.8566
	step [39/244], loss=105.0872
	step [40/244], loss=97.7463
	step [41/244], loss=84.4833
	step [42/244], loss=81.8471
	step [43/244], loss=83.9279
	step [44/244], loss=82.6999
	step [45/244], loss=95.2518
	step [46/244], loss=81.8942
	step [47/244], loss=86.0148
	step [48/244], loss=75.6855
	step [49/244], loss=66.9786
	step [50/244], loss=86.4020
	step [51/244], loss=76.1838
	step [52/244], loss=90.2228
	step [53/244], loss=89.1104
	step [54/244], loss=91.6456
	step [55/244], loss=80.8238
	step [56/244], loss=73.3132
	step [57/244], loss=90.1835
	step [58/244], loss=89.9584
	step [59/244], loss=61.8608
	step [60/244], loss=75.9674
	step [61/244], loss=97.9737
	step [62/244], loss=68.9067
	step [63/244], loss=77.7944
	step [64/244], loss=86.5349
	step [65/244], loss=67.7776
	step [66/244], loss=86.0533
	step [67/244], loss=95.2330
	step [68/244], loss=73.8594
	step [69/244], loss=93.9611
	step [70/244], loss=68.0978
	step [71/244], loss=79.2564
	step [72/244], loss=95.9371
	step [73/244], loss=93.0475
	step [74/244], loss=65.2198
	step [75/244], loss=75.3026
	step [76/244], loss=91.2698
	step [77/244], loss=76.8712
	step [78/244], loss=69.3949
	step [79/244], loss=79.2539
	step [80/244], loss=91.6661
	step [81/244], loss=85.6736
	step [82/244], loss=105.0885
	step [83/244], loss=82.0387
	step [84/244], loss=69.2367
	step [85/244], loss=83.9732
	step [86/244], loss=73.7093
	step [87/244], loss=94.2312
	step [88/244], loss=100.5207
	step [89/244], loss=105.8726
	step [90/244], loss=95.2722
	step [91/244], loss=92.7753
	step [92/244], loss=99.0736
	step [93/244], loss=100.2461
	step [94/244], loss=80.8517
	step [95/244], loss=90.9718
	step [96/244], loss=89.4141
	step [97/244], loss=69.7267
	step [98/244], loss=97.1796
	step [99/244], loss=90.1193
	step [100/244], loss=70.0545
	step [101/244], loss=90.2642
	step [102/244], loss=76.6004
	step [103/244], loss=80.9384
	step [104/244], loss=79.0470
	step [105/244], loss=79.3174
	step [106/244], loss=74.8812
	step [107/244], loss=77.5492
	step [108/244], loss=89.7400
	step [109/244], loss=77.3375
	step [110/244], loss=91.8347
	step [111/244], loss=68.2179
	step [112/244], loss=78.3158
	step [113/244], loss=96.8622
	step [114/244], loss=86.6230
	step [115/244], loss=89.0010
	step [116/244], loss=75.7431
	step [117/244], loss=62.6449
	step [118/244], loss=76.5027
	step [119/244], loss=87.7490
	step [120/244], loss=71.9722
	step [121/244], loss=90.6388
	step [122/244], loss=80.4885
	step [123/244], loss=94.1833
	step [124/244], loss=103.3456
	step [125/244], loss=91.9217
	step [126/244], loss=86.1735
	step [127/244], loss=62.5085
	step [128/244], loss=90.9780
	step [129/244], loss=76.2116
	step [130/244], loss=96.5702
	step [131/244], loss=84.8274
	step [132/244], loss=84.6480
	step [133/244], loss=80.8784
	step [134/244], loss=91.3750
	step [135/244], loss=96.4964
	step [136/244], loss=87.0803
	step [137/244], loss=73.2635
	step [138/244], loss=84.9299
	step [139/244], loss=102.1627
	step [140/244], loss=83.2864
	step [141/244], loss=81.4353
	step [142/244], loss=73.6824
	step [143/244], loss=96.8077
	step [144/244], loss=112.5999
	step [145/244], loss=95.8210
	step [146/244], loss=85.2871
	step [147/244], loss=69.7092
	step [148/244], loss=83.2465
	step [149/244], loss=87.1164
	step [150/244], loss=94.6301
	step [151/244], loss=83.1211
	step [152/244], loss=64.4439
	step [153/244], loss=85.4021
	step [154/244], loss=69.4628
	step [155/244], loss=74.6031
	step [156/244], loss=83.0583
	step [157/244], loss=113.1815
	step [158/244], loss=87.7800
	step [159/244], loss=95.6156
	step [160/244], loss=83.1799
	step [161/244], loss=89.5685
	step [162/244], loss=89.9266
	step [163/244], loss=82.8347
	step [164/244], loss=97.9844
	step [165/244], loss=94.7174
	step [166/244], loss=92.6667
	step [167/244], loss=95.5165
	step [168/244], loss=94.4100
	step [169/244], loss=74.2401
	step [170/244], loss=95.0257
	step [171/244], loss=103.1859
	step [172/244], loss=101.4534
	step [173/244], loss=101.6069
	step [174/244], loss=88.2701
	step [175/244], loss=83.8577
	step [176/244], loss=86.2733
	step [177/244], loss=86.9249
	step [178/244], loss=76.4801
	step [179/244], loss=92.1654
	step [180/244], loss=66.9622
	step [181/244], loss=83.4697
	step [182/244], loss=76.6769
	step [183/244], loss=88.5655
	step [184/244], loss=74.4245
	step [185/244], loss=79.1307
	step [186/244], loss=66.5349
	step [187/244], loss=82.6888
	step [188/244], loss=66.1034
	step [189/244], loss=88.0977
	step [190/244], loss=83.2681
	step [191/244], loss=84.6289
	step [192/244], loss=98.4660
	step [193/244], loss=81.1223
	step [194/244], loss=98.4126
	step [195/244], loss=76.5962
	step [196/244], loss=77.0038
	step [197/244], loss=82.5119
	step [198/244], loss=70.6550
	step [199/244], loss=87.0046
	step [200/244], loss=86.6145
	step [201/244], loss=65.4080
	step [202/244], loss=83.6232
	step [203/244], loss=78.6964
	step [204/244], loss=92.2817
	step [205/244], loss=81.3740
	step [206/244], loss=75.8250
	step [207/244], loss=88.5837
	step [208/244], loss=86.8102
	step [209/244], loss=94.1071
	step [210/244], loss=74.3049
	step [211/244], loss=76.1611
	step [212/244], loss=110.4902
	step [213/244], loss=77.1855
	step [214/244], loss=76.8505
	step [215/244], loss=101.2040
	step [216/244], loss=80.8808
	step [217/244], loss=82.5250
	step [218/244], loss=95.3548
	step [219/244], loss=77.4279
	step [220/244], loss=99.2767
	step [221/244], loss=74.1388
	step [222/244], loss=80.3472
	step [223/244], loss=89.6151
	step [224/244], loss=75.8535
	step [225/244], loss=80.7719
	step [226/244], loss=81.7644
	step [227/244], loss=93.1387
	step [228/244], loss=83.7932
	step [229/244], loss=82.3221
	step [230/244], loss=85.2955
	step [231/244], loss=95.1869
	step [232/244], loss=86.9662
	step [233/244], loss=75.6980
	step [234/244], loss=70.1333
	step [235/244], loss=68.4812
	step [236/244], loss=78.7418
	step [237/244], loss=79.6273
	step [238/244], loss=80.3191
	step [239/244], loss=87.6144
	step [240/244], loss=74.7440
	step [241/244], loss=81.2551
	step [242/244], loss=83.1977
	step [243/244], loss=89.6177
	step [244/244], loss=50.5895
	Evaluating
	loss=0.0099, precision=0.2990, recall=0.8654, f1=0.4444
saving model as: 2_saved_model.pth
Training epoch 62
	step [1/244], loss=88.3237
	step [2/244], loss=89.5835
	step [3/244], loss=96.2718
	step [4/244], loss=89.5665
	step [5/244], loss=87.8332
	step [6/244], loss=83.9923
	step [7/244], loss=85.9590
	step [8/244], loss=96.2440
	step [9/244], loss=78.4751
	step [10/244], loss=95.6961
	step [11/244], loss=78.1262
	step [12/244], loss=83.3350
	step [13/244], loss=68.7028
	step [14/244], loss=86.3687
	step [15/244], loss=93.7591
	step [16/244], loss=90.3158
	step [17/244], loss=80.3185
	step [18/244], loss=102.2144
	step [19/244], loss=78.7621
	step [20/244], loss=72.4443
	step [21/244], loss=79.0066
	step [22/244], loss=93.6288
	step [23/244], loss=93.0751
	step [24/244], loss=68.7708
	step [25/244], loss=75.5559
	step [26/244], loss=90.3939
	step [27/244], loss=78.1569
	step [28/244], loss=67.1486
	step [29/244], loss=98.0956
	step [30/244], loss=82.1868
	step [31/244], loss=87.1385
	step [32/244], loss=86.2628
	step [33/244], loss=76.4023
	step [34/244], loss=95.6541
	step [35/244], loss=88.6113
	step [36/244], loss=74.0187
	step [37/244], loss=94.3257
	step [38/244], loss=84.0534
	step [39/244], loss=83.7231
	step [40/244], loss=74.5455
	step [41/244], loss=84.2425
	step [42/244], loss=86.4081
	step [43/244], loss=88.5257
	step [44/244], loss=84.6349
	step [45/244], loss=78.8459
	step [46/244], loss=74.9167
	step [47/244], loss=85.8627
	step [48/244], loss=84.3775
	step [49/244], loss=93.9022
	step [50/244], loss=80.8584
	step [51/244], loss=88.1983
	step [52/244], loss=96.7207
	step [53/244], loss=100.3646
	step [54/244], loss=79.5666
	step [55/244], loss=77.8541
	step [56/244], loss=77.6313
	step [57/244], loss=72.1840
	step [58/244], loss=89.6281
	step [59/244], loss=76.2328
	step [60/244], loss=92.9522
	step [61/244], loss=74.0888
	step [62/244], loss=110.7262
	step [63/244], loss=68.2562
	step [64/244], loss=65.2704
	step [65/244], loss=86.1311
	step [66/244], loss=105.6268
	step [67/244], loss=85.2187
	step [68/244], loss=79.6001
	step [69/244], loss=88.9341
	step [70/244], loss=75.5130
	step [71/244], loss=90.4142
	step [72/244], loss=78.8149
	step [73/244], loss=78.4751
	step [74/244], loss=77.9994
	step [75/244], loss=89.7951
	step [76/244], loss=101.5252
	step [77/244], loss=82.6930
	step [78/244], loss=95.3588
	step [79/244], loss=85.5334
	step [80/244], loss=81.4020
	step [81/244], loss=65.8646
	step [82/244], loss=73.7303
	step [83/244], loss=101.2362
	step [84/244], loss=72.8408
	step [85/244], loss=80.1848
	step [86/244], loss=80.0399
	step [87/244], loss=71.9707
	step [88/244], loss=80.2691
	step [89/244], loss=104.7785
	step [90/244], loss=84.3826
	step [91/244], loss=74.4753
	step [92/244], loss=114.8535
	step [93/244], loss=91.7844
	step [94/244], loss=77.0684
	step [95/244], loss=78.5611
	step [96/244], loss=66.0138
	step [97/244], loss=78.4713
	step [98/244], loss=65.0159
	step [99/244], loss=86.9247
	step [100/244], loss=78.4115
	step [101/244], loss=79.6945
	step [102/244], loss=100.6505
	step [103/244], loss=75.4875
	step [104/244], loss=102.9139
	step [105/244], loss=75.3155
	step [106/244], loss=76.5118
	step [107/244], loss=103.7275
	step [108/244], loss=96.6064
	step [109/244], loss=91.9139
	step [110/244], loss=79.9944
	step [111/244], loss=89.7517
	step [112/244], loss=104.6635
	step [113/244], loss=106.8843
	step [114/244], loss=98.6478
	step [115/244], loss=99.9043
	step [116/244], loss=80.1969
	step [117/244], loss=89.2198
	step [118/244], loss=83.1570
	step [119/244], loss=83.4642
	step [120/244], loss=71.0392
	step [121/244], loss=89.0826
	step [122/244], loss=96.8931
	step [123/244], loss=70.1196
	step [124/244], loss=75.0735
	step [125/244], loss=91.2630
	step [126/244], loss=98.0028
	step [127/244], loss=86.3984
	step [128/244], loss=96.1596
	step [129/244], loss=85.6150
	step [130/244], loss=94.8152
	step [131/244], loss=85.6510
	step [132/244], loss=95.1068
	step [133/244], loss=78.7014
	step [134/244], loss=79.2858
	step [135/244], loss=79.9935
	step [136/244], loss=79.9800
	step [137/244], loss=67.5820
	step [138/244], loss=93.9800
	step [139/244], loss=79.9147
	step [140/244], loss=88.6324
	step [141/244], loss=79.5365
	step [142/244], loss=87.3724
	step [143/244], loss=64.3337
	step [144/244], loss=83.7014
	step [145/244], loss=62.8009
	step [146/244], loss=81.3007
	step [147/244], loss=92.3729
	step [148/244], loss=91.1024
	step [149/244], loss=77.6662
	step [150/244], loss=74.7903
	step [151/244], loss=65.9333
	step [152/244], loss=79.1083
	step [153/244], loss=105.2352
	step [154/244], loss=93.4164
	step [155/244], loss=81.1492
	step [156/244], loss=81.3489
	step [157/244], loss=85.9005
	step [158/244], loss=107.1873
	step [159/244], loss=75.3887
	step [160/244], loss=75.8585
	step [161/244], loss=85.0538
	step [162/244], loss=82.2593
	step [163/244], loss=96.7685
	step [164/244], loss=90.1167
	step [165/244], loss=80.5865
	step [166/244], loss=80.0413
	step [167/244], loss=79.1442
	step [168/244], loss=76.6463
	step [169/244], loss=85.6317
	step [170/244], loss=80.1115
	step [171/244], loss=89.7549
	step [172/244], loss=89.4311
	step [173/244], loss=88.4495
	step [174/244], loss=84.9321
	step [175/244], loss=82.0715
	step [176/244], loss=73.7780
	step [177/244], loss=72.9861
	step [178/244], loss=74.3785
	step [179/244], loss=82.9051
	step [180/244], loss=62.2122
	step [181/244], loss=100.3142
	step [182/244], loss=69.9494
	step [183/244], loss=59.6399
	step [184/244], loss=81.9217
	step [185/244], loss=79.4894
	step [186/244], loss=63.3704
	step [187/244], loss=96.0650
	step [188/244], loss=79.3114
	step [189/244], loss=72.6558
	step [190/244], loss=100.0563
	step [191/244], loss=80.5353
	step [192/244], loss=69.0053
	step [193/244], loss=73.2670
	step [194/244], loss=69.4459
	step [195/244], loss=81.7610
	step [196/244], loss=104.1704
	step [197/244], loss=71.0477
	step [198/244], loss=95.9117
	step [199/244], loss=75.2230
	step [200/244], loss=86.0556
	step [201/244], loss=87.8774
	step [202/244], loss=84.8340
	step [203/244], loss=97.4079
	step [204/244], loss=91.7785
	step [205/244], loss=81.7684
	step [206/244], loss=92.0774
	step [207/244], loss=95.8361
	step [208/244], loss=91.5567
	step [209/244], loss=83.9503
	step [210/244], loss=72.0861
	step [211/244], loss=81.6775
	step [212/244], loss=85.8152
	step [213/244], loss=93.3619
	step [214/244], loss=94.6497
	step [215/244], loss=84.6175
	step [216/244], loss=62.9568
	step [217/244], loss=98.2079
	step [218/244], loss=65.4753
	step [219/244], loss=85.6009
	step [220/244], loss=65.6771
	step [221/244], loss=100.5953
	step [222/244], loss=76.3432
	step [223/244], loss=65.7498
	step [224/244], loss=70.6965
	step [225/244], loss=84.1428
	step [226/244], loss=70.4741
	step [227/244], loss=80.4212
	step [228/244], loss=80.0998
	step [229/244], loss=99.7664
	step [230/244], loss=85.1679
	step [231/244], loss=76.3001
	step [232/244], loss=89.2018
	step [233/244], loss=78.6231
	step [234/244], loss=88.5737
	step [235/244], loss=87.5185
	step [236/244], loss=73.3070
	step [237/244], loss=99.7441
	step [238/244], loss=82.6235
	step [239/244], loss=78.6510
	step [240/244], loss=94.4824
	step [241/244], loss=87.2939
	step [242/244], loss=81.5445
	step [243/244], loss=87.7862
	step [244/244], loss=49.4677
	Evaluating
	loss=0.0097, precision=0.3008, recall=0.8772, f1=0.4480
saving model as: 2_saved_model.pth
Training epoch 63
	step [1/244], loss=106.3049
	step [2/244], loss=62.1695
	step [3/244], loss=81.5800
	step [4/244], loss=94.1107
	step [5/244], loss=72.7154
	step [6/244], loss=89.5646
	step [7/244], loss=79.0505
	step [8/244], loss=94.8833
	step [9/244], loss=86.6761
	step [10/244], loss=84.3658
	step [11/244], loss=107.0824
	step [12/244], loss=96.6452
	step [13/244], loss=60.5983
	step [14/244], loss=75.8228
	step [15/244], loss=80.9296
	step [16/244], loss=74.7716
	step [17/244], loss=79.3038
	step [18/244], loss=81.5398
	step [19/244], loss=82.4299
	step [20/244], loss=96.3633
	step [21/244], loss=63.0331
	step [22/244], loss=84.3529
	step [23/244], loss=85.4243
	step [24/244], loss=77.8957
	step [25/244], loss=101.7640
	step [26/244], loss=90.8958
	step [27/244], loss=73.5773
	step [28/244], loss=100.8714
	step [29/244], loss=71.1988
	step [30/244], loss=88.1541
	step [31/244], loss=77.3631
	step [32/244], loss=85.7998
	step [33/244], loss=99.7786
	step [34/244], loss=84.4367
	step [35/244], loss=83.8019
	step [36/244], loss=82.1045
	step [37/244], loss=80.2244
	step [38/244], loss=61.7517
	step [39/244], loss=106.2151
	step [40/244], loss=98.0345
	step [41/244], loss=79.0540
	step [42/244], loss=80.5674
	step [43/244], loss=73.8923
	step [44/244], loss=72.5853
	step [45/244], loss=86.9678
	step [46/244], loss=79.7888
	step [47/244], loss=83.6487
	step [48/244], loss=75.3880
	step [49/244], loss=89.0638
	step [50/244], loss=90.5251
	step [51/244], loss=94.4265
	step [52/244], loss=69.8117
	step [53/244], loss=69.1606
	step [54/244], loss=77.9115
	step [55/244], loss=79.9326
	step [56/244], loss=77.4821
	step [57/244], loss=69.7699
	step [58/244], loss=83.3862
	step [59/244], loss=70.3083
	step [60/244], loss=65.3207
	step [61/244], loss=90.1804
	step [62/244], loss=105.7838
	step [63/244], loss=72.7747
	step [64/244], loss=97.9502
	step [65/244], loss=88.3775
	step [66/244], loss=77.9763
	step [67/244], loss=83.1923
	step [68/244], loss=79.9633
	step [69/244], loss=98.0102
	step [70/244], loss=78.2179
	step [71/244], loss=68.7263
	step [72/244], loss=86.9210
	step [73/244], loss=83.5402
	step [74/244], loss=65.7890
	step [75/244], loss=94.9963
	step [76/244], loss=74.8994
	step [77/244], loss=96.4712
	step [78/244], loss=87.7520
	step [79/244], loss=94.8898
	step [80/244], loss=88.7174
	step [81/244], loss=94.3069
	step [82/244], loss=85.3884
	step [83/244], loss=96.4265
	step [84/244], loss=80.5386
	step [85/244], loss=85.7175
	step [86/244], loss=95.4320
	step [87/244], loss=83.1539
	step [88/244], loss=87.8695
	step [89/244], loss=95.0600
	step [90/244], loss=70.7685
	step [91/244], loss=100.9026
	step [92/244], loss=87.0092
	step [93/244], loss=80.0291
	step [94/244], loss=95.1435
	step [95/244], loss=90.5174
	step [96/244], loss=64.2213
	step [97/244], loss=82.3999
	step [98/244], loss=69.8524
	step [99/244], loss=84.9023
	step [100/244], loss=85.8649
	step [101/244], loss=74.9478
	step [102/244], loss=85.9905
	step [103/244], loss=84.2446
	step [104/244], loss=64.9077
	step [105/244], loss=66.5108
	step [106/244], loss=72.5337
	step [107/244], loss=83.4464
	step [108/244], loss=82.4508
	step [109/244], loss=79.5070
	step [110/244], loss=87.0972
	step [111/244], loss=71.4541
	step [112/244], loss=102.9319
	step [113/244], loss=81.6156
	step [114/244], loss=77.9354
	step [115/244], loss=76.2424
	step [116/244], loss=93.7433
	step [117/244], loss=70.1905
	step [118/244], loss=87.7867
	step [119/244], loss=97.7837
	step [120/244], loss=77.9927
	step [121/244], loss=83.1658
	step [122/244], loss=76.1422
	step [123/244], loss=73.5081
	step [124/244], loss=81.1641
	step [125/244], loss=73.9603
	step [126/244], loss=103.1651
	step [127/244], loss=77.7933
	step [128/244], loss=88.9617
	step [129/244], loss=103.7490
	step [130/244], loss=87.6141
	step [131/244], loss=81.6804
	step [132/244], loss=83.6790
	step [133/244], loss=74.0658
	step [134/244], loss=69.5802
	step [135/244], loss=84.2363
	step [136/244], loss=72.4364
	step [137/244], loss=78.8168
	step [138/244], loss=74.4575
	step [139/244], loss=87.5271
	step [140/244], loss=96.9058
	step [141/244], loss=80.8593
	step [142/244], loss=83.8313
	step [143/244], loss=78.2388
	step [144/244], loss=80.1631
	step [145/244], loss=84.0441
	step [146/244], loss=93.1167
	step [147/244], loss=80.1486
	step [148/244], loss=70.9493
	step [149/244], loss=92.2065
	step [150/244], loss=90.4938
	step [151/244], loss=76.6470
	step [152/244], loss=86.2174
	step [153/244], loss=79.3844
	step [154/244], loss=66.6748
	step [155/244], loss=78.8711
	step [156/244], loss=91.1899
	step [157/244], loss=89.3394
	step [158/244], loss=80.0852
	step [159/244], loss=91.9165
	step [160/244], loss=91.6064
	step [161/244], loss=87.6916
	step [162/244], loss=82.4223
	step [163/244], loss=86.3653
	step [164/244], loss=94.9214
	step [165/244], loss=90.4154
	step [166/244], loss=65.2575
	step [167/244], loss=69.0766
	step [168/244], loss=75.4213
	step [169/244], loss=75.5953
	step [170/244], loss=94.4403
	step [171/244], loss=86.8354
	step [172/244], loss=60.7287
	step [173/244], loss=89.0226
	step [174/244], loss=93.8906
	step [175/244], loss=92.3095
	step [176/244], loss=87.3552
	step [177/244], loss=97.7474
	step [178/244], loss=89.1570
	step [179/244], loss=86.9701
	step [180/244], loss=90.7285
	step [181/244], loss=83.5864
	step [182/244], loss=89.0568
	step [183/244], loss=99.4867
	step [184/244], loss=80.8149
	step [185/244], loss=77.0872
	step [186/244], loss=94.5133
	step [187/244], loss=95.0861
	step [188/244], loss=90.3464
	step [189/244], loss=81.5176
	step [190/244], loss=82.4173
	step [191/244], loss=77.9759
	step [192/244], loss=76.3831
	step [193/244], loss=93.2110
	step [194/244], loss=68.8187
	step [195/244], loss=101.3379
	step [196/244], loss=80.4243
	step [197/244], loss=95.7213
	step [198/244], loss=96.5057
	step [199/244], loss=79.5206
	step [200/244], loss=57.8438
	step [201/244], loss=86.6416
	step [202/244], loss=70.8588
	step [203/244], loss=90.3858
	step [204/244], loss=75.6016
	step [205/244], loss=81.5330
	step [206/244], loss=62.7236
	step [207/244], loss=101.3960
	step [208/244], loss=77.5745
	step [209/244], loss=70.8944
	step [210/244], loss=66.2973
	step [211/244], loss=80.4191
	step [212/244], loss=93.7390
	step [213/244], loss=91.2430
	step [214/244], loss=79.1912
	step [215/244], loss=86.5484
	step [216/244], loss=74.7983
	step [217/244], loss=71.8728
	step [218/244], loss=71.7835
	step [219/244], loss=77.6372
	step [220/244], loss=90.9779
	step [221/244], loss=62.3487
	step [222/244], loss=99.3965
	step [223/244], loss=81.6206
	step [224/244], loss=72.4546
	step [225/244], loss=69.1339
	step [226/244], loss=87.9800
	step [227/244], loss=92.8974
	step [228/244], loss=87.7801
	step [229/244], loss=82.4019
	step [230/244], loss=88.6523
	step [231/244], loss=94.3774
	step [232/244], loss=78.7795
	step [233/244], loss=82.4455
	step [234/244], loss=83.2785
	step [235/244], loss=89.2347
	step [236/244], loss=95.0010
	step [237/244], loss=97.3865
	step [238/244], loss=74.8931
	step [239/244], loss=78.0404
	step [240/244], loss=99.1948
	step [241/244], loss=97.2885
	step [242/244], loss=83.9868
	step [243/244], loss=88.0147
	step [244/244], loss=48.2697
	Evaluating
	loss=0.0099, precision=0.3014, recall=0.8779, f1=0.4487
saving model as: 2_saved_model.pth
Training epoch 64
	step [1/244], loss=79.4064
	step [2/244], loss=78.8485
	step [3/244], loss=85.4491
	step [4/244], loss=93.4742
	step [5/244], loss=79.5104
	step [6/244], loss=84.8563
	step [7/244], loss=85.6285
	step [8/244], loss=79.2983
	step [9/244], loss=79.3826
	step [10/244], loss=90.8404
	step [11/244], loss=84.0701
	step [12/244], loss=80.4685
	step [13/244], loss=86.3307
	step [14/244], loss=90.1979
	step [15/244], loss=73.9264
	step [16/244], loss=91.1608
	step [17/244], loss=84.6201
	step [18/244], loss=85.6832
	step [19/244], loss=75.6887
	step [20/244], loss=89.3298
	step [21/244], loss=83.1158
	step [22/244], loss=72.6565
	step [23/244], loss=83.0744
	step [24/244], loss=81.7428
	step [25/244], loss=67.3945
	step [26/244], loss=113.8015
	step [27/244], loss=85.6995
	step [28/244], loss=90.5996
	step [29/244], loss=88.2112
	step [30/244], loss=94.3527
	step [31/244], loss=73.9083
	step [32/244], loss=82.8567
	step [33/244], loss=82.9415
	step [34/244], loss=90.3228
	step [35/244], loss=61.4218
	step [36/244], loss=76.9437
	step [37/244], loss=88.7161
	step [38/244], loss=65.4877
	step [39/244], loss=88.6339
	step [40/244], loss=90.8881
	step [41/244], loss=80.4749
	step [42/244], loss=70.6094
	step [43/244], loss=97.8985
	step [44/244], loss=81.9274
	step [45/244], loss=81.8760
	step [46/244], loss=95.8387
	step [47/244], loss=91.1422
	step [48/244], loss=92.1847
	step [49/244], loss=92.7707
	step [50/244], loss=98.1218
	step [51/244], loss=94.5421
	step [52/244], loss=75.1373
	step [53/244], loss=81.4051
	step [54/244], loss=81.7931
	step [55/244], loss=84.2249
	step [56/244], loss=67.3024
	step [57/244], loss=74.3224
	step [58/244], loss=72.9699
	step [59/244], loss=64.3595
	step [60/244], loss=87.9435
	step [61/244], loss=69.5352
	step [62/244], loss=95.0479
	step [63/244], loss=74.0367
	step [64/244], loss=92.6160
	step [65/244], loss=109.8803
	step [66/244], loss=84.7238
	step [67/244], loss=85.2870
	step [68/244], loss=82.2241
	step [69/244], loss=91.0109
	step [70/244], loss=95.8311
	step [71/244], loss=97.6664
	step [72/244], loss=115.0092
	step [73/244], loss=90.2398
	step [74/244], loss=76.5482
	step [75/244], loss=89.3661
	step [76/244], loss=85.4681
	step [77/244], loss=73.0534
	step [78/244], loss=98.8074
	step [79/244], loss=77.5014
	step [80/244], loss=94.8738
	step [81/244], loss=99.9513
	step [82/244], loss=118.9895
	step [83/244], loss=68.3079
	step [84/244], loss=91.4666
	step [85/244], loss=75.2779
	step [86/244], loss=82.0061
	step [87/244], loss=77.2416
	step [88/244], loss=84.3201
	step [89/244], loss=84.6219
	step [90/244], loss=84.5372
	step [91/244], loss=74.6066
	step [92/244], loss=67.2139
	step [93/244], loss=78.4726
	step [94/244], loss=96.2200
	step [95/244], loss=87.1620
	step [96/244], loss=85.6900
	step [97/244], loss=65.6702
	step [98/244], loss=86.2751
	step [99/244], loss=79.8546
	step [100/244], loss=94.1667
	step [101/244], loss=80.1173
	step [102/244], loss=78.0440
	step [103/244], loss=97.8724
	step [104/244], loss=78.4102
	step [105/244], loss=95.6842
	step [106/244], loss=72.0550
	step [107/244], loss=77.4910
	step [108/244], loss=90.3714
	step [109/244], loss=81.8421
	step [110/244], loss=77.7926
	step [111/244], loss=105.8452
	step [112/244], loss=108.0843
	step [113/244], loss=96.7812
	step [114/244], loss=91.5377
	step [115/244], loss=90.1028
	step [116/244], loss=84.9837
	step [117/244], loss=78.4978
	step [118/244], loss=63.9236
	step [119/244], loss=86.0022
	step [120/244], loss=71.4280
	step [121/244], loss=94.0904
	step [122/244], loss=85.1308
	step [123/244], loss=89.8486
	step [124/244], loss=74.9889
	step [125/244], loss=76.8909
	step [126/244], loss=93.4036
	step [127/244], loss=83.1527
	step [128/244], loss=86.1511
	step [129/244], loss=87.1598
	step [130/244], loss=72.4726
	step [131/244], loss=83.3147
	step [132/244], loss=78.9668
	step [133/244], loss=78.1046
	step [134/244], loss=81.5328
	step [135/244], loss=102.8458
	step [136/244], loss=72.7736
	step [137/244], loss=63.0892
	step [138/244], loss=86.1307
	step [139/244], loss=60.5957
	step [140/244], loss=82.6258
	step [141/244], loss=83.7288
	step [142/244], loss=80.7374
	step [143/244], loss=91.5901
	step [144/244], loss=89.8063
	step [145/244], loss=87.8403
	step [146/244], loss=84.8228
	step [147/244], loss=68.8706
	step [148/244], loss=76.4829
	step [149/244], loss=90.2071
	step [150/244], loss=72.3489
	step [151/244], loss=89.9275
	step [152/244], loss=74.5730
	step [153/244], loss=86.2334
	step [154/244], loss=83.3865
	step [155/244], loss=93.3555
	step [156/244], loss=85.4082
	step [157/244], loss=91.1814
	step [158/244], loss=69.6368
	step [159/244], loss=102.8444
	step [160/244], loss=71.0150
	step [161/244], loss=80.8950
	step [162/244], loss=76.6300
	step [163/244], loss=105.7730
	step [164/244], loss=74.2346
	step [165/244], loss=87.9343
	step [166/244], loss=78.6063
	step [167/244], loss=77.8003
	step [168/244], loss=89.1634
	step [169/244], loss=80.5113
	step [170/244], loss=102.0698
	step [171/244], loss=75.4207
	step [172/244], loss=80.5495
	step [173/244], loss=73.8732
	step [174/244], loss=89.0072
	step [175/244], loss=72.3617
	step [176/244], loss=74.9347
	step [177/244], loss=80.7370
	step [178/244], loss=97.6089
	step [179/244], loss=75.2002
	step [180/244], loss=78.0119
	step [181/244], loss=77.2636
	step [182/244], loss=78.0899
	step [183/244], loss=75.3862
	step [184/244], loss=78.7122
	step [185/244], loss=76.0232
	step [186/244], loss=71.2502
	step [187/244], loss=95.8235
	step [188/244], loss=89.3673
	step [189/244], loss=91.8952
	step [190/244], loss=82.8674
	step [191/244], loss=72.4692
	step [192/244], loss=81.6116
	step [193/244], loss=88.2265
	step [194/244], loss=76.0040
	step [195/244], loss=77.7694
	step [196/244], loss=82.5456
	step [197/244], loss=96.7259
	step [198/244], loss=69.7457
	step [199/244], loss=65.2693
	step [200/244], loss=70.0731
	step [201/244], loss=95.6261
	step [202/244], loss=72.2790
	step [203/244], loss=78.5056
	step [204/244], loss=72.5111
	step [205/244], loss=89.6647
	step [206/244], loss=85.4536
	step [207/244], loss=72.4922
	step [208/244], loss=82.1694
	step [209/244], loss=88.0811
	step [210/244], loss=68.1266
	step [211/244], loss=87.5250
	step [212/244], loss=85.9052
	step [213/244], loss=108.7664
	step [214/244], loss=89.3900
	step [215/244], loss=89.3039
	step [216/244], loss=73.6964
	step [217/244], loss=98.8943
	step [218/244], loss=91.0168
	step [219/244], loss=75.9730
	step [220/244], loss=83.3114
	step [221/244], loss=79.9022
	step [222/244], loss=91.6261
	step [223/244], loss=81.7909
	step [224/244], loss=72.4041
	step [225/244], loss=72.7485
	step [226/244], loss=79.8709
	step [227/244], loss=86.7812
	step [228/244], loss=73.8730
	step [229/244], loss=85.6568
	step [230/244], loss=88.2327
	step [231/244], loss=89.6027
	step [232/244], loss=83.8356
	step [233/244], loss=81.2717
	step [234/244], loss=77.3609
	step [235/244], loss=84.9972
	step [236/244], loss=73.3579
	step [237/244], loss=89.7846
	step [238/244], loss=84.9675
	step [239/244], loss=70.2498
	step [240/244], loss=75.9185
	step [241/244], loss=93.0597
	step [242/244], loss=61.4171
	step [243/244], loss=77.0807
	step [244/244], loss=43.1506
	Evaluating
	loss=0.0096, precision=0.3119, recall=0.8882, f1=0.4617
saving model as: 2_saved_model.pth
Training epoch 65
	step [1/244], loss=90.0478
	step [2/244], loss=95.0894
	step [3/244], loss=80.0568
	step [4/244], loss=90.7381
	step [5/244], loss=71.2148
	step [6/244], loss=79.6510
	step [7/244], loss=81.1573
	step [8/244], loss=89.2437
	step [9/244], loss=93.5587
	step [10/244], loss=80.2297
	step [11/244], loss=83.4373
	step [12/244], loss=79.6461
	step [13/244], loss=74.6672
	step [14/244], loss=89.8963
	step [15/244], loss=70.7215
	step [16/244], loss=67.2997
	step [17/244], loss=72.4395
	step [18/244], loss=70.4111
	step [19/244], loss=84.2042
	step [20/244], loss=77.6157
	step [21/244], loss=100.4163
	step [22/244], loss=105.6114
	step [23/244], loss=91.1742
	step [24/244], loss=83.6124
	step [25/244], loss=100.6026
	step [26/244], loss=82.0090
	step [27/244], loss=79.3635
	step [28/244], loss=70.4713
	step [29/244], loss=105.3534
	step [30/244], loss=67.8422
	step [31/244], loss=68.7613
	step [32/244], loss=90.9105
	step [33/244], loss=74.7671
	step [34/244], loss=73.5928
	step [35/244], loss=78.3023
	step [36/244], loss=80.0551
	step [37/244], loss=76.2642
	step [38/244], loss=83.8790
	step [39/244], loss=74.3046
	step [40/244], loss=79.8326
	step [41/244], loss=84.2801
	step [42/244], loss=80.7887
	step [43/244], loss=72.1354
	step [44/244], loss=80.0715
	step [45/244], loss=101.0194
	step [46/244], loss=102.0388
	step [47/244], loss=83.2153
	step [48/244], loss=98.9448
	step [49/244], loss=77.1499
	step [50/244], loss=84.9530
	step [51/244], loss=76.3517
	step [52/244], loss=90.5885
	step [53/244], loss=73.5295
	step [54/244], loss=98.9492
	step [55/244], loss=75.1243
	step [56/244], loss=83.1297
	step [57/244], loss=85.8308
	step [58/244], loss=69.9305
	step [59/244], loss=86.3507
	step [60/244], loss=83.9844
	step [61/244], loss=83.2777
	step [62/244], loss=82.1248
	step [63/244], loss=89.1368
	step [64/244], loss=74.0384
	step [65/244], loss=82.4305
	step [66/244], loss=76.1665
	step [67/244], loss=90.4224
	step [68/244], loss=84.9274
	step [69/244], loss=84.2318
	step [70/244], loss=71.5934
	step [71/244], loss=89.0370
	step [72/244], loss=79.0550
	step [73/244], loss=94.6229
	step [74/244], loss=70.3835
	step [75/244], loss=67.4350
	step [76/244], loss=75.3391
	step [77/244], loss=93.9601
	step [78/244], loss=86.1906
	step [79/244], loss=106.3786
	step [80/244], loss=90.4899
	step [81/244], loss=71.3268
	step [82/244], loss=82.5819
	step [83/244], loss=84.4450
	step [84/244], loss=82.3347
	step [85/244], loss=87.1536
	step [86/244], loss=71.0584
	step [87/244], loss=73.8554
	step [88/244], loss=93.8551
	step [89/244], loss=85.9939
	step [90/244], loss=91.2345
	step [91/244], loss=86.9115
	step [92/244], loss=73.9188
	step [93/244], loss=79.8618
	step [94/244], loss=73.6633
	step [95/244], loss=86.8204
	step [96/244], loss=82.7508
	step [97/244], loss=68.1153
	step [98/244], loss=79.4198
	step [99/244], loss=73.4623
	step [100/244], loss=91.0140
	step [101/244], loss=80.3675
	step [102/244], loss=77.8276
	step [103/244], loss=84.5794
	step [104/244], loss=96.6343
	step [105/244], loss=83.9494
	step [106/244], loss=74.8958
	step [107/244], loss=91.6128
	step [108/244], loss=106.8750
	step [109/244], loss=71.7752
	step [110/244], loss=87.5527
	step [111/244], loss=71.1281
	step [112/244], loss=104.2907
	step [113/244], loss=89.9104
	step [114/244], loss=75.0356
	step [115/244], loss=87.5661
	step [116/244], loss=84.1158
	step [117/244], loss=87.5050
	step [118/244], loss=86.7430
	step [119/244], loss=80.4706
	step [120/244], loss=69.7751
	step [121/244], loss=90.1844
	step [122/244], loss=92.0106
	step [123/244], loss=91.1858
	step [124/244], loss=78.0169
	step [125/244], loss=90.1031
	step [126/244], loss=81.4895
	step [127/244], loss=71.2192
	step [128/244], loss=94.8921
	step [129/244], loss=94.0976
	step [130/244], loss=85.7976
	step [131/244], loss=74.3392
	step [132/244], loss=76.3972
	step [133/244], loss=94.9764
	step [134/244], loss=87.0368
	step [135/244], loss=73.4948
	step [136/244], loss=91.6857
	step [137/244], loss=85.9539
	step [138/244], loss=77.6983
	step [139/244], loss=77.1831
	step [140/244], loss=83.0864
	step [141/244], loss=87.0378
	step [142/244], loss=72.0316
	step [143/244], loss=96.6451
	step [144/244], loss=86.5557
	step [145/244], loss=87.1980
	step [146/244], loss=80.6221
	step [147/244], loss=87.9409
	step [148/244], loss=90.6967
	step [149/244], loss=72.4689
	step [150/244], loss=82.2898
	step [151/244], loss=90.5711
	step [152/244], loss=71.1086
	step [153/244], loss=83.4572
	step [154/244], loss=100.4580
	step [155/244], loss=81.8567
	step [156/244], loss=95.9525
	step [157/244], loss=62.2369
	step [158/244], loss=73.7258
	step [159/244], loss=61.8406
	step [160/244], loss=80.3597
	step [161/244], loss=86.4659
	step [162/244], loss=65.1940
	step [163/244], loss=84.7449
	step [164/244], loss=79.6731
	step [165/244], loss=72.6128
	step [166/244], loss=75.6002
	step [167/244], loss=82.5998
	step [168/244], loss=86.1062
	step [169/244], loss=80.6300
	step [170/244], loss=93.0873
	step [171/244], loss=70.7425
	step [172/244], loss=76.5424
	step [173/244], loss=111.4253
	step [174/244], loss=80.2569
	step [175/244], loss=66.4812
	step [176/244], loss=63.3322
	step [177/244], loss=81.5056
	step [178/244], loss=83.8964
	step [179/244], loss=106.1763
	step [180/244], loss=82.0654
	step [181/244], loss=86.6366
	step [182/244], loss=80.3586
	step [183/244], loss=87.7377
	step [184/244], loss=96.2141
	step [185/244], loss=70.0936
	step [186/244], loss=90.6471
	step [187/244], loss=102.0493
	step [188/244], loss=68.7490
	step [189/244], loss=92.4633
	step [190/244], loss=75.8877
	step [191/244], loss=97.4167
	step [192/244], loss=88.3913
	step [193/244], loss=85.3878
	step [194/244], loss=79.8952
	step [195/244], loss=83.5920
	step [196/244], loss=82.7994
	step [197/244], loss=90.3884
	step [198/244], loss=79.5928
	step [199/244], loss=77.5271
	step [200/244], loss=81.2251
	step [201/244], loss=83.6939
	step [202/244], loss=83.0663
	step [203/244], loss=78.6763
	step [204/244], loss=68.9987
	step [205/244], loss=77.4279
	step [206/244], loss=100.9380
	step [207/244], loss=87.6444
	step [208/244], loss=101.1602
	step [209/244], loss=67.4547
	step [210/244], loss=81.5536
	step [211/244], loss=102.9726
	step [212/244], loss=85.4280
	step [213/244], loss=92.4165
	step [214/244], loss=91.5709
	step [215/244], loss=83.3340
	step [216/244], loss=86.7735
	step [217/244], loss=83.9478
	step [218/244], loss=83.0214
	step [219/244], loss=80.8391
	step [220/244], loss=64.8005
	step [221/244], loss=82.2674
	step [222/244], loss=93.6127
	step [223/244], loss=96.9836
	step [224/244], loss=84.0900
	step [225/244], loss=81.5407
	step [226/244], loss=89.2036
	step [227/244], loss=91.6790
	step [228/244], loss=114.2466
	step [229/244], loss=68.1612
	step [230/244], loss=90.9135
	step [231/244], loss=86.7628
	step [232/244], loss=77.7897
	step [233/244], loss=77.9616
	step [234/244], loss=75.4761
	step [235/244], loss=68.3391
	step [236/244], loss=67.6205
	step [237/244], loss=60.6654
	step [238/244], loss=106.4519
	step [239/244], loss=63.3615
	step [240/244], loss=89.8028
	step [241/244], loss=82.5939
	step [242/244], loss=80.2776
	step [243/244], loss=70.3390
	step [244/244], loss=44.6005
	Evaluating
	loss=0.0106, precision=0.2778, recall=0.8751, f1=0.4217
Training epoch 66
	step [1/244], loss=80.0146
	step [2/244], loss=80.0242
	step [3/244], loss=84.0290
	step [4/244], loss=81.8298
	step [5/244], loss=97.0195
	step [6/244], loss=80.6069
	step [7/244], loss=78.1568
	step [8/244], loss=87.6142
	step [9/244], loss=92.5644
	step [10/244], loss=73.3631
	step [11/244], loss=77.1988
	step [12/244], loss=66.7537
	step [13/244], loss=85.9029
	step [14/244], loss=80.2975
	step [15/244], loss=78.1739
	step [16/244], loss=84.0560
	step [17/244], loss=81.7433
	step [18/244], loss=88.6941
	step [19/244], loss=91.6848
	step [20/244], loss=75.4432
	step [21/244], loss=92.5686
	step [22/244], loss=78.0326
	step [23/244], loss=81.0444
	step [24/244], loss=81.8149
	step [25/244], loss=100.5330
	step [26/244], loss=103.4879
	step [27/244], loss=74.8030
	step [28/244], loss=87.8547
	step [29/244], loss=79.6728
	step [30/244], loss=90.1017
	step [31/244], loss=73.4649
	step [32/244], loss=83.3231
	step [33/244], loss=84.4370
	step [34/244], loss=77.7545
	step [35/244], loss=72.1205
	step [36/244], loss=89.0117
	step [37/244], loss=93.8677
	step [38/244], loss=79.6268
	step [39/244], loss=80.4169
	step [40/244], loss=77.3674
	step [41/244], loss=84.8245
	step [42/244], loss=92.1384
	step [43/244], loss=75.1841
	step [44/244], loss=74.8518
	step [45/244], loss=78.6517
	step [46/244], loss=90.8303
	step [47/244], loss=90.5571
	step [48/244], loss=78.3629
	step [49/244], loss=64.1909
	step [50/244], loss=92.1548
	step [51/244], loss=82.0633
	step [52/244], loss=85.9267
	step [53/244], loss=64.2539
	step [54/244], loss=94.2738
	step [55/244], loss=86.5546
	step [56/244], loss=87.7264
	step [57/244], loss=78.8326
	step [58/244], loss=101.2917
	step [59/244], loss=86.8790
	step [60/244], loss=88.0033
	step [61/244], loss=90.5087
	step [62/244], loss=71.6935
	step [63/244], loss=80.3014
	step [64/244], loss=77.8403
	step [65/244], loss=84.4694
	step [66/244], loss=74.6341
	step [67/244], loss=103.0531
	step [68/244], loss=78.2346
	step [69/244], loss=75.6152
	step [70/244], loss=71.5792
	step [71/244], loss=72.1440
	step [72/244], loss=62.4866
	step [73/244], loss=82.5412
	step [74/244], loss=86.4465
	step [75/244], loss=80.7812
	step [76/244], loss=88.3921
	step [77/244], loss=82.7605
	step [78/244], loss=86.4055
	step [79/244], loss=90.1449
	step [80/244], loss=76.6728
	step [81/244], loss=87.8446
	step [82/244], loss=78.7482
	step [83/244], loss=102.0335
	step [84/244], loss=80.2996
	step [85/244], loss=82.9538
	step [86/244], loss=73.0139
	step [87/244], loss=97.8751
	step [88/244], loss=74.2085
	step [89/244], loss=78.4240
	step [90/244], loss=80.4328
	step [91/244], loss=76.5486
	step [92/244], loss=67.9040
	step [93/244], loss=89.8783
	step [94/244], loss=81.1824
	step [95/244], loss=73.2664
	step [96/244], loss=83.9896
	step [97/244], loss=80.7062
	step [98/244], loss=80.1637
	step [99/244], loss=59.6249
	step [100/244], loss=78.9848
	step [101/244], loss=88.6107
	step [102/244], loss=68.8636
	step [103/244], loss=75.6745
	step [104/244], loss=85.0224
	step [105/244], loss=75.1224
	step [106/244], loss=87.0510
	step [107/244], loss=107.5836
	step [108/244], loss=80.1084
	step [109/244], loss=80.2466
	step [110/244], loss=82.5372
	step [111/244], loss=76.5866
	step [112/244], loss=96.2872
	step [113/244], loss=88.2497
	step [114/244], loss=75.2693
	step [115/244], loss=76.5857
	step [116/244], loss=97.6081
	step [117/244], loss=72.3071
	step [118/244], loss=75.3823
	step [119/244], loss=95.4826
	step [120/244], loss=68.3282
	step [121/244], loss=95.5776
	step [122/244], loss=77.8692
	step [123/244], loss=77.8228
	step [124/244], loss=93.9616
	step [125/244], loss=82.3548
	step [126/244], loss=73.2869
	step [127/244], loss=72.8202
	step [128/244], loss=88.2417
	step [129/244], loss=76.7717
	step [130/244], loss=91.3520
	step [131/244], loss=73.4904
	step [132/244], loss=78.2378
	step [133/244], loss=80.1285
	step [134/244], loss=68.3302
	step [135/244], loss=97.5962
	step [136/244], loss=84.1267
	step [137/244], loss=91.3122
	step [138/244], loss=82.0642
	step [139/244], loss=90.5687
	step [140/244], loss=85.3563
	step [141/244], loss=94.2708
	step [142/244], loss=76.0595
	step [143/244], loss=78.7824
	step [144/244], loss=73.3554
	step [145/244], loss=92.3211
	step [146/244], loss=63.0334
	step [147/244], loss=112.2198
	step [148/244], loss=92.0273
	step [149/244], loss=78.1172
	step [150/244], loss=90.9208
	step [151/244], loss=77.6296
	step [152/244], loss=86.7521
	step [153/244], loss=86.9473
	step [154/244], loss=76.6657
	step [155/244], loss=91.6104
	step [156/244], loss=83.8188
	step [157/244], loss=97.0516
	step [158/244], loss=78.8942
	step [159/244], loss=71.0058
	step [160/244], loss=86.4601
	step [161/244], loss=91.5749
	step [162/244], loss=76.4202
	step [163/244], loss=78.6190
	step [164/244], loss=99.6032
	step [165/244], loss=74.9283
	step [166/244], loss=79.5423
	step [167/244], loss=95.9790
	step [168/244], loss=73.6869
	step [169/244], loss=74.7003
	step [170/244], loss=76.3122
	step [171/244], loss=78.0368
	step [172/244], loss=76.9253
	step [173/244], loss=71.8179
	step [174/244], loss=68.6268
	step [175/244], loss=95.2013
	step [176/244], loss=88.6567
	step [177/244], loss=91.8785
	step [178/244], loss=73.8826
	step [179/244], loss=78.4971
	step [180/244], loss=103.8420
	step [181/244], loss=89.2194
	step [182/244], loss=57.4865
	step [183/244], loss=75.7515
	step [184/244], loss=93.8952
	step [185/244], loss=91.0398
	step [186/244], loss=90.1883
	step [187/244], loss=95.3552
	step [188/244], loss=73.4019
	step [189/244], loss=75.9746
	step [190/244], loss=93.3036
	step [191/244], loss=73.9953
	step [192/244], loss=82.5336
	step [193/244], loss=82.4585
	step [194/244], loss=90.6434
	step [195/244], loss=76.3327
	step [196/244], loss=96.2541
	step [197/244], loss=86.2496
	step [198/244], loss=81.5864
	step [199/244], loss=90.8622
	step [200/244], loss=80.5977
	step [201/244], loss=81.7308
	step [202/244], loss=77.0064
	step [203/244], loss=88.3377
	step [204/244], loss=80.1994
	step [205/244], loss=85.3760
	step [206/244], loss=70.5068
	step [207/244], loss=87.8170
	step [208/244], loss=84.3006
	step [209/244], loss=73.5937
	step [210/244], loss=94.4769
	step [211/244], loss=88.4673
	step [212/244], loss=90.3053
	step [213/244], loss=76.4944
	step [214/244], loss=77.7765
	step [215/244], loss=86.6759
	step [216/244], loss=83.5515
	step [217/244], loss=94.4538
	step [218/244], loss=71.4276
	step [219/244], loss=82.8028
	step [220/244], loss=96.4236
	step [221/244], loss=85.1473
	step [222/244], loss=64.2725
	step [223/244], loss=81.8565
	step [224/244], loss=79.1052
	step [225/244], loss=62.6916
	step [226/244], loss=85.2445
	step [227/244], loss=84.8997
	step [228/244], loss=87.7274
	step [229/244], loss=67.2522
	step [230/244], loss=65.4853
	step [231/244], loss=75.4760
	step [232/244], loss=82.2428
	step [233/244], loss=95.2632
	step [234/244], loss=79.0869
	step [235/244], loss=83.6630
	step [236/244], loss=85.0165
	step [237/244], loss=91.2295
	step [238/244], loss=88.2475
	step [239/244], loss=72.6082
	step [240/244], loss=78.3607
	step [241/244], loss=82.3848
	step [242/244], loss=95.7688
	step [243/244], loss=73.1746
	step [244/244], loss=52.4553
	Evaluating
	loss=0.0096, precision=0.3043, recall=0.8779, f1=0.4519
Training epoch 67
	step [1/244], loss=82.8324
	step [2/244], loss=76.1908
	step [3/244], loss=74.1467
	step [4/244], loss=70.6160
	step [5/244], loss=81.1731
	step [6/244], loss=71.2254
	step [7/244], loss=95.3756
	step [8/244], loss=71.2390
	step [9/244], loss=76.4615
	step [10/244], loss=112.0509
	step [11/244], loss=90.3105
	step [12/244], loss=82.2148
	step [13/244], loss=78.5857
	step [14/244], loss=109.8339
	step [15/244], loss=79.2829
	step [16/244], loss=106.2218
	step [17/244], loss=79.1072
	step [18/244], loss=81.3539
	step [19/244], loss=68.3169
	step [20/244], loss=76.9778
	step [21/244], loss=96.2866
	step [22/244], loss=95.2225
	step [23/244], loss=103.6523
	step [24/244], loss=73.4642
	step [25/244], loss=80.4741
	step [26/244], loss=90.9197
	step [27/244], loss=85.2620
	step [28/244], loss=63.9646
	step [29/244], loss=77.2454
	step [30/244], loss=69.4791
	step [31/244], loss=86.8458
	step [32/244], loss=84.1062
	step [33/244], loss=72.1196
	step [34/244], loss=68.7386
	step [35/244], loss=71.8167
	step [36/244], loss=87.5624
	step [37/244], loss=87.7054
	step [38/244], loss=110.0397
	step [39/244], loss=89.8092
	step [40/244], loss=70.2864
	step [41/244], loss=96.4911
	step [42/244], loss=82.0154
	step [43/244], loss=74.6055
	step [44/244], loss=96.2050
	step [45/244], loss=82.6117
	step [46/244], loss=72.0220
	step [47/244], loss=68.6554
	step [48/244], loss=89.9633
	step [49/244], loss=85.5782
	step [50/244], loss=103.3254
	step [51/244], loss=64.7791
	step [52/244], loss=76.3700
	step [53/244], loss=86.8618
	step [54/244], loss=89.9830
	step [55/244], loss=71.1764
	step [56/244], loss=76.6424
	step [57/244], loss=93.9532
	step [58/244], loss=79.8934
	step [59/244], loss=82.5264
	step [60/244], loss=89.4104
	step [61/244], loss=81.3501
	step [62/244], loss=83.6094
	step [63/244], loss=90.4117
	step [64/244], loss=71.0602
	step [65/244], loss=86.9213
	step [66/244], loss=87.9076
	step [67/244], loss=71.2025
	step [68/244], loss=93.2547
	step [69/244], loss=85.4709
	step [70/244], loss=83.4901
	step [71/244], loss=90.3652
	step [72/244], loss=88.7302
	step [73/244], loss=75.4883
	step [74/244], loss=93.7878
	step [75/244], loss=77.5966
	step [76/244], loss=76.4904
	step [77/244], loss=87.6755
	step [78/244], loss=83.8328
	step [79/244], loss=76.3406
	step [80/244], loss=74.1703
	step [81/244], loss=81.1635
	step [82/244], loss=71.5539
	step [83/244], loss=69.4079
	step [84/244], loss=87.4472
	step [85/244], loss=69.8454
	step [86/244], loss=80.9246
	step [87/244], loss=83.1736
	step [88/244], loss=76.4821
	step [89/244], loss=69.0376
	step [90/244], loss=56.2955
	step [91/244], loss=74.9981
	step [92/244], loss=93.4077
	step [93/244], loss=82.5617
	step [94/244], loss=96.4339
	step [95/244], loss=75.3477
	step [96/244], loss=91.2816
	step [97/244], loss=77.3787
	step [98/244], loss=70.4519
	step [99/244], loss=85.9374
	step [100/244], loss=73.8235
	step [101/244], loss=68.7039
	step [102/244], loss=75.5877
	step [103/244], loss=76.7075
	step [104/244], loss=72.2732
	step [105/244], loss=92.3664
	step [106/244], loss=84.7209
	step [107/244], loss=85.8035
	step [108/244], loss=96.1821
	step [109/244], loss=87.8185
	step [110/244], loss=86.9517
	step [111/244], loss=70.9581
	step [112/244], loss=74.9359
	step [113/244], loss=82.7931
	step [114/244], loss=71.9325
	step [115/244], loss=93.2804
	step [116/244], loss=87.2517
	step [117/244], loss=83.4283
	step [118/244], loss=86.4038
	step [119/244], loss=86.5372
	step [120/244], loss=84.8542
	step [121/244], loss=89.9045
	step [122/244], loss=100.6073
	step [123/244], loss=82.4609
	step [124/244], loss=73.4127
	step [125/244], loss=90.1131
	step [126/244], loss=99.8960
	step [127/244], loss=73.9050
	step [128/244], loss=72.8191
	step [129/244], loss=86.2018
	step [130/244], loss=79.3961
	step [131/244], loss=95.7340
	step [132/244], loss=80.4482
	step [133/244], loss=71.7255
	step [134/244], loss=88.9105
	step [135/244], loss=64.8151
	step [136/244], loss=88.3514
	step [137/244], loss=58.3449
	step [138/244], loss=112.2221
	step [139/244], loss=82.9582
	step [140/244], loss=85.4124
	step [141/244], loss=86.5891
	step [142/244], loss=75.5852
	step [143/244], loss=92.3776
	step [144/244], loss=81.8489
	step [145/244], loss=69.8740
	step [146/244], loss=88.1193
	step [147/244], loss=95.1331
	step [148/244], loss=59.7993
	step [149/244], loss=83.0153
	step [150/244], loss=90.3267
	step [151/244], loss=91.6116
	step [152/244], loss=74.3235
	step [153/244], loss=88.4036
	step [154/244], loss=82.8237
	step [155/244], loss=86.3998
	step [156/244], loss=73.1889
	step [157/244], loss=86.5558
	step [158/244], loss=101.9800
	step [159/244], loss=80.1529
	step [160/244], loss=80.0373
	step [161/244], loss=82.7010
	step [162/244], loss=66.8337
	step [163/244], loss=69.2871
	step [164/244], loss=81.4654
	step [165/244], loss=86.5400
	step [166/244], loss=79.4527
	step [167/244], loss=83.3248
	step [168/244], loss=65.7515
	step [169/244], loss=96.5091
	step [170/244], loss=94.0650
	step [171/244], loss=96.7112
	step [172/244], loss=76.8009
	step [173/244], loss=103.6561
	step [174/244], loss=70.0167
	step [175/244], loss=83.4809
	step [176/244], loss=76.0039
	step [177/244], loss=95.2118
	step [178/244], loss=87.8965
	step [179/244], loss=77.0211
	step [180/244], loss=75.4229
	step [181/244], loss=86.7645
	step [182/244], loss=77.7819
	step [183/244], loss=72.3062
	step [184/244], loss=90.1010
	step [185/244], loss=77.6706
	step [186/244], loss=79.8543
	step [187/244], loss=87.3899
	step [188/244], loss=73.8157
	step [189/244], loss=96.9246
	step [190/244], loss=82.3323
	step [191/244], loss=76.5696
	step [192/244], loss=62.8960
	step [193/244], loss=80.0298
	step [194/244], loss=95.1640
	step [195/244], loss=79.6336
	step [196/244], loss=89.8869
	step [197/244], loss=78.0051
	step [198/244], loss=79.4824
	step [199/244], loss=93.1345
	step [200/244], loss=82.8738
	step [201/244], loss=88.0087
	step [202/244], loss=63.7288
	step [203/244], loss=93.8012
	step [204/244], loss=68.5040
	step [205/244], loss=77.6159
	step [206/244], loss=88.7639
	step [207/244], loss=101.9198
	step [208/244], loss=64.3818
	step [209/244], loss=70.9883
	step [210/244], loss=93.2259
	step [211/244], loss=83.5317
	step [212/244], loss=79.9347
	step [213/244], loss=92.5221
	step [214/244], loss=60.8304
	step [215/244], loss=76.8721
	step [216/244], loss=69.8532
	step [217/244], loss=92.2118
	step [218/244], loss=76.0069
	step [219/244], loss=83.8065
	step [220/244], loss=86.3908
	step [221/244], loss=83.7706
	step [222/244], loss=74.7625
	step [223/244], loss=86.1629
	step [224/244], loss=75.6611
	step [225/244], loss=66.2189
	step [226/244], loss=72.8272
	step [227/244], loss=71.6559
	step [228/244], loss=98.0855
	step [229/244], loss=71.5729
	step [230/244], loss=96.0444
	step [231/244], loss=90.5541
	step [232/244], loss=73.6764
	step [233/244], loss=89.3353
	step [234/244], loss=66.0441
	step [235/244], loss=77.4800
	step [236/244], loss=79.1467
	step [237/244], loss=65.5980
	step [238/244], loss=93.4624
	step [239/244], loss=80.7882
	step [240/244], loss=85.3936
	step [241/244], loss=86.9079
	step [242/244], loss=94.2985
	step [243/244], loss=96.4945
	step [244/244], loss=57.0273
	Evaluating
	loss=0.0110, precision=0.2674, recall=0.8847, f1=0.4107
Training epoch 68
	step [1/244], loss=96.3789
	step [2/244], loss=84.4841
	step [3/244], loss=92.4612
	step [4/244], loss=76.2807
	step [5/244], loss=75.2241
	step [6/244], loss=80.1480
	step [7/244], loss=82.6027
	step [8/244], loss=79.4400
	step [9/244], loss=77.4828
	step [10/244], loss=96.8465
	step [11/244], loss=90.4622
	step [12/244], loss=82.5952
	step [13/244], loss=62.3569
	step [14/244], loss=96.4921
	step [15/244], loss=80.7762
	step [16/244], loss=74.4676
	step [17/244], loss=88.5875
	step [18/244], loss=81.5358
	step [19/244], loss=93.8677
	step [20/244], loss=83.2860
	step [21/244], loss=83.4302
	step [22/244], loss=64.5280
	step [23/244], loss=70.1119
	step [24/244], loss=94.7638
	step [25/244], loss=78.5025
	step [26/244], loss=83.0199
	step [27/244], loss=80.0318
	step [28/244], loss=68.9860
	step [29/244], loss=85.8664
	step [30/244], loss=86.0473
	step [31/244], loss=84.3581
	step [32/244], loss=85.8911
	step [33/244], loss=75.2764
	step [34/244], loss=69.0995
	step [35/244], loss=90.2222
	step [36/244], loss=78.3631
	step [37/244], loss=77.3887
	step [38/244], loss=85.8376
	step [39/244], loss=81.9858
	step [40/244], loss=66.3239
	step [41/244], loss=78.8722
	step [42/244], loss=83.4824
	step [43/244], loss=70.5402
	step [44/244], loss=74.8822
	step [45/244], loss=87.4800
	step [46/244], loss=78.3986
	step [47/244], loss=86.9780
	step [48/244], loss=82.2469
	step [49/244], loss=96.6983
	step [50/244], loss=83.9669
	step [51/244], loss=92.8160
	step [52/244], loss=88.8062
	step [53/244], loss=72.5313
	step [54/244], loss=84.0467
	step [55/244], loss=73.1611
	step [56/244], loss=83.5730
	step [57/244], loss=75.3509
	step [58/244], loss=90.2300
	step [59/244], loss=77.3859
	step [60/244], loss=89.0661
	step [61/244], loss=71.9733
	step [62/244], loss=87.0765
	step [63/244], loss=69.8688
	step [64/244], loss=75.8213
	step [65/244], loss=56.6492
	step [66/244], loss=84.3645
	step [67/244], loss=78.2404
	step [68/244], loss=80.4285
	step [69/244], loss=75.2454
	step [70/244], loss=74.0984
	step [71/244], loss=65.3587
	step [72/244], loss=99.3791
	step [73/244], loss=85.3210
	step [74/244], loss=81.4958
	step [75/244], loss=84.0286
	step [76/244], loss=87.0861
	step [77/244], loss=73.2335
	step [78/244], loss=84.3119
	step [79/244], loss=98.6668
	step [80/244], loss=77.9622
	step [81/244], loss=74.1339
	step [82/244], loss=82.2449
	step [83/244], loss=90.9957
	step [84/244], loss=99.2299
	step [85/244], loss=89.2113
	step [86/244], loss=77.6932
	step [87/244], loss=88.6319
	step [88/244], loss=82.2999
	step [89/244], loss=99.8064
	step [90/244], loss=60.9252
	step [91/244], loss=90.2342
	step [92/244], loss=84.7758
	step [93/244], loss=92.8750
	step [94/244], loss=72.7298
	step [95/244], loss=76.9378
	step [96/244], loss=96.1668
	step [97/244], loss=82.5476
	step [98/244], loss=72.1012
	step [99/244], loss=74.9666
	step [100/244], loss=77.8114
	step [101/244], loss=89.6888
	step [102/244], loss=101.8161
	step [103/244], loss=82.5833
	step [104/244], loss=69.3028
	step [105/244], loss=75.1248
	step [106/244], loss=92.0707
	step [107/244], loss=77.6803
	step [108/244], loss=80.3872
	step [109/244], loss=78.1639
	step [110/244], loss=101.5244
	step [111/244], loss=95.2466
	step [112/244], loss=88.8801
	step [113/244], loss=78.2017
	step [114/244], loss=75.7760
	step [115/244], loss=80.7702
	step [116/244], loss=92.6850
	step [117/244], loss=104.0918
	step [118/244], loss=76.1951
	step [119/244], loss=65.0724
	step [120/244], loss=69.8634
	step [121/244], loss=81.4801
	step [122/244], loss=74.8498
	step [123/244], loss=99.4995
	step [124/244], loss=84.4778
	step [125/244], loss=72.1863
	step [126/244], loss=81.3680
	step [127/244], loss=94.6409
	step [128/244], loss=88.3855
	step [129/244], loss=82.8685
	step [130/244], loss=96.4819
	step [131/244], loss=66.5116
	step [132/244], loss=74.0147
	step [133/244], loss=85.1840
	step [134/244], loss=99.3635
	step [135/244], loss=89.6791
	step [136/244], loss=67.5734
	step [137/244], loss=71.7222
	step [138/244], loss=72.8166
	step [139/244], loss=89.9884
	step [140/244], loss=72.2724
	step [141/244], loss=70.1681
	step [142/244], loss=74.2808
	step [143/244], loss=66.9509
	step [144/244], loss=87.8039
	step [145/244], loss=87.6339
	step [146/244], loss=100.5178
	step [147/244], loss=73.4176
	step [148/244], loss=80.4271
	step [149/244], loss=83.3533
	step [150/244], loss=83.2910
	step [151/244], loss=88.9955
	step [152/244], loss=87.5280
	step [153/244], loss=72.2778
	step [154/244], loss=77.4676
	step [155/244], loss=80.3770
	step [156/244], loss=72.9776
	step [157/244], loss=87.0014
	step [158/244], loss=84.7278
	step [159/244], loss=92.2085
	step [160/244], loss=69.6975
	step [161/244], loss=91.6445
	step [162/244], loss=88.1748
	step [163/244], loss=74.0914
	step [164/244], loss=83.7499
	step [165/244], loss=71.5262
	step [166/244], loss=78.9469
	step [167/244], loss=69.2519
	step [168/244], loss=89.4485
	step [169/244], loss=77.1087
	step [170/244], loss=72.1880
	step [171/244], loss=74.5565
	step [172/244], loss=98.5508
	step [173/244], loss=70.9614
	step [174/244], loss=88.3537
	step [175/244], loss=95.6391
	step [176/244], loss=86.2546
	step [177/244], loss=80.2804
	step [178/244], loss=70.3929
	step [179/244], loss=106.8098
	step [180/244], loss=67.4067
	step [181/244], loss=63.0485
	step [182/244], loss=88.3403
	step [183/244], loss=100.6642
	step [184/244], loss=88.9741
	step [185/244], loss=74.4056
	step [186/244], loss=79.0719
	step [187/244], loss=74.5020
	step [188/244], loss=92.1560
	step [189/244], loss=75.6716
	step [190/244], loss=81.1585
	step [191/244], loss=83.2542
	step [192/244], loss=81.8864
	step [193/244], loss=74.0614
	step [194/244], loss=77.6961
	step [195/244], loss=94.1484
	step [196/244], loss=93.8114
	step [197/244], loss=84.8081
	step [198/244], loss=83.9016
	step [199/244], loss=70.2662
	step [200/244], loss=94.5899
	step [201/244], loss=88.8857
	step [202/244], loss=74.6583
	step [203/244], loss=88.9973
	step [204/244], loss=66.6397
	step [205/244], loss=84.8331
	step [206/244], loss=88.6496
	step [207/244], loss=84.2655
	step [208/244], loss=90.1581
	step [209/244], loss=80.8894
	step [210/244], loss=80.5347
	step [211/244], loss=78.8726
	step [212/244], loss=97.8167
	step [213/244], loss=97.5029
	step [214/244], loss=77.3962
	step [215/244], loss=84.9924
	step [216/244], loss=79.3115
	step [217/244], loss=95.1210
	step [218/244], loss=96.1472
	step [219/244], loss=88.1515
	step [220/244], loss=75.8093
	step [221/244], loss=96.0474
	step [222/244], loss=84.5859
	step [223/244], loss=81.4751
	step [224/244], loss=82.7796
	step [225/244], loss=78.7458
	step [226/244], loss=84.9319
	step [227/244], loss=76.1101
	step [228/244], loss=92.4092
	step [229/244], loss=81.2418
	step [230/244], loss=78.0913
	step [231/244], loss=75.0590
	step [232/244], loss=79.5017
	step [233/244], loss=79.3656
	step [234/244], loss=81.6658
	step [235/244], loss=82.6378
	step [236/244], loss=79.4042
	step [237/244], loss=95.7986
	step [238/244], loss=78.6509
	step [239/244], loss=67.0637
	step [240/244], loss=71.4973
	step [241/244], loss=78.5948
	step [242/244], loss=71.0691
	step [243/244], loss=70.9798
	step [244/244], loss=35.9202
	Evaluating
	loss=0.0111, precision=0.2649, recall=0.8714, f1=0.4062
Training epoch 69
	step [1/244], loss=67.6009
	step [2/244], loss=72.4272
	step [3/244], loss=111.8069
	step [4/244], loss=78.4129
	step [5/244], loss=74.0208
	step [6/244], loss=70.7332
	step [7/244], loss=81.6945
	step [8/244], loss=89.7212
	step [9/244], loss=81.7942
	step [10/244], loss=88.4085
	step [11/244], loss=85.6828
	step [12/244], loss=90.3374
	step [13/244], loss=96.1826
	step [14/244], loss=75.0750
	step [15/244], loss=89.5370
	step [16/244], loss=88.2987
	step [17/244], loss=76.1409
	step [18/244], loss=82.9489
	step [19/244], loss=79.8046
	step [20/244], loss=89.3974
	step [21/244], loss=92.7134
	step [22/244], loss=94.5698
	step [23/244], loss=100.8207
	step [24/244], loss=94.7574
	step [25/244], loss=83.7622
	step [26/244], loss=70.7478
	step [27/244], loss=82.3366
	step [28/244], loss=74.6913
	step [29/244], loss=89.4747
	step [30/244], loss=94.8840
	step [31/244], loss=79.7797
	step [32/244], loss=75.2147
	step [33/244], loss=67.4717
	step [34/244], loss=80.9966
	step [35/244], loss=83.0276
	step [36/244], loss=83.1965
	step [37/244], loss=74.5358
	step [38/244], loss=88.1011
	step [39/244], loss=85.0033
	step [40/244], loss=87.0643
	step [41/244], loss=81.7974
	step [42/244], loss=78.7740
	step [43/244], loss=67.9074
	step [44/244], loss=65.0098
	step [45/244], loss=74.1546
	step [46/244], loss=81.2567
	step [47/244], loss=71.2641
	step [48/244], loss=86.5574
	step [49/244], loss=93.3379
	step [50/244], loss=90.4784
	step [51/244], loss=88.8488
	step [52/244], loss=80.7087
	step [53/244], loss=88.1476
	step [54/244], loss=70.7551
	step [55/244], loss=74.8212
	step [56/244], loss=74.4210
	step [57/244], loss=77.3914
	step [58/244], loss=100.1737
	step [59/244], loss=86.8442
	step [60/244], loss=81.5611
	step [61/244], loss=72.0582
	step [62/244], loss=69.0031
	step [63/244], loss=98.6773
	step [64/244], loss=76.5104
	step [65/244], loss=62.5213
	step [66/244], loss=104.2574
	step [67/244], loss=85.5190
	step [68/244], loss=85.6670
	step [69/244], loss=72.3654
	step [70/244], loss=86.1704
	step [71/244], loss=80.8869
	step [72/244], loss=72.7856
	step [73/244], loss=86.1116
	step [74/244], loss=87.9320
	step [75/244], loss=84.2734
	step [76/244], loss=81.9387
	step [77/244], loss=95.1336
	step [78/244], loss=88.4308
	step [79/244], loss=89.6872
	step [80/244], loss=85.0776
	step [81/244], loss=81.8385
	step [82/244], loss=96.7840
	step [83/244], loss=73.4742
	step [84/244], loss=89.1412
	step [85/244], loss=99.3365
	step [86/244], loss=79.6335
	step [87/244], loss=74.6686
	step [88/244], loss=96.6220
	step [89/244], loss=77.2325
	step [90/244], loss=76.3004
	step [91/244], loss=109.0575
	step [92/244], loss=85.5796
	step [93/244], loss=100.3276
	step [94/244], loss=80.6092
	step [95/244], loss=84.6285
	step [96/244], loss=81.1434
	step [97/244], loss=70.3728
	step [98/244], loss=78.4530
	step [99/244], loss=68.2743
	step [100/244], loss=88.4962
	step [101/244], loss=76.7426
	step [102/244], loss=74.8511
	step [103/244], loss=76.0822
	step [104/244], loss=83.5949
	step [105/244], loss=75.4837
	step [106/244], loss=80.9830
	step [107/244], loss=79.5717
	step [108/244], loss=86.0556
	step [109/244], loss=72.1691
	step [110/244], loss=80.0498
	step [111/244], loss=99.0297
	step [112/244], loss=72.3197
	step [113/244], loss=80.4154
	step [114/244], loss=70.6887
	step [115/244], loss=91.9025
	step [116/244], loss=73.3180
	step [117/244], loss=79.2597
	step [118/244], loss=70.7245
	step [119/244], loss=75.0238
	step [120/244], loss=94.9612
	step [121/244], loss=77.2171
	step [122/244], loss=77.1558
	step [123/244], loss=70.6700
	step [124/244], loss=82.7530
	step [125/244], loss=79.1443
	step [126/244], loss=93.9671
	step [127/244], loss=76.1180
	step [128/244], loss=74.3893
	step [129/244], loss=67.3378
	step [130/244], loss=89.8881
	step [131/244], loss=82.8964
	step [132/244], loss=78.0973
	step [133/244], loss=84.0941
	step [134/244], loss=85.6124
	step [135/244], loss=85.8974
	step [136/244], loss=71.5397
	step [137/244], loss=83.9377
	step [138/244], loss=87.8332
	step [139/244], loss=76.9383
	step [140/244], loss=72.0580
	step [141/244], loss=77.0139
	step [142/244], loss=101.5313
	step [143/244], loss=78.0943
	step [144/244], loss=70.4326
	step [145/244], loss=100.6080
	step [146/244], loss=72.5741
	step [147/244], loss=71.2692
	step [148/244], loss=71.6093
	step [149/244], loss=76.7014
	step [150/244], loss=90.2151
	step [151/244], loss=108.7937
	step [152/244], loss=72.4137
	step [153/244], loss=87.1168
	step [154/244], loss=90.2504
	step [155/244], loss=59.1999
	step [156/244], loss=90.9731
	step [157/244], loss=84.6023
	step [158/244], loss=93.4109
	step [159/244], loss=102.3822
	step [160/244], loss=92.5312
	step [161/244], loss=86.6698
	step [162/244], loss=79.2609
	step [163/244], loss=72.2393
	step [164/244], loss=92.7746
	step [165/244], loss=86.2843
	step [166/244], loss=96.5170
	step [167/244], loss=79.8685
	step [168/244], loss=87.8568
	step [169/244], loss=86.1061
	step [170/244], loss=88.9129
	step [171/244], loss=83.3351
	step [172/244], loss=81.6664
	step [173/244], loss=70.0371
	step [174/244], loss=82.2851
	step [175/244], loss=81.8073
	step [176/244], loss=84.4740
	step [177/244], loss=76.2743
	step [178/244], loss=69.7305
	step [179/244], loss=79.9010
	step [180/244], loss=74.1013
	step [181/244], loss=79.1407
	step [182/244], loss=85.5104
	step [183/244], loss=90.8695
	step [184/244], loss=72.7228
	step [185/244], loss=67.4713
	step [186/244], loss=79.4621
	step [187/244], loss=95.7496
	step [188/244], loss=90.5465
	step [189/244], loss=62.6714
	step [190/244], loss=75.3415
	step [191/244], loss=74.0498
	step [192/244], loss=75.2922
	step [193/244], loss=96.8942
	step [194/244], loss=82.8161
	step [195/244], loss=69.9339
	step [196/244], loss=77.5498
	step [197/244], loss=78.8970
	step [198/244], loss=82.0290
	step [199/244], loss=80.9606
	step [200/244], loss=88.5280
	step [201/244], loss=77.3609
	step [202/244], loss=95.0088
	step [203/244], loss=82.6382
	step [204/244], loss=80.8975
	step [205/244], loss=75.3404
	step [206/244], loss=70.5687
	step [207/244], loss=72.4998
	step [208/244], loss=91.2009
	step [209/244], loss=74.8812
	step [210/244], loss=108.0741
	step [211/244], loss=71.1310
	step [212/244], loss=80.3271
	step [213/244], loss=88.2305
	step [214/244], loss=74.7336
	step [215/244], loss=78.8795
	step [216/244], loss=77.7875
	step [217/244], loss=69.6798
	step [218/244], loss=78.8878
	step [219/244], loss=71.7708
	step [220/244], loss=78.7078
	step [221/244], loss=97.6462
	step [222/244], loss=80.4834
	step [223/244], loss=69.0786
	step [224/244], loss=85.3013
	step [225/244], loss=75.8820
	step [226/244], loss=84.0666
	step [227/244], loss=81.1624
	step [228/244], loss=78.5491
	step [229/244], loss=71.1316
	step [230/244], loss=85.1773
	step [231/244], loss=71.5568
	step [232/244], loss=86.6590
	step [233/244], loss=85.9174
	step [234/244], loss=70.4859
	step [235/244], loss=91.0863
	step [236/244], loss=82.9260
	step [237/244], loss=72.6378
	step [238/244], loss=87.4844
	step [239/244], loss=70.6601
	step [240/244], loss=85.1673
	step [241/244], loss=78.9715
	step [242/244], loss=71.9858
	step [243/244], loss=82.3223
	step [244/244], loss=42.0473
	Evaluating
	loss=0.0100, precision=0.2924, recall=0.8767, f1=0.4386
Training epoch 70
	step [1/244], loss=70.9413
	step [2/244], loss=89.5974
	step [3/244], loss=71.0907
	step [4/244], loss=70.7117
	step [5/244], loss=75.9467
	step [6/244], loss=84.7975
	step [7/244], loss=82.0014
	step [8/244], loss=80.2924
	step [9/244], loss=75.4311
	step [10/244], loss=80.3614
	step [11/244], loss=87.6455
	step [12/244], loss=62.8779
	step [13/244], loss=77.2518
	step [14/244], loss=71.2972
	step [15/244], loss=77.6111
	step [16/244], loss=88.6523
	step [17/244], loss=78.4611
	step [18/244], loss=70.7873
	step [19/244], loss=78.0165
	step [20/244], loss=84.8925
	step [21/244], loss=71.7687
	step [22/244], loss=82.2021
	step [23/244], loss=71.9147
	step [24/244], loss=81.9659
	step [25/244], loss=79.0367
	step [26/244], loss=86.3590
	step [27/244], loss=91.8943
	step [28/244], loss=82.3408
	step [29/244], loss=95.2448
	step [30/244], loss=76.3248
	step [31/244], loss=87.7097
	step [32/244], loss=76.3276
	step [33/244], loss=77.8014
	step [34/244], loss=86.4998
	step [35/244], loss=81.5524
	step [36/244], loss=82.0621
	step [37/244], loss=81.7420
	step [38/244], loss=85.6634
	step [39/244], loss=78.6245
	step [40/244], loss=88.7692
	step [41/244], loss=84.2766
	step [42/244], loss=83.2839
	step [43/244], loss=79.6849
	step [44/244], loss=63.6889
	step [45/244], loss=96.2295
	step [46/244], loss=93.0325
	step [47/244], loss=88.6389
	step [48/244], loss=69.2713
	step [49/244], loss=88.7394
	step [50/244], loss=125.4175
	step [51/244], loss=66.2763
	step [52/244], loss=82.6909
	step [53/244], loss=80.3218
	step [54/244], loss=97.5783
	step [55/244], loss=79.1390
	step [56/244], loss=79.2715
	step [57/244], loss=89.3454
	step [58/244], loss=82.0258
	step [59/244], loss=79.2437
	step [60/244], loss=91.5419
	step [61/244], loss=77.2144
	step [62/244], loss=62.1822
	step [63/244], loss=89.5683
	step [64/244], loss=78.4700
	step [65/244], loss=81.9215
	step [66/244], loss=85.6905
	step [67/244], loss=95.6441
	step [68/244], loss=83.6289
	step [69/244], loss=88.1912
	step [70/244], loss=90.4081
	step [71/244], loss=83.4493
	step [72/244], loss=85.2767
	step [73/244], loss=79.5431
	step [74/244], loss=98.3450
	step [75/244], loss=68.2346
	step [76/244], loss=70.3954
	step [77/244], loss=83.9137
	step [78/244], loss=82.2239
	step [79/244], loss=87.9066
	step [80/244], loss=87.7122
	step [81/244], loss=69.4473
	step [82/244], loss=74.1086
	step [83/244], loss=65.0180
	step [84/244], loss=74.6898
	step [85/244], loss=67.2680
	step [86/244], loss=75.4906
	step [87/244], loss=79.0526
	step [88/244], loss=78.3581
	step [89/244], loss=83.9971
	step [90/244], loss=68.5918
	step [91/244], loss=85.5199
	step [92/244], loss=78.6456
	step [93/244], loss=87.8159
	step [94/244], loss=104.9740
	step [95/244], loss=81.7105
	step [96/244], loss=82.3420
	step [97/244], loss=90.2882
	step [98/244], loss=88.1016
	step [99/244], loss=78.5049
	step [100/244], loss=87.9903
	step [101/244], loss=95.0006
	step [102/244], loss=107.3687
	step [103/244], loss=91.4182
	step [104/244], loss=55.6481
	step [105/244], loss=73.8160
	step [106/244], loss=78.4939
	step [107/244], loss=95.4755
	step [108/244], loss=109.2847
	step [109/244], loss=75.5201
	step [110/244], loss=93.9727
	step [111/244], loss=92.5658
	step [112/244], loss=70.6242
	step [113/244], loss=84.3463
	step [114/244], loss=88.9169
	step [115/244], loss=99.6571
	step [116/244], loss=76.2863
	step [117/244], loss=78.5666
	step [118/244], loss=75.5388
	step [119/244], loss=88.7657
	step [120/244], loss=83.0304
	step [121/244], loss=55.3876
	step [122/244], loss=75.1059
	step [123/244], loss=84.0468
	step [124/244], loss=72.5756
	step [125/244], loss=77.1640
	step [126/244], loss=77.7562
	step [127/244], loss=95.4284
	step [128/244], loss=79.2174
	step [129/244], loss=83.5223
	step [130/244], loss=76.7608
	step [131/244], loss=88.8783
	step [132/244], loss=77.3164
	step [133/244], loss=69.9311
	step [134/244], loss=82.3540
	step [135/244], loss=94.7834
	step [136/244], loss=82.2168
	step [137/244], loss=86.7071
	step [138/244], loss=63.2890
	step [139/244], loss=70.1248
	step [140/244], loss=90.3570
	step [141/244], loss=86.6550
	step [142/244], loss=89.8066
	step [143/244], loss=83.8646
	step [144/244], loss=85.5336
	step [145/244], loss=81.7028
	step [146/244], loss=88.6528
	step [147/244], loss=61.8640
	step [148/244], loss=68.2238
	step [149/244], loss=93.0193
	step [150/244], loss=82.6827
	step [151/244], loss=80.8317
	step [152/244], loss=91.5985
	step [153/244], loss=67.8785
	step [154/244], loss=82.1839
	step [155/244], loss=84.9073
	step [156/244], loss=102.3410
	step [157/244], loss=69.0841
	step [158/244], loss=69.0658
	step [159/244], loss=99.3399
	step [160/244], loss=77.6413
	step [161/244], loss=73.3161
	step [162/244], loss=78.7289
	step [163/244], loss=86.3210
	step [164/244], loss=79.7683
	step [165/244], loss=74.1215
	step [166/244], loss=86.0262
	step [167/244], loss=64.2194
	step [168/244], loss=70.6769
	step [169/244], loss=82.4778
	step [170/244], loss=82.6419
	step [171/244], loss=75.0472
	step [172/244], loss=97.0330
	step [173/244], loss=94.4180
	step [174/244], loss=90.1963
	step [175/244], loss=83.4466
	step [176/244], loss=103.5924
	step [177/244], loss=71.5903
	step [178/244], loss=79.4531
	step [179/244], loss=95.0522
	step [180/244], loss=94.7863
	step [181/244], loss=84.2764
	step [182/244], loss=83.3382
	step [183/244], loss=68.7667
	step [184/244], loss=66.5951
	step [185/244], loss=72.7752
	step [186/244], loss=83.7217
	step [187/244], loss=74.8763
	step [188/244], loss=88.3319
	step [189/244], loss=74.2384
	step [190/244], loss=92.7475
	step [191/244], loss=97.4827
	step [192/244], loss=92.0493
	step [193/244], loss=82.0078
	step [194/244], loss=75.6432
	step [195/244], loss=78.2362
	step [196/244], loss=93.9959
	step [197/244], loss=75.1764
	step [198/244], loss=66.8159
	step [199/244], loss=73.1189
	step [200/244], loss=65.3560
	step [201/244], loss=77.3036
	step [202/244], loss=69.8967
	step [203/244], loss=82.1248
	step [204/244], loss=80.5775
	step [205/244], loss=87.2813
	step [206/244], loss=75.4598
	step [207/244], loss=84.8799
	step [208/244], loss=82.9623
	step [209/244], loss=76.6602
	step [210/244], loss=71.5620
	step [211/244], loss=86.1761
	step [212/244], loss=68.9679
	step [213/244], loss=74.3281
	step [214/244], loss=65.5853
	step [215/244], loss=90.7974
	step [216/244], loss=77.7346
	step [217/244], loss=76.1879
	step [218/244], loss=67.9144
	step [219/244], loss=86.9639
	step [220/244], loss=84.6381
	step [221/244], loss=71.7681
	step [222/244], loss=68.6085
	step [223/244], loss=90.9266
	step [224/244], loss=67.2166
	step [225/244], loss=68.0096
	step [226/244], loss=77.7522
	step [227/244], loss=82.5354
	step [228/244], loss=86.2527
	step [229/244], loss=77.9523
	step [230/244], loss=93.6324
	step [231/244], loss=81.0201
	step [232/244], loss=84.8287
	step [233/244], loss=90.6544
	step [234/244], loss=91.7091
	step [235/244], loss=74.8776
	step [236/244], loss=74.1876
	step [237/244], loss=80.6624
	step [238/244], loss=80.3175
	step [239/244], loss=101.1323
	step [240/244], loss=74.0200
	step [241/244], loss=94.5810
	step [242/244], loss=82.5538
	step [243/244], loss=81.0295
	step [244/244], loss=48.9943
	Evaluating
	loss=0.0098, precision=0.2923, recall=0.8787, f1=0.4387
Training epoch 71
	step [1/244], loss=72.3464
	step [2/244], loss=89.5414
	step [3/244], loss=84.6829
	step [4/244], loss=78.6944
	step [5/244], loss=74.3092
	step [6/244], loss=83.1394
	step [7/244], loss=72.5518
	step [8/244], loss=89.5227
	step [9/244], loss=61.9960
	step [10/244], loss=94.6729
	step [11/244], loss=99.6366
	step [12/244], loss=84.5246
	step [13/244], loss=93.8635
	step [14/244], loss=68.9953
	step [15/244], loss=62.5746
	step [16/244], loss=69.2665
	step [17/244], loss=57.7107
	step [18/244], loss=77.3367
	step [19/244], loss=80.0153
	step [20/244], loss=88.7923
	step [21/244], loss=85.3245
	step [22/244], loss=94.0415
	step [23/244], loss=85.7021
	step [24/244], loss=64.9525
	step [25/244], loss=75.7398
	step [26/244], loss=66.6619
	step [27/244], loss=84.6315
	step [28/244], loss=61.7526
	step [29/244], loss=77.3201
	step [30/244], loss=74.9282
	step [31/244], loss=78.8100
	step [32/244], loss=83.9785
	step [33/244], loss=96.7306
	step [34/244], loss=83.2478
	step [35/244], loss=74.7592
	step [36/244], loss=77.0792
	step [37/244], loss=98.6258
	step [38/244], loss=74.7349
	step [39/244], loss=80.1352
	step [40/244], loss=78.5487
	step [41/244], loss=92.5690
	step [42/244], loss=70.7405
	step [43/244], loss=67.6520
	step [44/244], loss=96.1398
	step [45/244], loss=73.4496
	step [46/244], loss=84.3621
	step [47/244], loss=71.7002
	step [48/244], loss=94.3996
	step [49/244], loss=74.9555
	step [50/244], loss=73.4467
	step [51/244], loss=95.6411
	step [52/244], loss=93.5243
	step [53/244], loss=97.8341
	step [54/244], loss=88.7735
	step [55/244], loss=85.4601
	step [56/244], loss=82.9576
	step [57/244], loss=84.2790
	step [58/244], loss=84.9047
	step [59/244], loss=67.2689
	step [60/244], loss=85.0587
	step [61/244], loss=79.1761
	step [62/244], loss=71.6313
	step [63/244], loss=79.9675
	step [64/244], loss=86.9586
	step [65/244], loss=79.4273
	step [66/244], loss=72.6000
	step [67/244], loss=89.8148
	step [68/244], loss=92.2561
	step [69/244], loss=82.0270
	step [70/244], loss=76.7637
	step [71/244], loss=92.6322
	step [72/244], loss=74.3755
	step [73/244], loss=77.4681
	step [74/244], loss=87.6072
	step [75/244], loss=77.8317
	step [76/244], loss=85.5562
	step [77/244], loss=85.3529
	step [78/244], loss=64.8700
	step [79/244], loss=75.0796
	step [80/244], loss=76.1712
	step [81/244], loss=80.8559
	step [82/244], loss=95.9402
	step [83/244], loss=87.1090
	step [84/244], loss=80.0549
	step [85/244], loss=95.1702
	step [86/244], loss=81.7543
	step [87/244], loss=106.4758
	step [88/244], loss=79.0351
	step [89/244], loss=100.4170
	step [90/244], loss=77.0073
	step [91/244], loss=84.4770
	step [92/244], loss=90.7722
	step [93/244], loss=67.1884
	step [94/244], loss=71.4765
	step [95/244], loss=69.6502
	step [96/244], loss=98.9232
	step [97/244], loss=70.6427
	step [98/244], loss=83.3392
	step [99/244], loss=74.0368
	step [100/244], loss=82.3831
	step [101/244], loss=73.4088
	step [102/244], loss=72.8688
	step [103/244], loss=88.7156
	step [104/244], loss=94.5495
	step [105/244], loss=96.1036
	step [106/244], loss=91.0777
	step [107/244], loss=85.2294
	step [108/244], loss=77.6396
	step [109/244], loss=94.0816
	step [110/244], loss=97.3747
	step [111/244], loss=85.5126
	step [112/244], loss=83.3458
	step [113/244], loss=78.1764
	step [114/244], loss=72.2951
	step [115/244], loss=95.6229
	step [116/244], loss=72.9845
	step [117/244], loss=70.1719
	step [118/244], loss=88.2921
	step [119/244], loss=86.7830
	step [120/244], loss=61.9560
	step [121/244], loss=87.1892
	step [122/244], loss=81.2276
	step [123/244], loss=72.4263
	step [124/244], loss=76.0674
	step [125/244], loss=99.6699
	step [126/244], loss=110.6597
	step [127/244], loss=88.5918
	step [128/244], loss=82.7485
	step [129/244], loss=94.0256
	step [130/244], loss=98.2230
	step [131/244], loss=76.6017
	step [132/244], loss=85.0245
	step [133/244], loss=79.7515
	step [134/244], loss=66.0567
	step [135/244], loss=82.7786
	step [136/244], loss=64.6967
	step [137/244], loss=72.1492
	step [138/244], loss=77.3959
	step [139/244], loss=77.0115
	step [140/244], loss=76.1734
	step [141/244], loss=86.3315
	step [142/244], loss=81.2970
	step [143/244], loss=79.1828
	step [144/244], loss=78.4304
	step [145/244], loss=64.0541
	step [146/244], loss=83.3220
	step [147/244], loss=85.6208
	step [148/244], loss=82.9392
	step [149/244], loss=93.0802
	step [150/244], loss=66.7308
	step [151/244], loss=77.1448
	step [152/244], loss=74.1695
	step [153/244], loss=95.8948
	step [154/244], loss=78.3606
	step [155/244], loss=66.4366
	step [156/244], loss=71.1740
	step [157/244], loss=70.7095
	step [158/244], loss=86.3479
	step [159/244], loss=73.8295
	step [160/244], loss=82.8543
	step [161/244], loss=72.5452
	step [162/244], loss=72.0271
	step [163/244], loss=96.5684
	step [164/244], loss=84.0793
	step [165/244], loss=82.0242
	step [166/244], loss=90.9597
	step [167/244], loss=100.4991
	step [168/244], loss=60.9075
	step [169/244], loss=103.0322
	step [170/244], loss=67.0436
	step [171/244], loss=78.4616
	step [172/244], loss=75.6965
	step [173/244], loss=67.6296
	step [174/244], loss=84.1323
	step [175/244], loss=85.3823
	step [176/244], loss=94.4709
	step [177/244], loss=70.0504
	step [178/244], loss=69.4047
	step [179/244], loss=67.9384
	step [180/244], loss=58.5556
	step [181/244], loss=82.6797
	step [182/244], loss=89.1077
	step [183/244], loss=101.1087
	step [184/244], loss=64.4276
	step [185/244], loss=60.4431
	step [186/244], loss=70.5032
	step [187/244], loss=79.5877
	step [188/244], loss=77.1246
	step [189/244], loss=94.5273
	step [190/244], loss=73.4943
	step [191/244], loss=85.9830
	step [192/244], loss=75.7666
	step [193/244], loss=95.5840
	step [194/244], loss=79.2091
	step [195/244], loss=91.6426
	step [196/244], loss=91.6862
	step [197/244], loss=67.1803
	step [198/244], loss=71.5499
	step [199/244], loss=77.9666
	step [200/244], loss=84.7791
	step [201/244], loss=83.7110
	step [202/244], loss=77.0526
	step [203/244], loss=90.5899
	step [204/244], loss=103.5950
	step [205/244], loss=66.3504
	step [206/244], loss=69.0959
	step [207/244], loss=84.9610
	step [208/244], loss=85.6194
	step [209/244], loss=75.6826
	step [210/244], loss=88.6373
	step [211/244], loss=104.5231
	step [212/244], loss=94.1009
	step [213/244], loss=76.2919
	step [214/244], loss=67.6608
	step [215/244], loss=87.9800
	step [216/244], loss=84.8721
	step [217/244], loss=94.3896
	step [218/244], loss=74.4677
	step [219/244], loss=85.2062
	step [220/244], loss=75.8258
	step [221/244], loss=80.3120
	step [222/244], loss=84.6610
	step [223/244], loss=97.0655
	step [224/244], loss=75.6093
	step [225/244], loss=77.0965
	step [226/244], loss=80.4946
	step [227/244], loss=80.8262
	step [228/244], loss=76.5699
	step [229/244], loss=82.2683
	step [230/244], loss=68.4202
	step [231/244], loss=90.2232
	step [232/244], loss=84.9693
	step [233/244], loss=90.5345
	step [234/244], loss=78.8741
	step [235/244], loss=80.8051
	step [236/244], loss=68.3205
	step [237/244], loss=84.3805
	step [238/244], loss=60.2038
	step [239/244], loss=69.6647
	step [240/244], loss=89.4572
	step [241/244], loss=89.2039
	step [242/244], loss=79.6402
	step [243/244], loss=106.1690
	step [244/244], loss=48.3713
	Evaluating
	loss=0.0100, precision=0.2953, recall=0.8694, f1=0.4408
Training epoch 72
	step [1/244], loss=89.5620
	step [2/244], loss=75.3238
	step [3/244], loss=89.1984
	step [4/244], loss=71.8800
	step [5/244], loss=92.3767
	step [6/244], loss=99.2586
	step [7/244], loss=87.0210
	step [8/244], loss=83.4255
	step [9/244], loss=77.0441
	step [10/244], loss=86.4047
	step [11/244], loss=83.1737
	step [12/244], loss=79.4116
	step [13/244], loss=89.0068
	step [14/244], loss=89.4932
	step [15/244], loss=71.6349
	step [16/244], loss=75.6387
	step [17/244], loss=73.0231
	step [18/244], loss=69.4181
	step [19/244], loss=77.7354
	step [20/244], loss=70.8910
	step [21/244], loss=78.4523
	step [22/244], loss=81.6736
	step [23/244], loss=72.9560
	step [24/244], loss=81.0786
	step [25/244], loss=80.6812
	step [26/244], loss=72.9789
	step [27/244], loss=76.2726
	step [28/244], loss=80.7947
	step [29/244], loss=77.5819
	step [30/244], loss=68.8568
	step [31/244], loss=66.7523
	step [32/244], loss=86.4720
	step [33/244], loss=70.7438
	step [34/244], loss=72.7984
	step [35/244], loss=88.5372
	step [36/244], loss=69.4659
	step [37/244], loss=75.8392
	step [38/244], loss=77.1998
	step [39/244], loss=79.5766
	step [40/244], loss=93.7579
	step [41/244], loss=91.0169
	step [42/244], loss=73.2758
	step [43/244], loss=71.8886
	step [44/244], loss=83.3727
	step [45/244], loss=92.0528
	step [46/244], loss=83.1123
	step [47/244], loss=84.3448
	step [48/244], loss=87.9055
	step [49/244], loss=76.8969
	step [50/244], loss=89.9667
	step [51/244], loss=88.2529
	step [52/244], loss=90.8068
	step [53/244], loss=77.9059
	step [54/244], loss=72.8954
	step [55/244], loss=91.3578
	step [56/244], loss=68.7160
	step [57/244], loss=78.8188
	step [58/244], loss=88.7546
	step [59/244], loss=84.5568
	step [60/244], loss=75.2791
	step [61/244], loss=88.1662
	step [62/244], loss=66.0010
	step [63/244], loss=80.2293
	step [64/244], loss=91.3770
	step [65/244], loss=83.4256
	step [66/244], loss=67.3805
	step [67/244], loss=86.1142
	step [68/244], loss=104.1522
	step [69/244], loss=78.7140
	step [70/244], loss=89.4733
	step [71/244], loss=83.1012
	step [72/244], loss=92.0263
	step [73/244], loss=84.4455
	step [74/244], loss=74.1685
	step [75/244], loss=78.4440
	step [76/244], loss=90.2548
	step [77/244], loss=70.1048
	step [78/244], loss=81.6372
	step [79/244], loss=80.9424
	step [80/244], loss=90.0150
	step [81/244], loss=76.5777
	step [82/244], loss=77.3570
	step [83/244], loss=86.2252
	step [84/244], loss=85.7847
	step [85/244], loss=59.9726
	step [86/244], loss=86.2347
	step [87/244], loss=86.7528
	step [88/244], loss=74.9312
	step [89/244], loss=93.1219
	step [90/244], loss=89.0537
	step [91/244], loss=73.9589
	step [92/244], loss=87.8026
	step [93/244], loss=79.8377
	step [94/244], loss=85.3068
	step [95/244], loss=90.9325
	step [96/244], loss=88.1228
	step [97/244], loss=66.6731
	step [98/244], loss=81.5860
	step [99/244], loss=67.3402
	step [100/244], loss=66.0071
	step [101/244], loss=77.4711
	step [102/244], loss=72.9301
	step [103/244], loss=92.9814
	step [104/244], loss=99.1378
	step [105/244], loss=79.4803
	step [106/244], loss=82.3400
	step [107/244], loss=70.5157
	step [108/244], loss=95.0798
	step [109/244], loss=73.2202
	step [110/244], loss=93.5442
	step [111/244], loss=88.1232
	step [112/244], loss=73.4713
	step [113/244], loss=69.6877
	step [114/244], loss=89.4354
	step [115/244], loss=82.4401
	step [116/244], loss=60.1893
	step [117/244], loss=73.1253
	step [118/244], loss=85.4897
	step [119/244], loss=95.1889
	step [120/244], loss=88.5307
	step [121/244], loss=82.1358
	step [122/244], loss=80.5589
	step [123/244], loss=71.6582
	step [124/244], loss=80.1521
	step [125/244], loss=92.9183
	step [126/244], loss=92.2113
	step [127/244], loss=75.1021
	step [128/244], loss=82.3718
	step [129/244], loss=76.5216
	step [130/244], loss=86.5852
	step [131/244], loss=77.3575
	step [132/244], loss=84.3057
	step [133/244], loss=92.3209
	step [134/244], loss=85.2075
	step [135/244], loss=85.1100
	step [136/244], loss=96.2487
	step [137/244], loss=83.3147
	step [138/244], loss=73.7053
	step [139/244], loss=95.2787
	step [140/244], loss=72.3054
	step [141/244], loss=74.1613
	step [142/244], loss=73.7946
	step [143/244], loss=63.2237
	step [144/244], loss=68.0387
	step [145/244], loss=91.8600
	step [146/244], loss=95.2385
	step [147/244], loss=82.6252
	step [148/244], loss=74.8178
	step [149/244], loss=73.8047
	step [150/244], loss=91.5646
	step [151/244], loss=82.8194
	step [152/244], loss=80.6829
	step [153/244], loss=68.9816
	step [154/244], loss=73.4756
	step [155/244], loss=74.1937
	step [156/244], loss=92.6875
	step [157/244], loss=84.5444
	step [158/244], loss=72.6530
	step [159/244], loss=72.0970
	step [160/244], loss=69.9491
	step [161/244], loss=82.7238
	step [162/244], loss=80.0276
	step [163/244], loss=71.2356
	step [164/244], loss=87.7542
	step [165/244], loss=90.3294
	step [166/244], loss=72.3229
	step [167/244], loss=73.7728
	step [168/244], loss=93.5995
	step [169/244], loss=84.6847
	step [170/244], loss=84.6188
	step [171/244], loss=68.4992
	step [172/244], loss=75.7761
	step [173/244], loss=94.6543
	step [174/244], loss=75.1848
	step [175/244], loss=86.8926
	step [176/244], loss=78.2271
	step [177/244], loss=70.4328
	step [178/244], loss=88.3995
	step [179/244], loss=85.0670
	step [180/244], loss=77.8298
	step [181/244], loss=84.4770
	step [182/244], loss=77.0086
	step [183/244], loss=67.6995
	step [184/244], loss=77.4423
	step [185/244], loss=74.0823
	step [186/244], loss=76.9925
	step [187/244], loss=80.5374
	step [188/244], loss=78.6592
	step [189/244], loss=74.6976
	step [190/244], loss=83.2335
	step [191/244], loss=70.7450
	step [192/244], loss=92.5490
	step [193/244], loss=74.2602
	step [194/244], loss=92.3199
	step [195/244], loss=90.2432
	step [196/244], loss=90.4660
	step [197/244], loss=71.0230
	step [198/244], loss=72.7320
	step [199/244], loss=58.7773
	step [200/244], loss=86.8467
	step [201/244], loss=66.7454
	step [202/244], loss=81.5618
	step [203/244], loss=93.8074
	step [204/244], loss=88.5534
	step [205/244], loss=79.4163
	step [206/244], loss=83.3333
	step [207/244], loss=74.5793
	step [208/244], loss=91.9302
	step [209/244], loss=80.6704
	step [210/244], loss=107.4640
	step [211/244], loss=78.9414
	step [212/244], loss=79.8815
	step [213/244], loss=73.5218
	step [214/244], loss=79.1962
	step [215/244], loss=75.5321
	step [216/244], loss=64.3518
	step [217/244], loss=71.8774
	step [218/244], loss=79.2256
	step [219/244], loss=83.1508
	step [220/244], loss=82.3702
	step [221/244], loss=81.8564
	step [222/244], loss=70.5279
	step [223/244], loss=69.6753
	step [224/244], loss=73.0936
	step [225/244], loss=82.3875
	step [226/244], loss=77.1392
	step [227/244], loss=103.6312
	step [228/244], loss=77.5621
	step [229/244], loss=81.8712
	step [230/244], loss=81.1798
	step [231/244], loss=91.8262
	step [232/244], loss=77.9520
	step [233/244], loss=89.6521
	step [234/244], loss=92.3029
	step [235/244], loss=80.6329
	step [236/244], loss=79.9616
	step [237/244], loss=75.3557
	step [238/244], loss=74.3104
	step [239/244], loss=78.8339
	step [240/244], loss=85.0660
	step [241/244], loss=96.2772
	step [242/244], loss=78.0077
	step [243/244], loss=92.2252
	step [244/244], loss=42.2480
	Evaluating
	loss=0.0088, precision=0.3263, recall=0.8691, f1=0.4745
saving model as: 2_saved_model.pth
Training epoch 73
	step [1/244], loss=69.1897
	step [2/244], loss=83.6367
	step [3/244], loss=78.2865
	step [4/244], loss=77.2603
	step [5/244], loss=70.4460
	step [6/244], loss=76.1148
	step [7/244], loss=75.1270
	step [8/244], loss=72.0370
	step [9/244], loss=80.1617
	step [10/244], loss=83.6207
	step [11/244], loss=72.1013
	step [12/244], loss=85.5834
	step [13/244], loss=82.9268
	step [14/244], loss=86.4672
	step [15/244], loss=72.2009
	step [16/244], loss=72.9393
	step [17/244], loss=90.9454
	step [18/244], loss=63.8496
	step [19/244], loss=79.8020
	step [20/244], loss=67.0899
	step [21/244], loss=62.1268
	step [22/244], loss=86.6211
	step [23/244], loss=85.6772
	step [24/244], loss=90.3805
	step [25/244], loss=76.1225
	step [26/244], loss=73.0951
	step [27/244], loss=95.8027
	step [28/244], loss=81.6667
	step [29/244], loss=89.1887
	step [30/244], loss=85.8080
	step [31/244], loss=85.7115
	step [32/244], loss=86.0360
	step [33/244], loss=81.2950
	step [34/244], loss=72.0028
	step [35/244], loss=83.7159
	step [36/244], loss=80.9660
	step [37/244], loss=75.3568
	step [38/244], loss=80.0032
	step [39/244], loss=93.3387
	step [40/244], loss=89.1070
	step [41/244], loss=69.2657
	step [42/244], loss=95.1751
	step [43/244], loss=58.6971
	step [44/244], loss=85.1271
	step [45/244], loss=87.8182
	step [46/244], loss=74.0393
	step [47/244], loss=79.8325
	step [48/244], loss=78.3953
	step [49/244], loss=70.3987
	step [50/244], loss=76.1143
	step [51/244], loss=96.5217
	step [52/244], loss=96.3391
	step [53/244], loss=92.8627
	step [54/244], loss=75.5972
	step [55/244], loss=78.5206
	step [56/244], loss=78.0148
	step [57/244], loss=85.7292
	step [58/244], loss=62.4397
	step [59/244], loss=62.8764
	step [60/244], loss=68.6238
	step [61/244], loss=79.2224
	step [62/244], loss=100.9700
	step [63/244], loss=77.6907
	step [64/244], loss=70.0613
	step [65/244], loss=81.2479
	step [66/244], loss=92.3885
	step [67/244], loss=78.0818
	step [68/244], loss=76.6136
	step [69/244], loss=95.8659
	step [70/244], loss=90.1549
	step [71/244], loss=84.9462
	step [72/244], loss=86.1462
	step [73/244], loss=84.1766
	step [74/244], loss=70.6753
	step [75/244], loss=92.9713
	step [76/244], loss=86.1161
	step [77/244], loss=71.9772
	step [78/244], loss=73.7444
	step [79/244], loss=81.5751
	step [80/244], loss=76.7091
	step [81/244], loss=75.4288
	step [82/244], loss=74.4564
	step [83/244], loss=69.8808
	step [84/244], loss=76.4767
	step [85/244], loss=96.7478
	step [86/244], loss=84.0726
	step [87/244], loss=79.2936
	step [88/244], loss=89.2546
	step [89/244], loss=84.0440
	step [90/244], loss=72.2490
	step [91/244], loss=83.9741
	step [92/244], loss=88.6195
	step [93/244], loss=70.9074
	step [94/244], loss=76.8425
	step [95/244], loss=93.4771
	step [96/244], loss=82.2860
	step [97/244], loss=67.4026
	step [98/244], loss=88.3729
	step [99/244], loss=69.8855
	step [100/244], loss=102.2874
	step [101/244], loss=72.1569
	step [102/244], loss=68.3224
	step [103/244], loss=80.5698
	step [104/244], loss=96.8148
	step [105/244], loss=71.7565
	step [106/244], loss=79.9507
	step [107/244], loss=85.5546
	step [108/244], loss=68.6958
	step [109/244], loss=84.5777
	step [110/244], loss=99.1711
	step [111/244], loss=73.3741
	step [112/244], loss=74.6946
	step [113/244], loss=77.6915
	step [114/244], loss=78.0106
	step [115/244], loss=96.5281
	step [116/244], loss=75.8982
	step [117/244], loss=73.2515
	step [118/244], loss=78.1652
	step [119/244], loss=87.4891
	step [120/244], loss=62.5092
	step [121/244], loss=80.5087
	step [122/244], loss=64.1795
	step [123/244], loss=72.8137
	step [124/244], loss=77.2175
	step [125/244], loss=76.5395
	step [126/244], loss=78.4955
	step [127/244], loss=90.7676
	step [128/244], loss=104.8399
	step [129/244], loss=76.2886
	step [130/244], loss=75.4151
	step [131/244], loss=75.1929
	step [132/244], loss=71.3607
	step [133/244], loss=60.1454
	step [134/244], loss=64.4026
	step [135/244], loss=93.5943
	step [136/244], loss=84.7969
	step [137/244], loss=61.0754
	step [138/244], loss=79.9275
	step [139/244], loss=87.5850
	step [140/244], loss=82.7915
	step [141/244], loss=93.2517
	step [142/244], loss=89.8383
	step [143/244], loss=85.3186
	step [144/244], loss=91.3488
	step [145/244], loss=91.5335
	step [146/244], loss=74.5487
	step [147/244], loss=87.8683
	step [148/244], loss=83.4891
	step [149/244], loss=96.1142
	step [150/244], loss=82.0349
	step [151/244], loss=74.9998
	step [152/244], loss=82.6691
	step [153/244], loss=80.2151
	step [154/244], loss=84.7195
	step [155/244], loss=102.7708
	step [156/244], loss=100.3221
	step [157/244], loss=77.4371
	step [158/244], loss=73.4185
	step [159/244], loss=77.8107
	step [160/244], loss=106.0294
	step [161/244], loss=75.6564
	step [162/244], loss=71.6798
	step [163/244], loss=86.9519
	step [164/244], loss=78.8263
	step [165/244], loss=78.9899
	step [166/244], loss=88.6978
	step [167/244], loss=89.3070
	step [168/244], loss=71.7383
	step [169/244], loss=77.7762
	step [170/244], loss=91.0631
	step [171/244], loss=85.2297
	step [172/244], loss=90.2016
	step [173/244], loss=69.2866
	step [174/244], loss=87.4922
	step [175/244], loss=68.2255
	step [176/244], loss=77.2847
	step [177/244], loss=82.0761
	step [178/244], loss=90.7915
	step [179/244], loss=75.0877
	step [180/244], loss=70.3036
	step [181/244], loss=76.5331
	step [182/244], loss=79.9192
	step [183/244], loss=92.6246
	step [184/244], loss=87.9366
	step [185/244], loss=82.6817
	step [186/244], loss=81.6525
	step [187/244], loss=76.5146
	step [188/244], loss=80.6015
	step [189/244], loss=77.2751
	step [190/244], loss=77.6394
	step [191/244], loss=92.3784
	step [192/244], loss=65.4926
	step [193/244], loss=69.3273
	step [194/244], loss=76.8328
	step [195/244], loss=58.1748
	step [196/244], loss=73.8117
	step [197/244], loss=73.7052
	step [198/244], loss=87.7288
	step [199/244], loss=79.5747
	step [200/244], loss=87.3040
	step [201/244], loss=75.7059
	step [202/244], loss=70.6336
	step [203/244], loss=93.2133
	step [204/244], loss=77.1351
	step [205/244], loss=89.5944
	step [206/244], loss=98.2039
	step [207/244], loss=71.4718
	step [208/244], loss=76.6956
	step [209/244], loss=109.7826
	step [210/244], loss=74.0984
	step [211/244], loss=78.5315
	step [212/244], loss=75.7871
	step [213/244], loss=86.7882
	step [214/244], loss=69.2614
	step [215/244], loss=67.7985
	step [216/244], loss=88.2309
	step [217/244], loss=82.6050
	step [218/244], loss=68.4772
	step [219/244], loss=81.8712
	step [220/244], loss=71.2341
	step [221/244], loss=76.3834
	step [222/244], loss=79.9974
	step [223/244], loss=79.7781
	step [224/244], loss=102.3049
	step [225/244], loss=89.3003
	step [226/244], loss=67.1918
	step [227/244], loss=83.5727
	step [228/244], loss=68.1876
	step [229/244], loss=89.4858
	step [230/244], loss=70.2359
	step [231/244], loss=89.3907
	step [232/244], loss=83.5994
	step [233/244], loss=83.0024
	step [234/244], loss=82.0140
	step [235/244], loss=85.5885
	step [236/244], loss=100.2937
	step [237/244], loss=72.3535
	step [238/244], loss=69.5828
	step [239/244], loss=76.0798
	step [240/244], loss=87.0066
	step [241/244], loss=89.0108
	step [242/244], loss=71.5047
	step [243/244], loss=85.7700
	step [244/244], loss=40.4696
	Evaluating
	loss=0.0094, precision=0.3137, recall=0.8733, f1=0.4616
Training epoch 74
	step [1/244], loss=75.1312
	step [2/244], loss=78.8618
	step [3/244], loss=64.0220
	step [4/244], loss=86.8815
	step [5/244], loss=76.3243
	step [6/244], loss=85.8683
	step [7/244], loss=93.6922
	step [8/244], loss=61.8386
	step [9/244], loss=74.2035
	step [10/244], loss=81.5471
	step [11/244], loss=75.0603
	step [12/244], loss=70.1614
	step [13/244], loss=68.8669
	step [14/244], loss=76.5020
	step [15/244], loss=73.3000
	step [16/244], loss=75.6790
	step [17/244], loss=71.9224
	step [18/244], loss=81.9116
	step [19/244], loss=83.8357
	step [20/244], loss=93.5395
	step [21/244], loss=79.6324
	step [22/244], loss=82.0099
	step [23/244], loss=74.8897
	step [24/244], loss=82.8303
	step [25/244], loss=60.0961
	step [26/244], loss=77.7642
	step [27/244], loss=88.7208
	step [28/244], loss=92.4485
	step [29/244], loss=69.5850
	step [30/244], loss=82.6868
	step [31/244], loss=89.9447
	step [32/244], loss=80.4706
	step [33/244], loss=69.2813
	step [34/244], loss=93.9468
	step [35/244], loss=86.1344
	step [36/244], loss=86.1866
	step [37/244], loss=81.6683
	step [38/244], loss=88.5235
	step [39/244], loss=96.4145
	step [40/244], loss=77.5066
	step [41/244], loss=102.5234
	step [42/244], loss=82.3643
	step [43/244], loss=86.8578
	step [44/244], loss=66.6625
	step [45/244], loss=84.2072
	step [46/244], loss=81.1109
	step [47/244], loss=89.3261
	step [48/244], loss=78.5544
	step [49/244], loss=73.8374
	step [50/244], loss=74.6398
	step [51/244], loss=81.6908
	step [52/244], loss=91.2307
	step [53/244], loss=72.6781
	step [54/244], loss=79.0517
	step [55/244], loss=68.4186
	step [56/244], loss=72.5435
	step [57/244], loss=77.8014
	step [58/244], loss=57.2285
	step [59/244], loss=73.9613
	step [60/244], loss=88.5990
	step [61/244], loss=93.9937
	step [62/244], loss=62.9269
	step [63/244], loss=82.0355
	step [64/244], loss=95.3675
	step [65/244], loss=92.5255
	step [66/244], loss=68.9418
	step [67/244], loss=68.8467
	step [68/244], loss=80.0397
	step [69/244], loss=99.8569
	step [70/244], loss=66.7085
	step [71/244], loss=86.3106
	step [72/244], loss=93.3212
	step [73/244], loss=77.0931
	step [74/244], loss=67.4727
	step [75/244], loss=73.9726
	step [76/244], loss=78.7446
	step [77/244], loss=92.4890
	step [78/244], loss=86.1084
	step [79/244], loss=88.7749
	step [80/244], loss=74.5149
	step [81/244], loss=85.8188
	step [82/244], loss=76.8929
	step [83/244], loss=76.5326
	step [84/244], loss=70.4381
	step [85/244], loss=91.2874
	step [86/244], loss=86.8873
	step [87/244], loss=69.9590
	step [88/244], loss=89.8743
	step [89/244], loss=79.5588
	step [90/244], loss=69.5661
	step [91/244], loss=74.8257
	step [92/244], loss=91.3411
	step [93/244], loss=84.1208
	step [94/244], loss=87.3234
	step [95/244], loss=96.5057
	step [96/244], loss=81.2469
	step [97/244], loss=71.3889
	step [98/244], loss=86.6707
	step [99/244], loss=87.8792
	step [100/244], loss=81.0647
	step [101/244], loss=84.1071
	step [102/244], loss=94.6657
	step [103/244], loss=67.2512
	step [104/244], loss=69.3047
	step [105/244], loss=84.8739
	step [106/244], loss=74.5967
	step [107/244], loss=68.5995
	step [108/244], loss=70.9290
	step [109/244], loss=93.2305
	step [110/244], loss=89.5422
	step [111/244], loss=71.7599
	step [112/244], loss=76.1180
	step [113/244], loss=80.7791
	step [114/244], loss=73.2148
	step [115/244], loss=83.8392
	step [116/244], loss=84.0742
	step [117/244], loss=64.8608
	step [118/244], loss=81.8684
	step [119/244], loss=74.6603
	step [120/244], loss=76.6124
	step [121/244], loss=87.2170
	step [122/244], loss=80.8278
	step [123/244], loss=90.4251
	step [124/244], loss=84.3861
	step [125/244], loss=69.0473
	step [126/244], loss=67.9849
	step [127/244], loss=84.2460
	step [128/244], loss=81.5670
	step [129/244], loss=86.0512
	step [130/244], loss=85.8379
	step [131/244], loss=71.8566
	step [132/244], loss=66.2375
	step [133/244], loss=66.7903
	step [134/244], loss=81.2268
	step [135/244], loss=72.2946
	step [136/244], loss=80.2062
	step [137/244], loss=85.9259
	step [138/244], loss=90.3130
	step [139/244], loss=78.6859
	step [140/244], loss=77.7000
	step [141/244], loss=79.5210
	step [142/244], loss=65.9690
	step [143/244], loss=66.9613
	step [144/244], loss=75.4511
	step [145/244], loss=81.0972
	step [146/244], loss=71.4403
	step [147/244], loss=74.7587
	step [148/244], loss=74.5893
	step [149/244], loss=74.1576
	step [150/244], loss=70.0024
	step [151/244], loss=69.5865
	step [152/244], loss=65.8418
	step [153/244], loss=86.7478
	step [154/244], loss=65.5758
	step [155/244], loss=81.2929
	step [156/244], loss=82.4165
	step [157/244], loss=95.3671
	step [158/244], loss=74.3060
	step [159/244], loss=69.4889
	step [160/244], loss=89.6644
	step [161/244], loss=75.6990
	step [162/244], loss=85.1041
	step [163/244], loss=69.4595
	step [164/244], loss=70.7153
	step [165/244], loss=100.2155
	step [166/244], loss=100.6053
	step [167/244], loss=80.8734
	step [168/244], loss=78.0306
	step [169/244], loss=82.0736
	step [170/244], loss=72.5852
	step [171/244], loss=68.5093
	step [172/244], loss=87.3249
	step [173/244], loss=77.1231
	step [174/244], loss=75.9640
	step [175/244], loss=93.5927
	step [176/244], loss=88.6801
	step [177/244], loss=64.5915
	step [178/244], loss=82.6392
	step [179/244], loss=83.9146
	step [180/244], loss=79.5985
	step [181/244], loss=75.7828
	step [182/244], loss=86.0449
	step [183/244], loss=84.1372
	step [184/244], loss=74.6274
	step [185/244], loss=85.3686
	step [186/244], loss=72.8671
	step [187/244], loss=78.9645
	step [188/244], loss=78.2124
	step [189/244], loss=107.3080
	step [190/244], loss=92.6941
	step [191/244], loss=89.9660
	step [192/244], loss=77.3208
	step [193/244], loss=96.8720
	step [194/244], loss=87.7832
	step [195/244], loss=78.7607
	step [196/244], loss=85.2194
	step [197/244], loss=91.7515
	step [198/244], loss=85.5871
	step [199/244], loss=77.5019
	step [200/244], loss=86.7548
	step [201/244], loss=87.6495
	step [202/244], loss=73.7603
	step [203/244], loss=84.2050
	step [204/244], loss=79.6203
	step [205/244], loss=77.7417
	step [206/244], loss=68.2777
	step [207/244], loss=83.2907
	step [208/244], loss=82.2958
	step [209/244], loss=71.6799
	step [210/244], loss=61.4481
	step [211/244], loss=73.9025
	step [212/244], loss=74.4795
	step [213/244], loss=79.0958
	step [214/244], loss=65.4418
	step [215/244], loss=94.9719
	step [216/244], loss=86.1151
	step [217/244], loss=81.8169
	step [218/244], loss=64.1713
	step [219/244], loss=81.0743
	step [220/244], loss=71.3186
	step [221/244], loss=95.2473
	step [222/244], loss=89.9810
	step [223/244], loss=78.2700
	step [224/244], loss=80.1717
	step [225/244], loss=83.6175
	step [226/244], loss=87.1066
	step [227/244], loss=86.8876
	step [228/244], loss=63.8621
	step [229/244], loss=99.5639
	step [230/244], loss=91.6069
	step [231/244], loss=91.7989
	step [232/244], loss=87.5538
	step [233/244], loss=82.3968
	step [234/244], loss=95.5038
	step [235/244], loss=82.0404
	step [236/244], loss=67.2655
	step [237/244], loss=89.6347
	step [238/244], loss=87.1002
	step [239/244], loss=71.3956
	step [240/244], loss=76.6895
	step [241/244], loss=64.5300
	step [242/244], loss=62.2884
	step [243/244], loss=94.9789
	step [244/244], loss=45.5272
	Evaluating
	loss=0.0105, precision=0.2668, recall=0.8696, f1=0.4084
Training epoch 75
	step [1/244], loss=71.1894
	step [2/244], loss=92.0930
	step [3/244], loss=94.2916
	step [4/244], loss=83.2960
	step [5/244], loss=86.0017
	step [6/244], loss=83.6172
	step [7/244], loss=93.2041
	step [8/244], loss=87.8065
	step [9/244], loss=74.6512
	step [10/244], loss=89.7478
	step [11/244], loss=84.3914
	step [12/244], loss=81.5702
	step [13/244], loss=81.3882
	step [14/244], loss=76.9697
	step [15/244], loss=88.1534
	step [16/244], loss=79.6659
	step [17/244], loss=81.0032
	step [18/244], loss=78.3380
	step [19/244], loss=61.2332
	step [20/244], loss=92.9732
	step [21/244], loss=76.9864
	step [22/244], loss=90.0726
	step [23/244], loss=82.9567
	step [24/244], loss=98.6602
	step [25/244], loss=87.6513
	step [26/244], loss=104.4856
	step [27/244], loss=85.9430
	step [28/244], loss=82.6687
	step [29/244], loss=72.6052
	step [30/244], loss=94.6633
	step [31/244], loss=79.3188
	step [32/244], loss=94.7733
	step [33/244], loss=65.8021
	step [34/244], loss=95.4185
	step [35/244], loss=64.6319
	step [36/244], loss=81.7166
	step [37/244], loss=79.2417
	step [38/244], loss=76.7266
	step [39/244], loss=75.2206
	step [40/244], loss=78.1344
	step [41/244], loss=91.7936
	step [42/244], loss=68.0588
	step [43/244], loss=77.8827
	step [44/244], loss=76.5480
	step [45/244], loss=70.4792
	step [46/244], loss=88.0360
	step [47/244], loss=84.4778
	step [48/244], loss=80.1013
	step [49/244], loss=86.9163
	step [50/244], loss=78.7310
	step [51/244], loss=94.0139
	step [52/244], loss=91.0047
	step [53/244], loss=81.5409
	step [54/244], loss=73.7096
	step [55/244], loss=75.6938
	step [56/244], loss=79.0332
	step [57/244], loss=95.3245
	step [58/244], loss=70.6411
	step [59/244], loss=81.7478
	step [60/244], loss=67.6995
	step [61/244], loss=61.8381
	step [62/244], loss=80.5428
	step [63/244], loss=82.4015
	step [64/244], loss=94.5379
	step [65/244], loss=79.9499
	step [66/244], loss=66.5641
	step [67/244], loss=73.4457
	step [68/244], loss=74.8319
	step [69/244], loss=85.9379
	step [70/244], loss=73.7632
	step [71/244], loss=71.0493
	step [72/244], loss=79.5355
	step [73/244], loss=73.0880
	step [74/244], loss=79.7355
	step [75/244], loss=90.7676
	step [76/244], loss=77.3278
	step [77/244], loss=67.4651
	step [78/244], loss=85.6660
	step [79/244], loss=96.7449
	step [80/244], loss=63.5255
	step [81/244], loss=76.9935
	step [82/244], loss=82.0524
	step [83/244], loss=75.3484
	step [84/244], loss=86.1714
	step [85/244], loss=90.7870
	step [86/244], loss=92.6297
	step [87/244], loss=88.3138
	step [88/244], loss=90.3134
	step [89/244], loss=79.1850
	step [90/244], loss=67.7422
	step [91/244], loss=57.3395
	step [92/244], loss=78.0543
	step [93/244], loss=83.8840
	step [94/244], loss=85.8301
	step [95/244], loss=68.7694
	step [96/244], loss=73.2058
	step [97/244], loss=56.1517
	step [98/244], loss=67.6061
	step [99/244], loss=70.5492
	step [100/244], loss=81.9095
	step [101/244], loss=91.1707
	step [102/244], loss=94.6902
	step [103/244], loss=77.3623
	step [104/244], loss=85.1478
	step [105/244], loss=77.5536
	step [106/244], loss=67.4918
	step [107/244], loss=92.9509
	step [108/244], loss=80.4613
	step [109/244], loss=78.9503
	step [110/244], loss=75.5906
	step [111/244], loss=71.9789
	step [112/244], loss=84.9888
	step [113/244], loss=97.6016
	step [114/244], loss=87.2548
	step [115/244], loss=91.6551
	step [116/244], loss=91.9872
	step [117/244], loss=83.8798
	step [118/244], loss=74.0002
	step [119/244], loss=70.6003
	step [120/244], loss=85.8653
	step [121/244], loss=89.2108
	step [122/244], loss=67.6775
	step [123/244], loss=79.0894
	step [124/244], loss=85.0754
	step [125/244], loss=88.6047
	step [126/244], loss=70.0900
	step [127/244], loss=91.3776
	step [128/244], loss=76.6035
	step [129/244], loss=64.6388
	step [130/244], loss=76.8379
	step [131/244], loss=80.3442
	step [132/244], loss=93.6604
	step [133/244], loss=68.9360
	step [134/244], loss=69.3230
	step [135/244], loss=80.5141
	step [136/244], loss=83.9692
	step [137/244], loss=71.9338
	step [138/244], loss=86.0097
	step [139/244], loss=82.7173
	step [140/244], loss=75.5851
	step [141/244], loss=82.7695
	step [142/244], loss=76.6427
	step [143/244], loss=78.4124
	step [144/244], loss=82.6661
	step [145/244], loss=73.5951
	step [146/244], loss=85.6797
	step [147/244], loss=72.4452
	step [148/244], loss=83.6778
	step [149/244], loss=77.9682
	step [150/244], loss=69.0392
	step [151/244], loss=92.5780
	step [152/244], loss=73.0381
	step [153/244], loss=70.4781
	step [154/244], loss=65.7536
	step [155/244], loss=81.2677
	step [156/244], loss=81.5147
	step [157/244], loss=85.1051
	step [158/244], loss=79.2685
	step [159/244], loss=64.7481
	step [160/244], loss=82.3429
	step [161/244], loss=76.9431
	step [162/244], loss=72.6575
	step [163/244], loss=67.5329
	step [164/244], loss=80.7782
	step [165/244], loss=86.2725
	step [166/244], loss=89.8146
	step [167/244], loss=70.9211
	step [168/244], loss=76.1724
	step [169/244], loss=85.3540
	step [170/244], loss=80.9568
	step [171/244], loss=54.7468
	step [172/244], loss=69.7716
	step [173/244], loss=76.6210
	step [174/244], loss=69.1053
	step [175/244], loss=86.4816
	step [176/244], loss=91.2410
	step [177/244], loss=73.3751
	step [178/244], loss=87.0631
	step [179/244], loss=101.5164
	step [180/244], loss=74.8140
	step [181/244], loss=92.3553
	step [182/244], loss=73.9320
	step [183/244], loss=73.0528
	step [184/244], loss=85.7298
	step [185/244], loss=83.5056
	step [186/244], loss=69.7574
	step [187/244], loss=88.0815
	step [188/244], loss=71.9014
	step [189/244], loss=86.0916
	step [190/244], loss=71.1094
	step [191/244], loss=86.6127
	step [192/244], loss=58.1772
	step [193/244], loss=77.4854
	step [194/244], loss=73.5214
	step [195/244], loss=80.6773
	step [196/244], loss=90.6177
	step [197/244], loss=87.2097
	step [198/244], loss=88.3341
	step [199/244], loss=74.4690
	step [200/244], loss=61.7208
	step [201/244], loss=66.4652
	step [202/244], loss=77.8085
	step [203/244], loss=80.8231
	step [204/244], loss=82.4208
	step [205/244], loss=84.0839
	step [206/244], loss=91.4608
	step [207/244], loss=94.5165
	step [208/244], loss=74.8028
	step [209/244], loss=67.9585
	step [210/244], loss=89.8204
	step [211/244], loss=83.6722
	step [212/244], loss=81.0884
	step [213/244], loss=86.6197
	step [214/244], loss=92.2119
	step [215/244], loss=69.7949
	step [216/244], loss=83.5241
	step [217/244], loss=97.2868
	step [218/244], loss=69.8684
	step [219/244], loss=97.7348
	step [220/244], loss=79.9231
	step [221/244], loss=66.3937
	step [222/244], loss=83.8913
	step [223/244], loss=80.1347
	step [224/244], loss=73.2960
	step [225/244], loss=79.2154
	step [226/244], loss=89.0276
	step [227/244], loss=93.8098
	step [228/244], loss=70.0744
	step [229/244], loss=93.9531
	step [230/244], loss=95.2346
	step [231/244], loss=77.7544
	step [232/244], loss=84.3513
	step [233/244], loss=68.0457
	step [234/244], loss=62.1938
	step [235/244], loss=76.5286
	step [236/244], loss=72.7564
	step [237/244], loss=76.5659
	step [238/244], loss=84.0517
	step [239/244], loss=64.4509
	step [240/244], loss=86.0956
	step [241/244], loss=69.4905
	step [242/244], loss=77.4618
	step [243/244], loss=82.8189
	step [244/244], loss=42.7704
	Evaluating
	loss=0.0098, precision=0.3010, recall=0.8875, f1=0.4496
Training epoch 76
	step [1/244], loss=78.8151
	step [2/244], loss=84.7323
	step [3/244], loss=80.4234
	step [4/244], loss=73.3446
	step [5/244], loss=75.4776
	step [6/244], loss=90.5580
	step [7/244], loss=84.1299
	step [8/244], loss=95.8468
	step [9/244], loss=73.0951
	step [10/244], loss=87.8020
	step [11/244], loss=64.2323
	step [12/244], loss=88.5494
	step [13/244], loss=86.9397
	step [14/244], loss=79.4509
	step [15/244], loss=66.7995
	step [16/244], loss=73.7477
	step [17/244], loss=71.3376
	step [18/244], loss=79.7959
	step [19/244], loss=71.8077
	step [20/244], loss=75.6518
	step [21/244], loss=83.9960
	step [22/244], loss=81.7513
	step [23/244], loss=74.6449
	step [24/244], loss=80.9633
	step [25/244], loss=66.0188
	step [26/244], loss=79.4422
	step [27/244], loss=80.3779
	step [28/244], loss=80.5797
	step [29/244], loss=70.2332
	step [30/244], loss=84.1575
	step [31/244], loss=85.6369
	step [32/244], loss=82.6867
	step [33/244], loss=76.7026
	step [34/244], loss=75.0885
	step [35/244], loss=69.8005
	step [36/244], loss=80.4610
	step [37/244], loss=75.1058
	step [38/244], loss=77.4783
	step [39/244], loss=65.6388
	step [40/244], loss=94.8680
	step [41/244], loss=77.5755
	step [42/244], loss=85.3764
	step [43/244], loss=96.1044
	step [44/244], loss=95.6873
	step [45/244], loss=72.7908
	step [46/244], loss=72.1773
	step [47/244], loss=68.1643
	step [48/244], loss=69.5487
	step [49/244], loss=96.4124
	step [50/244], loss=77.9286
	step [51/244], loss=76.3502
	step [52/244], loss=91.4614
	step [53/244], loss=78.1503
	step [54/244], loss=95.6558
	step [55/244], loss=88.3409
	step [56/244], loss=65.9430
	step [57/244], loss=73.7833
	step [58/244], loss=82.3810
	step [59/244], loss=78.2396
	step [60/244], loss=91.6831
	step [61/244], loss=73.9906
	step [62/244], loss=80.4884
	step [63/244], loss=100.5691
	step [64/244], loss=74.0588
	step [65/244], loss=81.2099
	step [66/244], loss=65.2215
	step [67/244], loss=93.3878
	step [68/244], loss=63.0549
	step [69/244], loss=84.5491
	step [70/244], loss=81.7632
	step [71/244], loss=87.4657
	step [72/244], loss=81.4663
	step [73/244], loss=86.1814
	step [74/244], loss=95.3847
	step [75/244], loss=72.1073
	step [76/244], loss=70.7903
	step [77/244], loss=84.4645
	step [78/244], loss=69.9490
	step [79/244], loss=74.3168
	step [80/244], loss=91.3694
	step [81/244], loss=63.6621
	step [82/244], loss=87.0431
	step [83/244], loss=66.5128
	step [84/244], loss=84.4824
	step [85/244], loss=71.5027
	step [86/244], loss=70.8463
	step [87/244], loss=79.4369
	step [88/244], loss=84.1926
	step [89/244], loss=70.2988
	step [90/244], loss=80.8067
	step [91/244], loss=88.7094
	step [92/244], loss=74.2203
	step [93/244], loss=88.6063
	step [94/244], loss=86.6214
	step [95/244], loss=64.2617
	step [96/244], loss=71.4745
	step [97/244], loss=75.5131
	step [98/244], loss=82.9327
	step [99/244], loss=63.9194
	step [100/244], loss=89.9045
	step [101/244], loss=68.0539
	step [102/244], loss=74.6938
	step [103/244], loss=93.5978
	step [104/244], loss=79.6573
	step [105/244], loss=98.0956
	step [106/244], loss=86.7417
	step [107/244], loss=86.0646
	step [108/244], loss=74.2716
	step [109/244], loss=86.4897
	step [110/244], loss=66.8842
	step [111/244], loss=91.0057
	step [112/244], loss=92.1353
	step [113/244], loss=82.1043
	step [114/244], loss=94.5899
	step [115/244], loss=88.5506
	step [116/244], loss=76.3420
	step [117/244], loss=83.9725
	step [118/244], loss=76.1178
	step [119/244], loss=83.9699
	step [120/244], loss=92.5130
	step [121/244], loss=91.1297
	step [122/244], loss=78.1144
	step [123/244], loss=80.0606
	step [124/244], loss=74.1214
	step [125/244], loss=79.3310
	step [126/244], loss=80.0821
	step [127/244], loss=72.0376
	step [128/244], loss=81.1946
	step [129/244], loss=66.3222
	step [130/244], loss=80.0768
	step [131/244], loss=81.1430
	step [132/244], loss=78.8445
	step [133/244], loss=73.7207
	step [134/244], loss=71.9776
	step [135/244], loss=77.4716
	step [136/244], loss=68.2327
	step [137/244], loss=105.3024
	step [138/244], loss=76.1920
	step [139/244], loss=74.2217
	step [140/244], loss=70.5605
	step [141/244], loss=94.8661
	step [142/244], loss=77.3196
	step [143/244], loss=71.7166
	step [144/244], loss=116.1699
	step [145/244], loss=82.2483
	step [146/244], loss=83.5204
	step [147/244], loss=90.6803
	step [148/244], loss=90.9844
	step [149/244], loss=76.0548
	step [150/244], loss=75.1882
	step [151/244], loss=76.7254
	step [152/244], loss=78.6751
	step [153/244], loss=95.6742
	step [154/244], loss=96.1140
	step [155/244], loss=81.2355
	step [156/244], loss=70.8687
	step [157/244], loss=61.6831
	step [158/244], loss=84.4358
	step [159/244], loss=71.1427
	step [160/244], loss=100.5667
	step [161/244], loss=82.8052
	step [162/244], loss=97.8124
	step [163/244], loss=92.2639
	step [164/244], loss=70.0403
	step [165/244], loss=85.9616
	step [166/244], loss=78.3447
	step [167/244], loss=85.3318
	step [168/244], loss=84.6923
	step [169/244], loss=79.6870
	step [170/244], loss=69.2863
	step [171/244], loss=80.0736
	step [172/244], loss=78.6518
	step [173/244], loss=72.6833
	step [174/244], loss=76.3162
	step [175/244], loss=80.8800
	step [176/244], loss=80.3428
	step [177/244], loss=93.1255
	step [178/244], loss=75.6506
	step [179/244], loss=94.0299
	step [180/244], loss=77.2223
	step [181/244], loss=74.4962
	step [182/244], loss=94.1321
	step [183/244], loss=77.3084
	step [184/244], loss=80.4105
	step [185/244], loss=84.1866
	step [186/244], loss=71.8493
	step [187/244], loss=66.8785
	step [188/244], loss=85.1722
	step [189/244], loss=63.7733
	step [190/244], loss=94.9227
	step [191/244], loss=74.1335
	step [192/244], loss=99.8793
	step [193/244], loss=86.4645
	step [194/244], loss=83.1401
	step [195/244], loss=91.1740
	step [196/244], loss=71.1792
	step [197/244], loss=85.1397
	step [198/244], loss=106.0169
	step [199/244], loss=97.9258
	step [200/244], loss=75.6243
	step [201/244], loss=76.6400
	step [202/244], loss=82.6741
	step [203/244], loss=64.7791
	step [204/244], loss=84.0748
	step [205/244], loss=96.4565
	step [206/244], loss=74.8393
	step [207/244], loss=81.7378
	step [208/244], loss=77.2374
	step [209/244], loss=77.2759
	step [210/244], loss=81.8893
	step [211/244], loss=57.4670
	step [212/244], loss=61.0655
	step [213/244], loss=92.2510
	step [214/244], loss=67.8258
	step [215/244], loss=56.1542
	step [216/244], loss=65.2381
	step [217/244], loss=53.2187
	step [218/244], loss=86.8629
	step [219/244], loss=69.7564
	step [220/244], loss=69.4874
	step [221/244], loss=64.6897
	step [222/244], loss=75.6349
	step [223/244], loss=70.3400
	step [224/244], loss=69.7242
	step [225/244], loss=71.2451
	step [226/244], loss=86.6198
	step [227/244], loss=83.0542
	step [228/244], loss=85.5148
	step [229/244], loss=85.1991
	step [230/244], loss=88.6988
	step [231/244], loss=78.7326
	step [232/244], loss=75.5551
	step [233/244], loss=89.0192
	step [234/244], loss=61.7298
	step [235/244], loss=78.7033
	step [236/244], loss=69.7154
	step [237/244], loss=72.8683
	step [238/244], loss=80.3334
	step [239/244], loss=67.5882
	step [240/244], loss=74.2101
	step [241/244], loss=82.2607
	step [242/244], loss=71.4235
	step [243/244], loss=69.2257
	step [244/244], loss=45.4519
	Evaluating
	loss=0.0085, precision=0.3309, recall=0.8485, f1=0.4761
saving model as: 2_saved_model.pth
Training epoch 77
	step [1/244], loss=74.7065
	step [2/244], loss=93.2979
	step [3/244], loss=75.2379
	step [4/244], loss=82.5254
	step [5/244], loss=64.1698
	step [6/244], loss=87.8398
	step [7/244], loss=95.0958
	step [8/244], loss=76.6195
	step [9/244], loss=64.5506
	step [10/244], loss=87.8909
	step [11/244], loss=71.7446
	step [12/244], loss=93.0042
	step [13/244], loss=82.1030
	step [14/244], loss=92.9411
	step [15/244], loss=81.3006
	step [16/244], loss=65.0509
	step [17/244], loss=78.1448
	step [18/244], loss=72.6501
	step [19/244], loss=93.8756
	step [20/244], loss=76.1136
	step [21/244], loss=78.8603
	step [22/244], loss=68.9617
	step [23/244], loss=70.0412
	step [24/244], loss=92.1310
	step [25/244], loss=73.1517
	step [26/244], loss=76.0789
	step [27/244], loss=80.2602
	step [28/244], loss=81.8998
	step [29/244], loss=80.3217
	step [30/244], loss=59.0003
	step [31/244], loss=92.9506
	step [32/244], loss=85.6215
	step [33/244], loss=64.1531
	step [34/244], loss=84.9883
	step [35/244], loss=76.6705
	step [36/244], loss=69.4454
	step [37/244], loss=89.0764
	step [38/244], loss=84.1105
	step [39/244], loss=92.5754
	step [40/244], loss=81.7979
	step [41/244], loss=90.9404
	step [42/244], loss=72.6525
	step [43/244], loss=79.6544
	step [44/244], loss=94.3212
	step [45/244], loss=62.9486
	step [46/244], loss=75.2158
	step [47/244], loss=88.5016
	step [48/244], loss=84.1665
	step [49/244], loss=81.3480
	step [50/244], loss=84.6029
	step [51/244], loss=103.5830
	step [52/244], loss=67.4648
	step [53/244], loss=75.6336
	step [54/244], loss=79.7803
	step [55/244], loss=72.5348
	step [56/244], loss=85.1024
	step [57/244], loss=86.6107
	step [58/244], loss=80.4082
	step [59/244], loss=72.2324
	step [60/244], loss=85.5461
	step [61/244], loss=69.6199
	step [62/244], loss=80.8305
	step [63/244], loss=88.1972
	step [64/244], loss=90.3218
	step [65/244], loss=82.7485
	step [66/244], loss=81.3254
	step [67/244], loss=72.3544
	step [68/244], loss=85.6790
	step [69/244], loss=85.0546
	step [70/244], loss=64.9576
	step [71/244], loss=84.7002
	step [72/244], loss=91.0463
	step [73/244], loss=78.9142
	step [74/244], loss=75.4998
	step [75/244], loss=80.0756
	step [76/244], loss=82.1094
	step [77/244], loss=90.6157
	step [78/244], loss=76.8426
	step [79/244], loss=86.6703
	step [80/244], loss=65.3241
	step [81/244], loss=75.0164
	step [82/244], loss=81.1439
	step [83/244], loss=75.5286
	step [84/244], loss=76.4953
	step [85/244], loss=91.3277
	step [86/244], loss=75.1000
	step [87/244], loss=79.0561
	step [88/244], loss=82.6197
	step [89/244], loss=67.0349
	step [90/244], loss=71.0039
	step [91/244], loss=63.2355
	step [92/244], loss=78.9355
	step [93/244], loss=71.5799
	step [94/244], loss=80.3739
	step [95/244], loss=66.6519
	step [96/244], loss=89.5569
	step [97/244], loss=66.3482
	step [98/244], loss=88.8140
	step [99/244], loss=74.9389
	step [100/244], loss=93.6012
	step [101/244], loss=77.2786
	step [102/244], loss=71.8331
	step [103/244], loss=79.8091
	step [104/244], loss=78.6083
	step [105/244], loss=82.8885
	step [106/244], loss=74.6551
	step [107/244], loss=82.4940
	step [108/244], loss=82.5251
	step [109/244], loss=77.3784
	step [110/244], loss=72.5441
	step [111/244], loss=70.9903
	step [112/244], loss=81.5855
	step [113/244], loss=83.9112
	step [114/244], loss=79.0264
	step [115/244], loss=87.6490
	step [116/244], loss=67.6636
	step [117/244], loss=71.4567
	step [118/244], loss=80.1100
	step [119/244], loss=79.4158
	step [120/244], loss=71.7128
	step [121/244], loss=77.4179
	step [122/244], loss=75.5237
	step [123/244], loss=82.7019
	step [124/244], loss=89.5939
	step [125/244], loss=84.1937
	step [126/244], loss=72.4904
	step [127/244], loss=74.7402
	step [128/244], loss=81.8253
	step [129/244], loss=72.4815
	step [130/244], loss=74.1328
	step [131/244], loss=68.0966
	step [132/244], loss=82.3059
	step [133/244], loss=62.8783
	step [134/244], loss=72.1247
	step [135/244], loss=95.3634
	step [136/244], loss=88.5820
	step [137/244], loss=92.4693
	step [138/244], loss=80.0403
	step [139/244], loss=68.6815
	step [140/244], loss=84.1938
	step [141/244], loss=73.6737
	step [142/244], loss=67.2788
	step [143/244], loss=85.1921
	step [144/244], loss=71.5931
	step [145/244], loss=77.1294
	step [146/244], loss=70.1414
	step [147/244], loss=72.5094
	step [148/244], loss=60.0147
	step [149/244], loss=81.2919
	step [150/244], loss=73.2204
	step [151/244], loss=80.2744
	step [152/244], loss=75.5406
	step [153/244], loss=89.4256
	step [154/244], loss=63.1583
	step [155/244], loss=78.7619
	step [156/244], loss=65.3948
	step [157/244], loss=87.2538
	step [158/244], loss=84.4070
	step [159/244], loss=74.6549
	step [160/244], loss=85.1554
	step [161/244], loss=73.3898
	step [162/244], loss=73.4776
	step [163/244], loss=69.7842
	step [164/244], loss=93.0110
	step [165/244], loss=75.3968
	step [166/244], loss=76.9015
	step [167/244], loss=65.3431
	step [168/244], loss=74.8997
	step [169/244], loss=71.1881
	step [170/244], loss=84.4986
	step [171/244], loss=79.3691
	step [172/244], loss=68.2965
	step [173/244], loss=97.3915
	step [174/244], loss=82.7264
	step [175/244], loss=90.1706
	step [176/244], loss=74.7929
	step [177/244], loss=76.1130
	step [178/244], loss=79.1591
	step [179/244], loss=68.5745
	step [180/244], loss=89.3516
	step [181/244], loss=91.4144
	step [182/244], loss=79.1550
	step [183/244], loss=90.1784
	step [184/244], loss=86.0191
	step [185/244], loss=82.8925
	step [186/244], loss=92.6662
	step [187/244], loss=92.6369
	step [188/244], loss=69.3917
	step [189/244], loss=75.5110
	step [190/244], loss=70.0035
	step [191/244], loss=75.0285
	step [192/244], loss=77.3850
	step [193/244], loss=90.0052
	step [194/244], loss=78.8687
	step [195/244], loss=83.3040
	step [196/244], loss=93.9627
	step [197/244], loss=74.1994
	step [198/244], loss=77.8979
	step [199/244], loss=70.3471
	step [200/244], loss=88.9285
	step [201/244], loss=82.2221
	step [202/244], loss=73.7184
	step [203/244], loss=84.7301
	step [204/244], loss=85.0069
	step [205/244], loss=69.9029
	step [206/244], loss=83.7061
	step [207/244], loss=82.3862
	step [208/244], loss=80.7981
	step [209/244], loss=84.1908
	step [210/244], loss=78.1905
	step [211/244], loss=79.4746
	step [212/244], loss=92.1728
	step [213/244], loss=96.9121
	step [214/244], loss=75.6583
	step [215/244], loss=69.0390
	step [216/244], loss=81.4605
	step [217/244], loss=75.3125
	step [218/244], loss=87.1284
	step [219/244], loss=59.2035
	step [220/244], loss=81.5197
	step [221/244], loss=69.3968
	step [222/244], loss=74.3566
	step [223/244], loss=70.7901
	step [224/244], loss=87.9257
	step [225/244], loss=86.7237
	step [226/244], loss=78.9145
	step [227/244], loss=87.6848
	step [228/244], loss=78.8333
	step [229/244], loss=86.7175
	step [230/244], loss=94.3776
	step [231/244], loss=70.0792
	step [232/244], loss=80.8082
	step [233/244], loss=77.8427
	step [234/244], loss=93.7091
	step [235/244], loss=74.8985
	step [236/244], loss=78.9669
	step [237/244], loss=77.6277
	step [238/244], loss=64.6786
	step [239/244], loss=79.8298
	step [240/244], loss=80.1666
	step [241/244], loss=100.6660
	step [242/244], loss=76.5329
	step [243/244], loss=83.5872
	step [244/244], loss=43.3382
	Evaluating
	loss=0.0088, precision=0.3225, recall=0.8567, f1=0.4686
Training epoch 78
	step [1/244], loss=77.9073
	step [2/244], loss=73.3991
	step [3/244], loss=96.0886
	step [4/244], loss=71.4861
	step [5/244], loss=51.2161
	step [6/244], loss=83.6991
	step [7/244], loss=75.1110
	step [8/244], loss=83.8260
	step [9/244], loss=50.9534
	step [10/244], loss=78.4956
	step [11/244], loss=74.1135
	step [12/244], loss=83.9437
	step [13/244], loss=80.2515
	step [14/244], loss=71.0568
	step [15/244], loss=65.7807
	step [16/244], loss=77.6384
	step [17/244], loss=69.3214
	step [18/244], loss=81.7545
	step [19/244], loss=73.4055
	step [20/244], loss=78.3024
	step [21/244], loss=78.5864
	step [22/244], loss=83.1691
	step [23/244], loss=58.0726
	step [24/244], loss=76.1645
	step [25/244], loss=74.8622
	step [26/244], loss=88.6190
	step [27/244], loss=87.1306
	step [28/244], loss=72.1392
	step [29/244], loss=83.4914
	step [30/244], loss=77.5825
	step [31/244], loss=75.4512
	step [32/244], loss=90.8040
	step [33/244], loss=89.4771
	step [34/244], loss=82.2621
	step [35/244], loss=65.1380
	step [36/244], loss=89.1870
	step [37/244], loss=97.2585
	step [38/244], loss=79.6311
	step [39/244], loss=81.4563
	step [40/244], loss=83.0801
	step [41/244], loss=86.4217
	step [42/244], loss=81.4149
	step [43/244], loss=71.4003
	step [44/244], loss=67.3264
	step [45/244], loss=83.0068
	step [46/244], loss=70.0768
	step [47/244], loss=73.9711
	step [48/244], loss=80.5691
	step [49/244], loss=67.2218
	step [50/244], loss=87.6604
	step [51/244], loss=79.6497
	step [52/244], loss=77.3548
	step [53/244], loss=60.2175
	step [54/244], loss=75.1029
	step [55/244], loss=84.0565
	step [56/244], loss=80.2489
	step [57/244], loss=70.5225
	step [58/244], loss=82.6723
	step [59/244], loss=84.0305
	step [60/244], loss=77.2120
	step [61/244], loss=95.8785
	step [62/244], loss=83.9532
	step [63/244], loss=77.8479
	step [64/244], loss=74.9506
	step [65/244], loss=73.6909
	step [66/244], loss=82.9472
	step [67/244], loss=87.2663
	step [68/244], loss=68.5004
	step [69/244], loss=70.4709
	step [70/244], loss=83.7584
	step [71/244], loss=91.9965
	step [72/244], loss=91.8486
	step [73/244], loss=87.3662
	step [74/244], loss=94.7881
	step [75/244], loss=85.4760
	step [76/244], loss=88.7703
	step [77/244], loss=77.0342
	step [78/244], loss=80.1018
	step [79/244], loss=72.9131
	step [80/244], loss=82.6984
	step [81/244], loss=86.8561
	step [82/244], loss=86.1908
	step [83/244], loss=77.9818
	step [84/244], loss=78.8193
	step [85/244], loss=72.3368
	step [86/244], loss=60.6205
	step [87/244], loss=65.7731
	step [88/244], loss=83.3942
	step [89/244], loss=92.4935
	step [90/244], loss=104.5934
	step [91/244], loss=64.9977
	step [92/244], loss=77.7723
	step [93/244], loss=72.3982
	step [94/244], loss=96.9108
	step [95/244], loss=82.1615
	step [96/244], loss=87.6817
	step [97/244], loss=70.7535
	step [98/244], loss=79.4807
	step [99/244], loss=74.9452
	step [100/244], loss=81.1221
	step [101/244], loss=58.8579
	step [102/244], loss=77.1505
	step [103/244], loss=73.5183
	step [104/244], loss=64.0872
	step [105/244], loss=83.0960
	step [106/244], loss=84.8546
	step [107/244], loss=87.9537
	step [108/244], loss=86.2846
	step [109/244], loss=69.8012
	step [110/244], loss=98.6530
	step [111/244], loss=75.9735
	step [112/244], loss=74.3507
	step [113/244], loss=90.0505
	step [114/244], loss=82.9157
	step [115/244], loss=92.4437
	step [116/244], loss=90.4529
	step [117/244], loss=101.1240
	step [118/244], loss=74.4892
	step [119/244], loss=75.2450
	step [120/244], loss=91.8474
	step [121/244], loss=62.3870
	step [122/244], loss=81.4754
	step [123/244], loss=87.5059
	step [124/244], loss=92.6705
	step [125/244], loss=77.7337
	step [126/244], loss=76.2710
	step [127/244], loss=83.8036
	step [128/244], loss=63.6959
	step [129/244], loss=77.1143
	step [130/244], loss=93.7011
	step [131/244], loss=70.6096
	step [132/244], loss=79.0954
	step [133/244], loss=76.0468
	step [134/244], loss=86.4011
	step [135/244], loss=93.9787
	step [136/244], loss=77.2095
	step [137/244], loss=71.2764
	step [138/244], loss=76.6465
	step [139/244], loss=87.7624
	step [140/244], loss=93.3605
	step [141/244], loss=81.7267
	step [142/244], loss=99.2948
	step [143/244], loss=89.6052
	step [144/244], loss=95.7112
	step [145/244], loss=73.3957
	step [146/244], loss=77.5681
	step [147/244], loss=75.8170
	step [148/244], loss=81.3772
	step [149/244], loss=68.9457
	step [150/244], loss=67.3886
	step [151/244], loss=93.8994
	step [152/244], loss=85.0122
	step [153/244], loss=75.0499
	step [154/244], loss=80.7739
	step [155/244], loss=87.1452
	step [156/244], loss=77.6784
	step [157/244], loss=70.3527
	step [158/244], loss=69.0861
	step [159/244], loss=73.4853
	step [160/244], loss=69.6204
	step [161/244], loss=72.2631
	step [162/244], loss=62.5556
	step [163/244], loss=81.3375
	step [164/244], loss=82.9612
	step [165/244], loss=91.5804
	step [166/244], loss=78.7372
	step [167/244], loss=81.4907
	step [168/244], loss=61.8972
	step [169/244], loss=71.8991
	step [170/244], loss=63.7642
	step [171/244], loss=81.1590
	step [172/244], loss=69.9134
	step [173/244], loss=71.9623
	step [174/244], loss=79.5858
	step [175/244], loss=79.4776
	step [176/244], loss=85.3401
	step [177/244], loss=77.9758
	step [178/244], loss=74.6631
	step [179/244], loss=86.5208
	step [180/244], loss=89.1292
	step [181/244], loss=65.0642
	step [182/244], loss=81.9566
	step [183/244], loss=90.2955
	step [184/244], loss=70.7168
	step [185/244], loss=80.8460
	step [186/244], loss=77.3674
	step [187/244], loss=98.9321
	step [188/244], loss=70.6993
	step [189/244], loss=62.4251
	step [190/244], loss=85.3843
	step [191/244], loss=90.4492
	step [192/244], loss=93.6633
	step [193/244], loss=63.2977
	step [194/244], loss=86.9422
	step [195/244], loss=65.3871
	step [196/244], loss=80.9003
	step [197/244], loss=82.0450
	step [198/244], loss=70.4704
	step [199/244], loss=93.7789
	step [200/244], loss=73.0430
	step [201/244], loss=70.9603
	step [202/244], loss=79.7202
	step [203/244], loss=77.5900
	step [204/244], loss=83.9792
	step [205/244], loss=78.2310
	step [206/244], loss=75.9680
	step [207/244], loss=78.3528
	step [208/244], loss=62.0824
	step [209/244], loss=85.8994
	step [210/244], loss=70.0208
	step [211/244], loss=67.1702
	step [212/244], loss=77.8411
	step [213/244], loss=60.7959
	step [214/244], loss=73.1773
	step [215/244], loss=76.7790
	step [216/244], loss=71.4510
	step [217/244], loss=64.6799
	step [218/244], loss=80.1860
	step [219/244], loss=69.5445
	step [220/244], loss=76.3681
	step [221/244], loss=76.6483
	step [222/244], loss=86.0262
	step [223/244], loss=83.6391
	step [224/244], loss=84.1937
	step [225/244], loss=82.0239
	step [226/244], loss=85.9597
	step [227/244], loss=77.0222
	step [228/244], loss=83.9876
	step [229/244], loss=92.9896
	step [230/244], loss=96.9016
	step [231/244], loss=80.9280
	step [232/244], loss=68.9512
	step [233/244], loss=72.2919
	step [234/244], loss=83.5903
	step [235/244], loss=78.3587
	step [236/244], loss=91.6962
	step [237/244], loss=89.6408
	step [238/244], loss=67.2436
	step [239/244], loss=75.0581
	step [240/244], loss=77.0076
	step [241/244], loss=82.0219
	step [242/244], loss=74.9448
	step [243/244], loss=83.0666
	step [244/244], loss=39.9437
	Evaluating
	loss=0.0085, precision=0.3328, recall=0.8715, f1=0.4816
saving model as: 2_saved_model.pth
Training epoch 79
	step [1/244], loss=70.5170
	step [2/244], loss=89.3639
	step [3/244], loss=59.1613
	step [4/244], loss=79.7403
	step [5/244], loss=81.4693
	step [6/244], loss=72.5508
	step [7/244], loss=86.6654
	step [8/244], loss=99.4978
	step [9/244], loss=67.5676
	step [10/244], loss=67.9909
	step [11/244], loss=80.4279
	step [12/244], loss=78.7385
	step [13/244], loss=78.5921
	step [14/244], loss=74.1771
	step [15/244], loss=81.0787
	step [16/244], loss=79.3236
	step [17/244], loss=79.8841
	step [18/244], loss=77.0870
	step [19/244], loss=71.1901
	step [20/244], loss=73.0049
	step [21/244], loss=89.1674
	step [22/244], loss=72.9240
	step [23/244], loss=73.9796
	step [24/244], loss=65.5356
	step [25/244], loss=69.3765
	step [26/244], loss=74.7717
	step [27/244], loss=65.5665
	step [28/244], loss=72.9980
	step [29/244], loss=77.6619
	step [30/244], loss=76.6944
	step [31/244], loss=71.4941
	step [32/244], loss=74.4616
	step [33/244], loss=65.2363
	step [34/244], loss=83.2978
	step [35/244], loss=82.6991
	step [36/244], loss=71.3670
	step [37/244], loss=68.6824
	step [38/244], loss=95.0529
	step [39/244], loss=79.0132
	step [40/244], loss=72.5368
	step [41/244], loss=90.8853
	step [42/244], loss=89.8957
	step [43/244], loss=94.0592
	step [44/244], loss=93.2904
	step [45/244], loss=73.9355
	step [46/244], loss=74.5891
	step [47/244], loss=95.5436
	step [48/244], loss=76.0399
	step [49/244], loss=88.6518
	step [50/244], loss=74.4333
	step [51/244], loss=83.1030
	step [52/244], loss=92.2453
	step [53/244], loss=63.4055
	step [54/244], loss=76.4226
	step [55/244], loss=80.5054
	step [56/244], loss=75.7636
	step [57/244], loss=96.6348
	step [58/244], loss=92.7290
	step [59/244], loss=66.3667
	step [60/244], loss=86.4693
	step [61/244], loss=64.2614
	step [62/244], loss=67.6456
	step [63/244], loss=97.6979
	step [64/244], loss=76.5877
	step [65/244], loss=82.5870
	step [66/244], loss=80.5195
	step [67/244], loss=75.6911
	step [68/244], loss=73.4682
	step [69/244], loss=69.1471
	step [70/244], loss=89.8908
	step [71/244], loss=76.5825
	step [72/244], loss=85.3667
	step [73/244], loss=75.8627
	step [74/244], loss=82.3609
	step [75/244], loss=69.2913
	step [76/244], loss=81.9898
	step [77/244], loss=70.3300
	step [78/244], loss=66.9146
	step [79/244], loss=80.7968
	step [80/244], loss=84.0368
	step [81/244], loss=85.1996
	step [82/244], loss=83.3570
	step [83/244], loss=102.3337
	step [84/244], loss=94.0809
	step [85/244], loss=73.7167
	step [86/244], loss=81.6390
	step [87/244], loss=86.5154
	step [88/244], loss=67.8669
	step [89/244], loss=69.6788
	step [90/244], loss=63.4197
	step [91/244], loss=82.1892
	step [92/244], loss=78.6304
	step [93/244], loss=87.4717
	step [94/244], loss=63.9294
	step [95/244], loss=70.3255
	step [96/244], loss=86.2891
	step [97/244], loss=72.9701
	step [98/244], loss=63.1356
	step [99/244], loss=65.2666
	step [100/244], loss=69.1539
	step [101/244], loss=84.8800
	step [102/244], loss=69.1388
	step [103/244], loss=57.4666
	step [104/244], loss=68.5463
	step [105/244], loss=74.5262
	step [106/244], loss=89.4841
	step [107/244], loss=70.9855
	step [108/244], loss=75.7352
	step [109/244], loss=75.9630
	step [110/244], loss=82.3689
	step [111/244], loss=62.8146
	step [112/244], loss=99.0347
	step [113/244], loss=90.9790
	step [114/244], loss=79.0542
	step [115/244], loss=92.8145
	step [116/244], loss=70.1542
	step [117/244], loss=82.7033
	step [118/244], loss=80.8393
	step [119/244], loss=68.2348
	step [120/244], loss=82.8104
	step [121/244], loss=80.9522
	step [122/244], loss=105.0369
	step [123/244], loss=76.7782
	step [124/244], loss=94.1615
	step [125/244], loss=58.0833
	step [126/244], loss=66.5537
	step [127/244], loss=65.5129
	step [128/244], loss=73.0428
	step [129/244], loss=80.5023
	step [130/244], loss=92.8953
	step [131/244], loss=74.5506
	step [132/244], loss=107.4296
	step [133/244], loss=94.4807
	step [134/244], loss=81.3136
	step [135/244], loss=70.9908
	step [136/244], loss=83.7268
	step [137/244], loss=71.0339
	step [138/244], loss=78.3882
	step [139/244], loss=86.2732
	step [140/244], loss=84.4750
	step [141/244], loss=77.1878
	step [142/244], loss=78.1014
	step [143/244], loss=100.0190
	step [144/244], loss=84.5435
	step [145/244], loss=74.9401
	step [146/244], loss=65.0620
	step [147/244], loss=91.9994
	step [148/244], loss=58.1476
	step [149/244], loss=72.8799
	step [150/244], loss=72.1645
	step [151/244], loss=79.1966
	step [152/244], loss=75.7627
	step [153/244], loss=74.9979
	step [154/244], loss=83.1467
	step [155/244], loss=80.9041
	step [156/244], loss=82.8652
	step [157/244], loss=80.2440
	step [158/244], loss=78.8231
	step [159/244], loss=71.9337
	step [160/244], loss=72.2108
	step [161/244], loss=90.8780
	step [162/244], loss=93.1927
	step [163/244], loss=87.0948
	step [164/244], loss=70.4723
	step [165/244], loss=83.5249
	step [166/244], loss=101.1927
	step [167/244], loss=94.5027
	step [168/244], loss=83.8509
	step [169/244], loss=97.3389
	step [170/244], loss=90.8485
	step [171/244], loss=92.5949
	step [172/244], loss=73.6979
	step [173/244], loss=71.9509
	step [174/244], loss=86.2813
	step [175/244], loss=71.8462
	step [176/244], loss=73.5353
	step [177/244], loss=80.5072
	step [178/244], loss=97.0400
	step [179/244], loss=85.1281
	step [180/244], loss=88.5972
	step [181/244], loss=78.9736
	step [182/244], loss=62.0782
	step [183/244], loss=67.7191
	step [184/244], loss=67.7122
	step [185/244], loss=77.2308
	step [186/244], loss=88.8160
	step [187/244], loss=92.0447
	step [188/244], loss=78.0918
	step [189/244], loss=84.2618
	step [190/244], loss=65.8965
	step [191/244], loss=82.5320
	step [192/244], loss=59.9611
	step [193/244], loss=84.0062
	step [194/244], loss=73.9423
	step [195/244], loss=67.6408
	step [196/244], loss=77.3378
	step [197/244], loss=69.9101
	step [198/244], loss=69.6698
	step [199/244], loss=74.9436
	step [200/244], loss=74.7936
	step [201/244], loss=68.3495
	step [202/244], loss=76.8711
	step [203/244], loss=63.5256
	step [204/244], loss=78.4012
	step [205/244], loss=81.9889
	step [206/244], loss=94.7065
	step [207/244], loss=91.1069
	step [208/244], loss=68.9278
	step [209/244], loss=71.7030
	step [210/244], loss=68.3634
	step [211/244], loss=64.1637
	step [212/244], loss=73.9329
	step [213/244], loss=73.5614
	step [214/244], loss=83.6104
	step [215/244], loss=72.8192
	step [216/244], loss=85.1216
	step [217/244], loss=78.0479
	step [218/244], loss=84.8682
	step [219/244], loss=95.4768
	step [220/244], loss=70.1296
	step [221/244], loss=77.5740
	step [222/244], loss=85.7367
	step [223/244], loss=86.9117
	step [224/244], loss=83.3029
	step [225/244], loss=81.0899
	step [226/244], loss=79.9765
	step [227/244], loss=75.5611
	step [228/244], loss=67.7676
	step [229/244], loss=96.3571
	step [230/244], loss=95.3005
	step [231/244], loss=77.9057
	step [232/244], loss=84.6182
	step [233/244], loss=71.7475
	step [234/244], loss=86.6487
	step [235/244], loss=89.3401
	step [236/244], loss=79.4876
	step [237/244], loss=77.2101
	step [238/244], loss=97.9724
	step [239/244], loss=82.3248
	step [240/244], loss=104.2117
	step [241/244], loss=83.5460
	step [242/244], loss=68.2557
	step [243/244], loss=68.2864
	step [244/244], loss=37.0071
	Evaluating
	loss=0.0084, precision=0.3326, recall=0.8825, f1=0.4831
saving model as: 2_saved_model.pth
Training epoch 80
	step [1/244], loss=67.9700
	step [2/244], loss=74.9029
	step [3/244], loss=84.3264
	step [4/244], loss=83.8099
	step [5/244], loss=86.3411
	step [6/244], loss=65.1860
	step [7/244], loss=86.6118
	step [8/244], loss=65.0621
	step [9/244], loss=70.3255
	step [10/244], loss=57.1634
	step [11/244], loss=92.0440
	step [12/244], loss=74.1605
	step [13/244], loss=70.2007
	step [14/244], loss=73.8406
	step [15/244], loss=75.7107
	step [16/244], loss=61.2839
	step [17/244], loss=77.3573
	step [18/244], loss=72.1758
	step [19/244], loss=89.0492
	step [20/244], loss=77.7576
	step [21/244], loss=93.9246
	step [22/244], loss=89.5446
	step [23/244], loss=72.8495
	step [24/244], loss=90.8819
	step [25/244], loss=80.8979
	step [26/244], loss=80.8672
	step [27/244], loss=73.9574
	step [28/244], loss=92.3082
	step [29/244], loss=67.0736
	step [30/244], loss=82.8050
	step [31/244], loss=98.7300
	step [32/244], loss=81.6850
	step [33/244], loss=67.5679
	step [34/244], loss=80.2235
	step [35/244], loss=70.2588
	step [36/244], loss=92.2926
	step [37/244], loss=92.6579
	step [38/244], loss=71.1479
	step [39/244], loss=82.3616
	step [40/244], loss=84.8636
	step [41/244], loss=83.9201
	step [42/244], loss=100.3167
	step [43/244], loss=76.6697
	step [44/244], loss=78.0884
	step [45/244], loss=79.5320
	step [46/244], loss=81.4802
	step [47/244], loss=94.3796
	step [48/244], loss=65.8114
	step [49/244], loss=73.2309
	step [50/244], loss=69.9795
	step [51/244], loss=85.9686
	step [52/244], loss=85.9871
	step [53/244], loss=73.4024
	step [54/244], loss=76.2766
	step [55/244], loss=72.2222
	step [56/244], loss=90.4160
	step [57/244], loss=71.3066
	step [58/244], loss=73.9180
	step [59/244], loss=91.1577
	step [60/244], loss=76.4999
	step [61/244], loss=86.9274
	step [62/244], loss=84.8440
	step [63/244], loss=74.8771
	step [64/244], loss=95.0388
	step [65/244], loss=71.2379
	step [66/244], loss=82.0471
	step [67/244], loss=72.9537
	step [68/244], loss=84.4170
	step [69/244], loss=73.7840
	step [70/244], loss=79.2089
	step [71/244], loss=63.7979
	step [72/244], loss=69.8551
	step [73/244], loss=81.7484
	step [74/244], loss=73.9444
	step [75/244], loss=50.2103
	step [76/244], loss=83.5545
	step [77/244], loss=80.0017
	step [78/244], loss=75.2694
	step [79/244], loss=60.8535
	step [80/244], loss=80.2833
	step [81/244], loss=93.8965
	step [82/244], loss=81.7349
	step [83/244], loss=66.2848
	step [84/244], loss=73.2617
	step [85/244], loss=67.1468
	step [86/244], loss=61.5701
	step [87/244], loss=80.8756
	step [88/244], loss=68.9825
	step [89/244], loss=69.1074
	step [90/244], loss=96.7409
	step [91/244], loss=82.4720
	step [92/244], loss=112.7999
	step [93/244], loss=61.5569
	step [94/244], loss=64.5117
	step [95/244], loss=95.9452
	step [96/244], loss=53.0137
	step [97/244], loss=85.0142
	step [98/244], loss=64.2031
	step [99/244], loss=81.8933
	step [100/244], loss=82.4022
	step [101/244], loss=76.4582
	step [102/244], loss=85.0187
	step [103/244], loss=78.1003
	step [104/244], loss=71.5068
	step [105/244], loss=80.2560
	step [106/244], loss=70.8187
	step [107/244], loss=81.6778
	step [108/244], loss=81.2848
	step [109/244], loss=93.8601
	step [110/244], loss=79.5180
	step [111/244], loss=76.6475
	step [112/244], loss=56.2900
	step [113/244], loss=78.2627
	step [114/244], loss=73.5615
	step [115/244], loss=79.6724
	step [116/244], loss=83.1880
	step [117/244], loss=77.5604
	step [118/244], loss=89.8225
	step [119/244], loss=79.9147
	step [120/244], loss=80.5517
	step [121/244], loss=74.9265
	step [122/244], loss=74.4906
	step [123/244], loss=83.3626
	step [124/244], loss=81.1762
	step [125/244], loss=87.2078
	step [126/244], loss=70.5283
	step [127/244], loss=86.9111
	step [128/244], loss=88.9697
	step [129/244], loss=81.2489
	step [130/244], loss=85.4701
	step [131/244], loss=90.3363
	step [132/244], loss=72.5730
	step [133/244], loss=78.8291
	step [134/244], loss=96.8243
	step [135/244], loss=68.4793
	step [136/244], loss=83.6223
	step [137/244], loss=78.2883
	step [138/244], loss=72.2397
	step [139/244], loss=91.2353
	step [140/244], loss=88.0216
	step [141/244], loss=66.1449
	step [142/244], loss=84.5606
	step [143/244], loss=66.0107
	step [144/244], loss=65.0750
	step [145/244], loss=80.4328
	step [146/244], loss=69.5406
	step [147/244], loss=73.9041
	step [148/244], loss=79.0752
	step [149/244], loss=82.5139
	step [150/244], loss=69.3088
	step [151/244], loss=82.2813
	step [152/244], loss=66.0730
	step [153/244], loss=77.1725
	step [154/244], loss=66.0397
	step [155/244], loss=84.2236
	step [156/244], loss=80.3668
	step [157/244], loss=76.4863
	step [158/244], loss=87.3529
	step [159/244], loss=79.9944
	step [160/244], loss=69.1108
	step [161/244], loss=77.9509
	step [162/244], loss=71.6146
	step [163/244], loss=65.5719
	step [164/244], loss=84.5416
	step [165/244], loss=82.6621
	step [166/244], loss=90.5752
	step [167/244], loss=87.8665
	step [168/244], loss=96.1269
	step [169/244], loss=82.6116
	step [170/244], loss=83.6060
	step [171/244], loss=66.6806
	step [172/244], loss=67.9957
	step [173/244], loss=86.9037
	step [174/244], loss=78.1624
	step [175/244], loss=74.7528
	step [176/244], loss=68.5741
	step [177/244], loss=102.6937
	step [178/244], loss=88.3584
	step [179/244], loss=71.9178
	step [180/244], loss=73.8952
	step [181/244], loss=98.5751
	step [182/244], loss=82.5523
	step [183/244], loss=74.0469
	step [184/244], loss=83.0895
	step [185/244], loss=74.6183
	step [186/244], loss=87.1174
	step [187/244], loss=71.5309
	step [188/244], loss=85.1895
	step [189/244], loss=95.9076
	step [190/244], loss=60.0960
	step [191/244], loss=81.4324
	step [192/244], loss=67.4261
	step [193/244], loss=70.5502
	step [194/244], loss=82.7840
	step [195/244], loss=91.6605
	step [196/244], loss=79.1122
	step [197/244], loss=83.1876
	step [198/244], loss=80.4906
	step [199/244], loss=76.5376
	step [200/244], loss=101.6881
	step [201/244], loss=84.9987
	step [202/244], loss=81.2589
	step [203/244], loss=75.2724
	step [204/244], loss=75.2759
	step [205/244], loss=71.1885
	step [206/244], loss=79.2748
	step [207/244], loss=88.2556
	step [208/244], loss=79.0499
	step [209/244], loss=64.3518
	step [210/244], loss=87.5032
	step [211/244], loss=85.6954
	step [212/244], loss=67.2398
	step [213/244], loss=93.1253
	step [214/244], loss=65.0499
	step [215/244], loss=80.5230
	step [216/244], loss=68.7684
	step [217/244], loss=86.6875
	step [218/244], loss=82.0971
	step [219/244], loss=71.0841
	step [220/244], loss=87.4710
	step [221/244], loss=63.5140
	step [222/244], loss=83.7993
	step [223/244], loss=78.8210
	step [224/244], loss=72.9159
	step [225/244], loss=73.5271
	step [226/244], loss=78.4691
	step [227/244], loss=69.2322
	step [228/244], loss=80.8132
	step [229/244], loss=88.0868
	step [230/244], loss=75.7065
	step [231/244], loss=79.2384
	step [232/244], loss=71.9249
	step [233/244], loss=72.8872
	step [234/244], loss=99.0166
	step [235/244], loss=77.6396
	step [236/244], loss=78.8756
	step [237/244], loss=83.9375
	step [238/244], loss=83.3731
	step [239/244], loss=84.1513
	step [240/244], loss=82.3503
	step [241/244], loss=85.2048
	step [242/244], loss=75.3455
	step [243/244], loss=72.2380
	step [244/244], loss=38.8789
	Evaluating
	loss=0.0090, precision=0.3220, recall=0.8616, f1=0.4688
Training epoch 81
	step [1/244], loss=82.2335
	step [2/244], loss=83.7317
	step [3/244], loss=69.6868
	step [4/244], loss=84.9852
	step [5/244], loss=82.1992
	step [6/244], loss=70.7936
	step [7/244], loss=76.7480
	step [8/244], loss=76.2022
	step [9/244], loss=72.7496
	step [10/244], loss=84.4944
	step [11/244], loss=70.2006
	step [12/244], loss=66.2177
	step [13/244], loss=74.7161
	step [14/244], loss=96.6396
	step [15/244], loss=86.6242
	step [16/244], loss=84.6689
	step [17/244], loss=74.8602
	step [18/244], loss=76.5773
	step [19/244], loss=77.7229
	step [20/244], loss=84.1767
	step [21/244], loss=92.6133
	step [22/244], loss=78.3475
	step [23/244], loss=75.1011
	step [24/244], loss=69.7849
	step [25/244], loss=77.9051
	step [26/244], loss=71.0176
	step [27/244], loss=88.5540
	step [28/244], loss=73.8829
	step [29/244], loss=76.4994
	step [30/244], loss=89.7721
	step [31/244], loss=73.1860
	step [32/244], loss=90.9395
	step [33/244], loss=89.9134
	step [34/244], loss=91.2229
	step [35/244], loss=76.2409
	step [36/244], loss=61.4790
	step [37/244], loss=69.4614
	step [38/244], loss=70.6977
	step [39/244], loss=66.9245
	step [40/244], loss=89.5211
	step [41/244], loss=74.9743
	step [42/244], loss=88.4800
	step [43/244], loss=66.3681
	step [44/244], loss=80.1536
	step [45/244], loss=57.5452
	step [46/244], loss=77.5346
	step [47/244], loss=68.5057
	step [48/244], loss=71.2930
	step [49/244], loss=69.9071
	step [50/244], loss=72.3410
	step [51/244], loss=68.1108
	step [52/244], loss=63.6425
	step [53/244], loss=82.0101
	step [54/244], loss=80.9376
	step [55/244], loss=89.1947
	step [56/244], loss=68.7965
	step [57/244], loss=71.4682
	step [58/244], loss=78.5156
	step [59/244], loss=70.3524
	step [60/244], loss=77.8431
	step [61/244], loss=79.0782
	step [62/244], loss=83.2590
	step [63/244], loss=78.8547
	step [64/244], loss=75.6272
	step [65/244], loss=77.5398
	step [66/244], loss=71.3474
	step [67/244], loss=88.2873
	step [68/244], loss=53.9465
	step [69/244], loss=76.5264
	step [70/244], loss=75.4135
	step [71/244], loss=68.9428
	step [72/244], loss=81.3053
	step [73/244], loss=71.2692
	step [74/244], loss=80.4900
	step [75/244], loss=71.6873
	step [76/244], loss=101.5206
	step [77/244], loss=79.7005
	step [78/244], loss=91.7692
	step [79/244], loss=85.4302
	step [80/244], loss=73.2576
	step [81/244], loss=72.8288
	step [82/244], loss=58.0901
	step [83/244], loss=62.2216
	step [84/244], loss=85.5509
	step [85/244], loss=71.3597
	step [86/244], loss=82.6896
	step [87/244], loss=76.4076
	step [88/244], loss=70.8822
	step [89/244], loss=96.1329
	step [90/244], loss=84.2983
	step [91/244], loss=69.9102
	step [92/244], loss=74.6202
	step [93/244], loss=71.1721
	step [94/244], loss=83.2845
	step [95/244], loss=87.2054
	step [96/244], loss=91.7560
	step [97/244], loss=71.9931
	step [98/244], loss=83.6620
	step [99/244], loss=85.2743
	step [100/244], loss=69.2353
	step [101/244], loss=105.6941
	step [102/244], loss=78.5142
	step [103/244], loss=86.6825
	step [104/244], loss=76.2688
	step [105/244], loss=85.9397
	step [106/244], loss=77.9445
	step [107/244], loss=81.1297
	step [108/244], loss=73.2079
	step [109/244], loss=56.3749
	step [110/244], loss=56.4564
	step [111/244], loss=52.2797
	step [112/244], loss=102.3250
	step [113/244], loss=72.7942
	step [114/244], loss=68.0298
	step [115/244], loss=78.5804
	step [116/244], loss=64.5372
	step [117/244], loss=94.4006
	step [118/244], loss=68.1980
	step [119/244], loss=50.9596
	step [120/244], loss=73.9645
	step [121/244], loss=79.7953
	step [122/244], loss=77.8453
	step [123/244], loss=75.5502
	step [124/244], loss=67.4938
	step [125/244], loss=88.4564
	step [126/244], loss=78.5380
	step [127/244], loss=88.9286
	step [128/244], loss=62.5319
	step [129/244], loss=88.1863
	step [130/244], loss=106.9234
	step [131/244], loss=83.2574
	step [132/244], loss=74.8270
	step [133/244], loss=71.8117
	step [134/244], loss=77.3683
	step [135/244], loss=84.2633
	step [136/244], loss=80.4589
	step [137/244], loss=73.3609
	step [138/244], loss=81.2816
	step [139/244], loss=75.6033
	step [140/244], loss=78.9560
	step [141/244], loss=87.0534
	step [142/244], loss=71.0416
	step [143/244], loss=77.1940
	step [144/244], loss=79.6909
	step [145/244], loss=81.8777
	step [146/244], loss=89.1025
	step [147/244], loss=88.5654
	step [148/244], loss=86.8318
	step [149/244], loss=95.2125
	step [150/244], loss=69.6305
	step [151/244], loss=71.5217
	step [152/244], loss=64.8538
	step [153/244], loss=48.0955
	step [154/244], loss=75.7423
	step [155/244], loss=65.4330
	step [156/244], loss=80.5355
	step [157/244], loss=70.3313
	step [158/244], loss=75.1638
	step [159/244], loss=75.6729
	step [160/244], loss=83.5581
	step [161/244], loss=71.6033
	step [162/244], loss=71.9739
	step [163/244], loss=88.2932
	step [164/244], loss=66.1760
	step [165/244], loss=95.2983
	step [166/244], loss=94.2981
	step [167/244], loss=79.6093
	step [168/244], loss=93.0079
	step [169/244], loss=78.0385
	step [170/244], loss=74.6053
	step [171/244], loss=72.9698
	step [172/244], loss=84.8394
	step [173/244], loss=84.2490
	step [174/244], loss=83.8403
	step [175/244], loss=72.7797
	step [176/244], loss=81.3622
	step [177/244], loss=78.8038
	step [178/244], loss=64.4861
	step [179/244], loss=86.1024
	step [180/244], loss=70.7491
	step [181/244], loss=83.0137
	step [182/244], loss=75.8428
	step [183/244], loss=71.0069
	step [184/244], loss=66.7112
	step [185/244], loss=78.0050
	step [186/244], loss=69.6745
	step [187/244], loss=81.4210
	step [188/244], loss=89.9798
	step [189/244], loss=77.0630
	step [190/244], loss=73.9192
	step [191/244], loss=90.3010
	step [192/244], loss=68.8563
	step [193/244], loss=91.0488
	step [194/244], loss=66.9535
	step [195/244], loss=97.6357
	step [196/244], loss=89.8943
	step [197/244], loss=75.3535
	step [198/244], loss=82.7906
	step [199/244], loss=69.1890
	step [200/244], loss=90.9736
	step [201/244], loss=79.4693
	step [202/244], loss=84.2386
	step [203/244], loss=82.5414
	step [204/244], loss=71.1059
	step [205/244], loss=85.3581
	step [206/244], loss=81.0856
	step [207/244], loss=90.9159
	step [208/244], loss=74.2122
	step [209/244], loss=89.6900
	step [210/244], loss=75.7156
	step [211/244], loss=82.4677
	step [212/244], loss=79.2217
	step [213/244], loss=92.1282
	step [214/244], loss=83.1953
	step [215/244], loss=85.2179
	step [216/244], loss=85.5443
	step [217/244], loss=69.3317
	step [218/244], loss=72.0124
	step [219/244], loss=88.2175
	step [220/244], loss=87.2004
	step [221/244], loss=66.6594
	step [222/244], loss=91.2116
	step [223/244], loss=72.9732
	step [224/244], loss=80.4532
	step [225/244], loss=79.3197
	step [226/244], loss=89.8725
	step [227/244], loss=70.7444
	step [228/244], loss=76.6848
	step [229/244], loss=68.7457
	step [230/244], loss=75.8627
	step [231/244], loss=94.1149
	step [232/244], loss=84.5444
	step [233/244], loss=75.6280
	step [234/244], loss=70.0797
	step [235/244], loss=83.3971
	step [236/244], loss=80.1164
	step [237/244], loss=79.9716
	step [238/244], loss=75.0587
	step [239/244], loss=81.9868
	step [240/244], loss=66.9774
	step [241/244], loss=86.6179
	step [242/244], loss=74.7756
	step [243/244], loss=113.6526
	step [244/244], loss=54.0667
	Evaluating
	loss=0.0085, precision=0.3317, recall=0.8652, f1=0.4796
Training epoch 82
	step [1/244], loss=77.6593
	step [2/244], loss=83.4247
	step [3/244], loss=88.8166
	step [4/244], loss=85.7672
	step [5/244], loss=58.8898
	step [6/244], loss=77.8588
	step [7/244], loss=77.4083
	step [8/244], loss=86.5193
	step [9/244], loss=75.4761
	step [10/244], loss=76.2885
	step [11/244], loss=69.9444
	step [12/244], loss=73.8541
	step [13/244], loss=83.7997
	step [14/244], loss=85.3791
	step [15/244], loss=78.0684
	step [16/244], loss=70.6745
	step [17/244], loss=73.3508
	step [18/244], loss=76.7239
	step [19/244], loss=81.8823
	step [20/244], loss=82.5852
	step [21/244], loss=62.3935
	step [22/244], loss=86.0067
	step [23/244], loss=65.7773
	step [24/244], loss=78.6334
	step [25/244], loss=73.0272
	step [26/244], loss=78.1825
	step [27/244], loss=70.0635
	step [28/244], loss=70.5786
	step [29/244], loss=78.6028
	step [30/244], loss=76.9715
	step [31/244], loss=65.2081
	step [32/244], loss=63.8837
	step [33/244], loss=84.9047
	step [34/244], loss=87.3054
	step [35/244], loss=68.1208
	step [36/244], loss=70.2849
	step [37/244], loss=86.0329
	step [38/244], loss=73.7077
	step [39/244], loss=62.8808
	step [40/244], loss=89.5325
	step [41/244], loss=64.7375
	step [42/244], loss=47.7950
	step [43/244], loss=65.3606
	step [44/244], loss=84.7620
	step [45/244], loss=101.1898
	step [46/244], loss=88.1869
	step [47/244], loss=87.0567
	step [48/244], loss=71.4298
	step [49/244], loss=82.2544
	step [50/244], loss=88.4821
	step [51/244], loss=81.1206
	step [52/244], loss=80.4687
	step [53/244], loss=79.1122
	step [54/244], loss=79.5545
	step [55/244], loss=73.6691
	step [56/244], loss=82.2529
	step [57/244], loss=90.2707
	step [58/244], loss=91.0341
	step [59/244], loss=79.9955
	step [60/244], loss=69.9336
	step [61/244], loss=65.4111
	step [62/244], loss=80.7395
	step [63/244], loss=84.6389
	step [64/244], loss=61.7810
	step [65/244], loss=84.4244
	step [66/244], loss=76.4435
	step [67/244], loss=87.8658
	step [68/244], loss=62.0835
	step [69/244], loss=85.6616
	step [70/244], loss=75.7735
	step [71/244], loss=91.7901
	step [72/244], loss=85.2232
	step [73/244], loss=76.3723
	step [74/244], loss=80.6395
	step [75/244], loss=76.7398
	step [76/244], loss=71.2201
	step [77/244], loss=80.3049
	step [78/244], loss=99.9331
	step [79/244], loss=75.6750
	step [80/244], loss=84.0454
	step [81/244], loss=81.0365
	step [82/244], loss=77.7930
	step [83/244], loss=76.8566
	step [84/244], loss=82.4863
	step [85/244], loss=58.0977
	step [86/244], loss=74.1477
	step [87/244], loss=64.4320
	step [88/244], loss=89.8865
	step [89/244], loss=66.9393
	step [90/244], loss=72.1926
	step [91/244], loss=60.9605
	step [92/244], loss=75.5413
	step [93/244], loss=85.0406
	step [94/244], loss=68.8917
	step [95/244], loss=100.8234
	step [96/244], loss=80.2920
	step [97/244], loss=66.9517
	step [98/244], loss=66.7614
	step [99/244], loss=94.5007
	step [100/244], loss=96.2722
	step [101/244], loss=88.7714
	step [102/244], loss=95.5129
	step [103/244], loss=74.0447
	step [104/244], loss=85.1023
	step [105/244], loss=69.6531
	step [106/244], loss=83.4240
	step [107/244], loss=81.0789
	step [108/244], loss=74.3354
	step [109/244], loss=79.5957
	step [110/244], loss=75.5031
	step [111/244], loss=74.9570
	step [112/244], loss=84.4970
	step [113/244], loss=60.8908
	step [114/244], loss=69.5293
	step [115/244], loss=82.6186
	step [116/244], loss=71.6769
	step [117/244], loss=74.5010
	step [118/244], loss=87.8593
	step [119/244], loss=72.0283
	step [120/244], loss=71.6393
	step [121/244], loss=78.0224
	step [122/244], loss=87.3991
	step [123/244], loss=75.0379
	step [124/244], loss=88.3211
	step [125/244], loss=78.8587
	step [126/244], loss=71.2068
	step [127/244], loss=74.0673
	step [128/244], loss=83.5434
	step [129/244], loss=81.2383
	step [130/244], loss=74.2283
	step [131/244], loss=97.2329
	step [132/244], loss=78.6028
	step [133/244], loss=65.9197
	step [134/244], loss=70.9744
	step [135/244], loss=71.7796
	step [136/244], loss=85.7782
	step [137/244], loss=77.0268
	step [138/244], loss=75.5511
	step [139/244], loss=72.9639
	step [140/244], loss=73.3116
	step [141/244], loss=60.7547
	step [142/244], loss=80.1202
	step [143/244], loss=74.8909
	step [144/244], loss=83.9177
	step [145/244], loss=80.4554
	step [146/244], loss=80.1390
	step [147/244], loss=69.9024
	step [148/244], loss=67.8828
	step [149/244], loss=71.7500
	step [150/244], loss=75.5175
	step [151/244], loss=69.6102
	step [152/244], loss=83.8045
	step [153/244], loss=72.2462
	step [154/244], loss=79.2588
	step [155/244], loss=92.1039
	step [156/244], loss=77.1132
	step [157/244], loss=93.5237
	step [158/244], loss=88.1787
	step [159/244], loss=77.1154
	step [160/244], loss=79.6505
	step [161/244], loss=83.7359
	step [162/244], loss=79.1809
	step [163/244], loss=71.1467
	step [164/244], loss=73.2202
	step [165/244], loss=75.9866
	step [166/244], loss=71.0006
	step [167/244], loss=69.4331
	step [168/244], loss=67.6723
	step [169/244], loss=83.6345
	step [170/244], loss=80.8070
	step [171/244], loss=87.5006
	step [172/244], loss=93.5471
	step [173/244], loss=70.8660
	step [174/244], loss=77.7474
	step [175/244], loss=87.6866
	step [176/244], loss=74.0910
	step [177/244], loss=102.2574
	step [178/244], loss=77.0302
	step [179/244], loss=80.2773
	step [180/244], loss=74.6460
	step [181/244], loss=78.1590
	step [182/244], loss=73.2823
	step [183/244], loss=77.8453
	step [184/244], loss=81.9975
	step [185/244], loss=81.7413
	step [186/244], loss=82.5467
	step [187/244], loss=84.7277
	step [188/244], loss=94.3091
	step [189/244], loss=65.4148
	step [190/244], loss=86.6523
	step [191/244], loss=66.9805
	step [192/244], loss=81.4588
	step [193/244], loss=87.0082
	step [194/244], loss=74.3819
	step [195/244], loss=77.6427
	step [196/244], loss=83.6795
	step [197/244], loss=69.6834
	step [198/244], loss=66.5354
	step [199/244], loss=71.1483
	step [200/244], loss=84.6107
	step [201/244], loss=73.6030
	step [202/244], loss=83.7751
	step [203/244], loss=73.5776
	step [204/244], loss=83.7371
	step [205/244], loss=83.0813
	step [206/244], loss=97.1012
	step [207/244], loss=89.8227
	step [208/244], loss=95.9704
	step [209/244], loss=83.0781
	step [210/244], loss=78.2681
	step [211/244], loss=80.7327
	step [212/244], loss=70.3119
	step [213/244], loss=75.5353
	step [214/244], loss=81.9313
	step [215/244], loss=69.3384
	step [216/244], loss=74.9862
	step [217/244], loss=81.3192
	step [218/244], loss=80.0550
	step [219/244], loss=82.2939
	step [220/244], loss=74.7086
	step [221/244], loss=67.6764
	step [222/244], loss=69.3180
	step [223/244], loss=69.9214
	step [224/244], loss=70.2615
	step [225/244], loss=61.0850
	step [226/244], loss=65.4813
	step [227/244], loss=63.8998
	step [228/244], loss=64.4302
	step [229/244], loss=81.4374
	step [230/244], loss=72.6013
	step [231/244], loss=75.2686
	step [232/244], loss=75.9449
	step [233/244], loss=79.5900
	step [234/244], loss=94.7624
	step [235/244], loss=68.1625
	step [236/244], loss=82.0425
	step [237/244], loss=82.1154
	step [238/244], loss=77.6930
	step [239/244], loss=84.7649
	step [240/244], loss=95.1056
	step [241/244], loss=70.1731
	step [242/244], loss=73.2990
	step [243/244], loss=64.4779
	step [244/244], loss=56.3069
	Evaluating
	loss=0.0075, precision=0.3665, recall=0.8715, f1=0.5160
saving model as: 2_saved_model.pth
Training epoch 83
	step [1/244], loss=74.1623
	step [2/244], loss=77.7686
	step [3/244], loss=72.6471
	step [4/244], loss=77.2575
	step [5/244], loss=51.7376
	step [6/244], loss=74.3783
	step [7/244], loss=62.0296
	step [8/244], loss=67.0901
	step [9/244], loss=73.8392
	step [10/244], loss=76.8435
	step [11/244], loss=94.4285
	step [12/244], loss=69.4912
	step [13/244], loss=78.5720
	step [14/244], loss=86.0958
	step [15/244], loss=71.0428
	step [16/244], loss=69.7971
	step [17/244], loss=81.0228
	step [18/244], loss=63.8185
	step [19/244], loss=88.2589
	step [20/244], loss=89.4733
	step [21/244], loss=85.6041
	step [22/244], loss=75.0347
	step [23/244], loss=76.3223
	step [24/244], loss=87.8647
	step [25/244], loss=87.2930
	step [26/244], loss=86.1490
	step [27/244], loss=68.3742
	step [28/244], loss=78.8088
	step [29/244], loss=111.1497
	step [30/244], loss=79.7557
	step [31/244], loss=81.0457
	step [32/244], loss=72.7582
	step [33/244], loss=82.2472
	step [34/244], loss=76.3729
	step [35/244], loss=87.8214
	step [36/244], loss=71.8006
	step [37/244], loss=83.6827
	step [38/244], loss=85.1614
	step [39/244], loss=58.0416
	step [40/244], loss=77.2610
	step [41/244], loss=90.4686
	step [42/244], loss=70.7617
	step [43/244], loss=91.2426
	step [44/244], loss=76.6005
	step [45/244], loss=77.4678
	step [46/244], loss=87.5289
	step [47/244], loss=82.4043
	step [48/244], loss=81.2365
	step [49/244], loss=96.6228
	step [50/244], loss=79.8541
	step [51/244], loss=81.2048
	step [52/244], loss=101.3111
	step [53/244], loss=69.6641
	step [54/244], loss=70.6637
	step [55/244], loss=87.0990
	step [56/244], loss=71.9833
	step [57/244], loss=94.9202
	step [58/244], loss=74.4793
	step [59/244], loss=76.6760
	step [60/244], loss=94.9280
	step [61/244], loss=63.7148
	step [62/244], loss=79.0685
	step [63/244], loss=64.1350
	step [64/244], loss=95.9950
	step [65/244], loss=85.1835
	step [66/244], loss=62.2549
	step [67/244], loss=74.9994
	step [68/244], loss=80.7466
	step [69/244], loss=56.7516
	step [70/244], loss=80.3732
	step [71/244], loss=69.0243
	step [72/244], loss=61.3763
	step [73/244], loss=76.6250
	step [74/244], loss=67.7858
	step [75/244], loss=83.0177
	step [76/244], loss=71.0857
	step [77/244], loss=84.5317
	step [78/244], loss=69.3338
	step [79/244], loss=96.6475
	step [80/244], loss=76.6649
	step [81/244], loss=63.3888
	step [82/244], loss=73.3142
	step [83/244], loss=73.5054
	step [84/244], loss=72.5200
	step [85/244], loss=79.7752
	step [86/244], loss=64.0421
	step [87/244], loss=72.2281
	step [88/244], loss=82.3841
	step [89/244], loss=68.6170
	step [90/244], loss=97.5943
	step [91/244], loss=68.8571
	step [92/244], loss=73.5460
	step [93/244], loss=94.4651
	step [94/244], loss=79.1720
	step [95/244], loss=68.2143
	step [96/244], loss=78.9951
	step [97/244], loss=72.5367
	step [98/244], loss=62.5294
	step [99/244], loss=65.8394
	step [100/244], loss=73.5281
	step [101/244], loss=74.5708
	step [102/244], loss=73.7504
	step [103/244], loss=83.8022
	step [104/244], loss=64.8063
	step [105/244], loss=70.1761
	step [106/244], loss=68.3295
	step [107/244], loss=76.9873
	step [108/244], loss=83.0714
	step [109/244], loss=76.7678
	step [110/244], loss=75.7732
	step [111/244], loss=62.1585
	step [112/244], loss=81.6207
	step [113/244], loss=79.7848
	step [114/244], loss=69.2602
	step [115/244], loss=97.9668
	step [116/244], loss=73.6694
	step [117/244], loss=87.3765
	step [118/244], loss=64.7883
	step [119/244], loss=68.7902
	step [120/244], loss=86.4661
	step [121/244], loss=66.2936
	step [122/244], loss=55.1381
	step [123/244], loss=87.5512
	step [124/244], loss=87.9698
	step [125/244], loss=87.9543
	step [126/244], loss=84.7085
	step [127/244], loss=67.5390
	step [128/244], loss=92.3754
	step [129/244], loss=73.4688
	step [130/244], loss=91.1351
	step [131/244], loss=75.4547
	step [132/244], loss=69.0168
	step [133/244], loss=77.8463
	step [134/244], loss=76.1414
	step [135/244], loss=80.2080
	step [136/244], loss=72.8172
	step [137/244], loss=82.0112
	step [138/244], loss=79.8808
	step [139/244], loss=77.5325
	step [140/244], loss=81.5910
	step [141/244], loss=79.0230
	step [142/244], loss=73.4008
	step [143/244], loss=70.9271
	step [144/244], loss=67.3843
	step [145/244], loss=88.4219
	step [146/244], loss=75.4170
	step [147/244], loss=62.5867
	step [148/244], loss=82.8496
	step [149/244], loss=77.6677
	step [150/244], loss=82.5320
	step [151/244], loss=82.6811
	step [152/244], loss=76.6798
	step [153/244], loss=73.4473
	step [154/244], loss=81.1955
	step [155/244], loss=91.3637
	step [156/244], loss=70.3705
	step [157/244], loss=70.8971
	step [158/244], loss=65.6980
	step [159/244], loss=109.7807
	step [160/244], loss=88.1206
	step [161/244], loss=67.3373
	step [162/244], loss=64.2997
	step [163/244], loss=83.3679
	step [164/244], loss=67.1018
	step [165/244], loss=76.8286
	step [166/244], loss=89.7062
	step [167/244], loss=65.7350
	step [168/244], loss=76.9467
	step [169/244], loss=72.7378
	step [170/244], loss=77.0285
	step [171/244], loss=87.3784
	step [172/244], loss=97.3121
	step [173/244], loss=101.5948
	step [174/244], loss=63.5262
	step [175/244], loss=88.3073
	step [176/244], loss=78.5980
	step [177/244], loss=90.0914
	step [178/244], loss=71.6843
	step [179/244], loss=75.3823
	step [180/244], loss=69.1997
	step [181/244], loss=90.6306
	step [182/244], loss=78.1558
	step [183/244], loss=79.5976
	step [184/244], loss=73.5778
	step [185/244], loss=74.7660
	step [186/244], loss=76.9295
	step [187/244], loss=84.9427
	step [188/244], loss=67.5276
	step [189/244], loss=93.9680
	step [190/244], loss=85.1756
	step [191/244], loss=78.6087
	step [192/244], loss=75.8849
	step [193/244], loss=73.5846
	step [194/244], loss=62.0128
	step [195/244], loss=68.9724
	step [196/244], loss=72.6302
	step [197/244], loss=84.3450
	step [198/244], loss=73.3313
	step [199/244], loss=80.4356
	step [200/244], loss=66.3650
	step [201/244], loss=80.9183
	step [202/244], loss=87.9824
	step [203/244], loss=75.3973
	step [204/244], loss=88.4389
	step [205/244], loss=88.5306
	step [206/244], loss=63.3823
	step [207/244], loss=80.6534
	step [208/244], loss=66.0294
	step [209/244], loss=89.4127
	step [210/244], loss=83.7438
	step [211/244], loss=48.3349
	step [212/244], loss=63.3009
	step [213/244], loss=67.6187
	step [214/244], loss=99.5968
	step [215/244], loss=96.8391
	step [216/244], loss=82.7234
	step [217/244], loss=75.0655
	step [218/244], loss=60.9854
	step [219/244], loss=68.4921
	step [220/244], loss=96.1866
	step [221/244], loss=74.9721
	step [222/244], loss=71.2959
	step [223/244], loss=73.9065
	step [224/244], loss=80.4120
	step [225/244], loss=77.0770
	step [226/244], loss=84.1671
	step [227/244], loss=89.6629
	step [228/244], loss=70.0481
	step [229/244], loss=76.3391
	step [230/244], loss=72.6239
	step [231/244], loss=71.3943
	step [232/244], loss=86.5328
	step [233/244], loss=90.3716
	step [234/244], loss=87.3520
	step [235/244], loss=76.2458
	step [236/244], loss=77.3138
	step [237/244], loss=72.0078
	step [238/244], loss=73.3119
	step [239/244], loss=78.5310
	step [240/244], loss=103.0321
	step [241/244], loss=77.8784
	step [242/244], loss=66.3117
	step [243/244], loss=63.3673
	step [244/244], loss=52.9144
	Evaluating
	loss=0.0086, precision=0.3160, recall=0.8466, f1=0.4602
Training epoch 84
	step [1/244], loss=58.1817
	step [2/244], loss=82.4986
	step [3/244], loss=75.7245
	step [4/244], loss=79.1410
	step [5/244], loss=73.7226
	step [6/244], loss=88.5724
	step [7/244], loss=85.0303
	step [8/244], loss=64.7215
	step [9/244], loss=70.6786
	step [10/244], loss=78.9581
	step [11/244], loss=83.8809
	step [12/244], loss=71.9493
	step [13/244], loss=87.3583
	step [14/244], loss=92.0775
	step [15/244], loss=81.9302
	step [16/244], loss=81.1390
	step [17/244], loss=70.4443
	step [18/244], loss=80.5331
	step [19/244], loss=95.0227
	step [20/244], loss=89.6764
	step [21/244], loss=78.4694
	step [22/244], loss=76.8182
	step [23/244], loss=75.6643
	step [24/244], loss=89.0982
	step [25/244], loss=81.8414
	step [26/244], loss=70.7504
	step [27/244], loss=67.5163
	step [28/244], loss=71.8251
	step [29/244], loss=89.4588
	step [30/244], loss=76.9759
	step [31/244], loss=73.7101
	step [32/244], loss=82.9335
	step [33/244], loss=66.7907
	step [34/244], loss=73.9555
	step [35/244], loss=76.0645
	step [36/244], loss=73.6888
	step [37/244], loss=98.6171
	step [38/244], loss=70.4988
	step [39/244], loss=76.9700
	step [40/244], loss=78.2446
	step [41/244], loss=87.5242
	step [42/244], loss=76.2950
	step [43/244], loss=77.1681
	step [44/244], loss=82.8600
	step [45/244], loss=77.0787
	step [46/244], loss=75.9187
	step [47/244], loss=81.8645
	step [48/244], loss=81.5992
	step [49/244], loss=88.7012
	step [50/244], loss=73.5110
	step [51/244], loss=78.0176
	step [52/244], loss=86.7425
	step [53/244], loss=75.8927
	step [54/244], loss=67.1468
	step [55/244], loss=57.1330
	step [56/244], loss=86.7506
	step [57/244], loss=82.6951
	step [58/244], loss=61.3234
	step [59/244], loss=83.0482
	step [60/244], loss=61.0420
	step [61/244], loss=67.6239
	step [62/244], loss=73.6615
	step [63/244], loss=72.7096
	step [64/244], loss=53.1001
	step [65/244], loss=79.3067
	step [66/244], loss=80.7834
	step [67/244], loss=71.9426
	step [68/244], loss=72.2816
	step [69/244], loss=73.6603
	step [70/244], loss=82.7112
	step [71/244], loss=73.1452
	step [72/244], loss=95.8138
	step [73/244], loss=84.1196
	step [74/244], loss=78.8029
	step [75/244], loss=73.4654
	step [76/244], loss=89.3040
	step [77/244], loss=67.8314
	step [78/244], loss=78.1756
	step [79/244], loss=92.3501
	step [80/244], loss=76.2768
	step [81/244], loss=59.7105
	step [82/244], loss=73.0111
	step [83/244], loss=79.4785
	step [84/244], loss=68.5997
	step [85/244], loss=83.6723
	step [86/244], loss=77.3778
	step [87/244], loss=91.2608
	step [88/244], loss=80.4272
	step [89/244], loss=69.6120
	step [90/244], loss=81.4832
	step [91/244], loss=78.4685
	step [92/244], loss=77.7292
	step [93/244], loss=79.5742
	step [94/244], loss=77.0659
	step [95/244], loss=77.8236
	step [96/244], loss=83.9351
	step [97/244], loss=75.4679
	step [98/244], loss=62.0828
	step [99/244], loss=75.5886
	step [100/244], loss=75.9116
	step [101/244], loss=79.2332
	step [102/244], loss=99.6769
	step [103/244], loss=84.2608
	step [104/244], loss=91.9468
	step [105/244], loss=73.4546
	step [106/244], loss=89.3623
	step [107/244], loss=62.0701
	step [108/244], loss=55.4351
	step [109/244], loss=78.2538
	step [110/244], loss=70.0792
	step [111/244], loss=97.6826
	step [112/244], loss=72.8476
	step [113/244], loss=74.1867
	step [114/244], loss=86.1401
	step [115/244], loss=66.3174
	step [116/244], loss=84.0702
	step [117/244], loss=75.7132
	step [118/244], loss=80.9134
	step [119/244], loss=83.4309
	step [120/244], loss=93.0313
	step [121/244], loss=60.5068
	step [122/244], loss=87.2872
	step [123/244], loss=91.5828
	step [124/244], loss=66.8407
	step [125/244], loss=61.6651
	step [126/244], loss=65.5291
	step [127/244], loss=78.5193
	step [128/244], loss=69.1436
	step [129/244], loss=82.7119
	step [130/244], loss=79.7227
	step [131/244], loss=60.8758
	step [132/244], loss=74.0369
	step [133/244], loss=87.9036
	step [134/244], loss=68.3289
	step [135/244], loss=93.5073
	step [136/244], loss=64.6433
	step [137/244], loss=83.2687
	step [138/244], loss=70.2376
	step [139/244], loss=89.5306
	step [140/244], loss=89.7814
	step [141/244], loss=76.1100
	step [142/244], loss=83.5950
	step [143/244], loss=83.3402
	step [144/244], loss=80.7560
	step [145/244], loss=85.2937
	step [146/244], loss=80.7765
	step [147/244], loss=68.1339
	step [148/244], loss=79.8512
	step [149/244], loss=84.7328
	step [150/244], loss=84.5652
	step [151/244], loss=66.4437
	step [152/244], loss=63.6849
	step [153/244], loss=68.3251
	step [154/244], loss=74.3736
	step [155/244], loss=83.2865
	step [156/244], loss=67.8619
	step [157/244], loss=62.8432
	step [158/244], loss=79.9574
	step [159/244], loss=84.5791
	step [160/244], loss=82.2131
	step [161/244], loss=89.4497
	step [162/244], loss=84.5867
	step [163/244], loss=86.9990
	step [164/244], loss=57.7751
	step [165/244], loss=73.5035
	step [166/244], loss=97.7088
	step [167/244], loss=70.2378
	step [168/244], loss=81.7214
	step [169/244], loss=85.6355
	step [170/244], loss=77.0380
	step [171/244], loss=69.4501
	step [172/244], loss=78.0453
	step [173/244], loss=86.2317
	step [174/244], loss=80.2876
	step [175/244], loss=80.0393
	step [176/244], loss=82.8134
	step [177/244], loss=80.0874
	step [178/244], loss=76.2022
	step [179/244], loss=91.2321
	step [180/244], loss=67.5845
	step [181/244], loss=78.9215
	step [182/244], loss=93.9707
	step [183/244], loss=79.3032
	step [184/244], loss=84.3062
	step [185/244], loss=74.5175
	step [186/244], loss=70.8917
	step [187/244], loss=75.3025
	step [188/244], loss=77.2735
	step [189/244], loss=80.0220
	step [190/244], loss=89.3099
	step [191/244], loss=87.2312
	step [192/244], loss=74.7775
	step [193/244], loss=90.6296
	step [194/244], loss=69.7247
	step [195/244], loss=81.3036
	step [196/244], loss=84.8794
	step [197/244], loss=76.0190
	step [198/244], loss=79.1583
	step [199/244], loss=95.6374
	step [200/244], loss=84.5768
	step [201/244], loss=65.0196
	step [202/244], loss=71.7915
	step [203/244], loss=65.6609
	step [204/244], loss=68.5343
	step [205/244], loss=65.3916
	step [206/244], loss=75.0066
	step [207/244], loss=73.1408
	step [208/244], loss=70.9597
	step [209/244], loss=70.8666
	step [210/244], loss=61.1900
	step [211/244], loss=81.5090
	step [212/244], loss=83.5014
	step [213/244], loss=95.0063
	step [214/244], loss=83.1070
	step [215/244], loss=64.7438
	step [216/244], loss=81.8527
	step [217/244], loss=65.2510
	step [218/244], loss=91.4413
	step [219/244], loss=73.8164
	step [220/244], loss=86.7874
	step [221/244], loss=69.9880
	step [222/244], loss=93.4003
	step [223/244], loss=65.3123
	step [224/244], loss=83.5440
	step [225/244], loss=63.4460
	step [226/244], loss=73.7063
	step [227/244], loss=65.7954
	step [228/244], loss=85.3132
	step [229/244], loss=71.2236
	step [230/244], loss=67.5525
	step [231/244], loss=70.7672
	step [232/244], loss=88.8639
	step [233/244], loss=75.3149
	step [234/244], loss=71.3287
	step [235/244], loss=74.4578
	step [236/244], loss=83.7584
	step [237/244], loss=82.8949
	step [238/244], loss=77.7253
	step [239/244], loss=93.9178
	step [240/244], loss=62.4884
	step [241/244], loss=66.1496
	step [242/244], loss=71.4018
	step [243/244], loss=72.8024
	step [244/244], loss=41.1435
	Evaluating
	loss=0.0072, precision=0.3791, recall=0.8684, f1=0.5278
saving model as: 2_saved_model.pth
Training epoch 85
	step [1/244], loss=72.1881
	step [2/244], loss=87.2787
	step [3/244], loss=84.4891
	step [4/244], loss=65.7424
	step [5/244], loss=60.4322
	step [6/244], loss=100.7076
	step [7/244], loss=63.4129
	step [8/244], loss=79.6679
	step [9/244], loss=81.5568
	step [10/244], loss=78.9081
	step [11/244], loss=83.4160
	step [12/244], loss=71.6913
	step [13/244], loss=73.0932
	step [14/244], loss=86.7888
	step [15/244], loss=71.0708
	step [16/244], loss=81.5800
	step [17/244], loss=84.6374
	step [18/244], loss=87.6004
	step [19/244], loss=72.6416
	step [20/244], loss=90.7155
	step [21/244], loss=99.4144
	step [22/244], loss=78.2349
	step [23/244], loss=82.9257
	step [24/244], loss=79.4172
	step [25/244], loss=86.4197
	step [26/244], loss=75.7601
	step [27/244], loss=83.1919
	step [28/244], loss=83.4877
	step [29/244], loss=90.7149
	step [30/244], loss=98.2799
	step [31/244], loss=85.6742
	step [32/244], loss=79.2724
	step [33/244], loss=65.1270
	step [34/244], loss=76.0101
	step [35/244], loss=74.4765
	step [36/244], loss=59.6377
	step [37/244], loss=71.1019
	step [38/244], loss=78.3475
	step [39/244], loss=87.8314
	step [40/244], loss=79.1599
	step [41/244], loss=81.2537
	step [42/244], loss=69.4228
	step [43/244], loss=73.7658
	step [44/244], loss=56.4253
	step [45/244], loss=80.5235
	step [46/244], loss=76.6289
	step [47/244], loss=81.5992
	step [48/244], loss=65.2120
	step [49/244], loss=79.7680
	step [50/244], loss=68.5586
	step [51/244], loss=60.2633
	step [52/244], loss=96.9838
	step [53/244], loss=68.8626
	step [54/244], loss=92.5994
	step [55/244], loss=96.1214
	step [56/244], loss=77.7261
	step [57/244], loss=73.2657
	step [58/244], loss=81.6571
	step [59/244], loss=61.5307
	step [60/244], loss=93.2941
	step [61/244], loss=69.9059
	step [62/244], loss=56.7378
	step [63/244], loss=75.7373
	step [64/244], loss=73.5594
	step [65/244], loss=90.2548
	step [66/244], loss=72.2233
	step [67/244], loss=83.2539
	step [68/244], loss=58.8427
	step [69/244], loss=84.9395
	step [70/244], loss=84.4179
	step [71/244], loss=73.5355
	step [72/244], loss=61.3231
	step [73/244], loss=79.9022
	step [74/244], loss=83.7146
	step [75/244], loss=67.8653
	step [76/244], loss=75.0369
	step [77/244], loss=75.6574
	step [78/244], loss=83.4183
	step [79/244], loss=99.7686
	step [80/244], loss=80.7794
	step [81/244], loss=77.6597
	step [82/244], loss=77.1696
	step [83/244], loss=71.7232
	step [84/244], loss=69.5510
	step [85/244], loss=76.4957
	step [86/244], loss=79.8563
	step [87/244], loss=70.5836
	step [88/244], loss=75.7500
	step [89/244], loss=77.4366
	step [90/244], loss=74.9944
	step [91/244], loss=81.5176
	step [92/244], loss=109.9554
	step [93/244], loss=84.5141
	step [94/244], loss=88.9106
	step [95/244], loss=74.5453
	step [96/244], loss=72.6445
	step [97/244], loss=64.7505
	step [98/244], loss=84.7092
	step [99/244], loss=75.9457
	step [100/244], loss=65.2825
	step [101/244], loss=80.9554
	step [102/244], loss=72.1802
	step [103/244], loss=75.1916
	step [104/244], loss=66.7517
	step [105/244], loss=74.1494
	step [106/244], loss=82.5412
	step [107/244], loss=74.6957
	step [108/244], loss=86.5692
	step [109/244], loss=73.5139
	step [110/244], loss=76.2912
	step [111/244], loss=74.4977
	step [112/244], loss=85.3115
	step [113/244], loss=81.2931
	step [114/244], loss=89.1620
	step [115/244], loss=73.0717
	step [116/244], loss=70.5483
	step [117/244], loss=90.9390
	step [118/244], loss=63.9408
	step [119/244], loss=74.3655
	step [120/244], loss=69.6259
	step [121/244], loss=66.7248
	step [122/244], loss=90.4072
	step [123/244], loss=61.3538
	step [124/244], loss=72.2749
	step [125/244], loss=80.9860
	step [126/244], loss=75.8481
	step [127/244], loss=66.5682
	step [128/244], loss=76.8958
	step [129/244], loss=68.2241
	step [130/244], loss=89.7636
	step [131/244], loss=75.6151
	step [132/244], loss=78.4603
	step [133/244], loss=68.4305
	step [134/244], loss=74.2912
	step [135/244], loss=74.3353
	step [136/244], loss=76.8189
	step [137/244], loss=76.4724
	step [138/244], loss=69.4124
	step [139/244], loss=67.8106
	step [140/244], loss=76.8115
	step [141/244], loss=78.6513
	step [142/244], loss=65.8843
	step [143/244], loss=72.9135
	step [144/244], loss=65.0256
	step [145/244], loss=73.6113
	step [146/244], loss=87.1284
	step [147/244], loss=69.7199
	step [148/244], loss=77.9173
	step [149/244], loss=87.1081
	step [150/244], loss=66.9636
	step [151/244], loss=86.0460
	step [152/244], loss=66.7762
	step [153/244], loss=67.6576
	step [154/244], loss=82.7805
	step [155/244], loss=83.6805
	step [156/244], loss=79.7083
	step [157/244], loss=89.6892
	step [158/244], loss=58.8238
	step [159/244], loss=73.3570
	step [160/244], loss=68.7042
	step [161/244], loss=82.7516
	step [162/244], loss=77.7429
	step [163/244], loss=67.4243
	step [164/244], loss=68.3719
	step [165/244], loss=91.2675
	step [166/244], loss=61.1789
	step [167/244], loss=80.3656
	step [168/244], loss=95.0550
	step [169/244], loss=76.6748
	step [170/244], loss=88.5508
	step [171/244], loss=93.4677
	step [172/244], loss=78.9099
	step [173/244], loss=80.8509
	step [174/244], loss=78.6339
	step [175/244], loss=57.9517
	step [176/244], loss=85.1388
	step [177/244], loss=65.2329
	step [178/244], loss=81.8188
	step [179/244], loss=74.6872
	step [180/244], loss=70.9801
	step [181/244], loss=72.5864
	step [182/244], loss=85.7382
	step [183/244], loss=86.0920
	step [184/244], loss=80.1831
	step [185/244], loss=80.8849
	step [186/244], loss=70.0854
	step [187/244], loss=59.1325
	step [188/244], loss=71.2591
	step [189/244], loss=78.0881
	step [190/244], loss=68.0864
	step [191/244], loss=73.8891
	step [192/244], loss=62.3635
	step [193/244], loss=83.2303
	step [194/244], loss=80.1247
	step [195/244], loss=69.0128
	step [196/244], loss=80.4153
	step [197/244], loss=57.2315
	step [198/244], loss=76.7157
	step [199/244], loss=72.6025
	step [200/244], loss=88.7593
	step [201/244], loss=80.3537
	step [202/244], loss=74.1819
	step [203/244], loss=76.2379
	step [204/244], loss=72.5164
	step [205/244], loss=70.9441
	step [206/244], loss=72.9713
	step [207/244], loss=73.6484
	step [208/244], loss=95.5072
	step [209/244], loss=80.4341
	step [210/244], loss=82.6817
	step [211/244], loss=73.4880
	step [212/244], loss=80.2796
	step [213/244], loss=78.3199
	step [214/244], loss=71.6265
	step [215/244], loss=81.9868
	step [216/244], loss=68.8768
	step [217/244], loss=77.7928
	step [218/244], loss=80.5975
	step [219/244], loss=82.9089
	step [220/244], loss=79.8853
	step [221/244], loss=57.0551
	step [222/244], loss=92.3956
	step [223/244], loss=77.1493
	step [224/244], loss=75.1017
	step [225/244], loss=67.9274
	step [226/244], loss=73.0110
	step [227/244], loss=82.6147
	step [228/244], loss=79.6042
	step [229/244], loss=86.3071
	step [230/244], loss=95.7254
	step [231/244], loss=70.6430
	step [232/244], loss=83.4644
	step [233/244], loss=77.3657
	step [234/244], loss=97.6528
	step [235/244], loss=82.6722
	step [236/244], loss=87.0305
	step [237/244], loss=99.1602
	step [238/244], loss=91.9955
	step [239/244], loss=71.7251
	step [240/244], loss=77.7268
	step [241/244], loss=57.9940
	step [242/244], loss=69.7818
	step [243/244], loss=74.8247
	step [244/244], loss=40.3763
	Evaluating
	loss=0.0078, precision=0.3545, recall=0.8677, f1=0.5034
Training epoch 86
	step [1/244], loss=76.6660
	step [2/244], loss=82.5853
	step [3/244], loss=61.5232
	step [4/244], loss=82.2459
	step [5/244], loss=85.6024
	step [6/244], loss=75.3286
	step [7/244], loss=68.0004
	step [8/244], loss=74.7247
	step [9/244], loss=65.9631
	step [10/244], loss=68.0114
	step [11/244], loss=80.5013
	step [12/244], loss=71.2252
	step [13/244], loss=57.8298
	step [14/244], loss=75.1001
	step [15/244], loss=77.5151
	step [16/244], loss=86.7764
	step [17/244], loss=69.8436
	step [18/244], loss=74.5520
	step [19/244], loss=78.3877
	step [20/244], loss=76.2496
	step [21/244], loss=88.9538
	step [22/244], loss=74.4061
	step [23/244], loss=66.4221
	step [24/244], loss=113.6692
	step [25/244], loss=85.5580
	step [26/244], loss=83.0307
	step [27/244], loss=80.7621
	step [28/244], loss=62.9642
	step [29/244], loss=87.2807
	step [30/244], loss=75.3616
	step [31/244], loss=67.6040
	step [32/244], loss=73.5954
	step [33/244], loss=71.4451
	step [34/244], loss=73.0404
	step [35/244], loss=74.8986
	step [36/244], loss=85.5025
	step [37/244], loss=66.7084
	step [38/244], loss=74.9849
	step [39/244], loss=70.3210
	step [40/244], loss=87.0780
	step [41/244], loss=78.3911
	step [42/244], loss=81.8635
	step [43/244], loss=72.4937
	step [44/244], loss=72.0914
	step [45/244], loss=73.6963
	step [46/244], loss=74.9426
	step [47/244], loss=69.5976
	step [48/244], loss=77.6874
	step [49/244], loss=85.7094
	step [50/244], loss=67.2959
	step [51/244], loss=80.5880
	step [52/244], loss=81.9575
	step [53/244], loss=76.4783
	step [54/244], loss=70.1685
	step [55/244], loss=63.5292
	step [56/244], loss=76.0433
	step [57/244], loss=75.4996
	step [58/244], loss=86.3045
	step [59/244], loss=75.6985
	step [60/244], loss=67.3150
	step [61/244], loss=75.7652
	step [62/244], loss=92.7571
	step [63/244], loss=84.6633
	step [64/244], loss=88.9537
	step [65/244], loss=91.2322
	step [66/244], loss=81.2534
	step [67/244], loss=75.3931
	step [68/244], loss=73.3395
	step [69/244], loss=69.3465
	step [70/244], loss=74.2426
	step [71/244], loss=73.5226
	step [72/244], loss=86.6854
	step [73/244], loss=72.4919
	step [74/244], loss=78.3344
	step [75/244], loss=80.9108
	step [76/244], loss=74.8176
	step [77/244], loss=75.2831
	step [78/244], loss=73.8826
	step [79/244], loss=92.0772
	step [80/244], loss=92.1293
	step [81/244], loss=72.9483
	step [82/244], loss=83.7456
	step [83/244], loss=72.3091
	step [84/244], loss=81.9289
	step [85/244], loss=93.8498
	step [86/244], loss=75.5893
	step [87/244], loss=100.7001
	step [88/244], loss=71.9667
	step [89/244], loss=88.3869
	step [90/244], loss=68.8992
	step [91/244], loss=67.3187
	step [92/244], loss=82.0995
	step [93/244], loss=71.9996
	step [94/244], loss=89.8643
	step [95/244], loss=72.8806
	step [96/244], loss=70.1162
	step [97/244], loss=98.5003
	step [98/244], loss=66.7317
	step [99/244], loss=86.4152
	step [100/244], loss=66.6922
	step [101/244], loss=83.2793
	step [102/244], loss=77.4071
	step [103/244], loss=80.1721
	step [104/244], loss=77.3276
	step [105/244], loss=83.9483
	step [106/244], loss=83.4965
	step [107/244], loss=80.6254
	step [108/244], loss=73.5257
	step [109/244], loss=77.6789
	step [110/244], loss=77.0328
	step [111/244], loss=60.7462
	step [112/244], loss=76.3154
	step [113/244], loss=87.1890
	step [114/244], loss=75.3888
	step [115/244], loss=84.4363
	step [116/244], loss=77.2102
	step [117/244], loss=71.4123
	step [118/244], loss=80.4506
	step [119/244], loss=71.8936
	step [120/244], loss=84.9931
	step [121/244], loss=76.8899
	step [122/244], loss=92.6712
	step [123/244], loss=84.1307
	step [124/244], loss=62.0369
	step [125/244], loss=75.5527
	step [126/244], loss=64.5891
	step [127/244], loss=64.4375
	step [128/244], loss=72.9575
	step [129/244], loss=68.9362
	step [130/244], loss=87.3367
	step [131/244], loss=68.4571
	step [132/244], loss=87.9219
	step [133/244], loss=69.1089
	step [134/244], loss=67.1030
	step [135/244], loss=73.4057
	step [136/244], loss=65.0290
	step [137/244], loss=63.7538
	step [138/244], loss=80.9659
	step [139/244], loss=67.7320
	step [140/244], loss=69.8304
	step [141/244], loss=79.6341
	step [142/244], loss=84.0683
	step [143/244], loss=71.9065
	step [144/244], loss=80.3863
	step [145/244], loss=69.9958
	step [146/244], loss=76.1861
	step [147/244], loss=85.7560
	step [148/244], loss=65.9731
	step [149/244], loss=83.4620
	step [150/244], loss=79.2965
	step [151/244], loss=71.5070
	step [152/244], loss=76.4332
	step [153/244], loss=86.9365
	step [154/244], loss=63.8231
	step [155/244], loss=60.0790
	step [156/244], loss=71.2599
	step [157/244], loss=87.8954
	step [158/244], loss=78.2268
	step [159/244], loss=74.2501
	step [160/244], loss=81.2340
	step [161/244], loss=94.9107
	step [162/244], loss=79.3887
	step [163/244], loss=59.5135
	step [164/244], loss=82.7999
	step [165/244], loss=79.6628
	step [166/244], loss=76.1209
	step [167/244], loss=82.2624
	step [168/244], loss=80.5125
	step [169/244], loss=69.3363
	step [170/244], loss=76.1932
	step [171/244], loss=74.6365
	step [172/244], loss=68.0759
	step [173/244], loss=73.5412
	step [174/244], loss=75.7543
	step [175/244], loss=94.4374
	step [176/244], loss=81.4072
	step [177/244], loss=77.8271
	step [178/244], loss=74.7583
	step [179/244], loss=66.9555
	step [180/244], loss=75.3139
	step [181/244], loss=76.7837
	step [182/244], loss=90.4807
	step [183/244], loss=58.7124
	step [184/244], loss=65.6856
	step [185/244], loss=75.2774
	step [186/244], loss=85.5924
	step [187/244], loss=76.7237
	step [188/244], loss=82.6025
	step [189/244], loss=88.1904
	step [190/244], loss=85.4719
	step [191/244], loss=77.2631
	step [192/244], loss=68.0596
	step [193/244], loss=83.9646
	step [194/244], loss=57.3104
	step [195/244], loss=65.8270
	step [196/244], loss=64.5311
	step [197/244], loss=77.6748
	step [198/244], loss=79.5873
	step [199/244], loss=82.4249
	step [200/244], loss=87.5986
	step [201/244], loss=69.3953
	step [202/244], loss=82.9636
	step [203/244], loss=68.4940
	step [204/244], loss=88.1820
	step [205/244], loss=83.6137
	step [206/244], loss=69.8757
	step [207/244], loss=73.9044
	step [208/244], loss=81.9623
	step [209/244], loss=78.0687
	step [210/244], loss=75.5934
	step [211/244], loss=77.5193
	step [212/244], loss=73.4009
	step [213/244], loss=74.4600
	step [214/244], loss=77.0124
	step [215/244], loss=84.7178
	step [216/244], loss=75.8951
	step [217/244], loss=72.1858
	step [218/244], loss=102.9584
	step [219/244], loss=69.8384
	step [220/244], loss=68.9672
	step [221/244], loss=62.7934
	step [222/244], loss=68.8701
	step [223/244], loss=55.7817
	step [224/244], loss=62.7959
	step [225/244], loss=86.1943
	step [226/244], loss=83.3607
	step [227/244], loss=87.1332
	step [228/244], loss=84.9466
	step [229/244], loss=79.4519
	step [230/244], loss=75.3015
	step [231/244], loss=79.2282
	step [232/244], loss=74.7185
	step [233/244], loss=70.1661
	step [234/244], loss=87.3725
	step [235/244], loss=75.6853
	step [236/244], loss=67.3291
	step [237/244], loss=95.9292
	step [238/244], loss=73.5983
	step [239/244], loss=78.5691
	step [240/244], loss=78.8792
	step [241/244], loss=60.8043
	step [242/244], loss=70.2038
	step [243/244], loss=58.8449
	step [244/244], loss=52.0117
	Evaluating
	loss=0.0077, precision=0.3505, recall=0.8764, f1=0.5007
Training epoch 87
	step [1/244], loss=80.2834
	step [2/244], loss=77.5616
	step [3/244], loss=60.7640
	step [4/244], loss=79.3169
	step [5/244], loss=69.7883
	step [6/244], loss=67.9819
	step [7/244], loss=80.5057
	step [8/244], loss=72.1606
	step [9/244], loss=86.3266
	step [10/244], loss=85.5364
	step [11/244], loss=81.0323
	step [12/244], loss=83.4378
	step [13/244], loss=97.0369
	step [14/244], loss=69.2131
	step [15/244], loss=79.5775
	step [16/244], loss=71.5420
	step [17/244], loss=69.0461
	step [18/244], loss=104.2825
	step [19/244], loss=79.4766
	step [20/244], loss=74.7516
	step [21/244], loss=84.4330
	step [22/244], loss=78.7954
	step [23/244], loss=87.9317
	step [24/244], loss=75.6268
	step [25/244], loss=80.0510
	step [26/244], loss=68.7505
	step [27/244], loss=79.2458
	step [28/244], loss=81.4461
	step [29/244], loss=68.6326
	step [30/244], loss=80.0674
	step [31/244], loss=66.6533
	step [32/244], loss=70.2488
	step [33/244], loss=83.8447
	step [34/244], loss=54.9007
	step [35/244], loss=74.4070
	step [36/244], loss=64.6842
	step [37/244], loss=82.0246
	step [38/244], loss=70.3291
	step [39/244], loss=66.5309
	step [40/244], loss=72.5977
	step [41/244], loss=75.7061
	step [42/244], loss=76.5424
	step [43/244], loss=78.1492
	step [44/244], loss=76.1250
	step [45/244], loss=85.1390
	step [46/244], loss=75.0315
	step [47/244], loss=73.7630
	step [48/244], loss=93.3704
	step [49/244], loss=63.9494
	step [50/244], loss=70.7071
	step [51/244], loss=77.8930
	step [52/244], loss=73.4941
	step [53/244], loss=64.7351
	step [54/244], loss=77.3624
	step [55/244], loss=90.0674
	step [56/244], loss=85.8794
	step [57/244], loss=65.2873
	step [58/244], loss=84.9218
	step [59/244], loss=80.9946
	step [60/244], loss=71.2636
	step [61/244], loss=69.9161
	step [62/244], loss=86.7726
	step [63/244], loss=81.5293
	step [64/244], loss=82.8148
	step [65/244], loss=81.4684
	step [66/244], loss=77.1913
	step [67/244], loss=79.2297
	step [68/244], loss=77.3510
	step [69/244], loss=76.6052
	step [70/244], loss=86.1024
	step [71/244], loss=56.5639
	step [72/244], loss=73.3196
	step [73/244], loss=73.1978
	step [74/244], loss=88.2323
	step [75/244], loss=68.4442
	step [76/244], loss=77.8343
	step [77/244], loss=79.4442
	step [78/244], loss=65.8448
	step [79/244], loss=84.1143
	step [80/244], loss=79.5942
	step [81/244], loss=76.7576
	step [82/244], loss=76.0249
	step [83/244], loss=60.8134
	step [84/244], loss=69.1716
	step [85/244], loss=89.0999
	step [86/244], loss=80.2574
	step [87/244], loss=74.7883
	step [88/244], loss=71.4648
	step [89/244], loss=62.3983
	step [90/244], loss=87.4215
	step [91/244], loss=80.2469
	step [92/244], loss=75.3430
	step [93/244], loss=81.0413
	step [94/244], loss=79.5206
	step [95/244], loss=68.2761
	step [96/244], loss=68.1457
	step [97/244], loss=94.4839
	step [98/244], loss=70.4308
	step [99/244], loss=75.2290
	step [100/244], loss=71.2203
	step [101/244], loss=71.1469
	step [102/244], loss=64.0779
	step [103/244], loss=72.5307
	step [104/244], loss=91.7968
	step [105/244], loss=62.8149
	step [106/244], loss=70.7860
	step [107/244], loss=69.6099
	step [108/244], loss=86.7684
	step [109/244], loss=75.1230
	step [110/244], loss=95.0432
	step [111/244], loss=73.3998
	step [112/244], loss=92.2731
	step [113/244], loss=74.8598
	step [114/244], loss=78.5269
	step [115/244], loss=88.2654
	step [116/244], loss=73.6567
	step [117/244], loss=73.5659
	step [118/244], loss=75.0363
	step [119/244], loss=70.2456
	step [120/244], loss=87.5096
	step [121/244], loss=74.6562
	step [122/244], loss=74.3528
	step [123/244], loss=73.5953
	step [124/244], loss=67.2861
	step [125/244], loss=86.0462
	step [126/244], loss=77.8361
	step [127/244], loss=71.4094
	step [128/244], loss=64.3738
	step [129/244], loss=71.8896
	step [130/244], loss=66.4055
	step [131/244], loss=82.0208
	step [132/244], loss=71.8135
	step [133/244], loss=72.2395
	step [134/244], loss=87.0590
	step [135/244], loss=66.5706
	step [136/244], loss=77.7543
	step [137/244], loss=82.8926
	step [138/244], loss=84.3156
	step [139/244], loss=82.0153
	step [140/244], loss=70.2945
	step [141/244], loss=59.0771
	step [142/244], loss=83.2028
	step [143/244], loss=70.4007
	step [144/244], loss=72.1622
	step [145/244], loss=66.1634
	step [146/244], loss=80.0709
	step [147/244], loss=79.4403
	step [148/244], loss=80.5642
	step [149/244], loss=84.4872
	step [150/244], loss=66.9021
	step [151/244], loss=89.5487
	step [152/244], loss=73.4377
	step [153/244], loss=88.2950
	step [154/244], loss=93.7049
	step [155/244], loss=60.7527
	step [156/244], loss=76.5644
	step [157/244], loss=68.5749
	step [158/244], loss=74.6434
	step [159/244], loss=91.9074
	step [160/244], loss=74.2203
	step [161/244], loss=74.7432
	step [162/244], loss=76.7216
	step [163/244], loss=91.4647
	step [164/244], loss=65.4388
	step [165/244], loss=87.2337
	step [166/244], loss=76.4279
	step [167/244], loss=106.0066
	step [168/244], loss=65.8700
	step [169/244], loss=80.3217
	step [170/244], loss=86.5755
	step [171/244], loss=69.9869
	step [172/244], loss=90.2853
	step [173/244], loss=81.3191
	step [174/244], loss=60.4442
	step [175/244], loss=63.1425
	step [176/244], loss=85.1150
	step [177/244], loss=74.8774
	step [178/244], loss=103.0731
	step [179/244], loss=74.5805
	step [180/244], loss=74.6966
	step [181/244], loss=68.4823
	step [182/244], loss=79.9613
	step [183/244], loss=64.8890
	step [184/244], loss=73.8112
	step [185/244], loss=71.5374
	step [186/244], loss=66.5125
	step [187/244], loss=63.2066
	step [188/244], loss=70.4024
	step [189/244], loss=75.4784
	step [190/244], loss=89.0589
	step [191/244], loss=69.6367
	step [192/244], loss=77.4186
	step [193/244], loss=77.7838
	step [194/244], loss=71.7761
	step [195/244], loss=73.2886
	step [196/244], loss=76.2947
	step [197/244], loss=67.9589
	step [198/244], loss=72.8412
	step [199/244], loss=84.8299
	step [200/244], loss=104.2594
	step [201/244], loss=73.7747
	step [202/244], loss=72.6817
	step [203/244], loss=85.9390
	step [204/244], loss=69.7729
	step [205/244], loss=58.7467
	step [206/244], loss=67.1909
	step [207/244], loss=90.4367
	step [208/244], loss=81.6356
	step [209/244], loss=59.3052
	step [210/244], loss=73.0056
	step [211/244], loss=75.9719
	step [212/244], loss=58.7012
	step [213/244], loss=66.1534
	step [214/244], loss=80.7274
	step [215/244], loss=82.8819
	step [216/244], loss=86.1216
	step [217/244], loss=69.4087
	step [218/244], loss=70.4659
	step [219/244], loss=88.9667
	step [220/244], loss=83.7173
	step [221/244], loss=88.7316
	step [222/244], loss=73.1963
	step [223/244], loss=76.7102
	step [224/244], loss=86.3285
	step [225/244], loss=64.9727
	step [226/244], loss=83.8715
	step [227/244], loss=73.3565
	step [228/244], loss=81.5391
	step [229/244], loss=81.1596
	step [230/244], loss=72.1227
	step [231/244], loss=93.8320
	step [232/244], loss=79.9286
	step [233/244], loss=71.3264
	step [234/244], loss=78.1700
	step [235/244], loss=71.6035
	step [236/244], loss=65.8130
	step [237/244], loss=89.4716
	step [238/244], loss=77.9370
	step [239/244], loss=75.9312
	step [240/244], loss=71.1088
	step [241/244], loss=71.2390
	step [242/244], loss=71.7144
	step [243/244], loss=65.3174
	step [244/244], loss=45.0006
	Evaluating
	loss=0.0083, precision=0.3391, recall=0.8674, f1=0.4876
Training epoch 88
	step [1/244], loss=89.4543
	step [2/244], loss=64.7288
	step [3/244], loss=79.1158
	step [4/244], loss=70.2249
	step [5/244], loss=80.5458
	step [6/244], loss=84.4747
	step [7/244], loss=79.4520
	step [8/244], loss=66.9847
	step [9/244], loss=102.6832
	step [10/244], loss=69.2375
	step [11/244], loss=90.7235
	step [12/244], loss=74.4799
	step [13/244], loss=80.9170
	step [14/244], loss=75.8758
	step [15/244], loss=85.1399
	step [16/244], loss=63.4607
	step [17/244], loss=63.6063
	step [18/244], loss=70.3859
	step [19/244], loss=81.4611
	step [20/244], loss=84.7049
	step [21/244], loss=81.2747
	step [22/244], loss=61.1039
	step [23/244], loss=74.8774
	step [24/244], loss=68.7389
	step [25/244], loss=76.3029
	step [26/244], loss=83.2520
	step [27/244], loss=81.1399
	step [28/244], loss=81.5271
	step [29/244], loss=64.6476
	step [30/244], loss=69.7029
	step [31/244], loss=71.0173
	step [32/244], loss=78.9899
	step [33/244], loss=69.4627
	step [34/244], loss=66.2098
	step [35/244], loss=97.5357
	step [36/244], loss=60.3811
	step [37/244], loss=77.7859
	step [38/244], loss=78.5202
	step [39/244], loss=81.3401
	step [40/244], loss=82.5417
	step [41/244], loss=70.6163
	step [42/244], loss=67.7186
	step [43/244], loss=101.0769
	step [44/244], loss=76.9325
	step [45/244], loss=80.2252
	step [46/244], loss=71.1381
	step [47/244], loss=81.8200
	step [48/244], loss=86.2981
	step [49/244], loss=97.6952
	step [50/244], loss=69.2488
	step [51/244], loss=71.1183
	step [52/244], loss=71.8553
	step [53/244], loss=62.3945
	step [54/244], loss=59.4821
	step [55/244], loss=67.0602
	step [56/244], loss=74.4245
	step [57/244], loss=71.3020
	step [58/244], loss=70.7187
	step [59/244], loss=77.2200
	step [60/244], loss=73.5322
	step [61/244], loss=85.1217
	step [62/244], loss=66.8818
	step [63/244], loss=66.9205
	step [64/244], loss=74.5099
	step [65/244], loss=88.2415
	step [66/244], loss=80.4995
	step [67/244], loss=89.2963
	step [68/244], loss=106.2789
	step [69/244], loss=69.4857
	step [70/244], loss=77.2012
	step [71/244], loss=77.0540
	step [72/244], loss=97.3152
	step [73/244], loss=64.3233
	step [74/244], loss=69.7278
	step [75/244], loss=67.7011
	step [76/244], loss=103.0477
	step [77/244], loss=71.3854
	step [78/244], loss=88.3319
	step [79/244], loss=71.1400
	step [80/244], loss=65.2773
	step [81/244], loss=61.2938
	step [82/244], loss=91.7972
	step [83/244], loss=92.5992
	step [84/244], loss=71.3076
	step [85/244], loss=74.2217
	step [86/244], loss=86.1339
	step [87/244], loss=66.4341
	step [88/244], loss=65.9432
	step [89/244], loss=80.0428
	step [90/244], loss=76.2807
	step [91/244], loss=92.3653
	step [92/244], loss=85.6395
	step [93/244], loss=78.8281
	step [94/244], loss=64.4358
	step [95/244], loss=68.0026
	step [96/244], loss=85.4243
	step [97/244], loss=73.9301
	step [98/244], loss=76.7438
	step [99/244], loss=76.8246
	step [100/244], loss=70.9625
	step [101/244], loss=75.2537
	step [102/244], loss=80.5814
	step [103/244], loss=67.0712
	step [104/244], loss=64.2470
	step [105/244], loss=73.9574
	step [106/244], loss=67.2058
	step [107/244], loss=82.4180
	step [108/244], loss=71.0477
	step [109/244], loss=66.0388
	step [110/244], loss=85.0649
	step [111/244], loss=85.3583
	step [112/244], loss=73.4836
	step [113/244], loss=66.7921
	step [114/244], loss=69.1737
	step [115/244], loss=66.5674
	step [116/244], loss=54.0731
	step [117/244], loss=74.5901
	step [118/244], loss=83.7845
	step [119/244], loss=78.6149
	step [120/244], loss=73.8875
	step [121/244], loss=86.6218
	step [122/244], loss=70.4601
	step [123/244], loss=78.5159
	step [124/244], loss=78.9182
	step [125/244], loss=74.6462
	step [126/244], loss=65.4518
	step [127/244], loss=88.9120
	step [128/244], loss=66.8601
	step [129/244], loss=66.0136
	step [130/244], loss=69.3242
	step [131/244], loss=80.5325
	step [132/244], loss=87.4926
	step [133/244], loss=78.6147
	step [134/244], loss=72.1904
	step [135/244], loss=74.8276
	step [136/244], loss=82.5818
	step [137/244], loss=73.2965
	step [138/244], loss=75.8691
	step [139/244], loss=60.8823
	step [140/244], loss=70.6830
	step [141/244], loss=83.2014
	step [142/244], loss=68.6062
	step [143/244], loss=70.4507
	step [144/244], loss=83.3154
	step [145/244], loss=56.6863
	step [146/244], loss=74.6376
	step [147/244], loss=81.6555
	step [148/244], loss=81.1861
	step [149/244], loss=78.6008
	step [150/244], loss=76.5688
	step [151/244], loss=79.3190
	step [152/244], loss=78.2333
	step [153/244], loss=76.1518
	step [154/244], loss=89.0230
	step [155/244], loss=84.6511
	step [156/244], loss=78.2499
	step [157/244], loss=70.2622
	step [158/244], loss=74.3092
	step [159/244], loss=70.1363
	step [160/244], loss=73.1037
	step [161/244], loss=73.0832
	step [162/244], loss=70.8012
	step [163/244], loss=78.4635
	step [164/244], loss=81.9017
	step [165/244], loss=65.3196
	step [166/244], loss=70.7872
	step [167/244], loss=85.0537
	step [168/244], loss=92.0727
	step [169/244], loss=81.3691
	step [170/244], loss=68.4158
	step [171/244], loss=73.8948
	step [172/244], loss=71.6006
	step [173/244], loss=81.9987
	step [174/244], loss=78.3649
	step [175/244], loss=67.3885
	step [176/244], loss=94.9921
	step [177/244], loss=73.6540
	step [178/244], loss=63.9330
	step [179/244], loss=84.9236
	step [180/244], loss=70.7550
	step [181/244], loss=76.5803
	step [182/244], loss=81.5160
	step [183/244], loss=67.9565
	step [184/244], loss=73.4091
	step [185/244], loss=76.6652
	step [186/244], loss=70.2928
	step [187/244], loss=82.2814
	step [188/244], loss=74.7584
	step [189/244], loss=56.8337
	step [190/244], loss=69.5033
	step [191/244], loss=74.8877
	step [192/244], loss=84.9230
	step [193/244], loss=75.3265
	step [194/244], loss=80.4005
	step [195/244], loss=75.9648
	step [196/244], loss=85.6426
	step [197/244], loss=81.8098
	step [198/244], loss=75.0871
	step [199/244], loss=79.6096
	step [200/244], loss=79.7624
	step [201/244], loss=79.5476
	step [202/244], loss=64.5331
	step [203/244], loss=77.2226
	step [204/244], loss=95.1947
	step [205/244], loss=61.7309
	step [206/244], loss=76.0262
	step [207/244], loss=73.5898
	step [208/244], loss=80.2868
	step [209/244], loss=79.6495
	step [210/244], loss=84.9640
	step [211/244], loss=83.8457
	step [212/244], loss=80.1161
	step [213/244], loss=67.2909
	step [214/244], loss=83.8799
	step [215/244], loss=70.5296
	step [216/244], loss=77.5940
	step [217/244], loss=73.0314
	step [218/244], loss=91.7041
	step [219/244], loss=79.3989
	step [220/244], loss=75.0915
	step [221/244], loss=63.5380
	step [222/244], loss=79.9378
	step [223/244], loss=75.1963
	step [224/244], loss=79.0334
	step [225/244], loss=85.0104
	step [226/244], loss=75.0707
	step [227/244], loss=66.5613
	step [228/244], loss=75.8210
	step [229/244], loss=73.2512
	step [230/244], loss=72.4790
	step [231/244], loss=76.6832
	step [232/244], loss=87.2052
	step [233/244], loss=77.5435
	step [234/244], loss=86.7900
	step [235/244], loss=75.4557
	step [236/244], loss=94.1633
	step [237/244], loss=66.2920
	step [238/244], loss=78.7759
	step [239/244], loss=63.7568
	step [240/244], loss=66.3887
	step [241/244], loss=92.3654
	step [242/244], loss=95.3367
	step [243/244], loss=76.8471
	step [244/244], loss=46.8588
	Evaluating
	loss=0.0092, precision=0.3046, recall=0.8825, f1=0.4529
Training epoch 89
	step [1/244], loss=67.1052
	step [2/244], loss=83.9763
	step [3/244], loss=77.6285
	step [4/244], loss=68.7052
	step [5/244], loss=71.9069
	step [6/244], loss=71.2633
	step [7/244], loss=75.8067
	step [8/244], loss=77.8095
	step [9/244], loss=69.1117
	step [10/244], loss=81.8053
	step [11/244], loss=79.6253
	step [12/244], loss=66.4289
	step [13/244], loss=78.1237
	step [14/244], loss=94.4755
	step [15/244], loss=75.9183
	step [16/244], loss=64.2858
	step [17/244], loss=73.7355
	step [18/244], loss=83.0290
	step [19/244], loss=74.1947
	step [20/244], loss=87.7054
	step [21/244], loss=76.1909
	step [22/244], loss=82.9879
	step [23/244], loss=62.9034
	step [24/244], loss=83.1081
	step [25/244], loss=66.8038
	step [26/244], loss=67.7100
	step [27/244], loss=68.5424
	step [28/244], loss=74.6281
	step [29/244], loss=86.7549
	step [30/244], loss=65.7669
	step [31/244], loss=83.9469
	step [32/244], loss=94.6826
	step [33/244], loss=88.6668
	step [34/244], loss=68.6265
	step [35/244], loss=80.5034
	step [36/244], loss=85.1174
	step [37/244], loss=66.4975
	step [38/244], loss=82.6945
	step [39/244], loss=72.1153
	step [40/244], loss=70.3081
	step [41/244], loss=64.0319
	step [42/244], loss=78.4100
	step [43/244], loss=64.8209
	step [44/244], loss=85.9881
	step [45/244], loss=76.3750
	step [46/244], loss=72.5596
	step [47/244], loss=70.5396
	step [48/244], loss=57.6856
	step [49/244], loss=77.4505
	step [50/244], loss=73.7001
	step [51/244], loss=65.8667
	step [52/244], loss=67.4159
	step [53/244], loss=77.5607
	step [54/244], loss=65.6443
	step [55/244], loss=61.6147
	step [56/244], loss=87.9062
	step [57/244], loss=92.9011
	step [58/244], loss=88.3629
	step [59/244], loss=73.8091
	step [60/244], loss=83.3085
	step [61/244], loss=66.9455
	step [62/244], loss=81.3397
	step [63/244], loss=63.8574
	step [64/244], loss=85.1920
	step [65/244], loss=73.8902
	step [66/244], loss=71.7915
	step [67/244], loss=82.6562
	step [68/244], loss=87.2952
	step [69/244], loss=64.3483
	step [70/244], loss=69.6097
	step [71/244], loss=91.0684
	step [72/244], loss=68.0657
	step [73/244], loss=85.9538
	step [74/244], loss=88.6320
	step [75/244], loss=68.1566
	step [76/244], loss=65.5611
	step [77/244], loss=72.0972
	step [78/244], loss=76.6902
	step [79/244], loss=70.9695
	step [80/244], loss=68.1412
	step [81/244], loss=85.5703
	step [82/244], loss=81.9854
	step [83/244], loss=86.6422
	step [84/244], loss=78.5947
	step [85/244], loss=86.9592
	step [86/244], loss=97.9761
	step [87/244], loss=69.6023
	step [88/244], loss=59.1160
	step [89/244], loss=74.0966
	step [90/244], loss=76.1419
	step [91/244], loss=82.0602
	step [92/244], loss=69.1051
	step [93/244], loss=85.5839
	step [94/244], loss=78.1878
	step [95/244], loss=73.8897
	step [96/244], loss=71.3182
	step [97/244], loss=76.9160
	step [98/244], loss=81.0661
	step [99/244], loss=89.0528
	step [100/244], loss=58.3256
	step [101/244], loss=81.3880
	step [102/244], loss=58.3437
	step [103/244], loss=61.4460
	step [104/244], loss=78.7082
	step [105/244], loss=61.7956
	step [106/244], loss=85.4477
	step [107/244], loss=53.3295
	step [108/244], loss=65.2974
	step [109/244], loss=77.4433
	step [110/244], loss=67.5455
	step [111/244], loss=79.2203
	step [112/244], loss=79.9213
	step [113/244], loss=72.2857
	step [114/244], loss=74.6401
	step [115/244], loss=88.9227
	step [116/244], loss=77.3184
	step [117/244], loss=86.5831
	step [118/244], loss=73.2077
	step [119/244], loss=79.3110
	step [120/244], loss=75.9860
	step [121/244], loss=77.4434
	step [122/244], loss=65.1017
	step [123/244], loss=61.8806
	step [124/244], loss=77.8810
	step [125/244], loss=69.5133
	step [126/244], loss=82.7845
	step [127/244], loss=82.4091
	step [128/244], loss=88.8385
	step [129/244], loss=72.1840
	step [130/244], loss=72.6655
	step [131/244], loss=76.8525
	step [132/244], loss=62.2376
	step [133/244], loss=76.8798
	step [134/244], loss=65.5454
	step [135/244], loss=71.8758
	step [136/244], loss=62.5950
	step [137/244], loss=66.3627
	step [138/244], loss=92.3622
	step [139/244], loss=79.3725
	step [140/244], loss=78.8747
	step [141/244], loss=79.5008
	step [142/244], loss=79.3538
	step [143/244], loss=83.4350
	step [144/244], loss=70.1370
	step [145/244], loss=78.9269
	step [146/244], loss=80.4554
	step [147/244], loss=78.0615
	step [148/244], loss=75.9162
	step [149/244], loss=86.0958
	step [150/244], loss=77.1495
	step [151/244], loss=87.6103
	step [152/244], loss=86.3599
	step [153/244], loss=74.8490
	step [154/244], loss=82.3097
	step [155/244], loss=76.9499
	step [156/244], loss=79.2386
	step [157/244], loss=83.2979
	step [158/244], loss=69.6238
	step [159/244], loss=77.9155
	step [160/244], loss=79.3920
	step [161/244], loss=72.7323
	step [162/244], loss=81.0028
	step [163/244], loss=82.0226
	step [164/244], loss=74.6866
	step [165/244], loss=75.1390
	step [166/244], loss=64.5721
	step [167/244], loss=76.7206
	step [168/244], loss=89.5770
	step [169/244], loss=73.3613
	step [170/244], loss=82.6463
	step [171/244], loss=69.7559
	step [172/244], loss=74.5764
	step [173/244], loss=80.9198
	step [174/244], loss=70.5474
	step [175/244], loss=73.7583
	step [176/244], loss=81.7148
	step [177/244], loss=74.1593
	step [178/244], loss=81.0196
	step [179/244], loss=75.7425
	step [180/244], loss=74.5118
	step [181/244], loss=64.9220
	step [182/244], loss=75.9088
	step [183/244], loss=80.3697
	step [184/244], loss=86.4291
	step [185/244], loss=74.1648
	step [186/244], loss=65.9729
	step [187/244], loss=77.9352
	step [188/244], loss=93.9287
	step [189/244], loss=100.2218
	step [190/244], loss=81.5630
	step [191/244], loss=62.8012
	step [192/244], loss=71.1023
	step [193/244], loss=84.3027
	step [194/244], loss=76.9718
	step [195/244], loss=92.6412
	step [196/244], loss=51.8796
	step [197/244], loss=78.7188
	step [198/244], loss=77.9516
	step [199/244], loss=76.7225
	step [200/244], loss=64.1976
	step [201/244], loss=81.3720
	step [202/244], loss=82.9549
	step [203/244], loss=66.2313
	step [204/244], loss=82.0681
	step [205/244], loss=83.0554
	step [206/244], loss=69.5190
	step [207/244], loss=86.2996
	step [208/244], loss=68.7677
	step [209/244], loss=51.7523
	step [210/244], loss=82.5927
	step [211/244], loss=85.6229
	step [212/244], loss=75.4575
	step [213/244], loss=87.4733
	step [214/244], loss=85.1651
	step [215/244], loss=63.8773
	step [216/244], loss=80.6576
	step [217/244], loss=82.2796
	step [218/244], loss=85.6774
	step [219/244], loss=85.2783
	step [220/244], loss=84.5810
	step [221/244], loss=65.3638
	step [222/244], loss=74.6058
	step [223/244], loss=80.7611
	step [224/244], loss=77.4821
	step [225/244], loss=77.1774
	step [226/244], loss=69.0169
	step [227/244], loss=75.5671
	step [228/244], loss=81.1560
	step [229/244], loss=92.0790
	step [230/244], loss=72.2946
	step [231/244], loss=62.5845
	step [232/244], loss=87.3181
	step [233/244], loss=75.1478
	step [234/244], loss=75.4005
	step [235/244], loss=70.0470
	step [236/244], loss=84.5765
	step [237/244], loss=78.1825
	step [238/244], loss=78.9539
	step [239/244], loss=83.9031
	step [240/244], loss=80.6440
	step [241/244], loss=64.4214
	step [242/244], loss=60.2561
	step [243/244], loss=76.7444
	step [244/244], loss=36.3904
	Evaluating
	loss=0.0076, precision=0.3595, recall=0.8677, f1=0.5083
Training epoch 90
	step [1/244], loss=72.6856
	step [2/244], loss=79.3203
	step [3/244], loss=74.3680
	step [4/244], loss=77.9215
	step [5/244], loss=81.1309
	step [6/244], loss=68.9734
	step [7/244], loss=87.6170
	step [8/244], loss=66.6161
	step [9/244], loss=69.3936
	step [10/244], loss=92.1611
	step [11/244], loss=77.4528
	step [12/244], loss=79.3780
	step [13/244], loss=67.3113
	step [14/244], loss=76.8236
	step [15/244], loss=66.5096
	step [16/244], loss=81.9797
	step [17/244], loss=73.7239
	step [18/244], loss=78.2728
	step [19/244], loss=65.6394
	step [20/244], loss=81.8931
	step [21/244], loss=77.9715
	step [22/244], loss=75.4032
	step [23/244], loss=79.7667
	step [24/244], loss=88.1015
	step [25/244], loss=64.7533
	step [26/244], loss=76.4169
	step [27/244], loss=84.9087
	step [28/244], loss=65.2230
	step [29/244], loss=74.6396
	step [30/244], loss=77.3657
	step [31/244], loss=90.5809
	step [32/244], loss=85.8971
	step [33/244], loss=58.1999
	step [34/244], loss=98.2971
	step [35/244], loss=74.8789
	step [36/244], loss=73.6093
	step [37/244], loss=71.0023
	step [38/244], loss=81.8409
	step [39/244], loss=75.7922
	step [40/244], loss=78.5028
	step [41/244], loss=67.1368
	step [42/244], loss=96.6607
	step [43/244], loss=85.1507
	step [44/244], loss=98.8838
	step [45/244], loss=59.1354
	step [46/244], loss=72.3434
	step [47/244], loss=67.5941
	step [48/244], loss=84.4520
	step [49/244], loss=82.1043
	step [50/244], loss=74.3822
	step [51/244], loss=77.4661
	step [52/244], loss=90.1850
	step [53/244], loss=84.4480
	step [54/244], loss=69.5102
	step [55/244], loss=72.6918
	step [56/244], loss=74.3251
	step [57/244], loss=73.4045
	step [58/244], loss=65.6111
	step [59/244], loss=84.6796
	step [60/244], loss=73.2413
	step [61/244], loss=78.0823
	step [62/244], loss=75.3171
	step [63/244], loss=67.9044
	step [64/244], loss=62.3950
	step [65/244], loss=77.2950
	step [66/244], loss=82.5016
	step [67/244], loss=72.2314
	step [68/244], loss=65.9523
	step [69/244], loss=73.0931
	step [70/244], loss=72.8017
	step [71/244], loss=83.5007
	step [72/244], loss=78.7276
	step [73/244], loss=81.5539
	step [74/244], loss=76.0851
	step [75/244], loss=80.9395
	step [76/244], loss=76.2686
	step [77/244], loss=72.5836
	step [78/244], loss=75.6101
	step [79/244], loss=81.9347
	step [80/244], loss=69.2914
	step [81/244], loss=78.7685
	step [82/244], loss=85.9792
	step [83/244], loss=81.6359
	step [84/244], loss=84.0121
	step [85/244], loss=66.9811
	step [86/244], loss=88.8380
	step [87/244], loss=73.2401
	step [88/244], loss=97.6982
	step [89/244], loss=82.4048
	step [90/244], loss=63.7312
	step [91/244], loss=67.8550
	step [92/244], loss=68.9917
	step [93/244], loss=74.9475
	step [94/244], loss=60.6691
	step [95/244], loss=71.1407
	step [96/244], loss=67.4454
	step [97/244], loss=75.6725
	step [98/244], loss=79.2421
	step [99/244], loss=78.1581
	step [100/244], loss=69.8986
	step [101/244], loss=87.0072
	step [102/244], loss=86.9052
	step [103/244], loss=72.6110
	step [104/244], loss=56.9032
	step [105/244], loss=74.5512
	step [106/244], loss=68.6544
	step [107/244], loss=74.8045
	step [108/244], loss=72.4578
	step [109/244], loss=58.7620
	step [110/244], loss=70.0919
	step [111/244], loss=63.0120
	step [112/244], loss=66.6059
	step [113/244], loss=92.7315
	step [114/244], loss=82.2355
	step [115/244], loss=62.2074
	step [116/244], loss=65.7925
	step [117/244], loss=70.9078
	step [118/244], loss=77.1509
	step [119/244], loss=76.9064
	step [120/244], loss=67.1390
	step [121/244], loss=75.9162
	step [122/244], loss=73.2375
	step [123/244], loss=77.2880
	step [124/244], loss=60.9024
	step [125/244], loss=81.7893
	step [126/244], loss=71.7572
	step [127/244], loss=69.6172
	step [128/244], loss=71.1136
	step [129/244], loss=60.9201
	step [130/244], loss=79.3475
	step [131/244], loss=71.5064
	step [132/244], loss=60.0721
	step [133/244], loss=68.7742
	step [134/244], loss=88.1903
	step [135/244], loss=64.9980
	step [136/244], loss=66.1172
	step [137/244], loss=68.3027
	step [138/244], loss=82.5350
	step [139/244], loss=87.3271
	step [140/244], loss=83.7863
	step [141/244], loss=89.1073
	step [142/244], loss=79.6323
	step [143/244], loss=69.4614
	step [144/244], loss=74.2911
	step [145/244], loss=65.8676
	step [146/244], loss=89.2922
	step [147/244], loss=62.2086
	step [148/244], loss=104.1677
	step [149/244], loss=83.2938
	step [150/244], loss=74.0768
	step [151/244], loss=59.9090
	step [152/244], loss=92.6443
	step [153/244], loss=88.7000
	step [154/244], loss=69.3996
	step [155/244], loss=74.0390
	step [156/244], loss=80.8743
	step [157/244], loss=71.3765
	step [158/244], loss=93.1872
	step [159/244], loss=55.0041
	step [160/244], loss=64.2690
	step [161/244], loss=84.5002
	step [162/244], loss=64.3040
	step [163/244], loss=68.8744
	step [164/244], loss=81.0722
	step [165/244], loss=83.7443
	step [166/244], loss=66.5193
	step [167/244], loss=93.1512
	step [168/244], loss=60.1820
	step [169/244], loss=75.3514
	step [170/244], loss=69.3562
	step [171/244], loss=99.5718
	step [172/244], loss=54.0337
	step [173/244], loss=55.3219
	step [174/244], loss=82.7001
	step [175/244], loss=80.0542
	step [176/244], loss=83.9032
	step [177/244], loss=87.9606
	step [178/244], loss=79.2483
	step [179/244], loss=68.9493
	step [180/244], loss=75.5794
	step [181/244], loss=72.3902
	step [182/244], loss=89.0273
	step [183/244], loss=80.2308
	step [184/244], loss=81.2468
	step [185/244], loss=67.4730
	step [186/244], loss=80.2083
	step [187/244], loss=87.7301
	step [188/244], loss=88.7379
	step [189/244], loss=69.3395
	step [190/244], loss=67.0190
	step [191/244], loss=76.5700
	step [192/244], loss=83.6900
	step [193/244], loss=72.8898
	step [194/244], loss=83.7711
	step [195/244], loss=54.3214
	step [196/244], loss=63.5255
	step [197/244], loss=69.0149
	step [198/244], loss=68.4828
	step [199/244], loss=63.2368
	step [200/244], loss=61.0409
	step [201/244], loss=72.6680
	step [202/244], loss=66.1445
	step [203/244], loss=77.5894
	step [204/244], loss=90.8392
	step [205/244], loss=69.4793
	step [206/244], loss=74.7520
	step [207/244], loss=77.0945
	step [208/244], loss=78.2037
	step [209/244], loss=92.6017
	step [210/244], loss=71.1507
	step [211/244], loss=74.5758
	step [212/244], loss=69.0429
	step [213/244], loss=83.1105
	step [214/244], loss=58.8934
	step [215/244], loss=95.5422
	step [216/244], loss=67.4429
	step [217/244], loss=67.6031
	step [218/244], loss=80.6948
	step [219/244], loss=74.7564
	step [220/244], loss=65.7715
	step [221/244], loss=79.5039
	step [222/244], loss=92.1399
	step [223/244], loss=92.8718
	step [224/244], loss=77.7203
	step [225/244], loss=85.8240
	step [226/244], loss=69.3862
	step [227/244], loss=90.5543
	step [228/244], loss=80.7650
	step [229/244], loss=76.5139
	step [230/244], loss=74.7366
	step [231/244], loss=83.6879
	step [232/244], loss=69.2332
	step [233/244], loss=69.3706
	step [234/244], loss=79.4169
	step [235/244], loss=63.6478
	step [236/244], loss=57.4892
	step [237/244], loss=83.4812
	step [238/244], loss=82.1998
	step [239/244], loss=81.3342
	step [240/244], loss=88.0680
	step [241/244], loss=90.2443
	step [242/244], loss=71.9888
	step [243/244], loss=79.4543
	step [244/244], loss=51.6272
	Evaluating
	loss=0.0081, precision=0.3436, recall=0.8811, f1=0.4944
Training epoch 91
	step [1/244], loss=73.1590
	step [2/244], loss=78.4359
	step [3/244], loss=86.4038
	step [4/244], loss=84.5548
	step [5/244], loss=85.1806
	step [6/244], loss=74.4743
	step [7/244], loss=78.0620
	step [8/244], loss=81.6924
	step [9/244], loss=67.2393
	step [10/244], loss=74.3299
	step [11/244], loss=73.2648
	step [12/244], loss=76.4936
	step [13/244], loss=83.6679
	step [14/244], loss=77.4058
	step [15/244], loss=66.6674
	step [16/244], loss=72.3998
	step [17/244], loss=86.6003
	step [18/244], loss=81.4749
	step [19/244], loss=73.9073
	step [20/244], loss=86.8318
	step [21/244], loss=82.0551
	step [22/244], loss=72.9297
	step [23/244], loss=75.0752
	step [24/244], loss=72.1037
	step [25/244], loss=64.8150
	step [26/244], loss=96.5683
	step [27/244], loss=69.2287
	step [28/244], loss=66.2036
	step [29/244], loss=98.9064
	step [30/244], loss=71.5851
	step [31/244], loss=62.0574
	step [32/244], loss=67.0716
	step [33/244], loss=79.9630
	step [34/244], loss=95.7032
	step [35/244], loss=67.6972
	step [36/244], loss=70.0422
	step [37/244], loss=87.3053
	step [38/244], loss=78.8626
	step [39/244], loss=60.8771
	step [40/244], loss=77.5981
	step [41/244], loss=73.0508
	step [42/244], loss=75.3750
	step [43/244], loss=62.2869
	step [44/244], loss=90.3342
	step [45/244], loss=68.9123
	step [46/244], loss=73.1947
	step [47/244], loss=60.9113
	step [48/244], loss=80.6500
	step [49/244], loss=73.2820
	step [50/244], loss=104.8108
	step [51/244], loss=69.2607
	step [52/244], loss=61.5006
	step [53/244], loss=66.9601
	step [54/244], loss=85.8895
	step [55/244], loss=100.0553
	step [56/244], loss=61.4736
	step [57/244], loss=68.0792
	step [58/244], loss=87.7239
	step [59/244], loss=94.8121
	step [60/244], loss=71.6457
	step [61/244], loss=68.9862
	step [62/244], loss=68.9923
	step [63/244], loss=94.5545
	step [64/244], loss=77.4910
	step [65/244], loss=84.9845
	step [66/244], loss=71.8013
	step [67/244], loss=68.8309
	step [68/244], loss=82.2042
	step [69/244], loss=58.2181
	step [70/244], loss=71.8619
	step [71/244], loss=80.0545
	step [72/244], loss=81.4921
	step [73/244], loss=73.5580
	step [74/244], loss=78.0120
	step [75/244], loss=64.6879
	step [76/244], loss=81.5061
	step [77/244], loss=58.3762
	step [78/244], loss=64.8371
	step [79/244], loss=60.4109
	step [80/244], loss=75.7456
	step [81/244], loss=79.6111
	step [82/244], loss=76.4158
	step [83/244], loss=70.9833
	step [84/244], loss=80.5788
	step [85/244], loss=67.9697
	step [86/244], loss=70.4827
	step [87/244], loss=75.5747
	step [88/244], loss=72.3046
	step [89/244], loss=79.6394
	step [90/244], loss=64.9795
	step [91/244], loss=64.7432
	step [92/244], loss=64.6982
	step [93/244], loss=61.9253
	step [94/244], loss=90.3341
	step [95/244], loss=62.1781
	step [96/244], loss=78.1218
	step [97/244], loss=79.7623
	step [98/244], loss=60.6173
	step [99/244], loss=72.8532
	step [100/244], loss=56.1833
	step [101/244], loss=73.7655
	step [102/244], loss=77.2840
	step [103/244], loss=88.5955
	step [104/244], loss=55.6268
	step [105/244], loss=83.3858
	step [106/244], loss=88.0383
	step [107/244], loss=68.0835
	step [108/244], loss=71.9906
	step [109/244], loss=70.6338
	step [110/244], loss=72.8764
	step [111/244], loss=75.9542
	step [112/244], loss=71.4350
	step [113/244], loss=60.6433
	step [114/244], loss=77.6747
	step [115/244], loss=68.6398
	step [116/244], loss=72.2162
	step [117/244], loss=74.8542
	step [118/244], loss=63.9219
	step [119/244], loss=80.5905
	step [120/244], loss=66.9085
	step [121/244], loss=79.3710
	step [122/244], loss=81.9984
	step [123/244], loss=77.7355
	step [124/244], loss=72.6088
	step [125/244], loss=65.9231
	step [126/244], loss=88.0690
	step [127/244], loss=57.9998
	step [128/244], loss=76.0716
	step [129/244], loss=69.4711
	step [130/244], loss=74.8677
	step [131/244], loss=73.7440
	step [132/244], loss=73.9577
	step [133/244], loss=74.1701
	step [134/244], loss=78.0260
	step [135/244], loss=72.9221
	step [136/244], loss=70.9732
	step [137/244], loss=55.5881
	step [138/244], loss=72.7083
	step [139/244], loss=67.6394
	step [140/244], loss=70.9823
	step [141/244], loss=69.8158
	step [142/244], loss=60.2095
	step [143/244], loss=71.4142
	step [144/244], loss=88.0674
	step [145/244], loss=76.3314
	step [146/244], loss=81.7537
	step [147/244], loss=72.7735
	step [148/244], loss=94.3584
	step [149/244], loss=84.2237
	step [150/244], loss=69.7479
	step [151/244], loss=72.6181
	step [152/244], loss=76.3222
	step [153/244], loss=87.4539
	step [154/244], loss=67.7851
	step [155/244], loss=75.9722
	step [156/244], loss=92.6062
	step [157/244], loss=90.3335
	step [158/244], loss=69.1405
	step [159/244], loss=85.8946
	step [160/244], loss=74.8012
	step [161/244], loss=86.9963
	step [162/244], loss=84.2981
	step [163/244], loss=76.1596
	step [164/244], loss=92.2375
	step [165/244], loss=73.5240
	step [166/244], loss=70.0974
	step [167/244], loss=85.9671
	step [168/244], loss=74.4694
	step [169/244], loss=71.4920
	step [170/244], loss=71.8875
	step [171/244], loss=87.9508
	step [172/244], loss=67.4666
	step [173/244], loss=74.7636
	step [174/244], loss=95.0355
	step [175/244], loss=69.3908
	step [176/244], loss=78.5477
	step [177/244], loss=80.6149
	step [178/244], loss=63.6431
	step [179/244], loss=86.9431
	step [180/244], loss=90.2938
	step [181/244], loss=85.8401
	step [182/244], loss=67.4902
	step [183/244], loss=64.4226
	step [184/244], loss=72.9848
	step [185/244], loss=82.1827
	step [186/244], loss=77.2337
	step [187/244], loss=88.2347
	step [188/244], loss=62.7341
	step [189/244], loss=64.6664
	step [190/244], loss=89.9742
	step [191/244], loss=74.4947
	step [192/244], loss=66.4242
	step [193/244], loss=86.7804
	step [194/244], loss=81.5794
	step [195/244], loss=77.0054
	step [196/244], loss=83.6285
	step [197/244], loss=67.8930
	step [198/244], loss=77.6119
	step [199/244], loss=79.4374
	step [200/244], loss=64.0361
	step [201/244], loss=79.1174
	step [202/244], loss=70.2187
	step [203/244], loss=82.9325
	step [204/244], loss=78.0848
	step [205/244], loss=73.5292
	step [206/244], loss=75.4232
	step [207/244], loss=80.5328
	step [208/244], loss=74.1286
	step [209/244], loss=57.5707
	step [210/244], loss=71.0020
	step [211/244], loss=82.9262
	step [212/244], loss=75.8001
	step [213/244], loss=80.6464
	step [214/244], loss=78.8943
	step [215/244], loss=78.4113
	step [216/244], loss=84.6431
	step [217/244], loss=75.5982
	step [218/244], loss=83.9951
	step [219/244], loss=95.3204
	step [220/244], loss=84.7953
	step [221/244], loss=68.9301
	step [222/244], loss=68.2934
	step [223/244], loss=69.6944
	step [224/244], loss=84.5154
	step [225/244], loss=72.4998
	step [226/244], loss=59.1674
	step [227/244], loss=90.5550
	step [228/244], loss=65.0531
	step [229/244], loss=77.2632
	step [230/244], loss=87.6139
	step [231/244], loss=70.2073
	step [232/244], loss=85.3445
	step [233/244], loss=91.9176
	step [234/244], loss=76.1715
	step [235/244], loss=69.7610
	step [236/244], loss=60.3701
	step [237/244], loss=70.2858
	step [238/244], loss=72.1440
	step [239/244], loss=62.2394
	step [240/244], loss=66.5782
	step [241/244], loss=95.5181
	step [242/244], loss=73.8670
	step [243/244], loss=86.3055
	step [244/244], loss=44.7044
	Evaluating
	loss=0.0073, precision=0.3713, recall=0.8795, f1=0.5222
Training epoch 92
	step [1/244], loss=87.2326
	step [2/244], loss=63.0788
	step [3/244], loss=85.0889
	step [4/244], loss=70.9940
	step [5/244], loss=87.4853
	step [6/244], loss=72.8174
	step [7/244], loss=68.7431
	step [8/244], loss=69.8476
	step [9/244], loss=69.6430
	step [10/244], loss=70.4318
	step [11/244], loss=68.1145
	step [12/244], loss=67.2334
	step [13/244], loss=71.3971
	step [14/244], loss=77.2958
	step [15/244], loss=72.5465
	step [16/244], loss=64.6748
	step [17/244], loss=56.5887
	step [18/244], loss=79.8724
	step [19/244], loss=76.0085
	step [20/244], loss=67.7575
	step [21/244], loss=60.9174
	step [22/244], loss=84.0851
	step [23/244], loss=75.3754
	step [24/244], loss=77.9812
	step [25/244], loss=73.1261
	step [26/244], loss=73.4426
	step [27/244], loss=72.4130
	step [28/244], loss=89.0366
	step [29/244], loss=74.9694
	step [30/244], loss=86.6656
	step [31/244], loss=97.8671
	step [32/244], loss=81.8154
	step [33/244], loss=70.4416
	step [34/244], loss=73.6956
	step [35/244], loss=73.9552
	step [36/244], loss=86.0109
	step [37/244], loss=66.7462
	step [38/244], loss=82.7804
	step [39/244], loss=86.1874
	step [40/244], loss=72.8891
	step [41/244], loss=64.9225
	step [42/244], loss=92.8984
	step [43/244], loss=84.2712
	step [44/244], loss=93.0982
	step [45/244], loss=60.5495
	step [46/244], loss=80.8046
	step [47/244], loss=91.6107
	step [48/244], loss=92.4980
	step [49/244], loss=73.3686
	step [50/244], loss=86.2368
	step [51/244], loss=67.8482
	step [52/244], loss=65.5291
	step [53/244], loss=65.9454
	step [54/244], loss=70.3404
	step [55/244], loss=89.1964
	step [56/244], loss=72.7276
	step [57/244], loss=69.6565
	step [58/244], loss=67.4307
	step [59/244], loss=74.5520
	step [60/244], loss=85.5986
	step [61/244], loss=58.9444
	step [62/244], loss=81.2826
	step [63/244], loss=68.9293
	step [64/244], loss=98.7756
	step [65/244], loss=70.1029
	step [66/244], loss=82.1176
	step [67/244], loss=81.5686
	step [68/244], loss=81.3946
	step [69/244], loss=75.0610
	step [70/244], loss=79.4072
	step [71/244], loss=82.1957
	step [72/244], loss=74.0498
	step [73/244], loss=71.6201
	step [74/244], loss=73.8361
	step [75/244], loss=78.8008
	step [76/244], loss=97.6599
	step [77/244], loss=86.1224
	step [78/244], loss=72.1996
	step [79/244], loss=72.6682
	step [80/244], loss=82.0534
	step [81/244], loss=70.6674
	step [82/244], loss=76.4619
	step [83/244], loss=74.5013
	step [84/244], loss=67.3058
	step [85/244], loss=60.0580
	step [86/244], loss=80.8360
	step [87/244], loss=68.3077
	step [88/244], loss=60.6195
	step [89/244], loss=83.9091
	step [90/244], loss=72.2613
	step [91/244], loss=71.6771
	step [92/244], loss=73.1964
	step [93/244], loss=61.7271
	step [94/244], loss=66.2966
	step [95/244], loss=61.2195
	step [96/244], loss=70.2039
	step [97/244], loss=72.1527
	step [98/244], loss=78.4350
	step [99/244], loss=64.1187
	step [100/244], loss=79.2836
	step [101/244], loss=87.0975
	step [102/244], loss=71.0133
	step [103/244], loss=61.2107
	step [104/244], loss=65.1839
	step [105/244], loss=79.0203
	step [106/244], loss=68.6223
	step [107/244], loss=85.9046
	step [108/244], loss=65.4402
	step [109/244], loss=72.8416
	step [110/244], loss=82.7096
	step [111/244], loss=63.1151
	step [112/244], loss=77.6910
	step [113/244], loss=70.3135
	step [114/244], loss=82.1213
	step [115/244], loss=93.8264
	step [116/244], loss=92.7227
	step [117/244], loss=81.6705
	step [118/244], loss=86.2938
	step [119/244], loss=85.9708
	step [120/244], loss=66.3253
	step [121/244], loss=82.7207
	step [122/244], loss=69.9266
	step [123/244], loss=72.4297
	step [124/244], loss=80.8027
	step [125/244], loss=85.3438
	step [126/244], loss=74.5487
	step [127/244], loss=71.7681
	step [128/244], loss=78.4041
	step [129/244], loss=84.3502
	step [130/244], loss=63.2408
	step [131/244], loss=73.2615
	step [132/244], loss=67.6664
	step [133/244], loss=66.4402
	step [134/244], loss=81.3645
	step [135/244], loss=74.5145
	step [136/244], loss=75.1506
	step [137/244], loss=74.1106
	step [138/244], loss=72.9829
	step [139/244], loss=90.5267
	step [140/244], loss=90.4600
	step [141/244], loss=68.6045
	step [142/244], loss=80.8130
	step [143/244], loss=83.3235
	step [144/244], loss=50.3995
	step [145/244], loss=107.2996
	step [146/244], loss=70.9393
	step [147/244], loss=79.0236
	step [148/244], loss=70.3362
	step [149/244], loss=56.2608
	step [150/244], loss=82.6896
	step [151/244], loss=69.3680
	step [152/244], loss=55.1545
	step [153/244], loss=55.4006
	step [154/244], loss=90.2826
	step [155/244], loss=61.9483
	step [156/244], loss=73.8497
	step [157/244], loss=65.9167
	step [158/244], loss=82.1053
	step [159/244], loss=64.2902
	step [160/244], loss=73.9991
	step [161/244], loss=79.7079
	step [162/244], loss=71.1883
	step [163/244], loss=63.7750
	step [164/244], loss=70.6162
	step [165/244], loss=84.0199
	step [166/244], loss=81.8262
	step [167/244], loss=68.4327
	step [168/244], loss=70.8537
	step [169/244], loss=73.7329
	step [170/244], loss=76.5067
	step [171/244], loss=99.0459
	step [172/244], loss=81.2938
	step [173/244], loss=81.7423
	step [174/244], loss=77.8871
	step [175/244], loss=70.4467
	step [176/244], loss=71.3019
	step [177/244], loss=56.2127
	step [178/244], loss=62.5948
	step [179/244], loss=81.8407
	step [180/244], loss=85.7263
	step [181/244], loss=84.9038
	step [182/244], loss=73.4265
	step [183/244], loss=78.3044
	step [184/244], loss=95.6017
	step [185/244], loss=75.8960
	step [186/244], loss=57.9846
	step [187/244], loss=78.0313
	step [188/244], loss=63.9306
	step [189/244], loss=81.6368
	step [190/244], loss=70.7050
	step [191/244], loss=84.6201
	step [192/244], loss=100.3862
	step [193/244], loss=57.5971
	step [194/244], loss=79.9529
	step [195/244], loss=67.4333
	step [196/244], loss=85.9731
	step [197/244], loss=82.5298
	step [198/244], loss=75.6470
	step [199/244], loss=68.5118
	step [200/244], loss=76.4364
	step [201/244], loss=81.8362
	step [202/244], loss=69.8607
	step [203/244], loss=75.2247
	step [204/244], loss=83.9722
	step [205/244], loss=70.7076
	step [206/244], loss=79.0480
	step [207/244], loss=86.0461
	step [208/244], loss=63.6564
	step [209/244], loss=80.3947
	step [210/244], loss=66.6100
	step [211/244], loss=75.9267
	step [212/244], loss=80.4821
	step [213/244], loss=63.8809
	step [214/244], loss=66.9641
	step [215/244], loss=72.9485
	step [216/244], loss=67.3741
	step [217/244], loss=71.5333
	step [218/244], loss=73.7677
	step [219/244], loss=67.4321
	step [220/244], loss=72.8182
	step [221/244], loss=69.0220
	step [222/244], loss=83.5246
	step [223/244], loss=85.9604
	step [224/244], loss=61.6581
	step [225/244], loss=78.4408
	step [226/244], loss=78.5824
	step [227/244], loss=66.9863
	step [228/244], loss=87.6187
	step [229/244], loss=68.8439
	step [230/244], loss=85.3426
	step [231/244], loss=80.3730
	step [232/244], loss=73.5982
	step [233/244], loss=75.2317
	step [234/244], loss=64.0688
	step [235/244], loss=95.2225
	step [236/244], loss=63.3796
	step [237/244], loss=73.2619
	step [238/244], loss=61.6374
	step [239/244], loss=68.9119
	step [240/244], loss=91.5616
	step [241/244], loss=76.3385
	step [242/244], loss=78.5840
	step [243/244], loss=69.8193
	step [244/244], loss=48.2135
	Evaluating
	loss=0.0075, precision=0.3619, recall=0.8778, f1=0.5125
Training epoch 93
	step [1/244], loss=68.0255
	step [2/244], loss=78.3678
	step [3/244], loss=78.2200
	step [4/244], loss=87.5055
	step [5/244], loss=68.2383
	step [6/244], loss=74.2738
	step [7/244], loss=67.0676
	step [8/244], loss=55.2017
	step [9/244], loss=82.6865
	step [10/244], loss=77.4424
	step [11/244], loss=64.9389
	step [12/244], loss=78.5944
	step [13/244], loss=92.9863
	step [14/244], loss=59.5210
	step [15/244], loss=58.4552
	step [16/244], loss=75.9510
	step [17/244], loss=73.4310
	step [18/244], loss=64.9334
	step [19/244], loss=76.6348
	step [20/244], loss=75.0532
	step [21/244], loss=100.5311
	step [22/244], loss=75.9458
	step [23/244], loss=79.9650
	step [24/244], loss=78.6845
	step [25/244], loss=66.9330
	step [26/244], loss=81.0700
	step [27/244], loss=79.1229
	step [28/244], loss=80.7162
	step [29/244], loss=74.6924
	step [30/244], loss=70.8744
	step [31/244], loss=82.7693
	step [32/244], loss=69.3322
	step [33/244], loss=69.4048
	step [34/244], loss=72.5072
	step [35/244], loss=64.6983
	step [36/244], loss=76.9301
	step [37/244], loss=82.1048
	step [38/244], loss=74.7665
	step [39/244], loss=72.8577
	step [40/244], loss=81.1900
	step [41/244], loss=70.3163
	step [42/244], loss=74.5026
	step [43/244], loss=77.9295
	step [44/244], loss=79.8526
	step [45/244], loss=57.3114
	step [46/244], loss=89.1424
	step [47/244], loss=80.9479
	step [48/244], loss=73.2633
	step [49/244], loss=60.1824
	step [50/244], loss=78.9238
	step [51/244], loss=76.0343
	step [52/244], loss=74.0269
	step [53/244], loss=68.4383
	step [54/244], loss=72.8165
	step [55/244], loss=68.6515
	step [56/244], loss=88.2007
	step [57/244], loss=72.8056
	step [58/244], loss=74.3250
	step [59/244], loss=75.2781
	step [60/244], loss=75.7379
	step [61/244], loss=78.7101
	step [62/244], loss=76.3484
	step [63/244], loss=83.6526
	step [64/244], loss=97.3752
	step [65/244], loss=63.6600
	step [66/244], loss=80.1993
	step [67/244], loss=78.0322
	step [68/244], loss=98.0500
	step [69/244], loss=90.0414
	step [70/244], loss=75.2640
	step [71/244], loss=82.2767
	step [72/244], loss=86.4485
	step [73/244], loss=83.7475
	step [74/244], loss=76.9904
	step [75/244], loss=89.0250
	step [76/244], loss=64.1447
	step [77/244], loss=79.8932
	step [78/244], loss=75.4357
	step [79/244], loss=77.4513
	step [80/244], loss=63.7882
	step [81/244], loss=73.0289
	step [82/244], loss=68.8581
	step [83/244], loss=71.2087
	step [84/244], loss=73.7850
	step [85/244], loss=74.7719
	step [86/244], loss=75.7208
	step [87/244], loss=62.3322
	step [88/244], loss=67.4827
	step [89/244], loss=65.1548
	step [90/244], loss=70.6360
	step [91/244], loss=63.2766
	step [92/244], loss=96.2478
	step [93/244], loss=69.8574
	step [94/244], loss=65.4836
	step [95/244], loss=68.0260
	step [96/244], loss=70.6526
	step [97/244], loss=90.5366
	step [98/244], loss=79.1128
	step [99/244], loss=78.3633
	step [100/244], loss=76.1838
	step [101/244], loss=74.8995
	step [102/244], loss=60.6112
	step [103/244], loss=75.7222
	step [104/244], loss=70.3298
	step [105/244], loss=67.5889
	step [106/244], loss=74.0838
	step [107/244], loss=63.8837
	step [108/244], loss=64.7712
	step [109/244], loss=70.3531
	step [110/244], loss=72.1269
	step [111/244], loss=73.4181
	step [112/244], loss=79.3714
	step [113/244], loss=86.2646
	step [114/244], loss=66.3921
	step [115/244], loss=76.6562
	step [116/244], loss=79.5439
	step [117/244], loss=69.5971
	step [118/244], loss=72.1911
	step [119/244], loss=76.3442
	step [120/244], loss=84.4960
	step [121/244], loss=66.3544
	step [122/244], loss=87.1297
	step [123/244], loss=66.2258
	step [124/244], loss=65.6666
	step [125/244], loss=58.9315
	step [126/244], loss=69.2356
	step [127/244], loss=64.4945
	step [128/244], loss=75.1842
	step [129/244], loss=71.2011
	step [130/244], loss=65.3649
	step [131/244], loss=73.5073
	step [132/244], loss=53.0929
	step [133/244], loss=79.0056
	step [134/244], loss=88.3934
	step [135/244], loss=80.0640
	step [136/244], loss=69.7435
	step [137/244], loss=81.5939
	step [138/244], loss=67.8912
	step [139/244], loss=73.2239
	step [140/244], loss=67.3039
	step [141/244], loss=56.4872
	step [142/244], loss=80.9141
	step [143/244], loss=85.6496
	step [144/244], loss=94.3099
	step [145/244], loss=71.6066
	step [146/244], loss=55.6990
	step [147/244], loss=83.9824
	step [148/244], loss=57.1831
	step [149/244], loss=76.3250
	step [150/244], loss=76.3560
	step [151/244], loss=72.2676
	step [152/244], loss=73.9849
	step [153/244], loss=81.3553
	step [154/244], loss=72.2653
	step [155/244], loss=82.1001
	step [156/244], loss=75.6470
	step [157/244], loss=88.1593
	step [158/244], loss=87.5018
	step [159/244], loss=65.7958
	step [160/244], loss=66.1624
	step [161/244], loss=65.0862
	step [162/244], loss=90.3428
	step [163/244], loss=60.9315
	step [164/244], loss=96.3317
	step [165/244], loss=86.9526
	step [166/244], loss=79.9334
	step [167/244], loss=78.4580
	step [168/244], loss=56.0131
	step [169/244], loss=71.7531
	step [170/244], loss=63.2407
	step [171/244], loss=76.9799
	step [172/244], loss=76.8783
	step [173/244], loss=77.5387
	step [174/244], loss=70.3041
	step [175/244], loss=73.3045
	step [176/244], loss=75.2606
	step [177/244], loss=98.4948
	step [178/244], loss=53.9550
	step [179/244], loss=76.3309
	step [180/244], loss=67.8518
	step [181/244], loss=79.0445
	step [182/244], loss=65.5517
	step [183/244], loss=82.2162
	step [184/244], loss=73.1128
	step [185/244], loss=77.3967
	step [186/244], loss=84.1344
	step [187/244], loss=68.5757
	step [188/244], loss=82.9700
	step [189/244], loss=66.9855
	step [190/244], loss=76.6037
	step [191/244], loss=92.9753
	step [192/244], loss=94.0029
	step [193/244], loss=84.0371
	step [194/244], loss=72.8334
	step [195/244], loss=67.3816
	step [196/244], loss=97.1034
	step [197/244], loss=77.7329
	step [198/244], loss=67.4731
	step [199/244], loss=73.3608
	step [200/244], loss=82.3327
	step [201/244], loss=76.4478
	step [202/244], loss=67.8760
	step [203/244], loss=81.2099
	step [204/244], loss=75.6125
	step [205/244], loss=73.1613
	step [206/244], loss=66.6158
	step [207/244], loss=68.7971
	step [208/244], loss=60.2273
	step [209/244], loss=62.2306
	step [210/244], loss=84.0598
	step [211/244], loss=73.2944
	step [212/244], loss=81.0353
	step [213/244], loss=66.9257
	step [214/244], loss=90.5209
	step [215/244], loss=78.3529
	step [216/244], loss=85.4299
	step [217/244], loss=76.0846
	step [218/244], loss=74.8475
	step [219/244], loss=86.7543
	step [220/244], loss=87.1186
	step [221/244], loss=79.1019
	step [222/244], loss=66.5110
	step [223/244], loss=81.8096
	step [224/244], loss=75.2811
	step [225/244], loss=84.1316
	step [226/244], loss=78.0594
	step [227/244], loss=100.4308
	step [228/244], loss=70.0102
	step [229/244], loss=82.6140
	step [230/244], loss=73.0867
	step [231/244], loss=78.6357
	step [232/244], loss=59.6755
	step [233/244], loss=63.6527
	step [234/244], loss=67.9618
	step [235/244], loss=80.3590
	step [236/244], loss=79.4621
	step [237/244], loss=77.2502
	step [238/244], loss=66.6953
	step [239/244], loss=80.3850
	step [240/244], loss=77.5643
	step [241/244], loss=80.1783
	step [242/244], loss=80.6444
	step [243/244], loss=89.3152
	step [244/244], loss=46.6270
	Evaluating
	loss=0.0073, precision=0.3690, recall=0.8837, f1=0.5206
Training epoch 94
	step [1/244], loss=80.2558
	step [2/244], loss=59.5069
	step [3/244], loss=76.0266
	step [4/244], loss=88.3147
	step [5/244], loss=73.2851
	step [6/244], loss=82.3318
	step [7/244], loss=82.8874
	step [8/244], loss=68.4140
	step [9/244], loss=85.4590
	step [10/244], loss=70.7566
	step [11/244], loss=73.2438
	step [12/244], loss=84.2322
	step [13/244], loss=83.3093
	step [14/244], loss=85.4406
	step [15/244], loss=89.0184
	step [16/244], loss=76.9864
	step [17/244], loss=79.3259
	step [18/244], loss=85.3299
	step [19/244], loss=57.2885
	step [20/244], loss=75.6434
	step [21/244], loss=78.7796
	step [22/244], loss=92.8282
	step [23/244], loss=80.3436
	step [24/244], loss=74.1443
	step [25/244], loss=82.7899
	step [26/244], loss=64.0094
	step [27/244], loss=67.6144
	step [28/244], loss=65.9337
	step [29/244], loss=74.2132
	step [30/244], loss=61.8463
	step [31/244], loss=81.0624
	step [32/244], loss=90.0442
	step [33/244], loss=74.1617
	step [34/244], loss=65.6743
	step [35/244], loss=81.4887
	step [36/244], loss=82.0760
	step [37/244], loss=91.5485
	step [38/244], loss=68.7616
	step [39/244], loss=65.8345
	step [40/244], loss=75.5892
	step [41/244], loss=68.8069
	step [42/244], loss=73.1355
	step [43/244], loss=68.5656
	step [44/244], loss=66.7314
	step [45/244], loss=62.5605
	step [46/244], loss=68.6074
	step [47/244], loss=75.3824
	step [48/244], loss=94.2150
	step [49/244], loss=53.0924
	step [50/244], loss=59.9172
	step [51/244], loss=70.6981
	step [52/244], loss=69.8172
	step [53/244], loss=76.8248
	step [54/244], loss=79.3410
	step [55/244], loss=73.8429
	step [56/244], loss=66.6199
	step [57/244], loss=82.3405
	step [58/244], loss=84.3583
	step [59/244], loss=67.6200
	step [60/244], loss=72.0994
	step [61/244], loss=97.8719
	step [62/244], loss=98.1312
	step [63/244], loss=74.1808
	step [64/244], loss=86.1976
	step [65/244], loss=71.2176
	step [66/244], loss=69.5544
	step [67/244], loss=72.7556
	step [68/244], loss=68.6704
	step [69/244], loss=62.2860
	step [70/244], loss=67.6711
	step [71/244], loss=69.4921
	step [72/244], loss=70.7852
	step [73/244], loss=74.2025
	step [74/244], loss=73.4819
	step [75/244], loss=87.5281
	step [76/244], loss=76.7437
	step [77/244], loss=69.0620
	step [78/244], loss=93.1992
	step [79/244], loss=71.7947
	step [80/244], loss=69.5936
	step [81/244], loss=77.3508
	step [82/244], loss=64.6901
	step [83/244], loss=73.6586
	step [84/244], loss=76.1749
	step [85/244], loss=68.2815
	step [86/244], loss=80.7421
	step [87/244], loss=65.7114
	step [88/244], loss=67.6453
	step [89/244], loss=70.3032
	step [90/244], loss=92.7185
	step [91/244], loss=75.6035
	step [92/244], loss=61.7434
	step [93/244], loss=75.5721
	step [94/244], loss=73.8886
	step [95/244], loss=70.5802
	step [96/244], loss=67.4321
	step [97/244], loss=61.2813
	step [98/244], loss=77.6807
	step [99/244], loss=91.3631
	step [100/244], loss=73.1674
	step [101/244], loss=76.9482
	step [102/244], loss=70.5753
	step [103/244], loss=83.4391
	step [104/244], loss=69.2633
	step [105/244], loss=76.2468
	step [106/244], loss=77.7182
	step [107/244], loss=65.6059
	step [108/244], loss=91.2081
	step [109/244], loss=78.3000
	step [110/244], loss=93.8189
	step [111/244], loss=77.4980
	step [112/244], loss=60.3521
	step [113/244], loss=64.4626
	step [114/244], loss=76.4829
	step [115/244], loss=70.5514
	step [116/244], loss=90.3426
	step [117/244], loss=76.3968
	step [118/244], loss=76.1201
	step [119/244], loss=75.4022
	step [120/244], loss=80.0449
	step [121/244], loss=62.7358
	step [122/244], loss=74.0586
	step [123/244], loss=79.3997
	step [124/244], loss=98.5918
	step [125/244], loss=68.6141
	step [126/244], loss=65.5318
	step [127/244], loss=64.5234
	step [128/244], loss=72.5586
	step [129/244], loss=79.5834
	step [130/244], loss=66.1746
	step [131/244], loss=80.8156
	step [132/244], loss=89.8389
	step [133/244], loss=62.0416
	step [134/244], loss=66.2212
	step [135/244], loss=77.3388
	step [136/244], loss=80.1969
	step [137/244], loss=77.2221
	step [138/244], loss=65.4714
	step [139/244], loss=67.1890
	step [140/244], loss=60.1641
	step [141/244], loss=58.2446
	step [142/244], loss=83.4129
	step [143/244], loss=68.9434
	step [144/244], loss=58.2853
	step [145/244], loss=77.1818
	step [146/244], loss=78.0836
	step [147/244], loss=71.8320
	step [148/244], loss=89.7232
	step [149/244], loss=68.3615
	step [150/244], loss=83.6918
	step [151/244], loss=76.8261
	step [152/244], loss=73.0043
	step [153/244], loss=69.0025
	step [154/244], loss=73.1188
	step [155/244], loss=77.6798
	step [156/244], loss=77.1499
	step [157/244], loss=71.7947
	step [158/244], loss=67.3859
	step [159/244], loss=85.8009
	step [160/244], loss=65.6157
	step [161/244], loss=78.0095
	step [162/244], loss=64.3025
	step [163/244], loss=79.5674
	step [164/244], loss=80.4108
	step [165/244], loss=89.0000
	step [166/244], loss=83.5014
	step [167/244], loss=72.5217
	step [168/244], loss=71.3889
	step [169/244], loss=84.3501
	step [170/244], loss=73.5276
	step [171/244], loss=73.2228
	step [172/244], loss=71.2434
	step [173/244], loss=74.9238
	step [174/244], loss=65.8547
	step [175/244], loss=77.2717
	step [176/244], loss=68.7786
	step [177/244], loss=104.2004
	step [178/244], loss=92.5466
	step [179/244], loss=70.1721
	step [180/244], loss=70.2367
	step [181/244], loss=67.3898
	step [182/244], loss=64.3758
	step [183/244], loss=70.7426
	step [184/244], loss=77.4352
	step [185/244], loss=75.7666
	step [186/244], loss=75.7326
	step [187/244], loss=79.1687
	step [188/244], loss=78.2454
	step [189/244], loss=78.3095
	step [190/244], loss=83.4813
	step [191/244], loss=73.9739
	step [192/244], loss=70.9179
	step [193/244], loss=78.0244
	step [194/244], loss=74.4429
	step [195/244], loss=91.4039
	step [196/244], loss=82.8399
	step [197/244], loss=74.6150
	step [198/244], loss=60.8265
	step [199/244], loss=75.9532
	step [200/244], loss=74.9923
	step [201/244], loss=66.3511
	step [202/244], loss=74.1478
	step [203/244], loss=67.5909
	step [204/244], loss=87.1671
	step [205/244], loss=68.6613
	step [206/244], loss=65.1863
	step [207/244], loss=66.4342
	step [208/244], loss=71.8831
	step [209/244], loss=69.3328
	step [210/244], loss=86.1861
	step [211/244], loss=60.2492
	step [212/244], loss=63.8148
	step [213/244], loss=78.9607
	step [214/244], loss=75.2286
	step [215/244], loss=83.3914
	step [216/244], loss=76.3618
	step [217/244], loss=63.2806
	step [218/244], loss=73.7850
	step [219/244], loss=71.3762
	step [220/244], loss=71.3721
	step [221/244], loss=63.3377
	step [222/244], loss=73.5370
	step [223/244], loss=90.7205
	step [224/244], loss=63.0816
	step [225/244], loss=78.4438
	step [226/244], loss=81.0415
	step [227/244], loss=84.9483
	step [228/244], loss=78.0249
	step [229/244], loss=81.9653
	step [230/244], loss=64.3626
	step [231/244], loss=62.9834
	step [232/244], loss=84.5266
	step [233/244], loss=94.3747
	step [234/244], loss=79.8703
	step [235/244], loss=77.4731
	step [236/244], loss=86.5084
	step [237/244], loss=82.3491
	step [238/244], loss=73.3218
	step [239/244], loss=59.9617
	step [240/244], loss=77.2675
	step [241/244], loss=57.9711
	step [242/244], loss=85.5476
	step [243/244], loss=77.8418
	step [244/244], loss=33.5616
	Evaluating
	loss=0.0082, precision=0.3355, recall=0.8794, f1=0.4857
Training epoch 95
	step [1/244], loss=96.1632
	step [2/244], loss=87.4237
	step [3/244], loss=74.7586
	step [4/244], loss=80.3704
	step [5/244], loss=67.3924
	step [6/244], loss=76.2435
	step [7/244], loss=66.0836
	step [8/244], loss=70.9960
	step [9/244], loss=75.7718
	step [10/244], loss=71.4558
	step [11/244], loss=70.5350
	step [12/244], loss=86.1890
	step [13/244], loss=74.1264
	step [14/244], loss=75.7175
	step [15/244], loss=85.6392
	step [16/244], loss=85.6556
	step [17/244], loss=70.5280
	step [18/244], loss=66.8783
	step [19/244], loss=52.9929
	step [20/244], loss=97.0607
	step [21/244], loss=72.0537
	step [22/244], loss=61.9169
	step [23/244], loss=82.0145
	step [24/244], loss=59.4133
	step [25/244], loss=73.9216
	step [26/244], loss=84.6280
	step [27/244], loss=69.5638
	step [28/244], loss=64.5537
	step [29/244], loss=86.1672
	step [30/244], loss=63.6344
	step [31/244], loss=70.3543
	step [32/244], loss=82.2497
	step [33/244], loss=89.6997
	step [34/244], loss=86.5241
	step [35/244], loss=59.2559
	step [36/244], loss=92.5850
	step [37/244], loss=63.5180
	step [38/244], loss=89.8765
	step [39/244], loss=75.5782
	step [40/244], loss=61.5502
	step [41/244], loss=69.6505
	step [42/244], loss=81.0379
	step [43/244], loss=79.9969
	step [44/244], loss=78.8196
	step [45/244], loss=69.6327
	step [46/244], loss=81.1160
	step [47/244], loss=71.1334
	step [48/244], loss=59.9940
	step [49/244], loss=76.5853
	step [50/244], loss=71.6033
	step [51/244], loss=74.3570
	step [52/244], loss=70.3871
	step [53/244], loss=75.1479
	step [54/244], loss=65.9731
	step [55/244], loss=72.7280
	step [56/244], loss=74.3236
	step [57/244], loss=87.3723
	step [58/244], loss=78.0665
	step [59/244], loss=79.1712
	step [60/244], loss=67.2503
	step [61/244], loss=82.5800
	step [62/244], loss=68.0831
	step [63/244], loss=87.2764
	step [64/244], loss=59.7485
	step [65/244], loss=79.8671
	step [66/244], loss=82.6803
	step [67/244], loss=81.7374
	step [68/244], loss=64.2865
	step [69/244], loss=78.0230
	step [70/244], loss=76.5327
	step [71/244], loss=57.5505
	step [72/244], loss=86.7005
	step [73/244], loss=89.5861
	step [74/244], loss=89.7183
	step [75/244], loss=73.5395
	step [76/244], loss=101.5420
	step [77/244], loss=75.9830
	step [78/244], loss=69.7764
	step [79/244], loss=72.8787
	step [80/244], loss=80.3643
	step [81/244], loss=66.7043
	step [82/244], loss=89.0650
	step [83/244], loss=66.8622
	step [84/244], loss=66.8604
	step [85/244], loss=70.2303
	step [86/244], loss=72.1301
	step [87/244], loss=73.5027
	step [88/244], loss=75.7437
	step [89/244], loss=75.0769
	step [90/244], loss=80.6801
	step [91/244], loss=69.2537
	step [92/244], loss=77.0296
	step [93/244], loss=68.9601
	step [94/244], loss=81.7771
	step [95/244], loss=75.2466
	step [96/244], loss=74.2446
	step [97/244], loss=51.7250
	step [98/244], loss=74.1866
	step [99/244], loss=68.9444
	step [100/244], loss=74.5704
	step [101/244], loss=60.1790
	step [102/244], loss=79.7058
	step [103/244], loss=78.6390
	step [104/244], loss=60.5944
	step [105/244], loss=71.9635
	step [106/244], loss=65.0850
	step [107/244], loss=80.9594
	step [108/244], loss=80.3296
	step [109/244], loss=80.7849
	step [110/244], loss=71.7362
	step [111/244], loss=84.8439
	step [112/244], loss=66.3596
	step [113/244], loss=71.0209
	step [114/244], loss=73.8561
	step [115/244], loss=70.4713
	step [116/244], loss=79.5873
	step [117/244], loss=71.7325
	step [118/244], loss=65.7683
	step [119/244], loss=82.8934
	step [120/244], loss=68.1809
	step [121/244], loss=80.8390
	step [122/244], loss=86.2568
	step [123/244], loss=76.7998
	step [124/244], loss=71.9227
	step [125/244], loss=82.8343
	step [126/244], loss=84.9001
	step [127/244], loss=76.1177
	step [128/244], loss=77.4969
	step [129/244], loss=88.3067
	step [130/244], loss=78.6404
	step [131/244], loss=83.6090
	step [132/244], loss=75.3431
	step [133/244], loss=80.2577
	step [134/244], loss=79.6755
	step [135/244], loss=69.4401
	step [136/244], loss=77.9413
	step [137/244], loss=88.0478
	step [138/244], loss=60.6040
	step [139/244], loss=70.2448
	step [140/244], loss=72.7328
	step [141/244], loss=71.1455
	step [142/244], loss=75.5656
	step [143/244], loss=78.0456
	step [144/244], loss=72.3640
	step [145/244], loss=76.0367
	step [146/244], loss=77.9546
	step [147/244], loss=59.9330
	step [148/244], loss=71.4983
	step [149/244], loss=61.5380
	step [150/244], loss=61.8748
	step [151/244], loss=80.9163
	step [152/244], loss=86.2572
	step [153/244], loss=78.3892
	step [154/244], loss=75.5662
	step [155/244], loss=75.8510
	step [156/244], loss=76.0055
	step [157/244], loss=73.1357
	step [158/244], loss=68.3891
	step [159/244], loss=70.6206
	step [160/244], loss=80.7174
	step [161/244], loss=71.0245
	step [162/244], loss=67.4993
	step [163/244], loss=79.6846
	step [164/244], loss=66.8341
	step [165/244], loss=76.3070
	step [166/244], loss=77.7934
	step [167/244], loss=73.0647
	step [168/244], loss=56.1121
	step [169/244], loss=70.9233
	step [170/244], loss=69.5200
	step [171/244], loss=71.5610
	step [172/244], loss=89.3965
	step [173/244], loss=69.9176
	step [174/244], loss=91.5128
	step [175/244], loss=83.4826
	step [176/244], loss=90.8556
	step [177/244], loss=81.9815
	step [178/244], loss=63.7352
	step [179/244], loss=75.1428
	step [180/244], loss=72.4055
	step [181/244], loss=70.3157
	step [182/244], loss=69.5963
	step [183/244], loss=65.3992
	step [184/244], loss=71.5683
	step [185/244], loss=64.1492
	step [186/244], loss=76.7513
	step [187/244], loss=85.4765
	step [188/244], loss=77.6504
	step [189/244], loss=70.1201
	step [190/244], loss=71.7930
	step [191/244], loss=67.9449
	step [192/244], loss=61.8477
	step [193/244], loss=78.1177
	step [194/244], loss=97.4326
	step [195/244], loss=72.1581
	step [196/244], loss=87.4479
	step [197/244], loss=69.9573
	step [198/244], loss=71.1728
	step [199/244], loss=74.5196
	step [200/244], loss=74.1231
	step [201/244], loss=72.2765
	step [202/244], loss=57.9787
	step [203/244], loss=82.2665
	step [204/244], loss=72.5256
	step [205/244], loss=73.9817
	step [206/244], loss=79.0773
	step [207/244], loss=71.0003
	step [208/244], loss=65.9959
	step [209/244], loss=72.6443
	step [210/244], loss=80.2760
	step [211/244], loss=79.2254
	step [212/244], loss=71.9993
	step [213/244], loss=70.6998
	step [214/244], loss=63.1336
	step [215/244], loss=67.4566
	step [216/244], loss=80.4252
	step [217/244], loss=84.4326
	step [218/244], loss=88.8585
	step [219/244], loss=79.7495
	step [220/244], loss=77.5629
	step [221/244], loss=74.5055
	step [222/244], loss=73.0423
	step [223/244], loss=69.7913
	step [224/244], loss=79.3229
	step [225/244], loss=70.7235
	step [226/244], loss=73.8012
	step [227/244], loss=59.7649
	step [228/244], loss=79.9040
	step [229/244], loss=68.7888
	step [230/244], loss=82.8023
	step [231/244], loss=72.7070
	step [232/244], loss=83.4004
	step [233/244], loss=72.9072
	step [234/244], loss=67.4120
	step [235/244], loss=59.5516
	step [236/244], loss=88.3535
	step [237/244], loss=73.1137
	step [238/244], loss=80.7538
	step [239/244], loss=64.3067
	step [240/244], loss=60.2034
	step [241/244], loss=93.5745
	step [242/244], loss=72.9626
	step [243/244], loss=80.0582
	step [244/244], loss=30.1360
	Evaluating
	loss=0.0072, precision=0.3800, recall=0.8642, f1=0.5279
saving model as: 2_saved_model.pth
Training epoch 96
	step [1/244], loss=62.5184
	step [2/244], loss=80.3074
	step [3/244], loss=77.2530
	step [4/244], loss=93.2317
	step [5/244], loss=75.9560
	step [6/244], loss=80.5880
	step [7/244], loss=84.2384
	step [8/244], loss=66.6518
	step [9/244], loss=81.1814
	step [10/244], loss=68.2136
	step [11/244], loss=98.4193
	step [12/244], loss=60.2432
	step [13/244], loss=74.0772
	step [14/244], loss=76.3619
	step [15/244], loss=61.0794
	step [16/244], loss=62.8644
	step [17/244], loss=79.1857
	step [18/244], loss=75.5176
	step [19/244], loss=89.8927
	step [20/244], loss=67.3047
	step [21/244], loss=65.7465
	step [22/244], loss=73.1970
	step [23/244], loss=77.0349
	step [24/244], loss=87.6076
	step [25/244], loss=93.6736
	step [26/244], loss=92.2561
	step [27/244], loss=61.9141
	step [28/244], loss=61.4015
	step [29/244], loss=62.7773
	step [30/244], loss=70.0798
	step [31/244], loss=65.0539
	step [32/244], loss=83.8864
	step [33/244], loss=79.5068
	step [34/244], loss=67.9960
	step [35/244], loss=83.7730
	step [36/244], loss=65.6753
	step [37/244], loss=62.6845
	step [38/244], loss=54.8226
	step [39/244], loss=66.0482
	step [40/244], loss=87.6437
	step [41/244], loss=71.7136
	step [42/244], loss=60.8037
	step [43/244], loss=102.3695
	step [44/244], loss=67.5784
	step [45/244], loss=65.6913
	step [46/244], loss=76.3144
	step [47/244], loss=68.4582
	step [48/244], loss=63.5328
	step [49/244], loss=80.6002
	step [50/244], loss=72.9592
	step [51/244], loss=89.1662
	step [52/244], loss=62.4195
	step [53/244], loss=66.3545
	step [54/244], loss=74.5025
	step [55/244], loss=67.1927
	step [56/244], loss=88.2099
	step [57/244], loss=74.7453
	step [58/244], loss=67.1911
	step [59/244], loss=99.5974
	step [60/244], loss=74.7006
	step [61/244], loss=77.0194
	step [62/244], loss=57.2842
	step [63/244], loss=73.9383
	step [64/244], loss=75.8782
	step [65/244], loss=66.4934
	step [66/244], loss=58.3592
	step [67/244], loss=81.2292
	step [68/244], loss=85.6600
	step [69/244], loss=70.2721
	step [70/244], loss=68.1360
	step [71/244], loss=66.9377
	step [72/244], loss=71.3199
	step [73/244], loss=77.2321
	step [74/244], loss=81.0141
	step [75/244], loss=80.4200
	step [76/244], loss=69.5263
	step [77/244], loss=79.9540
	step [78/244], loss=87.4998
	step [79/244], loss=83.6539
	step [80/244], loss=78.3283
	step [81/244], loss=72.8209
	step [82/244], loss=60.1794
	step [83/244], loss=69.0280
	step [84/244], loss=74.1681
	step [85/244], loss=60.9962
	step [86/244], loss=68.8984
	step [87/244], loss=66.3756
	step [88/244], loss=66.6875
	step [89/244], loss=62.0451
	step [90/244], loss=72.8521
	step [91/244], loss=68.4469
	step [92/244], loss=73.7988
	step [93/244], loss=68.8246
	step [94/244], loss=68.2258
	step [95/244], loss=59.4784
	step [96/244], loss=83.5004
	step [97/244], loss=77.7752
	step [98/244], loss=80.1103
	step [99/244], loss=82.8308
	step [100/244], loss=60.4182
	step [101/244], loss=75.0384
	step [102/244], loss=74.4252
	step [103/244], loss=60.4876
	step [104/244], loss=79.6793
	step [105/244], loss=79.1554
	step [106/244], loss=67.7373
	step [107/244], loss=85.7805
	step [108/244], loss=68.9346
	step [109/244], loss=64.5685
	step [110/244], loss=69.5911
	step [111/244], loss=65.0032
	step [112/244], loss=97.8496
	step [113/244], loss=89.5369
	step [114/244], loss=76.3831
	step [115/244], loss=77.8562
	step [116/244], loss=72.9801
	step [117/244], loss=77.4164
	step [118/244], loss=73.3183
	step [119/244], loss=98.4988
	step [120/244], loss=69.0786
	step [121/244], loss=78.9321
	step [122/244], loss=72.4901
	step [123/244], loss=78.1854
	step [124/244], loss=86.8601
	step [125/244], loss=69.2638
	step [126/244], loss=71.0283
	step [127/244], loss=87.0657
	step [128/244], loss=78.6592
	step [129/244], loss=68.1494
	step [130/244], loss=81.0042
	step [131/244], loss=72.0720
	step [132/244], loss=73.5457
	step [133/244], loss=86.5641
	step [134/244], loss=79.1880
	step [135/244], loss=76.4142
	step [136/244], loss=81.4011
	step [137/244], loss=71.1711
	step [138/244], loss=71.3546
	step [139/244], loss=85.3068
	step [140/244], loss=79.0899
	step [141/244], loss=63.6308
	step [142/244], loss=79.2659
	step [143/244], loss=76.3024
	step [144/244], loss=80.0199
	step [145/244], loss=60.1271
	step [146/244], loss=80.6615
	step [147/244], loss=70.4852
	step [148/244], loss=73.6897
	step [149/244], loss=55.3851
	step [150/244], loss=70.3290
	step [151/244], loss=80.8548
	step [152/244], loss=66.4574
	step [153/244], loss=89.9235
	step [154/244], loss=57.9449
	step [155/244], loss=72.7968
	step [156/244], loss=63.6540
	step [157/244], loss=93.2280
	step [158/244], loss=68.7649
	step [159/244], loss=68.5163
	step [160/244], loss=90.0981
	step [161/244], loss=71.0139
	step [162/244], loss=65.8270
	step [163/244], loss=76.2891
	step [164/244], loss=85.9283
	step [165/244], loss=65.4095
	step [166/244], loss=87.5700
	step [167/244], loss=75.6695
	step [168/244], loss=95.8360
	step [169/244], loss=97.8970
	step [170/244], loss=70.9748
	step [171/244], loss=71.4380
	step [172/244], loss=88.9882
	step [173/244], loss=72.7514
	step [174/244], loss=73.5226
	step [175/244], loss=77.9131
	step [176/244], loss=68.6066
	step [177/244], loss=81.3189
	step [178/244], loss=80.0615
	step [179/244], loss=72.1676
	step [180/244], loss=62.1953
	step [181/244], loss=73.8930
	step [182/244], loss=65.4720
	step [183/244], loss=66.6850
	step [184/244], loss=81.8301
	step [185/244], loss=65.5037
	step [186/244], loss=66.4304
	step [187/244], loss=79.3363
	step [188/244], loss=70.9099
	step [189/244], loss=73.0960
	step [190/244], loss=71.8375
	step [191/244], loss=66.8550
	step [192/244], loss=76.4377
	step [193/244], loss=53.6824
	step [194/244], loss=58.9721
	step [195/244], loss=65.9531
	step [196/244], loss=62.3750
	step [197/244], loss=86.8661
	step [198/244], loss=80.7585
	step [199/244], loss=77.4084
	step [200/244], loss=73.1979
	step [201/244], loss=61.1178
	step [202/244], loss=67.8611
	step [203/244], loss=74.8600
	step [204/244], loss=63.5505
	step [205/244], loss=68.9818
	step [206/244], loss=69.3943
	step [207/244], loss=74.5889
	step [208/244], loss=76.9565
	step [209/244], loss=87.0383
	step [210/244], loss=77.0821
	step [211/244], loss=83.3684
	step [212/244], loss=69.6279
	step [213/244], loss=69.7561
	step [214/244], loss=87.7422
	step [215/244], loss=67.6133
	step [216/244], loss=65.2543
	step [217/244], loss=80.9008
	step [218/244], loss=84.3595
	step [219/244], loss=78.1156
	step [220/244], loss=79.1383
	step [221/244], loss=65.5570
	step [222/244], loss=87.4442
	step [223/244], loss=92.7834
	step [224/244], loss=65.6004
	step [225/244], loss=74.9985
	step [226/244], loss=66.2219
	step [227/244], loss=68.8370
	step [228/244], loss=77.9586
	step [229/244], loss=74.9687
	step [230/244], loss=73.2574
	step [231/244], loss=78.8541
	step [232/244], loss=84.3234
	step [233/244], loss=88.9933
	step [234/244], loss=65.1148
	step [235/244], loss=77.6226
	step [236/244], loss=79.0908
	step [237/244], loss=76.3612
	step [238/244], loss=84.5421
	step [239/244], loss=80.2204
	step [240/244], loss=72.5811
	step [241/244], loss=63.5443
	step [242/244], loss=78.8939
	step [243/244], loss=78.4303
	step [244/244], loss=39.5014
	Evaluating
	loss=0.0073, precision=0.3691, recall=0.8664, f1=0.5177
Training epoch 97
	step [1/244], loss=82.3981
	step [2/244], loss=83.7312
	step [3/244], loss=64.6973
	step [4/244], loss=83.4515
	step [5/244], loss=63.5508
	step [6/244], loss=80.3688
	step [7/244], loss=84.0554
	step [8/244], loss=59.6846
	step [9/244], loss=63.0834
	step [10/244], loss=67.8775
	step [11/244], loss=61.8754
	step [12/244], loss=74.2619
	step [13/244], loss=86.7248
	step [14/244], loss=80.4464
	step [15/244], loss=53.5275
	step [16/244], loss=91.1913
	step [17/244], loss=77.5787
	step [18/244], loss=77.6755
	step [19/244], loss=74.6793
	step [20/244], loss=73.2922
	step [21/244], loss=66.5750
	step [22/244], loss=70.7211
	step [23/244], loss=65.6176
	step [24/244], loss=82.2694
	step [25/244], loss=86.3265
	step [26/244], loss=74.1267
	step [27/244], loss=73.1580
	step [28/244], loss=76.5559
	step [29/244], loss=77.5257
	step [30/244], loss=73.5761
	step [31/244], loss=75.6789
	step [32/244], loss=66.7167
	step [33/244], loss=79.5842
	step [34/244], loss=69.1010
	step [35/244], loss=72.9450
	step [36/244], loss=78.8724
	step [37/244], loss=87.1355
	step [38/244], loss=69.4865
	step [39/244], loss=84.4366
	step [40/244], loss=91.9443
	step [41/244], loss=77.1589
	step [42/244], loss=79.2584
	step [43/244], loss=82.2214
	step [44/244], loss=86.5593
	step [45/244], loss=78.7093
	step [46/244], loss=76.4044
	step [47/244], loss=67.0220
	step [48/244], loss=83.3144
	step [49/244], loss=67.5023
	step [50/244], loss=80.3678
	step [51/244], loss=83.2555
	step [52/244], loss=66.3527
	step [53/244], loss=59.3221
	step [54/244], loss=56.5835
	step [55/244], loss=84.5141
	step [56/244], loss=62.3857
	step [57/244], loss=65.3810
	step [58/244], loss=76.4101
	step [59/244], loss=73.4760
	step [60/244], loss=82.1739
	step [61/244], loss=74.4255
	step [62/244], loss=81.1912
	step [63/244], loss=64.2841
	step [64/244], loss=77.5242
	step [65/244], loss=68.4409
	step [66/244], loss=72.6710
	step [67/244], loss=67.6492
	step [68/244], loss=72.6096
	step [69/244], loss=70.5675
	step [70/244], loss=80.1456
	step [71/244], loss=63.9341
	step [72/244], loss=67.0126
	step [73/244], loss=74.6498
	step [74/244], loss=75.7545
	step [75/244], loss=98.2528
	step [76/244], loss=74.4831
	step [77/244], loss=72.4167
	step [78/244], loss=88.4231
	step [79/244], loss=51.1542
	step [80/244], loss=72.3818
	step [81/244], loss=54.3505
	step [82/244], loss=79.7209
	step [83/244], loss=66.0667
	step [84/244], loss=84.3816
	step [85/244], loss=77.6594
	step [86/244], loss=77.7427
	step [87/244], loss=81.7489
	step [88/244], loss=73.9883
	step [89/244], loss=82.2922
	step [90/244], loss=89.1699
	step [91/244], loss=79.9846
	step [92/244], loss=86.7250
	step [93/244], loss=77.0837
	step [94/244], loss=53.8032
	step [95/244], loss=79.8961
	step [96/244], loss=80.4954
	step [97/244], loss=75.8814
	step [98/244], loss=60.8879
	step [99/244], loss=60.6249
	step [100/244], loss=94.4089
	step [101/244], loss=82.7886
	step [102/244], loss=61.7598
	step [103/244], loss=70.8914
	step [104/244], loss=69.3369
	step [105/244], loss=85.8616
	step [106/244], loss=87.6157
	step [107/244], loss=78.0407
	step [108/244], loss=62.1922
	step [109/244], loss=81.3129
	step [110/244], loss=75.6011
	step [111/244], loss=63.0488
	step [112/244], loss=66.9805
	step [113/244], loss=64.5407
	step [114/244], loss=65.4381
	step [115/244], loss=89.7444
	step [116/244], loss=69.8858
	step [117/244], loss=66.7044
	step [118/244], loss=69.0332
	step [119/244], loss=95.1715
	step [120/244], loss=74.1920
	step [121/244], loss=71.7940
	step [122/244], loss=86.8592
	step [123/244], loss=64.6088
	step [124/244], loss=75.3864
	step [125/244], loss=79.0982
	step [126/244], loss=76.6469
	step [127/244], loss=75.3077
	step [128/244], loss=72.4719
	step [129/244], loss=64.9007
	step [130/244], loss=63.5834
	step [131/244], loss=71.7403
	step [132/244], loss=74.1246
	step [133/244], loss=62.6968
	step [134/244], loss=91.0204
	step [135/244], loss=54.2289
	step [136/244], loss=81.2218
	step [137/244], loss=72.5425
	step [138/244], loss=76.3304
	step [139/244], loss=71.8215
	step [140/244], loss=71.2020
	step [141/244], loss=52.7840
	step [142/244], loss=84.1187
	step [143/244], loss=70.5472
	step [144/244], loss=65.7869
	step [145/244], loss=66.5582
	step [146/244], loss=67.1229
	step [147/244], loss=62.7596
	step [148/244], loss=78.7821
	step [149/244], loss=73.3348
	step [150/244], loss=72.9302
	step [151/244], loss=66.1763
	step [152/244], loss=70.4700
	step [153/244], loss=62.2978
	step [154/244], loss=83.0165
	step [155/244], loss=54.6998
	step [156/244], loss=66.1874
	step [157/244], loss=72.4205
	step [158/244], loss=75.0434
	step [159/244], loss=73.2167
	step [160/244], loss=90.6837
	step [161/244], loss=80.0493
	step [162/244], loss=60.1225
	step [163/244], loss=69.4192
	step [164/244], loss=66.7615
	step [165/244], loss=72.9700
	step [166/244], loss=78.6700
	step [167/244], loss=74.8335
	step [168/244], loss=84.8904
	step [169/244], loss=62.8933
	step [170/244], loss=78.1475
	step [171/244], loss=83.0151
	step [172/244], loss=62.0437
	step [173/244], loss=64.7561
	step [174/244], loss=73.0064
	step [175/244], loss=85.6746
	step [176/244], loss=71.3426
	step [177/244], loss=83.5866
	step [178/244], loss=75.3190
	step [179/244], loss=59.9981
	step [180/244], loss=68.7817
	step [181/244], loss=73.4027
	step [182/244], loss=73.4368
	step [183/244], loss=92.8511
	step [184/244], loss=75.8160
	step [185/244], loss=79.9076
	step [186/244], loss=76.9725
	step [187/244], loss=69.9061
	step [188/244], loss=78.1004
	step [189/244], loss=82.5162
	step [190/244], loss=71.7519
	step [191/244], loss=68.5776
	step [192/244], loss=73.2871
	step [193/244], loss=78.2205
	step [194/244], loss=56.1453
	step [195/244], loss=84.6953
	step [196/244], loss=76.2176
	step [197/244], loss=80.2497
	step [198/244], loss=78.8425
	step [199/244], loss=75.3370
	step [200/244], loss=67.9148
	step [201/244], loss=81.1520
	step [202/244], loss=73.7700
	step [203/244], loss=78.1582
	step [204/244], loss=78.9119
	step [205/244], loss=71.0574
	step [206/244], loss=78.5970
	step [207/244], loss=75.7674
	step [208/244], loss=76.7908
	step [209/244], loss=67.2905
	step [210/244], loss=58.0717
	step [211/244], loss=87.7892
	step [212/244], loss=85.4722
	step [213/244], loss=59.8993
	step [214/244], loss=74.8212
	step [215/244], loss=66.8589
	step [216/244], loss=79.0426
	step [217/244], loss=76.1847
	step [218/244], loss=69.3845
	step [219/244], loss=76.6415
	step [220/244], loss=75.3847
	step [221/244], loss=73.0137
	step [222/244], loss=81.4102
	step [223/244], loss=68.0947
	step [224/244], loss=59.4374
	step [225/244], loss=79.5159
	step [226/244], loss=85.3842
	step [227/244], loss=70.6878
	step [228/244], loss=83.5177
	step [229/244], loss=71.1527
	step [230/244], loss=79.3959
	step [231/244], loss=70.6678
	step [232/244], loss=65.6469
	step [233/244], loss=91.5416
	step [234/244], loss=82.8812
	step [235/244], loss=62.3825
	step [236/244], loss=69.8060
	step [237/244], loss=75.9739
	step [238/244], loss=76.6110
	step [239/244], loss=87.8988
	step [240/244], loss=65.8505
	step [241/244], loss=64.8633
	step [242/244], loss=75.1031
	step [243/244], loss=78.4402
	step [244/244], loss=46.0129
	Evaluating
	loss=0.0092, precision=0.2917, recall=0.8674, f1=0.4365
Training epoch 98
	step [1/244], loss=78.5455
	step [2/244], loss=75.6477
	step [3/244], loss=78.6923
	step [4/244], loss=74.6243
	step [5/244], loss=63.9830
	step [6/244], loss=72.2901
	step [7/244], loss=71.4881
	step [8/244], loss=67.2215
	step [9/244], loss=71.3696
	step [10/244], loss=83.7242
	step [11/244], loss=77.2098
	step [12/244], loss=76.3298
	step [13/244], loss=78.0185
	step [14/244], loss=54.2778
	step [15/244], loss=89.7976
	step [16/244], loss=60.9959
	step [17/244], loss=67.4725
	step [18/244], loss=77.6194
	step [19/244], loss=64.8828
	step [20/244], loss=66.0988
	step [21/244], loss=70.5750
	step [22/244], loss=85.6994
	step [23/244], loss=47.3873
	step [24/244], loss=86.6794
	step [25/244], loss=73.3255
	step [26/244], loss=67.8785
	step [27/244], loss=79.2073
	step [28/244], loss=69.2889
	step [29/244], loss=83.3878
	step [30/244], loss=65.3261
	step [31/244], loss=61.9421
	step [32/244], loss=71.2747
	step [33/244], loss=68.1634
	step [34/244], loss=76.4265
	step [35/244], loss=62.3268
	step [36/244], loss=62.7505
	step [37/244], loss=68.1501
	step [38/244], loss=70.2389
	step [39/244], loss=60.8964
	step [40/244], loss=75.0173
	step [41/244], loss=67.0419
	step [42/244], loss=66.0705
	step [43/244], loss=62.4910
	step [44/244], loss=66.5675
	step [45/244], loss=72.1651
	step [46/244], loss=80.7413
	step [47/244], loss=79.5434
	step [48/244], loss=51.6117
	step [49/244], loss=58.4485
	step [50/244], loss=100.9444
	step [51/244], loss=76.7923
	step [52/244], loss=62.7551
	step [53/244], loss=85.3417
	step [54/244], loss=76.9607
	step [55/244], loss=88.8266
	step [56/244], loss=65.4417
	step [57/244], loss=83.0970
	step [58/244], loss=63.6382
	step [59/244], loss=70.2892
	step [60/244], loss=82.5021
	step [61/244], loss=73.3285
	step [62/244], loss=86.1423
	step [63/244], loss=86.3253
	step [64/244], loss=91.5958
	step [65/244], loss=67.7542
	step [66/244], loss=68.9279
	step [67/244], loss=81.8137
	step [68/244], loss=91.0877
	step [69/244], loss=78.9532
	step [70/244], loss=81.7835
	step [71/244], loss=66.3596
	step [72/244], loss=74.7221
	step [73/244], loss=71.8419
	step [74/244], loss=65.9340
	step [75/244], loss=75.7536
	step [76/244], loss=94.3623
	step [77/244], loss=67.4860
	step [78/244], loss=70.5523
	step [79/244], loss=65.4635
	step [80/244], loss=82.6479
	step [81/244], loss=69.8290
	step [82/244], loss=80.4895
	step [83/244], loss=81.9583
	step [84/244], loss=78.9035
	step [85/244], loss=89.0354
	step [86/244], loss=73.2376
	step [87/244], loss=72.1716
	step [88/244], loss=89.4950
	step [89/244], loss=73.5027
	step [90/244], loss=70.2391
	step [91/244], loss=69.3932
	step [92/244], loss=76.4095
	step [93/244], loss=74.4009
	step [94/244], loss=80.0595
	step [95/244], loss=76.4766
	step [96/244], loss=70.9519
	step [97/244], loss=69.2684
	step [98/244], loss=84.3062
	step [99/244], loss=89.9733
	step [100/244], loss=74.2872
	step [101/244], loss=63.1347
	step [102/244], loss=80.2951
	step [103/244], loss=75.9622
	step [104/244], loss=72.4865
	step [105/244], loss=89.1659
	step [106/244], loss=94.2878
	step [107/244], loss=63.1502
	step [108/244], loss=78.8405
	step [109/244], loss=82.6625
	step [110/244], loss=55.9137
	step [111/244], loss=66.5233
	step [112/244], loss=77.7673
	step [113/244], loss=93.2752
	step [114/244], loss=65.0292
	step [115/244], loss=68.1140
	step [116/244], loss=75.9114
	step [117/244], loss=85.2639
	step [118/244], loss=94.8269
	step [119/244], loss=76.8611
	step [120/244], loss=64.3771
	step [121/244], loss=77.7788
	step [122/244], loss=62.7141
	step [123/244], loss=73.8003
	step [124/244], loss=68.1536
	step [125/244], loss=84.6997
	step [126/244], loss=80.8733
	step [127/244], loss=71.4276
	step [128/244], loss=77.3708
	step [129/244], loss=57.0358
	step [130/244], loss=80.5917
	step [131/244], loss=73.7150
	step [132/244], loss=64.6206
	step [133/244], loss=68.4337
	step [134/244], loss=75.4293
	step [135/244], loss=70.8886
	step [136/244], loss=83.8649
	step [137/244], loss=81.4838
	step [138/244], loss=61.1043
	step [139/244], loss=76.9823
	step [140/244], loss=67.4896
	step [141/244], loss=71.4752
	step [142/244], loss=64.1331
	step [143/244], loss=79.6547
	step [144/244], loss=78.8675
	step [145/244], loss=64.3159
	step [146/244], loss=67.5024
	step [147/244], loss=91.4094
	step [148/244], loss=75.9682
	step [149/244], loss=75.7318
	step [150/244], loss=93.4946
	step [151/244], loss=69.3353
	step [152/244], loss=87.8401
	step [153/244], loss=72.1379
	step [154/244], loss=79.2570
	step [155/244], loss=82.8907
	step [156/244], loss=84.0369
	step [157/244], loss=66.7233
	step [158/244], loss=56.5044
	step [159/244], loss=69.7023
	step [160/244], loss=71.9017
	step [161/244], loss=68.5113
	step [162/244], loss=81.3909
	step [163/244], loss=80.0859
	step [164/244], loss=66.2536
	step [165/244], loss=68.7456
	step [166/244], loss=78.5666
	step [167/244], loss=91.5041
	step [168/244], loss=55.8310
	step [169/244], loss=62.4265
	step [170/244], loss=58.3962
	step [171/244], loss=95.8878
	step [172/244], loss=63.6451
	step [173/244], loss=83.2820
	step [174/244], loss=76.4315
	step [175/244], loss=69.4747
	step [176/244], loss=81.1487
	step [177/244], loss=72.1910
	step [178/244], loss=72.0203
	step [179/244], loss=70.4945
	step [180/244], loss=80.5932
	step [181/244], loss=79.0696
	step [182/244], loss=78.2495
	step [183/244], loss=71.7177
	step [184/244], loss=63.7292
	step [185/244], loss=64.0482
	step [186/244], loss=89.2511
	step [187/244], loss=66.0769
	step [188/244], loss=69.0778
	step [189/244], loss=64.6452
	step [190/244], loss=70.6733
	step [191/244], loss=58.8030
	step [192/244], loss=86.3917
	step [193/244], loss=80.2223
	step [194/244], loss=75.1128
	step [195/244], loss=76.6862
	step [196/244], loss=78.8852
	step [197/244], loss=68.9932
	step [198/244], loss=81.4238
	step [199/244], loss=81.0211
	step [200/244], loss=76.8127
	step [201/244], loss=71.7300
	step [202/244], loss=68.1728
	step [203/244], loss=74.8042
	step [204/244], loss=82.4391
	step [205/244], loss=72.1909
	step [206/244], loss=71.4872
	step [207/244], loss=58.5191
	step [208/244], loss=73.4193
	step [209/244], loss=69.1151
	step [210/244], loss=72.5806
	step [211/244], loss=69.7965
	step [212/244], loss=88.9097
	step [213/244], loss=76.7881
	step [214/244], loss=86.1964
	step [215/244], loss=77.4151
	step [216/244], loss=69.4926
	step [217/244], loss=71.8127
	step [218/244], loss=84.6009
	step [219/244], loss=77.8774
	step [220/244], loss=59.5360
	step [221/244], loss=64.8650
	step [222/244], loss=78.4360
	step [223/244], loss=88.8624
	step [224/244], loss=68.2567
	step [225/244], loss=59.6923
	step [226/244], loss=88.2387
	step [227/244], loss=67.7849
	step [228/244], loss=60.2974
	step [229/244], loss=70.2257
	step [230/244], loss=63.5216
	step [231/244], loss=80.1687
	step [232/244], loss=74.1907
	step [233/244], loss=86.1441
	step [234/244], loss=64.6942
	step [235/244], loss=83.3132
	step [236/244], loss=74.5752
	step [237/244], loss=61.8184
	step [238/244], loss=70.4860
	step [239/244], loss=82.1043
	step [240/244], loss=74.6170
	step [241/244], loss=79.2551
	step [242/244], loss=76.7657
	step [243/244], loss=76.6081
	step [244/244], loss=33.5172
	Evaluating
	loss=0.0071, precision=0.3708, recall=0.8630, f1=0.5187
Training epoch 99
	step [1/244], loss=69.9450
	step [2/244], loss=72.5701
	step [3/244], loss=71.9156
	step [4/244], loss=71.1133
	step [5/244], loss=62.1922
	step [6/244], loss=80.8077
	step [7/244], loss=68.9496
	step [8/244], loss=85.0933
	step [9/244], loss=75.2370
	step [10/244], loss=71.5664
	step [11/244], loss=81.6132
	step [12/244], loss=66.4662
	step [13/244], loss=65.0025
	step [14/244], loss=65.2103
	step [15/244], loss=71.6606
	step [16/244], loss=78.8692
	step [17/244], loss=90.4933
	step [18/244], loss=70.6535
	step [19/244], loss=91.1979
	step [20/244], loss=61.8841
	step [21/244], loss=66.9844
	step [22/244], loss=70.9634
	step [23/244], loss=88.6066
	step [24/244], loss=66.6591
	step [25/244], loss=101.6706
	step [26/244], loss=75.5365
	step [27/244], loss=96.7506
	step [28/244], loss=77.1257
	step [29/244], loss=80.9159
	step [30/244], loss=71.7681
	step [31/244], loss=83.2722
	step [32/244], loss=76.7698
	step [33/244], loss=73.8714
	step [34/244], loss=78.1559
	step [35/244], loss=74.0657
	step [36/244], loss=66.4003
	step [37/244], loss=67.2600
	step [38/244], loss=74.9229
	step [39/244], loss=78.8905
	step [40/244], loss=65.4440
	step [41/244], loss=76.9679
	step [42/244], loss=68.8681
	step [43/244], loss=72.9871
	step [44/244], loss=66.0852
	step [45/244], loss=92.2420
	step [46/244], loss=57.8671
	step [47/244], loss=71.9755
	step [48/244], loss=78.0318
	step [49/244], loss=72.1541
	step [50/244], loss=61.5244
	step [51/244], loss=77.6961
	step [52/244], loss=59.7021
	step [53/244], loss=65.8096
	step [54/244], loss=68.9413
	step [55/244], loss=66.0697
	step [56/244], loss=61.4908
	step [57/244], loss=69.3523
	step [58/244], loss=79.8339
	step [59/244], loss=70.4024
	step [60/244], loss=72.3855
	step [61/244], loss=72.4691
	step [62/244], loss=65.5969
	step [63/244], loss=79.7525
	step [64/244], loss=68.9162
	step [65/244], loss=78.0440
	step [66/244], loss=70.7399
	step [67/244], loss=70.5983
	step [68/244], loss=86.6037
	step [69/244], loss=72.1014
	step [70/244], loss=80.6653
	step [71/244], loss=74.1622
	step [72/244], loss=66.3473
	step [73/244], loss=74.3398
	step [74/244], loss=82.8967
	step [75/244], loss=98.1885
	step [76/244], loss=73.1047
	step [77/244], loss=67.4235
	step [78/244], loss=80.8879
	step [79/244], loss=73.8590
	step [80/244], loss=79.1661
	step [81/244], loss=56.3884
	step [82/244], loss=80.9924
	step [83/244], loss=81.4922
	step [84/244], loss=58.5005
	step [85/244], loss=66.5493
	step [86/244], loss=86.2247
	step [87/244], loss=73.6438
	step [88/244], loss=64.6451
	step [89/244], loss=73.1902
	step [90/244], loss=84.0790
	step [91/244], loss=77.8946
	step [92/244], loss=83.3876
	step [93/244], loss=55.3246
	step [94/244], loss=68.1756
	step [95/244], loss=66.8401
	step [96/244], loss=71.1335
	step [97/244], loss=69.7078
	step [98/244], loss=66.1816
	step [99/244], loss=92.7736
	step [100/244], loss=74.0521
	step [101/244], loss=76.7026
	step [102/244], loss=83.8152
	step [103/244], loss=65.5584
	step [104/244], loss=76.1271
	step [105/244], loss=65.0153
	step [106/244], loss=77.0914
	step [107/244], loss=77.8422
	step [108/244], loss=91.3589
	step [109/244], loss=77.7071
	step [110/244], loss=57.9987
	step [111/244], loss=85.5862
	step [112/244], loss=75.1417
	step [113/244], loss=74.8878
	step [114/244], loss=64.9025
	step [115/244], loss=74.4095
	step [116/244], loss=67.2245
	step [117/244], loss=81.2965
	step [118/244], loss=94.2554
	step [119/244], loss=80.0496
	step [120/244], loss=66.9942
	step [121/244], loss=74.6583
	step [122/244], loss=74.6488
	step [123/244], loss=83.6207
	step [124/244], loss=83.2845
	step [125/244], loss=87.0954
	step [126/244], loss=70.9570
	step [127/244], loss=69.1802
	step [128/244], loss=62.6483
	step [129/244], loss=64.7150
	step [130/244], loss=65.5007
	step [131/244], loss=80.1402
	step [132/244], loss=75.4725
	step [133/244], loss=85.6753
	step [134/244], loss=59.7707
	step [135/244], loss=75.4609
	step [136/244], loss=66.6164
	step [137/244], loss=67.1104
	step [138/244], loss=62.7427
	step [139/244], loss=60.5482
	step [140/244], loss=81.6733
	step [141/244], loss=78.5386
	step [142/244], loss=62.1658
	step [143/244], loss=77.7935
	step [144/244], loss=54.6274
	step [145/244], loss=73.1219
	step [146/244], loss=73.6664
	step [147/244], loss=74.2436
	step [148/244], loss=76.2586
	step [149/244], loss=62.3963
	step [150/244], loss=87.6511
	step [151/244], loss=80.0547
	step [152/244], loss=65.5563
	step [153/244], loss=76.6813
	step [154/244], loss=70.2671
	step [155/244], loss=68.8526
	step [156/244], loss=99.3380
	step [157/244], loss=56.9776
	step [158/244], loss=73.3035
	step [159/244], loss=78.7646
	step [160/244], loss=70.7673
	step [161/244], loss=72.9446
	step [162/244], loss=72.5140
	step [163/244], loss=67.9060
	step [164/244], loss=86.3522
	step [165/244], loss=66.2779
	step [166/244], loss=55.8839
	step [167/244], loss=70.1586
	step [168/244], loss=71.7783
	step [169/244], loss=72.0219
	step [170/244], loss=82.1384
	step [171/244], loss=89.8736
	step [172/244], loss=78.0829
	step [173/244], loss=59.0808
	step [174/244], loss=67.0074
	step [175/244], loss=96.0179
	step [176/244], loss=84.3594
	step [177/244], loss=77.3749
	step [178/244], loss=75.0286
	step [179/244], loss=69.9065
	step [180/244], loss=84.6901
	step [181/244], loss=71.4602
	step [182/244], loss=76.7597
	step [183/244], loss=71.3508
	step [184/244], loss=73.9111
	step [185/244], loss=61.2724
	step [186/244], loss=86.4075
	step [187/244], loss=87.3917
	step [188/244], loss=81.8785
	step [189/244], loss=74.8319
	step [190/244], loss=57.3062
	step [191/244], loss=69.6376
	step [192/244], loss=72.4322
	step [193/244], loss=72.6280
	step [194/244], loss=68.1268
	step [195/244], loss=76.0258
	step [196/244], loss=79.3880
	step [197/244], loss=68.3425
	step [198/244], loss=84.2163
	step [199/244], loss=90.1562
	step [200/244], loss=71.0190
	step [201/244], loss=59.2394
	step [202/244], loss=73.8124
	step [203/244], loss=79.5096
	step [204/244], loss=73.6843
	step [205/244], loss=81.5966
	step [206/244], loss=67.5501
	step [207/244], loss=59.5116
	step [208/244], loss=75.4171
	step [209/244], loss=56.0921
	step [210/244], loss=80.4381
	step [211/244], loss=73.6976
	step [212/244], loss=58.0829
	step [213/244], loss=89.5404
	step [214/244], loss=73.1859
	step [215/244], loss=84.6187
	step [216/244], loss=69.9837
	step [217/244], loss=93.9523
	step [218/244], loss=72.3266
	step [219/244], loss=65.4343
	step [220/244], loss=73.3932
	step [221/244], loss=78.6900
	step [222/244], loss=78.3514
	step [223/244], loss=62.8557
	step [224/244], loss=68.3717
	step [225/244], loss=79.7965
	step [226/244], loss=83.4127
	step [227/244], loss=78.5142
	step [228/244], loss=71.8254
	step [229/244], loss=78.9376
	step [230/244], loss=77.1795
	step [231/244], loss=72.2937
	step [232/244], loss=74.5482
	step [233/244], loss=76.2869
	step [234/244], loss=71.4111
	step [235/244], loss=67.6952
	step [236/244], loss=83.4376
	step [237/244], loss=73.5399
	step [238/244], loss=65.6064
	step [239/244], loss=83.8981
	step [240/244], loss=70.9336
	step [241/244], loss=77.5834
	step [242/244], loss=75.5491
	step [243/244], loss=66.7216
	step [244/244], loss=38.8327
	Evaluating
	loss=0.0065, precision=0.3974, recall=0.8662, f1=0.5449
saving model as: 2_saved_model.pth
Training epoch 100
	step [1/244], loss=63.7749
	step [2/244], loss=83.2188
	step [3/244], loss=78.7628
	step [4/244], loss=89.8645
	step [5/244], loss=77.4711
	step [6/244], loss=55.4480
	step [7/244], loss=58.9503
	step [8/244], loss=71.7948
	step [9/244], loss=73.8282
	step [10/244], loss=76.5973
	step [11/244], loss=75.6709
	step [12/244], loss=64.1969
	step [13/244], loss=75.0803
	step [14/244], loss=64.8168
	step [15/244], loss=84.0089
	step [16/244], loss=94.5796
	step [17/244], loss=65.0304
	step [18/244], loss=62.1983
	step [19/244], loss=72.6283
	step [20/244], loss=80.2590
	step [21/244], loss=72.1932
	step [22/244], loss=66.5281
	step [23/244], loss=75.2008
	step [24/244], loss=58.1606
	step [25/244], loss=75.4841
	step [26/244], loss=90.5644
	step [27/244], loss=71.6944
	step [28/244], loss=51.4133
	step [29/244], loss=63.2511
	step [30/244], loss=73.9543
	step [31/244], loss=76.4720
	step [32/244], loss=76.8396
	step [33/244], loss=76.3531
	step [34/244], loss=87.6158
	step [35/244], loss=76.0608
	step [36/244], loss=80.6779
	step [37/244], loss=84.7171
	step [38/244], loss=76.5962
	step [39/244], loss=64.7567
	step [40/244], loss=70.5283
	step [41/244], loss=87.7514
	step [42/244], loss=96.9255
	step [43/244], loss=71.8644
	step [44/244], loss=48.9051
	step [45/244], loss=68.9076
	step [46/244], loss=65.7676
	step [47/244], loss=67.7174
	step [48/244], loss=70.1985
	step [49/244], loss=68.9669
	step [50/244], loss=79.7219
	step [51/244], loss=63.9105
	step [52/244], loss=76.3816
	step [53/244], loss=68.3649
	step [54/244], loss=85.5203
	step [55/244], loss=71.8249
	step [56/244], loss=64.1988
	step [57/244], loss=70.5018
	step [58/244], loss=56.5533
	step [59/244], loss=88.3691
	step [60/244], loss=79.1228
	step [61/244], loss=82.1132
	step [62/244], loss=68.2276
	step [63/244], loss=85.6546
	step [64/244], loss=75.7709
	step [65/244], loss=102.0417
	step [66/244], loss=70.4200
	step [67/244], loss=83.5914
	step [68/244], loss=59.2251
	step [69/244], loss=74.6010
	step [70/244], loss=80.1784
	step [71/244], loss=75.5996
	step [72/244], loss=87.9686
	step [73/244], loss=75.6137
	step [74/244], loss=79.3269
	step [75/244], loss=75.5110
	step [76/244], loss=57.9136
	step [77/244], loss=59.9143
	step [78/244], loss=81.3792
	step [79/244], loss=54.8153
	step [80/244], loss=74.5394
	step [81/244], loss=62.7769
	step [82/244], loss=72.6815
	step [83/244], loss=59.7198
	step [84/244], loss=79.6110
	step [85/244], loss=59.8784
	step [86/244], loss=78.4525
	step [87/244], loss=81.5119
	step [88/244], loss=78.3699
	step [89/244], loss=63.6934
	step [90/244], loss=87.8284
	step [91/244], loss=85.1152
	step [92/244], loss=71.0586
	step [93/244], loss=81.0553
	step [94/244], loss=71.1477
	step [95/244], loss=62.6980
	step [96/244], loss=77.8767
	step [97/244], loss=60.3115
	step [98/244], loss=77.3282
	step [99/244], loss=78.6225
	step [100/244], loss=62.8627
	step [101/244], loss=80.7681
	step [102/244], loss=76.8325
	step [103/244], loss=68.4073
	step [104/244], loss=63.0267
	step [105/244], loss=75.5723
	step [106/244], loss=70.9532
	step [107/244], loss=67.8208
	step [108/244], loss=52.6760
	step [109/244], loss=58.2867
	step [110/244], loss=71.6203
	step [111/244], loss=78.2117
	step [112/244], loss=81.1365
	step [113/244], loss=63.3607
	step [114/244], loss=63.1892
	step [115/244], loss=70.6424
	step [116/244], loss=72.4144
	step [117/244], loss=55.9022
	step [118/244], loss=76.6963
	step [119/244], loss=74.3989
	step [120/244], loss=102.3480
	step [121/244], loss=77.8434
	step [122/244], loss=66.7105
	step [123/244], loss=65.5505
	step [124/244], loss=78.3529
	step [125/244], loss=74.4769
	step [126/244], loss=72.5679
	step [127/244], loss=76.7752
	step [128/244], loss=72.5809
	step [129/244], loss=72.3013
	step [130/244], loss=66.0236
	step [131/244], loss=81.9186
	step [132/244], loss=65.8868
	step [133/244], loss=78.0128
	step [134/244], loss=83.5690
	step [135/244], loss=90.1237
	step [136/244], loss=74.6158
	step [137/244], loss=66.5799
	step [138/244], loss=78.7295
	step [139/244], loss=60.3319
	step [140/244], loss=78.1119
	step [141/244], loss=80.4661
	step [142/244], loss=74.3877
	step [143/244], loss=64.2638
	step [144/244], loss=82.0834
	step [145/244], loss=70.5366
	step [146/244], loss=89.7322
	step [147/244], loss=78.3163
	step [148/244], loss=77.4111
	step [149/244], loss=78.6594
	step [150/244], loss=81.0258
	step [151/244], loss=70.2359
	step [152/244], loss=68.4814
	step [153/244], loss=62.8217
	step [154/244], loss=80.7309
	step [155/244], loss=72.4228
	step [156/244], loss=85.3941
	step [157/244], loss=88.7166
	step [158/244], loss=59.3886
	step [159/244], loss=66.5024
	step [160/244], loss=72.2535
	step [161/244], loss=76.3616
	step [162/244], loss=81.0332
	step [163/244], loss=69.8549
	step [164/244], loss=78.6035
	step [165/244], loss=60.6398
	step [166/244], loss=61.3912
	step [167/244], loss=73.6268
	step [168/244], loss=60.7178
	step [169/244], loss=73.6538
	step [170/244], loss=68.4509
	step [171/244], loss=76.8504
	step [172/244], loss=75.0639
	step [173/244], loss=71.2250
	step [174/244], loss=71.3432
	step [175/244], loss=89.7102
	step [176/244], loss=81.7207
	step [177/244], loss=68.9647
	step [178/244], loss=65.8751
	step [179/244], loss=78.8313
	step [180/244], loss=83.9855
	step [181/244], loss=80.1631
	step [182/244], loss=85.6616
	step [183/244], loss=63.7382
	step [184/244], loss=83.4081
	step [185/244], loss=70.5880
	step [186/244], loss=80.4928
	step [187/244], loss=67.5203
	step [188/244], loss=71.3630
	step [189/244], loss=50.1925
	step [190/244], loss=73.9973
	step [191/244], loss=66.0035
	step [192/244], loss=85.0442
	step [193/244], loss=73.3205
	step [194/244], loss=75.3597
	step [195/244], loss=75.8969
	step [196/244], loss=67.9221
	step [197/244], loss=64.9420
	step [198/244], loss=67.3603
	step [199/244], loss=80.2230
	step [200/244], loss=74.2231
	step [201/244], loss=88.9154
	step [202/244], loss=86.2507
	step [203/244], loss=69.3458
	step [204/244], loss=75.3483
	step [205/244], loss=84.2744
	step [206/244], loss=59.8331
	step [207/244], loss=56.1487
	step [208/244], loss=70.9302
	step [209/244], loss=70.8607
	step [210/244], loss=70.8810
	step [211/244], loss=67.0293
	step [212/244], loss=87.5641
	step [213/244], loss=74.1555
	step [214/244], loss=65.2920
	step [215/244], loss=60.2435
	step [216/244], loss=75.8603
	step [217/244], loss=63.1512
	step [218/244], loss=72.3224
	step [219/244], loss=85.8433
	step [220/244], loss=68.1875
	step [221/244], loss=76.3389
	step [222/244], loss=82.7259
	step [223/244], loss=80.1539
	step [224/244], loss=89.0511
	step [225/244], loss=86.6184
	step [226/244], loss=66.4838
	step [227/244], loss=65.3109
	step [228/244], loss=76.3908
	step [229/244], loss=69.7151
	step [230/244], loss=74.0919
	step [231/244], loss=85.1157
	step [232/244], loss=66.1998
	step [233/244], loss=86.6249
	step [234/244], loss=72.6510
	step [235/244], loss=69.8237
	step [236/244], loss=68.0213
	step [237/244], loss=85.3572
	step [238/244], loss=66.7266
	step [239/244], loss=84.4512
	step [240/244], loss=84.4463
	step [241/244], loss=67.4185
	step [242/244], loss=58.6851
	step [243/244], loss=91.5798
	step [244/244], loss=37.4244
	Evaluating
	loss=0.0071, precision=0.3724, recall=0.8669, f1=0.5210
Training epoch 101
	step [1/244], loss=81.4395
	step [2/244], loss=81.2026
	step [3/244], loss=61.9157
	step [4/244], loss=76.2250
	step [5/244], loss=82.9287
	step [6/244], loss=72.3831
	step [7/244], loss=76.5656
	step [8/244], loss=70.3888
	step [9/244], loss=62.8633
	step [10/244], loss=68.2235
	step [11/244], loss=80.0488
	step [12/244], loss=71.3920
	step [13/244], loss=86.2350
	step [14/244], loss=88.1263
	step [15/244], loss=70.0200
	step [16/244], loss=74.6372
	step [17/244], loss=64.9827
	step [18/244], loss=68.5835
	step [19/244], loss=68.8465
	step [20/244], loss=65.9391
	step [21/244], loss=67.4322
	step [22/244], loss=66.7110
	step [23/244], loss=68.3237
	step [24/244], loss=88.0873
	step [25/244], loss=64.0706
	step [26/244], loss=91.2137
	step [27/244], loss=67.7672
	step [28/244], loss=88.3318
	step [29/244], loss=57.4762
	step [30/244], loss=68.8067
	step [31/244], loss=91.6978
	step [32/244], loss=66.1198
	step [33/244], loss=72.3867
	step [34/244], loss=82.0586
	step [35/244], loss=82.1822
	step [36/244], loss=75.1034
	step [37/244], loss=75.4771
	step [38/244], loss=76.1061
	step [39/244], loss=79.5078
	step [40/244], loss=80.2014
	step [41/244], loss=70.5006
	step [42/244], loss=75.3769
	step [43/244], loss=75.9545
	step [44/244], loss=76.1871
	step [45/244], loss=78.7290
	step [46/244], loss=62.4847
	step [47/244], loss=73.2046
	step [48/244], loss=83.7519
	step [49/244], loss=73.6342
	step [50/244], loss=88.9008
	step [51/244], loss=66.0714
	step [52/244], loss=87.6237
	step [53/244], loss=84.0414
	step [54/244], loss=84.1293
	step [55/244], loss=74.1969
	step [56/244], loss=70.1774
	step [57/244], loss=70.5932
	step [58/244], loss=73.6170
	step [59/244], loss=65.0048
	step [60/244], loss=68.4954
	step [61/244], loss=67.5355
	step [62/244], loss=71.2894
	step [63/244], loss=67.4322
	step [64/244], loss=73.8559
	step [65/244], loss=65.9018
	step [66/244], loss=76.8255
	step [67/244], loss=77.9948
	step [68/244], loss=82.6414
	step [69/244], loss=68.1776
	step [70/244], loss=87.4195
	step [71/244], loss=76.1948
	step [72/244], loss=68.1394
	step [73/244], loss=73.7912
	step [74/244], loss=73.2574
	step [75/244], loss=97.3115
	step [76/244], loss=64.7238
	step [77/244], loss=67.6071
	step [78/244], loss=65.8360
	step [79/244], loss=70.8633
	step [80/244], loss=87.2900
	step [81/244], loss=76.3124
	step [82/244], loss=66.2120
	step [83/244], loss=74.9320
	step [84/244], loss=58.9801
	step [85/244], loss=74.1197
	step [86/244], loss=65.9776
	step [87/244], loss=81.7937
	step [88/244], loss=81.3873
	step [89/244], loss=65.9697
	step [90/244], loss=70.9101
	step [91/244], loss=67.6605
	step [92/244], loss=74.4426
	step [93/244], loss=61.8640
	step [94/244], loss=72.7232
	step [95/244], loss=85.2316
	step [96/244], loss=81.0837
	step [97/244], loss=67.0402
	step [98/244], loss=84.4914
	step [99/244], loss=77.5831
	step [100/244], loss=82.3965
	step [101/244], loss=62.8424
	step [102/244], loss=85.0101
	step [103/244], loss=69.1932
	step [104/244], loss=74.7876
	step [105/244], loss=71.7813
	step [106/244], loss=64.0389
	step [107/244], loss=64.2642
	step [108/244], loss=71.4757
	step [109/244], loss=73.6381
	step [110/244], loss=60.2659
	step [111/244], loss=75.1459
	step [112/244], loss=60.4538
	step [113/244], loss=64.0528
	step [114/244], loss=72.8316
	step [115/244], loss=80.6087
	step [116/244], loss=76.4757
	step [117/244], loss=70.4698
	step [118/244], loss=78.3221
	step [119/244], loss=72.4168
	step [120/244], loss=59.4728
	step [121/244], loss=76.6635
	step [122/244], loss=74.3437
	step [123/244], loss=65.3599
	step [124/244], loss=66.7738
	step [125/244], loss=87.7843
	step [126/244], loss=67.4151
	step [127/244], loss=68.0811
	step [128/244], loss=71.2397
	step [129/244], loss=74.2826
	step [130/244], loss=69.0198
	step [131/244], loss=68.7713
	step [132/244], loss=72.0592
	step [133/244], loss=72.2288
	step [134/244], loss=80.5050
	step [135/244], loss=55.9755
	step [136/244], loss=58.1775
	step [137/244], loss=75.8968
	step [138/244], loss=61.2352
	step [139/244], loss=71.7577
	step [140/244], loss=69.8156
	step [141/244], loss=81.7721
	step [142/244], loss=75.4479
	step [143/244], loss=77.3544
	step [144/244], loss=65.4581
	step [145/244], loss=69.6439
	step [146/244], loss=67.8838
	step [147/244], loss=68.8396
	step [148/244], loss=75.1338
	step [149/244], loss=84.7129
	step [150/244], loss=79.7074
	step [151/244], loss=71.1201
	step [152/244], loss=68.5585
	step [153/244], loss=68.0736
	step [154/244], loss=69.6695
	step [155/244], loss=92.0509
	step [156/244], loss=66.5495
	step [157/244], loss=72.8507
	step [158/244], loss=75.2871
	step [159/244], loss=83.6650
	step [160/244], loss=64.7548
	step [161/244], loss=68.1001
	step [162/244], loss=88.0927
	step [163/244], loss=67.1105
	step [164/244], loss=64.4333
	step [165/244], loss=84.1377
	step [166/244], loss=78.2481
	step [167/244], loss=63.4702
	step [168/244], loss=83.0410
	step [169/244], loss=69.6249
	step [170/244], loss=69.4575
	step [171/244], loss=62.8150
	step [172/244], loss=66.2861
	step [173/244], loss=65.5898
	step [174/244], loss=96.9351
	step [175/244], loss=78.3517
	step [176/244], loss=80.5584
	step [177/244], loss=84.0824
	step [178/244], loss=84.9281
	step [179/244], loss=78.5887
	step [180/244], loss=81.3467
	step [181/244], loss=73.7835
	step [182/244], loss=85.1539
	step [183/244], loss=70.7213
	step [184/244], loss=68.0487
	step [185/244], loss=83.0912
	step [186/244], loss=66.8336
	step [187/244], loss=66.7039
	step [188/244], loss=79.6322
	step [189/244], loss=77.6337
	step [190/244], loss=69.4765
	step [191/244], loss=78.2523
	step [192/244], loss=78.6625
	step [193/244], loss=70.9870
	step [194/244], loss=77.1020
	step [195/244], loss=69.1578
	step [196/244], loss=60.4855
	step [197/244], loss=57.8844
	step [198/244], loss=78.0348
	step [199/244], loss=75.7918
	step [200/244], loss=71.6141
	step [201/244], loss=73.7526
	step [202/244], loss=75.5155
	step [203/244], loss=86.8898
	step [204/244], loss=68.2105
	step [205/244], loss=60.2151
	step [206/244], loss=71.9618
	step [207/244], loss=73.3158
	step [208/244], loss=78.1049
	step [209/244], loss=72.0178
	step [210/244], loss=59.3898
	step [211/244], loss=73.1287
	step [212/244], loss=92.3778
	step [213/244], loss=68.0908
	step [214/244], loss=79.4797
	step [215/244], loss=97.1839
	step [216/244], loss=69.0488
	step [217/244], loss=75.0687
	step [218/244], loss=67.8074
	step [219/244], loss=81.4131
	step [220/244], loss=69.4618
	step [221/244], loss=74.0745
	step [222/244], loss=66.8647
	step [223/244], loss=83.0521
	step [224/244], loss=54.8931
	step [225/244], loss=77.3085
	step [226/244], loss=74.6642
	step [227/244], loss=72.5486
	step [228/244], loss=70.2718
	step [229/244], loss=66.3167
	step [230/244], loss=63.2789
	step [231/244], loss=63.5263
	step [232/244], loss=77.2419
	step [233/244], loss=76.1004
	step [234/244], loss=71.2392
	step [235/244], loss=80.2677
	step [236/244], loss=73.8348
	step [237/244], loss=60.5328
	step [238/244], loss=57.5945
	step [239/244], loss=68.4490
	step [240/244], loss=77.3921
	step [241/244], loss=64.3667
	step [242/244], loss=64.4886
	step [243/244], loss=78.2551
	step [244/244], loss=34.4835
	Evaluating
	loss=0.0069, precision=0.3786, recall=0.8871, f1=0.5307
Training epoch 102
	step [1/244], loss=82.1888
	step [2/244], loss=69.3796
	step [3/244], loss=63.3299
	step [4/244], loss=74.9779
	step [5/244], loss=76.4523
	step [6/244], loss=72.6926
	step [7/244], loss=62.3222
	step [8/244], loss=67.3828
	step [9/244], loss=79.3470
	step [10/244], loss=72.2692
	step [11/244], loss=66.2221
	step [12/244], loss=72.3492
	step [13/244], loss=64.7499
	step [14/244], loss=61.7027
	step [15/244], loss=69.9280
	step [16/244], loss=66.9407
	step [17/244], loss=74.0013
	step [18/244], loss=74.9486
	step [19/244], loss=74.7578
	step [20/244], loss=71.0471
	step [21/244], loss=80.8596
	step [22/244], loss=77.9409
	step [23/244], loss=68.6088
	step [24/244], loss=91.0905
	step [25/244], loss=65.4980
	step [26/244], loss=78.0350
	step [27/244], loss=67.7922
	step [28/244], loss=74.5033
	step [29/244], loss=91.2606
	step [30/244], loss=78.1494
	step [31/244], loss=67.2292
	step [32/244], loss=71.6230
	step [33/244], loss=64.8261
	step [34/244], loss=66.4312
	step [35/244], loss=64.2565
	step [36/244], loss=68.9355
	step [37/244], loss=63.6305
	step [38/244], loss=87.5910
	step [39/244], loss=59.1624
	step [40/244], loss=73.4212
	step [41/244], loss=74.3028
	step [42/244], loss=81.8814
	step [43/244], loss=83.8560
	step [44/244], loss=76.4822
	step [45/244], loss=81.8926
	step [46/244], loss=76.2574
	step [47/244], loss=74.1232
	step [48/244], loss=58.3837
	step [49/244], loss=84.0207
	step [50/244], loss=82.5412
	step [51/244], loss=70.3996
	step [52/244], loss=71.4950
	step [53/244], loss=68.5260
	step [54/244], loss=67.6358
	step [55/244], loss=58.4218
	step [56/244], loss=68.9737
	step [57/244], loss=68.5043
	step [58/244], loss=81.2470
	step [59/244], loss=63.1745
	step [60/244], loss=70.1465
	step [61/244], loss=84.6327
	step [62/244], loss=85.9909
	step [63/244], loss=61.3565
	step [64/244], loss=72.9652
	step [65/244], loss=52.2748
	step [66/244], loss=68.8088
	step [67/244], loss=74.3158
	step [68/244], loss=71.1073
	step [69/244], loss=55.4597
	step [70/244], loss=72.5561
	step [71/244], loss=77.4396
	step [72/244], loss=67.9616
	step [73/244], loss=73.0965
	step [74/244], loss=76.3046
	step [75/244], loss=75.5746
	step [76/244], loss=72.9152
	step [77/244], loss=72.7488
	step [78/244], loss=69.5989
	step [79/244], loss=69.4576
	step [80/244], loss=86.8730
	step [81/244], loss=53.6105
	step [82/244], loss=60.5152
	step [83/244], loss=71.3557
	step [84/244], loss=81.8652
	step [85/244], loss=83.8433
	step [86/244], loss=77.2781
	step [87/244], loss=83.0155
	step [88/244], loss=54.0837
	step [89/244], loss=86.0928
	step [90/244], loss=80.1949
	step [91/244], loss=81.5456
	step [92/244], loss=67.9914
	step [93/244], loss=82.4933
	step [94/244], loss=70.6816
	step [95/244], loss=61.1284
	step [96/244], loss=82.8515
	step [97/244], loss=73.1455
	step [98/244], loss=69.9263
	step [99/244], loss=72.2976
	step [100/244], loss=78.3619
	step [101/244], loss=67.6411
	step [102/244], loss=88.4134
	step [103/244], loss=73.5487
	step [104/244], loss=79.7820
	step [105/244], loss=60.5874
	step [106/244], loss=84.1379
	step [107/244], loss=84.1534
	step [108/244], loss=77.1795
	step [109/244], loss=60.9914
	step [110/244], loss=75.3496
	step [111/244], loss=62.9238
	step [112/244], loss=79.0717
	step [113/244], loss=89.7479
	step [114/244], loss=79.0917
	step [115/244], loss=59.1935
	step [116/244], loss=63.5472
	step [117/244], loss=79.5896
	step [118/244], loss=68.3377
	step [119/244], loss=69.8887
	step [120/244], loss=66.7586
	step [121/244], loss=76.5025
	step [122/244], loss=71.7815
	step [123/244], loss=73.1398
	step [124/244], loss=74.0363
	step [125/244], loss=82.4708
	step [126/244], loss=86.2914
	step [127/244], loss=56.2657
	step [128/244], loss=77.8958
	step [129/244], loss=71.9685
	step [130/244], loss=68.4832
	step [131/244], loss=85.5220
	step [132/244], loss=77.6524
	step [133/244], loss=85.3539
	step [134/244], loss=73.4257
	step [135/244], loss=67.7640
	step [136/244], loss=91.8305
	step [137/244], loss=85.4295
	step [138/244], loss=77.3368
	step [139/244], loss=77.5113
	step [140/244], loss=69.3260
	step [141/244], loss=60.3009
	step [142/244], loss=79.8408
	step [143/244], loss=78.7119
	step [144/244], loss=77.9934
	step [145/244], loss=80.2318
	step [146/244], loss=66.4697
	step [147/244], loss=61.6648
	step [148/244], loss=66.1470
	step [149/244], loss=73.6528
	step [150/244], loss=84.2161
	step [151/244], loss=63.3947
	step [152/244], loss=77.9173
	step [153/244], loss=58.4705
	step [154/244], loss=81.2718
	step [155/244], loss=61.4789
	step [156/244], loss=64.5751
	step [157/244], loss=76.3263
	step [158/244], loss=69.7015
	step [159/244], loss=58.5524
	step [160/244], loss=60.7480
	step [161/244], loss=72.6503
	step [162/244], loss=96.7298
	step [163/244], loss=78.5111
	step [164/244], loss=64.8602
	step [165/244], loss=74.0622
	step [166/244], loss=57.0339
	step [167/244], loss=76.1976
	step [168/244], loss=53.7266
	step [169/244], loss=81.7694
	step [170/244], loss=65.5751
	step [171/244], loss=80.0482
	step [172/244], loss=76.3062
	step [173/244], loss=71.9403
	step [174/244], loss=77.1764
	step [175/244], loss=72.8331
	step [176/244], loss=70.1836
	step [177/244], loss=61.2214
	step [178/244], loss=78.8332
	step [179/244], loss=66.7480
	step [180/244], loss=71.3079
	step [181/244], loss=66.6887
	step [182/244], loss=73.9245
	step [183/244], loss=69.2605
	step [184/244], loss=66.4269
	step [185/244], loss=80.6535
	step [186/244], loss=69.3424
	step [187/244], loss=71.8817
	step [188/244], loss=67.4823
	step [189/244], loss=63.0943
	step [190/244], loss=78.6354
	step [191/244], loss=77.6263
	step [192/244], loss=72.4409
	step [193/244], loss=69.1226
	step [194/244], loss=80.7502
	step [195/244], loss=80.8253
	step [196/244], loss=88.9348
	step [197/244], loss=83.1962
	step [198/244], loss=77.9997
	step [199/244], loss=86.9211
	step [200/244], loss=62.9811
	step [201/244], loss=71.1654
	step [202/244], loss=77.3883
	step [203/244], loss=72.0021
	step [204/244], loss=64.7886
	step [205/244], loss=80.9153
	step [206/244], loss=66.7107
	step [207/244], loss=86.1158
	step [208/244], loss=78.6886
	step [209/244], loss=65.0427
	step [210/244], loss=63.9673
	step [211/244], loss=58.9927
	step [212/244], loss=51.4193
	step [213/244], loss=73.7713
	step [214/244], loss=87.3382
	step [215/244], loss=61.3652
	step [216/244], loss=64.4910
	step [217/244], loss=70.6321
	step [218/244], loss=76.0076
	step [219/244], loss=86.6832
	step [220/244], loss=75.3846
	step [221/244], loss=83.3677
	step [222/244], loss=79.6052
	step [223/244], loss=65.2594
	step [224/244], loss=93.5902
	step [225/244], loss=65.3601
	step [226/244], loss=62.1324
	step [227/244], loss=76.0593
	step [228/244], loss=81.6850
	step [229/244], loss=76.6026
	step [230/244], loss=67.9443
	step [231/244], loss=64.8528
	step [232/244], loss=69.0082
	step [233/244], loss=66.0950
	step [234/244], loss=71.5863
	step [235/244], loss=75.7321
	step [236/244], loss=62.5362
	step [237/244], loss=77.0417
	step [238/244], loss=86.6658
	step [239/244], loss=72.9326
	step [240/244], loss=78.5074
	step [241/244], loss=73.0722
	step [242/244], loss=79.0142
	step [243/244], loss=96.0093
	step [244/244], loss=50.3995
	Evaluating
	loss=0.0068, precision=0.3817, recall=0.8621, f1=0.5292
Training epoch 103
	step [1/244], loss=84.4458
	step [2/244], loss=61.6295
	step [3/244], loss=63.1516
	step [4/244], loss=73.7415
	step [5/244], loss=78.5364
	step [6/244], loss=75.9857
	step [7/244], loss=74.3137
	step [8/244], loss=71.0147
	step [9/244], loss=74.3334
	step [10/244], loss=66.0397
	step [11/244], loss=77.3515
	step [12/244], loss=68.5517
	step [13/244], loss=85.9051
	step [14/244], loss=74.1589
	step [15/244], loss=65.4811
	step [16/244], loss=77.0299
	step [17/244], loss=64.7832
	step [18/244], loss=75.8431
	step [19/244], loss=81.5357
	step [20/244], loss=77.6314
	step [21/244], loss=77.2580
	step [22/244], loss=77.3393
	step [23/244], loss=73.1540
	step [24/244], loss=89.7255
	step [25/244], loss=84.3580
	step [26/244], loss=69.8661
	step [27/244], loss=67.2900
	step [28/244], loss=68.5480
	step [29/244], loss=70.0485
	step [30/244], loss=76.2698
	step [31/244], loss=65.2563
	step [32/244], loss=72.1939
	step [33/244], loss=61.4834
	step [34/244], loss=75.5549
	step [35/244], loss=57.2421
	step [36/244], loss=69.4023
	step [37/244], loss=65.9002
	step [38/244], loss=68.0198
	step [39/244], loss=71.5974
	step [40/244], loss=80.7564
	step [41/244], loss=74.1822
	step [42/244], loss=74.9572
	step [43/244], loss=83.7693
	step [44/244], loss=75.4869
	step [45/244], loss=66.4153
	step [46/244], loss=81.3972
	step [47/244], loss=65.9933
	step [48/244], loss=67.1519
	step [49/244], loss=75.9742
	step [50/244], loss=66.6447
	step [51/244], loss=75.9131
	step [52/244], loss=84.4562
	step [53/244], loss=77.0667
	step [54/244], loss=78.4731
	step [55/244], loss=68.7987
	step [56/244], loss=83.6962
	step [57/244], loss=95.5529
	step [58/244], loss=54.0007
	step [59/244], loss=71.0058
	step [60/244], loss=82.7948
	step [61/244], loss=64.0791
	step [62/244], loss=69.2454
	step [63/244], loss=74.1755
	step [64/244], loss=85.6922
	step [65/244], loss=78.7281
	step [66/244], loss=63.4996
	step [67/244], loss=79.9260
	step [68/244], loss=80.5354
	step [69/244], loss=80.5363
	step [70/244], loss=76.4514
	step [71/244], loss=62.4927
	step [72/244], loss=64.7504
	step [73/244], loss=78.4350
	step [74/244], loss=83.2448
	step [75/244], loss=61.2610
	step [76/244], loss=78.2433
	step [77/244], loss=73.6899
	step [78/244], loss=68.8992
	step [79/244], loss=44.3865
	step [80/244], loss=89.1470
	step [81/244], loss=76.3472
	step [82/244], loss=68.7747
	step [83/244], loss=76.7161
	step [84/244], loss=68.5543
	step [85/244], loss=89.2186
	step [86/244], loss=78.0892
	step [87/244], loss=70.3427
	step [88/244], loss=69.4386
	step [89/244], loss=79.7395
	step [90/244], loss=71.9144
	step [91/244], loss=76.0527
	step [92/244], loss=62.1623
	step [93/244], loss=98.8483
	step [94/244], loss=65.1492
	step [95/244], loss=69.2822
	step [96/244], loss=82.9492
	step [97/244], loss=67.1946
	step [98/244], loss=77.3023
	step [99/244], loss=59.3911
	step [100/244], loss=71.7791
	step [101/244], loss=75.5984
	step [102/244], loss=88.2506
	step [103/244], loss=66.9683
	step [104/244], loss=65.6457
	step [105/244], loss=72.6360
	step [106/244], loss=77.9344
	step [107/244], loss=74.5932
	step [108/244], loss=80.7566
	step [109/244], loss=90.5203
	step [110/244], loss=68.9910
	step [111/244], loss=74.6407
	step [112/244], loss=72.7753
	step [113/244], loss=75.8954
	step [114/244], loss=71.2594
	step [115/244], loss=68.4690
	step [116/244], loss=78.6537
	step [117/244], loss=62.9356
	step [118/244], loss=70.7210
	step [119/244], loss=63.9156
	step [120/244], loss=62.2010
	step [121/244], loss=65.6889
	step [122/244], loss=78.8727
	step [123/244], loss=64.7039
	step [124/244], loss=62.3034
	step [125/244], loss=69.1644
	step [126/244], loss=75.4477
	step [127/244], loss=74.2069
	step [128/244], loss=67.4118
	step [129/244], loss=57.3040
	step [130/244], loss=76.9026
	step [131/244], loss=68.6269
	step [132/244], loss=93.9538
	step [133/244], loss=79.5956
	step [134/244], loss=60.2347
	step [135/244], loss=87.7468
	step [136/244], loss=61.5654
	step [137/244], loss=68.2832
	step [138/244], loss=80.2794
	step [139/244], loss=83.0027
	step [140/244], loss=70.8214
	step [141/244], loss=61.7563
	step [142/244], loss=59.2106
	step [143/244], loss=78.9374
	step [144/244], loss=76.0479
	step [145/244], loss=58.7555
	step [146/244], loss=65.3049
	step [147/244], loss=79.3302
	step [148/244], loss=68.9923
	step [149/244], loss=66.1055
	step [150/244], loss=64.0484
	step [151/244], loss=81.5099
	step [152/244], loss=68.7151
	step [153/244], loss=76.8975
	step [154/244], loss=74.5960
	step [155/244], loss=84.0510
	step [156/244], loss=71.0552
	step [157/244], loss=84.4223
	step [158/244], loss=67.9181
	step [159/244], loss=86.5882
	step [160/244], loss=79.5434
	step [161/244], loss=67.4622
	step [162/244], loss=80.9356
	step [163/244], loss=62.3647
	step [164/244], loss=72.7624
	step [165/244], loss=87.2495
	step [166/244], loss=84.7233
	step [167/244], loss=68.1999
	step [168/244], loss=74.6797
	step [169/244], loss=69.9718
	step [170/244], loss=64.9429
	step [171/244], loss=78.1319
	step [172/244], loss=86.1424
	step [173/244], loss=63.9567
	step [174/244], loss=71.0088
	step [175/244], loss=76.7631
	step [176/244], loss=74.3310
	step [177/244], loss=75.1861
	step [178/244], loss=78.5404
	step [179/244], loss=85.6570
	step [180/244], loss=75.8242
	step [181/244], loss=64.3358
	step [182/244], loss=78.3868
	step [183/244], loss=78.7595
	step [184/244], loss=69.2136
	step [185/244], loss=80.3627
	step [186/244], loss=76.7736
	step [187/244], loss=61.4868
	step [188/244], loss=67.4955
	step [189/244], loss=53.8400
	step [190/244], loss=73.7774
	step [191/244], loss=76.7063
	step [192/244], loss=71.4334
	step [193/244], loss=60.8324
	step [194/244], loss=67.0571
	step [195/244], loss=66.0611
	step [196/244], loss=60.8350
	step [197/244], loss=75.3495
	step [198/244], loss=59.8827
	step [199/244], loss=62.2157
	step [200/244], loss=75.2398
	step [201/244], loss=70.6151
	step [202/244], loss=71.1843
	step [203/244], loss=68.6513
	step [204/244], loss=75.4209
	step [205/244], loss=82.7480
	step [206/244], loss=74.6476
	step [207/244], loss=78.1649
	step [208/244], loss=70.3028
	step [209/244], loss=71.3001
	step [210/244], loss=66.8711
	step [211/244], loss=75.4018
	step [212/244], loss=74.0799
	step [213/244], loss=72.0855
	step [214/244], loss=80.4156
	step [215/244], loss=65.2839
	step [216/244], loss=74.7250
	step [217/244], loss=72.9656
	step [218/244], loss=73.6591
	step [219/244], loss=68.7966
	step [220/244], loss=68.6934
	step [221/244], loss=84.4212
	step [222/244], loss=75.8357
	step [223/244], loss=79.8137
	step [224/244], loss=69.1887
	step [225/244], loss=67.6548
	step [226/244], loss=75.1579
	step [227/244], loss=82.1470
	step [228/244], loss=75.6216
	step [229/244], loss=77.7095
	step [230/244], loss=71.6556
	step [231/244], loss=83.2838
	step [232/244], loss=67.1087
	step [233/244], loss=63.8596
	step [234/244], loss=67.2567
	step [235/244], loss=71.2251
	step [236/244], loss=81.0656
	step [237/244], loss=69.2157
	step [238/244], loss=72.2320
	step [239/244], loss=75.2772
	step [240/244], loss=68.6491
	step [241/244], loss=73.8587
	step [242/244], loss=56.0150
	step [243/244], loss=88.2828
	step [244/244], loss=30.2233
	Evaluating
	loss=0.0072, precision=0.3635, recall=0.8623, f1=0.5114
Training epoch 104
	step [1/244], loss=66.7270
	step [2/244], loss=80.7931
	step [3/244], loss=66.5388
	step [4/244], loss=74.5485
	step [5/244], loss=74.3698
	step [6/244], loss=70.5838
	step [7/244], loss=59.0077
	step [8/244], loss=66.1086
	step [9/244], loss=71.8453
	step [10/244], loss=59.2994
	step [11/244], loss=77.2298
	step [12/244], loss=66.2755
	step [13/244], loss=66.0976
	step [14/244], loss=73.5887
	step [15/244], loss=79.6479
	step [16/244], loss=70.2786
	step [17/244], loss=56.8966
	step [18/244], loss=66.5926
	step [19/244], loss=70.5910
	step [20/244], loss=84.8434
	step [21/244], loss=60.8417
	step [22/244], loss=72.7647
	step [23/244], loss=68.1547
	step [24/244], loss=94.5658
	step [25/244], loss=82.8025
	step [26/244], loss=80.7188
	step [27/244], loss=79.7000
	step [28/244], loss=75.6230
	step [29/244], loss=76.9205
	step [30/244], loss=78.6770
	step [31/244], loss=68.9641
	step [32/244], loss=76.6772
	step [33/244], loss=70.6947
	step [34/244], loss=75.5433
	step [35/244], loss=60.9789
	step [36/244], loss=82.3347
	step [37/244], loss=65.5828
	step [38/244], loss=74.3986
	step [39/244], loss=95.5134
	step [40/244], loss=74.6328
	step [41/244], loss=77.1031
	step [42/244], loss=73.5068
	step [43/244], loss=74.4711
	step [44/244], loss=67.0466
	step [45/244], loss=75.5891
	step [46/244], loss=62.8515
	step [47/244], loss=67.3392
	step [48/244], loss=63.3117
	step [49/244], loss=69.7425
	step [50/244], loss=66.7224
	step [51/244], loss=78.2269
	step [52/244], loss=72.9149
	step [53/244], loss=68.2248
	step [54/244], loss=71.9936
	step [55/244], loss=75.4289
	step [56/244], loss=68.8914
	step [57/244], loss=70.7172
	step [58/244], loss=84.5389
	step [59/244], loss=81.0650
	step [60/244], loss=53.7753
	step [61/244], loss=74.2782
	step [62/244], loss=73.5635
	step [63/244], loss=69.4579
	step [64/244], loss=83.4775
	step [65/244], loss=73.4793
	step [66/244], loss=88.9329
	step [67/244], loss=77.8003
	step [68/244], loss=69.0954
	step [69/244], loss=70.7460
	step [70/244], loss=84.3497
	step [71/244], loss=66.1654
	step [72/244], loss=82.1447
	step [73/244], loss=84.4540
	step [74/244], loss=58.4047
	step [75/244], loss=60.6296
	step [76/244], loss=68.0388
	step [77/244], loss=79.6966
	step [78/244], loss=63.9342
	step [79/244], loss=73.9259
	step [80/244], loss=88.5964
	step [81/244], loss=78.6233
	step [82/244], loss=69.9475
	step [83/244], loss=75.5837
	step [84/244], loss=78.8752
	step [85/244], loss=73.8960
	step [86/244], loss=62.5981
	step [87/244], loss=77.2699
	step [88/244], loss=80.4600
	step [89/244], loss=66.3463
	step [90/244], loss=77.6780
	step [91/244], loss=79.2294
	step [92/244], loss=85.8870
	step [93/244], loss=71.8740
	step [94/244], loss=61.7401
	step [95/244], loss=82.7380
	step [96/244], loss=63.3492
	step [97/244], loss=80.7654
	step [98/244], loss=71.4605
	step [99/244], loss=56.3763
	step [100/244], loss=76.8872
	step [101/244], loss=74.1784
	step [102/244], loss=56.1274
	step [103/244], loss=60.2919
	step [104/244], loss=65.8028
	step [105/244], loss=78.5721
	step [106/244], loss=72.2149
	step [107/244], loss=70.7953
	step [108/244], loss=60.3186
	step [109/244], loss=61.6232
	step [110/244], loss=80.3951
	step [111/244], loss=65.4552
	step [112/244], loss=78.2629
	step [113/244], loss=68.0889
	step [114/244], loss=58.2871
	step [115/244], loss=71.4161
	step [116/244], loss=66.3985
	step [117/244], loss=68.1348
	step [118/244], loss=85.8736
	step [119/244], loss=75.1330
	step [120/244], loss=76.8047
	step [121/244], loss=78.3965
	step [122/244], loss=69.2399
	step [123/244], loss=61.2562
	step [124/244], loss=63.1955
	step [125/244], loss=62.5776
	step [126/244], loss=56.2611
	step [127/244], loss=67.5076
	step [128/244], loss=65.5378
	step [129/244], loss=76.5879
	step [130/244], loss=78.6240
	step [131/244], loss=81.4954
	step [132/244], loss=101.4347
	step [133/244], loss=71.7641
	step [134/244], loss=77.4791
	step [135/244], loss=71.7859
	step [136/244], loss=58.3641
	step [137/244], loss=77.2862
	step [138/244], loss=71.8909
	step [139/244], loss=68.6643
	step [140/244], loss=60.5716
	step [141/244], loss=68.9640
	step [142/244], loss=71.2157
	step [143/244], loss=82.8661
	step [144/244], loss=80.7437
	step [145/244], loss=62.0230
	step [146/244], loss=72.5053
	step [147/244], loss=67.5983
	step [148/244], loss=76.0026
	step [149/244], loss=76.7678
	step [150/244], loss=70.5307
	step [151/244], loss=84.7015
	step [152/244], loss=69.4844
	step [153/244], loss=74.9696
	step [154/244], loss=73.7822
	step [155/244], loss=70.0369
	step [156/244], loss=73.0029
	step [157/244], loss=52.8608
	step [158/244], loss=70.6280
	step [159/244], loss=65.9196
	step [160/244], loss=69.5878
	step [161/244], loss=73.2504
	step [162/244], loss=55.1863
	step [163/244], loss=71.3448
	step [164/244], loss=68.8178
	step [165/244], loss=73.0498
	step [166/244], loss=74.7406
	step [167/244], loss=77.4283
	step [168/244], loss=84.9563
	step [169/244], loss=65.0319
	step [170/244], loss=68.2542
	step [171/244], loss=78.4677
	step [172/244], loss=90.7378
	step [173/244], loss=74.4599
	step [174/244], loss=63.5068
	step [175/244], loss=77.3734
	step [176/244], loss=67.3984
	step [177/244], loss=72.5677
	step [178/244], loss=61.2704
	step [179/244], loss=77.0795
	step [180/244], loss=76.1209
	step [181/244], loss=70.1604
	step [182/244], loss=81.1801
	step [183/244], loss=71.4048
	step [184/244], loss=76.2044
	step [185/244], loss=69.1083
	step [186/244], loss=67.0762
	step [187/244], loss=72.2270
	step [188/244], loss=74.4432
	step [189/244], loss=77.2894
	step [190/244], loss=86.5151
	step [191/244], loss=66.1704
	step [192/244], loss=87.6792
	step [193/244], loss=68.1451
	step [194/244], loss=75.4285
	step [195/244], loss=75.4194
	step [196/244], loss=86.2075
	step [197/244], loss=64.0988
	step [198/244], loss=77.9693
	step [199/244], loss=67.6318
	step [200/244], loss=76.4154
	step [201/244], loss=72.4111
	step [202/244], loss=63.0604
	step [203/244], loss=67.9089
	step [204/244], loss=88.6300
	step [205/244], loss=60.9205
	step [206/244], loss=70.5875
	step [207/244], loss=83.8247
	step [208/244], loss=83.1113
	step [209/244], loss=74.5392
	step [210/244], loss=67.2451
	step [211/244], loss=80.7016
	step [212/244], loss=72.7800
	step [213/244], loss=78.6826
	step [214/244], loss=68.6039
	step [215/244], loss=82.1788
	step [216/244], loss=75.4711
	step [217/244], loss=71.0980
	step [218/244], loss=66.6526
	step [219/244], loss=83.0792
	step [220/244], loss=79.6805
	step [221/244], loss=79.1344
	step [222/244], loss=76.5950
	step [223/244], loss=63.0473
	step [224/244], loss=79.6633
	step [225/244], loss=72.6559
	step [226/244], loss=81.2736
	step [227/244], loss=74.7664
	step [228/244], loss=65.4082
	step [229/244], loss=80.5842
	step [230/244], loss=60.7006
	step [231/244], loss=81.4992
	step [232/244], loss=90.5667
	step [233/244], loss=74.5348
	step [234/244], loss=77.4232
	step [235/244], loss=66.9093
	step [236/244], loss=81.4665
	step [237/244], loss=78.5966
	step [238/244], loss=78.6618
	step [239/244], loss=70.1195
	step [240/244], loss=76.6325
	step [241/244], loss=73.6494
	step [242/244], loss=75.7313
	step [243/244], loss=73.0914
	step [244/244], loss=30.2534
	Evaluating
	loss=0.0065, precision=0.3972, recall=0.8653, f1=0.5444
Training epoch 105
	step [1/244], loss=62.7263
	step [2/244], loss=66.3131
	step [3/244], loss=67.9750
	step [4/244], loss=77.0708
	step [5/244], loss=80.3016
	step [6/244], loss=69.4459
	step [7/244], loss=93.4411
	step [8/244], loss=52.0726
	step [9/244], loss=98.4736
	step [10/244], loss=74.1726
	step [11/244], loss=96.9605
	step [12/244], loss=65.2620
	step [13/244], loss=87.4291
	step [14/244], loss=72.4295
	step [15/244], loss=74.4065
	step [16/244], loss=70.3661
	step [17/244], loss=104.2091
	step [18/244], loss=47.4307
	step [19/244], loss=80.1559
	step [20/244], loss=73.2740
	step [21/244], loss=66.6731
	step [22/244], loss=71.1307
	step [23/244], loss=78.3549
	step [24/244], loss=91.2919
	step [25/244], loss=74.1735
	step [26/244], loss=87.6446
	step [27/244], loss=78.8043
	step [28/244], loss=83.1038
	step [29/244], loss=68.8990
	step [30/244], loss=88.0664
	step [31/244], loss=69.8601
	step [32/244], loss=98.4124
	step [33/244], loss=69.9278
	step [34/244], loss=78.1286
	step [35/244], loss=72.4657
	step [36/244], loss=71.6674
	step [37/244], loss=66.9886
	step [38/244], loss=75.4795
	step [39/244], loss=73.1526
	step [40/244], loss=77.4127
	step [41/244], loss=61.8968
	step [42/244], loss=71.2881
	step [43/244], loss=65.7678
	step [44/244], loss=69.6598
	step [45/244], loss=77.0375
	step [46/244], loss=69.7494
	step [47/244], loss=62.3239
	step [48/244], loss=73.9468
	step [49/244], loss=95.7062
	step [50/244], loss=61.5959
	step [51/244], loss=53.2525
	step [52/244], loss=57.5814
	step [53/244], loss=66.9844
	step [54/244], loss=78.1480
	step [55/244], loss=59.8991
	step [56/244], loss=65.3311
	step [57/244], loss=68.6048
	step [58/244], loss=65.7362
	step [59/244], loss=74.1007
	step [60/244], loss=70.0779
	step [61/244], loss=79.1568
	step [62/244], loss=60.5861
	step [63/244], loss=81.1726
	step [64/244], loss=73.7148
	step [65/244], loss=66.7797
	step [66/244], loss=77.6240
	step [67/244], loss=68.6820
	step [68/244], loss=64.2057
	step [69/244], loss=83.3156
	step [70/244], loss=67.9572
	step [71/244], loss=61.1898
	step [72/244], loss=73.1403
	step [73/244], loss=73.2215
	step [74/244], loss=68.8125
	step [75/244], loss=80.9945
	step [76/244], loss=77.7178
	step [77/244], loss=72.5208
	step [78/244], loss=79.1682
	step [79/244], loss=72.7718
	step [80/244], loss=79.8370
	step [81/244], loss=76.8812
	step [82/244], loss=74.3830
	step [83/244], loss=69.1550
	step [84/244], loss=88.2621
	step [85/244], loss=76.5571
	step [86/244], loss=70.3401
	step [87/244], loss=81.4394
	step [88/244], loss=62.4951
	step [89/244], loss=66.5283
	step [90/244], loss=67.0911
	step [91/244], loss=65.1977
	step [92/244], loss=71.7283
	step [93/244], loss=64.7079
	step [94/244], loss=71.5480
	step [95/244], loss=72.7510
	step [96/244], loss=67.9458
	step [97/244], loss=84.3835
	step [98/244], loss=64.6438
	step [99/244], loss=68.5002
	step [100/244], loss=70.2449
	step [101/244], loss=74.8016
	step [102/244], loss=83.9730
	step [103/244], loss=71.8257
	step [104/244], loss=79.2496
	step [105/244], loss=77.9664
	step [106/244], loss=77.0885
	step [107/244], loss=71.1393
	step [108/244], loss=62.2613
	step [109/244], loss=73.3363
	step [110/244], loss=75.2664
	step [111/244], loss=71.5554
	step [112/244], loss=61.7710
	step [113/244], loss=72.2013
	step [114/244], loss=63.1502
	step [115/244], loss=76.4174
	step [116/244], loss=65.8301
	step [117/244], loss=69.7500
	step [118/244], loss=72.2562
	step [119/244], loss=69.0072
	step [120/244], loss=69.7544
	step [121/244], loss=70.7448
	step [122/244], loss=77.5822
	step [123/244], loss=69.8292
	step [124/244], loss=66.6834
	step [125/244], loss=60.4751
	step [126/244], loss=57.9231
	step [127/244], loss=77.2489
	step [128/244], loss=75.9069
	step [129/244], loss=77.8478
	step [130/244], loss=82.8934
	step [131/244], loss=73.4788
	step [132/244], loss=82.8217
	step [133/244], loss=66.2066
	step [134/244], loss=67.4560
	step [135/244], loss=85.2756
	step [136/244], loss=78.6486
	step [137/244], loss=68.9084
	step [138/244], loss=64.8326
	step [139/244], loss=71.0321
	step [140/244], loss=65.4750
	step [141/244], loss=88.3483
	step [142/244], loss=76.5649
	step [143/244], loss=77.0323
	step [144/244], loss=78.9712
	step [145/244], loss=69.2992
	step [146/244], loss=63.6634
	step [147/244], loss=78.5798
	step [148/244], loss=76.4485
	step [149/244], loss=75.1147
	step [150/244], loss=63.2749
	step [151/244], loss=80.0548
	step [152/244], loss=67.9787
	step [153/244], loss=76.8082
	step [154/244], loss=69.6674
	step [155/244], loss=66.5609
	step [156/244], loss=80.4455
	step [157/244], loss=68.5742
	step [158/244], loss=72.5499
	step [159/244], loss=85.1659
	step [160/244], loss=67.0446
	step [161/244], loss=94.3206
	step [162/244], loss=73.0189
	step [163/244], loss=73.8107
	step [164/244], loss=69.3974
	step [165/244], loss=60.5357
	step [166/244], loss=70.5243
	step [167/244], loss=76.8221
	step [168/244], loss=71.3313
	step [169/244], loss=79.1041
	step [170/244], loss=70.2872
	step [171/244], loss=64.1536
	step [172/244], loss=72.0179
	step [173/244], loss=65.7505
	step [174/244], loss=69.0380
	step [175/244], loss=65.3239
	step [176/244], loss=72.8849
	step [177/244], loss=74.3714
	step [178/244], loss=62.6628
	step [179/244], loss=65.3480
	step [180/244], loss=75.6319
	step [181/244], loss=67.3254
	step [182/244], loss=69.2150
	step [183/244], loss=73.6215
	step [184/244], loss=55.3342
	step [185/244], loss=69.9632
	step [186/244], loss=90.5066
	step [187/244], loss=75.6137
	step [188/244], loss=79.8471
	step [189/244], loss=61.9418
	step [190/244], loss=73.8256
	step [191/244], loss=68.2903
	step [192/244], loss=75.2014
	step [193/244], loss=64.0597
	step [194/244], loss=76.2590
	step [195/244], loss=79.2668
	step [196/244], loss=87.3132
	step [197/244], loss=68.5411
	step [198/244], loss=74.3716
	step [199/244], loss=76.7657
	step [200/244], loss=68.2223
	step [201/244], loss=61.0759
	step [202/244], loss=67.3346
	step [203/244], loss=75.1707
	step [204/244], loss=69.4335
	step [205/244], loss=63.9682
	step [206/244], loss=73.2247
	step [207/244], loss=67.5787
	step [208/244], loss=72.4940
	step [209/244], loss=84.1454
	step [210/244], loss=79.4295
	step [211/244], loss=71.3437
	step [212/244], loss=70.0550
	step [213/244], loss=60.7224
	step [214/244], loss=68.7101
	step [215/244], loss=78.3807
	step [216/244], loss=68.2472
	step [217/244], loss=69.8345
	step [218/244], loss=93.7363
	step [219/244], loss=65.7409
	step [220/244], loss=79.1760
	step [221/244], loss=58.3646
	step [222/244], loss=71.2619
	step [223/244], loss=70.1546
	step [224/244], loss=85.6678
	step [225/244], loss=68.0767
	step [226/244], loss=78.0172
	step [227/244], loss=74.7413
	step [228/244], loss=67.5072
	step [229/244], loss=64.4513
	step [230/244], loss=69.2371
	step [231/244], loss=77.1300
	step [232/244], loss=73.5738
	step [233/244], loss=89.8895
	step [234/244], loss=56.1983
	step [235/244], loss=63.2780
	step [236/244], loss=78.6428
	step [237/244], loss=73.8436
	step [238/244], loss=77.2578
	step [239/244], loss=60.8081
	step [240/244], loss=73.1298
	step [241/244], loss=78.7894
	step [242/244], loss=63.2155
	step [243/244], loss=71.1071
	step [244/244], loss=45.4589
	Evaluating
	loss=0.0076, precision=0.3514, recall=0.8758, f1=0.5016
Training epoch 106
	step [1/244], loss=78.8128
	step [2/244], loss=111.0138
	step [3/244], loss=59.9098
	step [4/244], loss=70.9396
	step [5/244], loss=69.5523
	step [6/244], loss=72.2715
	step [7/244], loss=63.7540
	step [8/244], loss=91.7230
	step [9/244], loss=71.5542
	step [10/244], loss=75.2254
	step [11/244], loss=82.0065
	step [12/244], loss=79.1681
	step [13/244], loss=63.0572
	step [14/244], loss=75.2256
	step [15/244], loss=78.7613
	step [16/244], loss=72.3723
	step [17/244], loss=72.4171
	step [18/244], loss=75.0647
	step [19/244], loss=59.6476
	step [20/244], loss=82.3213
	step [21/244], loss=64.0262
	step [22/244], loss=77.4098
	step [23/244], loss=80.3772
	step [24/244], loss=62.0594
	step [25/244], loss=74.8621
	step [26/244], loss=63.8409
	step [27/244], loss=71.0696
	step [28/244], loss=65.2894
	step [29/244], loss=77.1633
	step [30/244], loss=69.7522
	step [31/244], loss=67.1614
	step [32/244], loss=71.4348
	step [33/244], loss=73.9427
	step [34/244], loss=80.1792
	step [35/244], loss=84.3694
	step [36/244], loss=52.1100
	step [37/244], loss=72.8738
	step [38/244], loss=57.6223
	step [39/244], loss=73.8995
	step [40/244], loss=65.8174
	step [41/244], loss=69.7777
	step [42/244], loss=81.4361
	step [43/244], loss=83.8055
	step [44/244], loss=71.6457
	step [45/244], loss=67.4685
	step [46/244], loss=91.7075
	step [47/244], loss=66.1656
	step [48/244], loss=69.5558
	step [49/244], loss=70.9218
	step [50/244], loss=67.9920
	step [51/244], loss=60.2534
	step [52/244], loss=62.7200
	step [53/244], loss=72.1966
	step [54/244], loss=67.4497
	step [55/244], loss=69.9910
	step [56/244], loss=75.6661
	step [57/244], loss=60.7101
	step [58/244], loss=82.2025
	step [59/244], loss=60.9742
	step [60/244], loss=71.1140
	step [61/244], loss=65.9226
	step [62/244], loss=73.4769
	step [63/244], loss=72.7076
	step [64/244], loss=61.4996
	step [65/244], loss=68.2948
	step [66/244], loss=79.1836
	step [67/244], loss=82.7072
	step [68/244], loss=62.9254
	step [69/244], loss=75.8792
	step [70/244], loss=61.5290
	step [71/244], loss=66.6540
	step [72/244], loss=85.6436
	step [73/244], loss=74.6101
	step [74/244], loss=59.8138
	step [75/244], loss=67.4612
	step [76/244], loss=61.5695
	step [77/244], loss=80.8028
	step [78/244], loss=74.9940
	step [79/244], loss=68.2706
	step [80/244], loss=82.3603
	step [81/244], loss=69.6447
	step [82/244], loss=84.8345
	step [83/244], loss=74.2210
	step [84/244], loss=68.2745
	step [85/244], loss=72.0217
	step [86/244], loss=70.3004
	step [87/244], loss=57.1115
	step [88/244], loss=58.8322
	step [89/244], loss=64.4129
	step [90/244], loss=67.4967
	step [91/244], loss=73.9935
	step [92/244], loss=83.3972
	step [93/244], loss=72.9199
	step [94/244], loss=76.5425
	step [95/244], loss=63.3520
	step [96/244], loss=62.1696
	step [97/244], loss=68.8195
	step [98/244], loss=78.7167
	step [99/244], loss=76.6187
	step [100/244], loss=77.9950
	step [101/244], loss=73.5677
	step [102/244], loss=86.2881
	step [103/244], loss=82.4219
	step [104/244], loss=74.9998
	step [105/244], loss=77.7789
	step [106/244], loss=72.6109
	step [107/244], loss=69.4163
	step [108/244], loss=91.5188
	step [109/244], loss=79.8046
	step [110/244], loss=88.1932
	step [111/244], loss=53.9372
	step [112/244], loss=58.0534
	step [113/244], loss=75.0781
	step [114/244], loss=65.0927
	step [115/244], loss=72.2167
	step [116/244], loss=75.0149
	step [117/244], loss=72.8300
	step [118/244], loss=73.9578
	step [119/244], loss=89.7081
	step [120/244], loss=75.1002
	step [121/244], loss=87.3399
	step [122/244], loss=70.2965
	step [123/244], loss=61.9443
	step [124/244], loss=64.7947
	step [125/244], loss=70.0213
	step [126/244], loss=81.0281
	step [127/244], loss=73.4186
	step [128/244], loss=73.2766
	step [129/244], loss=58.0305
	step [130/244], loss=74.5602
	step [131/244], loss=67.6843
	step [132/244], loss=78.4202
	step [133/244], loss=82.9949
	step [134/244], loss=75.7477
	step [135/244], loss=72.4855
	step [136/244], loss=71.4928
	step [137/244], loss=62.8988
	step [138/244], loss=66.0827
	step [139/244], loss=75.7274
	step [140/244], loss=68.3978
	step [141/244], loss=72.6586
	step [142/244], loss=72.3166
	step [143/244], loss=81.2294
	step [144/244], loss=70.6410
	step [145/244], loss=75.1645
	step [146/244], loss=81.1780
	step [147/244], loss=69.5667
	step [148/244], loss=78.9544
	step [149/244], loss=76.2228
	step [150/244], loss=72.2098
	step [151/244], loss=59.7326
	step [152/244], loss=84.3745
	step [153/244], loss=86.1233
	step [154/244], loss=61.4011
	step [155/244], loss=78.9262
	step [156/244], loss=63.1389
	step [157/244], loss=63.9112
	step [158/244], loss=78.0438
	step [159/244], loss=74.8497
	step [160/244], loss=58.7161
	step [161/244], loss=54.8086
	step [162/244], loss=67.0015
	step [163/244], loss=69.0805
	step [164/244], loss=87.4798
	step [165/244], loss=77.4248
	step [166/244], loss=71.6472
	step [167/244], loss=73.4499
	step [168/244], loss=66.5314
	step [169/244], loss=67.3824
	step [170/244], loss=49.7378
	step [171/244], loss=80.0103
	step [172/244], loss=75.1952
	step [173/244], loss=70.8671
	step [174/244], loss=49.4915
	step [175/244], loss=59.6633
	step [176/244], loss=83.2257
	step [177/244], loss=80.5671
	step [178/244], loss=67.2690
	step [179/244], loss=71.4576
	step [180/244], loss=103.7628
	step [181/244], loss=65.3572
	step [182/244], loss=59.4408
	step [183/244], loss=67.3806
	step [184/244], loss=68.6573
	step [185/244], loss=70.8840
	step [186/244], loss=83.9198
	step [187/244], loss=62.0208
	step [188/244], loss=92.3704
	step [189/244], loss=75.7759
	step [190/244], loss=62.6862
	step [191/244], loss=92.8205
	step [192/244], loss=63.9693
	step [193/244], loss=55.8586
	step [194/244], loss=75.3849
	step [195/244], loss=78.5009
	step [196/244], loss=77.2225
	step [197/244], loss=66.6409
	step [198/244], loss=75.4122
	step [199/244], loss=75.0920
	step [200/244], loss=96.4645
	step [201/244], loss=65.5165
	step [202/244], loss=73.9304
	step [203/244], loss=65.1547
	step [204/244], loss=80.5445
	step [205/244], loss=61.8221
	step [206/244], loss=80.6936
	step [207/244], loss=71.1925
	step [208/244], loss=62.4431
	step [209/244], loss=77.4490
	step [210/244], loss=69.3531
	step [211/244], loss=78.5180
	step [212/244], loss=59.0922
	step [213/244], loss=84.5377
	step [214/244], loss=73.7386
	step [215/244], loss=69.2464
	step [216/244], loss=66.4594
	step [217/244], loss=83.7927
	step [218/244], loss=77.6040
	step [219/244], loss=74.7865
	step [220/244], loss=77.2885
	step [221/244], loss=70.5012
	step [222/244], loss=70.8754
	step [223/244], loss=80.2277
	step [224/244], loss=77.4268
	step [225/244], loss=68.8328
	step [226/244], loss=64.2584
	step [227/244], loss=86.2671
	step [228/244], loss=83.6160
	step [229/244], loss=55.9902
	step [230/244], loss=71.3271
	step [231/244], loss=77.8805
	step [232/244], loss=73.9664
	step [233/244], loss=74.8000
	step [234/244], loss=78.4986
	step [235/244], loss=84.4942
	step [236/244], loss=72.8358
	step [237/244], loss=73.9023
	step [238/244], loss=74.3117
	step [239/244], loss=80.4479
	step [240/244], loss=77.3374
	step [241/244], loss=59.4724
	step [242/244], loss=61.3350
	step [243/244], loss=64.9084
	step [244/244], loss=36.9631
	Evaluating
	loss=0.0067, precision=0.3884, recall=0.8769, f1=0.5384
Training epoch 107
	step [1/244], loss=70.2738
	step [2/244], loss=71.7135
	step [3/244], loss=72.7029
	step [4/244], loss=65.0181
	step [5/244], loss=77.2253
	step [6/244], loss=58.5102
	step [7/244], loss=72.9340
	step [8/244], loss=56.8577
	step [9/244], loss=81.7722
	step [10/244], loss=69.2252
	step [11/244], loss=80.5353
	step [12/244], loss=80.6227
	step [13/244], loss=73.0825
	step [14/244], loss=58.3498
	step [15/244], loss=77.9597
	step [16/244], loss=76.0486
	step [17/244], loss=73.3999
	step [18/244], loss=73.1554
	step [19/244], loss=70.1740
	step [20/244], loss=69.0642
	step [21/244], loss=63.1206
	step [22/244], loss=78.6417
	step [23/244], loss=69.3422
	step [24/244], loss=81.9815
	step [25/244], loss=67.3625
	step [26/244], loss=56.4970
	step [27/244], loss=80.5255
	step [28/244], loss=80.4291
	step [29/244], loss=64.0210
	step [30/244], loss=83.4775
	step [31/244], loss=71.5331
	step [32/244], loss=63.6038
	step [33/244], loss=93.6612
	step [34/244], loss=77.9738
	step [35/244], loss=63.4517
	step [36/244], loss=65.5161
	step [37/244], loss=64.1117
	step [38/244], loss=67.4613
	step [39/244], loss=58.5601
	step [40/244], loss=61.3059
	step [41/244], loss=92.5432
	step [42/244], loss=64.6816
	step [43/244], loss=64.4578
	step [44/244], loss=72.1297
	step [45/244], loss=57.9252
	step [46/244], loss=73.0845
	step [47/244], loss=64.1979
	step [48/244], loss=81.5686
	step [49/244], loss=63.0542
	step [50/244], loss=68.1564
	step [51/244], loss=69.4625
	step [52/244], loss=83.5400
	step [53/244], loss=81.3030
	step [54/244], loss=74.2413
	step [55/244], loss=61.7763
	step [56/244], loss=79.2051
	step [57/244], loss=93.4108
	step [58/244], loss=65.7879
	step [59/244], loss=81.7979
	step [60/244], loss=60.3860
	step [61/244], loss=74.6131
	step [62/244], loss=66.0942
	step [63/244], loss=72.9724
	step [64/244], loss=63.4661
	step [65/244], loss=50.5612
	step [66/244], loss=77.1592
	step [67/244], loss=77.4345
	step [68/244], loss=66.6439
	step [69/244], loss=72.7778
	step [70/244], loss=86.9164
	step [71/244], loss=69.6751
	step [72/244], loss=60.9806
	step [73/244], loss=73.5471
	step [74/244], loss=77.4241
	step [75/244], loss=63.5722
	step [76/244], loss=69.7008
	step [77/244], loss=71.9987
	step [78/244], loss=56.4968
	step [79/244], loss=67.5052
	step [80/244], loss=75.5969
	step [81/244], loss=86.0409
	step [82/244], loss=74.3130
	step [83/244], loss=74.3086
	step [84/244], loss=63.5340
	step [85/244], loss=73.9428
	step [86/244], loss=70.8790
	step [87/244], loss=89.2108
	step [88/244], loss=75.3582
	step [89/244], loss=71.5602
	step [90/244], loss=79.1657
	step [91/244], loss=76.8365
	step [92/244], loss=66.5348
	step [93/244], loss=72.9868
	step [94/244], loss=72.4746
	step [95/244], loss=69.2139
	step [96/244], loss=72.8801
	step [97/244], loss=68.2642
	step [98/244], loss=83.7019
	step [99/244], loss=79.1999
	step [100/244], loss=69.1583
	step [101/244], loss=80.9505
	step [102/244], loss=77.1090
	step [103/244], loss=70.3822
	step [104/244], loss=64.6401
	step [105/244], loss=67.0914
	step [106/244], loss=61.2582
	step [107/244], loss=78.2891
	step [108/244], loss=85.4796
	step [109/244], loss=82.1402
	step [110/244], loss=66.5244
	step [111/244], loss=60.9151
	step [112/244], loss=72.4844
	step [113/244], loss=74.5288
	step [114/244], loss=87.2185
	step [115/244], loss=69.2766
	step [116/244], loss=92.0919
	step [117/244], loss=82.2093
	step [118/244], loss=77.8035
	step [119/244], loss=81.2023
	step [120/244], loss=79.2731
	step [121/244], loss=78.1470
	step [122/244], loss=85.0436
	step [123/244], loss=71.9600
	step [124/244], loss=68.1587
	step [125/244], loss=81.9783
	step [126/244], loss=69.9923
	step [127/244], loss=68.0020
	step [128/244], loss=70.5392
	step [129/244], loss=61.7846
	step [130/244], loss=88.0673
	step [131/244], loss=62.4908
	step [132/244], loss=63.3432
	step [133/244], loss=68.4201
	step [134/244], loss=78.3572
	step [135/244], loss=69.5454
	step [136/244], loss=81.6399
	step [137/244], loss=71.8649
	step [138/244], loss=74.8090
	step [139/244], loss=86.4437
	step [140/244], loss=56.2518
	step [141/244], loss=70.0582
	step [142/244], loss=67.2151
	step [143/244], loss=74.9714
	step [144/244], loss=88.2282
	step [145/244], loss=74.0150
	step [146/244], loss=75.2650
	step [147/244], loss=76.1275
	step [148/244], loss=63.7053
	step [149/244], loss=57.3299
	step [150/244], loss=65.4717
	step [151/244], loss=76.1785
	step [152/244], loss=83.2761
	step [153/244], loss=62.6782
	step [154/244], loss=69.1286
	step [155/244], loss=70.8610
	step [156/244], loss=67.2935
	step [157/244], loss=77.5414
	step [158/244], loss=73.4936
	step [159/244], loss=71.4886
	step [160/244], loss=69.5904
	step [161/244], loss=77.2838
	step [162/244], loss=64.4120
	step [163/244], loss=59.6745
	step [164/244], loss=72.5019
	step [165/244], loss=85.0467
	step [166/244], loss=67.9763
	step [167/244], loss=75.0259
	step [168/244], loss=73.3910
	step [169/244], loss=70.3103
	step [170/244], loss=68.5505
	step [171/244], loss=84.6059
	step [172/244], loss=69.5601
	step [173/244], loss=77.5450
	step [174/244], loss=68.7558
	step [175/244], loss=60.9140
	step [176/244], loss=83.7820
	step [177/244], loss=61.0441
	step [178/244], loss=65.7548
	step [179/244], loss=74.0841
	step [180/244], loss=77.2059
	step [181/244], loss=99.2827
	step [182/244], loss=82.7283
	step [183/244], loss=60.1312
	step [184/244], loss=80.6926
	step [185/244], loss=68.7562
	step [186/244], loss=67.2346
	step [187/244], loss=58.6725
	step [188/244], loss=71.8085
	step [189/244], loss=72.2911
	step [190/244], loss=71.6317
	step [191/244], loss=72.1325
	step [192/244], loss=70.4510
	step [193/244], loss=81.5867
	step [194/244], loss=69.4523
	step [195/244], loss=69.4380
	step [196/244], loss=73.3538
	step [197/244], loss=64.9562
	step [198/244], loss=64.6738
	step [199/244], loss=66.6347
	step [200/244], loss=64.8719
	step [201/244], loss=74.3959
	step [202/244], loss=91.1705
	step [203/244], loss=77.0331
	step [204/244], loss=59.0255
	step [205/244], loss=76.7825
	step [206/244], loss=75.3347
	step [207/244], loss=61.1148
	step [208/244], loss=71.1587
	step [209/244], loss=56.8853
	step [210/244], loss=88.2827
	step [211/244], loss=74.3103
	step [212/244], loss=81.9347
	step [213/244], loss=75.8876
	step [214/244], loss=63.3506
	step [215/244], loss=57.2766
	step [216/244], loss=84.7813
	step [217/244], loss=65.3740
	step [218/244], loss=78.0216
	step [219/244], loss=89.9031
	step [220/244], loss=67.6250
	step [221/244], loss=71.7462
	step [222/244], loss=63.5593
	step [223/244], loss=65.7508
	step [224/244], loss=85.7896
	step [225/244], loss=75.3296
	step [226/244], loss=70.6663
	step [227/244], loss=74.2510
	step [228/244], loss=69.0038
	step [229/244], loss=70.3615
	step [230/244], loss=57.8022
	step [231/244], loss=65.8820
	step [232/244], loss=66.3550
	step [233/244], loss=77.7026
	step [234/244], loss=69.6946
	step [235/244], loss=71.2432
	step [236/244], loss=71.1463
	step [237/244], loss=75.9349
	step [238/244], loss=66.3599
	step [239/244], loss=68.8367
	step [240/244], loss=94.3202
	step [241/244], loss=74.3587
	step [242/244], loss=64.4290
	step [243/244], loss=65.3473
	step [244/244], loss=33.4371
	Evaluating
	loss=0.0068, precision=0.3822, recall=0.8819, f1=0.5332
Training epoch 108
	step [1/244], loss=70.9595
	step [2/244], loss=81.7131
	step [3/244], loss=72.7350
	step [4/244], loss=73.5829
	step [5/244], loss=69.5403
	step [6/244], loss=93.9132
	step [7/244], loss=72.0240
	step [8/244], loss=80.9533
	step [9/244], loss=74.1884
	step [10/244], loss=75.2224
	step [11/244], loss=77.4212
	step [12/244], loss=63.0266
	step [13/244], loss=74.2888
	step [14/244], loss=71.8310
	step [15/244], loss=65.0901
	step [16/244], loss=63.4087
	step [17/244], loss=62.0470
	step [18/244], loss=83.7788
	step [19/244], loss=70.2153
	step [20/244], loss=61.8005
	step [21/244], loss=61.2200
	step [22/244], loss=65.2083
	step [23/244], loss=67.9064
	step [24/244], loss=72.9351
	step [25/244], loss=77.5959
	step [26/244], loss=78.5884
	step [27/244], loss=77.8930
	step [28/244], loss=69.4822
	step [29/244], loss=67.0987
	step [30/244], loss=64.8033
	step [31/244], loss=90.8846
	step [32/244], loss=70.1896
	step [33/244], loss=66.6210
	step [34/244], loss=68.8880
	step [35/244], loss=73.9805
	step [36/244], loss=74.7514
	step [37/244], loss=72.1865
	step [38/244], loss=66.8127
	step [39/244], loss=63.5245
	step [40/244], loss=82.7707
	step [41/244], loss=81.8675
	step [42/244], loss=75.5766
	step [43/244], loss=65.7775
	step [44/244], loss=57.3632
	step [45/244], loss=77.4126
	step [46/244], loss=65.2252
	step [47/244], loss=78.6872
	step [48/244], loss=59.4262
	step [49/244], loss=87.1218
	step [50/244], loss=66.9428
	step [51/244], loss=68.2413
	step [52/244], loss=64.9052
	step [53/244], loss=106.4857
	step [54/244], loss=59.2361
	step [55/244], loss=60.2964
	step [56/244], loss=57.4288
	step [57/244], loss=76.1663
	step [58/244], loss=79.3473
	step [59/244], loss=63.8544
	step [60/244], loss=82.2257
	step [61/244], loss=72.9796
	step [62/244], loss=68.0391
	step [63/244], loss=88.0510
	step [64/244], loss=83.4184
	step [65/244], loss=77.3638
	step [66/244], loss=74.4469
	step [67/244], loss=63.8476
	step [68/244], loss=80.2595
	step [69/244], loss=68.5848
	step [70/244], loss=68.0049
	step [71/244], loss=76.2123
	step [72/244], loss=72.7618
	step [73/244], loss=65.9672
	step [74/244], loss=72.9650
	step [75/244], loss=74.9505
	step [76/244], loss=54.3532
	step [77/244], loss=74.9998
	step [78/244], loss=69.4324
	step [79/244], loss=84.4615
	step [80/244], loss=71.0252
	step [81/244], loss=87.4461
	step [82/244], loss=90.8110
	step [83/244], loss=69.4783
	step [84/244], loss=69.8925
	step [85/244], loss=68.8294
	step [86/244], loss=71.0163
	step [87/244], loss=66.8227
	step [88/244], loss=73.2953
	step [89/244], loss=82.0540
	step [90/244], loss=77.0828
	step [91/244], loss=61.7512
	step [92/244], loss=64.1005
	step [93/244], loss=76.6485
	step [94/244], loss=84.3827
	step [95/244], loss=76.2949
	step [96/244], loss=77.3716
	step [97/244], loss=47.4510
	step [98/244], loss=85.8859
	step [99/244], loss=81.0246
	step [100/244], loss=70.8453
	step [101/244], loss=75.1216
	step [102/244], loss=66.5078
	step [103/244], loss=65.1633
	step [104/244], loss=72.5247
	step [105/244], loss=76.7414
	step [106/244], loss=69.9637
	step [107/244], loss=53.7758
	step [108/244], loss=79.8876
	step [109/244], loss=83.6607
	step [110/244], loss=70.4293
	step [111/244], loss=69.2325
	step [112/244], loss=74.3705
	step [113/244], loss=86.3624
	step [114/244], loss=91.0073
	step [115/244], loss=78.6465
	step [116/244], loss=76.1968
	step [117/244], loss=79.8600
	step [118/244], loss=62.8207
	step [119/244], loss=65.9429
	step [120/244], loss=48.5319
	step [121/244], loss=58.0350
	step [122/244], loss=75.1521
	step [123/244], loss=87.5914
	step [124/244], loss=76.3633
	step [125/244], loss=68.4582
	step [126/244], loss=72.2952
	step [127/244], loss=70.5555
	step [128/244], loss=78.7614
	step [129/244], loss=57.8053
	step [130/244], loss=89.7196
	step [131/244], loss=57.8166
	step [132/244], loss=76.5634
	step [133/244], loss=66.1992
	step [134/244], loss=70.7036
	step [135/244], loss=74.4256
	step [136/244], loss=72.6131
	step [137/244], loss=69.1029
	step [138/244], loss=78.2288
	step [139/244], loss=74.2506
	step [140/244], loss=88.7508
	step [141/244], loss=60.9182
	step [142/244], loss=77.9416
	step [143/244], loss=72.7168
	step [144/244], loss=59.1894
	step [145/244], loss=66.3354
	step [146/244], loss=77.0962
	step [147/244], loss=87.6378
	step [148/244], loss=83.4523
	step [149/244], loss=66.5186
	step [150/244], loss=62.5398
	step [151/244], loss=60.8174
	step [152/244], loss=64.9525
	step [153/244], loss=79.5626
	step [154/244], loss=87.0624
	step [155/244], loss=64.1321
	step [156/244], loss=74.9151
	step [157/244], loss=64.0600
	step [158/244], loss=76.2171
	step [159/244], loss=60.6900
	step [160/244], loss=70.5871
	step [161/244], loss=69.3805
	step [162/244], loss=85.3141
	step [163/244], loss=77.0436
	step [164/244], loss=73.4576
	step [165/244], loss=67.5569
	step [166/244], loss=63.1707
	step [167/244], loss=63.9797
	step [168/244], loss=70.6873
	step [169/244], loss=72.6786
	step [170/244], loss=67.1207
	step [171/244], loss=80.8098
	step [172/244], loss=60.0717
	step [173/244], loss=87.6649
	step [174/244], loss=71.8963
	step [175/244], loss=66.9547
	step [176/244], loss=77.6974
	step [177/244], loss=69.8776
	step [178/244], loss=85.1602
	step [179/244], loss=70.2392
	step [180/244], loss=70.9468
	step [181/244], loss=71.1882
	step [182/244], loss=67.1226
	step [183/244], loss=71.1963
	step [184/244], loss=63.8180
	step [185/244], loss=80.8513
	step [186/244], loss=67.6522
	step [187/244], loss=76.4710
	step [188/244], loss=76.2716
	step [189/244], loss=67.8716
	step [190/244], loss=73.8483
	step [191/244], loss=74.5946
	step [192/244], loss=69.6727
	step [193/244], loss=50.6023
	step [194/244], loss=62.5976
	step [195/244], loss=66.6994
	step [196/244], loss=74.8907
	step [197/244], loss=54.6016
	step [198/244], loss=78.7864
	step [199/244], loss=64.0962
	step [200/244], loss=74.5322
	step [201/244], loss=54.6744
	step [202/244], loss=66.6389
	step [203/244], loss=71.2703
	step [204/244], loss=60.2381
	step [205/244], loss=77.5260
	step [206/244], loss=75.7497
	step [207/244], loss=71.7185
	step [208/244], loss=70.9170
	step [209/244], loss=78.1985
	step [210/244], loss=62.5754
	step [211/244], loss=69.9356
	step [212/244], loss=92.4954
	step [213/244], loss=71.5892
	step [214/244], loss=82.5510
	step [215/244], loss=66.0121
	step [216/244], loss=65.9199
	step [217/244], loss=63.6451
	step [218/244], loss=88.0357
	step [219/244], loss=69.9385
	step [220/244], loss=77.4549
	step [221/244], loss=57.0831
	step [222/244], loss=72.4843
	step [223/244], loss=57.7679
	step [224/244], loss=64.5707
	step [225/244], loss=80.9650
	step [226/244], loss=68.1251
	step [227/244], loss=90.3949
	step [228/244], loss=79.1668
	step [229/244], loss=60.5840
	step [230/244], loss=69.2102
	step [231/244], loss=72.6217
	step [232/244], loss=71.6025
	step [233/244], loss=71.5067
	step [234/244], loss=56.5765
	step [235/244], loss=87.5961
	step [236/244], loss=69.0250
	step [237/244], loss=66.1205
	step [238/244], loss=64.9115
	step [239/244], loss=70.7844
	step [240/244], loss=66.3565
	step [241/244], loss=70.0235
	step [242/244], loss=59.8376
	step [243/244], loss=70.3355
	step [244/244], loss=37.7259
	Evaluating
	loss=0.0065, precision=0.3948, recall=0.8754, f1=0.5442
Training epoch 109
	step [1/244], loss=73.7580
	step [2/244], loss=73.7689
	step [3/244], loss=69.3568
	step [4/244], loss=66.7568
	step [5/244], loss=67.1777
	step [6/244], loss=71.4829
	step [7/244], loss=71.5355
	step [8/244], loss=66.6535
	step [9/244], loss=74.6915
	step [10/244], loss=64.7759
	step [11/244], loss=65.7294
	step [12/244], loss=76.3742
	step [13/244], loss=79.6791
	step [14/244], loss=81.5044
	step [15/244], loss=73.9211
	step [16/244], loss=62.7989
	step [17/244], loss=77.3071
	step [18/244], loss=66.6013
	step [19/244], loss=67.8780
	step [20/244], loss=66.4431
	step [21/244], loss=72.9309
	step [22/244], loss=73.0373
	step [23/244], loss=85.5825
	step [24/244], loss=72.3691
	step [25/244], loss=76.5769
	step [26/244], loss=81.6187
	step [27/244], loss=67.2223
	step [28/244], loss=67.6429
	step [29/244], loss=70.0423
	step [30/244], loss=78.3488
	step [31/244], loss=86.2348
	step [32/244], loss=68.9458
	step [33/244], loss=69.4877
	step [34/244], loss=87.4876
	step [35/244], loss=70.8855
	step [36/244], loss=71.9119
	step [37/244], loss=63.5128
	step [38/244], loss=66.2504
	step [39/244], loss=58.8179
	step [40/244], loss=67.0811
	step [41/244], loss=64.0960
	step [42/244], loss=75.3431
	step [43/244], loss=62.9713
	step [44/244], loss=66.7522
	step [45/244], loss=65.3844
	step [46/244], loss=77.4014
	step [47/244], loss=74.9849
	step [48/244], loss=81.6002
	step [49/244], loss=61.0737
	step [50/244], loss=82.3844
	step [51/244], loss=81.5669
	step [52/244], loss=66.9945
	step [53/244], loss=65.3943
	step [54/244], loss=80.8479
	step [55/244], loss=76.7537
	step [56/244], loss=79.2782
	step [57/244], loss=72.7555
	step [58/244], loss=80.7780
	step [59/244], loss=63.7522
	step [60/244], loss=72.9214
	step [61/244], loss=76.1014
	step [62/244], loss=68.5006
	step [63/244], loss=78.8662
	step [64/244], loss=79.4675
	step [65/244], loss=80.9881
	step [66/244], loss=73.9452
	step [67/244], loss=73.3699
	step [68/244], loss=64.8410
	step [69/244], loss=62.3468
	step [70/244], loss=87.8307
	step [71/244], loss=73.5242
	step [72/244], loss=74.8442
	step [73/244], loss=73.3218
	step [74/244], loss=68.1028
	step [75/244], loss=77.9908
	step [76/244], loss=70.0207
	step [77/244], loss=69.1708
	step [78/244], loss=80.2811
	step [79/244], loss=70.1774
	step [80/244], loss=61.9059
	step [81/244], loss=63.1170
	step [82/244], loss=67.5336
	step [83/244], loss=79.5147
	step [84/244], loss=69.9062
	step [85/244], loss=60.3536
	step [86/244], loss=73.9321
	step [87/244], loss=67.5136
	step [88/244], loss=63.8400
	step [89/244], loss=66.9197
	step [90/244], loss=62.1319
	step [91/244], loss=88.2322
	step [92/244], loss=62.8280
	step [93/244], loss=72.3151
	step [94/244], loss=62.0066
	step [95/244], loss=68.7575
	step [96/244], loss=77.1622
	step [97/244], loss=77.1291
	step [98/244], loss=68.8824
	step [99/244], loss=68.6851
	step [100/244], loss=61.0710
	step [101/244], loss=73.1692
	step [102/244], loss=83.9447
	step [103/244], loss=88.1919
	step [104/244], loss=79.6394
	step [105/244], loss=78.4493
	step [106/244], loss=73.7640
	step [107/244], loss=75.1802
	step [108/244], loss=63.5197
	step [109/244], loss=64.5277
	step [110/244], loss=82.6907
	step [111/244], loss=61.1230
	step [112/244], loss=63.3525
	step [113/244], loss=69.0450
	step [114/244], loss=59.0353
	step [115/244], loss=65.0957
	step [116/244], loss=60.9414
	step [117/244], loss=80.1343
	step [118/244], loss=75.8096
	step [119/244], loss=53.6501
	step [120/244], loss=78.1662
	step [121/244], loss=83.4196
	step [122/244], loss=65.2339
	step [123/244], loss=81.7662
	step [124/244], loss=91.9280
	step [125/244], loss=78.9559
	step [126/244], loss=75.5724
	step [127/244], loss=66.4865
	step [128/244], loss=73.8746
	step [129/244], loss=60.0266
	step [130/244], loss=77.8152
	step [131/244], loss=74.5579
	step [132/244], loss=68.0853
	step [133/244], loss=69.5162
	step [134/244], loss=81.2967
	step [135/244], loss=59.6390
	step [136/244], loss=78.3428
	step [137/244], loss=72.1675
	step [138/244], loss=57.0958
	step [139/244], loss=71.6381
	step [140/244], loss=60.2051
	step [141/244], loss=77.0242
	step [142/244], loss=79.0302
	step [143/244], loss=69.3553
	step [144/244], loss=72.3521
	step [145/244], loss=59.5961
	step [146/244], loss=75.8178
	step [147/244], loss=87.3705
	step [148/244], loss=74.3735
	step [149/244], loss=70.6306
	step [150/244], loss=89.0892
	step [151/244], loss=69.3843
	step [152/244], loss=74.2866
	step [153/244], loss=72.7800
	step [154/244], loss=72.1802
	step [155/244], loss=69.4229
	step [156/244], loss=88.4672
	step [157/244], loss=59.0825
	step [158/244], loss=79.2249
	step [159/244], loss=64.9095
	step [160/244], loss=64.8786
	step [161/244], loss=64.4706
	step [162/244], loss=75.0082
	step [163/244], loss=85.5113
	step [164/244], loss=80.0916
	step [165/244], loss=75.5975
	step [166/244], loss=67.3158
	step [167/244], loss=89.4279
	step [168/244], loss=75.0802
	step [169/244], loss=80.2790
	step [170/244], loss=79.6539
	step [171/244], loss=78.2522
	step [172/244], loss=75.0749
	step [173/244], loss=78.0971
	step [174/244], loss=75.4770
	step [175/244], loss=64.9555
	step [176/244], loss=66.7248
	step [177/244], loss=78.6091
	step [178/244], loss=82.2542
	step [179/244], loss=64.9588
	step [180/244], loss=73.5706
	step [181/244], loss=75.3058
	step [182/244], loss=67.8585
	step [183/244], loss=56.9952
	step [184/244], loss=78.2222
	step [185/244], loss=77.3920
	step [186/244], loss=63.8637
	step [187/244], loss=57.7381
	step [188/244], loss=70.4085
	step [189/244], loss=69.8789
	step [190/244], loss=65.4924
	step [191/244], loss=67.7869
	step [192/244], loss=79.0807
	step [193/244], loss=54.0966
	step [194/244], loss=75.4536
	step [195/244], loss=67.8944
	step [196/244], loss=63.9159
	step [197/244], loss=68.2179
	step [198/244], loss=59.4407
	step [199/244], loss=83.8448
	step [200/244], loss=79.0701
	step [201/244], loss=66.5663
	step [202/244], loss=61.7091
	step [203/244], loss=61.3211
	step [204/244], loss=62.5521
	step [205/244], loss=57.7927
	step [206/244], loss=63.8927
	step [207/244], loss=65.0760
	step [208/244], loss=74.0786
	step [209/244], loss=69.6012
	step [210/244], loss=63.3778
	step [211/244], loss=77.4213
	step [212/244], loss=71.4627
	step [213/244], loss=63.0043
	step [214/244], loss=78.3748
	step [215/244], loss=73.7294
	step [216/244], loss=75.7412
	step [217/244], loss=53.0480
	step [218/244], loss=76.8826
	step [219/244], loss=78.3393
	step [220/244], loss=69.5799
	step [221/244], loss=69.3749
	step [222/244], loss=68.4170
	step [223/244], loss=65.4071
	step [224/244], loss=60.7834
	step [225/244], loss=58.5037
	step [226/244], loss=68.2685
	step [227/244], loss=107.6519
	step [228/244], loss=62.6099
	step [229/244], loss=87.7591
	step [230/244], loss=78.5126
	step [231/244], loss=79.9288
	step [232/244], loss=45.8546
	step [233/244], loss=85.8869
	step [234/244], loss=71.2500
	step [235/244], loss=74.5164
	step [236/244], loss=65.5225
	step [237/244], loss=70.3614
	step [238/244], loss=80.7037
	step [239/244], loss=84.4425
	step [240/244], loss=66.3084
	step [241/244], loss=65.1474
	step [242/244], loss=70.4583
	step [243/244], loss=62.1105
	step [244/244], loss=36.4655
	Evaluating
	loss=0.0072, precision=0.3608, recall=0.8618, f1=0.5086
Training epoch 110
	step [1/244], loss=77.9554
	step [2/244], loss=69.4839
	step [3/244], loss=78.6532
	step [4/244], loss=67.7041
	step [5/244], loss=77.2055
	step [6/244], loss=82.1174
	step [7/244], loss=64.0517
	step [8/244], loss=72.4819
	step [9/244], loss=67.0059
	step [10/244], loss=58.0538
	step [11/244], loss=75.1001
	step [12/244], loss=78.8673
	step [13/244], loss=71.2492
	step [14/244], loss=54.1719
	step [15/244], loss=68.6544
	step [16/244], loss=60.7772
	step [17/244], loss=75.0539
	step [18/244], loss=81.2037
	step [19/244], loss=72.7670
	step [20/244], loss=77.0204
	step [21/244], loss=73.0176
	step [22/244], loss=79.2401
	step [23/244], loss=84.7047
	step [24/244], loss=73.5210
	step [25/244], loss=81.6380
	step [26/244], loss=60.0896
	step [27/244], loss=74.3373
	step [28/244], loss=67.0393
	step [29/244], loss=81.2755
	step [30/244], loss=56.1894
	step [31/244], loss=74.1975
	step [32/244], loss=84.5050
	step [33/244], loss=88.4570
	step [34/244], loss=62.8761
	step [35/244], loss=71.3803
	step [36/244], loss=72.2068
	step [37/244], loss=66.0498
	step [38/244], loss=74.8409
	step [39/244], loss=89.7141
	step [40/244], loss=73.4655
	step [41/244], loss=70.9800
	step [42/244], loss=83.2610
	step [43/244], loss=57.3132
	step [44/244], loss=85.4662
	step [45/244], loss=55.1382
	step [46/244], loss=81.4299
	step [47/244], loss=84.5790
	step [48/244], loss=65.8587
	step [49/244], loss=69.2155
	step [50/244], loss=76.7371
	step [51/244], loss=70.5047
	step [52/244], loss=77.7179
	step [53/244], loss=56.5166
	step [54/244], loss=67.4494
	step [55/244], loss=68.7581
	step [56/244], loss=70.5017
	step [57/244], loss=84.1705
	step [58/244], loss=65.7477
	step [59/244], loss=61.3684
	step [60/244], loss=69.0837
	step [61/244], loss=68.0216
	step [62/244], loss=69.3650
	step [63/244], loss=56.7796
	step [64/244], loss=60.1984
	step [65/244], loss=64.7656
	step [66/244], loss=60.1973
	step [67/244], loss=59.8021
	step [68/244], loss=87.2625
	step [69/244], loss=76.5598
	step [70/244], loss=75.6244
	step [71/244], loss=72.6499
	step [72/244], loss=64.5888
	step [73/244], loss=78.0518
	step [74/244], loss=77.4412
	step [75/244], loss=77.9608
	step [76/244], loss=86.8900
	step [77/244], loss=59.9520
	step [78/244], loss=70.5859
	step [79/244], loss=66.6890
	step [80/244], loss=67.0445
	step [81/244], loss=69.0591
	step [82/244], loss=84.1028
	step [83/244], loss=62.4315
	step [84/244], loss=84.2772
	step [85/244], loss=63.7479
	step [86/244], loss=73.9524
	step [87/244], loss=84.6208
	step [88/244], loss=74.0660
	step [89/244], loss=72.8625
	step [90/244], loss=58.6429
	step [91/244], loss=62.4117
	step [92/244], loss=70.9431
	step [93/244], loss=69.4715
	step [94/244], loss=70.7649
	step [95/244], loss=61.5598
	step [96/244], loss=77.8692
	step [97/244], loss=80.7185
	step [98/244], loss=66.3384
	step [99/244], loss=74.6435
	step [100/244], loss=54.6993
	step [101/244], loss=63.7326
	step [102/244], loss=89.7021
	step [103/244], loss=77.5439
	step [104/244], loss=70.1648
	step [105/244], loss=80.1363
	step [106/244], loss=81.0710
	step [107/244], loss=66.0391
	step [108/244], loss=78.7324
	step [109/244], loss=95.0073
	step [110/244], loss=66.7707
	step [111/244], loss=67.5236
	step [112/244], loss=78.5980
	step [113/244], loss=79.7029
	step [114/244], loss=74.1946
	step [115/244], loss=60.2156
	step [116/244], loss=66.9554
	step [117/244], loss=74.3330
	step [118/244], loss=67.9749
	step [119/244], loss=72.6384
	step [120/244], loss=69.4144
	step [121/244], loss=73.6612
	step [122/244], loss=66.3249
	step [123/244], loss=74.0736
	step [124/244], loss=70.9257
	step [125/244], loss=80.0158
	step [126/244], loss=79.4793
	step [127/244], loss=89.6081
	step [128/244], loss=59.1155
	step [129/244], loss=83.9580
	step [130/244], loss=89.3585
	step [131/244], loss=74.3996
	step [132/244], loss=66.3824
	step [133/244], loss=62.6800
	step [134/244], loss=64.9881
	step [135/244], loss=80.3019
	step [136/244], loss=74.5281
	step [137/244], loss=58.6367
	step [138/244], loss=70.1224
	step [139/244], loss=77.2984
	step [140/244], loss=72.2680
	step [141/244], loss=67.5101
	step [142/244], loss=63.1433
	step [143/244], loss=64.9823
	step [144/244], loss=62.5590
	step [145/244], loss=66.8160
	step [146/244], loss=67.1763
	step [147/244], loss=80.6900
	step [148/244], loss=77.3397
	step [149/244], loss=50.1974
	step [150/244], loss=69.9058
	step [151/244], loss=74.0273
	step [152/244], loss=73.6980
	step [153/244], loss=76.8401
	step [154/244], loss=86.1261
	step [155/244], loss=74.3311
	step [156/244], loss=61.2389
	step [157/244], loss=74.8464
	step [158/244], loss=76.0339
	step [159/244], loss=62.4053
	step [160/244], loss=65.7976
	step [161/244], loss=63.7134
	step [162/244], loss=78.7185
	step [163/244], loss=60.1548
	step [164/244], loss=78.4184
	step [165/244], loss=79.6088
	step [166/244], loss=77.6030
	step [167/244], loss=80.7829
	step [168/244], loss=72.7220
	step [169/244], loss=67.6813
	step [170/244], loss=79.4999
	step [171/244], loss=74.0043
	step [172/244], loss=62.1426
	step [173/244], loss=69.9326
	step [174/244], loss=67.2022
	step [175/244], loss=74.7413
	step [176/244], loss=74.3406
	step [177/244], loss=81.7021
	step [178/244], loss=50.0609
	step [179/244], loss=51.9596
	step [180/244], loss=81.6956
	step [181/244], loss=69.3718
	step [182/244], loss=78.2484
	step [183/244], loss=61.7043
	step [184/244], loss=71.3398
	step [185/244], loss=71.9665
	step [186/244], loss=69.6038
	step [187/244], loss=60.5359
	step [188/244], loss=69.8943
	step [189/244], loss=61.9125
	step [190/244], loss=70.7550
	step [191/244], loss=61.6531
	step [192/244], loss=77.0838
	step [193/244], loss=72.1553
	step [194/244], loss=82.9559
	step [195/244], loss=62.4796
	step [196/244], loss=82.2971
	step [197/244], loss=67.1588
	step [198/244], loss=66.2511
	step [199/244], loss=77.7774
	step [200/244], loss=71.5159
	step [201/244], loss=78.3355
	step [202/244], loss=63.3802
	step [203/244], loss=74.6045
	step [204/244], loss=63.9717
	step [205/244], loss=63.9420
	step [206/244], loss=73.9490
	step [207/244], loss=85.2524
	step [208/244], loss=79.8828
	step [209/244], loss=80.1002
	step [210/244], loss=83.9493
	step [211/244], loss=64.2549
	step [212/244], loss=78.5374
	step [213/244], loss=82.1499
	step [214/244], loss=81.0307
	step [215/244], loss=71.4740
	step [216/244], loss=65.0751
	step [217/244], loss=68.6207
	step [218/244], loss=70.0933
	step [219/244], loss=60.9066
	step [220/244], loss=71.8247
	step [221/244], loss=61.4474
	step [222/244], loss=65.1715
	step [223/244], loss=69.1476
	step [224/244], loss=60.8023
	step [225/244], loss=57.6841
	step [226/244], loss=80.7421
	step [227/244], loss=62.6657
	step [228/244], loss=56.7018
	step [229/244], loss=56.0383
	step [230/244], loss=71.4014
	step [231/244], loss=91.4447
	step [232/244], loss=69.8409
	step [233/244], loss=69.6427
	step [234/244], loss=86.6728
	step [235/244], loss=68.9744
	step [236/244], loss=61.6121
	step [237/244], loss=79.8612
	step [238/244], loss=65.8186
	step [239/244], loss=70.7781
	step [240/244], loss=67.7529
	step [241/244], loss=62.4514
	step [242/244], loss=69.3710
	step [243/244], loss=72.6136
	step [244/244], loss=41.5314
	Evaluating
	loss=0.0063, precision=0.4067, recall=0.8685, f1=0.5540
saving model as: 2_saved_model.pth
Training finished
best_f1: 0.5539929050681446
directing: Y rim_enhanced: True test_id 3
removed wrong scan: weights_Y_335_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_383_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_262_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_311_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_260_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_225_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_320_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_192_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_319_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_226_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_346_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_271_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_277_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_290_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_269_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_379_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_375_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_378_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_366_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_309_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_200_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_327_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_193_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_174_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_253_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_220_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_265_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_360_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_303_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_232_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_243_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_240_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_237_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_256_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_254_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_254_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_231_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_359_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_172_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_340_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_263_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_388_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_307_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_278_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_196_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_272_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_314_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_350_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_361_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_194_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_323_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_201_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_299_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_259_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_276_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_167_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_185_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_215_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_268_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_264_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_384_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_261_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_280_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_287_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_385_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_216_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_349_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_348_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_371_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_357_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_234_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_288_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_324_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_229_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_184_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_148_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_168_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_370_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_286_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_302_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_265_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_153_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_274_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_255_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_217_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_355_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_294_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_159_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_338_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_236_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_152_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_292_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_222_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_376_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_183_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_258_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_282_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_278_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_284_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_300_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_163_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_197_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_203_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_243_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_224_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_238_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_374_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_218_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_259_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_282_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_387_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_353_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_386_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_296_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_341_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_206_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_220_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_239_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_231_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_202_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_358_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_246_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_315_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_275_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_169_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_316_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_221_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_188_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_208_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_273_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_251_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_224_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_333_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_263_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_173_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_243_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_241_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_326_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_230_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_236_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_345_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_206_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_258_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_187_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_87_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_154_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_253_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_337_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_232_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_147_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_377_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_362_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_233_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_250_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_297_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_354_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_262_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_225_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_365_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_356_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_242_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_245_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_230_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_318_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_181_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_305_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_255_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_298_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_301_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_293_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_229_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_332_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_257_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_295_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_239_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_260_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_228_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_317_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_364_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_262_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_162_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_310_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_160_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_237_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_254_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_257_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_291_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_279_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_215_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_372_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_218_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_210_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_158_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_156_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_339_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_190_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_373_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_205_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_329_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_380_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_271_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_235_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_244_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_312_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_186_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_308_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_347_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_241_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_270_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_261_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_222_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_189_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_289_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_146_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_252_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_344_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_331_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_313_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_328_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_265_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_283_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_248_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_381_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_211_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_214_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_176_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_221_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_150_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_343_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_249_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_322_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_351_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_234_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_195_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_330_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_334_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_306_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_213_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_325_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_342_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_247_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_352_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_225_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_226_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_256_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_242_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_368_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_382_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_157_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_238_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_267_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_367_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_321_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_233_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_281_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_161_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_336_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_170_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_304_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_44_xwqg-B00034_2020-04-03.npy
# all image files: 21521 # all weight files in weight_dir: 15917 # image files with weight 15917
removed wrong scan: weights_Y_335_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_383_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_262_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_311_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_260_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_225_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_320_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_192_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_319_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_226_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_346_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_271_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_277_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_290_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_269_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_379_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_375_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_378_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_366_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_309_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_200_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_327_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_193_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_174_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_253_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_220_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_265_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_360_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_303_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_232_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_243_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_240_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_237_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_256_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_254_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_254_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_231_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_359_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_172_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_340_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_263_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_388_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_307_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_278_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_196_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_272_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_314_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_350_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_361_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_194_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_323_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_201_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_299_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_259_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_276_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_167_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_185_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_215_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_268_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_264_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_384_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_261_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_280_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_287_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_385_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_216_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_349_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_348_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_371_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_357_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_234_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_288_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_324_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_229_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_184_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_148_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_168_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_370_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_286_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_302_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_265_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_153_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_274_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_255_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_217_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_355_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_294_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_159_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_338_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_236_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_152_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_292_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_222_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_376_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_183_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_258_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_282_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_278_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_284_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_300_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_163_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_197_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_203_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_243_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_224_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_238_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_374_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_218_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_259_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_282_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_387_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_353_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_386_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_296_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_341_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_206_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_220_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_239_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_231_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_202_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_358_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_246_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_315_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_275_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_169_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_316_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_221_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_188_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_208_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_273_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_251_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_224_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_333_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_263_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_173_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_243_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_241_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_326_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_230_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_236_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_345_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_206_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_258_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_187_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_87_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_154_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_253_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_337_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_232_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_147_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_377_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_362_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_233_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_250_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_297_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_354_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_262_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_225_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_365_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_356_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_242_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_245_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_230_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_318_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_181_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_305_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_255_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_298_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_301_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_293_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_229_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_332_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_257_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_295_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_239_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_260_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_228_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_317_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_364_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_262_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_162_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_310_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_160_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_237_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_254_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_257_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_291_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_279_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_215_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_372_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_218_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_210_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_158_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_156_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_339_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_190_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_373_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_205_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_329_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_380_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_271_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_235_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_244_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_312_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_186_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_308_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_347_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_241_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_270_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_261_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_222_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_189_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_289_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_146_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_252_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_344_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_331_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_313_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_328_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_265_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_283_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_248_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_381_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_211_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_214_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_176_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_221_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_150_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_343_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_249_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_322_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_351_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_234_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_195_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_330_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_334_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_306_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_213_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_325_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_342_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_247_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_352_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_225_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_226_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_256_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_242_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_368_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_382_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_157_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_238_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_267_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_367_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_321_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_233_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_281_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_161_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_336_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_170_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_304_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_44_xwqg-B00034_2020-04-03.npy
# all image files: 21521 # all weight files in weight_dir: 4155 # image files with weight 4155
train: /ibex/scratch/projects/c2052/air_tube_seg/training_samples/Y 15917
Using 4 GPUs
Going to train epochs [60-109]
Training epoch 60
	step [1/249], loss=88.0478
	step [2/249], loss=85.9776
	step [3/249], loss=90.8070
	step [4/249], loss=86.9803
	step [5/249], loss=79.0939
	step [6/249], loss=81.9375
	step [7/249], loss=93.8728
	step [8/249], loss=87.8972
	step [9/249], loss=88.7656
	step [10/249], loss=65.2135
	step [11/249], loss=93.4243
	step [12/249], loss=101.6200
	step [13/249], loss=84.7155
	step [14/249], loss=88.9452
	step [15/249], loss=86.0685
	step [16/249], loss=83.4354
	step [17/249], loss=83.8777
	step [18/249], loss=77.1923
	step [19/249], loss=85.5503
	step [20/249], loss=86.1208
	step [21/249], loss=94.1121
	step [22/249], loss=79.7542
	step [23/249], loss=90.6952
	step [24/249], loss=83.2827
	step [25/249], loss=83.4781
	step [26/249], loss=84.8632
	step [27/249], loss=78.2516
	step [28/249], loss=84.3349
	step [29/249], loss=77.3272
	step [30/249], loss=94.9778
	step [31/249], loss=61.1716
	step [32/249], loss=77.9189
	step [33/249], loss=88.2760
	step [34/249], loss=85.4288
	step [35/249], loss=84.8014
	step [36/249], loss=84.8963
	step [37/249], loss=93.6339
	step [38/249], loss=85.4957
	step [39/249], loss=89.2426
	step [40/249], loss=82.0606
	step [41/249], loss=67.9006
	step [42/249], loss=107.4760
	step [43/249], loss=86.3119
	step [44/249], loss=83.7382
	step [45/249], loss=94.8768
	step [46/249], loss=88.5256
	step [47/249], loss=76.3070
	step [48/249], loss=77.5967
	step [49/249], loss=72.8721
	step [50/249], loss=81.8966
	step [51/249], loss=100.5270
	step [52/249], loss=75.9793
	step [53/249], loss=65.7012
	step [54/249], loss=97.9626
	step [55/249], loss=67.4274
	step [56/249], loss=84.8203
	step [57/249], loss=84.0697
	step [58/249], loss=78.6249
	step [59/249], loss=79.8072
	step [60/249], loss=86.9073
	step [61/249], loss=88.4313
	step [62/249], loss=104.9361
	step [63/249], loss=84.3205
	step [64/249], loss=76.5628
	step [65/249], loss=58.5060
	step [66/249], loss=94.9445
	step [67/249], loss=104.7071
	step [68/249], loss=84.2804
	step [69/249], loss=75.6445
	step [70/249], loss=75.7345
	step [71/249], loss=67.7399
	step [72/249], loss=68.8952
	step [73/249], loss=83.1643
	step [74/249], loss=79.3961
	step [75/249], loss=89.9478
	step [76/249], loss=105.3281
	step [77/249], loss=67.4467
	step [78/249], loss=76.5119
	step [79/249], loss=95.2056
	step [80/249], loss=69.5236
	step [81/249], loss=102.2942
	step [82/249], loss=76.8659
	step [83/249], loss=94.9688
	step [84/249], loss=74.7982
	step [85/249], loss=73.0286
	step [86/249], loss=78.1442
	step [87/249], loss=83.0992
	step [88/249], loss=65.2271
	step [89/249], loss=65.2788
	step [90/249], loss=81.4660
	step [91/249], loss=85.4234
	step [92/249], loss=76.8955
	step [93/249], loss=69.6251
	step [94/249], loss=69.4142
	step [95/249], loss=80.0940
	step [96/249], loss=74.1809
	step [97/249], loss=78.4143
	step [98/249], loss=77.7486
	step [99/249], loss=74.6399
	step [100/249], loss=87.9515
	step [101/249], loss=104.7116
	step [102/249], loss=76.5012
	step [103/249], loss=82.5909
	step [104/249], loss=82.0237
	step [105/249], loss=68.9688
	step [106/249], loss=96.5377
	step [107/249], loss=98.6656
	step [108/249], loss=84.9455
	step [109/249], loss=95.0221
	step [110/249], loss=72.7137
	step [111/249], loss=89.9176
	step [112/249], loss=82.5323
	step [113/249], loss=91.1535
	step [114/249], loss=81.0960
	step [115/249], loss=96.9229
	step [116/249], loss=71.7242
	step [117/249], loss=90.2490
	step [118/249], loss=75.2772
	step [119/249], loss=75.4665
	step [120/249], loss=84.9006
	step [121/249], loss=74.9221
	step [122/249], loss=71.7025
	step [123/249], loss=65.4719
	step [124/249], loss=74.6095
	step [125/249], loss=91.8936
	step [126/249], loss=77.0448
	step [127/249], loss=87.0877
	step [128/249], loss=99.7296
	step [129/249], loss=67.8920
	step [130/249], loss=82.4529
	step [131/249], loss=81.8223
	step [132/249], loss=90.2460
	step [133/249], loss=93.1110
	step [134/249], loss=70.9266
	step [135/249], loss=84.8453
	step [136/249], loss=83.5242
	step [137/249], loss=72.2027
	step [138/249], loss=73.8178
	step [139/249], loss=84.6419
	step [140/249], loss=89.6791
	step [141/249], loss=90.9917
	step [142/249], loss=84.6411
	step [143/249], loss=87.1253
	step [144/249], loss=73.0544
	step [145/249], loss=78.1614
	step [146/249], loss=100.7660
	step [147/249], loss=91.2205
	step [148/249], loss=63.2090
	step [149/249], loss=81.8801
	step [150/249], loss=83.6821
	step [151/249], loss=95.2669
	step [152/249], loss=81.3198
	step [153/249], loss=83.9283
	step [154/249], loss=69.0884
	step [155/249], loss=92.7395
	step [156/249], loss=82.2911
	step [157/249], loss=80.1202
	step [158/249], loss=92.0016
	step [159/249], loss=72.9918
	step [160/249], loss=68.4332
	step [161/249], loss=82.7149
	step [162/249], loss=74.7511
	step [163/249], loss=96.5649
	step [164/249], loss=67.2126
	step [165/249], loss=81.1350
	step [166/249], loss=86.7028
	step [167/249], loss=72.6999
	step [168/249], loss=69.0456
	step [169/249], loss=82.4381
	step [170/249], loss=78.9097
	step [171/249], loss=99.0290
	step [172/249], loss=79.3699
	step [173/249], loss=79.1952
	step [174/249], loss=81.9850
	step [175/249], loss=83.7932
	step [176/249], loss=96.7897
	step [177/249], loss=90.5066
	step [178/249], loss=72.2590
	step [179/249], loss=77.8643
	step [180/249], loss=82.2615
	step [181/249], loss=107.6857
	step [182/249], loss=87.7418
	step [183/249], loss=65.1526
	step [184/249], loss=95.7124
	step [185/249], loss=80.0439
	step [186/249], loss=87.1015
	step [187/249], loss=69.0235
	step [188/249], loss=90.2289
	step [189/249], loss=83.5582
	step [190/249], loss=86.9125
	step [191/249], loss=90.6461
	step [192/249], loss=65.9001
	step [193/249], loss=83.3854
	step [194/249], loss=100.8540
	step [195/249], loss=80.7670
	step [196/249], loss=92.9854
	step [197/249], loss=65.4363
	step [198/249], loss=87.5895
	step [199/249], loss=98.7286
	step [200/249], loss=83.2875
	step [201/249], loss=77.8575
	step [202/249], loss=84.3506
	step [203/249], loss=93.2828
	step [204/249], loss=82.0941
	step [205/249], loss=95.5798
	step [206/249], loss=96.7447
	step [207/249], loss=99.9461
	step [208/249], loss=80.5205
	step [209/249], loss=81.1786
	step [210/249], loss=85.4132
	step [211/249], loss=67.5234
	step [212/249], loss=80.5939
	step [213/249], loss=72.6519
	step [214/249], loss=74.2489
	step [215/249], loss=94.0839
	step [216/249], loss=87.8516
	step [217/249], loss=77.3340
	step [218/249], loss=66.4155
	step [219/249], loss=75.2483
	step [220/249], loss=68.0731
	step [221/249], loss=89.4713
	step [222/249], loss=85.9973
	step [223/249], loss=85.9180
	step [224/249], loss=66.8851
	step [225/249], loss=81.5975
	step [226/249], loss=76.3921
	step [227/249], loss=85.5841
	step [228/249], loss=89.2930
	step [229/249], loss=78.0778
	step [230/249], loss=81.2475
	step [231/249], loss=72.0491
	step [232/249], loss=84.6461
	step [233/249], loss=98.7115
	step [234/249], loss=91.6811
	step [235/249], loss=82.3492
	step [236/249], loss=72.8890
	step [237/249], loss=95.4560
	step [238/249], loss=71.7516
	step [239/249], loss=79.3762
	step [240/249], loss=77.3720
	step [241/249], loss=80.0390
	step [242/249], loss=94.5876
	step [243/249], loss=82.1405
	step [244/249], loss=97.6205
	step [245/249], loss=79.0575
	step [246/249], loss=86.7400
	step [247/249], loss=80.3577
	step [248/249], loss=82.6147
	step [249/249], loss=65.5076
	Evaluating
	loss=0.0100, precision=0.2574, recall=0.8605, f1=0.3962
saving model as: 3_saved_model.pth
Training epoch 61
	step [1/249], loss=69.3594
	step [2/249], loss=105.7260
	step [3/249], loss=95.7718
	step [4/249], loss=87.4253
	step [5/249], loss=79.9620
	step [6/249], loss=90.7305
	step [7/249], loss=95.6420
	step [8/249], loss=90.4221
	step [9/249], loss=73.4736
	step [10/249], loss=81.8301
	step [11/249], loss=90.9884
	step [12/249], loss=83.1142
	step [13/249], loss=79.7820
	step [14/249], loss=81.3768
	step [15/249], loss=87.2227
	step [16/249], loss=95.7528
	step [17/249], loss=94.8565
	step [18/249], loss=74.2939
	step [19/249], loss=80.3117
	step [20/249], loss=78.1331
	step [21/249], loss=86.5568
	step [22/249], loss=88.9977
	step [23/249], loss=86.8188
	step [24/249], loss=78.2859
	step [25/249], loss=90.1462
	step [26/249], loss=76.1856
	step [27/249], loss=96.3681
	step [28/249], loss=84.0287
	step [29/249], loss=96.4951
	step [30/249], loss=90.7054
	step [31/249], loss=68.8779
	step [32/249], loss=92.8248
	step [33/249], loss=85.0755
	step [34/249], loss=67.6362
	step [35/249], loss=82.9059
	step [36/249], loss=88.1303
	step [37/249], loss=67.4669
	step [38/249], loss=75.7583
	step [39/249], loss=92.0188
	step [40/249], loss=81.3404
	step [41/249], loss=82.1935
	step [42/249], loss=87.4681
	step [43/249], loss=80.9424
	step [44/249], loss=92.4222
	step [45/249], loss=71.0154
	step [46/249], loss=90.9637
	step [47/249], loss=83.3756
	step [48/249], loss=71.0040
	step [49/249], loss=95.6746
	step [50/249], loss=85.4812
	step [51/249], loss=73.1137
	step [52/249], loss=83.4414
	step [53/249], loss=83.1570
	step [54/249], loss=73.7386
	step [55/249], loss=72.1627
	step [56/249], loss=100.2452
	step [57/249], loss=96.0248
	step [58/249], loss=118.6242
	step [59/249], loss=87.3641
	step [60/249], loss=65.2730
	step [61/249], loss=70.6617
	step [62/249], loss=79.3168
	step [63/249], loss=75.5804
	step [64/249], loss=77.5078
	step [65/249], loss=73.3768
	step [66/249], loss=76.4433
	step [67/249], loss=79.0750
	step [68/249], loss=93.6722
	step [69/249], loss=94.8569
	step [70/249], loss=85.6355
	step [71/249], loss=83.9704
	step [72/249], loss=87.6522
	step [73/249], loss=77.8659
	step [74/249], loss=59.9104
	step [75/249], loss=64.8634
	step [76/249], loss=78.3228
	step [77/249], loss=80.6829
	step [78/249], loss=74.0515
	step [79/249], loss=108.7241
	step [80/249], loss=80.2476
	step [81/249], loss=81.1686
	step [82/249], loss=70.6440
	step [83/249], loss=74.0685
	step [84/249], loss=84.7507
	step [85/249], loss=75.1474
	step [86/249], loss=60.6484
	step [87/249], loss=80.1615
	step [88/249], loss=82.2146
	step [89/249], loss=86.6525
	step [90/249], loss=77.7101
	step [91/249], loss=81.6669
	step [92/249], loss=71.9237
	step [93/249], loss=98.1329
	step [94/249], loss=67.8437
	step [95/249], loss=59.0448
	step [96/249], loss=87.3830
	step [97/249], loss=88.5415
	step [98/249], loss=81.4779
	step [99/249], loss=92.4929
	step [100/249], loss=69.3445
	step [101/249], loss=89.8161
	step [102/249], loss=88.7403
	step [103/249], loss=77.8583
	step [104/249], loss=103.5140
	step [105/249], loss=89.6180
	step [106/249], loss=87.4171
	step [107/249], loss=57.0984
	step [108/249], loss=82.7838
	step [109/249], loss=79.2598
	step [110/249], loss=96.4193
	step [111/249], loss=102.7658
	step [112/249], loss=77.0573
	step [113/249], loss=85.6891
	step [114/249], loss=73.5375
	step [115/249], loss=101.4117
	step [116/249], loss=85.3896
	step [117/249], loss=87.3298
	step [118/249], loss=89.4897
	step [119/249], loss=90.7293
	step [120/249], loss=76.9349
	step [121/249], loss=73.6768
	step [122/249], loss=72.1984
	step [123/249], loss=75.3988
	step [124/249], loss=79.3769
	step [125/249], loss=77.0385
	step [126/249], loss=89.8113
	step [127/249], loss=64.7188
	step [128/249], loss=93.7848
	step [129/249], loss=88.0527
	step [130/249], loss=77.7641
	step [131/249], loss=80.8661
	step [132/249], loss=90.5930
	step [133/249], loss=73.0423
	step [134/249], loss=82.7918
	step [135/249], loss=85.4483
	step [136/249], loss=71.8330
	step [137/249], loss=80.2152
	step [138/249], loss=72.0490
	step [139/249], loss=79.3918
	step [140/249], loss=87.9829
	step [141/249], loss=75.5623
	step [142/249], loss=87.1929
	step [143/249], loss=103.1474
	step [144/249], loss=86.4343
	step [145/249], loss=85.0552
	step [146/249], loss=67.5463
	step [147/249], loss=81.2764
	step [148/249], loss=75.1300
	step [149/249], loss=67.7078
	step [150/249], loss=82.3054
	step [151/249], loss=76.1554
	step [152/249], loss=90.1941
	step [153/249], loss=77.0346
	step [154/249], loss=79.7827
	step [155/249], loss=64.0573
	step [156/249], loss=93.1970
	step [157/249], loss=63.7019
	step [158/249], loss=94.9532
	step [159/249], loss=66.4834
	step [160/249], loss=84.6792
	step [161/249], loss=101.0723
	step [162/249], loss=69.0223
	step [163/249], loss=93.4119
	step [164/249], loss=87.3020
	step [165/249], loss=80.1454
	step [166/249], loss=69.9176
	step [167/249], loss=73.5342
	step [168/249], loss=81.6955
	step [169/249], loss=92.6044
	step [170/249], loss=85.0560
	step [171/249], loss=99.1167
	step [172/249], loss=92.6598
	step [173/249], loss=81.2784
	step [174/249], loss=75.5870
	step [175/249], loss=78.3099
	step [176/249], loss=77.0330
	step [177/249], loss=94.2396
	step [178/249], loss=70.4452
	step [179/249], loss=83.8383
	step [180/249], loss=84.0954
	step [181/249], loss=74.7249
	step [182/249], loss=82.3894
	step [183/249], loss=74.0243
	step [184/249], loss=80.6212
	step [185/249], loss=90.6900
	step [186/249], loss=96.2983
	step [187/249], loss=92.8790
	step [188/249], loss=69.4638
	step [189/249], loss=76.0087
	step [190/249], loss=89.3225
	step [191/249], loss=70.2796
	step [192/249], loss=85.1364
	step [193/249], loss=79.4951
	step [194/249], loss=78.1945
	step [195/249], loss=71.2770
	step [196/249], loss=84.1850
	step [197/249], loss=82.3927
	step [198/249], loss=89.6325
	step [199/249], loss=97.2802
	step [200/249], loss=91.7014
	step [201/249], loss=87.6394
	step [202/249], loss=72.7238
	step [203/249], loss=89.5488
	step [204/249], loss=76.5524
	step [205/249], loss=72.9376
	step [206/249], loss=87.0128
	step [207/249], loss=91.0404
	step [208/249], loss=76.3164
	step [209/249], loss=80.8485
	step [210/249], loss=75.5215
	step [211/249], loss=91.0961
	step [212/249], loss=88.3369
	step [213/249], loss=81.8745
	step [214/249], loss=87.4152
	step [215/249], loss=74.5371
	step [216/249], loss=78.0474
	step [217/249], loss=100.8633
	step [218/249], loss=86.3085
	step [219/249], loss=85.3472
	step [220/249], loss=81.1054
	step [221/249], loss=88.5434
	step [222/249], loss=76.9220
	step [223/249], loss=73.8152
	step [224/249], loss=63.3019
	step [225/249], loss=72.5211
	step [226/249], loss=91.6216
	step [227/249], loss=97.9870
	step [228/249], loss=71.0208
	step [229/249], loss=95.6654
	step [230/249], loss=79.7442
	step [231/249], loss=70.5385
	step [232/249], loss=69.2726
	step [233/249], loss=86.9629
	step [234/249], loss=70.2256
	step [235/249], loss=74.2804
	step [236/249], loss=68.0723
	step [237/249], loss=82.0536
	step [238/249], loss=82.7259
	step [239/249], loss=98.7803
	step [240/249], loss=84.8897
	step [241/249], loss=74.1926
	step [242/249], loss=62.5413
	step [243/249], loss=89.0598
	step [244/249], loss=78.8600
	step [245/249], loss=87.4445
	step [246/249], loss=63.5404
	step [247/249], loss=112.2573
	step [248/249], loss=83.2288
	step [249/249], loss=62.6364
	Evaluating
	loss=0.0075, precision=0.3326, recall=0.8425, f1=0.4769
saving model as: 3_saved_model.pth
Training epoch 62
	step [1/249], loss=89.6393
	step [2/249], loss=89.4691
	step [3/249], loss=75.6169
	step [4/249], loss=71.5845
	step [5/249], loss=90.0131
	step [6/249], loss=80.2121
	step [7/249], loss=96.2328
	step [8/249], loss=80.3739
	step [9/249], loss=74.6718
	step [10/249], loss=70.7243
	step [11/249], loss=81.8486
	step [12/249], loss=80.6349
	step [13/249], loss=95.8933
	step [14/249], loss=94.3150
	step [15/249], loss=71.6650
	step [16/249], loss=85.0616
	step [17/249], loss=84.2203
	step [18/249], loss=107.9969
	step [19/249], loss=89.5394
	step [20/249], loss=80.7480
	step [21/249], loss=93.3623
	step [22/249], loss=85.0828
	step [23/249], loss=67.0317
	step [24/249], loss=68.7930
	step [25/249], loss=78.7276
	step [26/249], loss=81.6555
	step [27/249], loss=80.7170
	step [28/249], loss=83.5137
	step [29/249], loss=76.7886
	step [30/249], loss=76.8900
	step [31/249], loss=93.8007
	step [32/249], loss=83.8740
	step [33/249], loss=85.4428
	step [34/249], loss=88.6106
	step [35/249], loss=76.0984
	step [36/249], loss=87.6105
	step [37/249], loss=87.7153
	step [38/249], loss=77.1485
	step [39/249], loss=93.8448
	step [40/249], loss=75.6160
	step [41/249], loss=69.8779
	step [42/249], loss=96.3511
	step [43/249], loss=85.0234
	step [44/249], loss=92.5063
	step [45/249], loss=81.9634
	step [46/249], loss=77.5131
	step [47/249], loss=93.6742
	step [48/249], loss=90.8656
	step [49/249], loss=80.3870
	step [50/249], loss=93.9581
	step [51/249], loss=77.8126
	step [52/249], loss=99.9883
	step [53/249], loss=75.5689
	step [54/249], loss=97.7402
	step [55/249], loss=72.5022
	step [56/249], loss=68.3843
	step [57/249], loss=91.4698
	step [58/249], loss=64.6120
	step [59/249], loss=75.3072
	step [60/249], loss=85.8707
	step [61/249], loss=77.8982
	step [62/249], loss=86.2579
	step [63/249], loss=84.2059
	step [64/249], loss=82.5831
	step [65/249], loss=66.8149
	step [66/249], loss=87.1459
	step [67/249], loss=68.4297
	step [68/249], loss=109.0156
	step [69/249], loss=89.7148
	step [70/249], loss=80.0852
	step [71/249], loss=70.8220
	step [72/249], loss=74.5686
	step [73/249], loss=93.3389
	step [74/249], loss=87.5240
	step [75/249], loss=80.3369
	step [76/249], loss=72.6857
	step [77/249], loss=93.8234
	step [78/249], loss=91.8148
	step [79/249], loss=68.5899
	step [80/249], loss=66.3087
	step [81/249], loss=75.4523
	step [82/249], loss=81.1342
	step [83/249], loss=63.4258
	step [84/249], loss=66.6716
	step [85/249], loss=83.9509
	step [86/249], loss=63.2364
	step [87/249], loss=95.1741
	step [88/249], loss=84.6425
	step [89/249], loss=72.6674
	step [90/249], loss=94.4755
	step [91/249], loss=85.8147
	step [92/249], loss=80.9836
	step [93/249], loss=68.7398
	step [94/249], loss=79.4940
	step [95/249], loss=82.8504
	step [96/249], loss=81.2418
	step [97/249], loss=78.8979
	step [98/249], loss=84.1133
	step [99/249], loss=84.8052
	step [100/249], loss=92.7717
	step [101/249], loss=84.8361
	step [102/249], loss=84.6474
	step [103/249], loss=90.4855
	step [104/249], loss=86.0205
	step [105/249], loss=81.6423
	step [106/249], loss=86.6898
	step [107/249], loss=80.1502
	step [108/249], loss=75.0453
	step [109/249], loss=80.6452
	step [110/249], loss=76.4548
	step [111/249], loss=69.7746
	step [112/249], loss=88.5090
	step [113/249], loss=73.6588
	step [114/249], loss=81.3001
	step [115/249], loss=70.1000
	step [116/249], loss=109.0853
	step [117/249], loss=81.1385
	step [118/249], loss=74.3609
	step [119/249], loss=66.0048
	step [120/249], loss=93.5271
	step [121/249], loss=101.4084
	step [122/249], loss=87.9879
	step [123/249], loss=98.2972
	step [124/249], loss=69.8422
	step [125/249], loss=77.2412
	step [126/249], loss=82.8404
	step [127/249], loss=75.4364
	step [128/249], loss=83.3927
	step [129/249], loss=86.6170
	step [130/249], loss=74.3702
	step [131/249], loss=66.0800
	step [132/249], loss=81.4909
	step [133/249], loss=72.8276
	step [134/249], loss=77.4152
	step [135/249], loss=90.8231
	step [136/249], loss=75.0831
	step [137/249], loss=91.7726
	step [138/249], loss=86.1625
	step [139/249], loss=79.6321
	step [140/249], loss=79.5081
	step [141/249], loss=78.7069
	step [142/249], loss=79.9344
	step [143/249], loss=76.1626
	step [144/249], loss=91.9193
	step [145/249], loss=97.9999
	step [146/249], loss=70.3719
	step [147/249], loss=90.0432
	step [148/249], loss=80.0484
	step [149/249], loss=86.4780
	step [150/249], loss=78.4346
	step [151/249], loss=88.1907
	step [152/249], loss=86.2938
	step [153/249], loss=80.0849
	step [154/249], loss=76.7289
	step [155/249], loss=78.6483
	step [156/249], loss=87.4274
	step [157/249], loss=83.7598
	step [158/249], loss=84.5357
	step [159/249], loss=102.1993
	step [160/249], loss=81.9598
	step [161/249], loss=73.5100
	step [162/249], loss=80.2994
	step [163/249], loss=75.0401
	step [164/249], loss=76.5820
	step [165/249], loss=81.1464
	step [166/249], loss=104.0203
	step [167/249], loss=73.0173
	step [168/249], loss=86.1441
	step [169/249], loss=68.7596
	step [170/249], loss=99.2300
	step [171/249], loss=91.7628
	step [172/249], loss=75.6167
	step [173/249], loss=75.6126
	step [174/249], loss=73.5895
	step [175/249], loss=82.6559
	step [176/249], loss=79.8700
	step [177/249], loss=90.8373
	step [178/249], loss=98.2862
	step [179/249], loss=89.5493
	step [180/249], loss=91.8824
	step [181/249], loss=82.1291
	step [182/249], loss=64.8663
	step [183/249], loss=87.2520
	step [184/249], loss=82.2403
	step [185/249], loss=74.3624
	step [186/249], loss=92.8665
	step [187/249], loss=85.7036
	step [188/249], loss=83.5885
	step [189/249], loss=71.2877
	step [190/249], loss=78.0648
	step [191/249], loss=84.8534
	step [192/249], loss=88.6890
	step [193/249], loss=95.2449
	step [194/249], loss=76.5015
	step [195/249], loss=62.0900
	step [196/249], loss=85.2974
	step [197/249], loss=86.1842
	step [198/249], loss=72.9883
	step [199/249], loss=74.7237
	step [200/249], loss=92.9626
	step [201/249], loss=90.2079
	step [202/249], loss=85.7757
	step [203/249], loss=73.4647
	step [204/249], loss=62.5866
	step [205/249], loss=97.9945
	step [206/249], loss=85.9035
	step [207/249], loss=73.4091
	step [208/249], loss=84.0600
	step [209/249], loss=89.2852
	step [210/249], loss=87.4024
	step [211/249], loss=83.7107
	step [212/249], loss=73.8305
	step [213/249], loss=79.1772
	step [214/249], loss=83.6061
	step [215/249], loss=73.9389
	step [216/249], loss=89.6065
	step [217/249], loss=75.9499
	step [218/249], loss=78.2718
	step [219/249], loss=78.0174
	step [220/249], loss=90.6081
	step [221/249], loss=92.6047
	step [222/249], loss=82.3567
	step [223/249], loss=85.5101
	step [224/249], loss=94.3771
	step [225/249], loss=89.5069
	step [226/249], loss=78.9208
	step [227/249], loss=70.1180
	step [228/249], loss=80.7763
	step [229/249], loss=76.7796
	step [230/249], loss=91.3088
	step [231/249], loss=81.4320
	step [232/249], loss=77.1988
	step [233/249], loss=83.5664
	step [234/249], loss=82.6097
	step [235/249], loss=85.6751
	step [236/249], loss=102.9574
	step [237/249], loss=64.4037
	step [238/249], loss=71.4661
	step [239/249], loss=77.8656
	step [240/249], loss=66.6743
	step [241/249], loss=83.6230
	step [242/249], loss=63.6550
	step [243/249], loss=77.2573
	step [244/249], loss=76.9573
	step [245/249], loss=101.5958
	step [246/249], loss=65.8041
	step [247/249], loss=85.4255
	step [248/249], loss=74.1520
	step [249/249], loss=57.3317
	Evaluating
	loss=0.0075, precision=0.3260, recall=0.8536, f1=0.4718
Training epoch 63
	step [1/249], loss=70.9396
	step [2/249], loss=82.8226
	step [3/249], loss=85.8052
	step [4/249], loss=62.3587
	step [5/249], loss=81.2939
	step [6/249], loss=91.4226
	step [7/249], loss=74.2457
	step [8/249], loss=77.0248
	step [9/249], loss=70.8486
	step [10/249], loss=69.7450
	step [11/249], loss=74.0525
	step [12/249], loss=83.9405
	step [13/249], loss=87.8314
	step [14/249], loss=109.3584
	step [15/249], loss=84.7894
	step [16/249], loss=91.9038
	step [17/249], loss=82.7650
	step [18/249], loss=72.9740
	step [19/249], loss=79.6614
	step [20/249], loss=77.0586
	step [21/249], loss=64.7815
	step [22/249], loss=86.3592
	step [23/249], loss=76.6788
	step [24/249], loss=82.0801
	step [25/249], loss=88.3626
	step [26/249], loss=83.5291
	step [27/249], loss=89.4673
	step [28/249], loss=82.4872
	step [29/249], loss=72.9296
	step [30/249], loss=109.8384
	step [31/249], loss=88.7180
	step [32/249], loss=76.3165
	step [33/249], loss=84.9875
	step [34/249], loss=74.0179
	step [35/249], loss=89.8327
	step [36/249], loss=90.9795
	step [37/249], loss=89.1491
	step [38/249], loss=88.0257
	step [39/249], loss=78.1724
	step [40/249], loss=78.6557
	step [41/249], loss=80.7411
	step [42/249], loss=80.9623
	step [43/249], loss=82.8471
	step [44/249], loss=88.8346
	step [45/249], loss=71.0191
	step [46/249], loss=73.6372
	step [47/249], loss=95.8476
	step [48/249], loss=88.8660
	step [49/249], loss=70.5071
	step [50/249], loss=87.6398
	step [51/249], loss=83.5400
	step [52/249], loss=81.2706
	step [53/249], loss=87.8256
	step [54/249], loss=100.4565
	step [55/249], loss=83.9054
	step [56/249], loss=83.1468
	step [57/249], loss=75.0281
	step [58/249], loss=82.3671
	step [59/249], loss=80.8391
	step [60/249], loss=74.3326
	step [61/249], loss=56.9636
	step [62/249], loss=94.1533
	step [63/249], loss=71.3180
	step [64/249], loss=76.9507
	step [65/249], loss=89.1694
	step [66/249], loss=76.7335
	step [67/249], loss=85.7513
	step [68/249], loss=104.8784
	step [69/249], loss=66.4887
	step [70/249], loss=84.2839
	step [71/249], loss=87.1789
	step [72/249], loss=89.3724
	step [73/249], loss=81.6735
	step [74/249], loss=72.2484
	step [75/249], loss=95.2260
	step [76/249], loss=74.9018
	step [77/249], loss=70.3949
	step [78/249], loss=71.0739
	step [79/249], loss=103.8306
	step [80/249], loss=92.9398
	step [81/249], loss=100.5699
	step [82/249], loss=82.5469
	step [83/249], loss=83.0882
	step [84/249], loss=86.4589
	step [85/249], loss=66.9432
	step [86/249], loss=65.8041
	step [87/249], loss=80.4997
	step [88/249], loss=78.8666
	step [89/249], loss=70.8364
	step [90/249], loss=82.8003
	step [91/249], loss=87.2738
	step [92/249], loss=78.4858
	step [93/249], loss=72.0755
	step [94/249], loss=97.3597
	step [95/249], loss=69.9302
	step [96/249], loss=73.3631
	step [97/249], loss=89.4912
	step [98/249], loss=88.1215
	step [99/249], loss=83.1927
	step [100/249], loss=77.7557
	step [101/249], loss=98.0642
	step [102/249], loss=77.8383
	step [103/249], loss=94.4788
	step [104/249], loss=87.7343
	step [105/249], loss=94.9942
	step [106/249], loss=97.0913
	step [107/249], loss=87.3642
	step [108/249], loss=89.7839
	step [109/249], loss=86.4680
	step [110/249], loss=72.2535
	step [111/249], loss=77.0572
	step [112/249], loss=88.6664
	step [113/249], loss=75.7496
	step [114/249], loss=100.1407
	step [115/249], loss=76.3217
	step [116/249], loss=75.2412
	step [117/249], loss=76.8882
	step [118/249], loss=65.4156
	step [119/249], loss=76.7608
	step [120/249], loss=75.5457
	step [121/249], loss=92.1451
	step [122/249], loss=76.9600
	step [123/249], loss=77.9759
	step [124/249], loss=65.5525
	step [125/249], loss=101.0770
	step [126/249], loss=87.9409
	step [127/249], loss=79.6531
	step [128/249], loss=78.4019
	step [129/249], loss=70.4875
	step [130/249], loss=79.5850
	step [131/249], loss=74.6559
	step [132/249], loss=73.2349
	step [133/249], loss=73.6781
	step [134/249], loss=82.8719
	step [135/249], loss=75.1983
	step [136/249], loss=77.9894
	step [137/249], loss=93.8058
	step [138/249], loss=89.2297
	step [139/249], loss=112.6351
	step [140/249], loss=91.1923
	step [141/249], loss=63.9967
	step [142/249], loss=77.6148
	step [143/249], loss=79.3384
	step [144/249], loss=82.7626
	step [145/249], loss=77.4920
	step [146/249], loss=84.6263
	step [147/249], loss=92.0157
	step [148/249], loss=86.8112
	step [149/249], loss=72.3246
	step [150/249], loss=79.4174
	step [151/249], loss=72.7827
	step [152/249], loss=80.4769
	step [153/249], loss=75.5798
	step [154/249], loss=78.1846
	step [155/249], loss=79.7622
	step [156/249], loss=81.6220
	step [157/249], loss=81.3715
	step [158/249], loss=84.4881
	step [159/249], loss=78.5877
	step [160/249], loss=69.2021
	step [161/249], loss=74.7335
	step [162/249], loss=72.0871
	step [163/249], loss=76.4970
	step [164/249], loss=78.6429
	step [165/249], loss=91.7100
	step [166/249], loss=91.7413
	step [167/249], loss=76.0666
	step [168/249], loss=91.5050
	step [169/249], loss=93.1488
	step [170/249], loss=85.8554
	step [171/249], loss=88.9378
	step [172/249], loss=72.2489
	step [173/249], loss=76.2810
	step [174/249], loss=85.3411
	step [175/249], loss=91.9051
	step [176/249], loss=85.2912
	step [177/249], loss=80.4142
	step [178/249], loss=89.4228
	step [179/249], loss=77.6594
	step [180/249], loss=78.1602
	step [181/249], loss=75.4822
	step [182/249], loss=83.9540
	step [183/249], loss=81.9429
	step [184/249], loss=72.8128
	step [185/249], loss=88.7976
	step [186/249], loss=81.8427
	step [187/249], loss=83.8593
	step [188/249], loss=94.8497
	step [189/249], loss=91.9634
	step [190/249], loss=73.3037
	step [191/249], loss=77.6721
	step [192/249], loss=74.7645
	step [193/249], loss=71.1491
	step [194/249], loss=103.4059
	step [195/249], loss=69.7647
	step [196/249], loss=83.9591
	step [197/249], loss=90.7233
	step [198/249], loss=80.6859
	step [199/249], loss=75.4868
	step [200/249], loss=75.7383
	step [201/249], loss=83.6819
	step [202/249], loss=86.9829
	step [203/249], loss=73.8691
	step [204/249], loss=80.2487
	step [205/249], loss=79.9094
	step [206/249], loss=69.0587
	step [207/249], loss=76.0651
	step [208/249], loss=74.8521
	step [209/249], loss=87.1579
	step [210/249], loss=91.9538
	step [211/249], loss=90.9885
	step [212/249], loss=80.8351
	step [213/249], loss=88.2448
	step [214/249], loss=70.2699
	step [215/249], loss=99.0983
	step [216/249], loss=74.5040
	step [217/249], loss=89.5784
	step [218/249], loss=81.2399
	step [219/249], loss=81.4973
	step [220/249], loss=62.8653
	step [221/249], loss=80.6102
	step [222/249], loss=77.4204
	step [223/249], loss=92.1779
	step [224/249], loss=104.8200
	step [225/249], loss=76.1628
	step [226/249], loss=82.3753
	step [227/249], loss=81.0845
	step [228/249], loss=76.1669
	step [229/249], loss=93.6568
	step [230/249], loss=95.5440
	step [231/249], loss=78.6821
	step [232/249], loss=74.4939
	step [233/249], loss=94.5446
	step [234/249], loss=84.9235
	step [235/249], loss=103.2417
	step [236/249], loss=84.5790
	step [237/249], loss=86.7527
	step [238/249], loss=83.9809
	step [239/249], loss=75.9296
	step [240/249], loss=82.5918
	step [241/249], loss=77.6577
	step [242/249], loss=76.8498
	step [243/249], loss=69.5274
	step [244/249], loss=68.8734
	step [245/249], loss=70.2739
	step [246/249], loss=73.1822
	step [247/249], loss=69.5722
	step [248/249], loss=90.6433
	step [249/249], loss=44.8832
	Evaluating
	loss=0.0082, precision=0.2954, recall=0.8566, f1=0.4393
Training epoch 64
	step [1/249], loss=63.8257
	step [2/249], loss=85.2627
	step [3/249], loss=92.3495
	step [4/249], loss=83.7911
	step [5/249], loss=83.4066
	step [6/249], loss=92.8711
	step [7/249], loss=108.2552
	step [8/249], loss=72.4004
	step [9/249], loss=85.4349
	step [10/249], loss=90.0503
	step [11/249], loss=85.1651
	step [12/249], loss=76.5715
	step [13/249], loss=85.4504
	step [14/249], loss=84.8286
	step [15/249], loss=65.7993
	step [16/249], loss=82.0841
	step [17/249], loss=78.2083
	step [18/249], loss=71.9088
	step [19/249], loss=93.8412
	step [20/249], loss=71.5373
	step [21/249], loss=93.7692
	step [22/249], loss=82.1407
	step [23/249], loss=78.2413
	step [24/249], loss=66.3454
	step [25/249], loss=90.4981
	step [26/249], loss=89.8461
	step [27/249], loss=81.6667
	step [28/249], loss=91.4821
	step [29/249], loss=97.8241
	step [30/249], loss=83.2155
	step [31/249], loss=83.4032
	step [32/249], loss=61.3802
	step [33/249], loss=92.5583
	step [34/249], loss=72.3114
	step [35/249], loss=71.2154
	step [36/249], loss=87.6563
	step [37/249], loss=78.4081
	step [38/249], loss=68.6103
	step [39/249], loss=77.5945
	step [40/249], loss=77.7131
	step [41/249], loss=81.6506
	step [42/249], loss=90.7167
	step [43/249], loss=96.1556
	step [44/249], loss=75.2751
	step [45/249], loss=87.0497
	step [46/249], loss=100.5171
	step [47/249], loss=88.8192
	step [48/249], loss=71.0712
	step [49/249], loss=76.1599
	step [50/249], loss=74.2029
	step [51/249], loss=85.9282
	step [52/249], loss=76.0013
	step [53/249], loss=82.7128
	step [54/249], loss=74.0831
	step [55/249], loss=66.5475
	step [56/249], loss=82.8595
	step [57/249], loss=59.1510
	step [58/249], loss=71.4159
	step [59/249], loss=84.5147
	step [60/249], loss=72.1011
	step [61/249], loss=79.1533
	step [62/249], loss=81.0604
	step [63/249], loss=79.2852
	step [64/249], loss=91.5357
	step [65/249], loss=85.9856
	step [66/249], loss=74.2127
	step [67/249], loss=76.7882
	step [68/249], loss=85.0248
	step [69/249], loss=70.6320
	step [70/249], loss=76.8793
	step [71/249], loss=79.1846
	step [72/249], loss=91.6566
	step [73/249], loss=79.6980
	step [74/249], loss=75.9382
	step [75/249], loss=64.4923
	step [76/249], loss=76.5514
	step [77/249], loss=63.0131
	step [78/249], loss=86.4934
	step [79/249], loss=96.5333
	step [80/249], loss=88.8547
	step [81/249], loss=86.8047
	step [82/249], loss=68.7345
	step [83/249], loss=71.0699
	step [84/249], loss=80.4763
	step [85/249], loss=75.9450
	step [86/249], loss=81.3933
	step [87/249], loss=72.6919
	step [88/249], loss=87.4738
	step [89/249], loss=78.5396
	step [90/249], loss=66.3699
	step [91/249], loss=80.9409
	step [92/249], loss=75.5116
	step [93/249], loss=85.6155
	step [94/249], loss=83.3055
	step [95/249], loss=82.5071
	step [96/249], loss=91.1154
	step [97/249], loss=81.3867
	step [98/249], loss=74.3976
	step [99/249], loss=82.2434
	step [100/249], loss=83.0506
	step [101/249], loss=80.1032
	step [102/249], loss=92.2585
	step [103/249], loss=55.2712
	step [104/249], loss=76.9185
	step [105/249], loss=87.2037
	step [106/249], loss=81.0326
	step [107/249], loss=93.8570
	step [108/249], loss=72.5312
	step [109/249], loss=90.4111
	step [110/249], loss=68.0424
	step [111/249], loss=73.4584
	step [112/249], loss=92.3425
	step [113/249], loss=77.6813
	step [114/249], loss=84.7217
	step [115/249], loss=89.6580
	step [116/249], loss=70.3669
	step [117/249], loss=79.5965
	step [118/249], loss=96.3449
	step [119/249], loss=82.0682
	step [120/249], loss=72.4369
	step [121/249], loss=109.2603
	step [122/249], loss=75.8770
	step [123/249], loss=82.3774
	step [124/249], loss=87.6091
	step [125/249], loss=89.2100
	step [126/249], loss=80.1822
	step [127/249], loss=90.5296
	step [128/249], loss=86.3232
	step [129/249], loss=70.8703
	step [130/249], loss=88.5938
	step [131/249], loss=78.7007
	step [132/249], loss=71.3838
	step [133/249], loss=87.8155
	step [134/249], loss=80.0052
	step [135/249], loss=76.8049
	step [136/249], loss=69.1708
	step [137/249], loss=75.1474
	step [138/249], loss=79.6357
	step [139/249], loss=72.4524
	step [140/249], loss=85.6940
	step [141/249], loss=76.1358
	step [142/249], loss=72.9227
	step [143/249], loss=88.2697
	step [144/249], loss=77.9866
	step [145/249], loss=92.7899
	step [146/249], loss=83.4650
	step [147/249], loss=86.8121
	step [148/249], loss=83.2275
	step [149/249], loss=73.5361
	step [150/249], loss=88.7392
	step [151/249], loss=78.4120
	step [152/249], loss=90.8103
	step [153/249], loss=88.2858
	step [154/249], loss=93.2026
	step [155/249], loss=84.4622
	step [156/249], loss=86.7016
	step [157/249], loss=77.8610
	step [158/249], loss=92.9184
	step [159/249], loss=72.6436
	step [160/249], loss=101.4564
	step [161/249], loss=86.1136
	step [162/249], loss=92.8612
	step [163/249], loss=85.4100
	step [164/249], loss=80.4376
	step [165/249], loss=83.4207
	step [166/249], loss=77.5661
	step [167/249], loss=71.1522
	step [168/249], loss=72.7345
	step [169/249], loss=75.1586
	step [170/249], loss=67.8174
	step [171/249], loss=87.9443
	step [172/249], loss=68.8530
	step [173/249], loss=89.1395
	step [174/249], loss=82.7388
	step [175/249], loss=73.2192
	step [176/249], loss=83.2967
	step [177/249], loss=65.9803
	step [178/249], loss=87.2631
	step [179/249], loss=77.9975
	step [180/249], loss=87.8833
	step [181/249], loss=66.8924
	step [182/249], loss=73.1841
	step [183/249], loss=67.8450
	step [184/249], loss=72.1150
	step [185/249], loss=72.1677
	step [186/249], loss=90.8299
	step [187/249], loss=80.6404
	step [188/249], loss=77.2319
	step [189/249], loss=65.8242
	step [190/249], loss=82.0832
	step [191/249], loss=78.8979
	step [192/249], loss=83.0011
	step [193/249], loss=92.9576
	step [194/249], loss=80.3988
	step [195/249], loss=75.3861
	step [196/249], loss=79.0274
	step [197/249], loss=72.4567
	step [198/249], loss=85.1284
	step [199/249], loss=72.1888
	step [200/249], loss=91.4532
	step [201/249], loss=92.6682
	step [202/249], loss=95.5191
	step [203/249], loss=79.2186
	step [204/249], loss=88.5841
	step [205/249], loss=73.5738
	step [206/249], loss=70.0648
	step [207/249], loss=86.6241
	step [208/249], loss=79.5174
	step [209/249], loss=81.7428
	step [210/249], loss=82.2927
	step [211/249], loss=95.7732
	step [212/249], loss=79.6939
	step [213/249], loss=88.8102
	step [214/249], loss=101.2621
	step [215/249], loss=93.0090
	step [216/249], loss=69.9792
	step [217/249], loss=91.9926
	step [218/249], loss=100.4641
	step [219/249], loss=71.3462
	step [220/249], loss=85.9690
	step [221/249], loss=69.0456
	step [222/249], loss=69.7430
	step [223/249], loss=98.0393
	step [224/249], loss=89.5378
	step [225/249], loss=74.4744
	step [226/249], loss=82.1673
	step [227/249], loss=94.3433
	step [228/249], loss=92.0570
	step [229/249], loss=89.9452
	step [230/249], loss=88.7290
	step [231/249], loss=93.9582
	step [232/249], loss=85.3549
	step [233/249], loss=85.5228
	step [234/249], loss=86.1717
	step [235/249], loss=82.3371
	step [236/249], loss=75.9272
	step [237/249], loss=109.1405
	step [238/249], loss=63.0024
	step [239/249], loss=89.0438
	step [240/249], loss=101.6652
	step [241/249], loss=91.9949
	step [242/249], loss=67.1921
	step [243/249], loss=81.0570
	step [244/249], loss=83.0036
	step [245/249], loss=87.6961
	step [246/249], loss=80.8221
	step [247/249], loss=66.0669
	step [248/249], loss=84.3875
	step [249/249], loss=52.1711
	Evaluating
	loss=0.0076, precision=0.3234, recall=0.8519, f1=0.4689
Training epoch 65
	step [1/249], loss=72.5983
	step [2/249], loss=80.4864
	step [3/249], loss=90.1623
	step [4/249], loss=87.5266
	step [5/249], loss=83.6677
	step [6/249], loss=73.9956
	step [7/249], loss=106.0431
	step [8/249], loss=83.8586
	step [9/249], loss=86.5614
	step [10/249], loss=80.1609
	step [11/249], loss=72.6285
	step [12/249], loss=102.7860
	step [13/249], loss=92.3373
	step [14/249], loss=61.6831
	step [15/249], loss=90.7931
	step [16/249], loss=90.6098
	step [17/249], loss=81.4741
	step [18/249], loss=73.9931
	step [19/249], loss=82.3406
	step [20/249], loss=87.5887
	step [21/249], loss=73.1075
	step [22/249], loss=68.9462
	step [23/249], loss=81.9941
	step [24/249], loss=75.1358
	step [25/249], loss=83.8477
	step [26/249], loss=70.2606
	step [27/249], loss=77.3559
	step [28/249], loss=80.7754
	step [29/249], loss=77.3709
	step [30/249], loss=73.0829
	step [31/249], loss=72.8831
	step [32/249], loss=70.6515
	step [33/249], loss=85.7767
	step [34/249], loss=86.7175
	step [35/249], loss=84.1357
	step [36/249], loss=80.7830
	step [37/249], loss=71.2060
	step [38/249], loss=107.6986
	step [39/249], loss=93.8460
	step [40/249], loss=80.6365
	step [41/249], loss=67.2308
	step [42/249], loss=82.5184
	step [43/249], loss=75.0471
	step [44/249], loss=70.4524
	step [45/249], loss=89.2761
	step [46/249], loss=83.9505
	step [47/249], loss=77.6226
	step [48/249], loss=83.8359
	step [49/249], loss=89.2953
	step [50/249], loss=80.7111
	step [51/249], loss=77.9990
	step [52/249], loss=87.0187
	step [53/249], loss=79.1997
	step [54/249], loss=80.7915
	step [55/249], loss=69.6101
	step [56/249], loss=77.8945
	step [57/249], loss=87.6503
	step [58/249], loss=83.9691
	step [59/249], loss=76.1994
	step [60/249], loss=89.7482
	step [61/249], loss=87.5789
	step [62/249], loss=77.0695
	step [63/249], loss=76.0848
	step [64/249], loss=93.4819
	step [65/249], loss=86.3640
	step [66/249], loss=78.6475
	step [67/249], loss=85.1562
	step [68/249], loss=69.5207
	step [69/249], loss=90.7412
	step [70/249], loss=94.8943
	step [71/249], loss=86.5075
	step [72/249], loss=94.6308
	step [73/249], loss=94.3921
	step [74/249], loss=69.0240
	step [75/249], loss=80.0783
	step [76/249], loss=88.3198
	step [77/249], loss=64.9655
	step [78/249], loss=98.4849
	step [79/249], loss=68.6596
	step [80/249], loss=65.2802
	step [81/249], loss=62.6032
	step [82/249], loss=90.0357
	step [83/249], loss=90.9676
	step [84/249], loss=90.7677
	step [85/249], loss=84.7472
	step [86/249], loss=86.6253
	step [87/249], loss=82.4749
	step [88/249], loss=87.4013
	step [89/249], loss=71.6994
	step [90/249], loss=74.7662
	step [91/249], loss=87.5494
	step [92/249], loss=71.7081
	step [93/249], loss=74.4769
	step [94/249], loss=69.3561
	step [95/249], loss=85.1852
	step [96/249], loss=81.2556
	step [97/249], loss=68.7840
	step [98/249], loss=85.2289
	step [99/249], loss=86.6200
	step [100/249], loss=80.7709
	step [101/249], loss=83.9891
	step [102/249], loss=88.5343
	step [103/249], loss=81.4599
	step [104/249], loss=98.6579
	step [105/249], loss=86.5935
	step [106/249], loss=87.9412
	step [107/249], loss=90.2147
	step [108/249], loss=67.4420
	step [109/249], loss=76.4607
	step [110/249], loss=72.0922
	step [111/249], loss=81.3956
	step [112/249], loss=79.8286
	step [113/249], loss=82.7561
	step [114/249], loss=81.2659
	step [115/249], loss=89.5441
	step [116/249], loss=79.8240
	step [117/249], loss=97.1175
	step [118/249], loss=78.5945
	step [119/249], loss=95.0020
	step [120/249], loss=77.3397
	step [121/249], loss=86.8354
	step [122/249], loss=99.3939
	step [123/249], loss=91.2932
	step [124/249], loss=74.4532
	step [125/249], loss=106.3250
	step [126/249], loss=71.5098
	step [127/249], loss=92.1198
	step [128/249], loss=81.8010
	step [129/249], loss=87.0018
	step [130/249], loss=92.4618
	step [131/249], loss=71.0783
	step [132/249], loss=74.8841
	step [133/249], loss=75.9578
	step [134/249], loss=78.1319
	step [135/249], loss=88.2391
	step [136/249], loss=68.9324
	step [137/249], loss=70.9759
	step [138/249], loss=86.1595
	step [139/249], loss=64.7100
	step [140/249], loss=78.9107
	step [141/249], loss=77.0509
	step [142/249], loss=90.6754
	step [143/249], loss=87.9898
	step [144/249], loss=98.1134
	step [145/249], loss=69.0073
	step [146/249], loss=82.3786
	step [147/249], loss=103.0501
	step [148/249], loss=79.8127
	step [149/249], loss=84.4470
	step [150/249], loss=87.6835
	step [151/249], loss=77.6502
	step [152/249], loss=78.6781
	step [153/249], loss=77.0850
	step [154/249], loss=97.4020
	step [155/249], loss=76.7660
	step [156/249], loss=82.8544
	step [157/249], loss=75.2514
	step [158/249], loss=80.5966
	step [159/249], loss=109.0883
	step [160/249], loss=79.7909
	step [161/249], loss=79.1393
	step [162/249], loss=84.3874
	step [163/249], loss=70.1322
	step [164/249], loss=80.9434
	step [165/249], loss=76.7629
	step [166/249], loss=78.7714
	step [167/249], loss=81.2333
	step [168/249], loss=77.6032
	step [169/249], loss=79.1250
	step [170/249], loss=79.9952
	step [171/249], loss=77.9938
	step [172/249], loss=73.3696
	step [173/249], loss=77.2734
	step [174/249], loss=73.4165
	step [175/249], loss=86.6427
	step [176/249], loss=72.9118
	step [177/249], loss=86.3436
	step [178/249], loss=83.9536
	step [179/249], loss=87.7969
	step [180/249], loss=85.2197
	step [181/249], loss=75.8782
	step [182/249], loss=68.6672
	step [183/249], loss=72.5885
	step [184/249], loss=99.7601
	step [185/249], loss=71.3526
	step [186/249], loss=75.4195
	step [187/249], loss=73.0093
	step [188/249], loss=74.8769
	step [189/249], loss=87.3512
	step [190/249], loss=81.7315
	step [191/249], loss=79.8749
	step [192/249], loss=88.5975
	step [193/249], loss=97.4917
	step [194/249], loss=79.7133
	step [195/249], loss=81.6971
	step [196/249], loss=84.0527
	step [197/249], loss=80.1032
	step [198/249], loss=86.9087
	step [199/249], loss=75.5111
	step [200/249], loss=85.5965
	step [201/249], loss=85.9628
	step [202/249], loss=81.4618
	step [203/249], loss=75.2584
	step [204/249], loss=73.5145
	step [205/249], loss=89.6514
	step [206/249], loss=67.2624
	step [207/249], loss=74.1469
	step [208/249], loss=74.6794
	step [209/249], loss=81.7320
	step [210/249], loss=77.6919
	step [211/249], loss=63.9535
	step [212/249], loss=61.3971
	step [213/249], loss=72.8547
	step [214/249], loss=91.4137
	step [215/249], loss=82.1104
	step [216/249], loss=82.6497
	step [217/249], loss=76.3788
	step [218/249], loss=64.8150
	step [219/249], loss=88.3141
	step [220/249], loss=86.1785
	step [221/249], loss=77.3116
	step [222/249], loss=70.9230
	step [223/249], loss=76.5633
	step [224/249], loss=69.9423
	step [225/249], loss=102.1813
	step [226/249], loss=75.3832
	step [227/249], loss=84.6390
	step [228/249], loss=73.3454
	step [229/249], loss=83.0267
	step [230/249], loss=79.0501
	step [231/249], loss=96.0532
	step [232/249], loss=94.3571
	step [233/249], loss=75.8952
	step [234/249], loss=71.0412
	step [235/249], loss=67.1233
	step [236/249], loss=81.2464
	step [237/249], loss=79.2217
	step [238/249], loss=73.0056
	step [239/249], loss=86.0065
	step [240/249], loss=80.3635
	step [241/249], loss=73.2622
	step [242/249], loss=67.0203
	step [243/249], loss=72.6121
	step [244/249], loss=79.4172
	step [245/249], loss=69.7144
	step [246/249], loss=82.9808
	step [247/249], loss=97.0587
	step [248/249], loss=78.4628
	step [249/249], loss=46.8282
	Evaluating
	loss=0.0071, precision=0.3423, recall=0.8545, f1=0.4888
saving model as: 3_saved_model.pth
Training epoch 66
	step [1/249], loss=70.1439
	step [2/249], loss=92.8493
	step [3/249], loss=70.0334
	step [4/249], loss=68.8836
	step [5/249], loss=79.3224
	step [6/249], loss=94.7666
	step [7/249], loss=74.0399
	step [8/249], loss=79.8181
	step [9/249], loss=72.3344
	step [10/249], loss=69.8489
	step [11/249], loss=91.0136
	step [12/249], loss=104.7928
	step [13/249], loss=78.6438
	step [14/249], loss=87.8148
	step [15/249], loss=82.9360
	step [16/249], loss=85.9696
	step [17/249], loss=89.7988
	step [18/249], loss=64.3746
	step [19/249], loss=77.4804
	step [20/249], loss=82.5097
	step [21/249], loss=79.5943
	step [22/249], loss=79.1731
	step [23/249], loss=79.3078
	step [24/249], loss=74.8348
	step [25/249], loss=96.1729
	step [26/249], loss=77.5452
	step [27/249], loss=105.8591
	step [28/249], loss=85.7313
	step [29/249], loss=87.5902
	step [30/249], loss=79.8432
	step [31/249], loss=84.5066
	step [32/249], loss=82.2273
	step [33/249], loss=94.8654
	step [34/249], loss=98.4723
	step [35/249], loss=75.0682
	step [36/249], loss=58.4257
	step [37/249], loss=74.2331
	step [38/249], loss=78.5524
	step [39/249], loss=83.0991
	step [40/249], loss=74.6750
	step [41/249], loss=76.1989
	step [42/249], loss=63.6337
	step [43/249], loss=84.5468
	step [44/249], loss=62.8653
	step [45/249], loss=79.9852
	step [46/249], loss=81.5532
	step [47/249], loss=76.6849
	step [48/249], loss=64.4023
	step [49/249], loss=78.7357
	step [50/249], loss=79.2104
	step [51/249], loss=72.3662
	step [52/249], loss=84.1729
	step [53/249], loss=84.0141
	step [54/249], loss=62.7938
	step [55/249], loss=75.8464
	step [56/249], loss=88.8532
	step [57/249], loss=79.3268
	step [58/249], loss=94.9803
	step [59/249], loss=88.2087
	step [60/249], loss=76.8813
	step [61/249], loss=100.8561
	step [62/249], loss=76.7909
	step [63/249], loss=90.8332
	step [64/249], loss=86.4059
	step [65/249], loss=80.0972
	step [66/249], loss=78.5069
	step [67/249], loss=71.7365
	step [68/249], loss=70.1593
	step [69/249], loss=91.9693
	step [70/249], loss=71.0257
	step [71/249], loss=86.9625
	step [72/249], loss=78.3204
	step [73/249], loss=70.0184
	step [74/249], loss=61.2056
	step [75/249], loss=84.4121
	step [76/249], loss=87.1884
	step [77/249], loss=89.7225
	step [78/249], loss=92.5365
	step [79/249], loss=86.4123
	step [80/249], loss=72.8696
	step [81/249], loss=62.5696
	step [82/249], loss=97.1865
	step [83/249], loss=63.3572
	step [84/249], loss=76.2859
	step [85/249], loss=78.7529
	step [86/249], loss=81.4494
	step [87/249], loss=81.8121
	step [88/249], loss=80.0646
	step [89/249], loss=88.8945
	step [90/249], loss=80.3071
	step [91/249], loss=96.8235
	step [92/249], loss=89.6275
	step [93/249], loss=95.1940
	step [94/249], loss=84.5017
	step [95/249], loss=74.5043
	step [96/249], loss=86.8792
	step [97/249], loss=66.8656
	step [98/249], loss=83.9826
	step [99/249], loss=65.4912
	step [100/249], loss=60.1358
	step [101/249], loss=82.4239
	step [102/249], loss=85.8529
	step [103/249], loss=75.8839
	step [104/249], loss=70.1265
	step [105/249], loss=71.3860
	step [106/249], loss=91.7193
	step [107/249], loss=65.1841
	step [108/249], loss=83.9846
	step [109/249], loss=77.9608
	step [110/249], loss=67.1070
	step [111/249], loss=77.0278
	step [112/249], loss=77.2097
	step [113/249], loss=70.5267
	step [114/249], loss=108.4953
	step [115/249], loss=98.8247
	step [116/249], loss=81.7326
	step [117/249], loss=72.2482
	step [118/249], loss=103.2573
	step [119/249], loss=79.8845
	step [120/249], loss=74.5168
	step [121/249], loss=65.3673
	step [122/249], loss=79.2222
	step [123/249], loss=71.2000
	step [124/249], loss=85.4329
	step [125/249], loss=80.0401
	step [126/249], loss=80.2024
	step [127/249], loss=79.8914
	step [128/249], loss=75.4088
	step [129/249], loss=81.4994
	step [130/249], loss=82.2877
	step [131/249], loss=87.7454
	step [132/249], loss=79.7847
	step [133/249], loss=79.4387
	step [134/249], loss=72.0396
	step [135/249], loss=79.6193
	step [136/249], loss=89.9092
	step [137/249], loss=74.8094
	step [138/249], loss=85.6600
	step [139/249], loss=75.9868
	step [140/249], loss=80.4794
	step [141/249], loss=84.6714
	step [142/249], loss=86.2540
	step [143/249], loss=84.6644
	step [144/249], loss=83.7787
	step [145/249], loss=74.5238
	step [146/249], loss=62.4563
	step [147/249], loss=94.2700
	step [148/249], loss=74.4049
	step [149/249], loss=82.8931
	step [150/249], loss=77.5078
	step [151/249], loss=53.5977
	step [152/249], loss=70.1603
	step [153/249], loss=80.5661
	step [154/249], loss=78.9665
	step [155/249], loss=85.0909
	step [156/249], loss=78.8967
	step [157/249], loss=73.6886
	step [158/249], loss=82.4738
	step [159/249], loss=98.0519
	step [160/249], loss=80.0168
	step [161/249], loss=101.2737
	step [162/249], loss=78.2672
	step [163/249], loss=75.1318
	step [164/249], loss=85.1208
	step [165/249], loss=75.7233
	step [166/249], loss=88.7104
	step [167/249], loss=75.8090
	step [168/249], loss=84.3224
	step [169/249], loss=84.2504
	step [170/249], loss=83.7236
	step [171/249], loss=72.2258
	step [172/249], loss=89.5611
	step [173/249], loss=95.5073
	step [174/249], loss=77.1342
	step [175/249], loss=94.7421
	step [176/249], loss=93.3007
	step [177/249], loss=78.2161
	step [178/249], loss=74.0166
	step [179/249], loss=94.0218
	step [180/249], loss=80.8279
	step [181/249], loss=76.4979
	step [182/249], loss=69.4645
	step [183/249], loss=78.4358
	step [184/249], loss=94.5768
	step [185/249], loss=80.5508
	step [186/249], loss=90.3831
	step [187/249], loss=82.1229
	step [188/249], loss=62.6599
	step [189/249], loss=77.1618
	step [190/249], loss=78.8586
	step [191/249], loss=91.3471
	step [192/249], loss=71.0830
	step [193/249], loss=55.6337
	step [194/249], loss=80.4826
	step [195/249], loss=80.5822
	step [196/249], loss=86.5511
	step [197/249], loss=68.5934
	step [198/249], loss=78.5764
	step [199/249], loss=75.6807
	step [200/249], loss=85.0323
	step [201/249], loss=82.1689
	step [202/249], loss=82.3412
	step [203/249], loss=63.2548
	step [204/249], loss=105.8982
	step [205/249], loss=67.1598
	step [206/249], loss=73.9892
	step [207/249], loss=84.5654
	step [208/249], loss=72.2629
	step [209/249], loss=82.8229
	step [210/249], loss=84.7579
	step [211/249], loss=89.1917
	step [212/249], loss=74.7178
	step [213/249], loss=85.7957
	step [214/249], loss=82.9682
	step [215/249], loss=85.8653
	step [216/249], loss=82.7268
	step [217/249], loss=76.9905
	step [218/249], loss=83.7401
	step [219/249], loss=87.9006
	step [220/249], loss=83.7311
	step [221/249], loss=92.8118
	step [222/249], loss=86.0884
	step [223/249], loss=84.3801
	step [224/249], loss=68.1425
	step [225/249], loss=76.5568
	step [226/249], loss=90.2640
	step [227/249], loss=69.0589
	step [228/249], loss=80.1265
	step [229/249], loss=90.4919
	step [230/249], loss=91.8713
	step [231/249], loss=84.5617
	step [232/249], loss=77.9039
	step [233/249], loss=86.6238
	step [234/249], loss=93.4975
	step [235/249], loss=81.6973
	step [236/249], loss=91.6067
	step [237/249], loss=83.5340
	step [238/249], loss=79.6376
	step [239/249], loss=72.9584
	step [240/249], loss=99.7386
	step [241/249], loss=81.4012
	step [242/249], loss=98.2930
	step [243/249], loss=73.9674
	step [244/249], loss=76.2768
	step [245/249], loss=75.3069
	step [246/249], loss=92.7516
	step [247/249], loss=73.5278
	step [248/249], loss=78.6262
	step [249/249], loss=57.6411
	Evaluating
	loss=0.0074, precision=0.3298, recall=0.8596, f1=0.4767
Training epoch 67
	step [1/249], loss=76.8889
	step [2/249], loss=77.9768
	step [3/249], loss=78.3101
	step [4/249], loss=82.9305
	step [5/249], loss=68.9440
	step [6/249], loss=63.9501
	step [7/249], loss=84.3264
	step [8/249], loss=72.0655
	step [9/249], loss=90.4922
	step [10/249], loss=84.5393
	step [11/249], loss=86.1811
	step [12/249], loss=71.3009
	step [13/249], loss=87.7187
	step [14/249], loss=91.3827
	step [15/249], loss=87.4768
	step [16/249], loss=72.1677
	step [17/249], loss=77.4132
	step [18/249], loss=78.7989
	step [19/249], loss=90.1514
	step [20/249], loss=97.7564
	step [21/249], loss=83.7220
	step [22/249], loss=80.3328
	step [23/249], loss=84.0666
	step [24/249], loss=65.2658
	step [25/249], loss=75.1305
	step [26/249], loss=64.7017
	step [27/249], loss=86.0779
	step [28/249], loss=61.6789
	step [29/249], loss=80.9611
	step [30/249], loss=64.7341
	step [31/249], loss=79.9981
	step [32/249], loss=93.9046
	step [33/249], loss=72.9433
	step [34/249], loss=97.2785
	step [35/249], loss=67.6366
	step [36/249], loss=87.9034
	step [37/249], loss=88.9980
	step [38/249], loss=72.2358
	step [39/249], loss=73.9142
	step [40/249], loss=78.0544
	step [41/249], loss=85.8646
	step [42/249], loss=76.7825
	step [43/249], loss=95.3919
	step [44/249], loss=77.6978
	step [45/249], loss=63.1760
	step [46/249], loss=104.7674
	step [47/249], loss=87.4009
	step [48/249], loss=82.7558
	step [49/249], loss=73.7710
	step [50/249], loss=86.6098
	step [51/249], loss=83.7566
	step [52/249], loss=82.7797
	step [53/249], loss=67.2271
	step [54/249], loss=82.8178
	step [55/249], loss=79.1001
	step [56/249], loss=91.6750
	step [57/249], loss=72.8966
	step [58/249], loss=97.7717
	step [59/249], loss=99.9644
	step [60/249], loss=66.8021
	step [61/249], loss=80.4797
	step [62/249], loss=86.2663
	step [63/249], loss=83.7358
	step [64/249], loss=69.1033
	step [65/249], loss=85.6405
	step [66/249], loss=75.2221
	step [67/249], loss=103.7961
	step [68/249], loss=78.1730
	step [69/249], loss=70.4273
	step [70/249], loss=106.7404
	step [71/249], loss=98.9364
	step [72/249], loss=86.9250
	step [73/249], loss=94.0562
	step [74/249], loss=78.2059
	step [75/249], loss=96.9196
	step [76/249], loss=90.4662
	step [77/249], loss=87.3389
	step [78/249], loss=83.7114
	step [79/249], loss=91.3078
	step [80/249], loss=78.8928
	step [81/249], loss=80.1217
	step [82/249], loss=72.7345
	step [83/249], loss=75.4116
	step [84/249], loss=73.9911
	step [85/249], loss=65.2403
	step [86/249], loss=83.1589
	step [87/249], loss=72.2971
	step [88/249], loss=72.5589
	step [89/249], loss=78.6247
	step [90/249], loss=74.6325
	step [91/249], loss=93.3055
	step [92/249], loss=85.4982
	step [93/249], loss=60.1425
	step [94/249], loss=81.3411
	step [95/249], loss=69.4332
	step [96/249], loss=64.6417
	step [97/249], loss=75.0218
	step [98/249], loss=71.0992
	step [99/249], loss=75.2410
	step [100/249], loss=76.7964
	step [101/249], loss=75.1752
	step [102/249], loss=68.5056
	step [103/249], loss=83.5232
	step [104/249], loss=80.7074
	step [105/249], loss=80.6398
	step [106/249], loss=62.7280
	step [107/249], loss=85.6604
	step [108/249], loss=88.3848
	step [109/249], loss=89.9925
	step [110/249], loss=80.3916
	step [111/249], loss=65.5140
	step [112/249], loss=79.9018
	step [113/249], loss=85.2337
	step [114/249], loss=71.1465
	step [115/249], loss=77.6235
	step [116/249], loss=93.0031
	step [117/249], loss=78.3503
	step [118/249], loss=78.5098
	step [119/249], loss=85.5505
	step [120/249], loss=93.8331
	step [121/249], loss=77.5803
	step [122/249], loss=76.2929
	step [123/249], loss=75.9404
	step [124/249], loss=71.2733
	step [125/249], loss=85.4335
	step [126/249], loss=82.3842
	step [127/249], loss=79.9057
	step [128/249], loss=67.7966
	step [129/249], loss=84.8015
	step [130/249], loss=81.3497
	step [131/249], loss=75.9645
	step [132/249], loss=97.9399
	step [133/249], loss=81.4502
	step [134/249], loss=83.5084
	step [135/249], loss=108.7224
	step [136/249], loss=76.4356
	step [137/249], loss=72.9858
	step [138/249], loss=80.9763
	step [139/249], loss=85.5081
	step [140/249], loss=73.8592
	step [141/249], loss=93.1554
	step [142/249], loss=83.1373
	step [143/249], loss=68.7694
	step [144/249], loss=75.3670
	step [145/249], loss=101.5856
	step [146/249], loss=93.7997
	step [147/249], loss=83.7587
	step [148/249], loss=73.3496
	step [149/249], loss=61.8728
	step [150/249], loss=72.2104
	step [151/249], loss=78.8940
	step [152/249], loss=79.8901
	step [153/249], loss=63.0205
	step [154/249], loss=65.2278
	step [155/249], loss=81.7735
	step [156/249], loss=87.5790
	step [157/249], loss=80.5264
	step [158/249], loss=76.2388
	step [159/249], loss=72.7973
	step [160/249], loss=80.6943
	step [161/249], loss=80.7949
	step [162/249], loss=77.3124
	step [163/249], loss=81.8733
	step [164/249], loss=87.0858
	step [165/249], loss=72.5290
	step [166/249], loss=95.2489
	step [167/249], loss=92.0405
	step [168/249], loss=75.6230
	step [169/249], loss=89.9978
	step [170/249], loss=72.2313
	step [171/249], loss=83.2799
	step [172/249], loss=69.4732
	step [173/249], loss=68.2872
	step [174/249], loss=71.5555
	step [175/249], loss=95.7499
	step [176/249], loss=76.1857
	step [177/249], loss=70.7509
	step [178/249], loss=70.7750
	step [179/249], loss=90.6718
	step [180/249], loss=87.0738
	step [181/249], loss=59.2716
	step [182/249], loss=86.7704
	step [183/249], loss=70.3094
	step [184/249], loss=85.7881
	step [185/249], loss=85.4902
	step [186/249], loss=79.2099
	step [187/249], loss=79.7481
	step [188/249], loss=105.7810
	step [189/249], loss=82.0492
	step [190/249], loss=85.8111
	step [191/249], loss=93.6493
	step [192/249], loss=82.1122
	step [193/249], loss=86.4094
	step [194/249], loss=90.8643
	step [195/249], loss=74.7531
	step [196/249], loss=96.3164
	step [197/249], loss=100.9415
	step [198/249], loss=58.9304
	step [199/249], loss=77.4778
	step [200/249], loss=74.0679
	step [201/249], loss=75.2787
	step [202/249], loss=76.9409
	step [203/249], loss=91.4613
	step [204/249], loss=79.1556
	step [205/249], loss=75.6890
	step [206/249], loss=93.9348
	step [207/249], loss=81.3464
	step [208/249], loss=84.4700
	step [209/249], loss=81.3498
	step [210/249], loss=82.3610
	step [211/249], loss=101.3036
	step [212/249], loss=92.3542
	step [213/249], loss=79.9946
	step [214/249], loss=68.4238
	step [215/249], loss=75.6545
	step [216/249], loss=82.2069
	step [217/249], loss=73.9402
	step [218/249], loss=75.9106
	step [219/249], loss=66.4731
	step [220/249], loss=83.4976
	step [221/249], loss=83.4373
	step [222/249], loss=98.9301
	step [223/249], loss=88.4202
	step [224/249], loss=83.7605
	step [225/249], loss=66.6530
	step [226/249], loss=66.2638
	step [227/249], loss=71.1782
	step [228/249], loss=83.9553
	step [229/249], loss=81.9678
	step [230/249], loss=77.3742
	step [231/249], loss=93.7134
	step [232/249], loss=75.7085
	step [233/249], loss=68.0682
	step [234/249], loss=74.1173
	step [235/249], loss=81.9629
	step [236/249], loss=83.6841
	step [237/249], loss=88.6405
	step [238/249], loss=80.0747
	step [239/249], loss=72.7433
	step [240/249], loss=98.7514
	step [241/249], loss=93.7519
	step [242/249], loss=57.2333
	step [243/249], loss=82.0312
	step [244/249], loss=63.7180
	step [245/249], loss=79.9167
	step [246/249], loss=70.6585
	step [247/249], loss=84.8147
	step [248/249], loss=68.9733
	step [249/249], loss=64.7768
	Evaluating
	loss=0.0062, precision=0.3820, recall=0.8379, f1=0.5248
saving model as: 3_saved_model.pth
Training epoch 68
	step [1/249], loss=82.2131
	step [2/249], loss=78.0676
	step [3/249], loss=65.4943
	step [4/249], loss=71.6454
	step [5/249], loss=63.6963
	step [6/249], loss=70.8601
	step [7/249], loss=67.0886
	step [8/249], loss=59.1125
	step [9/249], loss=101.8205
	step [10/249], loss=65.5401
	step [11/249], loss=78.8194
	step [12/249], loss=79.3797
	step [13/249], loss=85.2903
	step [14/249], loss=77.7207
	step [15/249], loss=87.9789
	step [16/249], loss=77.5407
	step [17/249], loss=85.9737
	step [18/249], loss=56.1057
	step [19/249], loss=70.1872
	step [20/249], loss=87.7828
	step [21/249], loss=84.0690
	step [22/249], loss=72.6036
	step [23/249], loss=78.6877
	step [24/249], loss=74.2526
	step [25/249], loss=72.2717
	step [26/249], loss=93.9421
	step [27/249], loss=71.7060
	step [28/249], loss=79.8302
	step [29/249], loss=70.7320
	step [30/249], loss=75.8468
	step [31/249], loss=79.5172
	step [32/249], loss=66.6401
	step [33/249], loss=80.4966
	step [34/249], loss=72.5483
	step [35/249], loss=82.4190
	step [36/249], loss=74.6942
	step [37/249], loss=73.1169
	step [38/249], loss=59.7020
	step [39/249], loss=65.8003
	step [40/249], loss=74.4487
	step [41/249], loss=68.8319
	step [42/249], loss=70.7985
	step [43/249], loss=73.6102
	step [44/249], loss=84.5806
	step [45/249], loss=71.1015
	step [46/249], loss=78.1420
	step [47/249], loss=100.1336
	step [48/249], loss=80.6724
	step [49/249], loss=90.3204
	step [50/249], loss=92.8730
	step [51/249], loss=79.0508
	step [52/249], loss=90.8150
	step [53/249], loss=67.8221
	step [54/249], loss=86.5794
	step [55/249], loss=90.8598
	step [56/249], loss=89.4157
	step [57/249], loss=91.9304
	step [58/249], loss=79.3365
	step [59/249], loss=95.8984
	step [60/249], loss=74.5185
	step [61/249], loss=72.4207
	step [62/249], loss=86.6642
	step [63/249], loss=86.7331
	step [64/249], loss=65.7672
	step [65/249], loss=88.8091
	step [66/249], loss=76.4132
	step [67/249], loss=81.4247
	step [68/249], loss=70.4792
	step [69/249], loss=75.1684
	step [70/249], loss=81.9677
	step [71/249], loss=71.9725
	step [72/249], loss=74.5411
	step [73/249], loss=62.7177
	step [74/249], loss=78.4867
	step [75/249], loss=72.6380
	step [76/249], loss=76.9136
	step [77/249], loss=82.9087
	step [78/249], loss=80.6965
	step [79/249], loss=64.3543
	step [80/249], loss=85.9875
	step [81/249], loss=79.9790
	step [82/249], loss=82.1104
	step [83/249], loss=61.1823
	step [84/249], loss=81.8806
	step [85/249], loss=84.2172
	step [86/249], loss=76.8102
	step [87/249], loss=97.7966
	step [88/249], loss=70.0188
	step [89/249], loss=69.1169
	step [90/249], loss=88.8598
	step [91/249], loss=69.0397
	step [92/249], loss=67.3694
	step [93/249], loss=79.8986
	step [94/249], loss=87.4794
	step [95/249], loss=84.8890
	step [96/249], loss=82.2927
	step [97/249], loss=80.6042
	step [98/249], loss=82.0563
	step [99/249], loss=90.9343
	step [100/249], loss=82.7713
	step [101/249], loss=77.8317
	step [102/249], loss=65.9902
	step [103/249], loss=85.2114
	step [104/249], loss=75.8319
	step [105/249], loss=74.8322
	step [106/249], loss=81.8794
	step [107/249], loss=83.8680
	step [108/249], loss=77.2272
	step [109/249], loss=76.6973
	step [110/249], loss=86.1939
	step [111/249], loss=72.9541
	step [112/249], loss=92.0727
	step [113/249], loss=83.2761
	step [114/249], loss=92.1504
	step [115/249], loss=53.8189
	step [116/249], loss=83.2187
	step [117/249], loss=93.4224
	step [118/249], loss=93.6815
	step [119/249], loss=84.6216
	step [120/249], loss=87.1360
	step [121/249], loss=83.0660
	step [122/249], loss=77.2885
	step [123/249], loss=82.2405
	step [124/249], loss=68.5667
	step [125/249], loss=78.9739
	step [126/249], loss=77.3931
	step [127/249], loss=83.5827
	step [128/249], loss=74.2299
	step [129/249], loss=87.3000
	step [130/249], loss=87.5140
	step [131/249], loss=96.3764
	step [132/249], loss=89.7066
	step [133/249], loss=77.7650
	step [134/249], loss=76.2541
	step [135/249], loss=78.4067
	step [136/249], loss=96.2074
	step [137/249], loss=100.7067
	step [138/249], loss=78.8096
	step [139/249], loss=98.7879
	step [140/249], loss=79.0932
	step [141/249], loss=93.0031
	step [142/249], loss=72.0057
	step [143/249], loss=78.2691
	step [144/249], loss=95.9951
	step [145/249], loss=76.0283
	step [146/249], loss=83.2289
	step [147/249], loss=84.3094
	step [148/249], loss=78.1744
	step [149/249], loss=83.3764
	step [150/249], loss=86.7126
	step [151/249], loss=70.4338
	step [152/249], loss=93.9436
	step [153/249], loss=64.9772
	step [154/249], loss=73.3999
	step [155/249], loss=69.3825
	step [156/249], loss=83.3047
	step [157/249], loss=111.0404
	step [158/249], loss=95.4082
	step [159/249], loss=90.1152
	step [160/249], loss=74.4170
	step [161/249], loss=85.5566
	step [162/249], loss=93.4020
	step [163/249], loss=76.6715
	step [164/249], loss=89.9014
	step [165/249], loss=78.2930
	step [166/249], loss=82.2267
	step [167/249], loss=89.2424
	step [168/249], loss=71.6811
	step [169/249], loss=88.1026
	step [170/249], loss=96.4965
	step [171/249], loss=87.7016
	step [172/249], loss=79.2163
	step [173/249], loss=75.6697
	step [174/249], loss=100.0046
	step [175/249], loss=76.4552
	step [176/249], loss=86.7236
	step [177/249], loss=71.0587
	step [178/249], loss=99.6818
	step [179/249], loss=89.3469
	step [180/249], loss=68.7457
	step [181/249], loss=80.2995
	step [182/249], loss=78.2607
	step [183/249], loss=77.3388
	step [184/249], loss=75.3163
	step [185/249], loss=83.9350
	step [186/249], loss=75.4570
	step [187/249], loss=81.6734
	step [188/249], loss=94.6929
	step [189/249], loss=107.1273
	step [190/249], loss=85.8752
	step [191/249], loss=79.3357
	step [192/249], loss=80.9704
	step [193/249], loss=65.3623
	step [194/249], loss=76.5224
	step [195/249], loss=71.2024
	step [196/249], loss=84.9900
	step [197/249], loss=88.4153
	step [198/249], loss=78.9014
	step [199/249], loss=76.1023
	step [200/249], loss=75.0158
	step [201/249], loss=77.1196
	step [202/249], loss=65.1722
	step [203/249], loss=85.3601
	step [204/249], loss=76.7029
	step [205/249], loss=86.3522
	step [206/249], loss=101.7536
	step [207/249], loss=74.0960
	step [208/249], loss=76.4110
	step [209/249], loss=71.3713
	step [210/249], loss=91.3586
	step [211/249], loss=80.3368
	step [212/249], loss=81.4181
	step [213/249], loss=67.2415
	step [214/249], loss=99.7530
	step [215/249], loss=74.2356
	step [216/249], loss=78.6846
	step [217/249], loss=77.3229
	step [218/249], loss=79.6031
	step [219/249], loss=81.9940
	step [220/249], loss=71.0703
	step [221/249], loss=68.2427
	step [222/249], loss=84.4008
	step [223/249], loss=61.5874
	step [224/249], loss=82.5963
	step [225/249], loss=65.8697
	step [226/249], loss=77.4902
	step [227/249], loss=67.0090
	step [228/249], loss=82.9689
	step [229/249], loss=81.7296
	step [230/249], loss=74.3729
	step [231/249], loss=81.2527
	step [232/249], loss=78.0251
	step [233/249], loss=86.2117
	step [234/249], loss=82.2398
	step [235/249], loss=108.6976
	step [236/249], loss=86.3633
	step [237/249], loss=65.0718
	step [238/249], loss=78.8629
	step [239/249], loss=74.4720
	step [240/249], loss=78.1619
	step [241/249], loss=87.3743
	step [242/249], loss=78.4271
	step [243/249], loss=85.4886
	step [244/249], loss=86.0052
	step [245/249], loss=87.8462
	step [246/249], loss=88.1802
	step [247/249], loss=63.0080
	step [248/249], loss=59.4683
	step [249/249], loss=51.5383
	Evaluating
	loss=0.0068, precision=0.3567, recall=0.8490, f1=0.5024
Training epoch 69
	step [1/249], loss=79.5569
	step [2/249], loss=91.4003
	step [3/249], loss=82.1557
	step [4/249], loss=75.0456
	step [5/249], loss=73.8496
	step [6/249], loss=73.3401
	step [7/249], loss=68.7534
	step [8/249], loss=94.8455
	step [9/249], loss=78.0971
	step [10/249], loss=91.6330
	step [11/249], loss=65.6352
	step [12/249], loss=78.6441
	step [13/249], loss=74.8571
	step [14/249], loss=99.9594
	step [15/249], loss=73.0450
	step [16/249], loss=58.0365
	step [17/249], loss=67.5881
	step [18/249], loss=73.1699
	step [19/249], loss=96.2593
	step [20/249], loss=93.8169
	step [21/249], loss=86.8654
	step [22/249], loss=73.8471
	step [23/249], loss=89.5743
	step [24/249], loss=81.4506
	step [25/249], loss=66.1747
	step [26/249], loss=97.2030
	step [27/249], loss=93.5295
	step [28/249], loss=72.3958
	step [29/249], loss=65.4158
	step [30/249], loss=70.2995
	step [31/249], loss=75.1622
	step [32/249], loss=86.8642
	step [33/249], loss=93.6878
	step [34/249], loss=74.6366
	step [35/249], loss=82.3851
	step [36/249], loss=72.2825
	step [37/249], loss=67.0894
	step [38/249], loss=90.0834
	step [39/249], loss=75.2593
	step [40/249], loss=72.5214
	step [41/249], loss=85.8306
	step [42/249], loss=73.3170
	step [43/249], loss=62.7368
	step [44/249], loss=86.5225
	step [45/249], loss=72.8007
	step [46/249], loss=66.6778
	step [47/249], loss=72.7298
	step [48/249], loss=100.4791
	step [49/249], loss=64.8317
	step [50/249], loss=69.2345
	step [51/249], loss=79.2817
	step [52/249], loss=73.9761
	step [53/249], loss=62.7538
	step [54/249], loss=58.7167
	step [55/249], loss=86.2526
	step [56/249], loss=87.0264
	step [57/249], loss=87.3084
	step [58/249], loss=74.0912
	step [59/249], loss=68.7742
	step [60/249], loss=74.5986
	step [61/249], loss=79.3662
	step [62/249], loss=74.9824
	step [63/249], loss=82.5645
	step [64/249], loss=73.5155
	step [65/249], loss=89.3046
	step [66/249], loss=84.7443
	step [67/249], loss=79.3047
	step [68/249], loss=92.9152
	step [69/249], loss=79.3772
	step [70/249], loss=82.6494
	step [71/249], loss=79.5903
	step [72/249], loss=81.2165
	step [73/249], loss=94.6398
	step [74/249], loss=75.2811
	step [75/249], loss=87.6678
	step [76/249], loss=90.8292
	step [77/249], loss=72.2731
	step [78/249], loss=85.1911
	step [79/249], loss=89.2496
	step [80/249], loss=63.9154
	step [81/249], loss=85.9310
	step [82/249], loss=83.4694
	step [83/249], loss=71.7920
	step [84/249], loss=91.5027
	step [85/249], loss=83.4859
	step [86/249], loss=73.1374
	step [87/249], loss=73.1212
	step [88/249], loss=88.3673
	step [89/249], loss=78.0434
	step [90/249], loss=109.0003
	step [91/249], loss=81.0848
	step [92/249], loss=87.1903
	step [93/249], loss=66.5973
	step [94/249], loss=87.6095
	step [95/249], loss=99.7110
	step [96/249], loss=78.8002
	step [97/249], loss=82.5796
	step [98/249], loss=98.4795
	step [99/249], loss=82.7482
	step [100/249], loss=87.0476
	step [101/249], loss=94.4911
	step [102/249], loss=75.3555
	step [103/249], loss=72.2845
	step [104/249], loss=77.9617
	step [105/249], loss=94.3885
	step [106/249], loss=73.7332
	step [107/249], loss=91.9487
	step [108/249], loss=76.8961
	step [109/249], loss=68.2565
	step [110/249], loss=74.7420
	step [111/249], loss=92.3296
	step [112/249], loss=77.2582
	step [113/249], loss=78.7568
	step [114/249], loss=73.3819
	step [115/249], loss=71.8918
	step [116/249], loss=84.6125
	step [117/249], loss=91.2524
	step [118/249], loss=85.9328
	step [119/249], loss=80.8309
	step [120/249], loss=86.3306
	step [121/249], loss=80.4505
	step [122/249], loss=64.3157
	step [123/249], loss=77.2225
	step [124/249], loss=94.0358
	step [125/249], loss=82.0684
	step [126/249], loss=75.2854
	step [127/249], loss=69.5017
	step [128/249], loss=75.7695
	step [129/249], loss=72.2177
	step [130/249], loss=84.3243
	step [131/249], loss=79.5978
	step [132/249], loss=83.4364
	step [133/249], loss=77.9668
	step [134/249], loss=83.2142
	step [135/249], loss=67.2746
	step [136/249], loss=65.1606
	step [137/249], loss=74.8019
	step [138/249], loss=83.9550
	step [139/249], loss=86.8181
	step [140/249], loss=70.8561
	step [141/249], loss=70.9363
	step [142/249], loss=64.5543
	step [143/249], loss=70.6175
	step [144/249], loss=92.6505
	step [145/249], loss=80.3708
	step [146/249], loss=79.8492
	step [147/249], loss=86.1935
	step [148/249], loss=74.5834
	step [149/249], loss=72.2991
	step [150/249], loss=79.4946
	step [151/249], loss=86.8461
	step [152/249], loss=81.1441
	step [153/249], loss=70.5806
	step [154/249], loss=73.7600
	step [155/249], loss=74.3157
	step [156/249], loss=100.1615
	step [157/249], loss=96.3979
	step [158/249], loss=78.8745
	step [159/249], loss=84.2379
	step [160/249], loss=94.0964
	step [161/249], loss=81.8877
	step [162/249], loss=89.6991
	step [163/249], loss=81.2234
	step [164/249], loss=77.8814
	step [165/249], loss=93.6030
	step [166/249], loss=70.2349
	step [167/249], loss=66.1563
	step [168/249], loss=73.6588
	step [169/249], loss=80.6174
	step [170/249], loss=83.3219
	step [171/249], loss=76.4810
	step [172/249], loss=84.8831
	step [173/249], loss=75.6105
	step [174/249], loss=70.5499
	step [175/249], loss=83.3249
	step [176/249], loss=78.3620
	step [177/249], loss=89.8987
	step [178/249], loss=85.9994
	step [179/249], loss=75.2433
	step [180/249], loss=86.7442
	step [181/249], loss=70.8266
	step [182/249], loss=97.1491
	step [183/249], loss=87.0673
	step [184/249], loss=75.6053
	step [185/249], loss=70.0913
	step [186/249], loss=96.5149
	step [187/249], loss=69.2921
	step [188/249], loss=71.1333
	step [189/249], loss=70.4662
	step [190/249], loss=68.0512
	step [191/249], loss=87.6441
	step [192/249], loss=85.1833
	step [193/249], loss=75.6505
	step [194/249], loss=90.9618
	step [195/249], loss=82.8367
	step [196/249], loss=63.1819
	step [197/249], loss=82.7969
	step [198/249], loss=66.5636
	step [199/249], loss=90.6417
	step [200/249], loss=80.0742
	step [201/249], loss=76.2551
	step [202/249], loss=73.2848
	step [203/249], loss=67.8928
	step [204/249], loss=91.9522
	step [205/249], loss=77.8243
	step [206/249], loss=67.8585
	step [207/249], loss=80.9164
	step [208/249], loss=83.1575
	step [209/249], loss=82.2630
	step [210/249], loss=69.2592
	step [211/249], loss=80.3284
	step [212/249], loss=72.1840
	step [213/249], loss=83.7690
	step [214/249], loss=66.8597
	step [215/249], loss=82.4795
	step [216/249], loss=72.5150
	step [217/249], loss=94.2115
	step [218/249], loss=87.7695
	step [219/249], loss=77.6692
	step [220/249], loss=73.9391
	step [221/249], loss=91.4402
	step [222/249], loss=74.2113
	step [223/249], loss=67.0952
	step [224/249], loss=82.2578
	step [225/249], loss=64.5640
	step [226/249], loss=74.5506
	step [227/249], loss=79.9950
	step [228/249], loss=59.1377
	step [229/249], loss=77.4958
	step [230/249], loss=79.1259
	step [231/249], loss=85.4210
	step [232/249], loss=92.5135
	step [233/249], loss=63.7702
	step [234/249], loss=79.4123
	step [235/249], loss=69.1541
	step [236/249], loss=96.1920
	step [237/249], loss=68.2545
	step [238/249], loss=84.9275
	step [239/249], loss=96.8136
	step [240/249], loss=90.8159
	step [241/249], loss=78.3503
	step [242/249], loss=86.7801
	step [243/249], loss=88.5310
	step [244/249], loss=75.4693
	step [245/249], loss=92.5803
	step [246/249], loss=75.0886
	step [247/249], loss=88.7341
	step [248/249], loss=67.1861
	step [249/249], loss=57.0315
	Evaluating
	loss=0.0062, precision=0.3814, recall=0.8528, f1=0.5270
saving model as: 3_saved_model.pth
Training epoch 70
	step [1/249], loss=76.3150
	step [2/249], loss=67.5814
	step [3/249], loss=94.0965
	step [4/249], loss=99.4165
	step [5/249], loss=85.1884
	step [6/249], loss=93.0476
	step [7/249], loss=70.3020
	step [8/249], loss=72.9606
	step [9/249], loss=92.2625
	step [10/249], loss=84.4240
	step [11/249], loss=64.3261
	step [12/249], loss=74.3900
	step [13/249], loss=76.7008
	step [14/249], loss=78.7927
	step [15/249], loss=85.3392
	step [16/249], loss=76.8352
	step [17/249], loss=88.5423
	step [18/249], loss=85.2350
	step [19/249], loss=79.5163
	step [20/249], loss=74.1959
	step [21/249], loss=87.5164
	step [22/249], loss=77.7141
	step [23/249], loss=78.0360
	step [24/249], loss=100.3158
	step [25/249], loss=81.3237
	step [26/249], loss=70.3588
	step [27/249], loss=83.7947
	step [28/249], loss=80.2113
	step [29/249], loss=80.4841
	step [30/249], loss=73.7780
	step [31/249], loss=95.6959
	step [32/249], loss=66.8584
	step [33/249], loss=70.3264
	step [34/249], loss=69.7312
	step [35/249], loss=72.8561
	step [36/249], loss=71.9232
	step [37/249], loss=77.2503
	step [38/249], loss=91.6908
	step [39/249], loss=77.5401
	step [40/249], loss=79.0283
	step [41/249], loss=83.3605
	step [42/249], loss=72.1509
	step [43/249], loss=64.3608
	step [44/249], loss=82.6229
	step [45/249], loss=75.6720
	step [46/249], loss=82.4788
	step [47/249], loss=65.3366
	step [48/249], loss=82.3199
	step [49/249], loss=88.4622
	step [50/249], loss=84.7784
	step [51/249], loss=88.9207
	step [52/249], loss=78.1935
	step [53/249], loss=86.2201
	step [54/249], loss=78.7876
	step [55/249], loss=79.7359
	step [56/249], loss=99.9802
	step [57/249], loss=97.4640
	step [58/249], loss=76.2257
	step [59/249], loss=72.0026
	step [60/249], loss=86.9853
	step [61/249], loss=92.4452
	step [62/249], loss=81.2481
	step [63/249], loss=66.2260
	step [64/249], loss=61.6014
	step [65/249], loss=58.5128
	step [66/249], loss=88.5931
	step [67/249], loss=65.5284
	step [68/249], loss=72.6329
	step [69/249], loss=80.8274
	step [70/249], loss=80.4021
	step [71/249], loss=89.4114
	step [72/249], loss=88.1453
	step [73/249], loss=79.0144
	step [74/249], loss=94.6356
	step [75/249], loss=88.1433
	step [76/249], loss=66.2115
	step [77/249], loss=74.3793
	step [78/249], loss=76.9194
	step [79/249], loss=105.1776
	step [80/249], loss=67.7203
	step [81/249], loss=76.3176
	step [82/249], loss=82.9933
	step [83/249], loss=77.5977
	step [84/249], loss=97.8675
	step [85/249], loss=89.5035
	step [86/249], loss=87.8688
	step [87/249], loss=64.4841
	step [88/249], loss=78.1239
	step [89/249], loss=63.6845
	step [90/249], loss=74.8011
	step [91/249], loss=100.2758
	step [92/249], loss=74.8393
	step [93/249], loss=75.0854
	step [94/249], loss=75.4681
	step [95/249], loss=81.3678
	step [96/249], loss=86.1185
	step [97/249], loss=72.3941
	step [98/249], loss=81.8259
	step [99/249], loss=65.6329
	step [100/249], loss=83.2717
	step [101/249], loss=81.1941
	step [102/249], loss=74.8672
	step [103/249], loss=71.7579
	step [104/249], loss=73.8523
	step [105/249], loss=79.4793
	step [106/249], loss=79.1804
	step [107/249], loss=92.4339
	step [108/249], loss=82.0611
	step [109/249], loss=84.4599
	step [110/249], loss=78.2883
	step [111/249], loss=68.2574
	step [112/249], loss=82.0552
	step [113/249], loss=83.2718
	step [114/249], loss=87.8014
	step [115/249], loss=85.6309
	step [116/249], loss=88.5621
	step [117/249], loss=78.7859
	step [118/249], loss=79.5164
	step [119/249], loss=80.3454
	step [120/249], loss=65.8045
	step [121/249], loss=69.5333
	step [122/249], loss=72.8013
	step [123/249], loss=78.1014
	step [124/249], loss=77.1797
	step [125/249], loss=75.0302
	step [126/249], loss=73.5046
	step [127/249], loss=64.2770
	step [128/249], loss=75.6648
	step [129/249], loss=70.5765
	step [130/249], loss=82.2636
	step [131/249], loss=75.5226
	step [132/249], loss=74.8836
	step [133/249], loss=88.8448
	step [134/249], loss=77.0635
	step [135/249], loss=65.8242
	step [136/249], loss=83.9623
	step [137/249], loss=78.7371
	step [138/249], loss=66.9539
	step [139/249], loss=85.1877
	step [140/249], loss=79.7122
	step [141/249], loss=75.0092
	step [142/249], loss=85.9549
	step [143/249], loss=77.1850
	step [144/249], loss=85.8829
	step [145/249], loss=77.6026
	step [146/249], loss=71.5041
	step [147/249], loss=83.4538
	step [148/249], loss=87.9522
	step [149/249], loss=80.0904
	step [150/249], loss=90.2376
	step [151/249], loss=71.6208
	step [152/249], loss=87.6210
	step [153/249], loss=108.3635
	step [154/249], loss=77.3898
	step [155/249], loss=70.7533
	step [156/249], loss=66.7859
	step [157/249], loss=70.4517
	step [158/249], loss=90.7611
	step [159/249], loss=65.0232
	step [160/249], loss=81.0377
	step [161/249], loss=76.0983
	step [162/249], loss=77.5082
	step [163/249], loss=95.0379
	step [164/249], loss=71.6190
	step [165/249], loss=76.6741
	step [166/249], loss=80.2376
	step [167/249], loss=81.8113
	step [168/249], loss=78.5485
	step [169/249], loss=63.1947
	step [170/249], loss=81.4671
	step [171/249], loss=62.7739
	step [172/249], loss=70.9215
	step [173/249], loss=68.8720
	step [174/249], loss=71.5747
	step [175/249], loss=76.6127
	step [176/249], loss=73.9336
	step [177/249], loss=70.1480
	step [178/249], loss=72.4022
	step [179/249], loss=76.9952
	step [180/249], loss=76.2530
	step [181/249], loss=90.8188
	step [182/249], loss=70.3822
	step [183/249], loss=76.7754
	step [184/249], loss=93.2508
	step [185/249], loss=66.8510
	step [186/249], loss=65.8384
	step [187/249], loss=74.2003
	step [188/249], loss=80.4823
	step [189/249], loss=66.6753
	step [190/249], loss=75.9380
	step [191/249], loss=96.4884
	step [192/249], loss=96.8301
	step [193/249], loss=75.9591
	step [194/249], loss=72.1526
	step [195/249], loss=87.7834
	step [196/249], loss=74.6157
	step [197/249], loss=75.5339
	step [198/249], loss=79.7645
	step [199/249], loss=71.9580
	step [200/249], loss=71.9176
	step [201/249], loss=95.4433
	step [202/249], loss=97.8604
	step [203/249], loss=94.6295
	step [204/249], loss=84.3426
	step [205/249], loss=68.3938
	step [206/249], loss=62.5833
	step [207/249], loss=69.9511
	step [208/249], loss=70.1380
	step [209/249], loss=79.5571
	step [210/249], loss=98.8863
	step [211/249], loss=80.7489
	step [212/249], loss=94.2796
	step [213/249], loss=87.9159
	step [214/249], loss=79.4507
	step [215/249], loss=87.3334
	step [216/249], loss=96.9845
	step [217/249], loss=81.9145
	step [218/249], loss=50.3298
	step [219/249], loss=87.8039
	step [220/249], loss=79.0420
	step [221/249], loss=82.7448
	step [222/249], loss=92.1048
	step [223/249], loss=86.4340
	step [224/249], loss=93.6516
	step [225/249], loss=91.2713
	step [226/249], loss=67.9795
	step [227/249], loss=80.4284
	step [228/249], loss=93.8913
	step [229/249], loss=80.1879
	step [230/249], loss=78.0271
	step [231/249], loss=77.7098
	step [232/249], loss=96.8131
	step [233/249], loss=64.9908
	step [234/249], loss=77.8452
	step [235/249], loss=83.0240
	step [236/249], loss=65.9736
	step [237/249], loss=72.9776
	step [238/249], loss=84.5909
	step [239/249], loss=75.4624
	step [240/249], loss=83.8932
	step [241/249], loss=76.5080
	step [242/249], loss=89.1742
	step [243/249], loss=84.9081
	step [244/249], loss=87.4435
	step [245/249], loss=87.6187
	step [246/249], loss=81.2718
	step [247/249], loss=68.8077
	step [248/249], loss=86.9084
	step [249/249], loss=48.0901
	Evaluating
	loss=0.0082, precision=0.3015, recall=0.8642, f1=0.4470
Training epoch 71
	step [1/249], loss=73.1150
	step [2/249], loss=96.9550
	step [3/249], loss=90.8397
	step [4/249], loss=82.8400
	step [5/249], loss=62.6032
	step [6/249], loss=71.1790
	step [7/249], loss=76.1661
	step [8/249], loss=75.7235
	step [9/249], loss=78.2233
	step [10/249], loss=74.3833
	step [11/249], loss=71.6188
	step [12/249], loss=85.8284
	step [13/249], loss=61.0304
	step [14/249], loss=82.4785
	step [15/249], loss=86.7124
	step [16/249], loss=69.3698
	step [17/249], loss=71.5343
	step [18/249], loss=89.5637
	step [19/249], loss=91.3831
	step [20/249], loss=60.9731
	step [21/249], loss=64.0987
	step [22/249], loss=81.1321
	step [23/249], loss=80.8619
	step [24/249], loss=98.8234
	step [25/249], loss=76.0356
	step [26/249], loss=80.5912
	step [27/249], loss=85.3690
	step [28/249], loss=75.7761
	step [29/249], loss=64.4044
	step [30/249], loss=83.4544
	step [31/249], loss=73.8613
	step [32/249], loss=85.7732
	step [33/249], loss=82.0592
	step [34/249], loss=82.7001
	step [35/249], loss=69.3745
	step [36/249], loss=72.7667
	step [37/249], loss=99.5994
	step [38/249], loss=88.1906
	step [39/249], loss=75.8595
	step [40/249], loss=98.9259
	step [41/249], loss=60.0412
	step [42/249], loss=68.8393
	step [43/249], loss=60.9995
	step [44/249], loss=77.7670
	step [45/249], loss=82.0694
	step [46/249], loss=86.2056
	step [47/249], loss=79.7033
	step [48/249], loss=78.6003
	step [49/249], loss=95.6587
	step [50/249], loss=66.2828
	step [51/249], loss=78.7022
	step [52/249], loss=67.6418
	step [53/249], loss=85.4392
	step [54/249], loss=66.9216
	step [55/249], loss=78.6083
	step [56/249], loss=79.8824
	step [57/249], loss=81.4423
	step [58/249], loss=75.3605
	step [59/249], loss=83.0090
	step [60/249], loss=79.5174
	step [61/249], loss=101.2452
	step [62/249], loss=80.2751
	step [63/249], loss=84.6710
	step [64/249], loss=82.5593
	step [65/249], loss=77.4830
	step [66/249], loss=80.3010
	step [67/249], loss=74.1918
	step [68/249], loss=69.3877
	step [69/249], loss=64.5247
	step [70/249], loss=88.5688
	step [71/249], loss=76.7260
	step [72/249], loss=91.1427
	step [73/249], loss=79.4143
	step [74/249], loss=78.1610
	step [75/249], loss=76.8175
	step [76/249], loss=74.5403
	step [77/249], loss=64.8869
	step [78/249], loss=84.2024
	step [79/249], loss=85.4532
	step [80/249], loss=83.0013
	step [81/249], loss=82.5274
	step [82/249], loss=68.9696
	step [83/249], loss=77.0803
	step [84/249], loss=90.6973
	step [85/249], loss=76.2165
	step [86/249], loss=77.3342
	step [87/249], loss=74.9098
	step [88/249], loss=79.5732
	step [89/249], loss=76.5937
	step [90/249], loss=81.3918
	step [91/249], loss=74.8378
	step [92/249], loss=59.8495
	step [93/249], loss=95.6358
	step [94/249], loss=82.2759
	step [95/249], loss=72.6195
	step [96/249], loss=64.9658
	step [97/249], loss=78.7627
	step [98/249], loss=74.5402
	step [99/249], loss=77.6764
	step [100/249], loss=79.5179
	step [101/249], loss=77.6836
	step [102/249], loss=92.0079
	step [103/249], loss=90.3802
	step [104/249], loss=82.2592
	step [105/249], loss=77.5246
	step [106/249], loss=76.5581
	step [107/249], loss=91.6389
	step [108/249], loss=67.7248
	step [109/249], loss=77.2050
	step [110/249], loss=92.4958
	step [111/249], loss=93.3311
	step [112/249], loss=73.5246
	step [113/249], loss=94.9323
	step [114/249], loss=75.0925
	step [115/249], loss=88.2093
	step [116/249], loss=77.2134
	step [117/249], loss=75.2660
	step [118/249], loss=94.1023
	step [119/249], loss=81.6598
	step [120/249], loss=79.0914
	step [121/249], loss=76.8146
	step [122/249], loss=69.8941
	step [123/249], loss=73.9837
	step [124/249], loss=94.5883
	step [125/249], loss=80.7660
	step [126/249], loss=70.9764
	step [127/249], loss=91.9202
	step [128/249], loss=84.5477
	step [129/249], loss=72.4863
	step [130/249], loss=88.6517
	step [131/249], loss=87.4190
	step [132/249], loss=85.8997
	step [133/249], loss=69.1983
	step [134/249], loss=64.2482
	step [135/249], loss=71.6850
	step [136/249], loss=79.7432
	step [137/249], loss=90.0905
	step [138/249], loss=73.3171
	step [139/249], loss=94.5541
	step [140/249], loss=84.5534
	step [141/249], loss=85.6102
	step [142/249], loss=87.8216
	step [143/249], loss=91.7906
	step [144/249], loss=81.6103
	step [145/249], loss=80.9334
	step [146/249], loss=77.9051
	step [147/249], loss=67.7679
	step [148/249], loss=74.0331
	step [149/249], loss=79.8253
	step [150/249], loss=79.5207
	step [151/249], loss=77.7089
	step [152/249], loss=100.4573
	step [153/249], loss=68.2414
	step [154/249], loss=70.0738
	step [155/249], loss=87.6195
	step [156/249], loss=82.7096
	step [157/249], loss=74.9411
	step [158/249], loss=81.2618
	step [159/249], loss=95.7542
	step [160/249], loss=74.1728
	step [161/249], loss=86.3652
	step [162/249], loss=80.9081
	step [163/249], loss=76.5302
	step [164/249], loss=74.3684
	step [165/249], loss=67.0978
	step [166/249], loss=78.2449
	step [167/249], loss=87.7483
	step [168/249], loss=91.8461
	step [169/249], loss=72.2269
	step [170/249], loss=93.8404
	step [171/249], loss=75.4120
	step [172/249], loss=79.6149
	step [173/249], loss=71.2074
	step [174/249], loss=95.8591
	step [175/249], loss=71.3866
	step [176/249], loss=79.4590
	step [177/249], loss=89.3242
	step [178/249], loss=65.6731
	step [179/249], loss=94.2061
	step [180/249], loss=85.9679
	step [181/249], loss=70.0585
	step [182/249], loss=74.8322
	step [183/249], loss=59.1110
	step [184/249], loss=93.8908
	step [185/249], loss=95.4310
	step [186/249], loss=73.6998
	step [187/249], loss=96.6597
	step [188/249], loss=80.0610
	step [189/249], loss=77.1917
	step [190/249], loss=87.9640
	step [191/249], loss=67.8913
	step [192/249], loss=86.8817
	step [193/249], loss=75.1665
	step [194/249], loss=82.8034
	step [195/249], loss=67.7950
	step [196/249], loss=75.4018
	step [197/249], loss=82.5292
	step [198/249], loss=76.1810
	step [199/249], loss=76.6752
	step [200/249], loss=81.1602
	step [201/249], loss=76.5257
	step [202/249], loss=82.4790
	step [203/249], loss=83.8007
	step [204/249], loss=76.7030
	step [205/249], loss=80.1873
	step [206/249], loss=89.5338
	step [207/249], loss=68.8629
	step [208/249], loss=76.7311
	step [209/249], loss=74.4340
	step [210/249], loss=84.5658
	step [211/249], loss=107.0127
	step [212/249], loss=80.5031
	step [213/249], loss=85.7602
	step [214/249], loss=78.5562
	step [215/249], loss=88.2793
	step [216/249], loss=86.1662
	step [217/249], loss=68.6008
	step [218/249], loss=78.9695
	step [219/249], loss=76.0139
	step [220/249], loss=76.7318
	step [221/249], loss=79.7298
	step [222/249], loss=85.4193
	step [223/249], loss=62.1818
	step [224/249], loss=76.3052
	step [225/249], loss=75.8599
	step [226/249], loss=80.6295
	step [227/249], loss=102.3965
	step [228/249], loss=79.2952
	step [229/249], loss=83.1805
	step [230/249], loss=81.4455
	step [231/249], loss=87.7493
	step [232/249], loss=65.5632
	step [233/249], loss=73.7506
	step [234/249], loss=84.3052
	step [235/249], loss=63.4145
	step [236/249], loss=82.1615
	step [237/249], loss=77.5858
	step [238/249], loss=64.7403
	step [239/249], loss=64.9635
	step [240/249], loss=81.6340
	step [241/249], loss=89.4096
	step [242/249], loss=66.0212
	step [243/249], loss=97.9972
	step [244/249], loss=64.8095
	step [245/249], loss=88.8418
	step [246/249], loss=77.8055
	step [247/249], loss=74.3992
	step [248/249], loss=66.2433
	step [249/249], loss=51.4585
	Evaluating
	loss=0.0072, precision=0.3244, recall=0.8421, f1=0.4684
Training epoch 72
	step [1/249], loss=80.8908
	step [2/249], loss=79.6618
	step [3/249], loss=87.8930
	step [4/249], loss=65.9705
	step [5/249], loss=75.9381
	step [6/249], loss=85.3351
	step [7/249], loss=73.4118
	step [8/249], loss=89.7985
	step [9/249], loss=76.9604
	step [10/249], loss=81.2921
	step [11/249], loss=63.0593
	step [12/249], loss=75.5475
	step [13/249], loss=72.3428
	step [14/249], loss=75.1287
	step [15/249], loss=85.8575
	step [16/249], loss=64.6230
	step [17/249], loss=85.0888
	step [18/249], loss=97.4979
	step [19/249], loss=76.3268
	step [20/249], loss=73.4043
	step [21/249], loss=78.0658
	step [22/249], loss=86.0632
	step [23/249], loss=73.7077
	step [24/249], loss=74.9186
	step [25/249], loss=79.5578
	step [26/249], loss=79.5470
	step [27/249], loss=89.0816
	step [28/249], loss=77.4592
	step [29/249], loss=73.9354
	step [30/249], loss=96.6110
	step [31/249], loss=98.0465
	step [32/249], loss=65.4231
	step [33/249], loss=72.4577
	step [34/249], loss=78.2670
	step [35/249], loss=74.4558
	step [36/249], loss=85.2099
	step [37/249], loss=78.3293
	step [38/249], loss=77.7388
	step [39/249], loss=82.3974
	step [40/249], loss=90.4109
	step [41/249], loss=98.6956
	step [42/249], loss=53.6055
	step [43/249], loss=77.5714
	step [44/249], loss=82.8382
	step [45/249], loss=75.9662
	step [46/249], loss=64.7725
	step [47/249], loss=84.5542
	step [48/249], loss=98.4974
	step [49/249], loss=69.5022
	step [50/249], loss=76.0300
	step [51/249], loss=77.2466
	step [52/249], loss=74.9017
	step [53/249], loss=82.6930
	step [54/249], loss=89.8743
	step [55/249], loss=70.3219
	step [56/249], loss=89.7560
	step [57/249], loss=90.8221
	step [58/249], loss=63.0643
	step [59/249], loss=80.2398
	step [60/249], loss=63.5030
	step [61/249], loss=82.5143
	step [62/249], loss=75.7753
	step [63/249], loss=70.1798
	step [64/249], loss=82.1790
	step [65/249], loss=72.7379
	step [66/249], loss=80.7763
	step [67/249], loss=77.7335
	step [68/249], loss=76.1503
	step [69/249], loss=54.2371
	step [70/249], loss=83.6460
	step [71/249], loss=83.9010
	step [72/249], loss=88.9547
	step [73/249], loss=67.0215
	step [74/249], loss=69.1576
	step [75/249], loss=86.6187
	step [76/249], loss=91.5563
	step [77/249], loss=79.2691
	step [78/249], loss=98.0843
	step [79/249], loss=81.0081
	step [80/249], loss=77.1323
	step [81/249], loss=87.0907
	step [82/249], loss=77.3405
	step [83/249], loss=70.2900
	step [84/249], loss=78.9468
	step [85/249], loss=91.6398
	step [86/249], loss=79.0719
	step [87/249], loss=86.6666
	step [88/249], loss=78.0985
	step [89/249], loss=65.3074
	step [90/249], loss=78.7875
	step [91/249], loss=72.9602
	step [92/249], loss=73.2687
	step [93/249], loss=94.0016
	step [94/249], loss=70.9880
	step [95/249], loss=78.9837
	step [96/249], loss=71.4738
	step [97/249], loss=81.6212
	step [98/249], loss=67.9679
	step [99/249], loss=91.1823
	step [100/249], loss=76.9879
	step [101/249], loss=73.8589
	step [102/249], loss=87.8777
	step [103/249], loss=111.3871
	step [104/249], loss=84.7611
	step [105/249], loss=98.7529
	step [106/249], loss=69.7209
	step [107/249], loss=59.8309
	step [108/249], loss=69.9609
	step [109/249], loss=77.2097
	step [110/249], loss=74.3980
	step [111/249], loss=62.8138
	step [112/249], loss=63.5649
	step [113/249], loss=71.9129
	step [114/249], loss=88.8573
	step [115/249], loss=82.4036
	step [116/249], loss=77.5058
	step [117/249], loss=80.9909
	step [118/249], loss=70.8347
	step [119/249], loss=68.0495
	step [120/249], loss=87.5047
	step [121/249], loss=80.6114
	step [122/249], loss=85.7005
	step [123/249], loss=71.3299
	step [124/249], loss=60.5500
	step [125/249], loss=75.2302
	step [126/249], loss=77.8976
	step [127/249], loss=75.8423
	step [128/249], loss=78.7414
	step [129/249], loss=80.0255
	step [130/249], loss=80.3360
	step [131/249], loss=92.4209
	step [132/249], loss=73.7983
	step [133/249], loss=71.8311
	step [134/249], loss=77.4084
	step [135/249], loss=108.9115
	step [136/249], loss=74.3566
	step [137/249], loss=91.2646
	step [138/249], loss=82.6068
	step [139/249], loss=84.1932
	step [140/249], loss=82.8324
	step [141/249], loss=74.6791
	step [142/249], loss=82.6910
	step [143/249], loss=88.0655
	step [144/249], loss=77.1332
	step [145/249], loss=83.9851
	step [146/249], loss=76.7923
	step [147/249], loss=77.0121
	step [148/249], loss=86.3255
	step [149/249], loss=92.3205
	step [150/249], loss=82.4895
	step [151/249], loss=85.6504
	step [152/249], loss=78.7668
	step [153/249], loss=85.6802
	step [154/249], loss=78.0728
	step [155/249], loss=83.2614
	step [156/249], loss=93.8954
	step [157/249], loss=89.8060
	step [158/249], loss=89.8524
	step [159/249], loss=64.9530
	step [160/249], loss=71.2322
	step [161/249], loss=83.7182
	step [162/249], loss=93.6286
	step [163/249], loss=75.6203
	step [164/249], loss=73.0348
	step [165/249], loss=91.7716
	step [166/249], loss=89.2809
	step [167/249], loss=73.2681
	step [168/249], loss=83.3478
	step [169/249], loss=61.4988
	step [170/249], loss=66.4621
	step [171/249], loss=84.1825
	step [172/249], loss=70.7861
	step [173/249], loss=77.5284
	step [174/249], loss=80.5471
	step [175/249], loss=80.5108
	step [176/249], loss=85.4027
	step [177/249], loss=73.2925
	step [178/249], loss=90.2133
	step [179/249], loss=76.5748
	step [180/249], loss=82.3289
	step [181/249], loss=77.2150
	step [182/249], loss=68.6012
	step [183/249], loss=56.6372
	step [184/249], loss=91.0788
	step [185/249], loss=63.0473
	step [186/249], loss=79.4604
	step [187/249], loss=92.6584
	step [188/249], loss=89.6510
	step [189/249], loss=78.0098
	step [190/249], loss=71.3309
	step [191/249], loss=81.3179
	step [192/249], loss=70.0750
	step [193/249], loss=68.6742
	step [194/249], loss=92.6109
	step [195/249], loss=61.3849
	step [196/249], loss=65.0472
	step [197/249], loss=76.1937
	step [198/249], loss=83.2083
	step [199/249], loss=69.2713
	step [200/249], loss=81.0441
	step [201/249], loss=85.3467
	step [202/249], loss=70.6843
	step [203/249], loss=82.9277
	step [204/249], loss=81.5443
	step [205/249], loss=88.6216
	step [206/249], loss=72.6021
	step [207/249], loss=80.4409
	step [208/249], loss=103.6755
	step [209/249], loss=78.9615
	step [210/249], loss=78.5430
	step [211/249], loss=75.3273
	step [212/249], loss=75.2563
	step [213/249], loss=85.9174
	step [214/249], loss=81.4517
	step [215/249], loss=83.7010
	step [216/249], loss=86.2378
	step [217/249], loss=87.7946
	step [218/249], loss=87.7946
	step [219/249], loss=67.3188
	step [220/249], loss=61.6023
	step [221/249], loss=88.3084
	step [222/249], loss=78.8324
	step [223/249], loss=82.8315
	step [224/249], loss=79.4766
	step [225/249], loss=72.4175
	step [226/249], loss=67.7198
	step [227/249], loss=87.9330
	step [228/249], loss=75.8652
	step [229/249], loss=84.6658
	step [230/249], loss=81.7726
	step [231/249], loss=84.2487
	step [232/249], loss=73.1015
	step [233/249], loss=65.9863
	step [234/249], loss=73.5128
	step [235/249], loss=82.7162
	step [236/249], loss=79.1583
	step [237/249], loss=67.6770
	step [238/249], loss=81.5100
	step [239/249], loss=77.5120
	step [240/249], loss=66.3956
	step [241/249], loss=81.0980
	step [242/249], loss=68.0259
	step [243/249], loss=69.7511
	step [244/249], loss=83.7139
	step [245/249], loss=78.8031
	step [246/249], loss=89.2718
	step [247/249], loss=70.7406
	step [248/249], loss=98.9524
	step [249/249], loss=50.3465
	Evaluating
	loss=0.0076, precision=0.3079, recall=0.8450, f1=0.4514
Training epoch 73
	step [1/249], loss=78.8445
	step [2/249], loss=68.7126
	step [3/249], loss=75.3111
	step [4/249], loss=77.5992
	step [5/249], loss=80.8693
	step [6/249], loss=64.6318
	step [7/249], loss=70.9674
	step [8/249], loss=71.4762
	step [9/249], loss=73.4189
	step [10/249], loss=66.9829
	step [11/249], loss=59.2973
	step [12/249], loss=63.0507
	step [13/249], loss=71.6224
	step [14/249], loss=70.1417
	step [15/249], loss=87.8192
	step [16/249], loss=76.5153
	step [17/249], loss=78.6534
	step [18/249], loss=81.2167
	step [19/249], loss=69.0603
	step [20/249], loss=82.4894
	step [21/249], loss=86.4809
	step [22/249], loss=71.1333
	step [23/249], loss=62.5461
	step [24/249], loss=84.9033
	step [25/249], loss=95.6353
	step [26/249], loss=78.4571
	step [27/249], loss=81.5833
	step [28/249], loss=76.9516
	step [29/249], loss=80.1625
	step [30/249], loss=65.5702
	step [31/249], loss=76.0839
	step [32/249], loss=83.5319
	step [33/249], loss=82.8448
	step [34/249], loss=71.2699
	step [35/249], loss=80.1510
	step [36/249], loss=102.4365
	step [37/249], loss=90.0452
	step [38/249], loss=90.1746
	step [39/249], loss=70.5564
	step [40/249], loss=86.9861
	step [41/249], loss=80.7597
	step [42/249], loss=75.6938
	step [43/249], loss=86.7250
	step [44/249], loss=96.8602
	step [45/249], loss=109.7879
	step [46/249], loss=87.9227
	step [47/249], loss=88.3015
	step [48/249], loss=74.4923
	step [49/249], loss=76.9686
	step [50/249], loss=70.0170
	step [51/249], loss=60.5279
	step [52/249], loss=84.3996
	step [53/249], loss=86.1870
	step [54/249], loss=73.0423
	step [55/249], loss=85.3555
	step [56/249], loss=54.0213
	step [57/249], loss=87.9269
	step [58/249], loss=80.6986
	step [59/249], loss=69.3306
	step [60/249], loss=67.6473
	step [61/249], loss=81.6056
	step [62/249], loss=84.3538
	step [63/249], loss=70.9249
	step [64/249], loss=88.8677
	step [65/249], loss=78.5756
	step [66/249], loss=89.0403
	step [67/249], loss=76.7331
	step [68/249], loss=83.1210
	step [69/249], loss=80.1717
	step [70/249], loss=93.0116
	step [71/249], loss=71.8629
	step [72/249], loss=60.8517
	step [73/249], loss=63.0254
	step [74/249], loss=81.7280
	step [75/249], loss=83.7179
	step [76/249], loss=86.7229
	step [77/249], loss=63.7103
	step [78/249], loss=82.2180
	step [79/249], loss=77.4228
	step [80/249], loss=92.1243
	step [81/249], loss=72.2734
	step [82/249], loss=53.7525
	step [83/249], loss=79.6840
	step [84/249], loss=74.6630
	step [85/249], loss=96.3974
	step [86/249], loss=84.1327
	step [87/249], loss=84.7382
	step [88/249], loss=75.6931
	step [89/249], loss=92.3737
	step [90/249], loss=79.2855
	step [91/249], loss=87.8313
	step [92/249], loss=77.4125
	step [93/249], loss=75.3675
	step [94/249], loss=82.8861
	step [95/249], loss=74.8221
	step [96/249], loss=87.3821
	step [97/249], loss=73.7020
	step [98/249], loss=89.3538
	step [99/249], loss=95.0047
	step [100/249], loss=80.9018
	step [101/249], loss=67.7878
	step [102/249], loss=82.2311
	step [103/249], loss=89.5227
	step [104/249], loss=79.2066
	step [105/249], loss=91.3886
	step [106/249], loss=88.4516
	step [107/249], loss=79.0837
	step [108/249], loss=98.8763
	step [109/249], loss=79.7343
	step [110/249], loss=68.8129
	step [111/249], loss=69.0646
	step [112/249], loss=71.1187
	step [113/249], loss=91.3686
	step [114/249], loss=61.1004
	step [115/249], loss=93.4431
	step [116/249], loss=80.1593
	step [117/249], loss=73.5443
	step [118/249], loss=80.6244
	step [119/249], loss=96.0873
	step [120/249], loss=79.7245
	step [121/249], loss=76.7806
	step [122/249], loss=92.6411
	step [123/249], loss=75.3189
	step [124/249], loss=76.2401
	step [125/249], loss=85.9645
	step [126/249], loss=83.6957
	step [127/249], loss=72.6216
	step [128/249], loss=81.9199
	step [129/249], loss=87.2876
	step [130/249], loss=76.1271
	step [131/249], loss=64.7721
	step [132/249], loss=100.6059
	step [133/249], loss=88.1653
	step [134/249], loss=79.7272
	step [135/249], loss=81.3453
	step [136/249], loss=78.4758
	step [137/249], loss=75.6642
	step [138/249], loss=90.3399
	step [139/249], loss=75.2291
	step [140/249], loss=80.0130
	step [141/249], loss=72.0476
	step [142/249], loss=72.3494
	step [143/249], loss=64.2352
	step [144/249], loss=53.5697
	step [145/249], loss=76.9929
	step [146/249], loss=59.7171
	step [147/249], loss=71.8461
	step [148/249], loss=75.1748
	step [149/249], loss=79.3178
	step [150/249], loss=86.9977
	step [151/249], loss=68.2886
	step [152/249], loss=73.6122
	step [153/249], loss=90.1204
	step [154/249], loss=73.7880
	step [155/249], loss=71.9988
	step [156/249], loss=81.6786
	step [157/249], loss=72.2630
	step [158/249], loss=95.8512
	step [159/249], loss=85.1722
	step [160/249], loss=68.3976
	step [161/249], loss=76.0503
	step [162/249], loss=73.9608
	step [163/249], loss=84.7327
	step [164/249], loss=76.1834
	step [165/249], loss=78.0640
	step [166/249], loss=80.2724
	step [167/249], loss=74.4645
	step [168/249], loss=76.1271
	step [169/249], loss=101.8848
	step [170/249], loss=79.8276
	step [171/249], loss=73.3315
	step [172/249], loss=86.0900
	step [173/249], loss=69.7974
	step [174/249], loss=84.8086
	step [175/249], loss=75.9125
	step [176/249], loss=76.7054
	step [177/249], loss=82.1962
	step [178/249], loss=75.2083
	step [179/249], loss=82.5486
	step [180/249], loss=75.4217
	step [181/249], loss=65.4337
	step [182/249], loss=73.5293
	step [183/249], loss=81.5360
	step [184/249], loss=74.5357
	step [185/249], loss=77.7096
	step [186/249], loss=77.7179
	step [187/249], loss=69.7913
	step [188/249], loss=79.1398
	step [189/249], loss=90.1317
	step [190/249], loss=92.3948
	step [191/249], loss=79.5405
	step [192/249], loss=80.4919
	step [193/249], loss=97.1447
	step [194/249], loss=90.7104
	step [195/249], loss=90.9126
	step [196/249], loss=77.0035
	step [197/249], loss=73.2774
	step [198/249], loss=82.4473
	step [199/249], loss=85.3033
	step [200/249], loss=80.7727
	step [201/249], loss=72.4752
	step [202/249], loss=69.8786
	step [203/249], loss=90.2258
	step [204/249], loss=73.7099
	step [205/249], loss=71.1961
	step [206/249], loss=51.8042
	step [207/249], loss=86.3561
	step [208/249], loss=76.5225
	step [209/249], loss=91.6340
	step [210/249], loss=64.1683
	step [211/249], loss=75.3685
	step [212/249], loss=63.0648
	step [213/249], loss=82.7798
	step [214/249], loss=68.5734
	step [215/249], loss=71.1849
	step [216/249], loss=76.8677
	step [217/249], loss=82.6362
	step [218/249], loss=104.3395
	step [219/249], loss=87.0402
	step [220/249], loss=82.7567
	step [221/249], loss=62.5070
	step [222/249], loss=86.3825
	step [223/249], loss=68.3631
	step [224/249], loss=78.6320
	step [225/249], loss=78.3841
	step [226/249], loss=74.1414
	step [227/249], loss=84.7936
	step [228/249], loss=73.6999
	step [229/249], loss=83.5676
	step [230/249], loss=75.4187
	step [231/249], loss=60.9545
	step [232/249], loss=85.9558
	step [233/249], loss=89.9815
	step [234/249], loss=80.6296
	step [235/249], loss=55.9592
	step [236/249], loss=86.9007
	step [237/249], loss=79.8726
	step [238/249], loss=69.5720
	step [239/249], loss=72.7701
	step [240/249], loss=82.3020
	step [241/249], loss=90.5057
	step [242/249], loss=70.6791
	step [243/249], loss=89.1417
	step [244/249], loss=75.0125
	step [245/249], loss=79.8861
	step [246/249], loss=82.1220
	step [247/249], loss=79.4662
	step [248/249], loss=83.5740
	step [249/249], loss=51.5404
	Evaluating
	loss=0.0078, precision=0.3015, recall=0.8554, f1=0.4459
Training epoch 74
	step [1/249], loss=77.3017
	step [2/249], loss=85.4354
	step [3/249], loss=88.0556
	step [4/249], loss=80.4567
	step [5/249], loss=91.2285
	step [6/249], loss=84.2784
	step [7/249], loss=69.1019
	step [8/249], loss=83.1718
	step [9/249], loss=90.9214
	step [10/249], loss=69.9125
	step [11/249], loss=80.0144
	step [12/249], loss=62.5652
	step [13/249], loss=87.0394
	step [14/249], loss=82.1709
	step [15/249], loss=84.5472
	step [16/249], loss=72.8982
	step [17/249], loss=70.8017
	step [18/249], loss=87.7394
	step [19/249], loss=72.0120
	step [20/249], loss=89.3189
	step [21/249], loss=72.1181
	step [22/249], loss=77.7477
	step [23/249], loss=71.6676
	step [24/249], loss=82.5903
	step [25/249], loss=85.3643
	step [26/249], loss=87.4628
	step [27/249], loss=73.3633
	step [28/249], loss=81.1309
	step [29/249], loss=69.2312
	step [30/249], loss=79.3030
	step [31/249], loss=80.8819
	step [32/249], loss=79.9523
	step [33/249], loss=88.8846
	step [34/249], loss=69.2486
	step [35/249], loss=68.6740
	step [36/249], loss=73.6163
	step [37/249], loss=79.0562
	step [38/249], loss=79.3064
	step [39/249], loss=72.9485
	step [40/249], loss=71.2772
	step [41/249], loss=78.8378
	step [42/249], loss=85.0370
	step [43/249], loss=75.2885
	step [44/249], loss=67.8112
	step [45/249], loss=77.4307
	step [46/249], loss=89.9781
	step [47/249], loss=86.5045
	step [48/249], loss=88.3153
	step [49/249], loss=78.8678
	step [50/249], loss=83.6150
	step [51/249], loss=71.4048
	step [52/249], loss=55.8162
	step [53/249], loss=67.5054
	step [54/249], loss=85.1036
	step [55/249], loss=81.8666
	step [56/249], loss=69.4322
	step [57/249], loss=65.1500
	step [58/249], loss=91.7879
	step [59/249], loss=74.3290
	step [60/249], loss=78.7454
	step [61/249], loss=72.7954
	step [62/249], loss=73.0481
	step [63/249], loss=72.7556
	step [64/249], loss=93.4668
	step [65/249], loss=71.6159
	step [66/249], loss=80.2673
	step [67/249], loss=95.0107
	step [68/249], loss=84.1594
	step [69/249], loss=97.4331
	step [70/249], loss=81.3195
	step [71/249], loss=79.2164
	step [72/249], loss=69.2125
	step [73/249], loss=88.7487
	step [74/249], loss=78.1994
	step [75/249], loss=77.0586
	step [76/249], loss=78.0145
	step [77/249], loss=70.0616
	step [78/249], loss=111.6836
	step [79/249], loss=93.0937
	step [80/249], loss=77.9630
	step [81/249], loss=78.0410
	step [82/249], loss=81.4148
	step [83/249], loss=80.8303
	step [84/249], loss=67.6592
	step [85/249], loss=79.5293
	step [86/249], loss=68.6261
	step [87/249], loss=73.8366
	step [88/249], loss=83.5510
	step [89/249], loss=72.1210
	step [90/249], loss=71.1768
	step [91/249], loss=88.6230
	step [92/249], loss=79.6320
	step [93/249], loss=76.6567
	step [94/249], loss=78.5138
	step [95/249], loss=86.4788
	step [96/249], loss=62.9480
	step [97/249], loss=72.9265
	step [98/249], loss=78.8228
	step [99/249], loss=77.2412
	step [100/249], loss=68.4445
	step [101/249], loss=81.1493
	step [102/249], loss=78.1437
	step [103/249], loss=89.7278
	step [104/249], loss=80.4709
	step [105/249], loss=74.5857
	step [106/249], loss=83.1874
	step [107/249], loss=99.8045
	step [108/249], loss=82.4309
	step [109/249], loss=77.2406
	step [110/249], loss=62.7633
	step [111/249], loss=93.4546
	step [112/249], loss=76.8898
	step [113/249], loss=83.8514
	step [114/249], loss=82.3345
	step [115/249], loss=94.3582
	step [116/249], loss=83.7825
	step [117/249], loss=77.3400
	step [118/249], loss=91.0986
	step [119/249], loss=80.0772
	step [120/249], loss=86.7604
	step [121/249], loss=75.0829
	step [122/249], loss=74.5769
	step [123/249], loss=80.4883
	step [124/249], loss=69.0152
	step [125/249], loss=67.9434
	step [126/249], loss=74.4271
	step [127/249], loss=80.8756
	step [128/249], loss=81.9876
	step [129/249], loss=69.4779
	step [130/249], loss=88.2027
	step [131/249], loss=87.0491
	step [132/249], loss=92.2238
	step [133/249], loss=70.1405
	step [134/249], loss=56.7418
	step [135/249], loss=76.4658
	step [136/249], loss=88.0293
	step [137/249], loss=80.6298
	step [138/249], loss=71.6198
	step [139/249], loss=66.4986
	step [140/249], loss=67.3613
	step [141/249], loss=72.7489
	step [142/249], loss=86.0954
	step [143/249], loss=88.6379
	step [144/249], loss=71.5809
	step [145/249], loss=81.5471
	step [146/249], loss=85.8476
	step [147/249], loss=85.7698
	step [148/249], loss=60.8198
	step [149/249], loss=81.4913
	step [150/249], loss=68.8147
	step [151/249], loss=75.4948
	step [152/249], loss=88.5074
	step [153/249], loss=85.4107
	step [154/249], loss=76.3287
	step [155/249], loss=92.8478
	step [156/249], loss=80.7513
	step [157/249], loss=81.2916
	step [158/249], loss=69.6391
	step [159/249], loss=71.9115
	step [160/249], loss=83.4191
	step [161/249], loss=94.8773
	step [162/249], loss=90.5875
	step [163/249], loss=80.2549
	step [164/249], loss=80.9552
	step [165/249], loss=67.9902
	step [166/249], loss=65.2369
	step [167/249], loss=57.8149
	step [168/249], loss=83.4661
	step [169/249], loss=72.0581
	step [170/249], loss=81.4742
	step [171/249], loss=60.7105
	step [172/249], loss=69.7680
	step [173/249], loss=78.1416
	step [174/249], loss=88.7723
	step [175/249], loss=61.4896
	step [176/249], loss=79.8816
	step [177/249], loss=80.5742
	step [178/249], loss=71.5334
	step [179/249], loss=73.6187
	step [180/249], loss=89.8343
	step [181/249], loss=66.9697
	step [182/249], loss=66.9698
	step [183/249], loss=87.7847
	step [184/249], loss=68.3141
	step [185/249], loss=68.7475
	step [186/249], loss=88.2147
	step [187/249], loss=86.6289
	step [188/249], loss=86.8999
	step [189/249], loss=77.1853
	step [190/249], loss=72.4341
	step [191/249], loss=74.6373
	step [192/249], loss=83.6214
	step [193/249], loss=81.6962
	step [194/249], loss=94.6152
	step [195/249], loss=84.1297
	step [196/249], loss=65.0178
	step [197/249], loss=65.3325
	step [198/249], loss=65.6903
	step [199/249], loss=94.2632
	step [200/249], loss=78.2588
	step [201/249], loss=65.4412
	step [202/249], loss=82.1533
	step [203/249], loss=67.5545
	step [204/249], loss=70.2378
	step [205/249], loss=65.4066
	step [206/249], loss=89.8552
	step [207/249], loss=80.4355
	step [208/249], loss=91.8140
	step [209/249], loss=88.3520
	step [210/249], loss=64.3668
	step [211/249], loss=80.3980
	step [212/249], loss=71.9817
	step [213/249], loss=85.7392
	step [214/249], loss=82.3931
	step [215/249], loss=68.1463
	step [216/249], loss=71.3161
	step [217/249], loss=70.4354
	step [218/249], loss=93.1589
	step [219/249], loss=70.6279
	step [220/249], loss=77.0684
	step [221/249], loss=80.9596
	step [222/249], loss=69.0254
	step [223/249], loss=78.1844
	step [224/249], loss=70.2572
	step [225/249], loss=80.6385
	step [226/249], loss=94.6280
	step [227/249], loss=80.4860
	step [228/249], loss=65.6504
	step [229/249], loss=59.1532
	step [230/249], loss=81.4492
	step [231/249], loss=68.3848
	step [232/249], loss=88.9935
	step [233/249], loss=70.5563
	step [234/249], loss=76.8465
	step [235/249], loss=92.8937
	step [236/249], loss=70.7915
	step [237/249], loss=88.3790
	step [238/249], loss=75.6706
	step [239/249], loss=68.0911
	step [240/249], loss=71.3805
	step [241/249], loss=87.2285
	step [242/249], loss=75.7899
	step [243/249], loss=81.8618
	step [244/249], loss=73.1831
	step [245/249], loss=89.2951
	step [246/249], loss=73.9993
	step [247/249], loss=69.7204
	step [248/249], loss=62.6115
	step [249/249], loss=57.3279
	Evaluating
	loss=0.0067, precision=0.3456, recall=0.8477, f1=0.4910
Training epoch 75
	step [1/249], loss=86.0609
	step [2/249], loss=74.9272
	step [3/249], loss=85.9408
	step [4/249], loss=68.9439
	step [5/249], loss=62.8649
	step [6/249], loss=65.3296
	step [7/249], loss=75.8358
	step [8/249], loss=60.4321
	step [9/249], loss=61.0356
	step [10/249], loss=68.2749
	step [11/249], loss=73.5582
	step [12/249], loss=73.4033
	step [13/249], loss=91.2058
	step [14/249], loss=63.1449
	step [15/249], loss=86.8488
	step [16/249], loss=66.4391
	step [17/249], loss=73.0397
	step [18/249], loss=92.2592
	step [19/249], loss=90.3351
	step [20/249], loss=75.6361
	step [21/249], loss=79.0519
	step [22/249], loss=74.5043
	step [23/249], loss=88.3606
	step [24/249], loss=91.1402
	step [25/249], loss=79.8399
	step [26/249], loss=90.2281
	step [27/249], loss=87.1470
	step [28/249], loss=95.1058
	step [29/249], loss=59.6868
	step [30/249], loss=75.4446
	step [31/249], loss=74.4487
	step [32/249], loss=77.0851
	step [33/249], loss=79.4412
	step [34/249], loss=90.1044
	step [35/249], loss=77.8125
	step [36/249], loss=97.9617
	step [37/249], loss=73.8295
	step [38/249], loss=72.7000
	step [39/249], loss=80.1866
	step [40/249], loss=73.1489
	step [41/249], loss=73.9202
	step [42/249], loss=60.8264
	step [43/249], loss=86.2673
	step [44/249], loss=75.4921
	step [45/249], loss=74.2015
	step [46/249], loss=67.7383
	step [47/249], loss=73.7657
	step [48/249], loss=100.0812
	step [49/249], loss=78.7912
	step [50/249], loss=67.6311
	step [51/249], loss=83.8362
	step [52/249], loss=84.6977
	step [53/249], loss=78.9476
	step [54/249], loss=83.0872
	step [55/249], loss=83.7181
	step [56/249], loss=85.9996
	step [57/249], loss=60.9419
	step [58/249], loss=79.9042
	step [59/249], loss=85.3630
	step [60/249], loss=71.3802
	step [61/249], loss=74.8427
	step [62/249], loss=65.7716
	step [63/249], loss=89.7020
	step [64/249], loss=91.7552
	step [65/249], loss=86.4202
	step [66/249], loss=67.6515
	step [67/249], loss=69.6408
	step [68/249], loss=77.0644
	step [69/249], loss=71.5128
	step [70/249], loss=79.0792
	step [71/249], loss=77.2838
	step [72/249], loss=88.3532
	step [73/249], loss=63.0960
	step [74/249], loss=75.5903
	step [75/249], loss=79.8050
	step [76/249], loss=47.1217
	step [77/249], loss=67.3606
	step [78/249], loss=88.5420
	step [79/249], loss=75.3732
	step [80/249], loss=66.7386
	step [81/249], loss=69.4194
	step [82/249], loss=59.5707
	step [83/249], loss=74.5677
	step [84/249], loss=84.4657
	step [85/249], loss=105.0515
	step [86/249], loss=72.3156
	step [87/249], loss=72.6582
	step [88/249], loss=75.6643
	step [89/249], loss=99.1522
	step [90/249], loss=65.9739
	step [91/249], loss=71.5772
	step [92/249], loss=72.8081
	step [93/249], loss=70.8940
	step [94/249], loss=81.2305
	step [95/249], loss=81.2134
	step [96/249], loss=65.3678
	step [97/249], loss=92.2191
	step [98/249], loss=86.8686
	step [99/249], loss=93.9284
	step [100/249], loss=81.1795
	step [101/249], loss=85.3222
	step [102/249], loss=65.7148
	step [103/249], loss=75.2106
	step [104/249], loss=76.1595
	step [105/249], loss=72.5093
	step [106/249], loss=66.4424
	step [107/249], loss=73.6246
	step [108/249], loss=74.0869
	step [109/249], loss=72.0739
	step [110/249], loss=61.3185
	step [111/249], loss=79.3914
	step [112/249], loss=63.4276
	step [113/249], loss=72.3540
	step [114/249], loss=78.1824
	step [115/249], loss=85.2161
	step [116/249], loss=95.4541
	step [117/249], loss=87.1929
	step [118/249], loss=81.1005
	step [119/249], loss=75.9307
	step [120/249], loss=80.9896
	step [121/249], loss=74.2005
	step [122/249], loss=81.3671
	step [123/249], loss=68.0161
	step [124/249], loss=75.8841
	step [125/249], loss=85.1331
	step [126/249], loss=80.3101
	step [127/249], loss=74.6914
	step [128/249], loss=69.0484
	step [129/249], loss=62.9240
	step [130/249], loss=81.6531
	step [131/249], loss=79.5970
	step [132/249], loss=69.9715
	step [133/249], loss=93.0268
	step [134/249], loss=77.4113
	step [135/249], loss=88.5860
	step [136/249], loss=84.5610
	step [137/249], loss=90.1345
	step [138/249], loss=73.9800
	step [139/249], loss=78.4998
	step [140/249], loss=83.9711
	step [141/249], loss=76.3209
	step [142/249], loss=85.5608
	step [143/249], loss=78.3742
	step [144/249], loss=80.4197
	step [145/249], loss=69.7415
	step [146/249], loss=72.3814
	step [147/249], loss=80.6187
	step [148/249], loss=86.0580
	step [149/249], loss=67.5496
	step [150/249], loss=66.2673
	step [151/249], loss=77.3479
	step [152/249], loss=70.8800
	step [153/249], loss=82.9083
	step [154/249], loss=85.4287
	step [155/249], loss=71.8705
	step [156/249], loss=77.2740
	step [157/249], loss=84.2269
	step [158/249], loss=78.7012
	step [159/249], loss=83.6597
	step [160/249], loss=79.1169
	step [161/249], loss=79.6444
	step [162/249], loss=83.4820
	step [163/249], loss=80.5466
	step [164/249], loss=87.8174
	step [165/249], loss=79.3304
	step [166/249], loss=100.0458
	step [167/249], loss=78.3111
	step [168/249], loss=82.1364
	step [169/249], loss=76.9462
	step [170/249], loss=88.9277
	step [171/249], loss=63.3321
	step [172/249], loss=71.9394
	step [173/249], loss=80.2497
	step [174/249], loss=86.6780
	step [175/249], loss=81.1562
	step [176/249], loss=83.4886
	step [177/249], loss=77.7217
	step [178/249], loss=77.1174
	step [179/249], loss=75.7090
	step [180/249], loss=75.5875
	step [181/249], loss=71.8256
	step [182/249], loss=86.1151
	step [183/249], loss=70.6011
	step [184/249], loss=71.6550
	step [185/249], loss=72.3741
	step [186/249], loss=67.2940
	step [187/249], loss=86.2323
	step [188/249], loss=71.4577
	step [189/249], loss=81.0542
	step [190/249], loss=81.4781
	step [191/249], loss=74.5082
	step [192/249], loss=71.9518
	step [193/249], loss=75.3773
	step [194/249], loss=76.7842
	step [195/249], loss=83.7378
	step [196/249], loss=86.5878
	step [197/249], loss=85.2099
	step [198/249], loss=84.5957
	step [199/249], loss=81.9384
	step [200/249], loss=77.8927
	step [201/249], loss=85.5481
	step [202/249], loss=62.2266
	step [203/249], loss=66.1553
	step [204/249], loss=85.4197
	step [205/249], loss=82.2560
	step [206/249], loss=97.6993
	step [207/249], loss=77.2977
	step [208/249], loss=54.9299
	step [209/249], loss=84.7733
	step [210/249], loss=81.0394
	step [211/249], loss=88.5788
	step [212/249], loss=95.1938
	step [213/249], loss=60.9656
	step [214/249], loss=67.0754
	step [215/249], loss=72.1750
	step [216/249], loss=85.6601
	step [217/249], loss=73.5745
	step [218/249], loss=85.2410
	step [219/249], loss=73.0755
	step [220/249], loss=69.4775
	step [221/249], loss=59.0391
	step [222/249], loss=76.3689
	step [223/249], loss=65.4920
	step [224/249], loss=75.9649
	step [225/249], loss=79.7464
	step [226/249], loss=74.9380
	step [227/249], loss=80.6749
	step [228/249], loss=79.2079
	step [229/249], loss=86.3364
	step [230/249], loss=76.6219
	step [231/249], loss=82.7350
	step [232/249], loss=84.4814
	step [233/249], loss=68.6050
	step [234/249], loss=68.0179
	step [235/249], loss=71.5153
	step [236/249], loss=80.8174
	step [237/249], loss=82.6799
	step [238/249], loss=69.8463
	step [239/249], loss=99.4261
	step [240/249], loss=86.8777
	step [241/249], loss=95.3805
	step [242/249], loss=63.4299
	step [243/249], loss=71.1993
	step [244/249], loss=75.3914
	step [245/249], loss=70.7337
	step [246/249], loss=89.3382
	step [247/249], loss=82.6108
	step [248/249], loss=80.9196
	step [249/249], loss=58.0553
	Evaluating
	loss=0.0063, precision=0.3596, recall=0.8537, f1=0.5060
Training epoch 76
	step [1/249], loss=77.9019
	step [2/249], loss=81.1217
	step [3/249], loss=92.8499
	step [4/249], loss=113.2299
	step [5/249], loss=85.9822
	step [6/249], loss=74.4518
	step [7/249], loss=92.9535
	step [8/249], loss=71.1709
	step [9/249], loss=96.8997
	step [10/249], loss=83.0533
	step [11/249], loss=74.6473
	step [12/249], loss=83.2891
	step [13/249], loss=80.4865
	step [14/249], loss=86.7766
	step [15/249], loss=64.9294
	step [16/249], loss=76.8342
	step [17/249], loss=70.6206
	step [18/249], loss=79.2879
	step [19/249], loss=77.8529
	step [20/249], loss=81.6183
	step [21/249], loss=76.9977
	step [22/249], loss=68.6467
	step [23/249], loss=71.5922
	step [24/249], loss=89.7502
	step [25/249], loss=82.4389
	step [26/249], loss=71.6227
	step [27/249], loss=66.3006
	step [28/249], loss=65.1895
	step [29/249], loss=85.9258
	step [30/249], loss=72.9663
	step [31/249], loss=68.2622
	step [32/249], loss=70.7124
	step [33/249], loss=86.3589
	step [34/249], loss=81.4284
	step [35/249], loss=80.6795
	step [36/249], loss=71.2509
	step [37/249], loss=88.1612
	step [38/249], loss=66.6793
	step [39/249], loss=100.9936
	step [40/249], loss=73.9859
	step [41/249], loss=100.8148
	step [42/249], loss=77.5665
	step [43/249], loss=86.8369
	step [44/249], loss=71.2333
	step [45/249], loss=63.2437
	step [46/249], loss=84.7706
	step [47/249], loss=75.5815
	step [48/249], loss=80.2912
	step [49/249], loss=73.1657
	step [50/249], loss=68.5913
	step [51/249], loss=85.7992
	step [52/249], loss=84.2946
	step [53/249], loss=91.7831
	step [54/249], loss=73.1520
	step [55/249], loss=74.1036
	step [56/249], loss=68.2941
	step [57/249], loss=71.7783
	step [58/249], loss=83.6859
	step [59/249], loss=81.0836
	step [60/249], loss=70.2163
	step [61/249], loss=75.2895
	step [62/249], loss=70.9799
	step [63/249], loss=74.9232
	step [64/249], loss=91.0225
	step [65/249], loss=76.5266
	step [66/249], loss=68.5934
	step [67/249], loss=66.8567
	step [68/249], loss=82.4966
	step [69/249], loss=89.4714
	step [70/249], loss=73.2078
	step [71/249], loss=63.1740
	step [72/249], loss=66.7638
	step [73/249], loss=67.6690
	step [74/249], loss=74.8015
	step [75/249], loss=92.2910
	step [76/249], loss=71.7039
	step [77/249], loss=74.3212
	step [78/249], loss=93.9606
	step [79/249], loss=83.4946
	step [80/249], loss=63.4351
	step [81/249], loss=84.1291
	step [82/249], loss=75.1224
	step [83/249], loss=73.5505
	step [84/249], loss=70.1885
	step [85/249], loss=66.2903
	step [86/249], loss=73.5771
	step [87/249], loss=75.6994
	step [88/249], loss=73.4693
	step [89/249], loss=72.8600
	step [90/249], loss=83.1676
	step [91/249], loss=57.8469
	step [92/249], loss=83.1232
	step [93/249], loss=107.9839
	step [94/249], loss=86.9024
	step [95/249], loss=85.8375
	step [96/249], loss=77.4330
	step [97/249], loss=76.9975
	step [98/249], loss=68.3127
	step [99/249], loss=84.6047
	step [100/249], loss=88.7401
	step [101/249], loss=85.5284
	step [102/249], loss=86.4433
	step [103/249], loss=82.3054
	step [104/249], loss=68.3415
	step [105/249], loss=66.6210
	step [106/249], loss=73.6465
	step [107/249], loss=70.9556
	step [108/249], loss=92.0192
	step [109/249], loss=78.5537
	step [110/249], loss=80.6758
	step [111/249], loss=73.0004
	step [112/249], loss=80.7657
	step [113/249], loss=64.4806
	step [114/249], loss=63.7265
	step [115/249], loss=79.8157
	step [116/249], loss=61.4731
	step [117/249], loss=83.2752
	step [118/249], loss=84.5864
	step [119/249], loss=71.2174
	step [120/249], loss=87.3316
	step [121/249], loss=90.7850
	step [122/249], loss=87.7770
	step [123/249], loss=63.1926
	step [124/249], loss=72.5275
	step [125/249], loss=64.9486
	step [126/249], loss=94.6544
	step [127/249], loss=64.2767
	step [128/249], loss=83.3864
	step [129/249], loss=89.3044
	step [130/249], loss=68.6694
	step [131/249], loss=75.1864
	step [132/249], loss=81.0212
	step [133/249], loss=86.1823
	step [134/249], loss=74.8984
	step [135/249], loss=78.5391
	step [136/249], loss=78.0091
	step [137/249], loss=74.1038
	step [138/249], loss=84.2971
	step [139/249], loss=85.8141
	step [140/249], loss=68.1240
	step [141/249], loss=85.1346
	step [142/249], loss=75.3283
	step [143/249], loss=73.5829
	step [144/249], loss=75.7547
	step [145/249], loss=80.2852
	step [146/249], loss=78.9036
	step [147/249], loss=73.8361
	step [148/249], loss=62.2136
	step [149/249], loss=68.0272
	step [150/249], loss=84.7230
	step [151/249], loss=74.0936
	step [152/249], loss=66.6590
	step [153/249], loss=73.0933
	step [154/249], loss=94.2410
	step [155/249], loss=71.9845
	step [156/249], loss=86.4237
	step [157/249], loss=72.4074
	step [158/249], loss=76.8246
	step [159/249], loss=80.9249
	step [160/249], loss=87.8273
	step [161/249], loss=82.5692
	step [162/249], loss=82.3990
	step [163/249], loss=80.7969
	step [164/249], loss=80.7795
	step [165/249], loss=57.1277
	step [166/249], loss=77.8652
	step [167/249], loss=106.4909
	step [168/249], loss=70.9868
	step [169/249], loss=100.2761
	step [170/249], loss=76.7750
	step [171/249], loss=95.8318
	step [172/249], loss=76.3254
	step [173/249], loss=84.6671
	step [174/249], loss=83.3857
	step [175/249], loss=73.4026
	step [176/249], loss=64.1373
	step [177/249], loss=90.4075
	step [178/249], loss=73.3232
	step [179/249], loss=60.8652
	step [180/249], loss=81.9727
	step [181/249], loss=72.7673
	step [182/249], loss=59.7078
	step [183/249], loss=74.2390
	step [184/249], loss=69.2377
	step [185/249], loss=83.0753
	step [186/249], loss=87.1194
	step [187/249], loss=78.5077
	step [188/249], loss=87.6272
	step [189/249], loss=80.1845
	step [190/249], loss=88.4615
	step [191/249], loss=75.9847
	step [192/249], loss=82.2090
	step [193/249], loss=61.8849
	step [194/249], loss=72.7897
	step [195/249], loss=74.0911
	step [196/249], loss=64.5561
	step [197/249], loss=77.6099
	step [198/249], loss=87.6938
	step [199/249], loss=71.2059
	step [200/249], loss=72.7318
	step [201/249], loss=80.6110
	step [202/249], loss=73.5166
	step [203/249], loss=60.7023
	step [204/249], loss=68.2235
	step [205/249], loss=75.3652
	step [206/249], loss=82.8010
	step [207/249], loss=73.2247
	step [208/249], loss=74.5051
	step [209/249], loss=72.1226
	step [210/249], loss=87.2070
	step [211/249], loss=86.9861
	step [212/249], loss=69.2548
	step [213/249], loss=74.4280
	step [214/249], loss=69.8332
	step [215/249], loss=85.9760
	step [216/249], loss=81.6992
	step [217/249], loss=63.1937
	step [218/249], loss=74.7651
	step [219/249], loss=72.4155
	step [220/249], loss=76.1511
	step [221/249], loss=68.1337
	step [222/249], loss=84.1431
	step [223/249], loss=78.5079
	step [224/249], loss=97.5474
	step [225/249], loss=78.9212
	step [226/249], loss=83.3948
	step [227/249], loss=74.7072
	step [228/249], loss=63.1547
	step [229/249], loss=69.4687
	step [230/249], loss=61.9335
	step [231/249], loss=76.6084
	step [232/249], loss=76.4159
	step [233/249], loss=73.1639
	step [234/249], loss=75.3036
	step [235/249], loss=87.5885
	step [236/249], loss=81.5191
	step [237/249], loss=71.1031
	step [238/249], loss=82.8934
	step [239/249], loss=89.2074
	step [240/249], loss=69.3488
	step [241/249], loss=76.2135
	step [242/249], loss=80.0439
	step [243/249], loss=80.7076
	step [244/249], loss=90.1678
	step [245/249], loss=79.0255
	step [246/249], loss=81.2241
	step [247/249], loss=74.4935
	step [248/249], loss=80.8071
	step [249/249], loss=46.1565
	Evaluating
	loss=0.0071, precision=0.3348, recall=0.8488, f1=0.4802
Training epoch 77
	step [1/249], loss=79.9049
	step [2/249], loss=79.9964
	step [3/249], loss=83.9115
	step [4/249], loss=70.1842
	step [5/249], loss=76.9614
	step [6/249], loss=78.8310
	step [7/249], loss=77.4908
	step [8/249], loss=64.7677
	step [9/249], loss=77.0748
	step [10/249], loss=94.3932
	step [11/249], loss=69.0467
	step [12/249], loss=76.5619
	step [13/249], loss=70.0537
	step [14/249], loss=70.4790
	step [15/249], loss=92.0507
	step [16/249], loss=81.9639
	step [17/249], loss=76.8947
	step [18/249], loss=70.4570
	step [19/249], loss=78.9096
	step [20/249], loss=66.4888
	step [21/249], loss=70.7158
	step [22/249], loss=74.8692
	step [23/249], loss=77.6063
	step [24/249], loss=70.9044
	step [25/249], loss=83.4763
	step [26/249], loss=62.1379
	step [27/249], loss=78.1691
	step [28/249], loss=91.5328
	step [29/249], loss=85.5410
	step [30/249], loss=77.3479
	step [31/249], loss=81.5625
	step [32/249], loss=69.3865
	step [33/249], loss=73.9748
	step [34/249], loss=81.2737
	step [35/249], loss=57.6164
	step [36/249], loss=77.7029
	step [37/249], loss=55.5337
	step [38/249], loss=91.4278
	step [39/249], loss=89.1648
	step [40/249], loss=83.5496
	step [41/249], loss=64.5106
	step [42/249], loss=76.3959
	step [43/249], loss=77.6001
	step [44/249], loss=59.2609
	step [45/249], loss=76.7517
	step [46/249], loss=76.9353
	step [47/249], loss=76.8469
	step [48/249], loss=82.2009
	step [49/249], loss=78.0267
	step [50/249], loss=82.8909
	step [51/249], loss=72.9905
	step [52/249], loss=65.4595
	step [53/249], loss=68.0159
	step [54/249], loss=86.6102
	step [55/249], loss=62.4286
	step [56/249], loss=82.6020
	step [57/249], loss=72.2672
	step [58/249], loss=82.7700
	step [59/249], loss=81.5081
	step [60/249], loss=84.5652
	step [61/249], loss=75.1901
	step [62/249], loss=72.6622
	step [63/249], loss=82.5296
	step [64/249], loss=70.6456
	step [65/249], loss=61.2355
	step [66/249], loss=97.8721
	step [67/249], loss=74.6639
	step [68/249], loss=78.7608
	step [69/249], loss=75.9311
	step [70/249], loss=65.9677
	step [71/249], loss=78.4138
	step [72/249], loss=73.7670
	step [73/249], loss=87.8404
	step [74/249], loss=79.1725
	step [75/249], loss=77.4130
	step [76/249], loss=76.3792
	step [77/249], loss=88.3681
	step [78/249], loss=75.8500
	step [79/249], loss=93.9139
	step [80/249], loss=72.0528
	step [81/249], loss=88.8310
	step [82/249], loss=78.8950
	step [83/249], loss=60.7849
	step [84/249], loss=73.9577
	step [85/249], loss=81.3048
	step [86/249], loss=81.4248
	step [87/249], loss=77.1208
	step [88/249], loss=70.5073
	step [89/249], loss=89.9795
	step [90/249], loss=81.9577
	step [91/249], loss=70.5024
	step [92/249], loss=95.6377
	step [93/249], loss=78.0196
	step [94/249], loss=75.0879
	step [95/249], loss=84.4177
	step [96/249], loss=66.5721
	step [97/249], loss=70.9256
	step [98/249], loss=60.3315
	step [99/249], loss=68.2876
	step [100/249], loss=76.3875
	step [101/249], loss=76.8062
	step [102/249], loss=65.3733
	step [103/249], loss=72.5260
	step [104/249], loss=72.6538
	step [105/249], loss=63.0783
	step [106/249], loss=84.0475
	step [107/249], loss=72.4573
	step [108/249], loss=74.5148
	step [109/249], loss=67.2760
	step [110/249], loss=70.2851
	step [111/249], loss=91.6250
	step [112/249], loss=72.6963
	step [113/249], loss=74.0063
	step [114/249], loss=61.8230
	step [115/249], loss=85.9913
	step [116/249], loss=89.0450
	step [117/249], loss=88.5072
	step [118/249], loss=77.8237
	step [119/249], loss=91.0189
	step [120/249], loss=93.1451
	step [121/249], loss=93.4655
	step [122/249], loss=78.8388
	step [123/249], loss=80.4966
	step [124/249], loss=76.3352
	step [125/249], loss=88.7877
	step [126/249], loss=93.9751
	step [127/249], loss=77.9065
	step [128/249], loss=77.5135
	step [129/249], loss=84.2658
	step [130/249], loss=63.0707
	step [131/249], loss=90.8959
	step [132/249], loss=66.5647
	step [133/249], loss=74.3156
	step [134/249], loss=89.1005
	step [135/249], loss=69.2006
	step [136/249], loss=74.8784
	step [137/249], loss=71.6727
	step [138/249], loss=61.9309
	step [139/249], loss=77.3711
	step [140/249], loss=80.5735
	step [141/249], loss=58.4806
	step [142/249], loss=73.5343
	step [143/249], loss=73.6001
	step [144/249], loss=70.2937
	step [145/249], loss=99.6743
	step [146/249], loss=57.2964
	step [147/249], loss=78.9238
	step [148/249], loss=80.7424
	step [149/249], loss=98.5539
	step [150/249], loss=79.1749
	step [151/249], loss=61.4715
	step [152/249], loss=73.9232
	step [153/249], loss=63.1938
	step [154/249], loss=72.2489
	step [155/249], loss=80.3976
	step [156/249], loss=76.0151
	step [157/249], loss=92.3573
	step [158/249], loss=75.7975
	step [159/249], loss=79.1641
	step [160/249], loss=101.9340
	step [161/249], loss=80.5547
	step [162/249], loss=72.5178
	step [163/249], loss=82.7916
	step [164/249], loss=86.0766
	step [165/249], loss=88.6593
	step [166/249], loss=78.5198
	step [167/249], loss=90.9289
	step [168/249], loss=67.7221
	step [169/249], loss=71.9478
	step [170/249], loss=74.2265
	step [171/249], loss=86.0103
	step [172/249], loss=93.9081
	step [173/249], loss=90.6383
	step [174/249], loss=63.7501
	step [175/249], loss=81.4944
	step [176/249], loss=89.0537
	step [177/249], loss=75.3033
	step [178/249], loss=74.3128
	step [179/249], loss=66.9235
	step [180/249], loss=85.9670
	step [181/249], loss=73.0451
	step [182/249], loss=68.6315
	step [183/249], loss=64.5531
	step [184/249], loss=76.8408
	step [185/249], loss=72.1543
	step [186/249], loss=68.1295
	step [187/249], loss=80.5637
	step [188/249], loss=76.1430
	step [189/249], loss=81.2229
	step [190/249], loss=80.9152
	step [191/249], loss=75.5991
	step [192/249], loss=84.1804
	step [193/249], loss=104.9674
	step [194/249], loss=80.8524
	step [195/249], loss=68.0712
	step [196/249], loss=81.8393
	step [197/249], loss=83.1512
	step [198/249], loss=67.7554
	step [199/249], loss=93.7376
	step [200/249], loss=85.7117
	step [201/249], loss=71.2337
	step [202/249], loss=75.1728
	step [203/249], loss=79.7403
	step [204/249], loss=85.2687
	step [205/249], loss=74.0170
	step [206/249], loss=79.6098
	step [207/249], loss=74.9026
	step [208/249], loss=71.7100
	step [209/249], loss=81.9700
	step [210/249], loss=83.3317
	step [211/249], loss=77.1222
	step [212/249], loss=76.2729
	step [213/249], loss=79.1333
	step [214/249], loss=66.9413
	step [215/249], loss=66.2130
	step [216/249], loss=90.6594
	step [217/249], loss=92.5022
	step [218/249], loss=64.4322
	step [219/249], loss=67.6358
	step [220/249], loss=75.2612
	step [221/249], loss=73.5094
	step [222/249], loss=94.6282
	step [223/249], loss=79.0038
	step [224/249], loss=77.2527
	step [225/249], loss=95.4827
	step [226/249], loss=89.6414
	step [227/249], loss=69.0477
	step [228/249], loss=75.2369
	step [229/249], loss=72.5402
	step [230/249], loss=98.3204
	step [231/249], loss=85.7217
	step [232/249], loss=75.4233
	step [233/249], loss=76.7520
	step [234/249], loss=74.3675
	step [235/249], loss=83.0811
	step [236/249], loss=56.4355
	step [237/249], loss=79.9985
	step [238/249], loss=79.4989
	step [239/249], loss=75.0940
	step [240/249], loss=78.8450
	step [241/249], loss=87.4113
	step [242/249], loss=54.5854
	step [243/249], loss=77.7977
	step [244/249], loss=68.7125
	step [245/249], loss=71.6171
	step [246/249], loss=82.4991
	step [247/249], loss=91.2669
	step [248/249], loss=74.4995
	step [249/249], loss=54.6187
	Evaluating
	loss=0.0077, precision=0.3117, recall=0.8516, f1=0.4564
Training epoch 78
	step [1/249], loss=77.8061
	step [2/249], loss=69.8172
	step [3/249], loss=64.9155
	step [4/249], loss=71.4639
	step [5/249], loss=73.2740
	step [6/249], loss=68.6130
	step [7/249], loss=78.1246
	step [8/249], loss=81.6985
	step [9/249], loss=75.8490
	step [10/249], loss=82.0210
	step [11/249], loss=74.4487
	step [12/249], loss=69.7582
	step [13/249], loss=67.9579
	step [14/249], loss=95.1993
	step [15/249], loss=86.9158
	step [16/249], loss=86.8529
	step [17/249], loss=68.6901
	step [18/249], loss=78.5822
	step [19/249], loss=76.7127
	step [20/249], loss=88.4767
	step [21/249], loss=68.8813
	step [22/249], loss=77.8038
	step [23/249], loss=71.8742
	step [24/249], loss=88.8078
	step [25/249], loss=77.8990
	step [26/249], loss=68.0056
	step [27/249], loss=82.9792
	step [28/249], loss=84.7073
	step [29/249], loss=75.8987
	step [30/249], loss=71.4969
	step [31/249], loss=76.6589
	step [32/249], loss=78.5302
	step [33/249], loss=89.2707
	step [34/249], loss=72.8470
	step [35/249], loss=81.3640
	step [36/249], loss=62.4268
	step [37/249], loss=83.0232
	step [38/249], loss=83.1831
	step [39/249], loss=79.9609
	step [40/249], loss=72.0106
	step [41/249], loss=61.9056
	step [42/249], loss=81.6356
	step [43/249], loss=70.9151
	step [44/249], loss=79.7401
	step [45/249], loss=70.1598
	step [46/249], loss=78.4730
	step [47/249], loss=68.7945
	step [48/249], loss=77.0106
	step [49/249], loss=61.3439
	step [50/249], loss=88.0413
	step [51/249], loss=68.3054
	step [52/249], loss=71.3310
	step [53/249], loss=94.8244
	step [54/249], loss=63.4985
	step [55/249], loss=74.9086
	step [56/249], loss=93.4239
	step [57/249], loss=62.4299
	step [58/249], loss=77.0409
	step [59/249], loss=82.8569
	step [60/249], loss=80.1074
	step [61/249], loss=63.7180
	step [62/249], loss=74.0280
	step [63/249], loss=79.7463
	step [64/249], loss=76.4948
	step [65/249], loss=70.1185
	step [66/249], loss=91.0073
	step [67/249], loss=84.7703
	step [68/249], loss=73.2828
	step [69/249], loss=86.1538
	step [70/249], loss=79.1884
	step [71/249], loss=96.5346
	step [72/249], loss=77.5754
	step [73/249], loss=63.0792
	step [74/249], loss=81.9544
	step [75/249], loss=74.7465
	step [76/249], loss=59.3974
	step [77/249], loss=74.5780
	step [78/249], loss=76.1141
	step [79/249], loss=60.8167
	step [80/249], loss=67.6303
	step [81/249], loss=91.3544
	step [82/249], loss=80.8886
	step [83/249], loss=68.7798
	step [84/249], loss=76.3641
	step [85/249], loss=67.4974
	step [86/249], loss=91.8380
	step [87/249], loss=79.0178
	step [88/249], loss=79.8993
	step [89/249], loss=77.1549
	step [90/249], loss=80.2376
	step [91/249], loss=81.8029
	step [92/249], loss=75.3094
	step [93/249], loss=84.8181
	step [94/249], loss=74.3334
	step [95/249], loss=77.6913
	step [96/249], loss=94.3853
	step [97/249], loss=81.4628
	step [98/249], loss=73.3851
	step [99/249], loss=95.0482
	step [100/249], loss=77.6998
	step [101/249], loss=83.6534
	step [102/249], loss=82.3315
	step [103/249], loss=67.7319
	step [104/249], loss=74.0890
	step [105/249], loss=74.5551
	step [106/249], loss=76.9783
	step [107/249], loss=63.7066
	step [108/249], loss=77.8666
	step [109/249], loss=92.8135
	step [110/249], loss=78.9937
	step [111/249], loss=90.2353
	step [112/249], loss=68.7956
	step [113/249], loss=72.8256
	step [114/249], loss=85.4357
	step [115/249], loss=73.9150
	step [116/249], loss=84.5906
	step [117/249], loss=86.6681
	step [118/249], loss=88.0276
	step [119/249], loss=77.3345
	step [120/249], loss=81.0822
	step [121/249], loss=83.0060
	step [122/249], loss=69.2192
	step [123/249], loss=71.9396
	step [124/249], loss=59.7685
	step [125/249], loss=64.2100
	step [126/249], loss=82.6976
	step [127/249], loss=85.6450
	step [128/249], loss=89.5701
	step [129/249], loss=77.5519
	step [130/249], loss=75.6501
	step [131/249], loss=69.3371
	step [132/249], loss=82.2923
	step [133/249], loss=79.2282
	step [134/249], loss=80.5832
	step [135/249], loss=77.9091
	step [136/249], loss=78.9840
	step [137/249], loss=69.1268
	step [138/249], loss=79.9047
	step [139/249], loss=70.2583
	step [140/249], loss=84.6207
	step [141/249], loss=72.5493
	step [142/249], loss=80.7411
	step [143/249], loss=74.0779
	step [144/249], loss=85.5800
	step [145/249], loss=58.5010
	step [146/249], loss=72.7082
	step [147/249], loss=74.0202
	step [148/249], loss=80.7131
	step [149/249], loss=73.7109
	step [150/249], loss=68.6848
	step [151/249], loss=73.1523
	step [152/249], loss=84.0942
	step [153/249], loss=72.6921
	step [154/249], loss=69.1121
	step [155/249], loss=81.4950
	step [156/249], loss=73.6724
	step [157/249], loss=77.9643
	step [158/249], loss=57.2877
	step [159/249], loss=54.2734
	step [160/249], loss=66.7496
	step [161/249], loss=67.0557
	step [162/249], loss=69.7813
	step [163/249], loss=70.7197
	step [164/249], loss=96.4150
	step [165/249], loss=87.2106
	step [166/249], loss=75.9737
	step [167/249], loss=76.1321
	step [168/249], loss=67.2455
	step [169/249], loss=79.3176
	step [170/249], loss=94.4399
	step [171/249], loss=69.7275
	step [172/249], loss=74.8340
	step [173/249], loss=69.8039
	step [174/249], loss=62.3327
	step [175/249], loss=67.0793
	step [176/249], loss=90.1720
	step [177/249], loss=94.7682
	step [178/249], loss=81.6990
	step [179/249], loss=77.9075
	step [180/249], loss=90.5901
	step [181/249], loss=95.9329
	step [182/249], loss=86.1383
	step [183/249], loss=75.1495
	step [184/249], loss=87.9547
	step [185/249], loss=62.9717
	step [186/249], loss=84.8173
	step [187/249], loss=70.6104
	step [188/249], loss=83.0585
	step [189/249], loss=74.0985
	step [190/249], loss=86.9036
	step [191/249], loss=78.8995
	step [192/249], loss=68.6887
	step [193/249], loss=65.1731
	step [194/249], loss=76.6008
	step [195/249], loss=72.6279
	step [196/249], loss=74.3978
	step [197/249], loss=78.3207
	step [198/249], loss=92.1592
	step [199/249], loss=71.9438
	step [200/249], loss=74.5909
	step [201/249], loss=75.7520
	step [202/249], loss=82.0106
	step [203/249], loss=70.1792
	step [204/249], loss=86.0430
	step [205/249], loss=85.7026
	step [206/249], loss=61.6675
	step [207/249], loss=63.6703
	step [208/249], loss=67.5150
	step [209/249], loss=78.8437
	step [210/249], loss=81.8121
	step [211/249], loss=62.7040
	step [212/249], loss=64.3861
	step [213/249], loss=59.1008
	step [214/249], loss=66.9169
	step [215/249], loss=88.6012
	step [216/249], loss=90.8661
	step [217/249], loss=89.1044
	step [218/249], loss=95.1081
	step [219/249], loss=69.3451
	step [220/249], loss=72.0824
	step [221/249], loss=70.8567
	step [222/249], loss=76.4856
	step [223/249], loss=82.5364
	step [224/249], loss=97.7173
	step [225/249], loss=71.2217
	step [226/249], loss=57.6233
	step [227/249], loss=84.5957
	step [228/249], loss=81.2665
	step [229/249], loss=82.3149
	step [230/249], loss=82.9905
	step [231/249], loss=74.4074
	step [232/249], loss=86.5430
	step [233/249], loss=82.1225
	step [234/249], loss=88.5465
	step [235/249], loss=94.4349
	step [236/249], loss=83.6474
	step [237/249], loss=77.4698
	step [238/249], loss=80.5724
	step [239/249], loss=73.3728
	step [240/249], loss=72.7184
	step [241/249], loss=96.2456
	step [242/249], loss=85.5253
	step [243/249], loss=78.0510
	step [244/249], loss=81.8980
	step [245/249], loss=67.6460
	step [246/249], loss=75.7185
	step [247/249], loss=63.1971
	step [248/249], loss=74.0413
	step [249/249], loss=57.1692
	Evaluating
	loss=0.0066, precision=0.3491, recall=0.8461, f1=0.4942
Training epoch 79
	step [1/249], loss=96.2213
	step [2/249], loss=63.7125
	step [3/249], loss=80.8911
	step [4/249], loss=70.1159
	step [5/249], loss=65.5925
	step [6/249], loss=70.6002
	step [7/249], loss=74.9801
	step [8/249], loss=76.6015
	step [9/249], loss=83.7498
	step [10/249], loss=72.5217
	step [11/249], loss=80.4575
	step [12/249], loss=81.4039
	step [13/249], loss=65.2465
	step [14/249], loss=85.8680
	step [15/249], loss=67.2148
	step [16/249], loss=86.0012
	step [17/249], loss=58.8105
	step [18/249], loss=74.3159
	step [19/249], loss=75.0285
	step [20/249], loss=93.8902
	step [21/249], loss=95.9719
	step [22/249], loss=76.5175
	step [23/249], loss=94.1507
	step [24/249], loss=74.6778
	step [25/249], loss=86.9093
	step [26/249], loss=62.9222
	step [27/249], loss=80.3173
	step [28/249], loss=70.5732
	step [29/249], loss=61.6081
	step [30/249], loss=73.2872
	step [31/249], loss=63.5356
	step [32/249], loss=78.6329
	step [33/249], loss=80.2641
	step [34/249], loss=91.4291
	step [35/249], loss=69.8456
	step [36/249], loss=79.5404
	step [37/249], loss=80.5205
	step [38/249], loss=64.6021
	step [39/249], loss=83.4528
	step [40/249], loss=74.4615
	step [41/249], loss=79.4512
	step [42/249], loss=82.9166
	step [43/249], loss=64.5651
	step [44/249], loss=61.7099
	step [45/249], loss=71.5074
	step [46/249], loss=75.3929
	step [47/249], loss=73.8121
	step [48/249], loss=80.0397
	step [49/249], loss=71.9291
	step [50/249], loss=79.0894
	step [51/249], loss=62.4906
	step [52/249], loss=78.5611
	step [53/249], loss=80.2515
	step [54/249], loss=75.7702
	step [55/249], loss=66.8795
	step [56/249], loss=75.1979
	step [57/249], loss=86.0576
	step [58/249], loss=88.2165
	step [59/249], loss=82.1448
	step [60/249], loss=84.5326
	step [61/249], loss=84.5547
	step [62/249], loss=74.9582
	step [63/249], loss=82.2067
	step [64/249], loss=66.6317
	step [65/249], loss=87.5302
	step [66/249], loss=83.0408
	step [67/249], loss=62.2561
	step [68/249], loss=87.1223
	step [69/249], loss=85.7699
	step [70/249], loss=62.3632
	step [71/249], loss=76.7499
	step [72/249], loss=82.4294
	step [73/249], loss=78.3320
	step [74/249], loss=86.7028
	step [75/249], loss=83.6998
	step [76/249], loss=83.7466
	step [77/249], loss=67.1815
	step [78/249], loss=75.9154
	step [79/249], loss=75.1830
	step [80/249], loss=74.7415
	step [81/249], loss=86.1489
	step [82/249], loss=78.8742
	step [83/249], loss=81.1344
	step [84/249], loss=65.1320
	step [85/249], loss=65.4216
	step [86/249], loss=77.7266
	step [87/249], loss=54.4224
	step [88/249], loss=90.1635
	step [89/249], loss=64.3815
	step [90/249], loss=99.0187
	step [91/249], loss=72.6268
	step [92/249], loss=77.6631
	step [93/249], loss=84.1039
	step [94/249], loss=74.8319
	step [95/249], loss=56.6900
	step [96/249], loss=81.5338
	step [97/249], loss=87.0917
	step [98/249], loss=64.7190
	step [99/249], loss=82.0066
	step [100/249], loss=82.5814
	step [101/249], loss=76.2406
	step [102/249], loss=70.0273
	step [103/249], loss=71.5668
	step [104/249], loss=87.3119
	step [105/249], loss=79.8058
	step [106/249], loss=76.4329
	step [107/249], loss=73.7936
	step [108/249], loss=72.1527
	step [109/249], loss=71.8160
	step [110/249], loss=67.9013
	step [111/249], loss=72.7957
	step [112/249], loss=71.9328
	step [113/249], loss=74.7326
	step [114/249], loss=73.7272
	step [115/249], loss=68.6103
	step [116/249], loss=72.9136
	step [117/249], loss=77.5890
	step [118/249], loss=93.0717
	step [119/249], loss=84.8116
	step [120/249], loss=78.5200
	step [121/249], loss=79.8350
	step [122/249], loss=85.8243
	step [123/249], loss=76.4017
	step [124/249], loss=72.0371
	step [125/249], loss=65.0428
	step [126/249], loss=77.9797
	step [127/249], loss=70.9537
	step [128/249], loss=76.0171
	step [129/249], loss=95.5561
	step [130/249], loss=78.4907
	step [131/249], loss=74.7061
	step [132/249], loss=92.6669
	step [133/249], loss=82.9371
	step [134/249], loss=75.5063
	step [135/249], loss=89.2160
	step [136/249], loss=88.6811
	step [137/249], loss=69.8770
	step [138/249], loss=92.9239
	step [139/249], loss=73.7686
	step [140/249], loss=86.8874
	step [141/249], loss=74.4745
	step [142/249], loss=75.4438
	step [143/249], loss=84.2975
	step [144/249], loss=85.1216
	step [145/249], loss=91.1482
	step [146/249], loss=85.4352
	step [147/249], loss=77.7857
	step [148/249], loss=56.9550
	step [149/249], loss=63.1641
	step [150/249], loss=77.1046
	step [151/249], loss=71.2407
	step [152/249], loss=76.2352
	step [153/249], loss=60.6339
	step [154/249], loss=72.7052
	step [155/249], loss=58.0237
	step [156/249], loss=61.9437
	step [157/249], loss=86.9601
	step [158/249], loss=77.4624
	step [159/249], loss=74.8321
	step [160/249], loss=85.3073
	step [161/249], loss=67.7992
	step [162/249], loss=83.6577
	step [163/249], loss=70.6916
	step [164/249], loss=82.2715
	step [165/249], loss=69.3653
	step [166/249], loss=75.8776
	step [167/249], loss=60.3390
	step [168/249], loss=88.7517
	step [169/249], loss=80.8154
	step [170/249], loss=83.1303
	step [171/249], loss=81.9197
	step [172/249], loss=64.9985
	step [173/249], loss=79.4716
	step [174/249], loss=68.9189
	step [175/249], loss=72.9166
	step [176/249], loss=74.0294
	step [177/249], loss=78.9592
	step [178/249], loss=90.4010
	step [179/249], loss=60.8885
	step [180/249], loss=80.9081
	step [181/249], loss=87.5215
	step [182/249], loss=78.5206
	step [183/249], loss=71.4788
	step [184/249], loss=71.9181
	step [185/249], loss=82.8650
	step [186/249], loss=71.1263
	step [187/249], loss=75.8906
	step [188/249], loss=60.8214
	step [189/249], loss=73.3576
	step [190/249], loss=79.9287
	step [191/249], loss=77.1201
	step [192/249], loss=82.3136
	step [193/249], loss=65.0205
	step [194/249], loss=105.4057
	step [195/249], loss=68.8499
	step [196/249], loss=77.5436
	step [197/249], loss=91.8869
	step [198/249], loss=74.1148
	step [199/249], loss=69.0877
	step [200/249], loss=57.0087
	step [201/249], loss=98.9064
	step [202/249], loss=83.2649
	step [203/249], loss=65.1515
	step [204/249], loss=63.7038
	step [205/249], loss=73.1167
	step [206/249], loss=67.3712
	step [207/249], loss=75.4247
	step [208/249], loss=87.2246
	step [209/249], loss=79.7778
	step [210/249], loss=79.0349
	step [211/249], loss=81.3548
	step [212/249], loss=79.8221
	step [213/249], loss=75.9319
	step [214/249], loss=94.6933
	step [215/249], loss=70.5282
	step [216/249], loss=68.5999
	step [217/249], loss=79.6032
	step [218/249], loss=91.9630
	step [219/249], loss=74.1508
	step [220/249], loss=74.9426
	step [221/249], loss=77.4931
	step [222/249], loss=103.9755
	step [223/249], loss=80.3502
	step [224/249], loss=75.7505
	step [225/249], loss=73.8048
	step [226/249], loss=68.3780
	step [227/249], loss=68.9605
	step [228/249], loss=84.1963
	step [229/249], loss=78.5640
	step [230/249], loss=76.2369
	step [231/249], loss=87.9384
	step [232/249], loss=71.1819
	step [233/249], loss=77.4804
	step [234/249], loss=69.7600
	step [235/249], loss=70.8058
	step [236/249], loss=71.7208
	step [237/249], loss=77.9702
	step [238/249], loss=74.4813
	step [239/249], loss=84.5425
	step [240/249], loss=92.7433
	step [241/249], loss=76.2825
	step [242/249], loss=86.6815
	step [243/249], loss=74.2318
	step [244/249], loss=73.6002
	step [245/249], loss=76.4220
	step [246/249], loss=81.6272
	step [247/249], loss=64.8878
	step [248/249], loss=76.6864
	step [249/249], loss=62.1032
	Evaluating
	loss=0.0063, precision=0.3521, recall=0.8474, f1=0.4975
Training epoch 80
	step [1/249], loss=73.6833
	step [2/249], loss=61.2828
	step [3/249], loss=72.1369
	step [4/249], loss=56.1517
	step [5/249], loss=49.0307
	step [6/249], loss=81.2650
	step [7/249], loss=71.9308
	step [8/249], loss=77.4188
	step [9/249], loss=72.2649
	step [10/249], loss=72.0185
	step [11/249], loss=72.2955
	step [12/249], loss=66.8614
	step [13/249], loss=78.7395
	step [14/249], loss=78.6874
	step [15/249], loss=85.2473
	step [16/249], loss=84.7610
	step [17/249], loss=79.9026
	step [18/249], loss=63.5554
	step [19/249], loss=81.0754
	step [20/249], loss=72.3249
	step [21/249], loss=82.8972
	step [22/249], loss=68.1109
	step [23/249], loss=78.4270
	step [24/249], loss=82.2826
	step [25/249], loss=63.6803
	step [26/249], loss=69.3120
	step [27/249], loss=66.9482
	step [28/249], loss=72.2347
	step [29/249], loss=72.3151
	step [30/249], loss=85.5728
	step [31/249], loss=83.8478
	step [32/249], loss=76.9582
	step [33/249], loss=72.5948
	step [34/249], loss=83.3905
	step [35/249], loss=79.8860
	step [36/249], loss=78.5187
	step [37/249], loss=72.2967
	step [38/249], loss=83.8348
	step [39/249], loss=79.3962
	step [40/249], loss=80.7257
	step [41/249], loss=64.5938
	step [42/249], loss=81.3741
	step [43/249], loss=68.7975
	step [44/249], loss=92.5183
	step [45/249], loss=90.5918
	step [46/249], loss=78.4305
	step [47/249], loss=70.2068
	step [48/249], loss=72.1996
	step [49/249], loss=74.2602
	step [50/249], loss=80.5096
	step [51/249], loss=77.2871
	step [52/249], loss=65.2285
	step [53/249], loss=79.1136
	step [54/249], loss=88.3285
	step [55/249], loss=56.6005
	step [56/249], loss=80.3289
	step [57/249], loss=67.1738
	step [58/249], loss=72.0629
	step [59/249], loss=63.3446
	step [60/249], loss=59.8313
	step [61/249], loss=73.4489
	step [62/249], loss=75.4141
	step [63/249], loss=81.2386
	step [64/249], loss=60.5997
	step [65/249], loss=81.1721
	step [66/249], loss=71.8281
	step [67/249], loss=67.7613
	step [68/249], loss=73.3275
	step [69/249], loss=72.6561
	step [70/249], loss=70.5184
	step [71/249], loss=85.8500
	step [72/249], loss=84.1371
	step [73/249], loss=64.5218
	step [74/249], loss=77.5777
	step [75/249], loss=93.7107
	step [76/249], loss=88.8133
	step [77/249], loss=86.8503
	step [78/249], loss=84.3504
	step [79/249], loss=67.3473
	step [80/249], loss=72.8363
	step [81/249], loss=85.9150
	step [82/249], loss=70.6681
	step [83/249], loss=70.4277
	step [84/249], loss=61.9887
	step [85/249], loss=64.1797
	step [86/249], loss=82.7582
	step [87/249], loss=63.2833
	step [88/249], loss=72.6608
	step [89/249], loss=71.3596
	step [90/249], loss=88.3679
	step [91/249], loss=91.6962
	step [92/249], loss=73.2812
	step [93/249], loss=85.4229
	step [94/249], loss=75.9662
	step [95/249], loss=84.6505
	step [96/249], loss=82.2415
	step [97/249], loss=80.3511
	step [98/249], loss=82.4858
	step [99/249], loss=82.9642
	step [100/249], loss=81.5328
	step [101/249], loss=91.8570
	step [102/249], loss=77.1819
	step [103/249], loss=83.7606
	step [104/249], loss=79.7167
	step [105/249], loss=76.8756
	step [106/249], loss=68.5643
	step [107/249], loss=72.3400
	step [108/249], loss=74.7106
	step [109/249], loss=54.4939
	step [110/249], loss=62.8790
	step [111/249], loss=93.7445
	step [112/249], loss=58.7536
	step [113/249], loss=72.0722
	step [114/249], loss=83.3080
	step [115/249], loss=82.6357
	step [116/249], loss=68.6298
	step [117/249], loss=87.0133
	step [118/249], loss=68.6427
	step [119/249], loss=79.2911
	step [120/249], loss=73.5113
	step [121/249], loss=79.2837
	step [122/249], loss=74.3018
	step [123/249], loss=71.9431
	step [124/249], loss=95.9622
	step [125/249], loss=74.9236
	step [126/249], loss=70.9245
	step [127/249], loss=83.4391
	step [128/249], loss=61.1562
	step [129/249], loss=78.6471
	step [130/249], loss=69.2846
	step [131/249], loss=81.9964
	step [132/249], loss=75.3483
	step [133/249], loss=83.0822
	step [134/249], loss=82.2833
	step [135/249], loss=82.7859
	step [136/249], loss=79.4483
	step [137/249], loss=94.8851
	step [138/249], loss=79.3397
	step [139/249], loss=68.9147
	step [140/249], loss=82.8348
	step [141/249], loss=77.5201
	step [142/249], loss=81.0681
	step [143/249], loss=83.1712
	step [144/249], loss=70.5820
	step [145/249], loss=74.2779
	step [146/249], loss=84.4513
	step [147/249], loss=73.1642
	step [148/249], loss=71.6258
	step [149/249], loss=77.6230
	step [150/249], loss=93.2647
	step [151/249], loss=76.8301
	step [152/249], loss=75.6087
	step [153/249], loss=82.6046
	step [154/249], loss=80.7676
	step [155/249], loss=68.1749
	step [156/249], loss=75.0017
	step [157/249], loss=74.5642
	step [158/249], loss=85.4426
	step [159/249], loss=83.7238
	step [160/249], loss=91.3008
	step [161/249], loss=73.7251
	step [162/249], loss=81.5563
	step [163/249], loss=64.9758
	step [164/249], loss=86.6054
	step [165/249], loss=58.8451
	step [166/249], loss=68.9520
	step [167/249], loss=83.5070
	step [168/249], loss=77.8335
	step [169/249], loss=74.6538
	step [170/249], loss=59.6869
	step [171/249], loss=64.8731
	step [172/249], loss=77.4490
	step [173/249], loss=75.2840
	step [174/249], loss=89.1926
	step [175/249], loss=71.5099
	step [176/249], loss=55.9454
	step [177/249], loss=77.7646
	step [178/249], loss=69.1735
	step [179/249], loss=77.0022
	step [180/249], loss=67.3176
	step [181/249], loss=68.0694
	step [182/249], loss=77.4367
	step [183/249], loss=67.9084
	step [184/249], loss=76.0067
	step [185/249], loss=86.0682
	step [186/249], loss=68.3461
	step [187/249], loss=81.3257
	step [188/249], loss=73.0923
	step [189/249], loss=79.0063
	step [190/249], loss=97.2285
	step [191/249], loss=77.8568
	step [192/249], loss=80.5842
	step [193/249], loss=69.3385
	step [194/249], loss=83.6576
	step [195/249], loss=78.7454
	step [196/249], loss=78.9897
	step [197/249], loss=89.3392
	step [198/249], loss=82.0756
	step [199/249], loss=80.9579
	step [200/249], loss=90.5094
	step [201/249], loss=77.5841
	step [202/249], loss=80.8794
	step [203/249], loss=66.3438
	step [204/249], loss=100.1072
	step [205/249], loss=95.5659
	step [206/249], loss=70.5368
	step [207/249], loss=84.8522
	step [208/249], loss=78.2969
	step [209/249], loss=73.8788
	step [210/249], loss=76.7446
	step [211/249], loss=91.0252
	step [212/249], loss=74.8711
	step [213/249], loss=74.8780
	step [214/249], loss=77.5248
	step [215/249], loss=80.1191
	step [216/249], loss=60.8576
	step [217/249], loss=74.6552
	step [218/249], loss=81.8311
	step [219/249], loss=90.7072
	step [220/249], loss=64.0854
	step [221/249], loss=71.4043
	step [222/249], loss=66.2478
	step [223/249], loss=77.4737
	step [224/249], loss=78.1115
	step [225/249], loss=89.6865
	step [226/249], loss=87.7006
	step [227/249], loss=80.8627
	step [228/249], loss=60.2580
	step [229/249], loss=79.3431
	step [230/249], loss=83.8169
	step [231/249], loss=69.0044
	step [232/249], loss=78.4848
	step [233/249], loss=68.6481
	step [234/249], loss=70.6702
	step [235/249], loss=70.7598
	step [236/249], loss=49.7397
	step [237/249], loss=84.8659
	step [238/249], loss=72.3928
	step [239/249], loss=93.8726
	step [240/249], loss=64.2328
	step [241/249], loss=76.7394
	step [242/249], loss=87.3333
	step [243/249], loss=92.1424
	step [244/249], loss=68.3762
	step [245/249], loss=75.9077
	step [246/249], loss=87.6348
	step [247/249], loss=72.5013
	step [248/249], loss=77.6366
	step [249/249], loss=50.7427
	Evaluating
	loss=0.0057, precision=0.3972, recall=0.8429, f1=0.5399
saving model as: 3_saved_model.pth
Training epoch 81
	step [1/249], loss=63.4090
	step [2/249], loss=63.9322
	step [3/249], loss=67.9729
	step [4/249], loss=73.0559
	step [5/249], loss=89.5760
	step [6/249], loss=68.1824
	step [7/249], loss=87.6594
	step [8/249], loss=82.7971
	step [9/249], loss=94.7215
	step [10/249], loss=67.8903
	step [11/249], loss=82.0444
	step [12/249], loss=83.3278
	step [13/249], loss=68.7437
	step [14/249], loss=58.7482
	step [15/249], loss=79.7726
	step [16/249], loss=72.8357
	step [17/249], loss=65.4538
	step [18/249], loss=71.2742
	step [19/249], loss=75.8310
	step [20/249], loss=104.3358
	step [21/249], loss=76.9161
	step [22/249], loss=85.3908
	step [23/249], loss=72.7004
	step [24/249], loss=84.4650
	step [25/249], loss=71.4768
	step [26/249], loss=90.9104
	step [27/249], loss=64.7773
	step [28/249], loss=82.3231
	step [29/249], loss=74.2230
	step [30/249], loss=92.4862
	step [31/249], loss=70.6128
	step [32/249], loss=74.4236
	step [33/249], loss=85.5246
	step [34/249], loss=61.5908
	step [35/249], loss=88.2165
	step [36/249], loss=78.3447
	step [37/249], loss=74.3127
	step [38/249], loss=73.9157
	step [39/249], loss=70.8775
	step [40/249], loss=73.9786
	step [41/249], loss=69.6981
	step [42/249], loss=81.8588
	step [43/249], loss=81.2292
	step [44/249], loss=81.1360
	step [45/249], loss=68.6734
	step [46/249], loss=85.9922
	step [47/249], loss=81.8443
	step [48/249], loss=58.8111
	step [49/249], loss=89.8059
	step [50/249], loss=75.7749
	step [51/249], loss=70.8812
	step [52/249], loss=72.9712
	step [53/249], loss=74.4188
	step [54/249], loss=79.4159
	step [55/249], loss=70.3057
	step [56/249], loss=85.3553
	step [57/249], loss=72.9843
	step [58/249], loss=83.7843
	step [59/249], loss=73.1133
	step [60/249], loss=76.7590
	step [61/249], loss=76.2216
	step [62/249], loss=86.2944
	step [63/249], loss=68.2012
	step [64/249], loss=85.2268
	step [65/249], loss=73.0091
	step [66/249], loss=64.2230
	step [67/249], loss=77.5090
	step [68/249], loss=94.8302
	step [69/249], loss=71.0347
	step [70/249], loss=75.4511
	step [71/249], loss=70.6884
	step [72/249], loss=75.1382
	step [73/249], loss=72.4327
	step [74/249], loss=79.2881
	step [75/249], loss=65.4055
	step [76/249], loss=58.8959
	step [77/249], loss=74.2395
	step [78/249], loss=63.2306
	step [79/249], loss=78.2027
	step [80/249], loss=75.7889
	step [81/249], loss=71.4034
	step [82/249], loss=61.9037
	step [83/249], loss=76.2235
	step [84/249], loss=57.9690
	step [85/249], loss=67.1411
	step [86/249], loss=75.5527
	step [87/249], loss=71.3359
	step [88/249], loss=62.5992
	step [89/249], loss=70.0259
	step [90/249], loss=69.8402
	step [91/249], loss=84.3825
	step [92/249], loss=76.1117
	step [93/249], loss=69.1984
	step [94/249], loss=65.8622
	step [95/249], loss=87.6462
	step [96/249], loss=69.8417
	step [97/249], loss=69.8156
	step [98/249], loss=74.0844
	step [99/249], loss=77.0860
	step [100/249], loss=82.6505
	step [101/249], loss=73.1236
	step [102/249], loss=73.6762
	step [103/249], loss=85.9572
	step [104/249], loss=88.9348
	step [105/249], loss=80.1899
	step [106/249], loss=63.9878
	step [107/249], loss=89.4947
	step [108/249], loss=101.1888
	step [109/249], loss=76.3113
	step [110/249], loss=78.7895
	step [111/249], loss=69.5938
	step [112/249], loss=84.5155
	step [113/249], loss=89.4954
	step [114/249], loss=85.6694
	step [115/249], loss=73.8767
	step [116/249], loss=63.5221
	step [117/249], loss=79.7564
	step [118/249], loss=86.0707
	step [119/249], loss=78.2069
	step [120/249], loss=96.7908
	step [121/249], loss=86.6074
	step [122/249], loss=82.3254
	step [123/249], loss=86.7146
	step [124/249], loss=71.6254
	step [125/249], loss=72.8394
	step [126/249], loss=74.1041
	step [127/249], loss=71.8862
	step [128/249], loss=84.6629
	step [129/249], loss=81.6896
	step [130/249], loss=82.9090
	step [131/249], loss=82.5202
	step [132/249], loss=75.3070
	step [133/249], loss=82.6551
	step [134/249], loss=89.4980
	step [135/249], loss=70.0045
	step [136/249], loss=82.1260
	step [137/249], loss=72.8194
	step [138/249], loss=77.7577
	step [139/249], loss=64.5369
	step [140/249], loss=88.2377
	step [141/249], loss=72.9118
	step [142/249], loss=66.4848
	step [143/249], loss=76.4710
	step [144/249], loss=85.4966
	step [145/249], loss=70.9551
	step [146/249], loss=62.1871
	step [147/249], loss=74.9688
	step [148/249], loss=70.4209
	step [149/249], loss=79.2603
	step [150/249], loss=82.3199
	step [151/249], loss=78.4498
	step [152/249], loss=71.6831
	step [153/249], loss=79.7775
	step [154/249], loss=71.6605
	step [155/249], loss=92.8276
	step [156/249], loss=72.5201
	step [157/249], loss=83.7166
	step [158/249], loss=89.4400
	step [159/249], loss=72.7652
	step [160/249], loss=89.8913
	step [161/249], loss=85.3216
	step [162/249], loss=70.5372
	step [163/249], loss=82.2073
	step [164/249], loss=69.7974
	step [165/249], loss=79.5888
	step [166/249], loss=89.2108
	step [167/249], loss=76.5966
	step [168/249], loss=67.4167
	step [169/249], loss=68.1909
	step [170/249], loss=104.7710
	step [171/249], loss=63.4837
	step [172/249], loss=75.5609
	step [173/249], loss=81.5548
	step [174/249], loss=62.8147
	step [175/249], loss=75.5535
	step [176/249], loss=72.5081
	step [177/249], loss=77.4413
	step [178/249], loss=65.7166
	step [179/249], loss=72.0595
	step [180/249], loss=70.4347
	step [181/249], loss=72.0620
	step [182/249], loss=74.1899
	step [183/249], loss=83.1168
	step [184/249], loss=77.0860
	step [185/249], loss=78.8984
	step [186/249], loss=77.0205
	step [187/249], loss=91.4664
	step [188/249], loss=72.1785
	step [189/249], loss=76.8680
	step [190/249], loss=69.4749
	step [191/249], loss=77.7641
	step [192/249], loss=62.5146
	step [193/249], loss=67.3078
	step [194/249], loss=65.5999
	step [195/249], loss=81.9270
	step [196/249], loss=82.7237
	step [197/249], loss=72.1196
	step [198/249], loss=72.8003
	step [199/249], loss=75.0326
	step [200/249], loss=77.6656
	step [201/249], loss=58.6014
	step [202/249], loss=77.4335
	step [203/249], loss=69.6804
	step [204/249], loss=85.3643
	step [205/249], loss=68.9812
	step [206/249], loss=70.6489
	step [207/249], loss=74.0139
	step [208/249], loss=80.2747
	step [209/249], loss=73.5559
	step [210/249], loss=83.9521
	step [211/249], loss=78.0058
	step [212/249], loss=86.4930
	step [213/249], loss=93.3675
	step [214/249], loss=77.4011
	step [215/249], loss=78.5303
	step [216/249], loss=84.0384
	step [217/249], loss=71.7104
	step [218/249], loss=76.1721
	step [219/249], loss=71.7996
	step [220/249], loss=90.0679
	step [221/249], loss=84.9856
	step [222/249], loss=63.3181
	step [223/249], loss=66.7961
	step [224/249], loss=61.2973
	step [225/249], loss=71.9948
	step [226/249], loss=81.7222
	step [227/249], loss=74.0812
	step [228/249], loss=83.5841
	step [229/249], loss=75.9095
	step [230/249], loss=75.6243
	step [231/249], loss=74.1659
	step [232/249], loss=88.3812
	step [233/249], loss=72.8892
	step [234/249], loss=82.5589
	step [235/249], loss=81.2747
	step [236/249], loss=67.1439
	step [237/249], loss=71.5186
	step [238/249], loss=63.8689
	step [239/249], loss=69.8277
	step [240/249], loss=66.0775
	step [241/249], loss=73.7534
	step [242/249], loss=74.5269
	step [243/249], loss=83.4695
	step [244/249], loss=77.2273
	step [245/249], loss=75.9589
	step [246/249], loss=59.1339
	step [247/249], loss=81.1626
	step [248/249], loss=78.0816
	step [249/249], loss=52.7958
	Evaluating
	loss=0.0071, precision=0.3292, recall=0.8483, f1=0.4743
Training epoch 82
	step [1/249], loss=67.5555
	step [2/249], loss=60.2414
	step [3/249], loss=96.3714
	step [4/249], loss=67.6445
	step [5/249], loss=76.1528
	step [6/249], loss=70.0099
	step [7/249], loss=73.3365
	step [8/249], loss=80.2477
	step [9/249], loss=71.4299
	step [10/249], loss=78.3603
	step [11/249], loss=77.6996
	step [12/249], loss=71.0122
	step [13/249], loss=72.4667
	step [14/249], loss=74.9032
	step [15/249], loss=76.8592
	step [16/249], loss=82.2635
	step [17/249], loss=75.3276
	step [18/249], loss=73.5591
	step [19/249], loss=85.7335
	step [20/249], loss=85.7636
	step [21/249], loss=75.1783
	step [22/249], loss=81.5940
	step [23/249], loss=63.4450
	step [24/249], loss=79.2540
	step [25/249], loss=70.9861
	step [26/249], loss=87.3780
	step [27/249], loss=68.0408
	step [28/249], loss=70.0559
	step [29/249], loss=104.8098
	step [30/249], loss=92.5240
	step [31/249], loss=80.3960
	step [32/249], loss=82.9122
	step [33/249], loss=77.8714
	step [34/249], loss=63.3897
	step [35/249], loss=75.4935
	step [36/249], loss=80.4877
	step [37/249], loss=70.8515
	step [38/249], loss=85.8786
	step [39/249], loss=73.6224
	step [40/249], loss=71.3505
	step [41/249], loss=55.4568
	step [42/249], loss=83.9891
	step [43/249], loss=69.1008
	step [44/249], loss=88.5816
	step [45/249], loss=73.9813
	step [46/249], loss=85.7029
	step [47/249], loss=77.9392
	step [48/249], loss=73.7866
	step [49/249], loss=71.4824
	step [50/249], loss=65.3773
	step [51/249], loss=74.3863
	step [52/249], loss=71.8149
	step [53/249], loss=82.7429
	step [54/249], loss=76.0426
	step [55/249], loss=74.3852
	step [56/249], loss=80.0188
	step [57/249], loss=76.7882
	step [58/249], loss=75.5730
	step [59/249], loss=69.3943
	step [60/249], loss=68.6892
	step [61/249], loss=71.8445
	step [62/249], loss=71.6464
	step [63/249], loss=72.9576
	step [64/249], loss=88.5981
	step [65/249], loss=62.6054
	step [66/249], loss=74.7856
	step [67/249], loss=69.7228
	step [68/249], loss=70.8116
	step [69/249], loss=60.4064
	step [70/249], loss=81.5040
	step [71/249], loss=79.6892
	step [72/249], loss=66.3600
	step [73/249], loss=71.1247
	step [74/249], loss=65.7850
	step [75/249], loss=75.7227
	step [76/249], loss=77.1255
	step [77/249], loss=64.7493
	step [78/249], loss=69.3658
	step [79/249], loss=81.0843
	step [80/249], loss=90.1505
	step [81/249], loss=74.1996
	step [82/249], loss=76.6098
	step [83/249], loss=76.4914
	step [84/249], loss=82.2643
	step [85/249], loss=59.6718
	step [86/249], loss=65.9023
	step [87/249], loss=59.5522
	step [88/249], loss=82.1754
	step [89/249], loss=60.1222
	step [90/249], loss=76.4891
	step [91/249], loss=69.5302
	step [92/249], loss=64.0266
	step [93/249], loss=92.3522
	step [94/249], loss=103.9987
	step [95/249], loss=81.4211
	step [96/249], loss=74.7077
	step [97/249], loss=81.4370
	step [98/249], loss=57.2885
	step [99/249], loss=68.9130
	step [100/249], loss=74.2986
	step [101/249], loss=84.7379
	step [102/249], loss=93.5555
	step [103/249], loss=82.6310
	step [104/249], loss=79.8464
	step [105/249], loss=69.6564
	step [106/249], loss=83.0962
	step [107/249], loss=64.7840
	step [108/249], loss=70.9142
	step [109/249], loss=76.6545
	step [110/249], loss=75.0714
	step [111/249], loss=80.2712
	step [112/249], loss=75.1753
	step [113/249], loss=73.9271
	step [114/249], loss=76.7726
	step [115/249], loss=78.0674
	step [116/249], loss=70.3747
	step [117/249], loss=74.2782
	step [118/249], loss=74.3479
	step [119/249], loss=86.6559
	step [120/249], loss=86.8734
	step [121/249], loss=79.7143
	step [122/249], loss=83.8722
	step [123/249], loss=91.1020
	step [124/249], loss=78.1751
	step [125/249], loss=76.3719
	step [126/249], loss=82.6866
	step [127/249], loss=61.4732
	step [128/249], loss=62.9356
	step [129/249], loss=63.5048
	step [130/249], loss=68.2403
	step [131/249], loss=86.9140
	step [132/249], loss=86.7477
	step [133/249], loss=78.7728
	step [134/249], loss=72.1747
	step [135/249], loss=66.1894
	step [136/249], loss=70.5482
	step [137/249], loss=79.8952
	step [138/249], loss=72.5505
	step [139/249], loss=60.9573
	step [140/249], loss=78.6526
	step [141/249], loss=79.3249
	step [142/249], loss=78.2382
	step [143/249], loss=73.2319
	step [144/249], loss=81.0307
	step [145/249], loss=102.7348
	step [146/249], loss=72.8812
	step [147/249], loss=79.8660
	step [148/249], loss=82.4260
	step [149/249], loss=73.9404
	step [150/249], loss=63.0395
	step [151/249], loss=62.2250
	step [152/249], loss=82.7222
	step [153/249], loss=83.1583
	step [154/249], loss=74.8962
	step [155/249], loss=74.3791
	step [156/249], loss=76.5195
	step [157/249], loss=83.8063
	step [158/249], loss=65.0597
	step [159/249], loss=73.2860
	step [160/249], loss=88.4580
	step [161/249], loss=87.5270
	step [162/249], loss=79.2356
	step [163/249], loss=95.1296
	step [164/249], loss=66.2995
	step [165/249], loss=64.3694
	step [166/249], loss=75.2350
	step [167/249], loss=82.0561
	step [168/249], loss=85.7490
	step [169/249], loss=74.0721
	step [170/249], loss=70.9646
	step [171/249], loss=81.7626
	step [172/249], loss=78.7730
	step [173/249], loss=83.2279
	step [174/249], loss=77.2727
	step [175/249], loss=93.2393
	step [176/249], loss=75.3391
	step [177/249], loss=88.9873
	step [178/249], loss=71.4971
	step [179/249], loss=53.9364
	step [180/249], loss=53.1131
	step [181/249], loss=68.0671
	step [182/249], loss=64.8500
	step [183/249], loss=73.1196
	step [184/249], loss=68.7796
	step [185/249], loss=90.3995
	step [186/249], loss=65.7663
	step [187/249], loss=87.4043
	step [188/249], loss=84.1738
	step [189/249], loss=76.2574
	step [190/249], loss=76.7212
	step [191/249], loss=78.9744
	step [192/249], loss=70.5250
	step [193/249], loss=73.1686
	step [194/249], loss=86.8716
	step [195/249], loss=89.6724
	step [196/249], loss=69.0107
	step [197/249], loss=68.3510
	step [198/249], loss=73.7185
	step [199/249], loss=57.1632
	step [200/249], loss=72.4561
	step [201/249], loss=59.6875
	step [202/249], loss=81.9018
	step [203/249], loss=77.4536
	step [204/249], loss=88.1441
	step [205/249], loss=67.9330
	step [206/249], loss=79.1063
	step [207/249], loss=84.2804
	step [208/249], loss=90.0305
	step [209/249], loss=89.4881
	step [210/249], loss=64.9868
	step [211/249], loss=75.6934
	step [212/249], loss=94.8015
	step [213/249], loss=93.6080
	step [214/249], loss=75.3725
	step [215/249], loss=68.6255
	step [216/249], loss=71.2413
	step [217/249], loss=80.3812
	step [218/249], loss=71.5121
	step [219/249], loss=73.3130
	step [220/249], loss=85.1190
	step [221/249], loss=75.7029
	step [222/249], loss=74.6772
	step [223/249], loss=71.9804
	step [224/249], loss=82.4344
	step [225/249], loss=67.2434
	step [226/249], loss=85.0473
	step [227/249], loss=81.5584
	step [228/249], loss=76.6142
	step [229/249], loss=81.7627
	step [230/249], loss=74.4051
	step [231/249], loss=80.8665
	step [232/249], loss=85.4355
	step [233/249], loss=64.5364
	step [234/249], loss=80.6196
	step [235/249], loss=72.2279
	step [236/249], loss=80.1966
	step [237/249], loss=80.6055
	step [238/249], loss=82.9139
	step [239/249], loss=90.7521
	step [240/249], loss=76.1486
	step [241/249], loss=75.8856
	step [242/249], loss=56.0100
	step [243/249], loss=91.6651
	step [244/249], loss=81.8152
	step [245/249], loss=71.8790
	step [246/249], loss=74.7035
	step [247/249], loss=72.7104
	step [248/249], loss=58.2454
	step [249/249], loss=40.3004
	Evaluating
	loss=0.0061, precision=0.3673, recall=0.8419, f1=0.5115
Training epoch 83
	step [1/249], loss=73.4408
	step [2/249], loss=81.7150
	step [3/249], loss=67.2641
	step [4/249], loss=74.6229
	step [5/249], loss=88.6201
	step [6/249], loss=66.8703
	step [7/249], loss=71.6467
	step [8/249], loss=73.3171
	step [9/249], loss=88.9740
	step [10/249], loss=88.6192
	step [11/249], loss=93.3943
	step [12/249], loss=72.2501
	step [13/249], loss=89.5491
	step [14/249], loss=80.5682
	step [15/249], loss=84.3083
	step [16/249], loss=84.2948
	step [17/249], loss=88.6921
	step [18/249], loss=71.7828
	step [19/249], loss=71.8242
	step [20/249], loss=67.0766
	step [21/249], loss=89.1176
	step [22/249], loss=64.7456
	step [23/249], loss=72.8290
	step [24/249], loss=82.3501
	step [25/249], loss=69.7430
	step [26/249], loss=79.0431
	step [27/249], loss=82.2655
	step [28/249], loss=97.2328
	step [29/249], loss=77.1809
	step [30/249], loss=65.2473
	step [31/249], loss=78.7538
	step [32/249], loss=66.7113
	step [33/249], loss=76.7279
	step [34/249], loss=76.7877
	step [35/249], loss=83.7819
	step [36/249], loss=91.9182
	step [37/249], loss=87.2365
	step [38/249], loss=75.3612
	step [39/249], loss=68.5519
	step [40/249], loss=71.2044
	step [41/249], loss=75.0794
	step [42/249], loss=95.3476
	step [43/249], loss=84.1425
	step [44/249], loss=76.5604
	step [45/249], loss=79.9488
	step [46/249], loss=81.9930
	step [47/249], loss=75.3086
	step [48/249], loss=78.2848
	step [49/249], loss=87.4355
	step [50/249], loss=89.4696
	step [51/249], loss=83.3376
	step [52/249], loss=79.4350
	step [53/249], loss=73.2682
	step [54/249], loss=83.8493
	step [55/249], loss=75.8170
	step [56/249], loss=80.4527
	step [57/249], loss=74.9047
	step [58/249], loss=61.0407
	step [59/249], loss=83.8333
	step [60/249], loss=76.0286
	step [61/249], loss=61.7306
	step [62/249], loss=69.2864
	step [63/249], loss=67.7181
	step [64/249], loss=81.5493
	step [65/249], loss=81.5410
	step [66/249], loss=69.7772
	step [67/249], loss=81.4131
	step [68/249], loss=69.8102
	step [69/249], loss=70.8176
	step [70/249], loss=78.8852
	step [71/249], loss=78.8143
	step [72/249], loss=68.7285
	step [73/249], loss=68.7022
	step [74/249], loss=70.3638
	step [75/249], loss=75.3341
	step [76/249], loss=86.8447
	step [77/249], loss=77.8219
	step [78/249], loss=88.0865
	step [79/249], loss=89.0716
	step [80/249], loss=88.7513
	step [81/249], loss=73.6050
	step [82/249], loss=78.9661
	step [83/249], loss=80.0780
	step [84/249], loss=77.4538
	step [85/249], loss=78.0250
	step [86/249], loss=70.3210
	step [87/249], loss=86.0798
	step [88/249], loss=76.3420
	step [89/249], loss=78.2462
	step [90/249], loss=70.3031
	step [91/249], loss=84.9691
	step [92/249], loss=65.1347
	step [93/249], loss=65.3132
	step [94/249], loss=83.4688
	step [95/249], loss=64.4853
	step [96/249], loss=67.7542
	step [97/249], loss=71.5972
	step [98/249], loss=87.8417
	step [99/249], loss=84.0624
	step [100/249], loss=58.7741
	step [101/249], loss=70.1538
	step [102/249], loss=74.2466
	step [103/249], loss=91.5323
	step [104/249], loss=67.8171
	step [105/249], loss=60.3458
	step [106/249], loss=82.0447
	step [107/249], loss=71.6197
	step [108/249], loss=57.4957
	step [109/249], loss=75.7756
	step [110/249], loss=66.8271
	step [111/249], loss=70.3355
	step [112/249], loss=72.6105
	step [113/249], loss=71.8099
	step [114/249], loss=61.5302
	step [115/249], loss=66.2403
	step [116/249], loss=72.1786
	step [117/249], loss=61.1955
	step [118/249], loss=69.7496
	step [119/249], loss=70.7267
	step [120/249], loss=77.1686
	step [121/249], loss=82.2348
	step [122/249], loss=78.4422
	step [123/249], loss=75.7716
	step [124/249], loss=67.7627
	step [125/249], loss=63.2433
	step [126/249], loss=92.2167
	step [127/249], loss=87.6855
	step [128/249], loss=65.5104
	step [129/249], loss=66.4552
	step [130/249], loss=80.3013
	step [131/249], loss=76.1086
	step [132/249], loss=79.1354
	step [133/249], loss=75.8683
	step [134/249], loss=87.7188
	step [135/249], loss=68.7606
	step [136/249], loss=72.2739
	step [137/249], loss=76.5042
	step [138/249], loss=80.0608
	step [139/249], loss=80.3014
	step [140/249], loss=78.1348
	step [141/249], loss=74.6200
	step [142/249], loss=59.8282
	step [143/249], loss=105.5666
	step [144/249], loss=75.2476
	step [145/249], loss=72.6133
	step [146/249], loss=79.6884
	step [147/249], loss=80.3020
	step [148/249], loss=81.5913
	step [149/249], loss=86.9453
	step [150/249], loss=86.7712
	step [151/249], loss=80.2841
	step [152/249], loss=64.3278
	step [153/249], loss=79.5331
	step [154/249], loss=91.7147
	step [155/249], loss=69.8523
	step [156/249], loss=67.0681
	step [157/249], loss=59.7446
	step [158/249], loss=77.2260
	step [159/249], loss=96.5495
	step [160/249], loss=78.0782
	step [161/249], loss=70.9232
	step [162/249], loss=63.3263
	step [163/249], loss=84.3645
	step [164/249], loss=74.1570
	step [165/249], loss=83.3718
	step [166/249], loss=80.7641
	step [167/249], loss=71.7377
	step [168/249], loss=80.6187
	step [169/249], loss=50.9366
	step [170/249], loss=68.1094
	step [171/249], loss=74.0559
	step [172/249], loss=74.9269
	step [173/249], loss=64.4079
	step [174/249], loss=68.9205
	step [175/249], loss=67.5784
	step [176/249], loss=74.2385
	step [177/249], loss=62.8582
	step [178/249], loss=58.7855
	step [179/249], loss=80.3788
	step [180/249], loss=92.4756
	step [181/249], loss=83.4349
	step [182/249], loss=94.0225
	step [183/249], loss=60.1574
	step [184/249], loss=81.8388
	step [185/249], loss=81.6315
	step [186/249], loss=73.0405
	step [187/249], loss=48.2448
	step [188/249], loss=78.4435
	step [189/249], loss=71.7999
	step [190/249], loss=82.2890
	step [191/249], loss=76.6131
	step [192/249], loss=68.8129
	step [193/249], loss=86.3183
	step [194/249], loss=60.3580
	step [195/249], loss=90.2961
	step [196/249], loss=54.8302
	step [197/249], loss=74.8930
	step [198/249], loss=75.6964
	step [199/249], loss=76.4332
	step [200/249], loss=76.5705
	step [201/249], loss=76.5030
	step [202/249], loss=81.5638
	step [203/249], loss=79.9254
	step [204/249], loss=85.9029
	step [205/249], loss=76.8288
	step [206/249], loss=84.7924
	step [207/249], loss=61.0153
	step [208/249], loss=77.7439
	step [209/249], loss=77.2777
	step [210/249], loss=57.0372
	step [211/249], loss=100.3697
	step [212/249], loss=87.6933
	step [213/249], loss=86.2518
	step [214/249], loss=76.9034
	step [215/249], loss=58.9721
	step [216/249], loss=74.6437
	step [217/249], loss=75.5881
	step [218/249], loss=72.5889
	step [219/249], loss=77.9309
	step [220/249], loss=64.4216
	step [221/249], loss=75.8237
	step [222/249], loss=90.9793
	step [223/249], loss=65.3523
	step [224/249], loss=72.4587
	step [225/249], loss=53.7956
	step [226/249], loss=86.7043
	step [227/249], loss=72.0413
	step [228/249], loss=60.6597
	step [229/249], loss=71.7810
	step [230/249], loss=82.0852
	step [231/249], loss=77.2026
	step [232/249], loss=77.4653
	step [233/249], loss=82.0905
	step [234/249], loss=85.6837
	step [235/249], loss=66.1617
	step [236/249], loss=76.9001
	step [237/249], loss=69.7035
	step [238/249], loss=66.5642
	step [239/249], loss=72.4359
	step [240/249], loss=81.4423
	step [241/249], loss=71.8817
	step [242/249], loss=77.8956
	step [243/249], loss=85.3098
	step [244/249], loss=72.8254
	step [245/249], loss=81.7148
	step [246/249], loss=79.7030
	step [247/249], loss=77.0320
	step [248/249], loss=64.3544
	step [249/249], loss=61.4849
	Evaluating
	loss=0.0060, precision=0.3712, recall=0.8492, f1=0.5166
Training epoch 84
	step [1/249], loss=63.2002
	step [2/249], loss=76.5917
	step [3/249], loss=78.5179
	step [4/249], loss=87.3295
	step [5/249], loss=70.3388
	step [6/249], loss=67.5953
	step [7/249], loss=74.9485
	step [8/249], loss=70.6172
	step [9/249], loss=76.6772
	step [10/249], loss=68.7537
	step [11/249], loss=77.1772
	step [12/249], loss=68.7192
	step [13/249], loss=92.1836
	step [14/249], loss=78.3545
	step [15/249], loss=82.7713
	step [16/249], loss=69.6659
	step [17/249], loss=72.6714
	step [18/249], loss=67.0939
	step [19/249], loss=84.4119
	step [20/249], loss=75.6988
	step [21/249], loss=74.4295
	step [22/249], loss=71.2121
	step [23/249], loss=75.1287
	step [24/249], loss=83.1814
	step [25/249], loss=62.3484
	step [26/249], loss=89.4986
	step [27/249], loss=67.9166
	step [28/249], loss=88.5739
	step [29/249], loss=68.0055
	step [30/249], loss=89.6958
	step [31/249], loss=85.2211
	step [32/249], loss=78.5093
	step [33/249], loss=79.0593
	step [34/249], loss=90.5143
	step [35/249], loss=65.6652
	step [36/249], loss=87.5323
	step [37/249], loss=72.0374
	step [38/249], loss=88.1205
	step [39/249], loss=74.9634
	step [40/249], loss=69.1989
	step [41/249], loss=65.9631
	step [42/249], loss=84.0657
	step [43/249], loss=73.6594
	step [44/249], loss=69.9553
	step [45/249], loss=69.5221
	step [46/249], loss=73.8278
	step [47/249], loss=78.8136
	step [48/249], loss=80.5249
	step [49/249], loss=95.9952
	step [50/249], loss=94.2388
	step [51/249], loss=85.6216
	step [52/249], loss=63.8013
	step [53/249], loss=75.2435
	step [54/249], loss=83.0282
	step [55/249], loss=55.4973
	step [56/249], loss=64.3551
	step [57/249], loss=78.5749
	step [58/249], loss=74.8795
	step [59/249], loss=80.0456
	step [60/249], loss=65.2978
	step [61/249], loss=85.1420
	step [62/249], loss=61.1941
	step [63/249], loss=83.0128
	step [64/249], loss=75.1535
	step [65/249], loss=82.1255
	step [66/249], loss=86.8201
	step [67/249], loss=79.1111
	step [68/249], loss=73.2863
	step [69/249], loss=77.0699
	step [70/249], loss=84.2283
	step [71/249], loss=79.3092
	step [72/249], loss=82.6286
	step [73/249], loss=84.8698
	step [74/249], loss=89.5242
	step [75/249], loss=78.5813
	step [76/249], loss=89.5675
	step [77/249], loss=68.1176
	step [78/249], loss=63.3313
	step [79/249], loss=89.0771
	step [80/249], loss=76.4089
	step [81/249], loss=66.8195
	step [82/249], loss=85.7146
	step [83/249], loss=74.9062
	step [84/249], loss=65.4028
	step [85/249], loss=68.5552
	step [86/249], loss=57.5044
	step [87/249], loss=99.5330
	step [88/249], loss=86.8412
	step [89/249], loss=74.9511
	step [90/249], loss=81.6729
	step [91/249], loss=78.7601
	step [92/249], loss=69.5907
	step [93/249], loss=67.6851
	step [94/249], loss=69.2524
	step [95/249], loss=85.3881
	step [96/249], loss=86.1876
	step [97/249], loss=65.5074
	step [98/249], loss=86.7727
	step [99/249], loss=71.5834
	step [100/249], loss=85.2535
	step [101/249], loss=83.2668
	step [102/249], loss=58.9264
	step [103/249], loss=60.2802
	step [104/249], loss=77.2888
	step [105/249], loss=79.8740
	step [106/249], loss=66.6479
	step [107/249], loss=87.8019
	step [108/249], loss=83.6338
	step [109/249], loss=79.6700
	step [110/249], loss=47.3617
	step [111/249], loss=66.6970
	step [112/249], loss=97.9861
	step [113/249], loss=72.2131
	step [114/249], loss=86.1522
	step [115/249], loss=69.7878
	step [116/249], loss=73.7593
	step [117/249], loss=88.9371
	step [118/249], loss=81.9310
	step [119/249], loss=65.0909
	step [120/249], loss=85.4161
	step [121/249], loss=70.8728
	step [122/249], loss=70.8622
	step [123/249], loss=70.5823
	step [124/249], loss=65.5270
	step [125/249], loss=77.5318
	step [126/249], loss=81.3940
	step [127/249], loss=63.3474
	step [128/249], loss=70.7098
	step [129/249], loss=98.5525
	step [130/249], loss=71.6164
	step [131/249], loss=85.2956
	step [132/249], loss=68.7224
	step [133/249], loss=77.5617
	step [134/249], loss=69.0893
	step [135/249], loss=61.4036
	step [136/249], loss=68.4485
	step [137/249], loss=75.9812
	step [138/249], loss=82.1885
	step [139/249], loss=84.4240
	step [140/249], loss=60.8366
	step [141/249], loss=67.8454
	step [142/249], loss=67.4303
	step [143/249], loss=74.7279
	step [144/249], loss=80.8046
	step [145/249], loss=63.5187
	step [146/249], loss=73.2069
	step [147/249], loss=74.7502
	step [148/249], loss=69.9936
	step [149/249], loss=66.0026
	step [150/249], loss=86.2575
	step [151/249], loss=52.3236
	step [152/249], loss=88.3246
	step [153/249], loss=81.4541
	step [154/249], loss=78.7636
	step [155/249], loss=91.3327
	step [156/249], loss=81.6871
	step [157/249], loss=73.9710
	step [158/249], loss=62.4026
	step [159/249], loss=52.7350
	step [160/249], loss=72.9780
	step [161/249], loss=70.0389
	step [162/249], loss=81.5837
	step [163/249], loss=69.7618
	step [164/249], loss=90.6856
	step [165/249], loss=73.0466
	step [166/249], loss=79.1093
	step [167/249], loss=81.0630
	step [168/249], loss=66.7634
	step [169/249], loss=65.2852
	step [170/249], loss=66.8645
	step [171/249], loss=78.7087
	step [172/249], loss=73.6129
	step [173/249], loss=60.8706
	step [174/249], loss=59.6868
	step [175/249], loss=72.0993
	step [176/249], loss=80.8921
	step [177/249], loss=77.3747
	step [178/249], loss=79.3957
	step [179/249], loss=83.7544
	step [180/249], loss=70.1342
	step [181/249], loss=80.9402
	step [182/249], loss=58.8174
	step [183/249], loss=78.5392
	step [184/249], loss=93.7633
	step [185/249], loss=78.2336
	step [186/249], loss=69.5803
	step [187/249], loss=73.1569
	step [188/249], loss=71.8909
	step [189/249], loss=79.5677
	step [190/249], loss=62.2423
	step [191/249], loss=70.3262
	step [192/249], loss=97.2640
	step [193/249], loss=91.3892
	step [194/249], loss=74.7234
	step [195/249], loss=69.5906
	step [196/249], loss=74.9156
	step [197/249], loss=80.1982
	step [198/249], loss=69.5534
	step [199/249], loss=72.5569
	step [200/249], loss=80.3897
	step [201/249], loss=71.1002
	step [202/249], loss=57.5752
	step [203/249], loss=94.0122
	step [204/249], loss=59.1706
	step [205/249], loss=77.3998
	step [206/249], loss=78.7914
	step [207/249], loss=88.6328
	step [208/249], loss=71.7014
	step [209/249], loss=95.9720
	step [210/249], loss=81.3219
	step [211/249], loss=72.4218
	step [212/249], loss=80.1714
	step [213/249], loss=69.8617
	step [214/249], loss=77.8156
	step [215/249], loss=78.3661
	step [216/249], loss=63.9081
	step [217/249], loss=77.6904
	step [218/249], loss=72.0681
	step [219/249], loss=69.8769
	step [220/249], loss=81.3718
	step [221/249], loss=62.7941
	step [222/249], loss=65.1042
	step [223/249], loss=70.0200
	step [224/249], loss=83.4899
	step [225/249], loss=71.5549
	step [226/249], loss=73.5276
	step [227/249], loss=74.6603
	step [228/249], loss=65.9317
	step [229/249], loss=82.1021
	step [230/249], loss=65.0532
	step [231/249], loss=73.2217
	step [232/249], loss=70.8080
	step [233/249], loss=71.9620
	step [234/249], loss=86.9760
	step [235/249], loss=54.3953
	step [236/249], loss=72.7215
	step [237/249], loss=74.5282
	step [238/249], loss=76.5140
	step [239/249], loss=86.4865
	step [240/249], loss=69.9559
	step [241/249], loss=89.0915
	step [242/249], loss=78.7124
	step [243/249], loss=78.1265
	step [244/249], loss=66.4055
	step [245/249], loss=88.3677
	step [246/249], loss=86.4604
	step [247/249], loss=66.6445
	step [248/249], loss=72.2796
	step [249/249], loss=61.1825
	Evaluating
	loss=0.0070, precision=0.3295, recall=0.8555, f1=0.4758
Training epoch 85
	step [1/249], loss=68.5649
	step [2/249], loss=73.9009
	step [3/249], loss=83.4654
	step [4/249], loss=89.3746
	step [5/249], loss=66.2817
	step [6/249], loss=58.7874
	step [7/249], loss=68.7234
	step [8/249], loss=78.2576
	step [9/249], loss=60.7623
	step [10/249], loss=79.3301
	step [11/249], loss=72.0160
	step [12/249], loss=76.0219
	step [13/249], loss=71.0281
	step [14/249], loss=80.9628
	step [15/249], loss=63.6120
	step [16/249], loss=72.1414
	step [17/249], loss=74.4904
	step [18/249], loss=59.0100
	step [19/249], loss=69.2276
	step [20/249], loss=71.4525
	step [21/249], loss=80.0714
	step [22/249], loss=73.7987
	step [23/249], loss=79.1208
	step [24/249], loss=82.4997
	step [25/249], loss=70.7576
	step [26/249], loss=76.0469
	step [27/249], loss=79.2498
	step [28/249], loss=71.9944
	step [29/249], loss=83.4129
	step [30/249], loss=55.2388
	step [31/249], loss=64.3919
	step [32/249], loss=71.0769
	step [33/249], loss=70.0420
	step [34/249], loss=80.4123
	step [35/249], loss=67.1597
	step [36/249], loss=79.4968
	step [37/249], loss=85.1071
	step [38/249], loss=72.8069
	step [39/249], loss=79.5905
	step [40/249], loss=75.2655
	step [41/249], loss=75.2835
	step [42/249], loss=77.3286
	step [43/249], loss=75.8761
	step [44/249], loss=73.4421
	step [45/249], loss=86.3557
	step [46/249], loss=74.1031
	step [47/249], loss=60.2598
	step [48/249], loss=65.0711
	step [49/249], loss=64.7603
	step [50/249], loss=83.6288
	step [51/249], loss=68.9941
	step [52/249], loss=88.2723
	step [53/249], loss=66.1118
	step [54/249], loss=69.2163
	step [55/249], loss=80.1357
	step [56/249], loss=79.3693
	step [57/249], loss=70.2724
	step [58/249], loss=56.3909
	step [59/249], loss=64.7468
	step [60/249], loss=87.1019
	step [61/249], loss=68.8572
	step [62/249], loss=76.6694
	step [63/249], loss=78.8264
	step [64/249], loss=78.3946
	step [65/249], loss=81.7663
	step [66/249], loss=103.5571
	step [67/249], loss=91.2153
	step [68/249], loss=89.7693
	step [69/249], loss=83.6298
	step [70/249], loss=84.1813
	step [71/249], loss=88.8060
	step [72/249], loss=66.1725
	step [73/249], loss=74.7131
	step [74/249], loss=81.8526
	step [75/249], loss=67.9900
	step [76/249], loss=73.4260
	step [77/249], loss=73.0616
	step [78/249], loss=67.7096
	step [79/249], loss=79.3035
	step [80/249], loss=74.1529
	step [81/249], loss=77.8715
	step [82/249], loss=84.0034
	step [83/249], loss=85.2357
	step [84/249], loss=70.8349
	step [85/249], loss=77.2625
	step [86/249], loss=82.0936
	step [87/249], loss=70.9439
	step [88/249], loss=80.6557
	step [89/249], loss=65.3765
	step [90/249], loss=68.9437
	step [91/249], loss=65.8640
	step [92/249], loss=69.6894
	step [93/249], loss=78.8790
	step [94/249], loss=71.3556
	step [95/249], loss=80.6354
	step [96/249], loss=79.2273
	step [97/249], loss=91.8289
	step [98/249], loss=61.7227
	step [99/249], loss=86.4562
	step [100/249], loss=60.9347
	step [101/249], loss=75.2708
	step [102/249], loss=97.8463
	step [103/249], loss=82.3869
	step [104/249], loss=77.6338
	step [105/249], loss=79.8149
	step [106/249], loss=94.6313
	step [107/249], loss=70.6107
	step [108/249], loss=72.0779
	step [109/249], loss=65.6515
	step [110/249], loss=58.5364
	step [111/249], loss=94.2896
	step [112/249], loss=65.3547
	step [113/249], loss=76.1721
	step [114/249], loss=79.0093
	step [115/249], loss=71.7625
	step [116/249], loss=58.6636
	step [117/249], loss=74.2365
	step [118/249], loss=82.7075
	step [119/249], loss=75.4735
	step [120/249], loss=66.5898
	step [121/249], loss=63.0181
	step [122/249], loss=55.9182
	step [123/249], loss=79.8612
	step [124/249], loss=78.6596
	step [125/249], loss=74.5758
	step [126/249], loss=74.5563
	step [127/249], loss=63.4351
	step [128/249], loss=80.4916
	step [129/249], loss=77.9198
	step [130/249], loss=64.4941
	step [131/249], loss=66.0727
	step [132/249], loss=86.1596
	step [133/249], loss=68.8869
	step [134/249], loss=79.6917
	step [135/249], loss=79.9392
	step [136/249], loss=88.2728
	step [137/249], loss=76.2446
	step [138/249], loss=73.7176
	step [139/249], loss=72.7388
	step [140/249], loss=68.2171
	step [141/249], loss=65.1592
	step [142/249], loss=73.3340
	step [143/249], loss=69.4310
	step [144/249], loss=69.7157
	step [145/249], loss=69.4640
	step [146/249], loss=72.3634
	step [147/249], loss=65.0543
	step [148/249], loss=57.7224
	step [149/249], loss=80.4285
	step [150/249], loss=91.4708
	step [151/249], loss=91.1713
	step [152/249], loss=74.4696
	step [153/249], loss=65.8669
	step [154/249], loss=68.4148
	step [155/249], loss=71.7565
	step [156/249], loss=74.6052
	step [157/249], loss=83.8342
	step [158/249], loss=88.9011
	step [159/249], loss=70.3099
	step [160/249], loss=87.7758
	step [161/249], loss=65.9448
	step [162/249], loss=72.2925
	step [163/249], loss=86.9573
	step [164/249], loss=68.5203
	step [165/249], loss=67.3900
	step [166/249], loss=83.0436
	step [167/249], loss=84.1548
	step [168/249], loss=62.5583
	step [169/249], loss=76.2802
	step [170/249], loss=73.5447
	step [171/249], loss=72.9083
	step [172/249], loss=79.8192
	step [173/249], loss=81.4040
	step [174/249], loss=82.0257
	step [175/249], loss=64.5549
	step [176/249], loss=71.1310
	step [177/249], loss=69.0090
	step [178/249], loss=75.0722
	step [179/249], loss=102.4386
	step [180/249], loss=90.6843
	step [181/249], loss=93.0092
	step [182/249], loss=83.4319
	step [183/249], loss=72.9668
	step [184/249], loss=69.9887
	step [185/249], loss=70.5590
	step [186/249], loss=88.0839
	step [187/249], loss=90.2662
	step [188/249], loss=64.7543
	step [189/249], loss=80.5600
	step [190/249], loss=88.9157
	step [191/249], loss=87.1633
	step [192/249], loss=55.6741
	step [193/249], loss=77.6079
	step [194/249], loss=75.4731
	step [195/249], loss=79.3297
	step [196/249], loss=74.9483
	step [197/249], loss=76.4495
	step [198/249], loss=68.2007
	step [199/249], loss=72.4728
	step [200/249], loss=61.5125
	step [201/249], loss=86.5961
	step [202/249], loss=65.5177
	step [203/249], loss=69.2710
	step [204/249], loss=74.0016
	step [205/249], loss=80.0834
	step [206/249], loss=78.7831
	step [207/249], loss=73.9718
	step [208/249], loss=63.0065
	step [209/249], loss=73.6114
	step [210/249], loss=86.8351
	step [211/249], loss=64.7509
	step [212/249], loss=74.0977
	step [213/249], loss=63.4289
	step [214/249], loss=64.9187
	step [215/249], loss=73.0070
	step [216/249], loss=87.6802
	step [217/249], loss=84.0818
	step [218/249], loss=59.8924
	step [219/249], loss=75.2653
	step [220/249], loss=85.7305
	step [221/249], loss=81.6533
	step [222/249], loss=75.1939
	step [223/249], loss=71.2063
	step [224/249], loss=81.6271
	step [225/249], loss=72.7571
	step [226/249], loss=76.0864
	step [227/249], loss=87.8102
	step [228/249], loss=82.2674
	step [229/249], loss=89.6341
	step [230/249], loss=72.3935
	step [231/249], loss=64.7387
	step [232/249], loss=71.1065
	step [233/249], loss=72.1359
	step [234/249], loss=75.3886
	step [235/249], loss=73.4785
	step [236/249], loss=77.4036
	step [237/249], loss=76.1613
	step [238/249], loss=77.6743
	step [239/249], loss=61.3955
	step [240/249], loss=84.3854
	step [241/249], loss=70.3050
	step [242/249], loss=74.4244
	step [243/249], loss=77.7373
	step [244/249], loss=83.6709
	step [245/249], loss=83.6751
	step [246/249], loss=92.8273
	step [247/249], loss=63.7397
	step [248/249], loss=68.8674
	step [249/249], loss=47.3403
	Evaluating
	loss=0.0065, precision=0.3486, recall=0.8509, f1=0.4946
Training epoch 86
	step [1/249], loss=76.0607
	step [2/249], loss=72.3763
	step [3/249], loss=74.2712
	step [4/249], loss=72.9425
	step [5/249], loss=72.4322
	step [6/249], loss=54.1098
	step [7/249], loss=79.1659
	step [8/249], loss=71.1647
	step [9/249], loss=88.3178
	step [10/249], loss=90.4818
	step [11/249], loss=68.2663
	step [12/249], loss=70.2625
	step [13/249], loss=73.0938
	step [14/249], loss=75.8989
	step [15/249], loss=78.8586
	step [16/249], loss=67.8858
	step [17/249], loss=66.7065
	step [18/249], loss=77.6183
	step [19/249], loss=83.1376
	step [20/249], loss=75.6279
	step [21/249], loss=79.2068
	step [22/249], loss=72.0610
	step [23/249], loss=91.7945
	step [24/249], loss=80.1976
	step [25/249], loss=76.9758
	step [26/249], loss=97.0651
	step [27/249], loss=82.3472
	step [28/249], loss=76.0523
	step [29/249], loss=65.7371
	step [30/249], loss=61.5090
	step [31/249], loss=74.2964
	step [32/249], loss=61.4947
	step [33/249], loss=69.3867
	step [34/249], loss=60.9193
	step [35/249], loss=87.8153
	step [36/249], loss=63.5454
	step [37/249], loss=67.5651
	step [38/249], loss=90.2828
	step [39/249], loss=72.5197
	step [40/249], loss=71.8954
	step [41/249], loss=81.6081
	step [42/249], loss=76.6278
	step [43/249], loss=76.5989
	step [44/249], loss=81.5163
	step [45/249], loss=73.6108
	step [46/249], loss=69.5511
	step [47/249], loss=77.3420
	step [48/249], loss=56.0622
	step [49/249], loss=69.7441
	step [50/249], loss=70.6429
	step [51/249], loss=81.4468
	step [52/249], loss=80.6313
	step [53/249], loss=75.7881
	step [54/249], loss=77.9682
	step [55/249], loss=81.5208
	step [56/249], loss=81.7637
	step [57/249], loss=71.5066
	step [58/249], loss=67.5456
	step [59/249], loss=76.3530
	step [60/249], loss=84.7479
	step [61/249], loss=65.3269
	step [62/249], loss=78.5383
	step [63/249], loss=77.8295
	step [64/249], loss=78.4021
	step [65/249], loss=70.8442
	step [66/249], loss=67.5979
	step [67/249], loss=81.2966
	step [68/249], loss=73.7580
	step [69/249], loss=68.5682
	step [70/249], loss=74.2236
	step [71/249], loss=78.8652
	step [72/249], loss=73.0036
	step [73/249], loss=93.0814
	step [74/249], loss=66.1447
	step [75/249], loss=73.6792
	step [76/249], loss=80.5227
	step [77/249], loss=86.6491
	step [78/249], loss=70.8873
	step [79/249], loss=79.0941
	step [80/249], loss=87.4033
	step [81/249], loss=70.0868
	step [82/249], loss=65.6521
	step [83/249], loss=76.8135
	step [84/249], loss=82.4316
	step [85/249], loss=73.4350
	step [86/249], loss=56.9114
	step [87/249], loss=71.6113
	step [88/249], loss=74.8674
	step [89/249], loss=73.9490
	step [90/249], loss=62.6581
	step [91/249], loss=77.6986
	step [92/249], loss=71.0978
	step [93/249], loss=77.8063
	step [94/249], loss=88.4254
	step [95/249], loss=87.3423
	step [96/249], loss=61.7196
	step [97/249], loss=74.4127
	step [98/249], loss=74.1373
	step [99/249], loss=73.1781
	step [100/249], loss=51.8236
	step [101/249], loss=70.0725
	step [102/249], loss=76.5866
	step [103/249], loss=66.9008
	step [104/249], loss=79.8485
	step [105/249], loss=73.0825
	step [106/249], loss=75.6266
	step [107/249], loss=64.7709
	step [108/249], loss=69.7138
	step [109/249], loss=79.3765
	step [110/249], loss=58.4199
	step [111/249], loss=87.6287
	step [112/249], loss=83.6225
	step [113/249], loss=77.8828
	step [114/249], loss=80.6900
	step [115/249], loss=57.2468
	step [116/249], loss=79.6047
	step [117/249], loss=68.4956
	step [118/249], loss=69.0238
	step [119/249], loss=81.4538
	step [120/249], loss=86.0851
	step [121/249], loss=72.8459
	step [122/249], loss=70.8488
	step [123/249], loss=85.8320
	step [124/249], loss=75.8156
	step [125/249], loss=75.2100
	step [126/249], loss=70.2852
	step [127/249], loss=67.6829
	step [128/249], loss=85.2252
	step [129/249], loss=81.3673
	step [130/249], loss=75.5137
	step [131/249], loss=70.8104
	step [132/249], loss=69.6195
	step [133/249], loss=68.9225
	step [134/249], loss=67.1986
	step [135/249], loss=70.0093
	step [136/249], loss=63.2023
	step [137/249], loss=71.4352
	step [138/249], loss=73.9997
	step [139/249], loss=63.9396
	step [140/249], loss=78.8646
	step [141/249], loss=77.8132
	step [142/249], loss=67.3149
	step [143/249], loss=87.9302
	step [144/249], loss=64.7729
	step [145/249], loss=73.4160
	step [146/249], loss=75.1027
	step [147/249], loss=69.7015
	step [148/249], loss=67.8730
	step [149/249], loss=71.0596
	step [150/249], loss=85.7157
	step [151/249], loss=65.8874
	step [152/249], loss=67.0980
	step [153/249], loss=76.5896
	step [154/249], loss=92.4610
	step [155/249], loss=60.9561
	step [156/249], loss=76.9996
	step [157/249], loss=69.0936
	step [158/249], loss=76.2771
	step [159/249], loss=74.6792
	step [160/249], loss=81.1212
	step [161/249], loss=72.1546
	step [162/249], loss=79.0511
	step [163/249], loss=84.6960
	step [164/249], loss=86.7664
	step [165/249], loss=87.3117
	step [166/249], loss=62.4050
	step [167/249], loss=70.2951
	step [168/249], loss=79.8649
	step [169/249], loss=85.4410
	step [170/249], loss=63.4575
	step [171/249], loss=86.6738
	step [172/249], loss=93.1788
	step [173/249], loss=73.8224
	step [174/249], loss=70.2258
	step [175/249], loss=78.7935
	step [176/249], loss=77.0078
	step [177/249], loss=79.5686
	step [178/249], loss=74.4413
	step [179/249], loss=68.9739
	step [180/249], loss=84.5643
	step [181/249], loss=55.8768
	step [182/249], loss=83.6410
	step [183/249], loss=77.3878
	step [184/249], loss=75.7706
	step [185/249], loss=81.8228
	step [186/249], loss=69.7029
	step [187/249], loss=82.7462
	step [188/249], loss=88.2144
	step [189/249], loss=73.8269
	step [190/249], loss=70.7372
	step [191/249], loss=81.8048
	step [192/249], loss=70.5584
	step [193/249], loss=70.1187
	step [194/249], loss=67.8706
	step [195/249], loss=56.9578
	step [196/249], loss=66.1809
	step [197/249], loss=67.6867
	step [198/249], loss=74.3762
	step [199/249], loss=77.2415
	step [200/249], loss=85.9482
	step [201/249], loss=68.2315
	step [202/249], loss=94.1613
	step [203/249], loss=83.7541
	step [204/249], loss=67.9687
	step [205/249], loss=84.9908
	step [206/249], loss=69.7769
	step [207/249], loss=63.5166
	step [208/249], loss=67.7841
	step [209/249], loss=67.5231
	step [210/249], loss=50.9866
	step [211/249], loss=74.4734
	step [212/249], loss=81.4693
	step [213/249], loss=62.3583
	step [214/249], loss=88.7126
	step [215/249], loss=65.3130
	step [216/249], loss=84.3079
	step [217/249], loss=65.2731
	step [218/249], loss=83.1784
	step [219/249], loss=82.2818
	step [220/249], loss=70.9718
	step [221/249], loss=82.2637
	step [222/249], loss=68.5753
	step [223/249], loss=84.8198
	step [224/249], loss=80.4888
	step [225/249], loss=77.8789
	step [226/249], loss=75.7408
	step [227/249], loss=87.2769
	step [228/249], loss=65.6409
	step [229/249], loss=84.4598
	step [230/249], loss=80.5007
	step [231/249], loss=67.7708
	step [232/249], loss=80.9603
	step [233/249], loss=76.5625
	step [234/249], loss=89.7483
	step [235/249], loss=86.2793
	step [236/249], loss=68.3504
	step [237/249], loss=78.1293
	step [238/249], loss=87.7234
	step [239/249], loss=75.2917
	step [240/249], loss=59.3048
	step [241/249], loss=72.3585
	step [242/249], loss=80.6310
	step [243/249], loss=70.2338
	step [244/249], loss=70.6033
	step [245/249], loss=76.4219
	step [246/249], loss=65.0467
	step [247/249], loss=74.4124
	step [248/249], loss=71.6669
	step [249/249], loss=63.2555
	Evaluating
	loss=0.0063, precision=0.3589, recall=0.8478, f1=0.5043
Training epoch 87
	step [1/249], loss=67.2863
	step [2/249], loss=72.1449
	step [3/249], loss=97.4813
	step [4/249], loss=67.4977
	step [5/249], loss=64.9809
	step [6/249], loss=61.5355
	step [7/249], loss=51.6869
	step [8/249], loss=72.4605
	step [9/249], loss=80.8233
	step [10/249], loss=72.5320
	step [11/249], loss=76.9228
	step [12/249], loss=70.2630
	step [13/249], loss=65.9207
	step [14/249], loss=81.3660
	step [15/249], loss=73.6259
	step [16/249], loss=77.7195
	step [17/249], loss=68.0717
	step [18/249], loss=88.9631
	step [19/249], loss=81.9126
	step [20/249], loss=63.1375
	step [21/249], loss=64.1683
	step [22/249], loss=72.8928
	step [23/249], loss=81.9176
	step [24/249], loss=73.0356
	step [25/249], loss=84.5244
	step [26/249], loss=79.2437
	step [27/249], loss=83.4560
	step [28/249], loss=82.1346
	step [29/249], loss=75.7774
	step [30/249], loss=63.3316
	step [31/249], loss=93.8962
	step [32/249], loss=73.2543
	step [33/249], loss=66.5700
	step [34/249], loss=71.5762
	step [35/249], loss=70.0242
	step [36/249], loss=67.9410
	step [37/249], loss=81.3200
	step [38/249], loss=69.7404
	step [39/249], loss=62.1032
	step [40/249], loss=70.9015
	step [41/249], loss=77.1137
	step [42/249], loss=72.7754
	step [43/249], loss=81.5576
	step [44/249], loss=84.2740
	step [45/249], loss=70.4958
	step [46/249], loss=76.9409
	step [47/249], loss=75.3982
	step [48/249], loss=67.8512
	step [49/249], loss=60.5021
	step [50/249], loss=67.8131
	step [51/249], loss=80.7009
	step [52/249], loss=76.7886
	step [53/249], loss=81.9577
	step [54/249], loss=81.3622
	step [55/249], loss=66.6548
	step [56/249], loss=80.3256
	step [57/249], loss=76.8705
	step [58/249], loss=71.3978
	step [59/249], loss=87.5277
	step [60/249], loss=62.5103
	step [61/249], loss=71.5973
	step [62/249], loss=79.4135
	step [63/249], loss=63.4392
	step [64/249], loss=80.5528
	step [65/249], loss=71.1119
	step [66/249], loss=72.7616
	step [67/249], loss=88.7770
	step [68/249], loss=66.1355
	step [69/249], loss=70.6342
	step [70/249], loss=80.1199
	step [71/249], loss=83.4019
	step [72/249], loss=57.5187
	step [73/249], loss=91.4610
	step [74/249], loss=97.3994
	step [75/249], loss=70.4033
	step [76/249], loss=75.8055
	step [77/249], loss=81.8173
	step [78/249], loss=77.0013
	step [79/249], loss=74.4018
	step [80/249], loss=68.4896
	step [81/249], loss=74.7670
	step [82/249], loss=75.7198
	step [83/249], loss=83.9310
	step [84/249], loss=84.2188
	step [85/249], loss=81.4692
	step [86/249], loss=87.6079
	step [87/249], loss=73.4085
	step [88/249], loss=88.4526
	step [89/249], loss=62.1595
	step [90/249], loss=84.9684
	step [91/249], loss=83.4349
	step [92/249], loss=68.1199
	step [93/249], loss=89.8774
	step [94/249], loss=69.4889
	step [95/249], loss=69.5968
	step [96/249], loss=69.4123
	step [97/249], loss=72.6913
	step [98/249], loss=60.9776
	step [99/249], loss=52.8027
	step [100/249], loss=70.0646
	step [101/249], loss=71.3414
	step [102/249], loss=80.3451
	step [103/249], loss=66.2633
	step [104/249], loss=65.7628
	step [105/249], loss=67.6642
	step [106/249], loss=57.4881
	step [107/249], loss=52.7140
	step [108/249], loss=57.0865
	step [109/249], loss=72.2058
	step [110/249], loss=82.2367
	step [111/249], loss=70.1123
	step [112/249], loss=57.3116
	step [113/249], loss=77.0805
	step [114/249], loss=72.9094
	step [115/249], loss=75.3950
	step [116/249], loss=59.6905
	step [117/249], loss=84.1064
	step [118/249], loss=71.4381
	step [119/249], loss=68.5264
	step [120/249], loss=82.8848
	step [121/249], loss=74.6624
	step [122/249], loss=80.4332
	step [123/249], loss=77.3372
	step [124/249], loss=75.4701
	step [125/249], loss=83.0240
	step [126/249], loss=67.9772
	step [127/249], loss=65.4214
	step [128/249], loss=70.4663
	step [129/249], loss=73.5263
	step [130/249], loss=78.0531
	step [131/249], loss=84.0329
	step [132/249], loss=70.9145
	step [133/249], loss=70.2583
	step [134/249], loss=68.8450
	step [135/249], loss=63.8665
	step [136/249], loss=68.0373
	step [137/249], loss=85.9989
	step [138/249], loss=63.3120
	step [139/249], loss=71.1432
	step [140/249], loss=64.6393
	step [141/249], loss=80.6315
	step [142/249], loss=58.1298
	step [143/249], loss=71.3408
	step [144/249], loss=105.6110
	step [145/249], loss=77.1608
	step [146/249], loss=86.0779
	step [147/249], loss=79.7822
	step [148/249], loss=78.8057
	step [149/249], loss=82.3055
	step [150/249], loss=65.9106
	step [151/249], loss=82.6499
	step [152/249], loss=73.5388
	step [153/249], loss=72.3013
	step [154/249], loss=78.1250
	step [155/249], loss=61.6747
	step [156/249], loss=71.5491
	step [157/249], loss=70.6523
	step [158/249], loss=92.9673
	step [159/249], loss=75.4074
	step [160/249], loss=74.1428
	step [161/249], loss=71.7296
	step [162/249], loss=70.2065
	step [163/249], loss=67.3783
	step [164/249], loss=76.5492
	step [165/249], loss=75.9003
	step [166/249], loss=67.6684
	step [167/249], loss=93.8001
	step [168/249], loss=80.2050
	step [169/249], loss=83.2017
	step [170/249], loss=70.8694
	step [171/249], loss=83.8410
	step [172/249], loss=69.2052
	step [173/249], loss=82.5345
	step [174/249], loss=78.9375
	step [175/249], loss=77.8598
	step [176/249], loss=81.4826
	step [177/249], loss=82.9112
	step [178/249], loss=90.2407
	step [179/249], loss=58.3009
	step [180/249], loss=107.3489
	step [181/249], loss=76.5487
	step [182/249], loss=78.0117
	step [183/249], loss=61.6194
	step [184/249], loss=73.9184
	step [185/249], loss=62.1094
	step [186/249], loss=81.0545
	step [187/249], loss=82.4629
	step [188/249], loss=69.2498
	step [189/249], loss=69.4609
	step [190/249], loss=77.3086
	step [191/249], loss=80.5944
	step [192/249], loss=80.5810
	step [193/249], loss=67.7156
	step [194/249], loss=91.5219
	step [195/249], loss=78.2325
	step [196/249], loss=81.7441
	step [197/249], loss=70.3648
	step [198/249], loss=96.2346
	step [199/249], loss=82.8717
	step [200/249], loss=76.0552
	step [201/249], loss=68.6971
	step [202/249], loss=61.5606
	step [203/249], loss=79.5053
	step [204/249], loss=81.0098
	step [205/249], loss=86.0246
	step [206/249], loss=64.0162
	step [207/249], loss=76.0191
	step [208/249], loss=92.0375
	step [209/249], loss=49.8609
	step [210/249], loss=75.8579
	step [211/249], loss=70.3716
	step [212/249], loss=67.6136
	step [213/249], loss=71.8956
	step [214/249], loss=72.8090
	step [215/249], loss=64.6995
	step [216/249], loss=72.3226
	step [217/249], loss=77.2648
	step [218/249], loss=66.9324
	step [219/249], loss=75.3844
	step [220/249], loss=84.2477
	step [221/249], loss=78.3297
	step [222/249], loss=75.7362
	step [223/249], loss=96.5776
	step [224/249], loss=83.6351
	step [225/249], loss=75.3892
	step [226/249], loss=65.4041
	step [227/249], loss=74.4079
	step [228/249], loss=57.4567
	step [229/249], loss=83.6337
	step [230/249], loss=77.1611
	step [231/249], loss=69.0062
	step [232/249], loss=69.0807
	step [233/249], loss=72.1484
	step [234/249], loss=85.0449
	step [235/249], loss=79.7606
	step [236/249], loss=74.4161
	step [237/249], loss=77.5854
	step [238/249], loss=70.3058
	step [239/249], loss=68.6593
	step [240/249], loss=76.6960
	step [241/249], loss=71.8468
	step [242/249], loss=64.9547
	step [243/249], loss=73.7467
	step [244/249], loss=79.0910
	step [245/249], loss=84.5379
	step [246/249], loss=95.1737
	step [247/249], loss=68.3641
	step [248/249], loss=78.8988
	step [249/249], loss=52.8649
	Evaluating
	loss=0.0066, precision=0.3487, recall=0.8432, f1=0.4934
Training epoch 88
	step [1/249], loss=87.7729
	step [2/249], loss=63.9440
	step [3/249], loss=76.8098
	step [4/249], loss=95.3677
	step [5/249], loss=70.6674
	step [6/249], loss=78.4397
	step [7/249], loss=75.8595
	step [8/249], loss=75.4639
	step [9/249], loss=59.4482
	step [10/249], loss=80.7589
	step [11/249], loss=94.6292
	step [12/249], loss=73.3677
	step [13/249], loss=83.0739
	step [14/249], loss=72.3824
	step [15/249], loss=87.3555
	step [16/249], loss=89.5159
	step [17/249], loss=71.3614
	step [18/249], loss=79.7485
	step [19/249], loss=80.0596
	step [20/249], loss=69.2767
	step [21/249], loss=63.8933
	step [22/249], loss=82.1233
	step [23/249], loss=59.2497
	step [24/249], loss=82.8767
	step [25/249], loss=74.7369
	step [26/249], loss=85.9644
	step [27/249], loss=71.3091
	step [28/249], loss=68.6915
	step [29/249], loss=69.9561
	step [30/249], loss=71.8260
	step [31/249], loss=66.3295
	step [32/249], loss=73.1922
	step [33/249], loss=77.7649
	step [34/249], loss=78.8266
	step [35/249], loss=67.0347
	step [36/249], loss=89.3610
	step [37/249], loss=63.5378
	step [38/249], loss=69.5537
	step [39/249], loss=60.8699
	step [40/249], loss=89.9332
	step [41/249], loss=63.6753
	step [42/249], loss=72.9546
	step [43/249], loss=74.5374
	step [44/249], loss=65.9229
	step [45/249], loss=80.7401
	step [46/249], loss=75.0991
	step [47/249], loss=78.9331
	step [48/249], loss=67.5615
	step [49/249], loss=74.9805
	step [50/249], loss=80.2304
	step [51/249], loss=73.4369
	step [52/249], loss=64.6306
	step [53/249], loss=78.1496
	step [54/249], loss=85.3117
	step [55/249], loss=65.7281
	step [56/249], loss=75.8289
	step [57/249], loss=78.5186
	step [58/249], loss=80.2844
	step [59/249], loss=70.3468
	step [60/249], loss=79.8707
	step [61/249], loss=71.7723
	step [62/249], loss=89.2144
	step [63/249], loss=61.1780
	step [64/249], loss=83.0703
	step [65/249], loss=84.9454
	step [66/249], loss=65.6948
	step [67/249], loss=62.0101
	step [68/249], loss=75.9336
	step [69/249], loss=68.0677
	step [70/249], loss=79.2660
	step [71/249], loss=59.4687
	step [72/249], loss=73.1053
	step [73/249], loss=85.4834
	step [74/249], loss=64.9370
	step [75/249], loss=71.0442
	step [76/249], loss=73.7983
	step [77/249], loss=66.3724
	step [78/249], loss=70.7312
	step [79/249], loss=74.8355
	step [80/249], loss=75.7301
	step [81/249], loss=81.6404
	step [82/249], loss=85.6249
	step [83/249], loss=70.3841
	step [84/249], loss=67.4821
	step [85/249], loss=64.5370
	step [86/249], loss=81.8301
	step [87/249], loss=78.3618
	step [88/249], loss=77.2621
	step [89/249], loss=65.0637
	step [90/249], loss=80.9769
	step [91/249], loss=75.7735
	step [92/249], loss=63.9932
	step [93/249], loss=73.6102
	step [94/249], loss=81.8629
	step [95/249], loss=87.3419
	step [96/249], loss=68.4507
	step [97/249], loss=76.5109
	step [98/249], loss=81.9972
	step [99/249], loss=65.6183
	step [100/249], loss=83.6870
	step [101/249], loss=53.4405
	step [102/249], loss=74.8828
	step [103/249], loss=69.6368
	step [104/249], loss=66.4412
	step [105/249], loss=71.7225
	step [106/249], loss=69.2340
	step [107/249], loss=76.2295
	step [108/249], loss=52.3716
	step [109/249], loss=71.5869
	step [110/249], loss=75.4020
	step [111/249], loss=88.1087
	step [112/249], loss=86.2627
	step [113/249], loss=72.4669
	step [114/249], loss=86.4456
	step [115/249], loss=67.4381
	step [116/249], loss=63.7569
	step [117/249], loss=76.9425
	step [118/249], loss=88.2835
	step [119/249], loss=76.6305
	step [120/249], loss=55.9659
	step [121/249], loss=70.1187
	step [122/249], loss=62.0986
	step [123/249], loss=66.9816
	step [124/249], loss=79.1662
	step [125/249], loss=68.6149
	step [126/249], loss=74.2160
	step [127/249], loss=95.0793
	step [128/249], loss=78.3879
	step [129/249], loss=81.2344
	step [130/249], loss=81.6842
	step [131/249], loss=67.4693
	step [132/249], loss=79.5164
	step [133/249], loss=60.8047
	step [134/249], loss=82.6076
	step [135/249], loss=61.0673
	step [136/249], loss=62.8854
	step [137/249], loss=74.5348
	step [138/249], loss=52.0349
	step [139/249], loss=69.9759
	step [140/249], loss=83.8752
	step [141/249], loss=76.6362
	step [142/249], loss=76.1350
	step [143/249], loss=63.4838
	step [144/249], loss=66.4956
	step [145/249], loss=69.7470
	step [146/249], loss=87.9971
	step [147/249], loss=85.7538
	step [148/249], loss=76.6742
	step [149/249], loss=106.2144
	step [150/249], loss=81.9629
	step [151/249], loss=80.2857
	step [152/249], loss=84.7557
	step [153/249], loss=72.2381
	step [154/249], loss=73.2101
	step [155/249], loss=55.1108
	step [156/249], loss=72.0224
	step [157/249], loss=80.5504
	step [158/249], loss=74.1430
	step [159/249], loss=65.1055
	step [160/249], loss=69.2682
	step [161/249], loss=74.0173
	step [162/249], loss=58.0015
	step [163/249], loss=87.9095
	step [164/249], loss=77.3356
	step [165/249], loss=75.1155
	step [166/249], loss=76.3877
	step [167/249], loss=73.4223
	step [168/249], loss=78.4165
	step [169/249], loss=77.9707
	step [170/249], loss=74.1876
	step [171/249], loss=81.8741
	step [172/249], loss=65.7141
	step [173/249], loss=73.0336
	step [174/249], loss=90.8573
	step [175/249], loss=71.0325
	step [176/249], loss=78.5762
	step [177/249], loss=80.7149
	step [178/249], loss=75.4399
	step [179/249], loss=78.8516
	step [180/249], loss=87.3453
	step [181/249], loss=62.5119
	step [182/249], loss=62.7245
	step [183/249], loss=70.1211
	step [184/249], loss=85.2026
	step [185/249], loss=72.0989
	step [186/249], loss=70.6349
	step [187/249], loss=69.1993
	step [188/249], loss=59.8361
	step [189/249], loss=92.7005
	step [190/249], loss=78.6846
	step [191/249], loss=66.1897
	step [192/249], loss=71.4693
	step [193/249], loss=78.0739
	step [194/249], loss=74.1839
	step [195/249], loss=78.0856
	step [196/249], loss=86.6542
	step [197/249], loss=69.7087
	step [198/249], loss=63.4594
	step [199/249], loss=78.4533
	step [200/249], loss=73.5630
	step [201/249], loss=72.7538
	step [202/249], loss=86.6129
	step [203/249], loss=89.2124
	step [204/249], loss=78.9841
	step [205/249], loss=72.2083
	step [206/249], loss=87.0205
	step [207/249], loss=69.8536
	step [208/249], loss=65.8996
	step [209/249], loss=92.2341
	step [210/249], loss=66.6820
	step [211/249], loss=71.2021
	step [212/249], loss=74.6502
	step [213/249], loss=66.4112
	step [214/249], loss=72.7915
	step [215/249], loss=73.1279
	step [216/249], loss=60.4380
	step [217/249], loss=87.2651
	step [218/249], loss=83.3315
	step [219/249], loss=69.7403
	step [220/249], loss=70.4885
	step [221/249], loss=66.9659
	step [222/249], loss=67.1915
	step [223/249], loss=70.0400
	step [224/249], loss=64.1291
	step [225/249], loss=60.1261
	step [226/249], loss=67.5201
	step [227/249], loss=72.7693
	step [228/249], loss=55.9861
	step [229/249], loss=110.6073
	step [230/249], loss=82.7644
	step [231/249], loss=88.5557
	step [232/249], loss=68.0607
	step [233/249], loss=69.1746
	step [234/249], loss=73.8585
	step [235/249], loss=65.6026
	step [236/249], loss=70.8705
	step [237/249], loss=65.1648
	step [238/249], loss=65.7566
	step [239/249], loss=73.4393
	step [240/249], loss=69.9719
	step [241/249], loss=74.9221
	step [242/249], loss=91.2965
	step [243/249], loss=75.5627
	step [244/249], loss=77.8786
	step [245/249], loss=81.6804
	step [246/249], loss=71.0344
	step [247/249], loss=81.7568
	step [248/249], loss=76.1034
	step [249/249], loss=50.6531
	Evaluating
	loss=0.0062, precision=0.3596, recall=0.8454, f1=0.5045
Training epoch 89
	step [1/249], loss=56.9723
	step [2/249], loss=75.5050
	step [3/249], loss=79.4458
	step [4/249], loss=82.0216
	step [5/249], loss=80.5629
	step [6/249], loss=82.5110
	step [7/249], loss=58.2142
	step [8/249], loss=90.4760
	step [9/249], loss=66.6399
	step [10/249], loss=78.5672
	step [11/249], loss=75.2107
	step [12/249], loss=66.8864
	step [13/249], loss=59.6282
	step [14/249], loss=75.6752
	step [15/249], loss=66.8457
	step [16/249], loss=67.1489
	step [17/249], loss=57.4732
	step [18/249], loss=67.3604
	step [19/249], loss=66.6150
	step [20/249], loss=62.5548
	step [21/249], loss=73.7605
	step [22/249], loss=76.3688
	step [23/249], loss=82.2464
	step [24/249], loss=65.4079
	step [25/249], loss=65.3768
	step [26/249], loss=63.9694
	step [27/249], loss=73.3577
	step [28/249], loss=69.9866
	step [29/249], loss=75.2256
	step [30/249], loss=77.2683
	step [31/249], loss=67.2099
	step [32/249], loss=84.5176
	step [33/249], loss=68.0624
	step [34/249], loss=88.8301
	step [35/249], loss=77.8792
	step [36/249], loss=67.3856
	step [37/249], loss=100.1869
	step [38/249], loss=62.0414
	step [39/249], loss=62.0039
	step [40/249], loss=62.8456
	step [41/249], loss=76.4076
	step [42/249], loss=69.4878
	step [43/249], loss=83.1993
	step [44/249], loss=61.5501
	step [45/249], loss=84.1453
	step [46/249], loss=59.3842
	step [47/249], loss=73.3357
	step [48/249], loss=72.4560
	step [49/249], loss=69.1963
	step [50/249], loss=77.6550
	step [51/249], loss=71.0045
	step [52/249], loss=79.1435
	step [53/249], loss=76.7963
	step [54/249], loss=77.5870
	step [55/249], loss=64.8345
	step [56/249], loss=61.9192
	step [57/249], loss=76.3462
	step [58/249], loss=66.7985
	step [59/249], loss=65.4320
	step [60/249], loss=85.8024
	step [61/249], loss=84.3099
	step [62/249], loss=68.6734
	step [63/249], loss=84.6165
	step [64/249], loss=80.0684
	step [65/249], loss=79.6866
	step [66/249], loss=70.6214
	step [67/249], loss=84.8255
	step [68/249], loss=69.5003
	step [69/249], loss=66.1307
	step [70/249], loss=76.0834
	step [71/249], loss=70.2453
	step [72/249], loss=72.8107
	step [73/249], loss=68.3201
	step [74/249], loss=75.2404
	step [75/249], loss=63.2140
	step [76/249], loss=68.7525
	step [77/249], loss=78.5792
	step [78/249], loss=84.1456
	step [79/249], loss=76.4636
	step [80/249], loss=78.1379
	step [81/249], loss=73.5438
	step [82/249], loss=93.9053
	step [83/249], loss=87.4545
	step [84/249], loss=75.3421
	step [85/249], loss=93.4721
	step [86/249], loss=76.1398
	step [87/249], loss=70.9597
	step [88/249], loss=69.9806
	step [89/249], loss=65.8332
	step [90/249], loss=63.6395
	step [91/249], loss=72.2281
	step [92/249], loss=65.9225
	step [93/249], loss=60.1387
	step [94/249], loss=68.9036
	step [95/249], loss=82.5570
	step [96/249], loss=67.3809
	step [97/249], loss=57.2974
	step [98/249], loss=106.4708
	step [99/249], loss=71.5345
	step [100/249], loss=79.3351
	step [101/249], loss=80.1674
	step [102/249], loss=78.3244
	step [103/249], loss=92.4065
	step [104/249], loss=59.3938
	step [105/249], loss=79.2989
	step [106/249], loss=76.4831
	step [107/249], loss=64.6758
	step [108/249], loss=66.8777
	step [109/249], loss=82.6405
	step [110/249], loss=70.5835
	step [111/249], loss=107.7402
	step [112/249], loss=92.2158
	step [113/249], loss=80.6925
	step [114/249], loss=84.9452
	step [115/249], loss=72.1037
	step [116/249], loss=67.2392
	step [117/249], loss=62.1230
	step [118/249], loss=63.4236
	step [119/249], loss=79.8345
	step [120/249], loss=63.7366
	step [121/249], loss=70.8780
	step [122/249], loss=80.5031
	step [123/249], loss=70.5502
	step [124/249], loss=66.1023
	step [125/249], loss=57.9482
	step [126/249], loss=72.2595
	step [127/249], loss=65.9505
	step [128/249], loss=94.8000
	step [129/249], loss=79.3165
	step [130/249], loss=73.3298
	step [131/249], loss=63.1601
	step [132/249], loss=81.5801
	step [133/249], loss=80.3733
	step [134/249], loss=64.2237
	step [135/249], loss=73.6100
	step [136/249], loss=76.4882
	step [137/249], loss=78.8374
	step [138/249], loss=81.6062
	step [139/249], loss=75.6086
	step [140/249], loss=76.3602
	step [141/249], loss=57.5690
	step [142/249], loss=70.4815
	step [143/249], loss=75.7945
	step [144/249], loss=68.2630
	step [145/249], loss=68.7212
	step [146/249], loss=79.9317
	step [147/249], loss=75.4025
	step [148/249], loss=73.9060
	step [149/249], loss=83.0735
	step [150/249], loss=76.5313
	step [151/249], loss=67.1172
	step [152/249], loss=58.9150
	step [153/249], loss=78.3241
	step [154/249], loss=75.6594
	step [155/249], loss=71.0984
	step [156/249], loss=66.4550
	step [157/249], loss=71.5514
	step [158/249], loss=74.9178
	step [159/249], loss=65.7161
	step [160/249], loss=71.8558
	step [161/249], loss=79.8474
	step [162/249], loss=73.5947
	step [163/249], loss=79.9493
	step [164/249], loss=89.2605
	step [165/249], loss=64.4772
	step [166/249], loss=82.4608
	step [167/249], loss=77.8165
	step [168/249], loss=70.6399
	step [169/249], loss=60.2470
	step [170/249], loss=69.2280
	step [171/249], loss=83.4931
	step [172/249], loss=84.6030
	step [173/249], loss=58.0194
	step [174/249], loss=85.0436
	step [175/249], loss=88.3401
	step [176/249], loss=71.8835
	step [177/249], loss=71.6457
	step [178/249], loss=71.5241
	step [179/249], loss=96.3915
	step [180/249], loss=75.3599
	step [181/249], loss=67.8338
	step [182/249], loss=82.9957
	step [183/249], loss=80.6786
	step [184/249], loss=67.4515
	step [185/249], loss=65.4028
	step [186/249], loss=85.0492
	step [187/249], loss=87.3306
	step [188/249], loss=81.4451
	step [189/249], loss=76.2251
	step [190/249], loss=73.0432
	step [191/249], loss=83.8443
	step [192/249], loss=78.0721
	step [193/249], loss=77.6949
	step [194/249], loss=67.4720
	step [195/249], loss=64.1177
	step [196/249], loss=85.4847
	step [197/249], loss=91.0351
	step [198/249], loss=74.5287
	step [199/249], loss=72.4802
	step [200/249], loss=80.1337
	step [201/249], loss=82.0272
	step [202/249], loss=69.9033
	step [203/249], loss=85.8897
	step [204/249], loss=81.8513
	step [205/249], loss=68.1389
	step [206/249], loss=76.5587
	step [207/249], loss=66.9908
	step [208/249], loss=80.5840
	step [209/249], loss=78.2178
	step [210/249], loss=75.9846
	step [211/249], loss=75.7687
	step [212/249], loss=76.3658
	step [213/249], loss=84.5433
	step [214/249], loss=62.2436
	step [215/249], loss=78.7333
	step [216/249], loss=75.3486
	step [217/249], loss=81.2474
	step [218/249], loss=76.1243
	step [219/249], loss=80.2804
	step [220/249], loss=66.5902
	step [221/249], loss=75.6476
	step [222/249], loss=86.1805
	step [223/249], loss=63.7572
	step [224/249], loss=70.8393
	step [225/249], loss=75.5576
	step [226/249], loss=67.4680
	step [227/249], loss=70.4863
	step [228/249], loss=78.4472
	step [229/249], loss=76.5710
	step [230/249], loss=64.8074
	step [231/249], loss=69.6185
	step [232/249], loss=83.2465
	step [233/249], loss=71.8152
	step [234/249], loss=72.2656
	step [235/249], loss=66.9678
	step [236/249], loss=82.2668
	step [237/249], loss=73.0000
	step [238/249], loss=68.8140
	step [239/249], loss=69.5210
	step [240/249], loss=49.2529
	step [241/249], loss=69.4713
	step [242/249], loss=85.4709
	step [243/249], loss=66.4280
	step [244/249], loss=67.6393
	step [245/249], loss=78.0728
	step [246/249], loss=87.5464
	step [247/249], loss=71.5760
	step [248/249], loss=82.4988
	step [249/249], loss=67.9200
	Evaluating
	loss=0.0066, precision=0.3460, recall=0.8480, f1=0.4915
Training epoch 90
	step [1/249], loss=65.6676
	step [2/249], loss=91.2870
	step [3/249], loss=77.9890
	step [4/249], loss=78.4641
	step [5/249], loss=84.3577
	step [6/249], loss=57.6573
	step [7/249], loss=72.4643
	step [8/249], loss=75.5546
	step [9/249], loss=89.7085
	step [10/249], loss=80.0424
	step [11/249], loss=73.7359
	step [12/249], loss=73.6801
	step [13/249], loss=77.2388
	step [14/249], loss=67.5582
	step [15/249], loss=64.0943
	step [16/249], loss=66.3076
	step [17/249], loss=63.3187
	step [18/249], loss=73.6736
	step [19/249], loss=62.3226
	step [20/249], loss=92.3036
	step [21/249], loss=81.0502
	step [22/249], loss=80.5605
	step [23/249], loss=75.4343
	step [24/249], loss=63.4900
	step [25/249], loss=65.9375
	step [26/249], loss=66.8730
	step [27/249], loss=69.7338
	step [28/249], loss=74.7784
	step [29/249], loss=62.6939
	step [30/249], loss=70.7027
	step [31/249], loss=76.9198
	step [32/249], loss=76.6724
	step [33/249], loss=83.9825
	step [34/249], loss=86.8484
	step [35/249], loss=78.3717
	step [36/249], loss=76.5940
	step [37/249], loss=72.5438
	step [38/249], loss=79.0764
	step [39/249], loss=77.4610
	step [40/249], loss=67.0614
	step [41/249], loss=75.2869
	step [42/249], loss=68.8860
	step [43/249], loss=76.7719
	step [44/249], loss=67.0423
	step [45/249], loss=69.8412
	step [46/249], loss=80.5821
	step [47/249], loss=80.8769
	step [48/249], loss=65.8543
	step [49/249], loss=70.2169
	step [50/249], loss=80.0984
	step [51/249], loss=57.4554
	step [52/249], loss=74.1792
	step [53/249], loss=64.5330
	step [54/249], loss=74.7609
	step [55/249], loss=75.2691
	step [56/249], loss=79.7557
	step [57/249], loss=74.6226
	step [58/249], loss=71.9573
	step [59/249], loss=79.1368
	step [60/249], loss=85.9662
	step [61/249], loss=72.8750
	step [62/249], loss=60.0021
	step [63/249], loss=77.0580
	step [64/249], loss=78.5775
	step [65/249], loss=73.9327
	step [66/249], loss=79.8027
	step [67/249], loss=82.2327
	step [68/249], loss=73.2340
	step [69/249], loss=76.0908
	step [70/249], loss=63.0541
	step [71/249], loss=76.8090
	step [72/249], loss=63.8120
	step [73/249], loss=82.6819
	step [74/249], loss=77.0834
	step [75/249], loss=64.6015
	step [76/249], loss=90.9135
	step [77/249], loss=81.6539
	step [78/249], loss=77.9150
	step [79/249], loss=74.1629
	step [80/249], loss=76.2973
	step [81/249], loss=77.8988
	step [82/249], loss=74.4406
	step [83/249], loss=73.0241
	step [84/249], loss=70.3411
	step [85/249], loss=69.1940
	step [86/249], loss=71.3521
	step [87/249], loss=65.0495
	step [88/249], loss=65.6563
	step [89/249], loss=65.2632
	step [90/249], loss=75.5779
	step [91/249], loss=66.3695
	step [92/249], loss=75.5520
	step [93/249], loss=84.7738
	step [94/249], loss=71.6174
	step [95/249], loss=60.5493
	step [96/249], loss=62.2396
	step [97/249], loss=68.3125
	step [98/249], loss=67.1414
	step [99/249], loss=75.8426
	step [100/249], loss=78.1561
	step [101/249], loss=71.2713
	step [102/249], loss=70.6995
	step [103/249], loss=59.2880
	step [104/249], loss=84.8077
	step [105/249], loss=71.1631
	step [106/249], loss=74.0507
	step [107/249], loss=65.3402
	step [108/249], loss=76.6096
	step [109/249], loss=70.3709
	step [110/249], loss=60.9335
	step [111/249], loss=71.3812
	step [112/249], loss=82.1145
	step [113/249], loss=67.6903
	step [114/249], loss=76.8856
	step [115/249], loss=69.9110
	step [116/249], loss=65.4057
	step [117/249], loss=81.7803
	step [118/249], loss=74.2662
	step [119/249], loss=85.9038
	step [120/249], loss=75.4131
	step [121/249], loss=67.6589
	step [122/249], loss=76.0197
	step [123/249], loss=92.8503
	step [124/249], loss=78.4554
	step [125/249], loss=72.6438
	step [126/249], loss=72.1329
	step [127/249], loss=66.5871
	step [128/249], loss=81.4578
	step [129/249], loss=70.9196
	step [130/249], loss=85.2862
	step [131/249], loss=80.3996
	step [132/249], loss=94.7787
	step [133/249], loss=64.1161
	step [134/249], loss=75.8125
	step [135/249], loss=70.6776
	step [136/249], loss=82.5277
	step [137/249], loss=66.5569
	step [138/249], loss=104.0154
	step [139/249], loss=77.9933
	step [140/249], loss=75.4793
	step [141/249], loss=81.6245
	step [142/249], loss=73.5702
	step [143/249], loss=59.9042
	step [144/249], loss=73.0874
	step [145/249], loss=81.1008
	step [146/249], loss=75.5910
	step [147/249], loss=75.4775
	step [148/249], loss=67.8983
	step [149/249], loss=82.7976
	step [150/249], loss=62.5220
	step [151/249], loss=77.2522
	step [152/249], loss=79.3066
	step [153/249], loss=79.4534
	step [154/249], loss=67.7171
	step [155/249], loss=76.1338
	step [156/249], loss=58.3463
	step [157/249], loss=72.5205
	step [158/249], loss=73.0671
	step [159/249], loss=71.8641
	step [160/249], loss=64.6236
	step [161/249], loss=80.3361
	step [162/249], loss=60.8285
	step [163/249], loss=71.4964
	step [164/249], loss=76.6762
	step [165/249], loss=81.7245
	step [166/249], loss=73.4601
	step [167/249], loss=73.5107
	step [168/249], loss=89.5496
	step [169/249], loss=87.5943
	step [170/249], loss=61.2747
	step [171/249], loss=81.3380
	step [172/249], loss=64.7331
	step [173/249], loss=69.6262
	step [174/249], loss=73.1285
	step [175/249], loss=77.9956
	step [176/249], loss=76.9819
	step [177/249], loss=70.2037
	step [178/249], loss=89.0285
	step [179/249], loss=71.8685
	step [180/249], loss=76.5307
	step [181/249], loss=65.8250
	step [182/249], loss=68.2337
	step [183/249], loss=67.0476
	step [184/249], loss=74.1063
	step [185/249], loss=78.4694
	step [186/249], loss=63.9456
	step [187/249], loss=69.4877
	step [188/249], loss=62.3467
	step [189/249], loss=98.5296
	step [190/249], loss=74.9944
	step [191/249], loss=80.4799
	step [192/249], loss=65.2062
	step [193/249], loss=92.7391
	step [194/249], loss=81.5553
	step [195/249], loss=68.0953
	step [196/249], loss=62.8660
	step [197/249], loss=76.0407
	step [198/249], loss=58.8784
	step [199/249], loss=80.5336
	step [200/249], loss=72.1395
	step [201/249], loss=62.5993
	step [202/249], loss=76.8188
	step [203/249], loss=70.2825
	step [204/249], loss=67.2364
	step [205/249], loss=92.6171
	step [206/249], loss=68.7296
	step [207/249], loss=72.1928
	step [208/249], loss=90.4902
	step [209/249], loss=74.7538
	step [210/249], loss=77.9287
	step [211/249], loss=81.8392
	step [212/249], loss=64.4513
	step [213/249], loss=71.3626
	step [214/249], loss=62.7006
	step [215/249], loss=70.4246
	step [216/249], loss=69.8602
	step [217/249], loss=66.9398
	step [218/249], loss=66.5281
	step [219/249], loss=82.0499
	step [220/249], loss=82.6583
	step [221/249], loss=82.5153
	step [222/249], loss=70.3440
	step [223/249], loss=69.6661
	step [224/249], loss=86.6903
	step [225/249], loss=78.9498
	step [226/249], loss=75.1312
	step [227/249], loss=63.5085
	step [228/249], loss=82.5510
	step [229/249], loss=60.9341
	step [230/249], loss=72.1568
	step [231/249], loss=73.2043
	step [232/249], loss=75.7947
	step [233/249], loss=60.1391
	step [234/249], loss=78.6111
	step [235/249], loss=75.1819
	step [236/249], loss=74.6343
	step [237/249], loss=68.5972
	step [238/249], loss=79.9972
	step [239/249], loss=76.5614
	step [240/249], loss=70.6761
	step [241/249], loss=67.9699
	step [242/249], loss=72.5295
	step [243/249], loss=59.5872
	step [244/249], loss=71.0706
	step [245/249], loss=105.9586
	step [246/249], loss=71.1087
	step [247/249], loss=72.5961
	step [248/249], loss=65.1930
	step [249/249], loss=57.2395
	Evaluating
	loss=0.0061, precision=0.3705, recall=0.8480, f1=0.5157
Training epoch 91
	step [1/249], loss=84.2258
	step [2/249], loss=89.7639
	step [3/249], loss=72.3874
	step [4/249], loss=66.6497
	step [5/249], loss=65.5241
	step [6/249], loss=61.7035
	step [7/249], loss=58.7860
	step [8/249], loss=71.3761
	step [9/249], loss=78.8074
	step [10/249], loss=64.3991
	step [11/249], loss=75.9816
	step [12/249], loss=78.1341
	step [13/249], loss=75.8475
	step [14/249], loss=66.9829
	step [15/249], loss=82.1719
	step [16/249], loss=65.0554
	step [17/249], loss=79.8486
	step [18/249], loss=63.1731
	step [19/249], loss=77.4896
	step [20/249], loss=75.3901
	step [21/249], loss=72.8305
	step [22/249], loss=81.4336
	step [23/249], loss=69.4384
	step [24/249], loss=68.5915
	step [25/249], loss=80.7996
	step [26/249], loss=64.6535
	step [27/249], loss=67.9330
	step [28/249], loss=77.9988
	step [29/249], loss=77.0855
	step [30/249], loss=60.2997
	step [31/249], loss=67.8538
	step [32/249], loss=68.8861
	step [33/249], loss=62.2132
	step [34/249], loss=86.0261
	step [35/249], loss=57.9625
	step [36/249], loss=73.6425
	step [37/249], loss=63.5396
	step [38/249], loss=61.4058
	step [39/249], loss=88.2636
	step [40/249], loss=75.4219
	step [41/249], loss=71.9000
	step [42/249], loss=69.7190
	step [43/249], loss=67.5193
	step [44/249], loss=73.2415
	step [45/249], loss=71.8531
	step [46/249], loss=69.8878
	step [47/249], loss=67.8243
	step [48/249], loss=84.9322
	step [49/249], loss=71.5910
	step [50/249], loss=71.4396
	step [51/249], loss=65.0459
	step [52/249], loss=82.9655
	step [53/249], loss=65.6263
	step [54/249], loss=74.8765
	step [55/249], loss=71.5666
	step [56/249], loss=87.9984
	step [57/249], loss=71.9965
	step [58/249], loss=62.3287
	step [59/249], loss=60.5177
	step [60/249], loss=68.9359
	step [61/249], loss=78.9930
	step [62/249], loss=95.2207
	step [63/249], loss=58.8521
	step [64/249], loss=93.9599
	step [65/249], loss=96.2053
	step [66/249], loss=82.8277
	step [67/249], loss=67.0563
	step [68/249], loss=69.4475
	step [69/249], loss=68.4532
	step [70/249], loss=76.6049
	step [71/249], loss=75.5163
	step [72/249], loss=78.4686
	step [73/249], loss=70.4635
	step [74/249], loss=85.7465
	step [75/249], loss=71.9504
	step [76/249], loss=78.5680
	step [77/249], loss=84.0848
	step [78/249], loss=80.0237
	step [79/249], loss=63.5346
	step [80/249], loss=77.6078
	step [81/249], loss=79.2554
	step [82/249], loss=64.8571
	step [83/249], loss=66.3251
	step [84/249], loss=78.1360
	step [85/249], loss=74.4884
	step [86/249], loss=63.5485
	step [87/249], loss=67.7331
	step [88/249], loss=64.3362
	step [89/249], loss=72.9715
	step [90/249], loss=66.7097
	step [91/249], loss=74.5600
	step [92/249], loss=76.7954
	step [93/249], loss=76.0472
	step [94/249], loss=68.8511
	step [95/249], loss=65.0460
	step [96/249], loss=88.6785
	step [97/249], loss=71.1634
	step [98/249], loss=70.8630
	step [99/249], loss=76.2322
	step [100/249], loss=70.2275
	step [101/249], loss=71.1775
	step [102/249], loss=71.4331
	step [103/249], loss=66.4072
	step [104/249], loss=59.7585
	step [105/249], loss=63.4349
	step [106/249], loss=82.1473
	step [107/249], loss=79.1407
	step [108/249], loss=65.3913
	step [109/249], loss=49.3576
	step [110/249], loss=72.6403
	step [111/249], loss=82.7914
	step [112/249], loss=69.5810
	step [113/249], loss=65.1210
	step [114/249], loss=78.0962
	step [115/249], loss=71.3815
	step [116/249], loss=61.6767
	step [117/249], loss=62.1498
	step [118/249], loss=71.6110
	step [119/249], loss=79.7234
	step [120/249], loss=60.8223
	step [121/249], loss=80.3896
	step [122/249], loss=71.9420
	step [123/249], loss=87.1545
	step [124/249], loss=80.4497
	step [125/249], loss=79.8012
	step [126/249], loss=69.3487
	step [127/249], loss=68.0833
	step [128/249], loss=58.6700
	step [129/249], loss=86.2087
	step [130/249], loss=78.9005
	step [131/249], loss=77.5958
	step [132/249], loss=59.4804
	step [133/249], loss=73.0129
	step [134/249], loss=70.7455
	step [135/249], loss=93.4185
	step [136/249], loss=81.5251
	step [137/249], loss=76.9847
	step [138/249], loss=76.9521
	step [139/249], loss=55.6336
	step [140/249], loss=69.8663
	step [141/249], loss=87.2323
	step [142/249], loss=61.1874
	step [143/249], loss=77.1659
	step [144/249], loss=73.1457
	step [145/249], loss=89.0668
	step [146/249], loss=88.4814
	step [147/249], loss=87.7627
	step [148/249], loss=83.2741
	step [149/249], loss=70.1866
	step [150/249], loss=76.6824
	step [151/249], loss=55.8341
	step [152/249], loss=71.3448
	step [153/249], loss=72.9150
	step [154/249], loss=65.5525
	step [155/249], loss=71.5358
	step [156/249], loss=79.7223
	step [157/249], loss=84.0041
	step [158/249], loss=57.7114
	step [159/249], loss=82.2233
	step [160/249], loss=69.2773
	step [161/249], loss=65.4861
	step [162/249], loss=70.3948
	step [163/249], loss=90.3940
	step [164/249], loss=75.0368
	step [165/249], loss=74.7054
	step [166/249], loss=73.0384
	step [167/249], loss=75.3172
	step [168/249], loss=68.0360
	step [169/249], loss=78.4542
	step [170/249], loss=74.3886
	step [171/249], loss=85.9023
	step [172/249], loss=67.8107
	step [173/249], loss=77.0080
	step [174/249], loss=81.5450
	step [175/249], loss=72.7050
	step [176/249], loss=77.4374
	step [177/249], loss=95.8905
	step [178/249], loss=63.0453
	step [179/249], loss=79.1278
	step [180/249], loss=63.3052
	step [181/249], loss=73.4201
	step [182/249], loss=90.6696
	step [183/249], loss=68.3858
	step [184/249], loss=69.9364
	step [185/249], loss=96.8059
	step [186/249], loss=76.0878
	step [187/249], loss=64.2431
	step [188/249], loss=82.1013
	step [189/249], loss=86.4166
	step [190/249], loss=72.5426
	step [191/249], loss=87.2467
	step [192/249], loss=77.9928
	step [193/249], loss=81.5064
	step [194/249], loss=101.1069
	step [195/249], loss=84.9492
	step [196/249], loss=64.4716
	step [197/249], loss=76.5450
	step [198/249], loss=73.3507
	step [199/249], loss=78.3393
	step [200/249], loss=67.4705
	step [201/249], loss=77.6383
	step [202/249], loss=67.1071
	step [203/249], loss=70.5414
	step [204/249], loss=72.8020
	step [205/249], loss=87.3085
	step [206/249], loss=72.1195
	step [207/249], loss=87.6694
	step [208/249], loss=66.9375
	step [209/249], loss=94.2726
	step [210/249], loss=63.8655
	step [211/249], loss=88.0192
	step [212/249], loss=81.6107
	step [213/249], loss=80.8541
	step [214/249], loss=67.9231
	step [215/249], loss=82.4208
	step [216/249], loss=67.3155
	step [217/249], loss=79.2354
	step [218/249], loss=66.1277
	step [219/249], loss=75.3116
	step [220/249], loss=73.2698
	step [221/249], loss=79.2782
	step [222/249], loss=90.3549
	step [223/249], loss=66.4828
	step [224/249], loss=66.3310
	step [225/249], loss=89.1697
	step [226/249], loss=73.6995
	step [227/249], loss=71.0181
	step [228/249], loss=75.4457
	step [229/249], loss=78.9396
	step [230/249], loss=63.2453
	step [231/249], loss=71.9061
	step [232/249], loss=68.4763
	step [233/249], loss=66.2762
	step [234/249], loss=73.0549
	step [235/249], loss=80.6272
	step [236/249], loss=75.2017
	step [237/249], loss=71.0404
	step [238/249], loss=68.9826
	step [239/249], loss=66.5404
	step [240/249], loss=68.2923
	step [241/249], loss=76.0350
	step [242/249], loss=69.2405
	step [243/249], loss=82.0803
	step [244/249], loss=69.1820
	step [245/249], loss=68.3425
	step [246/249], loss=77.0166
	step [247/249], loss=83.0580
	step [248/249], loss=83.0396
	step [249/249], loss=60.0624
	Evaluating
	loss=0.0058, precision=0.3773, recall=0.8460, f1=0.5219
Training epoch 92
	step [1/249], loss=72.4794
	step [2/249], loss=91.1271
	step [3/249], loss=63.6905
	step [4/249], loss=80.5158
	step [5/249], loss=71.5579
	step [6/249], loss=62.1572
	step [7/249], loss=94.9456
	step [8/249], loss=66.6771
	step [9/249], loss=70.2605
	step [10/249], loss=76.3143
	step [11/249], loss=65.5756
	step [12/249], loss=68.2507
	step [13/249], loss=59.9719
	step [14/249], loss=74.0637
	step [15/249], loss=70.8726
	step [16/249], loss=77.9065
	step [17/249], loss=80.1991
	step [18/249], loss=74.3006
	step [19/249], loss=83.3761
	step [20/249], loss=65.6495
	step [21/249], loss=69.2921
	step [22/249], loss=70.6055
	step [23/249], loss=86.8185
	step [24/249], loss=76.6519
	step [25/249], loss=72.4406
	step [26/249], loss=75.2251
	step [27/249], loss=66.2665
	step [28/249], loss=80.8416
	step [29/249], loss=92.0214
	step [30/249], loss=66.1160
	step [31/249], loss=87.0885
	step [32/249], loss=66.8025
	step [33/249], loss=66.1354
	step [34/249], loss=68.9726
	step [35/249], loss=80.8292
	step [36/249], loss=77.9741
	step [37/249], loss=81.4548
	step [38/249], loss=75.6920
	step [39/249], loss=65.9794
	step [40/249], loss=66.6669
	step [41/249], loss=66.4737
	step [42/249], loss=82.2744
	step [43/249], loss=76.1581
	step [44/249], loss=71.0676
	step [45/249], loss=68.1166
	step [46/249], loss=92.6132
	step [47/249], loss=65.9534
	step [48/249], loss=67.8973
	step [49/249], loss=64.6127
	step [50/249], loss=67.2403
	step [51/249], loss=82.7679
	step [52/249], loss=72.4185
	step [53/249], loss=74.6201
	step [54/249], loss=69.4352
	step [55/249], loss=66.0845
	step [56/249], loss=76.1958
	step [57/249], loss=90.1794
	step [58/249], loss=63.9521
	step [59/249], loss=65.4163
	step [60/249], loss=71.0013
	step [61/249], loss=84.4611
	step [62/249], loss=67.6770
	step [63/249], loss=74.3605
	step [64/249], loss=80.0583
	step [65/249], loss=78.2690
	step [66/249], loss=75.7822
	step [67/249], loss=66.1110
	step [68/249], loss=62.0089
	step [69/249], loss=67.8793
	step [70/249], loss=64.5138
	step [71/249], loss=86.4797
	step [72/249], loss=77.1114
	step [73/249], loss=89.3833
	step [74/249], loss=70.6570
	step [75/249], loss=80.0156
	step [76/249], loss=76.7482
	step [77/249], loss=65.2911
	step [78/249], loss=68.4565
	step [79/249], loss=78.6606
	step [80/249], loss=65.8676
	step [81/249], loss=76.3833
	step [82/249], loss=51.7616
	step [83/249], loss=65.7740
	step [84/249], loss=63.9048
	step [85/249], loss=73.4564
	step [86/249], loss=69.6913
	step [87/249], loss=72.7047
	step [88/249], loss=78.4862
	step [89/249], loss=53.5672
	step [90/249], loss=79.4827
	step [91/249], loss=91.9520
	step [92/249], loss=92.5294
	step [93/249], loss=77.3026
	step [94/249], loss=70.9932
	step [95/249], loss=71.7955
	step [96/249], loss=66.9965
	step [97/249], loss=77.7267
	step [98/249], loss=74.1277
	step [99/249], loss=70.3770
	step [100/249], loss=72.3356
	step [101/249], loss=69.4248
	step [102/249], loss=88.2218
	step [103/249], loss=72.3819
	step [104/249], loss=77.6468
	step [105/249], loss=87.2626
	step [106/249], loss=77.2161
	step [107/249], loss=72.7804
	step [108/249], loss=82.0642
	step [109/249], loss=85.3066
	step [110/249], loss=61.7825
	step [111/249], loss=70.0583
	step [112/249], loss=66.8140
	step [113/249], loss=57.3190
	step [114/249], loss=83.7510
	step [115/249], loss=61.4974
	step [116/249], loss=75.8205
	step [117/249], loss=65.0636
	step [118/249], loss=82.4206
	step [119/249], loss=83.5887
	step [120/249], loss=89.0075
	step [121/249], loss=75.0680
	step [122/249], loss=71.7994
	step [123/249], loss=76.7816
	step [124/249], loss=77.5933
	step [125/249], loss=82.0906
	step [126/249], loss=76.8222
	step [127/249], loss=80.7811
	step [128/249], loss=56.8049
	step [129/249], loss=89.9693
	step [130/249], loss=77.0268
	step [131/249], loss=70.3504
	step [132/249], loss=74.1755
	step [133/249], loss=88.7152
	step [134/249], loss=57.4187
	step [135/249], loss=78.9867
	step [136/249], loss=56.8142
	step [137/249], loss=83.3873
	step [138/249], loss=76.8264
	step [139/249], loss=81.1164
	step [140/249], loss=80.8927
	step [141/249], loss=63.6986
	step [142/249], loss=89.4840
	step [143/249], loss=83.4264
	step [144/249], loss=73.9485
	step [145/249], loss=77.7563
	step [146/249], loss=81.1006
	step [147/249], loss=62.9579
	step [148/249], loss=78.0956
	step [149/249], loss=75.6863
	step [150/249], loss=68.7128
	step [151/249], loss=81.7483
	step [152/249], loss=61.8189
	step [153/249], loss=85.8880
	step [154/249], loss=66.8686
	step [155/249], loss=72.3318
	step [156/249], loss=81.1818
	step [157/249], loss=82.3368
	step [158/249], loss=76.6364
	step [159/249], loss=59.8034
	step [160/249], loss=89.5736
	step [161/249], loss=82.3417
	step [162/249], loss=66.8640
	step [163/249], loss=72.6687
	step [164/249], loss=90.9070
	step [165/249], loss=71.1568
	step [166/249], loss=63.3610
	step [167/249], loss=72.5309
	step [168/249], loss=78.7273
	step [169/249], loss=85.6153
	step [170/249], loss=72.6773
	step [171/249], loss=82.6863
	step [172/249], loss=81.9575
	step [173/249], loss=69.1349
	step [174/249], loss=63.0081
	step [175/249], loss=67.7566
	step [176/249], loss=81.8946
	step [177/249], loss=71.1601
	step [178/249], loss=63.6329
	step [179/249], loss=70.1962
	step [180/249], loss=73.6240
	step [181/249], loss=57.0216
	step [182/249], loss=85.3342
	step [183/249], loss=60.5574
	step [184/249], loss=73.3707
	step [185/249], loss=69.6834
	step [186/249], loss=84.1459
	step [187/249], loss=75.8050
	step [188/249], loss=60.1355
	step [189/249], loss=84.8251
	step [190/249], loss=77.7785
	step [191/249], loss=64.0032
	step [192/249], loss=72.3694
	step [193/249], loss=83.6496
	step [194/249], loss=75.3284
	step [195/249], loss=66.5567
	step [196/249], loss=62.4253
	step [197/249], loss=68.8486
	step [198/249], loss=75.3891
	step [199/249], loss=61.7914
	step [200/249], loss=74.3215
	step [201/249], loss=78.5439
	step [202/249], loss=72.2058
	step [203/249], loss=67.4034
	step [204/249], loss=72.1644
	step [205/249], loss=71.3045
	step [206/249], loss=68.1255
	step [207/249], loss=77.2701
	step [208/249], loss=84.3679
	step [209/249], loss=73.7905
	step [210/249], loss=66.7967
	step [211/249], loss=69.9038
	step [212/249], loss=74.5965
	step [213/249], loss=59.8917
	step [214/249], loss=81.3894
	step [215/249], loss=72.3582
	step [216/249], loss=80.9705
	step [217/249], loss=79.5343
	step [218/249], loss=59.8048
	step [219/249], loss=77.2369
	step [220/249], loss=83.8037
	step [221/249], loss=69.7261
	step [222/249], loss=67.9792
	step [223/249], loss=77.8187
	step [224/249], loss=63.3933
	step [225/249], loss=69.3071
	step [226/249], loss=64.4275
	step [227/249], loss=74.4798
	step [228/249], loss=72.5769
	step [229/249], loss=70.8241
	step [230/249], loss=76.4788
	step [231/249], loss=60.8932
	step [232/249], loss=68.8596
	step [233/249], loss=62.6384
	step [234/249], loss=63.4513
	step [235/249], loss=83.4618
	step [236/249], loss=89.0670
	step [237/249], loss=75.3498
	step [238/249], loss=77.6490
	step [239/249], loss=64.6872
	step [240/249], loss=68.1237
	step [241/249], loss=74.4344
	step [242/249], loss=81.3781
	step [243/249], loss=74.8228
	step [244/249], loss=75.8116
	step [245/249], loss=70.0926
	step [246/249], loss=55.5370
	step [247/249], loss=75.4895
	step [248/249], loss=78.9114
	step [249/249], loss=47.3361
	Evaluating
	loss=0.0061, precision=0.3659, recall=0.8384, f1=0.5094
Training epoch 93
	step [1/249], loss=61.9561
	step [2/249], loss=85.5966
	step [3/249], loss=68.5656
	step [4/249], loss=83.7019
	step [5/249], loss=78.7686
	step [6/249], loss=70.8554
	step [7/249], loss=69.0047
	step [8/249], loss=72.8596
	step [9/249], loss=75.5950
	step [10/249], loss=93.9598
	step [11/249], loss=73.6142
	step [12/249], loss=65.0143
	step [13/249], loss=62.8976
	step [14/249], loss=75.0769
	step [15/249], loss=74.4381
	step [16/249], loss=80.3849
	step [17/249], loss=64.2877
	step [18/249], loss=56.6723
	step [19/249], loss=74.8975
	step [20/249], loss=81.2923
	step [21/249], loss=71.1120
	step [22/249], loss=76.0881
	step [23/249], loss=66.9529
	step [24/249], loss=63.2964
	step [25/249], loss=77.2416
	step [26/249], loss=86.4608
	step [27/249], loss=75.9022
	step [28/249], loss=80.4454
	step [29/249], loss=68.6450
	step [30/249], loss=79.5889
	step [31/249], loss=81.3362
	step [32/249], loss=70.5064
	step [33/249], loss=62.1038
	step [34/249], loss=79.9951
	step [35/249], loss=79.9082
	step [36/249], loss=60.4178
	step [37/249], loss=71.7328
	step [38/249], loss=75.3385
	step [39/249], loss=75.5586
	step [40/249], loss=56.8640
	step [41/249], loss=75.4320
	step [42/249], loss=76.5637
	step [43/249], loss=60.3907
	step [44/249], loss=93.0976
	step [45/249], loss=72.1521
	step [46/249], loss=81.5682
	step [47/249], loss=81.2365
	step [48/249], loss=82.9956
	step [49/249], loss=74.5806
	step [50/249], loss=71.1943
	step [51/249], loss=52.8828
	step [52/249], loss=73.2337
	step [53/249], loss=75.7827
	step [54/249], loss=65.1070
	step [55/249], loss=69.1993
	step [56/249], loss=78.5458
	step [57/249], loss=80.9091
	step [58/249], loss=79.3632
	step [59/249], loss=69.8473
	step [60/249], loss=76.3341
	step [61/249], loss=85.0568
	step [62/249], loss=69.9203
	step [63/249], loss=76.4509
	step [64/249], loss=78.8138
	step [65/249], loss=65.4892
	step [66/249], loss=74.7506
	step [67/249], loss=68.3222
	step [68/249], loss=70.3714
	step [69/249], loss=67.0002
	step [70/249], loss=76.5790
	step [71/249], loss=87.9490
	step [72/249], loss=61.1468
	step [73/249], loss=74.4677
	step [74/249], loss=72.3120
	step [75/249], loss=79.4038
	step [76/249], loss=71.5335
	step [77/249], loss=68.4445
	step [78/249], loss=64.4367
	step [79/249], loss=65.9055
	step [80/249], loss=65.6306
	step [81/249], loss=75.7761
	step [82/249], loss=75.9469
	step [83/249], loss=71.8474
	step [84/249], loss=67.2600
	step [85/249], loss=77.8438
	step [86/249], loss=72.4936
	step [87/249], loss=79.0035
	step [88/249], loss=71.3116
	step [89/249], loss=93.8973
	step [90/249], loss=76.4212
	step [91/249], loss=70.1606
	step [92/249], loss=77.5767
	step [93/249], loss=75.7581
	step [94/249], loss=82.6908
	step [95/249], loss=76.7085
	step [96/249], loss=68.1465
	step [97/249], loss=60.1068
	step [98/249], loss=65.4341
	step [99/249], loss=76.2567
	step [100/249], loss=75.0633
	step [101/249], loss=61.2960
	step [102/249], loss=67.3958
	step [103/249], loss=76.6165
	step [104/249], loss=58.2825
	step [105/249], loss=71.7658
	step [106/249], loss=69.2482
	step [107/249], loss=72.2576
	step [108/249], loss=62.1833
	step [109/249], loss=57.6230
	step [110/249], loss=58.4413
	step [111/249], loss=73.2812
	step [112/249], loss=66.7049
	step [113/249], loss=84.7866
	step [114/249], loss=66.7155
	step [115/249], loss=69.3301
	step [116/249], loss=66.4091
	step [117/249], loss=75.0108
	step [118/249], loss=86.3249
	step [119/249], loss=74.1115
	step [120/249], loss=79.3762
	step [121/249], loss=70.2203
	step [122/249], loss=65.0041
	step [123/249], loss=79.3979
	step [124/249], loss=85.1032
	step [125/249], loss=68.2955
	step [126/249], loss=75.4024
	step [127/249], loss=73.0277
	step [128/249], loss=79.6585
	step [129/249], loss=78.5323
	step [130/249], loss=58.4449
	step [131/249], loss=70.2013
	step [132/249], loss=77.4168
	step [133/249], loss=83.9844
	step [134/249], loss=73.1011
	step [135/249], loss=84.5441
	step [136/249], loss=73.8781
	step [137/249], loss=60.6207
	step [138/249], loss=53.6488
	step [139/249], loss=70.8279
	step [140/249], loss=79.3525
	step [141/249], loss=78.4334
	step [142/249], loss=70.4678
	step [143/249], loss=68.7913
	step [144/249], loss=64.6543
	step [145/249], loss=81.2621
	step [146/249], loss=86.3814
	step [147/249], loss=71.0936
	step [148/249], loss=75.2781
	step [149/249], loss=71.8884
	step [150/249], loss=60.6297
	step [151/249], loss=67.2826
	step [152/249], loss=81.3345
	step [153/249], loss=77.2373
	step [154/249], loss=77.2295
	step [155/249], loss=83.9941
	step [156/249], loss=58.7806
	step [157/249], loss=75.5970
	step [158/249], loss=84.3134
	step [159/249], loss=59.7350
	step [160/249], loss=72.7047
	step [161/249], loss=79.8875
	step [162/249], loss=62.9143
	step [163/249], loss=73.8916
	step [164/249], loss=63.0130
	step [165/249], loss=66.0697
	step [166/249], loss=70.7866
	step [167/249], loss=71.8931
	step [168/249], loss=77.1866
	step [169/249], loss=87.7265
	step [170/249], loss=84.1875
	step [171/249], loss=78.5244
	step [172/249], loss=66.6161
	step [173/249], loss=85.1399
	step [174/249], loss=79.1926
	step [175/249], loss=81.2867
	step [176/249], loss=88.8075
	step [177/249], loss=79.7813
	step [178/249], loss=80.9213
	step [179/249], loss=91.6204
	step [180/249], loss=70.1534
	step [181/249], loss=81.1476
	step [182/249], loss=69.9019
	step [183/249], loss=71.8624
	step [184/249], loss=74.7369
	step [185/249], loss=69.4699
	step [186/249], loss=88.2072
	step [187/249], loss=76.3108
	step [188/249], loss=71.6320
	step [189/249], loss=75.6082
	step [190/249], loss=61.8760
	step [191/249], loss=76.9139
	step [192/249], loss=61.9437
	step [193/249], loss=89.6034
	step [194/249], loss=63.7316
	step [195/249], loss=68.4049
	step [196/249], loss=65.5081
	step [197/249], loss=73.6770
	step [198/249], loss=82.8927
	step [199/249], loss=83.9291
	step [200/249], loss=69.5104
	step [201/249], loss=67.1460
	step [202/249], loss=76.6274
	step [203/249], loss=76.9349
	step [204/249], loss=68.2977
	step [205/249], loss=77.5576
	step [206/249], loss=63.2676
	step [207/249], loss=70.3738
	step [208/249], loss=67.6709
	step [209/249], loss=63.0657
	step [210/249], loss=80.9664
	step [211/249], loss=78.1323
	step [212/249], loss=85.1042
	step [213/249], loss=93.0614
	step [214/249], loss=71.5227
	step [215/249], loss=77.9604
	step [216/249], loss=70.6296
	step [217/249], loss=67.1522
	step [218/249], loss=67.0452
	step [219/249], loss=62.4421
	step [220/249], loss=68.5274
	step [221/249], loss=79.2976
	step [222/249], loss=68.4188
	step [223/249], loss=68.4197
	step [224/249], loss=68.5472
	step [225/249], loss=83.1803
	step [226/249], loss=65.9570
	step [227/249], loss=65.9917
	step [228/249], loss=80.9263
	step [229/249], loss=74.9658
	step [230/249], loss=91.1692
	step [231/249], loss=73.1631
	step [232/249], loss=74.9645
	step [233/249], loss=67.9542
	step [234/249], loss=78.9930
	step [235/249], loss=72.7190
	step [236/249], loss=70.1210
	step [237/249], loss=68.9445
	step [238/249], loss=56.8940
	step [239/249], loss=73.4192
	step [240/249], loss=79.9812
	step [241/249], loss=60.3227
	step [242/249], loss=68.0956
	step [243/249], loss=71.8757
	step [244/249], loss=89.4941
	step [245/249], loss=78.4575
	step [246/249], loss=56.9810
	step [247/249], loss=83.6615
	step [248/249], loss=67.3375
	step [249/249], loss=58.1198
	Evaluating
	loss=0.0059, precision=0.3782, recall=0.8507, f1=0.5236
Training epoch 94
	step [1/249], loss=71.8849
	step [2/249], loss=83.1005
	step [3/249], loss=66.7461
	step [4/249], loss=77.6973
	step [5/249], loss=85.7739
	step [6/249], loss=85.7564
	step [7/249], loss=59.8479
	step [8/249], loss=65.9910
	step [9/249], loss=82.6497
	step [10/249], loss=69.9964
	step [11/249], loss=90.5001
	step [12/249], loss=87.2313
	step [13/249], loss=66.5879
	step [14/249], loss=74.1944
	step [15/249], loss=56.8632
	step [16/249], loss=63.6550
	step [17/249], loss=74.8490
	step [18/249], loss=67.9635
	step [19/249], loss=80.5943
	step [20/249], loss=71.4046
	step [21/249], loss=72.4647
	step [22/249], loss=76.8265
	step [23/249], loss=79.2104
	step [24/249], loss=64.8346
	step [25/249], loss=75.8220
	step [26/249], loss=55.9013
	step [27/249], loss=63.8041
	step [28/249], loss=83.9726
	step [29/249], loss=75.3351
	step [30/249], loss=78.0752
	step [31/249], loss=72.1085
	step [32/249], loss=71.0880
	step [33/249], loss=69.1258
	step [34/249], loss=62.7147
	step [35/249], loss=72.5178
	step [36/249], loss=62.0403
	step [37/249], loss=59.0472
	step [38/249], loss=87.6432
	step [39/249], loss=69.3889
	step [40/249], loss=76.2162
	step [41/249], loss=88.7221
	step [42/249], loss=80.3209
	step [43/249], loss=79.1577
	step [44/249], loss=76.4891
	step [45/249], loss=72.0336
	step [46/249], loss=79.3041
	step [47/249], loss=61.4240
	step [48/249], loss=61.3456
	step [49/249], loss=77.8078
	step [50/249], loss=76.1753
	step [51/249], loss=69.9045
	step [52/249], loss=73.8097
	step [53/249], loss=65.4493
	step [54/249], loss=69.0854
	step [55/249], loss=68.7077
	step [56/249], loss=76.7163
	step [57/249], loss=81.5300
	step [58/249], loss=76.0635
	step [59/249], loss=82.9304
	step [60/249], loss=71.9876
	step [61/249], loss=67.1961
	step [62/249], loss=65.6843
	step [63/249], loss=77.4451
	step [64/249], loss=58.0971
	step [65/249], loss=72.1587
	step [66/249], loss=60.2404
	step [67/249], loss=73.8140
	step [68/249], loss=68.0666
	step [69/249], loss=80.1572
	step [70/249], loss=76.6913
	step [71/249], loss=65.4520
	step [72/249], loss=62.5809
	step [73/249], loss=71.6214
	step [74/249], loss=71.9821
	step [75/249], loss=70.6465
	step [76/249], loss=70.8592
	step [77/249], loss=64.4501
	step [78/249], loss=80.5944
	step [79/249], loss=56.4693
	step [80/249], loss=64.4613
	step [81/249], loss=94.6028
	step [82/249], loss=88.0094
	step [83/249], loss=82.1536
	step [84/249], loss=65.5947
	step [85/249], loss=67.9965
	step [86/249], loss=90.4443
	step [87/249], loss=60.2842
	step [88/249], loss=83.4265
	step [89/249], loss=62.8381
	step [90/249], loss=66.4070
	step [91/249], loss=62.7735
	step [92/249], loss=82.1948
	step [93/249], loss=62.8748
	step [94/249], loss=76.1815
	step [95/249], loss=86.7980
	step [96/249], loss=76.3208
	step [97/249], loss=66.8037
	step [98/249], loss=75.3043
	step [99/249], loss=80.9027
	step [100/249], loss=70.7014
	step [101/249], loss=69.8316
	step [102/249], loss=74.8308
	step [103/249], loss=74.6134
	step [104/249], loss=58.2305
	step [105/249], loss=76.6231
	step [106/249], loss=78.9053
	step [107/249], loss=76.9473
	step [108/249], loss=61.3070
	step [109/249], loss=71.3112
	step [110/249], loss=71.2599
	step [111/249], loss=77.1223
	step [112/249], loss=94.9260
	step [113/249], loss=79.0548
	step [114/249], loss=72.1702
	step [115/249], loss=86.8468
	step [116/249], loss=68.2842
	step [117/249], loss=75.2678
	step [118/249], loss=74.3956
	step [119/249], loss=61.3419
	step [120/249], loss=65.1529
	step [121/249], loss=86.5027
	step [122/249], loss=70.6704
	step [123/249], loss=79.1152
	step [124/249], loss=80.3269
	step [125/249], loss=80.8710
	step [126/249], loss=69.6206
	step [127/249], loss=70.9264
	step [128/249], loss=68.7517
	step [129/249], loss=74.1515
	step [130/249], loss=61.6787
	step [131/249], loss=63.7107
	step [132/249], loss=64.3708
	step [133/249], loss=70.2294
	step [134/249], loss=64.1621
	step [135/249], loss=74.9866
	step [136/249], loss=82.3237
	step [137/249], loss=60.3981
	step [138/249], loss=64.9573
	step [139/249], loss=83.8389
	step [140/249], loss=55.7142
	step [141/249], loss=61.0036
	step [142/249], loss=74.2543
	step [143/249], loss=78.2553
	step [144/249], loss=77.9108
	step [145/249], loss=67.0518
	step [146/249], loss=71.2645
	step [147/249], loss=74.8771
	step [148/249], loss=62.6666
	step [149/249], loss=58.1834
	step [150/249], loss=60.5754
	step [151/249], loss=79.3002
	step [152/249], loss=87.3170
	step [153/249], loss=78.1485
	step [154/249], loss=70.9126
	step [155/249], loss=75.9409
	step [156/249], loss=91.5178
	step [157/249], loss=59.1387
	step [158/249], loss=64.5899
	step [159/249], loss=76.8308
	step [160/249], loss=60.4219
	step [161/249], loss=81.2908
	step [162/249], loss=76.4743
	step [163/249], loss=78.0106
	step [164/249], loss=66.5808
	step [165/249], loss=62.9522
	step [166/249], loss=93.9596
	step [167/249], loss=72.3047
	step [168/249], loss=71.1615
	step [169/249], loss=80.0882
	step [170/249], loss=80.7899
	step [171/249], loss=70.8857
	step [172/249], loss=63.2644
	step [173/249], loss=72.3993
	step [174/249], loss=72.0367
	step [175/249], loss=81.5815
	step [176/249], loss=89.4987
	step [177/249], loss=64.9368
	step [178/249], loss=63.8702
	step [179/249], loss=89.1514
	step [180/249], loss=76.4537
	step [181/249], loss=74.3536
	step [182/249], loss=58.5283
	step [183/249], loss=74.3547
	step [184/249], loss=91.4437
	step [185/249], loss=61.7280
	step [186/249], loss=66.9806
	step [187/249], loss=82.6136
	step [188/249], loss=74.1163
	step [189/249], loss=71.5747
	step [190/249], loss=62.6363
	step [191/249], loss=72.2242
	step [192/249], loss=71.0153
	step [193/249], loss=55.6989
	step [194/249], loss=64.5668
	step [195/249], loss=77.2116
	step [196/249], loss=85.7870
	step [197/249], loss=66.6429
	step [198/249], loss=77.9254
	step [199/249], loss=81.0172
	step [200/249], loss=75.9562
	step [201/249], loss=66.9488
	step [202/249], loss=76.9599
	step [203/249], loss=61.2807
	step [204/249], loss=95.5889
	step [205/249], loss=67.3618
	step [206/249], loss=79.1070
	step [207/249], loss=67.3826
	step [208/249], loss=69.6931
	step [209/249], loss=73.2030
	step [210/249], loss=67.6090
	step [211/249], loss=79.1571
	step [212/249], loss=76.2535
	step [213/249], loss=78.8181
	step [214/249], loss=70.5256
	step [215/249], loss=58.0905
	step [216/249], loss=80.0093
	step [217/249], loss=81.2032
	step [218/249], loss=71.6938
	step [219/249], loss=75.8449
	step [220/249], loss=68.2031
	step [221/249], loss=89.6537
	step [222/249], loss=81.1111
	step [223/249], loss=72.9539
	step [224/249], loss=71.3939
	step [225/249], loss=66.2212
	step [226/249], loss=75.8176
	step [227/249], loss=75.3005
	step [228/249], loss=88.3854
	step [229/249], loss=67.4463
	step [230/249], loss=80.6492
	step [231/249], loss=75.7202
	step [232/249], loss=62.7493
	step [233/249], loss=70.8747
	step [234/249], loss=65.7295
	step [235/249], loss=57.2651
	step [236/249], loss=62.9583
	step [237/249], loss=96.7738
	step [238/249], loss=68.8936
	step [239/249], loss=69.1611
	step [240/249], loss=78.7473
	step [241/249], loss=72.7548
	step [242/249], loss=77.5088
	step [243/249], loss=79.5909
	step [244/249], loss=81.1804
	step [245/249], loss=88.4240
	step [246/249], loss=83.6353
	step [247/249], loss=67.3185
	step [248/249], loss=73.0985
	step [249/249], loss=66.0519
	Evaluating
	loss=0.0061, precision=0.3656, recall=0.8476, f1=0.5108
Training epoch 95
	step [1/249], loss=68.7941
	step [2/249], loss=74.4133
	step [3/249], loss=57.3211
	step [4/249], loss=65.5118
	step [5/249], loss=72.3715
	step [6/249], loss=67.0640
	step [7/249], loss=88.2919
	step [8/249], loss=85.2083
	step [9/249], loss=75.1354
	step [10/249], loss=77.9167
	step [11/249], loss=63.8786
	step [12/249], loss=60.6060
	step [13/249], loss=87.7575
	step [14/249], loss=79.8747
	step [15/249], loss=76.9602
	step [16/249], loss=75.2283
	step [17/249], loss=76.3967
	step [18/249], loss=61.8947
	step [19/249], loss=72.6042
	step [20/249], loss=69.0755
	step [21/249], loss=83.4153
	step [22/249], loss=81.7735
	step [23/249], loss=74.8838
	step [24/249], loss=89.0119
	step [25/249], loss=55.5479
	step [26/249], loss=73.7106
	step [27/249], loss=73.4541
	step [28/249], loss=79.6004
	step [29/249], loss=68.6322
	step [30/249], loss=75.1395
	step [31/249], loss=65.1310
	step [32/249], loss=68.0955
	step [33/249], loss=80.4008
	step [34/249], loss=58.1454
	step [35/249], loss=75.2048
	step [36/249], loss=69.6524
	step [37/249], loss=67.3975
	step [38/249], loss=63.5247
	step [39/249], loss=58.7116
	step [40/249], loss=60.3629
	step [41/249], loss=62.9138
	step [42/249], loss=64.4042
	step [43/249], loss=62.7785
	step [44/249], loss=67.3554
	step [45/249], loss=84.9531
	step [46/249], loss=72.0824
	step [47/249], loss=73.3423
	step [48/249], loss=76.4788
	step [49/249], loss=76.6188
	step [50/249], loss=66.9387
	step [51/249], loss=79.6466
	step [52/249], loss=83.3238
	step [53/249], loss=83.4416
	step [54/249], loss=70.0409
	step [55/249], loss=63.7216
	step [56/249], loss=82.3586
	step [57/249], loss=81.8030
	step [58/249], loss=59.3885
	step [59/249], loss=81.6708
	step [60/249], loss=75.1498
	step [61/249], loss=71.5384
	step [62/249], loss=77.4063
	step [63/249], loss=67.5307
	step [64/249], loss=75.1547
	step [65/249], loss=69.3894
	step [66/249], loss=93.9461
	step [67/249], loss=72.1875
	step [68/249], loss=67.6610
	step [69/249], loss=79.6276
	step [70/249], loss=69.4151
	step [71/249], loss=65.1543
	step [72/249], loss=68.6855
	step [73/249], loss=81.0826
	step [74/249], loss=73.2488
	step [75/249], loss=73.4566
	step [76/249], loss=71.0085
	step [77/249], loss=85.4012
	step [78/249], loss=68.0792
	step [79/249], loss=65.5362
	step [80/249], loss=77.9002
	step [81/249], loss=54.9475
	step [82/249], loss=58.9169
	step [83/249], loss=63.8841
	step [84/249], loss=73.2105
	step [85/249], loss=86.1961
	step [86/249], loss=73.6103
	step [87/249], loss=75.2772
	step [88/249], loss=81.1168
	step [89/249], loss=80.3601
	step [90/249], loss=76.3434
	step [91/249], loss=67.5297
	step [92/249], loss=73.9515
	step [93/249], loss=76.1263
	step [94/249], loss=77.8260
	step [95/249], loss=84.2128
	step [96/249], loss=57.8261
	step [97/249], loss=87.6924
	step [98/249], loss=70.6590
	step [99/249], loss=76.7872
	step [100/249], loss=67.1554
	step [101/249], loss=73.7446
	step [102/249], loss=69.7696
	step [103/249], loss=67.4848
	step [104/249], loss=76.8837
	step [105/249], loss=65.9450
	step [106/249], loss=80.6067
	step [107/249], loss=54.4945
	step [108/249], loss=82.8335
	step [109/249], loss=52.5427
	step [110/249], loss=74.2484
	step [111/249], loss=73.3993
	step [112/249], loss=69.7908
	step [113/249], loss=83.8343
	step [114/249], loss=81.9720
	step [115/249], loss=54.8266
	step [116/249], loss=77.5661
	step [117/249], loss=75.8566
	step [118/249], loss=64.8572
	step [119/249], loss=99.0284
	step [120/249], loss=81.3528
	step [121/249], loss=61.7316
	step [122/249], loss=59.4001
	step [123/249], loss=81.9721
	step [124/249], loss=68.6984
	step [125/249], loss=72.5310
	step [126/249], loss=65.0226
	step [127/249], loss=71.7676
	step [128/249], loss=69.6398
	step [129/249], loss=99.3457
	step [130/249], loss=60.5749
	step [131/249], loss=71.2984
	step [132/249], loss=68.8014
	step [133/249], loss=73.8196
	step [134/249], loss=77.7024
	step [135/249], loss=62.0579
	step [136/249], loss=82.6320
	step [137/249], loss=76.2855
	step [138/249], loss=102.1728
	step [139/249], loss=73.7827
	step [140/249], loss=67.7977
	step [141/249], loss=69.3242
	step [142/249], loss=69.8389
	step [143/249], loss=74.0715
	step [144/249], loss=71.4274
	step [145/249], loss=77.6629
	step [146/249], loss=74.0376
	step [147/249], loss=66.7729
	step [148/249], loss=68.4426
	step [149/249], loss=71.7597
	step [150/249], loss=66.5190
	step [151/249], loss=68.8403
	step [152/249], loss=76.1700
	step [153/249], loss=63.3847
	step [154/249], loss=61.2830
	step [155/249], loss=83.7841
	step [156/249], loss=75.2285
	step [157/249], loss=76.2373
	step [158/249], loss=66.7953
	step [159/249], loss=81.4138
	step [160/249], loss=65.4725
	step [161/249], loss=82.4419
	step [162/249], loss=68.1389
	step [163/249], loss=67.7135
	step [164/249], loss=71.9458
	step [165/249], loss=62.6987
	step [166/249], loss=76.5938
	step [167/249], loss=69.6485
	step [168/249], loss=66.7004
	step [169/249], loss=78.4289
	step [170/249], loss=76.9669
	step [171/249], loss=72.9323
	step [172/249], loss=70.1590
	step [173/249], loss=69.9762
	step [174/249], loss=83.3451
	step [175/249], loss=69.9887
	step [176/249], loss=87.3616
	step [177/249], loss=73.6666
	step [178/249], loss=79.8184
	step [179/249], loss=67.5031
	step [180/249], loss=87.7456
	step [181/249], loss=76.5986
	step [182/249], loss=75.5287
	step [183/249], loss=80.4763
	step [184/249], loss=79.5365
	step [185/249], loss=68.9672
	step [186/249], loss=63.6922
	step [187/249], loss=78.0808
	step [188/249], loss=88.8044
	step [189/249], loss=73.4943
	step [190/249], loss=72.9523
	step [191/249], loss=81.6245
	step [192/249], loss=70.5988
	step [193/249], loss=62.6439
	step [194/249], loss=66.9803
	step [195/249], loss=79.8448
	step [196/249], loss=87.0726
	step [197/249], loss=66.4798
	step [198/249], loss=78.8777
	step [199/249], loss=80.7971
	step [200/249], loss=68.7977
	step [201/249], loss=71.1585
	step [202/249], loss=71.7689
	step [203/249], loss=71.7560
	step [204/249], loss=75.9501
	step [205/249], loss=77.1877
	step [206/249], loss=69.5391
	step [207/249], loss=70.2552
	step [208/249], loss=69.7098
	step [209/249], loss=90.1308
	step [210/249], loss=65.9606
	step [211/249], loss=72.7259
	step [212/249], loss=67.4731
	step [213/249], loss=68.2576
	step [214/249], loss=82.5725
	step [215/249], loss=75.3319
	step [216/249], loss=73.8711
	step [217/249], loss=77.9953
	step [218/249], loss=62.6831
	step [219/249], loss=77.5842
	step [220/249], loss=76.9076
	step [221/249], loss=85.5691
	step [222/249], loss=69.1703
	step [223/249], loss=76.1192
	step [224/249], loss=72.7747
	step [225/249], loss=75.2850
	step [226/249], loss=64.0135
	step [227/249], loss=71.1637
	step [228/249], loss=63.8587
	step [229/249], loss=78.7945
	step [230/249], loss=65.8824
	step [231/249], loss=69.5914
	step [232/249], loss=67.2673
	step [233/249], loss=74.1627
	step [234/249], loss=62.8659
	step [235/249], loss=70.6520
	step [236/249], loss=79.8018
	step [237/249], loss=78.5156
	step [238/249], loss=72.3763
	step [239/249], loss=65.7654
	step [240/249], loss=71.9441
	step [241/249], loss=80.0429
	step [242/249], loss=59.5738
	step [243/249], loss=75.4312
	step [244/249], loss=68.5690
	step [245/249], loss=73.1839
	step [246/249], loss=68.4926
	step [247/249], loss=64.9210
	step [248/249], loss=78.2646
	step [249/249], loss=54.0055
	Evaluating
	loss=0.0058, precision=0.3828, recall=0.8488, f1=0.5276
Training epoch 96
	step [1/249], loss=60.5002
	step [2/249], loss=70.3867
	step [3/249], loss=57.2383
	step [4/249], loss=81.1302
	step [5/249], loss=75.8930
	step [6/249], loss=79.1805
	step [7/249], loss=71.9104
	step [8/249], loss=68.7021
	step [9/249], loss=63.2435
	step [10/249], loss=84.1643
	step [11/249], loss=72.3810
	step [12/249], loss=77.0101
	step [13/249], loss=83.8822
	step [14/249], loss=79.0402
	step [15/249], loss=94.5606
	step [16/249], loss=70.3221
	step [17/249], loss=69.5704
	step [18/249], loss=69.9010
	step [19/249], loss=72.8530
	step [20/249], loss=83.7586
	step [21/249], loss=72.8792
	step [22/249], loss=58.0066
	step [23/249], loss=66.6613
	step [24/249], loss=73.3076
	step [25/249], loss=65.2423
	step [26/249], loss=89.7593
	step [27/249], loss=84.5026
	step [28/249], loss=78.8560
	step [29/249], loss=72.3749
	step [30/249], loss=83.2423
	step [31/249], loss=72.1973
	step [32/249], loss=80.3753
	step [33/249], loss=89.3257
	step [34/249], loss=77.4258
	step [35/249], loss=74.3542
	step [36/249], loss=76.3322
	step [37/249], loss=79.4270
	step [38/249], loss=83.0154
	step [39/249], loss=76.2293
	step [40/249], loss=68.7155
	step [41/249], loss=76.9263
	step [42/249], loss=58.6544
	step [43/249], loss=73.6882
	step [44/249], loss=79.3462
	step [45/249], loss=78.2822
	step [46/249], loss=70.6197
	step [47/249], loss=65.5777
	step [48/249], loss=69.7018
	step [49/249], loss=60.6439
	step [50/249], loss=70.3750
	step [51/249], loss=60.3883
	step [52/249], loss=82.8861
	step [53/249], loss=72.0428
	step [54/249], loss=71.2599
	step [55/249], loss=63.6703
	step [56/249], loss=72.2683
	step [57/249], loss=83.9949
	step [58/249], loss=78.6631
	step [59/249], loss=79.4706
	step [60/249], loss=78.5588
	step [61/249], loss=70.5906
	step [62/249], loss=76.8997
	step [63/249], loss=60.9825
	step [64/249], loss=64.9111
	step [65/249], loss=65.6624
	step [66/249], loss=98.3729
	step [67/249], loss=66.6551
	step [68/249], loss=76.9885
	step [69/249], loss=57.2617
	step [70/249], loss=80.6601
	step [71/249], loss=82.4678
	step [72/249], loss=68.7791
	step [73/249], loss=71.3657
	step [74/249], loss=61.2742
	step [75/249], loss=65.8891
	step [76/249], loss=86.5333
	step [77/249], loss=79.4982
	step [78/249], loss=49.3931
	step [79/249], loss=69.7302
	step [80/249], loss=69.6079
	step [81/249], loss=73.6127
	step [82/249], loss=79.0997
	step [83/249], loss=69.6924
	step [84/249], loss=51.9316
	step [85/249], loss=72.2036
	step [86/249], loss=88.7858
	step [87/249], loss=78.8311
	step [88/249], loss=75.0551
	step [89/249], loss=62.5295
	step [90/249], loss=102.5916
	step [91/249], loss=82.4072
	step [92/249], loss=69.2433
	step [93/249], loss=60.7570
	step [94/249], loss=74.6192
	step [95/249], loss=66.3079
	step [96/249], loss=76.4540
	step [97/249], loss=72.2607
	step [98/249], loss=76.3327
	step [99/249], loss=83.3964
	step [100/249], loss=95.6332
	step [101/249], loss=67.3424
	step [102/249], loss=73.2076
	step [103/249], loss=68.1679
	step [104/249], loss=53.0369
	step [105/249], loss=79.9149
	step [106/249], loss=54.5532
	step [107/249], loss=78.0620
	step [108/249], loss=61.0153
	step [109/249], loss=70.3192
	step [110/249], loss=69.4668
	step [111/249], loss=77.1986
	step [112/249], loss=62.8813
	step [113/249], loss=76.9469
	step [114/249], loss=76.4231
	step [115/249], loss=73.7522
	step [116/249], loss=73.0518
	step [117/249], loss=80.5906
	step [118/249], loss=79.3303
	step [119/249], loss=77.2060
	step [120/249], loss=60.8052
	step [121/249], loss=74.1116
	step [122/249], loss=70.8685
	step [123/249], loss=68.3964
	step [124/249], loss=72.4028
	step [125/249], loss=71.5955
	step [126/249], loss=70.0552
	step [127/249], loss=69.6836
	step [128/249], loss=76.5902
	step [129/249], loss=81.8076
	step [130/249], loss=66.4484
	step [131/249], loss=63.1264
	step [132/249], loss=66.8938
	step [133/249], loss=73.1695
	step [134/249], loss=68.2896
	step [135/249], loss=56.8119
	step [136/249], loss=63.8569
	step [137/249], loss=78.9438
	step [138/249], loss=61.0606
	step [139/249], loss=76.7034
	step [140/249], loss=66.8593
	step [141/249], loss=72.2045
	step [142/249], loss=59.3714
	step [143/249], loss=66.6646
	step [144/249], loss=59.8528
	step [145/249], loss=65.1124
	step [146/249], loss=71.5798
	step [147/249], loss=73.1974
	step [148/249], loss=76.9775
	step [149/249], loss=102.4634
	step [150/249], loss=69.9932
	step [151/249], loss=65.3232
	step [152/249], loss=71.9897
	step [153/249], loss=64.7231
	step [154/249], loss=68.3586
	step [155/249], loss=64.1742
	step [156/249], loss=83.8868
	step [157/249], loss=65.3961
	step [158/249], loss=79.2104
	step [159/249], loss=78.7681
	step [160/249], loss=54.4234
	step [161/249], loss=82.9383
	step [162/249], loss=68.1298
	step [163/249], loss=56.2001
	step [164/249], loss=76.5751
	step [165/249], loss=71.2781
	step [166/249], loss=89.2688
	step [167/249], loss=73.6810
	step [168/249], loss=77.5497
	step [169/249], loss=87.7241
	step [170/249], loss=79.3289
	step [171/249], loss=65.9661
	step [172/249], loss=56.6379
	step [173/249], loss=89.6860
	step [174/249], loss=60.4439
	step [175/249], loss=73.5355
	step [176/249], loss=78.9260
	step [177/249], loss=65.4668
	step [178/249], loss=78.0920
	step [179/249], loss=79.7413
	step [180/249], loss=71.4909
	step [181/249], loss=74.3175
	step [182/249], loss=68.1504
	step [183/249], loss=62.7565
	step [184/249], loss=80.7189
	step [185/249], loss=62.7223
	step [186/249], loss=70.2902
	step [187/249], loss=89.0926
	step [188/249], loss=87.4106
	step [189/249], loss=71.6731
	step [190/249], loss=69.8231
	step [191/249], loss=80.2142
	step [192/249], loss=67.9032
	step [193/249], loss=71.9921
	step [194/249], loss=71.5611
	step [195/249], loss=79.9974
	step [196/249], loss=57.5905
	step [197/249], loss=78.8039
	step [198/249], loss=79.5526
	step [199/249], loss=71.2015
	step [200/249], loss=73.9757
	step [201/249], loss=61.9586
	step [202/249], loss=79.4622
	step [203/249], loss=63.8289
	step [204/249], loss=62.1663
	step [205/249], loss=73.9070
	step [206/249], loss=68.7020
	step [207/249], loss=71.3878
	step [208/249], loss=73.6032
	step [209/249], loss=58.4404
	step [210/249], loss=53.6688
	step [211/249], loss=95.1866
	step [212/249], loss=73.5328
	step [213/249], loss=75.0823
	step [214/249], loss=89.5237
	step [215/249], loss=60.1341
	step [216/249], loss=73.5359
	step [217/249], loss=79.1196
	step [218/249], loss=79.7484
	step [219/249], loss=71.1268
	step [220/249], loss=71.8164
	step [221/249], loss=71.1703
	step [222/249], loss=96.5090
	step [223/249], loss=85.7974
	step [224/249], loss=70.7980
	step [225/249], loss=74.5155
	step [226/249], loss=83.0774
	step [227/249], loss=53.5557
	step [228/249], loss=53.4905
	step [229/249], loss=72.3870
	step [230/249], loss=61.4080
	step [231/249], loss=72.6680
	step [232/249], loss=72.8598
	step [233/249], loss=74.8129
	step [234/249], loss=59.1346
	step [235/249], loss=73.0766
	step [236/249], loss=76.9458
	step [237/249], loss=64.4391
	step [238/249], loss=73.4269
	step [239/249], loss=73.1582
	step [240/249], loss=70.6967
	step [241/249], loss=59.8515
	step [242/249], loss=85.1092
	step [243/249], loss=69.4018
	step [244/249], loss=74.3421
	step [245/249], loss=73.3465
	step [246/249], loss=76.9338
	step [247/249], loss=48.0655
	step [248/249], loss=70.4489
	step [249/249], loss=52.0134
	Evaluating
	loss=0.0070, precision=0.3196, recall=0.8407, f1=0.4631
Training epoch 97
	step [1/249], loss=81.6521
	step [2/249], loss=65.5913
	step [3/249], loss=71.1700
	step [4/249], loss=59.6270
	step [5/249], loss=78.0813
	step [6/249], loss=62.8718
	step [7/249], loss=73.0118
	step [8/249], loss=57.4599
	step [9/249], loss=72.2388
	step [10/249], loss=72.9458
	step [11/249], loss=76.3555
	step [12/249], loss=77.1675
	step [13/249], loss=71.5603
	step [14/249], loss=86.7996
	step [15/249], loss=73.4018
	step [16/249], loss=71.3626
	step [17/249], loss=82.0282
	step [18/249], loss=60.7038
	step [19/249], loss=81.1666
	step [20/249], loss=75.0691
	step [21/249], loss=64.7733
	step [22/249], loss=65.6672
	step [23/249], loss=60.2526
	step [24/249], loss=65.1603
	step [25/249], loss=68.1875
	step [26/249], loss=73.7078
	step [27/249], loss=69.5983
	step [28/249], loss=85.7879
	step [29/249], loss=75.8516
	step [30/249], loss=71.4731
	step [31/249], loss=77.4877
	step [32/249], loss=78.2685
	step [33/249], loss=83.0113
	step [34/249], loss=64.0737
	step [35/249], loss=48.5020
	step [36/249], loss=68.7095
	step [37/249], loss=76.5293
	step [38/249], loss=59.4693
	step [39/249], loss=74.4327
	step [40/249], loss=71.3106
	step [41/249], loss=83.7547
	step [42/249], loss=59.2715
	step [43/249], loss=76.6242
	step [44/249], loss=80.3151
	step [45/249], loss=64.3067
	step [46/249], loss=71.7266
	step [47/249], loss=62.9360
	step [48/249], loss=65.5242
	step [49/249], loss=73.9133
	step [50/249], loss=70.9529
	step [51/249], loss=72.1346
	step [52/249], loss=85.0151
	step [53/249], loss=69.9492
	step [54/249], loss=64.0418
	step [55/249], loss=69.3950
	step [56/249], loss=77.3646
	step [57/249], loss=69.6772
	step [58/249], loss=73.4206
	step [59/249], loss=72.5172
	step [60/249], loss=75.1940
	step [61/249], loss=79.8168
	step [62/249], loss=80.5867
	step [63/249], loss=63.0855
	step [64/249], loss=74.5908
	step [65/249], loss=77.8018
	step [66/249], loss=68.3444
	step [67/249], loss=74.6103
	step [68/249], loss=61.6886
	step [69/249], loss=73.0454
	step [70/249], loss=75.2780
	step [71/249], loss=75.8347
	step [72/249], loss=64.7384
	step [73/249], loss=68.4866
	step [74/249], loss=67.0677
	step [75/249], loss=57.0988
	step [76/249], loss=63.0912
	step [77/249], loss=67.2032
	step [78/249], loss=74.1218
	step [79/249], loss=85.9863
	step [80/249], loss=76.4042
	step [81/249], loss=70.0293
	step [82/249], loss=69.1909
	step [83/249], loss=85.6119
	step [84/249], loss=77.3560
	step [85/249], loss=70.4588
	step [86/249], loss=70.8448
	step [87/249], loss=69.5751
	step [88/249], loss=64.1543
	step [89/249], loss=77.4593
	step [90/249], loss=76.2275
	step [91/249], loss=81.8380
	step [92/249], loss=68.7253
	step [93/249], loss=63.4599
	step [94/249], loss=58.8628
	step [95/249], loss=75.2315
	step [96/249], loss=74.6241
	step [97/249], loss=81.0749
	step [98/249], loss=93.5098
	step [99/249], loss=78.6377
	step [100/249], loss=77.7482
	step [101/249], loss=84.9980
	step [102/249], loss=59.3257
	step [103/249], loss=76.0036
	step [104/249], loss=80.2720
	step [105/249], loss=84.7338
	step [106/249], loss=64.9905
	step [107/249], loss=79.1327
	step [108/249], loss=67.7233
	step [109/249], loss=48.0536
	step [110/249], loss=64.6178
	step [111/249], loss=65.6319
	step [112/249], loss=71.6976
	step [113/249], loss=73.7281
	step [114/249], loss=71.8013
	step [115/249], loss=72.3726
	step [116/249], loss=71.6330
	step [117/249], loss=68.2354
	step [118/249], loss=82.3646
	step [119/249], loss=67.6531
	step [120/249], loss=77.8028
	step [121/249], loss=64.9411
	step [122/249], loss=65.0585
	step [123/249], loss=79.9470
	step [124/249], loss=83.3866
	step [125/249], loss=76.4553
	step [126/249], loss=65.6675
	step [127/249], loss=74.1269
	step [128/249], loss=77.3597
	step [129/249], loss=72.4082
	step [130/249], loss=68.7655
	step [131/249], loss=88.8473
	step [132/249], loss=67.5489
	step [133/249], loss=80.0590
	step [134/249], loss=88.1656
	step [135/249], loss=80.2015
	step [136/249], loss=79.6528
	step [137/249], loss=70.1953
	step [138/249], loss=68.5099
	step [139/249], loss=74.4641
	step [140/249], loss=74.8901
	step [141/249], loss=71.4692
	step [142/249], loss=76.9754
	step [143/249], loss=72.4600
	step [144/249], loss=75.5810
	step [145/249], loss=76.4404
	step [146/249], loss=70.9939
	step [147/249], loss=81.0080
	step [148/249], loss=61.7458
	step [149/249], loss=73.4146
	step [150/249], loss=71.7767
	step [151/249], loss=65.1782
	step [152/249], loss=59.0742
	step [153/249], loss=71.9941
	step [154/249], loss=77.4517
	step [155/249], loss=82.2630
	step [156/249], loss=73.5044
	step [157/249], loss=62.6365
	step [158/249], loss=69.1388
	step [159/249], loss=69.6757
	step [160/249], loss=79.1073
	step [161/249], loss=70.9825
	step [162/249], loss=78.7104
	step [163/249], loss=65.6265
	step [164/249], loss=69.9156
	step [165/249], loss=72.1862
	step [166/249], loss=58.9831
	step [167/249], loss=79.8274
	step [168/249], loss=82.7411
	step [169/249], loss=65.2160
	step [170/249], loss=66.5007
	step [171/249], loss=72.7428
	step [172/249], loss=69.4258
	step [173/249], loss=62.2300
	step [174/249], loss=77.9892
	step [175/249], loss=82.6576
	step [176/249], loss=66.5114
	step [177/249], loss=74.4497
	step [178/249], loss=87.8292
	step [179/249], loss=73.0912
	step [180/249], loss=70.1485
	step [181/249], loss=87.4148
	step [182/249], loss=70.0322
	step [183/249], loss=62.9275
	step [184/249], loss=71.4578
	step [185/249], loss=86.9487
	step [186/249], loss=77.9532
	step [187/249], loss=79.9987
	step [188/249], loss=64.9493
	step [189/249], loss=79.6903
	step [190/249], loss=63.5663
	step [191/249], loss=58.3603
	step [192/249], loss=78.9504
	step [193/249], loss=75.0120
	step [194/249], loss=74.2521
	step [195/249], loss=74.0652
	step [196/249], loss=76.5578
	step [197/249], loss=86.3586
	step [198/249], loss=76.7212
	step [199/249], loss=66.2997
	step [200/249], loss=69.3020
	step [201/249], loss=66.1831
	step [202/249], loss=76.4949
	step [203/249], loss=59.2888
	step [204/249], loss=85.5424
	step [205/249], loss=63.5684
	step [206/249], loss=62.4260
	step [207/249], loss=69.1466
	step [208/249], loss=78.0944
	step [209/249], loss=69.1176
	step [210/249], loss=72.4765
	step [211/249], loss=67.9523
	step [212/249], loss=81.1586
	step [213/249], loss=60.1794
	step [214/249], loss=80.0340
	step [215/249], loss=64.3895
	step [216/249], loss=69.9469
	step [217/249], loss=59.5513
	step [218/249], loss=76.3551
	step [219/249], loss=83.3998
	step [220/249], loss=90.9049
	step [221/249], loss=73.7262
	step [222/249], loss=59.8571
	step [223/249], loss=60.3582
	step [224/249], loss=79.2127
	step [225/249], loss=65.6918
	step [226/249], loss=73.9858
	step [227/249], loss=67.1713
	step [228/249], loss=82.9268
	step [229/249], loss=85.6772
	step [230/249], loss=64.9011
	step [231/249], loss=91.4487
	step [232/249], loss=75.0512
	step [233/249], loss=73.0764
	step [234/249], loss=79.1238
	step [235/249], loss=63.4134
	step [236/249], loss=63.7746
	step [237/249], loss=71.1354
	step [238/249], loss=63.9704
	step [239/249], loss=85.8073
	step [240/249], loss=70.6296
	step [241/249], loss=54.5075
	step [242/249], loss=78.4705
	step [243/249], loss=75.2196
	step [244/249], loss=68.9876
	step [245/249], loss=62.0584
	step [246/249], loss=69.9544
	step [247/249], loss=64.2756
	step [248/249], loss=67.9553
	step [249/249], loss=60.3852
	Evaluating
	loss=0.0058, precision=0.3747, recall=0.8360, f1=0.5175
Training epoch 98
	step [1/249], loss=54.7807
	step [2/249], loss=63.5267
	step [3/249], loss=70.3635
	step [4/249], loss=61.4394
	step [5/249], loss=72.7775
	step [6/249], loss=80.4173
	step [7/249], loss=72.6683
	step [8/249], loss=73.3215
	step [9/249], loss=70.5263
	step [10/249], loss=67.6439
	step [11/249], loss=71.3443
	step [12/249], loss=64.9881
	step [13/249], loss=72.6072
	step [14/249], loss=63.3027
	step [15/249], loss=73.3679
	step [16/249], loss=67.6549
	step [17/249], loss=57.6688
	step [18/249], loss=64.2637
	step [19/249], loss=57.9762
	step [20/249], loss=55.5743
	step [21/249], loss=83.5723
	step [22/249], loss=62.1736
	step [23/249], loss=75.6914
	step [24/249], loss=73.1756
	step [25/249], loss=65.9180
	step [26/249], loss=85.7670
	step [27/249], loss=66.3070
	step [28/249], loss=75.7578
	step [29/249], loss=86.8358
	step [30/249], loss=71.9455
	step [31/249], loss=69.5171
	step [32/249], loss=74.5912
	step [33/249], loss=87.7073
	step [34/249], loss=69.5959
	step [35/249], loss=79.5399
	step [36/249], loss=86.0766
	step [37/249], loss=70.6347
	step [38/249], loss=68.6955
	step [39/249], loss=67.8723
	step [40/249], loss=74.2710
	step [41/249], loss=80.3051
	step [42/249], loss=83.6529
	step [43/249], loss=75.4301
	step [44/249], loss=59.3828
	step [45/249], loss=60.0726
	step [46/249], loss=71.3842
	step [47/249], loss=70.4747
	step [48/249], loss=92.5589
	step [49/249], loss=75.6627
	step [50/249], loss=71.4824
	step [51/249], loss=79.7260
	step [52/249], loss=62.3656
	step [53/249], loss=69.5963
	step [54/249], loss=67.2322
	step [55/249], loss=82.8320
	step [56/249], loss=82.1018
	step [57/249], loss=59.6139
	step [58/249], loss=69.7251
	step [59/249], loss=86.8141
	step [60/249], loss=67.7162
	step [61/249], loss=61.5666
	step [62/249], loss=72.9288
	step [63/249], loss=52.0136
	step [64/249], loss=68.1830
	step [65/249], loss=77.3143
	step [66/249], loss=66.3215
	step [67/249], loss=79.6438
	step [68/249], loss=63.8209
	step [69/249], loss=58.9232
	step [70/249], loss=78.0861
	step [71/249], loss=71.0220
	step [72/249], loss=87.3447
	step [73/249], loss=86.1117
	step [74/249], loss=75.8499
	step [75/249], loss=83.2585
	step [76/249], loss=72.5896
	step [77/249], loss=66.3005
	step [78/249], loss=71.3494
	step [79/249], loss=71.8908
	step [80/249], loss=75.9235
	step [81/249], loss=60.8610
	step [82/249], loss=70.0002
	step [83/249], loss=67.5825
	step [84/249], loss=77.0488
	step [85/249], loss=59.4098
	step [86/249], loss=59.6416
	step [87/249], loss=70.0671
	step [88/249], loss=85.5164
	step [89/249], loss=66.1517
	step [90/249], loss=81.6068
	step [91/249], loss=80.1569
	step [92/249], loss=68.5670
	step [93/249], loss=69.1543
	step [94/249], loss=75.4574
	step [95/249], loss=70.5141
	step [96/249], loss=59.3348
	step [97/249], loss=70.8980
	step [98/249], loss=70.9527
	step [99/249], loss=83.1518
	step [100/249], loss=73.1204
	step [101/249], loss=81.5170
	step [102/249], loss=72.9029
	step [103/249], loss=65.1241
	step [104/249], loss=64.9485
	step [105/249], loss=88.3505
	step [106/249], loss=57.6720
	step [107/249], loss=68.1156
	step [108/249], loss=58.4289
	step [109/249], loss=82.2467
	step [110/249], loss=65.7934
	step [111/249], loss=66.6725
	step [112/249], loss=73.5616
	step [113/249], loss=79.3789
	step [114/249], loss=61.5633
	step [115/249], loss=62.2443
	step [116/249], loss=69.5706
	step [117/249], loss=70.0796
	step [118/249], loss=81.9668
	step [119/249], loss=63.3346
	step [120/249], loss=62.0950
	step [121/249], loss=81.7777
	step [122/249], loss=61.3645
	step [123/249], loss=62.4015
	step [124/249], loss=80.2687
	step [125/249], loss=81.5992
	step [126/249], loss=70.3188
	step [127/249], loss=76.6266
	step [128/249], loss=69.3278
	step [129/249], loss=77.2800
	step [130/249], loss=84.5788
	step [131/249], loss=74.0000
	step [132/249], loss=71.5769
	step [133/249], loss=91.2523
	step [134/249], loss=77.8099
	step [135/249], loss=91.5223
	step [136/249], loss=69.2694
	step [137/249], loss=92.1024
	step [138/249], loss=75.7325
	step [139/249], loss=63.5866
	step [140/249], loss=64.1813
	step [141/249], loss=88.3844
	step [142/249], loss=83.4124
	step [143/249], loss=64.9607
	step [144/249], loss=83.7237
	step [145/249], loss=83.0848
	step [146/249], loss=64.4714
	step [147/249], loss=85.9287
	step [148/249], loss=71.7372
	step [149/249], loss=70.4767
	step [150/249], loss=74.3344
	step [151/249], loss=76.8763
	step [152/249], loss=69.8445
	step [153/249], loss=62.6127
	step [154/249], loss=79.5959
	step [155/249], loss=67.8870
	step [156/249], loss=72.2539
	step [157/249], loss=62.5819
	step [158/249], loss=68.9208
	step [159/249], loss=93.5565
	step [160/249], loss=66.4329
	step [161/249], loss=72.5431
	step [162/249], loss=66.8492
	step [163/249], loss=67.2224
	step [164/249], loss=77.8939
	step [165/249], loss=75.4367
	step [166/249], loss=77.5506
	step [167/249], loss=80.8276
	step [168/249], loss=62.6985
	step [169/249], loss=73.4337
	step [170/249], loss=56.7215
	step [171/249], loss=67.3835
	step [172/249], loss=65.0355
	step [173/249], loss=75.5043
	step [174/249], loss=80.0089
	step [175/249], loss=94.9880
	step [176/249], loss=76.3818
	step [177/249], loss=69.2720
	step [178/249], loss=78.1137
	step [179/249], loss=72.7481
	step [180/249], loss=72.2754
	step [181/249], loss=69.2637
	step [182/249], loss=77.2843
	step [183/249], loss=82.3692
	step [184/249], loss=73.9977
	step [185/249], loss=69.0758
	step [186/249], loss=64.4689
	step [187/249], loss=79.1046
	step [188/249], loss=68.1520
	step [189/249], loss=76.6482
	step [190/249], loss=73.8376
	step [191/249], loss=78.9750
	step [192/249], loss=84.7263
	step [193/249], loss=87.3684
	step [194/249], loss=70.8244
	step [195/249], loss=67.2184
	step [196/249], loss=81.6308
	step [197/249], loss=82.9871
	step [198/249], loss=77.2983
	step [199/249], loss=75.3016
	step [200/249], loss=76.3388
	step [201/249], loss=60.3575
	step [202/249], loss=61.2363
	step [203/249], loss=85.3710
	step [204/249], loss=60.0649
	step [205/249], loss=77.9967
	step [206/249], loss=71.7209
	step [207/249], loss=55.8283
	step [208/249], loss=79.8187
	step [209/249], loss=71.2960
	step [210/249], loss=83.5700
	step [211/249], loss=71.4261
	step [212/249], loss=66.0487
	step [213/249], loss=76.8015
	step [214/249], loss=75.0943
	step [215/249], loss=71.8661
	step [216/249], loss=96.9230
	step [217/249], loss=68.8834
	step [218/249], loss=60.2761
	step [219/249], loss=59.8259
	step [220/249], loss=66.7946
	step [221/249], loss=73.4530
	step [222/249], loss=67.0891
	step [223/249], loss=76.9672
	step [224/249], loss=72.8166
	step [225/249], loss=81.2973
	step [226/249], loss=67.6526
	step [227/249], loss=67.7495
	step [228/249], loss=59.9965
	step [229/249], loss=59.3412
	step [230/249], loss=68.4128
	step [231/249], loss=81.5107
	step [232/249], loss=79.4718
	step [233/249], loss=74.8243
	step [234/249], loss=62.3634
	step [235/249], loss=72.0292
	step [236/249], loss=68.7588
	step [237/249], loss=67.1293
	step [238/249], loss=80.9138
	step [239/249], loss=64.6966
	step [240/249], loss=86.3311
	step [241/249], loss=65.0031
	step [242/249], loss=79.0598
	step [243/249], loss=68.9780
	step [244/249], loss=68.6664
	step [245/249], loss=67.1834
	step [246/249], loss=72.3826
	step [247/249], loss=73.8603
	step [248/249], loss=62.8641
	step [249/249], loss=50.1191
	Evaluating
	loss=0.0052, precision=0.4111, recall=0.8393, f1=0.5519
saving model as: 3_saved_model.pth
Training epoch 99
	step [1/249], loss=60.8580
	step [2/249], loss=65.9753
	step [3/249], loss=69.8406
	step [4/249], loss=78.2875
	step [5/249], loss=64.1969
	step [6/249], loss=63.8556
	step [7/249], loss=73.3535
	step [8/249], loss=68.4928
	step [9/249], loss=80.5106
	step [10/249], loss=68.9884
	step [11/249], loss=64.2180
	step [12/249], loss=79.9704
	step [13/249], loss=97.3525
	step [14/249], loss=71.0344
	step [15/249], loss=57.3575
	step [16/249], loss=71.2209
	step [17/249], loss=73.6145
	step [18/249], loss=63.7484
	step [19/249], loss=85.1403
	step [20/249], loss=57.5716
	step [21/249], loss=73.1489
	step [22/249], loss=85.0173
	step [23/249], loss=88.0053
	step [24/249], loss=70.8036
	step [25/249], loss=69.5969
	step [26/249], loss=71.7746
	step [27/249], loss=81.5333
	step [28/249], loss=68.4394
	step [29/249], loss=72.0144
	step [30/249], loss=79.1067
	step [31/249], loss=85.5635
	step [32/249], loss=67.9261
	step [33/249], loss=61.6537
	step [34/249], loss=89.2372
	step [35/249], loss=74.7962
	step [36/249], loss=62.7490
	step [37/249], loss=95.2254
	step [38/249], loss=81.8627
	step [39/249], loss=60.6263
	step [40/249], loss=79.8210
	step [41/249], loss=74.3385
	step [42/249], loss=56.8902
	step [43/249], loss=72.5667
	step [44/249], loss=72.3150
	step [45/249], loss=69.4539
	step [46/249], loss=87.6196
	step [47/249], loss=73.7546
	step [48/249], loss=58.1316
	step [49/249], loss=64.8043
	step [50/249], loss=64.3345
	step [51/249], loss=74.5854
	step [52/249], loss=63.9948
	step [53/249], loss=81.2289
	step [54/249], loss=67.3571
	step [55/249], loss=61.2227
	step [56/249], loss=69.4067
	step [57/249], loss=58.1120
	step [58/249], loss=67.8364
	step [59/249], loss=54.6643
	step [60/249], loss=67.9040
	step [61/249], loss=69.0691
	step [62/249], loss=94.3330
	step [63/249], loss=73.3996
	step [64/249], loss=76.1042
	step [65/249], loss=80.6184
	step [66/249], loss=60.4406
	step [67/249], loss=72.7743
	step [68/249], loss=80.1055
	step [69/249], loss=78.2284
	step [70/249], loss=64.0413
	step [71/249], loss=85.4980
	step [72/249], loss=79.8238
	step [73/249], loss=68.7984
	step [74/249], loss=71.4219
	step [75/249], loss=77.4295
	step [76/249], loss=77.4842
	step [77/249], loss=65.8356
	step [78/249], loss=63.7488
	step [79/249], loss=70.8143
	step [80/249], loss=57.1531
	step [81/249], loss=71.6810
	step [82/249], loss=71.0190
	step [83/249], loss=68.5468
	step [84/249], loss=64.4617
	step [85/249], loss=76.0614
	step [86/249], loss=76.3477
	step [87/249], loss=64.5754
	step [88/249], loss=64.1889
	step [89/249], loss=67.6968
	step [90/249], loss=77.7755
	step [91/249], loss=60.9781
	step [92/249], loss=58.4260
	step [93/249], loss=69.6507
	step [94/249], loss=62.0004
	step [95/249], loss=88.2121
	step [96/249], loss=62.5684
	step [97/249], loss=69.9336
	step [98/249], loss=75.1572
	step [99/249], loss=73.7517
	step [100/249], loss=78.9654
	step [101/249], loss=88.3395
	step [102/249], loss=66.2391
	step [103/249], loss=72.2066
	step [104/249], loss=84.8514
	step [105/249], loss=75.5532
	step [106/249], loss=66.4298
	step [107/249], loss=62.5085
	step [108/249], loss=65.1657
	step [109/249], loss=77.4749
	step [110/249], loss=70.2182
	step [111/249], loss=86.2541
	step [112/249], loss=77.4887
	step [113/249], loss=80.8345
	step [114/249], loss=86.8903
	step [115/249], loss=81.5720
	step [116/249], loss=70.4808
	step [117/249], loss=74.3363
	step [118/249], loss=73.8674
	step [119/249], loss=75.4552
	step [120/249], loss=79.1785
	step [121/249], loss=64.4674
	step [122/249], loss=55.4690
	step [123/249], loss=55.8691
	step [124/249], loss=67.8326
	step [125/249], loss=75.4617
	step [126/249], loss=81.4702
	step [127/249], loss=67.5999
	step [128/249], loss=88.3281
	step [129/249], loss=72.1153
	step [130/249], loss=62.1314
	step [131/249], loss=81.1207
	step [132/249], loss=65.7507
	step [133/249], loss=62.2358
	step [134/249], loss=63.3596
	step [135/249], loss=71.5043
	step [136/249], loss=90.7742
	step [137/249], loss=58.4907
	step [138/249], loss=74.3765
	step [139/249], loss=59.4694
	step [140/249], loss=62.3253
	step [141/249], loss=81.5635
	step [142/249], loss=83.9697
	step [143/249], loss=87.1507
	step [144/249], loss=63.4493
	step [145/249], loss=77.3709
	step [146/249], loss=65.1971
	step [147/249], loss=70.7695
	step [148/249], loss=64.0747
	step [149/249], loss=60.2394
	step [150/249], loss=70.0900
	step [151/249], loss=79.3041
	step [152/249], loss=79.8119
	step [153/249], loss=65.6745
	step [154/249], loss=65.1318
	step [155/249], loss=82.3844
	step [156/249], loss=64.4619
	step [157/249], loss=68.8169
	step [158/249], loss=83.0195
	step [159/249], loss=62.9115
	step [160/249], loss=66.8965
	step [161/249], loss=76.8432
	step [162/249], loss=75.4799
	step [163/249], loss=68.0849
	step [164/249], loss=66.0126
	step [165/249], loss=79.7146
	step [166/249], loss=75.2113
	step [167/249], loss=64.6242
	step [168/249], loss=67.5682
	step [169/249], loss=69.5311
	step [170/249], loss=71.2800
	step [171/249], loss=70.1349
	step [172/249], loss=68.3602
	step [173/249], loss=85.1481
	step [174/249], loss=69.3534
	step [175/249], loss=80.5867
	step [176/249], loss=65.7481
	step [177/249], loss=70.5547
	step [178/249], loss=76.3960
	step [179/249], loss=70.7430
	step [180/249], loss=56.0084
	step [181/249], loss=64.6939
	step [182/249], loss=92.0530
	step [183/249], loss=70.3419
	step [184/249], loss=59.0338
	step [185/249], loss=84.5604
	step [186/249], loss=73.6843
	step [187/249], loss=55.2676
	step [188/249], loss=71.8923
	step [189/249], loss=75.6941
	step [190/249], loss=71.7957
	step [191/249], loss=63.5837
	step [192/249], loss=71.7225
	step [193/249], loss=81.2925
	step [194/249], loss=88.8565
	step [195/249], loss=60.9072
	step [196/249], loss=66.1599
	step [197/249], loss=87.3664
	step [198/249], loss=57.1692
	step [199/249], loss=88.0570
	step [200/249], loss=58.1780
	step [201/249], loss=74.9814
	step [202/249], loss=77.7119
	step [203/249], loss=70.8982
	step [204/249], loss=71.4976
	step [205/249], loss=79.9684
	step [206/249], loss=67.7315
	step [207/249], loss=66.3777
	step [208/249], loss=72.8054
	step [209/249], loss=66.3970
	step [210/249], loss=88.3416
	step [211/249], loss=62.2300
	step [212/249], loss=71.5442
	step [213/249], loss=86.8204
	step [214/249], loss=56.8139
	step [215/249], loss=68.8298
	step [216/249], loss=62.7845
	step [217/249], loss=72.3358
	step [218/249], loss=69.2473
	step [219/249], loss=81.5845
	step [220/249], loss=63.6718
	step [221/249], loss=85.5993
	step [222/249], loss=81.4748
	step [223/249], loss=78.3878
	step [224/249], loss=61.0405
	step [225/249], loss=75.6448
	step [226/249], loss=75.7383
	step [227/249], loss=76.7903
	step [228/249], loss=66.8077
	step [229/249], loss=81.1241
	step [230/249], loss=75.1104
	step [231/249], loss=83.2895
	step [232/249], loss=66.7741
	step [233/249], loss=80.9116
	step [234/249], loss=70.5161
	step [235/249], loss=68.2753
	step [236/249], loss=82.4208
	step [237/249], loss=78.6590
	step [238/249], loss=67.9317
	step [239/249], loss=82.8098
	step [240/249], loss=75.0983
	step [241/249], loss=68.7409
	step [242/249], loss=71.0331
	step [243/249], loss=71.5278
	step [244/249], loss=74.7031
	step [245/249], loss=79.3831
	step [246/249], loss=65.6391
	step [247/249], loss=73.8047
	step [248/249], loss=60.1786
	step [249/249], loss=43.3240
	Evaluating
	loss=0.0051, precision=0.4160, recall=0.8426, f1=0.5570
saving model as: 3_saved_model.pth
Training epoch 100
	step [1/249], loss=60.6342
	step [2/249], loss=58.0548
	step [3/249], loss=79.2333
	step [4/249], loss=61.7842
	step [5/249], loss=59.5679
	step [6/249], loss=69.7438
	step [7/249], loss=71.4971
	step [8/249], loss=77.2047
	step [9/249], loss=65.0348
	step [10/249], loss=68.3857
	step [11/249], loss=80.8308
	step [12/249], loss=79.3927
	step [13/249], loss=61.7188
	step [14/249], loss=80.9422
	step [15/249], loss=65.1850
	step [16/249], loss=65.5776
	step [17/249], loss=66.0154
	step [18/249], loss=72.3310
	step [19/249], loss=68.0829
	step [20/249], loss=82.6768
	step [21/249], loss=69.1140
	step [22/249], loss=81.2494
	step [23/249], loss=59.9304
	step [24/249], loss=73.7314
	step [25/249], loss=70.7736
	step [26/249], loss=81.4630
	step [27/249], loss=69.2149
	step [28/249], loss=69.7132
	step [29/249], loss=81.4348
	step [30/249], loss=66.3399
	step [31/249], loss=77.2598
	step [32/249], loss=74.2439
	step [33/249], loss=81.3777
	step [34/249], loss=66.0968
	step [35/249], loss=67.7039
	step [36/249], loss=62.6352
	step [37/249], loss=71.1420
	step [38/249], loss=67.0197
	step [39/249], loss=74.0222
	step [40/249], loss=87.1569
	step [41/249], loss=73.1699
	step [42/249], loss=74.2116
	step [43/249], loss=72.7072
	step [44/249], loss=89.9978
	step [45/249], loss=59.5348
	step [46/249], loss=63.8852
	step [47/249], loss=65.5537
	step [48/249], loss=73.6392
	step [49/249], loss=63.7253
	step [50/249], loss=62.4976
	step [51/249], loss=84.1291
	step [52/249], loss=73.9006
	step [53/249], loss=83.0997
	step [54/249], loss=65.0198
	step [55/249], loss=70.4139
	step [56/249], loss=67.0205
	step [57/249], loss=61.2142
	step [58/249], loss=65.8335
	step [59/249], loss=85.9084
	step [60/249], loss=90.1857
	step [61/249], loss=66.6252
	step [62/249], loss=70.1694
	step [63/249], loss=71.5552
	step [64/249], loss=84.9218
	step [65/249], loss=50.1973
	step [66/249], loss=66.9134
	step [67/249], loss=67.2290
	step [68/249], loss=81.2926
	step [69/249], loss=74.3114
	step [70/249], loss=74.9667
	step [71/249], loss=69.4283
	step [72/249], loss=86.3358
	step [73/249], loss=85.2551
	step [74/249], loss=67.4861
	step [75/249], loss=62.3540
	step [76/249], loss=74.0911
	step [77/249], loss=71.1919
	step [78/249], loss=72.7818
	step [79/249], loss=63.2532
	step [80/249], loss=57.3165
	step [81/249], loss=69.6991
	step [82/249], loss=78.2766
	step [83/249], loss=78.4571
	step [84/249], loss=62.0225
	step [85/249], loss=73.7825
	step [86/249], loss=76.4073
	step [87/249], loss=70.3632
	step [88/249], loss=70.9270
	step [89/249], loss=68.4868
	step [90/249], loss=74.8639
	step [91/249], loss=72.4331
	step [92/249], loss=76.3819
	step [93/249], loss=65.3267
	step [94/249], loss=76.3086
	step [95/249], loss=84.8735
	step [96/249], loss=79.6393
	step [97/249], loss=70.0283
	step [98/249], loss=80.8281
	step [99/249], loss=72.6594
	step [100/249], loss=63.7323
	step [101/249], loss=76.2986
	step [102/249], loss=68.5545
	step [103/249], loss=63.5133
	step [104/249], loss=70.9649
	step [105/249], loss=80.8422
	step [106/249], loss=72.0643
	step [107/249], loss=64.0293
	step [108/249], loss=71.5352
	step [109/249], loss=65.2552
	step [110/249], loss=55.2309
	step [111/249], loss=76.4876
	step [112/249], loss=85.1798
	step [113/249], loss=81.4517
	step [114/249], loss=53.2338
	step [115/249], loss=60.4860
	step [116/249], loss=73.3498
	step [117/249], loss=69.3982
	step [118/249], loss=57.1876
	step [119/249], loss=73.4988
	step [120/249], loss=74.1304
	step [121/249], loss=86.2550
	step [122/249], loss=74.2966
	step [123/249], loss=71.9971
	step [124/249], loss=65.1068
	step [125/249], loss=65.4772
	step [126/249], loss=74.8798
	step [127/249], loss=60.8262
	step [128/249], loss=69.1732
	step [129/249], loss=70.9178
	step [130/249], loss=84.2663
	step [131/249], loss=59.0335
	step [132/249], loss=71.4629
	step [133/249], loss=74.7648
	step [134/249], loss=83.7238
	step [135/249], loss=62.3675
	step [136/249], loss=75.1779
	step [137/249], loss=79.2274
	step [138/249], loss=60.4655
	step [139/249], loss=73.5811
	step [140/249], loss=63.1856
	step [141/249], loss=73.8388
	step [142/249], loss=61.1735
	step [143/249], loss=79.5361
	step [144/249], loss=69.4482
	step [145/249], loss=56.0175
	step [146/249], loss=63.9556
	step [147/249], loss=71.2418
	step [148/249], loss=73.3300
	step [149/249], loss=81.0226
	step [150/249], loss=56.5590
	step [151/249], loss=60.5310
	step [152/249], loss=65.0062
	step [153/249], loss=71.1579
	step [154/249], loss=81.0747
	step [155/249], loss=67.9687
	step [156/249], loss=84.2337
	step [157/249], loss=77.1183
	step [158/249], loss=92.8065
	step [159/249], loss=83.1843
	step [160/249], loss=75.0121
	step [161/249], loss=79.0542
	step [162/249], loss=71.2108
	step [163/249], loss=70.8536
	step [164/249], loss=64.6025
	step [165/249], loss=69.3771
	step [166/249], loss=62.9450
	step [167/249], loss=74.0287
	step [168/249], loss=64.1901
	step [169/249], loss=77.9877
	step [170/249], loss=63.8280
	step [171/249], loss=81.7300
	step [172/249], loss=77.2850
	step [173/249], loss=66.0270
	step [174/249], loss=80.3934
	step [175/249], loss=83.7612
	step [176/249], loss=64.3658
	step [177/249], loss=67.5803
	step [178/249], loss=82.1382
	step [179/249], loss=75.5449
	step [180/249], loss=69.6517
	step [181/249], loss=85.7433
	step [182/249], loss=84.2830
	step [183/249], loss=75.2377
	step [184/249], loss=77.2671
	step [185/249], loss=78.8663
	step [186/249], loss=65.0479
	step [187/249], loss=86.5524
	step [188/249], loss=76.4169
	step [189/249], loss=82.5940
	step [190/249], loss=85.2231
	step [191/249], loss=69.5039
	step [192/249], loss=65.8230
	step [193/249], loss=67.2617
	step [194/249], loss=79.1264
	step [195/249], loss=82.9551
	step [196/249], loss=64.3879
	step [197/249], loss=75.6184
	step [198/249], loss=70.4269
	step [199/249], loss=90.8618
	step [200/249], loss=67.7268
	step [201/249], loss=73.7910
	step [202/249], loss=56.4714
	step [203/249], loss=70.3804
	step [204/249], loss=76.0289
	step [205/249], loss=62.6201
	step [206/249], loss=62.3522
	step [207/249], loss=68.3631
	step [208/249], loss=69.5094
	step [209/249], loss=77.6350
	step [210/249], loss=72.9019
	step [211/249], loss=70.8760
	step [212/249], loss=83.5138
	step [213/249], loss=68.1086
	step [214/249], loss=82.9351
	step [215/249], loss=70.8789
	step [216/249], loss=77.1952
	step [217/249], loss=75.8863
	step [218/249], loss=77.5381
	step [219/249], loss=63.1171
	step [220/249], loss=57.8841
	step [221/249], loss=66.7502
	step [222/249], loss=81.4351
	step [223/249], loss=66.3680
	step [224/249], loss=66.9874
	step [225/249], loss=81.4603
	step [226/249], loss=69.4498
	step [227/249], loss=65.6108
	step [228/249], loss=70.3673
	step [229/249], loss=74.5608
	step [230/249], loss=68.9466
	step [231/249], loss=66.9343
	step [232/249], loss=68.4235
	step [233/249], loss=56.8546
	step [234/249], loss=69.9324
	step [235/249], loss=70.1761
	step [236/249], loss=74.4866
	step [237/249], loss=75.7121
	step [238/249], loss=79.2889
	step [239/249], loss=66.2873
	step [240/249], loss=65.7875
	step [241/249], loss=61.4118
	step [242/249], loss=79.0307
	step [243/249], loss=91.4663
	step [244/249], loss=81.6046
	step [245/249], loss=74.9120
	step [246/249], loss=82.9118
	step [247/249], loss=59.7647
	step [248/249], loss=62.3572
	step [249/249], loss=49.7198
	Evaluating
	loss=0.0055, precision=0.4039, recall=0.8356, f1=0.5445
Training epoch 101
	step [1/249], loss=70.7646
	step [2/249], loss=53.7986
	step [3/249], loss=62.4817
	step [4/249], loss=76.0323
	step [5/249], loss=76.8008
	step [6/249], loss=79.9161
	step [7/249], loss=72.3675
	step [8/249], loss=67.2492
	step [9/249], loss=71.9644
	step [10/249], loss=71.1807
	step [11/249], loss=66.9239
	step [12/249], loss=74.3236
	step [13/249], loss=72.1937
	step [14/249], loss=75.2972
	step [15/249], loss=54.4068
	step [16/249], loss=58.7479
	step [17/249], loss=66.0837
	step [18/249], loss=90.8000
	step [19/249], loss=66.5984
	step [20/249], loss=77.7389
	step [21/249], loss=77.2224
	step [22/249], loss=63.2221
	step [23/249], loss=64.7195
	step [24/249], loss=73.6119
	step [25/249], loss=69.1036
	step [26/249], loss=72.8180
	step [27/249], loss=77.0360
	step [28/249], loss=73.0715
	step [29/249], loss=89.2163
	step [30/249], loss=60.8363
	step [31/249], loss=63.8601
	step [32/249], loss=65.5845
	step [33/249], loss=68.4423
	step [34/249], loss=62.0649
	step [35/249], loss=73.2428
	step [36/249], loss=78.1076
	step [37/249], loss=77.2202
	step [38/249], loss=72.4227
	step [39/249], loss=76.0609
	step [40/249], loss=68.1751
	step [41/249], loss=71.8272
	step [42/249], loss=63.8681
	step [43/249], loss=86.0369
	step [44/249], loss=75.5706
	step [45/249], loss=65.2631
	step [46/249], loss=55.8879
	step [47/249], loss=80.2203
	step [48/249], loss=73.9143
	step [49/249], loss=75.3234
	step [50/249], loss=78.7061
	step [51/249], loss=85.2357
	step [52/249], loss=63.2896
	step [53/249], loss=71.7935
	step [54/249], loss=64.6130
	step [55/249], loss=55.9218
	step [56/249], loss=59.7199
	step [57/249], loss=57.8341
	step [58/249], loss=63.7632
	step [59/249], loss=64.7274
	step [60/249], loss=81.1516
	step [61/249], loss=77.9486
	step [62/249], loss=89.5019
	step [63/249], loss=75.9922
	step [64/249], loss=56.2876
	step [65/249], loss=60.9449
	step [66/249], loss=68.9703
	step [67/249], loss=76.6138
	step [68/249], loss=63.1493
	step [69/249], loss=71.3978
	step [70/249], loss=70.0435
	step [71/249], loss=66.8626
	step [72/249], loss=65.7148
	step [73/249], loss=76.3900
	step [74/249], loss=75.5148
	step [75/249], loss=64.8319
	step [76/249], loss=67.5671
	step [77/249], loss=62.4893
	step [78/249], loss=75.6740
	step [79/249], loss=66.1167
	step [80/249], loss=67.8885
	step [81/249], loss=79.2340
	step [82/249], loss=95.0480
	step [83/249], loss=73.7910
	step [84/249], loss=78.0346
	step [85/249], loss=76.6480
	step [86/249], loss=71.7153
	step [87/249], loss=87.1918
	step [88/249], loss=82.3694
	step [89/249], loss=74.6876
	step [90/249], loss=70.5597
	step [91/249], loss=79.6202
	step [92/249], loss=65.4782
	step [93/249], loss=76.8931
	step [94/249], loss=61.6233
	step [95/249], loss=65.6577
	step [96/249], loss=79.7649
	step [97/249], loss=87.4947
	step [98/249], loss=62.8523
	step [99/249], loss=62.0144
	step [100/249], loss=75.4504
	step [101/249], loss=76.2070
	step [102/249], loss=62.1233
	step [103/249], loss=68.8852
	step [104/249], loss=66.1382
	step [105/249], loss=76.3981
	step [106/249], loss=69.2031
	step [107/249], loss=73.0602
	step [108/249], loss=70.6293
	step [109/249], loss=72.2975
	step [110/249], loss=71.3579
	step [111/249], loss=68.1244
	step [112/249], loss=59.5317
	step [113/249], loss=72.9095
	step [114/249], loss=64.8279
	step [115/249], loss=73.9069
	step [116/249], loss=87.4189
	step [117/249], loss=67.2761
	step [118/249], loss=62.5417
	step [119/249], loss=64.5586
	step [120/249], loss=73.2456
	step [121/249], loss=75.0145
	step [122/249], loss=87.7885
	step [123/249], loss=71.2880
	step [124/249], loss=84.9171
	step [125/249], loss=85.8540
	step [126/249], loss=74.4927
	step [127/249], loss=62.6558
	step [128/249], loss=85.4453
	step [129/249], loss=76.1029
	step [130/249], loss=61.0306
	step [131/249], loss=70.7060
	step [132/249], loss=59.6722
	step [133/249], loss=68.0430
	step [134/249], loss=70.7067
	step [135/249], loss=63.5790
	step [136/249], loss=71.5007
	step [137/249], loss=86.6617
	step [138/249], loss=74.9658
	step [139/249], loss=90.3768
	step [140/249], loss=64.9035
	step [141/249], loss=69.5517
	step [142/249], loss=59.6052
	step [143/249], loss=77.2190
	step [144/249], loss=67.9660
	step [145/249], loss=74.5761
	step [146/249], loss=67.7094
	step [147/249], loss=58.5034
	step [148/249], loss=68.9007
	step [149/249], loss=78.0999
	step [150/249], loss=58.2750
	step [151/249], loss=67.6080
	step [152/249], loss=63.5816
	step [153/249], loss=71.4130
	step [154/249], loss=71.0057
	step [155/249], loss=79.4921
	step [156/249], loss=77.8338
	step [157/249], loss=65.4501
	step [158/249], loss=65.7769
	step [159/249], loss=78.4006
	step [160/249], loss=65.7569
	step [161/249], loss=69.4389
	step [162/249], loss=67.1852
	step [163/249], loss=69.0553
	step [164/249], loss=67.9788
	step [165/249], loss=64.7715
	step [166/249], loss=69.1875
	step [167/249], loss=70.3916
	step [168/249], loss=66.0401
	step [169/249], loss=67.7106
	step [170/249], loss=74.2962
	step [171/249], loss=77.8447
	step [172/249], loss=68.7435
	step [173/249], loss=90.1411
	step [174/249], loss=75.6460
	step [175/249], loss=75.9479
	step [176/249], loss=60.7772
	step [177/249], loss=79.1941
	step [178/249], loss=67.1370
	step [179/249], loss=60.6583
	step [180/249], loss=86.1683
	step [181/249], loss=77.8954
	step [182/249], loss=74.6315
	step [183/249], loss=76.6602
	step [184/249], loss=84.3452
	step [185/249], loss=77.5320
	step [186/249], loss=67.2058
	step [187/249], loss=64.7837
	step [188/249], loss=82.3664
	step [189/249], loss=82.6896
	step [190/249], loss=64.7343
	step [191/249], loss=82.1433
	step [192/249], loss=71.7334
	step [193/249], loss=78.8861
	step [194/249], loss=71.9941
	step [195/249], loss=72.2286
	step [196/249], loss=77.3329
	step [197/249], loss=70.8896
	step [198/249], loss=66.0879
	step [199/249], loss=69.3320
	step [200/249], loss=75.7551
	step [201/249], loss=72.7256
	step [202/249], loss=70.0459
	step [203/249], loss=73.8130
	step [204/249], loss=72.0819
	step [205/249], loss=54.9898
	step [206/249], loss=58.3337
	step [207/249], loss=72.5989
	step [208/249], loss=71.2766
	step [209/249], loss=67.6434
	step [210/249], loss=75.8981
	step [211/249], loss=78.0859
	step [212/249], loss=68.7998
	step [213/249], loss=85.0822
	step [214/249], loss=79.4183
	step [215/249], loss=86.5908
	step [216/249], loss=57.8947
	step [217/249], loss=66.5301
	step [218/249], loss=64.2575
	step [219/249], loss=82.4735
	step [220/249], loss=68.2362
	step [221/249], loss=80.0988
	step [222/249], loss=78.9787
	step [223/249], loss=78.2985
	step [224/249], loss=80.5483
	step [225/249], loss=72.2521
	step [226/249], loss=72.2452
	step [227/249], loss=72.9486
	step [228/249], loss=62.4352
	step [229/249], loss=67.9563
	step [230/249], loss=66.7183
	step [231/249], loss=84.3480
	step [232/249], loss=77.1615
	step [233/249], loss=62.6785
	step [234/249], loss=80.4390
	step [235/249], loss=70.2120
	step [236/249], loss=78.7026
	step [237/249], loss=59.3392
	step [238/249], loss=68.9017
	step [239/249], loss=78.9041
	step [240/249], loss=74.6656
	step [241/249], loss=68.7189
	step [242/249], loss=63.8101
	step [243/249], loss=59.6021
	step [244/249], loss=70.4493
	step [245/249], loss=65.2430
	step [246/249], loss=71.2635
	step [247/249], loss=82.5676
	step [248/249], loss=73.9983
	step [249/249], loss=47.3842
	Evaluating
	loss=0.0060, precision=0.3692, recall=0.8501, f1=0.5148
Training epoch 102
	step [1/249], loss=71.6603
	step [2/249], loss=73.0228
	step [3/249], loss=60.1450
	step [4/249], loss=68.4645
	step [5/249], loss=61.0769
	step [6/249], loss=63.8666
	step [7/249], loss=78.5689
	step [8/249], loss=72.8755
	step [9/249], loss=64.6858
	step [10/249], loss=78.5723
	step [11/249], loss=63.6612
	step [12/249], loss=86.3980
	step [13/249], loss=69.2539
	step [14/249], loss=91.3525
	step [15/249], loss=68.4287
	step [16/249], loss=64.1153
	step [17/249], loss=78.0585
	step [18/249], loss=66.2386
	step [19/249], loss=84.2724
	step [20/249], loss=70.1740
	step [21/249], loss=60.9474
	step [22/249], loss=67.4469
	step [23/249], loss=61.5294
	step [24/249], loss=68.1345
	step [25/249], loss=77.5088
	step [26/249], loss=66.5258
	step [27/249], loss=65.1671
	step [28/249], loss=85.6139
	step [29/249], loss=65.5910
	step [30/249], loss=77.1150
	step [31/249], loss=77.0677
	step [32/249], loss=67.6906
	step [33/249], loss=75.0904
	step [34/249], loss=67.5445
	step [35/249], loss=56.4233
	step [36/249], loss=68.6157
	step [37/249], loss=67.9143
	step [38/249], loss=69.3990
	step [39/249], loss=75.6981
	step [40/249], loss=65.9308
	step [41/249], loss=62.8502
	step [42/249], loss=63.2231
	step [43/249], loss=88.3444
	step [44/249], loss=73.6806
	step [45/249], loss=61.9253
	step [46/249], loss=85.2308
	step [47/249], loss=55.8074
	step [48/249], loss=81.4568
	step [49/249], loss=71.6671
	step [50/249], loss=71.4672
	step [51/249], loss=73.7638
	step [52/249], loss=61.0410
	step [53/249], loss=61.7286
	step [54/249], loss=74.7189
	step [55/249], loss=87.2265
	step [56/249], loss=73.5809
	step [57/249], loss=76.9729
	step [58/249], loss=76.3658
	step [59/249], loss=64.6884
	step [60/249], loss=69.6884
	step [61/249], loss=74.1956
	step [62/249], loss=61.8047
	step [63/249], loss=86.7392
	step [64/249], loss=77.4294
	step [65/249], loss=74.2742
	step [66/249], loss=76.2263
	step [67/249], loss=64.4731
	step [68/249], loss=78.9382
	step [69/249], loss=72.5206
	step [70/249], loss=67.4835
	step [71/249], loss=84.0617
	step [72/249], loss=65.2336
	step [73/249], loss=72.8072
	step [74/249], loss=72.1727
	step [75/249], loss=73.4309
	step [76/249], loss=66.9587
	step [77/249], loss=74.9712
	step [78/249], loss=71.1894
	step [79/249], loss=83.5073
	step [80/249], loss=82.0390
	step [81/249], loss=86.3898
	step [82/249], loss=63.8033
	step [83/249], loss=75.2697
	step [84/249], loss=84.0008
	step [85/249], loss=84.4843
	step [86/249], loss=69.9002
	step [87/249], loss=73.3441
	step [88/249], loss=66.6984
	step [89/249], loss=72.2578
	step [90/249], loss=70.8293
	step [91/249], loss=61.4442
	step [92/249], loss=69.6075
	step [93/249], loss=84.2366
	step [94/249], loss=89.3191
	step [95/249], loss=64.7799
	step [96/249], loss=60.3601
	step [97/249], loss=89.6293
	step [98/249], loss=64.6981
	step [99/249], loss=67.1641
	step [100/249], loss=75.6741
	step [101/249], loss=75.7882
	step [102/249], loss=85.3927
	step [103/249], loss=64.9273
	step [104/249], loss=71.3100
	step [105/249], loss=70.7582
	step [106/249], loss=68.7769
	step [107/249], loss=72.9582
	step [108/249], loss=92.5940
	step [109/249], loss=68.2410
	step [110/249], loss=75.2704
	step [111/249], loss=80.4452
	step [112/249], loss=61.3322
	step [113/249], loss=64.2106
	step [114/249], loss=69.6989
	step [115/249], loss=73.8186
	step [116/249], loss=76.1494
	step [117/249], loss=77.4213
	step [118/249], loss=72.0528
	step [119/249], loss=79.0864
	step [120/249], loss=67.2063
	step [121/249], loss=74.0107
	step [122/249], loss=58.2408
	step [123/249], loss=70.9335
	step [124/249], loss=95.3425
	step [125/249], loss=77.6071
	step [126/249], loss=57.2967
	step [127/249], loss=70.6460
	step [128/249], loss=63.7492
	step [129/249], loss=57.3937
	step [130/249], loss=91.3397
	step [131/249], loss=71.5380
	step [132/249], loss=69.8237
	step [133/249], loss=65.2676
	step [134/249], loss=60.1545
	step [135/249], loss=72.4094
	step [136/249], loss=61.6759
	step [137/249], loss=62.2474
	step [138/249], loss=78.4082
	step [139/249], loss=88.9309
	step [140/249], loss=64.9051
	step [141/249], loss=76.0429
	step [142/249], loss=64.1861
	step [143/249], loss=77.0950
	step [144/249], loss=65.1719
	step [145/249], loss=59.8306
	step [146/249], loss=60.3466
	step [147/249], loss=62.7736
	step [148/249], loss=65.7028
	step [149/249], loss=64.9011
	step [150/249], loss=78.2622
	step [151/249], loss=65.5832
	step [152/249], loss=75.3954
	step [153/249], loss=87.4410
	step [154/249], loss=68.8102
	step [155/249], loss=71.0300
	step [156/249], loss=88.7354
	step [157/249], loss=66.1520
	step [158/249], loss=67.6345
	step [159/249], loss=82.6404
	step [160/249], loss=69.0584
	step [161/249], loss=73.6423
	step [162/249], loss=59.8404
	step [163/249], loss=59.5383
	step [164/249], loss=62.4252
	step [165/249], loss=87.2627
	step [166/249], loss=55.0792
	step [167/249], loss=57.1584
	step [168/249], loss=69.5441
	step [169/249], loss=70.8447
	step [170/249], loss=71.6527
	step [171/249], loss=67.6123
	step [172/249], loss=70.9466
	step [173/249], loss=65.4346
	step [174/249], loss=62.0714
	step [175/249], loss=58.3473
	step [176/249], loss=68.3176
	step [177/249], loss=62.2107
	step [178/249], loss=87.6921
	step [179/249], loss=63.4597
	step [180/249], loss=57.5876
	step [181/249], loss=74.5381
	step [182/249], loss=83.5234
	step [183/249], loss=72.2862
	step [184/249], loss=74.7250
	step [185/249], loss=85.1507
	step [186/249], loss=77.6742
	step [187/249], loss=75.6955
	step [188/249], loss=58.1321
	step [189/249], loss=69.6257
	step [190/249], loss=77.4820
	step [191/249], loss=85.5968
	step [192/249], loss=69.1018
	step [193/249], loss=68.7173
	step [194/249], loss=78.4449
	step [195/249], loss=66.5950
	step [196/249], loss=62.6988
	step [197/249], loss=75.0765
	step [198/249], loss=58.3044
	step [199/249], loss=76.9507
	step [200/249], loss=66.5710
	step [201/249], loss=72.2088
	step [202/249], loss=56.0750
	step [203/249], loss=69.6576
	step [204/249], loss=76.2840
	step [205/249], loss=70.9944
	step [206/249], loss=85.5820
	step [207/249], loss=70.0721
	step [208/249], loss=80.3848
	step [209/249], loss=80.3808
	step [210/249], loss=81.1750
	step [211/249], loss=66.2536
	step [212/249], loss=72.3293
	step [213/249], loss=88.6825
	step [214/249], loss=62.5524
	step [215/249], loss=86.0144
	step [216/249], loss=76.3609
	step [217/249], loss=81.4577
	step [218/249], loss=66.4422
	step [219/249], loss=76.9114
	step [220/249], loss=70.0315
	step [221/249], loss=62.0808
	step [222/249], loss=79.3754
	step [223/249], loss=70.8663
	step [224/249], loss=64.9812
	step [225/249], loss=77.1192
	step [226/249], loss=53.1477
	step [227/249], loss=82.0528
	step [228/249], loss=69.6615
	step [229/249], loss=52.1485
	step [230/249], loss=69.2856
	step [231/249], loss=54.7471
	step [232/249], loss=63.8547
	step [233/249], loss=64.2126
	step [234/249], loss=66.0175
	step [235/249], loss=77.0040
	step [236/249], loss=66.2195
	step [237/249], loss=63.6250
	step [238/249], loss=84.2139
	step [239/249], loss=66.6585
	step [240/249], loss=86.5929
	step [241/249], loss=62.7156
	step [242/249], loss=76.7654
	step [243/249], loss=56.0069
	step [244/249], loss=68.4924
	step [245/249], loss=70.7769
	step [246/249], loss=57.2416
	step [247/249], loss=107.6596
	step [248/249], loss=68.0145
	step [249/249], loss=54.3253
	Evaluating
	loss=0.0054, precision=0.4045, recall=0.8517, f1=0.5485
Training epoch 103
	step [1/249], loss=61.6193
	step [2/249], loss=89.1784
	step [3/249], loss=67.8800
	step [4/249], loss=70.0455
	step [5/249], loss=81.4955
	step [6/249], loss=75.2693
	step [7/249], loss=58.9861
	step [8/249], loss=80.9795
	step [9/249], loss=57.8749
	step [10/249], loss=72.6980
	step [11/249], loss=61.3550
	step [12/249], loss=75.3089
	step [13/249], loss=76.0325
	step [14/249], loss=78.5497
	step [15/249], loss=53.7989
	step [16/249], loss=60.2760
	step [17/249], loss=71.8372
	step [18/249], loss=55.5543
	step [19/249], loss=68.2347
	step [20/249], loss=68.0311
	step [21/249], loss=60.9987
	step [22/249], loss=69.1276
	step [23/249], loss=73.2271
	step [24/249], loss=84.3341
	step [25/249], loss=84.4900
	step [26/249], loss=69.2753
	step [27/249], loss=85.0497
	step [28/249], loss=58.1071
	step [29/249], loss=60.6337
	step [30/249], loss=77.5650
	step [31/249], loss=76.9976
	step [32/249], loss=66.0253
	step [33/249], loss=71.2445
	step [34/249], loss=70.5648
	step [35/249], loss=64.1724
	step [36/249], loss=75.2428
	step [37/249], loss=73.3090
	step [38/249], loss=91.3605
	step [39/249], loss=64.2795
	step [40/249], loss=52.8651
	step [41/249], loss=80.4709
	step [42/249], loss=83.7256
	step [43/249], loss=76.2741
	step [44/249], loss=88.9872
	step [45/249], loss=71.1985
	step [46/249], loss=58.3723
	step [47/249], loss=76.3030
	step [48/249], loss=61.6060
	step [49/249], loss=74.9938
	step [50/249], loss=85.2107
	step [51/249], loss=61.4262
	step [52/249], loss=82.2490
	step [53/249], loss=73.3275
	step [54/249], loss=71.3571
	step [55/249], loss=74.1354
	step [56/249], loss=79.5547
	step [57/249], loss=66.7596
	step [58/249], loss=66.6442
	step [59/249], loss=66.7497
	step [60/249], loss=68.3225
	step [61/249], loss=76.2168
	step [62/249], loss=74.6939
	step [63/249], loss=75.7955
	step [64/249], loss=77.6524
	step [65/249], loss=64.2486
	step [66/249], loss=60.7355
	step [67/249], loss=68.3987
	step [68/249], loss=75.9631
	step [69/249], loss=87.8921
	step [70/249], loss=58.4768
	step [71/249], loss=73.9621
	step [72/249], loss=65.1426
	step [73/249], loss=67.7470
	step [74/249], loss=67.9563
	step [75/249], loss=75.0367
	step [76/249], loss=61.4938
	step [77/249], loss=85.5967
	step [78/249], loss=73.2416
	step [79/249], loss=79.0706
	step [80/249], loss=72.2406
	step [81/249], loss=81.0306
	step [82/249], loss=61.0900
	step [83/249], loss=76.3984
	step [84/249], loss=74.5411
	step [85/249], loss=73.2879
	step [86/249], loss=82.2440
	step [87/249], loss=58.3640
	step [88/249], loss=82.8820
	step [89/249], loss=58.5067
	step [90/249], loss=65.8791
	step [91/249], loss=59.6737
	step [92/249], loss=71.9103
	step [93/249], loss=68.3711
	step [94/249], loss=71.0664
	step [95/249], loss=75.5857
	step [96/249], loss=64.3772
	step [97/249], loss=64.6427
	step [98/249], loss=72.8087
	step [99/249], loss=75.8050
	step [100/249], loss=67.8795
	step [101/249], loss=75.0995
	step [102/249], loss=59.0945
	step [103/249], loss=78.0771
	step [104/249], loss=72.7445
	step [105/249], loss=77.9479
	step [106/249], loss=64.8783
	step [107/249], loss=66.3919
	step [108/249], loss=80.8680
	step [109/249], loss=72.7567
	step [110/249], loss=67.1769
	step [111/249], loss=73.2883
	step [112/249], loss=58.6769
	step [113/249], loss=93.7257
	step [114/249], loss=74.9128
	step [115/249], loss=73.8648
	step [116/249], loss=63.0678
	step [117/249], loss=65.0271
	step [118/249], loss=74.0175
	step [119/249], loss=75.0878
	step [120/249], loss=71.2250
	step [121/249], loss=78.7954
	step [122/249], loss=72.0595
	step [123/249], loss=61.0040
	step [124/249], loss=60.8789
	step [125/249], loss=72.3216
	step [126/249], loss=73.1966
	step [127/249], loss=56.8967
	step [128/249], loss=78.4770
	step [129/249], loss=62.6335
	step [130/249], loss=66.8294
	step [131/249], loss=77.3158
	step [132/249], loss=69.2811
	step [133/249], loss=80.2221
	step [134/249], loss=61.5891
	step [135/249], loss=70.9321
	step [136/249], loss=84.2319
	step [137/249], loss=66.6195
	step [138/249], loss=76.5177
	step [139/249], loss=66.4220
	step [140/249], loss=64.1054
	step [141/249], loss=52.8461
	step [142/249], loss=74.3704
	step [143/249], loss=65.8867
	step [144/249], loss=70.5967
	step [145/249], loss=73.8330
	step [146/249], loss=78.1780
	step [147/249], loss=68.2150
	step [148/249], loss=74.8274
	step [149/249], loss=82.9654
	step [150/249], loss=82.2101
	step [151/249], loss=53.6808
	step [152/249], loss=67.3933
	step [153/249], loss=74.9610
	step [154/249], loss=61.0218
	step [155/249], loss=71.2471
	step [156/249], loss=68.1339
	step [157/249], loss=62.6948
	step [158/249], loss=73.6777
	step [159/249], loss=63.6755
	step [160/249], loss=61.1387
	step [161/249], loss=68.0085
	step [162/249], loss=84.1883
	step [163/249], loss=78.1726
	step [164/249], loss=54.8210
	step [165/249], loss=77.3578
	step [166/249], loss=77.9261
	step [167/249], loss=51.7964
	step [168/249], loss=88.6909
	step [169/249], loss=65.7260
	step [170/249], loss=68.1275
	step [171/249], loss=66.2302
	step [172/249], loss=86.7412
	step [173/249], loss=75.3597
	step [174/249], loss=66.0497
	step [175/249], loss=61.7183
	step [176/249], loss=63.3018
	step [177/249], loss=75.6709
	step [178/249], loss=85.9379
	step [179/249], loss=79.5483
	step [180/249], loss=71.7895
	step [181/249], loss=63.7253
	step [182/249], loss=61.9433
	step [183/249], loss=86.6330
	step [184/249], loss=76.3674
	step [185/249], loss=76.0108
	step [186/249], loss=65.6822
	step [187/249], loss=70.6789
	step [188/249], loss=69.6682
	step [189/249], loss=73.4457
	step [190/249], loss=75.4391
	step [191/249], loss=59.3867
	step [192/249], loss=69.6956
	step [193/249], loss=81.0104
	step [194/249], loss=69.1307
	step [195/249], loss=84.9817
	step [196/249], loss=63.5342
	step [197/249], loss=77.5758
	step [198/249], loss=69.3657
	step [199/249], loss=90.9739
	step [200/249], loss=62.3575
	step [201/249], loss=67.5556
	step [202/249], loss=73.1486
	step [203/249], loss=69.9882
	step [204/249], loss=73.1169
	step [205/249], loss=62.2666
	step [206/249], loss=77.2043
	step [207/249], loss=77.6077
	step [208/249], loss=72.1393
	step [209/249], loss=64.4780
	step [210/249], loss=67.8502
	step [211/249], loss=79.7164
	step [212/249], loss=75.8529
	step [213/249], loss=72.9676
	step [214/249], loss=59.6699
	step [215/249], loss=71.1348
	step [216/249], loss=60.9065
	step [217/249], loss=70.2821
	step [218/249], loss=70.7347
	step [219/249], loss=85.9855
	step [220/249], loss=68.2457
	step [221/249], loss=65.3349
	step [222/249], loss=64.6162
	step [223/249], loss=72.5141
	step [224/249], loss=81.3145
	step [225/249], loss=83.9435
	step [226/249], loss=71.0582
	step [227/249], loss=86.3501
	step [228/249], loss=65.9458
	step [229/249], loss=57.3669
	step [230/249], loss=66.1662
	step [231/249], loss=66.5864
	step [232/249], loss=76.1008
	step [233/249], loss=78.8071
	step [234/249], loss=65.1455
	step [235/249], loss=73.9736
	step [236/249], loss=61.3984
	step [237/249], loss=70.1280
	step [238/249], loss=68.7157
	step [239/249], loss=76.6904
	step [240/249], loss=66.1486
	step [241/249], loss=69.4631
	step [242/249], loss=99.6814
	step [243/249], loss=76.2186
	step [244/249], loss=79.8370
	step [245/249], loss=62.8816
	step [246/249], loss=80.0402
	step [247/249], loss=69.5812
	step [248/249], loss=64.0732
	step [249/249], loss=47.0789
	Evaluating
	loss=0.0053, precision=0.4054, recall=0.8357, f1=0.5460
Training epoch 104
	step [1/249], loss=66.7984
	step [2/249], loss=84.2573
	step [3/249], loss=60.5717
	step [4/249], loss=64.6609
	step [5/249], loss=66.6722
	step [6/249], loss=68.8242
	step [7/249], loss=61.4460
	step [8/249], loss=78.0746
	step [9/249], loss=96.8436
	step [10/249], loss=72.7951
	step [11/249], loss=68.7808
	step [12/249], loss=87.3283
	step [13/249], loss=67.8979
	step [14/249], loss=66.6167
	step [15/249], loss=71.4318
	step [16/249], loss=49.8487
	step [17/249], loss=77.7162
	step [18/249], loss=73.8070
	step [19/249], loss=69.1700
	step [20/249], loss=67.6310
	step [21/249], loss=67.4938
	step [22/249], loss=82.8602
	step [23/249], loss=67.4340
	step [24/249], loss=76.2406
	step [25/249], loss=73.1798
	step [26/249], loss=78.1600
	step [27/249], loss=77.1246
	step [28/249], loss=80.3252
	step [29/249], loss=84.2942
	step [30/249], loss=70.1408
	step [31/249], loss=80.5799
	step [32/249], loss=68.5239
	step [33/249], loss=64.0523
	step [34/249], loss=69.0942
	step [35/249], loss=74.7672
	step [36/249], loss=79.3897
	step [37/249], loss=85.0873
	step [38/249], loss=81.5544
	step [39/249], loss=66.8498
	step [40/249], loss=60.4726
	step [41/249], loss=73.4801
	step [42/249], loss=84.0370
	step [43/249], loss=70.1822
	step [44/249], loss=71.4022
	step [45/249], loss=98.6305
	step [46/249], loss=74.6062
	step [47/249], loss=74.2202
	step [48/249], loss=80.5247
	step [49/249], loss=83.6638
	step [50/249], loss=103.4421
	step [51/249], loss=69.7275
	step [52/249], loss=67.0410
	step [53/249], loss=77.5521
	step [54/249], loss=79.0515
	step [55/249], loss=82.4635
	step [56/249], loss=63.8314
	step [57/249], loss=66.4270
	step [58/249], loss=71.8796
	step [59/249], loss=63.7231
	step [60/249], loss=77.3737
	step [61/249], loss=63.1076
	step [62/249], loss=78.8386
	step [63/249], loss=71.0526
	step [64/249], loss=70.8158
	step [65/249], loss=71.1068
	step [66/249], loss=65.0523
	step [67/249], loss=75.8492
	step [68/249], loss=75.3249
	step [69/249], loss=80.4360
	step [70/249], loss=74.7600
	step [71/249], loss=69.7281
	step [72/249], loss=82.1104
	step [73/249], loss=77.6204
	step [74/249], loss=63.3412
	step [75/249], loss=70.5363
	step [76/249], loss=68.6724
	step [77/249], loss=82.9828
	step [78/249], loss=67.5842
	step [79/249], loss=70.3323
	step [80/249], loss=71.8831
	step [81/249], loss=59.6716
	step [82/249], loss=64.7972
	step [83/249], loss=76.8778
	step [84/249], loss=76.7781
	step [85/249], loss=80.5306
	step [86/249], loss=66.3038
	step [87/249], loss=62.6782
	step [88/249], loss=73.4273
	step [89/249], loss=79.2541
	step [90/249], loss=74.1146
	step [91/249], loss=66.0645
	step [92/249], loss=65.3569
	step [93/249], loss=82.4246
	step [94/249], loss=64.9931
	step [95/249], loss=78.1415
	step [96/249], loss=63.1142
	step [97/249], loss=65.0743
	step [98/249], loss=89.8389
	step [99/249], loss=69.2671
	step [100/249], loss=78.3869
	step [101/249], loss=70.4951
	step [102/249], loss=81.6635
	step [103/249], loss=69.2612
	step [104/249], loss=70.2642
	step [105/249], loss=68.5063
	step [106/249], loss=74.1529
	step [107/249], loss=72.4988
	step [108/249], loss=62.3621
	step [109/249], loss=61.8935
	step [110/249], loss=59.6833
	step [111/249], loss=65.1768
	step [112/249], loss=72.9342
	step [113/249], loss=58.8240
	step [114/249], loss=72.5316
	step [115/249], loss=61.0246
	step [116/249], loss=63.8015
	step [117/249], loss=83.1594
	step [118/249], loss=60.7820
	step [119/249], loss=75.5296
	step [120/249], loss=67.1626
	step [121/249], loss=73.2311
	step [122/249], loss=81.7999
	step [123/249], loss=65.8340
	step [124/249], loss=76.6154
	step [125/249], loss=81.9045
	step [126/249], loss=74.7789
	step [127/249], loss=73.1556
	step [128/249], loss=71.5791
	step [129/249], loss=83.0170
	step [130/249], loss=63.1567
	step [131/249], loss=57.9572
	step [132/249], loss=62.8478
	step [133/249], loss=62.6045
	step [134/249], loss=78.5156
	step [135/249], loss=75.8548
	step [136/249], loss=69.8333
	step [137/249], loss=64.7665
	step [138/249], loss=63.2084
	step [139/249], loss=74.0103
	step [140/249], loss=71.2665
	step [141/249], loss=53.0977
	step [142/249], loss=73.3326
	step [143/249], loss=57.7347
	step [144/249], loss=78.4793
	step [145/249], loss=58.9580
	step [146/249], loss=86.2564
	step [147/249], loss=83.0545
	step [148/249], loss=73.4969
	step [149/249], loss=62.0514
	step [150/249], loss=56.5935
	step [151/249], loss=71.6323
	step [152/249], loss=74.4713
	step [153/249], loss=66.0357
	step [154/249], loss=71.7436
	step [155/249], loss=66.3429
	step [156/249], loss=68.4682
	step [157/249], loss=67.0909
	step [158/249], loss=78.8732
	step [159/249], loss=56.2342
	step [160/249], loss=72.0949
	step [161/249], loss=60.8520
	step [162/249], loss=70.3327
	step [163/249], loss=70.3179
	step [164/249], loss=65.1223
	step [165/249], loss=69.4185
	step [166/249], loss=65.3934
	step [167/249], loss=75.3432
	step [168/249], loss=76.8555
	step [169/249], loss=63.8820
	step [170/249], loss=76.6943
	step [171/249], loss=60.4354
	step [172/249], loss=74.2022
	step [173/249], loss=53.6218
	step [174/249], loss=77.4432
	step [175/249], loss=67.1483
	step [176/249], loss=69.5918
	step [177/249], loss=64.3824
	step [178/249], loss=69.5760
	step [179/249], loss=73.0633
	step [180/249], loss=64.9187
	step [181/249], loss=70.2016
	step [182/249], loss=64.2908
	step [183/249], loss=69.2724
	step [184/249], loss=81.5470
	step [185/249], loss=63.1660
	step [186/249], loss=77.3523
	step [187/249], loss=62.9794
	step [188/249], loss=73.2779
	step [189/249], loss=72.1994
	step [190/249], loss=76.9221
	step [191/249], loss=68.3368
	step [192/249], loss=66.7562
	step [193/249], loss=62.4739
	step [194/249], loss=59.0962
	step [195/249], loss=72.5059
	step [196/249], loss=66.1430
	step [197/249], loss=72.1798
	step [198/249], loss=59.0743
	step [199/249], loss=68.6199
	step [200/249], loss=70.6970
	step [201/249], loss=76.6574
	step [202/249], loss=73.7524
	step [203/249], loss=68.9169
	step [204/249], loss=77.2822
	step [205/249], loss=66.8816
	step [206/249], loss=61.5639
	step [207/249], loss=73.4087
	step [208/249], loss=74.1469
	step [209/249], loss=68.8458
	step [210/249], loss=49.6015
	step [211/249], loss=72.6553
	step [212/249], loss=67.6263
	step [213/249], loss=87.8855
	step [214/249], loss=79.7428
	step [215/249], loss=56.6313
	step [216/249], loss=60.3508
	step [217/249], loss=73.7574
	step [218/249], loss=55.8987
	step [219/249], loss=77.1676
	step [220/249], loss=67.4234
	step [221/249], loss=69.8617
	step [222/249], loss=66.0449
	step [223/249], loss=73.1543
	step [224/249], loss=67.7711
	step [225/249], loss=63.1339
	step [226/249], loss=62.0094
	step [227/249], loss=76.3753
	step [228/249], loss=64.3456
	step [229/249], loss=74.7008
	step [230/249], loss=60.2692
	step [231/249], loss=52.4546
	step [232/249], loss=83.0203
	step [233/249], loss=82.2156
	step [234/249], loss=78.0465
	step [235/249], loss=69.4253
	step [236/249], loss=92.8919
	step [237/249], loss=62.6318
	step [238/249], loss=66.6339
	step [239/249], loss=76.3735
	step [240/249], loss=64.8561
	step [241/249], loss=82.9206
	step [242/249], loss=56.5973
	step [243/249], loss=62.8850
	step [244/249], loss=89.4755
	step [245/249], loss=74.3421
	step [246/249], loss=65.9773
	step [247/249], loss=52.1611
	step [248/249], loss=81.0328
	step [249/249], loss=53.3277
	Evaluating
	loss=0.0053, precision=0.4061, recall=0.8290, f1=0.5451
Training epoch 105
	step [1/249], loss=57.8241
	step [2/249], loss=53.2711
	step [3/249], loss=72.7720
	step [4/249], loss=72.1384
	step [5/249], loss=74.3210
	step [6/249], loss=60.7769
	step [7/249], loss=77.0468
	step [8/249], loss=71.5200
	step [9/249], loss=67.9522
	step [10/249], loss=84.2881
	step [11/249], loss=77.3272
	step [12/249], loss=72.4351
	step [13/249], loss=65.4733
	step [14/249], loss=62.9988
	step [15/249], loss=70.3640
	step [16/249], loss=72.4438
	step [17/249], loss=76.4401
	step [18/249], loss=86.0452
	step [19/249], loss=52.6801
	step [20/249], loss=75.7849
	step [21/249], loss=71.1035
	step [22/249], loss=73.4259
	step [23/249], loss=69.4555
	step [24/249], loss=81.5214
	step [25/249], loss=92.2360
	step [26/249], loss=63.8357
	step [27/249], loss=76.3317
	step [28/249], loss=61.9525
	step [29/249], loss=77.2115
	step [30/249], loss=56.0953
	step [31/249], loss=66.4192
	step [32/249], loss=59.4008
	step [33/249], loss=69.1534
	step [34/249], loss=77.8639
	step [35/249], loss=65.6362
	step [36/249], loss=68.3388
	step [37/249], loss=69.3330
	step [38/249], loss=65.7783
	step [39/249], loss=71.4861
	step [40/249], loss=71.0997
	step [41/249], loss=73.6338
	step [42/249], loss=59.3579
	step [43/249], loss=67.0918
	step [44/249], loss=78.5797
	step [45/249], loss=70.2248
	step [46/249], loss=69.5107
	step [47/249], loss=65.2673
	step [48/249], loss=69.7355
	step [49/249], loss=71.0440
	step [50/249], loss=82.6257
	step [51/249], loss=70.1486
	step [52/249], loss=70.8491
	step [53/249], loss=77.7783
	step [54/249], loss=62.6256
	step [55/249], loss=73.4690
	step [56/249], loss=70.8065
	step [57/249], loss=68.5555
	step [58/249], loss=69.8866
	step [59/249], loss=69.5461
	step [60/249], loss=73.9747
	step [61/249], loss=84.0938
	step [62/249], loss=81.0187
	step [63/249], loss=71.5387
	step [64/249], loss=73.4558
	step [65/249], loss=71.4047
	step [66/249], loss=69.1989
	step [67/249], loss=75.8796
	step [68/249], loss=69.4855
	step [69/249], loss=76.8377
	step [70/249], loss=65.4384
	step [71/249], loss=70.1060
	step [72/249], loss=73.6487
	step [73/249], loss=72.5922
	step [74/249], loss=77.5077
	step [75/249], loss=68.4893
	step [76/249], loss=67.3108
	step [77/249], loss=74.0907
	step [78/249], loss=70.0234
	step [79/249], loss=69.9542
	step [80/249], loss=66.2775
	step [81/249], loss=68.8634
	step [82/249], loss=66.5908
	step [83/249], loss=79.6354
	step [84/249], loss=60.7300
	step [85/249], loss=73.8330
	step [86/249], loss=77.7479
	step [87/249], loss=69.6355
	step [88/249], loss=66.6112
	step [89/249], loss=60.4493
	step [90/249], loss=72.3177
	step [91/249], loss=83.7464
	step [92/249], loss=80.9938
	step [93/249], loss=66.1095
	step [94/249], loss=59.7502
	step [95/249], loss=80.0676
	step [96/249], loss=83.6959
	step [97/249], loss=71.9022
	step [98/249], loss=65.7092
	step [99/249], loss=70.7395
	step [100/249], loss=81.2188
	step [101/249], loss=76.7325
	step [102/249], loss=66.4095
	step [103/249], loss=71.6037
	step [104/249], loss=68.0146
	step [105/249], loss=74.0941
	step [106/249], loss=67.2245
	step [107/249], loss=86.9157
	step [108/249], loss=70.4976
	step [109/249], loss=72.9510
	step [110/249], loss=63.8080
	step [111/249], loss=67.6673
	step [112/249], loss=76.8593
	step [113/249], loss=65.2819
	step [114/249], loss=70.9055
	step [115/249], loss=76.1568
	step [116/249], loss=61.6081
	step [117/249], loss=71.1222
	step [118/249], loss=59.2068
	step [119/249], loss=73.5713
	step [120/249], loss=65.7611
	step [121/249], loss=71.1869
	step [122/249], loss=67.6016
	step [123/249], loss=71.9068
	step [124/249], loss=84.1075
	step [125/249], loss=68.2124
	step [126/249], loss=72.7550
	step [127/249], loss=79.8867
	step [128/249], loss=82.3208
	step [129/249], loss=75.7941
	step [130/249], loss=72.0105
	step [131/249], loss=63.7788
	step [132/249], loss=78.4087
	step [133/249], loss=65.0638
	step [134/249], loss=65.8031
	step [135/249], loss=70.0307
	step [136/249], loss=75.3860
	step [137/249], loss=76.7453
	step [138/249], loss=73.4685
	step [139/249], loss=59.2812
	step [140/249], loss=64.9429
	step [141/249], loss=66.6811
	step [142/249], loss=64.3171
	step [143/249], loss=74.5990
	step [144/249], loss=55.9232
	step [145/249], loss=57.0410
	step [146/249], loss=70.6249
	step [147/249], loss=64.1348
	step [148/249], loss=77.8817
	step [149/249], loss=78.5364
	step [150/249], loss=66.2688
	step [151/249], loss=58.4359
	step [152/249], loss=81.7604
	step [153/249], loss=72.2636
	step [154/249], loss=75.9563
	step [155/249], loss=65.5901
	step [156/249], loss=71.3501
	step [157/249], loss=68.8921
	step [158/249], loss=78.9207
	step [159/249], loss=59.1437
	step [160/249], loss=73.1936
	step [161/249], loss=69.0312
	step [162/249], loss=58.4534
	step [163/249], loss=82.8104
	step [164/249], loss=66.0907
	step [165/249], loss=84.1246
	step [166/249], loss=73.2094
	step [167/249], loss=63.8845
	step [168/249], loss=77.1343
	step [169/249], loss=53.7745
	step [170/249], loss=63.8237
	step [171/249], loss=50.5367
	step [172/249], loss=91.6466
	step [173/249], loss=61.3545
	step [174/249], loss=69.5378
	step [175/249], loss=73.1894
	step [176/249], loss=74.2901
	step [177/249], loss=74.9198
	step [178/249], loss=76.4736
	step [179/249], loss=78.7268
	step [180/249], loss=64.9237
	step [181/249], loss=81.7275
	step [182/249], loss=79.3752
	step [183/249], loss=83.0123
	step [184/249], loss=81.4359
	step [185/249], loss=65.3928
	step [186/249], loss=70.4033
	step [187/249], loss=68.0634
	step [188/249], loss=71.8920
	step [189/249], loss=66.0666
	step [190/249], loss=89.2413
	step [191/249], loss=69.8405
	step [192/249], loss=67.3372
	step [193/249], loss=71.4346
	step [194/249], loss=73.9177
	step [195/249], loss=60.4303
	step [196/249], loss=53.9388
	step [197/249], loss=76.4756
	step [198/249], loss=66.6460
	step [199/249], loss=71.0620
	step [200/249], loss=62.7947
	step [201/249], loss=75.2394
	step [202/249], loss=75.2056
	step [203/249], loss=67.6908
	step [204/249], loss=65.0465
	step [205/249], loss=63.2498
	step [206/249], loss=75.8797
	step [207/249], loss=70.5748
	step [208/249], loss=73.1926
	step [209/249], loss=67.9031
	step [210/249], loss=68.4541
	step [211/249], loss=66.7222
	step [212/249], loss=56.2025
	step [213/249], loss=63.1354
	step [214/249], loss=74.6528
	step [215/249], loss=70.0548
	step [216/249], loss=76.5033
	step [217/249], loss=57.5627
	step [218/249], loss=76.5733
	step [219/249], loss=76.0887
	step [220/249], loss=85.7559
	step [221/249], loss=81.7677
	step [222/249], loss=61.2631
	step [223/249], loss=79.4686
	step [224/249], loss=67.0589
	step [225/249], loss=86.7086
	step [226/249], loss=76.1657
	step [227/249], loss=74.9874
	step [228/249], loss=58.1185
	step [229/249], loss=69.8631
	step [230/249], loss=65.7533
	step [231/249], loss=62.8884
	step [232/249], loss=74.9724
	step [233/249], loss=74.6880
	step [234/249], loss=57.6648
	step [235/249], loss=56.2532
	step [236/249], loss=83.9509
	step [237/249], loss=91.4435
	step [238/249], loss=70.2339
	step [239/249], loss=61.2721
	step [240/249], loss=70.5853
	step [241/249], loss=81.9922
	step [242/249], loss=72.1528
	step [243/249], loss=58.6348
	step [244/249], loss=75.4661
	step [245/249], loss=66.8320
	step [246/249], loss=74.7001
	step [247/249], loss=59.9921
	step [248/249], loss=73.6084
	step [249/249], loss=49.4585
	Evaluating
	loss=0.0059, precision=0.3679, recall=0.8367, f1=0.5111
Training epoch 106
	step [1/249], loss=73.8970
	step [2/249], loss=78.6296
	step [3/249], loss=73.4772
	step [4/249], loss=78.9670
	step [5/249], loss=77.8432
	step [6/249], loss=62.8333
	step [7/249], loss=77.0655
	step [8/249], loss=66.3166
	step [9/249], loss=63.2385
	step [10/249], loss=74.1532
	step [11/249], loss=77.4467
	step [12/249], loss=61.5312
	step [13/249], loss=67.2676
	step [14/249], loss=84.5327
	step [15/249], loss=69.3084
	step [16/249], loss=77.3886
	step [17/249], loss=69.7859
	step [18/249], loss=69.2747
	step [19/249], loss=67.8777
	step [20/249], loss=64.0249
	step [21/249], loss=58.0497
	step [22/249], loss=77.2313
	step [23/249], loss=62.4094
	step [24/249], loss=64.8599
	step [25/249], loss=76.3959
	step [26/249], loss=69.7886
	step [27/249], loss=67.3261
	step [28/249], loss=69.8735
	step [29/249], loss=70.2010
	step [30/249], loss=72.9459
	step [31/249], loss=67.7285
	step [32/249], loss=73.8103
	step [33/249], loss=56.4632
	step [34/249], loss=72.5236
	step [35/249], loss=80.1730
	step [36/249], loss=70.6604
	step [37/249], loss=68.0283
	step [38/249], loss=79.7171
	step [39/249], loss=70.7184
	step [40/249], loss=81.0329
	step [41/249], loss=74.9076
	step [42/249], loss=78.2699
	step [43/249], loss=83.9309
	step [44/249], loss=75.7786
	step [45/249], loss=63.3596
	step [46/249], loss=97.6073
	step [47/249], loss=74.0507
	step [48/249], loss=65.2706
	step [49/249], loss=72.3899
	step [50/249], loss=84.1714
	step [51/249], loss=75.0814
	step [52/249], loss=59.2051
	step [53/249], loss=88.8963
	step [54/249], loss=64.9246
	step [55/249], loss=75.6087
	step [56/249], loss=65.5264
	step [57/249], loss=65.5292
	step [58/249], loss=66.0122
	step [59/249], loss=73.4188
	step [60/249], loss=69.8877
	step [61/249], loss=72.1974
	step [62/249], loss=74.1373
	step [63/249], loss=63.5399
	step [64/249], loss=68.1217
	step [65/249], loss=62.5094
	step [66/249], loss=61.7809
	step [67/249], loss=78.1698
	step [68/249], loss=72.9041
	step [69/249], loss=82.2825
	step [70/249], loss=72.0169
	step [71/249], loss=69.1878
	step [72/249], loss=61.1679
	step [73/249], loss=67.0481
	step [74/249], loss=75.7662
	step [75/249], loss=64.2511
	step [76/249], loss=82.2200
	step [77/249], loss=52.5265
	step [78/249], loss=65.9300
	step [79/249], loss=67.8314
	step [80/249], loss=70.8255
	step [81/249], loss=70.6924
	step [82/249], loss=69.1935
	step [83/249], loss=77.7583
	step [84/249], loss=80.0305
	step [85/249], loss=72.8101
	step [86/249], loss=64.9987
	step [87/249], loss=77.5866
	step [88/249], loss=74.0011
	step [89/249], loss=72.4954
	step [90/249], loss=61.6555
	step [91/249], loss=55.1558
	step [92/249], loss=62.1718
	step [93/249], loss=60.3000
	step [94/249], loss=67.2359
	step [95/249], loss=46.0059
	step [96/249], loss=71.6846
	step [97/249], loss=70.9296
	step [98/249], loss=89.3645
	step [99/249], loss=67.8833
	step [100/249], loss=70.0222
	step [101/249], loss=75.9498
	step [102/249], loss=87.1919
	step [103/249], loss=74.5132
	step [104/249], loss=66.6847
	step [105/249], loss=77.7238
	step [106/249], loss=58.6389
	step [107/249], loss=80.3103
	step [108/249], loss=77.3464
	step [109/249], loss=79.3161
	step [110/249], loss=69.4115
	step [111/249], loss=73.9458
	step [112/249], loss=53.4647
	step [113/249], loss=73.7370
	step [114/249], loss=69.1254
	step [115/249], loss=79.4195
	step [116/249], loss=68.9018
	step [117/249], loss=72.7485
	step [118/249], loss=66.2874
	step [119/249], loss=71.7462
	step [120/249], loss=73.0881
	step [121/249], loss=89.2478
	step [122/249], loss=68.6205
	step [123/249], loss=81.8253
	step [124/249], loss=75.0904
	step [125/249], loss=82.9270
	step [126/249], loss=71.4763
	step [127/249], loss=79.3375
	step [128/249], loss=67.5420
	step [129/249], loss=68.8481
	step [130/249], loss=74.1534
	step [131/249], loss=64.8407
	step [132/249], loss=97.8110
	step [133/249], loss=63.5646
	step [134/249], loss=69.7041
	step [135/249], loss=72.6538
	step [136/249], loss=77.0632
	step [137/249], loss=69.0542
	step [138/249], loss=76.0486
	step [139/249], loss=59.5529
	step [140/249], loss=83.3315
	step [141/249], loss=68.1521
	step [142/249], loss=65.6733
	step [143/249], loss=66.6059
	step [144/249], loss=74.7350
	step [145/249], loss=58.5221
	step [146/249], loss=75.7113
	step [147/249], loss=67.9989
	step [148/249], loss=63.9963
	step [149/249], loss=72.0694
	step [150/249], loss=66.5179
	step [151/249], loss=67.5609
	step [152/249], loss=73.4070
	step [153/249], loss=83.2791
	step [154/249], loss=78.9646
	step [155/249], loss=57.1107
	step [156/249], loss=68.5780
	step [157/249], loss=76.4836
	step [158/249], loss=66.0675
	step [159/249], loss=61.6074
	step [160/249], loss=66.9974
	step [161/249], loss=53.1451
	step [162/249], loss=69.1573
	step [163/249], loss=69.4874
	step [164/249], loss=80.2229
	step [165/249], loss=72.4432
	step [166/249], loss=84.0285
	step [167/249], loss=57.1407
	step [168/249], loss=69.5855
	step [169/249], loss=54.2078
	step [170/249], loss=64.9434
	step [171/249], loss=76.0821
	step [172/249], loss=56.7797
	step [173/249], loss=67.7656
	step [174/249], loss=68.8597
	step [175/249], loss=65.1071
	step [176/249], loss=71.2498
	step [177/249], loss=91.3453
	step [178/249], loss=54.8075
	step [179/249], loss=61.2796
	step [180/249], loss=57.1101
	step [181/249], loss=61.1416
	step [182/249], loss=73.7858
	step [183/249], loss=79.9220
	step [184/249], loss=72.8507
	step [185/249], loss=65.8480
	step [186/249], loss=77.4139
	step [187/249], loss=56.3564
	step [188/249], loss=79.3683
	step [189/249], loss=79.4582
	step [190/249], loss=89.1191
	step [191/249], loss=68.4538
	step [192/249], loss=68.7000
	step [193/249], loss=72.0250
	step [194/249], loss=67.6523
	step [195/249], loss=64.9308
	step [196/249], loss=71.3815
	step [197/249], loss=77.1960
	step [198/249], loss=74.8633
	step [199/249], loss=56.1724
	step [200/249], loss=67.0729
	step [201/249], loss=77.5984
	step [202/249], loss=75.8267
	step [203/249], loss=62.4817
	step [204/249], loss=68.3104
	step [205/249], loss=72.4448
	step [206/249], loss=77.5727
	step [207/249], loss=65.1976
	step [208/249], loss=52.4961
	step [209/249], loss=79.0819
	step [210/249], loss=70.6008
	step [211/249], loss=61.0490
	step [212/249], loss=74.1015
	step [213/249], loss=69.8282
	step [214/249], loss=68.5719
	step [215/249], loss=65.6723
	step [216/249], loss=55.2172
	step [217/249], loss=73.2934
	step [218/249], loss=62.7267
	step [219/249], loss=78.0191
	step [220/249], loss=67.2279
	step [221/249], loss=69.9328
	step [222/249], loss=62.4631
	step [223/249], loss=69.7087
	step [224/249], loss=72.8694
	step [225/249], loss=74.2527
	step [226/249], loss=69.6314
	step [227/249], loss=95.4601
	step [228/249], loss=78.4346
	step [229/249], loss=69.3424
	step [230/249], loss=75.4103
	step [231/249], loss=62.6713
	step [232/249], loss=66.6295
	step [233/249], loss=79.5787
	step [234/249], loss=67.3195
	step [235/249], loss=81.8542
	step [236/249], loss=59.2455
	step [237/249], loss=84.2708
	step [238/249], loss=77.0122
	step [239/249], loss=77.3110
	step [240/249], loss=69.3613
	step [241/249], loss=59.8231
	step [242/249], loss=65.0253
	step [243/249], loss=51.4417
	step [244/249], loss=63.1485
	step [245/249], loss=68.4685
	step [246/249], loss=79.5108
	step [247/249], loss=87.3360
	step [248/249], loss=62.6471
	step [249/249], loss=42.2198
	Evaluating
	loss=0.0049, precision=0.4275, recall=0.8392, f1=0.5665
saving model as: 3_saved_model.pth
Training epoch 107
	step [1/249], loss=63.7873
	step [2/249], loss=75.4667
	step [3/249], loss=66.1380
	step [4/249], loss=62.4574
	step [5/249], loss=60.0696
	step [6/249], loss=69.4604
	step [7/249], loss=61.6996
	step [8/249], loss=72.7769
	step [9/249], loss=61.1548
	step [10/249], loss=65.4761
	step [11/249], loss=57.4289
	step [12/249], loss=71.1992
	step [13/249], loss=66.5736
	step [14/249], loss=66.6277
	step [15/249], loss=78.3713
	step [16/249], loss=66.6703
	step [17/249], loss=79.7069
	step [18/249], loss=88.3418
	step [19/249], loss=61.4020
	step [20/249], loss=62.0387
	step [21/249], loss=76.7927
	step [22/249], loss=79.2826
	step [23/249], loss=75.8106
	step [24/249], loss=68.5424
	step [25/249], loss=61.6015
	step [26/249], loss=61.5219
	step [27/249], loss=71.5546
	step [28/249], loss=62.0067
	step [29/249], loss=72.0920
	step [30/249], loss=71.5354
	step [31/249], loss=80.0454
	step [32/249], loss=62.7576
	step [33/249], loss=77.6044
	step [34/249], loss=70.3141
	step [35/249], loss=56.7154
	step [36/249], loss=59.4900
	step [37/249], loss=79.4616
	step [38/249], loss=60.3759
	step [39/249], loss=73.2817
	step [40/249], loss=73.1982
	step [41/249], loss=61.9022
	step [42/249], loss=64.6362
	step [43/249], loss=73.8258
	step [44/249], loss=81.9744
	step [45/249], loss=70.6901
	step [46/249], loss=72.0854
	step [47/249], loss=61.1252
	step [48/249], loss=70.0118
	step [49/249], loss=68.5370
	step [50/249], loss=47.9443
	step [51/249], loss=68.8769
	step [52/249], loss=68.6878
	step [53/249], loss=66.1231
	step [54/249], loss=72.1503
	step [55/249], loss=68.6358
	step [56/249], loss=66.4348
	step [57/249], loss=59.0284
	step [58/249], loss=69.9355
	step [59/249], loss=80.2720
	step [60/249], loss=71.3736
	step [61/249], loss=62.2995
	step [62/249], loss=72.5835
	step [63/249], loss=86.6740
	step [64/249], loss=82.9354
	step [65/249], loss=65.6591
	step [66/249], loss=76.6458
	step [67/249], loss=69.3208
	step [68/249], loss=71.9787
	step [69/249], loss=61.0711
	step [70/249], loss=63.2672
	step [71/249], loss=65.7428
	step [72/249], loss=79.4127
	step [73/249], loss=72.2458
	step [74/249], loss=75.9859
	step [75/249], loss=64.9540
	step [76/249], loss=73.8310
	step [77/249], loss=67.4081
	step [78/249], loss=73.1362
	step [79/249], loss=76.4791
	step [80/249], loss=50.1690
	step [81/249], loss=69.0708
	step [82/249], loss=71.0053
	step [83/249], loss=63.8671
	step [84/249], loss=59.5703
	step [85/249], loss=57.5844
	step [86/249], loss=66.3382
	step [87/249], loss=61.9849
	step [88/249], loss=75.5828
	step [89/249], loss=70.1304
	step [90/249], loss=59.9775
	step [91/249], loss=64.4957
	step [92/249], loss=71.2408
	step [93/249], loss=72.4657
	step [94/249], loss=64.6347
	step [95/249], loss=66.5644
	step [96/249], loss=75.8314
	step [97/249], loss=73.0994
	step [98/249], loss=69.0174
	step [99/249], loss=75.0953
	step [100/249], loss=75.2182
	step [101/249], loss=61.6762
	step [102/249], loss=82.2549
	step [103/249], loss=72.3516
	step [104/249], loss=66.9056
	step [105/249], loss=86.1373
	step [106/249], loss=74.9807
	step [107/249], loss=68.3190
	step [108/249], loss=70.6091
	step [109/249], loss=73.2100
	step [110/249], loss=80.2252
	step [111/249], loss=71.6134
	step [112/249], loss=64.1032
	step [113/249], loss=71.1919
	step [114/249], loss=79.6409
	step [115/249], loss=60.8316
	step [116/249], loss=59.3157
	step [117/249], loss=72.5233
	step [118/249], loss=68.5885
	step [119/249], loss=67.3919
	step [120/249], loss=76.5911
	step [121/249], loss=77.8822
	step [122/249], loss=60.8423
	step [123/249], loss=66.9115
	step [124/249], loss=67.6831
	step [125/249], loss=71.1259
	step [126/249], loss=71.2253
	step [127/249], loss=61.0452
	step [128/249], loss=66.7970
	step [129/249], loss=76.5797
	step [130/249], loss=75.3604
	step [131/249], loss=68.6019
	step [132/249], loss=71.4589
	step [133/249], loss=65.9508
	step [134/249], loss=81.3165
	step [135/249], loss=61.1163
	step [136/249], loss=70.3184
	step [137/249], loss=65.5760
	step [138/249], loss=82.2142
	step [139/249], loss=84.8483
	step [140/249], loss=89.2138
	step [141/249], loss=75.2434
	step [142/249], loss=66.3262
	step [143/249], loss=72.6771
	step [144/249], loss=64.9679
	step [145/249], loss=69.7457
	step [146/249], loss=73.9048
	step [147/249], loss=77.0714
	step [148/249], loss=64.4671
	step [149/249], loss=85.8680
	step [150/249], loss=64.1462
	step [151/249], loss=74.7388
	step [152/249], loss=61.4398
	step [153/249], loss=59.0759
	step [154/249], loss=78.5071
	step [155/249], loss=81.2351
	step [156/249], loss=64.8973
	step [157/249], loss=70.0685
	step [158/249], loss=74.0048
	step [159/249], loss=65.4822
	step [160/249], loss=69.0869
	step [161/249], loss=72.1425
	step [162/249], loss=67.3008
	step [163/249], loss=71.0665
	step [164/249], loss=73.6373
	step [165/249], loss=80.3087
	step [166/249], loss=79.3117
	step [167/249], loss=69.5346
	step [168/249], loss=72.5374
	step [169/249], loss=59.0488
	step [170/249], loss=68.8946
	step [171/249], loss=60.4129
	step [172/249], loss=79.2952
	step [173/249], loss=85.2050
	step [174/249], loss=69.9807
	step [175/249], loss=76.8936
	step [176/249], loss=74.3790
	step [177/249], loss=67.9881
	step [178/249], loss=66.3466
	step [179/249], loss=71.9381
	step [180/249], loss=62.5789
	step [181/249], loss=87.1731
	step [182/249], loss=78.3085
	step [183/249], loss=67.6044
	step [184/249], loss=78.5196
	step [185/249], loss=69.1590
	step [186/249], loss=74.7623
	step [187/249], loss=83.7760
	step [188/249], loss=65.9982
	step [189/249], loss=85.2547
	step [190/249], loss=78.0087
	step [191/249], loss=77.6889
	step [192/249], loss=70.1066
	step [193/249], loss=63.8692
	step [194/249], loss=81.2994
	step [195/249], loss=64.3183
	step [196/249], loss=71.0684
	step [197/249], loss=65.4603
	step [198/249], loss=60.5492
	step [199/249], loss=72.2602
	step [200/249], loss=61.6584
	step [201/249], loss=75.2408
	step [202/249], loss=71.9438
	step [203/249], loss=84.7012
	step [204/249], loss=73.1898
	step [205/249], loss=84.6482
	step [206/249], loss=66.7174
	step [207/249], loss=72.5826
	step [208/249], loss=82.9590
	step [209/249], loss=63.6895
	step [210/249], loss=63.0502
	step [211/249], loss=73.4594
	step [212/249], loss=67.9925
	step [213/249], loss=64.8711
	step [214/249], loss=56.7874
	step [215/249], loss=63.6308
	step [216/249], loss=72.8599
	step [217/249], loss=71.6509
	step [218/249], loss=83.6049
	step [219/249], loss=66.4445
	step [220/249], loss=66.0556
	step [221/249], loss=75.0952
	step [222/249], loss=75.1922
	step [223/249], loss=61.1386
	step [224/249], loss=74.8417
	step [225/249], loss=86.6578
	step [226/249], loss=62.5935
	step [227/249], loss=71.0420
	step [228/249], loss=63.5648
	step [229/249], loss=88.3780
	step [230/249], loss=65.1244
	step [231/249], loss=89.1295
	step [232/249], loss=72.8664
	step [233/249], loss=68.6435
	step [234/249], loss=61.7036
	step [235/249], loss=59.9776
	step [236/249], loss=85.6917
	step [237/249], loss=70.5989
	step [238/249], loss=55.4722
	step [239/249], loss=66.5540
	step [240/249], loss=69.6194
	step [241/249], loss=69.0208
	step [242/249], loss=72.5320
	step [243/249], loss=70.0245
	step [244/249], loss=72.5928
	step [245/249], loss=76.0076
	step [246/249], loss=71.1426
	step [247/249], loss=76.3347
	step [248/249], loss=70.2481
	step [249/249], loss=37.7530
	Evaluating
	loss=0.0051, precision=0.4142, recall=0.8344, f1=0.5536
Training epoch 108
	step [1/249], loss=71.0247
	step [2/249], loss=75.2695
	step [3/249], loss=83.6434
	step [4/249], loss=77.4837
	step [5/249], loss=77.3305
	step [6/249], loss=81.5883
	step [7/249], loss=64.9232
	step [8/249], loss=61.7269
	step [9/249], loss=69.7501
	step [10/249], loss=84.0143
	step [11/249], loss=69.4190
	step [12/249], loss=67.6824
	step [13/249], loss=62.6158
	step [14/249], loss=51.2658
	step [15/249], loss=61.0304
	step [16/249], loss=67.6762
	step [17/249], loss=72.3184
	step [18/249], loss=62.2541
	step [19/249], loss=72.2934
	step [20/249], loss=74.1309
	step [21/249], loss=74.2867
	step [22/249], loss=61.9068
	step [23/249], loss=77.1758
	step [24/249], loss=69.1073
	step [25/249], loss=66.3689
	step [26/249], loss=82.3927
	step [27/249], loss=67.0141
	step [28/249], loss=60.9622
	step [29/249], loss=72.9442
	step [30/249], loss=83.5529
	step [31/249], loss=78.1468
	step [32/249], loss=61.9995
	step [33/249], loss=56.4893
	step [34/249], loss=76.6919
	step [35/249], loss=82.0180
	step [36/249], loss=77.5668
	step [37/249], loss=63.1518
	step [38/249], loss=54.7714
	step [39/249], loss=66.8720
	step [40/249], loss=81.9619
	step [41/249], loss=79.4747
	step [42/249], loss=69.1515
	step [43/249], loss=91.3963
	step [44/249], loss=70.5787
	step [45/249], loss=68.0288
	step [46/249], loss=67.9458
	step [47/249], loss=61.7671
	step [48/249], loss=80.4111
	step [49/249], loss=75.7448
	step [50/249], loss=78.3517
	step [51/249], loss=72.0849
	step [52/249], loss=56.8197
	step [53/249], loss=73.0522
	step [54/249], loss=61.1721
	step [55/249], loss=66.1758
	step [56/249], loss=65.3229
	step [57/249], loss=66.7650
	step [58/249], loss=59.0475
	step [59/249], loss=65.6496
	step [60/249], loss=66.5423
	step [61/249], loss=65.4748
	step [62/249], loss=64.2489
	step [63/249], loss=76.8088
	step [64/249], loss=78.5005
	step [65/249], loss=87.8225
	step [66/249], loss=74.9197
	step [67/249], loss=66.4925
	step [68/249], loss=68.4001
	step [69/249], loss=68.2062
	step [70/249], loss=63.6887
	step [71/249], loss=63.0820
	step [72/249], loss=81.7543
	step [73/249], loss=75.7111
	step [74/249], loss=61.6047
	step [75/249], loss=57.8562
	step [76/249], loss=68.1571
	step [77/249], loss=61.6833
	step [78/249], loss=73.2895
	step [79/249], loss=68.4052
	step [80/249], loss=74.6276
	step [81/249], loss=76.5375
	step [82/249], loss=82.8552
	step [83/249], loss=66.4196
	step [84/249], loss=80.8982
	step [85/249], loss=77.6927
	step [86/249], loss=64.3636
	step [87/249], loss=81.4132
	step [88/249], loss=74.5265
	step [89/249], loss=83.7352
	step [90/249], loss=72.0068
	step [91/249], loss=73.7138
	step [92/249], loss=75.7227
	step [93/249], loss=92.9017
	step [94/249], loss=60.8645
	step [95/249], loss=69.8138
	step [96/249], loss=61.6014
	step [97/249], loss=59.4002
	step [98/249], loss=61.7257
	step [99/249], loss=81.4411
	step [100/249], loss=72.9568
	step [101/249], loss=57.3409
	step [102/249], loss=65.6020
	step [103/249], loss=64.3033
	step [104/249], loss=77.8338
	step [105/249], loss=71.8108
	step [106/249], loss=68.3623
	step [107/249], loss=66.7267
	step [108/249], loss=73.3288
	step [109/249], loss=75.1251
	step [110/249], loss=69.2587
	step [111/249], loss=68.0015
	step [112/249], loss=66.6333
	step [113/249], loss=68.3149
	step [114/249], loss=59.9264
	step [115/249], loss=81.9076
	step [116/249], loss=73.3474
	step [117/249], loss=75.8627
	step [118/249], loss=70.6334
	step [119/249], loss=68.7715
	step [120/249], loss=75.5923
	step [121/249], loss=72.1910
	step [122/249], loss=74.9050
	step [123/249], loss=57.0598
	step [124/249], loss=61.4648
	step [125/249], loss=69.8188
	step [126/249], loss=90.2889
	step [127/249], loss=69.4987
	step [128/249], loss=73.1499
	step [129/249], loss=73.3649
	step [130/249], loss=61.2913
	step [131/249], loss=67.1875
	step [132/249], loss=71.5378
	step [133/249], loss=68.8699
	step [134/249], loss=65.5399
	step [135/249], loss=81.2320
	step [136/249], loss=61.0222
	step [137/249], loss=70.1202
	step [138/249], loss=62.3071
	step [139/249], loss=78.6960
	step [140/249], loss=62.1987
	step [141/249], loss=55.5756
	step [142/249], loss=72.6104
	step [143/249], loss=70.6763
	step [144/249], loss=65.5656
	step [145/249], loss=68.0000
	step [146/249], loss=73.9678
	step [147/249], loss=80.5657
	step [148/249], loss=64.2481
	step [149/249], loss=52.4787
	step [150/249], loss=78.5660
	step [151/249], loss=53.1217
	step [152/249], loss=74.0918
	step [153/249], loss=60.3157
	step [154/249], loss=72.7677
	step [155/249], loss=71.5876
	step [156/249], loss=70.2295
	step [157/249], loss=81.2159
	step [158/249], loss=86.0007
	step [159/249], loss=73.9707
	step [160/249], loss=74.4496
	step [161/249], loss=61.5017
	step [162/249], loss=73.9968
	step [163/249], loss=67.6347
	step [164/249], loss=72.9385
	step [165/249], loss=74.9000
	step [166/249], loss=97.5450
	step [167/249], loss=77.6571
	step [168/249], loss=70.4352
	step [169/249], loss=62.7740
	step [170/249], loss=64.6428
	step [171/249], loss=66.5092
	step [172/249], loss=74.5234
	step [173/249], loss=83.9807
	step [174/249], loss=82.1421
	step [175/249], loss=68.1986
	step [176/249], loss=50.9990
	step [177/249], loss=65.2776
	step [178/249], loss=58.6520
	step [179/249], loss=60.6052
	step [180/249], loss=62.2761
	step [181/249], loss=65.4141
	step [182/249], loss=65.2441
	step [183/249], loss=78.0418
	step [184/249], loss=66.7999
	step [185/249], loss=70.5450
	step [186/249], loss=73.5747
	step [187/249], loss=64.1384
	step [188/249], loss=73.8010
	step [189/249], loss=71.9933
	step [190/249], loss=76.5948
	step [191/249], loss=49.2458
	step [192/249], loss=57.6987
	step [193/249], loss=64.3744
	step [194/249], loss=69.0079
	step [195/249], loss=90.3303
	step [196/249], loss=67.9167
	step [197/249], loss=59.8283
	step [198/249], loss=82.0255
	step [199/249], loss=57.3720
	step [200/249], loss=75.4689
	step [201/249], loss=80.8117
	step [202/249], loss=65.0948
	step [203/249], loss=64.7283
	step [204/249], loss=80.4020
	step [205/249], loss=63.7508
	step [206/249], loss=63.5459
	step [207/249], loss=60.9680
	step [208/249], loss=74.1822
	step [209/249], loss=76.5686
	step [210/249], loss=62.5017
	step [211/249], loss=77.6807
	step [212/249], loss=73.3137
	step [213/249], loss=68.8362
	step [214/249], loss=69.9612
	step [215/249], loss=68.3799
	step [216/249], loss=78.9626
	step [217/249], loss=83.6476
	step [218/249], loss=58.0375
	step [219/249], loss=72.9062
	step [220/249], loss=73.4945
	step [221/249], loss=68.7543
	step [222/249], loss=74.3836
	step [223/249], loss=65.3269
	step [224/249], loss=68.0349
	step [225/249], loss=47.6828
	step [226/249], loss=84.4481
	step [227/249], loss=71.5852
	step [228/249], loss=59.8776
	step [229/249], loss=69.4375
	step [230/249], loss=64.4251
	step [231/249], loss=69.5637
	step [232/249], loss=76.1635
	step [233/249], loss=77.4223
	step [234/249], loss=60.1890
	step [235/249], loss=75.0340
	step [236/249], loss=72.9483
	step [237/249], loss=73.4685
	step [238/249], loss=72.4395
	step [239/249], loss=97.2284
	step [240/249], loss=68.7016
	step [241/249], loss=70.8592
	step [242/249], loss=66.9527
	step [243/249], loss=66.3754
	step [244/249], loss=66.2205
	step [245/249], loss=67.0740
	step [246/249], loss=65.0642
	step [247/249], loss=67.8597
	step [248/249], loss=74.7443
	step [249/249], loss=41.6298
	Evaluating
	loss=0.0050, precision=0.4230, recall=0.8394, f1=0.5626
Training epoch 109
	step [1/249], loss=74.4341
	step [2/249], loss=74.9738
	step [3/249], loss=68.8588
	step [4/249], loss=78.2136
	step [5/249], loss=65.8435
	step [6/249], loss=59.2569
	step [7/249], loss=77.2525
	step [8/249], loss=78.0510
	step [9/249], loss=55.3348
	step [10/249], loss=80.7191
	step [11/249], loss=65.4859
	step [12/249], loss=73.2153
	step [13/249], loss=76.1583
	step [14/249], loss=77.2583
	step [15/249], loss=70.3772
	step [16/249], loss=66.9632
	step [17/249], loss=77.4007
	step [18/249], loss=83.1391
	step [19/249], loss=61.8884
	step [20/249], loss=58.2800
	step [21/249], loss=71.5164
	step [22/249], loss=63.1430
	step [23/249], loss=64.3093
	step [24/249], loss=70.7363
	step [25/249], loss=75.1778
	step [26/249], loss=71.4629
	step [27/249], loss=59.1610
	step [28/249], loss=79.0556
	step [29/249], loss=66.7491
	step [30/249], loss=72.7322
	step [31/249], loss=72.0869
	step [32/249], loss=69.3950
	step [33/249], loss=76.3299
	step [34/249], loss=76.3341
	step [35/249], loss=69.4304
	step [36/249], loss=67.0725
	step [37/249], loss=80.2325
	step [38/249], loss=80.1716
	step [39/249], loss=79.0101
	step [40/249], loss=60.6406
	step [41/249], loss=78.9655
	step [42/249], loss=68.7241
	step [43/249], loss=72.2939
	step [44/249], loss=82.0670
	step [45/249], loss=77.7724
	step [46/249], loss=73.6179
	step [47/249], loss=73.9799
	step [48/249], loss=62.4134
	step [49/249], loss=70.5528
	step [50/249], loss=79.3716
	step [51/249], loss=69.4112
	step [52/249], loss=54.9880
	step [53/249], loss=71.0219
	step [54/249], loss=71.6878
	step [55/249], loss=74.6805
	step [56/249], loss=60.1234
	step [57/249], loss=74.2592
	step [58/249], loss=85.4708
	step [59/249], loss=55.4547
	step [60/249], loss=69.5678
	step [61/249], loss=76.7130
	step [62/249], loss=60.8470
	step [63/249], loss=68.1778
	step [64/249], loss=75.5664
	step [65/249], loss=80.3861
	step [66/249], loss=90.6890
	step [67/249], loss=66.2339
	step [68/249], loss=87.2447
	step [69/249], loss=68.7582
	step [70/249], loss=51.6928
	step [71/249], loss=63.5224
	step [72/249], loss=71.7808
	step [73/249], loss=71.8188
	step [74/249], loss=64.6799
	step [75/249], loss=62.3666
	step [76/249], loss=88.7448
	step [77/249], loss=80.0964
	step [78/249], loss=72.5426
	step [79/249], loss=74.4015
	step [80/249], loss=70.1853
	step [81/249], loss=73.3141
	step [82/249], loss=84.7795
	step [83/249], loss=76.7702
	step [84/249], loss=59.0836
	step [85/249], loss=80.3678
	step [86/249], loss=67.7264
	step [87/249], loss=76.0694
	step [88/249], loss=77.0379
	step [89/249], loss=69.1924
	step [90/249], loss=90.3173
	step [91/249], loss=56.8549
	step [92/249], loss=71.9049
	step [93/249], loss=82.0753
	step [94/249], loss=63.8515
	step [95/249], loss=59.8103
	step [96/249], loss=88.2225
	step [97/249], loss=54.9010
	step [98/249], loss=70.1933
	step [99/249], loss=74.7764
	step [100/249], loss=66.3476
	step [101/249], loss=67.2307
	step [102/249], loss=69.5137
	step [103/249], loss=57.9476
	step [104/249], loss=66.9102
	step [105/249], loss=64.3298
	step [106/249], loss=65.2334
	step [107/249], loss=77.3439
	step [108/249], loss=69.7221
	step [109/249], loss=57.3470
	step [110/249], loss=72.6689
	step [111/249], loss=64.5772
	step [112/249], loss=65.9271
	step [113/249], loss=69.4370
	step [114/249], loss=66.1517
	step [115/249], loss=70.7469
	step [116/249], loss=71.6228
	step [117/249], loss=87.0376
	step [118/249], loss=74.9022
	step [119/249], loss=76.6490
	step [120/249], loss=74.8285
	step [121/249], loss=71.0001
	step [122/249], loss=67.6311
	step [123/249], loss=65.8775
	step [124/249], loss=70.5144
	step [125/249], loss=65.9858
	step [126/249], loss=55.2337
	step [127/249], loss=75.9303
	step [128/249], loss=79.0503
	step [129/249], loss=71.5264
	step [130/249], loss=69.4164
	step [131/249], loss=64.8603
	step [132/249], loss=70.5150
	step [133/249], loss=59.9221
	step [134/249], loss=72.4056
	step [135/249], loss=58.3178
	step [136/249], loss=76.0191
	step [137/249], loss=56.4087
	step [138/249], loss=61.4500
	step [139/249], loss=74.8267
	step [140/249], loss=74.5746
	step [141/249], loss=67.1924
	step [142/249], loss=55.5571
	step [143/249], loss=73.1001
	step [144/249], loss=73.9836
	step [145/249], loss=68.7892
	step [146/249], loss=62.6590
	step [147/249], loss=64.5217
	step [148/249], loss=54.1047
	step [149/249], loss=77.5615
	step [150/249], loss=66.4822
	step [151/249], loss=75.4347
	step [152/249], loss=61.3679
	step [153/249], loss=61.2706
	step [154/249], loss=70.1784
	step [155/249], loss=76.7215
	step [156/249], loss=85.1213
	step [157/249], loss=69.7346
	step [158/249], loss=60.1066
	step [159/249], loss=70.7144
	step [160/249], loss=74.1310
	step [161/249], loss=55.8787
	step [162/249], loss=78.3007
	step [163/249], loss=64.9595
	step [164/249], loss=79.7655
	step [165/249], loss=56.4915
	step [166/249], loss=71.7228
	step [167/249], loss=60.8131
	step [168/249], loss=56.5098
	step [169/249], loss=67.0046
	step [170/249], loss=62.1464
	step [171/249], loss=80.2088
	step [172/249], loss=60.2049
	step [173/249], loss=63.4121
	step [174/249], loss=78.1911
	step [175/249], loss=69.9282
	step [176/249], loss=66.0896
	step [177/249], loss=57.5837
	step [178/249], loss=75.1566
	step [179/249], loss=63.9783
	step [180/249], loss=75.0491
	step [181/249], loss=62.9681
	step [182/249], loss=84.6805
	step [183/249], loss=65.1840
	step [184/249], loss=81.5710
	step [185/249], loss=67.0594
	step [186/249], loss=74.6418
	step [187/249], loss=51.6438
	step [188/249], loss=59.9244
	step [189/249], loss=64.7796
	step [190/249], loss=68.3593
	step [191/249], loss=72.3245
	step [192/249], loss=60.4785
	step [193/249], loss=58.4000
	step [194/249], loss=67.2065
	step [195/249], loss=75.8551
	step [196/249], loss=77.2775
	step [197/249], loss=77.9656
	step [198/249], loss=75.1886
	step [199/249], loss=67.0889
	step [200/249], loss=71.8482
	step [201/249], loss=67.7093
	step [202/249], loss=86.7034
	step [203/249], loss=66.9854
	step [204/249], loss=65.1943
	step [205/249], loss=67.0101
	step [206/249], loss=66.1632
	step [207/249], loss=74.3889
	step [208/249], loss=61.9406
	step [209/249], loss=78.8807
	step [210/249], loss=89.3765
	step [211/249], loss=64.5835
	step [212/249], loss=66.9722
	step [213/249], loss=59.6736
	step [214/249], loss=63.2314
	step [215/249], loss=60.8445
	step [216/249], loss=55.4322
	step [217/249], loss=72.2019
	step [218/249], loss=70.5965
	step [219/249], loss=69.5169
	step [220/249], loss=81.5056
	step [221/249], loss=67.8218
	step [222/249], loss=61.4340
	step [223/249], loss=80.7852
	step [224/249], loss=69.9087
	step [225/249], loss=66.9188
	step [226/249], loss=71.3855
	step [227/249], loss=73.5829
	step [228/249], loss=70.8867
	step [229/249], loss=74.7142
	step [230/249], loss=63.5799
	step [231/249], loss=65.2646
	step [232/249], loss=74.4542
	step [233/249], loss=69.2200
	step [234/249], loss=64.3592
	step [235/249], loss=76.3484
	step [236/249], loss=68.4031
	step [237/249], loss=74.4325
	step [238/249], loss=61.0252
	step [239/249], loss=64.0577
	step [240/249], loss=69.8067
	step [241/249], loss=68.1153
	step [242/249], loss=72.5123
	step [243/249], loss=71.3089
	step [244/249], loss=56.7923
	step [245/249], loss=62.4893
	step [246/249], loss=90.3632
	step [247/249], loss=70.3340
	step [248/249], loss=94.8979
	step [249/249], loss=64.8648
	Evaluating
	loss=0.0059, precision=0.3725, recall=0.8580, f1=0.5195
Training finished
best_f1: 0.566470992373221
directing: Y rim_enhanced: True test_id 4
removed wrong scan: weights_Y_335_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_383_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_262_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_311_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_260_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_225_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_320_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_192_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_319_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_226_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_346_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_271_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_277_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_290_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_269_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_379_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_375_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_378_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_366_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_309_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_200_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_327_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_193_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_174_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_253_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_220_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_265_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_360_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_303_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_232_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_243_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_240_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_237_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_256_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_254_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_254_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_231_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_359_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_172_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_340_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_263_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_388_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_307_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_278_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_196_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_272_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_314_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_350_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_361_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_194_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_323_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_201_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_299_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_259_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_276_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_167_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_185_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_215_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_268_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_264_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_384_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_261_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_280_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_287_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_385_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_216_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_349_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_348_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_371_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_357_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_234_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_288_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_324_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_229_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_184_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_148_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_168_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_370_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_286_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_302_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_265_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_153_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_274_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_255_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_217_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_355_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_294_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_159_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_338_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_236_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_152_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_292_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_222_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_376_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_183_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_258_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_282_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_278_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_284_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_300_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_163_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_197_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_203_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_243_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_224_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_238_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_374_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_218_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_259_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_282_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_387_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_353_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_386_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_296_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_341_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_206_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_220_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_239_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_231_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_202_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_358_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_246_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_315_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_275_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_169_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_316_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_221_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_188_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_208_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_273_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_251_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_224_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_333_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_263_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_173_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_243_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_241_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_326_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_230_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_236_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_345_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_206_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_258_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_187_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_87_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_154_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_253_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_337_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_232_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_147_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_377_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_362_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_233_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_250_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_297_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_354_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_262_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_225_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_365_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_356_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_242_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_245_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_230_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_318_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_181_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_305_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_255_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_298_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_301_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_293_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_229_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_332_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_257_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_295_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_239_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_260_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_228_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_317_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_364_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_262_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_162_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_310_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_160_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_237_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_254_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_257_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_291_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_279_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_215_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_372_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_218_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_210_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_158_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_156_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_339_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_190_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_373_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_205_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_329_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_380_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_271_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_235_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_244_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_312_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_186_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_308_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_347_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_241_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_270_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_261_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_222_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_189_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_289_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_146_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_252_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_344_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_331_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_313_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_328_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_265_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_283_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_248_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_381_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_211_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_214_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_176_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_221_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_150_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_343_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_249_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_322_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_351_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_234_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_195_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_330_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_334_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_306_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_213_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_325_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_342_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_247_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_352_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_225_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_226_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_256_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_242_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_368_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_382_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_157_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_238_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_267_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_367_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_321_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_233_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_281_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_161_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_336_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_170_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_304_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_44_xwqg-B00034_2020-04-03.npy
# all image files: 21521 # all weight files in weight_dir: 17227 # image files with weight 17227
removed wrong scan: weights_Y_335_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_383_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_262_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_311_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_260_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_225_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_320_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_192_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_319_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_226_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_346_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_271_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_277_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_290_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_269_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_379_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_375_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_378_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_366_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_309_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_200_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_327_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_193_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_174_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_253_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_220_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_265_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_360_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_303_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_232_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_243_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_240_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_237_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_256_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_254_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_254_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_231_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_359_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_172_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_340_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_263_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_388_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_307_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_278_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_196_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_272_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_314_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_350_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_361_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_194_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_323_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_201_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_299_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_259_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_276_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_167_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_185_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_215_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_268_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_264_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_384_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_261_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_280_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_287_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_385_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_216_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_349_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_348_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_371_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_357_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_234_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_288_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_324_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_229_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_184_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_148_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_168_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_370_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_286_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_302_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_265_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_153_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_274_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_255_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_217_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_355_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_294_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_159_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_338_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_236_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_152_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_292_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_222_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_376_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_183_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_258_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_282_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_278_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_284_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_300_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_163_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_197_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_203_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_243_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_224_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_238_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_374_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_218_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_259_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_282_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_387_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_353_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_386_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_296_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_341_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_206_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_220_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_239_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_231_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_202_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_358_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_246_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_315_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_275_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_169_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_316_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_221_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_188_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_208_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_273_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_251_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_224_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_333_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_263_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_173_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_243_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_241_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_326_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_230_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_236_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_345_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_206_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_258_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_187_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_87_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_154_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_253_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_337_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_232_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_147_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_377_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_362_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_233_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_250_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_297_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_354_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_262_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_225_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_365_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_356_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_242_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_245_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_230_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_318_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_181_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_305_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_255_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_298_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_301_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_293_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_229_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_332_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_257_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_295_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_239_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_260_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_228_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_317_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_364_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_262_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_162_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_310_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_160_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_237_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_254_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_257_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_291_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_279_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_215_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_372_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_218_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_210_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_158_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_156_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_339_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_190_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_373_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_205_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_329_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_380_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_271_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_235_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_244_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_312_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_186_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_308_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_347_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_241_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_270_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_261_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_222_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_189_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_289_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_146_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_252_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_344_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_331_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_313_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_328_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_265_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_283_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_248_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_381_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_211_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_214_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_176_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_221_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_150_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_343_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_249_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_322_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_351_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_234_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_195_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_330_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_334_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_306_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_213_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_325_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_342_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_247_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_352_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_225_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_226_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_256_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_242_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_368_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_382_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_157_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_238_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_267_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_367_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_321_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_233_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_281_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_161_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_336_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_170_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_304_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_44_xwqg-B00034_2020-04-03.npy
# all image files: 21521 # all weight files in weight_dir: 2845 # image files with weight 2845
train: /ibex/scratch/projects/c2052/air_tube_seg/training_samples/Y 17227
Using 4 GPUs
Going to train epochs [56-105]
Training epoch 56
	step [1/270], loss=78.4730
	step [2/270], loss=78.6449
	step [3/270], loss=70.3205
	step [4/270], loss=83.9727
	step [5/270], loss=91.4191
	step [6/270], loss=68.8836
	step [7/270], loss=84.6598
	step [8/270], loss=77.6251
	step [9/270], loss=88.8841
	step [10/270], loss=79.2347
	step [11/270], loss=85.4238
	step [12/270], loss=83.1114
	step [13/270], loss=83.3715
	step [14/270], loss=77.2348
	step [15/270], loss=100.7320
	step [16/270], loss=79.9394
	step [17/270], loss=74.1345
	step [18/270], loss=98.3571
	step [19/270], loss=92.3998
	step [20/270], loss=82.1358
	step [21/270], loss=90.3891
	step [22/270], loss=86.9769
	step [23/270], loss=81.7470
	step [24/270], loss=80.4048
	step [25/270], loss=99.8748
	step [26/270], loss=86.6526
	step [27/270], loss=97.4978
	step [28/270], loss=78.6151
	step [29/270], loss=73.5253
	step [30/270], loss=90.1780
	step [31/270], loss=87.0498
	step [32/270], loss=83.6738
	step [33/270], loss=82.5612
	step [34/270], loss=90.2933
	step [35/270], loss=79.5333
	step [36/270], loss=74.8639
	step [37/270], loss=92.5134
	step [38/270], loss=83.9078
	step [39/270], loss=59.1764
	step [40/270], loss=82.2576
	step [41/270], loss=72.1997
	step [42/270], loss=65.4094
	step [43/270], loss=76.2793
	step [44/270], loss=63.0351
	step [45/270], loss=68.9750
	step [46/270], loss=109.4012
	step [47/270], loss=97.6551
	step [48/270], loss=77.2764
	step [49/270], loss=94.8196
	step [50/270], loss=97.0858
	step [51/270], loss=78.8133
	step [52/270], loss=88.9440
	step [53/270], loss=69.3432
	step [54/270], loss=75.4405
	step [55/270], loss=79.3123
	step [56/270], loss=86.7589
	step [57/270], loss=72.4694
	step [58/270], loss=92.7894
	step [59/270], loss=86.6453
	step [60/270], loss=100.7825
	step [61/270], loss=84.9146
	step [62/270], loss=98.5412
	step [63/270], loss=105.6673
	step [64/270], loss=84.5630
	step [65/270], loss=85.8089
	step [66/270], loss=60.7653
	step [67/270], loss=80.3344
	step [68/270], loss=92.7890
	step [69/270], loss=76.8092
	step [70/270], loss=80.1645
	step [71/270], loss=85.4908
	step [72/270], loss=75.6380
	step [73/270], loss=75.2110
	step [74/270], loss=79.2439
	step [75/270], loss=88.3000
	step [76/270], loss=60.8848
	step [77/270], loss=91.7042
	step [78/270], loss=103.8237
	step [79/270], loss=75.4032
	step [80/270], loss=71.3511
	step [81/270], loss=68.6425
	step [82/270], loss=76.4832
	step [83/270], loss=94.6606
	step [84/270], loss=80.6632
	step [85/270], loss=103.5118
	step [86/270], loss=79.4864
	step [87/270], loss=80.0412
	step [88/270], loss=74.7223
	step [89/270], loss=84.4574
	step [90/270], loss=90.0288
	step [91/270], loss=81.5440
	step [92/270], loss=70.0825
	step [93/270], loss=74.9327
	step [94/270], loss=66.5837
	step [95/270], loss=79.1831
	step [96/270], loss=90.4018
	step [97/270], loss=81.0106
	step [98/270], loss=74.7456
	step [99/270], loss=82.7106
	step [100/270], loss=77.6737
	step [101/270], loss=102.2649
	step [102/270], loss=98.3466
	step [103/270], loss=71.7419
	step [104/270], loss=82.7246
	step [105/270], loss=81.7912
	step [106/270], loss=67.8344
	step [107/270], loss=85.5505
	step [108/270], loss=78.5238
	step [109/270], loss=89.9166
	step [110/270], loss=98.8433
	step [111/270], loss=79.6572
	step [112/270], loss=62.2771
	step [113/270], loss=94.1567
	step [114/270], loss=92.7906
	step [115/270], loss=75.9241
	step [116/270], loss=94.1526
	step [117/270], loss=91.0764
	step [118/270], loss=86.4436
	step [119/270], loss=70.9052
	step [120/270], loss=81.0718
	step [121/270], loss=98.5642
	step [122/270], loss=78.3119
	step [123/270], loss=116.5451
	step [124/270], loss=83.1938
	step [125/270], loss=74.8009
	step [126/270], loss=75.6899
	step [127/270], loss=87.0566
	step [128/270], loss=87.2721
	step [129/270], loss=83.0168
	step [130/270], loss=100.3866
	step [131/270], loss=74.2483
	step [132/270], loss=85.4830
	step [133/270], loss=66.6096
	step [134/270], loss=75.0866
	step [135/270], loss=76.9159
	step [136/270], loss=86.8158
	step [137/270], loss=96.7089
	step [138/270], loss=101.0418
	step [139/270], loss=95.1539
	step [140/270], loss=118.6357
	step [141/270], loss=68.3095
	step [142/270], loss=87.7931
	step [143/270], loss=81.7343
	step [144/270], loss=87.5865
	step [145/270], loss=86.3851
	step [146/270], loss=107.4799
	step [147/270], loss=80.7945
	step [148/270], loss=88.0182
	step [149/270], loss=104.1338
	step [150/270], loss=84.7378
	step [151/270], loss=81.2904
	step [152/270], loss=81.6879
	step [153/270], loss=71.9343
	step [154/270], loss=83.8858
	step [155/270], loss=85.4173
	step [156/270], loss=77.5004
	step [157/270], loss=81.7902
	step [158/270], loss=94.7099
	step [159/270], loss=83.1263
	step [160/270], loss=86.2791
	step [161/270], loss=75.7935
	step [162/270], loss=97.6588
	step [163/270], loss=91.7813
	step [164/270], loss=107.0775
	step [165/270], loss=92.9227
	step [166/270], loss=92.5812
	step [167/270], loss=72.0646
	step [168/270], loss=88.4918
	step [169/270], loss=90.1755
	step [170/270], loss=87.6830
	step [171/270], loss=84.0346
	step [172/270], loss=76.2659
	step [173/270], loss=70.5594
	step [174/270], loss=98.2990
	step [175/270], loss=72.5149
	step [176/270], loss=89.2741
	step [177/270], loss=103.2664
	step [178/270], loss=86.4394
	step [179/270], loss=71.2590
	step [180/270], loss=80.8511
	step [181/270], loss=93.8777
	step [182/270], loss=89.1235
	step [183/270], loss=85.5141
	step [184/270], loss=94.7242
	step [185/270], loss=70.1519
	step [186/270], loss=72.6289
	step [187/270], loss=77.2941
	step [188/270], loss=78.9136
	step [189/270], loss=82.5590
	step [190/270], loss=98.4546
	step [191/270], loss=82.2699
	step [192/270], loss=80.8250
	step [193/270], loss=82.0809
	step [194/270], loss=82.0622
	step [195/270], loss=81.4132
	step [196/270], loss=80.6650
	step [197/270], loss=77.6656
	step [198/270], loss=97.4999
	step [199/270], loss=80.9236
	step [200/270], loss=78.7144
	step [201/270], loss=74.1889
	step [202/270], loss=84.1111
	step [203/270], loss=93.6450
	step [204/270], loss=79.6635
	step [205/270], loss=90.4976
	step [206/270], loss=65.0648
	step [207/270], loss=84.3916
	step [208/270], loss=91.8163
	step [209/270], loss=82.3127
	step [210/270], loss=83.2745
	step [211/270], loss=77.7418
	step [212/270], loss=72.7328
	step [213/270], loss=73.6334
	step [214/270], loss=75.7098
	step [215/270], loss=91.0599
	step [216/270], loss=76.7492
	step [217/270], loss=81.0096
	step [218/270], loss=70.4477
	step [219/270], loss=84.5605
	step [220/270], loss=83.5316
	step [221/270], loss=94.1791
	step [222/270], loss=86.8502
	step [223/270], loss=76.5890
	step [224/270], loss=70.1506
	step [225/270], loss=98.2656
	step [226/270], loss=85.9740
	step [227/270], loss=70.9176
	step [228/270], loss=96.7895
	step [229/270], loss=99.2026
	step [230/270], loss=85.5743
	step [231/270], loss=74.5388
	step [232/270], loss=87.0425
	step [233/270], loss=93.0182
	step [234/270], loss=88.0031
	step [235/270], loss=94.7794
	step [236/270], loss=106.0007
	step [237/270], loss=74.4916
	step [238/270], loss=112.5083
	step [239/270], loss=91.7115
	step [240/270], loss=90.4436
	step [241/270], loss=92.2314
	step [242/270], loss=75.9323
	step [243/270], loss=90.3885
	step [244/270], loss=64.1766
	step [245/270], loss=85.7066
	step [246/270], loss=65.8052
	step [247/270], loss=90.5971
	step [248/270], loss=68.0875
	step [249/270], loss=87.2027
	step [250/270], loss=102.7158
	step [251/270], loss=87.8877
	step [252/270], loss=73.1249
	step [253/270], loss=69.3840
	step [254/270], loss=90.1117
	step [255/270], loss=82.6643
	step [256/270], loss=79.2161
	step [257/270], loss=81.6633
	step [258/270], loss=91.4126
	step [259/270], loss=84.7430
	step [260/270], loss=88.3780
	step [261/270], loss=90.1562
	step [262/270], loss=91.4662
	step [263/270], loss=98.3651
	step [264/270], loss=100.4803
	step [265/270], loss=92.5465
	step [266/270], loss=83.5312
	step [267/270], loss=86.4476
	step [268/270], loss=64.8610
	step [269/270], loss=98.2399
	step [270/270], loss=14.0016
	Evaluating
	loss=0.0093, precision=0.3007, recall=0.9077, f1=0.4517
saving model as: 4_saved_model.pth
Training epoch 57
	step [1/270], loss=74.5019
	step [2/270], loss=96.0813
	step [3/270], loss=73.9101
	step [4/270], loss=72.0599
	step [5/270], loss=97.4615
	step [6/270], loss=76.3746
	step [7/270], loss=97.2459
	step [8/270], loss=84.6185
	step [9/270], loss=79.9782
	step [10/270], loss=77.3959
	step [11/270], loss=79.5600
	step [12/270], loss=88.1657
	step [13/270], loss=107.7786
	step [14/270], loss=84.4161
	step [15/270], loss=88.0280
	step [16/270], loss=89.7782
	step [17/270], loss=82.3763
	step [18/270], loss=71.9652
	step [19/270], loss=74.8956
	step [20/270], loss=77.6341
	step [21/270], loss=91.9855
	step [22/270], loss=94.8880
	step [23/270], loss=68.6564
	step [24/270], loss=75.4928
	step [25/270], loss=90.9199
	step [26/270], loss=89.0996
	step [27/270], loss=82.7666
	step [28/270], loss=85.4584
	step [29/270], loss=90.1715
	step [30/270], loss=111.4875
	step [31/270], loss=88.2577
	step [32/270], loss=68.2082
	step [33/270], loss=87.5106
	step [34/270], loss=85.4174
	step [35/270], loss=89.2931
	step [36/270], loss=81.6302
	step [37/270], loss=78.8262
	step [38/270], loss=79.8129
	step [39/270], loss=82.4788
	step [40/270], loss=92.7132
	step [41/270], loss=88.0789
	step [42/270], loss=94.8768
	step [43/270], loss=95.2852
	step [44/270], loss=79.8396
	step [45/270], loss=81.9787
	step [46/270], loss=72.8322
	step [47/270], loss=77.6332
	step [48/270], loss=66.6560
	step [49/270], loss=102.1916
	step [50/270], loss=79.3871
	step [51/270], loss=78.9525
	step [52/270], loss=88.1697
	step [53/270], loss=78.4699
	step [54/270], loss=78.3081
	step [55/270], loss=104.0040
	step [56/270], loss=82.4058
	step [57/270], loss=69.8452
	step [58/270], loss=83.4506
	step [59/270], loss=78.7211
	step [60/270], loss=80.8122
	step [61/270], loss=97.9200
	step [62/270], loss=82.0111
	step [63/270], loss=87.5654
	step [64/270], loss=101.1023
	step [65/270], loss=87.3439
	step [66/270], loss=74.0333
	step [67/270], loss=74.0601
	step [68/270], loss=93.9594
	step [69/270], loss=89.1580
	step [70/270], loss=91.9612
	step [71/270], loss=87.8013
	step [72/270], loss=88.3058
	step [73/270], loss=97.3928
	step [74/270], loss=90.0024
	step [75/270], loss=68.4985
	step [76/270], loss=85.5187
	step [77/270], loss=83.9203
	step [78/270], loss=97.6670
	step [79/270], loss=87.2231
	step [80/270], loss=98.6281
	step [81/270], loss=86.1566
	step [82/270], loss=73.1777
	step [83/270], loss=92.0740
	step [84/270], loss=63.8427
	step [85/270], loss=83.0257
	step [86/270], loss=72.5911
	step [87/270], loss=97.8960
	step [88/270], loss=81.2114
	step [89/270], loss=65.5653
	step [90/270], loss=65.7457
	step [91/270], loss=73.3424
	step [92/270], loss=95.6236
	step [93/270], loss=116.0311
	step [94/270], loss=116.8627
	step [95/270], loss=98.6371
	step [96/270], loss=79.9807
	step [97/270], loss=66.6635
	step [98/270], loss=85.3362
	step [99/270], loss=79.6869
	step [100/270], loss=72.2627
	step [101/270], loss=73.4326
	step [102/270], loss=97.9620
	step [103/270], loss=83.1142
	step [104/270], loss=94.1488
	step [105/270], loss=79.4972
	step [106/270], loss=79.8047
	step [107/270], loss=82.9578
	step [108/270], loss=90.1512
	step [109/270], loss=84.9136
	step [110/270], loss=79.4803
	step [111/270], loss=85.1993
	step [112/270], loss=61.1485
	step [113/270], loss=74.6414
	step [114/270], loss=98.9346
	step [115/270], loss=92.2966
	step [116/270], loss=88.7647
	step [117/270], loss=79.0300
	step [118/270], loss=91.5172
	step [119/270], loss=81.1968
	step [120/270], loss=84.8628
	step [121/270], loss=85.5538
	step [122/270], loss=70.8681
	step [123/270], loss=82.6952
	step [124/270], loss=78.3132
	step [125/270], loss=88.9405
	step [126/270], loss=90.1457
	step [127/270], loss=86.9336
	step [128/270], loss=85.6102
	step [129/270], loss=80.4040
	step [130/270], loss=81.2537
	step [131/270], loss=72.8022
	step [132/270], loss=97.4706
	step [133/270], loss=93.1578
	step [134/270], loss=88.7182
	step [135/270], loss=77.3852
	step [136/270], loss=79.9024
	step [137/270], loss=85.3590
	step [138/270], loss=93.8395
	step [139/270], loss=57.0323
	step [140/270], loss=76.5691
	step [141/270], loss=99.4339
	step [142/270], loss=90.7902
	step [143/270], loss=65.0851
	step [144/270], loss=94.9072
	step [145/270], loss=72.8439
	step [146/270], loss=84.2640
	step [147/270], loss=74.8251
	step [148/270], loss=79.2069
	step [149/270], loss=71.1650
	step [150/270], loss=85.4798
	step [151/270], loss=83.0496
	step [152/270], loss=85.9893
	step [153/270], loss=73.2217
	step [154/270], loss=67.3335
	step [155/270], loss=84.6686
	step [156/270], loss=84.1464
	step [157/270], loss=86.3106
	step [158/270], loss=95.7203
	step [159/270], loss=85.6059
	step [160/270], loss=81.3321
	step [161/270], loss=65.8618
	step [162/270], loss=89.6496
	step [163/270], loss=86.3137
	step [164/270], loss=89.1954
	step [165/270], loss=99.2257
	step [166/270], loss=81.1322
	step [167/270], loss=97.8557
	step [168/270], loss=78.1036
	step [169/270], loss=83.8280
	step [170/270], loss=64.9993
	step [171/270], loss=99.3296
	step [172/270], loss=76.1968
	step [173/270], loss=79.2524
	step [174/270], loss=72.3821
	step [175/270], loss=74.4737
	step [176/270], loss=101.6741
	step [177/270], loss=93.2171
	step [178/270], loss=86.4333
	step [179/270], loss=91.5249
	step [180/270], loss=74.0895
	step [181/270], loss=86.3690
	step [182/270], loss=68.9241
	step [183/270], loss=84.9608
	step [184/270], loss=78.4842
	step [185/270], loss=86.8676
	step [186/270], loss=86.7448
	step [187/270], loss=85.2738
	step [188/270], loss=95.2237
	step [189/270], loss=87.9745
	step [190/270], loss=98.7344
	step [191/270], loss=85.6338
	step [192/270], loss=84.1845
	step [193/270], loss=68.6942
	step [194/270], loss=70.1276
	step [195/270], loss=82.0174
	step [196/270], loss=78.9766
	step [197/270], loss=88.0779
	step [198/270], loss=87.8796
	step [199/270], loss=83.2145
	step [200/270], loss=81.2722
	step [201/270], loss=90.0784
	step [202/270], loss=89.0083
	step [203/270], loss=99.6569
	step [204/270], loss=80.8628
	step [205/270], loss=81.6998
	step [206/270], loss=89.2082
	step [207/270], loss=82.9323
	step [208/270], loss=77.1062
	step [209/270], loss=93.9212
	step [210/270], loss=82.7658
	step [211/270], loss=79.8282
	step [212/270], loss=72.5850
	step [213/270], loss=87.7619
	step [214/270], loss=82.9418
	step [215/270], loss=89.8107
	step [216/270], loss=85.7907
	step [217/270], loss=83.1763
	step [218/270], loss=72.6705
	step [219/270], loss=65.1991
	step [220/270], loss=85.6180
	step [221/270], loss=74.9403
	step [222/270], loss=77.9789
	step [223/270], loss=81.4368
	step [224/270], loss=89.9107
	step [225/270], loss=78.7805
	step [226/270], loss=70.9163
	step [227/270], loss=90.0383
	step [228/270], loss=89.8075
	step [229/270], loss=73.6594
	step [230/270], loss=88.7742
	step [231/270], loss=97.2598
	step [232/270], loss=91.1848
	step [233/270], loss=86.6694
	step [234/270], loss=91.2935
	step [235/270], loss=69.7849
	step [236/270], loss=82.3151
	step [237/270], loss=72.4081
	step [238/270], loss=84.7464
	step [239/270], loss=91.8032
	step [240/270], loss=107.3043
	step [241/270], loss=81.0817
	step [242/270], loss=82.8590
	step [243/270], loss=71.5512
	step [244/270], loss=83.2963
	step [245/270], loss=86.9230
	step [246/270], loss=87.4723
	step [247/270], loss=76.7914
	step [248/270], loss=77.9923
	step [249/270], loss=79.5389
	step [250/270], loss=75.4668
	step [251/270], loss=83.6526
	step [252/270], loss=86.1490
	step [253/270], loss=66.6885
	step [254/270], loss=83.8731
	step [255/270], loss=56.3536
	step [256/270], loss=73.0127
	step [257/270], loss=76.2860
	step [258/270], loss=88.4629
	step [259/270], loss=84.8643
	step [260/270], loss=70.6453
	step [261/270], loss=87.0792
	step [262/270], loss=75.9477
	step [263/270], loss=68.5151
	step [264/270], loss=83.6054
	step [265/270], loss=92.5261
	step [266/270], loss=95.7585
	step [267/270], loss=94.1638
	step [268/270], loss=89.6660
	step [269/270], loss=112.8325
	step [270/270], loss=18.1486
	Evaluating
	loss=0.0093, precision=0.2888, recall=0.9104, f1=0.4385
Training epoch 58
	step [1/270], loss=85.5434
	step [2/270], loss=76.1294
	step [3/270], loss=80.0250
	step [4/270], loss=77.0653
	step [5/270], loss=84.3609
	step [6/270], loss=83.9749
	step [7/270], loss=76.9435
	step [8/270], loss=83.7789
	step [9/270], loss=83.0693
	step [10/270], loss=74.6481
	step [11/270], loss=84.3664
	step [12/270], loss=67.8568
	step [13/270], loss=107.8379
	step [14/270], loss=102.3558
	step [15/270], loss=97.6820
	step [16/270], loss=91.6335
	step [17/270], loss=79.5841
	step [18/270], loss=64.8214
	step [19/270], loss=84.5571
	step [20/270], loss=69.3947
	step [21/270], loss=82.1830
	step [22/270], loss=84.4672
	step [23/270], loss=80.9195
	step [24/270], loss=70.7612
	step [25/270], loss=91.5940
	step [26/270], loss=83.8261
	step [27/270], loss=74.8422
	step [28/270], loss=87.4431
	step [29/270], loss=95.2226
	step [30/270], loss=86.0834
	step [31/270], loss=91.4624
	step [32/270], loss=86.9888
	step [33/270], loss=87.7186
	step [34/270], loss=75.8519
	step [35/270], loss=88.7787
	step [36/270], loss=77.0171
	step [37/270], loss=83.0273
	step [38/270], loss=74.0952
	step [39/270], loss=78.0725
	step [40/270], loss=84.4126
	step [41/270], loss=92.1291
	step [42/270], loss=81.9105
	step [43/270], loss=82.6294
	step [44/270], loss=91.5374
	step [45/270], loss=75.7626
	step [46/270], loss=79.7368
	step [47/270], loss=64.4827
	step [48/270], loss=67.9980
	step [49/270], loss=66.0428
	step [50/270], loss=80.4912
	step [51/270], loss=70.5957
	step [52/270], loss=90.6348
	step [53/270], loss=72.0037
	step [54/270], loss=86.9278
	step [55/270], loss=71.2255
	step [56/270], loss=82.8327
	step [57/270], loss=84.6122
	step [58/270], loss=87.3929
	step [59/270], loss=93.0171
	step [60/270], loss=90.2670
	step [61/270], loss=76.1928
	step [62/270], loss=74.6302
	step [63/270], loss=85.2925
	step [64/270], loss=78.9662
	step [65/270], loss=80.8642
	step [66/270], loss=73.7201
	step [67/270], loss=85.1766
	step [68/270], loss=78.4934
	step [69/270], loss=100.7034
	step [70/270], loss=62.3591
	step [71/270], loss=90.3436
	step [72/270], loss=78.0909
	step [73/270], loss=78.1250
	step [74/270], loss=85.9232
	step [75/270], loss=87.9265
	step [76/270], loss=88.4934
	step [77/270], loss=91.0286
	step [78/270], loss=103.6391
	step [79/270], loss=71.2360
	step [80/270], loss=78.2172
	step [81/270], loss=86.1896
	step [82/270], loss=87.9635
	step [83/270], loss=78.6494
	step [84/270], loss=77.8789
	step [85/270], loss=91.1887
	step [86/270], loss=89.6868
	step [87/270], loss=83.0090
	step [88/270], loss=104.9626
	step [89/270], loss=78.2990
	step [90/270], loss=87.9019
	step [91/270], loss=73.5302
	step [92/270], loss=82.0291
	step [93/270], loss=67.7040
	step [94/270], loss=76.1355
	step [95/270], loss=60.6387
	step [96/270], loss=101.5117
	step [97/270], loss=82.7752
	step [98/270], loss=79.4156
	step [99/270], loss=81.6493
	step [100/270], loss=94.7772
	step [101/270], loss=81.9470
	step [102/270], loss=81.1934
	step [103/270], loss=93.6579
	step [104/270], loss=85.0727
	step [105/270], loss=85.5754
	step [106/270], loss=83.5215
	step [107/270], loss=104.8249
	step [108/270], loss=88.7656
	step [109/270], loss=108.0435
	step [110/270], loss=90.0619
	step [111/270], loss=78.0884
	step [112/270], loss=71.4182
	step [113/270], loss=79.4630
	step [114/270], loss=79.9204
	step [115/270], loss=101.2370
	step [116/270], loss=86.0409
	step [117/270], loss=87.5477
	step [118/270], loss=86.9927
	step [119/270], loss=93.7090
	step [120/270], loss=107.5652
	step [121/270], loss=78.9295
	step [122/270], loss=88.3930
	step [123/270], loss=76.0587
	step [124/270], loss=91.2750
	step [125/270], loss=93.0372
	step [126/270], loss=74.0566
	step [127/270], loss=74.6509
	step [128/270], loss=85.4778
	step [129/270], loss=78.3880
	step [130/270], loss=98.1673
	step [131/270], loss=78.8330
	step [132/270], loss=78.3587
	step [133/270], loss=84.3308
	step [134/270], loss=83.2520
	step [135/270], loss=81.3127
	step [136/270], loss=90.7404
	step [137/270], loss=80.0463
	step [138/270], loss=77.7003
	step [139/270], loss=81.2883
	step [140/270], loss=86.9147
	step [141/270], loss=88.8902
	step [142/270], loss=62.2358
	step [143/270], loss=77.2408
	step [144/270], loss=78.4367
	step [145/270], loss=82.1000
	step [146/270], loss=95.6720
	step [147/270], loss=78.9490
	step [148/270], loss=88.9154
	step [149/270], loss=73.6053
	step [150/270], loss=94.4144
	step [151/270], loss=74.7647
	step [152/270], loss=72.1801
	step [153/270], loss=81.2759
	step [154/270], loss=87.7178
	step [155/270], loss=66.3478
	step [156/270], loss=82.1253
	step [157/270], loss=89.6115
	step [158/270], loss=102.4044
	step [159/270], loss=85.0384
	step [160/270], loss=103.6993
	step [161/270], loss=63.7277
	step [162/270], loss=87.1503
	step [163/270], loss=74.3687
	step [164/270], loss=82.3513
	step [165/270], loss=77.6181
	step [166/270], loss=76.5992
	step [167/270], loss=81.4690
	step [168/270], loss=62.3377
	step [169/270], loss=95.2601
	step [170/270], loss=83.6013
	step [171/270], loss=84.9194
	step [172/270], loss=79.1552
	step [173/270], loss=84.6983
	step [174/270], loss=85.9335
	step [175/270], loss=84.2791
	step [176/270], loss=99.3245
	step [177/270], loss=75.8254
	step [178/270], loss=93.2205
	step [179/270], loss=84.0156
	step [180/270], loss=93.5499
	step [181/270], loss=85.3821
	step [182/270], loss=86.1660
	step [183/270], loss=82.8490
	step [184/270], loss=78.7596
	step [185/270], loss=85.7613
	step [186/270], loss=119.6172
	step [187/270], loss=87.5552
	step [188/270], loss=82.6331
	step [189/270], loss=84.9037
	step [190/270], loss=76.3513
	step [191/270], loss=100.1093
	step [192/270], loss=90.0014
	step [193/270], loss=75.3455
	step [194/270], loss=75.3088
	step [195/270], loss=73.4621
	step [196/270], loss=91.1912
	step [197/270], loss=67.7652
	step [198/270], loss=76.0570
	step [199/270], loss=100.7321
	step [200/270], loss=73.3625
	step [201/270], loss=73.4542
	step [202/270], loss=82.5177
	step [203/270], loss=101.8683
	step [204/270], loss=91.1997
	step [205/270], loss=87.9534
	step [206/270], loss=68.3874
	step [207/270], loss=84.9116
	step [208/270], loss=80.3540
	step [209/270], loss=77.2430
	step [210/270], loss=80.1425
	step [211/270], loss=88.3040
	step [212/270], loss=71.2338
	step [213/270], loss=80.1624
	step [214/270], loss=82.1337
	step [215/270], loss=86.8759
	step [216/270], loss=89.9732
	step [217/270], loss=83.3563
	step [218/270], loss=73.9147
	step [219/270], loss=75.2548
	step [220/270], loss=98.4482
	step [221/270], loss=92.9714
	step [222/270], loss=77.3766
	step [223/270], loss=76.3771
	step [224/270], loss=83.8041
	step [225/270], loss=92.6104
	step [226/270], loss=86.0785
	step [227/270], loss=76.4087
	step [228/270], loss=85.5048
	step [229/270], loss=84.4869
	step [230/270], loss=80.7023
	step [231/270], loss=91.5008
	step [232/270], loss=102.3085
	step [233/270], loss=90.7953
	step [234/270], loss=91.4481
	step [235/270], loss=81.5620
	step [236/270], loss=84.0269
	step [237/270], loss=73.9087
	step [238/270], loss=77.1835
	step [239/270], loss=87.4059
	step [240/270], loss=85.6297
	step [241/270], loss=85.0821
	step [242/270], loss=90.9299
	step [243/270], loss=93.4627
	step [244/270], loss=105.6799
	step [245/270], loss=88.9708
	step [246/270], loss=96.9855
	step [247/270], loss=63.4226
	step [248/270], loss=73.5358
	step [249/270], loss=83.7532
	step [250/270], loss=83.7779
	step [251/270], loss=77.0523
	step [252/270], loss=75.1907
	step [253/270], loss=72.2251
	step [254/270], loss=62.4136
	step [255/270], loss=74.1014
	step [256/270], loss=73.6934
	step [257/270], loss=84.0650
	step [258/270], loss=86.1255
	step [259/270], loss=71.0928
	step [260/270], loss=73.9875
	step [261/270], loss=71.4360
	step [262/270], loss=95.1585
	step [263/270], loss=89.8092
	step [264/270], loss=73.2127
	step [265/270], loss=94.8534
	step [266/270], loss=99.2554
	step [267/270], loss=92.3232
	step [268/270], loss=77.5164
	step [269/270], loss=74.0874
	step [270/270], loss=14.2718
	Evaluating
	loss=0.0076, precision=0.3473, recall=0.9074, f1=0.5024
saving model as: 4_saved_model.pth
Training epoch 59
	step [1/270], loss=67.4319
	step [2/270], loss=66.3681
	step [3/270], loss=86.1518
	step [4/270], loss=91.0860
	step [5/270], loss=74.1826
	step [6/270], loss=85.6608
	step [7/270], loss=78.8520
	step [8/270], loss=77.0958
	step [9/270], loss=87.8005
	step [10/270], loss=72.6487
	step [11/270], loss=89.7772
	step [12/270], loss=85.5475
	step [13/270], loss=94.8529
	step [14/270], loss=93.9472
	step [15/270], loss=80.9825
	step [16/270], loss=81.6370
	step [17/270], loss=81.7267
	step [18/270], loss=86.1580
	step [19/270], loss=84.2835
	step [20/270], loss=71.7572
	step [21/270], loss=70.1504
	step [22/270], loss=86.2210
	step [23/270], loss=90.1710
	step [24/270], loss=58.2551
	step [25/270], loss=85.9813
	step [26/270], loss=74.0733
	step [27/270], loss=74.1656
	step [28/270], loss=78.9376
	step [29/270], loss=73.9506
	step [30/270], loss=97.7591
	step [31/270], loss=95.3390
	step [32/270], loss=83.3721
	step [33/270], loss=88.3234
	step [34/270], loss=73.9439
	step [35/270], loss=78.0319
	step [36/270], loss=78.6763
	step [37/270], loss=85.6635
	step [38/270], loss=85.2309
	step [39/270], loss=77.8168
	step [40/270], loss=99.8072
	step [41/270], loss=81.1751
	step [42/270], loss=67.5947
	step [43/270], loss=76.5094
	step [44/270], loss=82.7052
	step [45/270], loss=82.1889
	step [46/270], loss=73.8846
	step [47/270], loss=94.0532
	step [48/270], loss=92.3050
	step [49/270], loss=80.1225
	step [50/270], loss=93.4725
	step [51/270], loss=80.6701
	step [52/270], loss=86.7928
	step [53/270], loss=76.7422
	step [54/270], loss=95.6035
	step [55/270], loss=67.6806
	step [56/270], loss=65.4786
	step [57/270], loss=84.8867
	step [58/270], loss=84.2204
	step [59/270], loss=83.9648
	step [60/270], loss=90.9992
	step [61/270], loss=89.5612
	step [62/270], loss=84.7274
	step [63/270], loss=87.4612
	step [64/270], loss=77.2405
	step [65/270], loss=105.4006
	step [66/270], loss=76.0029
	step [67/270], loss=95.0522
	step [68/270], loss=100.6821
	step [69/270], loss=83.0134
	step [70/270], loss=88.8453
	step [71/270], loss=84.3628
	step [72/270], loss=78.8321
	step [73/270], loss=74.0858
	step [74/270], loss=69.5849
	step [75/270], loss=92.1028
	step [76/270], loss=89.5749
	step [77/270], loss=90.3064
	step [78/270], loss=94.3684
	step [79/270], loss=90.0396
	step [80/270], loss=87.2012
	step [81/270], loss=76.5382
	step [82/270], loss=75.1546
	step [83/270], loss=102.0143
	step [84/270], loss=98.1263
	step [85/270], loss=80.2325
	step [86/270], loss=77.7767
	step [87/270], loss=79.6322
	step [88/270], loss=78.3814
	step [89/270], loss=77.7932
	step [90/270], loss=81.4332
	step [91/270], loss=87.6051
	step [92/270], loss=44.7361
	step [93/270], loss=92.2889
	step [94/270], loss=79.5189
	step [95/270], loss=91.0075
	step [96/270], loss=117.6634
	step [97/270], loss=76.9285
	step [98/270], loss=90.3163
	step [99/270], loss=79.3209
	step [100/270], loss=88.3457
	step [101/270], loss=91.7545
	step [102/270], loss=111.3434
	step [103/270], loss=87.2170
	step [104/270], loss=60.3316
	step [105/270], loss=87.2844
	step [106/270], loss=98.6348
	step [107/270], loss=80.2275
	step [108/270], loss=87.6872
	step [109/270], loss=87.4847
	step [110/270], loss=71.8487
	step [111/270], loss=94.3034
	step [112/270], loss=79.5494
	step [113/270], loss=82.2987
	step [114/270], loss=70.6682
	step [115/270], loss=80.0912
	step [116/270], loss=86.8951
	step [117/270], loss=87.8306
	step [118/270], loss=91.9870
	step [119/270], loss=90.2812
	step [120/270], loss=87.3388
	step [121/270], loss=86.5351
	step [122/270], loss=77.0840
	step [123/270], loss=76.3098
	step [124/270], loss=82.0165
	step [125/270], loss=86.5306
	step [126/270], loss=77.6101
	step [127/270], loss=91.1821
	step [128/270], loss=75.9468
	step [129/270], loss=92.5011
	step [130/270], loss=80.3113
	step [131/270], loss=86.1256
	step [132/270], loss=90.8964
	step [133/270], loss=75.1328
	step [134/270], loss=74.1408
	step [135/270], loss=92.7566
	step [136/270], loss=88.8765
	step [137/270], loss=94.7747
	step [138/270], loss=65.7112
	step [139/270], loss=67.2168
	step [140/270], loss=84.6638
	step [141/270], loss=71.6648
	step [142/270], loss=80.3139
	step [143/270], loss=76.8096
	step [144/270], loss=74.7733
	step [145/270], loss=92.4152
	step [146/270], loss=81.2021
	step [147/270], loss=81.1748
	step [148/270], loss=86.2751
	step [149/270], loss=64.4834
	step [150/270], loss=104.5070
	step [151/270], loss=84.0869
	step [152/270], loss=77.3734
	step [153/270], loss=78.2736
	step [154/270], loss=74.0887
	step [155/270], loss=79.0702
	step [156/270], loss=67.8456
	step [157/270], loss=84.2929
	step [158/270], loss=111.0807
	step [159/270], loss=88.0202
	step [160/270], loss=93.5279
	step [161/270], loss=86.7618
	step [162/270], loss=90.6301
	step [163/270], loss=81.8906
	step [164/270], loss=85.3377
	step [165/270], loss=86.8100
	step [166/270], loss=85.6887
	step [167/270], loss=84.6092
	step [168/270], loss=92.0613
	step [169/270], loss=90.1057
	step [170/270], loss=79.3162
	step [171/270], loss=77.5854
	step [172/270], loss=69.2635
	step [173/270], loss=75.4053
	step [174/270], loss=85.2298
	step [175/270], loss=69.1211
	step [176/270], loss=69.7430
	step [177/270], loss=96.3260
	step [178/270], loss=77.1268
	step [179/270], loss=93.9681
	step [180/270], loss=64.4175
	step [181/270], loss=102.4378
	step [182/270], loss=89.5948
	step [183/270], loss=80.9746
	step [184/270], loss=70.4665
	step [185/270], loss=69.4378
	step [186/270], loss=79.9708
	step [187/270], loss=97.9801
	step [188/270], loss=69.8957
	step [189/270], loss=74.8223
	step [190/270], loss=82.2974
	step [191/270], loss=89.4933
	step [192/270], loss=67.9981
	step [193/270], loss=74.5421
	step [194/270], loss=85.7136
	step [195/270], loss=89.5592
	step [196/270], loss=87.5967
	step [197/270], loss=86.0181
	step [198/270], loss=96.1920
	step [199/270], loss=73.9824
	step [200/270], loss=121.8988
	step [201/270], loss=77.5525
	step [202/270], loss=78.0922
	step [203/270], loss=95.7549
	step [204/270], loss=94.8790
	step [205/270], loss=83.1213
	step [206/270], loss=76.5354
	step [207/270], loss=79.6157
	step [208/270], loss=78.2593
	step [209/270], loss=87.1784
	step [210/270], loss=82.3608
	step [211/270], loss=93.5690
	step [212/270], loss=87.4018
	step [213/270], loss=85.1984
	step [214/270], loss=76.7155
	step [215/270], loss=76.7514
	step [216/270], loss=84.3284
	step [217/270], loss=77.3806
	step [218/270], loss=84.6293
	step [219/270], loss=73.2490
	step [220/270], loss=80.0148
	step [221/270], loss=75.5442
	step [222/270], loss=65.6388
	step [223/270], loss=92.1105
	step [224/270], loss=91.7716
	step [225/270], loss=87.2418
	step [226/270], loss=68.2336
	step [227/270], loss=64.5363
	step [228/270], loss=75.1223
	step [229/270], loss=76.4808
	step [230/270], loss=96.1175
	step [231/270], loss=87.0246
	step [232/270], loss=81.1894
	step [233/270], loss=70.5051
	step [234/270], loss=68.3517
	step [235/270], loss=76.9179
	step [236/270], loss=87.9412
	step [237/270], loss=99.2337
	step [238/270], loss=98.7664
	step [239/270], loss=87.8505
	step [240/270], loss=95.7131
	step [241/270], loss=87.6243
	step [242/270], loss=84.4848
	step [243/270], loss=74.8296
	step [244/270], loss=90.9895
	step [245/270], loss=86.4550
	step [246/270], loss=92.0089
	step [247/270], loss=95.3326
	step [248/270], loss=89.0815
	step [249/270], loss=78.4447
	step [250/270], loss=80.7927
	step [251/270], loss=93.3990
	step [252/270], loss=80.2094
	step [253/270], loss=86.2311
	step [254/270], loss=74.9852
	step [255/270], loss=76.2481
	step [256/270], loss=79.1304
	step [257/270], loss=72.6774
	step [258/270], loss=86.4626
	step [259/270], loss=84.0375
	step [260/270], loss=80.0081
	step [261/270], loss=105.5240
	step [262/270], loss=78.0792
	step [263/270], loss=79.0166
	step [264/270], loss=84.1626
	step [265/270], loss=96.2136
	step [266/270], loss=68.1213
	step [267/270], loss=78.0016
	step [268/270], loss=76.1980
	step [269/270], loss=89.4898
	step [270/270], loss=15.2781
	Evaluating
	loss=0.0083, precision=0.3242, recall=0.9025, f1=0.4770
Training epoch 60
	step [1/270], loss=74.8337
	step [2/270], loss=77.6782
	step [3/270], loss=87.6780
	step [4/270], loss=89.0211
	step [5/270], loss=78.2761
	step [6/270], loss=68.2937
	step [7/270], loss=87.7924
	step [8/270], loss=81.2675
	step [9/270], loss=85.1401
	step [10/270], loss=86.2766
	step [11/270], loss=72.2880
	step [12/270], loss=70.3600
	step [13/270], loss=98.7955
	step [14/270], loss=82.0294
	step [15/270], loss=82.7951
	step [16/270], loss=79.7980
	step [17/270], loss=97.1864
	step [18/270], loss=70.1811
	step [19/270], loss=85.8855
	step [20/270], loss=79.0942
	step [21/270], loss=67.2246
	step [22/270], loss=87.6884
	step [23/270], loss=85.1820
	step [24/270], loss=94.3090
	step [25/270], loss=90.5504
	step [26/270], loss=76.6166
	step [27/270], loss=94.8954
	step [28/270], loss=99.9763
	step [29/270], loss=90.8651
	step [30/270], loss=88.2223
	step [31/270], loss=83.8884
	step [32/270], loss=82.2055
	step [33/270], loss=79.7699
	step [34/270], loss=90.0310
	step [35/270], loss=82.8439
	step [36/270], loss=90.2895
	step [37/270], loss=63.5786
	step [38/270], loss=81.5158
	step [39/270], loss=80.2072
	step [40/270], loss=97.7928
	step [41/270], loss=66.5867
	step [42/270], loss=71.1218
	step [43/270], loss=80.4634
	step [44/270], loss=90.5682
	step [45/270], loss=70.5328
	step [46/270], loss=66.6889
	step [47/270], loss=97.9618
	step [48/270], loss=85.0213
	step [49/270], loss=69.4826
	step [50/270], loss=78.3781
	step [51/270], loss=78.2066
	step [52/270], loss=88.0322
	step [53/270], loss=85.8208
	step [54/270], loss=76.9537
	step [55/270], loss=74.9444
	step [56/270], loss=84.6159
	step [57/270], loss=79.5307
	step [58/270], loss=72.9521
	step [59/270], loss=91.8004
	step [60/270], loss=88.1962
	step [61/270], loss=86.8260
	step [62/270], loss=87.4968
	step [63/270], loss=92.2793
	step [64/270], loss=101.7755
	step [65/270], loss=92.8047
	step [66/270], loss=79.6911
	step [67/270], loss=76.3332
	step [68/270], loss=81.1234
	step [69/270], loss=81.0851
	step [70/270], loss=85.1572
	step [71/270], loss=81.5024
	step [72/270], loss=88.2330
	step [73/270], loss=90.6789
	step [74/270], loss=84.0040
	step [75/270], loss=75.4050
	step [76/270], loss=76.9773
	step [77/270], loss=86.0879
	step [78/270], loss=86.1563
	step [79/270], loss=79.1535
	step [80/270], loss=88.8027
	step [81/270], loss=66.2245
	step [82/270], loss=90.7666
	step [83/270], loss=71.6860
	step [84/270], loss=89.2668
	step [85/270], loss=65.5507
	step [86/270], loss=65.7055
	step [87/270], loss=75.3225
	step [88/270], loss=61.7870
	step [89/270], loss=78.8913
	step [90/270], loss=98.3871
	step [91/270], loss=78.8671
	step [92/270], loss=102.4677
	step [93/270], loss=87.4192
	step [94/270], loss=90.3262
	step [95/270], loss=91.6630
	step [96/270], loss=73.1238
	step [97/270], loss=87.0465
	step [98/270], loss=66.9808
	step [99/270], loss=78.6863
	step [100/270], loss=79.1282
	step [101/270], loss=106.6874
	step [102/270], loss=77.5382
	step [103/270], loss=78.2137
	step [104/270], loss=79.5420
	step [105/270], loss=82.0511
	step [106/270], loss=87.1223
	step [107/270], loss=97.3803
	step [108/270], loss=83.2989
	step [109/270], loss=89.1047
	step [110/270], loss=96.0870
	step [111/270], loss=74.6255
	step [112/270], loss=82.7189
	step [113/270], loss=69.1979
	step [114/270], loss=79.3289
	step [115/270], loss=80.9061
	step [116/270], loss=74.5572
	step [117/270], loss=65.2274
	step [118/270], loss=79.9623
	step [119/270], loss=82.9594
	step [120/270], loss=79.4351
	step [121/270], loss=80.4016
	step [122/270], loss=60.4129
	step [123/270], loss=93.0020
	step [124/270], loss=87.0150
	step [125/270], loss=80.8263
	step [126/270], loss=63.8765
	step [127/270], loss=79.4042
	step [128/270], loss=90.4623
	step [129/270], loss=91.5402
	step [130/270], loss=88.5365
	step [131/270], loss=84.5131
	step [132/270], loss=81.4991
	step [133/270], loss=74.8326
	step [134/270], loss=81.9535
	step [135/270], loss=80.6135
	step [136/270], loss=84.2203
	step [137/270], loss=87.1284
	step [138/270], loss=87.7423
	step [139/270], loss=83.2894
	step [140/270], loss=85.4054
	step [141/270], loss=74.3145
	step [142/270], loss=88.5204
	step [143/270], loss=90.1075
	step [144/270], loss=82.3972
	step [145/270], loss=77.8852
	step [146/270], loss=91.4256
	step [147/270], loss=66.6150
	step [148/270], loss=92.8813
	step [149/270], loss=79.0899
	step [150/270], loss=101.4347
	step [151/270], loss=84.8489
	step [152/270], loss=82.2060
	step [153/270], loss=95.7440
	step [154/270], loss=70.7063
	step [155/270], loss=88.7767
	step [156/270], loss=96.6490
	step [157/270], loss=69.1003
	step [158/270], loss=92.0337
	step [159/270], loss=70.3436
	step [160/270], loss=92.7626
	step [161/270], loss=78.2630
	step [162/270], loss=84.7230
	step [163/270], loss=80.7907
	step [164/270], loss=76.8390
	step [165/270], loss=81.4299
	step [166/270], loss=102.8911
	step [167/270], loss=73.6137
	step [168/270], loss=80.4365
	step [169/270], loss=92.6546
	step [170/270], loss=79.9264
	step [171/270], loss=86.3891
	step [172/270], loss=92.3325
	step [173/270], loss=79.8515
	step [174/270], loss=91.0471
	step [175/270], loss=95.0492
	step [176/270], loss=69.2501
	step [177/270], loss=80.9436
	step [178/270], loss=73.6952
	step [179/270], loss=92.0294
	step [180/270], loss=90.6490
	step [181/270], loss=85.1555
	step [182/270], loss=84.0765
	step [183/270], loss=73.1276
	step [184/270], loss=82.0613
	step [185/270], loss=74.5311
	step [186/270], loss=66.2266
	step [187/270], loss=80.0758
	step [188/270], loss=92.4939
	step [189/270], loss=80.2420
	step [190/270], loss=91.7634
	step [191/270], loss=90.4830
	step [192/270], loss=69.6627
	step [193/270], loss=70.7301
	step [194/270], loss=74.0275
	step [195/270], loss=83.1895
	step [196/270], loss=92.8111
	step [197/270], loss=103.9979
	step [198/270], loss=93.4992
	step [199/270], loss=85.4954
	step [200/270], loss=75.2517
	step [201/270], loss=76.4070
	step [202/270], loss=87.6037
	step [203/270], loss=82.1693
	step [204/270], loss=82.9229
	step [205/270], loss=75.5048
	step [206/270], loss=89.0295
	step [207/270], loss=98.9693
	step [208/270], loss=73.9771
	step [209/270], loss=72.1075
	step [210/270], loss=93.6176
	step [211/270], loss=85.9596
	step [212/270], loss=80.9355
	step [213/270], loss=74.4619
	step [214/270], loss=78.0848
	step [215/270], loss=65.2670
	step [216/270], loss=82.2537
	step [217/270], loss=97.6646
	step [218/270], loss=96.9725
	step [219/270], loss=77.6805
	step [220/270], loss=76.7008
	step [221/270], loss=73.1668
	step [222/270], loss=85.9357
	step [223/270], loss=72.6387
	step [224/270], loss=89.1260
	step [225/270], loss=69.0119
	step [226/270], loss=89.0009
	step [227/270], loss=65.2561
	step [228/270], loss=73.4429
	step [229/270], loss=95.0689
	step [230/270], loss=86.1738
	step [231/270], loss=79.4573
	step [232/270], loss=77.0824
	step [233/270], loss=76.2419
	step [234/270], loss=84.8203
	step [235/270], loss=75.0313
	step [236/270], loss=61.0730
	step [237/270], loss=74.1380
	step [238/270], loss=83.0691
	step [239/270], loss=86.3612
	step [240/270], loss=80.9896
	step [241/270], loss=84.5600
	step [242/270], loss=68.3101
	step [243/270], loss=92.5920
	step [244/270], loss=75.7464
	step [245/270], loss=91.0626
	step [246/270], loss=97.2480
	step [247/270], loss=82.5936
	step [248/270], loss=95.5818
	step [249/270], loss=81.5355
	step [250/270], loss=99.1187
	step [251/270], loss=92.4277
	step [252/270], loss=69.6604
	step [253/270], loss=76.2829
	step [254/270], loss=90.3322
	step [255/270], loss=81.1831
	step [256/270], loss=106.7139
	step [257/270], loss=110.7490
	step [258/270], loss=73.1982
	step [259/270], loss=94.7513
	step [260/270], loss=71.5824
	step [261/270], loss=82.7968
	step [262/270], loss=94.5053
	step [263/270], loss=91.5751
	step [264/270], loss=91.6643
	step [265/270], loss=91.8062
	step [266/270], loss=94.7361
	step [267/270], loss=76.3805
	step [268/270], loss=86.3262
	step [269/270], loss=85.2383
	step [270/270], loss=11.5577
	Evaluating
	loss=0.0111, precision=0.2516, recall=0.9044, f1=0.3937
Training epoch 61
	step [1/270], loss=81.4029
	step [2/270], loss=87.1860
	step [3/270], loss=64.2712
	step [4/270], loss=81.6826
	step [5/270], loss=76.1448
	step [6/270], loss=90.9406
	step [7/270], loss=83.7804
	step [8/270], loss=88.8474
	step [9/270], loss=72.2466
	step [10/270], loss=80.8553
	step [11/270], loss=71.8627
	step [12/270], loss=80.2686
	step [13/270], loss=91.3340
	step [14/270], loss=78.2936
	step [15/270], loss=82.8582
	step [16/270], loss=65.8659
	step [17/270], loss=79.3393
	step [18/270], loss=93.3726
	step [19/270], loss=74.6301
	step [20/270], loss=70.7444
	step [21/270], loss=93.1162
	step [22/270], loss=93.1986
	step [23/270], loss=95.4771
	step [24/270], loss=74.0977
	step [25/270], loss=100.0287
	step [26/270], loss=60.2942
	step [27/270], loss=93.5776
	step [28/270], loss=94.6509
	step [29/270], loss=83.9742
	step [30/270], loss=83.2287
	step [31/270], loss=94.1643
	step [32/270], loss=88.8816
	step [33/270], loss=76.1376
	step [34/270], loss=87.5809
	step [35/270], loss=104.2772
	step [36/270], loss=79.8257
	step [37/270], loss=76.2742
	step [38/270], loss=71.7810
	step [39/270], loss=77.4958
	step [40/270], loss=71.8180
	step [41/270], loss=74.9366
	step [42/270], loss=71.7665
	step [43/270], loss=92.9773
	step [44/270], loss=82.9118
	step [45/270], loss=91.1733
	step [46/270], loss=85.4726
	step [47/270], loss=106.2526
	step [48/270], loss=93.3343
	step [49/270], loss=83.7737
	step [50/270], loss=85.4523
	step [51/270], loss=103.0249
	step [52/270], loss=90.1979
	step [53/270], loss=78.1699
	step [54/270], loss=95.8304
	step [55/270], loss=89.7411
	step [56/270], loss=70.6358
	step [57/270], loss=83.4708
	step [58/270], loss=95.8507
	step [59/270], loss=86.9842
	step [60/270], loss=90.2066
	step [61/270], loss=67.1303
	step [62/270], loss=90.7164
	step [63/270], loss=90.7860
	step [64/270], loss=74.8016
	step [65/270], loss=89.2518
	step [66/270], loss=96.6009
	step [67/270], loss=88.5808
	step [68/270], loss=82.5966
	step [69/270], loss=82.6831
	step [70/270], loss=63.4946
	step [71/270], loss=93.4381
	step [72/270], loss=100.0792
	step [73/270], loss=76.2806
	step [74/270], loss=81.1068
	step [75/270], loss=77.4175
	step [76/270], loss=75.4880
	step [77/270], loss=70.7588
	step [78/270], loss=68.9100
	step [79/270], loss=72.9741
	step [80/270], loss=85.6495
	step [81/270], loss=69.2076
	step [82/270], loss=98.7831
	step [83/270], loss=70.0209
	step [84/270], loss=106.9265
	step [85/270], loss=81.0662
	step [86/270], loss=79.0982
	step [87/270], loss=74.5430
	step [88/270], loss=75.6504
	step [89/270], loss=87.7007
	step [90/270], loss=82.8863
	step [91/270], loss=71.1669
	step [92/270], loss=74.0600
	step [93/270], loss=78.4962
	step [94/270], loss=83.0985
	step [95/270], loss=80.9072
	step [96/270], loss=87.4786
	step [97/270], loss=74.6753
	step [98/270], loss=78.1515
	step [99/270], loss=102.1123
	step [100/270], loss=75.4868
	step [101/270], loss=76.9692
	step [102/270], loss=79.1353
	step [103/270], loss=86.4607
	step [104/270], loss=93.7161
	step [105/270], loss=100.1926
	step [106/270], loss=75.9737
	step [107/270], loss=78.8941
	step [108/270], loss=93.0634
	step [109/270], loss=87.3025
	step [110/270], loss=74.2632
	step [111/270], loss=99.3217
	step [112/270], loss=82.4072
	step [113/270], loss=93.6176
	step [114/270], loss=71.6205
	step [115/270], loss=81.0890
	step [116/270], loss=80.7855
	step [117/270], loss=73.6488
	step [118/270], loss=67.2428
	step [119/270], loss=84.4425
	step [120/270], loss=83.0018
	step [121/270], loss=91.3203
	step [122/270], loss=69.1703
	step [123/270], loss=102.8820
	step [124/270], loss=102.1860
	step [125/270], loss=80.8421
	step [126/270], loss=80.2247
	step [127/270], loss=97.4397
	step [128/270], loss=85.4359
	step [129/270], loss=83.7099
	step [130/270], loss=84.2288
	step [131/270], loss=80.0662
	step [132/270], loss=86.5335
	step [133/270], loss=98.7745
	step [134/270], loss=95.7340
	step [135/270], loss=93.0951
	step [136/270], loss=76.8467
	step [137/270], loss=75.3444
	step [138/270], loss=76.2850
	step [139/270], loss=78.1660
	step [140/270], loss=86.4673
	step [141/270], loss=75.7296
	step [142/270], loss=71.3128
	step [143/270], loss=87.7158
	step [144/270], loss=86.0894
	step [145/270], loss=85.4154
	step [146/270], loss=74.0003
	step [147/270], loss=97.6116
	step [148/270], loss=89.0046
	step [149/270], loss=76.7120
	step [150/270], loss=73.9712
	step [151/270], loss=83.6948
	step [152/270], loss=87.5380
	step [153/270], loss=93.5321
	step [154/270], loss=64.7270
	step [155/270], loss=84.5568
	step [156/270], loss=82.8792
	step [157/270], loss=91.1408
	step [158/270], loss=86.1484
	step [159/270], loss=83.6503
	step [160/270], loss=69.9651
	step [161/270], loss=83.3695
	step [162/270], loss=88.0738
	step [163/270], loss=83.0447
	step [164/270], loss=83.1988
	step [165/270], loss=72.2724
	step [166/270], loss=70.7114
	step [167/270], loss=90.6564
	step [168/270], loss=71.0233
	step [169/270], loss=80.4568
	step [170/270], loss=89.2016
	step [171/270], loss=87.5814
	step [172/270], loss=67.8567
	step [173/270], loss=78.5987
	step [174/270], loss=80.7025
	step [175/270], loss=61.2436
	step [176/270], loss=87.3401
	step [177/270], loss=86.3582
	step [178/270], loss=96.5805
	step [179/270], loss=84.2332
	step [180/270], loss=80.7112
	step [181/270], loss=81.4595
	step [182/270], loss=84.3405
	step [183/270], loss=70.7853
	step [184/270], loss=88.4326
	step [185/270], loss=89.4182
	step [186/270], loss=83.4089
	step [187/270], loss=81.1151
	step [188/270], loss=73.5355
	step [189/270], loss=87.7457
	step [190/270], loss=88.7531
	step [191/270], loss=74.3524
	step [192/270], loss=82.5925
	step [193/270], loss=83.7164
	step [194/270], loss=87.2013
	step [195/270], loss=71.7060
	step [196/270], loss=76.8635
	step [197/270], loss=82.0029
	step [198/270], loss=65.0271
	step [199/270], loss=79.3877
	step [200/270], loss=100.3532
	step [201/270], loss=77.6247
	step [202/270], loss=109.7460
	step [203/270], loss=77.7143
	step [204/270], loss=79.6030
	step [205/270], loss=86.3350
	step [206/270], loss=73.6767
	step [207/270], loss=80.9648
	step [208/270], loss=75.3886
	step [209/270], loss=77.7861
	step [210/270], loss=78.4330
	step [211/270], loss=69.6236
	step [212/270], loss=79.7112
	step [213/270], loss=86.6320
	step [214/270], loss=77.1425
	step [215/270], loss=78.8998
	step [216/270], loss=96.8698
	step [217/270], loss=89.6434
	step [218/270], loss=73.6203
	step [219/270], loss=89.5368
	step [220/270], loss=78.6599
	step [221/270], loss=79.2557
	step [222/270], loss=84.9492
	step [223/270], loss=76.2726
	step [224/270], loss=79.5503
	step [225/270], loss=76.2182
	step [226/270], loss=87.8260
	step [227/270], loss=80.1673
	step [228/270], loss=70.0655
	step [229/270], loss=76.7637
	step [230/270], loss=87.8624
	step [231/270], loss=87.8148
	step [232/270], loss=93.4904
	step [233/270], loss=77.7556
	step [234/270], loss=83.6554
	step [235/270], loss=92.0892
	step [236/270], loss=93.7568
	step [237/270], loss=96.1921
	step [238/270], loss=62.0202
	step [239/270], loss=69.0773
	step [240/270], loss=85.1975
	step [241/270], loss=77.7006
	step [242/270], loss=75.6386
	step [243/270], loss=86.1989
	step [244/270], loss=93.9586
	step [245/270], loss=75.0088
	step [246/270], loss=69.3376
	step [247/270], loss=93.5807
	step [248/270], loss=86.1264
	step [249/270], loss=67.9194
	step [250/270], loss=82.9711
	step [251/270], loss=79.2663
	step [252/270], loss=64.5650
	step [253/270], loss=91.6636
	step [254/270], loss=81.7318
	step [255/270], loss=75.1558
	step [256/270], loss=69.7702
	step [257/270], loss=87.3688
	step [258/270], loss=83.3065
	step [259/270], loss=86.5442
	step [260/270], loss=79.5885
	step [261/270], loss=68.1244
	step [262/270], loss=85.3987
	step [263/270], loss=68.8165
	step [264/270], loss=79.6606
	step [265/270], loss=83.0055
	step [266/270], loss=78.3369
	step [267/270], loss=66.5140
	step [268/270], loss=83.6330
	step [269/270], loss=85.7316
	step [270/270], loss=11.0392
	Evaluating
	loss=0.0097, precision=0.2770, recall=0.9013, f1=0.4238
Training epoch 62
	step [1/270], loss=81.6622
	step [2/270], loss=83.0082
	step [3/270], loss=58.1890
	step [4/270], loss=97.3407
	step [5/270], loss=89.7619
	step [6/270], loss=68.3011
	step [7/270], loss=76.3126
	step [8/270], loss=76.0679
	step [9/270], loss=87.0225
	step [10/270], loss=74.0052
	step [11/270], loss=85.5099
	step [12/270], loss=64.9516
	step [13/270], loss=76.9664
	step [14/270], loss=88.5786
	step [15/270], loss=67.7288
	step [16/270], loss=75.2863
	step [17/270], loss=73.7543
	step [18/270], loss=65.2763
	step [19/270], loss=59.2251
	step [20/270], loss=74.2690
	step [21/270], loss=83.9586
	step [22/270], loss=81.6003
	step [23/270], loss=71.9908
	step [24/270], loss=79.8619
	step [25/270], loss=75.2476
	step [26/270], loss=86.6070
	step [27/270], loss=73.6494
	step [28/270], loss=78.3807
	step [29/270], loss=73.0087
	step [30/270], loss=91.3142
	step [31/270], loss=81.5498
	step [32/270], loss=99.6338
	step [33/270], loss=84.6794
	step [34/270], loss=82.3083
	step [35/270], loss=91.4459
	step [36/270], loss=83.0296
	step [37/270], loss=59.4098
	step [38/270], loss=92.7046
	step [39/270], loss=82.3218
	step [40/270], loss=79.0539
	step [41/270], loss=76.0143
	step [42/270], loss=74.6925
	step [43/270], loss=86.4342
	step [44/270], loss=82.6831
	step [45/270], loss=73.0329
	step [46/270], loss=75.7688
	step [47/270], loss=84.7957
	step [48/270], loss=81.8296
	step [49/270], loss=79.4320
	step [50/270], loss=71.5095
	step [51/270], loss=82.6127
	step [52/270], loss=88.6474
	step [53/270], loss=68.0249
	step [54/270], loss=80.9222
	step [55/270], loss=75.7803
	step [56/270], loss=83.3229
	step [57/270], loss=98.6156
	step [58/270], loss=96.0457
	step [59/270], loss=75.5750
	step [60/270], loss=75.0909
	step [61/270], loss=80.1553
	step [62/270], loss=99.6717
	step [63/270], loss=81.6119
	step [64/270], loss=87.8055
	step [65/270], loss=87.2788
	step [66/270], loss=96.0650
	step [67/270], loss=77.0255
	step [68/270], loss=72.8868
	step [69/270], loss=83.3109
	step [70/270], loss=83.1180
	step [71/270], loss=96.0052
	step [72/270], loss=67.1366
	step [73/270], loss=91.7104
	step [74/270], loss=85.4809
	step [75/270], loss=105.2914
	step [76/270], loss=91.4457
	step [77/270], loss=97.2969
	step [78/270], loss=88.4585
	step [79/270], loss=93.5220
	step [80/270], loss=87.8865
	step [81/270], loss=77.3478
	step [82/270], loss=96.2841
	step [83/270], loss=76.8209
	step [84/270], loss=83.4269
	step [85/270], loss=76.2614
	step [86/270], loss=77.9685
	step [87/270], loss=76.8729
	step [88/270], loss=69.9728
	step [89/270], loss=65.7921
	step [90/270], loss=96.9200
	step [91/270], loss=87.5821
	step [92/270], loss=83.9537
	step [93/270], loss=89.9708
	step [94/270], loss=77.5752
	step [95/270], loss=90.0089
	step [96/270], loss=82.7474
	step [97/270], loss=81.4814
	step [98/270], loss=72.8031
	step [99/270], loss=81.0370
	step [100/270], loss=74.3541
	step [101/270], loss=77.6362
	step [102/270], loss=81.2240
	step [103/270], loss=97.2836
	step [104/270], loss=68.3910
	step [105/270], loss=76.5891
	step [106/270], loss=90.2458
	step [107/270], loss=81.8245
	step [108/270], loss=71.9149
	step [109/270], loss=81.7987
	step [110/270], loss=81.6881
	step [111/270], loss=83.5286
	step [112/270], loss=78.7933
	step [113/270], loss=66.6859
	step [114/270], loss=82.7784
	step [115/270], loss=67.0524
	step [116/270], loss=98.8617
	step [117/270], loss=94.2929
	step [118/270], loss=75.5070
	step [119/270], loss=81.4846
	step [120/270], loss=76.1883
	step [121/270], loss=74.0430
	step [122/270], loss=71.5207
	step [123/270], loss=69.7303
	step [124/270], loss=70.0630
	step [125/270], loss=63.0071
	step [126/270], loss=96.5025
	step [127/270], loss=81.7207
	step [128/270], loss=90.4398
	step [129/270], loss=101.0367
	step [130/270], loss=76.6630
	step [131/270], loss=82.8021
	step [132/270], loss=87.4303
	step [133/270], loss=88.0022
	step [134/270], loss=90.7533
	step [135/270], loss=89.6872
	step [136/270], loss=90.0621
	step [137/270], loss=75.0152
	step [138/270], loss=88.8142
	step [139/270], loss=78.4787
	step [140/270], loss=77.4029
	step [141/270], loss=93.7671
	step [142/270], loss=76.9274
	step [143/270], loss=65.2049
	step [144/270], loss=85.3803
	step [145/270], loss=68.9462
	step [146/270], loss=89.9724
	step [147/270], loss=78.9092
	step [148/270], loss=70.4834
	step [149/270], loss=71.6397
	step [150/270], loss=69.5417
	step [151/270], loss=77.6148
	step [152/270], loss=73.1120
	step [153/270], loss=73.0624
	step [154/270], loss=93.2868
	step [155/270], loss=86.4161
	step [156/270], loss=68.5825
	step [157/270], loss=66.7063
	step [158/270], loss=71.3907
	step [159/270], loss=87.7888
	step [160/270], loss=70.3445
	step [161/270], loss=83.6537
	step [162/270], loss=56.9851
	step [163/270], loss=106.2592
	step [164/270], loss=95.7458
	step [165/270], loss=91.1893
	step [166/270], loss=76.8314
	step [167/270], loss=96.6240
	step [168/270], loss=77.4961
	step [169/270], loss=82.9391
	step [170/270], loss=62.6224
	step [171/270], loss=85.7977
	step [172/270], loss=90.7843
	step [173/270], loss=81.3649
	step [174/270], loss=101.8055
	step [175/270], loss=84.5523
	step [176/270], loss=69.9681
	step [177/270], loss=85.0472
	step [178/270], loss=68.7498
	step [179/270], loss=108.5343
	step [180/270], loss=86.3424
	step [181/270], loss=75.5625
	step [182/270], loss=88.4200
	step [183/270], loss=95.5509
	step [184/270], loss=87.8025
	step [185/270], loss=90.7572
	step [186/270], loss=69.6591
	step [187/270], loss=77.6721
	step [188/270], loss=92.5802
	step [189/270], loss=93.1444
	step [190/270], loss=87.5827
	step [191/270], loss=81.2864
	step [192/270], loss=85.1467
	step [193/270], loss=78.4751
	step [194/270], loss=96.7002
	step [195/270], loss=83.3846
	step [196/270], loss=87.9644
	step [197/270], loss=79.8226
	step [198/270], loss=90.1335
	step [199/270], loss=79.6350
	step [200/270], loss=75.9557
	step [201/270], loss=97.6496
	step [202/270], loss=90.3486
	step [203/270], loss=79.1291
	step [204/270], loss=80.4719
	step [205/270], loss=92.0527
	step [206/270], loss=79.5966
	step [207/270], loss=87.4211
	step [208/270], loss=88.2218
	step [209/270], loss=89.3445
	step [210/270], loss=86.7463
	step [211/270], loss=74.8473
	step [212/270], loss=69.5111
	step [213/270], loss=99.9194
	step [214/270], loss=96.5576
	step [215/270], loss=72.5128
	step [216/270], loss=90.9419
	step [217/270], loss=65.6766
	step [218/270], loss=80.0506
	step [219/270], loss=93.4258
	step [220/270], loss=79.9933
	step [221/270], loss=83.3032
	step [222/270], loss=87.2023
	step [223/270], loss=74.3864
	step [224/270], loss=86.9484
	step [225/270], loss=64.9809
	step [226/270], loss=75.6834
	step [227/270], loss=73.0160
	step [228/270], loss=96.4497
	step [229/270], loss=90.2717
	step [230/270], loss=90.7896
	step [231/270], loss=82.2743
	step [232/270], loss=87.5548
	step [233/270], loss=78.3314
	step [234/270], loss=80.0009
	step [235/270], loss=84.0427
	step [236/270], loss=85.8980
	step [237/270], loss=78.9814
	step [238/270], loss=75.2257
	step [239/270], loss=76.6535
	step [240/270], loss=98.6758
	step [241/270], loss=85.3836
	step [242/270], loss=73.8202
	step [243/270], loss=74.0204
	step [244/270], loss=94.2068
	step [245/270], loss=69.5089
	step [246/270], loss=81.0988
	step [247/270], loss=95.1577
	step [248/270], loss=90.7216
	step [249/270], loss=73.8779
	step [250/270], loss=86.5862
	step [251/270], loss=77.1763
	step [252/270], loss=87.1582
	step [253/270], loss=64.0574
	step [254/270], loss=88.8154
	step [255/270], loss=84.2927
	step [256/270], loss=85.9295
	step [257/270], loss=84.0827
	step [258/270], loss=83.0746
	step [259/270], loss=80.0222
	step [260/270], loss=90.5680
	step [261/270], loss=77.9209
	step [262/270], loss=83.7353
	step [263/270], loss=91.6278
	step [264/270], loss=99.6455
	step [265/270], loss=74.1943
	step [266/270], loss=90.2299
	step [267/270], loss=85.1888
	step [268/270], loss=81.2755
	step [269/270], loss=88.8779
	step [270/270], loss=11.3925
	Evaluating
	loss=0.0089, precision=0.3071, recall=0.9065, f1=0.4588
Training epoch 63
	step [1/270], loss=72.5101
	step [2/270], loss=80.5898
	step [3/270], loss=90.0504
	step [4/270], loss=87.4701
	step [5/270], loss=87.6801
	step [6/270], loss=74.5994
	step [7/270], loss=88.0807
	step [8/270], loss=84.2761
	step [9/270], loss=84.0020
	step [10/270], loss=78.8340
	step [11/270], loss=76.4693
	step [12/270], loss=96.3978
	step [13/270], loss=74.0528
	step [14/270], loss=77.0480
	step [15/270], loss=97.5042
	step [16/270], loss=70.3829
	step [17/270], loss=75.9546
	step [18/270], loss=78.7085
	step [19/270], loss=92.3822
	step [20/270], loss=70.2888
	step [21/270], loss=68.4479
	step [22/270], loss=76.4441
	step [23/270], loss=64.3891
	step [24/270], loss=77.0135
	step [25/270], loss=67.3840
	step [26/270], loss=73.7287
	step [27/270], loss=90.9891
	step [28/270], loss=94.4730
	step [29/270], loss=86.4612
	step [30/270], loss=72.0047
	step [31/270], loss=81.3575
	step [32/270], loss=81.2194
	step [33/270], loss=101.9886
	step [34/270], loss=73.6543
	step [35/270], loss=75.3814
	step [36/270], loss=87.5485
	step [37/270], loss=74.5006
	step [38/270], loss=95.6300
	step [39/270], loss=73.0514
	step [40/270], loss=86.7338
	step [41/270], loss=74.1456
	step [42/270], loss=79.2142
	step [43/270], loss=105.3537
	step [44/270], loss=83.8618
	step [45/270], loss=78.4410
	step [46/270], loss=77.4578
	step [47/270], loss=87.4905
	step [48/270], loss=77.9023
	step [49/270], loss=79.4149
	step [50/270], loss=82.8282
	step [51/270], loss=68.8115
	step [52/270], loss=100.2927
	step [53/270], loss=69.6149
	step [54/270], loss=81.0693
	step [55/270], loss=88.1276
	step [56/270], loss=105.8406
	step [57/270], loss=80.5692
	step [58/270], loss=89.5773
	step [59/270], loss=80.6922
	step [60/270], loss=68.2357
	step [61/270], loss=78.4759
	step [62/270], loss=69.9733
	step [63/270], loss=108.4583
	step [64/270], loss=83.9098
	step [65/270], loss=85.5001
	step [66/270], loss=75.5349
	step [67/270], loss=57.4075
	step [68/270], loss=61.2542
	step [69/270], loss=87.4265
	step [70/270], loss=65.6800
	step [71/270], loss=70.9138
	step [72/270], loss=84.9935
	step [73/270], loss=81.8633
	step [74/270], loss=81.4392
	step [75/270], loss=74.6959
	step [76/270], loss=102.3010
	step [77/270], loss=83.6381
	step [78/270], loss=75.3448
	step [79/270], loss=96.1755
	step [80/270], loss=78.8142
	step [81/270], loss=90.9740
	step [82/270], loss=72.9688
	step [83/270], loss=65.6328
	step [84/270], loss=67.2477
	step [85/270], loss=82.6726
	step [86/270], loss=102.1266
	step [87/270], loss=78.0509
	step [88/270], loss=88.9179
	step [89/270], loss=70.5905
	step [90/270], loss=87.5233
	step [91/270], loss=80.2464
	step [92/270], loss=85.8891
	step [93/270], loss=68.2064
	step [94/270], loss=93.1629
	step [95/270], loss=82.3335
	step [96/270], loss=85.7187
	step [97/270], loss=76.2785
	step [98/270], loss=69.9921
	step [99/270], loss=86.7386
	step [100/270], loss=73.1789
	step [101/270], loss=78.3730
	step [102/270], loss=84.4902
	step [103/270], loss=103.1859
	step [104/270], loss=85.9637
	step [105/270], loss=80.8387
	step [106/270], loss=78.7400
	step [107/270], loss=98.2069
	step [108/270], loss=84.4067
	step [109/270], loss=86.7970
	step [110/270], loss=71.1869
	step [111/270], loss=75.7843
	step [112/270], loss=85.8441
	step [113/270], loss=79.5474
	step [114/270], loss=74.0649
	step [115/270], loss=64.9677
	step [116/270], loss=69.6955
	step [117/270], loss=75.9407
	step [118/270], loss=106.9500
	step [119/270], loss=73.5721
	step [120/270], loss=65.6104
	step [121/270], loss=69.7175
	step [122/270], loss=74.7169
	step [123/270], loss=80.3081
	step [124/270], loss=71.6815
	step [125/270], loss=85.3542
	step [126/270], loss=63.4488
	step [127/270], loss=95.8982
	step [128/270], loss=89.3248
	step [129/270], loss=80.1773
	step [130/270], loss=102.3406
	step [131/270], loss=70.5564
	step [132/270], loss=92.8247
	step [133/270], loss=68.5142
	step [134/270], loss=79.2027
	step [135/270], loss=85.8486
	step [136/270], loss=76.9056
	step [137/270], loss=53.0291
	step [138/270], loss=82.9688
	step [139/270], loss=94.1726
	step [140/270], loss=81.5550
	step [141/270], loss=76.8136
	step [142/270], loss=80.8005
	step [143/270], loss=90.3680
	step [144/270], loss=80.3409
	step [145/270], loss=88.5825
	step [146/270], loss=89.3207
	step [147/270], loss=81.5322
	step [148/270], loss=67.0646
	step [149/270], loss=97.8270
	step [150/270], loss=90.2671
	step [151/270], loss=75.8897
	step [152/270], loss=86.2265
	step [153/270], loss=90.5424
	step [154/270], loss=100.8862
	step [155/270], loss=78.3469
	step [156/270], loss=108.8107
	step [157/270], loss=77.8133
	step [158/270], loss=74.6858
	step [159/270], loss=67.2906
	step [160/270], loss=86.7766
	step [161/270], loss=89.1255
	step [162/270], loss=85.9153
	step [163/270], loss=88.5954
	step [164/270], loss=90.8795
	step [165/270], loss=91.7850
	step [166/270], loss=69.3410
	step [167/270], loss=69.6567
	step [168/270], loss=69.8659
	step [169/270], loss=75.8134
	step [170/270], loss=91.3229
	step [171/270], loss=91.4359
	step [172/270], loss=81.6486
	step [173/270], loss=87.8022
	step [174/270], loss=71.6351
	step [175/270], loss=80.3862
	step [176/270], loss=105.9366
	step [177/270], loss=80.6109
	step [178/270], loss=91.1891
	step [179/270], loss=72.1249
	step [180/270], loss=68.7897
	step [181/270], loss=79.1691
	step [182/270], loss=85.6979
	step [183/270], loss=89.3003
	step [184/270], loss=65.6949
	step [185/270], loss=62.9698
	step [186/270], loss=78.1435
	step [187/270], loss=94.0793
	step [188/270], loss=83.8848
	step [189/270], loss=76.2405
	step [190/270], loss=97.8785
	step [191/270], loss=82.9437
	step [192/270], loss=77.4530
	step [193/270], loss=76.3781
	step [194/270], loss=75.5889
	step [195/270], loss=82.1413
	step [196/270], loss=86.0361
	step [197/270], loss=76.0688
	step [198/270], loss=97.9677
	step [199/270], loss=79.6889
	step [200/270], loss=94.3552
	step [201/270], loss=94.2453
	step [202/270], loss=84.1358
	step [203/270], loss=88.0060
	step [204/270], loss=87.6326
	step [205/270], loss=87.6719
	step [206/270], loss=58.1639
	step [207/270], loss=70.8667
	step [208/270], loss=87.9589
	step [209/270], loss=78.9984
	step [210/270], loss=67.6573
	step [211/270], loss=76.4026
	step [212/270], loss=88.9904
	step [213/270], loss=91.8258
	step [214/270], loss=81.3619
	step [215/270], loss=84.2295
	step [216/270], loss=96.4914
	step [217/270], loss=68.2688
	step [218/270], loss=94.0966
	step [219/270], loss=79.1705
	step [220/270], loss=76.2873
	step [221/270], loss=92.3027
	step [222/270], loss=89.3772
	step [223/270], loss=83.9171
	step [224/270], loss=81.6026
	step [225/270], loss=81.3967
	step [226/270], loss=91.7819
	step [227/270], loss=90.6308
	step [228/270], loss=87.3136
	step [229/270], loss=88.7837
	step [230/270], loss=72.2276
	step [231/270], loss=82.8164
	step [232/270], loss=75.2452
	step [233/270], loss=92.0112
	step [234/270], loss=89.6211
	step [235/270], loss=85.6903
	step [236/270], loss=81.0183
	step [237/270], loss=69.3105
	step [238/270], loss=104.9412
	step [239/270], loss=69.9954
	step [240/270], loss=78.6982
	step [241/270], loss=75.7708
	step [242/270], loss=91.2918
	step [243/270], loss=71.6583
	step [244/270], loss=79.5867
	step [245/270], loss=88.8285
	step [246/270], loss=73.6534
	step [247/270], loss=81.0147
	step [248/270], loss=70.4047
	step [249/270], loss=81.6750
	step [250/270], loss=89.3284
	step [251/270], loss=79.0122
	step [252/270], loss=82.1759
	step [253/270], loss=78.0587
	step [254/270], loss=71.5997
	step [255/270], loss=92.6847
	step [256/270], loss=108.4255
	step [257/270], loss=93.2858
	step [258/270], loss=73.0445
	step [259/270], loss=82.1546
	step [260/270], loss=69.0027
	step [261/270], loss=76.4086
	step [262/270], loss=87.0894
	step [263/270], loss=89.0666
	step [264/270], loss=72.4967
	step [265/270], loss=62.8156
	step [266/270], loss=87.7599
	step [267/270], loss=90.3679
	step [268/270], loss=78.4356
	step [269/270], loss=66.2468
	step [270/270], loss=12.6351
	Evaluating
	loss=0.0080, precision=0.3318, recall=0.9022, f1=0.4852
Training epoch 64
	step [1/270], loss=80.2672
	step [2/270], loss=84.2809
	step [3/270], loss=62.1529
	step [4/270], loss=81.9306
	step [5/270], loss=98.0412
	step [6/270], loss=62.5401
	step [7/270], loss=90.0861
	step [8/270], loss=68.9475
	step [9/270], loss=80.4244
	step [10/270], loss=82.1458
	step [11/270], loss=79.5961
	step [12/270], loss=101.3052
	step [13/270], loss=95.8092
	step [14/270], loss=87.6241
	step [15/270], loss=76.8508
	step [16/270], loss=85.9008
	step [17/270], loss=81.8982
	step [18/270], loss=76.7859
	step [19/270], loss=78.0707
	step [20/270], loss=84.4691
	step [21/270], loss=74.4019
	step [22/270], loss=76.1413
	step [23/270], loss=66.3354
	step [24/270], loss=79.7175
	step [25/270], loss=78.5978
	step [26/270], loss=82.8485
	step [27/270], loss=87.9714
	step [28/270], loss=81.2254
	step [29/270], loss=81.1613
	step [30/270], loss=84.7965
	step [31/270], loss=96.8827
	step [32/270], loss=70.6443
	step [33/270], loss=105.0855
	step [34/270], loss=85.0732
	step [35/270], loss=75.0073
	step [36/270], loss=80.0526
	step [37/270], loss=74.2326
	step [38/270], loss=95.4184
	step [39/270], loss=81.9070
	step [40/270], loss=75.5260
	step [41/270], loss=74.8053
	step [42/270], loss=65.3596
	step [43/270], loss=87.9664
	step [44/270], loss=65.8383
	step [45/270], loss=74.0312
	step [46/270], loss=76.8112
	step [47/270], loss=79.4618
	step [48/270], loss=75.2673
	step [49/270], loss=79.3229
	step [50/270], loss=89.0623
	step [51/270], loss=84.4977
	step [52/270], loss=124.9510
	step [53/270], loss=82.9034
	step [54/270], loss=77.0364
	step [55/270], loss=88.9585
	step [56/270], loss=79.1051
	step [57/270], loss=89.0672
	step [58/270], loss=74.9562
	step [59/270], loss=99.7394
	step [60/270], loss=91.2387
	step [61/270], loss=81.6325
	step [62/270], loss=90.9240
	step [63/270], loss=64.9913
	step [64/270], loss=84.6825
	step [65/270], loss=78.8402
	step [66/270], loss=72.3257
	step [67/270], loss=81.8860
	step [68/270], loss=74.5893
	step [69/270], loss=74.0200
	step [70/270], loss=89.9071
	step [71/270], loss=85.2844
	step [72/270], loss=69.4842
	step [73/270], loss=81.4467
	step [74/270], loss=88.1229
	step [75/270], loss=75.4002
	step [76/270], loss=55.2526
	step [77/270], loss=85.3684
	step [78/270], loss=80.2920
	step [79/270], loss=78.3753
	step [80/270], loss=85.7014
	step [81/270], loss=92.2244
	step [82/270], loss=72.3902
	step [83/270], loss=70.2163
	step [84/270], loss=76.0628
	step [85/270], loss=78.3274
	step [86/270], loss=76.9886
	step [87/270], loss=72.4736
	step [88/270], loss=61.1362
	step [89/270], loss=105.4091
	step [90/270], loss=90.5207
	step [91/270], loss=79.9648
	step [92/270], loss=75.4800
	step [93/270], loss=71.2820
	step [94/270], loss=79.7536
	step [95/270], loss=82.9332
	step [96/270], loss=88.8146
	step [97/270], loss=79.2115
	step [98/270], loss=90.2797
	step [99/270], loss=79.3970
	step [100/270], loss=98.0648
	step [101/270], loss=79.8929
	step [102/270], loss=80.2715
	step [103/270], loss=106.1674
	step [104/270], loss=80.7887
	step [105/270], loss=86.9820
	step [106/270], loss=84.7617
	step [107/270], loss=73.2887
	step [108/270], loss=94.4706
	step [109/270], loss=72.7816
	step [110/270], loss=72.4348
	step [111/270], loss=87.5068
	step [112/270], loss=88.5063
	step [113/270], loss=84.5292
	step [114/270], loss=82.7384
	step [115/270], loss=76.2064
	step [116/270], loss=93.2328
	step [117/270], loss=80.2948
	step [118/270], loss=74.5274
	step [119/270], loss=70.4921
	step [120/270], loss=69.9574
	step [121/270], loss=75.9917
	step [122/270], loss=77.4710
	step [123/270], loss=77.8073
	step [124/270], loss=86.2006
	step [125/270], loss=72.0850
	step [126/270], loss=84.7042
	step [127/270], loss=95.5222
	step [128/270], loss=77.8609
	step [129/270], loss=87.1283
	step [130/270], loss=103.6604
	step [131/270], loss=89.8186
	step [132/270], loss=69.8171
	step [133/270], loss=83.1904
	step [134/270], loss=67.3320
	step [135/270], loss=88.3887
	step [136/270], loss=78.2834
	step [137/270], loss=68.4917
	step [138/270], loss=74.1754
	step [139/270], loss=87.9470
	step [140/270], loss=92.9676
	step [141/270], loss=82.1123
	step [142/270], loss=76.2012
	step [143/270], loss=75.9569
	step [144/270], loss=88.4371
	step [145/270], loss=92.1119
	step [146/270], loss=82.7288
	step [147/270], loss=63.8544
	step [148/270], loss=60.4936
	step [149/270], loss=90.2338
	step [150/270], loss=74.5300
	step [151/270], loss=95.7555
	step [152/270], loss=76.5038
	step [153/270], loss=87.6219
	step [154/270], loss=69.0212
	step [155/270], loss=77.0348
	step [156/270], loss=85.5497
	step [157/270], loss=84.1111
	step [158/270], loss=75.6038
	step [159/270], loss=84.1894
	step [160/270], loss=71.1638
	step [161/270], loss=79.3919
	step [162/270], loss=89.1971
	step [163/270], loss=72.7274
	step [164/270], loss=85.7883
	step [165/270], loss=83.8260
	step [166/270], loss=71.2596
	step [167/270], loss=78.6050
	step [168/270], loss=68.5386
	step [169/270], loss=85.3223
	step [170/270], loss=82.1049
	step [171/270], loss=78.5566
	step [172/270], loss=77.1396
	step [173/270], loss=81.1891
	step [174/270], loss=74.0588
	step [175/270], loss=75.4498
	step [176/270], loss=90.5553
	step [177/270], loss=75.6510
	step [178/270], loss=84.7150
	step [179/270], loss=87.6637
	step [180/270], loss=88.6080
	step [181/270], loss=80.9176
	step [182/270], loss=93.9722
	step [183/270], loss=81.1891
	step [184/270], loss=91.5021
	step [185/270], loss=62.8102
	step [186/270], loss=79.9349
	step [187/270], loss=91.0459
	step [188/270], loss=80.1809
	step [189/270], loss=94.3128
	step [190/270], loss=91.9755
	step [191/270], loss=76.5352
	step [192/270], loss=76.5798
	step [193/270], loss=75.2836
	step [194/270], loss=93.2545
	step [195/270], loss=89.8809
	step [196/270], loss=73.7703
	step [197/270], loss=78.0088
	step [198/270], loss=84.9662
	step [199/270], loss=82.9438
	step [200/270], loss=83.0038
	step [201/270], loss=80.4759
	step [202/270], loss=92.6614
	step [203/270], loss=75.9090
	step [204/270], loss=80.8155
	step [205/270], loss=93.6073
	step [206/270], loss=85.3042
	step [207/270], loss=77.1274
	step [208/270], loss=78.8934
	step [209/270], loss=65.7017
	step [210/270], loss=74.3654
	step [211/270], loss=76.1635
	step [212/270], loss=85.4368
	step [213/270], loss=90.9186
	step [214/270], loss=73.2187
	step [215/270], loss=81.3953
	step [216/270], loss=67.0140
	step [217/270], loss=87.5915
	step [218/270], loss=69.9830
	step [219/270], loss=96.9417
	step [220/270], loss=92.4839
	step [221/270], loss=95.5291
	step [222/270], loss=89.7638
	step [223/270], loss=106.4533
	step [224/270], loss=69.8296
	step [225/270], loss=80.9429
	step [226/270], loss=87.6969
	step [227/270], loss=83.8879
	step [228/270], loss=72.6425
	step [229/270], loss=75.0295
	step [230/270], loss=80.6573
	step [231/270], loss=97.6025
	step [232/270], loss=84.1765
	step [233/270], loss=77.1849
	step [234/270], loss=91.7759
	step [235/270], loss=98.8050
	step [236/270], loss=78.3774
	step [237/270], loss=82.9015
	step [238/270], loss=87.2307
	step [239/270], loss=85.8015
	step [240/270], loss=80.3525
	step [241/270], loss=78.4519
	step [242/270], loss=61.2616
	step [243/270], loss=79.2131
	step [244/270], loss=87.7589
	step [245/270], loss=86.3122
	step [246/270], loss=77.6709
	step [247/270], loss=96.1163
	step [248/270], loss=89.4997
	step [249/270], loss=76.7018
	step [250/270], loss=83.8467
	step [251/270], loss=72.5456
	step [252/270], loss=81.8269
	step [253/270], loss=87.5310
	step [254/270], loss=83.5193
	step [255/270], loss=71.7319
	step [256/270], loss=85.4732
	step [257/270], loss=80.8558
	step [258/270], loss=93.4451
	step [259/270], loss=74.2224
	step [260/270], loss=79.8090
	step [261/270], loss=76.6342
	step [262/270], loss=76.7966
	step [263/270], loss=62.5746
	step [264/270], loss=71.7569
	step [265/270], loss=92.8446
	step [266/270], loss=77.7474
	step [267/270], loss=85.9907
	step [268/270], loss=86.1404
	step [269/270], loss=62.7686
	step [270/270], loss=12.1798
	Evaluating
	loss=0.0082, precision=0.3219, recall=0.9005, f1=0.4743
Training epoch 65
	step [1/270], loss=78.6717
	step [2/270], loss=85.1421
	step [3/270], loss=75.4211
	step [4/270], loss=89.3632
	step [5/270], loss=64.7033
	step [6/270], loss=93.4606
	step [7/270], loss=76.6575
	step [8/270], loss=76.4725
	step [9/270], loss=85.0539
	step [10/270], loss=91.6986
	step [11/270], loss=65.7955
	step [12/270], loss=70.5397
	step [13/270], loss=84.5411
	step [14/270], loss=73.6839
	step [15/270], loss=89.1071
	step [16/270], loss=77.0266
	step [17/270], loss=77.4479
	step [18/270], loss=98.1843
	step [19/270], loss=86.6561
	step [20/270], loss=89.1750
	step [21/270], loss=65.1390
	step [22/270], loss=75.7596
	step [23/270], loss=88.2118
	step [24/270], loss=87.8558
	step [25/270], loss=75.3909
	step [26/270], loss=89.4683
	step [27/270], loss=90.8883
	step [28/270], loss=81.5295
	step [29/270], loss=99.3810
	step [30/270], loss=67.0903
	step [31/270], loss=112.0915
	step [32/270], loss=89.3326
	step [33/270], loss=81.2543
	step [34/270], loss=95.4474
	step [35/270], loss=80.6401
	step [36/270], loss=72.7738
	step [37/270], loss=93.0010
	step [38/270], loss=82.1172
	step [39/270], loss=89.0487
	step [40/270], loss=62.0450
	step [41/270], loss=83.6561
	step [42/270], loss=80.1493
	step [43/270], loss=79.4768
	step [44/270], loss=88.7811
	step [45/270], loss=88.1910
	step [46/270], loss=75.3501
	step [47/270], loss=84.5821
	step [48/270], loss=75.9936
	step [49/270], loss=77.0382
	step [50/270], loss=86.7876
	step [51/270], loss=103.0063
	step [52/270], loss=70.8573
	step [53/270], loss=85.9455
	step [54/270], loss=72.2203
	step [55/270], loss=77.3559
	step [56/270], loss=80.7448
	step [57/270], loss=92.2729
	step [58/270], loss=86.7806
	step [59/270], loss=70.8267
	step [60/270], loss=76.8960
	step [61/270], loss=91.2621
	step [62/270], loss=85.4664
	step [63/270], loss=84.8751
	step [64/270], loss=86.2995
	step [65/270], loss=92.4987
	step [66/270], loss=85.9110
	step [67/270], loss=89.7916
	step [68/270], loss=58.3576
	step [69/270], loss=74.2090
	step [70/270], loss=84.1533
	step [71/270], loss=64.2865
	step [72/270], loss=74.0229
	step [73/270], loss=76.5541
	step [74/270], loss=88.8505
	step [75/270], loss=76.5690
	step [76/270], loss=89.2190
	step [77/270], loss=72.1692
	step [78/270], loss=69.5080
	step [79/270], loss=83.9192
	step [80/270], loss=81.0680
	step [81/270], loss=83.6280
	step [82/270], loss=80.9238
	step [83/270], loss=82.5054
	step [84/270], loss=74.0840
	step [85/270], loss=80.4482
	step [86/270], loss=70.3981
	step [87/270], loss=92.8126
	step [88/270], loss=100.8957
	step [89/270], loss=72.9386
	step [90/270], loss=86.6884
	step [91/270], loss=72.4622
	step [92/270], loss=88.3706
	step [93/270], loss=81.3481
	step [94/270], loss=76.4570
	step [95/270], loss=73.8043
	step [96/270], loss=74.7919
	step [97/270], loss=81.7624
	step [98/270], loss=96.2175
	step [99/270], loss=89.5345
	step [100/270], loss=83.6770
	step [101/270], loss=79.1526
	step [102/270], loss=84.3365
	step [103/270], loss=92.5763
	step [104/270], loss=87.2685
	step [105/270], loss=79.1792
	step [106/270], loss=78.9437
	step [107/270], loss=84.4220
	step [108/270], loss=81.0157
	step [109/270], loss=70.0683
	step [110/270], loss=80.5349
	step [111/270], loss=76.0270
	step [112/270], loss=86.5110
	step [113/270], loss=76.7952
	step [114/270], loss=85.6471
	step [115/270], loss=69.5301
	step [116/270], loss=73.4217
	step [117/270], loss=86.9573
	step [118/270], loss=93.3289
	step [119/270], loss=73.8970
	step [120/270], loss=79.6972
	step [121/270], loss=74.3636
	step [122/270], loss=74.7472
	step [123/270], loss=72.7903
	step [124/270], loss=87.6735
	step [125/270], loss=79.8603
	step [126/270], loss=77.4801
	step [127/270], loss=77.3441
	step [128/270], loss=79.3988
	step [129/270], loss=75.7685
	step [130/270], loss=81.5749
	step [131/270], loss=73.0102
	step [132/270], loss=89.2815
	step [133/270], loss=69.6492
	step [134/270], loss=100.5649
	step [135/270], loss=68.9888
	step [136/270], loss=81.8131
	step [137/270], loss=67.8930
	step [138/270], loss=77.6217
	step [139/270], loss=84.2113
	step [140/270], loss=100.7824
	step [141/270], loss=86.0450
	step [142/270], loss=74.1590
	step [143/270], loss=85.6390
	step [144/270], loss=85.0710
	step [145/270], loss=64.0484
	step [146/270], loss=67.2909
	step [147/270], loss=83.3079
	step [148/270], loss=85.2897
	step [149/270], loss=80.0504
	step [150/270], loss=78.8581
	step [151/270], loss=88.7524
	step [152/270], loss=92.5841
	step [153/270], loss=87.1739
	step [154/270], loss=74.9329
	step [155/270], loss=74.8323
	step [156/270], loss=95.4458
	step [157/270], loss=82.5127
	step [158/270], loss=77.4369
	step [159/270], loss=78.3232
	step [160/270], loss=79.8125
	step [161/270], loss=80.3115
	step [162/270], loss=86.3882
	step [163/270], loss=80.2514
	step [164/270], loss=83.2035
	step [165/270], loss=78.7000
	step [166/270], loss=74.1622
	step [167/270], loss=89.6727
	step [168/270], loss=98.4094
	step [169/270], loss=80.8317
	step [170/270], loss=82.3307
	step [171/270], loss=87.8037
	step [172/270], loss=80.5533
	step [173/270], loss=80.3986
	step [174/270], loss=94.8374
	step [175/270], loss=80.8281
	step [176/270], loss=70.8075
	step [177/270], loss=76.0451
	step [178/270], loss=76.6129
	step [179/270], loss=80.6728
	step [180/270], loss=94.7158
	step [181/270], loss=79.5105
	step [182/270], loss=72.8849
	step [183/270], loss=76.3995
	step [184/270], loss=85.9954
	step [185/270], loss=68.1177
	step [186/270], loss=76.0581
	step [187/270], loss=85.7342
	step [188/270], loss=87.6117
	step [189/270], loss=70.6890
	step [190/270], loss=81.1594
	step [191/270], loss=79.0809
	step [192/270], loss=92.7841
	step [193/270], loss=75.7012
	step [194/270], loss=71.7271
	step [195/270], loss=79.5389
	step [196/270], loss=105.5205
	step [197/270], loss=88.8647
	step [198/270], loss=88.8956
	step [199/270], loss=81.8886
	step [200/270], loss=84.6717
	step [201/270], loss=83.9027
	step [202/270], loss=83.3326
	step [203/270], loss=98.2913
	step [204/270], loss=85.4928
	step [205/270], loss=75.6658
	step [206/270], loss=99.9708
	step [207/270], loss=93.7241
	step [208/270], loss=87.0778
	step [209/270], loss=101.2719
	step [210/270], loss=78.2549
	step [211/270], loss=67.7103
	step [212/270], loss=83.8710
	step [213/270], loss=90.3756
	step [214/270], loss=81.8695
	step [215/270], loss=75.2017
	step [216/270], loss=80.5394
	step [217/270], loss=81.4534
	step [218/270], loss=73.6145
	step [219/270], loss=86.0019
	step [220/270], loss=77.3399
	step [221/270], loss=69.7700
	step [222/270], loss=72.9450
	step [223/270], loss=81.3427
	step [224/270], loss=86.6126
	step [225/270], loss=65.0560
	step [226/270], loss=63.7037
	step [227/270], loss=87.6087
	step [228/270], loss=70.6276
	step [229/270], loss=76.0924
	step [230/270], loss=90.6843
	step [231/270], loss=65.0411
	step [232/270], loss=87.3369
	step [233/270], loss=89.2406
	step [234/270], loss=70.2603
	step [235/270], loss=79.3351
	step [236/270], loss=79.2017
	step [237/270], loss=97.2701
	step [238/270], loss=81.8724
	step [239/270], loss=76.3003
	step [240/270], loss=68.1066
	step [241/270], loss=76.4312
	step [242/270], loss=67.6901
	step [243/270], loss=80.3934
	step [244/270], loss=70.3725
	step [245/270], loss=92.6642
	step [246/270], loss=75.5770
	step [247/270], loss=61.8319
	step [248/270], loss=84.8830
	step [249/270], loss=86.1465
	step [250/270], loss=77.2875
	step [251/270], loss=78.2910
	step [252/270], loss=75.6181
	step [253/270], loss=83.1804
	step [254/270], loss=81.6414
	step [255/270], loss=75.4766
	step [256/270], loss=92.0694
	step [257/270], loss=74.4215
	step [258/270], loss=84.3458
	step [259/270], loss=61.1728
	step [260/270], loss=79.5663
	step [261/270], loss=88.2825
	step [262/270], loss=73.5203
	step [263/270], loss=72.4474
	step [264/270], loss=74.2190
	step [265/270], loss=79.2400
	step [266/270], loss=60.4835
	step [267/270], loss=81.6581
	step [268/270], loss=87.5501
	step [269/270], loss=77.7848
	step [270/270], loss=18.2508
	Evaluating
	loss=0.0085, precision=0.3183, recall=0.9129, f1=0.4720
Training epoch 66
	step [1/270], loss=80.9004
	step [2/270], loss=80.9234
	step [3/270], loss=102.2751
	step [4/270], loss=70.8608
	step [5/270], loss=67.5914
	step [6/270], loss=79.3565
	step [7/270], loss=67.7677
	step [8/270], loss=73.7058
	step [9/270], loss=89.7777
	step [10/270], loss=66.0619
	step [11/270], loss=79.2137
	step [12/270], loss=90.3924
	step [13/270], loss=58.5129
	step [14/270], loss=63.3992
	step [15/270], loss=83.4924
	step [16/270], loss=92.8569
	step [17/270], loss=69.8703
	step [18/270], loss=95.2887
	step [19/270], loss=86.5691
	step [20/270], loss=83.6560
	step [21/270], loss=77.3778
	step [22/270], loss=74.7010
	step [23/270], loss=86.2154
	step [24/270], loss=100.5357
	step [25/270], loss=79.5585
	step [26/270], loss=82.2551
	step [27/270], loss=76.8614
	step [28/270], loss=88.8412
	step [29/270], loss=80.6359
	step [30/270], loss=74.3667
	step [31/270], loss=80.5485
	step [32/270], loss=75.9650
	step [33/270], loss=77.8890
	step [34/270], loss=86.6797
	step [35/270], loss=93.1165
	step [36/270], loss=80.4375
	step [37/270], loss=88.0631
	step [38/270], loss=79.2626
	step [39/270], loss=87.3331
	step [40/270], loss=84.3370
	step [41/270], loss=74.9382
	step [42/270], loss=91.7458
	step [43/270], loss=84.5096
	step [44/270], loss=69.7364
	step [45/270], loss=84.3761
	step [46/270], loss=85.5808
	step [47/270], loss=78.9150
	step [48/270], loss=69.4387
	step [49/270], loss=76.5044
	step [50/270], loss=69.4134
	step [51/270], loss=77.2543
	step [52/270], loss=91.1657
	step [53/270], loss=91.8366
	step [54/270], loss=75.4236
	step [55/270], loss=64.8527
	step [56/270], loss=67.6781
	step [57/270], loss=76.4633
	step [58/270], loss=67.7351
	step [59/270], loss=85.7796
	step [60/270], loss=91.0183
	step [61/270], loss=71.4151
	step [62/270], loss=76.6858
	step [63/270], loss=59.5066
	step [64/270], loss=80.7255
	step [65/270], loss=69.3896
	step [66/270], loss=89.8093
	step [67/270], loss=73.6836
	step [68/270], loss=91.3377
	step [69/270], loss=79.4866
	step [70/270], loss=76.0725
	step [71/270], loss=54.7844
	step [72/270], loss=84.6301
	step [73/270], loss=85.4060
	step [74/270], loss=95.5004
	step [75/270], loss=84.9020
	step [76/270], loss=65.5633
	step [77/270], loss=71.3997
	step [78/270], loss=91.5736
	step [79/270], loss=81.1607
	step [80/270], loss=93.1490
	step [81/270], loss=94.2426
	step [82/270], loss=78.0823
	step [83/270], loss=73.3443
	step [84/270], loss=76.2732
	step [85/270], loss=84.3560
	step [86/270], loss=75.5960
	step [87/270], loss=88.5115
	step [88/270], loss=84.7074
	step [89/270], loss=75.5656
	step [90/270], loss=68.8540
	step [91/270], loss=76.6801
	step [92/270], loss=81.5380
	step [93/270], loss=82.4093
	step [94/270], loss=85.8066
	step [95/270], loss=69.8181
	step [96/270], loss=74.4697
	step [97/270], loss=74.0114
	step [98/270], loss=71.4923
	step [99/270], loss=102.7000
	step [100/270], loss=66.9878
	step [101/270], loss=76.3304
	step [102/270], loss=76.9243
	step [103/270], loss=97.8689
	step [104/270], loss=93.2019
	step [105/270], loss=81.7800
	step [106/270], loss=95.9436
	step [107/270], loss=89.3713
	step [108/270], loss=78.1569
	step [109/270], loss=81.6944
	step [110/270], loss=78.8298
	step [111/270], loss=83.8160
	step [112/270], loss=72.8338
	step [113/270], loss=102.1415
	step [114/270], loss=81.5855
	step [115/270], loss=63.0208
	step [116/270], loss=55.6503
	step [117/270], loss=80.1418
	step [118/270], loss=87.6384
	step [119/270], loss=90.9474
	step [120/270], loss=73.5224
	step [121/270], loss=85.5167
	step [122/270], loss=79.1587
	step [123/270], loss=90.2545
	step [124/270], loss=70.9111
	step [125/270], loss=71.0983
	step [126/270], loss=79.2067
	step [127/270], loss=77.5222
	step [128/270], loss=76.1098
	step [129/270], loss=67.2135
	step [130/270], loss=87.1522
	step [131/270], loss=84.3527
	step [132/270], loss=77.4581
	step [133/270], loss=84.7510
	step [134/270], loss=82.2312
	step [135/270], loss=81.0819
	step [136/270], loss=86.6462
	step [137/270], loss=89.6911
	step [138/270], loss=72.0195
	step [139/270], loss=74.3307
	step [140/270], loss=82.2160
	step [141/270], loss=78.5516
	step [142/270], loss=85.8236
	step [143/270], loss=79.8969
	step [144/270], loss=81.5008
	step [145/270], loss=88.4798
	step [146/270], loss=72.9350
	step [147/270], loss=67.9313
	step [148/270], loss=60.9784
	step [149/270], loss=80.2398
	step [150/270], loss=85.2848
	step [151/270], loss=83.5237
	step [152/270], loss=75.9650
	step [153/270], loss=91.5049
	step [154/270], loss=74.9918
	step [155/270], loss=89.6979
	step [156/270], loss=90.3332
	step [157/270], loss=83.2739
	step [158/270], loss=92.5081
	step [159/270], loss=73.9590
	step [160/270], loss=69.5864
	step [161/270], loss=72.9871
	step [162/270], loss=87.2707
	step [163/270], loss=100.0941
	step [164/270], loss=76.8823
	step [165/270], loss=69.5357
	step [166/270], loss=81.3278
	step [167/270], loss=97.4103
	step [168/270], loss=63.7124
	step [169/270], loss=82.6360
	step [170/270], loss=92.1554
	step [171/270], loss=64.6702
	step [172/270], loss=79.6037
	step [173/270], loss=78.0586
	step [174/270], loss=83.8518
	step [175/270], loss=75.4922
	step [176/270], loss=86.9830
	step [177/270], loss=77.7256
	step [178/270], loss=102.7552
	step [179/270], loss=84.4703
	step [180/270], loss=76.0501
	step [181/270], loss=82.3664
	step [182/270], loss=85.1012
	step [183/270], loss=74.1816
	step [184/270], loss=91.7514
	step [185/270], loss=96.8825
	step [186/270], loss=86.8132
	step [187/270], loss=91.7554
	step [188/270], loss=76.1004
	step [189/270], loss=87.9928
	step [190/270], loss=78.1595
	step [191/270], loss=78.0540
	step [192/270], loss=74.6682
	step [193/270], loss=78.4715
	step [194/270], loss=74.4171
	step [195/270], loss=95.3835
	step [196/270], loss=86.0770
	step [197/270], loss=71.2258
	step [198/270], loss=70.1014
	step [199/270], loss=78.4714
	step [200/270], loss=77.6310
	step [201/270], loss=85.2486
	step [202/270], loss=93.9736
	step [203/270], loss=98.2046
	step [204/270], loss=79.7114
	step [205/270], loss=85.1077
	step [206/270], loss=71.6212
	step [207/270], loss=88.9832
	step [208/270], loss=92.9022
	step [209/270], loss=66.7377
	step [210/270], loss=79.9197
	step [211/270], loss=83.4404
	step [212/270], loss=71.5358
	step [213/270], loss=66.6645
	step [214/270], loss=58.6463
	step [215/270], loss=80.5231
	step [216/270], loss=70.1243
	step [217/270], loss=106.6198
	step [218/270], loss=89.2185
	step [219/270], loss=87.8204
	step [220/270], loss=78.1880
	step [221/270], loss=103.6527
	step [222/270], loss=96.1473
	step [223/270], loss=69.0133
	step [224/270], loss=66.5334
	step [225/270], loss=66.3566
	step [226/270], loss=82.6487
	step [227/270], loss=74.9985
	step [228/270], loss=81.0237
	step [229/270], loss=69.2249
	step [230/270], loss=83.7602
	step [231/270], loss=87.9015
	step [232/270], loss=92.7976
	step [233/270], loss=86.3350
	step [234/270], loss=84.5313
	step [235/270], loss=89.8056
	step [236/270], loss=95.0860
	step [237/270], loss=84.4603
	step [238/270], loss=66.7268
	step [239/270], loss=83.9547
	step [240/270], loss=76.6736
	step [241/270], loss=83.2004
	step [242/270], loss=77.6583
	step [243/270], loss=89.8689
	step [244/270], loss=88.0268
	step [245/270], loss=98.7716
	step [246/270], loss=95.2189
	step [247/270], loss=93.1071
	step [248/270], loss=78.2520
	step [249/270], loss=80.2411
	step [250/270], loss=84.2965
	step [251/270], loss=68.7707
	step [252/270], loss=82.2071
	step [253/270], loss=86.7115
	step [254/270], loss=80.1494
	step [255/270], loss=93.2048
	step [256/270], loss=66.2655
	step [257/270], loss=78.4212
	step [258/270], loss=61.6079
	step [259/270], loss=101.2514
	step [260/270], loss=70.9060
	step [261/270], loss=81.9383
	step [262/270], loss=79.5128
	step [263/270], loss=72.8908
	step [264/270], loss=80.0434
	step [265/270], loss=67.4680
	step [266/270], loss=84.2848
	step [267/270], loss=86.6678
	step [268/270], loss=59.9591
	step [269/270], loss=76.6429
	step [270/270], loss=13.5456
	Evaluating
	loss=0.0082, precision=0.3262, recall=0.9041, f1=0.4794
Training epoch 67
	step [1/270], loss=97.1045
	step [2/270], loss=56.8503
	step [3/270], loss=71.9136
	step [4/270], loss=77.6334
	step [5/270], loss=82.6540
	step [6/270], loss=95.7088
	step [7/270], loss=71.3159
	step [8/270], loss=83.7558
	step [9/270], loss=86.5012
	step [10/270], loss=86.2293
	step [11/270], loss=67.6434
	step [12/270], loss=82.5038
	step [13/270], loss=88.6899
	step [14/270], loss=78.1687
	step [15/270], loss=90.7737
	step [16/270], loss=100.6319
	step [17/270], loss=97.1857
	step [18/270], loss=80.1537
	step [19/270], loss=94.5307
	step [20/270], loss=72.7266
	step [21/270], loss=82.4274
	step [22/270], loss=91.8954
	step [23/270], loss=84.6248
	step [24/270], loss=77.9357
	step [25/270], loss=81.8315
	step [26/270], loss=97.0924
	step [27/270], loss=86.1397
	step [28/270], loss=73.7019
	step [29/270], loss=76.5874
	step [30/270], loss=96.0628
	step [31/270], loss=98.3868
	step [32/270], loss=71.6343
	step [33/270], loss=54.5028
	step [34/270], loss=77.2242
	step [35/270], loss=74.4140
	step [36/270], loss=54.8239
	step [37/270], loss=85.9348
	step [38/270], loss=77.9556
	step [39/270], loss=86.2674
	step [40/270], loss=71.5668
	step [41/270], loss=87.7315
	step [42/270], loss=76.3395
	step [43/270], loss=75.9069
	step [44/270], loss=74.6332
	step [45/270], loss=86.6660
	step [46/270], loss=77.6380
	step [47/270], loss=91.4290
	step [48/270], loss=67.3279
	step [49/270], loss=73.4283
	step [50/270], loss=76.6231
	step [51/270], loss=87.8243
	step [52/270], loss=71.5485
	step [53/270], loss=74.1073
	step [54/270], loss=81.3297
	step [55/270], loss=80.3474
	step [56/270], loss=82.1638
	step [57/270], loss=88.8778
	step [58/270], loss=87.0743
	step [59/270], loss=79.1585
	step [60/270], loss=87.8022
	step [61/270], loss=87.9402
	step [62/270], loss=79.7822
	step [63/270], loss=87.3081
	step [64/270], loss=82.1310
	step [65/270], loss=70.3135
	step [66/270], loss=70.1188
	step [67/270], loss=68.7646
	step [68/270], loss=77.2308
	step [69/270], loss=79.0805
	step [70/270], loss=76.2776
	step [71/270], loss=77.9119
	step [72/270], loss=88.8143
	step [73/270], loss=84.5962
	step [74/270], loss=90.5899
	step [75/270], loss=81.0758
	step [76/270], loss=92.2644
	step [77/270], loss=57.5217
	step [78/270], loss=79.4269
	step [79/270], loss=78.9650
	step [80/270], loss=70.9242
	step [81/270], loss=99.4651
	step [82/270], loss=73.0192
	step [83/270], loss=82.9151
	step [84/270], loss=73.1305
	step [85/270], loss=76.6220
	step [86/270], loss=82.7000
	step [87/270], loss=77.9152
	step [88/270], loss=73.9951
	step [89/270], loss=100.9685
	step [90/270], loss=66.9977
	step [91/270], loss=78.3722
	step [92/270], loss=69.5114
	step [93/270], loss=69.5137
	step [94/270], loss=73.9412
	step [95/270], loss=74.6212
	step [96/270], loss=74.5505
	step [97/270], loss=72.9746
	step [98/270], loss=89.6835
	step [99/270], loss=92.8382
	step [100/270], loss=80.5055
	step [101/270], loss=83.8876
	step [102/270], loss=84.7611
	step [103/270], loss=93.1227
	step [104/270], loss=74.4682
	step [105/270], loss=75.2711
	step [106/270], loss=82.4703
	step [107/270], loss=78.9678
	step [108/270], loss=76.8029
	step [109/270], loss=81.3372
	step [110/270], loss=78.3287
	step [111/270], loss=93.7909
	step [112/270], loss=70.7323
	step [113/270], loss=81.8963
	step [114/270], loss=92.1909
	step [115/270], loss=85.5558
	step [116/270], loss=79.6978
	step [117/270], loss=68.6940
	step [118/270], loss=80.3514
	step [119/270], loss=76.6746
	step [120/270], loss=73.5828
	step [121/270], loss=61.3387
	step [122/270], loss=94.0304
	step [123/270], loss=95.7041
	step [124/270], loss=87.6746
	step [125/270], loss=69.8677
	step [126/270], loss=66.9192
	step [127/270], loss=74.2413
	step [128/270], loss=84.3775
	step [129/270], loss=83.5985
	step [130/270], loss=63.4155
	step [131/270], loss=78.4283
	step [132/270], loss=92.7328
	step [133/270], loss=68.4939
	step [134/270], loss=66.7494
	step [135/270], loss=81.8725
	step [136/270], loss=86.2828
	step [137/270], loss=68.8946
	step [138/270], loss=68.7554
	step [139/270], loss=81.3097
	step [140/270], loss=87.0495
	step [141/270], loss=105.2679
	step [142/270], loss=93.0352
	step [143/270], loss=73.3524
	step [144/270], loss=75.4842
	step [145/270], loss=94.0454
	step [146/270], loss=75.6035
	step [147/270], loss=80.4365
	step [148/270], loss=83.2402
	step [149/270], loss=83.6282
	step [150/270], loss=83.8199
	step [151/270], loss=77.7891
	step [152/270], loss=60.8711
	step [153/270], loss=76.5223
	step [154/270], loss=79.9513
	step [155/270], loss=80.6920
	step [156/270], loss=83.0033
	step [157/270], loss=75.1419
	step [158/270], loss=91.2597
	step [159/270], loss=78.9689
	step [160/270], loss=95.8398
	step [161/270], loss=80.8547
	step [162/270], loss=95.1861
	step [163/270], loss=59.3616
	step [164/270], loss=71.2579
	step [165/270], loss=84.2002
	step [166/270], loss=91.0432
	step [167/270], loss=73.6060
	step [168/270], loss=76.7056
	step [169/270], loss=77.5097
	step [170/270], loss=68.3421
	step [171/270], loss=77.5312
	step [172/270], loss=103.4086
	step [173/270], loss=84.9474
	step [174/270], loss=76.3663
	step [175/270], loss=79.5494
	step [176/270], loss=62.5635
	step [177/270], loss=69.8816
	step [178/270], loss=89.0819
	step [179/270], loss=102.5896
	step [180/270], loss=81.3828
	step [181/270], loss=82.9415
	step [182/270], loss=94.0922
	step [183/270], loss=78.9379
	step [184/270], loss=91.5299
	step [185/270], loss=75.8082
	step [186/270], loss=78.8834
	step [187/270], loss=98.7491
	step [188/270], loss=82.5775
	step [189/270], loss=81.7604
	step [190/270], loss=85.7121
	step [191/270], loss=80.8728
	step [192/270], loss=96.5472
	step [193/270], loss=76.8191
	step [194/270], loss=88.5049
	step [195/270], loss=77.3280
	step [196/270], loss=65.3920
	step [197/270], loss=73.8942
	step [198/270], loss=80.5315
	step [199/270], loss=84.6809
	step [200/270], loss=94.7940
	step [201/270], loss=69.9987
	step [202/270], loss=86.6872
	step [203/270], loss=84.9724
	step [204/270], loss=81.2491
	step [205/270], loss=57.9692
	step [206/270], loss=61.7032
	step [207/270], loss=86.1797
	step [208/270], loss=83.9604
	step [209/270], loss=72.1879
	step [210/270], loss=89.6893
	step [211/270], loss=81.1405
	step [212/270], loss=79.1222
	step [213/270], loss=75.5266
	step [214/270], loss=96.2739
	step [215/270], loss=65.4561
	step [216/270], loss=89.1651
	step [217/270], loss=73.8641
	step [218/270], loss=79.5639
	step [219/270], loss=104.3928
	step [220/270], loss=80.8605
	step [221/270], loss=74.7518
	step [222/270], loss=84.6880
	step [223/270], loss=87.3249
	step [224/270], loss=69.3970
	step [225/270], loss=87.5864
	step [226/270], loss=92.0157
	step [227/270], loss=88.7714
	step [228/270], loss=82.6300
	step [229/270], loss=72.9943
	step [230/270], loss=85.3774
	step [231/270], loss=83.7209
	step [232/270], loss=95.6602
	step [233/270], loss=78.4042
	step [234/270], loss=88.2417
	step [235/270], loss=80.3191
	step [236/270], loss=87.9247
	step [237/270], loss=74.5540
	step [238/270], loss=77.7412
	step [239/270], loss=78.4906
	step [240/270], loss=81.6578
	step [241/270], loss=82.6965
	step [242/270], loss=74.1490
	step [243/270], loss=88.7240
	step [244/270], loss=92.4129
	step [245/270], loss=87.6223
	step [246/270], loss=82.9772
	step [247/270], loss=95.7205
	step [248/270], loss=79.1182
	step [249/270], loss=86.5616
	step [250/270], loss=90.4423
	step [251/270], loss=76.4236
	step [252/270], loss=68.2259
	step [253/270], loss=71.2863
	step [254/270], loss=71.0954
	step [255/270], loss=63.7476
	step [256/270], loss=75.3751
	step [257/270], loss=70.1421
	step [258/270], loss=82.7765
	step [259/270], loss=64.7083
	step [260/270], loss=83.5853
	step [261/270], loss=72.2656
	step [262/270], loss=68.9235
	step [263/270], loss=68.1061
	step [264/270], loss=83.6659
	step [265/270], loss=77.3635
	step [266/270], loss=78.2647
	step [267/270], loss=84.9617
	step [268/270], loss=75.7675
	step [269/270], loss=70.3008
	step [270/270], loss=14.3859
	Evaluating
	loss=0.0089, precision=0.2933, recall=0.9127, f1=0.4439
Training epoch 68
	step [1/270], loss=65.1528
	step [2/270], loss=103.9525
	step [3/270], loss=92.2779
	step [4/270], loss=92.5413
	step [5/270], loss=80.9245
	step [6/270], loss=76.2828
	step [7/270], loss=73.6794
	step [8/270], loss=81.5020
	step [9/270], loss=67.6389
	step [10/270], loss=84.8572
	step [11/270], loss=69.7691
	step [12/270], loss=80.0378
	step [13/270], loss=85.1310
	step [14/270], loss=76.2715
	step [15/270], loss=72.9958
	step [16/270], loss=67.0713
	step [17/270], loss=78.1805
	step [18/270], loss=75.3698
	step [19/270], loss=81.6862
	step [20/270], loss=76.4767
	step [21/270], loss=98.2509
	step [22/270], loss=75.2093
	step [23/270], loss=81.4779
	step [24/270], loss=80.6700
	step [25/270], loss=85.1108
	step [26/270], loss=88.4337
	step [27/270], loss=86.7699
	step [28/270], loss=67.7324
	step [29/270], loss=81.0142
	step [30/270], loss=76.7981
	step [31/270], loss=88.3789
	step [32/270], loss=79.7156
	step [33/270], loss=79.2842
	step [34/270], loss=76.9936
	step [35/270], loss=76.4261
	step [36/270], loss=87.4082
	step [37/270], loss=76.2789
	step [38/270], loss=72.2827
	step [39/270], loss=74.9358
	step [40/270], loss=82.3895
	step [41/270], loss=80.1019
	step [42/270], loss=84.6476
	step [43/270], loss=77.3110
	step [44/270], loss=82.4789
	step [45/270], loss=77.0591
	step [46/270], loss=78.1182
	step [47/270], loss=75.6650
	step [48/270], loss=85.4129
	step [49/270], loss=70.9527
	step [50/270], loss=66.2672
	step [51/270], loss=69.3084
	step [52/270], loss=74.0113
	step [53/270], loss=83.2304
	step [54/270], loss=82.1032
	step [55/270], loss=79.0946
	step [56/270], loss=76.2938
	step [57/270], loss=83.5042
	step [58/270], loss=85.8436
	step [59/270], loss=81.1026
	step [60/270], loss=70.6000
	step [61/270], loss=91.4677
	step [62/270], loss=83.2893
	step [63/270], loss=83.6415
	step [64/270], loss=89.5771
	step [65/270], loss=61.2474
	step [66/270], loss=101.0255
	step [67/270], loss=81.8276
	step [68/270], loss=102.4031
	step [69/270], loss=75.1044
	step [70/270], loss=79.8134
	step [71/270], loss=99.0889
	step [72/270], loss=79.6357
	step [73/270], loss=91.5666
	step [74/270], loss=74.5669
	step [75/270], loss=73.0313
	step [76/270], loss=86.0315
	step [77/270], loss=100.3987
	step [78/270], loss=69.6728
	step [79/270], loss=76.7081
	step [80/270], loss=78.6496
	step [81/270], loss=70.8006
	step [82/270], loss=71.0875
	step [83/270], loss=76.8905
	step [84/270], loss=73.7302
	step [85/270], loss=82.5239
	step [86/270], loss=82.0272
	step [87/270], loss=74.0598
	step [88/270], loss=64.2265
	step [89/270], loss=93.5185
	step [90/270], loss=88.3935
	step [91/270], loss=74.8346
	step [92/270], loss=72.4092
	step [93/270], loss=65.7127
	step [94/270], loss=98.0632
	step [95/270], loss=88.1964
	step [96/270], loss=66.2909
	step [97/270], loss=81.4935
	step [98/270], loss=72.6589
	step [99/270], loss=66.1137
	step [100/270], loss=83.5210
	step [101/270], loss=88.8330
	step [102/270], loss=79.8879
	step [103/270], loss=80.4350
	step [104/270], loss=89.6687
	step [105/270], loss=92.4853
	step [106/270], loss=82.3871
	step [107/270], loss=70.6717
	step [108/270], loss=69.9325
	step [109/270], loss=80.0898
	step [110/270], loss=63.7889
	step [111/270], loss=78.0779
	step [112/270], loss=77.9835
	step [113/270], loss=82.7462
	step [114/270], loss=80.4055
	step [115/270], loss=69.1659
	step [116/270], loss=85.5467
	step [117/270], loss=113.5609
	step [118/270], loss=78.2426
	step [119/270], loss=84.9042
	step [120/270], loss=84.3950
	step [121/270], loss=74.2378
	step [122/270], loss=83.6125
	step [123/270], loss=74.6671
	step [124/270], loss=88.4599
	step [125/270], loss=67.4567
	step [126/270], loss=80.4061
	step [127/270], loss=85.8004
	step [128/270], loss=83.1098
	step [129/270], loss=65.7270
	step [130/270], loss=78.4563
	step [131/270], loss=57.9949
	step [132/270], loss=86.7311
	step [133/270], loss=84.1559
	step [134/270], loss=109.6391
	step [135/270], loss=71.3837
	step [136/270], loss=88.6769
	step [137/270], loss=105.6794
	step [138/270], loss=81.1632
	step [139/270], loss=68.0590
	step [140/270], loss=68.6639
	step [141/270], loss=74.7801
	step [142/270], loss=79.6702
	step [143/270], loss=83.1491
	step [144/270], loss=64.3625
	step [145/270], loss=78.2343
	step [146/270], loss=67.2948
	step [147/270], loss=93.6194
	step [148/270], loss=66.3273
	step [149/270], loss=81.7472
	step [150/270], loss=78.2815
	step [151/270], loss=88.0878
	step [152/270], loss=89.1845
	step [153/270], loss=73.4828
	step [154/270], loss=83.7442
	step [155/270], loss=90.2352
	step [156/270], loss=79.5463
	step [157/270], loss=80.2355
	step [158/270], loss=93.0126
	step [159/270], loss=80.3011
	step [160/270], loss=66.3903
	step [161/270], loss=74.9698
	step [162/270], loss=95.9804
	step [163/270], loss=76.5136
	step [164/270], loss=89.2614
	step [165/270], loss=85.9438
	step [166/270], loss=79.5127
	step [167/270], loss=88.6273
	step [168/270], loss=71.0883
	step [169/270], loss=92.2434
	step [170/270], loss=71.4768
	step [171/270], loss=94.6435
	step [172/270], loss=82.1629
	step [173/270], loss=74.4206
	step [174/270], loss=94.5380
	step [175/270], loss=80.2377
	step [176/270], loss=81.9234
	step [177/270], loss=91.1259
	step [178/270], loss=81.1726
	step [179/270], loss=72.2131
	step [180/270], loss=94.2493
	step [181/270], loss=71.1124
	step [182/270], loss=78.8663
	step [183/270], loss=81.8021
	step [184/270], loss=87.9908
	step [185/270], loss=101.0125
	step [186/270], loss=80.3306
	step [187/270], loss=68.8540
	step [188/270], loss=63.2748
	step [189/270], loss=65.3076
	step [190/270], loss=70.7843
	step [191/270], loss=83.2605
	step [192/270], loss=91.5773
	step [193/270], loss=78.0639
	step [194/270], loss=68.5945
	step [195/270], loss=79.1934
	step [196/270], loss=71.0863
	step [197/270], loss=75.7563
	step [198/270], loss=70.2296
	step [199/270], loss=78.5435
	step [200/270], loss=76.6784
	step [201/270], loss=73.2624
	step [202/270], loss=77.1008
	step [203/270], loss=79.4416
	step [204/270], loss=68.2974
	step [205/270], loss=72.4085
	step [206/270], loss=69.6640
	step [207/270], loss=76.6521
	step [208/270], loss=89.7772
	step [209/270], loss=78.0527
	step [210/270], loss=86.4689
	step [211/270], loss=94.2399
	step [212/270], loss=75.1260
	step [213/270], loss=73.7819
	step [214/270], loss=88.3292
	step [215/270], loss=91.9636
	step [216/270], loss=94.3164
	step [217/270], loss=75.4799
	step [218/270], loss=65.4104
	step [219/270], loss=83.8029
	step [220/270], loss=71.8172
	step [221/270], loss=86.4803
	step [222/270], loss=91.4773
	step [223/270], loss=86.8203
	step [224/270], loss=75.4969
	step [225/270], loss=82.5350
	step [226/270], loss=78.0406
	step [227/270], loss=96.9749
	step [228/270], loss=70.8124
	step [229/270], loss=72.2517
	step [230/270], loss=68.1140
	step [231/270], loss=76.6118
	step [232/270], loss=81.7183
	step [233/270], loss=87.3477
	step [234/270], loss=85.6143
	step [235/270], loss=74.5652
	step [236/270], loss=75.2036
	step [237/270], loss=71.6421
	step [238/270], loss=72.7962
	step [239/270], loss=73.2003
	step [240/270], loss=81.3245
	step [241/270], loss=92.2093
	step [242/270], loss=91.9646
	step [243/270], loss=92.9432
	step [244/270], loss=85.9341
	step [245/270], loss=98.8901
	step [246/270], loss=85.4313
	step [247/270], loss=86.3242
	step [248/270], loss=71.7136
	step [249/270], loss=76.4842
	step [250/270], loss=77.4240
	step [251/270], loss=79.8729
	step [252/270], loss=91.3349
	step [253/270], loss=73.9384
	step [254/270], loss=74.2870
	step [255/270], loss=66.3477
	step [256/270], loss=82.3814
	step [257/270], loss=81.1849
	step [258/270], loss=67.2820
	step [259/270], loss=59.8929
	step [260/270], loss=72.9749
	step [261/270], loss=88.9361
	step [262/270], loss=93.0099
	step [263/270], loss=88.7583
	step [264/270], loss=69.6340
	step [265/270], loss=64.2104
	step [266/270], loss=86.7036
	step [267/270], loss=74.7328
	step [268/270], loss=68.0586
	step [269/270], loss=83.5000
	step [270/270], loss=13.1437
	Evaluating
	loss=0.0085, precision=0.3096, recall=0.9066, f1=0.4615
Training epoch 69
	step [1/270], loss=96.1010
	step [2/270], loss=101.3682
	step [3/270], loss=69.8841
	step [4/270], loss=86.2049
	step [5/270], loss=112.3899
	step [6/270], loss=107.6099
	step [7/270], loss=70.4675
	step [8/270], loss=84.6591
	step [9/270], loss=85.0074
	step [10/270], loss=73.5204
	step [11/270], loss=93.4369
	step [12/270], loss=79.4460
	step [13/270], loss=85.0044
	step [14/270], loss=70.7928
	step [15/270], loss=81.9543
	step [16/270], loss=89.3611
	step [17/270], loss=75.7552
	step [18/270], loss=77.5136
	step [19/270], loss=81.3161
	step [20/270], loss=84.2971
	step [21/270], loss=87.5108
	step [22/270], loss=84.6226
	step [23/270], loss=78.0126
	step [24/270], loss=68.2485
	step [25/270], loss=64.8440
	step [26/270], loss=97.0558
	step [27/270], loss=83.6443
	step [28/270], loss=87.1903
	step [29/270], loss=71.5223
	step [30/270], loss=82.2120
	step [31/270], loss=68.4114
	step [32/270], loss=91.9357
	step [33/270], loss=83.0789
	step [34/270], loss=88.5476
	step [35/270], loss=71.2475
	step [36/270], loss=85.5086
	step [37/270], loss=67.3045
	step [38/270], loss=83.8746
	step [39/270], loss=77.2403
	step [40/270], loss=84.4068
	step [41/270], loss=74.9008
	step [42/270], loss=88.4927
	step [43/270], loss=76.1889
	step [44/270], loss=73.4450
	step [45/270], loss=73.4822
	step [46/270], loss=77.8838
	step [47/270], loss=90.8006
	step [48/270], loss=73.0423
	step [49/270], loss=76.2617
	step [50/270], loss=78.8048
	step [51/270], loss=79.9951
	step [52/270], loss=80.3322
	step [53/270], loss=78.8567
	step [54/270], loss=87.4262
	step [55/270], loss=69.5834
	step [56/270], loss=77.5489
	step [57/270], loss=68.5279
	step [58/270], loss=82.7268
	step [59/270], loss=74.5524
	step [60/270], loss=82.6931
	step [61/270], loss=78.0251
	step [62/270], loss=79.7427
	step [63/270], loss=80.1559
	step [64/270], loss=84.4779
	step [65/270], loss=71.9929
	step [66/270], loss=81.3826
	step [67/270], loss=71.5384
	step [68/270], loss=85.7958
	step [69/270], loss=78.0403
	step [70/270], loss=79.7530
	step [71/270], loss=72.8196
	step [72/270], loss=61.0833
	step [73/270], loss=86.5184
	step [74/270], loss=74.4616
	step [75/270], loss=87.4647
	step [76/270], loss=80.9084
	step [77/270], loss=78.3283
	step [78/270], loss=82.5892
	step [79/270], loss=73.8755
	step [80/270], loss=95.6957
	step [81/270], loss=80.2838
	step [82/270], loss=90.8483
	step [83/270], loss=80.5667
	step [84/270], loss=74.5712
	step [85/270], loss=69.6779
	step [86/270], loss=82.2972
	step [87/270], loss=78.6425
	step [88/270], loss=67.3027
	step [89/270], loss=68.7405
	step [90/270], loss=94.7706
	step [91/270], loss=81.7435
	step [92/270], loss=56.6504
	step [93/270], loss=74.7053
	step [94/270], loss=77.9988
	step [95/270], loss=105.4173
	step [96/270], loss=90.9884
	step [97/270], loss=76.5933
	step [98/270], loss=73.3489
	step [99/270], loss=94.4317
	step [100/270], loss=72.2478
	step [101/270], loss=82.4130
	step [102/270], loss=62.2655
	step [103/270], loss=85.5434
	step [104/270], loss=70.5114
	step [105/270], loss=84.7558
	step [106/270], loss=102.0900
	step [107/270], loss=73.5908
	step [108/270], loss=61.1701
	step [109/270], loss=68.6090
	step [110/270], loss=84.4745
	step [111/270], loss=65.5200
	step [112/270], loss=81.9626
	step [113/270], loss=79.3068
	step [114/270], loss=71.7116
	step [115/270], loss=79.7880
	step [116/270], loss=83.4372
	step [117/270], loss=83.1339
	step [118/270], loss=72.6576
	step [119/270], loss=72.3760
	step [120/270], loss=87.6977
	step [121/270], loss=74.0290
	step [122/270], loss=80.3253
	step [123/270], loss=75.3833
	step [124/270], loss=62.0141
	step [125/270], loss=78.9071
	step [126/270], loss=92.6742
	step [127/270], loss=92.3418
	step [128/270], loss=79.2987
	step [129/270], loss=95.5484
	step [130/270], loss=73.7335
	step [131/270], loss=74.0214
	step [132/270], loss=94.3008
	step [133/270], loss=74.6365
	step [134/270], loss=75.3628
	step [135/270], loss=60.1140
	step [136/270], loss=77.1280
	step [137/270], loss=79.3650
	step [138/270], loss=70.8966
	step [139/270], loss=64.3278
	step [140/270], loss=67.2015
	step [141/270], loss=84.2013
	step [142/270], loss=88.6600
	step [143/270], loss=73.9343
	step [144/270], loss=77.9588
	step [145/270], loss=76.6122
	step [146/270], loss=82.5565
	step [147/270], loss=74.0461
	step [148/270], loss=83.6237
	step [149/270], loss=84.6047
	step [150/270], loss=83.1322
	step [151/270], loss=74.4521
	step [152/270], loss=78.1900
	step [153/270], loss=87.0666
	step [154/270], loss=71.8482
	step [155/270], loss=81.9061
	step [156/270], loss=82.0113
	step [157/270], loss=98.2592
	step [158/270], loss=80.4424
	step [159/270], loss=81.2328
	step [160/270], loss=82.7378
	step [161/270], loss=77.4873
	step [162/270], loss=84.8959
	step [163/270], loss=90.3953
	step [164/270], loss=68.0836
	step [165/270], loss=80.4970
	step [166/270], loss=77.7246
	step [167/270], loss=87.9256
	step [168/270], loss=83.8121
	step [169/270], loss=62.3630
	step [170/270], loss=80.2657
	step [171/270], loss=83.9543
	step [172/270], loss=65.6776
	step [173/270], loss=76.5365
	step [174/270], loss=79.0464
	step [175/270], loss=82.9947
	step [176/270], loss=68.6063
	step [177/270], loss=72.3379
	step [178/270], loss=86.4011
	step [179/270], loss=79.5913
	step [180/270], loss=88.4829
	step [181/270], loss=89.0914
	step [182/270], loss=77.2262
	step [183/270], loss=83.6563
	step [184/270], loss=65.0864
	step [185/270], loss=79.9633
	step [186/270], loss=87.0377
	step [187/270], loss=68.7082
	step [188/270], loss=81.8492
	step [189/270], loss=83.2177
	step [190/270], loss=70.8123
	step [191/270], loss=75.0444
	step [192/270], loss=91.4274
	step [193/270], loss=80.9324
	step [194/270], loss=80.9675
	step [195/270], loss=89.7168
	step [196/270], loss=79.7815
	step [197/270], loss=77.3131
	step [198/270], loss=77.7800
	step [199/270], loss=74.0051
	step [200/270], loss=80.6300
	step [201/270], loss=75.8330
	step [202/270], loss=88.4544
	step [203/270], loss=85.5566
	step [204/270], loss=75.4896
	step [205/270], loss=69.5006
	step [206/270], loss=94.9357
	step [207/270], loss=76.8836
	step [208/270], loss=75.9914
	step [209/270], loss=94.0199
	step [210/270], loss=71.6079
	step [211/270], loss=76.4843
	step [212/270], loss=79.2828
	step [213/270], loss=76.5783
	step [214/270], loss=73.2953
	step [215/270], loss=72.9750
	step [216/270], loss=73.9132
	step [217/270], loss=87.3105
	step [218/270], loss=92.0489
	step [219/270], loss=87.1566
	step [220/270], loss=80.0089
	step [221/270], loss=85.3770
	step [222/270], loss=84.2297
	step [223/270], loss=73.7099
	step [224/270], loss=63.9409
	step [225/270], loss=87.0825
	step [226/270], loss=67.1015
	step [227/270], loss=89.5894
	step [228/270], loss=81.8911
	step [229/270], loss=80.5050
	step [230/270], loss=83.5740
	step [231/270], loss=83.1509
	step [232/270], loss=77.4420
	step [233/270], loss=76.0253
	step [234/270], loss=73.5617
	step [235/270], loss=89.4307
	step [236/270], loss=70.7760
	step [237/270], loss=76.4666
	step [238/270], loss=60.4841
	step [239/270], loss=77.2351
	step [240/270], loss=77.9736
	step [241/270], loss=62.5913
	step [242/270], loss=80.9118
	step [243/270], loss=81.6805
	step [244/270], loss=86.3002
	step [245/270], loss=81.5463
	step [246/270], loss=92.4560
	step [247/270], loss=77.7803
	step [248/270], loss=78.9401
	step [249/270], loss=73.9185
	step [250/270], loss=91.6931
	step [251/270], loss=89.4661
	step [252/270], loss=69.3629
	step [253/270], loss=82.6566
	step [254/270], loss=88.7785
	step [255/270], loss=84.1805
	step [256/270], loss=81.1418
	step [257/270], loss=75.9217
	step [258/270], loss=73.7653
	step [259/270], loss=88.9625
	step [260/270], loss=80.6910
	step [261/270], loss=89.5736
	step [262/270], loss=93.5544
	step [263/270], loss=63.3457
	step [264/270], loss=83.0350
	step [265/270], loss=75.5308
	step [266/270], loss=75.2028
	step [267/270], loss=83.7780
	step [268/270], loss=88.4496
	step [269/270], loss=80.7400
	step [270/270], loss=14.2043
	Evaluating
	loss=0.0079, precision=0.3319, recall=0.8965, f1=0.4845
Training epoch 70
	step [1/270], loss=82.0326
	step [2/270], loss=62.6286
	step [3/270], loss=86.8540
	step [4/270], loss=73.3525
	step [5/270], loss=93.4736
	step [6/270], loss=77.7667
	step [7/270], loss=84.5092
	step [8/270], loss=87.2924
	step [9/270], loss=79.3428
	step [10/270], loss=71.4691
	step [11/270], loss=78.0142
	step [12/270], loss=69.5946
	step [13/270], loss=91.3643
	step [14/270], loss=68.0828
	step [15/270], loss=83.8478
	step [16/270], loss=76.5149
	step [17/270], loss=78.6009
	step [18/270], loss=90.3828
	step [19/270], loss=95.8706
	step [20/270], loss=63.2952
	step [21/270], loss=80.8052
	step [22/270], loss=63.4183
	step [23/270], loss=73.8492
	step [24/270], loss=73.7249
	step [25/270], loss=82.9834
	step [26/270], loss=95.8665
	step [27/270], loss=63.8970
	step [28/270], loss=76.3838
	step [29/270], loss=78.4754
	step [30/270], loss=87.3951
	step [31/270], loss=93.5386
	step [32/270], loss=88.1927
	step [33/270], loss=83.9201
	step [34/270], loss=91.6791
	step [35/270], loss=81.4847
	step [36/270], loss=84.0498
	step [37/270], loss=100.0667
	step [38/270], loss=63.2288
	step [39/270], loss=77.8047
	step [40/270], loss=87.0966
	step [41/270], loss=77.3724
	step [42/270], loss=88.9589
	step [43/270], loss=75.3393
	step [44/270], loss=87.7785
	step [45/270], loss=74.1285
	step [46/270], loss=76.2870
	step [47/270], loss=74.3962
	step [48/270], loss=67.5641
	step [49/270], loss=88.6323
	step [50/270], loss=68.5539
	step [51/270], loss=83.1243
	step [52/270], loss=89.1586
	step [53/270], loss=70.3349
	step [54/270], loss=75.0410
	step [55/270], loss=82.8159
	step [56/270], loss=87.1523
	step [57/270], loss=76.2148
	step [58/270], loss=94.4626
	step [59/270], loss=60.0398
	step [60/270], loss=84.3823
	step [61/270], loss=61.4463
	step [62/270], loss=75.0432
	step [63/270], loss=64.7500
	step [64/270], loss=66.2949
	step [65/270], loss=67.7059
	step [66/270], loss=74.5797
	step [67/270], loss=76.3326
	step [68/270], loss=86.5812
	step [69/270], loss=82.1750
	step [70/270], loss=75.2455
	step [71/270], loss=77.2567
	step [72/270], loss=77.8852
	step [73/270], loss=94.4236
	step [74/270], loss=73.0814
	step [75/270], loss=72.6084
	step [76/270], loss=90.8512
	step [77/270], loss=89.6970
	step [78/270], loss=105.2982
	step [79/270], loss=81.0917
	step [80/270], loss=68.1635
	step [81/270], loss=89.4016
	step [82/270], loss=66.0723
	step [83/270], loss=95.1219
	step [84/270], loss=70.5747
	step [85/270], loss=77.1377
	step [86/270], loss=81.8458
	step [87/270], loss=69.2525
	step [88/270], loss=68.1529
	step [89/270], loss=75.1195
	step [90/270], loss=94.7876
	step [91/270], loss=88.0748
	step [92/270], loss=73.3680
	step [93/270], loss=64.0801
	step [94/270], loss=73.7945
	step [95/270], loss=74.7913
	step [96/270], loss=79.7051
	step [97/270], loss=84.1511
	step [98/270], loss=80.0687
	step [99/270], loss=89.5433
	step [100/270], loss=77.8298
	step [101/270], loss=88.0015
	step [102/270], loss=82.8430
	step [103/270], loss=79.2402
	step [104/270], loss=60.8087
	step [105/270], loss=81.3855
	step [106/270], loss=87.3379
	step [107/270], loss=76.3115
	step [108/270], loss=83.6730
	step [109/270], loss=64.9559
	step [110/270], loss=87.5708
	step [111/270], loss=77.8304
	step [112/270], loss=77.8669
	step [113/270], loss=62.5056
	step [114/270], loss=69.9527
	step [115/270], loss=77.8534
	step [116/270], loss=82.9162
	step [117/270], loss=89.6415
	step [118/270], loss=99.2986
	step [119/270], loss=91.4142
	step [120/270], loss=88.3855
	step [121/270], loss=94.4622
	step [122/270], loss=62.9892
	step [123/270], loss=91.6133
	step [124/270], loss=67.1670
	step [125/270], loss=76.2061
	step [126/270], loss=86.5603
	step [127/270], loss=61.0451
	step [128/270], loss=70.4581
	step [129/270], loss=82.2152
	step [130/270], loss=87.7679
	step [131/270], loss=77.9170
	step [132/270], loss=80.5493
	step [133/270], loss=90.3735
	step [134/270], loss=78.5011
	step [135/270], loss=73.5566
	step [136/270], loss=79.5402
	step [137/270], loss=92.4358
	step [138/270], loss=67.2188
	step [139/270], loss=62.4047
	step [140/270], loss=76.7695
	step [141/270], loss=59.5154
	step [142/270], loss=68.9353
	step [143/270], loss=67.0300
	step [144/270], loss=88.2577
	step [145/270], loss=79.8954
	step [146/270], loss=84.8639
	step [147/270], loss=93.2219
	step [148/270], loss=80.5774
	step [149/270], loss=70.1226
	step [150/270], loss=80.4594
	step [151/270], loss=87.9635
	step [152/270], loss=69.0497
	step [153/270], loss=69.7644
	step [154/270], loss=79.9527
	step [155/270], loss=93.4748
	step [156/270], loss=98.9331
	step [157/270], loss=80.3814
	step [158/270], loss=82.0453
	step [159/270], loss=72.1172
	step [160/270], loss=78.3996
	step [161/270], loss=75.6500
	step [162/270], loss=81.5184
	step [163/270], loss=99.3485
	step [164/270], loss=81.3619
	step [165/270], loss=73.9460
	step [166/270], loss=74.7508
	step [167/270], loss=72.5591
	step [168/270], loss=69.0151
	step [169/270], loss=78.7854
	step [170/270], loss=89.0759
	step [171/270], loss=75.8587
	step [172/270], loss=79.0213
	step [173/270], loss=76.2898
	step [174/270], loss=69.4576
	step [175/270], loss=78.1843
	step [176/270], loss=70.3679
	step [177/270], loss=78.6001
	step [178/270], loss=87.2587
	step [179/270], loss=63.1456
	step [180/270], loss=80.8073
	step [181/270], loss=81.3830
	step [182/270], loss=88.5311
	step [183/270], loss=87.6108
	step [184/270], loss=74.8288
	step [185/270], loss=77.5476
	step [186/270], loss=66.6129
	step [187/270], loss=79.2334
	step [188/270], loss=77.1831
	step [189/270], loss=93.1813
	step [190/270], loss=87.3719
	step [191/270], loss=74.3592
	step [192/270], loss=77.9616
	step [193/270], loss=98.1697
	step [194/270], loss=71.9274
	step [195/270], loss=75.0837
	step [196/270], loss=79.4538
	step [197/270], loss=105.0384
	step [198/270], loss=68.5983
	step [199/270], loss=78.9732
	step [200/270], loss=93.9096
	step [201/270], loss=80.6556
	step [202/270], loss=75.3595
	step [203/270], loss=78.0511
	step [204/270], loss=84.6267
	step [205/270], loss=99.7850
	step [206/270], loss=69.7468
	step [207/270], loss=85.8627
	step [208/270], loss=73.9630
	step [209/270], loss=91.5439
	step [210/270], loss=85.9729
	step [211/270], loss=74.3696
	step [212/270], loss=90.7222
	step [213/270], loss=76.0414
	step [214/270], loss=70.7747
	step [215/270], loss=68.8511
	step [216/270], loss=77.5372
	step [217/270], loss=77.9558
	step [218/270], loss=72.0621
	step [219/270], loss=66.5077
	step [220/270], loss=72.6946
	step [221/270], loss=78.4856
	step [222/270], loss=74.6132
	step [223/270], loss=78.7748
	step [224/270], loss=92.2712
	step [225/270], loss=79.9657
	step [226/270], loss=80.1330
	step [227/270], loss=82.5181
	step [228/270], loss=79.7353
	step [229/270], loss=76.6964
	step [230/270], loss=62.2052
	step [231/270], loss=63.4055
	step [232/270], loss=70.3239
	step [233/270], loss=74.6626
	step [234/270], loss=94.3728
	step [235/270], loss=95.5976
	step [236/270], loss=76.3698
	step [237/270], loss=76.8772
	step [238/270], loss=103.5158
	step [239/270], loss=77.5420
	step [240/270], loss=80.7098
	step [241/270], loss=79.0069
	step [242/270], loss=85.1317
	step [243/270], loss=85.3253
	step [244/270], loss=77.4780
	step [245/270], loss=79.0619
	step [246/270], loss=101.5952
	step [247/270], loss=87.5473
	step [248/270], loss=63.9693
	step [249/270], loss=66.9772
	step [250/270], loss=82.5961
	step [251/270], loss=85.6978
	step [252/270], loss=70.6815
	step [253/270], loss=89.9277
	step [254/270], loss=67.5220
	step [255/270], loss=96.9196
	step [256/270], loss=76.3275
	step [257/270], loss=94.6948
	step [258/270], loss=103.7328
	step [259/270], loss=77.1287
	step [260/270], loss=76.9032
	step [261/270], loss=76.3556
	step [262/270], loss=77.3158
	step [263/270], loss=91.0814
	step [264/270], loss=99.1958
	step [265/270], loss=83.3838
	step [266/270], loss=106.4978
	step [267/270], loss=74.4384
	step [268/270], loss=76.3995
	step [269/270], loss=83.9990
	step [270/270], loss=15.2565
	Evaluating
	loss=0.0101, precision=0.2700, recall=0.8977, f1=0.4151
Training epoch 71
	step [1/270], loss=69.7593
	step [2/270], loss=67.6304
	step [3/270], loss=83.2331
	step [4/270], loss=77.5576
	step [5/270], loss=65.4886
	step [6/270], loss=59.7496
	step [7/270], loss=80.3677
	step [8/270], loss=84.4293
	step [9/270], loss=90.8515
	step [10/270], loss=84.4238
	step [11/270], loss=69.2821
	step [12/270], loss=75.0118
	step [13/270], loss=78.4542
	step [14/270], loss=88.8003
	step [15/270], loss=85.0857
	step [16/270], loss=93.9976
	step [17/270], loss=78.7128
	step [18/270], loss=61.9349
	step [19/270], loss=73.0411
	step [20/270], loss=89.4870
	step [21/270], loss=74.2127
	step [22/270], loss=73.1981
	step [23/270], loss=82.4957
	step [24/270], loss=75.9072
	step [25/270], loss=84.7347
	step [26/270], loss=87.6507
	step [27/270], loss=81.6145
	step [28/270], loss=63.0959
	step [29/270], loss=85.7923
	step [30/270], loss=100.7081
	step [31/270], loss=67.9523
	step [32/270], loss=80.6605
	step [33/270], loss=85.3886
	step [34/270], loss=87.5955
	step [35/270], loss=77.0365
	step [36/270], loss=85.7356
	step [37/270], loss=70.9474
	step [38/270], loss=86.8735
	step [39/270], loss=86.6263
	step [40/270], loss=97.8133
	step [41/270], loss=74.1393
	step [42/270], loss=76.0428
	step [43/270], loss=83.9795
	step [44/270], loss=86.3402
	step [45/270], loss=71.3138
	step [46/270], loss=87.8419
	step [47/270], loss=66.9866
	step [48/270], loss=77.7528
	step [49/270], loss=75.7192
	step [50/270], loss=79.3167
	step [51/270], loss=73.5278
	step [52/270], loss=70.5883
	step [53/270], loss=71.2237
	step [54/270], loss=87.3410
	step [55/270], loss=71.5393
	step [56/270], loss=85.0734
	step [57/270], loss=90.4165
	step [58/270], loss=76.0021
	step [59/270], loss=89.6423
	step [60/270], loss=80.3138
	step [61/270], loss=74.2233
	step [62/270], loss=90.0581
	step [63/270], loss=86.8025
	step [64/270], loss=70.2530
	step [65/270], loss=65.7593
	step [66/270], loss=88.7087
	step [67/270], loss=87.3259
	step [68/270], loss=68.3711
	step [69/270], loss=78.2109
	step [70/270], loss=70.4482
	step [71/270], loss=59.1475
	step [72/270], loss=76.8318
	step [73/270], loss=77.2872
	step [74/270], loss=80.5848
	step [75/270], loss=73.7428
	step [76/270], loss=71.9114
	step [77/270], loss=74.6713
	step [78/270], loss=90.6540
	step [79/270], loss=86.2442
	step [80/270], loss=68.0294
	step [81/270], loss=78.4640
	step [82/270], loss=70.8989
	step [83/270], loss=89.1547
	step [84/270], loss=70.1018
	step [85/270], loss=69.4277
	step [86/270], loss=85.1215
	step [87/270], loss=83.3122
	step [88/270], loss=79.8750
	step [89/270], loss=73.8125
	step [90/270], loss=76.1700
	step [91/270], loss=75.0994
	step [92/270], loss=84.4005
	step [93/270], loss=78.0683
	step [94/270], loss=110.3492
	step [95/270], loss=80.3065
	step [96/270], loss=83.3223
	step [97/270], loss=78.6570
	step [98/270], loss=51.7275
	step [99/270], loss=68.6371
	step [100/270], loss=73.1738
	step [101/270], loss=72.5043
	step [102/270], loss=78.6277
	step [103/270], loss=80.4103
	step [104/270], loss=71.2180
	step [105/270], loss=96.6786
	step [106/270], loss=85.5634
	step [107/270], loss=79.6278
	step [108/270], loss=89.5253
	step [109/270], loss=86.4350
	step [110/270], loss=77.0705
	step [111/270], loss=81.7915
	step [112/270], loss=75.5063
	step [113/270], loss=92.1886
	step [114/270], loss=86.2188
	step [115/270], loss=85.5803
	step [116/270], loss=77.3713
	step [117/270], loss=57.4495
	step [118/270], loss=75.8506
	step [119/270], loss=80.0051
	step [120/270], loss=79.7302
	step [121/270], loss=76.0561
	step [122/270], loss=92.9051
	step [123/270], loss=82.0723
	step [124/270], loss=89.0138
	step [125/270], loss=92.9249
	step [126/270], loss=81.1399
	step [127/270], loss=76.0706
	step [128/270], loss=74.3765
	step [129/270], loss=70.9324
	step [130/270], loss=100.2945
	step [131/270], loss=67.3944
	step [132/270], loss=80.3502
	step [133/270], loss=80.8318
	step [134/270], loss=62.6279
	step [135/270], loss=70.3391
	step [136/270], loss=63.6844
	step [137/270], loss=78.6088
	step [138/270], loss=71.1759
	step [139/270], loss=98.4585
	step [140/270], loss=92.7504
	step [141/270], loss=84.3919
	step [142/270], loss=61.0791
	step [143/270], loss=78.0547
	step [144/270], loss=64.2482
	step [145/270], loss=102.3336
	step [146/270], loss=64.0024
	step [147/270], loss=73.0106
	step [148/270], loss=83.1928
	step [149/270], loss=85.3404
	step [150/270], loss=77.5092
	step [151/270], loss=71.6538
	step [152/270], loss=91.0376
	step [153/270], loss=72.4858
	step [154/270], loss=75.8270
	step [155/270], loss=91.2299
	step [156/270], loss=80.4633
	step [157/270], loss=93.2052
	step [158/270], loss=69.7941
	step [159/270], loss=66.8469
	step [160/270], loss=82.9181
	step [161/270], loss=74.8188
	step [162/270], loss=80.9813
	step [163/270], loss=97.5943
	step [164/270], loss=86.4921
	step [165/270], loss=73.0191
	step [166/270], loss=73.3890
	step [167/270], loss=70.0356
	step [168/270], loss=86.0203
	step [169/270], loss=66.6157
	step [170/270], loss=85.2110
	step [171/270], loss=75.1013
	step [172/270], loss=84.6616
	step [173/270], loss=76.1607
	step [174/270], loss=86.1993
	step [175/270], loss=68.7913
	step [176/270], loss=74.7308
	step [177/270], loss=78.2892
	step [178/270], loss=79.2448
	step [179/270], loss=75.3373
	step [180/270], loss=81.5362
	step [181/270], loss=85.5611
	step [182/270], loss=68.8294
	step [183/270], loss=80.4118
	step [184/270], loss=78.1774
	step [185/270], loss=76.3921
	step [186/270], loss=90.4044
	step [187/270], loss=92.3651
	step [188/270], loss=80.6255
	step [189/270], loss=79.3949
	step [190/270], loss=70.2753
	step [191/270], loss=88.9122
	step [192/270], loss=86.8572
	step [193/270], loss=81.3192
	step [194/270], loss=85.7583
	step [195/270], loss=68.1420
	step [196/270], loss=74.4933
	step [197/270], loss=66.7070
	step [198/270], loss=90.5833
	step [199/270], loss=67.3528
	step [200/270], loss=79.5793
	step [201/270], loss=87.4459
	step [202/270], loss=78.6932
	step [203/270], loss=77.9150
	step [204/270], loss=77.3622
	step [205/270], loss=80.3440
	step [206/270], loss=78.0448
	step [207/270], loss=75.2994
	step [208/270], loss=71.7826
	step [209/270], loss=72.0735
	step [210/270], loss=87.4308
	step [211/270], loss=82.4821
	step [212/270], loss=86.4625
	step [213/270], loss=84.1476
	step [214/270], loss=80.1223
	step [215/270], loss=86.0510
	step [216/270], loss=86.1130
	step [217/270], loss=65.1087
	step [218/270], loss=80.6718
	step [219/270], loss=79.5631
	step [220/270], loss=64.2875
	step [221/270], loss=66.7218
	step [222/270], loss=61.0054
	step [223/270], loss=115.7618
	step [224/270], loss=80.6739
	step [225/270], loss=82.8588
	step [226/270], loss=75.1705
	step [227/270], loss=76.7954
	step [228/270], loss=75.6743
	step [229/270], loss=70.4843
	step [230/270], loss=81.1927
	step [231/270], loss=72.9791
	step [232/270], loss=78.6423
	step [233/270], loss=93.7410
	step [234/270], loss=74.6608
	step [235/270], loss=81.4571
	step [236/270], loss=63.6540
	step [237/270], loss=66.1229
	step [238/270], loss=90.3452
	step [239/270], loss=71.8417
	step [240/270], loss=83.6754
	step [241/270], loss=77.4647
	step [242/270], loss=77.4862
	step [243/270], loss=70.5233
	step [244/270], loss=72.5333
	step [245/270], loss=66.0760
	step [246/270], loss=71.4064
	step [247/270], loss=87.1171
	step [248/270], loss=87.7416
	step [249/270], loss=75.8991
	step [250/270], loss=90.9648
	step [251/270], loss=73.6461
	step [252/270], loss=84.3344
	step [253/270], loss=87.0859
	step [254/270], loss=69.4727
	step [255/270], loss=82.9387
	step [256/270], loss=91.4813
	step [257/270], loss=76.0352
	step [258/270], loss=80.6885
	step [259/270], loss=83.0885
	step [260/270], loss=89.4259
	step [261/270], loss=93.9289
	step [262/270], loss=82.5870
	step [263/270], loss=87.1295
	step [264/270], loss=84.2616
	step [265/270], loss=98.9483
	step [266/270], loss=91.2748
	step [267/270], loss=84.1869
	step [268/270], loss=80.8892
	step [269/270], loss=71.9307
	step [270/270], loss=19.6149
	Evaluating
	loss=0.0072, precision=0.3625, recall=0.8953, f1=0.5160
saving model as: 4_saved_model.pth
Training epoch 72
	step [1/270], loss=97.5458
	step [2/270], loss=74.8974
	step [3/270], loss=92.0657
	step [4/270], loss=86.1436
	step [5/270], loss=87.8610
	step [6/270], loss=69.8025
	step [7/270], loss=92.0847
	step [8/270], loss=67.4782
	step [9/270], loss=96.4317
	step [10/270], loss=83.0871
	step [11/270], loss=79.6427
	step [12/270], loss=72.4089
	step [13/270], loss=87.5349
	step [14/270], loss=74.9916
	step [15/270], loss=78.4798
	step [16/270], loss=90.0156
	step [17/270], loss=86.4682
	step [18/270], loss=71.4129
	step [19/270], loss=82.9940
	step [20/270], loss=99.7776
	step [21/270], loss=77.2071
	step [22/270], loss=87.1578
	step [23/270], loss=71.9568
	step [24/270], loss=74.1107
	step [25/270], loss=68.1789
	step [26/270], loss=86.7329
	step [27/270], loss=75.9803
	step [28/270], loss=89.2702
	step [29/270], loss=70.1363
	step [30/270], loss=86.4760
	step [31/270], loss=78.3670
	step [32/270], loss=70.6100
	step [33/270], loss=68.3956
	step [34/270], loss=85.0686
	step [35/270], loss=78.3536
	step [36/270], loss=78.0602
	step [37/270], loss=78.0795
	step [38/270], loss=87.6874
	step [39/270], loss=75.8944
	step [40/270], loss=93.4296
	step [41/270], loss=77.0616
	step [42/270], loss=79.3820
	step [43/270], loss=71.0740
	step [44/270], loss=71.6606
	step [45/270], loss=84.4348
	step [46/270], loss=80.0031
	step [47/270], loss=89.3246
	step [48/270], loss=79.6708
	step [49/270], loss=63.6985
	step [50/270], loss=72.1406
	step [51/270], loss=76.6456
	step [52/270], loss=69.0896
	step [53/270], loss=85.5417
	step [54/270], loss=83.0469
	step [55/270], loss=81.8148
	step [56/270], loss=73.0572
	step [57/270], loss=74.4914
	step [58/270], loss=82.6701
	step [59/270], loss=73.1751
	step [60/270], loss=80.6434
	step [61/270], loss=69.7741
	step [62/270], loss=69.5843
	step [63/270], loss=62.1720
	step [64/270], loss=78.4090
	step [65/270], loss=80.5099
	step [66/270], loss=72.9553
	step [67/270], loss=77.7321
	step [68/270], loss=78.1947
	step [69/270], loss=77.1336
	step [70/270], loss=81.8208
	step [71/270], loss=64.1682
	step [72/270], loss=93.7939
	step [73/270], loss=78.0323
	step [74/270], loss=73.6185
	step [75/270], loss=82.0016
	step [76/270], loss=79.6151
	step [77/270], loss=83.1890
	step [78/270], loss=69.2771
	step [79/270], loss=76.3345
	step [80/270], loss=84.8278
	step [81/270], loss=85.2930
	step [82/270], loss=78.6435
	step [83/270], loss=85.9809
	step [84/270], loss=77.5299
	step [85/270], loss=76.9913
	step [86/270], loss=84.4700
	step [87/270], loss=85.0157
	step [88/270], loss=65.8546
	step [89/270], loss=89.5139
	step [90/270], loss=90.6907
	step [91/270], loss=69.5668
	step [92/270], loss=79.7399
	step [93/270], loss=86.9941
	step [94/270], loss=75.2265
	step [95/270], loss=72.1814
	step [96/270], loss=72.2646
	step [97/270], loss=81.8240
	step [98/270], loss=68.9598
	step [99/270], loss=77.1540
	step [100/270], loss=71.2790
	step [101/270], loss=86.5793
	step [102/270], loss=58.0707
	step [103/270], loss=79.1609
	step [104/270], loss=88.0723
	step [105/270], loss=71.4096
	step [106/270], loss=85.9119
	step [107/270], loss=85.2565
	step [108/270], loss=84.8363
	step [109/270], loss=76.2024
	step [110/270], loss=92.5139
	step [111/270], loss=74.3769
	step [112/270], loss=87.7937
	step [113/270], loss=78.7367
	step [114/270], loss=84.4838
	step [115/270], loss=88.5043
	step [116/270], loss=77.1393
	step [117/270], loss=64.4961
	step [118/270], loss=79.1934
	step [119/270], loss=70.7903
	step [120/270], loss=72.7838
	step [121/270], loss=65.0597
	step [122/270], loss=72.7420
	step [123/270], loss=88.9129
	step [124/270], loss=82.2585
	step [125/270], loss=82.5429
	step [126/270], loss=74.1806
	step [127/270], loss=83.1539
	step [128/270], loss=81.6458
	step [129/270], loss=74.2383
	step [130/270], loss=71.7406
	step [131/270], loss=93.4633
	step [132/270], loss=80.6914
	step [133/270], loss=81.0563
	step [134/270], loss=68.0246
	step [135/270], loss=85.2716
	step [136/270], loss=83.4101
	step [137/270], loss=90.8371
	step [138/270], loss=73.6621
	step [139/270], loss=72.0626
	step [140/270], loss=73.8004
	step [141/270], loss=74.9105
	step [142/270], loss=60.1084
	step [143/270], loss=69.3843
	step [144/270], loss=79.8675
	step [145/270], loss=76.0890
	step [146/270], loss=72.4684
	step [147/270], loss=81.4368
	step [148/270], loss=61.9274
	step [149/270], loss=69.9378
	step [150/270], loss=66.9767
	step [151/270], loss=92.0182
	step [152/270], loss=87.6307
	step [153/270], loss=72.5801
	step [154/270], loss=76.6042
	step [155/270], loss=70.8156
	step [156/270], loss=77.9775
	step [157/270], loss=83.0107
	step [158/270], loss=79.8910
	step [159/270], loss=88.8018
	step [160/270], loss=76.0297
	step [161/270], loss=80.4955
	step [162/270], loss=94.7429
	step [163/270], loss=71.6891
	step [164/270], loss=89.5906
	step [165/270], loss=84.2905
	step [166/270], loss=76.2671
	step [167/270], loss=58.1775
	step [168/270], loss=68.0169
	step [169/270], loss=67.4259
	step [170/270], loss=82.2107
	step [171/270], loss=82.7949
	step [172/270], loss=74.4675
	step [173/270], loss=81.8226
	step [174/270], loss=78.4822
	step [175/270], loss=81.9270
	step [176/270], loss=70.1516
	step [177/270], loss=78.2413
	step [178/270], loss=80.5887
	step [179/270], loss=83.6790
	step [180/270], loss=67.0457
	step [181/270], loss=82.5530
	step [182/270], loss=65.0408
	step [183/270], loss=72.5475
	step [184/270], loss=62.9486
	step [185/270], loss=69.2778
	step [186/270], loss=79.4410
	step [187/270], loss=77.0402
	step [188/270], loss=97.2653
	step [189/270], loss=87.4683
	step [190/270], loss=79.6024
	step [191/270], loss=87.8692
	step [192/270], loss=73.6492
	step [193/270], loss=90.4991
	step [194/270], loss=82.2831
	step [195/270], loss=69.8757
	step [196/270], loss=81.1601
	step [197/270], loss=67.1241
	step [198/270], loss=88.0657
	step [199/270], loss=82.9801
	step [200/270], loss=77.6960
	step [201/270], loss=62.5965
	step [202/270], loss=86.1422
	step [203/270], loss=75.3849
	step [204/270], loss=90.2582
	step [205/270], loss=96.4155
	step [206/270], loss=85.7987
	step [207/270], loss=75.2994
	step [208/270], loss=67.4811
	step [209/270], loss=73.1641
	step [210/270], loss=97.8735
	step [211/270], loss=90.7780
	step [212/270], loss=71.8805
	step [213/270], loss=81.6397
	step [214/270], loss=77.1694
	step [215/270], loss=87.8811
	step [216/270], loss=94.4563
	step [217/270], loss=76.1752
	step [218/270], loss=69.4120
	step [219/270], loss=75.8806
	step [220/270], loss=86.8060
	step [221/270], loss=73.6297
	step [222/270], loss=94.0270
	step [223/270], loss=79.4633
	step [224/270], loss=90.6032
	step [225/270], loss=70.6559
	step [226/270], loss=78.0460
	step [227/270], loss=73.3325
	step [228/270], loss=74.8064
	step [229/270], loss=85.1557
	step [230/270], loss=77.7356
	step [231/270], loss=75.3982
	step [232/270], loss=74.1526
	step [233/270], loss=81.9784
	step [234/270], loss=79.7943
	step [235/270], loss=92.4163
	step [236/270], loss=80.5794
	step [237/270], loss=84.1674
	step [238/270], loss=78.5788
	step [239/270], loss=84.7515
	step [240/270], loss=72.1289
	step [241/270], loss=75.6673
	step [242/270], loss=82.7472
	step [243/270], loss=63.5969
	step [244/270], loss=99.4090
	step [245/270], loss=71.9072
	step [246/270], loss=73.0721
	step [247/270], loss=69.5405
	step [248/270], loss=75.5288
	step [249/270], loss=85.2246
	step [250/270], loss=80.4131
	step [251/270], loss=84.5233
	step [252/270], loss=81.0071
	step [253/270], loss=91.9941
	step [254/270], loss=73.7619
	step [255/270], loss=84.5098
	step [256/270], loss=72.2458
	step [257/270], loss=85.3313
	step [258/270], loss=82.0651
	step [259/270], loss=62.9863
	step [260/270], loss=86.1275
	step [261/270], loss=81.0382
	step [262/270], loss=78.9831
	step [263/270], loss=75.8160
	step [264/270], loss=60.9045
	step [265/270], loss=86.1631
	step [266/270], loss=80.5630
	step [267/270], loss=62.7657
	step [268/270], loss=107.1662
	step [269/270], loss=81.7635
	step [270/270], loss=7.4252
	Evaluating
	loss=0.0081, precision=0.3179, recall=0.9030, f1=0.4702
Training epoch 73
	step [1/270], loss=95.0043
	step [2/270], loss=64.8240
	step [3/270], loss=96.1461
	step [4/270], loss=76.1047
	step [5/270], loss=84.2850
	step [6/270], loss=89.0562
	step [7/270], loss=71.2836
	step [8/270], loss=70.3649
	step [9/270], loss=90.8187
	step [10/270], loss=68.0770
	step [11/270], loss=68.0966
	step [12/270], loss=68.7202
	step [13/270], loss=85.6646
	step [14/270], loss=94.9857
	step [15/270], loss=66.4602
	step [16/270], loss=75.7168
	step [17/270], loss=78.5048
	step [18/270], loss=71.3626
	step [19/270], loss=73.1122
	step [20/270], loss=77.4358
	step [21/270], loss=70.5990
	step [22/270], loss=65.0970
	step [23/270], loss=78.1713
	step [24/270], loss=73.1393
	step [25/270], loss=90.3865
	step [26/270], loss=95.9383
	step [27/270], loss=87.8233
	step [28/270], loss=97.6693
	step [29/270], loss=59.8493
	step [30/270], loss=70.6834
	step [31/270], loss=86.1129
	step [32/270], loss=76.1799
	step [33/270], loss=77.9767
	step [34/270], loss=67.4959
	step [35/270], loss=66.8888
	step [36/270], loss=72.5327
	step [37/270], loss=88.1916
	step [38/270], loss=76.1741
	step [39/270], loss=87.2640
	step [40/270], loss=77.6404
	step [41/270], loss=84.0211
	step [42/270], loss=63.9817
	step [43/270], loss=68.2046
	step [44/270], loss=76.8284
	step [45/270], loss=76.4397
	step [46/270], loss=87.7546
	step [47/270], loss=98.4060
	step [48/270], loss=70.4913
	step [49/270], loss=67.2927
	step [50/270], loss=81.1660
	step [51/270], loss=67.6518
	step [52/270], loss=74.1960
	step [53/270], loss=73.3903
	step [54/270], loss=81.1317
	step [55/270], loss=85.9323
	step [56/270], loss=84.8570
	step [57/270], loss=88.1182
	step [58/270], loss=68.2832
	step [59/270], loss=82.6523
	step [60/270], loss=80.9627
	step [61/270], loss=72.6300
	step [62/270], loss=61.9803
	step [63/270], loss=86.4406
	step [64/270], loss=79.8724
	step [65/270], loss=89.4510
	step [66/270], loss=74.1844
	step [67/270], loss=87.0149
	step [68/270], loss=76.0942
	step [69/270], loss=82.9995
	step [70/270], loss=76.9553
	step [71/270], loss=65.2147
	step [72/270], loss=87.4638
	step [73/270], loss=77.4407
	step [74/270], loss=88.7590
	step [75/270], loss=89.5969
	step [76/270], loss=85.0738
	step [77/270], loss=66.7147
	step [78/270], loss=91.1109
	step [79/270], loss=75.9164
	step [80/270], loss=62.5906
	step [81/270], loss=73.5617
	step [82/270], loss=71.6006
	step [83/270], loss=90.1517
	step [84/270], loss=88.0947
	step [85/270], loss=66.1311
	step [86/270], loss=67.1726
	step [87/270], loss=76.6241
	step [88/270], loss=90.6533
	step [89/270], loss=63.4641
	step [90/270], loss=86.5145
	step [91/270], loss=78.3945
	step [92/270], loss=81.2855
	step [93/270], loss=91.9920
	step [94/270], loss=73.7441
	step [95/270], loss=82.3141
	step [96/270], loss=62.6226
	step [97/270], loss=85.5296
	step [98/270], loss=75.1460
	step [99/270], loss=85.4114
	step [100/270], loss=81.3687
	step [101/270], loss=96.7037
	step [102/270], loss=80.3195
	step [103/270], loss=71.5936
	step [104/270], loss=83.2925
	step [105/270], loss=76.4302
	step [106/270], loss=63.7325
	step [107/270], loss=70.1570
	step [108/270], loss=58.0611
	step [109/270], loss=78.7188
	step [110/270], loss=70.3208
	step [111/270], loss=81.8406
	step [112/270], loss=71.5564
	step [113/270], loss=93.1643
	step [114/270], loss=90.3436
	step [115/270], loss=76.3448
	step [116/270], loss=84.3700
	step [117/270], loss=75.2329
	step [118/270], loss=91.6356
	step [119/270], loss=78.0725
	step [120/270], loss=53.5090
	step [121/270], loss=79.4602
	step [122/270], loss=63.0937
	step [123/270], loss=87.0184
	step [124/270], loss=81.8862
	step [125/270], loss=83.5602
	step [126/270], loss=78.4926
	step [127/270], loss=80.9201
	step [128/270], loss=69.3619
	step [129/270], loss=83.0148
	step [130/270], loss=59.2258
	step [131/270], loss=82.5142
	step [132/270], loss=75.0022
	step [133/270], loss=72.4903
	step [134/270], loss=73.0784
	step [135/270], loss=81.2009
	step [136/270], loss=89.1843
	step [137/270], loss=82.2476
	step [138/270], loss=67.1176
	step [139/270], loss=79.8596
	step [140/270], loss=86.1702
	step [141/270], loss=85.6932
	step [142/270], loss=93.1383
	step [143/270], loss=68.5239
	step [144/270], loss=70.2551
	step [145/270], loss=65.9418
	step [146/270], loss=76.2454
	step [147/270], loss=76.5699
	step [148/270], loss=98.2742
	step [149/270], loss=77.5879
	step [150/270], loss=85.2312
	step [151/270], loss=77.3456
	step [152/270], loss=91.7181
	step [153/270], loss=80.8461
	step [154/270], loss=76.9447
	step [155/270], loss=71.6147
	step [156/270], loss=63.4183
	step [157/270], loss=89.3689
	step [158/270], loss=85.6851
	step [159/270], loss=86.6198
	step [160/270], loss=73.1193
	step [161/270], loss=86.6876
	step [162/270], loss=78.3039
	step [163/270], loss=95.4072
	step [164/270], loss=79.9306
	step [165/270], loss=85.7085
	step [166/270], loss=69.8261
	step [167/270], loss=82.9162
	step [168/270], loss=86.4174
	step [169/270], loss=84.0592
	step [170/270], loss=69.6401
	step [171/270], loss=77.7823
	step [172/270], loss=66.2606
	step [173/270], loss=80.6799
	step [174/270], loss=68.1969
	step [175/270], loss=74.5460
	step [176/270], loss=82.4351
	step [177/270], loss=86.0485
	step [178/270], loss=67.3129
	step [179/270], loss=68.6172
	step [180/270], loss=74.6798
	step [181/270], loss=82.0507
	step [182/270], loss=68.5839
	step [183/270], loss=104.0126
	step [184/270], loss=81.2424
	step [185/270], loss=87.1849
	step [186/270], loss=79.0569
	step [187/270], loss=72.9096
	step [188/270], loss=85.6633
	step [189/270], loss=71.6588
	step [190/270], loss=76.9462
	step [191/270], loss=88.8486
	step [192/270], loss=79.1224
	step [193/270], loss=81.3679
	step [194/270], loss=72.8431
	step [195/270], loss=87.4395
	step [196/270], loss=79.6494
	step [197/270], loss=66.8694
	step [198/270], loss=72.5082
	step [199/270], loss=77.0661
	step [200/270], loss=89.6324
	step [201/270], loss=74.6890
	step [202/270], loss=82.7603
	step [203/270], loss=82.2807
	step [204/270], loss=80.1134
	step [205/270], loss=88.3875
	step [206/270], loss=93.1181
	step [207/270], loss=74.7913
	step [208/270], loss=72.4426
	step [209/270], loss=74.5673
	step [210/270], loss=72.8568
	step [211/270], loss=78.9646
	step [212/270], loss=69.0250
	step [213/270], loss=71.1500
	step [214/270], loss=68.5618
	step [215/270], loss=76.6952
	step [216/270], loss=90.6147
	step [217/270], loss=85.5516
	step [218/270], loss=75.2877
	step [219/270], loss=84.0402
	step [220/270], loss=67.7238
	step [221/270], loss=84.7352
	step [222/270], loss=69.0084
	step [223/270], loss=72.7058
	step [224/270], loss=78.8380
	step [225/270], loss=83.2918
	step [226/270], loss=84.7679
	step [227/270], loss=79.8050
	step [228/270], loss=78.5076
	step [229/270], loss=68.0444
	step [230/270], loss=76.2435
	step [231/270], loss=70.8565
	step [232/270], loss=80.1363
	step [233/270], loss=90.7371
	step [234/270], loss=97.8441
	step [235/270], loss=74.1674
	step [236/270], loss=83.6381
	step [237/270], loss=70.0171
	step [238/270], loss=89.9369
	step [239/270], loss=86.5712
	step [240/270], loss=84.7479
	step [241/270], loss=89.1000
	step [242/270], loss=96.6475
	step [243/270], loss=66.6244
	step [244/270], loss=83.3956
	step [245/270], loss=62.1753
	step [246/270], loss=87.8661
	step [247/270], loss=74.9181
	step [248/270], loss=91.7159
	step [249/270], loss=87.4107
	step [250/270], loss=88.4030
	step [251/270], loss=78.9376
	step [252/270], loss=87.6365
	step [253/270], loss=90.8401
	step [254/270], loss=78.3561
	step [255/270], loss=63.1295
	step [256/270], loss=81.7808
	step [257/270], loss=78.8896
	step [258/270], loss=57.7926
	step [259/270], loss=86.8822
	step [260/270], loss=94.3402
	step [261/270], loss=99.2095
	step [262/270], loss=78.5242
	step [263/270], loss=73.6598
	step [264/270], loss=71.4986
	step [265/270], loss=65.1956
	step [266/270], loss=70.6170
	step [267/270], loss=75.4569
	step [268/270], loss=88.1659
	step [269/270], loss=89.7041
	step [270/270], loss=13.3113
	Evaluating
	loss=0.0092, precision=0.2915, recall=0.9143, f1=0.4421
Training epoch 74
	step [1/270], loss=77.3891
	step [2/270], loss=90.8664
	step [3/270], loss=64.0911
	step [4/270], loss=71.4610
	step [5/270], loss=63.2458
	step [6/270], loss=68.8997
	step [7/270], loss=90.7751
	step [8/270], loss=92.5208
	step [9/270], loss=80.4343
	step [10/270], loss=70.3621
	step [11/270], loss=86.8101
	step [12/270], loss=83.2999
	step [13/270], loss=66.6212
	step [14/270], loss=65.4234
	step [15/270], loss=85.2939
	step [16/270], loss=79.5305
	step [17/270], loss=70.6590
	step [18/270], loss=69.6049
	step [19/270], loss=76.1252
	step [20/270], loss=75.7154
	step [21/270], loss=77.2516
	step [22/270], loss=59.5745
	step [23/270], loss=70.5285
	step [24/270], loss=79.6078
	step [25/270], loss=73.1637
	step [26/270], loss=80.5297
	step [27/270], loss=96.0567
	step [28/270], loss=75.2973
	step [29/270], loss=83.8805
	step [30/270], loss=66.7853
	step [31/270], loss=67.5465
	step [32/270], loss=87.3879
	step [33/270], loss=87.2954
	step [34/270], loss=80.8530
	step [35/270], loss=70.1426
	step [36/270], loss=84.8222
	step [37/270], loss=64.5332
	step [38/270], loss=73.4912
	step [39/270], loss=68.2302
	step [40/270], loss=71.5629
	step [41/270], loss=77.6108
	step [42/270], loss=70.0729
	step [43/270], loss=76.3321
	step [44/270], loss=78.9613
	step [45/270], loss=92.6338
	step [46/270], loss=86.5750
	step [47/270], loss=86.1427
	step [48/270], loss=77.6210
	step [49/270], loss=81.8360
	step [50/270], loss=84.3964
	step [51/270], loss=91.1205
	step [52/270], loss=69.1485
	step [53/270], loss=85.3015
	step [54/270], loss=71.5534
	step [55/270], loss=77.9395
	step [56/270], loss=77.3292
	step [57/270], loss=90.3116
	step [58/270], loss=76.6790
	step [59/270], loss=73.8162
	step [60/270], loss=69.8914
	step [61/270], loss=67.3442
	step [62/270], loss=84.3565
	step [63/270], loss=66.5458
	step [64/270], loss=71.1547
	step [65/270], loss=69.9267
	step [66/270], loss=81.0062
	step [67/270], loss=84.0764
	step [68/270], loss=90.2377
	step [69/270], loss=68.0205
	step [70/270], loss=78.5638
	step [71/270], loss=73.2770
	step [72/270], loss=70.2343
	step [73/270], loss=79.8102
	step [74/270], loss=79.9281
	step [75/270], loss=72.5840
	step [76/270], loss=73.9177
	step [77/270], loss=76.7049
	step [78/270], loss=79.5775
	step [79/270], loss=83.7724
	step [80/270], loss=87.4295
	step [81/270], loss=89.2817
	step [82/270], loss=84.8276
	step [83/270], loss=79.1377
	step [84/270], loss=90.6784
	step [85/270], loss=68.0851
	step [86/270], loss=91.8150
	step [87/270], loss=60.7494
	step [88/270], loss=94.9969
	step [89/270], loss=80.9395
	step [90/270], loss=78.5005
	step [91/270], loss=75.6902
	step [92/270], loss=81.1210
	step [93/270], loss=73.9278
	step [94/270], loss=76.6071
	step [95/270], loss=78.7323
	step [96/270], loss=61.6626
	step [97/270], loss=77.3426
	step [98/270], loss=77.3446
	step [99/270], loss=65.4237
	step [100/270], loss=71.5282
	step [101/270], loss=71.4362
	step [102/270], loss=83.5481
	step [103/270], loss=72.0993
	step [104/270], loss=79.8470
	step [105/270], loss=84.9762
	step [106/270], loss=88.0831
	step [107/270], loss=71.8162
	step [108/270], loss=56.0634
	step [109/270], loss=71.9599
	step [110/270], loss=81.9968
	step [111/270], loss=91.0705
	step [112/270], loss=71.8377
	step [113/270], loss=83.2926
	step [114/270], loss=70.8292
	step [115/270], loss=80.6705
	step [116/270], loss=79.1585
	step [117/270], loss=86.2843
	step [118/270], loss=77.2596
	step [119/270], loss=73.5592
	step [120/270], loss=78.5678
	step [121/270], loss=73.2090
	step [122/270], loss=97.7716
	step [123/270], loss=82.6641
	step [124/270], loss=84.0305
	step [125/270], loss=68.5037
	step [126/270], loss=84.0958
	step [127/270], loss=86.7417
	step [128/270], loss=86.9268
	step [129/270], loss=86.8442
	step [130/270], loss=79.6553
	step [131/270], loss=88.7886
	step [132/270], loss=90.9763
	step [133/270], loss=87.9466
	step [134/270], loss=72.8679
	step [135/270], loss=60.5766
	step [136/270], loss=87.6610
	step [137/270], loss=77.8661
	step [138/270], loss=83.2301
	step [139/270], loss=73.1928
	step [140/270], loss=74.8224
	step [141/270], loss=69.5621
	step [142/270], loss=86.9625
	step [143/270], loss=88.6847
	step [144/270], loss=62.2059
	step [145/270], loss=96.1703
	step [146/270], loss=69.1896
	step [147/270], loss=66.6944
	step [148/270], loss=86.9090
	step [149/270], loss=74.3163
	step [150/270], loss=86.1658
	step [151/270], loss=79.3177
	step [152/270], loss=65.8855
	step [153/270], loss=84.2768
	step [154/270], loss=82.6525
	step [155/270], loss=79.0524
	step [156/270], loss=91.2957
	step [157/270], loss=62.8758
	step [158/270], loss=81.0374
	step [159/270], loss=78.8939
	step [160/270], loss=82.6420
	step [161/270], loss=77.0864
	step [162/270], loss=75.1325
	step [163/270], loss=67.6675
	step [164/270], loss=81.1492
	step [165/270], loss=87.2551
	step [166/270], loss=63.2237
	step [167/270], loss=77.3764
	step [168/270], loss=65.9023
	step [169/270], loss=76.7061
	step [170/270], loss=83.0083
	step [171/270], loss=99.2391
	step [172/270], loss=80.4882
	step [173/270], loss=96.0394
	step [174/270], loss=79.1939
	step [175/270], loss=70.5504
	step [176/270], loss=78.7724
	step [177/270], loss=77.5577
	step [178/270], loss=74.9851
	step [179/270], loss=85.8753
	step [180/270], loss=93.3519
	step [181/270], loss=81.7185
	step [182/270], loss=61.1135
	step [183/270], loss=74.1232
	step [184/270], loss=75.7032
	step [185/270], loss=60.3392
	step [186/270], loss=87.7279
	step [187/270], loss=81.2953
	step [188/270], loss=90.7383
	step [189/270], loss=74.5562
	step [190/270], loss=78.5472
	step [191/270], loss=87.4080
	step [192/270], loss=76.5622
	step [193/270], loss=78.9192
	step [194/270], loss=88.3190
	step [195/270], loss=95.9113
	step [196/270], loss=75.3850
	step [197/270], loss=86.9474
	step [198/270], loss=81.6306
	step [199/270], loss=70.1668
	step [200/270], loss=86.8329
	step [201/270], loss=68.6124
	step [202/270], loss=72.7807
	step [203/270], loss=57.6581
	step [204/270], loss=77.2654
	step [205/270], loss=72.9214
	step [206/270], loss=93.8476
	step [207/270], loss=69.5100
	step [208/270], loss=85.6843
	step [209/270], loss=76.1999
	step [210/270], loss=63.1233
	step [211/270], loss=77.1677
	step [212/270], loss=58.8853
	step [213/270], loss=80.1238
	step [214/270], loss=84.0658
	step [215/270], loss=88.7983
	step [216/270], loss=87.2165
	step [217/270], loss=79.3223
	step [218/270], loss=81.0417
	step [219/270], loss=71.5766
	step [220/270], loss=70.8736
	step [221/270], loss=88.1720
	step [222/270], loss=72.2350
	step [223/270], loss=79.8053
	step [224/270], loss=68.0185
	step [225/270], loss=75.0901
	step [226/270], loss=83.5083
	step [227/270], loss=70.7166
	step [228/270], loss=81.1336
	step [229/270], loss=74.0945
	step [230/270], loss=76.3343
	step [231/270], loss=77.8098
	step [232/270], loss=73.9541
	step [233/270], loss=80.1680
	step [234/270], loss=93.3377
	step [235/270], loss=67.0868
	step [236/270], loss=79.0313
	step [237/270], loss=81.9200
	step [238/270], loss=79.7563
	step [239/270], loss=89.3373
	step [240/270], loss=74.2063
	step [241/270], loss=78.6117
	step [242/270], loss=83.7352
	step [243/270], loss=77.8562
	step [244/270], loss=68.4789
	step [245/270], loss=67.6109
	step [246/270], loss=70.4074
	step [247/270], loss=85.4948
	step [248/270], loss=72.1048
	step [249/270], loss=95.4081
	step [250/270], loss=63.9014
	step [251/270], loss=79.6171
	step [252/270], loss=74.4575
	step [253/270], loss=88.7595
	step [254/270], loss=73.9717
	step [255/270], loss=82.5281
	step [256/270], loss=83.3551
	step [257/270], loss=75.6761
	step [258/270], loss=74.0729
	step [259/270], loss=84.5207
	step [260/270], loss=67.7631
	step [261/270], loss=84.3168
	step [262/270], loss=80.9020
	step [263/270], loss=68.7241
	step [264/270], loss=93.1596
	step [265/270], loss=79.4437
	step [266/270], loss=89.5709
	step [267/270], loss=83.1352
	step [268/270], loss=82.6588
	step [269/270], loss=82.4009
	step [270/270], loss=14.7833
	Evaluating
	loss=0.0085, precision=0.3054, recall=0.8963, f1=0.4556
Training epoch 75
	step [1/270], loss=74.3966
	step [2/270], loss=81.6879
	step [3/270], loss=74.3961
	step [4/270], loss=91.2491
	step [5/270], loss=84.7610
	step [6/270], loss=70.0851
	step [7/270], loss=75.8964
	step [8/270], loss=95.8878
	step [9/270], loss=66.7701
	step [10/270], loss=69.2477
	step [11/270], loss=84.6886
	step [12/270], loss=77.9125
	step [13/270], loss=71.3709
	step [14/270], loss=86.9145
	step [15/270], loss=82.0920
	step [16/270], loss=87.3237
	step [17/270], loss=59.0805
	step [18/270], loss=74.6716
	step [19/270], loss=76.3968
	step [20/270], loss=71.0336
	step [21/270], loss=73.2344
	step [22/270], loss=63.9243
	step [23/270], loss=81.2672
	step [24/270], loss=87.0677
	step [25/270], loss=94.6087
	step [26/270], loss=82.6375
	step [27/270], loss=70.5810
	step [28/270], loss=81.4439
	step [29/270], loss=83.6324
	step [30/270], loss=91.7613
	step [31/270], loss=77.3281
	step [32/270], loss=75.9152
	step [33/270], loss=57.5712
	step [34/270], loss=74.5760
	step [35/270], loss=65.8660
	step [36/270], loss=88.9911
	step [37/270], loss=78.9851
	step [38/270], loss=77.5655
	step [39/270], loss=97.3008
	step [40/270], loss=78.9004
	step [41/270], loss=73.0145
	step [42/270], loss=72.5134
	step [43/270], loss=83.2553
	step [44/270], loss=88.5223
	step [45/270], loss=81.2181
	step [46/270], loss=78.3159
	step [47/270], loss=86.0018
	step [48/270], loss=68.6296
	step [49/270], loss=73.3361
	step [50/270], loss=74.6886
	step [51/270], loss=73.2955
	step [52/270], loss=75.2386
	step [53/270], loss=89.0913
	step [54/270], loss=71.0515
	step [55/270], loss=82.7809
	step [56/270], loss=78.4220
	step [57/270], loss=69.3893
	step [58/270], loss=78.1394
	step [59/270], loss=75.7268
	step [60/270], loss=87.6752
	step [61/270], loss=69.5897
	step [62/270], loss=75.4076
	step [63/270], loss=83.3653
	step [64/270], loss=75.9941
	step [65/270], loss=74.3136
	step [66/270], loss=67.0938
	step [67/270], loss=76.9032
	step [68/270], loss=74.0983
	step [69/270], loss=76.3809
	step [70/270], loss=77.5922
	step [71/270], loss=100.1403
	step [72/270], loss=78.2717
	step [73/270], loss=75.4230
	step [74/270], loss=81.8605
	step [75/270], loss=83.0697
	step [76/270], loss=79.3220
	step [77/270], loss=101.7933
	step [78/270], loss=84.4129
	step [79/270], loss=68.8208
	step [80/270], loss=81.0337
	step [81/270], loss=95.3908
	step [82/270], loss=74.2509
	step [83/270], loss=73.6882
	step [84/270], loss=73.7318
	step [85/270], loss=78.3705
	step [86/270], loss=98.0491
	step [87/270], loss=70.8501
	step [88/270], loss=77.3214
	step [89/270], loss=73.9084
	step [90/270], loss=74.8477
	step [91/270], loss=109.4164
	step [92/270], loss=64.9842
	step [93/270], loss=68.7820
	step [94/270], loss=76.7763
	step [95/270], loss=93.2409
	step [96/270], loss=72.3698
	step [97/270], loss=74.8220
	step [98/270], loss=75.2111
	step [99/270], loss=76.8236
	step [100/270], loss=68.3106
	step [101/270], loss=79.8994
	step [102/270], loss=74.2700
	step [103/270], loss=64.8904
	step [104/270], loss=87.6554
	step [105/270], loss=69.3375
	step [106/270], loss=75.6508
	step [107/270], loss=69.2487
	step [108/270], loss=88.8892
	step [109/270], loss=89.3899
	step [110/270], loss=72.2979
	step [111/270], loss=79.5976
	step [112/270], loss=87.4880
	step [113/270], loss=60.4389
	step [114/270], loss=85.1379
	step [115/270], loss=74.9551
	step [116/270], loss=78.1562
	step [117/270], loss=71.5409
	step [118/270], loss=75.7993
	step [119/270], loss=75.4436
	step [120/270], loss=69.8670
	step [121/270], loss=70.2361
	step [122/270], loss=72.6975
	step [123/270], loss=72.1481
	step [124/270], loss=71.9349
	step [125/270], loss=77.0451
	step [126/270], loss=66.5784
	step [127/270], loss=78.3888
	step [128/270], loss=83.6368
	step [129/270], loss=80.1698
	step [130/270], loss=81.6444
	step [131/270], loss=89.7597
	step [132/270], loss=73.2274
	step [133/270], loss=68.8524
	step [134/270], loss=71.5806
	step [135/270], loss=80.2609
	step [136/270], loss=82.2379
	step [137/270], loss=60.3195
	step [138/270], loss=80.1427
	step [139/270], loss=75.3318
	step [140/270], loss=86.4426
	step [141/270], loss=93.7538
	step [142/270], loss=79.8502
	step [143/270], loss=69.2699
	step [144/270], loss=76.7267
	step [145/270], loss=93.9747
	step [146/270], loss=80.7586
	step [147/270], loss=74.5122
	step [148/270], loss=63.3606
	step [149/270], loss=90.7191
	step [150/270], loss=77.6245
	step [151/270], loss=75.5455
	step [152/270], loss=77.4911
	step [153/270], loss=80.3427
	step [154/270], loss=68.2551
	step [155/270], loss=66.9021
	step [156/270], loss=76.4106
	step [157/270], loss=79.6956
	step [158/270], loss=99.6971
	step [159/270], loss=85.3953
	step [160/270], loss=80.6001
	step [161/270], loss=85.6420
	step [162/270], loss=76.0590
	step [163/270], loss=77.1411
	step [164/270], loss=87.9641
	step [165/270], loss=64.9071
	step [166/270], loss=68.6648
	step [167/270], loss=79.3399
	step [168/270], loss=81.0116
	step [169/270], loss=78.0369
	step [170/270], loss=73.5291
	step [171/270], loss=89.1652
	step [172/270], loss=75.7849
	step [173/270], loss=80.3505
	step [174/270], loss=67.6974
	step [175/270], loss=85.7092
	step [176/270], loss=97.4374
	step [177/270], loss=83.7043
	step [178/270], loss=88.5048
	step [179/270], loss=60.1250
	step [180/270], loss=81.8170
	step [181/270], loss=69.6455
	step [182/270], loss=76.0004
	step [183/270], loss=78.9997
	step [184/270], loss=83.2574
	step [185/270], loss=63.7668
	step [186/270], loss=69.1257
	step [187/270], loss=87.3905
	step [188/270], loss=83.1653
	step [189/270], loss=62.7788
	step [190/270], loss=90.2313
	step [191/270], loss=62.9186
	step [192/270], loss=67.4900
	step [193/270], loss=72.5767
	step [194/270], loss=80.6468
	step [195/270], loss=93.3804
	step [196/270], loss=85.3539
	step [197/270], loss=62.2016
	step [198/270], loss=86.7003
	step [199/270], loss=75.6439
	step [200/270], loss=88.5873
	step [201/270], loss=74.4305
	step [202/270], loss=79.6370
	step [203/270], loss=73.8019
	step [204/270], loss=91.1732
	step [205/270], loss=72.5947
	step [206/270], loss=84.8266
	step [207/270], loss=71.3024
	step [208/270], loss=77.5722
	step [209/270], loss=83.3203
	step [210/270], loss=75.2753
	step [211/270], loss=84.1196
	step [212/270], loss=76.3030
	step [213/270], loss=88.7428
	step [214/270], loss=72.4982
	step [215/270], loss=63.6137
	step [216/270], loss=73.3233
	step [217/270], loss=90.4837
	step [218/270], loss=80.6265
	step [219/270], loss=72.9219
	step [220/270], loss=75.6556
	step [221/270], loss=77.8094
	step [222/270], loss=88.3692
	step [223/270], loss=88.9268
	step [224/270], loss=74.1324
	step [225/270], loss=89.0042
	step [226/270], loss=76.3416
	step [227/270], loss=71.1505
	step [228/270], loss=73.7814
	step [229/270], loss=78.2115
	step [230/270], loss=70.2995
	step [231/270], loss=84.5087
	step [232/270], loss=70.1903
	step [233/270], loss=69.8875
	step [234/270], loss=73.6374
	step [235/270], loss=76.1112
	step [236/270], loss=73.4538
	step [237/270], loss=68.3170
	step [238/270], loss=72.4263
	step [239/270], loss=68.0546
	step [240/270], loss=72.5877
	step [241/270], loss=76.5825
	step [242/270], loss=82.3420
	step [243/270], loss=79.4173
	step [244/270], loss=78.9595
	step [245/270], loss=69.3881
	step [246/270], loss=78.1720
	step [247/270], loss=63.0938
	step [248/270], loss=83.0673
	step [249/270], loss=86.4457
	step [250/270], loss=92.5017
	step [251/270], loss=78.8816
	step [252/270], loss=59.9731
	step [253/270], loss=68.4644
	step [254/270], loss=87.8231
	step [255/270], loss=63.5706
	step [256/270], loss=81.9845
	step [257/270], loss=92.6980
	step [258/270], loss=65.1843
	step [259/270], loss=63.8817
	step [260/270], loss=87.8440
	step [261/270], loss=59.3807
	step [262/270], loss=89.6145
	step [263/270], loss=72.6718
	step [264/270], loss=86.6674
	step [265/270], loss=76.1877
	step [266/270], loss=98.6828
	step [267/270], loss=86.2558
	step [268/270], loss=74.1032
	step [269/270], loss=90.9149
	step [270/270], loss=14.3455
	Evaluating
	loss=0.0076, precision=0.3325, recall=0.8927, f1=0.4845
Training epoch 76
	step [1/270], loss=85.7354
	step [2/270], loss=84.4754
	step [3/270], loss=94.0910
	step [4/270], loss=83.4869
	step [5/270], loss=74.7691
	step [6/270], loss=64.9187
	step [7/270], loss=77.6271
	step [8/270], loss=65.3038
	step [9/270], loss=84.7146
	step [10/270], loss=82.7867
	step [11/270], loss=80.0394
	step [12/270], loss=93.7950
	step [13/270], loss=79.4882
	step [14/270], loss=73.3440
	step [15/270], loss=89.1970
	step [16/270], loss=73.1836
	step [17/270], loss=79.0445
	step [18/270], loss=83.0390
	step [19/270], loss=71.7959
	step [20/270], loss=89.5684
	step [21/270], loss=82.2387
	step [22/270], loss=65.2600
	step [23/270], loss=68.6134
	step [24/270], loss=78.3938
	step [25/270], loss=91.7981
	step [26/270], loss=88.2823
	step [27/270], loss=85.7394
	step [28/270], loss=77.1134
	step [29/270], loss=78.1748
	step [30/270], loss=85.6600
	step [31/270], loss=72.7704
	step [32/270], loss=75.0678
	step [33/270], loss=76.7241
	step [34/270], loss=80.5643
	step [35/270], loss=78.4000
	step [36/270], loss=63.0078
	step [37/270], loss=82.8306
	step [38/270], loss=80.1980
	step [39/270], loss=83.0275
	step [40/270], loss=70.3030
	step [41/270], loss=82.4868
	step [42/270], loss=81.4537
	step [43/270], loss=87.8927
	step [44/270], loss=60.5647
	step [45/270], loss=69.1294
	step [46/270], loss=75.9255
	step [47/270], loss=88.3448
	step [48/270], loss=84.2758
	step [49/270], loss=82.1484
	step [50/270], loss=74.7705
	step [51/270], loss=81.7477
	step [52/270], loss=82.1370
	step [53/270], loss=88.8583
	step [54/270], loss=77.1143
	step [55/270], loss=88.9217
	step [56/270], loss=80.5143
	step [57/270], loss=79.2018
	step [58/270], loss=92.5258
	step [59/270], loss=75.4140
	step [60/270], loss=85.1272
	step [61/270], loss=62.3670
	step [62/270], loss=74.3730
	step [63/270], loss=81.7484
	step [64/270], loss=72.1407
	step [65/270], loss=76.5969
	step [66/270], loss=74.1913
	step [67/270], loss=72.3646
	step [68/270], loss=81.5077
	step [69/270], loss=76.1820
	step [70/270], loss=89.1628
	step [71/270], loss=89.1258
	step [72/270], loss=69.9648
	step [73/270], loss=77.6680
	step [74/270], loss=77.5710
	step [75/270], loss=74.9993
	step [76/270], loss=72.2310
	step [77/270], loss=66.4672
	step [78/270], loss=76.8071
	step [79/270], loss=76.0172
	step [80/270], loss=90.8016
	step [81/270], loss=66.4139
	step [82/270], loss=73.9834
	step [83/270], loss=61.0791
	step [84/270], loss=70.9958
	step [85/270], loss=77.8947
	step [86/270], loss=56.6140
	step [87/270], loss=69.9459
	step [88/270], loss=80.1960
	step [89/270], loss=75.0913
	step [90/270], loss=67.5699
	step [91/270], loss=80.4872
	step [92/270], loss=87.6245
	step [93/270], loss=80.6026
	step [94/270], loss=82.7871
	step [95/270], loss=67.2064
	step [96/270], loss=83.2492
	step [97/270], loss=65.4917
	step [98/270], loss=75.3640
	step [99/270], loss=89.0248
	step [100/270], loss=75.8215
	step [101/270], loss=59.2353
	step [102/270], loss=73.7802
	step [103/270], loss=71.2046
	step [104/270], loss=74.5423
	step [105/270], loss=80.5087
	step [106/270], loss=68.5258
	step [107/270], loss=65.4422
	step [108/270], loss=97.7749
	step [109/270], loss=86.2428
	step [110/270], loss=85.7095
	step [111/270], loss=77.7460
	step [112/270], loss=74.7862
	step [113/270], loss=81.9227
	step [114/270], loss=82.2402
	step [115/270], loss=67.7117
	step [116/270], loss=85.3353
	step [117/270], loss=92.6711
	step [118/270], loss=82.1955
	step [119/270], loss=80.7011
	step [120/270], loss=82.2254
	step [121/270], loss=75.3791
	step [122/270], loss=96.0634
	step [123/270], loss=73.2069
	step [124/270], loss=67.1646
	step [125/270], loss=86.7994
	step [126/270], loss=79.4128
	step [127/270], loss=94.2327
	step [128/270], loss=59.9310
	step [129/270], loss=73.3317
	step [130/270], loss=92.3096
	step [131/270], loss=60.1816
	step [132/270], loss=67.9324
	step [133/270], loss=74.2696
	step [134/270], loss=87.9229
	step [135/270], loss=89.4423
	step [136/270], loss=83.3523
	step [137/270], loss=79.7454
	step [138/270], loss=54.8490
	step [139/270], loss=91.5951
	step [140/270], loss=58.7066
	step [141/270], loss=92.0214
	step [142/270], loss=74.9282
	step [143/270], loss=65.2256
	step [144/270], loss=96.0750
	step [145/270], loss=77.1102
	step [146/270], loss=84.6309
	step [147/270], loss=84.7435
	step [148/270], loss=94.1842
	step [149/270], loss=67.6265
	step [150/270], loss=60.9755
	step [151/270], loss=69.7291
	step [152/270], loss=74.1263
	step [153/270], loss=64.2933
	step [154/270], loss=72.8044
	step [155/270], loss=76.8451
	step [156/270], loss=90.0196
	step [157/270], loss=90.9207
	step [158/270], loss=78.6638
	step [159/270], loss=78.2374
	step [160/270], loss=79.6439
	step [161/270], loss=85.4968
	step [162/270], loss=58.9642
	step [163/270], loss=89.9436
	step [164/270], loss=73.2130
	step [165/270], loss=79.8405
	step [166/270], loss=85.1906
	step [167/270], loss=90.4379
	step [168/270], loss=84.6717
	step [169/270], loss=67.6319
	step [170/270], loss=78.1985
	step [171/270], loss=79.6419
	step [172/270], loss=83.0869
	step [173/270], loss=61.2016
	step [174/270], loss=74.1034
	step [175/270], loss=94.6656
	step [176/270], loss=81.1336
	step [177/270], loss=63.8292
	step [178/270], loss=73.8182
	step [179/270], loss=76.6066
	step [180/270], loss=73.7845
	step [181/270], loss=72.8373
	step [182/270], loss=89.0509
	step [183/270], loss=63.0180
	step [184/270], loss=92.1229
	step [185/270], loss=74.5032
	step [186/270], loss=74.3288
	step [187/270], loss=76.9653
	step [188/270], loss=65.1430
	step [189/270], loss=79.9924
	step [190/270], loss=62.4201
	step [191/270], loss=83.7748
	step [192/270], loss=90.1471
	step [193/270], loss=79.1903
	step [194/270], loss=70.2111
	step [195/270], loss=68.8527
	step [196/270], loss=82.4631
	step [197/270], loss=71.9767
	step [198/270], loss=74.1254
	step [199/270], loss=92.2562
	step [200/270], loss=73.2795
	step [201/270], loss=84.8923
	step [202/270], loss=77.5128
	step [203/270], loss=65.2140
	step [204/270], loss=77.6657
	step [205/270], loss=68.6769
	step [206/270], loss=78.8700
	step [207/270], loss=79.1163
	step [208/270], loss=73.0010
	step [209/270], loss=68.9436
	step [210/270], loss=85.8266
	step [211/270], loss=89.0394
	step [212/270], loss=82.5332
	step [213/270], loss=77.7217
	step [214/270], loss=84.2095
	step [215/270], loss=82.5286
	step [216/270], loss=68.7108
	step [217/270], loss=72.1824
	step [218/270], loss=78.6740
	step [219/270], loss=89.8739
	step [220/270], loss=68.0417
	step [221/270], loss=73.9395
	step [222/270], loss=74.7543
	step [223/270], loss=70.4421
	step [224/270], loss=81.0317
	step [225/270], loss=73.0092
	step [226/270], loss=73.4925
	step [227/270], loss=81.6587
	step [228/270], loss=88.9229
	step [229/270], loss=93.5971
	step [230/270], loss=83.5402
	step [231/270], loss=80.5499
	step [232/270], loss=68.7962
	step [233/270], loss=76.8136
	step [234/270], loss=87.8114
	step [235/270], loss=78.8425
	step [236/270], loss=77.0285
	step [237/270], loss=66.8289
	step [238/270], loss=80.7882
	step [239/270], loss=81.0571
	step [240/270], loss=74.5940
	step [241/270], loss=82.3262
	step [242/270], loss=68.4528
	step [243/270], loss=80.7894
	step [244/270], loss=82.0835
	step [245/270], loss=82.3974
	step [246/270], loss=77.5324
	step [247/270], loss=73.4683
	step [248/270], loss=78.0401
	step [249/270], loss=118.3873
	step [250/270], loss=88.1259
	step [251/270], loss=67.5295
	step [252/270], loss=81.3403
	step [253/270], loss=81.4494
	step [254/270], loss=76.0403
	step [255/270], loss=82.3597
	step [256/270], loss=66.0057
	step [257/270], loss=70.2115
	step [258/270], loss=67.4932
	step [259/270], loss=91.6386
	step [260/270], loss=78.5120
	step [261/270], loss=80.2669
	step [262/270], loss=63.4379
	step [263/270], loss=80.3741
	step [264/270], loss=69.0617
	step [265/270], loss=87.3836
	step [266/270], loss=67.8082
	step [267/270], loss=74.8364
	step [268/270], loss=79.4590
	step [269/270], loss=71.4865
	step [270/270], loss=9.2957
	Evaluating
	loss=0.0080, precision=0.3334, recall=0.9033, f1=0.4870
Training epoch 77
	step [1/270], loss=90.7044
	step [2/270], loss=83.6819
	step [3/270], loss=71.2905
	step [4/270], loss=77.7092
	step [5/270], loss=93.9579
	step [6/270], loss=75.5844
	step [7/270], loss=83.7665
	step [8/270], loss=72.6468
	step [9/270], loss=82.0150
	step [10/270], loss=77.8074
	step [11/270], loss=85.4709
	step [12/270], loss=81.1628
	step [13/270], loss=76.6983
	step [14/270], loss=71.9806
	step [15/270], loss=74.1148
	step [16/270], loss=73.8837
	step [17/270], loss=66.6690
	step [18/270], loss=71.5275
	step [19/270], loss=69.2275
	step [20/270], loss=58.6534
	step [21/270], loss=70.1219
	step [22/270], loss=87.5909
	step [23/270], loss=82.1126
	step [24/270], loss=90.1433
	step [25/270], loss=82.4408
	step [26/270], loss=74.5352
	step [27/270], loss=84.9714
	step [28/270], loss=51.8626
	step [29/270], loss=64.2243
	step [30/270], loss=72.8532
	step [31/270], loss=80.4338
	step [32/270], loss=85.3768
	step [33/270], loss=76.5397
	step [34/270], loss=85.0125
	step [35/270], loss=75.7177
	step [36/270], loss=78.5388
	step [37/270], loss=72.6513
	step [38/270], loss=76.4827
	step [39/270], loss=67.1804
	step [40/270], loss=74.7602
	step [41/270], loss=75.3759
	step [42/270], loss=92.3606
	step [43/270], loss=69.4721
	step [44/270], loss=74.6954
	step [45/270], loss=72.3131
	step [46/270], loss=76.9892
	step [47/270], loss=80.0192
	step [48/270], loss=85.1400
	step [49/270], loss=77.0602
	step [50/270], loss=80.1642
	step [51/270], loss=76.8579
	step [52/270], loss=78.6112
	step [53/270], loss=66.6380
	step [54/270], loss=82.0377
	step [55/270], loss=77.8367
	step [56/270], loss=76.2954
	step [57/270], loss=64.0920
	step [58/270], loss=86.1042
	step [59/270], loss=63.9077
	step [60/270], loss=81.8969
	step [61/270], loss=82.1341
	step [62/270], loss=67.0688
	step [63/270], loss=83.9023
	step [64/270], loss=78.4578
	step [65/270], loss=78.7458
	step [66/270], loss=84.0333
	step [67/270], loss=85.8672
	step [68/270], loss=71.3769
	step [69/270], loss=58.8475
	step [70/270], loss=66.7184
	step [71/270], loss=66.8121
	step [72/270], loss=74.0218
	step [73/270], loss=70.3181
	step [74/270], loss=63.4295
	step [75/270], loss=75.0628
	step [76/270], loss=83.1836
	step [77/270], loss=67.6050
	step [78/270], loss=81.1748
	step [79/270], loss=68.7813
	step [80/270], loss=78.3302
	step [81/270], loss=80.8787
	step [82/270], loss=79.4187
	step [83/270], loss=67.4672
	step [84/270], loss=87.8655
	step [85/270], loss=77.5303
	step [86/270], loss=87.2245
	step [87/270], loss=91.6211
	step [88/270], loss=76.4768
	step [89/270], loss=76.8965
	step [90/270], loss=70.2339
	step [91/270], loss=66.3886
	step [92/270], loss=75.7695
	step [93/270], loss=73.7788
	step [94/270], loss=85.8069
	step [95/270], loss=79.9496
	step [96/270], loss=63.4768
	step [97/270], loss=78.9671
	step [98/270], loss=85.9157
	step [99/270], loss=66.1464
	step [100/270], loss=81.1024
	step [101/270], loss=65.0849
	step [102/270], loss=99.1402
	step [103/270], loss=81.6346
	step [104/270], loss=68.2861
	step [105/270], loss=64.6877
	step [106/270], loss=84.9935
	step [107/270], loss=73.5213
	step [108/270], loss=68.6062
	step [109/270], loss=71.2091
	step [110/270], loss=71.0554
	step [111/270], loss=69.4841
	step [112/270], loss=71.5293
	step [113/270], loss=77.3837
	step [114/270], loss=77.0495
	step [115/270], loss=88.2722
	step [116/270], loss=82.6935
	step [117/270], loss=83.1699
	step [118/270], loss=74.7838
	step [119/270], loss=72.7382
	step [120/270], loss=82.9521
	step [121/270], loss=70.7668
	step [122/270], loss=81.8854
	step [123/270], loss=79.2914
	step [124/270], loss=76.8054
	step [125/270], loss=75.2701
	step [126/270], loss=85.9702
	step [127/270], loss=69.6078
	step [128/270], loss=77.5662
	step [129/270], loss=76.4685
	step [130/270], loss=85.0083
	step [131/270], loss=82.5261
	step [132/270], loss=79.8401
	step [133/270], loss=91.8406
	step [134/270], loss=98.7328
	step [135/270], loss=77.1331
	step [136/270], loss=72.5070
	step [137/270], loss=89.4348
	step [138/270], loss=80.1982
	step [139/270], loss=78.9027
	step [140/270], loss=90.7846
	step [141/270], loss=85.1404
	step [142/270], loss=83.5045
	step [143/270], loss=68.1633
	step [144/270], loss=86.4444
	step [145/270], loss=78.1715
	step [146/270], loss=72.1665
	step [147/270], loss=75.4028
	step [148/270], loss=81.3792
	step [149/270], loss=68.9785
	step [150/270], loss=76.2208
	step [151/270], loss=59.8987
	step [152/270], loss=70.6647
	step [153/270], loss=84.4538
	step [154/270], loss=76.3131
	step [155/270], loss=100.6605
	step [156/270], loss=73.9530
	step [157/270], loss=87.3855
	step [158/270], loss=89.7691
	step [159/270], loss=64.6663
	step [160/270], loss=55.2436
	step [161/270], loss=69.8119
	step [162/270], loss=81.2345
	step [163/270], loss=77.3395
	step [164/270], loss=76.6951
	step [165/270], loss=71.3550
	step [166/270], loss=93.3382
	step [167/270], loss=87.0145
	step [168/270], loss=75.5819
	step [169/270], loss=101.7914
	step [170/270], loss=82.3242
	step [171/270], loss=94.0385
	step [172/270], loss=62.7717
	step [173/270], loss=77.6291
	step [174/270], loss=84.3457
	step [175/270], loss=84.0959
	step [176/270], loss=64.5288
	step [177/270], loss=69.8431
	step [178/270], loss=88.8362
	step [179/270], loss=83.5185
	step [180/270], loss=72.5339
	step [181/270], loss=68.5136
	step [182/270], loss=67.5641
	step [183/270], loss=80.8549
	step [184/270], loss=71.9273
	step [185/270], loss=84.4769
	step [186/270], loss=72.9567
	step [187/270], loss=77.6740
	step [188/270], loss=85.5576
	step [189/270], loss=66.5573
	step [190/270], loss=70.7995
	step [191/270], loss=82.8113
	step [192/270], loss=90.3361
	step [193/270], loss=78.9513
	step [194/270], loss=74.4179
	step [195/270], loss=75.6547
	step [196/270], loss=62.6254
	step [197/270], loss=66.0889
	step [198/270], loss=61.3907
	step [199/270], loss=90.7905
	step [200/270], loss=62.0113
	step [201/270], loss=90.1545
	step [202/270], loss=77.7916
	step [203/270], loss=86.0803
	step [204/270], loss=78.0803
	step [205/270], loss=71.8651
	step [206/270], loss=84.7042
	step [207/270], loss=72.6687
	step [208/270], loss=90.7484
	step [209/270], loss=80.0135
	step [210/270], loss=69.3591
	step [211/270], loss=80.4316
	step [212/270], loss=78.7478
	step [213/270], loss=67.8735
	step [214/270], loss=83.0986
	step [215/270], loss=63.5731
	step [216/270], loss=80.4477
	step [217/270], loss=67.9158
	step [218/270], loss=82.8021
	step [219/270], loss=69.0005
	step [220/270], loss=84.4960
	step [221/270], loss=61.2299
	step [222/270], loss=72.0718
	step [223/270], loss=95.9711
	step [224/270], loss=79.6727
	step [225/270], loss=79.1120
	step [226/270], loss=58.3400
	step [227/270], loss=109.2038
	step [228/270], loss=85.0560
	step [229/270], loss=94.3289
	step [230/270], loss=71.4614
	step [231/270], loss=84.9916
	step [232/270], loss=75.4932
	step [233/270], loss=85.3871
	step [234/270], loss=79.2062
	step [235/270], loss=97.6859
	step [236/270], loss=75.3976
	step [237/270], loss=81.6230
	step [238/270], loss=72.0231
	step [239/270], loss=88.6813
	step [240/270], loss=69.4109
	step [241/270], loss=77.3129
	step [242/270], loss=67.4510
	step [243/270], loss=87.1512
	step [244/270], loss=67.9268
	step [245/270], loss=91.8037
	step [246/270], loss=72.9346
	step [247/270], loss=76.2715
	step [248/270], loss=74.3208
	step [249/270], loss=82.7372
	step [250/270], loss=77.2126
	step [251/270], loss=82.0182
	step [252/270], loss=77.1070
	step [253/270], loss=89.6823
	step [254/270], loss=87.0043
	step [255/270], loss=82.2736
	step [256/270], loss=69.9247
	step [257/270], loss=81.1197
	step [258/270], loss=79.1378
	step [259/270], loss=81.4613
	step [260/270], loss=92.6021
	step [261/270], loss=75.3813
	step [262/270], loss=70.9787
	step [263/270], loss=75.9635
	step [264/270], loss=75.6844
	step [265/270], loss=84.1528
	step [266/270], loss=68.7091
	step [267/270], loss=73.2993
	step [268/270], loss=84.2179
	step [269/270], loss=72.6327
	step [270/270], loss=19.6038
	Evaluating
	loss=0.0083, precision=0.3069, recall=0.9020, f1=0.4580
Training epoch 78
	step [1/270], loss=86.7327
	step [2/270], loss=87.5792
	step [3/270], loss=84.1020
	step [4/270], loss=60.6904
	step [5/270], loss=71.5572
	step [6/270], loss=71.3296
	step [7/270], loss=82.7476
	step [8/270], loss=62.0321
	step [9/270], loss=71.1865
	step [10/270], loss=89.4040
	step [11/270], loss=72.1439
	step [12/270], loss=71.2347
	step [13/270], loss=65.4161
	step [14/270], loss=63.9047
	step [15/270], loss=82.4530
	step [16/270], loss=66.9118
	step [17/270], loss=76.9673
	step [18/270], loss=74.1099
	step [19/270], loss=82.2319
	step [20/270], loss=80.5497
	step [21/270], loss=89.6871
	step [22/270], loss=68.2850
	step [23/270], loss=84.9094
	step [24/270], loss=83.9164
	step [25/270], loss=63.0430
	step [26/270], loss=80.4091
	step [27/270], loss=74.9992
	step [28/270], loss=86.5795
	step [29/270], loss=70.8131
	step [30/270], loss=79.7244
	step [31/270], loss=79.5490
	step [32/270], loss=79.5032
	step [33/270], loss=65.6447
	step [34/270], loss=76.2481
	step [35/270], loss=80.8535
	step [36/270], loss=77.3705
	step [37/270], loss=84.8743
	step [38/270], loss=76.3194
	step [39/270], loss=69.8256
	step [40/270], loss=87.2818
	step [41/270], loss=65.7514
	step [42/270], loss=79.6792
	step [43/270], loss=68.8859
	step [44/270], loss=79.7104
	step [45/270], loss=65.4790
	step [46/270], loss=74.6858
	step [47/270], loss=78.9381
	step [48/270], loss=67.0609
	step [49/270], loss=76.6758
	step [50/270], loss=68.4697
	step [51/270], loss=74.3019
	step [52/270], loss=75.9349
	step [53/270], loss=81.9761
	step [54/270], loss=72.3454
	step [55/270], loss=79.5366
	step [56/270], loss=71.1856
	step [57/270], loss=71.8317
	step [58/270], loss=63.9334
	step [59/270], loss=79.7117
	step [60/270], loss=86.5601
	step [61/270], loss=61.8358
	step [62/270], loss=60.5279
	step [63/270], loss=72.0663
	step [64/270], loss=80.3175
	step [65/270], loss=80.6563
	step [66/270], loss=71.4553
	step [67/270], loss=96.4660
	step [68/270], loss=82.0859
	step [69/270], loss=83.1792
	step [70/270], loss=73.8579
	step [71/270], loss=79.9645
	step [72/270], loss=69.7934
	step [73/270], loss=76.7743
	step [74/270], loss=80.4652
	step [75/270], loss=82.4833
	step [76/270], loss=76.0227
	step [77/270], loss=78.4675
	step [78/270], loss=76.1324
	step [79/270], loss=73.0969
	step [80/270], loss=85.4989
	step [81/270], loss=79.2420
	step [82/270], loss=70.8770
	step [83/270], loss=72.2162
	step [84/270], loss=83.5414
	step [85/270], loss=58.0736
	step [86/270], loss=80.4201
	step [87/270], loss=80.9903
	step [88/270], loss=81.6435
	step [89/270], loss=63.1107
	step [90/270], loss=81.2214
	step [91/270], loss=77.6198
	step [92/270], loss=91.8461
	step [93/270], loss=81.9415
	step [94/270], loss=69.4601
	step [95/270], loss=78.9712
	step [96/270], loss=70.9195
	step [97/270], loss=83.5294
	step [98/270], loss=86.7257
	step [99/270], loss=86.6500
	step [100/270], loss=69.6527
	step [101/270], loss=71.1539
	step [102/270], loss=82.8395
	step [103/270], loss=79.6052
	step [104/270], loss=72.0746
	step [105/270], loss=74.4998
	step [106/270], loss=79.9012
	step [107/270], loss=80.0338
	step [108/270], loss=76.1506
	step [109/270], loss=71.8529
	step [110/270], loss=68.8986
	step [111/270], loss=81.6617
	step [112/270], loss=79.9529
	step [113/270], loss=77.1682
	step [114/270], loss=56.1922
	step [115/270], loss=78.1522
	step [116/270], loss=61.9839
	step [117/270], loss=97.8219
	step [118/270], loss=77.0306
	step [119/270], loss=85.0295
	step [120/270], loss=79.0842
	step [121/270], loss=84.0665
	step [122/270], loss=89.5300
	step [123/270], loss=81.3487
	step [124/270], loss=93.7697
	step [125/270], loss=76.1653
	step [126/270], loss=82.5623
	step [127/270], loss=72.9330
	step [128/270], loss=77.1031
	step [129/270], loss=78.7501
	step [130/270], loss=72.6975
	step [131/270], loss=79.7776
	step [132/270], loss=80.4740
	step [133/270], loss=90.9735
	step [134/270], loss=75.3885
	step [135/270], loss=69.0633
	step [136/270], loss=79.1429
	step [137/270], loss=71.1869
	step [138/270], loss=81.9545
	step [139/270], loss=75.0385
	step [140/270], loss=71.5153
	step [141/270], loss=78.0026
	step [142/270], loss=67.7712
	step [143/270], loss=66.0234
	step [144/270], loss=71.3842
	step [145/270], loss=71.1626
	step [146/270], loss=84.5760
	step [147/270], loss=66.7611
	step [148/270], loss=81.1928
	step [149/270], loss=79.0452
	step [150/270], loss=63.6735
	step [151/270], loss=86.3967
	step [152/270], loss=75.9440
	step [153/270], loss=81.8593
	step [154/270], loss=81.3568
	step [155/270], loss=71.5485
	step [156/270], loss=86.6529
	step [157/270], loss=95.0202
	step [158/270], loss=82.4760
	step [159/270], loss=87.5424
	step [160/270], loss=76.7578
	step [161/270], loss=87.9470
	step [162/270], loss=71.1339
	step [163/270], loss=82.2852
	step [164/270], loss=78.1575
	step [165/270], loss=76.6336
	step [166/270], loss=86.5988
	step [167/270], loss=68.5201
	step [168/270], loss=84.8497
	step [169/270], loss=70.4107
	step [170/270], loss=70.9980
	step [171/270], loss=75.7447
	step [172/270], loss=76.0311
	step [173/270], loss=65.3144
	step [174/270], loss=80.3192
	step [175/270], loss=70.3891
	step [176/270], loss=63.2286
	step [177/270], loss=87.0107
	step [178/270], loss=68.6628
	step [179/270], loss=72.6991
	step [180/270], loss=68.9606
	step [181/270], loss=70.9997
	step [182/270], loss=79.5818
	step [183/270], loss=91.8302
	step [184/270], loss=70.9518
	step [185/270], loss=78.7971
	step [186/270], loss=96.6632
	step [187/270], loss=74.5038
	step [188/270], loss=89.5454
	step [189/270], loss=77.2819
	step [190/270], loss=75.7789
	step [191/270], loss=65.1916
	step [192/270], loss=78.7456
	step [193/270], loss=76.2630
	step [194/270], loss=78.1878
	step [195/270], loss=75.0966
	step [196/270], loss=91.7657
	step [197/270], loss=73.4079
	step [198/270], loss=63.2051
	step [199/270], loss=95.1178
	step [200/270], loss=66.3501
	step [201/270], loss=77.8010
	step [202/270], loss=68.6570
	step [203/270], loss=71.6570
	step [204/270], loss=83.7847
	step [205/270], loss=76.9828
	step [206/270], loss=69.6749
	step [207/270], loss=69.4103
	step [208/270], loss=83.1600
	step [209/270], loss=67.4293
	step [210/270], loss=70.5079
	step [211/270], loss=79.4854
	step [212/270], loss=71.8377
	step [213/270], loss=71.8187
	step [214/270], loss=67.7905
	step [215/270], loss=69.6699
	step [216/270], loss=80.3242
	step [217/270], loss=85.3066
	step [218/270], loss=82.7662
	step [219/270], loss=78.7978
	step [220/270], loss=73.9223
	step [221/270], loss=75.6376
	step [222/270], loss=63.5522
	step [223/270], loss=92.7228
	step [224/270], loss=76.8232
	step [225/270], loss=77.1587
	step [226/270], loss=100.2020
	step [227/270], loss=81.9998
	step [228/270], loss=84.6790
	step [229/270], loss=99.8043
	step [230/270], loss=69.2681
	step [231/270], loss=69.1112
	step [232/270], loss=71.2822
	step [233/270], loss=97.5672
	step [234/270], loss=88.1660
	step [235/270], loss=68.8298
	step [236/270], loss=95.3484
	step [237/270], loss=58.8064
	step [238/270], loss=76.9288
	step [239/270], loss=72.7095
	step [240/270], loss=73.9818
	step [241/270], loss=91.5229
	step [242/270], loss=76.0011
	step [243/270], loss=68.0533
	step [244/270], loss=82.6686
	step [245/270], loss=78.8876
	step [246/270], loss=61.5068
	step [247/270], loss=71.2532
	step [248/270], loss=86.3400
	step [249/270], loss=76.7305
	step [250/270], loss=72.2118
	step [251/270], loss=78.3890
	step [252/270], loss=88.0836
	step [253/270], loss=96.1667
	step [254/270], loss=83.2559
	step [255/270], loss=74.2220
	step [256/270], loss=75.8008
	step [257/270], loss=78.5133
	step [258/270], loss=87.5414
	step [259/270], loss=82.9876
	step [260/270], loss=89.2061
	step [261/270], loss=77.2205
	step [262/270], loss=81.7509
	step [263/270], loss=71.8722
	step [264/270], loss=69.6579
	step [265/270], loss=70.2777
	step [266/270], loss=80.4681
	step [267/270], loss=79.9694
	step [268/270], loss=89.4317
	step [269/270], loss=73.3189
	step [270/270], loss=12.2850
	Evaluating
	loss=0.0076, precision=0.3334, recall=0.9023, f1=0.4869
Training epoch 79
	step [1/270], loss=89.6652
	step [2/270], loss=83.3146
	step [3/270], loss=91.8851
	step [4/270], loss=75.8712
	step [5/270], loss=78.6601
	step [6/270], loss=81.5701
	step [7/270], loss=84.7690
	step [8/270], loss=78.6141
	step [9/270], loss=85.9036
	step [10/270], loss=73.2523
	step [11/270], loss=83.9323
	step [12/270], loss=75.6172
	step [13/270], loss=84.4689
	step [14/270], loss=86.6646
	step [15/270], loss=82.3247
	step [16/270], loss=59.8887
	step [17/270], loss=94.6787
	step [18/270], loss=84.9995
	step [19/270], loss=85.1407
	step [20/270], loss=97.2184
	step [21/270], loss=91.3645
	step [22/270], loss=90.7345
	step [23/270], loss=77.0943
	step [24/270], loss=101.6514
	step [25/270], loss=88.0943
	step [26/270], loss=76.2973
	step [27/270], loss=67.0782
	step [28/270], loss=87.2606
	step [29/270], loss=63.9789
	step [30/270], loss=76.6385
	step [31/270], loss=76.6890
	step [32/270], loss=68.7383
	step [33/270], loss=62.5228
	step [34/270], loss=61.9079
	step [35/270], loss=76.5100
	step [36/270], loss=91.1510
	step [37/270], loss=60.9827
	step [38/270], loss=69.0300
	step [39/270], loss=77.4895
	step [40/270], loss=66.7300
	step [41/270], loss=87.9025
	step [42/270], loss=76.5396
	step [43/270], loss=80.1238
	step [44/270], loss=82.6623
	step [45/270], loss=83.7937
	step [46/270], loss=79.4856
	step [47/270], loss=73.9638
	step [48/270], loss=76.2152
	step [49/270], loss=64.0614
	step [50/270], loss=74.4215
	step [51/270], loss=78.2989
	step [52/270], loss=73.0923
	step [53/270], loss=74.2954
	step [54/270], loss=60.4383
	step [55/270], loss=83.5257
	step [56/270], loss=69.7338
	step [57/270], loss=77.8189
	step [58/270], loss=69.8421
	step [59/270], loss=75.4520
	step [60/270], loss=81.1909
	step [61/270], loss=59.4753
	step [62/270], loss=76.2077
	step [63/270], loss=86.9709
	step [64/270], loss=71.1707
	step [65/270], loss=84.8657
	step [66/270], loss=76.1168
	step [67/270], loss=74.1042
	step [68/270], loss=75.1458
	step [69/270], loss=85.1095
	step [70/270], loss=81.4551
	step [71/270], loss=88.5128
	step [72/270], loss=73.1589
	step [73/270], loss=70.9675
	step [74/270], loss=69.4472
	step [75/270], loss=81.3073
	step [76/270], loss=77.0326
	step [77/270], loss=76.9404
	step [78/270], loss=63.4283
	step [79/270], loss=74.2707
	step [80/270], loss=71.6340
	step [81/270], loss=76.4495
	step [82/270], loss=66.1866
	step [83/270], loss=69.9618
	step [84/270], loss=85.6037
	step [85/270], loss=70.5028
	step [86/270], loss=65.1828
	step [87/270], loss=86.6458
	step [88/270], loss=80.8913
	step [89/270], loss=64.4041
	step [90/270], loss=85.6222
	step [91/270], loss=88.2129
	step [92/270], loss=75.8481
	step [93/270], loss=62.8139
	step [94/270], loss=68.3941
	step [95/270], loss=72.9820
	step [96/270], loss=72.5121
	step [97/270], loss=84.0560
	step [98/270], loss=80.1018
	step [99/270], loss=73.2770
	step [100/270], loss=74.9789
	step [101/270], loss=86.6250
	step [102/270], loss=67.9313
	step [103/270], loss=90.5434
	step [104/270], loss=76.6514
	step [105/270], loss=80.0228
	step [106/270], loss=59.6114
	step [107/270], loss=75.0546
	step [108/270], loss=82.1103
	step [109/270], loss=63.6677
	step [110/270], loss=77.8000
	step [111/270], loss=88.8321
	step [112/270], loss=77.8186
	step [113/270], loss=75.3702
	step [114/270], loss=83.1769
	step [115/270], loss=79.6399
	step [116/270], loss=73.7039
	step [117/270], loss=86.2414
	step [118/270], loss=76.6846
	step [119/270], loss=76.3578
	step [120/270], loss=74.7372
	step [121/270], loss=83.5275
	step [122/270], loss=66.7682
	step [123/270], loss=75.5791
	step [124/270], loss=66.4120
	step [125/270], loss=77.3836
	step [126/270], loss=73.0047
	step [127/270], loss=68.5502
	step [128/270], loss=74.4740
	step [129/270], loss=86.8815
	step [130/270], loss=104.8189
	step [131/270], loss=79.3860
	step [132/270], loss=78.5165
	step [133/270], loss=69.2178
	step [134/270], loss=95.0676
	step [135/270], loss=71.9455
	step [136/270], loss=71.8183
	step [137/270], loss=73.1444
	step [138/270], loss=64.3220
	step [139/270], loss=79.0234
	step [140/270], loss=58.8254
	step [141/270], loss=65.7122
	step [142/270], loss=57.7979
	step [143/270], loss=69.7632
	step [144/270], loss=69.5032
	step [145/270], loss=78.8276
	step [146/270], loss=88.6234
	step [147/270], loss=79.5842
	step [148/270], loss=77.2814
	step [149/270], loss=82.9303
	step [150/270], loss=83.8914
	step [151/270], loss=76.5635
	step [152/270], loss=80.3089
	step [153/270], loss=66.4483
	step [154/270], loss=80.4385
	step [155/270], loss=79.3751
	step [156/270], loss=86.2834
	step [157/270], loss=70.4382
	step [158/270], loss=80.3535
	step [159/270], loss=69.1513
	step [160/270], loss=86.7640
	step [161/270], loss=82.2012
	step [162/270], loss=71.8873
	step [163/270], loss=69.7163
	step [164/270], loss=68.7209
	step [165/270], loss=75.1036
	step [166/270], loss=76.5001
	step [167/270], loss=80.8456
	step [168/270], loss=63.3658
	step [169/270], loss=85.7225
	step [170/270], loss=80.5110
	step [171/270], loss=76.0212
	step [172/270], loss=96.0399
	step [173/270], loss=76.8620
	step [174/270], loss=70.8150
	step [175/270], loss=68.8115
	step [176/270], loss=73.8602
	step [177/270], loss=71.1024
	step [178/270], loss=84.2946
	step [179/270], loss=64.0536
	step [180/270], loss=90.1054
	step [181/270], loss=74.0844
	step [182/270], loss=65.4172
	step [183/270], loss=76.0812
	step [184/270], loss=83.9803
	step [185/270], loss=85.7566
	step [186/270], loss=84.7116
	step [187/270], loss=68.6455
	step [188/270], loss=78.8663
	step [189/270], loss=85.0313
	step [190/270], loss=87.0803
	step [191/270], loss=66.5401
	step [192/270], loss=76.5290
	step [193/270], loss=74.5265
	step [194/270], loss=66.3941
	step [195/270], loss=72.4994
	step [196/270], loss=66.2152
	step [197/270], loss=74.2843
	step [198/270], loss=73.4717
	step [199/270], loss=79.4806
	step [200/270], loss=74.2584
	step [201/270], loss=72.1143
	step [202/270], loss=87.1977
	step [203/270], loss=75.3895
	step [204/270], loss=78.6712
	step [205/270], loss=72.8303
	step [206/270], loss=69.5873
	step [207/270], loss=68.5534
	step [208/270], loss=73.9657
	step [209/270], loss=84.0321
	step [210/270], loss=72.2395
	step [211/270], loss=67.6498
	step [212/270], loss=92.9684
	step [213/270], loss=67.2184
	step [214/270], loss=70.7960
	step [215/270], loss=73.3343
	step [216/270], loss=84.3426
	step [217/270], loss=71.1041
	step [218/270], loss=76.0029
	step [219/270], loss=77.0527
	step [220/270], loss=79.1150
	step [221/270], loss=73.3096
	step [222/270], loss=72.4014
	step [223/270], loss=85.6996
	step [224/270], loss=70.9470
	step [225/270], loss=92.3354
	step [226/270], loss=71.4941
	step [227/270], loss=87.2059
	step [228/270], loss=79.7076
	step [229/270], loss=74.8154
	step [230/270], loss=95.6212
	step [231/270], loss=71.5910
	step [232/270], loss=84.8315
	step [233/270], loss=76.1109
	step [234/270], loss=69.6296
	step [235/270], loss=89.5849
	step [236/270], loss=70.2916
	step [237/270], loss=73.3121
	step [238/270], loss=74.9331
	step [239/270], loss=67.8331
	step [240/270], loss=88.6650
	step [241/270], loss=73.2360
	step [242/270], loss=104.5565
	step [243/270], loss=81.5152
	step [244/270], loss=65.4711
	step [245/270], loss=75.8857
	step [246/270], loss=66.0417
	step [247/270], loss=84.2737
	step [248/270], loss=64.0276
	step [249/270], loss=75.2091
	step [250/270], loss=77.0476
	step [251/270], loss=69.2587
	step [252/270], loss=77.2251
	step [253/270], loss=74.7505
	step [254/270], loss=81.3196
	step [255/270], loss=87.8047
	step [256/270], loss=72.0641
	step [257/270], loss=75.0297
	step [258/270], loss=80.1768
	step [259/270], loss=66.1738
	step [260/270], loss=84.9181
	step [261/270], loss=83.7684
	step [262/270], loss=86.9423
	step [263/270], loss=86.6238
	step [264/270], loss=75.2368
	step [265/270], loss=95.1317
	step [266/270], loss=73.4148
	step [267/270], loss=76.8238
	step [268/270], loss=72.3415
	step [269/270], loss=67.5755
	step [270/270], loss=15.6965
	Evaluating
	loss=0.0081, precision=0.3149, recall=0.9045, f1=0.4671
Training epoch 80
	step [1/270], loss=67.3377
	step [2/270], loss=76.7951
	step [3/270], loss=75.2187
	step [4/270], loss=72.8607
	step [5/270], loss=64.8582
	step [6/270], loss=76.4015
	step [7/270], loss=71.9688
	step [8/270], loss=80.8709
	step [9/270], loss=79.6187
	step [10/270], loss=77.1500
	step [11/270], loss=82.7708
	step [12/270], loss=73.3504
	step [13/270], loss=71.8088
	step [14/270], loss=79.1822
	step [15/270], loss=80.2814
	step [16/270], loss=60.7019
	step [17/270], loss=75.1475
	step [18/270], loss=78.2888
	step [19/270], loss=78.4393
	step [20/270], loss=77.1105
	step [21/270], loss=74.7023
	step [22/270], loss=72.3951
	step [23/270], loss=78.5475
	step [24/270], loss=86.8402
	step [25/270], loss=99.5090
	step [26/270], loss=83.6813
	step [27/270], loss=69.4322
	step [28/270], loss=81.7764
	step [29/270], loss=67.2769
	step [30/270], loss=86.6340
	step [31/270], loss=81.0401
	step [32/270], loss=77.8122
	step [33/270], loss=76.2396
	step [34/270], loss=72.8745
	step [35/270], loss=80.3447
	step [36/270], loss=63.9746
	step [37/270], loss=88.9228
	step [38/270], loss=75.7089
	step [39/270], loss=70.6425
	step [40/270], loss=72.0985
	step [41/270], loss=82.3402
	step [42/270], loss=83.6584
	step [43/270], loss=68.2145
	step [44/270], loss=97.9349
	step [45/270], loss=71.5761
	step [46/270], loss=72.7268
	step [47/270], loss=68.0317
	step [48/270], loss=73.2643
	step [49/270], loss=73.8028
	step [50/270], loss=72.1985
	step [51/270], loss=80.6461
	step [52/270], loss=77.6403
	step [53/270], loss=71.7183
	step [54/270], loss=95.4917
	step [55/270], loss=88.4268
	step [56/270], loss=67.5995
	step [57/270], loss=70.7569
	step [58/270], loss=79.1782
	step [59/270], loss=59.4433
	step [60/270], loss=74.0516
	step [61/270], loss=62.6647
	step [62/270], loss=62.1321
	step [63/270], loss=86.6791
	step [64/270], loss=68.5811
	step [65/270], loss=81.7310
	step [66/270], loss=88.1790
	step [67/270], loss=94.2459
	step [68/270], loss=77.8967
	step [69/270], loss=87.7435
	step [70/270], loss=70.7315
	step [71/270], loss=73.7592
	step [72/270], loss=66.3180
	step [73/270], loss=72.9059
	step [74/270], loss=76.6500
	step [75/270], loss=83.3072
	step [76/270], loss=79.5677
	step [77/270], loss=78.3941
	step [78/270], loss=68.2323
	step [79/270], loss=77.3511
	step [80/270], loss=78.2903
	step [81/270], loss=81.4382
	step [82/270], loss=82.9450
	step [83/270], loss=87.5463
	step [84/270], loss=65.4638
	step [85/270], loss=70.3608
	step [86/270], loss=78.7468
	step [87/270], loss=69.4203
	step [88/270], loss=70.7735
	step [89/270], loss=82.1834
	step [90/270], loss=67.5773
	step [91/270], loss=69.5557
	step [92/270], loss=108.8950
	step [93/270], loss=66.9033
	step [94/270], loss=86.9499
	step [95/270], loss=90.9716
	step [96/270], loss=70.5177
	step [97/270], loss=73.1773
	step [98/270], loss=77.8057
	step [99/270], loss=82.2875
	step [100/270], loss=77.6752
	step [101/270], loss=78.3938
	step [102/270], loss=79.5475
	step [103/270], loss=85.2487
	step [104/270], loss=75.3115
	step [105/270], loss=65.9811
	step [106/270], loss=76.7972
	step [107/270], loss=86.9433
	step [108/270], loss=65.2090
	step [109/270], loss=84.1876
	step [110/270], loss=71.7686
	step [111/270], loss=73.6526
	step [112/270], loss=65.9580
	step [113/270], loss=76.2893
	step [114/270], loss=79.6508
	step [115/270], loss=94.5893
	step [116/270], loss=61.3878
	step [117/270], loss=79.2739
	step [118/270], loss=79.0725
	step [119/270], loss=83.4376
	step [120/270], loss=77.7570
	step [121/270], loss=65.4421
	step [122/270], loss=86.8259
	step [123/270], loss=77.9695
	step [124/270], loss=67.7859
	step [125/270], loss=72.8416
	step [126/270], loss=76.1098
	step [127/270], loss=56.8694
	step [128/270], loss=94.3810
	step [129/270], loss=84.0643
	step [130/270], loss=77.3939
	step [131/270], loss=72.6590
	step [132/270], loss=73.3561
	step [133/270], loss=86.4807
	step [134/270], loss=72.8792
	step [135/270], loss=77.4849
	step [136/270], loss=74.2729
	step [137/270], loss=69.3788
	step [138/270], loss=70.2115
	step [139/270], loss=59.6231
	step [140/270], loss=75.1970
	step [141/270], loss=81.0730
	step [142/270], loss=69.3000
	step [143/270], loss=72.1570
	step [144/270], loss=77.2561
	step [145/270], loss=61.2643
	step [146/270], loss=68.2393
	step [147/270], loss=78.1638
	step [148/270], loss=80.7190
	step [149/270], loss=70.2037
	step [150/270], loss=89.8071
	step [151/270], loss=71.8226
	step [152/270], loss=64.6322
	step [153/270], loss=82.8125
	step [154/270], loss=83.8606
	step [155/270], loss=68.0351
	step [156/270], loss=66.2701
	step [157/270], loss=70.9926
	step [158/270], loss=82.0854
	step [159/270], loss=84.3136
	step [160/270], loss=83.9355
	step [161/270], loss=89.3638
	step [162/270], loss=72.1987
	step [163/270], loss=61.9520
	step [164/270], loss=85.9927
	step [165/270], loss=72.7886
	step [166/270], loss=80.1371
	step [167/270], loss=69.9675
	step [168/270], loss=64.0988
	step [169/270], loss=72.8559
	step [170/270], loss=76.7665
	step [171/270], loss=76.0449
	step [172/270], loss=55.4043
	step [173/270], loss=88.1992
	step [174/270], loss=71.8981
	step [175/270], loss=65.7822
	step [176/270], loss=86.4095
	step [177/270], loss=76.9333
	step [178/270], loss=81.0245
	step [179/270], loss=89.7613
	step [180/270], loss=70.8700
	step [181/270], loss=68.5377
	step [182/270], loss=56.7571
	step [183/270], loss=85.2155
	step [184/270], loss=69.0574
	step [185/270], loss=69.1793
	step [186/270], loss=72.8062
	step [187/270], loss=80.6025
	step [188/270], loss=75.7718
	step [189/270], loss=79.5410
	step [190/270], loss=72.8693
	step [191/270], loss=74.8456
	step [192/270], loss=88.1202
	step [193/270], loss=76.8777
	step [194/270], loss=70.2345
	step [195/270], loss=66.8755
	step [196/270], loss=64.2784
	step [197/270], loss=70.5299
	step [198/270], loss=81.3829
	step [199/270], loss=71.3045
	step [200/270], loss=82.6001
	step [201/270], loss=72.7616
	step [202/270], loss=83.9965
	step [203/270], loss=79.8399
	step [204/270], loss=83.9029
	step [205/270], loss=83.0661
	step [206/270], loss=81.1785
	step [207/270], loss=70.6951
	step [208/270], loss=86.3201
	step [209/270], loss=103.2097
	step [210/270], loss=73.3829
	step [211/270], loss=66.7029
	step [212/270], loss=84.5191
	step [213/270], loss=86.3905
	step [214/270], loss=79.8594
	step [215/270], loss=77.8399
	step [216/270], loss=83.6928
	step [217/270], loss=70.8075
	step [218/270], loss=73.0176
	step [219/270], loss=90.0009
	step [220/270], loss=84.2153
	step [221/270], loss=76.0582
	step [222/270], loss=72.9704
	step [223/270], loss=76.4537
	step [224/270], loss=77.4289
	step [225/270], loss=79.5859
	step [226/270], loss=67.5132
	step [227/270], loss=71.7442
	step [228/270], loss=68.8322
	step [229/270], loss=96.5935
	step [230/270], loss=81.2236
	step [231/270], loss=84.1543
	step [232/270], loss=92.5325
	step [233/270], loss=74.6025
	step [234/270], loss=63.8642
	step [235/270], loss=89.4994
	step [236/270], loss=94.3927
	step [237/270], loss=71.8427
	step [238/270], loss=77.8118
	step [239/270], loss=74.4039
	step [240/270], loss=88.3077
	step [241/270], loss=78.6381
	step [242/270], loss=82.6134
	step [243/270], loss=80.0911
	step [244/270], loss=87.0601
	step [245/270], loss=75.1798
	step [246/270], loss=81.1720
	step [247/270], loss=78.4347
	step [248/270], loss=62.8526
	step [249/270], loss=73.6027
	step [250/270], loss=76.8931
	step [251/270], loss=79.2654
	step [252/270], loss=79.4659
	step [253/270], loss=78.8486
	step [254/270], loss=64.6379
	step [255/270], loss=71.0836
	step [256/270], loss=90.5621
	step [257/270], loss=83.9539
	step [258/270], loss=77.6159
	step [259/270], loss=72.7391
	step [260/270], loss=78.3908
	step [261/270], loss=76.1812
	step [262/270], loss=96.9193
	step [263/270], loss=69.2128
	step [264/270], loss=80.9908
	step [265/270], loss=62.2378
	step [266/270], loss=95.6620
	step [267/270], loss=79.2628
	step [268/270], loss=90.5451
	step [269/270], loss=66.5205
	step [270/270], loss=9.8930
	Evaluating
	loss=0.0066, precision=0.3708, recall=0.8969, f1=0.5247
saving model as: 4_saved_model.pth
Training epoch 81
	step [1/270], loss=77.7490
	step [2/270], loss=78.4725
	step [3/270], loss=65.7667
	step [4/270], loss=72.5833
	step [5/270], loss=68.2953
	step [6/270], loss=76.6483
	step [7/270], loss=85.9887
	step [8/270], loss=74.3085
	step [9/270], loss=83.8787
	step [10/270], loss=79.1981
	step [11/270], loss=71.5234
	step [12/270], loss=74.6346
	step [13/270], loss=80.6463
	step [14/270], loss=79.3173
	step [15/270], loss=77.1808
	step [16/270], loss=68.1146
	step [17/270], loss=71.1749
	step [18/270], loss=74.1735
	step [19/270], loss=74.9467
	step [20/270], loss=76.0913
	step [21/270], loss=72.7328
	step [22/270], loss=81.0680
	step [23/270], loss=81.5046
	step [24/270], loss=69.4207
	step [25/270], loss=66.6488
	step [26/270], loss=71.0853
	step [27/270], loss=62.1373
	step [28/270], loss=71.4445
	step [29/270], loss=79.8657
	step [30/270], loss=59.0819
	step [31/270], loss=85.1220
	step [32/270], loss=76.1485
	step [33/270], loss=59.9291
	step [34/270], loss=88.5541
	step [35/270], loss=81.9258
	step [36/270], loss=76.5720
	step [37/270], loss=74.8407
	step [38/270], loss=80.3433
	step [39/270], loss=73.9284
	step [40/270], loss=80.0938
	step [41/270], loss=69.7914
	step [42/270], loss=85.5428
	step [43/270], loss=74.4771
	step [44/270], loss=83.0576
	step [45/270], loss=89.3245
	step [46/270], loss=81.5212
	step [47/270], loss=71.7163
	step [48/270], loss=74.4046
	step [49/270], loss=82.0244
	step [50/270], loss=88.4231
	step [51/270], loss=74.3413
	step [52/270], loss=80.1765
	step [53/270], loss=76.2009
	step [54/270], loss=81.8650
	step [55/270], loss=63.4820
	step [56/270], loss=78.5267
	step [57/270], loss=73.7933
	step [58/270], loss=74.7696
	step [59/270], loss=75.7873
	step [60/270], loss=83.9312
	step [61/270], loss=79.6171
	step [62/270], loss=92.7005
	step [63/270], loss=75.4062
	step [64/270], loss=73.7514
	step [65/270], loss=68.4942
	step [66/270], loss=67.2808
	step [67/270], loss=89.5020
	step [68/270], loss=85.9619
	step [69/270], loss=73.9553
	step [70/270], loss=61.8960
	step [71/270], loss=76.4701
	step [72/270], loss=77.5729
	step [73/270], loss=84.8281
	step [74/270], loss=75.6102
	step [75/270], loss=73.5297
	step [76/270], loss=79.9769
	step [77/270], loss=83.5217
	step [78/270], loss=72.1284
	step [79/270], loss=85.0660
	step [80/270], loss=81.4483
	step [81/270], loss=77.5923
	step [82/270], loss=82.5084
	step [83/270], loss=70.8497
	step [84/270], loss=72.7617
	step [85/270], loss=73.9585
	step [86/270], loss=79.8121
	step [87/270], loss=87.5718
	step [88/270], loss=73.5569
	step [89/270], loss=77.9483
	step [90/270], loss=78.3702
	step [91/270], loss=83.2449
	step [92/270], loss=71.5546
	step [93/270], loss=85.9258
	step [94/270], loss=75.7623
	step [95/270], loss=85.9424
	step [96/270], loss=68.9916
	step [97/270], loss=66.2380
	step [98/270], loss=61.5602
	step [99/270], loss=70.6441
	step [100/270], loss=90.9767
	step [101/270], loss=85.0873
	step [102/270], loss=72.9995
	step [103/270], loss=84.7256
	step [104/270], loss=77.4823
	step [105/270], loss=71.8933
	step [106/270], loss=68.3764
	step [107/270], loss=68.2505
	step [108/270], loss=77.5194
	step [109/270], loss=75.3243
	step [110/270], loss=73.7779
	step [111/270], loss=72.3609
	step [112/270], loss=77.0917
	step [113/270], loss=78.2422
	step [114/270], loss=70.8000
	step [115/270], loss=79.9137
	step [116/270], loss=87.9449
	step [117/270], loss=65.5981
	step [118/270], loss=70.6752
	step [119/270], loss=67.3658
	step [120/270], loss=77.7394
	step [121/270], loss=68.0030
	step [122/270], loss=60.6378
	step [123/270], loss=80.8702
	step [124/270], loss=86.7506
	step [125/270], loss=72.5420
	step [126/270], loss=80.9192
	step [127/270], loss=72.7171
	step [128/270], loss=77.1727
	step [129/270], loss=71.0058
	step [130/270], loss=70.4806
	step [131/270], loss=92.5493
	step [132/270], loss=84.2765
	step [133/270], loss=75.6122
	step [134/270], loss=54.8316
	step [135/270], loss=83.2811
	step [136/270], loss=82.4901
	step [137/270], loss=73.8140
	step [138/270], loss=92.6947
	step [139/270], loss=53.3221
	step [140/270], loss=80.1420
	step [141/270], loss=78.4899
	step [142/270], loss=72.6616
	step [143/270], loss=78.4104
	step [144/270], loss=62.1115
	step [145/270], loss=62.4937
	step [146/270], loss=73.9414
	step [147/270], loss=83.6014
	step [148/270], loss=81.3526
	step [149/270], loss=88.8655
	step [150/270], loss=61.8766
	step [151/270], loss=74.0312
	step [152/270], loss=71.1375
	step [153/270], loss=87.3540
	step [154/270], loss=73.3740
	step [155/270], loss=77.1972
	step [156/270], loss=55.2189
	step [157/270], loss=76.7317
	step [158/270], loss=63.2167
	step [159/270], loss=91.2549
	step [160/270], loss=78.2695
	step [161/270], loss=85.2465
	step [162/270], loss=77.2661
	step [163/270], loss=81.7818
	step [164/270], loss=74.7449
	step [165/270], loss=92.4651
	step [166/270], loss=84.2920
	step [167/270], loss=67.4363
	step [168/270], loss=65.3140
	step [169/270], loss=71.8696
	step [170/270], loss=69.0680
	step [171/270], loss=96.3987
	step [172/270], loss=75.1481
	step [173/270], loss=74.6042
	step [174/270], loss=90.9420
	step [175/270], loss=81.6462
	step [176/270], loss=79.1875
	step [177/270], loss=87.4872
	step [178/270], loss=77.0121
	step [179/270], loss=57.5889
	step [180/270], loss=75.1456
	step [181/270], loss=79.2223
	step [182/270], loss=75.4794
	step [183/270], loss=85.7495
	step [184/270], loss=86.8242
	step [185/270], loss=88.4228
	step [186/270], loss=74.3698
	step [187/270], loss=68.3931
	step [188/270], loss=81.6864
	step [189/270], loss=71.3501
	step [190/270], loss=83.4115
	step [191/270], loss=76.0488
	step [192/270], loss=81.0795
	step [193/270], loss=86.0131
	step [194/270], loss=79.1420
	step [195/270], loss=82.4939
	step [196/270], loss=63.5577
	step [197/270], loss=71.5065
	step [198/270], loss=88.7741
	step [199/270], loss=80.0669
	step [200/270], loss=73.7091
	step [201/270], loss=76.3747
	step [202/270], loss=77.2025
	step [203/270], loss=70.8299
	step [204/270], loss=75.9887
	step [205/270], loss=78.8086
	step [206/270], loss=87.4648
	step [207/270], loss=80.3716
	step [208/270], loss=83.9642
	step [209/270], loss=73.9749
	step [210/270], loss=78.8268
	step [211/270], loss=78.2988
	step [212/270], loss=70.6802
	step [213/270], loss=70.5461
	step [214/270], loss=65.0059
	step [215/270], loss=71.4944
	step [216/270], loss=87.6564
	step [217/270], loss=58.5808
	step [218/270], loss=99.0537
	step [219/270], loss=58.7718
	step [220/270], loss=82.5633
	step [221/270], loss=84.9021
	step [222/270], loss=71.0827
	step [223/270], loss=85.8486
	step [224/270], loss=73.8899
	step [225/270], loss=78.5926
	step [226/270], loss=65.3295
	step [227/270], loss=84.6021
	step [228/270], loss=83.6288
	step [229/270], loss=79.7822
	step [230/270], loss=82.0936
	step [231/270], loss=69.7230
	step [232/270], loss=74.2388
	step [233/270], loss=75.2785
	step [234/270], loss=64.7333
	step [235/270], loss=69.8043
	step [236/270], loss=71.5175
	step [237/270], loss=79.2463
	step [238/270], loss=61.8159
	step [239/270], loss=74.0340
	step [240/270], loss=77.9970
	step [241/270], loss=77.7841
	step [242/270], loss=87.0866
	step [243/270], loss=80.0790
	step [244/270], loss=56.0979
	step [245/270], loss=68.3594
	step [246/270], loss=85.8179
	step [247/270], loss=89.7802
	step [248/270], loss=73.8447
	step [249/270], loss=71.2794
	step [250/270], loss=79.5675
	step [251/270], loss=74.7622
	step [252/270], loss=70.4512
	step [253/270], loss=79.6996
	step [254/270], loss=84.1804
	step [255/270], loss=67.4796
	step [256/270], loss=85.9650
	step [257/270], loss=88.8056
	step [258/270], loss=75.6041
	step [259/270], loss=69.8610
	step [260/270], loss=73.2987
	step [261/270], loss=71.9967
	step [262/270], loss=70.2479
	step [263/270], loss=71.6141
	step [264/270], loss=75.8240
	step [265/270], loss=88.3698
	step [266/270], loss=77.9117
	step [267/270], loss=78.1712
	step [268/270], loss=61.3225
	step [269/270], loss=84.9482
	step [270/270], loss=14.7333
	Evaluating
	loss=0.0072, precision=0.3527, recall=0.9036, f1=0.5073
Training epoch 82
	step [1/270], loss=71.7992
	step [2/270], loss=85.6167
	step [3/270], loss=85.2155
	step [4/270], loss=80.8635
	step [5/270], loss=76.2022
	step [6/270], loss=73.1805
	step [7/270], loss=70.5770
	step [8/270], loss=79.3933
	step [9/270], loss=103.0296
	step [10/270], loss=68.3664
	step [11/270], loss=77.4180
	step [12/270], loss=81.7332
	step [13/270], loss=81.4176
	step [14/270], loss=92.2171
	step [15/270], loss=66.5971
	step [16/270], loss=67.3348
	step [17/270], loss=78.6539
	step [18/270], loss=78.1279
	step [19/270], loss=89.9988
	step [20/270], loss=65.9643
	step [21/270], loss=80.8074
	step [22/270], loss=86.9221
	step [23/270], loss=71.6406
	step [24/270], loss=84.5238
	step [25/270], loss=60.1657
	step [26/270], loss=78.7567
	step [27/270], loss=71.0585
	step [28/270], loss=72.0073
	step [29/270], loss=72.1473
	step [30/270], loss=61.9803
	step [31/270], loss=60.9274
	step [32/270], loss=81.3939
	step [33/270], loss=76.3453
	step [34/270], loss=71.9508
	step [35/270], loss=85.9698
	step [36/270], loss=69.8349
	step [37/270], loss=83.8755
	step [38/270], loss=77.3529
	step [39/270], loss=87.8381
	step [40/270], loss=66.2175
	step [41/270], loss=72.4196
	step [42/270], loss=68.9458
	step [43/270], loss=77.7068
	step [44/270], loss=82.8653
	step [45/270], loss=76.5029
	step [46/270], loss=65.4688
	step [47/270], loss=79.1058
	step [48/270], loss=82.1654
	step [49/270], loss=101.8743
	step [50/270], loss=82.2526
	step [51/270], loss=83.8776
	step [52/270], loss=88.5510
	step [53/270], loss=85.4090
	step [54/270], loss=61.6527
	step [55/270], loss=69.2506
	step [56/270], loss=78.6918
	step [57/270], loss=75.1864
	step [58/270], loss=84.8042
	step [59/270], loss=79.6158
	step [60/270], loss=64.5634
	step [61/270], loss=57.8557
	step [62/270], loss=75.0519
	step [63/270], loss=88.0813
	step [64/270], loss=87.6499
	step [65/270], loss=75.7140
	step [66/270], loss=73.5995
	step [67/270], loss=72.4715
	step [68/270], loss=80.1037
	step [69/270], loss=79.6113
	step [70/270], loss=71.3007
	step [71/270], loss=66.0723
	step [72/270], loss=69.2402
	step [73/270], loss=66.3605
	step [74/270], loss=74.3107
	step [75/270], loss=58.6856
	step [76/270], loss=96.3468
	step [77/270], loss=75.0160
	step [78/270], loss=72.0679
	step [79/270], loss=79.6802
	step [80/270], loss=87.1578
	step [81/270], loss=83.3290
	step [82/270], loss=78.1456
	step [83/270], loss=73.1622
	step [84/270], loss=81.1625
	step [85/270], loss=89.4675
	step [86/270], loss=78.1834
	step [87/270], loss=105.3312
	step [88/270], loss=74.2101
	step [89/270], loss=85.1127
	step [90/270], loss=79.6000
	step [91/270], loss=67.1899
	step [92/270], loss=66.4878
	step [93/270], loss=71.2848
	step [94/270], loss=85.9365
	step [95/270], loss=79.2634
	step [96/270], loss=76.4416
	step [97/270], loss=80.8428
	step [98/270], loss=75.3234
	step [99/270], loss=81.8779
	step [100/270], loss=72.3367
	step [101/270], loss=84.1319
	step [102/270], loss=77.0973
	step [103/270], loss=83.6134
	step [104/270], loss=88.3334
	step [105/270], loss=94.5782
	step [106/270], loss=68.3753
	step [107/270], loss=69.0123
	step [108/270], loss=67.7350
	step [109/270], loss=81.9030
	step [110/270], loss=69.3283
	step [111/270], loss=70.2102
	step [112/270], loss=77.5388
	step [113/270], loss=68.6634
	step [114/270], loss=81.9801
	step [115/270], loss=63.2473
	step [116/270], loss=77.0565
	step [117/270], loss=79.0581
	step [118/270], loss=62.9433
	step [119/270], loss=74.7744
	step [120/270], loss=80.3399
	step [121/270], loss=76.2513
	step [122/270], loss=82.5214
	step [123/270], loss=69.7630
	step [124/270], loss=73.7105
	step [125/270], loss=71.0569
	step [126/270], loss=76.2086
	step [127/270], loss=64.5049
	step [128/270], loss=64.2598
	step [129/270], loss=76.7755
	step [130/270], loss=68.5073
	step [131/270], loss=83.2106
	step [132/270], loss=64.4436
	step [133/270], loss=66.7809
	step [134/270], loss=65.5162
	step [135/270], loss=68.6881
	step [136/270], loss=80.7194
	step [137/270], loss=80.8749
	step [138/270], loss=71.3394
	step [139/270], loss=65.2431
	step [140/270], loss=64.1023
	step [141/270], loss=81.5685
	step [142/270], loss=75.0568
	step [143/270], loss=75.6902
	step [144/270], loss=75.7041
	step [145/270], loss=75.9777
	step [146/270], loss=68.8889
	step [147/270], loss=76.6608
	step [148/270], loss=75.1435
	step [149/270], loss=74.4211
	step [150/270], loss=75.1878
	step [151/270], loss=74.1649
	step [152/270], loss=82.3249
	step [153/270], loss=73.3134
	step [154/270], loss=68.9608
	step [155/270], loss=86.9449
	step [156/270], loss=77.9505
	step [157/270], loss=84.7687
	step [158/270], loss=65.0471
	step [159/270], loss=67.9107
	step [160/270], loss=67.0594
	step [161/270], loss=72.5994
	step [162/270], loss=80.4222
	step [163/270], loss=77.1265
	step [164/270], loss=79.1681
	step [165/270], loss=84.4859
	step [166/270], loss=66.7838
	step [167/270], loss=69.9849
	step [168/270], loss=72.1127
	step [169/270], loss=88.3566
	step [170/270], loss=80.7908
	step [171/270], loss=67.8582
	step [172/270], loss=69.3157
	step [173/270], loss=86.1400
	step [174/270], loss=83.6611
	step [175/270], loss=77.9256
	step [176/270], loss=68.5859
	step [177/270], loss=70.0000
	step [178/270], loss=82.7863
	step [179/270], loss=84.1637
	step [180/270], loss=84.5729
	step [181/270], loss=78.2535
	step [182/270], loss=86.8689
	step [183/270], loss=80.6881
	step [184/270], loss=75.6798
	step [185/270], loss=72.6597
	step [186/270], loss=60.0024
	step [187/270], loss=77.8335
	step [188/270], loss=77.2599
	step [189/270], loss=95.1879
	step [190/270], loss=82.3644
	step [191/270], loss=90.5234
	step [192/270], loss=72.2336
	step [193/270], loss=76.6754
	step [194/270], loss=80.0696
	step [195/270], loss=84.6825
	step [196/270], loss=87.9477
	step [197/270], loss=80.4403
	step [198/270], loss=75.1920
	step [199/270], loss=75.3029
	step [200/270], loss=71.2811
	step [201/270], loss=75.1949
	step [202/270], loss=87.9436
	step [203/270], loss=73.8811
	step [204/270], loss=62.8071
	step [205/270], loss=79.1850
	step [206/270], loss=68.7188
	step [207/270], loss=78.8322
	step [208/270], loss=78.5919
	step [209/270], loss=88.8761
	step [210/270], loss=79.1863
	step [211/270], loss=72.9174
	step [212/270], loss=65.3479
	step [213/270], loss=70.9020
	step [214/270], loss=73.0637
	step [215/270], loss=86.3949
	step [216/270], loss=67.8623
	step [217/270], loss=83.1950
	step [218/270], loss=65.4049
	step [219/270], loss=87.3282
	step [220/270], loss=58.2275
	step [221/270], loss=66.9132
	step [222/270], loss=85.0600
	step [223/270], loss=79.8003
	step [224/270], loss=67.0428
	step [225/270], loss=77.2958
	step [226/270], loss=77.1726
	step [227/270], loss=64.8232
	step [228/270], loss=65.5662
	step [229/270], loss=74.4670
	step [230/270], loss=81.2935
	step [231/270], loss=71.4384
	step [232/270], loss=80.1877
	step [233/270], loss=61.9242
	step [234/270], loss=74.7510
	step [235/270], loss=78.5676
	step [236/270], loss=81.8250
	step [237/270], loss=81.7370
	step [238/270], loss=73.2143
	step [239/270], loss=69.6362
	step [240/270], loss=73.6116
	step [241/270], loss=65.1155
	step [242/270], loss=90.0418
	step [243/270], loss=78.3798
	step [244/270], loss=87.2245
	step [245/270], loss=65.1829
	step [246/270], loss=63.5789
	step [247/270], loss=62.4743
	step [248/270], loss=73.2056
	step [249/270], loss=73.1382
	step [250/270], loss=58.6146
	step [251/270], loss=81.0846
	step [252/270], loss=77.7114
	step [253/270], loss=73.1632
	step [254/270], loss=89.1624
	step [255/270], loss=77.8777
	step [256/270], loss=74.8479
	step [257/270], loss=89.6888
	step [258/270], loss=76.0210
	step [259/270], loss=87.6295
	step [260/270], loss=65.9424
	step [261/270], loss=74.3184
	step [262/270], loss=68.4729
	step [263/270], loss=82.1284
	step [264/270], loss=66.4118
	step [265/270], loss=69.2926
	step [266/270], loss=83.5943
	step [267/270], loss=77.7658
	step [268/270], loss=69.9383
	step [269/270], loss=76.8918
	step [270/270], loss=10.2565
	Evaluating
	loss=0.0085, precision=0.2997, recall=0.9114, f1=0.4511
Training epoch 83
	step [1/270], loss=69.8338
	step [2/270], loss=75.7601
	step [3/270], loss=82.0009
	step [4/270], loss=80.4031
	step [5/270], loss=73.3245
	step [6/270], loss=86.6640
	step [7/270], loss=71.1235
	step [8/270], loss=75.3573
	step [9/270], loss=70.3046
	step [10/270], loss=69.1047
	step [11/270], loss=83.8772
	step [12/270], loss=80.9672
	step [13/270], loss=69.3851
	step [14/270], loss=86.2954
	step [15/270], loss=79.3041
	step [16/270], loss=66.1405
	step [17/270], loss=62.5336
	step [18/270], loss=68.5841
	step [19/270], loss=59.0955
	step [20/270], loss=80.7318
	step [21/270], loss=78.8996
	step [22/270], loss=73.8840
	step [23/270], loss=66.5453
	step [24/270], loss=82.9773
	step [25/270], loss=83.5949
	step [26/270], loss=63.3912
	step [27/270], loss=68.0454
	step [28/270], loss=74.3326
	step [29/270], loss=62.2951
	step [30/270], loss=76.4514
	step [31/270], loss=78.2894
	step [32/270], loss=75.2745
	step [33/270], loss=76.0872
	step [34/270], loss=66.0340
	step [35/270], loss=81.1179
	step [36/270], loss=63.3802
	step [37/270], loss=72.6372
	step [38/270], loss=73.0853
	step [39/270], loss=91.2182
	step [40/270], loss=61.8776
	step [41/270], loss=70.4171
	step [42/270], loss=68.5784
	step [43/270], loss=87.8665
	step [44/270], loss=79.2423
	step [45/270], loss=75.4201
	step [46/270], loss=64.1521
	step [47/270], loss=78.8708
	step [48/270], loss=76.8972
	step [49/270], loss=74.3824
	step [50/270], loss=85.1342
	step [51/270], loss=77.1104
	step [52/270], loss=67.0379
	step [53/270], loss=84.6540
	step [54/270], loss=71.0795
	step [55/270], loss=79.2795
	step [56/270], loss=77.3368
	step [57/270], loss=71.2806
	step [58/270], loss=75.6026
	step [59/270], loss=80.5339
	step [60/270], loss=76.1610
	step [61/270], loss=77.7848
	step [62/270], loss=74.4509
	step [63/270], loss=97.2887
	step [64/270], loss=84.1672
	step [65/270], loss=71.1981
	step [66/270], loss=79.2781
	step [67/270], loss=84.3058
	step [68/270], loss=75.3457
	step [69/270], loss=72.9977
	step [70/270], loss=85.3705
	step [71/270], loss=63.1873
	step [72/270], loss=79.5440
	step [73/270], loss=61.7841
	step [74/270], loss=86.8629
	step [75/270], loss=76.0973
	step [76/270], loss=75.4624
	step [77/270], loss=80.3638
	step [78/270], loss=102.0168
	step [79/270], loss=72.6560
	step [80/270], loss=70.6060
	step [81/270], loss=77.5520
	step [82/270], loss=75.8237
	step [83/270], loss=68.1614
	step [84/270], loss=74.7043
	step [85/270], loss=71.7827
	step [86/270], loss=71.4415
	step [87/270], loss=68.9857
	step [88/270], loss=75.8039
	step [89/270], loss=81.0748
	step [90/270], loss=71.9761
	step [91/270], loss=77.5782
	step [92/270], loss=71.9756
	step [93/270], loss=85.0733
	step [94/270], loss=93.4558
	step [95/270], loss=69.6042
	step [96/270], loss=72.1164
	step [97/270], loss=75.0440
	step [98/270], loss=63.7770
	step [99/270], loss=73.1584
	step [100/270], loss=81.1216
	step [101/270], loss=88.6858
	step [102/270], loss=70.3899
	step [103/270], loss=76.9043
	step [104/270], loss=92.7071
	step [105/270], loss=68.1802
	step [106/270], loss=72.6087
	step [107/270], loss=79.2141
	step [108/270], loss=79.0639
	step [109/270], loss=81.9584
	step [110/270], loss=59.9802
	step [111/270], loss=82.6387
	step [112/270], loss=74.3571
	step [113/270], loss=78.3644
	step [114/270], loss=71.5979
	step [115/270], loss=49.4193
	step [116/270], loss=76.3279
	step [117/270], loss=70.4506
	step [118/270], loss=83.6085
	step [119/270], loss=77.9687
	step [120/270], loss=72.5426
	step [121/270], loss=79.5587
	step [122/270], loss=66.0822
	step [123/270], loss=77.5793
	step [124/270], loss=68.6649
	step [125/270], loss=64.1480
	step [126/270], loss=94.4257
	step [127/270], loss=85.7155
	step [128/270], loss=77.7768
	step [129/270], loss=66.7546
	step [130/270], loss=69.9263
	step [131/270], loss=81.5707
	step [132/270], loss=94.1962
	step [133/270], loss=85.7878
	step [134/270], loss=93.9059
	step [135/270], loss=83.6013
	step [136/270], loss=63.9848
	step [137/270], loss=82.3557
	step [138/270], loss=87.4851
	step [139/270], loss=85.0092
	step [140/270], loss=85.0255
	step [141/270], loss=83.6109
	step [142/270], loss=77.4515
	step [143/270], loss=75.6546
	step [144/270], loss=89.3339
	step [145/270], loss=60.1298
	step [146/270], loss=82.3254
	step [147/270], loss=69.6339
	step [148/270], loss=77.8149
	step [149/270], loss=63.9521
	step [150/270], loss=77.9299
	step [151/270], loss=78.1803
	step [152/270], loss=79.6882
	step [153/270], loss=69.3771
	step [154/270], loss=81.3219
	step [155/270], loss=66.2368
	step [156/270], loss=81.8365
	step [157/270], loss=63.8545
	step [158/270], loss=91.0583
	step [159/270], loss=64.9236
	step [160/270], loss=83.8893
	step [161/270], loss=67.4738
	step [162/270], loss=76.7540
	step [163/270], loss=99.3585
	step [164/270], loss=79.6504
	step [165/270], loss=84.1739
	step [166/270], loss=62.3709
	step [167/270], loss=72.9995
	step [168/270], loss=82.4061
	step [169/270], loss=86.4099
	step [170/270], loss=66.4726
	step [171/270], loss=57.0581
	step [172/270], loss=76.6467
	step [173/270], loss=56.3739
	step [174/270], loss=80.2494
	step [175/270], loss=78.2858
	step [176/270], loss=91.6808
	step [177/270], loss=77.4354
	step [178/270], loss=70.1148
	step [179/270], loss=76.0768
	step [180/270], loss=76.4244
	step [181/270], loss=72.7580
	step [182/270], loss=84.4962
	step [183/270], loss=88.2297
	step [184/270], loss=81.3074
	step [185/270], loss=80.6457
	step [186/270], loss=62.2907
	step [187/270], loss=87.7166
	step [188/270], loss=70.7822
	step [189/270], loss=71.8787
	step [190/270], loss=77.7789
	step [191/270], loss=78.4007
	step [192/270], loss=65.4361
	step [193/270], loss=68.9174
	step [194/270], loss=69.8590
	step [195/270], loss=86.6001
	step [196/270], loss=84.6142
	step [197/270], loss=69.8240
	step [198/270], loss=64.9826
	step [199/270], loss=73.5197
	step [200/270], loss=73.9555
	step [201/270], loss=90.6739
	step [202/270], loss=82.6683
	step [203/270], loss=64.3618
	step [204/270], loss=80.3535
	step [205/270], loss=81.3552
	step [206/270], loss=82.4390
	step [207/270], loss=78.8945
	step [208/270], loss=94.7235
	step [209/270], loss=64.9692
	step [210/270], loss=80.9859
	step [211/270], loss=81.2086
	step [212/270], loss=72.4974
	step [213/270], loss=68.5702
	step [214/270], loss=101.4315
	step [215/270], loss=77.8979
	step [216/270], loss=74.6343
	step [217/270], loss=93.8134
	step [218/270], loss=70.2387
	step [219/270], loss=82.6674
	step [220/270], loss=90.9979
	step [221/270], loss=75.4117
	step [222/270], loss=64.5684
	step [223/270], loss=71.3689
	step [224/270], loss=77.2089
	step [225/270], loss=96.4925
	step [226/270], loss=76.0803
	step [227/270], loss=63.3015
	step [228/270], loss=69.9495
	step [229/270], loss=64.5587
	step [230/270], loss=64.9634
	step [231/270], loss=81.2806
	step [232/270], loss=78.1135
	step [233/270], loss=87.2239
	step [234/270], loss=66.2759
	step [235/270], loss=58.3614
	step [236/270], loss=58.8389
	step [237/270], loss=83.6878
	step [238/270], loss=79.6078
	step [239/270], loss=69.9879
	step [240/270], loss=70.9279
	step [241/270], loss=82.1647
	step [242/270], loss=70.5048
	step [243/270], loss=81.4976
	step [244/270], loss=70.4731
	step [245/270], loss=81.7984
	step [246/270], loss=71.5000
	step [247/270], loss=76.6610
	step [248/270], loss=77.8896
	step [249/270], loss=78.4012
	step [250/270], loss=80.1962
	step [251/270], loss=82.8503
	step [252/270], loss=71.5936
	step [253/270], loss=70.8699
	step [254/270], loss=65.7346
	step [255/270], loss=56.2809
	step [256/270], loss=75.5733
	step [257/270], loss=75.3446
	step [258/270], loss=71.0087
	step [259/270], loss=78.1593
	step [260/270], loss=78.4792
	step [261/270], loss=78.4816
	step [262/270], loss=78.9686
	step [263/270], loss=84.5871
	step [264/270], loss=71.0119
	step [265/270], loss=84.5126
	step [266/270], loss=67.5498
	step [267/270], loss=82.5993
	step [268/270], loss=63.4037
	step [269/270], loss=79.8815
	step [270/270], loss=10.1061
	Evaluating
	loss=0.0072, precision=0.3472, recall=0.8990, f1=0.5009
Training epoch 84
	step [1/270], loss=72.6618
	step [2/270], loss=76.4312
	step [3/270], loss=77.3752
	step [4/270], loss=81.8536
	step [5/270], loss=74.0896
	step [6/270], loss=66.4624
	step [7/270], loss=93.5990
	step [8/270], loss=74.9569
	step [9/270], loss=68.4182
	step [10/270], loss=70.7048
	step [11/270], loss=73.2897
	step [12/270], loss=69.1658
	step [13/270], loss=89.7805
	step [14/270], loss=62.3227
	step [15/270], loss=84.7091
	step [16/270], loss=70.8576
	step [17/270], loss=73.1348
	step [18/270], loss=66.0587
	step [19/270], loss=76.9577
	step [20/270], loss=74.1797
	step [21/270], loss=89.0597
	step [22/270], loss=75.7499
	step [23/270], loss=65.2758
	step [24/270], loss=79.1391
	step [25/270], loss=80.1481
	step [26/270], loss=80.9821
	step [27/270], loss=73.4219
	step [28/270], loss=65.7939
	step [29/270], loss=80.2651
	step [30/270], loss=92.1767
	step [31/270], loss=70.9791
	step [32/270], loss=63.8560
	step [33/270], loss=86.0590
	step [34/270], loss=63.0313
	step [35/270], loss=79.9282
	step [36/270], loss=79.7996
	step [37/270], loss=80.6205
	step [38/270], loss=73.7753
	step [39/270], loss=74.2429
	step [40/270], loss=83.9142
	step [41/270], loss=62.0810
	step [42/270], loss=90.6763
	step [43/270], loss=55.5560
	step [44/270], loss=77.7369
	step [45/270], loss=71.1782
	step [46/270], loss=86.0492
	step [47/270], loss=70.6745
	step [48/270], loss=84.7670
	step [49/270], loss=64.9224
	step [50/270], loss=65.2726
	step [51/270], loss=80.7001
	step [52/270], loss=73.3634
	step [53/270], loss=103.0369
	step [54/270], loss=70.1258
	step [55/270], loss=59.5277
	step [56/270], loss=74.0276
	step [57/270], loss=75.1429
	step [58/270], loss=81.5506
	step [59/270], loss=89.3408
	step [60/270], loss=86.2188
	step [61/270], loss=66.1598
	step [62/270], loss=69.2321
	step [63/270], loss=75.0325
	step [64/270], loss=77.8958
	step [65/270], loss=82.7893
	step [66/270], loss=68.4475
	step [67/270], loss=66.2962
	step [68/270], loss=65.4965
	step [69/270], loss=70.8293
	step [70/270], loss=65.2119
	step [71/270], loss=84.6780
	step [72/270], loss=84.3461
	step [73/270], loss=78.9652
	step [74/270], loss=85.3326
	step [75/270], loss=82.6743
	step [76/270], loss=89.2389
	step [77/270], loss=67.5658
	step [78/270], loss=70.8496
	step [79/270], loss=94.8599
	step [80/270], loss=72.4031
	step [81/270], loss=75.9192
	step [82/270], loss=71.3624
	step [83/270], loss=59.4072
	step [84/270], loss=67.7590
	step [85/270], loss=68.4610
	step [86/270], loss=79.5546
	step [87/270], loss=69.3497
	step [88/270], loss=98.8791
	step [89/270], loss=60.6978
	step [90/270], loss=76.6975
	step [91/270], loss=65.6735
	step [92/270], loss=76.9073
	step [93/270], loss=71.4171
	step [94/270], loss=66.9142
	step [95/270], loss=65.5850
	step [96/270], loss=65.0431
	step [97/270], loss=73.7474
	step [98/270], loss=78.0182
	step [99/270], loss=77.4054
	step [100/270], loss=74.4902
	step [101/270], loss=82.7157
	step [102/270], loss=86.8625
	step [103/270], loss=76.0730
	step [104/270], loss=73.3598
	step [105/270], loss=76.8505
	step [106/270], loss=66.2908
	step [107/270], loss=92.2709
	step [108/270], loss=75.7260
	step [109/270], loss=77.5776
	step [110/270], loss=65.5428
	step [111/270], loss=76.0645
	step [112/270], loss=80.9739
	step [113/270], loss=74.4868
	step [114/270], loss=68.4425
	step [115/270], loss=71.3853
	step [116/270], loss=82.7530
	step [117/270], loss=75.9534
	step [118/270], loss=68.0013
	step [119/270], loss=85.1397
	step [120/270], loss=77.1557
	step [121/270], loss=76.4744
	step [122/270], loss=82.0694
	step [123/270], loss=64.8293
	step [124/270], loss=87.8965
	step [125/270], loss=75.0586
	step [126/270], loss=72.8579
	step [127/270], loss=88.9465
	step [128/270], loss=70.9522
	step [129/270], loss=63.2004
	step [130/270], loss=80.8545
	step [131/270], loss=68.8057
	step [132/270], loss=65.3970
	step [133/270], loss=75.1957
	step [134/270], loss=82.0790
	step [135/270], loss=69.1125
	step [136/270], loss=77.7598
	step [137/270], loss=71.2513
	step [138/270], loss=75.1897
	step [139/270], loss=69.9444
	step [140/270], loss=80.0536
	step [141/270], loss=67.3479
	step [142/270], loss=72.3169
	step [143/270], loss=63.6429
	step [144/270], loss=71.2906
	step [145/270], loss=67.0425
	step [146/270], loss=73.7292
	step [147/270], loss=67.0472
	step [148/270], loss=65.3972
	step [149/270], loss=81.5965
	step [150/270], loss=76.3151
	step [151/270], loss=74.0123
	step [152/270], loss=89.9435
	step [153/270], loss=82.9297
	step [154/270], loss=85.2722
	step [155/270], loss=81.8617
	step [156/270], loss=78.9159
	step [157/270], loss=74.7399
	step [158/270], loss=65.7506
	step [159/270], loss=68.9816
	step [160/270], loss=70.4672
	step [161/270], loss=69.7530
	step [162/270], loss=80.7251
	step [163/270], loss=77.7659
	step [164/270], loss=62.7695
	step [165/270], loss=73.4552
	step [166/270], loss=63.2272
	step [167/270], loss=72.7171
	step [168/270], loss=72.2136
	step [169/270], loss=68.7115
	step [170/270], loss=77.4355
	step [171/270], loss=82.1531
	step [172/270], loss=92.6545
	step [173/270], loss=79.1692
	step [174/270], loss=77.0787
	step [175/270], loss=87.7860
	step [176/270], loss=73.5076
	step [177/270], loss=76.9384
	step [178/270], loss=74.4104
	step [179/270], loss=69.0436
	step [180/270], loss=88.3836
	step [181/270], loss=73.8395
	step [182/270], loss=108.1995
	step [183/270], loss=91.2035
	step [184/270], loss=69.6856
	step [185/270], loss=77.4959
	step [186/270], loss=87.4911
	step [187/270], loss=73.1390
	step [188/270], loss=70.5414
	step [189/270], loss=84.2310
	step [190/270], loss=66.5597
	step [191/270], loss=77.3198
	step [192/270], loss=83.0955
	step [193/270], loss=82.5844
	step [194/270], loss=74.9908
	step [195/270], loss=94.4111
	step [196/270], loss=73.5474
	step [197/270], loss=74.2821
	step [198/270], loss=79.8714
	step [199/270], loss=63.0510
	step [200/270], loss=70.2129
	step [201/270], loss=75.6377
	step [202/270], loss=87.7438
	step [203/270], loss=80.8924
	step [204/270], loss=79.1628
	step [205/270], loss=71.7324
	step [206/270], loss=67.6508
	step [207/270], loss=73.1045
	step [208/270], loss=69.1001
	step [209/270], loss=79.1747
	step [210/270], loss=83.3524
	step [211/270], loss=98.1352
	step [212/270], loss=74.5000
	step [213/270], loss=85.3374
	step [214/270], loss=96.0979
	step [215/270], loss=70.3161
	step [216/270], loss=64.8371
	step [217/270], loss=84.2271
	step [218/270], loss=67.6928
	step [219/270], loss=59.2850
	step [220/270], loss=85.1980
	step [221/270], loss=72.8191
	step [222/270], loss=72.9085
	step [223/270], loss=71.7799
	step [224/270], loss=69.5856
	step [225/270], loss=71.6226
	step [226/270], loss=67.9997
	step [227/270], loss=81.0766
	step [228/270], loss=80.8757
	step [229/270], loss=72.5899
	step [230/270], loss=78.7887
	step [231/270], loss=67.5234
	step [232/270], loss=84.6191
	step [233/270], loss=69.4215
	step [234/270], loss=56.5350
	step [235/270], loss=79.1377
	step [236/270], loss=65.4345
	step [237/270], loss=61.8276
	step [238/270], loss=82.8249
	step [239/270], loss=85.9812
	step [240/270], loss=82.5616
	step [241/270], loss=81.2649
	step [242/270], loss=77.4430
	step [243/270], loss=84.4455
	step [244/270], loss=62.2036
	step [245/270], loss=73.0469
	step [246/270], loss=80.9040
	step [247/270], loss=68.2431
	step [248/270], loss=83.6651
	step [249/270], loss=72.6430
	step [250/270], loss=66.3025
	step [251/270], loss=80.1877
	step [252/270], loss=65.7262
	step [253/270], loss=78.1783
	step [254/270], loss=82.1045
	step [255/270], loss=77.7433
	step [256/270], loss=82.0611
	step [257/270], loss=88.2694
	step [258/270], loss=84.0457
	step [259/270], loss=78.9453
	step [260/270], loss=57.9158
	step [261/270], loss=87.3376
	step [262/270], loss=70.7377
	step [263/270], loss=72.0149
	step [264/270], loss=79.7089
	step [265/270], loss=86.1730
	step [266/270], loss=75.8881
	step [267/270], loss=84.5268
	step [268/270], loss=77.5530
	step [269/270], loss=72.6202
	step [270/270], loss=7.7000
	Evaluating
	loss=0.0067, precision=0.3795, recall=0.8948, f1=0.5329
saving model as: 4_saved_model.pth
Training epoch 85
	step [1/270], loss=67.2362
	step [2/270], loss=71.8254
	step [3/270], loss=61.9977
	step [4/270], loss=61.5577
	step [5/270], loss=72.9284
	step [6/270], loss=60.3241
	step [7/270], loss=70.8642
	step [8/270], loss=71.4895
	step [9/270], loss=68.0071
	step [10/270], loss=74.8445
	step [11/270], loss=84.4085
	step [12/270], loss=85.4229
	step [13/270], loss=57.2316
	step [14/270], loss=86.0538
	step [15/270], loss=77.0460
	step [16/270], loss=74.0364
	step [17/270], loss=73.4860
	step [18/270], loss=65.5122
	step [19/270], loss=83.8588
	step [20/270], loss=63.2165
	step [21/270], loss=86.3854
	step [22/270], loss=77.4122
	step [23/270], loss=71.2968
	step [24/270], loss=79.4120
	step [25/270], loss=90.3344
	step [26/270], loss=75.2604
	step [27/270], loss=85.3544
	step [28/270], loss=72.7938
	step [29/270], loss=83.8606
	step [30/270], loss=71.2125
	step [31/270], loss=98.0690
	step [32/270], loss=87.2133
	step [33/270], loss=65.8817
	step [34/270], loss=79.4356
	step [35/270], loss=73.6673
	step [36/270], loss=95.4670
	step [37/270], loss=69.5528
	step [38/270], loss=64.8616
	step [39/270], loss=92.7383
	step [40/270], loss=78.1953
	step [41/270], loss=74.4143
	step [42/270], loss=73.8784
	step [43/270], loss=75.1169
	step [44/270], loss=75.7810
	step [45/270], loss=72.1461
	step [46/270], loss=85.7690
	step [47/270], loss=75.0451
	step [48/270], loss=81.7138
	step [49/270], loss=78.4597
	step [50/270], loss=74.5158
	step [51/270], loss=70.7151
	step [52/270], loss=77.7847
	step [53/270], loss=72.2554
	step [54/270], loss=80.8655
	step [55/270], loss=69.4074
	step [56/270], loss=75.5813
	step [57/270], loss=71.0509
	step [58/270], loss=71.3542
	step [59/270], loss=69.2872
	step [60/270], loss=65.2156
	step [61/270], loss=95.1381
	step [62/270], loss=74.0107
	step [63/270], loss=73.5412
	step [64/270], loss=61.2494
	step [65/270], loss=78.2705
	step [66/270], loss=83.0748
	step [67/270], loss=74.4909
	step [68/270], loss=77.1985
	step [69/270], loss=80.7982
	step [70/270], loss=81.1088
	step [71/270], loss=81.4438
	step [72/270], loss=82.8542
	step [73/270], loss=98.9017
	step [74/270], loss=81.5046
	step [75/270], loss=87.0855
	step [76/270], loss=94.0920
	step [77/270], loss=80.1308
	step [78/270], loss=69.5210
	step [79/270], loss=60.9135
	step [80/270], loss=70.8090
	step [81/270], loss=71.3410
	step [82/270], loss=83.3069
	step [83/270], loss=71.0608
	step [84/270], loss=93.5631
	step [85/270], loss=79.9870
	step [86/270], loss=74.3806
	step [87/270], loss=74.5368
	step [88/270], loss=83.8205
	step [89/270], loss=65.9153
	step [90/270], loss=77.1467
	step [91/270], loss=75.9024
	step [92/270], loss=67.4858
	step [93/270], loss=79.3544
	step [94/270], loss=77.2022
	step [95/270], loss=79.8978
	step [96/270], loss=64.2269
	step [97/270], loss=77.9570
	step [98/270], loss=67.9254
	step [99/270], loss=81.3313
	step [100/270], loss=75.7046
	step [101/270], loss=73.5364
	step [102/270], loss=77.1523
	step [103/270], loss=85.3001
	step [104/270], loss=60.9016
	step [105/270], loss=66.3610
	step [106/270], loss=73.2770
	step [107/270], loss=77.1962
	step [108/270], loss=60.4739
	step [109/270], loss=72.1907
	step [110/270], loss=81.1515
	step [111/270], loss=63.1467
	step [112/270], loss=75.6476
	step [113/270], loss=59.5535
	step [114/270], loss=74.9985
	step [115/270], loss=81.9422
	step [116/270], loss=63.3350
	step [117/270], loss=62.7575
	step [118/270], loss=75.8575
	step [119/270], loss=92.9437
	step [120/270], loss=72.6928
	step [121/270], loss=71.8323
	step [122/270], loss=82.5566
	step [123/270], loss=89.6863
	step [124/270], loss=70.9777
	step [125/270], loss=68.1772
	step [126/270], loss=73.9744
	step [127/270], loss=66.8722
	step [128/270], loss=67.7032
	step [129/270], loss=96.2670
	step [130/270], loss=75.8389
	step [131/270], loss=79.1849
	step [132/270], loss=66.3779
	step [133/270], loss=72.8954
	step [134/270], loss=69.6023
	step [135/270], loss=77.3326
	step [136/270], loss=89.0355
	step [137/270], loss=74.6301
	step [138/270], loss=73.3155
	step [139/270], loss=81.6976
	step [140/270], loss=58.7318
	step [141/270], loss=71.7098
	step [142/270], loss=93.3716
	step [143/270], loss=76.7812
	step [144/270], loss=71.6179
	step [145/270], loss=73.2168
	step [146/270], loss=58.1719
	step [147/270], loss=71.0779
	step [148/270], loss=76.1776
	step [149/270], loss=67.0758
	step [150/270], loss=73.7992
	step [151/270], loss=77.4993
	step [152/270], loss=70.5198
	step [153/270], loss=89.8753
	step [154/270], loss=78.1685
	step [155/270], loss=75.2371
	step [156/270], loss=77.3472
	step [157/270], loss=66.8525
	step [158/270], loss=71.1924
	step [159/270], loss=72.3708
	step [160/270], loss=70.2953
	step [161/270], loss=84.6886
	step [162/270], loss=70.4268
	step [163/270], loss=77.6313
	step [164/270], loss=67.7563
	step [165/270], loss=76.8421
	step [166/270], loss=73.5162
	step [167/270], loss=97.7242
	step [168/270], loss=85.3670
	step [169/270], loss=71.2015
	step [170/270], loss=81.7680
	step [171/270], loss=91.4880
	step [172/270], loss=87.7730
	step [173/270], loss=84.7837
	step [174/270], loss=63.4115
	step [175/270], loss=81.0812
	step [176/270], loss=68.8222
	step [177/270], loss=70.9769
	step [178/270], loss=80.2704
	step [179/270], loss=82.3855
	step [180/270], loss=69.6595
	step [181/270], loss=78.2001
	step [182/270], loss=78.2253
	step [183/270], loss=70.8127
	step [184/270], loss=77.9700
	step [185/270], loss=70.2176
	step [186/270], loss=75.3615
	step [187/270], loss=70.8046
	step [188/270], loss=89.0642
	step [189/270], loss=78.3469
	step [190/270], loss=65.6810
	step [191/270], loss=76.1705
	step [192/270], loss=62.4112
	step [193/270], loss=74.6476
	step [194/270], loss=88.9524
	step [195/270], loss=76.5580
	step [196/270], loss=80.8159
	step [197/270], loss=75.6184
	step [198/270], loss=75.5427
	step [199/270], loss=77.0638
	step [200/270], loss=78.7995
	step [201/270], loss=77.0315
	step [202/270], loss=57.7806
	step [203/270], loss=82.9766
	step [204/270], loss=62.7629
	step [205/270], loss=56.3304
	step [206/270], loss=95.6203
	step [207/270], loss=81.7596
	step [208/270], loss=77.9850
	step [209/270], loss=71.5225
	step [210/270], loss=72.8873
	step [211/270], loss=66.4115
	step [212/270], loss=51.0470
	step [213/270], loss=77.6253
	step [214/270], loss=81.2951
	step [215/270], loss=75.1742
	step [216/270], loss=80.2690
	step [217/270], loss=79.2921
	step [218/270], loss=80.2595
	step [219/270], loss=75.0751
	step [220/270], loss=54.5849
	step [221/270], loss=54.2411
	step [222/270], loss=65.3082
	step [223/270], loss=76.9435
	step [224/270], loss=94.5364
	step [225/270], loss=67.3958
	step [226/270], loss=66.0665
	step [227/270], loss=84.8255
	step [228/270], loss=85.2209
	step [229/270], loss=103.2156
	step [230/270], loss=73.7252
	step [231/270], loss=82.3707
	step [232/270], loss=77.1666
	step [233/270], loss=88.2124
	step [234/270], loss=67.8647
	step [235/270], loss=70.6414
	step [236/270], loss=79.5922
	step [237/270], loss=75.0529
	step [238/270], loss=77.6478
	step [239/270], loss=62.1360
	step [240/270], loss=83.6106
	step [241/270], loss=84.7061
	step [242/270], loss=72.8906
	step [243/270], loss=70.8415
	step [244/270], loss=60.9371
	step [245/270], loss=76.4307
	step [246/270], loss=76.5062
	step [247/270], loss=83.2892
	step [248/270], loss=69.0099
	step [249/270], loss=82.9311
	step [250/270], loss=83.7970
	step [251/270], loss=72.3226
	step [252/270], loss=74.7973
	step [253/270], loss=81.2127
	step [254/270], loss=81.5241
	step [255/270], loss=76.2633
	step [256/270], loss=66.0643
	step [257/270], loss=57.8521
	step [258/270], loss=85.3995
	step [259/270], loss=80.9468
	step [260/270], loss=66.5500
	step [261/270], loss=83.8515
	step [262/270], loss=70.3345
	step [263/270], loss=78.6038
	step [264/270], loss=73.9046
	step [265/270], loss=79.9610
	step [266/270], loss=56.5931
	step [267/270], loss=80.0161
	step [268/270], loss=75.1194
	step [269/270], loss=76.5591
	step [270/270], loss=5.8997
	Evaluating
	loss=0.0084, precision=0.3084, recall=0.9006, f1=0.4595
Training epoch 86
	step [1/270], loss=69.0094
	step [2/270], loss=81.5040
	step [3/270], loss=57.8539
	step [4/270], loss=81.4361
	step [5/270], loss=77.2785
	step [6/270], loss=73.6264
	step [7/270], loss=74.2476
	step [8/270], loss=60.6305
	step [9/270], loss=89.0332
	step [10/270], loss=81.9360
	step [11/270], loss=71.7092
	step [12/270], loss=67.7851
	step [13/270], loss=59.6750
	step [14/270], loss=74.5814
	step [15/270], loss=71.0900
	step [16/270], loss=78.8147
	step [17/270], loss=79.3829
	step [18/270], loss=74.3197
	step [19/270], loss=88.8571
	step [20/270], loss=66.7795
	step [21/270], loss=68.7570
	step [22/270], loss=70.9083
	step [23/270], loss=71.1072
	step [24/270], loss=65.5572
	step [25/270], loss=74.6077
	step [26/270], loss=73.2633
	step [27/270], loss=73.6527
	step [28/270], loss=88.2673
	step [29/270], loss=69.6781
	step [30/270], loss=80.7006
	step [31/270], loss=77.6060
	step [32/270], loss=81.6859
	step [33/270], loss=70.5606
	step [34/270], loss=68.5636
	step [35/270], loss=72.8350
	step [36/270], loss=75.8515
	step [37/270], loss=78.2445
	step [38/270], loss=78.6173
	step [39/270], loss=65.4641
	step [40/270], loss=82.5086
	step [41/270], loss=70.4935
	step [42/270], loss=57.8090
	step [43/270], loss=77.3160
	step [44/270], loss=78.9665
	step [45/270], loss=75.4778
	step [46/270], loss=76.5990
	step [47/270], loss=73.8219
	step [48/270], loss=80.7001
	step [49/270], loss=86.8655
	step [50/270], loss=71.1981
	step [51/270], loss=86.2656
	step [52/270], loss=86.4022
	step [53/270], loss=81.1418
	step [54/270], loss=74.3895
	step [55/270], loss=68.5124
	step [56/270], loss=83.1066
	step [57/270], loss=66.0180
	step [58/270], loss=65.3237
	step [59/270], loss=77.2955
	step [60/270], loss=59.1897
	step [61/270], loss=88.7754
	step [62/270], loss=70.5846
	step [63/270], loss=63.3018
	step [64/270], loss=61.2660
	step [65/270], loss=76.1453
	step [66/270], loss=79.2731
	step [67/270], loss=67.3597
	step [68/270], loss=93.1521
	step [69/270], loss=81.9031
	step [70/270], loss=66.5859
	step [71/270], loss=78.3768
	step [72/270], loss=73.6198
	step [73/270], loss=74.2823
	step [74/270], loss=92.9948
	step [75/270], loss=81.0135
	step [76/270], loss=69.3432
	step [77/270], loss=74.4091
	step [78/270], loss=74.4230
	step [79/270], loss=62.0822
	step [80/270], loss=75.9179
	step [81/270], loss=58.1997
	step [82/270], loss=82.9290
	step [83/270], loss=69.8483
	step [84/270], loss=76.3256
	step [85/270], loss=71.9525
	step [86/270], loss=85.1419
	step [87/270], loss=87.3304
	step [88/270], loss=81.6156
	step [89/270], loss=83.2310
	step [90/270], loss=67.6660
	step [91/270], loss=70.5928
	step [92/270], loss=79.5311
	step [93/270], loss=53.0447
	step [94/270], loss=75.8352
	step [95/270], loss=77.0388
	step [96/270], loss=61.8468
	step [97/270], loss=67.7384
	step [98/270], loss=79.6056
	step [99/270], loss=73.8941
	step [100/270], loss=72.9318
	step [101/270], loss=75.3943
	step [102/270], loss=83.1895
	step [103/270], loss=71.3615
	step [104/270], loss=83.0811
	step [105/270], loss=80.7748
	step [106/270], loss=69.0838
	step [107/270], loss=80.5023
	step [108/270], loss=77.7454
	step [109/270], loss=84.1449
	step [110/270], loss=69.0000
	step [111/270], loss=77.0649
	step [112/270], loss=77.4177
	step [113/270], loss=66.4979
	step [114/270], loss=84.6641
	step [115/270], loss=65.7902
	step [116/270], loss=71.1287
	step [117/270], loss=67.5859
	step [118/270], loss=75.6607
	step [119/270], loss=66.9873
	step [120/270], loss=77.7550
	step [121/270], loss=67.1323
	step [122/270], loss=85.4753
	step [123/270], loss=70.0431
	step [124/270], loss=87.6263
	step [125/270], loss=66.1311
	step [126/270], loss=82.8899
	step [127/270], loss=77.3827
	step [128/270], loss=84.2351
	step [129/270], loss=75.7524
	step [130/270], loss=79.9183
	step [131/270], loss=72.2308
	step [132/270], loss=89.4074
	step [133/270], loss=75.0612
	step [134/270], loss=74.8380
	step [135/270], loss=72.8183
	step [136/270], loss=80.3415
	step [137/270], loss=62.6486
	step [138/270], loss=77.6074
	step [139/270], loss=88.6147
	step [140/270], loss=59.0757
	step [141/270], loss=81.1476
	step [142/270], loss=82.2417
	step [143/270], loss=74.4567
	step [144/270], loss=82.3984
	step [145/270], loss=68.8290
	step [146/270], loss=95.1253
	step [147/270], loss=78.3745
	step [148/270], loss=73.9495
	step [149/270], loss=81.1924
	step [150/270], loss=72.4646
	step [151/270], loss=58.7788
	step [152/270], loss=72.9555
	step [153/270], loss=73.1797
	step [154/270], loss=80.9106
	step [155/270], loss=73.7118
	step [156/270], loss=87.4902
	step [157/270], loss=80.0161
	step [158/270], loss=70.6864
	step [159/270], loss=71.4897
	step [160/270], loss=78.1028
	step [161/270], loss=83.1636
	step [162/270], loss=79.1652
	step [163/270], loss=63.5902
	step [164/270], loss=68.1064
	step [165/270], loss=87.5506
	step [166/270], loss=65.5949
	step [167/270], loss=72.5654
	step [168/270], loss=84.1317
	step [169/270], loss=69.7092
	step [170/270], loss=73.3907
	step [171/270], loss=59.5533
	step [172/270], loss=66.5505
	step [173/270], loss=76.3654
	step [174/270], loss=71.9246
	step [175/270], loss=60.8981
	step [176/270], loss=76.7541
	step [177/270], loss=64.0885
	step [178/270], loss=66.4201
	step [179/270], loss=59.9652
	step [180/270], loss=69.5892
	step [181/270], loss=70.3721
	step [182/270], loss=63.6797
	step [183/270], loss=80.4035
	step [184/270], loss=80.7210
	step [185/270], loss=75.4787
	step [186/270], loss=79.1397
	step [187/270], loss=57.1371
	step [188/270], loss=70.6166
	step [189/270], loss=75.7301
	step [190/270], loss=93.6567
	step [191/270], loss=67.7580
	step [192/270], loss=85.0871
	step [193/270], loss=79.9353
	step [194/270], loss=81.8957
	step [195/270], loss=76.4663
	step [196/270], loss=75.5108
	step [197/270], loss=71.3553
	step [198/270], loss=80.3112
	step [199/270], loss=81.8075
	step [200/270], loss=81.5095
	step [201/270], loss=88.2352
	step [202/270], loss=77.0913
	step [203/270], loss=79.2718
	step [204/270], loss=74.2185
	step [205/270], loss=86.0124
	step [206/270], loss=66.5517
	step [207/270], loss=69.6714
	step [208/270], loss=62.0526
	step [209/270], loss=72.2684
	step [210/270], loss=78.9049
	step [211/270], loss=77.2021
	step [212/270], loss=87.1363
	step [213/270], loss=82.9263
	step [214/270], loss=69.5612
	step [215/270], loss=75.6288
	step [216/270], loss=76.3188
	step [217/270], loss=95.8190
	step [218/270], loss=61.3660
	step [219/270], loss=62.2197
	step [220/270], loss=72.5946
	step [221/270], loss=75.4913
	step [222/270], loss=69.1901
	step [223/270], loss=85.0183
	step [224/270], loss=78.0559
	step [225/270], loss=77.6759
	step [226/270], loss=84.7181
	step [227/270], loss=77.8721
	step [228/270], loss=69.1897
	step [229/270], loss=72.0253
	step [230/270], loss=69.1074
	step [231/270], loss=81.6043
	step [232/270], loss=72.9773
	step [233/270], loss=88.8488
	step [234/270], loss=80.1980
	step [235/270], loss=65.3200
	step [236/270], loss=91.6302
	step [237/270], loss=84.8250
	step [238/270], loss=67.4364
	step [239/270], loss=73.9525
	step [240/270], loss=76.8376
	step [241/270], loss=74.2664
	step [242/270], loss=80.6068
	step [243/270], loss=68.8258
	step [244/270], loss=57.6909
	step [245/270], loss=75.7816
	step [246/270], loss=76.8978
	step [247/270], loss=67.9580
	step [248/270], loss=82.6695
	step [249/270], loss=78.6674
	step [250/270], loss=72.6339
	step [251/270], loss=76.6009
	step [252/270], loss=67.9923
	step [253/270], loss=76.3540
	step [254/270], loss=90.0430
	step [255/270], loss=79.2782
	step [256/270], loss=84.2665
	step [257/270], loss=77.1048
	step [258/270], loss=79.2420
	step [259/270], loss=80.9688
	step [260/270], loss=72.3940
	step [261/270], loss=84.9374
	step [262/270], loss=96.1908
	step [263/270], loss=69.5462
	step [264/270], loss=64.4232
	step [265/270], loss=78.8042
	step [266/270], loss=76.7524
	step [267/270], loss=72.3424
	step [268/270], loss=74.8570
	step [269/270], loss=55.5520
	step [270/270], loss=13.3168
	Evaluating
	loss=0.0074, precision=0.3367, recall=0.9030, f1=0.4905
Training epoch 87
	step [1/270], loss=69.9065
	step [2/270], loss=71.1441
	step [3/270], loss=62.8924
	step [4/270], loss=83.9732
	step [5/270], loss=64.4917
	step [6/270], loss=75.2831
	step [7/270], loss=69.0754
	step [8/270], loss=84.8069
	step [9/270], loss=71.6967
	step [10/270], loss=77.6825
	step [11/270], loss=58.0547
	step [12/270], loss=68.2180
	step [13/270], loss=74.8474
	step [14/270], loss=74.1354
	step [15/270], loss=77.6795
	step [16/270], loss=82.2402
	step [17/270], loss=77.9450
	step [18/270], loss=86.9568
	step [19/270], loss=64.0208
	step [20/270], loss=78.5863
	step [21/270], loss=57.8969
	step [22/270], loss=73.4245
	step [23/270], loss=72.4088
	step [24/270], loss=88.3923
	step [25/270], loss=90.0948
	step [26/270], loss=73.4604
	step [27/270], loss=97.4275
	step [28/270], loss=73.6306
	step [29/270], loss=77.0121
	step [30/270], loss=76.2170
	step [31/270], loss=70.9559
	step [32/270], loss=69.9652
	step [33/270], loss=63.6628
	step [34/270], loss=78.3899
	step [35/270], loss=65.1006
	step [36/270], loss=96.4611
	step [37/270], loss=77.6302
	step [38/270], loss=85.4885
	step [39/270], loss=74.6275
	step [40/270], loss=68.0131
	step [41/270], loss=79.1335
	step [42/270], loss=83.3066
	step [43/270], loss=74.8879
	step [44/270], loss=78.3133
	step [45/270], loss=79.2214
	step [46/270], loss=83.9900
	step [47/270], loss=71.1928
	step [48/270], loss=68.7654
	step [49/270], loss=71.4339
	step [50/270], loss=75.9402
	step [51/270], loss=87.5846
	step [52/270], loss=57.2999
	step [53/270], loss=79.1209
	step [54/270], loss=74.8560
	step [55/270], loss=68.1421
	step [56/270], loss=63.1258
	step [57/270], loss=64.9694
	step [58/270], loss=69.5993
	step [59/270], loss=66.5235
	step [60/270], loss=59.4741
	step [61/270], loss=86.9131
	step [62/270], loss=75.0652
	step [63/270], loss=72.0227
	step [64/270], loss=68.3657
	step [65/270], loss=90.3583
	step [66/270], loss=85.8543
	step [67/270], loss=80.0280
	step [68/270], loss=72.8386
	step [69/270], loss=68.3377
	step [70/270], loss=75.6199
	step [71/270], loss=79.1747
	step [72/270], loss=87.0584
	step [73/270], loss=60.6043
	step [74/270], loss=68.9364
	step [75/270], loss=72.4731
	step [76/270], loss=76.4625
	step [77/270], loss=75.7409
	step [78/270], loss=81.9372
	step [79/270], loss=75.8214
	step [80/270], loss=76.3460
	step [81/270], loss=72.4626
	step [82/270], loss=98.7019
	step [83/270], loss=72.0438
	step [84/270], loss=67.1606
	step [85/270], loss=74.6476
	step [86/270], loss=68.8932
	step [87/270], loss=76.6325
	step [88/270], loss=84.6208
	step [89/270], loss=82.0146
	step [90/270], loss=79.5526
	step [91/270], loss=74.5595
	step [92/270], loss=64.6240
	step [93/270], loss=72.9308
	step [94/270], loss=68.2514
	step [95/270], loss=87.7483
	step [96/270], loss=83.3435
	step [97/270], loss=76.1442
	step [98/270], loss=81.6187
	step [99/270], loss=79.9282
	step [100/270], loss=72.0831
	step [101/270], loss=71.7512
	step [102/270], loss=67.4115
	step [103/270], loss=78.4690
	step [104/270], loss=81.3067
	step [105/270], loss=77.4572
	step [106/270], loss=68.0245
	step [107/270], loss=71.6509
	step [108/270], loss=96.6529
	step [109/270], loss=77.5634
	step [110/270], loss=75.9392
	step [111/270], loss=65.6698
	step [112/270], loss=68.5470
	step [113/270], loss=76.8154
	step [114/270], loss=80.9931
	step [115/270], loss=93.0668
	step [116/270], loss=76.5564
	step [117/270], loss=72.2203
	step [118/270], loss=73.9928
	step [119/270], loss=65.8145
	step [120/270], loss=79.8573
	step [121/270], loss=82.2421
	step [122/270], loss=66.5664
	step [123/270], loss=75.4026
	step [124/270], loss=84.6560
	step [125/270], loss=66.8440
	step [126/270], loss=72.8879
	step [127/270], loss=75.9278
	step [128/270], loss=70.0081
	step [129/270], loss=74.6186
	step [130/270], loss=80.4249
	step [131/270], loss=81.4245
	step [132/270], loss=75.3842
	step [133/270], loss=76.5651
	step [134/270], loss=64.4049
	step [135/270], loss=74.2330
	step [136/270], loss=85.8925
	step [137/270], loss=78.7794
	step [138/270], loss=79.0517
	step [139/270], loss=81.8929
	step [140/270], loss=72.4734
	step [141/270], loss=70.5979
	step [142/270], loss=76.8344
	step [143/270], loss=81.1449
	step [144/270], loss=68.2099
	step [145/270], loss=65.3445
	step [146/270], loss=67.3986
	step [147/270], loss=78.2432
	step [148/270], loss=74.8817
	step [149/270], loss=88.8562
	step [150/270], loss=77.8579
	step [151/270], loss=90.5365
	step [152/270], loss=73.2067
	step [153/270], loss=79.2403
	step [154/270], loss=66.3896
	step [155/270], loss=70.1875
	step [156/270], loss=53.6845
	step [157/270], loss=83.7432
	step [158/270], loss=76.3930
	step [159/270], loss=75.3640
	step [160/270], loss=61.8046
	step [161/270], loss=92.5827
	step [162/270], loss=86.9017
	step [163/270], loss=87.0962
	step [164/270], loss=82.0002
	step [165/270], loss=73.8124
	step [166/270], loss=65.1574
	step [167/270], loss=68.6111
	step [168/270], loss=69.7136
	step [169/270], loss=72.4300
	step [170/270], loss=60.3436
	step [171/270], loss=72.7539
	step [172/270], loss=78.4864
	step [173/270], loss=61.2468
	step [174/270], loss=69.5446
	step [175/270], loss=76.5317
	step [176/270], loss=69.0894
	step [177/270], loss=71.5167
	step [178/270], loss=79.1889
	step [179/270], loss=68.2602
	step [180/270], loss=72.4142
	step [181/270], loss=80.2216
	step [182/270], loss=77.8111
	step [183/270], loss=78.9156
	step [184/270], loss=63.3678
	step [185/270], loss=93.4621
	step [186/270], loss=84.7388
	step [187/270], loss=87.3386
	step [188/270], loss=60.6973
	step [189/270], loss=81.6776
	step [190/270], loss=84.8115
	step [191/270], loss=70.7646
	step [192/270], loss=86.6776
	step [193/270], loss=75.9424
	step [194/270], loss=84.8797
	step [195/270], loss=80.9496
	step [196/270], loss=83.3311
	step [197/270], loss=66.4821
	step [198/270], loss=70.3603
	step [199/270], loss=81.3863
	step [200/270], loss=74.4122
	step [201/270], loss=85.9113
	step [202/270], loss=71.0521
	step [203/270], loss=64.6795
	step [204/270], loss=56.9100
	step [205/270], loss=85.8952
	step [206/270], loss=84.8217
	step [207/270], loss=60.9030
	step [208/270], loss=74.8979
	step [209/270], loss=73.9336
	step [210/270], loss=76.1731
	step [211/270], loss=66.1140
	step [212/270], loss=86.3870
	step [213/270], loss=74.7211
	step [214/270], loss=63.5424
	step [215/270], loss=70.9693
	step [216/270], loss=70.2356
	step [217/270], loss=72.3009
	step [218/270], loss=56.6743
	step [219/270], loss=78.2401
	step [220/270], loss=67.3785
	step [221/270], loss=84.9417
	step [222/270], loss=57.8028
	step [223/270], loss=75.1173
	step [224/270], loss=78.3878
	step [225/270], loss=64.5924
	step [226/270], loss=58.0878
	step [227/270], loss=90.4265
	step [228/270], loss=88.6790
	step [229/270], loss=70.4214
	step [230/270], loss=75.6102
	step [231/270], loss=81.7225
	step [232/270], loss=76.6984
	step [233/270], loss=70.2113
	step [234/270], loss=70.5298
	step [235/270], loss=69.9831
	step [236/270], loss=77.2588
	step [237/270], loss=72.6650
	step [238/270], loss=81.8671
	step [239/270], loss=77.3513
	step [240/270], loss=72.0676
	step [241/270], loss=76.1654
	step [242/270], loss=70.7143
	step [243/270], loss=74.2317
	step [244/270], loss=65.7409
	step [245/270], loss=58.9399
	step [246/270], loss=73.2903
	step [247/270], loss=66.5772
	step [248/270], loss=78.1783
	step [249/270], loss=76.5629
	step [250/270], loss=66.8423
	step [251/270], loss=85.5686
	step [252/270], loss=65.7216
	step [253/270], loss=67.9679
	step [254/270], loss=85.4335
	step [255/270], loss=79.7971
	step [256/270], loss=85.0189
	step [257/270], loss=80.4873
	step [258/270], loss=84.8639
	step [259/270], loss=83.9935
	step [260/270], loss=80.3558
	step [261/270], loss=71.7622
	step [262/270], loss=60.3191
	step [263/270], loss=82.9291
	step [264/270], loss=73.5042
	step [265/270], loss=88.5466
	step [266/270], loss=80.0978
	step [267/270], loss=64.8379
	step [268/270], loss=71.8839
	step [269/270], loss=75.6522
	step [270/270], loss=9.3071
	Evaluating
	loss=0.0073, precision=0.3570, recall=0.9051, f1=0.5121
Training epoch 88
	step [1/270], loss=68.8567
	step [2/270], loss=60.6259
	step [3/270], loss=83.0228
	step [4/270], loss=65.4004
	step [5/270], loss=80.9840
	step [6/270], loss=72.4066
	step [7/270], loss=73.2422
	step [8/270], loss=64.3111
	step [9/270], loss=71.0374
	step [10/270], loss=72.4641
	step [11/270], loss=79.6706
	step [12/270], loss=87.9039
	step [13/270], loss=76.9094
	step [14/270], loss=77.3384
	step [15/270], loss=55.6817
	step [16/270], loss=69.8671
	step [17/270], loss=73.8652
	step [18/270], loss=84.8362
	step [19/270], loss=61.5654
	step [20/270], loss=77.6422
	step [21/270], loss=75.1993
	step [22/270], loss=72.4229
	step [23/270], loss=82.0741
	step [24/270], loss=72.9995
	step [25/270], loss=73.8460
	step [26/270], loss=73.0861
	step [27/270], loss=72.2044
	step [28/270], loss=66.8185
	step [29/270], loss=56.2912
	step [30/270], loss=72.3568
	step [31/270], loss=62.4536
	step [32/270], loss=83.0151
	step [33/270], loss=76.6346
	step [34/270], loss=74.5485
	step [35/270], loss=67.9514
	step [36/270], loss=55.9675
	step [37/270], loss=77.1039
	step [38/270], loss=66.3552
	step [39/270], loss=81.6395
	step [40/270], loss=78.0377
	step [41/270], loss=79.4969
	step [42/270], loss=84.5160
	step [43/270], loss=88.2238
	step [44/270], loss=79.6565
	step [45/270], loss=94.2489
	step [46/270], loss=83.8988
	step [47/270], loss=76.1849
	step [48/270], loss=72.1149
	step [49/270], loss=55.9068
	step [50/270], loss=74.6329
	step [51/270], loss=60.5473
	step [52/270], loss=70.0896
	step [53/270], loss=73.7894
	step [54/270], loss=89.0183
	step [55/270], loss=69.0373
	step [56/270], loss=74.3835
	step [57/270], loss=80.4797
	step [58/270], loss=78.5027
	step [59/270], loss=77.2512
	step [60/270], loss=70.5213
	step [61/270], loss=77.9263
	step [62/270], loss=91.0241
	step [63/270], loss=76.1760
	step [64/270], loss=85.2050
	step [65/270], loss=74.9805
	step [66/270], loss=55.2899
	step [67/270], loss=70.5614
	step [68/270], loss=71.7965
	step [69/270], loss=62.8718
	step [70/270], loss=70.1728
	step [71/270], loss=60.7567
	step [72/270], loss=66.2896
	step [73/270], loss=78.0034
	step [74/270], loss=97.0401
	step [75/270], loss=74.3893
	step [76/270], loss=74.5840
	step [77/270], loss=74.8511
	step [78/270], loss=72.9420
	step [79/270], loss=79.9775
	step [80/270], loss=70.3232
	step [81/270], loss=90.3927
	step [82/270], loss=72.9291
	step [83/270], loss=74.4957
	step [84/270], loss=58.6110
	step [85/270], loss=78.8374
	step [86/270], loss=69.5766
	step [87/270], loss=73.0502
	step [88/270], loss=79.9575
	step [89/270], loss=78.6009
	step [90/270], loss=76.5522
	step [91/270], loss=61.9720
	step [92/270], loss=73.8310
	step [93/270], loss=79.2339
	step [94/270], loss=88.2523
	step [95/270], loss=74.0317
	step [96/270], loss=81.0024
	step [97/270], loss=83.9050
	step [98/270], loss=66.0071
	step [99/270], loss=86.4558
	step [100/270], loss=78.2601
	step [101/270], loss=60.6636
	step [102/270], loss=66.1347
	step [103/270], loss=77.4407
	step [104/270], loss=73.8961
	step [105/270], loss=76.9861
	step [106/270], loss=77.6874
	step [107/270], loss=73.1096
	step [108/270], loss=81.5243
	step [109/270], loss=67.4912
	step [110/270], loss=85.4467
	step [111/270], loss=72.6093
	step [112/270], loss=73.9027
	step [113/270], loss=71.2731
	step [114/270], loss=74.4498
	step [115/270], loss=81.2597
	step [116/270], loss=65.7104
	step [117/270], loss=76.3671
	step [118/270], loss=70.8666
	step [119/270], loss=85.4005
	step [120/270], loss=59.9448
	step [121/270], loss=77.6104
	step [122/270], loss=77.7658
	step [123/270], loss=71.1075
	step [124/270], loss=92.1129
	step [125/270], loss=82.6383
	step [126/270], loss=76.3633
	step [127/270], loss=76.1906
	step [128/270], loss=61.3715
	step [129/270], loss=74.5329
	step [130/270], loss=74.5458
	step [131/270], loss=79.6297
	step [132/270], loss=65.9498
	step [133/270], loss=84.9397
	step [134/270], loss=72.7753
	step [135/270], loss=66.9608
	step [136/270], loss=74.0122
	step [137/270], loss=78.8163
	step [138/270], loss=60.1551
	step [139/270], loss=95.1457
	step [140/270], loss=88.5370
	step [141/270], loss=74.2838
	step [142/270], loss=64.9005
	step [143/270], loss=83.7931
	step [144/270], loss=69.4417
	step [145/270], loss=72.4548
	step [146/270], loss=78.6821
	step [147/270], loss=65.5330
	step [148/270], loss=63.0311
	step [149/270], loss=80.3976
	step [150/270], loss=65.1309
	step [151/270], loss=72.7517
	step [152/270], loss=71.5562
	step [153/270], loss=81.4759
	step [154/270], loss=76.8946
	step [155/270], loss=80.5073
	step [156/270], loss=73.5362
	step [157/270], loss=74.5521
	step [158/270], loss=70.1426
	step [159/270], loss=73.2257
	step [160/270], loss=60.4068
	step [161/270], loss=71.5075
	step [162/270], loss=62.7296
	step [163/270], loss=70.2309
	step [164/270], loss=66.4765
	step [165/270], loss=69.0088
	step [166/270], loss=65.4624
	step [167/270], loss=83.8554
	step [168/270], loss=84.1243
	step [169/270], loss=68.9832
	step [170/270], loss=62.8979
	step [171/270], loss=76.6723
	step [172/270], loss=63.8213
	step [173/270], loss=77.0268
	step [174/270], loss=75.3268
	step [175/270], loss=81.1904
	step [176/270], loss=73.8657
	step [177/270], loss=75.8396
	step [178/270], loss=63.9711
	step [179/270], loss=82.5762
	step [180/270], loss=61.3938
	step [181/270], loss=72.6926
	step [182/270], loss=74.5139
	step [183/270], loss=71.1276
	step [184/270], loss=70.7199
	step [185/270], loss=75.4508
	step [186/270], loss=84.2397
	step [187/270], loss=74.0905
	step [188/270], loss=83.7310
	step [189/270], loss=59.3571
	step [190/270], loss=71.0045
	step [191/270], loss=76.0504
	step [192/270], loss=100.7294
	step [193/270], loss=86.3297
	step [194/270], loss=58.5004
	step [195/270], loss=65.3722
	step [196/270], loss=83.7167
	step [197/270], loss=74.9501
	step [198/270], loss=85.9786
	step [199/270], loss=63.7401
	step [200/270], loss=85.9573
	step [201/270], loss=73.8067
	step [202/270], loss=71.2001
	step [203/270], loss=86.0448
	step [204/270], loss=72.0061
	step [205/270], loss=82.1301
	step [206/270], loss=68.5069
	step [207/270], loss=76.6848
	step [208/270], loss=88.1068
	step [209/270], loss=72.9494
	step [210/270], loss=62.1834
	step [211/270], loss=81.6503
	step [212/270], loss=66.7657
	step [213/270], loss=65.5865
	step [214/270], loss=90.4467
	step [215/270], loss=88.1511
	step [216/270], loss=78.7177
	step [217/270], loss=74.8561
	step [218/270], loss=76.6094
	step [219/270], loss=73.5502
	step [220/270], loss=82.3175
	step [221/270], loss=72.8411
	step [222/270], loss=90.4370
	step [223/270], loss=81.6786
	step [224/270], loss=76.7345
	step [225/270], loss=76.0916
	step [226/270], loss=80.9772
	step [227/270], loss=82.1181
	step [228/270], loss=79.5337
	step [229/270], loss=75.0244
	step [230/270], loss=73.8900
	step [231/270], loss=65.8019
	step [232/270], loss=74.4839
	step [233/270], loss=69.9374
	step [234/270], loss=71.8972
	step [235/270], loss=77.7565
	step [236/270], loss=86.9315
	step [237/270], loss=64.8333
	step [238/270], loss=83.1904
	step [239/270], loss=60.2225
	step [240/270], loss=79.5332
	step [241/270], loss=80.7514
	step [242/270], loss=76.0206
	step [243/270], loss=79.6705
	step [244/270], loss=68.3150
	step [245/270], loss=77.9456
	step [246/270], loss=81.0434
	step [247/270], loss=84.2762
	step [248/270], loss=67.9949
	step [249/270], loss=81.8242
	step [250/270], loss=73.3598
	step [251/270], loss=74.2536
	step [252/270], loss=80.5944
	step [253/270], loss=72.6580
	step [254/270], loss=78.4844
	step [255/270], loss=58.6451
	step [256/270], loss=72.1352
	step [257/270], loss=56.0005
	step [258/270], loss=84.2792
	step [259/270], loss=78.8180
	step [260/270], loss=84.7834
	step [261/270], loss=74.7024
	step [262/270], loss=68.5580
	step [263/270], loss=94.6758
	step [264/270], loss=69.4912
	step [265/270], loss=78.9719
	step [266/270], loss=77.7611
	step [267/270], loss=66.9869
	step [268/270], loss=80.6544
	step [269/270], loss=66.9585
	step [270/270], loss=18.3689
	Evaluating
	loss=0.0068, precision=0.3604, recall=0.8980, f1=0.5143
Training epoch 89
	step [1/270], loss=75.9702
	step [2/270], loss=73.6268
	step [3/270], loss=79.9501
	step [4/270], loss=78.2578
	step [5/270], loss=79.3306
	step [6/270], loss=73.6949
	step [7/270], loss=67.6739
	step [8/270], loss=82.3671
	step [9/270], loss=86.6614
	step [10/270], loss=63.7178
	step [11/270], loss=84.5296
	step [12/270], loss=77.8644
	step [13/270], loss=70.0549
	step [14/270], loss=80.2482
	step [15/270], loss=81.2515
	step [16/270], loss=67.8680
	step [17/270], loss=75.5995
	step [18/270], loss=75.5920
	step [19/270], loss=63.6922
	step [20/270], loss=69.0278
	step [21/270], loss=64.3872
	step [22/270], loss=75.9019
	step [23/270], loss=77.3588
	step [24/270], loss=72.2234
	step [25/270], loss=82.0523
	step [26/270], loss=71.0968
	step [27/270], loss=75.5674
	step [28/270], loss=76.6293
	step [29/270], loss=69.1229
	step [30/270], loss=74.1097
	step [31/270], loss=57.7784
	step [32/270], loss=70.8040
	step [33/270], loss=92.3191
	step [34/270], loss=74.2950
	step [35/270], loss=71.2019
	step [36/270], loss=73.6315
	step [37/270], loss=79.5385
	step [38/270], loss=70.1180
	step [39/270], loss=82.7443
	step [40/270], loss=79.4692
	step [41/270], loss=83.1145
	step [42/270], loss=73.8706
	step [43/270], loss=72.5173
	step [44/270], loss=57.7567
	step [45/270], loss=67.5418
	step [46/270], loss=74.3348
	step [47/270], loss=68.6830
	step [48/270], loss=67.7323
	step [49/270], loss=72.8461
	step [50/270], loss=69.8231
	step [51/270], loss=91.6172
	step [52/270], loss=73.6958
	step [53/270], loss=62.5494
	step [54/270], loss=69.5954
	step [55/270], loss=71.0465
	step [56/270], loss=84.7539
	step [57/270], loss=77.1557
	step [58/270], loss=76.0581
	step [59/270], loss=76.1804
	step [60/270], loss=64.0975
	step [61/270], loss=66.2742
	step [62/270], loss=70.8259
	step [63/270], loss=90.9197
	step [64/270], loss=93.7500
	step [65/270], loss=60.1339
	step [66/270], loss=70.2156
	step [67/270], loss=80.0533
	step [68/270], loss=76.8306
	step [69/270], loss=76.2834
	step [70/270], loss=88.0345
	step [71/270], loss=59.0705
	step [72/270], loss=79.0850
	step [73/270], loss=64.3818
	step [74/270], loss=62.7294
	step [75/270], loss=69.1729
	step [76/270], loss=77.2587
	step [77/270], loss=81.0906
	step [78/270], loss=82.9648
	step [79/270], loss=60.4690
	step [80/270], loss=77.1649
	step [81/270], loss=77.1334
	step [82/270], loss=65.0739
	step [83/270], loss=58.9882
	step [84/270], loss=87.5400
	step [85/270], loss=75.4238
	step [86/270], loss=69.6418
	step [87/270], loss=71.0209
	step [88/270], loss=72.7256
	step [89/270], loss=68.3435
	step [90/270], loss=87.6637
	step [91/270], loss=63.5383
	step [92/270], loss=68.6373
	step [93/270], loss=81.1354
	step [94/270], loss=73.2158
	step [95/270], loss=83.6918
	step [96/270], loss=82.5432
	step [97/270], loss=84.5776
	step [98/270], loss=72.3176
	step [99/270], loss=77.1028
	step [100/270], loss=68.0540
	step [101/270], loss=81.8731
	step [102/270], loss=61.6944
	step [103/270], loss=74.9602
	step [104/270], loss=70.7480
	step [105/270], loss=67.3595
	step [106/270], loss=78.3501
	step [107/270], loss=67.0545
	step [108/270], loss=75.5253
	step [109/270], loss=77.2606
	step [110/270], loss=62.9204
	step [111/270], loss=80.1147
	step [112/270], loss=76.1814
	step [113/270], loss=83.0716
	step [114/270], loss=72.8121
	step [115/270], loss=69.3772
	step [116/270], loss=84.2281
	step [117/270], loss=79.3574
	step [118/270], loss=77.8404
	step [119/270], loss=76.1268
	step [120/270], loss=68.1916
	step [121/270], loss=69.5272
	step [122/270], loss=75.4128
	step [123/270], loss=84.1041
	step [124/270], loss=70.2901
	step [125/270], loss=79.3070
	step [126/270], loss=86.5035
	step [127/270], loss=85.9050
	step [128/270], loss=77.7068
	step [129/270], loss=78.6912
	step [130/270], loss=84.4515
	step [131/270], loss=74.1490
	step [132/270], loss=74.0085
	step [133/270], loss=70.7386
	step [134/270], loss=88.3067
	step [135/270], loss=71.5868
	step [136/270], loss=67.8874
	step [137/270], loss=71.8156
	step [138/270], loss=82.9979
	step [139/270], loss=74.6486
	step [140/270], loss=97.0293
	step [141/270], loss=69.1949
	step [142/270], loss=89.6728
	step [143/270], loss=72.7898
	step [144/270], loss=69.2790
	step [145/270], loss=73.7809
	step [146/270], loss=68.0814
	step [147/270], loss=62.2028
	step [148/270], loss=81.8086
	step [149/270], loss=72.8702
	step [150/270], loss=78.8137
	step [151/270], loss=55.2246
	step [152/270], loss=72.1434
	step [153/270], loss=74.1066
	step [154/270], loss=78.0290
	step [155/270], loss=75.0591
	step [156/270], loss=71.3003
	step [157/270], loss=64.5409
	step [158/270], loss=72.9039
	step [159/270], loss=84.6862
	step [160/270], loss=75.7230
	step [161/270], loss=79.7188
	step [162/270], loss=72.9271
	step [163/270], loss=77.1006
	step [164/270], loss=76.9651
	step [165/270], loss=88.9126
	step [166/270], loss=73.9497
	step [167/270], loss=62.4637
	step [168/270], loss=92.0903
	step [169/270], loss=74.8669
	step [170/270], loss=79.6826
	step [171/270], loss=73.1608
	step [172/270], loss=62.4971
	step [173/270], loss=71.1325
	step [174/270], loss=76.2764
	step [175/270], loss=67.8037
	step [176/270], loss=73.3570
	step [177/270], loss=71.3464
	step [178/270], loss=61.2020
	step [179/270], loss=61.1991
	step [180/270], loss=67.9522
	step [181/270], loss=75.0184
	step [182/270], loss=64.2581
	step [183/270], loss=85.2135
	step [184/270], loss=79.9167
	step [185/270], loss=65.4162
	step [186/270], loss=67.4262
	step [187/270], loss=66.8229
	step [188/270], loss=62.3366
	step [189/270], loss=72.1893
	step [190/270], loss=55.7474
	step [191/270], loss=71.1461
	step [192/270], loss=71.2510
	step [193/270], loss=74.2925
	step [194/270], loss=65.2957
	step [195/270], loss=89.3241
	step [196/270], loss=84.1274
	step [197/270], loss=93.8632
	step [198/270], loss=74.0905
	step [199/270], loss=96.3362
	step [200/270], loss=63.6581
	step [201/270], loss=74.3570
	step [202/270], loss=90.7535
	step [203/270], loss=74.2721
	step [204/270], loss=78.3090
	step [205/270], loss=68.2541
	step [206/270], loss=78.8453
	step [207/270], loss=70.3548
	step [208/270], loss=82.2509
	step [209/270], loss=75.4433
	step [210/270], loss=75.3315
	step [211/270], loss=69.5770
	step [212/270], loss=80.6098
	step [213/270], loss=57.7077
	step [214/270], loss=63.4044
	step [215/270], loss=77.7653
	step [216/270], loss=74.8775
	step [217/270], loss=75.5170
	step [218/270], loss=58.4905
	step [219/270], loss=71.4062
	step [220/270], loss=72.5564
	step [221/270], loss=90.5558
	step [222/270], loss=80.2163
	step [223/270], loss=82.7743
	step [224/270], loss=93.3219
	step [225/270], loss=68.0495
	step [226/270], loss=90.5557
	step [227/270], loss=91.5288
	step [228/270], loss=78.7412
	step [229/270], loss=58.9316
	step [230/270], loss=61.0122
	step [231/270], loss=72.5100
	step [232/270], loss=70.2901
	step [233/270], loss=82.1905
	step [234/270], loss=69.3707
	step [235/270], loss=58.0031
	step [236/270], loss=62.1931
	step [237/270], loss=79.8626
	step [238/270], loss=72.6917
	step [239/270], loss=84.6677
	step [240/270], loss=66.8472
	step [241/270], loss=62.4027
	step [242/270], loss=58.6454
	step [243/270], loss=76.0303
	step [244/270], loss=67.1709
	step [245/270], loss=92.8610
	step [246/270], loss=62.9711
	step [247/270], loss=78.4528
	step [248/270], loss=92.8833
	step [249/270], loss=77.1780
	step [250/270], loss=79.2129
	step [251/270], loss=85.2837
	step [252/270], loss=70.0435
	step [253/270], loss=88.2118
	step [254/270], loss=72.3412
	step [255/270], loss=86.9695
	step [256/270], loss=76.8244
	step [257/270], loss=70.0425
	step [258/270], loss=63.2149
	step [259/270], loss=59.2034
	step [260/270], loss=64.0074
	step [261/270], loss=67.4823
	step [262/270], loss=74.7150
	step [263/270], loss=75.7533
	step [264/270], loss=91.4326
	step [265/270], loss=72.9407
	step [266/270], loss=65.0665
	step [267/270], loss=67.3505
	step [268/270], loss=85.4728
	step [269/270], loss=81.9855
	step [270/270], loss=17.4214
	Evaluating
	loss=0.0073, precision=0.3349, recall=0.9007, f1=0.4882
Training epoch 90
	step [1/270], loss=71.8607
	step [2/270], loss=74.8121
	step [3/270], loss=75.1637
	step [4/270], loss=87.7113
	step [5/270], loss=91.1131
	step [6/270], loss=86.3704
	step [7/270], loss=76.6016
	step [8/270], loss=78.1445
	step [9/270], loss=68.8111
	step [10/270], loss=63.0594
	step [11/270], loss=79.0775
	step [12/270], loss=85.5102
	step [13/270], loss=82.7775
	step [14/270], loss=62.5538
	step [15/270], loss=72.4290
	step [16/270], loss=70.1417
	step [17/270], loss=68.7560
	step [18/270], loss=81.0188
	step [19/270], loss=64.3081
	step [20/270], loss=71.7955
	step [21/270], loss=76.0704
	step [22/270], loss=68.4746
	step [23/270], loss=76.5925
	step [24/270], loss=78.6140
	step [25/270], loss=88.4478
	step [26/270], loss=73.6852
	step [27/270], loss=53.1163
	step [28/270], loss=76.5984
	step [29/270], loss=68.4703
	step [30/270], loss=72.9692
	step [31/270], loss=78.7942
	step [32/270], loss=83.3058
	step [33/270], loss=78.8739
	step [34/270], loss=82.0105
	step [35/270], loss=69.9917
	step [36/270], loss=71.8052
	step [37/270], loss=63.6949
	step [38/270], loss=86.2838
	step [39/270], loss=69.7310
	step [40/270], loss=79.6510
	step [41/270], loss=61.0943
	step [42/270], loss=58.9411
	step [43/270], loss=59.3571
	step [44/270], loss=83.9015
	step [45/270], loss=78.1521
	step [46/270], loss=79.7714
	step [47/270], loss=51.0039
	step [48/270], loss=79.4659
	step [49/270], loss=84.4205
	step [50/270], loss=89.1373
	step [51/270], loss=67.7256
	step [52/270], loss=90.5736
	step [53/270], loss=66.7757
	step [54/270], loss=81.3427
	step [55/270], loss=72.7622
	step [56/270], loss=64.5555
	step [57/270], loss=80.3870
	step [58/270], loss=71.4329
	step [59/270], loss=72.7293
	step [60/270], loss=66.6585
	step [61/270], loss=78.1779
	step [62/270], loss=78.0555
	step [63/270], loss=78.4004
	step [64/270], loss=78.7642
	step [65/270], loss=72.6448
	step [66/270], loss=100.8071
	step [67/270], loss=70.1231
	step [68/270], loss=74.7393
	step [69/270], loss=83.6158
	step [70/270], loss=70.4310
	step [71/270], loss=81.8186
	step [72/270], loss=82.9703
	step [73/270], loss=63.3050
	step [74/270], loss=85.4702
	step [75/270], loss=58.8713
	step [76/270], loss=77.9386
	step [77/270], loss=66.8715
	step [78/270], loss=67.0616
	step [79/270], loss=79.3550
	step [80/270], loss=84.2149
	step [81/270], loss=70.0746
	step [82/270], loss=71.7233
	step [83/270], loss=77.2439
	step [84/270], loss=70.2055
	step [85/270], loss=76.8190
	step [86/270], loss=71.4932
	step [87/270], loss=61.2191
	step [88/270], loss=65.5353
	step [89/270], loss=66.3317
	step [90/270], loss=79.1229
	step [91/270], loss=69.4783
	step [92/270], loss=66.7178
	step [93/270], loss=74.2525
	step [94/270], loss=68.3031
	step [95/270], loss=72.5591
	step [96/270], loss=62.1918
	step [97/270], loss=75.0467
	step [98/270], loss=59.1973
	step [99/270], loss=69.7391
	step [100/270], loss=81.9783
	step [101/270], loss=87.5630
	step [102/270], loss=78.5921
	step [103/270], loss=81.7130
	step [104/270], loss=72.0916
	step [105/270], loss=60.0672
	step [106/270], loss=74.4433
	step [107/270], loss=83.3837
	step [108/270], loss=72.7974
	step [109/270], loss=69.2185
	step [110/270], loss=63.4117
	step [111/270], loss=82.8372
	step [112/270], loss=57.6460
	step [113/270], loss=84.2551
	step [114/270], loss=70.0492
	step [115/270], loss=88.5091
	step [116/270], loss=85.8769
	step [117/270], loss=89.0787
	step [118/270], loss=62.6836
	step [119/270], loss=70.4726
	step [120/270], loss=65.5551
	step [121/270], loss=77.6137
	step [122/270], loss=79.7851
	step [123/270], loss=82.1475
	step [124/270], loss=90.5521
	step [125/270], loss=77.4324
	step [126/270], loss=60.1921
	step [127/270], loss=73.4917
	step [128/270], loss=59.5779
	step [129/270], loss=83.6308
	step [130/270], loss=74.0230
	step [131/270], loss=75.4013
	step [132/270], loss=76.6294
	step [133/270], loss=71.3289
	step [134/270], loss=79.7034
	step [135/270], loss=74.8866
	step [136/270], loss=52.6004
	step [137/270], loss=74.6229
	step [138/270], loss=67.2020
	step [139/270], loss=73.4048
	step [140/270], loss=74.7338
	step [141/270], loss=68.2229
	step [142/270], loss=71.6980
	step [143/270], loss=65.9889
	step [144/270], loss=86.8263
	step [145/270], loss=87.4933
	step [146/270], loss=55.1681
	step [147/270], loss=68.2606
	step [148/270], loss=87.0301
	step [149/270], loss=75.9205
	step [150/270], loss=91.2247
	step [151/270], loss=66.3962
	step [152/270], loss=77.8462
	step [153/270], loss=67.9685
	step [154/270], loss=76.6960
	step [155/270], loss=78.9681
	step [156/270], loss=76.2396
	step [157/270], loss=76.5248
	step [158/270], loss=84.4009
	step [159/270], loss=56.0737
	step [160/270], loss=70.0207
	step [161/270], loss=77.7749
	step [162/270], loss=85.5138
	step [163/270], loss=78.3104
	step [164/270], loss=71.0452
	step [165/270], loss=76.9805
	step [166/270], loss=85.0429
	step [167/270], loss=79.4492
	step [168/270], loss=76.9935
	step [169/270], loss=61.1317
	step [170/270], loss=53.0511
	step [171/270], loss=78.9862
	step [172/270], loss=76.4490
	step [173/270], loss=58.3424
	step [174/270], loss=70.3061
	step [175/270], loss=65.1856
	step [176/270], loss=70.1408
	step [177/270], loss=69.7796
	step [178/270], loss=68.4634
	step [179/270], loss=69.6609
	step [180/270], loss=90.6826
	step [181/270], loss=90.4518
	step [182/270], loss=68.8291
	step [183/270], loss=80.3357
	step [184/270], loss=74.0092
	step [185/270], loss=86.1994
	step [186/270], loss=62.9905
	step [187/270], loss=75.7629
	step [188/270], loss=78.0216
	step [189/270], loss=84.9346
	step [190/270], loss=68.9303
	step [191/270], loss=79.0768
	step [192/270], loss=78.5794
	step [193/270], loss=70.4900
	step [194/270], loss=56.2881
	step [195/270], loss=81.1727
	step [196/270], loss=55.0266
	step [197/270], loss=74.9037
	step [198/270], loss=75.9164
	step [199/270], loss=73.0655
	step [200/270], loss=77.0982
	step [201/270], loss=73.2343
	step [202/270], loss=74.7639
	step [203/270], loss=58.2632
	step [204/270], loss=80.8624
	step [205/270], loss=87.2047
	step [206/270], loss=67.5242
	step [207/270], loss=73.6505
	step [208/270], loss=78.0190
	step [209/270], loss=77.0159
	step [210/270], loss=67.2730
	step [211/270], loss=73.0398
	step [212/270], loss=77.1514
	step [213/270], loss=60.6031
	step [214/270], loss=78.6550
	step [215/270], loss=62.4957
	step [216/270], loss=83.0435
	step [217/270], loss=77.4882
	step [218/270], loss=73.2648
	step [219/270], loss=80.8779
	step [220/270], loss=77.3588
	step [221/270], loss=77.7663
	step [222/270], loss=64.6763
	step [223/270], loss=89.5173
	step [224/270], loss=71.5464
	step [225/270], loss=72.7195
	step [226/270], loss=78.6125
	step [227/270], loss=64.1618
	step [228/270], loss=81.9244
	step [229/270], loss=61.3601
	step [230/270], loss=70.7726
	step [231/270], loss=77.4688
	step [232/270], loss=72.2446
	step [233/270], loss=82.0570
	step [234/270], loss=68.0575
	step [235/270], loss=73.4844
	step [236/270], loss=72.8738
	step [237/270], loss=78.6287
	step [238/270], loss=75.7568
	step [239/270], loss=79.0967
	step [240/270], loss=74.1004
	step [241/270], loss=79.7119
	step [242/270], loss=61.1708
	step [243/270], loss=79.8467
	step [244/270], loss=77.3899
	step [245/270], loss=73.9115
	step [246/270], loss=79.3215
	step [247/270], loss=72.2339
	step [248/270], loss=73.2979
	step [249/270], loss=70.8778
	step [250/270], loss=82.7714
	step [251/270], loss=77.9175
	step [252/270], loss=84.4951
	step [253/270], loss=87.0745
	step [254/270], loss=73.1106
	step [255/270], loss=65.3881
	step [256/270], loss=81.5325
	step [257/270], loss=75.2030
	step [258/270], loss=66.3312
	step [259/270], loss=68.9962
	step [260/270], loss=69.2926
	step [261/270], loss=63.4767
	step [262/270], loss=75.7005
	step [263/270], loss=69.0020
	step [264/270], loss=73.4043
	step [265/270], loss=68.1071
	step [266/270], loss=85.9663
	step [267/270], loss=83.2102
	step [268/270], loss=81.4589
	step [269/270], loss=68.3243
	step [270/270], loss=16.7814
	Evaluating
	loss=0.0077, precision=0.3226, recall=0.9009, f1=0.4751
Training epoch 91
	step [1/270], loss=64.4518
	step [2/270], loss=67.4248
	step [3/270], loss=76.3166
	step [4/270], loss=71.1242
	step [5/270], loss=66.6058
	step [6/270], loss=77.9139
	step [7/270], loss=56.3809
	step [8/270], loss=66.6588
	step [9/270], loss=78.7458
	step [10/270], loss=64.9589
	step [11/270], loss=83.7584
	step [12/270], loss=80.6580
	step [13/270], loss=86.7172
	step [14/270], loss=72.3615
	step [15/270], loss=79.3132
	step [16/270], loss=71.5755
	step [17/270], loss=64.1827
	step [18/270], loss=79.8205
	step [19/270], loss=81.1540
	step [20/270], loss=62.6915
	step [21/270], loss=88.2306
	step [22/270], loss=68.8036
	step [23/270], loss=81.0354
	step [24/270], loss=78.2629
	step [25/270], loss=76.8721
	step [26/270], loss=85.2243
	step [27/270], loss=59.3199
	step [28/270], loss=88.5414
	step [29/270], loss=90.5619
	step [30/270], loss=78.1182
	step [31/270], loss=95.2140
	step [32/270], loss=73.7360
	step [33/270], loss=74.1143
	step [34/270], loss=69.5438
	step [35/270], loss=72.5473
	step [36/270], loss=72.7326
	step [37/270], loss=73.5519
	step [38/270], loss=58.7947
	step [39/270], loss=82.4368
	step [40/270], loss=76.1042
	step [41/270], loss=66.2056
	step [42/270], loss=74.2161
	step [43/270], loss=73.0560
	step [44/270], loss=80.4650
	step [45/270], loss=70.8505
	step [46/270], loss=76.4757
	step [47/270], loss=66.8544
	step [48/270], loss=69.8123
	step [49/270], loss=74.2001
	step [50/270], loss=69.8156
	step [51/270], loss=75.3732
	step [52/270], loss=76.6651
	step [53/270], loss=67.3914
	step [54/270], loss=86.7815
	step [55/270], loss=76.9316
	step [56/270], loss=70.0211
	step [57/270], loss=70.0419
	step [58/270], loss=100.1033
	step [59/270], loss=82.7530
	step [60/270], loss=72.8718
	step [61/270], loss=74.7792
	step [62/270], loss=61.5267
	step [63/270], loss=78.2102
	step [64/270], loss=72.7915
	step [65/270], loss=65.0184
	step [66/270], loss=61.7316
	step [67/270], loss=64.5001
	step [68/270], loss=77.3764
	step [69/270], loss=77.0494
	step [70/270], loss=82.8065
	step [71/270], loss=83.1427
	step [72/270], loss=87.5539
	step [73/270], loss=81.7015
	step [74/270], loss=77.1603
	step [75/270], loss=74.1294
	step [76/270], loss=72.5660
	step [77/270], loss=79.0276
	step [78/270], loss=85.0733
	step [79/270], loss=74.7224
	step [80/270], loss=70.2297
	step [81/270], loss=55.2762
	step [82/270], loss=65.2146
	step [83/270], loss=69.7851
	step [84/270], loss=68.8715
	step [85/270], loss=67.5632
	step [86/270], loss=73.1124
	step [87/270], loss=73.0627
	step [88/270], loss=90.4207
	step [89/270], loss=58.2321
	step [90/270], loss=63.7465
	step [91/270], loss=75.9949
	step [92/270], loss=85.0409
	step [93/270], loss=65.9816
	step [94/270], loss=67.6378
	step [95/270], loss=74.6808
	step [96/270], loss=61.1897
	step [97/270], loss=70.1108
	step [98/270], loss=71.0759
	step [99/270], loss=76.4661
	step [100/270], loss=71.2339
	step [101/270], loss=70.7167
	step [102/270], loss=68.1303
	step [103/270], loss=78.9609
	step [104/270], loss=81.8641
	step [105/270], loss=66.9611
	step [106/270], loss=64.0581
	step [107/270], loss=79.1127
	step [108/270], loss=64.1016
	step [109/270], loss=65.0109
	step [110/270], loss=65.7140
	step [111/270], loss=72.9307
	step [112/270], loss=64.9977
	step [113/270], loss=70.2255
	step [114/270], loss=83.8231
	step [115/270], loss=76.8964
	step [116/270], loss=75.9734
	step [117/270], loss=70.9081
	step [118/270], loss=76.2688
	step [119/270], loss=68.7215
	step [120/270], loss=81.9705
	step [121/270], loss=67.3349
	step [122/270], loss=94.0036
	step [123/270], loss=68.7216
	step [124/270], loss=89.2290
	step [125/270], loss=63.0255
	step [126/270], loss=61.1546
	step [127/270], loss=78.1827
	step [128/270], loss=78.1845
	step [129/270], loss=61.3284
	step [130/270], loss=80.3581
	step [131/270], loss=72.0693
	step [132/270], loss=74.3837
	step [133/270], loss=67.2986
	step [134/270], loss=66.7281
	step [135/270], loss=79.7671
	step [136/270], loss=94.5368
	step [137/270], loss=71.3832
	step [138/270], loss=74.7711
	step [139/270], loss=77.7222
	step [140/270], loss=61.4785
	step [141/270], loss=77.2060
	step [142/270], loss=85.3876
	step [143/270], loss=86.7744
	step [144/270], loss=74.2853
	step [145/270], loss=68.8246
	step [146/270], loss=78.4832
	step [147/270], loss=77.2122
	step [148/270], loss=63.2994
	step [149/270], loss=69.9162
	step [150/270], loss=68.0668
	step [151/270], loss=80.0946
	step [152/270], loss=62.5344
	step [153/270], loss=82.0032
	step [154/270], loss=73.8005
	step [155/270], loss=72.6424
	step [156/270], loss=70.8832
	step [157/270], loss=64.0718
	step [158/270], loss=75.8707
	step [159/270], loss=62.1332
	step [160/270], loss=72.5664
	step [161/270], loss=75.9563
	step [162/270], loss=72.3885
	step [163/270], loss=72.6936
	step [164/270], loss=70.9920
	step [165/270], loss=80.4965
	step [166/270], loss=77.5606
	step [167/270], loss=79.2493
	step [168/270], loss=67.1559
	step [169/270], loss=75.1480
	step [170/270], loss=55.7131
	step [171/270], loss=63.8492
	step [172/270], loss=75.5539
	step [173/270], loss=80.6117
	step [174/270], loss=80.2820
	step [175/270], loss=74.7053
	step [176/270], loss=90.1641
	step [177/270], loss=61.8946
	step [178/270], loss=76.5203
	step [179/270], loss=77.0294
	step [180/270], loss=76.3458
	step [181/270], loss=75.4953
	step [182/270], loss=60.1290
	step [183/270], loss=79.4449
	step [184/270], loss=61.6750
	step [185/270], loss=65.3202
	step [186/270], loss=79.3551
	step [187/270], loss=62.0091
	step [188/270], loss=69.6331
	step [189/270], loss=80.6961
	step [190/270], loss=54.4973
	step [191/270], loss=85.4876
	step [192/270], loss=66.5826
	step [193/270], loss=88.9265
	step [194/270], loss=70.2976
	step [195/270], loss=84.4154
	step [196/270], loss=70.8451
	step [197/270], loss=83.8530
	step [198/270], loss=66.0883
	step [199/270], loss=72.9811
	step [200/270], loss=73.4460
	step [201/270], loss=65.6673
	step [202/270], loss=72.5965
	step [203/270], loss=77.8355
	step [204/270], loss=77.9026
	step [205/270], loss=76.9568
	step [206/270], loss=69.9055
	step [207/270], loss=69.4540
	step [208/270], loss=61.4529
	step [209/270], loss=58.8789
	step [210/270], loss=66.6212
	step [211/270], loss=72.8023
	step [212/270], loss=61.7227
	step [213/270], loss=82.4416
	step [214/270], loss=71.8954
	step [215/270], loss=91.4787
	step [216/270], loss=79.7781
	step [217/270], loss=77.6881
	step [218/270], loss=75.7973
	step [219/270], loss=87.8758
	step [220/270], loss=84.0706
	step [221/270], loss=62.0738
	step [222/270], loss=67.0474
	step [223/270], loss=64.2329
	step [224/270], loss=80.1066
	step [225/270], loss=67.5886
	step [226/270], loss=86.2933
	step [227/270], loss=83.1908
	step [228/270], loss=75.2508
	step [229/270], loss=64.9701
	step [230/270], loss=78.7368
	step [231/270], loss=79.1411
	step [232/270], loss=79.6882
	step [233/270], loss=78.5816
	step [234/270], loss=61.3345
	step [235/270], loss=77.7994
	step [236/270], loss=84.1706
	step [237/270], loss=76.5999
	step [238/270], loss=60.8182
	step [239/270], loss=64.7784
	step [240/270], loss=80.9854
	step [241/270], loss=69.5896
	step [242/270], loss=86.2074
	step [243/270], loss=81.5268
	step [244/270], loss=66.9298
	step [245/270], loss=98.0863
	step [246/270], loss=74.6151
	step [247/270], loss=60.2457
	step [248/270], loss=73.0148
	step [249/270], loss=62.8097
	step [250/270], loss=79.5060
	step [251/270], loss=82.4324
	step [252/270], loss=83.7993
	step [253/270], loss=84.8370
	step [254/270], loss=67.3676
	step [255/270], loss=66.8893
	step [256/270], loss=92.2626
	step [257/270], loss=89.5712
	step [258/270], loss=63.4356
	step [259/270], loss=80.1518
	step [260/270], loss=86.3015
	step [261/270], loss=76.0743
	step [262/270], loss=82.4168
	step [263/270], loss=78.4428
	step [264/270], loss=92.1105
	step [265/270], loss=82.8103
	step [266/270], loss=66.2596
	step [267/270], loss=65.4718
	step [268/270], loss=75.7970
	step [269/270], loss=64.9875
	step [270/270], loss=17.1419
	Evaluating
	loss=0.0066, precision=0.3698, recall=0.8905, f1=0.5226
Training epoch 92
	step [1/270], loss=71.1845
	step [2/270], loss=88.6834
	step [3/270], loss=67.2805
	step [4/270], loss=70.7931
	step [5/270], loss=73.7324
	step [6/270], loss=73.7642
	step [7/270], loss=76.9874
	step [8/270], loss=89.4441
	step [9/270], loss=79.2336
	step [10/270], loss=55.0562
	step [11/270], loss=73.9384
	step [12/270], loss=80.4112
	step [13/270], loss=68.2899
	step [14/270], loss=80.7940
	step [15/270], loss=76.8481
	step [16/270], loss=64.9240
	step [17/270], loss=75.5844
	step [18/270], loss=79.8014
	step [19/270], loss=58.6386
	step [20/270], loss=68.8809
	step [21/270], loss=70.0896
	step [22/270], loss=77.5796
	step [23/270], loss=69.9138
	step [24/270], loss=77.6018
	step [25/270], loss=70.6141
	step [26/270], loss=69.0418
	step [27/270], loss=62.9913
	step [28/270], loss=78.2886
	step [29/270], loss=60.3222
	step [30/270], loss=67.5579
	step [31/270], loss=83.6291
	step [32/270], loss=79.5619
	step [33/270], loss=71.9696
	step [34/270], loss=72.4364
	step [35/270], loss=77.2719
	step [36/270], loss=78.3804
	step [37/270], loss=67.7828
	step [38/270], loss=62.8010
	step [39/270], loss=80.5459
	step [40/270], loss=75.1553
	step [41/270], loss=71.9371
	step [42/270], loss=66.0917
	step [43/270], loss=79.6003
	step [44/270], loss=77.0892
	step [45/270], loss=62.7492
	step [46/270], loss=58.0827
	step [47/270], loss=75.8068
	step [48/270], loss=64.9544
	step [49/270], loss=73.5075
	step [50/270], loss=83.1141
	step [51/270], loss=74.9317
	step [52/270], loss=73.4848
	step [53/270], loss=71.8076
	step [54/270], loss=70.5366
	step [55/270], loss=77.0965
	step [56/270], loss=54.9949
	step [57/270], loss=70.9619
	step [58/270], loss=78.3401
	step [59/270], loss=72.0193
	step [60/270], loss=63.6941
	step [61/270], loss=74.2138
	step [62/270], loss=72.5911
	step [63/270], loss=73.9888
	step [64/270], loss=65.0198
	step [65/270], loss=68.8556
	step [66/270], loss=74.8398
	step [67/270], loss=64.5226
	step [68/270], loss=64.9648
	step [69/270], loss=80.4454
	step [70/270], loss=100.6510
	step [71/270], loss=71.0849
	step [72/270], loss=74.4041
	step [73/270], loss=76.1155
	step [74/270], loss=71.2546
	step [75/270], loss=73.1874
	step [76/270], loss=64.5080
	step [77/270], loss=76.3541
	step [78/270], loss=67.8747
	step [79/270], loss=63.6004
	step [80/270], loss=88.1718
	step [81/270], loss=74.0839
	step [82/270], loss=65.0723
	step [83/270], loss=67.5304
	step [84/270], loss=95.6421
	step [85/270], loss=61.1390
	step [86/270], loss=84.2001
	step [87/270], loss=73.8726
	step [88/270], loss=73.1604
	step [89/270], loss=83.2977
	step [90/270], loss=68.1735
	step [91/270], loss=66.9363
	step [92/270], loss=78.3217
	step [93/270], loss=61.5463
	step [94/270], loss=68.7934
	step [95/270], loss=67.1771
	step [96/270], loss=71.8821
	step [97/270], loss=68.8683
	step [98/270], loss=63.7912
	step [99/270], loss=74.4236
	step [100/270], loss=69.5004
	step [101/270], loss=72.5940
	step [102/270], loss=81.5139
	step [103/270], loss=67.7887
	step [104/270], loss=57.1125
	step [105/270], loss=66.7410
	step [106/270], loss=72.8089
	step [107/270], loss=86.4000
	step [108/270], loss=86.0568
	step [109/270], loss=83.1458
	step [110/270], loss=62.7693
	step [111/270], loss=75.7825
	step [112/270], loss=72.9571
	step [113/270], loss=68.1665
	step [114/270], loss=66.0777
	step [115/270], loss=73.4354
	step [116/270], loss=57.6565
	step [117/270], loss=66.9259
	step [118/270], loss=71.5650
	step [119/270], loss=76.1325
	step [120/270], loss=79.1925
	step [121/270], loss=88.6881
	step [122/270], loss=71.7136
	step [123/270], loss=83.7097
	step [124/270], loss=76.3876
	step [125/270], loss=83.8150
	step [126/270], loss=82.4495
	step [127/270], loss=71.8531
	step [128/270], loss=85.4800
	step [129/270], loss=69.7350
	step [130/270], loss=74.5835
	step [131/270], loss=78.7394
	step [132/270], loss=74.2663
	step [133/270], loss=84.1537
	step [134/270], loss=75.1262
	step [135/270], loss=69.0817
	step [136/270], loss=72.6726
	step [137/270], loss=65.7247
	step [138/270], loss=88.1113
	step [139/270], loss=71.7707
	step [140/270], loss=70.8594
	step [141/270], loss=72.4143
	step [142/270], loss=68.6337
	step [143/270], loss=71.5073
	step [144/270], loss=80.2123
	step [145/270], loss=83.4630
	step [146/270], loss=74.3040
	step [147/270], loss=72.7273
	step [148/270], loss=70.7252
	step [149/270], loss=78.9925
	step [150/270], loss=75.3539
	step [151/270], loss=62.7854
	step [152/270], loss=77.7930
	step [153/270], loss=77.0356
	step [154/270], loss=57.0634
	step [155/270], loss=65.4226
	step [156/270], loss=80.3260
	step [157/270], loss=73.2327
	step [158/270], loss=65.4362
	step [159/270], loss=79.4050
	step [160/270], loss=78.5300
	step [161/270], loss=75.3682
	step [162/270], loss=64.8020
	step [163/270], loss=69.9792
	step [164/270], loss=73.4871
	step [165/270], loss=71.7108
	step [166/270], loss=71.4685
	step [167/270], loss=71.8393
	step [168/270], loss=75.4794
	step [169/270], loss=71.1797
	step [170/270], loss=70.1512
	step [171/270], loss=71.2200
	step [172/270], loss=69.9208
	step [173/270], loss=69.8425
	step [174/270], loss=62.9688
	step [175/270], loss=67.9041
	step [176/270], loss=68.9632
	step [177/270], loss=61.3929
	step [178/270], loss=76.8713
	step [179/270], loss=66.8566
	step [180/270], loss=51.5708
	step [181/270], loss=90.8584
	step [182/270], loss=63.6149
	step [183/270], loss=80.8538
	step [184/270], loss=69.7543
	step [185/270], loss=68.7330
	step [186/270], loss=77.0051
	step [187/270], loss=60.6548
	step [188/270], loss=86.1441
	step [189/270], loss=68.3898
	step [190/270], loss=71.9349
	step [191/270], loss=87.0816
	step [192/270], loss=81.8178
	step [193/270], loss=94.7591
	step [194/270], loss=80.6546
	step [195/270], loss=81.4879
	step [196/270], loss=69.3800
	step [197/270], loss=73.4367
	step [198/270], loss=81.7640
	step [199/270], loss=68.6066
	step [200/270], loss=74.0074
	step [201/270], loss=70.1211
	step [202/270], loss=83.7125
	step [203/270], loss=56.1222
	step [204/270], loss=78.6450
	step [205/270], loss=77.6299
	step [206/270], loss=65.9857
	step [207/270], loss=56.7059
	step [208/270], loss=76.9589
	step [209/270], loss=76.9944
	step [210/270], loss=75.0345
	step [211/270], loss=73.8334
	step [212/270], loss=79.5015
	step [213/270], loss=65.9718
	step [214/270], loss=77.1697
	step [215/270], loss=76.4348
	step [216/270], loss=81.6138
	step [217/270], loss=76.3839
	step [218/270], loss=73.2714
	step [219/270], loss=74.2948
	step [220/270], loss=75.3773
	step [221/270], loss=76.5005
	step [222/270], loss=66.6584
	step [223/270], loss=62.8405
	step [224/270], loss=67.6814
	step [225/270], loss=83.2493
	step [226/270], loss=84.5969
	step [227/270], loss=75.4287
	step [228/270], loss=85.8039
	step [229/270], loss=67.5447
	step [230/270], loss=80.5579
	step [231/270], loss=72.6265
	step [232/270], loss=67.5877
	step [233/270], loss=78.5990
	step [234/270], loss=88.7762
	step [235/270], loss=63.5723
	step [236/270], loss=75.1726
	step [237/270], loss=90.5290
	step [238/270], loss=79.7348
	step [239/270], loss=78.1532
	step [240/270], loss=81.2983
	step [241/270], loss=86.2576
	step [242/270], loss=91.2827
	step [243/270], loss=61.4486
	step [244/270], loss=77.6375
	step [245/270], loss=78.8748
	step [246/270], loss=73.2752
	step [247/270], loss=86.6613
	step [248/270], loss=72.3203
	step [249/270], loss=84.8753
	step [250/270], loss=76.3128
	step [251/270], loss=71.9652
	step [252/270], loss=74.5632
	step [253/270], loss=65.2871
	step [254/270], loss=69.2613
	step [255/270], loss=79.2279
	step [256/270], loss=66.0167
	step [257/270], loss=82.7825
	step [258/270], loss=63.8510
	step [259/270], loss=87.4543
	step [260/270], loss=84.2183
	step [261/270], loss=83.2795
	step [262/270], loss=73.3904
	step [263/270], loss=79.8931
	step [264/270], loss=82.8314
	step [265/270], loss=73.0815
	step [266/270], loss=81.3399
	step [267/270], loss=87.3489
	step [268/270], loss=85.2635
	step [269/270], loss=68.8603
	step [270/270], loss=15.1352
	Evaluating
	loss=0.0063, precision=0.3789, recall=0.8894, f1=0.5314
Training epoch 93
	step [1/270], loss=84.8842
	step [2/270], loss=67.3248
	step [3/270], loss=91.2525
	step [4/270], loss=68.7968
	step [5/270], loss=78.5361
	step [6/270], loss=68.8987
	step [7/270], loss=91.4742
	step [8/270], loss=93.8058
	step [9/270], loss=75.0228
	step [10/270], loss=78.9998
	step [11/270], loss=65.2792
	step [12/270], loss=73.7920
	step [13/270], loss=75.7430
	step [14/270], loss=67.7794
	step [15/270], loss=82.4318
	step [16/270], loss=65.8771
	step [17/270], loss=78.0886
	step [18/270], loss=70.3159
	step [19/270], loss=68.4027
	step [20/270], loss=75.2498
	step [21/270], loss=56.0191
	step [22/270], loss=72.0765
	step [23/270], loss=73.5022
	step [24/270], loss=74.8904
	step [25/270], loss=82.3646
	step [26/270], loss=72.9108
	step [27/270], loss=94.7124
	step [28/270], loss=63.7943
	step [29/270], loss=71.1164
	step [30/270], loss=78.9297
	step [31/270], loss=81.4982
	step [32/270], loss=66.6529
	step [33/270], loss=80.7922
	step [34/270], loss=76.1190
	step [35/270], loss=79.1517
	step [36/270], loss=71.4757
	step [37/270], loss=58.1308
	step [38/270], loss=87.3593
	step [39/270], loss=87.9702
	step [40/270], loss=65.2126
	step [41/270], loss=66.7664
	step [42/270], loss=64.4999
	step [43/270], loss=79.8170
	step [44/270], loss=66.7013
	step [45/270], loss=70.6690
	step [46/270], loss=72.4069
	step [47/270], loss=77.2278
	step [48/270], loss=66.4887
	step [49/270], loss=92.6136
	step [50/270], loss=68.4727
	step [51/270], loss=86.3090
	step [52/270], loss=56.9621
	step [53/270], loss=79.9859
	step [54/270], loss=71.0120
	step [55/270], loss=77.3608
	step [56/270], loss=65.7097
	step [57/270], loss=77.3215
	step [58/270], loss=83.6361
	step [59/270], loss=67.3370
	step [60/270], loss=74.2457
	step [61/270], loss=88.3965
	step [62/270], loss=77.2935
	step [63/270], loss=66.5936
	step [64/270], loss=76.7127
	step [65/270], loss=60.1166
	step [66/270], loss=82.2120
	step [67/270], loss=60.4337
	step [68/270], loss=65.6691
	step [69/270], loss=71.2569
	step [70/270], loss=71.9577
	step [71/270], loss=64.2395
	step [72/270], loss=84.6030
	step [73/270], loss=66.2960
	step [74/270], loss=78.1430
	step [75/270], loss=68.9189
	step [76/270], loss=66.5658
	step [77/270], loss=77.0735
	step [78/270], loss=63.9733
	step [79/270], loss=72.4122
	step [80/270], loss=73.7634
	step [81/270], loss=81.9288
	step [82/270], loss=64.0091
	step [83/270], loss=61.6398
	step [84/270], loss=79.2752
	step [85/270], loss=82.4965
	step [86/270], loss=88.5065
	step [87/270], loss=70.1370
	step [88/270], loss=77.1250
	step [89/270], loss=71.8349
	step [90/270], loss=84.1538
	step [91/270], loss=85.5384
	step [92/270], loss=69.4362
	step [93/270], loss=73.9451
	step [94/270], loss=63.4369
	step [95/270], loss=69.4718
	step [96/270], loss=99.6752
	step [97/270], loss=70.0806
	step [98/270], loss=79.9390
	step [99/270], loss=60.7731
	step [100/270], loss=87.9554
	step [101/270], loss=60.9191
	step [102/270], loss=87.9691
	step [103/270], loss=77.2102
	step [104/270], loss=64.2717
	step [105/270], loss=66.4724
	step [106/270], loss=74.4965
	step [107/270], loss=70.1772
	step [108/270], loss=78.5233
	step [109/270], loss=60.5754
	step [110/270], loss=76.6111
	step [111/270], loss=71.1781
	step [112/270], loss=57.9297
	step [113/270], loss=73.1954
	step [114/270], loss=78.2664
	step [115/270], loss=78.7059
	step [116/270], loss=74.9562
	step [117/270], loss=68.6660
	step [118/270], loss=80.7955
	step [119/270], loss=65.4418
	step [120/270], loss=71.7150
	step [121/270], loss=74.5259
	step [122/270], loss=75.8962
	step [123/270], loss=68.6909
	step [124/270], loss=75.6401
	step [125/270], loss=68.0773
	step [126/270], loss=85.6136
	step [127/270], loss=83.1639
	step [128/270], loss=84.0838
	step [129/270], loss=83.6618
	step [130/270], loss=70.8356
	step [131/270], loss=78.9568
	step [132/270], loss=54.3790
	step [133/270], loss=75.8365
	step [134/270], loss=71.7027
	step [135/270], loss=71.1226
	step [136/270], loss=66.0117
	step [137/270], loss=65.3395
	step [138/270], loss=76.8147
	step [139/270], loss=63.4958
	step [140/270], loss=94.1609
	step [141/270], loss=76.1637
	step [142/270], loss=78.7013
	step [143/270], loss=68.7398
	step [144/270], loss=80.9266
	step [145/270], loss=70.6880
	step [146/270], loss=80.2713
	step [147/270], loss=61.6005
	step [148/270], loss=84.3908
	step [149/270], loss=82.8672
	step [150/270], loss=66.4141
	step [151/270], loss=66.7108
	step [152/270], loss=66.8105
	step [153/270], loss=70.2680
	step [154/270], loss=73.9142
	step [155/270], loss=78.2616
	step [156/270], loss=68.9445
	step [157/270], loss=73.9943
	step [158/270], loss=75.5983
	step [159/270], loss=65.8017
	step [160/270], loss=93.1910
	step [161/270], loss=61.3929
	step [162/270], loss=65.1646
	step [163/270], loss=70.8672
	step [164/270], loss=83.3200
	step [165/270], loss=67.3496
	step [166/270], loss=69.7542
	step [167/270], loss=85.1046
	step [168/270], loss=70.1923
	step [169/270], loss=81.0363
	step [170/270], loss=63.9730
	step [171/270], loss=71.8458
	step [172/270], loss=72.0189
	step [173/270], loss=73.7677
	step [174/270], loss=81.6536
	step [175/270], loss=61.9694
	step [176/270], loss=79.7126
	step [177/270], loss=74.0467
	step [178/270], loss=75.3930
	step [179/270], loss=75.3623
	step [180/270], loss=65.9782
	step [181/270], loss=79.9832
	step [182/270], loss=89.0348
	step [183/270], loss=65.0856
	step [184/270], loss=73.5816
	step [185/270], loss=76.1012
	step [186/270], loss=69.0236
	step [187/270], loss=70.7140
	step [188/270], loss=73.5988
	step [189/270], loss=72.1184
	step [190/270], loss=72.2597
	step [191/270], loss=80.0553
	step [192/270], loss=71.1789
	step [193/270], loss=70.2867
	step [194/270], loss=70.0647
	step [195/270], loss=81.1735
	step [196/270], loss=65.4588
	step [197/270], loss=75.1014
	step [198/270], loss=77.0813
	step [199/270], loss=83.9766
	step [200/270], loss=90.7513
	step [201/270], loss=61.4083
	step [202/270], loss=64.7057
	step [203/270], loss=70.8068
	step [204/270], loss=81.4333
	step [205/270], loss=67.2863
	step [206/270], loss=72.0233
	step [207/270], loss=63.2418
	step [208/270], loss=71.3150
	step [209/270], loss=78.8042
	step [210/270], loss=75.2811
	step [211/270], loss=67.1340
	step [212/270], loss=61.9159
	step [213/270], loss=62.2488
	step [214/270], loss=73.7173
	step [215/270], loss=78.6576
	step [216/270], loss=76.5141
	step [217/270], loss=73.3055
	step [218/270], loss=73.7939
	step [219/270], loss=71.0912
	step [220/270], loss=78.7995
	step [221/270], loss=69.8370
	step [222/270], loss=67.5104
	step [223/270], loss=61.0597
	step [224/270], loss=86.8844
	step [225/270], loss=79.9689
	step [226/270], loss=75.2085
	step [227/270], loss=66.9090
	step [228/270], loss=70.8991
	step [229/270], loss=61.9501
	step [230/270], loss=69.5833
	step [231/270], loss=59.1724
	step [232/270], loss=70.0229
	step [233/270], loss=70.9216
	step [234/270], loss=72.6172
	step [235/270], loss=75.8113
	step [236/270], loss=70.1020
	step [237/270], loss=81.2421
	step [238/270], loss=71.5770
	step [239/270], loss=76.7468
	step [240/270], loss=69.4761
	step [241/270], loss=58.4839
	step [242/270], loss=82.9864
	step [243/270], loss=73.1410
	step [244/270], loss=71.5968
	step [245/270], loss=71.7880
	step [246/270], loss=75.1211
	step [247/270], loss=68.6755
	step [248/270], loss=68.7555
	step [249/270], loss=81.9575
	step [250/270], loss=75.4191
	step [251/270], loss=86.0485
	step [252/270], loss=61.5715
	step [253/270], loss=89.1911
	step [254/270], loss=86.0914
	step [255/270], loss=75.8833
	step [256/270], loss=79.9249
	step [257/270], loss=78.3885
	step [258/270], loss=81.1559
	step [259/270], loss=66.3788
	step [260/270], loss=77.2199
	step [261/270], loss=59.4990
	step [262/270], loss=76.7349
	step [263/270], loss=82.6545
	step [264/270], loss=71.1765
	step [265/270], loss=65.1210
	step [266/270], loss=84.3213
	step [267/270], loss=82.0290
	step [268/270], loss=71.2623
	step [269/270], loss=63.7320
	step [270/270], loss=9.5552
	Evaluating
	loss=0.0070, precision=0.3513, recall=0.9029, f1=0.5058
Training epoch 94
	step [1/270], loss=64.6058
	step [2/270], loss=72.0961
	step [3/270], loss=57.6094
	step [4/270], loss=69.6963
	step [5/270], loss=77.5104
	step [6/270], loss=79.4909
	step [7/270], loss=75.1152
	step [8/270], loss=72.9623
	step [9/270], loss=65.7263
	step [10/270], loss=68.4725
	step [11/270], loss=80.3475
	step [12/270], loss=67.5413
	step [13/270], loss=64.5512
	step [14/270], loss=79.7247
	step [15/270], loss=57.1425
	step [16/270], loss=65.0780
	step [17/270], loss=88.7022
	step [18/270], loss=67.7994
	step [19/270], loss=67.4535
	step [20/270], loss=87.4400
	step [21/270], loss=69.8300
	step [22/270], loss=73.7355
	step [23/270], loss=65.3798
	step [24/270], loss=86.0534
	step [25/270], loss=71.6116
	step [26/270], loss=62.0736
	step [27/270], loss=64.5716
	step [28/270], loss=76.9470
	step [29/270], loss=86.6397
	step [30/270], loss=98.4805
	step [31/270], loss=75.9476
	step [32/270], loss=72.1835
	step [33/270], loss=58.1310
	step [34/270], loss=68.2774
	step [35/270], loss=63.3354
	step [36/270], loss=61.2905
	step [37/270], loss=69.3342
	step [38/270], loss=78.4795
	step [39/270], loss=86.2512
	step [40/270], loss=67.4178
	step [41/270], loss=73.1150
	step [42/270], loss=64.5743
	step [43/270], loss=65.5266
	step [44/270], loss=99.7790
	step [45/270], loss=69.8614
	step [46/270], loss=59.1964
	step [47/270], loss=74.1367
	step [48/270], loss=70.8578
	step [49/270], loss=75.9330
	step [50/270], loss=65.5158
	step [51/270], loss=84.3307
	step [52/270], loss=66.5877
	step [53/270], loss=63.2600
	step [54/270], loss=69.0601
	step [55/270], loss=80.3264
	step [56/270], loss=67.2259
	step [57/270], loss=77.1448
	step [58/270], loss=68.2568
	step [59/270], loss=73.1724
	step [60/270], loss=65.1359
	step [61/270], loss=64.6007
	step [62/270], loss=74.7789
	step [63/270], loss=82.9073
	step [64/270], loss=70.8612
	step [65/270], loss=54.0374
	step [66/270], loss=71.9448
	step [67/270], loss=59.6820
	step [68/270], loss=65.9191
	step [69/270], loss=68.4789
	step [70/270], loss=65.3691
	step [71/270], loss=83.9300
	step [72/270], loss=90.1090
	step [73/270], loss=67.0841
	step [74/270], loss=84.6683
	step [75/270], loss=88.8655
	step [76/270], loss=77.7870
	step [77/270], loss=82.2514
	step [78/270], loss=58.8027
	step [79/270], loss=69.2510
	step [80/270], loss=65.4526
	step [81/270], loss=70.6602
	step [82/270], loss=83.8620
	step [83/270], loss=64.7338
	step [84/270], loss=79.1354
	step [85/270], loss=80.0439
	step [86/270], loss=76.2198
	step [87/270], loss=66.1643
	step [88/270], loss=83.9896
	step [89/270], loss=70.2670
	step [90/270], loss=79.6616
	step [91/270], loss=82.1669
	step [92/270], loss=78.3504
	step [93/270], loss=67.7274
	step [94/270], loss=87.3244
	step [95/270], loss=80.8151
	step [96/270], loss=69.9705
	step [97/270], loss=72.2691
	step [98/270], loss=85.5197
	step [99/270], loss=69.3355
	step [100/270], loss=74.4917
	step [101/270], loss=80.1824
	step [102/270], loss=87.7418
	step [103/270], loss=68.3451
	step [104/270], loss=79.4090
	step [105/270], loss=67.5143
	step [106/270], loss=59.8227
	step [107/270], loss=65.6692
	step [108/270], loss=73.2987
	step [109/270], loss=80.8008
	step [110/270], loss=79.4343
	step [111/270], loss=77.5482
	step [112/270], loss=65.2546
	step [113/270], loss=74.8624
	step [114/270], loss=69.9669
	step [115/270], loss=83.4433
	step [116/270], loss=75.0529
	step [117/270], loss=63.0009
	step [118/270], loss=66.2432
	step [119/270], loss=73.4667
	step [120/270], loss=65.3928
	step [121/270], loss=69.4220
	step [122/270], loss=81.4448
	step [123/270], loss=68.5368
	step [124/270], loss=81.5734
	step [125/270], loss=70.6928
	step [126/270], loss=67.0603
	step [127/270], loss=72.6396
	step [128/270], loss=68.9363
	step [129/270], loss=82.4104
	step [130/270], loss=75.5845
	step [131/270], loss=78.8486
	step [132/270], loss=73.1324
	step [133/270], loss=64.2087
	step [134/270], loss=65.6787
	step [135/270], loss=69.1299
	step [136/270], loss=82.6783
	step [137/270], loss=60.2522
	step [138/270], loss=67.8207
	step [139/270], loss=63.6558
	step [140/270], loss=61.4311
	step [141/270], loss=76.0729
	step [142/270], loss=71.5026
	step [143/270], loss=80.6007
	step [144/270], loss=83.0544
	step [145/270], loss=89.3487
	step [146/270], loss=79.1474
	step [147/270], loss=70.4673
	step [148/270], loss=80.8613
	step [149/270], loss=82.9532
	step [150/270], loss=61.1201
	step [151/270], loss=62.5637
	step [152/270], loss=69.2951
	step [153/270], loss=72.6958
	step [154/270], loss=70.8128
	step [155/270], loss=89.7374
	step [156/270], loss=72.8725
	step [157/270], loss=67.7644
	step [158/270], loss=64.5268
	step [159/270], loss=68.9699
	step [160/270], loss=82.8717
	step [161/270], loss=85.2161
	step [162/270], loss=67.5939
	step [163/270], loss=74.7722
	step [164/270], loss=64.3788
	step [165/270], loss=79.7267
	step [166/270], loss=74.5174
	step [167/270], loss=76.3077
	step [168/270], loss=85.1040
	step [169/270], loss=85.8944
	step [170/270], loss=55.5877
	step [171/270], loss=71.3239
	step [172/270], loss=69.0432
	step [173/270], loss=66.2851
	step [174/270], loss=69.5461
	step [175/270], loss=83.1875
	step [176/270], loss=78.3410
	step [177/270], loss=69.8358
	step [178/270], loss=63.8604
	step [179/270], loss=92.2551
	step [180/270], loss=84.8756
	step [181/270], loss=93.2418
	step [182/270], loss=72.5288
	step [183/270], loss=75.5938
	step [184/270], loss=95.2031
	step [185/270], loss=80.1886
	step [186/270], loss=73.9395
	step [187/270], loss=69.0336
	step [188/270], loss=81.9964
	step [189/270], loss=101.1596
	step [190/270], loss=71.7199
	step [191/270], loss=81.5720
	step [192/270], loss=61.6072
	step [193/270], loss=58.5071
	step [194/270], loss=62.0781
	step [195/270], loss=72.4303
	step [196/270], loss=79.4484
	step [197/270], loss=87.4662
	step [198/270], loss=71.1509
	step [199/270], loss=75.7977
	step [200/270], loss=64.4833
	step [201/270], loss=81.0669
	step [202/270], loss=63.6538
	step [203/270], loss=89.1786
	step [204/270], loss=69.6624
	step [205/270], loss=80.0188
	step [206/270], loss=70.8468
	step [207/270], loss=71.3783
	step [208/270], loss=64.6094
	step [209/270], loss=77.3005
	step [210/270], loss=70.8336
	step [211/270], loss=68.0920
	step [212/270], loss=65.0690
	step [213/270], loss=70.1371
	step [214/270], loss=78.1102
	step [215/270], loss=70.0866
	step [216/270], loss=81.2458
	step [217/270], loss=70.2052
	step [218/270], loss=90.6849
	step [219/270], loss=81.8504
	step [220/270], loss=67.5974
	step [221/270], loss=78.9644
	step [222/270], loss=70.5507
	step [223/270], loss=86.7026
	step [224/270], loss=59.1696
	step [225/270], loss=81.4375
	step [226/270], loss=80.1303
	step [227/270], loss=78.8126
	step [228/270], loss=74.9038
	step [229/270], loss=78.0279
	step [230/270], loss=70.1144
	step [231/270], loss=45.3541
	step [232/270], loss=68.0356
	step [233/270], loss=68.3266
	step [234/270], loss=78.2338
	step [235/270], loss=59.8737
	step [236/270], loss=76.7939
	step [237/270], loss=69.0772
	step [238/270], loss=79.2906
	step [239/270], loss=63.5267
	step [240/270], loss=84.0946
	step [241/270], loss=84.4252
	step [242/270], loss=77.1826
	step [243/270], loss=55.8390
	step [244/270], loss=85.6913
	step [245/270], loss=72.7630
	step [246/270], loss=62.1208
	step [247/270], loss=72.8777
	step [248/270], loss=60.4704
	step [249/270], loss=74.1189
	step [250/270], loss=75.6024
	step [251/270], loss=82.7313
	step [252/270], loss=69.1501
	step [253/270], loss=89.1982
	step [254/270], loss=78.5404
	step [255/270], loss=73.5584
	step [256/270], loss=70.3712
	step [257/270], loss=79.4738
	step [258/270], loss=84.5642
	step [259/270], loss=75.3676
	step [260/270], loss=83.1684
	step [261/270], loss=89.6549
	step [262/270], loss=69.0527
	step [263/270], loss=75.4528
	step [264/270], loss=64.1383
	step [265/270], loss=75.1564
	step [266/270], loss=76.9438
	step [267/270], loss=56.1932
	step [268/270], loss=77.1223
	step [269/270], loss=72.0403
	step [270/270], loss=10.3612
	Evaluating
	loss=0.0071, precision=0.3401, recall=0.8860, f1=0.4916
Training epoch 95
	step [1/270], loss=78.8224
	step [2/270], loss=79.0443
	step [3/270], loss=83.4404
	step [4/270], loss=60.8100
	step [5/270], loss=72.6790
	step [6/270], loss=70.7839
	step [7/270], loss=79.1490
	step [8/270], loss=74.0494
	step [9/270], loss=74.9793
	step [10/270], loss=59.9636
	step [11/270], loss=73.8769
	step [12/270], loss=66.1462
	step [13/270], loss=64.5728
	step [14/270], loss=73.7311
	step [15/270], loss=76.8641
	step [16/270], loss=75.0349
	step [17/270], loss=63.8018
	step [18/270], loss=52.6194
	step [19/270], loss=83.5686
	step [20/270], loss=89.6206
	step [21/270], loss=62.3572
	step [22/270], loss=75.6594
	step [23/270], loss=75.8696
	step [24/270], loss=67.0361
	step [25/270], loss=68.6156
	step [26/270], loss=82.5343
	step [27/270], loss=75.1151
	step [28/270], loss=78.9682
	step [29/270], loss=70.9707
	step [30/270], loss=71.7306
	step [31/270], loss=71.3607
	step [32/270], loss=78.7372
	step [33/270], loss=77.8981
	step [34/270], loss=63.2038
	step [35/270], loss=78.7458
	step [36/270], loss=66.6275
	step [37/270], loss=69.9867
	step [38/270], loss=62.3821
	step [39/270], loss=70.7700
	step [40/270], loss=101.8992
	step [41/270], loss=65.0341
	step [42/270], loss=84.8636
	step [43/270], loss=89.3756
	step [44/270], loss=79.7670
	step [45/270], loss=94.1534
	step [46/270], loss=72.5018
	step [47/270], loss=78.0817
	step [48/270], loss=104.1415
	step [49/270], loss=76.3665
	step [50/270], loss=76.8050
	step [51/270], loss=93.0875
	step [52/270], loss=79.7160
	step [53/270], loss=86.5220
	step [54/270], loss=73.6574
	step [55/270], loss=72.7438
	step [56/270], loss=76.2078
	step [57/270], loss=66.3458
	step [58/270], loss=73.0219
	step [59/270], loss=67.6237
	step [60/270], loss=80.7699
	step [61/270], loss=57.0008
	step [62/270], loss=63.1967
	step [63/270], loss=66.4197
	step [64/270], loss=70.3189
	step [65/270], loss=82.5920
	step [66/270], loss=68.4383
	step [67/270], loss=52.6430
	step [68/270], loss=85.7962
	step [69/270], loss=78.3854
	step [70/270], loss=79.4234
	step [71/270], loss=70.1723
	step [72/270], loss=75.6617
	step [73/270], loss=70.4859
	step [74/270], loss=63.4731
	step [75/270], loss=75.0538
	step [76/270], loss=80.2831
	step [77/270], loss=71.0159
	step [78/270], loss=81.3567
	step [79/270], loss=79.6030
	step [80/270], loss=81.3823
	step [81/270], loss=86.4547
	step [82/270], loss=95.3467
	step [83/270], loss=70.5868
	step [84/270], loss=69.5353
	step [85/270], loss=98.4721
	step [86/270], loss=59.5975
	step [87/270], loss=68.3095
	step [88/270], loss=78.5784
	step [89/270], loss=88.6231
	step [90/270], loss=77.4445
	step [91/270], loss=77.1185
	step [92/270], loss=70.0183
	step [93/270], loss=72.4916
	step [94/270], loss=70.7012
	step [95/270], loss=68.9788
	step [96/270], loss=76.2775
	step [97/270], loss=78.9521
	step [98/270], loss=72.5595
	step [99/270], loss=79.5696
	step [100/270], loss=54.5952
	step [101/270], loss=79.6979
	step [102/270], loss=71.4807
	step [103/270], loss=78.2807
	step [104/270], loss=68.0710
	step [105/270], loss=65.4432
	step [106/270], loss=70.2566
	step [107/270], loss=70.5565
	step [108/270], loss=74.2572
	step [109/270], loss=62.8582
	step [110/270], loss=76.2435
	step [111/270], loss=76.0518
	step [112/270], loss=60.9546
	step [113/270], loss=65.2565
	step [114/270], loss=77.9563
	step [115/270], loss=78.7289
	step [116/270], loss=73.5910
	step [117/270], loss=75.4349
	step [118/270], loss=84.2440
	step [119/270], loss=71.7451
	step [120/270], loss=76.4847
	step [121/270], loss=66.7713
	step [122/270], loss=77.7362
	step [123/270], loss=63.9817
	step [124/270], loss=71.0004
	step [125/270], loss=72.5832
	step [126/270], loss=66.1957
	step [127/270], loss=75.2565
	step [128/270], loss=57.2014
	step [129/270], loss=64.5724
	step [130/270], loss=87.4587
	step [131/270], loss=70.3839
	step [132/270], loss=63.9547
	step [133/270], loss=88.7040
	step [134/270], loss=72.5445
	step [135/270], loss=73.9271
	step [136/270], loss=80.3278
	step [137/270], loss=78.6493
	step [138/270], loss=86.6488
	step [139/270], loss=61.1926
	step [140/270], loss=76.4702
	step [141/270], loss=59.7611
	step [142/270], loss=73.5271
	step [143/270], loss=66.3485
	step [144/270], loss=63.6346
	step [145/270], loss=66.5649
	step [146/270], loss=71.1027
	step [147/270], loss=64.9420
	step [148/270], loss=68.8895
	step [149/270], loss=73.0055
	step [150/270], loss=67.6836
	step [151/270], loss=81.2876
	step [152/270], loss=75.3744
	step [153/270], loss=64.3547
	step [154/270], loss=79.1272
	step [155/270], loss=76.6657
	step [156/270], loss=84.2169
	step [157/270], loss=72.6870
	step [158/270], loss=75.3932
	step [159/270], loss=78.2326
	step [160/270], loss=65.5386
	step [161/270], loss=69.8257
	step [162/270], loss=79.5464
	step [163/270], loss=77.5236
	step [164/270], loss=69.1527
	step [165/270], loss=67.7641
	step [166/270], loss=70.4514
	step [167/270], loss=73.3570
	step [168/270], loss=70.0457
	step [169/270], loss=91.9512
	step [170/270], loss=68.2807
	step [171/270], loss=80.1990
	step [172/270], loss=68.3500
	step [173/270], loss=80.8201
	step [174/270], loss=77.4442
	step [175/270], loss=78.3005
	step [176/270], loss=70.0615
	step [177/270], loss=69.7537
	step [178/270], loss=69.9373
	step [179/270], loss=68.9151
	step [180/270], loss=69.5524
	step [181/270], loss=62.2386
	step [182/270], loss=62.0645
	step [183/270], loss=54.2493
	step [184/270], loss=78.5076
	step [185/270], loss=82.5820
	step [186/270], loss=81.6831
	step [187/270], loss=77.8671
	step [188/270], loss=63.6133
	step [189/270], loss=73.9358
	step [190/270], loss=74.0835
	step [191/270], loss=62.6701
	step [192/270], loss=75.3341
	step [193/270], loss=78.0090
	step [194/270], loss=71.6856
	step [195/270], loss=63.0751
	step [196/270], loss=74.0613
	step [197/270], loss=77.9957
	step [198/270], loss=75.6992
	step [199/270], loss=75.3138
	step [200/270], loss=79.9174
	step [201/270], loss=67.6906
	step [202/270], loss=67.0356
	step [203/270], loss=86.3455
	step [204/270], loss=78.5701
	step [205/270], loss=66.2592
	step [206/270], loss=63.7142
	step [207/270], loss=64.3974
	step [208/270], loss=73.7115
	step [209/270], loss=68.2113
	step [210/270], loss=76.8795
	step [211/270], loss=67.0347
	step [212/270], loss=60.4395
	step [213/270], loss=81.2457
	step [214/270], loss=81.4543
	step [215/270], loss=66.5904
	step [216/270], loss=95.5138
	step [217/270], loss=69.6429
	step [218/270], loss=60.2102
	step [219/270], loss=68.5457
	step [220/270], loss=77.4555
	step [221/270], loss=56.8525
	step [222/270], loss=62.3334
	step [223/270], loss=74.7892
	step [224/270], loss=51.7799
	step [225/270], loss=82.0862
	step [226/270], loss=73.6261
	step [227/270], loss=58.3536
	step [228/270], loss=70.3606
	step [229/270], loss=71.4364
	step [230/270], loss=75.9424
	step [231/270], loss=70.9477
	step [232/270], loss=80.3665
	step [233/270], loss=81.2281
	step [234/270], loss=75.7514
	step [235/270], loss=71.7617
	step [236/270], loss=75.1558
	step [237/270], loss=85.8863
	step [238/270], loss=65.9630
	step [239/270], loss=83.6549
	step [240/270], loss=71.0352
	step [241/270], loss=72.3462
	step [242/270], loss=72.5085
	step [243/270], loss=76.6252
	step [244/270], loss=68.9567
	step [245/270], loss=69.7703
	step [246/270], loss=68.5910
	step [247/270], loss=67.0371
	step [248/270], loss=63.9263
	step [249/270], loss=83.7939
	step [250/270], loss=74.8255
	step [251/270], loss=73.8951
	step [252/270], loss=70.1171
	step [253/270], loss=73.6137
	step [254/270], loss=70.6209
	step [255/270], loss=77.4766
	step [256/270], loss=76.2696
	step [257/270], loss=68.4781
	step [258/270], loss=72.7095
	step [259/270], loss=68.9847
	step [260/270], loss=88.9177
	step [261/270], loss=68.3400
	step [262/270], loss=69.3512
	step [263/270], loss=68.3355
	step [264/270], loss=67.4826
	step [265/270], loss=64.1977
	step [266/270], loss=66.0408
	step [267/270], loss=64.8134
	step [268/270], loss=85.0187
	step [269/270], loss=69.7925
	step [270/270], loss=13.8737
	Evaluating
	loss=0.0066, precision=0.3635, recall=0.8920, f1=0.5165
Training epoch 96
	step [1/270], loss=78.3918
	step [2/270], loss=70.1373
	step [3/270], loss=91.5684
	step [4/270], loss=66.8609
	step [5/270], loss=71.3805
	step [6/270], loss=67.0672
	step [7/270], loss=71.8569
	step [8/270], loss=81.5612
	step [9/270], loss=74.5694
	step [10/270], loss=98.3119
	step [11/270], loss=80.6079
	step [12/270], loss=78.2189
	step [13/270], loss=86.1013
	step [14/270], loss=63.7683
	step [15/270], loss=72.0212
	step [16/270], loss=68.6203
	step [17/270], loss=66.0595
	step [18/270], loss=67.4164
	step [19/270], loss=82.7876
	step [20/270], loss=71.7181
	step [21/270], loss=72.5520
	step [22/270], loss=66.0962
	step [23/270], loss=56.2385
	step [24/270], loss=65.2578
	step [25/270], loss=73.6083
	step [26/270], loss=89.5857
	step [27/270], loss=75.9364
	step [28/270], loss=67.7420
	step [29/270], loss=79.7971
	step [30/270], loss=59.4152
	step [31/270], loss=64.3900
	step [32/270], loss=63.7113
	step [33/270], loss=72.0798
	step [34/270], loss=78.6935
	step [35/270], loss=69.4163
	step [36/270], loss=62.9610
	step [37/270], loss=80.9249
	step [38/270], loss=65.0289
	step [39/270], loss=82.0405
	step [40/270], loss=70.4037
	step [41/270], loss=76.1251
	step [42/270], loss=81.5903
	step [43/270], loss=71.2852
	step [44/270], loss=72.6385
	step [45/270], loss=52.0502
	step [46/270], loss=65.4134
	step [47/270], loss=74.7674
	step [48/270], loss=71.1206
	step [49/270], loss=63.7690
	step [50/270], loss=83.9962
	step [51/270], loss=88.7911
	step [52/270], loss=67.4846
	step [53/270], loss=87.4704
	step [54/270], loss=78.5791
	step [55/270], loss=92.1735
	step [56/270], loss=61.6317
	step [57/270], loss=78.9583
	step [58/270], loss=83.3710
	step [59/270], loss=83.9803
	step [60/270], loss=66.9012
	step [61/270], loss=74.0783
	step [62/270], loss=61.6827
	step [63/270], loss=66.3480
	step [64/270], loss=69.3964
	step [65/270], loss=63.7279
	step [66/270], loss=64.4982
	step [67/270], loss=61.9743
	step [68/270], loss=83.9063
	step [69/270], loss=78.1737
	step [70/270], loss=75.2336
	step [71/270], loss=71.2393
	step [72/270], loss=77.4746
	step [73/270], loss=81.2150
	step [74/270], loss=85.4717
	step [75/270], loss=78.9798
	step [76/270], loss=69.7069
	step [77/270], loss=70.8841
	step [78/270], loss=62.6624
	step [79/270], loss=67.6197
	step [80/270], loss=100.0062
	step [81/270], loss=64.7474
	step [82/270], loss=71.7775
	step [83/270], loss=69.2898
	step [84/270], loss=79.4664
	step [85/270], loss=80.5249
	step [86/270], loss=89.2856
	step [87/270], loss=71.0578
	step [88/270], loss=78.1068
	step [89/270], loss=73.0958
	step [90/270], loss=58.8842
	step [91/270], loss=82.5253
	step [92/270], loss=64.2578
	step [93/270], loss=88.8984
	step [94/270], loss=65.2580
	step [95/270], loss=65.0405
	step [96/270], loss=73.6815
	step [97/270], loss=68.7951
	step [98/270], loss=64.5991
	step [99/270], loss=59.5231
	step [100/270], loss=66.3808
	step [101/270], loss=64.1578
	step [102/270], loss=72.0817
	step [103/270], loss=71.2399
	step [104/270], loss=79.5737
	step [105/270], loss=65.5855
	step [106/270], loss=67.8985
	step [107/270], loss=56.8802
	step [108/270], loss=76.3301
	step [109/270], loss=63.5331
	step [110/270], loss=81.0324
	step [111/270], loss=84.5803
	step [112/270], loss=66.3558
	step [113/270], loss=59.5865
	step [114/270], loss=70.9796
	step [115/270], loss=63.0950
	step [116/270], loss=64.0207
	step [117/270], loss=73.7802
	step [118/270], loss=86.5106
	step [119/270], loss=78.0436
	step [120/270], loss=73.7161
	step [121/270], loss=76.6355
	step [122/270], loss=73.4904
	step [123/270], loss=59.3581
	step [124/270], loss=68.3501
	step [125/270], loss=75.5200
	step [126/270], loss=82.6486
	step [127/270], loss=64.6296
	step [128/270], loss=66.2458
	step [129/270], loss=59.4274
	step [130/270], loss=59.6003
	step [131/270], loss=69.8883
	step [132/270], loss=70.7168
	step [133/270], loss=68.8466
	step [134/270], loss=64.5240
	step [135/270], loss=69.2278
	step [136/270], loss=65.3553
	step [137/270], loss=73.2439
	step [138/270], loss=72.9989
	step [139/270], loss=86.2776
	step [140/270], loss=87.6421
	step [141/270], loss=69.1651
	step [142/270], loss=87.1929
	step [143/270], loss=71.4250
	step [144/270], loss=74.4559
	step [145/270], loss=64.7870
	step [146/270], loss=95.0599
	step [147/270], loss=66.8914
	step [148/270], loss=67.8781
	step [149/270], loss=79.5314
	step [150/270], loss=71.6269
	step [151/270], loss=73.0785
	step [152/270], loss=76.7443
	step [153/270], loss=84.5804
	step [154/270], loss=69.8935
	step [155/270], loss=64.9422
	step [156/270], loss=74.5409
	step [157/270], loss=75.6633
	step [158/270], loss=86.6708
	step [159/270], loss=81.8836
	step [160/270], loss=86.0346
	step [161/270], loss=69.7555
	step [162/270], loss=70.9772
	step [163/270], loss=54.5750
	step [164/270], loss=68.5172
	step [165/270], loss=70.8994
	step [166/270], loss=77.6979
	step [167/270], loss=73.5356
	step [168/270], loss=77.2850
	step [169/270], loss=61.4941
	step [170/270], loss=80.1034
	step [171/270], loss=83.1097
	step [172/270], loss=68.6223
	step [173/270], loss=79.8326
	step [174/270], loss=66.2478
	step [175/270], loss=87.9168
	step [176/270], loss=69.7259
	step [177/270], loss=83.8188
	step [178/270], loss=71.7453
	step [179/270], loss=79.5681
	step [180/270], loss=70.7605
	step [181/270], loss=67.4507
	step [182/270], loss=76.4925
	step [183/270], loss=88.6410
	step [184/270], loss=77.1190
	step [185/270], loss=84.3901
	step [186/270], loss=77.8696
	step [187/270], loss=72.0181
	step [188/270], loss=63.7635
	step [189/270], loss=65.1570
	step [190/270], loss=77.2687
	step [191/270], loss=66.8664
	step [192/270], loss=61.2681
	step [193/270], loss=95.7008
	step [194/270], loss=72.5803
	step [195/270], loss=72.6645
	step [196/270], loss=70.5340
	step [197/270], loss=79.9445
	step [198/270], loss=76.8440
	step [199/270], loss=65.7757
	step [200/270], loss=67.4817
	step [201/270], loss=66.2210
	step [202/270], loss=75.0996
	step [203/270], loss=63.0233
	step [204/270], loss=86.8912
	step [205/270], loss=65.7980
	step [206/270], loss=65.0425
	step [207/270], loss=76.1773
	step [208/270], loss=70.6196
	step [209/270], loss=68.2308
	step [210/270], loss=71.3788
	step [211/270], loss=71.2593
	step [212/270], loss=70.3051
	step [213/270], loss=76.8029
	step [214/270], loss=82.4205
	step [215/270], loss=68.5297
	step [216/270], loss=68.8194
	step [217/270], loss=78.4633
	step [218/270], loss=71.2608
	step [219/270], loss=69.6471
	step [220/270], loss=84.0635
	step [221/270], loss=71.5492
	step [222/270], loss=72.0717
	step [223/270], loss=72.3333
	step [224/270], loss=65.7206
	step [225/270], loss=71.3407
	step [226/270], loss=67.2438
	step [227/270], loss=63.8470
	step [228/270], loss=79.6119
	step [229/270], loss=66.0247
	step [230/270], loss=76.0540
	step [231/270], loss=83.2912
	step [232/270], loss=73.1117
	step [233/270], loss=72.5956
	step [234/270], loss=62.1958
	step [235/270], loss=63.6117
	step [236/270], loss=62.2048
	step [237/270], loss=77.1835
	step [238/270], loss=71.9983
	step [239/270], loss=66.1777
	step [240/270], loss=56.1301
	step [241/270], loss=68.4408
	step [242/270], loss=68.0346
	step [243/270], loss=65.0911
	step [244/270], loss=81.1046
	step [245/270], loss=76.8673
	step [246/270], loss=80.0114
	step [247/270], loss=77.6246
	step [248/270], loss=70.7540
	step [249/270], loss=61.3626
	step [250/270], loss=84.4450
	step [251/270], loss=76.7537
	step [252/270], loss=75.7307
	step [253/270], loss=72.1249
	step [254/270], loss=79.7177
	step [255/270], loss=70.4958
	step [256/270], loss=66.0390
	step [257/270], loss=73.5661
	step [258/270], loss=55.5935
	step [259/270], loss=69.2732
	step [260/270], loss=73.4609
	step [261/270], loss=79.5891
	step [262/270], loss=72.8680
	step [263/270], loss=71.0984
	step [264/270], loss=83.0232
	step [265/270], loss=70.1343
	step [266/270], loss=68.9175
	step [267/270], loss=71.4647
	step [268/270], loss=71.2091
	step [269/270], loss=64.8433
	step [270/270], loss=16.7874
	Evaluating
	loss=0.0072, precision=0.3470, recall=0.9030, f1=0.5013
Training epoch 97
	step [1/270], loss=80.3772
	step [2/270], loss=73.8737
	step [3/270], loss=91.2067
	step [4/270], loss=83.4654
	step [5/270], loss=73.2487
	step [6/270], loss=60.6160
	step [7/270], loss=68.6267
	step [8/270], loss=70.2008
	step [9/270], loss=86.1623
	step [10/270], loss=79.2987
	step [11/270], loss=76.5444
	step [12/270], loss=83.5843
	step [13/270], loss=83.3938
	step [14/270], loss=59.7641
	step [15/270], loss=73.9048
	step [16/270], loss=89.5551
	step [17/270], loss=76.4617
	step [18/270], loss=71.4503
	step [19/270], loss=63.9113
	step [20/270], loss=74.5365
	step [21/270], loss=79.5129
	step [22/270], loss=77.2110
	step [23/270], loss=64.9017
	step [24/270], loss=68.4958
	step [25/270], loss=74.8671
	step [26/270], loss=70.8121
	step [27/270], loss=81.1086
	step [28/270], loss=65.7398
	step [29/270], loss=86.9979
	step [30/270], loss=59.8254
	step [31/270], loss=62.6135
	step [32/270], loss=64.0414
	step [33/270], loss=67.1947
	step [34/270], loss=78.9150
	step [35/270], loss=68.9802
	step [36/270], loss=66.1876
	step [37/270], loss=66.0404
	step [38/270], loss=68.4018
	step [39/270], loss=61.3663
	step [40/270], loss=84.7089
	step [41/270], loss=68.2245
	step [42/270], loss=67.1904
	step [43/270], loss=88.6796
	step [44/270], loss=68.6885
	step [45/270], loss=71.1468
	step [46/270], loss=68.5868
	step [47/270], loss=65.3981
	step [48/270], loss=69.5981
	step [49/270], loss=66.9075
	step [50/270], loss=62.9886
	step [51/270], loss=75.7156
	step [52/270], loss=71.5671
	step [53/270], loss=78.7009
	step [54/270], loss=72.8804
	step [55/270], loss=68.3240
	step [56/270], loss=88.9016
	step [57/270], loss=61.7027
	step [58/270], loss=78.4910
	step [59/270], loss=80.8025
	step [60/270], loss=62.5281
	step [61/270], loss=68.3203
	step [62/270], loss=67.7274
	step [63/270], loss=69.6494
	step [64/270], loss=82.1955
	step [65/270], loss=78.6416
	step [66/270], loss=76.9460
	step [67/270], loss=72.2754
	step [68/270], loss=82.7787
	step [69/270], loss=79.2134
	step [70/270], loss=46.8718
	step [71/270], loss=68.8970
	step [72/270], loss=78.5518
	step [73/270], loss=65.7845
	step [74/270], loss=88.1158
	step [75/270], loss=78.2958
	step [76/270], loss=74.9284
	step [77/270], loss=80.6085
	step [78/270], loss=76.7252
	step [79/270], loss=67.5552
	step [80/270], loss=70.8406
	step [81/270], loss=72.4166
	step [82/270], loss=81.8705
	step [83/270], loss=94.4830
	step [84/270], loss=73.5984
	step [85/270], loss=73.2774
	step [86/270], loss=62.8001
	step [87/270], loss=68.8251
	step [88/270], loss=79.8516
	step [89/270], loss=72.4896
	step [90/270], loss=59.6599
	step [91/270], loss=75.5239
	step [92/270], loss=75.2677
	step [93/270], loss=77.6769
	step [94/270], loss=80.4592
	step [95/270], loss=88.5905
	step [96/270], loss=65.4013
	step [97/270], loss=66.6127
	step [98/270], loss=66.5752
	step [99/270], loss=90.7959
	step [100/270], loss=97.6080
	step [101/270], loss=71.1392
	step [102/270], loss=63.0326
	step [103/270], loss=77.8041
	step [104/270], loss=67.5290
	step [105/270], loss=65.7530
	step [106/270], loss=82.1944
	step [107/270], loss=67.8477
	step [108/270], loss=70.6637
	step [109/270], loss=72.1940
	step [110/270], loss=73.0104
	step [111/270], loss=66.1723
	step [112/270], loss=83.1123
	step [113/270], loss=72.7579
	step [114/270], loss=80.7063
	step [115/270], loss=88.3171
	step [116/270], loss=63.2838
	step [117/270], loss=71.6682
	step [118/270], loss=72.0670
	step [119/270], loss=67.1626
	step [120/270], loss=83.9589
	step [121/270], loss=70.2253
	step [122/270], loss=68.5872
	step [123/270], loss=63.2344
	step [124/270], loss=78.9558
	step [125/270], loss=71.7642
	step [126/270], loss=74.6079
	step [127/270], loss=70.6533
	step [128/270], loss=73.3190
	step [129/270], loss=60.0837
	step [130/270], loss=83.9681
	step [131/270], loss=79.3856
	step [132/270], loss=65.6366
	step [133/270], loss=85.3524
	step [134/270], loss=73.0092
	step [135/270], loss=64.5893
	step [136/270], loss=66.8011
	step [137/270], loss=73.9748
	step [138/270], loss=66.3136
	step [139/270], loss=82.0893
	step [140/270], loss=63.0550
	step [141/270], loss=63.8999
	step [142/270], loss=82.8137
	step [143/270], loss=60.8337
	step [144/270], loss=85.0569
	step [145/270], loss=83.0580
	step [146/270], loss=62.4739
	step [147/270], loss=69.5079
	step [148/270], loss=78.6526
	step [149/270], loss=72.3016
	step [150/270], loss=69.0913
	step [151/270], loss=79.7088
	step [152/270], loss=62.9840
	step [153/270], loss=62.1256
	step [154/270], loss=84.0113
	step [155/270], loss=86.4837
	step [156/270], loss=66.2616
	step [157/270], loss=61.3248
	step [158/270], loss=71.7432
	step [159/270], loss=72.6333
	step [160/270], loss=84.0280
	step [161/270], loss=68.8000
	step [162/270], loss=82.6016
	step [163/270], loss=72.9048
	step [164/270], loss=64.9586
	step [165/270], loss=71.9670
	step [166/270], loss=77.4507
	step [167/270], loss=70.5788
	step [168/270], loss=76.5564
	step [169/270], loss=74.7824
	step [170/270], loss=77.1036
	step [171/270], loss=83.5929
	step [172/270], loss=92.0275
	step [173/270], loss=70.2739
	step [174/270], loss=57.2152
	step [175/270], loss=75.8508
	step [176/270], loss=64.6358
	step [177/270], loss=77.8897
	step [178/270], loss=73.3805
	step [179/270], loss=70.5479
	step [180/270], loss=72.4818
	step [181/270], loss=68.6540
	step [182/270], loss=82.8898
	step [183/270], loss=81.6011
	step [184/270], loss=69.8382
	step [185/270], loss=70.4090
	step [186/270], loss=69.7117
	step [187/270], loss=76.5717
	step [188/270], loss=73.4286
	step [189/270], loss=60.3777
	step [190/270], loss=74.5540
	step [191/270], loss=62.8977
	step [192/270], loss=59.9117
	step [193/270], loss=85.6200
	step [194/270], loss=78.3826
	step [195/270], loss=69.2954
	step [196/270], loss=68.3873
	step [197/270], loss=63.7513
	step [198/270], loss=74.7476
	step [199/270], loss=74.0061
	step [200/270], loss=73.8576
	step [201/270], loss=64.6975
	step [202/270], loss=90.0178
	step [203/270], loss=77.3698
	step [204/270], loss=87.3717
	step [205/270], loss=76.1217
	step [206/270], loss=56.9382
	step [207/270], loss=80.6553
	step [208/270], loss=69.3531
	step [209/270], loss=77.0107
	step [210/270], loss=77.1856
	step [211/270], loss=68.6424
	step [212/270], loss=63.2915
	step [213/270], loss=67.2868
	step [214/270], loss=73.8341
	step [215/270], loss=64.5325
	step [216/270], loss=75.5345
	step [217/270], loss=65.6846
	step [218/270], loss=65.3659
	step [219/270], loss=77.3268
	step [220/270], loss=86.8786
	step [221/270], loss=72.0147
	step [222/270], loss=71.3113
	step [223/270], loss=80.3085
	step [224/270], loss=65.4097
	step [225/270], loss=76.5735
	step [226/270], loss=65.1590
	step [227/270], loss=89.3473
	step [228/270], loss=70.6256
	step [229/270], loss=62.3441
	step [230/270], loss=66.1191
	step [231/270], loss=69.1369
	step [232/270], loss=79.8340
	step [233/270], loss=81.1255
	step [234/270], loss=70.4522
	step [235/270], loss=66.9608
	step [236/270], loss=71.0062
	step [237/270], loss=64.8169
	step [238/270], loss=80.3184
	step [239/270], loss=46.7219
	step [240/270], loss=61.6643
	step [241/270], loss=68.4580
	step [242/270], loss=79.7158
	step [243/270], loss=68.2977
	step [244/270], loss=74.5586
	step [245/270], loss=79.3441
	step [246/270], loss=65.3007
	step [247/270], loss=65.1851
	step [248/270], loss=60.4543
	step [249/270], loss=63.1431
	step [250/270], loss=73.3409
	step [251/270], loss=69.5709
	step [252/270], loss=69.4901
	step [253/270], loss=72.7423
	step [254/270], loss=63.7001
	step [255/270], loss=50.3181
	step [256/270], loss=61.9166
	step [257/270], loss=69.9287
	step [258/270], loss=62.6999
	step [259/270], loss=66.9080
	step [260/270], loss=72.0021
	step [261/270], loss=68.5427
	step [262/270], loss=88.6134
	step [263/270], loss=77.5810
	step [264/270], loss=68.0757
	step [265/270], loss=74.0529
	step [266/270], loss=76.2771
	step [267/270], loss=83.8651
	step [268/270], loss=70.8374
	step [269/270], loss=68.2240
	step [270/270], loss=12.6831
	Evaluating
	loss=0.0071, precision=0.3422, recall=0.8990, f1=0.4957
Training epoch 98
	step [1/270], loss=86.7685
	step [2/270], loss=81.2058
	step [3/270], loss=79.2380
	step [4/270], loss=87.1356
	step [5/270], loss=79.3235
	step [6/270], loss=78.0683
	step [7/270], loss=68.8335
	step [8/270], loss=88.9802
	step [9/270], loss=72.5863
	step [10/270], loss=78.8074
	step [11/270], loss=81.4248
	step [12/270], loss=72.3911
	step [13/270], loss=56.3691
	step [14/270], loss=76.5269
	step [15/270], loss=54.5118
	step [16/270], loss=79.3232
	step [17/270], loss=64.9194
	step [18/270], loss=73.4047
	step [19/270], loss=64.6878
	step [20/270], loss=73.1183
	step [21/270], loss=69.1321
	step [22/270], loss=79.8226
	step [23/270], loss=62.2875
	step [24/270], loss=66.9497
	step [25/270], loss=82.6541
	step [26/270], loss=76.2602
	step [27/270], loss=66.5907
	step [28/270], loss=76.7321
	step [29/270], loss=74.0421
	step [30/270], loss=68.7155
	step [31/270], loss=67.2285
	step [32/270], loss=72.3143
	step [33/270], loss=78.9362
	step [34/270], loss=72.3280
	step [35/270], loss=70.2021
	step [36/270], loss=70.1310
	step [37/270], loss=71.1515
	step [38/270], loss=69.7602
	step [39/270], loss=68.1411
	step [40/270], loss=64.9293
	step [41/270], loss=74.8768
	step [42/270], loss=73.1476
	step [43/270], loss=63.6461
	step [44/270], loss=79.5947
	step [45/270], loss=70.3301
	step [46/270], loss=74.2386
	step [47/270], loss=60.4208
	step [48/270], loss=91.8190
	step [49/270], loss=76.1693
	step [50/270], loss=75.8167
	step [51/270], loss=73.9261
	step [52/270], loss=60.2331
	step [53/270], loss=80.0036
	step [54/270], loss=63.5388
	step [55/270], loss=59.4582
	step [56/270], loss=62.9244
	step [57/270], loss=75.5821
	step [58/270], loss=72.2062
	step [59/270], loss=53.4907
	step [60/270], loss=71.1577
	step [61/270], loss=69.5840
	step [62/270], loss=60.0150
	step [63/270], loss=61.0107
	step [64/270], loss=73.2027
	step [65/270], loss=73.2608
	step [66/270], loss=61.8263
	step [67/270], loss=57.8577
	step [68/270], loss=78.1494
	step [69/270], loss=64.3888
	step [70/270], loss=57.2353
	step [71/270], loss=70.7230
	step [72/270], loss=74.9358
	step [73/270], loss=73.1840
	step [74/270], loss=69.8088
	step [75/270], loss=67.0659
	step [76/270], loss=77.3008
	step [77/270], loss=78.9566
	step [78/270], loss=65.5370
	step [79/270], loss=81.4626
	step [80/270], loss=57.8538
	step [81/270], loss=70.6194
	step [82/270], loss=73.6791
	step [83/270], loss=73.9550
	step [84/270], loss=75.9394
	step [85/270], loss=61.8496
	step [86/270], loss=70.4351
	step [87/270], loss=91.8960
	step [88/270], loss=61.8417
	step [89/270], loss=73.6138
	step [90/270], loss=77.6206
	step [91/270], loss=69.7352
	step [92/270], loss=68.2985
	step [93/270], loss=58.4316
	step [94/270], loss=68.4419
	step [95/270], loss=67.8582
	step [96/270], loss=81.7442
	step [97/270], loss=79.4116
	step [98/270], loss=74.2709
	step [99/270], loss=61.4488
	step [100/270], loss=62.5026
	step [101/270], loss=67.2749
	step [102/270], loss=71.3401
	step [103/270], loss=73.5588
	step [104/270], loss=72.0001
	step [105/270], loss=77.2562
	step [106/270], loss=74.4855
	step [107/270], loss=78.6489
	step [108/270], loss=62.3289
	step [109/270], loss=72.0797
	step [110/270], loss=59.3008
	step [111/270], loss=82.0442
	step [112/270], loss=76.2846
	step [113/270], loss=76.3531
	step [114/270], loss=77.4407
	step [115/270], loss=63.3248
	step [116/270], loss=55.3348
	step [117/270], loss=60.6556
	step [118/270], loss=79.5370
	step [119/270], loss=67.4337
	step [120/270], loss=64.5640
	step [121/270], loss=75.4794
	step [122/270], loss=57.3531
	step [123/270], loss=61.2679
	step [124/270], loss=59.1986
	step [125/270], loss=69.3479
	step [126/270], loss=61.4208
	step [127/270], loss=72.4740
	step [128/270], loss=77.6834
	step [129/270], loss=56.4129
	step [130/270], loss=84.6082
	step [131/270], loss=73.6783
	step [132/270], loss=89.2810
	step [133/270], loss=69.0170
	step [134/270], loss=87.1511
	step [135/270], loss=64.5947
	step [136/270], loss=83.7025
	step [137/270], loss=79.8064
	step [138/270], loss=77.6348
	step [139/270], loss=68.8363
	step [140/270], loss=66.9115
	step [141/270], loss=58.9440
	step [142/270], loss=73.4203
	step [143/270], loss=73.5920
	step [144/270], loss=67.6946
	step [145/270], loss=83.1623
	step [146/270], loss=78.8031
	step [147/270], loss=78.2823
	step [148/270], loss=70.1250
	step [149/270], loss=80.3646
	step [150/270], loss=75.5235
	step [151/270], loss=83.0619
	step [152/270], loss=55.5382
	step [153/270], loss=87.3082
	step [154/270], loss=90.4414
	step [155/270], loss=75.8189
	step [156/270], loss=66.5032
	step [157/270], loss=85.4059
	step [158/270], loss=57.8212
	step [159/270], loss=71.6301
	step [160/270], loss=52.9008
	step [161/270], loss=77.3680
	step [162/270], loss=70.1621
	step [163/270], loss=68.1707
	step [164/270], loss=75.4024
	step [165/270], loss=82.2983
	step [166/270], loss=79.0776
	step [167/270], loss=72.1510
	step [168/270], loss=68.7987
	step [169/270], loss=78.1871
	step [170/270], loss=82.6510
	step [171/270], loss=84.1547
	step [172/270], loss=65.4050
	step [173/270], loss=74.3816
	step [174/270], loss=69.6132
	step [175/270], loss=76.8384
	step [176/270], loss=82.1383
	step [177/270], loss=67.3380
	step [178/270], loss=73.6480
	step [179/270], loss=73.9618
	step [180/270], loss=77.7721
	step [181/270], loss=59.5241
	step [182/270], loss=68.9313
	step [183/270], loss=91.2220
	step [184/270], loss=76.6365
	step [185/270], loss=73.2224
	step [186/270], loss=81.4459
	step [187/270], loss=62.0726
	step [188/270], loss=70.4203
	step [189/270], loss=78.1055
	step [190/270], loss=59.8180
	step [191/270], loss=84.6724
	step [192/270], loss=65.2457
	step [193/270], loss=76.1071
	step [194/270], loss=71.6818
	step [195/270], loss=75.0623
	step [196/270], loss=67.2122
	step [197/270], loss=77.1154
	step [198/270], loss=75.9297
	step [199/270], loss=62.4161
	step [200/270], loss=78.7780
	step [201/270], loss=53.8229
	step [202/270], loss=75.1109
	step [203/270], loss=68.1150
	step [204/270], loss=69.5258
	step [205/270], loss=85.2152
	step [206/270], loss=64.9578
	step [207/270], loss=73.8579
	step [208/270], loss=84.0706
	step [209/270], loss=84.9556
	step [210/270], loss=66.7494
	step [211/270], loss=87.6687
	step [212/270], loss=80.2250
	step [213/270], loss=84.1330
	step [214/270], loss=85.2672
	step [215/270], loss=66.7807
	step [216/270], loss=81.7740
	step [217/270], loss=80.2257
	step [218/270], loss=71.8067
	step [219/270], loss=77.4198
	step [220/270], loss=67.3577
	step [221/270], loss=76.6964
	step [222/270], loss=85.3329
	step [223/270], loss=77.5244
	step [224/270], loss=78.0863
	step [225/270], loss=76.4915
	step [226/270], loss=61.4197
	step [227/270], loss=80.5417
	step [228/270], loss=64.2920
	step [229/270], loss=73.5387
	step [230/270], loss=66.1258
	step [231/270], loss=74.9130
	step [232/270], loss=90.5490
	step [233/270], loss=70.5734
	step [234/270], loss=66.0708
	step [235/270], loss=72.4074
	step [236/270], loss=75.0185
	step [237/270], loss=78.1477
	step [238/270], loss=72.1137
	step [239/270], loss=64.8205
	step [240/270], loss=67.1301
	step [241/270], loss=69.6958
	step [242/270], loss=81.1526
	step [243/270], loss=58.8784
	step [244/270], loss=76.3834
	step [245/270], loss=64.9650
	step [246/270], loss=69.7380
	step [247/270], loss=75.3008
	step [248/270], loss=82.7809
	step [249/270], loss=90.5225
	step [250/270], loss=74.5943
	step [251/270], loss=80.5743
	step [252/270], loss=60.8041
	step [253/270], loss=83.1721
	step [254/270], loss=70.4229
	step [255/270], loss=72.0541
	step [256/270], loss=69.1261
	step [257/270], loss=68.4367
	step [258/270], loss=82.5069
	step [259/270], loss=68.4581
	step [260/270], loss=85.2529
	step [261/270], loss=77.5457
	step [262/270], loss=88.1778
	step [263/270], loss=63.7104
	step [264/270], loss=66.9529
	step [265/270], loss=71.4420
	step [266/270], loss=82.9198
	step [267/270], loss=69.6516
	step [268/270], loss=80.8747
	step [269/270], loss=73.3791
	step [270/270], loss=9.8303
	Evaluating
	loss=0.0083, precision=0.3066, recall=0.9090, f1=0.4586
Training epoch 99
	step [1/270], loss=64.0283
	step [2/270], loss=63.2293
	step [3/270], loss=66.1890
	step [4/270], loss=95.2050
	step [5/270], loss=67.2046
	step [6/270], loss=70.4489
	step [7/270], loss=70.3889
	step [8/270], loss=64.7643
	step [9/270], loss=93.3070
	step [10/270], loss=75.3570
	step [11/270], loss=69.5026
	step [12/270], loss=71.7921
	step [13/270], loss=73.5009
	step [14/270], loss=76.0770
	step [15/270], loss=75.4527
	step [16/270], loss=64.5308
	step [17/270], loss=67.2619
	step [18/270], loss=66.1954
	step [19/270], loss=69.0061
	step [20/270], loss=77.7420
	step [21/270], loss=71.1791
	step [22/270], loss=81.7396
	step [23/270], loss=64.3951
	step [24/270], loss=78.5789
	step [25/270], loss=64.6597
	step [26/270], loss=81.1520
	step [27/270], loss=77.0401
	step [28/270], loss=49.1359
	step [29/270], loss=68.5613
	step [30/270], loss=81.3863
	step [31/270], loss=63.1238
	step [32/270], loss=59.2918
	step [33/270], loss=67.2236
	step [34/270], loss=73.5685
	step [35/270], loss=60.8318
	step [36/270], loss=91.1140
	step [37/270], loss=87.4517
	step [38/270], loss=72.0048
	step [39/270], loss=63.8793
	step [40/270], loss=78.9571
	step [41/270], loss=62.1636
	step [42/270], loss=76.9795
	step [43/270], loss=76.8195
	step [44/270], loss=63.4670
	step [45/270], loss=75.1197
	step [46/270], loss=67.5226
	step [47/270], loss=71.0460
	step [48/270], loss=51.0254
	step [49/270], loss=82.9725
	step [50/270], loss=71.5916
	step [51/270], loss=70.5225
	step [52/270], loss=80.0990
	step [53/270], loss=71.7541
	step [54/270], loss=75.7432
	step [55/270], loss=65.3135
	step [56/270], loss=66.6177
	step [57/270], loss=73.7371
	step [58/270], loss=66.6360
	step [59/270], loss=83.8436
	step [60/270], loss=74.6449
	step [61/270], loss=64.2645
	step [62/270], loss=82.0431
	step [63/270], loss=77.3219
	step [64/270], loss=71.6186
	step [65/270], loss=78.9519
	step [66/270], loss=70.5641
	step [67/270], loss=82.0267
	step [68/270], loss=71.1657
	step [69/270], loss=62.7574
	step [70/270], loss=70.3585
	step [71/270], loss=59.4279
	step [72/270], loss=69.2540
	step [73/270], loss=68.3742
	step [74/270], loss=62.8774
	step [75/270], loss=64.6386
	step [76/270], loss=66.1500
	step [77/270], loss=90.1943
	step [78/270], loss=67.6789
	step [79/270], loss=82.8288
	step [80/270], loss=70.1408
	step [81/270], loss=83.3362
	step [82/270], loss=71.6289
	step [83/270], loss=66.1481
	step [84/270], loss=79.7627
	step [85/270], loss=65.0442
	step [86/270], loss=81.2971
	step [87/270], loss=78.6158
	step [88/270], loss=67.1727
	step [89/270], loss=72.5395
	step [90/270], loss=73.9007
	step [91/270], loss=81.2709
	step [92/270], loss=82.6215
	step [93/270], loss=79.0384
	step [94/270], loss=67.2541
	step [95/270], loss=75.8667
	step [96/270], loss=87.2818
	step [97/270], loss=76.0842
	step [98/270], loss=84.1584
	step [99/270], loss=75.1480
	step [100/270], loss=79.2137
	step [101/270], loss=74.5486
	step [102/270], loss=70.5752
	step [103/270], loss=68.4831
	step [104/270], loss=62.3784
	step [105/270], loss=72.5241
	step [106/270], loss=67.1578
	step [107/270], loss=81.0821
	step [108/270], loss=77.8216
	step [109/270], loss=77.8150
	step [110/270], loss=83.3203
	step [111/270], loss=75.6768
	step [112/270], loss=83.8268
	step [113/270], loss=81.2142
	step [114/270], loss=70.6702
	step [115/270], loss=77.5001
	step [116/270], loss=68.7785
	step [117/270], loss=57.2839
	step [118/270], loss=74.1096
	step [119/270], loss=70.5895
	step [120/270], loss=66.0570
	step [121/270], loss=63.3632
	step [122/270], loss=67.5092
	step [123/270], loss=72.9752
	step [124/270], loss=77.9345
	step [125/270], loss=67.9679
	step [126/270], loss=69.6752
	step [127/270], loss=83.8524
	step [128/270], loss=71.5139
	step [129/270], loss=66.2231
	step [130/270], loss=74.1957
	step [131/270], loss=83.8292
	step [132/270], loss=66.9117
	step [133/270], loss=77.9256
	step [134/270], loss=68.6925
	step [135/270], loss=67.4293
	step [136/270], loss=69.8049
	step [137/270], loss=69.0064
	step [138/270], loss=60.3732
	step [139/270], loss=72.0470
	step [140/270], loss=68.8616
	step [141/270], loss=71.5158
	step [142/270], loss=80.2513
	step [143/270], loss=65.0546
	step [144/270], loss=71.1656
	step [145/270], loss=66.9473
	step [146/270], loss=66.0282
	step [147/270], loss=65.1802
	step [148/270], loss=68.7717
	step [149/270], loss=76.4943
	step [150/270], loss=66.2204
	step [151/270], loss=68.5450
	step [152/270], loss=78.5690
	step [153/270], loss=85.0914
	step [154/270], loss=67.6631
	step [155/270], loss=74.9147
	step [156/270], loss=55.7179
	step [157/270], loss=57.5452
	step [158/270], loss=77.2047
	step [159/270], loss=66.9610
	step [160/270], loss=81.3244
	step [161/270], loss=67.2128
	step [162/270], loss=80.3840
	step [163/270], loss=72.0622
	step [164/270], loss=75.6793
	step [165/270], loss=78.4066
	step [166/270], loss=66.2908
	step [167/270], loss=64.7319
	step [168/270], loss=77.1721
	step [169/270], loss=84.3606
	step [170/270], loss=68.9424
	step [171/270], loss=78.9217
	step [172/270], loss=76.0595
	step [173/270], loss=57.3414
	step [174/270], loss=56.8299
	step [175/270], loss=87.2524
	step [176/270], loss=67.6975
	step [177/270], loss=63.2228
	step [178/270], loss=74.6205
	step [179/270], loss=73.7479
	step [180/270], loss=62.4757
	step [181/270], loss=65.5471
	step [182/270], loss=65.3963
	step [183/270], loss=80.4158
	step [184/270], loss=66.2455
	step [185/270], loss=74.7202
	step [186/270], loss=83.3385
	step [187/270], loss=81.5379
	step [188/270], loss=86.4237
	step [189/270], loss=62.9394
	step [190/270], loss=84.5471
	step [191/270], loss=79.4778
	step [192/270], loss=74.8301
	step [193/270], loss=76.1508
	step [194/270], loss=74.9008
	step [195/270], loss=61.5471
	step [196/270], loss=68.3533
	step [197/270], loss=65.4562
	step [198/270], loss=72.2450
	step [199/270], loss=57.6022
	step [200/270], loss=75.8893
	step [201/270], loss=63.8962
	step [202/270], loss=72.5823
	step [203/270], loss=79.9018
	step [204/270], loss=71.7458
	step [205/270], loss=78.0640
	step [206/270], loss=77.5529
	step [207/270], loss=61.8937
	step [208/270], loss=68.6014
	step [209/270], loss=82.5412
	step [210/270], loss=65.5199
	step [211/270], loss=79.4400
	step [212/270], loss=62.9082
	step [213/270], loss=69.8911
	step [214/270], loss=75.2750
	step [215/270], loss=60.0510
	step [216/270], loss=74.7037
	step [217/270], loss=81.8509
	step [218/270], loss=72.7216
	step [219/270], loss=64.8957
	step [220/270], loss=81.7646
	step [221/270], loss=85.9198
	step [222/270], loss=58.3496
	step [223/270], loss=89.4108
	step [224/270], loss=64.9161
	step [225/270], loss=83.9393
	step [226/270], loss=68.6890
	step [227/270], loss=74.5007
	step [228/270], loss=87.2368
	step [229/270], loss=86.9594
	step [230/270], loss=77.3940
	step [231/270], loss=59.8624
	step [232/270], loss=61.6120
	step [233/270], loss=68.8701
	step [234/270], loss=74.5213
	step [235/270], loss=67.8671
	step [236/270], loss=69.2136
	step [237/270], loss=68.4947
	step [238/270], loss=82.4847
	step [239/270], loss=76.1873
	step [240/270], loss=74.9084
	step [241/270], loss=71.8866
	step [242/270], loss=83.6676
	step [243/270], loss=65.4057
	step [244/270], loss=73.0288
	step [245/270], loss=69.3962
	step [246/270], loss=75.4126
	step [247/270], loss=59.1578
	step [248/270], loss=74.7502
	step [249/270], loss=62.3367
	step [250/270], loss=67.1652
	step [251/270], loss=80.3733
	step [252/270], loss=77.4599
	step [253/270], loss=64.2218
	step [254/270], loss=73.2364
	step [255/270], loss=59.2195
	step [256/270], loss=60.6389
	step [257/270], loss=68.9512
	step [258/270], loss=66.2203
	step [259/270], loss=83.8864
	step [260/270], loss=69.2201
	step [261/270], loss=80.6245
	step [262/270], loss=71.0234
	step [263/270], loss=89.9175
	step [264/270], loss=73.3993
	step [265/270], loss=79.3320
	step [266/270], loss=80.2386
	step [267/270], loss=64.0277
	step [268/270], loss=96.0967
	step [269/270], loss=79.0002
	step [270/270], loss=12.8566
	Evaluating
	loss=0.0067, precision=0.3607, recall=0.8931, f1=0.5139
Training epoch 100
	step [1/270], loss=76.2181
	step [2/270], loss=94.2945
	step [3/270], loss=72.1609
	step [4/270], loss=78.8484
	step [5/270], loss=73.1343
	step [6/270], loss=67.3028
	step [7/270], loss=71.0372
	step [8/270], loss=66.9245
	step [9/270], loss=74.3797
	step [10/270], loss=72.0384
	step [11/270], loss=55.4139
	step [12/270], loss=75.2276
	step [13/270], loss=67.5966
	step [14/270], loss=74.9569
	step [15/270], loss=65.0942
	step [16/270], loss=79.5045
	step [17/270], loss=86.3370
	step [18/270], loss=59.5491
	step [19/270], loss=67.5968
	step [20/270], loss=83.8126
	step [21/270], loss=71.8120
	step [22/270], loss=66.3331
	step [23/270], loss=74.5479
	step [24/270], loss=73.4564
	step [25/270], loss=75.4686
	step [26/270], loss=71.2111
	step [27/270], loss=78.8407
	step [28/270], loss=67.9120
	step [29/270], loss=65.6579
	step [30/270], loss=64.5527
	step [31/270], loss=68.6087
	step [32/270], loss=79.7938
	step [33/270], loss=78.3812
	step [34/270], loss=72.3888
	step [35/270], loss=77.4352
	step [36/270], loss=63.0288
	step [37/270], loss=76.6463
	step [38/270], loss=81.1486
	step [39/270], loss=81.2210
	step [40/270], loss=75.2777
	step [41/270], loss=75.8455
	step [42/270], loss=75.7050
	step [43/270], loss=85.0711
	step [44/270], loss=68.0828
	step [45/270], loss=69.9972
	step [46/270], loss=77.1495
	step [47/270], loss=82.3024
	step [48/270], loss=73.0595
	step [49/270], loss=69.6250
	step [50/270], loss=69.5606
	step [51/270], loss=67.7619
	step [52/270], loss=66.7264
	step [53/270], loss=77.2420
	step [54/270], loss=67.0334
	step [55/270], loss=77.5082
	step [56/270], loss=59.5234
	step [57/270], loss=69.2043
	step [58/270], loss=74.6915
	step [59/270], loss=56.9941
	step [60/270], loss=87.1094
	step [61/270], loss=55.7562
	step [62/270], loss=55.0494
	step [63/270], loss=70.4817
	step [64/270], loss=73.3788
	step [65/270], loss=89.8894
	step [66/270], loss=64.0662
	step [67/270], loss=82.0490
	step [68/270], loss=72.7462
	step [69/270], loss=66.0853
	step [70/270], loss=74.1575
	step [71/270], loss=65.4941
	step [72/270], loss=88.0328
	step [73/270], loss=72.5317
	step [74/270], loss=68.6229
	step [75/270], loss=79.6800
	step [76/270], loss=66.7313
	step [77/270], loss=81.9928
	step [78/270], loss=75.9560
	step [79/270], loss=55.3381
	step [80/270], loss=85.1134
	step [81/270], loss=75.8323
	step [82/270], loss=73.7415
	step [83/270], loss=66.4029
	step [84/270], loss=71.3385
	step [85/270], loss=60.1228
	step [86/270], loss=69.6642
	step [87/270], loss=71.9091
	step [88/270], loss=79.4099
	step [89/270], loss=69.6429
	step [90/270], loss=69.0569
	step [91/270], loss=73.1290
	step [92/270], loss=78.3061
	step [93/270], loss=63.7387
	step [94/270], loss=74.9669
	step [95/270], loss=69.9512
	step [96/270], loss=72.3703
	step [97/270], loss=84.0659
	step [98/270], loss=58.8566
	step [99/270], loss=72.1954
	step [100/270], loss=73.5930
	step [101/270], loss=73.4479
	step [102/270], loss=63.7285
	step [103/270], loss=86.9350
	step [104/270], loss=78.1738
	step [105/270], loss=80.0924
	step [106/270], loss=65.2444
	step [107/270], loss=66.3581
	step [108/270], loss=77.5346
	step [109/270], loss=77.1967
	step [110/270], loss=79.2781
	step [111/270], loss=71.3573
	step [112/270], loss=68.8174
	step [113/270], loss=63.5474
	step [114/270], loss=61.3630
	step [115/270], loss=72.9269
	step [116/270], loss=69.5698
	step [117/270], loss=65.5831
	step [118/270], loss=75.7867
	step [119/270], loss=83.0483
	step [120/270], loss=81.7000
	step [121/270], loss=67.2727
	step [122/270], loss=59.8725
	step [123/270], loss=69.5568
	step [124/270], loss=71.1922
	step [125/270], loss=80.6784
	step [126/270], loss=68.0863
	step [127/270], loss=70.2579
	step [128/270], loss=63.0975
	step [129/270], loss=79.4866
	step [130/270], loss=69.5242
	step [131/270], loss=71.8103
	step [132/270], loss=91.3201
	step [133/270], loss=67.2443
	step [134/270], loss=71.4967
	step [135/270], loss=67.8577
	step [136/270], loss=84.1691
	step [137/270], loss=68.7650
	step [138/270], loss=74.2842
	step [139/270], loss=65.4417
	step [140/270], loss=73.1605
	step [141/270], loss=65.1646
	step [142/270], loss=73.2348
	step [143/270], loss=63.9207
	step [144/270], loss=60.3238
	step [145/270], loss=77.3989
	step [146/270], loss=82.3233
	step [147/270], loss=76.5793
	step [148/270], loss=62.1530
	step [149/270], loss=59.2868
	step [150/270], loss=69.2245
	step [151/270], loss=75.1235
	step [152/270], loss=72.4638
	step [153/270], loss=70.2190
	step [154/270], loss=79.1768
	step [155/270], loss=68.0537
	step [156/270], loss=89.8285
	step [157/270], loss=74.5894
	step [158/270], loss=70.6257
	step [159/270], loss=81.1595
	step [160/270], loss=75.1097
	step [161/270], loss=63.5948
	step [162/270], loss=79.4637
	step [163/270], loss=72.2660
	step [164/270], loss=63.9672
	step [165/270], loss=79.8437
	step [166/270], loss=79.4134
	step [167/270], loss=74.8838
	step [168/270], loss=52.6075
	step [169/270], loss=92.8867
	step [170/270], loss=82.4847
	step [171/270], loss=69.5746
	step [172/270], loss=69.5292
	step [173/270], loss=78.9615
	step [174/270], loss=65.9042
	step [175/270], loss=67.3156
	step [176/270], loss=84.0830
	step [177/270], loss=78.1439
	step [178/270], loss=71.6973
	step [179/270], loss=67.8854
	step [180/270], loss=61.2871
	step [181/270], loss=67.0043
	step [182/270], loss=68.6846
	step [183/270], loss=60.2488
	step [184/270], loss=67.7350
	step [185/270], loss=70.2805
	step [186/270], loss=71.7362
	step [187/270], loss=65.1709
	step [188/270], loss=80.3425
	step [189/270], loss=86.0749
	step [190/270], loss=70.3434
	step [191/270], loss=77.6169
	step [192/270], loss=96.7933
	step [193/270], loss=66.1540
	step [194/270], loss=79.3307
	step [195/270], loss=71.9019
	step [196/270], loss=52.2323
	step [197/270], loss=57.4810
	step [198/270], loss=70.9586
	step [199/270], loss=84.1967
	step [200/270], loss=76.7169
	step [201/270], loss=69.3154
	step [202/270], loss=75.4124
	step [203/270], loss=80.1867
	step [204/270], loss=87.4952
	step [205/270], loss=73.4273
	step [206/270], loss=65.5715
	step [207/270], loss=73.0415
	step [208/270], loss=65.1946
	step [209/270], loss=75.2634
	step [210/270], loss=66.4099
	step [211/270], loss=68.0597
	step [212/270], loss=77.5002
	step [213/270], loss=71.5017
	step [214/270], loss=62.3768
	step [215/270], loss=58.1848
	step [216/270], loss=72.3453
	step [217/270], loss=64.7869
	step [218/270], loss=74.8918
	step [219/270], loss=68.8087
	step [220/270], loss=71.0891
	step [221/270], loss=72.8162
	step [222/270], loss=75.2370
	step [223/270], loss=69.4798
	step [224/270], loss=68.8696
	step [225/270], loss=74.1398
	step [226/270], loss=71.5580
	step [227/270], loss=72.3632
	step [228/270], loss=63.5319
	step [229/270], loss=73.1598
	step [230/270], loss=57.5414
	step [231/270], loss=63.2568
	step [232/270], loss=65.3308
	step [233/270], loss=66.5055
	step [234/270], loss=71.8137
	step [235/270], loss=73.6314
	step [236/270], loss=68.1039
	step [237/270], loss=83.3376
	step [238/270], loss=77.6697
	step [239/270], loss=71.9357
	step [240/270], loss=76.8240
	step [241/270], loss=68.9013
	step [242/270], loss=76.9128
	step [243/270], loss=70.4025
	step [244/270], loss=82.5500
	step [245/270], loss=87.7700
	step [246/270], loss=88.8331
	step [247/270], loss=68.5874
	step [248/270], loss=66.6457
	step [249/270], loss=65.7256
	step [250/270], loss=75.1580
	step [251/270], loss=76.1374
	step [252/270], loss=63.4106
	step [253/270], loss=76.2274
	step [254/270], loss=65.8527
	step [255/270], loss=92.9199
	step [256/270], loss=67.1924
	step [257/270], loss=56.3591
	step [258/270], loss=81.0865
	step [259/270], loss=66.3352
	step [260/270], loss=72.9684
	step [261/270], loss=63.7420
	step [262/270], loss=75.6816
	step [263/270], loss=69.8102
	step [264/270], loss=74.1077
	step [265/270], loss=78.1963
	step [266/270], loss=68.9841
	step [267/270], loss=65.6041
	step [268/270], loss=63.3152
	step [269/270], loss=68.4324
	step [270/270], loss=15.6865
	Evaluating
	loss=0.0067, precision=0.3513, recall=0.8911, f1=0.5039
Training epoch 101
	step [1/270], loss=55.1557
	step [2/270], loss=69.2355
	step [3/270], loss=67.9329
	step [4/270], loss=74.4848
	step [5/270], loss=69.3917
	step [6/270], loss=76.3012
	step [7/270], loss=74.7692
	step [8/270], loss=84.1238
	step [9/270], loss=84.2538
	step [10/270], loss=72.1829
	step [11/270], loss=61.9645
	step [12/270], loss=64.7686
	step [13/270], loss=79.6236
	step [14/270], loss=73.8750
	step [15/270], loss=80.6669
	step [16/270], loss=69.1144
	step [17/270], loss=65.6384
	step [18/270], loss=63.3124
	step [19/270], loss=63.4504
	step [20/270], loss=67.4280
	step [21/270], loss=89.6162
	step [22/270], loss=61.4288
	step [23/270], loss=70.9520
	step [24/270], loss=80.2127
	step [25/270], loss=69.8205
	step [26/270], loss=82.0997
	step [27/270], loss=75.2746
	step [28/270], loss=67.3238
	step [29/270], loss=87.5783
	step [30/270], loss=85.7008
	step [31/270], loss=66.5157
	step [32/270], loss=77.1493
	step [33/270], loss=83.4409
	step [34/270], loss=73.1328
	step [35/270], loss=56.7673
	step [36/270], loss=68.1361
	step [37/270], loss=65.0482
	step [38/270], loss=65.1942
	step [39/270], loss=68.4046
	step [40/270], loss=77.3325
	step [41/270], loss=71.3204
	step [42/270], loss=72.1499
	step [43/270], loss=63.5997
	step [44/270], loss=79.4164
	step [45/270], loss=79.6065
	step [46/270], loss=73.5887
	step [47/270], loss=68.6263
	step [48/270], loss=63.6578
	step [49/270], loss=79.8397
	step [50/270], loss=56.1769
	step [51/270], loss=81.0207
	step [52/270], loss=76.5035
	step [53/270], loss=67.1050
	step [54/270], loss=81.1604
	step [55/270], loss=56.0894
	step [56/270], loss=58.9621
	step [57/270], loss=73.7810
	step [58/270], loss=68.2011
	step [59/270], loss=77.8949
	step [60/270], loss=70.7510
	step [61/270], loss=50.5329
	step [62/270], loss=81.5804
	step [63/270], loss=76.4185
	step [64/270], loss=77.0242
	step [65/270], loss=66.4822
	step [66/270], loss=72.3753
	step [67/270], loss=76.4231
	step [68/270], loss=80.1186
	step [69/270], loss=71.0583
	step [70/270], loss=71.6320
	step [71/270], loss=69.8984
	step [72/270], loss=64.3750
	step [73/270], loss=76.7626
	step [74/270], loss=68.8332
	step [75/270], loss=67.7701
	step [76/270], loss=79.5907
	step [77/270], loss=73.9241
	step [78/270], loss=79.5874
	step [79/270], loss=82.2726
	step [80/270], loss=66.8997
	step [81/270], loss=65.7632
	step [82/270], loss=58.8899
	step [83/270], loss=75.8505
	step [84/270], loss=68.2176
	step [85/270], loss=83.6614
	step [86/270], loss=93.0480
	step [87/270], loss=80.5651
	step [88/270], loss=73.0863
	step [89/270], loss=89.2243
	step [90/270], loss=91.8954
	step [91/270], loss=78.5512
	step [92/270], loss=63.1497
	step [93/270], loss=62.8385
	step [94/270], loss=83.3459
	step [95/270], loss=68.9617
	step [96/270], loss=76.4086
	step [97/270], loss=67.5983
	step [98/270], loss=73.7917
	step [99/270], loss=70.3533
	step [100/270], loss=77.6104
	step [101/270], loss=79.6793
	step [102/270], loss=75.8014
	step [103/270], loss=78.1526
	step [104/270], loss=78.2190
	step [105/270], loss=76.0957
	step [106/270], loss=69.8970
	step [107/270], loss=81.0291
	step [108/270], loss=50.9169
	step [109/270], loss=66.5726
	step [110/270], loss=70.2030
	step [111/270], loss=69.2868
	step [112/270], loss=72.6401
	step [113/270], loss=73.6107
	step [114/270], loss=73.7589
	step [115/270], loss=81.0092
	step [116/270], loss=63.6120
	step [117/270], loss=74.5239
	step [118/270], loss=80.9511
	step [119/270], loss=56.7721
	step [120/270], loss=69.2784
	step [121/270], loss=73.9669
	step [122/270], loss=77.9198
	step [123/270], loss=80.9486
	step [124/270], loss=60.8088
	step [125/270], loss=70.0041
	step [126/270], loss=87.4024
	step [127/270], loss=59.2495
	step [128/270], loss=75.0361
	step [129/270], loss=51.5277
	step [130/270], loss=65.1982
	step [131/270], loss=86.6088
	step [132/270], loss=81.7143
	step [133/270], loss=75.2009
	step [134/270], loss=73.2195
	step [135/270], loss=73.8366
	step [136/270], loss=70.9885
	step [137/270], loss=76.2281
	step [138/270], loss=84.7103
	step [139/270], loss=84.4865
	step [140/270], loss=73.3433
	step [141/270], loss=66.6716
	step [142/270], loss=62.5311
	step [143/270], loss=79.0354
	step [144/270], loss=65.9023
	step [145/270], loss=66.4478
	step [146/270], loss=62.5717
	step [147/270], loss=62.7498
	step [148/270], loss=79.1656
	step [149/270], loss=56.1451
	step [150/270], loss=76.6840
	step [151/270], loss=56.4308
	step [152/270], loss=87.3920
	step [153/270], loss=75.8463
	step [154/270], loss=81.1993
	step [155/270], loss=58.4465
	step [156/270], loss=54.0781
	step [157/270], loss=78.6080
	step [158/270], loss=58.6125
	step [159/270], loss=61.1189
	step [160/270], loss=77.5883
	step [161/270], loss=64.5766
	step [162/270], loss=60.3187
	step [163/270], loss=83.2763
	step [164/270], loss=73.4413
	step [165/270], loss=65.7749
	step [166/270], loss=77.2209
	step [167/270], loss=82.4652
	step [168/270], loss=66.8085
	step [169/270], loss=85.4994
	step [170/270], loss=64.4674
	step [171/270], loss=64.1250
	step [172/270], loss=61.7101
	step [173/270], loss=83.9367
	step [174/270], loss=62.3465
	step [175/270], loss=76.9063
	step [176/270], loss=73.5931
	step [177/270], loss=80.3145
	step [178/270], loss=59.7577
	step [179/270], loss=66.1448
	step [180/270], loss=64.5646
	step [181/270], loss=56.2062
	step [182/270], loss=69.5664
	step [183/270], loss=80.3676
	step [184/270], loss=65.4779
	step [185/270], loss=71.2277
	step [186/270], loss=71.3916
	step [187/270], loss=68.3458
	step [188/270], loss=70.0700
	step [189/270], loss=80.3365
	step [190/270], loss=70.7607
	step [191/270], loss=72.0016
	step [192/270], loss=78.4115
	step [193/270], loss=50.2661
	step [194/270], loss=62.1782
	step [195/270], loss=72.8330
	step [196/270], loss=79.6846
	step [197/270], loss=56.0315
	step [198/270], loss=57.8968
	step [199/270], loss=74.0124
	step [200/270], loss=76.6267
	step [201/270], loss=75.7853
	step [202/270], loss=68.9072
	step [203/270], loss=63.4596
	step [204/270], loss=73.5003
	step [205/270], loss=58.6146
	step [206/270], loss=63.0637
	step [207/270], loss=65.5420
	step [208/270], loss=86.1634
	step [209/270], loss=73.5447
	step [210/270], loss=70.6517
	step [211/270], loss=71.6832
	step [212/270], loss=76.6577
	step [213/270], loss=80.7882
	step [214/270], loss=60.0042
	step [215/270], loss=78.6944
	step [216/270], loss=72.0164
	step [217/270], loss=76.3908
	step [218/270], loss=69.8362
	step [219/270], loss=79.5783
	step [220/270], loss=87.8530
	step [221/270], loss=74.1026
	step [222/270], loss=74.0715
	step [223/270], loss=74.5850
	step [224/270], loss=71.5404
	step [225/270], loss=60.9404
	step [226/270], loss=67.8234
	step [227/270], loss=74.4691
	step [228/270], loss=71.7366
	step [229/270], loss=79.1993
	step [230/270], loss=74.5965
	step [231/270], loss=69.0592
	step [232/270], loss=83.1110
	step [233/270], loss=83.9843
	step [234/270], loss=70.8917
	step [235/270], loss=80.7937
	step [236/270], loss=84.7104
	step [237/270], loss=67.6938
	step [238/270], loss=68.0399
	step [239/270], loss=63.4263
	step [240/270], loss=64.5354
	step [241/270], loss=69.3209
	step [242/270], loss=68.6547
	step [243/270], loss=80.9859
	step [244/270], loss=72.7398
	step [245/270], loss=77.3622
	step [246/270], loss=66.2782
	step [247/270], loss=70.9426
	step [248/270], loss=60.8424
	step [249/270], loss=84.6614
	step [250/270], loss=77.3452
	step [251/270], loss=72.5591
	step [252/270], loss=55.4776
	step [253/270], loss=69.7119
	step [254/270], loss=71.7376
	step [255/270], loss=67.0621
	step [256/270], loss=61.6516
	step [257/270], loss=60.6139
	step [258/270], loss=78.0596
	step [259/270], loss=68.3624
	step [260/270], loss=67.6886
	step [261/270], loss=89.4276
	step [262/270], loss=76.1471
	step [263/270], loss=76.8045
	step [264/270], loss=76.2633
	step [265/270], loss=76.1418
	step [266/270], loss=70.6487
	step [267/270], loss=73.5244
	step [268/270], loss=61.2266
	step [269/270], loss=68.5398
	step [270/270], loss=13.9829
	Evaluating
	loss=0.0062, precision=0.3820, recall=0.8829, f1=0.5333
saving model as: 4_saved_model.pth
Training epoch 102
	step [1/270], loss=71.0253
	step [2/270], loss=81.2659
	step [3/270], loss=82.7062
	step [4/270], loss=84.7615
	step [5/270], loss=86.1420
	step [6/270], loss=73.3570
	step [7/270], loss=72.2714
	step [8/270], loss=65.5338
	step [9/270], loss=68.8785
	step [10/270], loss=56.0888
	step [11/270], loss=74.9534
	step [12/270], loss=85.3671
	step [13/270], loss=70.2653
	step [14/270], loss=65.5919
	step [15/270], loss=77.0000
	step [16/270], loss=81.1377
	step [17/270], loss=76.8949
	step [18/270], loss=72.6969
	step [19/270], loss=53.6537
	step [20/270], loss=68.5306
	step [21/270], loss=66.5903
	step [22/270], loss=85.3362
	step [23/270], loss=75.3217
	step [24/270], loss=78.5671
	step [25/270], loss=72.2907
	step [26/270], loss=89.1393
	step [27/270], loss=69.8349
	step [28/270], loss=79.4727
	step [29/270], loss=68.1047
	step [30/270], loss=71.1604
	step [31/270], loss=60.4530
	step [32/270], loss=62.9702
	step [33/270], loss=67.6117
	step [34/270], loss=78.5457
	step [35/270], loss=75.5425
	step [36/270], loss=72.9268
	step [37/270], loss=72.5305
	step [38/270], loss=64.0263
	step [39/270], loss=74.4397
	step [40/270], loss=70.9912
	step [41/270], loss=79.0356
	step [42/270], loss=76.7573
	step [43/270], loss=64.2254
	step [44/270], loss=76.9725
	step [45/270], loss=68.1122
	step [46/270], loss=71.9478
	step [47/270], loss=78.9290
	step [48/270], loss=78.1043
	step [49/270], loss=54.1937
	step [50/270], loss=84.6627
	step [51/270], loss=66.2642
	step [52/270], loss=70.2482
	step [53/270], loss=63.6952
	step [54/270], loss=72.0250
	step [55/270], loss=69.3426
	step [56/270], loss=56.5768
	step [57/270], loss=68.1608
	step [58/270], loss=72.2215
	step [59/270], loss=74.9999
	step [60/270], loss=61.6522
	step [61/270], loss=73.1071
	step [62/270], loss=79.5264
	step [63/270], loss=67.5878
	step [64/270], loss=83.0359
	step [65/270], loss=60.9802
	step [66/270], loss=68.7249
	step [67/270], loss=64.2323
	step [68/270], loss=65.5732
	step [69/270], loss=78.3813
	step [70/270], loss=64.2224
	step [71/270], loss=79.5374
	step [72/270], loss=72.7352
	step [73/270], loss=66.6367
	step [74/270], loss=76.7137
	step [75/270], loss=62.5965
	step [76/270], loss=71.7732
	step [77/270], loss=81.8904
	step [78/270], loss=64.0284
	step [79/270], loss=68.4675
	step [80/270], loss=75.6867
	step [81/270], loss=71.2208
	step [82/270], loss=74.5150
	step [83/270], loss=77.1881
	step [84/270], loss=74.7677
	step [85/270], loss=86.2741
	step [86/270], loss=75.2543
	step [87/270], loss=73.3622
	step [88/270], loss=62.2072
	step [89/270], loss=74.0224
	step [90/270], loss=67.0123
	step [91/270], loss=65.8132
	step [92/270], loss=64.8419
	step [93/270], loss=67.5618
	step [94/270], loss=77.0282
	step [95/270], loss=79.9195
	step [96/270], loss=67.2477
	step [97/270], loss=67.6817
	step [98/270], loss=52.4040
	step [99/270], loss=68.4789
	step [100/270], loss=73.0144
	step [101/270], loss=66.9129
	step [102/270], loss=73.9310
	step [103/270], loss=76.7433
	step [104/270], loss=79.2297
	step [105/270], loss=68.8968
	step [106/270], loss=82.8074
	step [107/270], loss=61.5802
	step [108/270], loss=66.7494
	step [109/270], loss=72.7395
	step [110/270], loss=82.0772
	step [111/270], loss=67.9049
	step [112/270], loss=76.7335
	step [113/270], loss=75.3870
	step [114/270], loss=65.7186
	step [115/270], loss=74.1685
	step [116/270], loss=93.5320
	step [117/270], loss=69.9269
	step [118/270], loss=64.4681
	step [119/270], loss=70.4378
	step [120/270], loss=63.4424
	step [121/270], loss=70.4834
	step [122/270], loss=78.3754
	step [123/270], loss=64.9903
	step [124/270], loss=66.6971
	step [125/270], loss=71.1308
	step [126/270], loss=79.7146
	step [127/270], loss=60.0562
	step [128/270], loss=58.0843
	step [129/270], loss=62.2731
	step [130/270], loss=73.7048
	step [131/270], loss=77.8227
	step [132/270], loss=83.9015
	step [133/270], loss=82.7115
	step [134/270], loss=66.4269
	step [135/270], loss=62.2389
	step [136/270], loss=61.8822
	step [137/270], loss=74.2933
	step [138/270], loss=62.7019
	step [139/270], loss=70.4964
	step [140/270], loss=82.7180
	step [141/270], loss=70.3270
	step [142/270], loss=78.4754
	step [143/270], loss=90.0756
	step [144/270], loss=77.3308
	step [145/270], loss=65.6159
	step [146/270], loss=61.6136
	step [147/270], loss=62.8265
	step [148/270], loss=88.1976
	step [149/270], loss=65.1862
	step [150/270], loss=69.2606
	step [151/270], loss=62.2331
	step [152/270], loss=64.0252
	step [153/270], loss=68.8523
	step [154/270], loss=63.4325
	step [155/270], loss=83.7288
	step [156/270], loss=90.2357
	step [157/270], loss=67.4357
	step [158/270], loss=57.0261
	step [159/270], loss=61.6371
	step [160/270], loss=68.2533
	step [161/270], loss=72.8137
	step [162/270], loss=72.6695
	step [163/270], loss=88.5604
	step [164/270], loss=74.8526
	step [165/270], loss=71.6192
	step [166/270], loss=67.5814
	step [167/270], loss=65.6614
	step [168/270], loss=64.7210
	step [169/270], loss=68.3111
	step [170/270], loss=61.0930
	step [171/270], loss=74.0293
	step [172/270], loss=60.0154
	step [173/270], loss=74.8337
	step [174/270], loss=60.1652
	step [175/270], loss=70.5350
	step [176/270], loss=69.1710
	step [177/270], loss=56.2022
	step [178/270], loss=76.8963
	step [179/270], loss=85.4559
	step [180/270], loss=67.9667
	step [181/270], loss=76.9656
	step [182/270], loss=69.3553
	step [183/270], loss=84.4831
	step [184/270], loss=72.1822
	step [185/270], loss=65.7530
	step [186/270], loss=81.5336
	step [187/270], loss=70.5480
	step [188/270], loss=94.0400
	step [189/270], loss=70.0581
	step [190/270], loss=76.8343
	step [191/270], loss=73.6406
	step [192/270], loss=77.7884
	step [193/270], loss=79.4796
	step [194/270], loss=63.8145
	step [195/270], loss=74.7827
	step [196/270], loss=75.1572
	step [197/270], loss=67.0685
	step [198/270], loss=70.4605
	step [199/270], loss=79.9225
	step [200/270], loss=75.7786
	step [201/270], loss=68.5876
	step [202/270], loss=53.8176
	step [203/270], loss=79.7500
	step [204/270], loss=75.4811
	step [205/270], loss=63.6085
	step [206/270], loss=62.8725
	step [207/270], loss=70.7363
	step [208/270], loss=65.6671
	step [209/270], loss=77.9522
	step [210/270], loss=64.7169
	step [211/270], loss=74.9179
	step [212/270], loss=75.5194
	step [213/270], loss=51.2983
	step [214/270], loss=71.8124
	step [215/270], loss=67.8196
	step [216/270], loss=74.1273
	step [217/270], loss=71.0205
	step [218/270], loss=77.9122
	step [219/270], loss=58.4681
	step [220/270], loss=75.0272
	step [221/270], loss=64.0990
	step [222/270], loss=84.6233
	step [223/270], loss=74.6920
	step [224/270], loss=66.5813
	step [225/270], loss=71.8512
	step [226/270], loss=69.8698
	step [227/270], loss=78.5409
	step [228/270], loss=72.5368
	step [229/270], loss=63.4499
	step [230/270], loss=73.7554
	step [231/270], loss=72.8491
	step [232/270], loss=65.9242
	step [233/270], loss=70.8272
	step [234/270], loss=69.2490
	step [235/270], loss=89.4231
	step [236/270], loss=70.6189
	step [237/270], loss=69.4517
	step [238/270], loss=63.0169
	step [239/270], loss=83.2186
	step [240/270], loss=66.4928
	step [241/270], loss=82.1096
	step [242/270], loss=67.9878
	step [243/270], loss=75.5397
	step [244/270], loss=70.4055
	step [245/270], loss=73.9786
	step [246/270], loss=72.3294
	step [247/270], loss=80.5260
	step [248/270], loss=81.9515
	step [249/270], loss=61.8652
	step [250/270], loss=73.5033
	step [251/270], loss=71.1090
	step [252/270], loss=72.3930
	step [253/270], loss=78.6926
	step [254/270], loss=86.8706
	step [255/270], loss=84.9227
	step [256/270], loss=77.3656
	step [257/270], loss=68.6093
	step [258/270], loss=70.2610
	step [259/270], loss=68.5452
	step [260/270], loss=71.0557
	step [261/270], loss=78.6659
	step [262/270], loss=80.9854
	step [263/270], loss=57.9924
	step [264/270], loss=64.4386
	step [265/270], loss=58.0488
	step [266/270], loss=72.0026
	step [267/270], loss=63.8263
	step [268/270], loss=69.5294
	step [269/270], loss=71.0162
	step [270/270], loss=8.2168
	Evaluating
	loss=0.0063, precision=0.3724, recall=0.8966, f1=0.5263
Training epoch 103
	step [1/270], loss=61.7437
	step [2/270], loss=71.7375
	step [3/270], loss=74.8257
	step [4/270], loss=63.4925
	step [5/270], loss=57.8566
	step [6/270], loss=61.9402
	step [7/270], loss=61.9922
	step [8/270], loss=61.1019
	step [9/270], loss=58.9899
	step [10/270], loss=78.8017
	step [11/270], loss=73.2407
	step [12/270], loss=79.3677
	step [13/270], loss=62.5710
	step [14/270], loss=73.1961
	step [15/270], loss=75.5289
	step [16/270], loss=89.3967
	step [17/270], loss=78.1671
	step [18/270], loss=71.6138
	step [19/270], loss=73.2828
	step [20/270], loss=69.2911
	step [21/270], loss=84.8875
	step [22/270], loss=63.7247
	step [23/270], loss=69.5329
	step [24/270], loss=60.7464
	step [25/270], loss=76.3938
	step [26/270], loss=79.9481
	step [27/270], loss=59.5841
	step [28/270], loss=64.8589
	step [29/270], loss=82.7585
	step [30/270], loss=74.7908
	step [31/270], loss=65.8723
	step [32/270], loss=72.5318
	step [33/270], loss=69.3311
	step [34/270], loss=84.2485
	step [35/270], loss=71.0653
	step [36/270], loss=66.7738
	step [37/270], loss=67.4185
	step [38/270], loss=59.5406
	step [39/270], loss=63.1994
	step [40/270], loss=83.5328
	step [41/270], loss=77.4402
	step [42/270], loss=66.3164
	step [43/270], loss=61.9433
	step [44/270], loss=65.6064
	step [45/270], loss=77.3898
	step [46/270], loss=79.5010
	step [47/270], loss=69.7492
	step [48/270], loss=71.1316
	step [49/270], loss=70.3335
	step [50/270], loss=70.9034
	step [51/270], loss=66.1241
	step [52/270], loss=74.7664
	step [53/270], loss=70.8017
	step [54/270], loss=75.9410
	step [55/270], loss=72.1738
	step [56/270], loss=71.2734
	step [57/270], loss=71.2713
	step [58/270], loss=72.3474
	step [59/270], loss=63.9438
	step [60/270], loss=75.0381
	step [61/270], loss=71.5488
	step [62/270], loss=62.6654
	step [63/270], loss=68.3358
	step [64/270], loss=60.1689
	step [65/270], loss=76.8864
	step [66/270], loss=77.5389
	step [67/270], loss=69.8796
	step [68/270], loss=51.4810
	step [69/270], loss=87.2884
	step [70/270], loss=87.5951
	step [71/270], loss=78.9818
	step [72/270], loss=80.2628
	step [73/270], loss=81.3740
	step [74/270], loss=78.4435
	step [75/270], loss=70.7984
	step [76/270], loss=71.5086
	step [77/270], loss=60.6870
	step [78/270], loss=58.1523
	step [79/270], loss=70.8828
	step [80/270], loss=75.7766
	step [81/270], loss=76.5554
	step [82/270], loss=70.1777
	step [83/270], loss=64.5054
	step [84/270], loss=66.9888
	step [85/270], loss=75.8055
	step [86/270], loss=70.4681
	step [87/270], loss=77.0174
	step [88/270], loss=65.9185
	step [89/270], loss=71.4284
	step [90/270], loss=68.0138
	step [91/270], loss=78.6289
	step [92/270], loss=66.2777
	step [93/270], loss=70.9431
	step [94/270], loss=69.7982
	step [95/270], loss=81.0178
	step [96/270], loss=69.4329
	step [97/270], loss=82.7845
	step [98/270], loss=76.9322
	step [99/270], loss=70.5665
	step [100/270], loss=72.3216
	step [101/270], loss=73.7000
	step [102/270], loss=72.5755
	step [103/270], loss=62.1440
	step [104/270], loss=80.2090
	step [105/270], loss=68.0479
	step [106/270], loss=69.5802
	step [107/270], loss=67.8886
	step [108/270], loss=76.8820
	step [109/270], loss=69.7505
	step [110/270], loss=78.4800
	step [111/270], loss=57.4155
	step [112/270], loss=69.5645
	step [113/270], loss=68.2951
	step [114/270], loss=76.3079
	step [115/270], loss=60.3946
	step [116/270], loss=80.8703
	step [117/270], loss=74.5502
	step [118/270], loss=73.3148
	step [119/270], loss=60.2676
	step [120/270], loss=71.5168
	step [121/270], loss=75.6846
	step [122/270], loss=67.7440
	step [123/270], loss=60.2575
	step [124/270], loss=81.1287
	step [125/270], loss=90.1388
	step [126/270], loss=66.5007
	step [127/270], loss=66.6786
	step [128/270], loss=64.4283
	step [129/270], loss=67.5688
	step [130/270], loss=74.0077
	step [131/270], loss=63.9602
	step [132/270], loss=74.5614
	step [133/270], loss=77.1406
	step [134/270], loss=85.1584
	step [135/270], loss=66.3260
	step [136/270], loss=74.5788
	step [137/270], loss=82.7946
	step [138/270], loss=63.4128
	step [139/270], loss=58.7924
	step [140/270], loss=67.2706
	step [141/270], loss=59.6789
	step [142/270], loss=64.8399
	step [143/270], loss=69.1486
	step [144/270], loss=83.2142
	step [145/270], loss=66.1165
	step [146/270], loss=75.7446
	step [147/270], loss=83.7170
	step [148/270], loss=78.1142
	step [149/270], loss=79.2998
	step [150/270], loss=58.9092
	step [151/270], loss=58.4186
	step [152/270], loss=72.5097
	step [153/270], loss=70.8071
	step [154/270], loss=71.9345
	step [155/270], loss=72.7966
	step [156/270], loss=92.6231
	step [157/270], loss=69.2163
	step [158/270], loss=85.0897
	step [159/270], loss=62.6458
	step [160/270], loss=65.0249
	step [161/270], loss=74.5034
	step [162/270], loss=88.2160
	step [163/270], loss=72.4341
	step [164/270], loss=76.9656
	step [165/270], loss=62.7580
	step [166/270], loss=65.6579
	step [167/270], loss=66.3694
	step [168/270], loss=60.3356
	step [169/270], loss=79.0916
	step [170/270], loss=65.0845
	step [171/270], loss=67.1753
	step [172/270], loss=60.0871
	step [173/270], loss=60.7272
	step [174/270], loss=72.8023
	step [175/270], loss=79.6536
	step [176/270], loss=81.0409
	step [177/270], loss=67.0583
	step [178/270], loss=61.4686
	step [179/270], loss=69.3482
	step [180/270], loss=61.5854
	step [181/270], loss=66.5384
	step [182/270], loss=61.4751
	step [183/270], loss=69.1806
	step [184/270], loss=71.6135
	step [185/270], loss=68.4645
	step [186/270], loss=69.6338
	step [187/270], loss=71.5303
	step [188/270], loss=85.8804
	step [189/270], loss=72.8366
	step [190/270], loss=64.6319
	step [191/270], loss=57.9723
	step [192/270], loss=63.1611
	step [193/270], loss=76.5182
	step [194/270], loss=62.4143
	step [195/270], loss=78.1843
	step [196/270], loss=74.3160
	step [197/270], loss=63.2555
	step [198/270], loss=77.5858
	step [199/270], loss=66.1604
	step [200/270], loss=62.8619
	step [201/270], loss=76.8078
	step [202/270], loss=79.6479
	step [203/270], loss=80.5964
	step [204/270], loss=82.8710
	step [205/270], loss=80.5484
	step [206/270], loss=60.7784
	step [207/270], loss=66.3625
	step [208/270], loss=81.9457
	step [209/270], loss=76.1693
	step [210/270], loss=72.6842
	step [211/270], loss=71.9605
	step [212/270], loss=93.3810
	step [213/270], loss=51.1977
	step [214/270], loss=65.4563
	step [215/270], loss=80.6033
	step [216/270], loss=74.4951
	step [217/270], loss=69.9436
	step [218/270], loss=67.9845
	step [219/270], loss=67.9672
	step [220/270], loss=74.2737
	step [221/270], loss=69.2759
	step [222/270], loss=70.3431
	step [223/270], loss=80.3012
	step [224/270], loss=65.1170
	step [225/270], loss=59.5676
	step [226/270], loss=71.0558
	step [227/270], loss=78.3440
	step [228/270], loss=66.0835
	step [229/270], loss=83.3221
	step [230/270], loss=74.6908
	step [231/270], loss=93.0923
	step [232/270], loss=68.8343
	step [233/270], loss=80.8242
	step [234/270], loss=65.2465
	step [235/270], loss=79.8200
	step [236/270], loss=73.7447
	step [237/270], loss=67.9153
	step [238/270], loss=62.5914
	step [239/270], loss=71.5824
	step [240/270], loss=76.0627
	step [241/270], loss=74.8091
	step [242/270], loss=64.8909
	step [243/270], loss=79.3902
	step [244/270], loss=71.1216
	step [245/270], loss=69.3368
	step [246/270], loss=75.3990
	step [247/270], loss=68.1402
	step [248/270], loss=67.8922
	step [249/270], loss=60.1238
	step [250/270], loss=68.3411
	step [251/270], loss=81.5311
	step [252/270], loss=76.5427
	step [253/270], loss=81.7811
	step [254/270], loss=77.7462
	step [255/270], loss=66.2166
	step [256/270], loss=81.3969
	step [257/270], loss=83.9558
	step [258/270], loss=75.0189
	step [259/270], loss=75.8183
	step [260/270], loss=60.5369
	step [261/270], loss=93.3313
	step [262/270], loss=76.4925
	step [263/270], loss=63.9177
	step [264/270], loss=72.5710
	step [265/270], loss=55.2730
	step [266/270], loss=67.7386
	step [267/270], loss=75.9184
	step [268/270], loss=89.6155
	step [269/270], loss=64.2612
	step [270/270], loss=11.3441
	Evaluating
	loss=0.0065, precision=0.3683, recall=0.8954, f1=0.5219
Training epoch 104
	step [1/270], loss=70.4200
	step [2/270], loss=75.4246
	step [3/270], loss=69.1797
	step [4/270], loss=67.8831
	step [5/270], loss=64.0625
	step [6/270], loss=71.6794
	step [7/270], loss=70.9024
	step [8/270], loss=80.9284
	step [9/270], loss=71.2883
	step [10/270], loss=63.1131
	step [11/270], loss=73.9427
	step [12/270], loss=84.4751
	step [13/270], loss=78.4222
	step [14/270], loss=66.8197
	step [15/270], loss=66.5536
	step [16/270], loss=68.6758
	step [17/270], loss=76.7733
	step [18/270], loss=76.5841
	step [19/270], loss=67.9547
	step [20/270], loss=59.3233
	step [21/270], loss=74.7097
	step [22/270], loss=70.0229
	step [23/270], loss=64.1540
	step [24/270], loss=62.4933
	step [25/270], loss=75.8448
	step [26/270], loss=78.6752
	step [27/270], loss=75.9845
	step [28/270], loss=66.3290
	step [29/270], loss=60.4642
	step [30/270], loss=71.4989
	step [31/270], loss=74.5301
	step [32/270], loss=83.1508
	step [33/270], loss=66.8297
	step [34/270], loss=78.2639
	step [35/270], loss=65.0380
	step [36/270], loss=78.5553
	step [37/270], loss=77.9079
	step [38/270], loss=63.2048
	step [39/270], loss=83.8492
	step [40/270], loss=68.6445
	step [41/270], loss=74.4213
	step [42/270], loss=71.9139
	step [43/270], loss=77.4585
	step [44/270], loss=83.8165
	step [45/270], loss=65.7664
	step [46/270], loss=74.1941
	step [47/270], loss=77.6910
	step [48/270], loss=73.0983
	step [49/270], loss=76.5556
	step [50/270], loss=75.6638
	step [51/270], loss=72.1375
	step [52/270], loss=57.1673
	step [53/270], loss=62.1421
	step [54/270], loss=60.8268
	step [55/270], loss=65.5922
	step [56/270], loss=64.3796
	step [57/270], loss=63.9449
	step [58/270], loss=75.3960
	step [59/270], loss=59.6784
	step [60/270], loss=48.3663
	step [61/270], loss=71.0544
	step [62/270], loss=61.5008
	step [63/270], loss=65.2114
	step [64/270], loss=72.9328
	step [65/270], loss=65.9417
	step [66/270], loss=75.2843
	step [67/270], loss=79.3673
	step [68/270], loss=77.5455
	step [69/270], loss=79.4944
	step [70/270], loss=70.5120
	step [71/270], loss=66.9307
	step [72/270], loss=73.6828
	step [73/270], loss=65.7197
	step [74/270], loss=61.4421
	step [75/270], loss=75.3750
	step [76/270], loss=74.2852
	step [77/270], loss=80.8176
	step [78/270], loss=78.8382
	step [79/270], loss=69.1757
	step [80/270], loss=67.1649
	step [81/270], loss=55.9185
	step [82/270], loss=73.4604
	step [83/270], loss=72.7557
	step [84/270], loss=78.3851
	step [85/270], loss=59.4696
	step [86/270], loss=70.4389
	step [87/270], loss=77.7883
	step [88/270], loss=62.0077
	step [89/270], loss=75.0531
	step [90/270], loss=69.5020
	step [91/270], loss=64.3458
	step [92/270], loss=53.9078
	step [93/270], loss=54.7081
	step [94/270], loss=77.0688
	step [95/270], loss=67.1639
	step [96/270], loss=71.0077
	step [97/270], loss=72.6244
	step [98/270], loss=71.8231
	step [99/270], loss=63.7197
	step [100/270], loss=65.8020
	step [101/270], loss=60.3257
	step [102/270], loss=80.4000
	step [103/270], loss=78.0684
	step [104/270], loss=73.7005
	step [105/270], loss=70.5186
	step [106/270], loss=55.4793
	step [107/270], loss=77.1976
	step [108/270], loss=71.6940
	step [109/270], loss=59.3611
	step [110/270], loss=71.7886
	step [111/270], loss=74.8336
	step [112/270], loss=80.4984
	step [113/270], loss=70.7366
	step [114/270], loss=72.4692
	step [115/270], loss=77.2145
	step [116/270], loss=70.6703
	step [117/270], loss=68.4757
	step [118/270], loss=64.8027
	step [119/270], loss=84.3730
	step [120/270], loss=81.4912
	step [121/270], loss=78.7975
	step [122/270], loss=75.2044
	step [123/270], loss=74.0061
	step [124/270], loss=59.9800
	step [125/270], loss=58.5866
	step [126/270], loss=64.0515
	step [127/270], loss=91.7431
	step [128/270], loss=49.6870
	step [129/270], loss=56.1466
	step [130/270], loss=80.1473
	step [131/270], loss=75.3707
	step [132/270], loss=72.6027
	step [133/270], loss=69.5157
	step [134/270], loss=76.8282
	step [135/270], loss=82.5906
	step [136/270], loss=74.1687
	step [137/270], loss=76.2843
	step [138/270], loss=67.6851
	step [139/270], loss=66.9228
	step [140/270], loss=56.4221
	step [141/270], loss=76.2109
	step [142/270], loss=67.2039
	step [143/270], loss=76.5686
	step [144/270], loss=86.5322
	step [145/270], loss=62.4098
	step [146/270], loss=87.0353
	step [147/270], loss=68.4455
	step [148/270], loss=74.2548
	step [149/270], loss=83.7195
	step [150/270], loss=88.2132
	step [151/270], loss=74.0720
	step [152/270], loss=79.0926
	step [153/270], loss=67.7404
	step [154/270], loss=71.9369
	step [155/270], loss=75.0590
	step [156/270], loss=84.1594
	step [157/270], loss=83.5074
	step [158/270], loss=65.7235
	step [159/270], loss=61.1452
	step [160/270], loss=67.7024
	step [161/270], loss=79.9116
	step [162/270], loss=46.7732
	step [163/270], loss=68.6620
	step [164/270], loss=70.5357
	step [165/270], loss=69.3040
	step [166/270], loss=72.9723
	step [167/270], loss=53.0184
	step [168/270], loss=91.9538
	step [169/270], loss=76.4350
	step [170/270], loss=73.2043
	step [171/270], loss=63.2513
	step [172/270], loss=84.0270
	step [173/270], loss=83.6558
	step [174/270], loss=83.8731
	step [175/270], loss=62.2449
	step [176/270], loss=82.7253
	step [177/270], loss=69.4674
	step [178/270], loss=82.8709
	step [179/270], loss=90.4827
	step [180/270], loss=83.0846
	step [181/270], loss=74.5474
	step [182/270], loss=77.6464
	step [183/270], loss=63.0414
	step [184/270], loss=66.6175
	step [185/270], loss=73.2845
	step [186/270], loss=57.9034
	step [187/270], loss=69.3158
	step [188/270], loss=70.9619
	step [189/270], loss=68.3584
	step [190/270], loss=66.2746
	step [191/270], loss=90.3358
	step [192/270], loss=76.2050
	step [193/270], loss=70.2901
	step [194/270], loss=60.9176
	step [195/270], loss=81.8679
	step [196/270], loss=71.5331
	step [197/270], loss=53.2000
	step [198/270], loss=73.9521
	step [199/270], loss=79.3044
	step [200/270], loss=80.0072
	step [201/270], loss=72.1981
	step [202/270], loss=73.8187
	step [203/270], loss=77.4519
	step [204/270], loss=65.0315
	step [205/270], loss=74.3947
	step [206/270], loss=61.7071
	step [207/270], loss=69.3264
	step [208/270], loss=71.6044
	step [209/270], loss=64.3266
	step [210/270], loss=84.7997
	step [211/270], loss=68.8824
	step [212/270], loss=60.2306
	step [213/270], loss=68.6206
	step [214/270], loss=80.5659
	step [215/270], loss=69.0657
	step [216/270], loss=74.5828
	step [217/270], loss=71.4405
	step [218/270], loss=69.7847
	step [219/270], loss=74.3867
	step [220/270], loss=77.7574
	step [221/270], loss=72.4989
	step [222/270], loss=76.3425
	step [223/270], loss=73.2331
	step [224/270], loss=88.9518
	step [225/270], loss=72.6476
	step [226/270], loss=73.4398
	step [227/270], loss=67.5323
	step [228/270], loss=72.1753
	step [229/270], loss=69.9239
	step [230/270], loss=68.4279
	step [231/270], loss=63.3635
	step [232/270], loss=61.2698
	step [233/270], loss=73.5830
	step [234/270], loss=73.0970
	step [235/270], loss=68.7751
	step [236/270], loss=59.0223
	step [237/270], loss=69.8958
	step [238/270], loss=62.6002
	step [239/270], loss=73.0924
	step [240/270], loss=67.6679
	step [241/270], loss=63.5784
	step [242/270], loss=78.4948
	step [243/270], loss=59.0515
	step [244/270], loss=71.6411
	step [245/270], loss=74.2576
	step [246/270], loss=69.9454
	step [247/270], loss=68.8485
	step [248/270], loss=78.3418
	step [249/270], loss=68.0609
	step [250/270], loss=59.4518
	step [251/270], loss=65.8662
	step [252/270], loss=70.8849
	step [253/270], loss=76.4396
	step [254/270], loss=49.1879
	step [255/270], loss=81.1292
	step [256/270], loss=64.2013
	step [257/270], loss=72.2865
	step [258/270], loss=77.7134
	step [259/270], loss=85.9683
	step [260/270], loss=81.0426
	step [261/270], loss=78.1200
	step [262/270], loss=69.6137
	step [263/270], loss=69.4365
	step [264/270], loss=63.8798
	step [265/270], loss=61.1768
	step [266/270], loss=84.8561
	step [267/270], loss=73.9407
	step [268/270], loss=74.2431
	step [269/270], loss=64.4981
	step [270/270], loss=13.2411
	Evaluating
	loss=0.0062, precision=0.3834, recall=0.8959, f1=0.5370
saving model as: 4_saved_model.pth
Training epoch 105
	step [1/270], loss=70.9203
	step [2/270], loss=58.1185
	step [3/270], loss=79.5199
	step [4/270], loss=64.4994
	step [5/270], loss=77.8255
	step [6/270], loss=78.9532
	step [7/270], loss=70.0090
	step [8/270], loss=53.8091
	step [9/270], loss=73.7140
	step [10/270], loss=80.4468
	step [11/270], loss=80.3545
	step [12/270], loss=78.1966
	step [13/270], loss=72.4441
	step [14/270], loss=69.3259
	step [15/270], loss=71.2812
	step [16/270], loss=80.0146
	step [17/270], loss=72.7694
	step [18/270], loss=73.8433
	step [19/270], loss=60.5314
	step [20/270], loss=69.0085
	step [21/270], loss=67.7929
	step [22/270], loss=74.8380
	step [23/270], loss=67.6969
	step [24/270], loss=71.5625
	step [25/270], loss=75.5983
	step [26/270], loss=82.8156
	step [27/270], loss=68.3828
	step [28/270], loss=74.3765
	step [29/270], loss=61.3812
	step [30/270], loss=78.7361
	step [31/270], loss=73.9531
	step [32/270], loss=62.8886
	step [33/270], loss=69.5564
	step [34/270], loss=73.7907
	step [35/270], loss=78.8319
	step [36/270], loss=74.3518
	step [37/270], loss=75.7212
	step [38/270], loss=70.0856
	step [39/270], loss=67.4945
	step [40/270], loss=80.1768
	step [41/270], loss=56.3400
	step [42/270], loss=63.5357
	step [43/270], loss=71.5072
	step [44/270], loss=66.0952
	step [45/270], loss=70.0226
	step [46/270], loss=68.5380
	step [47/270], loss=65.5950
	step [48/270], loss=71.9631
	step [49/270], loss=61.1615
	step [50/270], loss=78.8843
	step [51/270], loss=79.4989
	step [52/270], loss=67.7414
	step [53/270], loss=68.6808
	step [54/270], loss=73.7545
	step [55/270], loss=63.4215
	step [56/270], loss=57.1640
	step [57/270], loss=67.9075
	step [58/270], loss=75.4222
	step [59/270], loss=56.9907
	step [60/270], loss=69.3008
	step [61/270], loss=77.8205
	step [62/270], loss=88.5594
	step [63/270], loss=67.7933
	step [64/270], loss=65.2350
	step [65/270], loss=60.8224
	step [66/270], loss=79.3303
	step [67/270], loss=70.7649
	step [68/270], loss=64.9724
	step [69/270], loss=56.1567
	step [70/270], loss=79.0917
	step [71/270], loss=78.2626
	step [72/270], loss=81.3663
	step [73/270], loss=72.7828
	step [74/270], loss=61.2249
	step [75/270], loss=73.8245
	step [76/270], loss=70.5383
	step [77/270], loss=72.3560
	step [78/270], loss=63.1297
	step [79/270], loss=63.3901
	step [80/270], loss=91.4743
	step [81/270], loss=59.3811
	step [82/270], loss=68.8031
	step [83/270], loss=68.2361
	step [84/270], loss=66.8799
	step [85/270], loss=59.3101
	step [86/270], loss=76.0255
	step [87/270], loss=77.8125
	step [88/270], loss=88.1891
	step [89/270], loss=77.3553
	step [90/270], loss=72.6001
	step [91/270], loss=79.4804
	step [92/270], loss=68.7156
	step [93/270], loss=70.7149
	step [94/270], loss=66.5427
	step [95/270], loss=70.7603
	step [96/270], loss=73.7005
	step [97/270], loss=74.5829
	step [98/270], loss=79.1905
	step [99/270], loss=76.3925
	step [100/270], loss=74.6987
	step [101/270], loss=59.0794
	step [102/270], loss=71.0172
	step [103/270], loss=64.9793
	step [104/270], loss=69.0778
	step [105/270], loss=66.5510
	step [106/270], loss=79.1341
	step [107/270], loss=70.0599
	step [108/270], loss=61.8202
	step [109/270], loss=77.5294
	step [110/270], loss=89.9115
	step [111/270], loss=62.6416
	step [112/270], loss=65.1120
	step [113/270], loss=63.3861
	step [114/270], loss=56.9023
	step [115/270], loss=76.8940
	step [116/270], loss=87.3385
	step [117/270], loss=57.1573
	step [118/270], loss=75.2849
	step [119/270], loss=58.2898
	step [120/270], loss=77.6846
	step [121/270], loss=66.5737
	step [122/270], loss=62.7083
	step [123/270], loss=59.2399
	step [124/270], loss=68.5291
	step [125/270], loss=69.9677
	step [126/270], loss=70.0569
	step [127/270], loss=83.2168
	step [128/270], loss=69.1025
	step [129/270], loss=61.9459
	step [130/270], loss=81.7629
	step [131/270], loss=82.2322
	step [132/270], loss=71.1875
	step [133/270], loss=70.0241
	step [134/270], loss=69.6393
	step [135/270], loss=60.3250
	step [136/270], loss=71.5664
	step [137/270], loss=74.3431
	step [138/270], loss=57.2425
	step [139/270], loss=69.9913
	step [140/270], loss=65.1277
	step [141/270], loss=66.3988
	step [142/270], loss=72.1098
	step [143/270], loss=76.2651
	step [144/270], loss=79.8549
	step [145/270], loss=72.3118
	step [146/270], loss=73.2588
	step [147/270], loss=64.1967
	step [148/270], loss=75.6478
	step [149/270], loss=71.5974
	step [150/270], loss=82.1572
	step [151/270], loss=72.8464
	step [152/270], loss=67.2559
	step [153/270], loss=73.6532
	step [154/270], loss=58.1753
	step [155/270], loss=71.4543
	step [156/270], loss=77.1967
	step [157/270], loss=74.5961
	step [158/270], loss=72.7994
	step [159/270], loss=78.8445
	step [160/270], loss=69.2804
	step [161/270], loss=66.9899
	step [162/270], loss=71.5182
	step [163/270], loss=56.7220
	step [164/270], loss=76.8735
	step [165/270], loss=63.4515
	step [166/270], loss=77.5894
	step [167/270], loss=67.6574
	step [168/270], loss=83.3014
	step [169/270], loss=72.1855
	step [170/270], loss=80.0227
	step [171/270], loss=86.4726
	step [172/270], loss=65.0208
	step [173/270], loss=78.6401
	step [174/270], loss=77.3888
	step [175/270], loss=66.6609
	step [176/270], loss=69.7451
	step [177/270], loss=75.0252
	step [178/270], loss=67.7394
	step [179/270], loss=69.5718
	step [180/270], loss=99.8298
	step [181/270], loss=67.6094
	step [182/270], loss=59.1579
	step [183/270], loss=68.4237
	step [184/270], loss=62.0817
	step [185/270], loss=71.7340
	step [186/270], loss=66.9995
	step [187/270], loss=70.2656
	step [188/270], loss=70.8474
	step [189/270], loss=75.3361
	step [190/270], loss=68.5817
	step [191/270], loss=71.9007
	step [192/270], loss=67.1977
	step [193/270], loss=61.8781
	step [194/270], loss=64.3348
	step [195/270], loss=76.3903
	step [196/270], loss=63.1206
	step [197/270], loss=73.3806
	step [198/270], loss=65.9101
	step [199/270], loss=55.6441
	step [200/270], loss=77.3250
	step [201/270], loss=70.3655
	step [202/270], loss=76.4335
	step [203/270], loss=68.9031
	step [204/270], loss=83.2113
	step [205/270], loss=65.9298
	step [206/270], loss=69.2304
	step [207/270], loss=72.9990
	step [208/270], loss=79.3322
	step [209/270], loss=65.1103
	step [210/270], loss=60.9690
	step [211/270], loss=77.0468
	step [212/270], loss=63.8837
	step [213/270], loss=75.9899
	step [214/270], loss=69.3210
	step [215/270], loss=75.9957
	step [216/270], loss=79.0267
	step [217/270], loss=65.3534
	step [218/270], loss=66.2491
	step [219/270], loss=64.3487
	step [220/270], loss=78.0529
	step [221/270], loss=64.4420
	step [222/270], loss=75.6886
	step [223/270], loss=75.6333
	step [224/270], loss=67.1260
	step [225/270], loss=86.3747
	step [226/270], loss=70.0991
	step [227/270], loss=65.9133
	step [228/270], loss=80.9031
	step [229/270], loss=67.9265
	step [230/270], loss=67.2449
	step [231/270], loss=68.3569
	step [232/270], loss=61.3989
	step [233/270], loss=77.5496
	step [234/270], loss=86.8437
	step [235/270], loss=75.2295
	step [236/270], loss=76.4983
	step [237/270], loss=89.2465
	step [238/270], loss=66.9191
	step [239/270], loss=81.0765
	step [240/270], loss=69.2984
	step [241/270], loss=58.6246
	step [242/270], loss=71.0859
	step [243/270], loss=69.2644
	step [244/270], loss=65.6100
	step [245/270], loss=82.1820
	step [246/270], loss=52.7633
	step [247/270], loss=83.3380
	step [248/270], loss=70.2422
	step [249/270], loss=70.6667
	step [250/270], loss=70.1183
	step [251/270], loss=80.9809
	step [252/270], loss=74.3031
	step [253/270], loss=75.1963
	step [254/270], loss=71.1111
	step [255/270], loss=74.9641
	step [256/270], loss=78.2613
	step [257/270], loss=74.4105
	step [258/270], loss=63.6711
	step [259/270], loss=74.1784
	step [260/270], loss=65.9702
	step [261/270], loss=64.1263
	step [262/270], loss=85.1499
	step [263/270], loss=68.8965
	step [264/270], loss=59.6760
	step [265/270], loss=68.9904
	step [266/270], loss=76.4834
	step [267/270], loss=58.1909
	step [268/270], loss=68.2014
	step [269/270], loss=70.8876
	step [270/270], loss=10.5717
	Evaluating
	loss=0.0063, precision=0.3756, recall=0.8992, f1=0.5299
Training finished
best_f1: 0.5369575555212504
directing: Z rim_enhanced: True test_id 0
removed wrong scan: weights_Z_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_278_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_276_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_298_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_210_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_262_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_254_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_58_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_293_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_291_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_206_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_252_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_265_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_295_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_221_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_292_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_294_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_297_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_214_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_296_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_225_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_300_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_243_xwqg-A00121_2019-05-15.npy
# all image files: 16489 # all weight files in weight_dir: 12281 # image files with weight 12281
removed wrong scan: weights_Z_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_278_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_276_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_298_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_210_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_262_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_254_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_58_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_293_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_291_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_206_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_252_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_265_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_295_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_221_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_292_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_294_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_297_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_214_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_296_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_225_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_300_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_243_xwqg-A00121_2019-05-15.npy
# all image files: 16489 # all weight files in weight_dir: 3263 # image files with weight 3263
train: /ibex/scratch/projects/c2052/air_tube_seg/training_samples/Z 12281
Using 4 GPUs
Going to train epochs [52-101]
Training epoch 52
	step [1/192], loss=77.0372
	step [2/192], loss=89.4738
	step [3/192], loss=91.8273
	step [4/192], loss=86.1547
	step [5/192], loss=85.9966
	step [6/192], loss=68.0369
	step [7/192], loss=88.7979
	step [8/192], loss=75.2040
	step [9/192], loss=68.1574
	step [10/192], loss=76.0085
	step [11/192], loss=87.6131
	step [12/192], loss=72.9193
	step [13/192], loss=84.6055
	step [14/192], loss=84.1410
	step [15/192], loss=71.6931
	step [16/192], loss=76.5449
	step [17/192], loss=76.7741
	step [18/192], loss=81.4298
	step [19/192], loss=80.7289
	step [20/192], loss=84.3735
	step [21/192], loss=78.3774
	step [22/192], loss=68.2389
	step [23/192], loss=78.8655
	step [24/192], loss=65.9555
	step [25/192], loss=68.9218
	step [26/192], loss=72.4945
	step [27/192], loss=82.4031
	step [28/192], loss=87.4779
	step [29/192], loss=85.6667
	step [30/192], loss=83.1074
	step [31/192], loss=70.8569
	step [32/192], loss=78.8920
	step [33/192], loss=71.3910
	step [34/192], loss=73.2061
	step [35/192], loss=75.5806
	step [36/192], loss=67.5248
	step [37/192], loss=83.6362
	step [38/192], loss=66.9361
	step [39/192], loss=88.6466
	step [40/192], loss=87.8954
	step [41/192], loss=67.0397
	step [42/192], loss=73.1858
	step [43/192], loss=90.5290
	step [44/192], loss=78.2340
	step [45/192], loss=77.3744
	step [46/192], loss=95.4494
	step [47/192], loss=93.6344
	step [48/192], loss=80.4748
	step [49/192], loss=76.4699
	step [50/192], loss=94.7113
	step [51/192], loss=84.5292
	step [52/192], loss=98.3556
	step [53/192], loss=72.3290
	step [54/192], loss=71.3201
	step [55/192], loss=83.7654
	step [56/192], loss=87.3763
	step [57/192], loss=77.2238
	step [58/192], loss=84.2041
	step [59/192], loss=69.2282
	step [60/192], loss=67.1155
	step [61/192], loss=69.3734
	step [62/192], loss=75.5766
	step [63/192], loss=79.5155
	step [64/192], loss=73.4381
	step [65/192], loss=78.4928
	step [66/192], loss=94.5867
	step [67/192], loss=68.9157
	step [68/192], loss=75.5450
	step [69/192], loss=85.6068
	step [70/192], loss=73.7419
	step [71/192], loss=82.3585
	step [72/192], loss=71.6655
	step [73/192], loss=80.2590
	step [74/192], loss=75.0467
	step [75/192], loss=71.4122
	step [76/192], loss=72.5713
	step [77/192], loss=81.4020
	step [78/192], loss=83.3063
	step [79/192], loss=85.7772
	step [80/192], loss=85.5732
	step [81/192], loss=83.3341
	step [82/192], loss=79.0116
	step [83/192], loss=71.3589
	step [84/192], loss=87.3157
	step [85/192], loss=89.7841
	step [86/192], loss=67.0284
	step [87/192], loss=83.4198
	step [88/192], loss=74.7833
	step [89/192], loss=86.0886
	step [90/192], loss=72.5348
	step [91/192], loss=72.1561
	step [92/192], loss=93.1663
	step [93/192], loss=84.5005
	step [94/192], loss=78.6797
	step [95/192], loss=81.3191
	step [96/192], loss=68.8798
	step [97/192], loss=82.1854
	step [98/192], loss=71.6940
	step [99/192], loss=80.5745
	step [100/192], loss=79.0571
	step [101/192], loss=88.1739
	step [102/192], loss=75.7041
	step [103/192], loss=62.9438
	step [104/192], loss=82.1791
	step [105/192], loss=94.9326
	step [106/192], loss=79.5109
	step [107/192], loss=92.6078
	step [108/192], loss=90.0175
	step [109/192], loss=87.1068
	step [110/192], loss=76.9246
	step [111/192], loss=79.7578
	step [112/192], loss=77.0951
	step [113/192], loss=82.7299
	step [114/192], loss=78.4308
	step [115/192], loss=99.4787
	step [116/192], loss=78.6468
	step [117/192], loss=94.0561
	step [118/192], loss=70.0138
	step [119/192], loss=79.8187
	step [120/192], loss=79.7499
	step [121/192], loss=81.6072
	step [122/192], loss=84.0426
	step [123/192], loss=70.9367
	step [124/192], loss=80.8295
	step [125/192], loss=74.0674
	step [126/192], loss=83.9288
	step [127/192], loss=68.6338
	step [128/192], loss=90.4568
	step [129/192], loss=91.8941
	step [130/192], loss=74.0706
	step [131/192], loss=82.3871
	step [132/192], loss=80.2449
	step [133/192], loss=76.0542
	step [134/192], loss=78.2075
	step [135/192], loss=87.4162
	step [136/192], loss=83.1944
	step [137/192], loss=76.9993
	step [138/192], loss=91.3330
	step [139/192], loss=84.0486
	step [140/192], loss=82.1719
	step [141/192], loss=71.7285
	step [142/192], loss=82.4477
	step [143/192], loss=82.0219
	step [144/192], loss=82.2332
	step [145/192], loss=74.5415
	step [146/192], loss=68.0590
	step [147/192], loss=82.1372
	step [148/192], loss=72.8005
	step [149/192], loss=89.9238
	step [150/192], loss=76.2367
	step [151/192], loss=72.3214
	step [152/192], loss=69.0545
	step [153/192], loss=70.4115
	step [154/192], loss=85.5713
	step [155/192], loss=81.2952
	step [156/192], loss=78.5796
	step [157/192], loss=84.3964
	step [158/192], loss=79.2233
	step [159/192], loss=71.5304
	step [160/192], loss=82.4165
	step [161/192], loss=83.1223
	step [162/192], loss=79.6214
	step [163/192], loss=83.3024
	step [164/192], loss=85.4980
	step [165/192], loss=88.2665
	step [166/192], loss=68.8948
	step [167/192], loss=79.1030
	step [168/192], loss=74.3855
	step [169/192], loss=61.7221
	step [170/192], loss=88.4392
	step [171/192], loss=70.9687
	step [172/192], loss=88.3902
	step [173/192], loss=76.6454
	step [174/192], loss=83.3439
	step [175/192], loss=73.3367
	step [176/192], loss=71.3366
	step [177/192], loss=78.4019
	step [178/192], loss=101.0517
	step [179/192], loss=83.0805
	step [180/192], loss=70.8950
	step [181/192], loss=83.0814
	step [182/192], loss=79.8333
	step [183/192], loss=69.8451
	step [184/192], loss=69.0728
	step [185/192], loss=77.7585
	step [186/192], loss=86.7667
	step [187/192], loss=87.8539
	step [188/192], loss=73.8067
	step [189/192], loss=80.7174
	step [190/192], loss=75.6015
	step [191/192], loss=84.0161
	step [192/192], loss=74.1027
	Evaluating
	loss=0.0107, precision=0.2928, recall=0.8862, f1=0.4401
saving model as: 0_saved_model.pth
Training epoch 53
	step [1/192], loss=77.2212
	step [2/192], loss=86.2520
	step [3/192], loss=71.8436
	step [4/192], loss=71.6040
	step [5/192], loss=71.6987
	step [6/192], loss=75.9794
	step [7/192], loss=79.2922
	step [8/192], loss=94.7417
	step [9/192], loss=82.7312
	step [10/192], loss=72.6343
	step [11/192], loss=72.6685
	step [12/192], loss=81.1963
	step [13/192], loss=84.9019
	step [14/192], loss=99.6340
	step [15/192], loss=78.2671
	step [16/192], loss=80.8756
	step [17/192], loss=59.1101
	step [18/192], loss=70.3914
	step [19/192], loss=75.5441
	step [20/192], loss=88.9386
	step [21/192], loss=76.6796
	step [22/192], loss=74.3358
	step [23/192], loss=88.0009
	step [24/192], loss=66.4070
	step [25/192], loss=74.4388
	step [26/192], loss=83.8690
	step [27/192], loss=65.8236
	step [28/192], loss=83.2039
	step [29/192], loss=81.8912
	step [30/192], loss=74.4556
	step [31/192], loss=76.9329
	step [32/192], loss=78.4105
	step [33/192], loss=62.8062
	step [34/192], loss=87.2166
	step [35/192], loss=78.9610
	step [36/192], loss=78.8863
	step [37/192], loss=80.2537
	step [38/192], loss=59.7396
	step [39/192], loss=68.3958
	step [40/192], loss=77.7607
	step [41/192], loss=81.2303
	step [42/192], loss=80.0658
	step [43/192], loss=79.5020
	step [44/192], loss=86.4363
	step [45/192], loss=70.5850
	step [46/192], loss=68.2760
	step [47/192], loss=95.4577
	step [48/192], loss=71.9716
	step [49/192], loss=73.0210
	step [50/192], loss=83.3121
	step [51/192], loss=69.0361
	step [52/192], loss=86.9848
	step [53/192], loss=87.0881
	step [54/192], loss=78.5544
	step [55/192], loss=91.0945
	step [56/192], loss=74.6078
	step [57/192], loss=70.9016
	step [58/192], loss=64.7166
	step [59/192], loss=80.6362
	step [60/192], loss=79.4368
	step [61/192], loss=69.0683
	step [62/192], loss=84.8022
	step [63/192], loss=75.4886
	step [64/192], loss=65.5395
	step [65/192], loss=67.5100
	step [66/192], loss=85.0374
	step [67/192], loss=70.4106
	step [68/192], loss=78.9439
	step [69/192], loss=77.3853
	step [70/192], loss=80.4418
	step [71/192], loss=80.4818
	step [72/192], loss=77.8557
	step [73/192], loss=73.3179
	step [74/192], loss=74.3356
	step [75/192], loss=85.7865
	step [76/192], loss=83.6666
	step [77/192], loss=77.8846
	step [78/192], loss=74.9270
	step [79/192], loss=79.5464
	step [80/192], loss=82.4906
	step [81/192], loss=65.3089
	step [82/192], loss=86.6829
	step [83/192], loss=72.9158
	step [84/192], loss=83.9233
	step [85/192], loss=83.3448
	step [86/192], loss=85.4512
	step [87/192], loss=77.3819
	step [88/192], loss=93.6250
	step [89/192], loss=75.6481
	step [90/192], loss=59.2965
	step [91/192], loss=76.5753
	step [92/192], loss=72.0961
	step [93/192], loss=80.5375
	step [94/192], loss=74.8447
	step [95/192], loss=77.1150
	step [96/192], loss=87.0664
	step [97/192], loss=74.3204
	step [98/192], loss=80.9364
	step [99/192], loss=86.1758
	step [100/192], loss=86.9278
	step [101/192], loss=79.7838
	step [102/192], loss=86.7148
	step [103/192], loss=82.1304
	step [104/192], loss=81.6420
	step [105/192], loss=86.1672
	step [106/192], loss=91.4735
	step [107/192], loss=76.9331
	step [108/192], loss=86.2028
	step [109/192], loss=81.6436
	step [110/192], loss=87.0543
	step [111/192], loss=78.4678
	step [112/192], loss=87.8874
	step [113/192], loss=80.7898
	step [114/192], loss=85.8265
	step [115/192], loss=75.1711
	step [116/192], loss=87.3481
	step [117/192], loss=78.6746
	step [118/192], loss=79.0297
	step [119/192], loss=68.4631
	step [120/192], loss=89.0242
	step [121/192], loss=80.3945
	step [122/192], loss=70.5432
	step [123/192], loss=72.8998
	step [124/192], loss=72.7522
	step [125/192], loss=81.0793
	step [126/192], loss=82.5628
	step [127/192], loss=85.8451
	step [128/192], loss=85.0744
	step [129/192], loss=63.0775
	step [130/192], loss=82.2418
	step [131/192], loss=97.8424
	step [132/192], loss=59.0336
	step [133/192], loss=68.1844
	step [134/192], loss=75.9768
	step [135/192], loss=76.7188
	step [136/192], loss=72.7910
	step [137/192], loss=92.1346
	step [138/192], loss=74.9767
	step [139/192], loss=81.6844
	step [140/192], loss=102.8589
	step [141/192], loss=86.5123
	step [142/192], loss=84.1769
	step [143/192], loss=81.1421
	step [144/192], loss=82.3544
	step [145/192], loss=77.2564
	step [146/192], loss=89.8401
	step [147/192], loss=83.9335
	step [148/192], loss=65.7735
	step [149/192], loss=66.8740
	step [150/192], loss=76.6110
	step [151/192], loss=94.4902
	step [152/192], loss=79.9248
	step [153/192], loss=74.6629
	step [154/192], loss=82.3065
	step [155/192], loss=71.1874
	step [156/192], loss=66.6147
	step [157/192], loss=99.2148
	step [158/192], loss=86.4410
	step [159/192], loss=76.3240
	step [160/192], loss=86.5911
	step [161/192], loss=82.9919
	step [162/192], loss=77.4550
	step [163/192], loss=70.6963
	step [164/192], loss=74.2256
	step [165/192], loss=80.6265
	step [166/192], loss=86.4915
	step [167/192], loss=64.0416
	step [168/192], loss=70.7813
	step [169/192], loss=74.8926
	step [170/192], loss=76.2198
	step [171/192], loss=78.5273
	step [172/192], loss=92.5625
	step [173/192], loss=78.6938
	step [174/192], loss=91.9649
	step [175/192], loss=77.9435
	step [176/192], loss=77.8889
	step [177/192], loss=84.3738
	step [178/192], loss=86.0147
	step [179/192], loss=99.9007
	step [180/192], loss=82.0068
	step [181/192], loss=77.3654
	step [182/192], loss=76.0166
	step [183/192], loss=93.2010
	step [184/192], loss=64.7178
	step [185/192], loss=78.2526
	step [186/192], loss=88.7673
	step [187/192], loss=67.7376
	step [188/192], loss=79.6749
	step [189/192], loss=76.8005
	step [190/192], loss=70.6873
	step [191/192], loss=74.5057
	step [192/192], loss=65.4291
	Evaluating
	loss=0.0094, precision=0.3424, recall=0.8704, f1=0.4915
saving model as: 0_saved_model.pth
Training epoch 54
	step [1/192], loss=72.1938
	step [2/192], loss=80.3109
	step [3/192], loss=68.0721
	step [4/192], loss=81.0745
	step [5/192], loss=79.0029
	step [6/192], loss=82.6456
	step [7/192], loss=72.7323
	step [8/192], loss=92.1884
	step [9/192], loss=70.1487
	step [10/192], loss=69.2121
	step [11/192], loss=82.5371
	step [12/192], loss=94.9232
	step [13/192], loss=78.4630
	step [14/192], loss=67.4814
	step [15/192], loss=84.2902
	step [16/192], loss=86.1635
	step [17/192], loss=84.1262
	step [18/192], loss=81.1831
	step [19/192], loss=70.2712
	step [20/192], loss=89.8720
	step [21/192], loss=88.0120
	step [22/192], loss=85.0141
	step [23/192], loss=80.1941
	step [24/192], loss=72.4048
	step [25/192], loss=90.3111
	step [26/192], loss=78.6289
	step [27/192], loss=80.8327
	step [28/192], loss=72.1968
	step [29/192], loss=83.9611
	step [30/192], loss=86.6477
	step [31/192], loss=76.8111
	step [32/192], loss=91.6960
	step [33/192], loss=76.8421
	step [34/192], loss=89.2123
	step [35/192], loss=85.6725
	step [36/192], loss=85.6904
	step [37/192], loss=80.2489
	step [38/192], loss=83.5229
	step [39/192], loss=87.4147
	step [40/192], loss=76.7128
	step [41/192], loss=68.3035
	step [42/192], loss=78.0853
	step [43/192], loss=79.3153
	step [44/192], loss=76.1638
	step [45/192], loss=75.8611
	step [46/192], loss=76.2106
	step [47/192], loss=80.1212
	step [48/192], loss=86.7516
	step [49/192], loss=79.1592
	step [50/192], loss=74.1525
	step [51/192], loss=77.6858
	step [52/192], loss=81.8106
	step [53/192], loss=69.7409
	step [54/192], loss=75.7998
	step [55/192], loss=67.7411
	step [56/192], loss=72.9845
	step [57/192], loss=72.1437
	step [58/192], loss=90.8568
	step [59/192], loss=75.3347
	step [60/192], loss=81.1649
	step [61/192], loss=87.7430
	step [62/192], loss=75.2811
	step [63/192], loss=76.2457
	step [64/192], loss=69.7431
	step [65/192], loss=71.9128
	step [66/192], loss=94.3594
	step [67/192], loss=83.1289
	step [68/192], loss=75.7416
	step [69/192], loss=82.4461
	step [70/192], loss=76.2959
	step [71/192], loss=83.0038
	step [72/192], loss=86.8980
	step [73/192], loss=86.5500
	step [74/192], loss=77.2899
	step [75/192], loss=75.2891
	step [76/192], loss=79.1100
	step [77/192], loss=90.1164
	step [78/192], loss=73.5364
	step [79/192], loss=80.5181
	step [80/192], loss=72.3708
	step [81/192], loss=73.2078
	step [82/192], loss=73.7803
	step [83/192], loss=76.1723
	step [84/192], loss=67.9535
	step [85/192], loss=85.2581
	step [86/192], loss=73.1225
	step [87/192], loss=74.0812
	step [88/192], loss=77.1835
	step [89/192], loss=97.5455
	step [90/192], loss=75.8997
	step [91/192], loss=85.9403
	step [92/192], loss=74.5073
	step [93/192], loss=79.9311
	step [94/192], loss=70.6172
	step [95/192], loss=89.2062
	step [96/192], loss=84.1148
	step [97/192], loss=77.4835
	step [98/192], loss=67.1471
	step [99/192], loss=77.9093
	step [100/192], loss=68.7216
	step [101/192], loss=81.1474
	step [102/192], loss=95.0771
	step [103/192], loss=74.1460
	step [104/192], loss=54.9386
	step [105/192], loss=86.6009
	step [106/192], loss=67.1747
	step [107/192], loss=89.9886
	step [108/192], loss=66.4336
	step [109/192], loss=89.4776
	step [110/192], loss=70.6143
	step [111/192], loss=68.8955
	step [112/192], loss=89.6669
	step [113/192], loss=67.5855
	step [114/192], loss=85.6526
	step [115/192], loss=77.8650
	step [116/192], loss=77.5389
	step [117/192], loss=81.5201
	step [118/192], loss=89.8239
	step [119/192], loss=69.7882
	step [120/192], loss=85.4284
	step [121/192], loss=85.4271
	step [122/192], loss=90.2756
	step [123/192], loss=76.6145
	step [124/192], loss=79.5541
	step [125/192], loss=72.3823
	step [126/192], loss=76.9278
	step [127/192], loss=83.7304
	step [128/192], loss=78.1895
	step [129/192], loss=78.3256
	step [130/192], loss=76.4110
	step [131/192], loss=84.7893
	step [132/192], loss=86.7945
	step [133/192], loss=90.7281
	step [134/192], loss=85.2845
	step [135/192], loss=78.4954
	step [136/192], loss=89.4915
	step [137/192], loss=76.0811
	step [138/192], loss=76.6851
	step [139/192], loss=80.5937
	step [140/192], loss=61.1373
	step [141/192], loss=80.3594
	step [142/192], loss=74.2974
	step [143/192], loss=88.8388
	step [144/192], loss=79.5070
	step [145/192], loss=77.7739
	step [146/192], loss=73.4335
	step [147/192], loss=87.4796
	step [148/192], loss=77.7253
	step [149/192], loss=73.9989
	step [150/192], loss=72.9535
	step [151/192], loss=78.0938
	step [152/192], loss=75.7530
	step [153/192], loss=70.6247
	step [154/192], loss=69.0546
	step [155/192], loss=85.3823
	step [156/192], loss=71.0059
	step [157/192], loss=78.5314
	step [158/192], loss=81.8594
	step [159/192], loss=78.2954
	step [160/192], loss=83.0979
	step [161/192], loss=73.9420
	step [162/192], loss=79.2690
	step [163/192], loss=80.8502
	step [164/192], loss=89.7637
	step [165/192], loss=87.0175
	step [166/192], loss=67.9821
	step [167/192], loss=73.6773
	step [168/192], loss=71.9633
	step [169/192], loss=75.8849
	step [170/192], loss=77.5941
	step [171/192], loss=74.9367
	step [172/192], loss=74.2035
	step [173/192], loss=80.5883
	step [174/192], loss=78.0663
	step [175/192], loss=91.5000
	step [176/192], loss=84.5013
	step [177/192], loss=77.9665
	step [178/192], loss=70.5507
	step [179/192], loss=70.6691
	step [180/192], loss=78.9937
	step [181/192], loss=74.9132
	step [182/192], loss=78.6286
	step [183/192], loss=81.1319
	step [184/192], loss=64.7581
	step [185/192], loss=75.8537
	step [186/192], loss=89.0002
	step [187/192], loss=76.5130
	step [188/192], loss=74.6440
	step [189/192], loss=76.8145
	step [190/192], loss=79.6803
	step [191/192], loss=68.3546
	step [192/192], loss=66.4427
	Evaluating
	loss=0.0114, precision=0.2764, recall=0.8742, f1=0.4200
Training epoch 55
	step [1/192], loss=74.4667
	step [2/192], loss=73.6582
	step [3/192], loss=81.8397
	step [4/192], loss=80.7113
	step [5/192], loss=89.1413
	step [6/192], loss=83.0201
	step [7/192], loss=82.9939
	step [8/192], loss=68.7756
	step [9/192], loss=86.7052
	step [10/192], loss=84.7913
	step [11/192], loss=72.8930
	step [12/192], loss=83.9107
	step [13/192], loss=78.2448
	step [14/192], loss=77.1344
	step [15/192], loss=78.0046
	step [16/192], loss=75.5256
	step [17/192], loss=81.3448
	step [18/192], loss=74.6435
	step [19/192], loss=88.4633
	step [20/192], loss=70.5294
	step [21/192], loss=82.5025
	step [22/192], loss=79.4568
	step [23/192], loss=82.8333
	step [24/192], loss=88.1034
	step [25/192], loss=75.8795
	step [26/192], loss=99.4023
	step [27/192], loss=86.4439
	step [28/192], loss=89.4320
	step [29/192], loss=70.7347
	step [30/192], loss=81.1819
	step [31/192], loss=91.6030
	step [32/192], loss=86.7458
	step [33/192], loss=76.3100
	step [34/192], loss=77.5398
	step [35/192], loss=87.4795
	step [36/192], loss=78.1559
	step [37/192], loss=87.9001
	step [38/192], loss=87.2409
	step [39/192], loss=85.1776
	step [40/192], loss=76.4192
	step [41/192], loss=71.9940
	step [42/192], loss=72.0888
	step [43/192], loss=73.0686
	step [44/192], loss=70.1561
	step [45/192], loss=80.9754
	step [46/192], loss=86.4540
	step [47/192], loss=88.9252
	step [48/192], loss=77.2753
	step [49/192], loss=86.5571
	step [50/192], loss=65.7454
	step [51/192], loss=75.9342
	step [52/192], loss=72.6106
	step [53/192], loss=77.4393
	step [54/192], loss=71.8514
	step [55/192], loss=70.3120
	step [56/192], loss=82.8821
	step [57/192], loss=84.8155
	step [58/192], loss=79.7313
	step [59/192], loss=73.4240
	step [60/192], loss=86.8334
	step [61/192], loss=90.0512
	step [62/192], loss=84.7356
	step [63/192], loss=71.5700
	step [64/192], loss=87.2073
	step [65/192], loss=93.7654
	step [66/192], loss=76.6295
	step [67/192], loss=80.7090
	step [68/192], loss=73.0032
	step [69/192], loss=76.0993
	step [70/192], loss=86.8610
	step [71/192], loss=76.2559
	step [72/192], loss=69.4529
	step [73/192], loss=63.5084
	step [74/192], loss=81.4084
	step [75/192], loss=69.0609
	step [76/192], loss=82.6617
	step [77/192], loss=78.6609
	step [78/192], loss=75.3624
	step [79/192], loss=74.3974
	step [80/192], loss=78.7862
	step [81/192], loss=75.8941
	step [82/192], loss=55.9104
	step [83/192], loss=88.0600
	step [84/192], loss=74.8594
	step [85/192], loss=75.6846
	step [86/192], loss=85.2253
	step [87/192], loss=77.1819
	step [88/192], loss=77.6780
	step [89/192], loss=66.6434
	step [90/192], loss=85.2411
	step [91/192], loss=70.2881
	step [92/192], loss=72.0207
	step [93/192], loss=81.4937
	step [94/192], loss=91.2699
	step [95/192], loss=77.5078
	step [96/192], loss=82.6100
	step [97/192], loss=90.0762
	step [98/192], loss=83.6531
	step [99/192], loss=73.7668
	step [100/192], loss=81.9284
	step [101/192], loss=83.6551
	step [102/192], loss=77.8017
	step [103/192], loss=80.8125
	step [104/192], loss=76.2263
	step [105/192], loss=83.8947
	step [106/192], loss=76.8426
	step [107/192], loss=79.1989
	step [108/192], loss=68.7503
	step [109/192], loss=84.0547
	step [110/192], loss=75.6987
	step [111/192], loss=74.3475
	step [112/192], loss=78.2907
	step [113/192], loss=60.6369
	step [114/192], loss=73.5657
	step [115/192], loss=94.7082
	step [116/192], loss=76.3848
	step [117/192], loss=73.6175
	step [118/192], loss=85.5913
	step [119/192], loss=81.6222
	step [120/192], loss=74.1321
	step [121/192], loss=73.1138
	step [122/192], loss=79.7612
	step [123/192], loss=77.6330
	step [124/192], loss=77.9403
	step [125/192], loss=60.3119
	step [126/192], loss=68.5230
	step [127/192], loss=91.7491
	step [128/192], loss=73.0093
	step [129/192], loss=73.4722
	step [130/192], loss=82.0754
	step [131/192], loss=84.1614
	step [132/192], loss=84.3273
	step [133/192], loss=80.0250
	step [134/192], loss=81.1807
	step [135/192], loss=83.1088
	step [136/192], loss=67.0847
	step [137/192], loss=87.7744
	step [138/192], loss=77.3916
	step [139/192], loss=76.4807
	step [140/192], loss=86.7576
	step [141/192], loss=89.5061
	step [142/192], loss=75.6332
	step [143/192], loss=77.5252
	step [144/192], loss=81.0153
	step [145/192], loss=86.3396
	step [146/192], loss=82.5569
	step [147/192], loss=75.2419
	step [148/192], loss=78.7195
	step [149/192], loss=72.8658
	step [150/192], loss=82.7758
	step [151/192], loss=85.6079
	step [152/192], loss=78.9624
	step [153/192], loss=69.7293
	step [154/192], loss=73.8546
	step [155/192], loss=78.4628
	step [156/192], loss=68.5358
	step [157/192], loss=69.1268
	step [158/192], loss=93.9250
	step [159/192], loss=90.9477
	step [160/192], loss=77.2381
	step [161/192], loss=66.2077
	step [162/192], loss=76.8502
	step [163/192], loss=85.4421
	step [164/192], loss=86.0231
	step [165/192], loss=70.3996
	step [166/192], loss=67.0586
	step [167/192], loss=92.8156
	step [168/192], loss=75.9861
	step [169/192], loss=92.7471
	step [170/192], loss=80.5956
	step [171/192], loss=88.1376
	step [172/192], loss=59.7203
	step [173/192], loss=92.3128
	step [174/192], loss=73.9537
	step [175/192], loss=76.9003
	step [176/192], loss=74.9779
	step [177/192], loss=65.9640
	step [178/192], loss=76.6289
	step [179/192], loss=65.1333
	step [180/192], loss=81.4834
	step [181/192], loss=75.6191
	step [182/192], loss=70.0986
	step [183/192], loss=73.2619
	step [184/192], loss=71.4079
	step [185/192], loss=74.2374
	step [186/192], loss=71.6343
	step [187/192], loss=76.8652
	step [188/192], loss=85.9072
	step [189/192], loss=64.4856
	step [190/192], loss=59.6768
	step [191/192], loss=62.2097
	step [192/192], loss=63.8748
	Evaluating
	loss=0.0100, precision=0.3065, recall=0.8687, f1=0.4531
Training epoch 56
	step [1/192], loss=68.0859
	step [2/192], loss=68.2109
	step [3/192], loss=88.5642
	step [4/192], loss=67.5134
	step [5/192], loss=72.9295
	step [6/192], loss=66.5985
	step [7/192], loss=85.4731
	step [8/192], loss=82.9962
	step [9/192], loss=72.9834
	step [10/192], loss=77.8865
	step [11/192], loss=74.1119
	step [12/192], loss=70.2766
	step [13/192], loss=92.6515
	step [14/192], loss=65.5906
	step [15/192], loss=72.1184
	step [16/192], loss=74.8374
	step [17/192], loss=74.5089
	step [18/192], loss=81.7698
	step [19/192], loss=75.5234
	step [20/192], loss=85.2376
	step [21/192], loss=74.9865
	step [22/192], loss=79.8409
	step [23/192], loss=70.2838
	step [24/192], loss=73.2036
	step [25/192], loss=75.2543
	step [26/192], loss=76.6936
	step [27/192], loss=76.1985
	step [28/192], loss=92.4123
	step [29/192], loss=84.6643
	step [30/192], loss=74.5444
	step [31/192], loss=88.6353
	step [32/192], loss=85.4974
	step [33/192], loss=74.0966
	step [34/192], loss=67.1581
	step [35/192], loss=61.0160
	step [36/192], loss=86.5581
	step [37/192], loss=67.5531
	step [38/192], loss=68.1663
	step [39/192], loss=77.1631
	step [40/192], loss=72.7451
	step [41/192], loss=81.2127
	step [42/192], loss=83.6070
	step [43/192], loss=83.2409
	step [44/192], loss=72.9999
	step [45/192], loss=66.3489
	step [46/192], loss=77.1550
	step [47/192], loss=89.7117
	step [48/192], loss=71.7221
	step [49/192], loss=79.5894
	step [50/192], loss=76.7988
	step [51/192], loss=83.9047
	step [52/192], loss=81.2454
	step [53/192], loss=72.6107
	step [54/192], loss=85.3643
	step [55/192], loss=80.1294
	step [56/192], loss=78.2403
	step [57/192], loss=64.6048
	step [58/192], loss=75.0709
	step [59/192], loss=65.7202
	step [60/192], loss=72.2041
	step [61/192], loss=73.0493
	step [62/192], loss=83.1801
	step [63/192], loss=71.6176
	step [64/192], loss=67.2460
	step [65/192], loss=71.6516
	step [66/192], loss=76.9904
	step [67/192], loss=63.3134
	step [68/192], loss=57.3702
	step [69/192], loss=80.7670
	step [70/192], loss=82.7585
	step [71/192], loss=67.2229
	step [72/192], loss=77.5313
	step [73/192], loss=70.0520
	step [74/192], loss=61.1292
	step [75/192], loss=85.4924
	step [76/192], loss=86.7192
	step [77/192], loss=83.2879
	step [78/192], loss=87.6802
	step [79/192], loss=72.2514
	step [80/192], loss=87.0566
	step [81/192], loss=77.1396
	step [82/192], loss=82.2506
	step [83/192], loss=91.6391
	step [84/192], loss=65.7711
	step [85/192], loss=72.0496
	step [86/192], loss=80.1453
	step [87/192], loss=85.5749
	step [88/192], loss=74.9870
	step [89/192], loss=79.5866
	step [90/192], loss=80.5084
	step [91/192], loss=86.5161
	step [92/192], loss=78.7962
	step [93/192], loss=69.9688
	step [94/192], loss=71.9615
	step [95/192], loss=87.9363
	step [96/192], loss=80.6916
	step [97/192], loss=65.1320
	step [98/192], loss=76.7851
	step [99/192], loss=70.4150
	step [100/192], loss=92.4991
	step [101/192], loss=87.7005
	step [102/192], loss=62.6597
	step [103/192], loss=74.4999
	step [104/192], loss=77.1289
	step [105/192], loss=80.9864
	step [106/192], loss=68.9482
	step [107/192], loss=68.5979
	step [108/192], loss=84.0484
	step [109/192], loss=72.2521
	step [110/192], loss=68.7293
	step [111/192], loss=77.6693
	step [112/192], loss=79.2285
	step [113/192], loss=84.1198
	step [114/192], loss=78.8130
	step [115/192], loss=77.3616
	step [116/192], loss=88.0773
	step [117/192], loss=69.1445
	step [118/192], loss=95.9229
	step [119/192], loss=72.7593
	step [120/192], loss=80.3178
	step [121/192], loss=83.6078
	step [122/192], loss=85.0197
	step [123/192], loss=85.3908
	step [124/192], loss=80.8539
	step [125/192], loss=63.3796
	step [126/192], loss=85.5436
	step [127/192], loss=87.6223
	step [128/192], loss=85.5170
	step [129/192], loss=68.8168
	step [130/192], loss=77.5651
	step [131/192], loss=81.0428
	step [132/192], loss=111.9828
	step [133/192], loss=87.7159
	step [134/192], loss=73.2252
	step [135/192], loss=66.4047
	step [136/192], loss=92.9564
	step [137/192], loss=78.7739
	step [138/192], loss=74.2215
	step [139/192], loss=79.9143
	step [140/192], loss=87.2454
	step [141/192], loss=66.5981
	step [142/192], loss=81.5918
	step [143/192], loss=102.5731
	step [144/192], loss=63.5914
	step [145/192], loss=86.7028
	step [146/192], loss=81.5377
	step [147/192], loss=73.5771
	step [148/192], loss=81.0858
	step [149/192], loss=67.1948
	step [150/192], loss=71.9158
	step [151/192], loss=74.4540
	step [152/192], loss=78.7368
	step [153/192], loss=82.3180
	step [154/192], loss=97.4773
	step [155/192], loss=74.8906
	step [156/192], loss=78.6353
	step [157/192], loss=72.5914
	step [158/192], loss=92.8519
	step [159/192], loss=89.0754
	step [160/192], loss=90.9120
	step [161/192], loss=69.8527
	step [162/192], loss=70.2342
	step [163/192], loss=83.8927
	step [164/192], loss=86.8483
	step [165/192], loss=67.2348
	step [166/192], loss=93.4571
	step [167/192], loss=67.0931
	step [168/192], loss=84.1175
	step [169/192], loss=77.0855
	step [170/192], loss=71.5568
	step [171/192], loss=81.3274
	step [172/192], loss=90.3440
	step [173/192], loss=82.8936
	step [174/192], loss=89.6767
	step [175/192], loss=82.2887
	step [176/192], loss=79.7324
	step [177/192], loss=68.4365
	step [178/192], loss=78.2760
	step [179/192], loss=77.2074
	step [180/192], loss=86.0328
	step [181/192], loss=81.1786
	step [182/192], loss=73.6954
	step [183/192], loss=77.2041
	step [184/192], loss=89.5231
	step [185/192], loss=86.2551
	step [186/192], loss=68.9973
	step [187/192], loss=79.2920
	step [188/192], loss=85.3841
	step [189/192], loss=88.3015
	step [190/192], loss=69.3847
	step [191/192], loss=72.6183
	step [192/192], loss=76.6900
	Evaluating
	loss=0.0095, precision=0.3371, recall=0.8748, f1=0.4867
Training epoch 57
	step [1/192], loss=75.9050
	step [2/192], loss=81.1800
	step [3/192], loss=83.2428
	step [4/192], loss=73.8714
	step [5/192], loss=80.9362
	step [6/192], loss=68.4848
	step [7/192], loss=75.5636
	step [8/192], loss=65.4597
	step [9/192], loss=69.1907
	step [10/192], loss=83.4580
	step [11/192], loss=99.2745
	step [12/192], loss=82.5137
	step [13/192], loss=82.1535
	step [14/192], loss=88.1274
	step [15/192], loss=94.3419
	step [16/192], loss=80.0085
	step [17/192], loss=97.0042
	step [18/192], loss=80.3939
	step [19/192], loss=78.2906
	step [20/192], loss=68.1565
	step [21/192], loss=90.4886
	step [22/192], loss=76.0943
	step [23/192], loss=74.2040
	step [24/192], loss=84.5784
	step [25/192], loss=75.7696
	step [26/192], loss=76.8801
	step [27/192], loss=80.0032
	step [28/192], loss=81.1527
	step [29/192], loss=75.7794
	step [30/192], loss=81.4432
	step [31/192], loss=66.3368
	step [32/192], loss=80.3449
	step [33/192], loss=88.7879
	step [34/192], loss=63.0221
	step [35/192], loss=73.9540
	step [36/192], loss=67.7193
	step [37/192], loss=84.7625
	step [38/192], loss=75.9909
	step [39/192], loss=81.7847
	step [40/192], loss=100.7813
	step [41/192], loss=84.4736
	step [42/192], loss=81.4808
	step [43/192], loss=76.7738
	step [44/192], loss=86.7384
	step [45/192], loss=83.3671
	step [46/192], loss=76.8714
	step [47/192], loss=78.8058
	step [48/192], loss=95.2453
	step [49/192], loss=77.4321
	step [50/192], loss=84.0477
	step [51/192], loss=87.9704
	step [52/192], loss=74.1473
	step [53/192], loss=53.0071
	step [54/192], loss=81.9346
	step [55/192], loss=63.0805
	step [56/192], loss=76.8427
	step [57/192], loss=77.2621
	step [58/192], loss=66.0966
	step [59/192], loss=84.1009
	step [60/192], loss=90.0604
	step [61/192], loss=83.2123
	step [62/192], loss=80.1248
	step [63/192], loss=73.5993
	step [64/192], loss=72.4412
	step [65/192], loss=77.5678
	step [66/192], loss=86.1604
	step [67/192], loss=70.8956
	step [68/192], loss=80.4321
	step [69/192], loss=68.6633
	step [70/192], loss=74.8217
	step [71/192], loss=74.9645
	step [72/192], loss=79.5176
	step [73/192], loss=79.5677
	step [74/192], loss=73.0708
	step [75/192], loss=82.3142
	step [76/192], loss=81.5232
	step [77/192], loss=71.4398
	step [78/192], loss=73.1442
	step [79/192], loss=86.1264
	step [80/192], loss=75.5591
	step [81/192], loss=61.9749
	step [82/192], loss=68.4296
	step [83/192], loss=76.6942
	step [84/192], loss=78.2873
	step [85/192], loss=74.2575
	step [86/192], loss=77.7427
	step [87/192], loss=74.6198
	step [88/192], loss=74.7876
	step [89/192], loss=67.7150
	step [90/192], loss=87.4280
	step [91/192], loss=82.2371
	step [92/192], loss=78.5079
	step [93/192], loss=85.6367
	step [94/192], loss=71.3295
	step [95/192], loss=79.8151
	step [96/192], loss=65.9946
	step [97/192], loss=81.3267
	step [98/192], loss=81.4579
	step [99/192], loss=63.2558
	step [100/192], loss=80.0651
	step [101/192], loss=72.9609
	step [102/192], loss=77.0822
	step [103/192], loss=81.8093
	step [104/192], loss=80.0996
	step [105/192], loss=76.7121
	step [106/192], loss=70.6419
	step [107/192], loss=75.7735
	step [108/192], loss=84.2074
	step [109/192], loss=81.3567
	step [110/192], loss=93.7641
	step [111/192], loss=67.5904
	step [112/192], loss=81.6661
	step [113/192], loss=70.2237
	step [114/192], loss=71.2728
	step [115/192], loss=80.6254
	step [116/192], loss=74.3294
	step [117/192], loss=73.6659
	step [118/192], loss=57.8655
	step [119/192], loss=72.0726
	step [120/192], loss=75.9095
	step [121/192], loss=89.8306
	step [122/192], loss=84.1262
	step [123/192], loss=66.9538
	step [124/192], loss=70.9267
	step [125/192], loss=65.2250
	step [126/192], loss=69.8353
	step [127/192], loss=76.1926
	step [128/192], loss=81.3660
	step [129/192], loss=65.5152
	step [130/192], loss=74.7548
	step [131/192], loss=66.4769
	step [132/192], loss=78.4047
	step [133/192], loss=78.7263
	step [134/192], loss=70.4079
	step [135/192], loss=79.1936
	step [136/192], loss=69.1530
	step [137/192], loss=72.2979
	step [138/192], loss=74.4900
	step [139/192], loss=78.5622
	step [140/192], loss=71.9770
	step [141/192], loss=89.1383
	step [142/192], loss=78.2868
	step [143/192], loss=71.7460
	step [144/192], loss=75.3466
	step [145/192], loss=80.9702
	step [146/192], loss=68.4427
	step [147/192], loss=89.7004
	step [148/192], loss=71.6681
	step [149/192], loss=76.2058
	step [150/192], loss=93.8692
	step [151/192], loss=91.9970
	step [152/192], loss=83.1771
	step [153/192], loss=69.9340
	step [154/192], loss=82.2490
	step [155/192], loss=103.5642
	step [156/192], loss=72.9160
	step [157/192], loss=71.9105
	step [158/192], loss=78.8464
	step [159/192], loss=72.4980
	step [160/192], loss=69.8377
	step [161/192], loss=64.9103
	step [162/192], loss=74.1347
	step [163/192], loss=91.3008
	step [164/192], loss=68.2564
	step [165/192], loss=83.5847
	step [166/192], loss=86.7526
	step [167/192], loss=82.1824
	step [168/192], loss=81.0950
	step [169/192], loss=74.6405
	step [170/192], loss=74.6066
	step [171/192], loss=90.6986
	step [172/192], loss=84.2225
	step [173/192], loss=75.6010
	step [174/192], loss=80.0777
	step [175/192], loss=86.4447
	step [176/192], loss=73.1702
	step [177/192], loss=72.8134
	step [178/192], loss=81.2571
	step [179/192], loss=82.8450
	step [180/192], loss=93.8760
	step [181/192], loss=77.5985
	step [182/192], loss=65.1755
	step [183/192], loss=84.1129
	step [184/192], loss=76.3096
	step [185/192], loss=66.5189
	step [186/192], loss=83.0536
	step [187/192], loss=82.8365
	step [188/192], loss=78.5307
	step [189/192], loss=86.7705
	step [190/192], loss=99.9346
	step [191/192], loss=71.8164
	step [192/192], loss=59.7130
	Evaluating
	loss=0.0090, precision=0.3613, recall=0.8826, f1=0.5128
saving model as: 0_saved_model.pth
Training epoch 58
	step [1/192], loss=73.0442
	step [2/192], loss=74.5150
	step [3/192], loss=69.8274
	step [4/192], loss=87.7510
	step [5/192], loss=83.4994
	step [6/192], loss=83.2463
	step [7/192], loss=73.6348
	step [8/192], loss=80.0399
	step [9/192], loss=77.0455
	step [10/192], loss=83.7066
	step [11/192], loss=72.1513
	step [12/192], loss=61.3635
	step [13/192], loss=76.9232
	step [14/192], loss=85.9407
	step [15/192], loss=64.6586
	step [16/192], loss=72.7787
	step [17/192], loss=80.3013
	step [18/192], loss=74.4120
	step [19/192], loss=85.4627
	step [20/192], loss=88.0987
	step [21/192], loss=77.1805
	step [22/192], loss=83.8812
	step [23/192], loss=77.7938
	step [24/192], loss=80.0578
	step [25/192], loss=73.1113
	step [26/192], loss=78.6518
	step [27/192], loss=82.0950
	step [28/192], loss=74.7549
	step [29/192], loss=75.9130
	step [30/192], loss=73.6547
	step [31/192], loss=83.7481
	step [32/192], loss=79.6789
	step [33/192], loss=77.1393
	step [34/192], loss=82.4988
	step [35/192], loss=86.4598
	step [36/192], loss=76.6215
	step [37/192], loss=67.8329
	step [38/192], loss=81.4837
	step [39/192], loss=75.4709
	step [40/192], loss=68.2075
	step [41/192], loss=77.5058
	step [42/192], loss=66.7110
	step [43/192], loss=72.4755
	step [44/192], loss=58.6861
	step [45/192], loss=76.4062
	step [46/192], loss=69.3705
	step [47/192], loss=73.1627
	step [48/192], loss=81.6404
	step [49/192], loss=70.0397
	step [50/192], loss=68.9639
	step [51/192], loss=87.2327
	step [52/192], loss=74.3180
	step [53/192], loss=81.4909
	step [54/192], loss=71.6776
	step [55/192], loss=72.7865
	step [56/192], loss=77.4277
	step [57/192], loss=65.3003
	step [58/192], loss=78.8977
	step [59/192], loss=78.5625
	step [60/192], loss=62.8208
	step [61/192], loss=77.7608
	step [62/192], loss=89.2821
	step [63/192], loss=69.3254
	step [64/192], loss=75.0156
	step [65/192], loss=82.0954
	step [66/192], loss=81.3335
	step [67/192], loss=81.0188
	step [68/192], loss=70.4516
	step [69/192], loss=67.5233
	step [70/192], loss=76.3963
	step [71/192], loss=74.0745
	step [72/192], loss=89.2000
	step [73/192], loss=85.5710
	step [74/192], loss=69.8629
	step [75/192], loss=74.2944
	step [76/192], loss=71.0440
	step [77/192], loss=83.2771
	step [78/192], loss=66.6595
	step [79/192], loss=76.0707
	step [80/192], loss=64.0014
	step [81/192], loss=77.9315
	step [82/192], loss=73.1546
	step [83/192], loss=80.0144
	step [84/192], loss=74.0623
	step [85/192], loss=61.2800
	step [86/192], loss=79.2457
	step [87/192], loss=78.2407
	step [88/192], loss=76.4185
	step [89/192], loss=66.7485
	step [90/192], loss=80.0957
	step [91/192], loss=70.8603
	step [92/192], loss=77.8218
	step [93/192], loss=78.9523
	step [94/192], loss=74.0997
	step [95/192], loss=93.4271
	step [96/192], loss=69.0586
	step [97/192], loss=100.1301
	step [98/192], loss=77.3812
	step [99/192], loss=74.4199
	step [100/192], loss=89.1228
	step [101/192], loss=75.0542
	step [102/192], loss=83.3892
	step [103/192], loss=87.1114
	step [104/192], loss=92.2501
	step [105/192], loss=82.9949
	step [106/192], loss=64.6875
	step [107/192], loss=68.6386
	step [108/192], loss=80.9537
	step [109/192], loss=95.3899
	step [110/192], loss=82.4967
	step [111/192], loss=72.7350
	step [112/192], loss=77.2963
	step [113/192], loss=77.0500
	step [114/192], loss=74.4831
	step [115/192], loss=77.5190
	step [116/192], loss=77.2920
	step [117/192], loss=74.2479
	step [118/192], loss=74.9745
	step [119/192], loss=65.8297
	step [120/192], loss=63.4078
	step [121/192], loss=71.9177
	step [122/192], loss=75.0595
	step [123/192], loss=81.8801
	step [124/192], loss=86.9452
	step [125/192], loss=59.1507
	step [126/192], loss=74.8539
	step [127/192], loss=86.2218
	step [128/192], loss=78.9826
	step [129/192], loss=71.1100
	step [130/192], loss=75.6660
	step [131/192], loss=83.9470
	step [132/192], loss=83.0807
	step [133/192], loss=74.5467
	step [134/192], loss=75.8414
	step [135/192], loss=88.6901
	step [136/192], loss=84.2103
	step [137/192], loss=84.6734
	step [138/192], loss=75.8365
	step [139/192], loss=81.0792
	step [140/192], loss=77.7971
	step [141/192], loss=71.8505
	step [142/192], loss=87.3327
	step [143/192], loss=90.8947
	step [144/192], loss=76.8107
	step [145/192], loss=82.1561
	step [146/192], loss=69.6971
	step [147/192], loss=81.2822
	step [148/192], loss=67.3171
	step [149/192], loss=62.0083
	step [150/192], loss=70.7097
	step [151/192], loss=81.8263
	step [152/192], loss=87.2290
	step [153/192], loss=82.1140
	step [154/192], loss=73.2905
	step [155/192], loss=91.6687
	step [156/192], loss=70.6490
	step [157/192], loss=84.3785
	step [158/192], loss=79.5486
	step [159/192], loss=75.2149
	step [160/192], loss=87.0454
	step [161/192], loss=83.4963
	step [162/192], loss=81.4510
	step [163/192], loss=94.4486
	step [164/192], loss=82.9656
	step [165/192], loss=88.8723
	step [166/192], loss=77.8049
	step [167/192], loss=73.1126
	step [168/192], loss=82.4245
	step [169/192], loss=72.2089
	step [170/192], loss=82.6726
	step [171/192], loss=72.2614
	step [172/192], loss=84.2965
	step [173/192], loss=76.7080
	step [174/192], loss=74.4143
	step [175/192], loss=70.7820
	step [176/192], loss=81.4415
	step [177/192], loss=73.0490
	step [178/192], loss=90.8896
	step [179/192], loss=71.3136
	step [180/192], loss=77.5932
	step [181/192], loss=82.8826
	step [182/192], loss=66.7771
	step [183/192], loss=83.1806
	step [184/192], loss=71.5764
	step [185/192], loss=89.7442
	step [186/192], loss=70.9461
	step [187/192], loss=81.2149
	step [188/192], loss=66.1581
	step [189/192], loss=77.8144
	step [190/192], loss=83.1002
	step [191/192], loss=90.8011
	step [192/192], loss=80.4247
	Evaluating
	loss=0.0089, precision=0.3334, recall=0.8800, f1=0.4836
Training epoch 59
	step [1/192], loss=75.6310
	step [2/192], loss=79.0741
	step [3/192], loss=86.6050
	step [4/192], loss=72.8951
	step [5/192], loss=67.0522
	step [6/192], loss=78.0029
	step [7/192], loss=80.1474
	step [8/192], loss=85.4958
	step [9/192], loss=69.4912
	step [10/192], loss=85.0680
	step [11/192], loss=81.6378
	step [12/192], loss=75.3691
	step [13/192], loss=90.0575
	step [14/192], loss=73.3835
	step [15/192], loss=76.9981
	step [16/192], loss=81.0072
	step [17/192], loss=68.6443
	step [18/192], loss=66.4237
	step [19/192], loss=91.2265
	step [20/192], loss=69.3011
	step [21/192], loss=80.6256
	step [22/192], loss=64.6713
	step [23/192], loss=76.5945
	step [24/192], loss=63.6163
	step [25/192], loss=68.3547
	step [26/192], loss=63.9969
	step [27/192], loss=81.7522
	step [28/192], loss=70.3834
	step [29/192], loss=86.9262
	step [30/192], loss=74.5137
	step [31/192], loss=68.7056
	step [32/192], loss=69.6855
	step [33/192], loss=86.5686
	step [34/192], loss=70.5580
	step [35/192], loss=82.8426
	step [36/192], loss=80.6736
	step [37/192], loss=70.2787
	step [38/192], loss=83.0976
	step [39/192], loss=69.1105
	step [40/192], loss=72.7575
	step [41/192], loss=78.4172
	step [42/192], loss=74.8787
	step [43/192], loss=76.1638
	step [44/192], loss=71.6693
	step [45/192], loss=67.9518
	step [46/192], loss=81.5628
	step [47/192], loss=85.7177
	step [48/192], loss=83.0217
	step [49/192], loss=72.0349
	step [50/192], loss=77.6905
	step [51/192], loss=81.8114
	step [52/192], loss=75.8924
	step [53/192], loss=83.9572
	step [54/192], loss=72.2618
	step [55/192], loss=78.4468
	step [56/192], loss=74.2404
	step [57/192], loss=78.3178
	step [58/192], loss=72.6990
	step [59/192], loss=80.8453
	step [60/192], loss=67.6155
	step [61/192], loss=85.8422
	step [62/192], loss=82.4467
	step [63/192], loss=85.5269
	step [64/192], loss=66.9002
	step [65/192], loss=65.1918
	step [66/192], loss=69.3972
	step [67/192], loss=81.3022
	step [68/192], loss=76.9321
	step [69/192], loss=83.5891
	step [70/192], loss=64.7728
	step [71/192], loss=83.0165
	step [72/192], loss=66.9592
	step [73/192], loss=81.5168
	step [74/192], loss=76.9877
	step [75/192], loss=87.2535
	step [76/192], loss=73.6829
	step [77/192], loss=81.3726
	step [78/192], loss=72.4630
	step [79/192], loss=87.3144
	step [80/192], loss=94.1924
	step [81/192], loss=72.0213
	step [82/192], loss=71.9637
	step [83/192], loss=90.9327
	step [84/192], loss=69.6926
	step [85/192], loss=77.8321
	step [86/192], loss=67.0713
	step [87/192], loss=75.4499
	step [88/192], loss=69.9020
	step [89/192], loss=71.3221
	step [90/192], loss=90.4277
	step [91/192], loss=82.0556
	step [92/192], loss=82.5412
	step [93/192], loss=93.4723
	step [94/192], loss=77.1192
	step [95/192], loss=80.0961
	step [96/192], loss=66.7889
	step [97/192], loss=74.2751
	step [98/192], loss=71.0712
	step [99/192], loss=95.5351
	step [100/192], loss=76.4278
	step [101/192], loss=73.0822
	step [102/192], loss=78.6544
	step [103/192], loss=71.2926
	step [104/192], loss=88.2286
	step [105/192], loss=94.9640
	step [106/192], loss=91.9359
	step [107/192], loss=69.2394
	step [108/192], loss=72.7382
	step [109/192], loss=84.9841
	step [110/192], loss=82.7182
	step [111/192], loss=76.8985
	step [112/192], loss=82.2541
	step [113/192], loss=85.5044
	step [114/192], loss=66.9569
	step [115/192], loss=83.1130
	step [116/192], loss=74.0870
	step [117/192], loss=62.3509
	step [118/192], loss=86.7716
	step [119/192], loss=72.1786
	step [120/192], loss=76.5201
	step [121/192], loss=77.9456
	step [122/192], loss=75.3723
	step [123/192], loss=79.0878
	step [124/192], loss=85.4248
	step [125/192], loss=75.1449
	step [126/192], loss=67.4485
	step [127/192], loss=85.1188
	step [128/192], loss=77.7009
	step [129/192], loss=79.2192
	step [130/192], loss=79.9291
	step [131/192], loss=81.0834
	step [132/192], loss=59.5763
	step [133/192], loss=79.2062
	step [134/192], loss=84.4039
	step [135/192], loss=82.0065
	step [136/192], loss=72.3196
	step [137/192], loss=70.0088
	step [138/192], loss=83.3172
	step [139/192], loss=82.9593
	step [140/192], loss=77.5341
	step [141/192], loss=73.9019
	step [142/192], loss=77.9768
	step [143/192], loss=79.3902
	step [144/192], loss=71.0501
	step [145/192], loss=66.7678
	step [146/192], loss=74.5715
	step [147/192], loss=80.6268
	step [148/192], loss=71.3462
	step [149/192], loss=64.8393
	step [150/192], loss=75.5805
	step [151/192], loss=74.6324
	step [152/192], loss=82.7233
	step [153/192], loss=78.5667
	step [154/192], loss=66.8176
	step [155/192], loss=69.6728
	step [156/192], loss=79.1508
	step [157/192], loss=85.1404
	step [158/192], loss=82.6965
	step [159/192], loss=91.5242
	step [160/192], loss=86.5843
	step [161/192], loss=90.2155
	step [162/192], loss=89.0575
	step [163/192], loss=71.0486
	step [164/192], loss=72.9248
	step [165/192], loss=70.5477
	step [166/192], loss=70.6700
	step [167/192], loss=73.5763
	step [168/192], loss=77.8732
	step [169/192], loss=79.3928
	step [170/192], loss=89.5298
	step [171/192], loss=87.0324
	step [172/192], loss=68.7601
	step [173/192], loss=70.6122
	step [174/192], loss=85.1143
	step [175/192], loss=76.6584
	step [176/192], loss=78.9961
	step [177/192], loss=77.4901
	step [178/192], loss=70.7089
	step [179/192], loss=104.0090
	step [180/192], loss=71.7621
	step [181/192], loss=79.5138
	step [182/192], loss=68.5101
	step [183/192], loss=83.2785
	step [184/192], loss=78.7896
	step [185/192], loss=82.5671
	step [186/192], loss=85.1250
	step [187/192], loss=83.5651
	step [188/192], loss=78.0082
	step [189/192], loss=72.3176
	step [190/192], loss=82.4668
	step [191/192], loss=65.9662
	step [192/192], loss=75.1610
	Evaluating
	loss=0.0114, precision=0.2570, recall=0.8883, f1=0.3987
Training epoch 60
	step [1/192], loss=86.8751
	step [2/192], loss=71.3370
	step [3/192], loss=88.4101
	step [4/192], loss=77.3212
	step [5/192], loss=72.3763
	step [6/192], loss=81.2683
	step [7/192], loss=78.1894
	step [8/192], loss=76.8641
	step [9/192], loss=79.1722
	step [10/192], loss=80.6165
	step [11/192], loss=84.8974
	step [12/192], loss=62.2330
	step [13/192], loss=93.0978
	step [14/192], loss=73.8348
	step [15/192], loss=70.7400
	step [16/192], loss=80.7352
	step [17/192], loss=72.4615
	step [18/192], loss=99.0285
	step [19/192], loss=73.1929
	step [20/192], loss=69.5797
	step [21/192], loss=76.1176
	step [22/192], loss=80.7909
	step [23/192], loss=55.4900
	step [24/192], loss=81.2093
	step [25/192], loss=94.1513
	step [26/192], loss=66.3433
	step [27/192], loss=90.7796
	step [28/192], loss=73.6658
	step [29/192], loss=65.4073
	step [30/192], loss=61.4940
	step [31/192], loss=79.2173
	step [32/192], loss=73.1115
	step [33/192], loss=82.2898
	step [34/192], loss=84.8729
	step [35/192], loss=70.0789
	step [36/192], loss=73.9839
	step [37/192], loss=73.7066
	step [38/192], loss=84.3525
	step [39/192], loss=67.5017
	step [40/192], loss=91.8949
	step [41/192], loss=69.7565
	step [42/192], loss=71.2072
	step [43/192], loss=79.6691
	step [44/192], loss=75.2128
	step [45/192], loss=71.1861
	step [46/192], loss=60.0967
	step [47/192], loss=75.6455
	step [48/192], loss=81.3680
	step [49/192], loss=80.3131
	step [50/192], loss=71.1706
	step [51/192], loss=79.1091
	step [52/192], loss=75.5013
	step [53/192], loss=82.8705
	step [54/192], loss=78.0682
	step [55/192], loss=76.9444
	step [56/192], loss=65.7527
	step [57/192], loss=65.7994
	step [58/192], loss=75.2217
	step [59/192], loss=88.0187
	step [60/192], loss=81.2231
	step [61/192], loss=73.8885
	step [62/192], loss=79.1279
	step [63/192], loss=76.6263
	step [64/192], loss=84.7608
	step [65/192], loss=61.2484
	step [66/192], loss=70.6827
	step [67/192], loss=81.3569
	step [68/192], loss=67.7941
	step [69/192], loss=73.8300
	step [70/192], loss=86.5783
	step [71/192], loss=83.1871
	step [72/192], loss=68.3661
	step [73/192], loss=73.9984
	step [74/192], loss=60.8825
	step [75/192], loss=87.3296
	step [76/192], loss=70.0762
	step [77/192], loss=85.7140
	step [78/192], loss=76.4748
	step [79/192], loss=77.9678
	step [80/192], loss=67.8034
	step [81/192], loss=74.5825
	step [82/192], loss=71.1577
	step [83/192], loss=93.2931
	step [84/192], loss=78.5683
	step [85/192], loss=76.9733
	step [86/192], loss=68.0987
	step [87/192], loss=73.9868
	step [88/192], loss=81.6305
	step [89/192], loss=74.5570
	step [90/192], loss=73.0966
	step [91/192], loss=88.3231
	step [92/192], loss=78.5578
	step [93/192], loss=77.1328
	step [94/192], loss=86.4411
	step [95/192], loss=65.9483
	step [96/192], loss=74.2913
	step [97/192], loss=65.7617
	step [98/192], loss=66.2095
	step [99/192], loss=77.1340
	step [100/192], loss=61.9597
	step [101/192], loss=64.5045
	step [102/192], loss=87.4182
	step [103/192], loss=71.8547
	step [104/192], loss=73.8378
	step [105/192], loss=80.1654
	step [106/192], loss=84.7650
	step [107/192], loss=90.5102
	step [108/192], loss=70.6716
	step [109/192], loss=73.6127
	step [110/192], loss=72.1692
	step [111/192], loss=66.0514
	step [112/192], loss=85.4874
	step [113/192], loss=80.4947
	step [114/192], loss=93.0547
	step [115/192], loss=60.8627
	step [116/192], loss=71.3176
	step [117/192], loss=85.8831
	step [118/192], loss=86.6200
	step [119/192], loss=81.2951
	step [120/192], loss=83.2174
	step [121/192], loss=63.2294
	step [122/192], loss=81.3704
	step [123/192], loss=76.4122
	step [124/192], loss=87.0716
	step [125/192], loss=78.4721
	step [126/192], loss=76.3418
	step [127/192], loss=90.7472
	step [128/192], loss=78.6296
	step [129/192], loss=77.4669
	step [130/192], loss=66.7609
	step [131/192], loss=64.0054
	step [132/192], loss=70.3728
	step [133/192], loss=74.7610
	step [134/192], loss=89.4131
	step [135/192], loss=89.4309
	step [136/192], loss=86.1965
	step [137/192], loss=74.0672
	step [138/192], loss=77.5765
	step [139/192], loss=85.8192
	step [140/192], loss=67.7540
	step [141/192], loss=91.5711
	step [142/192], loss=81.6564
	step [143/192], loss=67.4716
	step [144/192], loss=71.7343
	step [145/192], loss=87.6436
	step [146/192], loss=81.1089
	step [147/192], loss=80.2406
	step [148/192], loss=79.5781
	step [149/192], loss=64.5741
	step [150/192], loss=63.9210
	step [151/192], loss=70.3473
	step [152/192], loss=73.1048
	step [153/192], loss=70.7253
	step [154/192], loss=72.6552
	step [155/192], loss=96.2346
	step [156/192], loss=88.0824
	step [157/192], loss=74.6389
	step [158/192], loss=70.8372
	step [159/192], loss=71.9376
	step [160/192], loss=82.5774
	step [161/192], loss=63.5452
	step [162/192], loss=89.0462
	step [163/192], loss=69.0999
	step [164/192], loss=77.4693
	step [165/192], loss=86.7514
	step [166/192], loss=77.4275
	step [167/192], loss=77.2626
	step [168/192], loss=84.0698
	step [169/192], loss=85.8530
	step [170/192], loss=69.8515
	step [171/192], loss=78.9731
	step [172/192], loss=72.0188
	step [173/192], loss=74.7814
	step [174/192], loss=75.2269
	step [175/192], loss=80.8197
	step [176/192], loss=104.7242
	step [177/192], loss=83.6865
	step [178/192], loss=73.5801
	step [179/192], loss=76.5792
	step [180/192], loss=65.9202
	step [181/192], loss=72.2859
	step [182/192], loss=67.7932
	step [183/192], loss=91.3436
	step [184/192], loss=69.3106
	step [185/192], loss=66.3728
	step [186/192], loss=74.3907
	step [187/192], loss=70.4195
	step [188/192], loss=85.6497
	step [189/192], loss=85.6179
	step [190/192], loss=71.7906
	step [191/192], loss=84.1282
	step [192/192], loss=81.8258
	Evaluating
	loss=0.0082, precision=0.3615, recall=0.8687, f1=0.5105
Training epoch 61
	step [1/192], loss=68.0972
	step [2/192], loss=66.9655
	step [3/192], loss=74.5284
	step [4/192], loss=85.5467
	step [5/192], loss=70.5255
	step [6/192], loss=80.7285
	step [7/192], loss=56.3714
	step [8/192], loss=73.2113
	step [9/192], loss=73.9582
	step [10/192], loss=56.2040
	step [11/192], loss=79.6156
	step [12/192], loss=69.1282
	step [13/192], loss=81.7369
	step [14/192], loss=70.4038
	step [15/192], loss=83.3891
	step [16/192], loss=85.6605
	step [17/192], loss=66.6260
	step [18/192], loss=75.2153
	step [19/192], loss=65.8426
	step [20/192], loss=80.4663
	step [21/192], loss=72.9391
	step [22/192], loss=71.0751
	step [23/192], loss=72.3528
	step [24/192], loss=74.5117
	step [25/192], loss=81.8438
	step [26/192], loss=90.3369
	step [27/192], loss=74.1520
	step [28/192], loss=83.4843
	step [29/192], loss=58.8127
	step [30/192], loss=68.1791
	step [31/192], loss=58.2823
	step [32/192], loss=81.6925
	step [33/192], loss=85.2764
	step [34/192], loss=75.3825
	step [35/192], loss=68.4858
	step [36/192], loss=97.9354
	step [37/192], loss=78.1201
	step [38/192], loss=82.5649
	step [39/192], loss=82.6226
	step [40/192], loss=90.5999
	step [41/192], loss=75.0380
	step [42/192], loss=67.6197
	step [43/192], loss=74.4613
	step [44/192], loss=74.5760
	step [45/192], loss=82.0294
	step [46/192], loss=71.3503
	step [47/192], loss=74.8652
	step [48/192], loss=67.2555
	step [49/192], loss=80.2271
	step [50/192], loss=80.8173
	step [51/192], loss=65.1151
	step [52/192], loss=71.9971
	step [53/192], loss=77.6618
	step [54/192], loss=78.8329
	step [55/192], loss=65.6686
	step [56/192], loss=94.5255
	step [57/192], loss=67.9084
	step [58/192], loss=75.4789
	step [59/192], loss=77.6122
	step [60/192], loss=90.8057
	step [61/192], loss=75.1631
	step [62/192], loss=78.1769
	step [63/192], loss=58.3676
	step [64/192], loss=85.0924
	step [65/192], loss=76.2226
	step [66/192], loss=66.1122
	step [67/192], loss=81.4530
	step [68/192], loss=70.9227
	step [69/192], loss=90.4293
	step [70/192], loss=89.7156
	step [71/192], loss=87.4456
	step [72/192], loss=75.1993
	step [73/192], loss=95.2808
	step [74/192], loss=76.2609
	step [75/192], loss=69.0607
	step [76/192], loss=83.6087
	step [77/192], loss=82.5826
	step [78/192], loss=72.0510
	step [79/192], loss=68.5021
	step [80/192], loss=70.8746
	step [81/192], loss=66.9991
	step [82/192], loss=78.4462
	step [83/192], loss=66.9053
	step [84/192], loss=74.5061
	step [85/192], loss=74.7811
	step [86/192], loss=66.3814
	step [87/192], loss=78.5687
	step [88/192], loss=72.7006
	step [89/192], loss=73.9568
	step [90/192], loss=64.1252
	step [91/192], loss=68.1613
	step [92/192], loss=85.2079
	step [93/192], loss=73.2244
	step [94/192], loss=85.7814
	step [95/192], loss=75.2958
	step [96/192], loss=77.6349
	step [97/192], loss=72.3476
	step [98/192], loss=81.3559
	step [99/192], loss=77.1399
	step [100/192], loss=75.1608
	step [101/192], loss=70.6567
	step [102/192], loss=77.2037
	step [103/192], loss=87.3134
	step [104/192], loss=78.6790
	step [105/192], loss=84.7152
	step [106/192], loss=78.5856
	step [107/192], loss=76.7511
	step [108/192], loss=71.9793
	step [109/192], loss=85.7790
	step [110/192], loss=76.1912
	step [111/192], loss=82.5141
	step [112/192], loss=69.8496
	step [113/192], loss=76.7713
	step [114/192], loss=84.1744
	step [115/192], loss=79.9985
	step [116/192], loss=81.8836
	step [117/192], loss=67.0234
	step [118/192], loss=70.6216
	step [119/192], loss=78.2383
	step [120/192], loss=75.7473
	step [121/192], loss=78.4337
	step [122/192], loss=79.5000
	step [123/192], loss=76.5247
	step [124/192], loss=81.7452
	step [125/192], loss=70.6222
	step [126/192], loss=80.3803
	step [127/192], loss=79.9570
	step [128/192], loss=77.6216
	step [129/192], loss=81.9867
	step [130/192], loss=85.2512
	step [131/192], loss=89.6273
	step [132/192], loss=69.7316
	step [133/192], loss=68.9599
	step [134/192], loss=80.3358
	step [135/192], loss=81.5608
	step [136/192], loss=72.1881
	step [137/192], loss=92.2244
	step [138/192], loss=72.0348
	step [139/192], loss=91.7634
	step [140/192], loss=81.3220
	step [141/192], loss=67.0403
	step [142/192], loss=79.0679
	step [143/192], loss=86.0762
	step [144/192], loss=73.3127
	step [145/192], loss=66.0392
	step [146/192], loss=78.6246
	step [147/192], loss=79.9584
	step [148/192], loss=80.2580
	step [149/192], loss=83.0669
	step [150/192], loss=81.5113
	step [151/192], loss=63.7037
	step [152/192], loss=80.0599
	step [153/192], loss=81.0413
	step [154/192], loss=67.5459
	step [155/192], loss=76.8752
	step [156/192], loss=78.6034
	step [157/192], loss=89.6272
	step [158/192], loss=75.0479
	step [159/192], loss=84.6590
	step [160/192], loss=70.6812
	step [161/192], loss=76.2132
	step [162/192], loss=75.0102
	step [163/192], loss=87.1837
	step [164/192], loss=80.7217
	step [165/192], loss=65.6803
	step [166/192], loss=73.5857
	step [167/192], loss=70.9993
	step [168/192], loss=80.3437
	step [169/192], loss=78.9557
	step [170/192], loss=90.4499
	step [171/192], loss=78.3407
	step [172/192], loss=72.1709
	step [173/192], loss=72.3659
	step [174/192], loss=74.6720
	step [175/192], loss=73.3535
	step [176/192], loss=78.3012
	step [177/192], loss=80.1535
	step [178/192], loss=85.6316
	step [179/192], loss=70.7303
	step [180/192], loss=73.0228
	step [181/192], loss=79.1982
	step [182/192], loss=68.1249
	step [183/192], loss=83.7270
	step [184/192], loss=81.7637
	step [185/192], loss=66.5737
	step [186/192], loss=78.3425
	step [187/192], loss=88.2921
	step [188/192], loss=77.1212
	step [189/192], loss=78.1017
	step [190/192], loss=85.1030
	step [191/192], loss=87.4067
	step [192/192], loss=67.3884
	Evaluating
	loss=0.0109, precision=0.2795, recall=0.8756, f1=0.4238
Training epoch 62
	step [1/192], loss=109.1912
	step [2/192], loss=82.7343
	step [3/192], loss=80.5927
	step [4/192], loss=74.3692
	step [5/192], loss=85.0485
	step [6/192], loss=70.5612
	step [7/192], loss=77.4357
	step [8/192], loss=62.7421
	step [9/192], loss=64.3117
	step [10/192], loss=73.7913
	step [11/192], loss=74.6432
	step [12/192], loss=88.0966
	step [13/192], loss=80.0405
	step [14/192], loss=86.2711
	step [15/192], loss=67.0188
	step [16/192], loss=57.5112
	step [17/192], loss=87.9683
	step [18/192], loss=88.1685
	step [19/192], loss=75.4484
	step [20/192], loss=70.6425
	step [21/192], loss=71.0684
	step [22/192], loss=74.4381
	step [23/192], loss=72.7979
	step [24/192], loss=77.0333
	step [25/192], loss=63.0033
	step [26/192], loss=73.0523
	step [27/192], loss=90.4495
	step [28/192], loss=69.1770
	step [29/192], loss=76.5599
	step [30/192], loss=81.3884
	step [31/192], loss=89.8577
	step [32/192], loss=90.7329
	step [33/192], loss=72.5667
	step [34/192], loss=73.5019
	step [35/192], loss=79.4645
	step [36/192], loss=76.6882
	step [37/192], loss=88.9432
	step [38/192], loss=75.5422
	step [39/192], loss=75.3030
	step [40/192], loss=77.3283
	step [41/192], loss=73.7390
	step [42/192], loss=73.6718
	step [43/192], loss=82.4547
	step [44/192], loss=70.9899
	step [45/192], loss=90.2376
	step [46/192], loss=75.4169
	step [47/192], loss=58.8320
	step [48/192], loss=84.1658
	step [49/192], loss=82.2540
	step [50/192], loss=64.0838
	step [51/192], loss=83.2288
	step [52/192], loss=71.7676
	step [53/192], loss=71.8672
	step [54/192], loss=87.5203
	step [55/192], loss=77.9105
	step [56/192], loss=61.7614
	step [57/192], loss=73.8036
	step [58/192], loss=76.2993
	step [59/192], loss=73.2229
	step [60/192], loss=92.7868
	step [61/192], loss=72.5495
	step [62/192], loss=73.2577
	step [63/192], loss=88.4003
	step [64/192], loss=74.6943
	step [65/192], loss=78.4272
	step [66/192], loss=58.3595
	step [67/192], loss=76.3797
	step [68/192], loss=68.7312
	step [69/192], loss=72.3381
	step [70/192], loss=59.6982
	step [71/192], loss=76.5247
	step [72/192], loss=80.6120
	step [73/192], loss=77.6125
	step [74/192], loss=80.7234
	step [75/192], loss=69.9059
	step [76/192], loss=83.6359
	step [77/192], loss=81.2972
	step [78/192], loss=69.0725
	step [79/192], loss=80.1376
	step [80/192], loss=80.3343
	step [81/192], loss=69.1945
	step [82/192], loss=88.3870
	step [83/192], loss=77.7728
	step [84/192], loss=78.5276
	step [85/192], loss=77.7714
	step [86/192], loss=74.1585
	step [87/192], loss=77.2308
	step [88/192], loss=71.3727
	step [89/192], loss=80.3399
	step [90/192], loss=71.9696
	step [91/192], loss=78.2488
	step [92/192], loss=76.5835
	step [93/192], loss=69.1598
	step [94/192], loss=70.3566
	step [95/192], loss=89.6587
	step [96/192], loss=82.7548
	step [97/192], loss=80.8016
	step [98/192], loss=83.0117
	step [99/192], loss=83.2294
	step [100/192], loss=74.4950
	step [101/192], loss=86.2895
	step [102/192], loss=74.7916
	step [103/192], loss=73.7900
	step [104/192], loss=87.1680
	step [105/192], loss=72.3062
	step [106/192], loss=56.7032
	step [107/192], loss=78.1877
	step [108/192], loss=78.4907
	step [109/192], loss=85.0823
	step [110/192], loss=78.0493
	step [111/192], loss=78.0518
	step [112/192], loss=76.3466
	step [113/192], loss=68.0251
	step [114/192], loss=75.3468
	step [115/192], loss=71.7093
	step [116/192], loss=72.1184
	step [117/192], loss=66.9901
	step [118/192], loss=60.7798
	step [119/192], loss=80.7393
	step [120/192], loss=77.0498
	step [121/192], loss=77.3525
	step [122/192], loss=72.7625
	step [123/192], loss=73.2649
	step [124/192], loss=81.0585
	step [125/192], loss=74.1443
	step [126/192], loss=75.1155
	step [127/192], loss=82.9926
	step [128/192], loss=69.3196
	step [129/192], loss=68.2229
	step [130/192], loss=91.1153
	step [131/192], loss=74.8953
	step [132/192], loss=61.3892
	step [133/192], loss=77.9787
	step [134/192], loss=82.1500
	step [135/192], loss=72.5734
	step [136/192], loss=72.0641
	step [137/192], loss=80.6693
	step [138/192], loss=75.0688
	step [139/192], loss=87.4208
	step [140/192], loss=77.7149
	step [141/192], loss=68.3987
	step [142/192], loss=78.6501
	step [143/192], loss=76.9456
	step [144/192], loss=80.3926
	step [145/192], loss=74.4676
	step [146/192], loss=76.8353
	step [147/192], loss=80.6653
	step [148/192], loss=68.0839
	step [149/192], loss=75.8883
	step [150/192], loss=71.6499
	step [151/192], loss=76.9548
	step [152/192], loss=79.1689
	step [153/192], loss=81.3840
	step [154/192], loss=71.8575
	step [155/192], loss=71.2335
	step [156/192], loss=76.2218
	step [157/192], loss=89.2263
	step [158/192], loss=83.6568
	step [159/192], loss=80.6822
	step [160/192], loss=78.7213
	step [161/192], loss=71.4665
	step [162/192], loss=76.4558
	step [163/192], loss=85.8803
	step [164/192], loss=80.8232
	step [165/192], loss=85.4257
	step [166/192], loss=65.3362
	step [167/192], loss=89.7801
	step [168/192], loss=78.7499
	step [169/192], loss=84.6128
	step [170/192], loss=72.9609
	step [171/192], loss=69.7555
	step [172/192], loss=79.3445
	step [173/192], loss=78.7456
	step [174/192], loss=76.5819
	step [175/192], loss=83.3903
	step [176/192], loss=82.4122
	step [177/192], loss=75.8247
	step [178/192], loss=71.8039
	step [179/192], loss=86.5449
	step [180/192], loss=94.9185
	step [181/192], loss=74.4805
	step [182/192], loss=62.9806
	step [183/192], loss=75.1927
	step [184/192], loss=76.5227
	step [185/192], loss=78.6100
	step [186/192], loss=82.0548
	step [187/192], loss=69.1100
	step [188/192], loss=65.1925
	step [189/192], loss=73.6324
	step [190/192], loss=63.1402
	step [191/192], loss=70.6764
	step [192/192], loss=68.5574
	Evaluating
	loss=0.0090, precision=0.3274, recall=0.8807, f1=0.4773
Training epoch 63
	step [1/192], loss=82.4019
	step [2/192], loss=73.5916
	step [3/192], loss=64.1889
	step [4/192], loss=65.4442
	step [5/192], loss=78.3596
	step [6/192], loss=82.8129
	step [7/192], loss=68.6985
	step [8/192], loss=86.2460
	step [9/192], loss=76.6569
	step [10/192], loss=72.5096
	step [11/192], loss=74.0568
	step [12/192], loss=68.9129
	step [13/192], loss=74.0822
	step [14/192], loss=75.3899
	step [15/192], loss=80.0401
	step [16/192], loss=81.4610
	step [17/192], loss=89.5551
	step [18/192], loss=70.7855
	step [19/192], loss=64.8002
	step [20/192], loss=71.1137
	step [21/192], loss=87.0174
	step [22/192], loss=62.9329
	step [23/192], loss=85.3685
	step [24/192], loss=79.1692
	step [25/192], loss=71.4288
	step [26/192], loss=80.5507
	step [27/192], loss=82.2538
	step [28/192], loss=68.9692
	step [29/192], loss=88.2275
	step [30/192], loss=77.7346
	step [31/192], loss=78.3675
	step [32/192], loss=77.4082
	step [33/192], loss=86.2782
	step [34/192], loss=67.9096
	step [35/192], loss=76.0317
	step [36/192], loss=86.4928
	step [37/192], loss=84.2788
	step [38/192], loss=73.0684
	step [39/192], loss=59.8703
	step [40/192], loss=68.2646
	step [41/192], loss=66.0026
	step [42/192], loss=74.7098
	step [43/192], loss=80.7838
	step [44/192], loss=85.6044
	step [45/192], loss=69.1576
	step [46/192], loss=67.3936
	step [47/192], loss=88.8505
	step [48/192], loss=68.9739
	step [49/192], loss=75.8883
	step [50/192], loss=72.8221
	step [51/192], loss=72.6938
	step [52/192], loss=75.3396
	step [53/192], loss=65.3594
	step [54/192], loss=77.3627
	step [55/192], loss=75.5993
	step [56/192], loss=66.2864
	step [57/192], loss=77.5758
	step [58/192], loss=71.3179
	step [59/192], loss=81.1859
	step [60/192], loss=71.1619
	step [61/192], loss=84.0967
	step [62/192], loss=64.9841
	step [63/192], loss=74.5404
	step [64/192], loss=84.5161
	step [65/192], loss=88.8992
	step [66/192], loss=78.7119
	step [67/192], loss=79.7821
	step [68/192], loss=74.1018
	step [69/192], loss=82.1831
	step [70/192], loss=76.4247
	step [71/192], loss=71.7501
	step [72/192], loss=95.9390
	step [73/192], loss=66.7451
	step [74/192], loss=75.7992
	step [75/192], loss=74.1984
	step [76/192], loss=71.5077
	step [77/192], loss=72.5975
	step [78/192], loss=69.9137
	step [79/192], loss=69.3797
	step [80/192], loss=69.6878
	step [81/192], loss=88.5414
	step [82/192], loss=77.3625
	step [83/192], loss=92.3493
	step [84/192], loss=90.0677
	step [85/192], loss=77.2536
	step [86/192], loss=74.4190
	step [87/192], loss=79.0745
	step [88/192], loss=77.5997
	step [89/192], loss=77.5972
	step [90/192], loss=81.7680
	step [91/192], loss=74.8998
	step [92/192], loss=82.8887
	step [93/192], loss=84.3281
	step [94/192], loss=77.7522
	step [95/192], loss=76.5297
	step [96/192], loss=82.3854
	step [97/192], loss=88.6320
	step [98/192], loss=81.0025
	step [99/192], loss=74.7954
	step [100/192], loss=77.6896
	step [101/192], loss=92.5875
	step [102/192], loss=77.9136
	step [103/192], loss=72.2211
	step [104/192], loss=66.6527
	step [105/192], loss=72.3909
	step [106/192], loss=51.3660
	step [107/192], loss=69.7244
	step [108/192], loss=66.3251
	step [109/192], loss=86.0000
	step [110/192], loss=66.3420
	step [111/192], loss=89.1240
	step [112/192], loss=70.8329
	step [113/192], loss=85.6566
	step [114/192], loss=78.1194
	step [115/192], loss=72.4809
	step [116/192], loss=79.3658
	step [117/192], loss=74.7125
	step [118/192], loss=70.0672
	step [119/192], loss=71.9948
	step [120/192], loss=75.8020
	step [121/192], loss=85.5815
	step [122/192], loss=68.8040
	step [123/192], loss=85.3487
	step [124/192], loss=77.8884
	step [125/192], loss=84.9260
	step [126/192], loss=78.8405
	step [127/192], loss=90.8722
	step [128/192], loss=71.9374
	step [129/192], loss=60.8962
	step [130/192], loss=82.0103
	step [131/192], loss=68.2178
	step [132/192], loss=81.7883
	step [133/192], loss=79.4660
	step [134/192], loss=66.9758
	step [135/192], loss=80.4372
	step [136/192], loss=80.2611
	step [137/192], loss=78.2897
	step [138/192], loss=76.8239
	step [139/192], loss=83.3431
	step [140/192], loss=73.7923
	step [141/192], loss=75.7234
	step [142/192], loss=82.2311
	step [143/192], loss=77.8890
	step [144/192], loss=88.8612
	step [145/192], loss=80.3725
	step [146/192], loss=89.6898
	step [147/192], loss=79.6887
	step [148/192], loss=85.8692
	step [149/192], loss=79.9103
	step [150/192], loss=76.5583
	step [151/192], loss=77.2145
	step [152/192], loss=66.1862
	step [153/192], loss=78.7605
	step [154/192], loss=79.8782
	step [155/192], loss=76.1878
	step [156/192], loss=66.8012
	step [157/192], loss=70.5340
	step [158/192], loss=88.4467
	step [159/192], loss=84.9270
	step [160/192], loss=81.0934
	step [161/192], loss=73.4596
	step [162/192], loss=77.6254
	step [163/192], loss=70.1456
	step [164/192], loss=77.6681
	step [165/192], loss=77.0674
	step [166/192], loss=78.8663
	step [167/192], loss=71.8290
	step [168/192], loss=81.7266
	step [169/192], loss=80.4248
	step [170/192], loss=83.8618
	step [171/192], loss=67.2412
	step [172/192], loss=83.9400
	step [173/192], loss=77.1907
	step [174/192], loss=82.8932
	step [175/192], loss=79.8831
	step [176/192], loss=66.0263
	step [177/192], loss=88.8617
	step [178/192], loss=79.9539
	step [179/192], loss=84.5598
	step [180/192], loss=80.0093
	step [181/192], loss=87.0736
	step [182/192], loss=78.0785
	step [183/192], loss=73.2290
	step [184/192], loss=79.7655
	step [185/192], loss=77.4050
	step [186/192], loss=78.5378
	step [187/192], loss=82.6073
	step [188/192], loss=84.7059
	step [189/192], loss=80.5815
	step [190/192], loss=73.0066
	step [191/192], loss=77.9467
	step [192/192], loss=78.2937
	Evaluating
	loss=0.0110, precision=0.2818, recall=0.8809, f1=0.4270
Training epoch 64
	step [1/192], loss=64.9527
	step [2/192], loss=86.6302
	step [3/192], loss=68.6970
	step [4/192], loss=73.3126
	step [5/192], loss=69.2091
	step [6/192], loss=82.8764
	step [7/192], loss=88.3949
	step [8/192], loss=79.1863
	step [9/192], loss=72.7557
	step [10/192], loss=82.8524
	step [11/192], loss=86.2123
	step [12/192], loss=80.8099
	step [13/192], loss=79.0521
	step [14/192], loss=83.8593
	step [15/192], loss=73.3187
	step [16/192], loss=69.4635
	step [17/192], loss=72.2156
	step [18/192], loss=88.9503
	step [19/192], loss=69.9878
	step [20/192], loss=75.9283
	step [21/192], loss=76.6177
	step [22/192], loss=64.6530
	step [23/192], loss=85.1213
	step [24/192], loss=70.6067
	step [25/192], loss=88.4066
	step [26/192], loss=85.2474
	step [27/192], loss=97.7610
	step [28/192], loss=76.9107
	step [29/192], loss=74.7212
	step [30/192], loss=74.9276
	step [31/192], loss=70.9010
	step [32/192], loss=73.3556
	step [33/192], loss=78.9990
	step [34/192], loss=79.9650
	step [35/192], loss=77.1602
	step [36/192], loss=71.1260
	step [37/192], loss=83.9078
	step [38/192], loss=71.4593
	step [39/192], loss=74.9534
	step [40/192], loss=73.6099
	step [41/192], loss=72.5077
	step [42/192], loss=72.8357
	step [43/192], loss=74.5507
	step [44/192], loss=87.6035
	step [45/192], loss=95.6098
	step [46/192], loss=89.4242
	step [47/192], loss=73.5243
	step [48/192], loss=70.6842
	step [49/192], loss=73.5027
	step [50/192], loss=86.2641
	step [51/192], loss=80.5456
	step [52/192], loss=82.5861
	step [53/192], loss=69.6685
	step [54/192], loss=83.7638
	step [55/192], loss=68.7941
	step [56/192], loss=77.9207
	step [57/192], loss=75.9318
	step [58/192], loss=82.4158
	step [59/192], loss=84.1029
	step [60/192], loss=85.4478
	step [61/192], loss=79.8465
	step [62/192], loss=80.6229
	step [63/192], loss=80.8128
	step [64/192], loss=67.2132
	step [65/192], loss=80.8883
	step [66/192], loss=75.5129
	step [67/192], loss=75.5964
	step [68/192], loss=77.7808
	step [69/192], loss=64.6541
	step [70/192], loss=77.5409
	step [71/192], loss=71.3143
	step [72/192], loss=69.9863
	step [73/192], loss=89.7011
	step [74/192], loss=65.6652
	step [75/192], loss=81.2271
	step [76/192], loss=71.1215
	step [77/192], loss=89.5426
	step [78/192], loss=72.5947
	step [79/192], loss=83.9194
	step [80/192], loss=69.7719
	step [81/192], loss=79.8229
	step [82/192], loss=69.5641
	step [83/192], loss=78.6942
	step [84/192], loss=74.0530
	step [85/192], loss=77.2212
	step [86/192], loss=61.7159
	step [87/192], loss=75.3191
	step [88/192], loss=74.3370
	step [89/192], loss=79.0581
	step [90/192], loss=76.7229
	step [91/192], loss=74.4541
	step [92/192], loss=77.0189
	step [93/192], loss=66.5265
	step [94/192], loss=73.5476
	step [95/192], loss=88.9120
	step [96/192], loss=70.9691
	step [97/192], loss=72.2460
	step [98/192], loss=78.0223
	step [99/192], loss=82.3723
	step [100/192], loss=75.7762
	step [101/192], loss=91.0489
	step [102/192], loss=80.1424
	step [103/192], loss=82.7680
	step [104/192], loss=65.1215
	step [105/192], loss=88.5658
	step [106/192], loss=66.2320
	step [107/192], loss=77.8179
	step [108/192], loss=60.6570
	step [109/192], loss=75.2847
	step [110/192], loss=83.5954
	step [111/192], loss=68.9142
	step [112/192], loss=90.2769
	step [113/192], loss=78.2340
	step [114/192], loss=76.4851
	step [115/192], loss=79.9035
	step [116/192], loss=81.9626
	step [117/192], loss=69.5356
	step [118/192], loss=76.4318
	step [119/192], loss=82.6272
	step [120/192], loss=66.7162
	step [121/192], loss=81.8694
	step [122/192], loss=70.7153
	step [123/192], loss=72.9256
	step [124/192], loss=73.6156
	step [125/192], loss=71.5989
	step [126/192], loss=75.1105
	step [127/192], loss=72.2799
	step [128/192], loss=90.7089
	step [129/192], loss=74.0364
	step [130/192], loss=79.7383
	step [131/192], loss=60.0450
	step [132/192], loss=74.4412
	step [133/192], loss=74.3244
	step [134/192], loss=76.4330
	step [135/192], loss=77.0419
	step [136/192], loss=75.2766
	step [137/192], loss=79.4695
	step [138/192], loss=77.7924
	step [139/192], loss=88.9118
	step [140/192], loss=78.3676
	step [141/192], loss=73.0733
	step [142/192], loss=75.6485
	step [143/192], loss=81.3011
	step [144/192], loss=77.9660
	step [145/192], loss=69.9922
	step [146/192], loss=75.2770
	step [147/192], loss=77.5764
	step [148/192], loss=68.6949
	step [149/192], loss=68.8891
	step [150/192], loss=86.3784
	step [151/192], loss=67.1264
	step [152/192], loss=66.8783
	step [153/192], loss=56.3611
	step [154/192], loss=70.1161
	step [155/192], loss=71.0861
	step [156/192], loss=85.3927
	step [157/192], loss=82.2988
	step [158/192], loss=80.1865
	step [159/192], loss=65.6109
	step [160/192], loss=62.8476
	step [161/192], loss=70.9482
	step [162/192], loss=81.6152
	step [163/192], loss=77.4596
	step [164/192], loss=75.2328
	step [165/192], loss=84.0935
	step [166/192], loss=80.1816
	step [167/192], loss=72.7112
	step [168/192], loss=74.3666
	step [169/192], loss=76.6368
	step [170/192], loss=81.0301
	step [171/192], loss=93.9302
	step [172/192], loss=73.9775
	step [173/192], loss=79.9296
	step [174/192], loss=76.0788
	step [175/192], loss=70.4226
	step [176/192], loss=73.7867
	step [177/192], loss=83.3422
	step [178/192], loss=71.2491
	step [179/192], loss=84.7165
	step [180/192], loss=64.0816
	step [181/192], loss=72.2843
	step [182/192], loss=86.1300
	step [183/192], loss=65.0929
	step [184/192], loss=70.9308
	step [185/192], loss=84.0428
	step [186/192], loss=72.1896
	step [187/192], loss=74.8425
	step [188/192], loss=58.0665
	step [189/192], loss=65.5670
	step [190/192], loss=69.3044
	step [191/192], loss=71.5431
	step [192/192], loss=64.0922
	Evaluating
	loss=0.0098, precision=0.3423, recall=0.8746, f1=0.4920
Training epoch 65
	step [1/192], loss=78.7284
	step [2/192], loss=63.7166
	step [3/192], loss=89.1141
	step [4/192], loss=76.9390
	step [5/192], loss=71.8087
	step [6/192], loss=80.8394
	step [7/192], loss=78.4853
	step [8/192], loss=73.6994
	step [9/192], loss=75.3639
	step [10/192], loss=73.0470
	step [11/192], loss=84.0416
	step [12/192], loss=81.0562
	step [13/192], loss=66.6105
	step [14/192], loss=83.1218
	step [15/192], loss=78.2121
	step [16/192], loss=66.1727
	step [17/192], loss=83.1789
	step [18/192], loss=69.5637
	step [19/192], loss=74.7860
	step [20/192], loss=70.3762
	step [21/192], loss=62.9376
	step [22/192], loss=68.3620
	step [23/192], loss=65.3779
	step [24/192], loss=74.4232
	step [25/192], loss=70.0688
	step [26/192], loss=69.2333
	step [27/192], loss=70.4922
	step [28/192], loss=77.2545
	step [29/192], loss=61.5665
	step [30/192], loss=69.7241
	step [31/192], loss=79.4168
	step [32/192], loss=68.8764
	step [33/192], loss=80.6898
	step [34/192], loss=81.3316
	step [35/192], loss=77.8564
	step [36/192], loss=70.5846
	step [37/192], loss=77.9471
	step [38/192], loss=88.4579
	step [39/192], loss=83.7590
	step [40/192], loss=78.1850
	step [41/192], loss=60.3216
	step [42/192], loss=83.6307
	step [43/192], loss=66.0338
	step [44/192], loss=86.9503
	step [45/192], loss=81.1232
	step [46/192], loss=76.4219
	step [47/192], loss=72.1973
	step [48/192], loss=63.4745
	step [49/192], loss=57.2878
	step [50/192], loss=74.3013
	step [51/192], loss=81.0151
	step [52/192], loss=74.1704
	step [53/192], loss=79.0714
	step [54/192], loss=68.9798
	step [55/192], loss=82.8195
	step [56/192], loss=76.1019
	step [57/192], loss=71.8636
	step [58/192], loss=78.2426
	step [59/192], loss=87.0093
	step [60/192], loss=77.0711
	step [61/192], loss=84.4479
	step [62/192], loss=71.1556
	step [63/192], loss=80.0956
	step [64/192], loss=79.4259
	step [65/192], loss=76.6584
	step [66/192], loss=80.2948
	step [67/192], loss=77.3640
	step [68/192], loss=60.6286
	step [69/192], loss=76.8150
	step [70/192], loss=67.3640
	step [71/192], loss=70.1256
	step [72/192], loss=89.2444
	step [73/192], loss=73.6436
	step [74/192], loss=80.3774
	step [75/192], loss=63.1470
	step [76/192], loss=78.8192
	step [77/192], loss=80.7583
	step [78/192], loss=69.4791
	step [79/192], loss=80.4155
	step [80/192], loss=70.4424
	step [81/192], loss=76.7638
	step [82/192], loss=79.4844
	step [83/192], loss=76.0447
	step [84/192], loss=74.4140
	step [85/192], loss=76.1684
	step [86/192], loss=69.0960
	step [87/192], loss=74.3901
	step [88/192], loss=62.2125
	step [89/192], loss=83.4678
	step [90/192], loss=67.0047
	step [91/192], loss=86.2997
	step [92/192], loss=79.6469
	step [93/192], loss=65.1921
	step [94/192], loss=72.2366
	step [95/192], loss=73.8807
	step [96/192], loss=79.8159
	step [97/192], loss=86.8113
	step [98/192], loss=74.4344
	step [99/192], loss=69.3169
	step [100/192], loss=81.6658
	step [101/192], loss=62.8764
	step [102/192], loss=77.7887
	step [103/192], loss=74.3288
	step [104/192], loss=67.3353
	step [105/192], loss=73.3157
	step [106/192], loss=68.6929
	step [107/192], loss=74.8720
	step [108/192], loss=100.5040
	step [109/192], loss=75.2217
	step [110/192], loss=77.8905
	step [111/192], loss=78.1757
	step [112/192], loss=82.0138
	step [113/192], loss=79.9377
	step [114/192], loss=83.3395
	step [115/192], loss=69.2809
	step [116/192], loss=70.8913
	step [117/192], loss=71.6084
	step [118/192], loss=87.7505
	step [119/192], loss=75.3769
	step [120/192], loss=71.0397
	step [121/192], loss=72.8784
	step [122/192], loss=68.8444
	step [123/192], loss=79.3050
	step [124/192], loss=70.3949
	step [125/192], loss=72.9990
	step [126/192], loss=77.3316
	step [127/192], loss=69.8515
	step [128/192], loss=66.5032
	step [129/192], loss=82.8369
	step [130/192], loss=80.0502
	step [131/192], loss=74.0488
	step [132/192], loss=71.1573
	step [133/192], loss=70.4928
	step [134/192], loss=75.2527
	step [135/192], loss=84.1573
	step [136/192], loss=61.7902
	step [137/192], loss=70.3463
	step [138/192], loss=69.3561
	step [139/192], loss=71.5734
	step [140/192], loss=78.6541
	step [141/192], loss=89.1344
	step [142/192], loss=75.7763
	step [143/192], loss=72.9717
	step [144/192], loss=73.8961
	step [145/192], loss=63.5120
	step [146/192], loss=83.7906
	step [147/192], loss=74.9335
	step [148/192], loss=73.5073
	step [149/192], loss=79.7456
	step [150/192], loss=72.8273
	step [151/192], loss=83.4924
	step [152/192], loss=84.4164
	step [153/192], loss=73.5884
	step [154/192], loss=75.6931
	step [155/192], loss=79.4733
	step [156/192], loss=76.2334
	step [157/192], loss=81.4255
	step [158/192], loss=85.2569
	step [159/192], loss=72.7729
	step [160/192], loss=82.0150
	step [161/192], loss=67.2554
	step [162/192], loss=71.9390
	step [163/192], loss=80.0714
	step [164/192], loss=80.0301
	step [165/192], loss=77.4341
	step [166/192], loss=74.4037
	step [167/192], loss=72.7077
	step [168/192], loss=77.6296
	step [169/192], loss=86.5459
	step [170/192], loss=83.0182
	step [171/192], loss=69.4064
	step [172/192], loss=72.0181
	step [173/192], loss=77.7281
	step [174/192], loss=72.7603
	step [175/192], loss=83.0558
	step [176/192], loss=79.1012
	step [177/192], loss=75.4680
	step [178/192], loss=68.8042
	step [179/192], loss=71.7101
	step [180/192], loss=97.1036
	step [181/192], loss=78.3837
	step [182/192], loss=79.1632
	step [183/192], loss=86.5690
	step [184/192], loss=81.9344
	step [185/192], loss=70.7554
	step [186/192], loss=65.3732
	step [187/192], loss=68.6725
	step [188/192], loss=81.1004
	step [189/192], loss=92.2720
	step [190/192], loss=74.6416
	step [191/192], loss=72.8485
	step [192/192], loss=77.9984
	Evaluating
	loss=0.0092, precision=0.3129, recall=0.8841, f1=0.4623
Training epoch 66
	step [1/192], loss=77.1554
	step [2/192], loss=66.3091
	step [3/192], loss=82.4003
	step [4/192], loss=72.8348
	step [5/192], loss=72.4944
	step [6/192], loss=73.7861
	step [7/192], loss=77.6984
	step [8/192], loss=84.0459
	step [9/192], loss=82.2482
	step [10/192], loss=88.5569
	step [11/192], loss=88.3190
	step [12/192], loss=82.1649
	step [13/192], loss=84.1041
	step [14/192], loss=77.2258
	step [15/192], loss=82.6658
	step [16/192], loss=72.1110
	step [17/192], loss=78.0218
	step [18/192], loss=59.5877
	step [19/192], loss=80.6064
	step [20/192], loss=62.4106
	step [21/192], loss=75.5150
	step [22/192], loss=70.4480
	step [23/192], loss=78.9959
	step [24/192], loss=78.5862
	step [25/192], loss=84.9148
	step [26/192], loss=78.5431
	step [27/192], loss=78.3246
	step [28/192], loss=74.1981
	step [29/192], loss=63.5222
	step [30/192], loss=71.2234
	step [31/192], loss=76.0520
	step [32/192], loss=71.9397
	step [33/192], loss=92.5380
	step [34/192], loss=70.6841
	step [35/192], loss=74.5381
	step [36/192], loss=69.9072
	step [37/192], loss=76.8747
	step [38/192], loss=62.2375
	step [39/192], loss=70.3073
	step [40/192], loss=75.2038
	step [41/192], loss=81.0314
	step [42/192], loss=82.9670
	step [43/192], loss=65.4108
	step [44/192], loss=66.8388
	step [45/192], loss=80.6227
	step [46/192], loss=77.6292
	step [47/192], loss=74.2534
	step [48/192], loss=64.0728
	step [49/192], loss=72.2438
	step [50/192], loss=86.3051
	step [51/192], loss=71.5600
	step [52/192], loss=72.5455
	step [53/192], loss=85.0074
	step [54/192], loss=74.1195
	step [55/192], loss=67.0306
	step [56/192], loss=73.5573
	step [57/192], loss=79.6142
	step [58/192], loss=70.0055
	step [59/192], loss=69.6459
	step [60/192], loss=72.1338
	step [61/192], loss=56.1926
	step [62/192], loss=77.0601
	step [63/192], loss=75.0295
	step [64/192], loss=75.1132
	step [65/192], loss=69.0298
	step [66/192], loss=78.4999
	step [67/192], loss=71.8439
	step [68/192], loss=73.9695
	step [69/192], loss=104.7953
	step [70/192], loss=74.8795
	step [71/192], loss=84.9015
	step [72/192], loss=86.0672
	step [73/192], loss=65.7374
	step [74/192], loss=77.2805
	step [75/192], loss=75.3601
	step [76/192], loss=66.6487
	step [77/192], loss=75.3020
	step [78/192], loss=86.4418
	step [79/192], loss=76.6917
	step [80/192], loss=85.4203
	step [81/192], loss=64.3893
	step [82/192], loss=66.2698
	step [83/192], loss=77.4278
	step [84/192], loss=90.5472
	step [85/192], loss=79.2784
	step [86/192], loss=81.9712
	step [87/192], loss=90.4030
	step [88/192], loss=81.6594
	step [89/192], loss=75.4854
	step [90/192], loss=65.5556
	step [91/192], loss=81.3215
	step [92/192], loss=85.9120
	step [93/192], loss=70.1247
	step [94/192], loss=85.7627
	step [95/192], loss=80.4888
	step [96/192], loss=74.9149
	step [97/192], loss=50.5831
	step [98/192], loss=69.5813
	step [99/192], loss=74.5370
	step [100/192], loss=75.7112
	step [101/192], loss=67.5632
	step [102/192], loss=66.0069
	step [103/192], loss=73.2323
	step [104/192], loss=67.7707
	step [105/192], loss=69.7319
	step [106/192], loss=70.6697
	step [107/192], loss=71.7253
	step [108/192], loss=77.2273
	step [109/192], loss=63.9719
	step [110/192], loss=69.6039
	step [111/192], loss=88.9076
	step [112/192], loss=70.8830
	step [113/192], loss=72.3645
	step [114/192], loss=60.2050
	step [115/192], loss=69.3499
	step [116/192], loss=80.8503
	step [117/192], loss=78.9036
	step [118/192], loss=67.4770
	step [119/192], loss=75.9959
	step [120/192], loss=80.3825
	step [121/192], loss=68.7086
	step [122/192], loss=80.8288
	step [123/192], loss=83.1321
	step [124/192], loss=80.4637
	step [125/192], loss=82.9771
	step [126/192], loss=69.6696
	step [127/192], loss=79.0306
	step [128/192], loss=74.8744
	step [129/192], loss=76.5099
	step [130/192], loss=72.0070
	step [131/192], loss=84.9137
	step [132/192], loss=90.0616
	step [133/192], loss=74.8147
	step [134/192], loss=72.5572
	step [135/192], loss=69.9859
	step [136/192], loss=82.7467
	step [137/192], loss=70.4008
	step [138/192], loss=74.5990
	step [139/192], loss=61.8145
	step [140/192], loss=77.5598
	step [141/192], loss=72.9025
	step [142/192], loss=66.3857
	step [143/192], loss=73.1728
	step [144/192], loss=75.4287
	step [145/192], loss=76.2588
	step [146/192], loss=75.2010
	step [147/192], loss=88.1765
	step [148/192], loss=74.9769
	step [149/192], loss=67.5389
	step [150/192], loss=70.6866
	step [151/192], loss=82.1720
	step [152/192], loss=72.2224
	step [153/192], loss=77.5769
	step [154/192], loss=70.2454
	step [155/192], loss=88.9870
	step [156/192], loss=69.7676
	step [157/192], loss=87.1694
	step [158/192], loss=67.4919
	step [159/192], loss=87.4859
	step [160/192], loss=63.1813
	step [161/192], loss=78.0487
	step [162/192], loss=64.5378
	step [163/192], loss=72.2926
	step [164/192], loss=67.8034
	step [165/192], loss=77.6116
	step [166/192], loss=85.3547
	step [167/192], loss=79.8340
	step [168/192], loss=73.8392
	step [169/192], loss=74.6732
	step [170/192], loss=85.5471
	step [171/192], loss=63.6815
	step [172/192], loss=67.9493
	step [173/192], loss=78.8017
	step [174/192], loss=74.6127
	step [175/192], loss=72.0596
	step [176/192], loss=76.8011
	step [177/192], loss=91.7491
	step [178/192], loss=66.9223
	step [179/192], loss=70.6953
	step [180/192], loss=80.3325
	step [181/192], loss=80.5310
	step [182/192], loss=76.5118
	step [183/192], loss=64.7161
	step [184/192], loss=92.1953
	step [185/192], loss=72.8951
	step [186/192], loss=82.1917
	step [187/192], loss=69.3905
	step [188/192], loss=72.6804
	step [189/192], loss=77.2510
	step [190/192], loss=62.6697
	step [191/192], loss=81.2739
	step [192/192], loss=64.0906
	Evaluating
	loss=0.0093, precision=0.3164, recall=0.8772, f1=0.4650
Training epoch 67
	step [1/192], loss=68.1626
	step [2/192], loss=69.2640
	step [3/192], loss=82.3756
	step [4/192], loss=81.3276
	step [5/192], loss=87.1956
	step [6/192], loss=75.4982
	step [7/192], loss=83.3483
	step [8/192], loss=85.3387
	step [9/192], loss=72.8766
	step [10/192], loss=89.8501
	step [11/192], loss=65.1046
	step [12/192], loss=69.3579
	step [13/192], loss=79.1348
	step [14/192], loss=80.0270
	step [15/192], loss=71.5541
	step [16/192], loss=74.1568
	step [17/192], loss=74.6521
	step [18/192], loss=73.9179
	step [19/192], loss=70.6240
	step [20/192], loss=62.7086
	step [21/192], loss=81.8474
	step [22/192], loss=72.2532
	step [23/192], loss=63.4024
	step [24/192], loss=72.9970
	step [25/192], loss=75.8227
	step [26/192], loss=62.9043
	step [27/192], loss=87.8216
	step [28/192], loss=71.4887
	step [29/192], loss=72.6983
	step [30/192], loss=71.4620
	step [31/192], loss=80.8121
	step [32/192], loss=74.4076
	step [33/192], loss=75.6040
	step [34/192], loss=74.7470
	step [35/192], loss=86.4836
	step [36/192], loss=87.2960
	step [37/192], loss=81.2055
	step [38/192], loss=68.2506
	step [39/192], loss=71.7030
	step [40/192], loss=71.1600
	step [41/192], loss=92.6330
	step [42/192], loss=84.8193
	step [43/192], loss=64.0854
	step [44/192], loss=83.5240
	step [45/192], loss=80.6842
	step [46/192], loss=71.2890
	step [47/192], loss=65.6302
	step [48/192], loss=69.2980
	step [49/192], loss=82.6855
	step [50/192], loss=74.3788
	step [51/192], loss=87.5555
	step [52/192], loss=68.1768
	step [53/192], loss=84.8731
	step [54/192], loss=89.0030
	step [55/192], loss=70.4500
	step [56/192], loss=73.7663
	step [57/192], loss=70.0449
	step [58/192], loss=84.6513
	step [59/192], loss=70.4968
	step [60/192], loss=73.1530
	step [61/192], loss=74.2558
	step [62/192], loss=85.6641
	step [63/192], loss=81.7993
	step [64/192], loss=82.1028
	step [65/192], loss=64.6028
	step [66/192], loss=74.9752
	step [67/192], loss=75.3533
	step [68/192], loss=84.2890
	step [69/192], loss=82.0314
	step [70/192], loss=68.3181
	step [71/192], loss=82.5954
	step [72/192], loss=74.7494
	step [73/192], loss=75.7692
	step [74/192], loss=85.0417
	step [75/192], loss=80.6171
	step [76/192], loss=66.2255
	step [77/192], loss=73.8804
	step [78/192], loss=81.1650
	step [79/192], loss=76.3489
	step [80/192], loss=83.8457
	step [81/192], loss=77.0429
	step [82/192], loss=71.7424
	step [83/192], loss=75.2891
	step [84/192], loss=84.0038
	step [85/192], loss=72.1145
	step [86/192], loss=69.7360
	step [87/192], loss=70.3810
	step [88/192], loss=80.9011
	step [89/192], loss=76.1212
	step [90/192], loss=77.7086
	step [91/192], loss=75.3813
	step [92/192], loss=73.7579
	step [93/192], loss=85.7822
	step [94/192], loss=67.4782
	step [95/192], loss=75.9387
	step [96/192], loss=71.1017
	step [97/192], loss=85.2381
	step [98/192], loss=77.3890
	step [99/192], loss=60.2276
	step [100/192], loss=68.7552
	step [101/192], loss=69.1366
	step [102/192], loss=79.4663
	step [103/192], loss=80.9371
	step [104/192], loss=59.3510
	step [105/192], loss=76.3494
	step [106/192], loss=74.9453
	step [107/192], loss=67.5574
	step [108/192], loss=73.3816
	step [109/192], loss=79.4243
	step [110/192], loss=74.5641
	step [111/192], loss=80.3433
	step [112/192], loss=67.7126
	step [113/192], loss=80.1005
	step [114/192], loss=82.1467
	step [115/192], loss=77.3613
	step [116/192], loss=78.4316
	step [117/192], loss=75.4761
	step [118/192], loss=70.2296
	step [119/192], loss=83.8447
	step [120/192], loss=82.5655
	step [121/192], loss=76.2156
	step [122/192], loss=65.3440
	step [123/192], loss=68.0451
	step [124/192], loss=81.5103
	step [125/192], loss=61.7804
	step [126/192], loss=68.3900
	step [127/192], loss=70.3331
	step [128/192], loss=77.0030
	step [129/192], loss=66.5764
	step [130/192], loss=64.6146
	step [131/192], loss=84.9770
	step [132/192], loss=82.2888
	step [133/192], loss=87.1534
	step [134/192], loss=84.2028
	step [135/192], loss=68.8869
	step [136/192], loss=65.5077
	step [137/192], loss=89.8926
	step [138/192], loss=82.0386
	step [139/192], loss=71.4256
	step [140/192], loss=66.3885
	step [141/192], loss=61.2760
	step [142/192], loss=71.3531
	step [143/192], loss=80.0107
	step [144/192], loss=81.0547
	step [145/192], loss=81.7008
	step [146/192], loss=88.8109
	step [147/192], loss=70.8004
	step [148/192], loss=65.3481
	step [149/192], loss=78.6096
	step [150/192], loss=67.9946
	step [151/192], loss=73.7283
	step [152/192], loss=73.5707
	step [153/192], loss=68.2367
	step [154/192], loss=72.7808
	step [155/192], loss=60.3947
	step [156/192], loss=71.6878
	step [157/192], loss=77.9652
	step [158/192], loss=66.5544
	step [159/192], loss=72.4250
	step [160/192], loss=82.6607
	step [161/192], loss=76.5001
	step [162/192], loss=63.8359
	step [163/192], loss=81.4567
	step [164/192], loss=69.9878
	step [165/192], loss=66.6180
	step [166/192], loss=66.9870
	step [167/192], loss=78.8502
	step [168/192], loss=68.3259
	step [169/192], loss=77.9965
	step [170/192], loss=72.2182
	step [171/192], loss=71.7310
	step [172/192], loss=60.6624
	step [173/192], loss=71.2144
	step [174/192], loss=71.6769
	step [175/192], loss=62.1222
	step [176/192], loss=65.9667
	step [177/192], loss=74.1469
	step [178/192], loss=75.9143
	step [179/192], loss=68.6049
	step [180/192], loss=68.8485
	step [181/192], loss=76.9856
	step [182/192], loss=80.8902
	step [183/192], loss=77.2985
	step [184/192], loss=68.0188
	step [185/192], loss=81.7329
	step [186/192], loss=78.4599
	step [187/192], loss=83.4778
	step [188/192], loss=76.7061
	step [189/192], loss=76.0695
	step [190/192], loss=84.5127
	step [191/192], loss=81.1600
	step [192/192], loss=59.7719
	Evaluating
	loss=0.0085, precision=0.3509, recall=0.8788, f1=0.5015
Training epoch 68
	step [1/192], loss=64.2130
	step [2/192], loss=65.9098
	step [3/192], loss=67.0209
	step [4/192], loss=58.2217
	step [5/192], loss=67.7165
	step [6/192], loss=78.1425
	step [7/192], loss=79.7862
	step [8/192], loss=68.9026
	step [9/192], loss=90.5108
	step [10/192], loss=69.1704
	step [11/192], loss=80.2824
	step [12/192], loss=75.7699
	step [13/192], loss=79.0358
	step [14/192], loss=78.2399
	step [15/192], loss=61.3498
	step [16/192], loss=82.8965
	step [17/192], loss=82.8306
	step [18/192], loss=68.3593
	step [19/192], loss=69.7116
	step [20/192], loss=76.9304
	step [21/192], loss=69.4876
	step [22/192], loss=65.9899
	step [23/192], loss=82.4820
	step [24/192], loss=82.5824
	step [25/192], loss=80.9627
	step [26/192], loss=72.8080
	step [27/192], loss=81.5993
	step [28/192], loss=80.5582
	step [29/192], loss=74.3026
	step [30/192], loss=77.1797
	step [31/192], loss=74.2883
	step [32/192], loss=69.7323
	step [33/192], loss=82.5174
	step [34/192], loss=92.9394
	step [35/192], loss=70.6685
	step [36/192], loss=76.5624
	step [37/192], loss=107.0480
	step [38/192], loss=86.1058
	step [39/192], loss=72.4648
	step [40/192], loss=87.1911
	step [41/192], loss=79.9824
	step [42/192], loss=82.1127
	step [43/192], loss=74.8080
	step [44/192], loss=85.2292
	step [45/192], loss=81.7920
	step [46/192], loss=72.1759
	step [47/192], loss=89.7793
	step [48/192], loss=74.8135
	step [49/192], loss=67.8218
	step [50/192], loss=85.9341
	step [51/192], loss=60.7236
	step [52/192], loss=75.6734
	step [53/192], loss=80.5505
	step [54/192], loss=81.0236
	step [55/192], loss=55.0409
	step [56/192], loss=71.4174
	step [57/192], loss=63.5678
	step [58/192], loss=82.0452
	step [59/192], loss=68.5186
	step [60/192], loss=70.2485
	step [61/192], loss=73.4503
	step [62/192], loss=75.2521
	step [63/192], loss=75.8369
	step [64/192], loss=61.5332
	step [65/192], loss=73.4162
	step [66/192], loss=71.3497
	step [67/192], loss=81.6135
	step [68/192], loss=84.1595
	step [69/192], loss=63.6013
	step [70/192], loss=65.0967
	step [71/192], loss=71.3988
	step [72/192], loss=77.1661
	step [73/192], loss=70.9688
	step [74/192], loss=89.9906
	step [75/192], loss=60.4696
	step [76/192], loss=91.2561
	step [77/192], loss=68.5893
	step [78/192], loss=67.8013
	step [79/192], loss=64.6046
	step [80/192], loss=71.5676
	step [81/192], loss=83.9035
	step [82/192], loss=61.3238
	step [83/192], loss=73.9133
	step [84/192], loss=68.4417
	step [85/192], loss=67.3710
	step [86/192], loss=67.2814
	step [87/192], loss=75.3808
	step [88/192], loss=65.8726
	step [89/192], loss=64.9646
	step [90/192], loss=76.1095
	step [91/192], loss=64.9551
	step [92/192], loss=91.8176
	step [93/192], loss=78.5039
	step [94/192], loss=83.3061
	step [95/192], loss=71.6939
	step [96/192], loss=74.6070
	step [97/192], loss=71.6369
	step [98/192], loss=73.8629
	step [99/192], loss=74.0824
	step [100/192], loss=91.9648
	step [101/192], loss=73.1251
	step [102/192], loss=73.3210
	step [103/192], loss=89.2566
	step [104/192], loss=71.2520
	step [105/192], loss=65.2374
	step [106/192], loss=64.9155
	step [107/192], loss=74.4827
	step [108/192], loss=68.2658
	step [109/192], loss=64.8247
	step [110/192], loss=63.4662
	step [111/192], loss=76.1895
	step [112/192], loss=70.6581
	step [113/192], loss=73.5526
	step [114/192], loss=73.8848
	step [115/192], loss=68.5517
	step [116/192], loss=82.9375
	step [117/192], loss=63.2506
	step [118/192], loss=90.3359
	step [119/192], loss=72.7396
	step [120/192], loss=89.7643
	step [121/192], loss=76.7508
	step [122/192], loss=77.4894
	step [123/192], loss=84.6418
	step [124/192], loss=73.0795
	step [125/192], loss=64.8761
	step [126/192], loss=73.2783
	step [127/192], loss=76.0790
	step [128/192], loss=75.3128
	step [129/192], loss=55.9875
	step [130/192], loss=81.9401
	step [131/192], loss=68.3815
	step [132/192], loss=69.3321
	step [133/192], loss=68.3322
	step [134/192], loss=81.9074
	step [135/192], loss=83.8195
	step [136/192], loss=85.2759
	step [137/192], loss=92.1935
	step [138/192], loss=79.1026
	step [139/192], loss=68.0959
	step [140/192], loss=68.0786
	step [141/192], loss=84.6654
	step [142/192], loss=73.7985
	step [143/192], loss=77.3506
	step [144/192], loss=75.5871
	step [145/192], loss=70.2509
	step [146/192], loss=65.2158
	step [147/192], loss=75.2456
	step [148/192], loss=70.2134
	step [149/192], loss=79.7986
	step [150/192], loss=70.6612
	step [151/192], loss=82.0597
	step [152/192], loss=70.4435
	step [153/192], loss=71.2777
	step [154/192], loss=62.8770
	step [155/192], loss=65.9428
	step [156/192], loss=70.2691
	step [157/192], loss=70.8320
	step [158/192], loss=65.4864
	step [159/192], loss=69.0825
	step [160/192], loss=70.2989
	step [161/192], loss=79.4936
	step [162/192], loss=71.9114
	step [163/192], loss=76.8043
	step [164/192], loss=86.7272
	step [165/192], loss=66.1061
	step [166/192], loss=77.4201
	step [167/192], loss=84.1498
	step [168/192], loss=75.1586
	step [169/192], loss=75.7154
	step [170/192], loss=71.9150
	step [171/192], loss=83.6028
	step [172/192], loss=68.7747
	step [173/192], loss=73.8668
	step [174/192], loss=85.1196
	step [175/192], loss=76.6701
	step [176/192], loss=86.7464
	step [177/192], loss=77.9854
	step [178/192], loss=64.5840
	step [179/192], loss=72.9162
	step [180/192], loss=75.6660
	step [181/192], loss=63.9777
	step [182/192], loss=78.3184
	step [183/192], loss=85.1082
	step [184/192], loss=60.7587
	step [185/192], loss=84.4434
	step [186/192], loss=82.9402
	step [187/192], loss=93.2346
	step [188/192], loss=71.9011
	step [189/192], loss=77.5305
	step [190/192], loss=82.8918
	step [191/192], loss=70.3990
	step [192/192], loss=71.1598
	Evaluating
	loss=0.0097, precision=0.2824, recall=0.8695, f1=0.4264
Training epoch 69
	step [1/192], loss=66.9751
	step [2/192], loss=72.9072
	step [3/192], loss=70.4721
	step [4/192], loss=63.1532
	step [5/192], loss=70.0860
	step [6/192], loss=76.1048
	step [7/192], loss=80.9100
	step [8/192], loss=74.8584
	step [9/192], loss=68.7473
	step [10/192], loss=74.7281
	step [11/192], loss=79.8783
	step [12/192], loss=73.2643
	step [13/192], loss=63.8680
	step [14/192], loss=63.8186
	step [15/192], loss=77.0567
	step [16/192], loss=73.0435
	step [17/192], loss=83.0726
	step [18/192], loss=74.4365
	step [19/192], loss=81.1176
	step [20/192], loss=84.1096
	step [21/192], loss=71.5347
	step [22/192], loss=83.1718
	step [23/192], loss=68.9898
	step [24/192], loss=75.9975
	step [25/192], loss=82.0136
	step [26/192], loss=60.4272
	step [27/192], loss=72.6916
	step [28/192], loss=83.4463
	step [29/192], loss=90.1262
	step [30/192], loss=68.4183
	step [31/192], loss=74.4192
	step [32/192], loss=90.2846
	step [33/192], loss=75.5944
	step [34/192], loss=63.5729
	step [35/192], loss=67.1663
	step [36/192], loss=65.2602
	step [37/192], loss=77.7180
	step [38/192], loss=72.0203
	step [39/192], loss=71.5654
	step [40/192], loss=69.4513
	step [41/192], loss=70.9725
	step [42/192], loss=73.7680
	step [43/192], loss=78.5787
	step [44/192], loss=87.2023
	step [45/192], loss=79.5297
	step [46/192], loss=74.5102
	step [47/192], loss=74.7919
	step [48/192], loss=70.2991
	step [49/192], loss=81.4530
	step [50/192], loss=63.2419
	step [51/192], loss=88.2926
	step [52/192], loss=75.2921
	step [53/192], loss=80.7945
	step [54/192], loss=84.2185
	step [55/192], loss=73.8484
	step [56/192], loss=79.1128
	step [57/192], loss=70.8281
	step [58/192], loss=89.7580
	step [59/192], loss=78.1449
	step [60/192], loss=73.1520
	step [61/192], loss=68.2341
	step [62/192], loss=71.6664
	step [63/192], loss=64.0856
	step [64/192], loss=74.4092
	step [65/192], loss=70.7724
	step [66/192], loss=76.3494
	step [67/192], loss=72.2528
	step [68/192], loss=66.5936
	step [69/192], loss=81.9002
	step [70/192], loss=67.1818
	step [71/192], loss=80.9823
	step [72/192], loss=74.7936
	step [73/192], loss=71.5126
	step [74/192], loss=73.3362
	step [75/192], loss=73.8057
	step [76/192], loss=86.5885
	step [77/192], loss=78.2376
	step [78/192], loss=59.3962
	step [79/192], loss=64.7565
	step [80/192], loss=84.5330
	step [81/192], loss=78.1590
	step [82/192], loss=59.8447
	step [83/192], loss=68.0091
	step [84/192], loss=72.0089
	step [85/192], loss=76.0072
	step [86/192], loss=76.4609
	step [87/192], loss=85.1800
	step [88/192], loss=75.2195
	step [89/192], loss=82.5646
	step [90/192], loss=78.3465
	step [91/192], loss=66.2953
	step [92/192], loss=83.9775
	step [93/192], loss=76.3922
	step [94/192], loss=81.9873
	step [95/192], loss=83.2483
	step [96/192], loss=67.9879
	step [97/192], loss=74.8332
	step [98/192], loss=69.5072
	step [99/192], loss=75.5409
	step [100/192], loss=74.8617
	step [101/192], loss=69.2940
	step [102/192], loss=67.7610
	step [103/192], loss=69.2655
	step [104/192], loss=74.2156
	step [105/192], loss=77.0996
	step [106/192], loss=83.4661
	step [107/192], loss=84.5824
	step [108/192], loss=67.7052
	step [109/192], loss=72.5855
	step [110/192], loss=67.4733
	step [111/192], loss=76.5939
	step [112/192], loss=69.8516
	step [113/192], loss=88.6751
	step [114/192], loss=71.0964
	step [115/192], loss=86.9997
	step [116/192], loss=76.0723
	step [117/192], loss=60.9269
	step [118/192], loss=84.3820
	step [119/192], loss=82.5766
	step [120/192], loss=74.9966
	step [121/192], loss=66.8633
	step [122/192], loss=76.0803
	step [123/192], loss=74.2067
	step [124/192], loss=81.0101
	step [125/192], loss=70.4046
	step [126/192], loss=68.2890
	step [127/192], loss=78.3185
	step [128/192], loss=73.0104
	step [129/192], loss=73.7785
	step [130/192], loss=73.9432
	step [131/192], loss=72.0776
	step [132/192], loss=79.4197
	step [133/192], loss=66.8724
	step [134/192], loss=83.0112
	step [135/192], loss=82.4636
	step [136/192], loss=75.7330
	step [137/192], loss=75.7085
	step [138/192], loss=72.2453
	step [139/192], loss=71.4607
	step [140/192], loss=63.3235
	step [141/192], loss=79.0010
	step [142/192], loss=58.2062
	step [143/192], loss=87.1500
	step [144/192], loss=73.2593
	step [145/192], loss=75.3998
	step [146/192], loss=80.3401
	step [147/192], loss=69.5742
	step [148/192], loss=87.7437
	step [149/192], loss=64.0144
	step [150/192], loss=76.6793
	step [151/192], loss=87.0997
	step [152/192], loss=62.4000
	step [153/192], loss=81.2922
	step [154/192], loss=56.6099
	step [155/192], loss=67.5504
	step [156/192], loss=93.6280
	step [157/192], loss=65.4278
	step [158/192], loss=62.6803
	step [159/192], loss=67.7086
	step [160/192], loss=78.4514
	step [161/192], loss=73.0806
	step [162/192], loss=80.4402
	step [163/192], loss=73.9317
	step [164/192], loss=86.3965
	step [165/192], loss=86.6967
	step [166/192], loss=77.7724
	step [167/192], loss=85.4772
	step [168/192], loss=68.7270
	step [169/192], loss=81.7044
	step [170/192], loss=63.6097
	step [171/192], loss=74.8331
	step [172/192], loss=71.3305
	step [173/192], loss=72.3242
	step [174/192], loss=73.8768
	step [175/192], loss=70.2032
	step [176/192], loss=73.3863
	step [177/192], loss=72.3207
	step [178/192], loss=72.4882
	step [179/192], loss=78.7793
	step [180/192], loss=77.6809
	step [181/192], loss=70.8676
	step [182/192], loss=77.8365
	step [183/192], loss=82.4208
	step [184/192], loss=68.9519
	step [185/192], loss=76.9077
	step [186/192], loss=64.8631
	step [187/192], loss=61.2432
	step [188/192], loss=75.4334
	step [189/192], loss=78.5846
	step [190/192], loss=70.3230
	step [191/192], loss=82.2594
	step [192/192], loss=68.6111
	Evaluating
	loss=0.0083, precision=0.3571, recall=0.8924, f1=0.5101
Training epoch 70
	step [1/192], loss=70.1927
	step [2/192], loss=80.4541
	step [3/192], loss=72.4635
	step [4/192], loss=72.6101
	step [5/192], loss=74.4380
	step [6/192], loss=68.6421
	step [7/192], loss=76.7199
	step [8/192], loss=67.1842
	step [9/192], loss=74.0556
	step [10/192], loss=71.2944
	step [11/192], loss=74.2629
	step [12/192], loss=71.9996
	step [13/192], loss=80.0009
	step [14/192], loss=74.5308
	step [15/192], loss=70.9759
	step [16/192], loss=65.5406
	step [17/192], loss=86.9518
	step [18/192], loss=69.6015
	step [19/192], loss=67.2439
	step [20/192], loss=72.6072
	step [21/192], loss=79.1876
	step [22/192], loss=76.2408
	step [23/192], loss=68.0588
	step [24/192], loss=80.5632
	step [25/192], loss=69.9413
	step [26/192], loss=68.5696
	step [27/192], loss=72.4350
	step [28/192], loss=71.5176
	step [29/192], loss=78.0990
	step [30/192], loss=73.9253
	step [31/192], loss=72.9723
	step [32/192], loss=67.0066
	step [33/192], loss=69.9315
	step [34/192], loss=80.0100
	step [35/192], loss=58.2126
	step [36/192], loss=70.7144
	step [37/192], loss=72.8535
	step [38/192], loss=84.1268
	step [39/192], loss=68.5194
	step [40/192], loss=77.8865
	step [41/192], loss=71.1016
	step [42/192], loss=64.9761
	step [43/192], loss=71.1168
	step [44/192], loss=69.1257
	step [45/192], loss=80.1744
	step [46/192], loss=62.0158
	step [47/192], loss=84.9063
	step [48/192], loss=70.9658
	step [49/192], loss=83.0149
	step [50/192], loss=81.6699
	step [51/192], loss=78.8243
	step [52/192], loss=78.1224
	step [53/192], loss=78.2428
	step [54/192], loss=62.6946
	step [55/192], loss=78.7576
	step [56/192], loss=66.7555
	step [57/192], loss=67.3536
	step [58/192], loss=78.2752
	step [59/192], loss=65.4930
	step [60/192], loss=76.5871
	step [61/192], loss=63.0659
	step [62/192], loss=85.0242
	step [63/192], loss=74.5287
	step [64/192], loss=79.1508
	step [65/192], loss=67.9797
	step [66/192], loss=75.6012
	step [67/192], loss=70.9806
	step [68/192], loss=79.8424
	step [69/192], loss=79.3427
	step [70/192], loss=97.4011
	step [71/192], loss=74.4253
	step [72/192], loss=69.3128
	step [73/192], loss=64.1917
	step [74/192], loss=61.5630
	step [75/192], loss=89.8815
	step [76/192], loss=77.7237
	step [77/192], loss=73.6504
	step [78/192], loss=67.8502
	step [79/192], loss=73.7759
	step [80/192], loss=78.2012
	step [81/192], loss=70.1227
	step [82/192], loss=88.2753
	step [83/192], loss=78.4644
	step [84/192], loss=72.6660
	step [85/192], loss=100.5398
	step [86/192], loss=64.4693
	step [87/192], loss=74.8499
	step [88/192], loss=68.4097
	step [89/192], loss=83.6358
	step [90/192], loss=74.2488
	step [91/192], loss=79.3659
	step [92/192], loss=75.3721
	step [93/192], loss=69.7216
	step [94/192], loss=75.3482
	step [95/192], loss=82.1108
	step [96/192], loss=78.0076
	step [97/192], loss=92.4692
	step [98/192], loss=71.8316
	step [99/192], loss=84.3943
	step [100/192], loss=82.2388
	step [101/192], loss=73.8962
	step [102/192], loss=85.3669
	step [103/192], loss=69.1007
	step [104/192], loss=76.4212
	step [105/192], loss=65.7434
	step [106/192], loss=68.0562
	step [107/192], loss=65.9584
	step [108/192], loss=80.3793
	step [109/192], loss=86.4430
	step [110/192], loss=64.8579
	step [111/192], loss=75.1076
	step [112/192], loss=67.3721
	step [113/192], loss=68.7414
	step [114/192], loss=64.3550
	step [115/192], loss=97.2942
	step [116/192], loss=62.6289
	step [117/192], loss=79.1897
	step [118/192], loss=76.4937
	step [119/192], loss=70.9218
	step [120/192], loss=68.8883
	step [121/192], loss=87.7189
	step [122/192], loss=66.2945
	step [123/192], loss=78.1950
	step [124/192], loss=71.0593
	step [125/192], loss=76.4063
	step [126/192], loss=64.5152
	step [127/192], loss=76.7928
	step [128/192], loss=72.0656
	step [129/192], loss=69.4722
	step [130/192], loss=80.9839
	step [131/192], loss=81.6679
	step [132/192], loss=82.6133
	step [133/192], loss=80.1112
	step [134/192], loss=69.2412
	step [135/192], loss=74.4869
	step [136/192], loss=89.5204
	step [137/192], loss=82.8648
	step [138/192], loss=71.7747
	step [139/192], loss=67.7509
	step [140/192], loss=60.6138
	step [141/192], loss=74.0995
	step [142/192], loss=62.1889
	step [143/192], loss=69.0000
	step [144/192], loss=78.1025
	step [145/192], loss=78.1565
	step [146/192], loss=68.6607
	step [147/192], loss=78.7022
	step [148/192], loss=66.8646
	step [149/192], loss=62.9166
	step [150/192], loss=76.1171
	step [151/192], loss=73.5657
	step [152/192], loss=81.1311
	step [153/192], loss=87.4141
	step [154/192], loss=62.1471
	step [155/192], loss=76.4008
	step [156/192], loss=73.0577
	step [157/192], loss=70.0291
	step [158/192], loss=66.8515
	step [159/192], loss=67.8397
	step [160/192], loss=71.2045
	step [161/192], loss=79.0691
	step [162/192], loss=69.3682
	step [163/192], loss=89.5790
	step [164/192], loss=74.3037
	step [165/192], loss=72.5125
	step [166/192], loss=79.0869
	step [167/192], loss=74.1560
	step [168/192], loss=77.8646
	step [169/192], loss=72.2211
	step [170/192], loss=81.5880
	step [171/192], loss=76.1046
	step [172/192], loss=73.8365
	step [173/192], loss=75.0433
	step [174/192], loss=70.2710
	step [175/192], loss=75.8939
	step [176/192], loss=76.2134
	step [177/192], loss=74.2225
	step [178/192], loss=81.1878
	step [179/192], loss=71.6188
	step [180/192], loss=70.3531
	step [181/192], loss=68.3269
	step [182/192], loss=78.8238
	step [183/192], loss=74.7864
	step [184/192], loss=94.8042
	step [185/192], loss=86.2588
	step [186/192], loss=74.7718
	step [187/192], loss=64.4871
	step [188/192], loss=75.6895
	step [189/192], loss=74.1692
	step [190/192], loss=74.5240
	step [191/192], loss=68.0240
	step [192/192], loss=66.9301
	Evaluating
	loss=0.0095, precision=0.2977, recall=0.8822, f1=0.4452
Training epoch 71
	step [1/192], loss=80.9879
	step [2/192], loss=79.6381
	step [3/192], loss=73.1087
	step [4/192], loss=73.0952
	step [5/192], loss=77.0048
	step [6/192], loss=85.9239
	step [7/192], loss=78.5438
	step [8/192], loss=83.4989
	step [9/192], loss=86.7157
	step [10/192], loss=54.8129
	step [11/192], loss=68.2399
	step [12/192], loss=76.4266
	step [13/192], loss=81.8843
	step [14/192], loss=81.9362
	step [15/192], loss=59.5835
	step [16/192], loss=70.8504
	step [17/192], loss=66.5854
	step [18/192], loss=61.8819
	step [19/192], loss=79.3261
	step [20/192], loss=75.2860
	step [21/192], loss=72.8511
	step [22/192], loss=57.7890
	step [23/192], loss=64.6912
	step [24/192], loss=73.9907
	step [25/192], loss=64.7793
	step [26/192], loss=69.2257
	step [27/192], loss=84.4825
	step [28/192], loss=64.9975
	step [29/192], loss=72.2486
	step [30/192], loss=69.2185
	step [31/192], loss=83.6400
	step [32/192], loss=74.1870
	step [33/192], loss=81.2725
	step [34/192], loss=78.2251
	step [35/192], loss=69.2016
	step [36/192], loss=72.5681
	step [37/192], loss=76.4013
	step [38/192], loss=75.9856
	step [39/192], loss=74.1739
	step [40/192], loss=74.7494
	step [41/192], loss=65.0835
	step [42/192], loss=64.9138
	step [43/192], loss=69.0575
	step [44/192], loss=63.4991
	step [45/192], loss=68.6399
	step [46/192], loss=71.4183
	step [47/192], loss=75.3720
	step [48/192], loss=76.1743
	step [49/192], loss=81.2812
	step [50/192], loss=75.3285
	step [51/192], loss=65.9927
	step [52/192], loss=73.8129
	step [53/192], loss=85.8925
	step [54/192], loss=74.2013
	step [55/192], loss=79.6614
	step [56/192], loss=57.9376
	step [57/192], loss=72.7742
	step [58/192], loss=72.5934
	step [59/192], loss=69.4376
	step [60/192], loss=75.3929
	step [61/192], loss=73.0547
	step [62/192], loss=86.4236
	step [63/192], loss=64.0068
	step [64/192], loss=71.6743
	step [65/192], loss=62.6515
	step [66/192], loss=80.8691
	step [67/192], loss=62.3155
	step [68/192], loss=86.1786
	step [69/192], loss=61.5323
	step [70/192], loss=63.9047
	step [71/192], loss=72.8192
	step [72/192], loss=70.4373
	step [73/192], loss=78.5509
	step [74/192], loss=85.8688
	step [75/192], loss=72.5547
	step [76/192], loss=73.2787
	step [77/192], loss=79.7090
	step [78/192], loss=65.1204
	step [79/192], loss=67.3288
	step [80/192], loss=81.6559
	step [81/192], loss=62.7054
	step [82/192], loss=71.8878
	step [83/192], loss=76.1277
	step [84/192], loss=78.6165
	step [85/192], loss=76.6571
	step [86/192], loss=89.2329
	step [87/192], loss=70.2425
	step [88/192], loss=76.6598
	step [89/192], loss=79.6012
	step [90/192], loss=68.7027
	step [91/192], loss=80.3690
	step [92/192], loss=73.7233
	step [93/192], loss=75.7623
	step [94/192], loss=74.1274
	step [95/192], loss=79.8878
	step [96/192], loss=83.5352
	step [97/192], loss=74.5361
	step [98/192], loss=70.2692
	step [99/192], loss=74.0410
	step [100/192], loss=71.0581
	step [101/192], loss=72.4612
	step [102/192], loss=70.4534
	step [103/192], loss=84.5086
	step [104/192], loss=72.6821
	step [105/192], loss=71.0975
	step [106/192], loss=62.9184
	step [107/192], loss=67.2518
	step [108/192], loss=101.0978
	step [109/192], loss=83.0074
	step [110/192], loss=80.0030
	step [111/192], loss=81.2782
	step [112/192], loss=63.5545
	step [113/192], loss=77.5251
	step [114/192], loss=75.5166
	step [115/192], loss=76.1950
	step [116/192], loss=77.6554
	step [117/192], loss=71.3090
	step [118/192], loss=75.9410
	step [119/192], loss=74.8770
	step [120/192], loss=64.5374
	step [121/192], loss=86.0058
	step [122/192], loss=71.3981
	step [123/192], loss=73.1048
	step [124/192], loss=76.5019
	step [125/192], loss=77.1235
	step [126/192], loss=73.1483
	step [127/192], loss=71.7003
	step [128/192], loss=72.7489
	step [129/192], loss=78.0107
	step [130/192], loss=67.8985
	step [131/192], loss=74.3510
	step [132/192], loss=67.2789
	step [133/192], loss=73.3118
	step [134/192], loss=81.2662
	step [135/192], loss=79.1333
	step [136/192], loss=83.3906
	step [137/192], loss=65.7995
	step [138/192], loss=70.3565
	step [139/192], loss=78.9435
	step [140/192], loss=65.9139
	step [141/192], loss=75.0211
	step [142/192], loss=65.6691
	step [143/192], loss=84.3202
	step [144/192], loss=69.4452
	step [145/192], loss=85.4800
	step [146/192], loss=86.3595
	step [147/192], loss=78.7368
	step [148/192], loss=73.6272
	step [149/192], loss=65.5794
	step [150/192], loss=79.4512
	step [151/192], loss=74.2712
	step [152/192], loss=73.7464
	step [153/192], loss=82.7544
	step [154/192], loss=81.7121
	step [155/192], loss=64.1867
	step [156/192], loss=67.3151
	step [157/192], loss=59.6385
	step [158/192], loss=69.2863
	step [159/192], loss=70.0945
	step [160/192], loss=64.0605
	step [161/192], loss=79.5339
	step [162/192], loss=72.0992
	step [163/192], loss=81.3197
	step [164/192], loss=83.9752
	step [165/192], loss=75.1864
	step [166/192], loss=68.4550
	step [167/192], loss=88.8272
	step [168/192], loss=79.6438
	step [169/192], loss=81.3260
	step [170/192], loss=98.3468
	step [171/192], loss=66.9171
	step [172/192], loss=76.0591
	step [173/192], loss=77.8797
	step [174/192], loss=70.8170
	step [175/192], loss=64.1042
	step [176/192], loss=70.0816
	step [177/192], loss=61.1154
	step [178/192], loss=72.6224
	step [179/192], loss=67.3105
	step [180/192], loss=76.9337
	step [181/192], loss=75.8970
	step [182/192], loss=69.7881
	step [183/192], loss=87.4419
	step [184/192], loss=68.4737
	step [185/192], loss=67.8738
	step [186/192], loss=69.8258
	step [187/192], loss=71.9459
	step [188/192], loss=90.8298
	step [189/192], loss=71.9382
	step [190/192], loss=79.4100
	step [191/192], loss=80.5002
	step [192/192], loss=62.8799
	Evaluating
	loss=0.0086, precision=0.3570, recall=0.8731, f1=0.5068
Training epoch 72
	step [1/192], loss=71.6647
	step [2/192], loss=68.0914
	step [3/192], loss=80.6504
	step [4/192], loss=59.3539
	step [5/192], loss=80.6043
	step [6/192], loss=78.1881
	step [7/192], loss=76.0796
	step [8/192], loss=84.5534
	step [9/192], loss=70.7948
	step [10/192], loss=72.3653
	step [11/192], loss=66.4872
	step [12/192], loss=81.0831
	step [13/192], loss=83.3629
	step [14/192], loss=67.3379
	step [15/192], loss=72.3041
	step [16/192], loss=62.1578
	step [17/192], loss=74.1768
	step [18/192], loss=69.9728
	step [19/192], loss=64.5735
	step [20/192], loss=75.6689
	step [21/192], loss=71.4289
	step [22/192], loss=79.7260
	step [23/192], loss=75.1384
	step [24/192], loss=84.2891
	step [25/192], loss=83.1765
	step [26/192], loss=71.8234
	step [27/192], loss=82.1715
	step [28/192], loss=69.3319
	step [29/192], loss=80.1633
	step [30/192], loss=76.4490
	step [31/192], loss=79.0914
	step [32/192], loss=76.7633
	step [33/192], loss=77.9508
	step [34/192], loss=72.1898
	step [35/192], loss=86.4507
	step [36/192], loss=71.2159
	step [37/192], loss=79.8105
	step [38/192], loss=71.2455
	step [39/192], loss=76.6496
	step [40/192], loss=80.7282
	step [41/192], loss=82.8544
	step [42/192], loss=77.2979
	step [43/192], loss=76.4285
	step [44/192], loss=67.6448
	step [45/192], loss=73.5555
	step [46/192], loss=81.5968
	step [47/192], loss=72.1838
	step [48/192], loss=73.8385
	step [49/192], loss=80.1605
	step [50/192], loss=84.6842
	step [51/192], loss=69.4100
	step [52/192], loss=75.9885
	step [53/192], loss=73.2294
	step [54/192], loss=72.4057
	step [55/192], loss=73.8788
	step [56/192], loss=71.2820
	step [57/192], loss=66.3327
	step [58/192], loss=64.5735
	step [59/192], loss=73.9192
	step [60/192], loss=66.9620
	step [61/192], loss=63.6497
	step [62/192], loss=58.7600
	step [63/192], loss=54.3181
	step [64/192], loss=66.7084
	step [65/192], loss=65.6326
	step [66/192], loss=85.4283
	step [67/192], loss=72.6488
	step [68/192], loss=57.7936
	step [69/192], loss=81.0663
	step [70/192], loss=73.6153
	step [71/192], loss=72.9995
	step [72/192], loss=74.5716
	step [73/192], loss=69.0997
	step [74/192], loss=65.3779
	step [75/192], loss=71.8645
	step [76/192], loss=69.2441
	step [77/192], loss=67.0647
	step [78/192], loss=74.6343
	step [79/192], loss=81.7464
	step [80/192], loss=55.9076
	step [81/192], loss=76.5994
	step [82/192], loss=74.7728
	step [83/192], loss=73.5170
	step [84/192], loss=61.2069
	step [85/192], loss=77.1739
	step [86/192], loss=81.1871
	step [87/192], loss=72.0503
	step [88/192], loss=68.9137
	step [89/192], loss=58.4438
	step [90/192], loss=78.3536
	step [91/192], loss=77.0069
	step [92/192], loss=93.0948
	step [93/192], loss=66.1866
	step [94/192], loss=74.3917
	step [95/192], loss=84.5043
	step [96/192], loss=75.6063
	step [97/192], loss=69.3091
	step [98/192], loss=65.3625
	step [99/192], loss=78.2577
	step [100/192], loss=77.0996
	step [101/192], loss=70.0119
	step [102/192], loss=78.9323
	step [103/192], loss=73.4671
	step [104/192], loss=77.1461
	step [105/192], loss=81.9556
	step [106/192], loss=91.0251
	step [107/192], loss=63.5980
	step [108/192], loss=69.0262
	step [109/192], loss=82.6170
	step [110/192], loss=69.7292
	step [111/192], loss=70.8737
	step [112/192], loss=77.8220
	step [113/192], loss=75.0727
	step [114/192], loss=72.5851
	step [115/192], loss=66.6739
	step [116/192], loss=77.3628
	step [117/192], loss=73.2498
	step [118/192], loss=72.4688
	step [119/192], loss=83.9594
	step [120/192], loss=80.5290
	step [121/192], loss=69.1523
	step [122/192], loss=83.5643
	step [123/192], loss=69.0067
	step [124/192], loss=59.7425
	step [125/192], loss=63.4029
	step [126/192], loss=62.5986
	step [127/192], loss=75.5854
	step [128/192], loss=70.2458
	step [129/192], loss=66.4736
	step [130/192], loss=69.5114
	step [131/192], loss=73.2285
	step [132/192], loss=91.2502
	step [133/192], loss=67.8375
	step [134/192], loss=76.7973
	step [135/192], loss=68.5020
	step [136/192], loss=69.2783
	step [137/192], loss=78.0463
	step [138/192], loss=70.6684
	step [139/192], loss=85.7067
	step [140/192], loss=68.5130
	step [141/192], loss=66.1973
	step [142/192], loss=71.3976
	step [143/192], loss=77.1657
	step [144/192], loss=69.8078
	step [145/192], loss=88.3997
	step [146/192], loss=68.6609
	step [147/192], loss=91.4549
	step [148/192], loss=77.5065
	step [149/192], loss=77.1311
	step [150/192], loss=76.5559
	step [151/192], loss=69.3032
	step [152/192], loss=64.6765
	step [153/192], loss=68.2596
	step [154/192], loss=75.2844
	step [155/192], loss=82.7725
	step [156/192], loss=73.6891
	step [157/192], loss=68.9062
	step [158/192], loss=79.1395
	step [159/192], loss=70.7914
	step [160/192], loss=66.5678
	step [161/192], loss=63.9162
	step [162/192], loss=68.2484
	step [163/192], loss=70.2432
	step [164/192], loss=74.6935
	step [165/192], loss=85.1451
	step [166/192], loss=89.4464
	step [167/192], loss=81.6971
	step [168/192], loss=67.4718
	step [169/192], loss=79.7448
	step [170/192], loss=62.1353
	step [171/192], loss=77.4829
	step [172/192], loss=74.4478
	step [173/192], loss=76.7640
	step [174/192], loss=76.6444
	step [175/192], loss=77.6173
	step [176/192], loss=73.9506
	step [177/192], loss=82.4885
	step [178/192], loss=70.4586
	step [179/192], loss=81.7842
	step [180/192], loss=74.5356
	step [181/192], loss=83.3568
	step [182/192], loss=65.3837
	step [183/192], loss=82.8161
	step [184/192], loss=78.5925
	step [185/192], loss=85.0861
	step [186/192], loss=62.8435
	step [187/192], loss=68.4941
	step [188/192], loss=76.2272
	step [189/192], loss=80.6562
	step [190/192], loss=76.3711
	step [191/192], loss=80.2242
	step [192/192], loss=65.9118
	Evaluating
	loss=0.0109, precision=0.2375, recall=0.8704, f1=0.3732
Training epoch 73
	step [1/192], loss=77.4836
	step [2/192], loss=66.1655
	step [3/192], loss=82.5391
	step [4/192], loss=81.3359
	step [5/192], loss=75.6009
	step [6/192], loss=70.6349
	step [7/192], loss=63.0571
	step [8/192], loss=66.5817
	step [9/192], loss=93.8421
	step [10/192], loss=70.5166
	step [11/192], loss=83.6352
	step [12/192], loss=79.6421
	step [13/192], loss=82.2636
	step [14/192], loss=73.9326
	step [15/192], loss=79.6230
	step [16/192], loss=77.6691
	step [17/192], loss=64.2402
	step [18/192], loss=62.8370
	step [19/192], loss=72.3988
	step [20/192], loss=84.4456
	step [21/192], loss=76.3286
	step [22/192], loss=84.5948
	step [23/192], loss=58.0123
	step [24/192], loss=74.6262
	step [25/192], loss=67.8662
	step [26/192], loss=64.3674
	step [27/192], loss=76.7160
	step [28/192], loss=64.6680
	step [29/192], loss=62.3070
	step [30/192], loss=91.7361
	step [31/192], loss=70.6842
	step [32/192], loss=73.6103
	step [33/192], loss=68.3426
	step [34/192], loss=73.9197
	step [35/192], loss=82.5011
	step [36/192], loss=73.1268
	step [37/192], loss=80.0355
	step [38/192], loss=68.4679
	step [39/192], loss=72.5777
	step [40/192], loss=69.6588
	step [41/192], loss=86.1373
	step [42/192], loss=65.0738
	step [43/192], loss=63.4527
	step [44/192], loss=71.5398
	step [45/192], loss=60.0267
	step [46/192], loss=69.5682
	step [47/192], loss=84.6147
	step [48/192], loss=76.5292
	step [49/192], loss=61.5162
	step [50/192], loss=82.6043
	step [51/192], loss=76.5073
	step [52/192], loss=72.0907
	step [53/192], loss=80.8663
	step [54/192], loss=62.8018
	step [55/192], loss=67.2204
	step [56/192], loss=66.9694
	step [57/192], loss=82.9396
	step [58/192], loss=81.7576
	step [59/192], loss=70.3258
	step [60/192], loss=78.2554
	step [61/192], loss=64.6864
	step [62/192], loss=89.0804
	step [63/192], loss=64.7912
	step [64/192], loss=65.2872
	step [65/192], loss=81.8223
	step [66/192], loss=89.6111
	step [67/192], loss=76.8169
	step [68/192], loss=73.8335
	step [69/192], loss=74.9631
	step [70/192], loss=66.0363
	step [71/192], loss=80.8853
	step [72/192], loss=74.2959
	step [73/192], loss=71.8402
	step [74/192], loss=66.5581
	step [75/192], loss=68.7556
	step [76/192], loss=79.6176
	step [77/192], loss=78.4540
	step [78/192], loss=73.9988
	step [79/192], loss=84.3664
	step [80/192], loss=62.7371
	step [81/192], loss=75.6962
	step [82/192], loss=93.9220
	step [83/192], loss=70.6667
	step [84/192], loss=77.1215
	step [85/192], loss=73.8093
	step [86/192], loss=64.1176
	step [87/192], loss=70.9332
	step [88/192], loss=78.5931
	step [89/192], loss=85.3259
	step [90/192], loss=74.4125
	step [91/192], loss=74.1352
	step [92/192], loss=73.5556
	step [93/192], loss=83.5880
	step [94/192], loss=74.6789
	step [95/192], loss=70.6297
	step [96/192], loss=64.7335
	step [97/192], loss=76.0517
	step [98/192], loss=74.7154
	step [99/192], loss=79.0795
	step [100/192], loss=73.8047
	step [101/192], loss=64.9567
	step [102/192], loss=69.7180
	step [103/192], loss=83.4434
	step [104/192], loss=76.7243
	step [105/192], loss=81.8771
	step [106/192], loss=60.9254
	step [107/192], loss=72.8928
	step [108/192], loss=75.0730
	step [109/192], loss=69.8098
	step [110/192], loss=72.1837
	step [111/192], loss=70.2804
	step [112/192], loss=68.7374
	step [113/192], loss=63.7032
	step [114/192], loss=70.7429
	step [115/192], loss=67.5886
	step [116/192], loss=71.6292
	step [117/192], loss=75.4016
	step [118/192], loss=84.6451
	step [119/192], loss=74.6989
	step [120/192], loss=79.2400
	step [121/192], loss=64.7915
	step [122/192], loss=76.5851
	step [123/192], loss=63.9250
	step [124/192], loss=76.9085
	step [125/192], loss=71.6407
	step [126/192], loss=78.6657
	step [127/192], loss=61.8223
	step [128/192], loss=81.1270
	step [129/192], loss=71.8215
	step [130/192], loss=63.6157
	step [131/192], loss=83.5642
	step [132/192], loss=67.9213
	step [133/192], loss=81.1678
	step [134/192], loss=76.0994
	step [135/192], loss=74.2903
	step [136/192], loss=58.8231
	step [137/192], loss=67.1338
	step [138/192], loss=73.6982
	step [139/192], loss=85.5028
	step [140/192], loss=70.1944
	step [141/192], loss=70.5548
	step [142/192], loss=66.1763
	step [143/192], loss=66.4901
	step [144/192], loss=65.0324
	step [145/192], loss=57.2077
	step [146/192], loss=68.9689
	step [147/192], loss=84.3934
	step [148/192], loss=70.8021
	step [149/192], loss=62.5525
	step [150/192], loss=76.1254
	step [151/192], loss=73.5229
	step [152/192], loss=73.3041
	step [153/192], loss=67.9979
	step [154/192], loss=77.0338
	step [155/192], loss=73.3919
	step [156/192], loss=62.3493
	step [157/192], loss=69.8482
	step [158/192], loss=80.3891
	step [159/192], loss=71.8920
	step [160/192], loss=82.1208
	step [161/192], loss=79.9369
	step [162/192], loss=74.1784
	step [163/192], loss=83.9538
	step [164/192], loss=84.8453
	step [165/192], loss=73.4057
	step [166/192], loss=71.5270
	step [167/192], loss=72.3969
	step [168/192], loss=74.7882
	step [169/192], loss=77.4020
	step [170/192], loss=75.8654
	step [171/192], loss=91.2512
	step [172/192], loss=77.1649
	step [173/192], loss=72.6609
	step [174/192], loss=73.0420
	step [175/192], loss=73.3871
	step [176/192], loss=66.8684
	step [177/192], loss=80.4881
	step [178/192], loss=83.6999
	step [179/192], loss=67.5527
	step [180/192], loss=68.7391
	step [181/192], loss=91.9285
	step [182/192], loss=74.4608
	step [183/192], loss=82.8679
	step [184/192], loss=67.1515
	step [185/192], loss=68.4037
	step [186/192], loss=65.7881
	step [187/192], loss=62.8407
	step [188/192], loss=68.7351
	step [189/192], loss=79.6365
	step [190/192], loss=80.4276
	step [191/192], loss=85.4573
	step [192/192], loss=70.2825
	Evaluating
	loss=0.0091, precision=0.2919, recall=0.8826, f1=0.4387
Training epoch 74
	step [1/192], loss=74.0088
	step [2/192], loss=69.7720
	step [3/192], loss=69.7232
	step [4/192], loss=81.7996
	step [5/192], loss=67.4151
	step [6/192], loss=71.4376
	step [7/192], loss=79.6875
	step [8/192], loss=64.7365
	step [9/192], loss=76.7402
	step [10/192], loss=80.0677
	step [11/192], loss=68.1135
	step [12/192], loss=71.0680
	step [13/192], loss=76.3196
	step [14/192], loss=69.9596
	step [15/192], loss=65.2258
	step [16/192], loss=80.0618
	step [17/192], loss=92.5014
	step [18/192], loss=82.8032
	step [19/192], loss=69.1162
	step [20/192], loss=72.3895
	step [21/192], loss=75.3410
	step [22/192], loss=74.6733
	step [23/192], loss=82.4801
	step [24/192], loss=66.3765
	step [25/192], loss=72.1146
	step [26/192], loss=68.4289
	step [27/192], loss=89.2692
	step [28/192], loss=84.7984
	step [29/192], loss=60.8079
	step [30/192], loss=81.1879
	step [31/192], loss=66.1187
	step [32/192], loss=75.8557
	step [33/192], loss=68.2840
	step [34/192], loss=79.3899
	step [35/192], loss=84.9582
	step [36/192], loss=76.6670
	step [37/192], loss=74.8874
	step [38/192], loss=80.9458
	step [39/192], loss=75.7239
	step [40/192], loss=79.9338
	step [41/192], loss=73.8299
	step [42/192], loss=74.7963
	step [43/192], loss=72.7077
	step [44/192], loss=74.5049
	step [45/192], loss=81.5185
	step [46/192], loss=73.0500
	step [47/192], loss=71.4466
	step [48/192], loss=75.1502
	step [49/192], loss=82.4156
	step [50/192], loss=72.4556
	step [51/192], loss=70.1339
	step [52/192], loss=60.1952
	step [53/192], loss=83.3089
	step [54/192], loss=62.7704
	step [55/192], loss=77.7949
	step [56/192], loss=75.6196
	step [57/192], loss=67.0491
	step [58/192], loss=72.1993
	step [59/192], loss=86.1506
	step [60/192], loss=72.2923
	step [61/192], loss=79.5903
	step [62/192], loss=77.0897
	step [63/192], loss=57.0251
	step [64/192], loss=83.1705
	step [65/192], loss=77.7262
	step [66/192], loss=90.9085
	step [67/192], loss=67.1158
	step [68/192], loss=79.5898
	step [69/192], loss=80.4830
	step [70/192], loss=69.8359
	step [71/192], loss=73.9392
	step [72/192], loss=70.8377
	step [73/192], loss=67.1073
	step [74/192], loss=83.5755
	step [75/192], loss=73.8417
	step [76/192], loss=67.0405
	step [77/192], loss=77.1014
	step [78/192], loss=93.2820
	step [79/192], loss=70.9294
	step [80/192], loss=67.1547
	step [81/192], loss=76.5767
	step [82/192], loss=72.0406
	step [83/192], loss=77.2800
	step [84/192], loss=67.0451
	step [85/192], loss=62.3559
	step [86/192], loss=65.6451
	step [87/192], loss=68.7522
	step [88/192], loss=76.2778
	step [89/192], loss=74.0182
	step [90/192], loss=69.4340
	step [91/192], loss=82.5084
	step [92/192], loss=66.6954
	step [93/192], loss=69.1202
	step [94/192], loss=71.8837
	step [95/192], loss=70.8502
	step [96/192], loss=71.0643
	step [97/192], loss=93.3061
	step [98/192], loss=66.3956
	step [99/192], loss=78.2720
	step [100/192], loss=70.5289
	step [101/192], loss=64.1508
	step [102/192], loss=76.9464
	step [103/192], loss=66.1129
	step [104/192], loss=70.3501
	step [105/192], loss=81.0495
	step [106/192], loss=77.4603
	step [107/192], loss=81.2007
	step [108/192], loss=72.3932
	step [109/192], loss=69.5549
	step [110/192], loss=62.7353
	step [111/192], loss=81.0387
	step [112/192], loss=59.9235
	step [113/192], loss=73.1416
	step [114/192], loss=81.7225
	step [115/192], loss=79.4919
	step [116/192], loss=82.7975
	step [117/192], loss=75.4646
	step [118/192], loss=61.5240
	step [119/192], loss=71.2425
	step [120/192], loss=71.4331
	step [121/192], loss=63.3384
	step [122/192], loss=82.4957
	step [123/192], loss=77.8491
	step [124/192], loss=55.0760
	step [125/192], loss=58.7102
	step [126/192], loss=67.8613
	step [127/192], loss=70.2166
	step [128/192], loss=69.2013
	step [129/192], loss=70.1434
	step [130/192], loss=63.7583
	step [131/192], loss=63.9477
	step [132/192], loss=75.3451
	step [133/192], loss=83.4480
	step [134/192], loss=67.0388
	step [135/192], loss=69.8785
	step [136/192], loss=77.9703
	step [137/192], loss=69.1641
	step [138/192], loss=75.4957
	step [139/192], loss=65.6010
	step [140/192], loss=73.9421
	step [141/192], loss=65.4172
	step [142/192], loss=72.0386
	step [143/192], loss=73.9628
	step [144/192], loss=71.6538
	step [145/192], loss=74.6274
	step [146/192], loss=69.0812
	step [147/192], loss=85.0831
	step [148/192], loss=69.4442
	step [149/192], loss=65.9746
	step [150/192], loss=83.9902
	step [151/192], loss=74.0504
	step [152/192], loss=78.2204
	step [153/192], loss=71.1976
	step [154/192], loss=76.2249
	step [155/192], loss=65.4203
	step [156/192], loss=82.4407
	step [157/192], loss=72.5918
	step [158/192], loss=78.4962
	step [159/192], loss=79.1065
	step [160/192], loss=80.0549
	step [161/192], loss=65.4247
	step [162/192], loss=64.4575
	step [163/192], loss=75.4783
	step [164/192], loss=80.9584
	step [165/192], loss=71.8828
	step [166/192], loss=77.8877
	step [167/192], loss=75.3391
	step [168/192], loss=74.0008
	step [169/192], loss=82.2392
	step [170/192], loss=70.4850
	step [171/192], loss=73.5313
	step [172/192], loss=74.6712
	step [173/192], loss=74.4191
	step [174/192], loss=62.5375
	step [175/192], loss=75.3245
	step [176/192], loss=71.9847
	step [177/192], loss=74.6576
	step [178/192], loss=70.0650
	step [179/192], loss=65.5425
	step [180/192], loss=75.7461
	step [181/192], loss=75.3981
	step [182/192], loss=70.1183
	step [183/192], loss=78.0602
	step [184/192], loss=60.9884
	step [185/192], loss=69.0507
	step [186/192], loss=72.9997
	step [187/192], loss=68.5610
	step [188/192], loss=71.1095
	step [189/192], loss=80.9968
	step [190/192], loss=75.9751
	step [191/192], loss=68.3854
	step [192/192], loss=47.8172
	Evaluating
	loss=0.0093, precision=0.3225, recall=0.8827, f1=0.4724
Training epoch 75
	step [1/192], loss=72.3288
	step [2/192], loss=78.5913
	step [3/192], loss=80.1669
	step [4/192], loss=79.8217
	step [5/192], loss=89.6128
	step [6/192], loss=66.7987
	step [7/192], loss=64.3150
	step [8/192], loss=88.1597
	step [9/192], loss=70.0255
	step [10/192], loss=68.0439
	step [11/192], loss=77.3189
	step [12/192], loss=68.3590
	step [13/192], loss=71.0256
	step [14/192], loss=78.6376
	step [15/192], loss=58.5667
	step [16/192], loss=72.6894
	step [17/192], loss=73.5948
	step [18/192], loss=79.6764
	step [19/192], loss=73.9305
	step [20/192], loss=70.7401
	step [21/192], loss=73.9502
	step [22/192], loss=78.8849
	step [23/192], loss=67.7733
	step [24/192], loss=74.7091
	step [25/192], loss=73.6010
	step [26/192], loss=84.2662
	step [27/192], loss=63.0941
	step [28/192], loss=73.2986
	step [29/192], loss=67.1702
	step [30/192], loss=79.6393
	step [31/192], loss=65.3115
	step [32/192], loss=75.4506
	step [33/192], loss=70.7784
	step [34/192], loss=83.4741
	step [35/192], loss=81.5184
	step [36/192], loss=73.5025
	step [37/192], loss=78.5722
	step [38/192], loss=73.8069
	step [39/192], loss=69.7812
	step [40/192], loss=64.0096
	step [41/192], loss=79.5648
	step [42/192], loss=68.6996
	step [43/192], loss=66.7602
	step [44/192], loss=73.0939
	step [45/192], loss=69.1268
	step [46/192], loss=71.4413
	step [47/192], loss=69.1475
	step [48/192], loss=69.7156
	step [49/192], loss=68.8832
	step [50/192], loss=67.0961
	step [51/192], loss=73.1924
	step [52/192], loss=78.0314
	step [53/192], loss=65.1839
	step [54/192], loss=62.8745
	step [55/192], loss=66.0677
	step [56/192], loss=74.7526
	step [57/192], loss=84.4356
	step [58/192], loss=83.7374
	step [59/192], loss=70.3186
	step [60/192], loss=81.5953
	step [61/192], loss=62.5726
	step [62/192], loss=71.4341
	step [63/192], loss=78.1992
	step [64/192], loss=68.0064
	step [65/192], loss=71.5910
	step [66/192], loss=73.7946
	step [67/192], loss=68.6336
	step [68/192], loss=80.0025
	step [69/192], loss=77.8464
	step [70/192], loss=67.8772
	step [71/192], loss=68.5240
	step [72/192], loss=70.1012
	step [73/192], loss=78.2977
	step [74/192], loss=69.4688
	step [75/192], loss=78.6494
	step [76/192], loss=72.9159
	step [77/192], loss=64.2024
	step [78/192], loss=82.8460
	step [79/192], loss=62.4352
	step [80/192], loss=82.9276
	step [81/192], loss=67.8816
	step [82/192], loss=65.4867
	step [83/192], loss=81.9866
	step [84/192], loss=74.6651
	step [85/192], loss=68.8116
	step [86/192], loss=78.6770
	step [87/192], loss=63.7577
	step [88/192], loss=71.1298
	step [89/192], loss=76.9179
	step [90/192], loss=66.8801
	step [91/192], loss=68.8452
	step [92/192], loss=68.4090
	step [93/192], loss=74.8470
	step [94/192], loss=74.3487
	step [95/192], loss=75.4949
	step [96/192], loss=78.1529
	step [97/192], loss=78.5099
	step [98/192], loss=72.4335
	step [99/192], loss=81.4861
	step [100/192], loss=68.8628
	step [101/192], loss=52.2840
	step [102/192], loss=80.2359
	step [103/192], loss=74.2391
	step [104/192], loss=67.6331
	step [105/192], loss=78.3589
	step [106/192], loss=76.1883
	step [107/192], loss=73.2752
	step [108/192], loss=76.3388
	step [109/192], loss=81.2470
	step [110/192], loss=68.2811
	step [111/192], loss=67.6323
	step [112/192], loss=78.9798
	step [113/192], loss=69.3721
	step [114/192], loss=92.4250
	step [115/192], loss=77.4057
	step [116/192], loss=73.3031
	step [117/192], loss=77.2186
	step [118/192], loss=84.9156
	step [119/192], loss=71.2122
	step [120/192], loss=65.3678
	step [121/192], loss=66.9255
	step [122/192], loss=65.3996
	step [123/192], loss=71.7073
	step [124/192], loss=78.8878
	step [125/192], loss=65.5149
	step [126/192], loss=66.7579
	step [127/192], loss=57.6653
	step [128/192], loss=63.1581
	step [129/192], loss=67.6245
	step [130/192], loss=66.2840
	step [131/192], loss=85.1411
	step [132/192], loss=72.3152
	step [133/192], loss=60.6274
	step [134/192], loss=80.2246
	step [135/192], loss=80.9845
	step [136/192], loss=68.2133
	step [137/192], loss=79.1419
	step [138/192], loss=70.8100
	step [139/192], loss=82.7407
	step [140/192], loss=66.1156
	step [141/192], loss=73.6919
	step [142/192], loss=81.7466
	step [143/192], loss=82.8213
	step [144/192], loss=76.0853
	step [145/192], loss=72.6391
	step [146/192], loss=83.8488
	step [147/192], loss=63.5298
	step [148/192], loss=80.3456
	step [149/192], loss=69.4202
	step [150/192], loss=79.8655
	step [151/192], loss=72.8394
	step [152/192], loss=86.5434
	step [153/192], loss=66.1576
	step [154/192], loss=76.6618
	step [155/192], loss=66.9590
	step [156/192], loss=72.9634
	step [157/192], loss=69.3291
	step [158/192], loss=77.0703
	step [159/192], loss=68.2051
	step [160/192], loss=64.2427
	step [161/192], loss=77.7006
	step [162/192], loss=77.9987
	step [163/192], loss=79.0142
	step [164/192], loss=80.5703
	step [165/192], loss=70.5445
	step [166/192], loss=70.9910
	step [167/192], loss=73.8970
	step [168/192], loss=84.5299
	step [169/192], loss=64.8809
	step [170/192], loss=76.4960
	step [171/192], loss=71.4614
	step [172/192], loss=84.7382
	step [173/192], loss=58.7243
	step [174/192], loss=70.7536
	step [175/192], loss=80.8277
	step [176/192], loss=82.1622
	step [177/192], loss=75.4125
	step [178/192], loss=72.1115
	step [179/192], loss=68.2092
	step [180/192], loss=73.2131
	step [181/192], loss=65.9607
	step [182/192], loss=68.2423
	step [183/192], loss=73.2416
	step [184/192], loss=72.5252
	step [185/192], loss=70.3680
	step [186/192], loss=67.3056
	step [187/192], loss=72.3618
	step [188/192], loss=82.0856
	step [189/192], loss=78.2087
	step [190/192], loss=80.9589
	step [191/192], loss=70.8071
	step [192/192], loss=74.9692
	Evaluating
	loss=0.0083, precision=0.3287, recall=0.8835, f1=0.4791
Training epoch 76
	step [1/192], loss=73.0636
	step [2/192], loss=75.0828
	step [3/192], loss=68.5387
	step [4/192], loss=73.6346
	step [5/192], loss=82.4651
	step [6/192], loss=73.5045
	step [7/192], loss=62.7437
	step [8/192], loss=67.5085
	step [9/192], loss=66.4844
	step [10/192], loss=75.6812
	step [11/192], loss=78.5652
	step [12/192], loss=76.5909
	step [13/192], loss=67.4058
	step [14/192], loss=75.0696
	step [15/192], loss=61.0286
	step [16/192], loss=77.1021
	step [17/192], loss=61.7031
	step [18/192], loss=72.1901
	step [19/192], loss=68.4600
	step [20/192], loss=69.5302
	step [21/192], loss=72.1840
	step [22/192], loss=73.1909
	step [23/192], loss=77.4230
	step [24/192], loss=74.9169
	step [25/192], loss=73.4168
	step [26/192], loss=62.7440
	step [27/192], loss=73.4275
	step [28/192], loss=78.1753
	step [29/192], loss=72.0280
	step [30/192], loss=77.4545
	step [31/192], loss=80.0111
	step [32/192], loss=80.9254
	step [33/192], loss=81.1547
	step [34/192], loss=80.7758
	step [35/192], loss=74.2328
	step [36/192], loss=73.1927
	step [37/192], loss=68.1097
	step [38/192], loss=65.3636
	step [39/192], loss=60.3496
	step [40/192], loss=63.3646
	step [41/192], loss=61.2893
	step [42/192], loss=73.9347
	step [43/192], loss=70.0378
	step [44/192], loss=80.1032
	step [45/192], loss=70.6335
	step [46/192], loss=65.5551
	step [47/192], loss=70.2085
	step [48/192], loss=71.7761
	step [49/192], loss=74.7228
	step [50/192], loss=63.5264
	step [51/192], loss=74.3176
	step [52/192], loss=68.5526
	step [53/192], loss=70.4474
	step [54/192], loss=92.2973
	step [55/192], loss=73.8483
	step [56/192], loss=65.4624
	step [57/192], loss=71.4917
	step [58/192], loss=75.3076
	step [59/192], loss=72.2691
	step [60/192], loss=68.3220
	step [61/192], loss=66.5517
	step [62/192], loss=74.7485
	step [63/192], loss=64.5181
	step [64/192], loss=78.1411
	step [65/192], loss=71.5601
	step [66/192], loss=78.6212
	step [67/192], loss=74.8564
	step [68/192], loss=66.0961
	step [69/192], loss=74.1182
	step [70/192], loss=75.8043
	step [71/192], loss=67.8725
	step [72/192], loss=76.7040
	step [73/192], loss=68.2887
	step [74/192], loss=73.7466
	step [75/192], loss=78.5144
	step [76/192], loss=65.9621
	step [77/192], loss=73.1565
	step [78/192], loss=65.2774
	step [79/192], loss=61.7223
	step [80/192], loss=84.8404
	step [81/192], loss=80.1397
	step [82/192], loss=74.8447
	step [83/192], loss=82.1970
	step [84/192], loss=66.4548
	step [85/192], loss=61.4959
	step [86/192], loss=74.6519
	step [87/192], loss=68.1635
	step [88/192], loss=66.1059
	step [89/192], loss=68.6049
	step [90/192], loss=76.4016
	step [91/192], loss=62.9283
	step [92/192], loss=75.4522
	step [93/192], loss=72.8681
	step [94/192], loss=73.7912
	step [95/192], loss=65.1115
	step [96/192], loss=81.9527
	step [97/192], loss=71.3619
	step [98/192], loss=69.1083
	step [99/192], loss=88.4212
	step [100/192], loss=72.2502
	step [101/192], loss=72.1370
	step [102/192], loss=73.8560
	step [103/192], loss=71.8548
	step [104/192], loss=68.4425
	step [105/192], loss=58.8978
	step [106/192], loss=75.3507
	step [107/192], loss=77.6541
	step [108/192], loss=77.9315
	step [109/192], loss=80.3962
	step [110/192], loss=61.7408
	step [111/192], loss=80.4411
	step [112/192], loss=66.5118
	step [113/192], loss=67.2637
	step [114/192], loss=82.1675
	step [115/192], loss=81.2881
	step [116/192], loss=82.1814
	step [117/192], loss=67.3493
	step [118/192], loss=70.8925
	step [119/192], loss=82.5489
	step [120/192], loss=72.2464
	step [121/192], loss=70.5642
	step [122/192], loss=71.8615
	step [123/192], loss=77.5656
	step [124/192], loss=75.4065
	step [125/192], loss=63.6819
	step [126/192], loss=78.2210
	step [127/192], loss=88.1230
	step [128/192], loss=76.7038
	step [129/192], loss=81.9588
	step [130/192], loss=88.6961
	step [131/192], loss=76.0213
	step [132/192], loss=88.3135
	step [133/192], loss=68.6652
	step [134/192], loss=78.0043
	step [135/192], loss=65.4516
	step [136/192], loss=64.0546
	step [137/192], loss=82.7219
	step [138/192], loss=70.5439
	step [139/192], loss=76.2390
	step [140/192], loss=74.2212
	step [141/192], loss=81.1358
	step [142/192], loss=85.1428
	step [143/192], loss=69.8749
	step [144/192], loss=76.1000
	step [145/192], loss=55.4441
	step [146/192], loss=57.7781
	step [147/192], loss=75.6183
	step [148/192], loss=90.7076
	step [149/192], loss=63.0773
	step [150/192], loss=63.4568
	step [151/192], loss=65.7134
	step [152/192], loss=75.2060
	step [153/192], loss=86.7626
	step [154/192], loss=75.3238
	step [155/192], loss=77.0131
	step [156/192], loss=61.9758
	step [157/192], loss=78.5145
	step [158/192], loss=66.7200
	step [159/192], loss=86.6234
	step [160/192], loss=64.7524
	step [161/192], loss=82.3180
	step [162/192], loss=75.6629
	step [163/192], loss=68.9265
	step [164/192], loss=69.7930
	step [165/192], loss=72.4572
	step [166/192], loss=70.4169
	step [167/192], loss=74.7201
	step [168/192], loss=75.6972
	step [169/192], loss=76.0435
	step [170/192], loss=62.4112
	step [171/192], loss=70.7057
	step [172/192], loss=70.5231
	step [173/192], loss=59.5814
	step [174/192], loss=72.4129
	step [175/192], loss=64.0992
	step [176/192], loss=74.6837
	step [177/192], loss=72.3453
	step [178/192], loss=82.4494
	step [179/192], loss=70.6753
	step [180/192], loss=82.7673
	step [181/192], loss=81.6709
	step [182/192], loss=73.1125
	step [183/192], loss=69.1475
	step [184/192], loss=74.2611
	step [185/192], loss=70.1091
	step [186/192], loss=87.3520
	step [187/192], loss=66.4567
	step [188/192], loss=84.1460
	step [189/192], loss=76.2618
	step [190/192], loss=71.6243
	step [191/192], loss=74.9929
	step [192/192], loss=63.7827
	Evaluating
	loss=0.0102, precision=0.2588, recall=0.8735, f1=0.3993
Training epoch 77
	step [1/192], loss=79.5007
	step [2/192], loss=71.6220
	step [3/192], loss=65.0705
	step [4/192], loss=76.9448
	step [5/192], loss=69.0090
	step [6/192], loss=67.7458
	step [7/192], loss=57.8944
	step [8/192], loss=66.3187
	step [9/192], loss=68.3416
	step [10/192], loss=75.4355
	step [11/192], loss=68.5969
	step [12/192], loss=77.6501
	step [13/192], loss=75.4380
	step [14/192], loss=94.2405
	step [15/192], loss=72.8537
	step [16/192], loss=74.5930
	step [17/192], loss=73.3437
	step [18/192], loss=66.2995
	step [19/192], loss=60.3638
	step [20/192], loss=72.9654
	step [21/192], loss=71.6857
	step [22/192], loss=71.7160
	step [23/192], loss=62.8014
	step [24/192], loss=68.7457
	step [25/192], loss=71.9727
	step [26/192], loss=78.0885
	step [27/192], loss=71.0952
	step [28/192], loss=66.1995
	step [29/192], loss=69.0945
	step [30/192], loss=81.3025
	step [31/192], loss=83.1306
	step [32/192], loss=81.1159
	step [33/192], loss=73.8180
	step [34/192], loss=75.2834
	step [35/192], loss=68.1242
	step [36/192], loss=64.3329
	step [37/192], loss=78.7768
	step [38/192], loss=59.7721
	step [39/192], loss=71.3366
	step [40/192], loss=62.6021
	step [41/192], loss=74.7785
	step [42/192], loss=92.6425
	step [43/192], loss=65.8600
	step [44/192], loss=68.4674
	step [45/192], loss=68.0979
	step [46/192], loss=70.8833
	step [47/192], loss=72.2457
	step [48/192], loss=65.7049
	step [49/192], loss=85.5839
	step [50/192], loss=75.7251
	step [51/192], loss=79.5985
	step [52/192], loss=76.4579
	step [53/192], loss=74.1743
	step [54/192], loss=78.2853
	step [55/192], loss=77.7949
	step [56/192], loss=69.4209
	step [57/192], loss=73.4273
	step [58/192], loss=85.1929
	step [59/192], loss=89.4798
	step [60/192], loss=65.9063
	step [61/192], loss=79.0423
	step [62/192], loss=72.5437
	step [63/192], loss=68.4239
	step [64/192], loss=61.5524
	step [65/192], loss=66.9963
	step [66/192], loss=87.9479
	step [67/192], loss=66.6604
	step [68/192], loss=87.5745
	step [69/192], loss=79.9052
	step [70/192], loss=84.5277
	step [71/192], loss=82.1721
	step [72/192], loss=62.0685
	step [73/192], loss=94.7435
	step [74/192], loss=80.0832
	step [75/192], loss=73.4209
	step [76/192], loss=69.6582
	step [77/192], loss=80.7372
	step [78/192], loss=70.5810
	step [79/192], loss=87.9634
	step [80/192], loss=80.3885
	step [81/192], loss=70.6543
	step [82/192], loss=87.6156
	step [83/192], loss=76.1682
	step [84/192], loss=73.4479
	step [85/192], loss=68.7753
	step [86/192], loss=67.8664
	step [87/192], loss=88.0716
	step [88/192], loss=63.9846
	step [89/192], loss=60.6713
	step [90/192], loss=65.2357
	step [91/192], loss=76.4568
	step [92/192], loss=87.4439
	step [93/192], loss=68.6216
	step [94/192], loss=64.0122
	step [95/192], loss=68.0595
	step [96/192], loss=85.3757
	step [97/192], loss=83.1779
	step [98/192], loss=60.6117
	step [99/192], loss=83.8616
	step [100/192], loss=61.0520
	step [101/192], loss=72.2384
	step [102/192], loss=61.3978
	step [103/192], loss=70.4628
	step [104/192], loss=81.3938
	step [105/192], loss=62.6997
	step [106/192], loss=69.5060
	step [107/192], loss=77.6814
	step [108/192], loss=69.1027
	step [109/192], loss=70.9464
	step [110/192], loss=74.8388
	step [111/192], loss=81.2047
	step [112/192], loss=77.6460
	step [113/192], loss=63.4867
	step [114/192], loss=78.7709
	step [115/192], loss=60.1471
	step [116/192], loss=65.2926
	step [117/192], loss=60.1822
	step [118/192], loss=65.5110
	step [119/192], loss=55.5567
	step [120/192], loss=80.4468
	step [121/192], loss=77.3974
	step [122/192], loss=65.3011
	step [123/192], loss=70.1603
	step [124/192], loss=74.3829
	step [125/192], loss=66.2202
	step [126/192], loss=88.0226
	step [127/192], loss=71.2064
	step [128/192], loss=88.7972
	step [129/192], loss=62.2223
	step [130/192], loss=79.7901
	step [131/192], loss=78.9382
	step [132/192], loss=75.0301
	step [133/192], loss=77.9007
	step [134/192], loss=71.4825
	step [135/192], loss=74.9520
	step [136/192], loss=70.0518
	step [137/192], loss=67.6431
	step [138/192], loss=82.3514
	step [139/192], loss=65.9962
	step [140/192], loss=66.0258
	step [141/192], loss=67.9255
	step [142/192], loss=73.6294
	step [143/192], loss=66.9777
	step [144/192], loss=79.9539
	step [145/192], loss=60.9585
	step [146/192], loss=65.8020
	step [147/192], loss=78.4483
	step [148/192], loss=69.2951
	step [149/192], loss=75.8949
	step [150/192], loss=70.9194
	step [151/192], loss=71.6874
	step [152/192], loss=82.3057
	step [153/192], loss=78.3226
	step [154/192], loss=70.1339
	step [155/192], loss=63.2852
	step [156/192], loss=70.8077
	step [157/192], loss=66.0465
	step [158/192], loss=74.0298
	step [159/192], loss=77.5046
	step [160/192], loss=69.0918
	step [161/192], loss=78.4783
	step [162/192], loss=65.3559
	step [163/192], loss=80.3633
	step [164/192], loss=79.4948
	step [165/192], loss=68.5931
	step [166/192], loss=60.6471
	step [167/192], loss=70.5540
	step [168/192], loss=76.7745
	step [169/192], loss=64.7609
	step [170/192], loss=81.6390
	step [171/192], loss=71.8322
	step [172/192], loss=75.2428
	step [173/192], loss=72.5349
	step [174/192], loss=73.1583
	step [175/192], loss=79.1684
	step [176/192], loss=62.2845
	step [177/192], loss=70.2025
	step [178/192], loss=77.8458
	step [179/192], loss=69.1178
	step [180/192], loss=74.4073
	step [181/192], loss=67.9444
	step [182/192], loss=65.8292
	step [183/192], loss=76.5484
	step [184/192], loss=62.1637
	step [185/192], loss=81.0616
	step [186/192], loss=64.1885
	step [187/192], loss=80.6071
	step [188/192], loss=66.6783
	step [189/192], loss=78.9024
	step [190/192], loss=81.4169
	step [191/192], loss=71.8858
	step [192/192], loss=61.1469
	Evaluating
	loss=0.0079, precision=0.3560, recall=0.8720, f1=0.5056
Training epoch 78
	step [1/192], loss=57.2349
	step [2/192], loss=76.9691
	step [3/192], loss=84.2419
	step [4/192], loss=73.9930
	step [5/192], loss=73.1005
	step [6/192], loss=77.6191
	step [7/192], loss=77.1210
	step [8/192], loss=78.3537
	step [9/192], loss=75.5942
	step [10/192], loss=56.8425
	step [11/192], loss=74.6130
	step [12/192], loss=74.0958
	step [13/192], loss=64.1505
	step [14/192], loss=78.0867
	step [15/192], loss=70.9935
	step [16/192], loss=73.4054
	step [17/192], loss=70.0073
	step [18/192], loss=91.6037
	step [19/192], loss=60.9838
	step [20/192], loss=70.3650
	step [21/192], loss=78.2251
	step [22/192], loss=73.7758
	step [23/192], loss=60.4686
	step [24/192], loss=67.8387
	step [25/192], loss=76.3816
	step [26/192], loss=68.6798
	step [27/192], loss=71.8863
	step [28/192], loss=63.8946
	step [29/192], loss=70.8748
	step [30/192], loss=68.6694
	step [31/192], loss=66.5539
	step [32/192], loss=73.1013
	step [33/192], loss=77.3313
	step [34/192], loss=76.7498
	step [35/192], loss=80.5235
	step [36/192], loss=69.9254
	step [37/192], loss=63.4748
	step [38/192], loss=66.6349
	step [39/192], loss=72.8133
	step [40/192], loss=75.8595
	step [41/192], loss=71.7990
	step [42/192], loss=65.7445
	step [43/192], loss=66.4376
	step [44/192], loss=76.4184
	step [45/192], loss=80.0586
	step [46/192], loss=81.9827
	step [47/192], loss=76.2463
	step [48/192], loss=86.9724
	step [49/192], loss=69.7880
	step [50/192], loss=70.6431
	step [51/192], loss=79.8419
	step [52/192], loss=72.6579
	step [53/192], loss=69.7556
	step [54/192], loss=67.1351
	step [55/192], loss=70.4279
	step [56/192], loss=64.4920
	step [57/192], loss=85.8205
	step [58/192], loss=62.1501
	step [59/192], loss=64.3580
	step [60/192], loss=62.3072
	step [61/192], loss=68.8797
	step [62/192], loss=74.9284
	step [63/192], loss=78.6217
	step [64/192], loss=75.3319
	step [65/192], loss=73.6141
	step [66/192], loss=81.1129
	step [67/192], loss=76.0652
	step [68/192], loss=66.8266
	step [69/192], loss=74.9335
	step [70/192], loss=85.2714
	step [71/192], loss=75.7738
	step [72/192], loss=72.5856
	step [73/192], loss=66.3710
	step [74/192], loss=68.0818
	step [75/192], loss=61.6740
	step [76/192], loss=71.8414
	step [77/192], loss=65.3462
	step [78/192], loss=78.1985
	step [79/192], loss=74.4298
	step [80/192], loss=69.0528
	step [81/192], loss=74.1569
	step [82/192], loss=74.7358
	step [83/192], loss=81.7786
	step [84/192], loss=74.5000
	step [85/192], loss=72.9950
	step [86/192], loss=69.9793
	step [87/192], loss=76.7409
	step [88/192], loss=51.7881
	step [89/192], loss=69.9567
	step [90/192], loss=85.5051
	step [91/192], loss=58.0406
	step [92/192], loss=62.7248
	step [93/192], loss=65.7322
	step [94/192], loss=71.3932
	step [95/192], loss=74.0690
	step [96/192], loss=67.9637
	step [97/192], loss=82.7342
	step [98/192], loss=79.8878
	step [99/192], loss=59.0342
	step [100/192], loss=65.0908
	step [101/192], loss=74.7557
	step [102/192], loss=61.0268
	step [103/192], loss=66.1092
	step [104/192], loss=65.0485
	step [105/192], loss=71.0867
	step [106/192], loss=70.5073
	step [107/192], loss=82.4740
	step [108/192], loss=72.6211
	step [109/192], loss=74.0644
	step [110/192], loss=72.5078
	step [111/192], loss=64.2584
	step [112/192], loss=74.1852
	step [113/192], loss=79.2574
	step [114/192], loss=77.9532
	step [115/192], loss=74.4449
	step [116/192], loss=83.4880
	step [117/192], loss=58.9260
	step [118/192], loss=74.2611
	step [119/192], loss=70.3033
	step [120/192], loss=77.7605
	step [121/192], loss=75.3917
	step [122/192], loss=75.1037
	step [123/192], loss=70.8378
	step [124/192], loss=65.1233
	step [125/192], loss=69.7658
	step [126/192], loss=50.0385
	step [127/192], loss=80.4307
	step [128/192], loss=79.6355
	step [129/192], loss=78.8669
	step [130/192], loss=78.1741
	step [131/192], loss=73.8951
	step [132/192], loss=74.3967
	step [133/192], loss=70.2714
	step [134/192], loss=66.9818
	step [135/192], loss=73.2298
	step [136/192], loss=65.0912
	step [137/192], loss=74.9131
	step [138/192], loss=82.9296
	step [139/192], loss=81.8136
	step [140/192], loss=79.7181
	step [141/192], loss=71.2678
	step [142/192], loss=68.5203
	step [143/192], loss=75.7814
	step [144/192], loss=70.2709
	step [145/192], loss=72.6008
	step [146/192], loss=87.7186
	step [147/192], loss=78.4870
	step [148/192], loss=82.5203
	step [149/192], loss=77.1452
	step [150/192], loss=72.8526
	step [151/192], loss=69.6467
	step [152/192], loss=73.6979
	step [153/192], loss=75.7747
	step [154/192], loss=77.1152
	step [155/192], loss=76.0221
	step [156/192], loss=63.5022
	step [157/192], loss=79.6998
	step [158/192], loss=73.3537
	step [159/192], loss=68.5019
	step [160/192], loss=86.2031
	step [161/192], loss=70.9231
	step [162/192], loss=59.0589
	step [163/192], loss=77.2407
	step [164/192], loss=77.7442
	step [165/192], loss=65.4516
	step [166/192], loss=71.4343
	step [167/192], loss=78.5537
	step [168/192], loss=72.8308
	step [169/192], loss=69.3559
	step [170/192], loss=68.7742
	step [171/192], loss=71.4371
	step [172/192], loss=76.6140
	step [173/192], loss=84.3921
	step [174/192], loss=63.6909
	step [175/192], loss=72.4364
	step [176/192], loss=68.0159
	step [177/192], loss=76.2791
	step [178/192], loss=65.8371
	step [179/192], loss=66.8836
	step [180/192], loss=69.6907
	step [181/192], loss=76.4823
	step [182/192], loss=79.4453
	step [183/192], loss=65.8381
	step [184/192], loss=87.0618
	step [185/192], loss=82.9805
	step [186/192], loss=60.6955
	step [187/192], loss=67.8784
	step [188/192], loss=82.3531
	step [189/192], loss=66.0674
	step [190/192], loss=71.8229
	step [191/192], loss=64.0754
	step [192/192], loss=53.9450
	Evaluating
	loss=0.0074, precision=0.3652, recall=0.8772, f1=0.5157
saving model as: 0_saved_model.pth
Training epoch 79
	step [1/192], loss=72.7258
	step [2/192], loss=75.6589
	step [3/192], loss=70.3411
	step [4/192], loss=61.4506
	step [5/192], loss=63.0352
	step [6/192], loss=71.3864
	step [7/192], loss=72.5225
	step [8/192], loss=70.0559
	step [9/192], loss=57.3196
	step [10/192], loss=78.3942
	step [11/192], loss=86.9000
	step [12/192], loss=70.8742
	step [13/192], loss=72.0418
	step [14/192], loss=63.6025
	step [15/192], loss=87.3188
	step [16/192], loss=52.5539
	step [17/192], loss=74.0480
	step [18/192], loss=73.9809
	step [19/192], loss=71.1933
	step [20/192], loss=64.1178
	step [21/192], loss=85.7790
	step [22/192], loss=75.7067
	step [23/192], loss=81.5162
	step [24/192], loss=62.3134
	step [25/192], loss=80.7985
	step [26/192], loss=67.5773
	step [27/192], loss=62.9743
	step [28/192], loss=75.4153
	step [29/192], loss=60.5369
	step [30/192], loss=65.1803
	step [31/192], loss=86.4924
	step [32/192], loss=79.0592
	step [33/192], loss=67.9639
	step [34/192], loss=72.4348
	step [35/192], loss=68.1739
	step [36/192], loss=73.0951
	step [37/192], loss=76.5721
	step [38/192], loss=69.1380
	step [39/192], loss=63.3585
	step [40/192], loss=63.9812
	step [41/192], loss=85.1272
	step [42/192], loss=68.3570
	step [43/192], loss=79.5338
	step [44/192], loss=68.3984
	step [45/192], loss=76.8434
	step [46/192], loss=70.8827
	step [47/192], loss=81.2900
	step [48/192], loss=77.6197
	step [49/192], loss=73.4785
	step [50/192], loss=69.7694
	step [51/192], loss=55.0643
	step [52/192], loss=76.3282
	step [53/192], loss=75.0971
	step [54/192], loss=75.3406
	step [55/192], loss=80.9816
	step [56/192], loss=72.0828
	step [57/192], loss=83.8986
	step [58/192], loss=73.0376
	step [59/192], loss=88.5901
	step [60/192], loss=65.3334
	step [61/192], loss=77.4748
	step [62/192], loss=60.3582
	step [63/192], loss=82.1047
	step [64/192], loss=74.7482
	step [65/192], loss=80.6000
	step [66/192], loss=63.6112
	step [67/192], loss=77.4429
	step [68/192], loss=66.9988
	step [69/192], loss=81.4261
	step [70/192], loss=73.5973
	step [71/192], loss=66.7781
	step [72/192], loss=61.0157
	step [73/192], loss=65.3895
	step [74/192], loss=74.9186
	step [75/192], loss=75.8360
	step [76/192], loss=74.7829
	step [77/192], loss=78.6334
	step [78/192], loss=61.9179
	step [79/192], loss=76.0710
	step [80/192], loss=72.8754
	step [81/192], loss=73.1624
	step [82/192], loss=69.2942
	step [83/192], loss=79.0726
	step [84/192], loss=73.7037
	step [85/192], loss=76.9921
	step [86/192], loss=67.7452
	step [87/192], loss=66.0131
	step [88/192], loss=84.0631
	step [89/192], loss=71.8916
	step [90/192], loss=73.9228
	step [91/192], loss=75.2081
	step [92/192], loss=75.5042
	step [93/192], loss=84.2530
	step [94/192], loss=76.2518
	step [95/192], loss=68.0612
	step [96/192], loss=60.5759
	step [97/192], loss=80.6105
	step [98/192], loss=72.9697
	step [99/192], loss=77.7274
	step [100/192], loss=80.3650
	step [101/192], loss=70.5675
	step [102/192], loss=71.6892
	step [103/192], loss=70.9673
	step [104/192], loss=69.5807
	step [105/192], loss=81.4397
	step [106/192], loss=75.5729
	step [107/192], loss=76.6864
	step [108/192], loss=62.9329
	step [109/192], loss=73.5030
	step [110/192], loss=72.3753
	step [111/192], loss=72.3136
	step [112/192], loss=70.4158
	step [113/192], loss=65.6969
	step [114/192], loss=70.7073
	step [115/192], loss=72.1380
	step [116/192], loss=73.8381
	step [117/192], loss=85.0467
	step [118/192], loss=71.6287
	step [119/192], loss=77.8383
	step [120/192], loss=70.4491
	step [121/192], loss=64.9117
	step [122/192], loss=82.2529
	step [123/192], loss=73.8465
	step [124/192], loss=71.4249
	step [125/192], loss=62.9911
	step [126/192], loss=64.4512
	step [127/192], loss=72.2669
	step [128/192], loss=67.4466
	step [129/192], loss=75.5788
	step [130/192], loss=69.9545
	step [131/192], loss=72.2253
	step [132/192], loss=68.1582
	step [133/192], loss=74.2830
	step [134/192], loss=80.0340
	step [135/192], loss=81.2917
	step [136/192], loss=71.4597
	step [137/192], loss=68.0667
	step [138/192], loss=70.1531
	step [139/192], loss=62.9607
	step [140/192], loss=69.7243
	step [141/192], loss=80.7873
	step [142/192], loss=68.3508
	step [143/192], loss=73.0211
	step [144/192], loss=72.5824
	step [145/192], loss=63.3575
	step [146/192], loss=91.8950
	step [147/192], loss=74.2296
	step [148/192], loss=69.4394
	step [149/192], loss=68.2048
	step [150/192], loss=75.6866
	step [151/192], loss=73.9848
	step [152/192], loss=77.3356
	step [153/192], loss=68.9520
	step [154/192], loss=70.5501
	step [155/192], loss=68.2003
	step [156/192], loss=79.2785
	step [157/192], loss=83.9049
	step [158/192], loss=75.8462
	step [159/192], loss=74.5529
	step [160/192], loss=78.0541
	step [161/192], loss=63.9042
	step [162/192], loss=56.4625
	step [163/192], loss=68.4129
	step [164/192], loss=83.6247
	step [165/192], loss=75.9800
	step [166/192], loss=73.2199
	step [167/192], loss=64.5219
	step [168/192], loss=61.6267
	step [169/192], loss=68.1236
	step [170/192], loss=71.0181
	step [171/192], loss=66.2252
	step [172/192], loss=82.4814
	step [173/192], loss=78.0941
	step [174/192], loss=60.8930
	step [175/192], loss=68.7433
	step [176/192], loss=68.5113
	step [177/192], loss=65.4615
	step [178/192], loss=78.3829
	step [179/192], loss=76.1105
	step [180/192], loss=82.8473
	step [181/192], loss=76.6814
	step [182/192], loss=78.7450
	step [183/192], loss=61.9673
	step [184/192], loss=70.2946
	step [185/192], loss=80.8106
	step [186/192], loss=80.7097
	step [187/192], loss=57.5842
	step [188/192], loss=65.5763
	step [189/192], loss=77.5588
	step [190/192], loss=69.2161
	step [191/192], loss=77.3021
	step [192/192], loss=65.7892
	Evaluating
	loss=0.0081, precision=0.3546, recall=0.8727, f1=0.5043
Training epoch 80
	step [1/192], loss=66.4358
	step [2/192], loss=67.9505
	step [3/192], loss=84.7842
	step [4/192], loss=79.3081
	step [5/192], loss=72.4864
	step [6/192], loss=73.2160
	step [7/192], loss=79.7530
	step [8/192], loss=66.6291
	step [9/192], loss=67.8197
	step [10/192], loss=78.3195
	step [11/192], loss=71.1940
	step [12/192], loss=67.9336
	step [13/192], loss=70.6949
	step [14/192], loss=66.7356
	step [15/192], loss=61.6068
	step [16/192], loss=72.9082
	step [17/192], loss=68.1008
	step [18/192], loss=62.4465
	step [19/192], loss=75.1376
	step [20/192], loss=85.6225
	step [21/192], loss=56.2968
	step [22/192], loss=69.5860
	step [23/192], loss=69.5342
	step [24/192], loss=82.2896
	step [25/192], loss=87.6575
	step [26/192], loss=68.3057
	step [27/192], loss=77.3086
	step [28/192], loss=68.3573
	step [29/192], loss=79.6316
	step [30/192], loss=77.2977
	step [31/192], loss=75.2408
	step [32/192], loss=83.9670
	step [33/192], loss=79.2967
	step [34/192], loss=72.0610
	step [35/192], loss=74.1664
	step [36/192], loss=85.2587
	step [37/192], loss=79.1278
	step [38/192], loss=80.0674
	step [39/192], loss=77.5406
	step [40/192], loss=75.4429
	step [41/192], loss=67.3314
	step [42/192], loss=77.5945
	step [43/192], loss=79.5054
	step [44/192], loss=78.7672
	step [45/192], loss=56.6758
	step [46/192], loss=73.2765
	step [47/192], loss=66.3340
	step [48/192], loss=80.3014
	step [49/192], loss=52.8102
	step [50/192], loss=79.7460
	step [51/192], loss=59.7410
	step [52/192], loss=50.5374
	step [53/192], loss=84.9550
	step [54/192], loss=63.2549
	step [55/192], loss=73.1050
	step [56/192], loss=69.4400
	step [57/192], loss=76.2787
	step [58/192], loss=79.5080
	step [59/192], loss=73.4804
	step [60/192], loss=71.0848
	step [61/192], loss=77.5961
	step [62/192], loss=80.5862
	step [63/192], loss=69.3622
	step [64/192], loss=73.3935
	step [65/192], loss=74.3531
	step [66/192], loss=70.7757
	step [67/192], loss=68.1948
	step [68/192], loss=63.2285
	step [69/192], loss=72.2783
	step [70/192], loss=82.6342
	step [71/192], loss=85.6932
	step [72/192], loss=69.8842
	step [73/192], loss=61.8301
	step [74/192], loss=67.9163
	step [75/192], loss=94.2326
	step [76/192], loss=69.6642
	step [77/192], loss=76.1717
	step [78/192], loss=81.1589
	step [79/192], loss=82.7825
	step [80/192], loss=67.2670
	step [81/192], loss=84.5487
	step [82/192], loss=69.4433
	step [83/192], loss=73.0277
	step [84/192], loss=72.7305
	step [85/192], loss=75.9799
	step [86/192], loss=77.2303
	step [87/192], loss=75.3453
	step [88/192], loss=61.9308
	step [89/192], loss=64.2277
	step [90/192], loss=68.9778
	step [91/192], loss=72.5470
	step [92/192], loss=67.2983
	step [93/192], loss=72.2488
	step [94/192], loss=58.4124
	step [95/192], loss=78.5724
	step [96/192], loss=80.7212
	step [97/192], loss=62.7596
	step [98/192], loss=75.2973
	step [99/192], loss=85.6493
	step [100/192], loss=76.9058
	step [101/192], loss=67.6764
	step [102/192], loss=79.4035
	step [103/192], loss=86.2879
	step [104/192], loss=82.2269
	step [105/192], loss=71.3870
	step [106/192], loss=72.8884
	step [107/192], loss=63.6615
	step [108/192], loss=74.2246
	step [109/192], loss=66.2750
	step [110/192], loss=63.9812
	step [111/192], loss=75.5007
	step [112/192], loss=65.1753
	step [113/192], loss=65.9445
	step [114/192], loss=70.3800
	step [115/192], loss=83.4227
	step [116/192], loss=69.3647
	step [117/192], loss=73.0529
	step [118/192], loss=76.9578
	step [119/192], loss=53.4783
	step [120/192], loss=66.7350
	step [121/192], loss=72.1975
	step [122/192], loss=69.5945
	step [123/192], loss=72.4219
	step [124/192], loss=68.8366
	step [125/192], loss=80.3176
	step [126/192], loss=71.5905
	step [127/192], loss=59.3585
	step [128/192], loss=73.1676
	step [129/192], loss=57.5809
	step [130/192], loss=76.7634
	step [131/192], loss=65.4809
	step [132/192], loss=70.0660
	step [133/192], loss=72.4153
	step [134/192], loss=65.6955
	step [135/192], loss=70.4149
	step [136/192], loss=78.1456
	step [137/192], loss=63.1813
	step [138/192], loss=74.7902
	step [139/192], loss=66.8989
	step [140/192], loss=65.6787
	step [141/192], loss=62.0340
	step [142/192], loss=74.5543
	step [143/192], loss=60.5567
	step [144/192], loss=59.7780
	step [145/192], loss=62.6274
	step [146/192], loss=75.8724
	step [147/192], loss=82.9120
	step [148/192], loss=82.5806
	step [149/192], loss=66.1766
	step [150/192], loss=83.5103
	step [151/192], loss=85.8535
	step [152/192], loss=68.0737
	step [153/192], loss=80.5941
	step [154/192], loss=68.0248
	step [155/192], loss=78.6055
	step [156/192], loss=76.6344
	step [157/192], loss=55.9632
	step [158/192], loss=78.8287
	step [159/192], loss=67.8980
	step [160/192], loss=82.1908
	step [161/192], loss=80.4471
	step [162/192], loss=75.0712
	step [163/192], loss=76.1049
	step [164/192], loss=76.2525
	step [165/192], loss=77.6578
	step [166/192], loss=69.5609
	step [167/192], loss=69.6835
	step [168/192], loss=66.1397
	step [169/192], loss=84.3440
	step [170/192], loss=65.6636
	step [171/192], loss=68.9958
	step [172/192], loss=63.8479
	step [173/192], loss=71.4104
	step [174/192], loss=67.7054
	step [175/192], loss=70.1624
	step [176/192], loss=65.3428
	step [177/192], loss=82.7922
	step [178/192], loss=77.0814
	step [179/192], loss=69.5900
	step [180/192], loss=89.9321
	step [181/192], loss=66.8051
	step [182/192], loss=64.5850
	step [183/192], loss=65.4965
	step [184/192], loss=64.8967
	step [185/192], loss=70.3340
	step [186/192], loss=73.1163
	step [187/192], loss=72.2115
	step [188/192], loss=68.7607
	step [189/192], loss=62.6052
	step [190/192], loss=68.8815
	step [191/192], loss=67.3000
	step [192/192], loss=68.7451
	Evaluating
	loss=0.0075, precision=0.3631, recall=0.8773, f1=0.5136
Training epoch 81
	step [1/192], loss=63.7472
	step [2/192], loss=69.3021
	step [3/192], loss=71.7825
	step [4/192], loss=50.0995
	step [5/192], loss=85.5744
	step [6/192], loss=72.7253
	step [7/192], loss=83.4979
	step [8/192], loss=55.0221
	step [9/192], loss=68.2873
	step [10/192], loss=67.0174
	step [11/192], loss=67.3079
	step [12/192], loss=74.0563
	step [13/192], loss=70.2373
	step [14/192], loss=70.1914
	step [15/192], loss=77.9414
	step [16/192], loss=70.4122
	step [17/192], loss=62.6002
	step [18/192], loss=74.4410
	step [19/192], loss=82.7704
	step [20/192], loss=71.9857
	step [21/192], loss=75.6715
	step [22/192], loss=70.3915
	step [23/192], loss=67.4740
	step [24/192], loss=62.4180
	step [25/192], loss=73.7819
	step [26/192], loss=69.0419
	step [27/192], loss=77.2159
	step [28/192], loss=67.1412
	step [29/192], loss=67.6856
	step [30/192], loss=80.2699
	step [31/192], loss=85.3921
	step [32/192], loss=66.3137
	step [33/192], loss=73.1572
	step [34/192], loss=78.2014
	step [35/192], loss=73.9713
	step [36/192], loss=70.8195
	step [37/192], loss=79.3771
	step [38/192], loss=67.9954
	step [39/192], loss=78.9197
	step [40/192], loss=77.9102
	step [41/192], loss=69.8138
	step [42/192], loss=75.5915
	step [43/192], loss=67.1391
	step [44/192], loss=80.7155
	step [45/192], loss=82.5555
	step [46/192], loss=79.0728
	step [47/192], loss=71.1868
	step [48/192], loss=67.0817
	step [49/192], loss=78.6762
	step [50/192], loss=85.4650
	step [51/192], loss=66.3020
	step [52/192], loss=62.9407
	step [53/192], loss=73.5252
	step [54/192], loss=83.9213
	step [55/192], loss=69.6373
	step [56/192], loss=64.6713
	step [57/192], loss=65.1027
	step [58/192], loss=63.1835
	step [59/192], loss=75.7631
	step [60/192], loss=69.5159
	step [61/192], loss=74.1587
	step [62/192], loss=80.6122
	step [63/192], loss=58.5287
	step [64/192], loss=60.6107
	step [65/192], loss=67.9732
	step [66/192], loss=57.3368
	step [67/192], loss=75.2588
	step [68/192], loss=73.9714
	step [69/192], loss=74.8015
	step [70/192], loss=82.9507
	step [71/192], loss=68.8743
	step [72/192], loss=70.8217
	step [73/192], loss=72.5513
	step [74/192], loss=62.1275
	step [75/192], loss=72.5852
	step [76/192], loss=73.6901
	step [77/192], loss=69.9186
	step [78/192], loss=71.4685
	step [79/192], loss=70.9429
	step [80/192], loss=75.0208
	step [81/192], loss=65.1042
	step [82/192], loss=72.2622
	step [83/192], loss=83.3830
	step [84/192], loss=63.4288
	step [85/192], loss=69.9188
	step [86/192], loss=81.2369
	step [87/192], loss=67.9145
	step [88/192], loss=74.4255
	step [89/192], loss=60.9065
	step [90/192], loss=71.4555
	step [91/192], loss=82.6926
	step [92/192], loss=79.5124
	step [93/192], loss=64.0684
	step [94/192], loss=91.7445
	step [95/192], loss=60.4156
	step [96/192], loss=69.5568
	step [97/192], loss=66.3879
	step [98/192], loss=64.8445
	step [99/192], loss=64.0379
	step [100/192], loss=70.2469
	step [101/192], loss=66.1034
	step [102/192], loss=78.1993
	step [103/192], loss=75.8419
	step [104/192], loss=78.4086
	step [105/192], loss=65.0478
	step [106/192], loss=73.4949
	step [107/192], loss=75.9198
	step [108/192], loss=77.3653
	step [109/192], loss=66.8518
	step [110/192], loss=73.7947
	step [111/192], loss=76.9923
	step [112/192], loss=76.0641
	step [113/192], loss=77.7533
	step [114/192], loss=70.8451
	step [115/192], loss=70.5616
	step [116/192], loss=79.5146
	step [117/192], loss=67.5050
	step [118/192], loss=79.3281
	step [119/192], loss=58.9473
	step [120/192], loss=78.0697
	step [121/192], loss=64.1946
	step [122/192], loss=74.1449
	step [123/192], loss=80.0869
	step [124/192], loss=85.7055
	step [125/192], loss=83.7352
	step [126/192], loss=61.3544
	step [127/192], loss=79.0965
	step [128/192], loss=71.9111
	step [129/192], loss=72.5515
	step [130/192], loss=73.1219
	step [131/192], loss=70.8496
	step [132/192], loss=76.7102
	step [133/192], loss=74.1361
	step [134/192], loss=76.1306
	step [135/192], loss=67.9831
	step [136/192], loss=78.5214
	step [137/192], loss=70.2477
	step [138/192], loss=68.0388
	step [139/192], loss=69.8612
	step [140/192], loss=81.4026
	step [141/192], loss=68.6005
	step [142/192], loss=73.4800
	step [143/192], loss=72.6831
	step [144/192], loss=82.5340
	step [145/192], loss=67.4633
	step [146/192], loss=71.1248
	step [147/192], loss=72.7811
	step [148/192], loss=75.5162
	step [149/192], loss=74.1166
	step [150/192], loss=68.4673
	step [151/192], loss=63.3381
	step [152/192], loss=79.9366
	step [153/192], loss=78.5527
	step [154/192], loss=70.0383
	step [155/192], loss=81.2454
	step [156/192], loss=80.5381
	step [157/192], loss=79.3719
	step [158/192], loss=71.1689
	step [159/192], loss=63.8276
	step [160/192], loss=60.1874
	step [161/192], loss=58.3743
	step [162/192], loss=67.7013
	step [163/192], loss=65.1114
	step [164/192], loss=69.1642
	step [165/192], loss=57.8069
	step [166/192], loss=80.9196
	step [167/192], loss=65.1571
	step [168/192], loss=79.7866
	step [169/192], loss=59.2803
	step [170/192], loss=92.1836
	step [171/192], loss=82.5328
	step [172/192], loss=68.9399
	step [173/192], loss=61.0076
	step [174/192], loss=77.2184
	step [175/192], loss=67.7923
	step [176/192], loss=60.8084
	step [177/192], loss=77.5967
	step [178/192], loss=76.9936
	step [179/192], loss=76.3973
	step [180/192], loss=69.0218
	step [181/192], loss=69.4651
	step [182/192], loss=72.4218
	step [183/192], loss=61.7014
	step [184/192], loss=66.3761
	step [185/192], loss=73.9948
	step [186/192], loss=67.0605
	step [187/192], loss=80.2732
	step [188/192], loss=76.0186
	step [189/192], loss=68.1232
	step [190/192], loss=65.2001
	step [191/192], loss=76.2153
	step [192/192], loss=67.8309
	Evaluating
	loss=0.0073, precision=0.3737, recall=0.8710, f1=0.5230
saving model as: 0_saved_model.pth
Training epoch 82
	step [1/192], loss=69.7757
	step [2/192], loss=64.9978
	step [3/192], loss=76.6274
	step [4/192], loss=83.5317
	step [5/192], loss=67.5730
	step [6/192], loss=67.3121
	step [7/192], loss=78.6195
	step [8/192], loss=74.2324
	step [9/192], loss=74.8087
	step [10/192], loss=74.3583
	step [11/192], loss=74.8252
	step [12/192], loss=81.7736
	step [13/192], loss=66.9820
	step [14/192], loss=67.0517
	step [15/192], loss=67.4887
	step [16/192], loss=65.1571
	step [17/192], loss=75.1225
	step [18/192], loss=65.5886
	step [19/192], loss=81.5096
	step [20/192], loss=76.3296
	step [21/192], loss=57.6627
	step [22/192], loss=65.1128
	step [23/192], loss=63.5819
	step [24/192], loss=72.0734
	step [25/192], loss=81.5247
	step [26/192], loss=60.5425
	step [27/192], loss=64.2409
	step [28/192], loss=55.6390
	step [29/192], loss=67.9070
	step [30/192], loss=82.5010
	step [31/192], loss=76.6499
	step [32/192], loss=73.8364
	step [33/192], loss=55.0285
	step [34/192], loss=71.5476
	step [35/192], loss=79.7673
	step [36/192], loss=64.2177
	step [37/192], loss=74.3731
	step [38/192], loss=81.7170
	step [39/192], loss=72.1386
	step [40/192], loss=86.6890
	step [41/192], loss=78.3313
	step [42/192], loss=63.2086
	step [43/192], loss=70.4850
	step [44/192], loss=75.2530
	step [45/192], loss=63.0594
	step [46/192], loss=81.7790
	step [47/192], loss=61.5057
	step [48/192], loss=76.2726
	step [49/192], loss=70.0006
	step [50/192], loss=86.8210
	step [51/192], loss=72.6546
	step [52/192], loss=58.0590
	step [53/192], loss=76.9233
	step [54/192], loss=73.1107
	step [55/192], loss=58.3354
	step [56/192], loss=66.0744
	step [57/192], loss=64.5600
	step [58/192], loss=65.3869
	step [59/192], loss=66.3455
	step [60/192], loss=74.8126
	step [61/192], loss=63.0693
	step [62/192], loss=80.4324
	step [63/192], loss=78.1083
	step [64/192], loss=72.2500
	step [65/192], loss=72.2356
	step [66/192], loss=76.6854
	step [67/192], loss=75.7082
	step [68/192], loss=71.2035
	step [69/192], loss=65.9109
	step [70/192], loss=71.1552
	step [71/192], loss=78.1172
	step [72/192], loss=88.6966
	step [73/192], loss=65.9962
	step [74/192], loss=66.8116
	step [75/192], loss=66.5274
	step [76/192], loss=66.2355
	step [77/192], loss=66.3782
	step [78/192], loss=74.3561
	step [79/192], loss=80.2644
	step [80/192], loss=87.8407
	step [81/192], loss=68.3449
	step [82/192], loss=90.1227
	step [83/192], loss=89.2358
	step [84/192], loss=75.4427
	step [85/192], loss=79.6959
	step [86/192], loss=64.7577
	step [87/192], loss=71.6999
	step [88/192], loss=76.2761
	step [89/192], loss=74.6468
	step [90/192], loss=68.5190
	step [91/192], loss=67.1691
	step [92/192], loss=66.2278
	step [93/192], loss=63.5769
	step [94/192], loss=75.2881
	step [95/192], loss=72.5983
	step [96/192], loss=78.3262
	step [97/192], loss=83.7244
	step [98/192], loss=85.4340
	step [99/192], loss=83.4408
	step [100/192], loss=70.9589
	step [101/192], loss=70.2404
	step [102/192], loss=68.9080
	step [103/192], loss=68.7176
	step [104/192], loss=75.3507
	step [105/192], loss=80.8937
	step [106/192], loss=70.9669
	step [107/192], loss=81.2160
	step [108/192], loss=61.4481
	step [109/192], loss=62.0042
	step [110/192], loss=84.0695
	step [111/192], loss=59.4718
	step [112/192], loss=67.9771
	step [113/192], loss=83.5870
	step [114/192], loss=75.1577
	step [115/192], loss=66.6840
	step [116/192], loss=72.7288
	step [117/192], loss=76.4870
	step [118/192], loss=72.7087
	step [119/192], loss=77.2853
	step [120/192], loss=74.4224
	step [121/192], loss=66.6119
	step [122/192], loss=61.0473
	step [123/192], loss=65.6694
	step [124/192], loss=69.5756
	step [125/192], loss=61.3707
	step [126/192], loss=67.9577
	step [127/192], loss=58.7429
	step [128/192], loss=74.9914
	step [129/192], loss=69.3204
	step [130/192], loss=62.0880
	step [131/192], loss=70.1996
	step [132/192], loss=81.6840
	step [133/192], loss=66.4762
	step [134/192], loss=61.0172
	step [135/192], loss=68.3757
	step [136/192], loss=73.8210
	step [137/192], loss=83.4321
	step [138/192], loss=60.4842
	step [139/192], loss=62.6613
	step [140/192], loss=85.7695
	step [141/192], loss=72.6911
	step [142/192], loss=83.1482
	step [143/192], loss=64.5670
	step [144/192], loss=67.6895
	step [145/192], loss=83.5361
	step [146/192], loss=78.1794
	step [147/192], loss=74.8458
	step [148/192], loss=70.0627
	step [149/192], loss=75.9964
	step [150/192], loss=71.0969
	step [151/192], loss=68.7018
	step [152/192], loss=65.1488
	step [153/192], loss=71.4253
	step [154/192], loss=75.1154
	step [155/192], loss=63.9699
	step [156/192], loss=63.1283
	step [157/192], loss=70.5414
	step [158/192], loss=66.6810
	step [159/192], loss=77.0953
	step [160/192], loss=73.9537
	step [161/192], loss=72.0716
	step [162/192], loss=63.6240
	step [163/192], loss=83.7689
	step [164/192], loss=82.9607
	step [165/192], loss=79.4768
	step [166/192], loss=64.4622
	step [167/192], loss=64.9328
	step [168/192], loss=75.1318
	step [169/192], loss=52.3873
	step [170/192], loss=62.2888
	step [171/192], loss=69.9786
	step [172/192], loss=71.8251
	step [173/192], loss=79.2274
	step [174/192], loss=74.1306
	step [175/192], loss=83.5186
	step [176/192], loss=67.9803
	step [177/192], loss=69.1230
	step [178/192], loss=65.9669
	step [179/192], loss=80.0725
	step [180/192], loss=62.0567
	step [181/192], loss=73.2824
	step [182/192], loss=77.1325
	step [183/192], loss=61.2954
	step [184/192], loss=74.0396
	step [185/192], loss=64.1146
	step [186/192], loss=75.5677
	step [187/192], loss=75.4983
	step [188/192], loss=88.4588
	step [189/192], loss=60.3548
	step [190/192], loss=66.5407
	step [191/192], loss=69.8988
	step [192/192], loss=61.6805
	Evaluating
	loss=0.0076, precision=0.3445, recall=0.8725, f1=0.4940
Training epoch 83
	step [1/192], loss=60.6122
	step [2/192], loss=73.1736
	step [3/192], loss=67.6769
	step [4/192], loss=64.4882
	step [5/192], loss=65.0262
	step [6/192], loss=61.7130
	step [7/192], loss=77.4541
	step [8/192], loss=85.6538
	step [9/192], loss=65.8610
	step [10/192], loss=68.1262
	step [11/192], loss=70.4143
	step [12/192], loss=70.9689
	step [13/192], loss=67.6399
	step [14/192], loss=66.4304
	step [15/192], loss=73.0980
	step [16/192], loss=78.2356
	step [17/192], loss=83.1768
	step [18/192], loss=70.6385
	step [19/192], loss=88.1617
	step [20/192], loss=70.7590
	step [21/192], loss=75.9752
	step [22/192], loss=68.1777
	step [23/192], loss=75.0275
	step [24/192], loss=84.8639
	step [25/192], loss=68.6216
	step [26/192], loss=78.0906
	step [27/192], loss=61.8801
	step [28/192], loss=68.4700
	step [29/192], loss=69.8753
	step [30/192], loss=77.3298
	step [31/192], loss=78.7821
	step [32/192], loss=78.8526
	step [33/192], loss=76.4407
	step [34/192], loss=55.1374
	step [35/192], loss=70.8068
	step [36/192], loss=85.3792
	step [37/192], loss=69.8162
	step [38/192], loss=79.1591
	step [39/192], loss=61.7148
	step [40/192], loss=70.6774
	step [41/192], loss=70.3144
	step [42/192], loss=76.2103
	step [43/192], loss=75.6356
	step [44/192], loss=73.0985
	step [45/192], loss=56.0873
	step [46/192], loss=70.4754
	step [47/192], loss=65.5609
	step [48/192], loss=67.1817
	step [49/192], loss=68.8962
	step [50/192], loss=75.6983
	step [51/192], loss=83.4954
	step [52/192], loss=69.1923
	step [53/192], loss=68.9266
	step [54/192], loss=74.9522
	step [55/192], loss=56.4524
	step [56/192], loss=78.7209
	step [57/192], loss=69.6179
	step [58/192], loss=62.0778
	step [59/192], loss=62.4137
	step [60/192], loss=65.9846
	step [61/192], loss=68.0032
	step [62/192], loss=70.2965
	step [63/192], loss=68.4060
	step [64/192], loss=74.2305
	step [65/192], loss=68.3539
	step [66/192], loss=69.1731
	step [67/192], loss=72.7670
	step [68/192], loss=70.0957
	step [69/192], loss=72.0693
	step [70/192], loss=77.9940
	step [71/192], loss=73.9026
	step [72/192], loss=71.3669
	step [73/192], loss=69.0418
	step [74/192], loss=66.2583
	step [75/192], loss=73.9166
	step [76/192], loss=88.7643
	step [77/192], loss=74.3140
	step [78/192], loss=73.2340
	step [79/192], loss=79.6494
	step [80/192], loss=69.4713
	step [81/192], loss=63.1981
	step [82/192], loss=58.1732
	step [83/192], loss=65.8807
	step [84/192], loss=80.7729
	step [85/192], loss=68.1618
	step [86/192], loss=69.2145
	step [87/192], loss=77.9780
	step [88/192], loss=74.6935
	step [89/192], loss=59.2800
	step [90/192], loss=63.0348
	step [91/192], loss=59.7574
	step [92/192], loss=74.3683
	step [93/192], loss=82.6693
	step [94/192], loss=64.8931
	step [95/192], loss=79.8967
	step [96/192], loss=76.2461
	step [97/192], loss=75.1916
	step [98/192], loss=83.5793
	step [99/192], loss=75.0884
	step [100/192], loss=85.9263
	step [101/192], loss=68.8633
	step [102/192], loss=73.1376
	step [103/192], loss=70.1082
	step [104/192], loss=62.8616
	step [105/192], loss=69.9761
	step [106/192], loss=71.2723
	step [107/192], loss=70.9473
	step [108/192], loss=77.6776
	step [109/192], loss=65.9084
	step [110/192], loss=72.7353
	step [111/192], loss=85.4249
	step [112/192], loss=67.9871
	step [113/192], loss=77.5732
	step [114/192], loss=52.4885
	step [115/192], loss=73.7534
	step [116/192], loss=62.1668
	step [117/192], loss=73.9050
	step [118/192], loss=63.7506
	step [119/192], loss=71.0874
	step [120/192], loss=81.9472
	step [121/192], loss=73.7278
	step [122/192], loss=73.3472
	step [123/192], loss=57.2736
	step [124/192], loss=62.7231
	step [125/192], loss=74.8242
	step [126/192], loss=79.5971
	step [127/192], loss=68.1042
	step [128/192], loss=66.3274
	step [129/192], loss=62.8876
	step [130/192], loss=70.9602
	step [131/192], loss=72.9409
	step [132/192], loss=72.9213
	step [133/192], loss=67.8335
	step [134/192], loss=65.1191
	step [135/192], loss=65.4608
	step [136/192], loss=74.6195
	step [137/192], loss=69.7507
	step [138/192], loss=88.2786
	step [139/192], loss=85.5357
	step [140/192], loss=64.3189
	step [141/192], loss=65.9558
	step [142/192], loss=79.6722
	step [143/192], loss=71.7055
	step [144/192], loss=74.5839
	step [145/192], loss=69.9853
	step [146/192], loss=69.9168
	step [147/192], loss=70.5332
	step [148/192], loss=64.7596
	step [149/192], loss=81.3310
	step [150/192], loss=61.4150
	step [151/192], loss=69.2082
	step [152/192], loss=69.4876
	step [153/192], loss=75.6880
	step [154/192], loss=69.4412
	step [155/192], loss=72.2078
	step [156/192], loss=69.3676
	step [157/192], loss=72.0287
	step [158/192], loss=78.7538
	step [159/192], loss=83.8646
	step [160/192], loss=75.7556
	step [161/192], loss=79.7370
	step [162/192], loss=80.7571
	step [163/192], loss=82.6588
	step [164/192], loss=69.9954
	step [165/192], loss=72.3072
	step [166/192], loss=82.0462
	step [167/192], loss=66.3474
	step [168/192], loss=76.6068
	step [169/192], loss=66.0671
	step [170/192], loss=70.6487
	step [171/192], loss=65.8058
	step [172/192], loss=67.2305
	step [173/192], loss=84.7000
	step [174/192], loss=64.2741
	step [175/192], loss=93.5989
	step [176/192], loss=70.7379
	step [177/192], loss=92.1830
	step [178/192], loss=71.2511
	step [179/192], loss=75.4325
	step [180/192], loss=75.1425
	step [181/192], loss=77.1820
	step [182/192], loss=56.1851
	step [183/192], loss=85.1178
	step [184/192], loss=71.7242
	step [185/192], loss=74.6258
	step [186/192], loss=81.2191
	step [187/192], loss=55.8358
	step [188/192], loss=64.6051
	step [189/192], loss=65.7349
	step [190/192], loss=63.2504
	step [191/192], loss=67.5451
	step [192/192], loss=59.1423
	Evaluating
	loss=0.0076, precision=0.3594, recall=0.8676, f1=0.5083
Training epoch 84
	step [1/192], loss=68.8153
	step [2/192], loss=73.8562
	step [3/192], loss=65.7590
	step [4/192], loss=68.3820
	step [5/192], loss=60.6456
	step [6/192], loss=70.4732
	step [7/192], loss=74.3129
	step [8/192], loss=66.7446
	step [9/192], loss=77.2343
	step [10/192], loss=69.3179
	step [11/192], loss=69.9557
	step [12/192], loss=78.8381
	step [13/192], loss=88.1098
	step [14/192], loss=72.2820
	step [15/192], loss=72.0457
	step [16/192], loss=70.3646
	step [17/192], loss=73.5751
	step [18/192], loss=83.5477
	step [19/192], loss=84.9355
	step [20/192], loss=76.7349
	step [21/192], loss=63.2188
	step [22/192], loss=67.1643
	step [23/192], loss=72.3474
	step [24/192], loss=78.8239
	step [25/192], loss=85.8280
	step [26/192], loss=51.1125
	step [27/192], loss=66.5709
	step [28/192], loss=64.0899
	step [29/192], loss=74.7457
	step [30/192], loss=66.0079
	step [31/192], loss=74.5948
	step [32/192], loss=65.8170
	step [33/192], loss=77.5474
	step [34/192], loss=69.6625
	step [35/192], loss=86.4865
	step [36/192], loss=66.4757
	step [37/192], loss=69.8065
	step [38/192], loss=62.7185
	step [39/192], loss=59.8828
	step [40/192], loss=67.9372
	step [41/192], loss=64.3573
	step [42/192], loss=72.3902
	step [43/192], loss=75.4006
	step [44/192], loss=70.7505
	step [45/192], loss=81.8225
	step [46/192], loss=80.0575
	step [47/192], loss=73.5974
	step [48/192], loss=70.4487
	step [49/192], loss=70.5143
	step [50/192], loss=62.2122
	step [51/192], loss=81.3980
	step [52/192], loss=66.0716
	step [53/192], loss=59.8844
	step [54/192], loss=83.1488
	step [55/192], loss=66.8203
	step [56/192], loss=71.8878
	step [57/192], loss=82.1205
	step [58/192], loss=68.8965
	step [59/192], loss=64.7309
	step [60/192], loss=69.1132
	step [61/192], loss=80.8474
	step [62/192], loss=73.9898
	step [63/192], loss=69.4721
	step [64/192], loss=74.5987
	step [65/192], loss=88.5385
	step [66/192], loss=72.9967
	step [67/192], loss=70.3474
	step [68/192], loss=81.3601
	step [69/192], loss=67.8944
	step [70/192], loss=68.2964
	step [71/192], loss=70.9352
	step [72/192], loss=78.2715
	step [73/192], loss=70.1545
	step [74/192], loss=70.7433
	step [75/192], loss=68.1868
	step [76/192], loss=74.1428
	step [77/192], loss=71.1122
	step [78/192], loss=75.1834
	step [79/192], loss=74.8404
	step [80/192], loss=76.0813
	step [81/192], loss=85.6179
	step [82/192], loss=70.8681
	step [83/192], loss=58.7608
	step [84/192], loss=78.4175
	step [85/192], loss=66.1047
	step [86/192], loss=73.1021
	step [87/192], loss=69.3555
	step [88/192], loss=77.4630
	step [89/192], loss=74.5130
	step [90/192], loss=74.6650
	step [91/192], loss=67.4291
	step [92/192], loss=56.8750
	step [93/192], loss=79.5002
	step [94/192], loss=71.0039
	step [95/192], loss=76.7275
	step [96/192], loss=88.0983
	step [97/192], loss=62.6567
	step [98/192], loss=74.2505
	step [99/192], loss=71.2961
	step [100/192], loss=75.9701
	step [101/192], loss=73.7626
	step [102/192], loss=53.3891
	step [103/192], loss=70.1260
	step [104/192], loss=62.9763
	step [105/192], loss=88.7490
	step [106/192], loss=82.3069
	step [107/192], loss=68.5221
	step [108/192], loss=66.5477
	step [109/192], loss=74.3841
	step [110/192], loss=71.2207
	step [111/192], loss=76.0960
	step [112/192], loss=82.3902
	step [113/192], loss=76.4830
	step [114/192], loss=54.3779
	step [115/192], loss=62.5623
	step [116/192], loss=57.7511
	step [117/192], loss=52.0358
	step [118/192], loss=68.6282
	step [119/192], loss=67.8705
	step [120/192], loss=71.8953
	step [121/192], loss=62.5578
	step [122/192], loss=75.0999
	step [123/192], loss=71.9192
	step [124/192], loss=67.2529
	step [125/192], loss=65.5246
	step [126/192], loss=69.0384
	step [127/192], loss=60.7804
	step [128/192], loss=77.9670
	step [129/192], loss=79.4275
	step [130/192], loss=66.2587
	step [131/192], loss=64.8839
	step [132/192], loss=77.1176
	step [133/192], loss=70.7263
	step [134/192], loss=76.3011
	step [135/192], loss=66.4069
	step [136/192], loss=89.2138
	step [137/192], loss=69.4464
	step [138/192], loss=63.6141
	step [139/192], loss=65.0934
	step [140/192], loss=71.1945
	step [141/192], loss=68.5871
	step [142/192], loss=64.5678
	step [143/192], loss=64.1084
	step [144/192], loss=83.5717
	step [145/192], loss=74.6082
	step [146/192], loss=67.8996
	step [147/192], loss=73.7747
	step [148/192], loss=83.6024
	step [149/192], loss=63.1734
	step [150/192], loss=70.2184
	step [151/192], loss=67.2140
	step [152/192], loss=71.9487
	step [153/192], loss=64.4828
	step [154/192], loss=75.5726
	step [155/192], loss=65.7585
	step [156/192], loss=64.0984
	step [157/192], loss=59.9471
	step [158/192], loss=68.8086
	step [159/192], loss=79.2293
	step [160/192], loss=61.4868
	step [161/192], loss=73.9945
	step [162/192], loss=82.2957
	step [163/192], loss=71.2328
	step [164/192], loss=65.6551
	step [165/192], loss=71.8101
	step [166/192], loss=65.2816
	step [167/192], loss=76.2726
	step [168/192], loss=70.1507
	step [169/192], loss=57.7439
	step [170/192], loss=82.8499
	step [171/192], loss=69.0318
	step [172/192], loss=67.4379
	step [173/192], loss=71.4536
	step [174/192], loss=74.2320
	step [175/192], loss=68.4617
	step [176/192], loss=82.7717
	step [177/192], loss=81.7122
	step [178/192], loss=67.8064
	step [179/192], loss=68.8906
	step [180/192], loss=67.5854
	step [181/192], loss=78.3132
	step [182/192], loss=75.0535
	step [183/192], loss=68.2623
	step [184/192], loss=79.0829
	step [185/192], loss=64.5221
	step [186/192], loss=72.5340
	step [187/192], loss=84.2780
	step [188/192], loss=74.7244
	step [189/192], loss=74.0730
	step [190/192], loss=74.9058
	step [191/192], loss=70.2883
	step [192/192], loss=57.4543
	Evaluating
	loss=0.0078, precision=0.3710, recall=0.8734, f1=0.5207
Training epoch 85
	step [1/192], loss=64.3166
	step [2/192], loss=68.9981
	step [3/192], loss=56.5698
	step [4/192], loss=77.0476
	step [5/192], loss=73.5808
	step [6/192], loss=81.3645
	step [7/192], loss=68.6819
	step [8/192], loss=64.0357
	step [9/192], loss=65.7697
	step [10/192], loss=71.0496
	step [11/192], loss=65.6523
	step [12/192], loss=67.7246
	step [13/192], loss=65.3721
	step [14/192], loss=64.8006
	step [15/192], loss=68.1159
	step [16/192], loss=66.9264
	step [17/192], loss=59.0164
	step [18/192], loss=76.5230
	step [19/192], loss=75.2998
	step [20/192], loss=68.9322
	step [21/192], loss=72.5977
	step [22/192], loss=69.7109
	step [23/192], loss=74.5367
	step [24/192], loss=67.9067
	step [25/192], loss=80.4220
	step [26/192], loss=78.0965
	step [27/192], loss=76.3496
	step [28/192], loss=88.0320
	step [29/192], loss=78.7205
	step [30/192], loss=88.5300
	step [31/192], loss=61.6957
	step [32/192], loss=70.6985
	step [33/192], loss=65.2899
	step [34/192], loss=80.6099
	step [35/192], loss=71.3179
	step [36/192], loss=73.5757
	step [37/192], loss=63.4133
	step [38/192], loss=70.1899
	step [39/192], loss=67.5274
	step [40/192], loss=60.9977
	step [41/192], loss=61.1508
	step [42/192], loss=55.5467
	step [43/192], loss=74.6687
	step [44/192], loss=71.7925
	step [45/192], loss=72.3120
	step [46/192], loss=69.4062
	step [47/192], loss=70.0324
	step [48/192], loss=64.4384
	step [49/192], loss=71.9965
	step [50/192], loss=61.1096
	step [51/192], loss=72.4852
	step [52/192], loss=71.0289
	step [53/192], loss=70.9370
	step [54/192], loss=70.1478
	step [55/192], loss=81.5223
	step [56/192], loss=69.5229
	step [57/192], loss=81.6399
	step [58/192], loss=82.9203
	step [59/192], loss=64.9235
	step [60/192], loss=76.2514
	step [61/192], loss=64.8011
	step [62/192], loss=62.6097
	step [63/192], loss=57.0879
	step [64/192], loss=76.1185
	step [65/192], loss=72.5766
	step [66/192], loss=69.0134
	step [67/192], loss=75.5622
	step [68/192], loss=76.0489
	step [69/192], loss=78.4257
	step [70/192], loss=65.4758
	step [71/192], loss=59.9985
	step [72/192], loss=81.1956
	step [73/192], loss=74.9386
	step [74/192], loss=79.3190
	step [75/192], loss=64.6113
	step [76/192], loss=55.9443
	step [77/192], loss=76.6146
	step [78/192], loss=68.9757
	step [79/192], loss=71.4140
	step [80/192], loss=79.5981
	step [81/192], loss=76.3919
	step [82/192], loss=60.3984
	step [83/192], loss=68.5258
	step [84/192], loss=75.7597
	step [85/192], loss=73.7812
	step [86/192], loss=81.6139
	step [87/192], loss=86.2330
	step [88/192], loss=67.7314
	step [89/192], loss=69.0198
	step [90/192], loss=69.2350
	step [91/192], loss=79.7847
	step [92/192], loss=58.8254
	step [93/192], loss=69.1824
	step [94/192], loss=72.4171
	step [95/192], loss=77.0526
	step [96/192], loss=73.7729
	step [97/192], loss=68.5961
	step [98/192], loss=66.1427
	step [99/192], loss=65.0573
	step [100/192], loss=65.7947
	step [101/192], loss=63.7339
	step [102/192], loss=76.1580
	step [103/192], loss=61.0753
	step [104/192], loss=75.0959
	step [105/192], loss=81.3930
	step [106/192], loss=72.0774
	step [107/192], loss=62.1161
	step [108/192], loss=74.7367
	step [109/192], loss=79.9680
	step [110/192], loss=65.7617
	step [111/192], loss=52.8281
	step [112/192], loss=75.5143
	step [113/192], loss=69.0238
	step [114/192], loss=65.3724
	step [115/192], loss=73.4300
	step [116/192], loss=72.4523
	step [117/192], loss=77.3407
	step [118/192], loss=67.0154
	step [119/192], loss=73.6083
	step [120/192], loss=62.9666
	step [121/192], loss=74.7755
	step [122/192], loss=63.3751
	step [123/192], loss=67.6068
	step [124/192], loss=68.5674
	step [125/192], loss=67.2933
	step [126/192], loss=72.9649
	step [127/192], loss=62.7625
	step [128/192], loss=74.3606
	step [129/192], loss=67.7163
	step [130/192], loss=84.6257
	step [131/192], loss=72.6258
	step [132/192], loss=57.6952
	step [133/192], loss=64.0385
	step [134/192], loss=74.8323
	step [135/192], loss=71.7997
	step [136/192], loss=73.1709
	step [137/192], loss=60.0458
	step [138/192], loss=64.9921
	step [139/192], loss=64.1150
	step [140/192], loss=75.5512
	step [141/192], loss=75.0983
	step [142/192], loss=65.8383
	step [143/192], loss=86.7902
	step [144/192], loss=70.0869
	step [145/192], loss=71.4546
	step [146/192], loss=69.9058
	step [147/192], loss=66.6858
	step [148/192], loss=68.4114
	step [149/192], loss=75.9166
	step [150/192], loss=71.7952
	step [151/192], loss=60.2161
	step [152/192], loss=77.1257
	step [153/192], loss=66.6186
	step [154/192], loss=71.9566
	step [155/192], loss=89.7698
	step [156/192], loss=88.1233
	step [157/192], loss=69.8350
	step [158/192], loss=66.9427
	step [159/192], loss=65.4203
	step [160/192], loss=74.3846
	step [161/192], loss=78.4768
	step [162/192], loss=70.9378
	step [163/192], loss=79.9685
	step [164/192], loss=66.7260
	step [165/192], loss=78.2748
	step [166/192], loss=77.7532
	step [167/192], loss=72.3300
	step [168/192], loss=58.9409
	step [169/192], loss=63.2598
	step [170/192], loss=76.9562
	step [171/192], loss=67.7760
	step [172/192], loss=68.0738
	step [173/192], loss=71.7018
	step [174/192], loss=68.4791
	step [175/192], loss=69.4619
	step [176/192], loss=73.7932
	step [177/192], loss=82.0428
	step [178/192], loss=87.5947
	step [179/192], loss=76.7649
	step [180/192], loss=74.9057
	step [181/192], loss=70.4524
	step [182/192], loss=81.0909
	step [183/192], loss=78.7380
	step [184/192], loss=68.6480
	step [185/192], loss=86.2143
	step [186/192], loss=73.9844
	step [187/192], loss=65.5599
	step [188/192], loss=70.5828
	step [189/192], loss=75.5639
	step [190/192], loss=79.5328
	step [191/192], loss=69.1361
	step [192/192], loss=78.5313
	Evaluating
	loss=0.0079, precision=0.3655, recall=0.8832, f1=0.5170
Training epoch 86
	step [1/192], loss=73.8587
	step [2/192], loss=76.9610
	step [3/192], loss=72.0325
	step [4/192], loss=67.2036
	step [5/192], loss=79.8469
	step [6/192], loss=72.1827
	step [7/192], loss=82.4609
	step [8/192], loss=72.0330
	step [9/192], loss=61.3397
	step [10/192], loss=65.7948
	step [11/192], loss=75.1135
	step [12/192], loss=65.2580
	step [13/192], loss=67.4538
	step [14/192], loss=76.6927
	step [15/192], loss=84.1079
	step [16/192], loss=81.6233
	step [17/192], loss=61.0137
	step [18/192], loss=84.3573
	step [19/192], loss=64.6782
	step [20/192], loss=61.6040
	step [21/192], loss=57.3873
	step [22/192], loss=74.6111
	step [23/192], loss=77.3596
	step [24/192], loss=70.4495
	step [25/192], loss=77.8501
	step [26/192], loss=66.3999
	step [27/192], loss=71.6209
	step [28/192], loss=66.1704
	step [29/192], loss=75.8207
	step [30/192], loss=73.5623
	step [31/192], loss=62.1900
	step [32/192], loss=66.4913
	step [33/192], loss=71.5888
	step [34/192], loss=90.8035
	step [35/192], loss=72.2534
	step [36/192], loss=67.4244
	step [37/192], loss=78.1033
	step [38/192], loss=65.6838
	step [39/192], loss=70.8482
	step [40/192], loss=60.7228
	step [41/192], loss=77.1715
	step [42/192], loss=68.2335
	step [43/192], loss=73.0173
	step [44/192], loss=66.0176
	step [45/192], loss=68.9968
	step [46/192], loss=68.9355
	step [47/192], loss=65.8665
	step [48/192], loss=80.7555
	step [49/192], loss=70.5551
	step [50/192], loss=79.5679
	step [51/192], loss=63.0655
	step [52/192], loss=60.1228
	step [53/192], loss=65.7348
	step [54/192], loss=76.3685
	step [55/192], loss=66.6372
	step [56/192], loss=65.6219
	step [57/192], loss=82.1372
	step [58/192], loss=58.6279
	step [59/192], loss=78.0269
	step [60/192], loss=72.6527
	step [61/192], loss=63.5304
	step [62/192], loss=73.2975
	step [63/192], loss=67.4091
	step [64/192], loss=85.1271
	step [65/192], loss=75.4264
	step [66/192], loss=66.5024
	step [67/192], loss=82.2863
	step [68/192], loss=80.4774
	step [69/192], loss=79.6945
	step [70/192], loss=82.3516
	step [71/192], loss=71.7286
	step [72/192], loss=65.2782
	step [73/192], loss=67.9310
	step [74/192], loss=68.5593
	step [75/192], loss=79.5094
	step [76/192], loss=78.4602
	step [77/192], loss=72.1788
	step [78/192], loss=80.7138
	step [79/192], loss=56.4464
	step [80/192], loss=78.5908
	step [81/192], loss=76.8387
	step [82/192], loss=63.6201
	step [83/192], loss=70.9536
	step [84/192], loss=65.9239
	step [85/192], loss=81.4161
	step [86/192], loss=76.0785
	step [87/192], loss=61.9681
	step [88/192], loss=69.1964
	step [89/192], loss=61.4871
	step [90/192], loss=70.8613
	step [91/192], loss=79.1742
	step [92/192], loss=71.7453
	step [93/192], loss=71.5350
	step [94/192], loss=79.5517
	step [95/192], loss=67.0892
	step [96/192], loss=75.4157
	step [97/192], loss=70.9799
	step [98/192], loss=70.5636
	step [99/192], loss=68.6273
	step [100/192], loss=74.8591
	step [101/192], loss=61.9096
	step [102/192], loss=74.6674
	step [103/192], loss=63.6201
	step [104/192], loss=69.3474
	step [105/192], loss=65.5790
	step [106/192], loss=57.0926
	step [107/192], loss=77.4267
	step [108/192], loss=66.1704
	step [109/192], loss=60.8786
	step [110/192], loss=91.9557
	step [111/192], loss=81.8969
	step [112/192], loss=74.5366
	step [113/192], loss=74.7946
	step [114/192], loss=73.0159
	step [115/192], loss=75.8184
	step [116/192], loss=84.0673
	step [117/192], loss=64.6909
	step [118/192], loss=77.3258
	step [119/192], loss=78.8755
	step [120/192], loss=58.1333
	step [121/192], loss=62.3303
	step [122/192], loss=65.6251
	step [123/192], loss=68.1543
	step [124/192], loss=75.8733
	step [125/192], loss=85.5181
	step [126/192], loss=71.9729
	step [127/192], loss=63.9337
	step [128/192], loss=68.5780
	step [129/192], loss=73.1781
	step [130/192], loss=70.6856
	step [131/192], loss=75.8497
	step [132/192], loss=63.3403
	step [133/192], loss=82.3073
	step [134/192], loss=67.5371
	step [135/192], loss=65.2108
	step [136/192], loss=65.5349
	step [137/192], loss=64.7803
	step [138/192], loss=62.3033
	step [139/192], loss=82.6509
	step [140/192], loss=84.8312
	step [141/192], loss=73.1656
	step [142/192], loss=70.4632
	step [143/192], loss=64.0062
	step [144/192], loss=67.9593
	step [145/192], loss=75.1216
	step [146/192], loss=69.4223
	step [147/192], loss=72.2640
	step [148/192], loss=73.5069
	step [149/192], loss=64.7957
	step [150/192], loss=62.8489
	step [151/192], loss=53.0374
	step [152/192], loss=65.3072
	step [153/192], loss=70.2371
	step [154/192], loss=66.8971
	step [155/192], loss=65.3104
	step [156/192], loss=79.8270
	step [157/192], loss=67.6540
	step [158/192], loss=73.1452
	step [159/192], loss=82.5018
	step [160/192], loss=80.4354
	step [161/192], loss=77.8931
	step [162/192], loss=52.2987
	step [163/192], loss=79.4772
	step [164/192], loss=63.7938
	step [165/192], loss=68.7342
	step [166/192], loss=73.0422
	step [167/192], loss=71.1088
	step [168/192], loss=69.9042
	step [169/192], loss=69.8178
	step [170/192], loss=73.8800
	step [171/192], loss=75.0988
	step [172/192], loss=55.9063
	step [173/192], loss=71.8220
	step [174/192], loss=75.2353
	step [175/192], loss=66.6377
	step [176/192], loss=73.6107
	step [177/192], loss=68.2575
	step [178/192], loss=82.4281
	step [179/192], loss=67.0026
	step [180/192], loss=66.0087
	step [181/192], loss=95.5095
	step [182/192], loss=72.2389
	step [183/192], loss=67.3240
	step [184/192], loss=83.2065
	step [185/192], loss=79.3229
	step [186/192], loss=73.7317
	step [187/192], loss=74.6004
	step [188/192], loss=63.2565
	step [189/192], loss=67.0298
	step [190/192], loss=62.2619
	step [191/192], loss=68.6211
	step [192/192], loss=64.4210
	Evaluating
	loss=0.0083, precision=0.3177, recall=0.8815, f1=0.4670
Training epoch 87
	step [1/192], loss=78.8864
	step [2/192], loss=71.9905
	step [3/192], loss=82.7700
	step [4/192], loss=66.0083
	step [5/192], loss=72.1416
	step [6/192], loss=70.3209
	step [7/192], loss=63.8214
	step [8/192], loss=66.7749
	step [9/192], loss=72.1218
	step [10/192], loss=67.3685
	step [11/192], loss=65.5523
	step [12/192], loss=65.9951
	step [13/192], loss=59.6637
	step [14/192], loss=80.2956
	step [15/192], loss=86.3614
	step [16/192], loss=79.5198
	step [17/192], loss=77.9375
	step [18/192], loss=65.1356
	step [19/192], loss=82.3326
	step [20/192], loss=68.1465
	step [21/192], loss=76.8847
	step [22/192], loss=57.1700
	step [23/192], loss=72.7737
	step [24/192], loss=70.5159
	step [25/192], loss=71.4063
	step [26/192], loss=70.6901
	step [27/192], loss=79.3055
	step [28/192], loss=68.7488
	step [29/192], loss=63.6012
	step [30/192], loss=75.7826
	step [31/192], loss=78.0531
	step [32/192], loss=74.7507
	step [33/192], loss=84.0125
	step [34/192], loss=64.4348
	step [35/192], loss=69.2560
	step [36/192], loss=78.9244
	step [37/192], loss=66.1928
	step [38/192], loss=68.3432
	step [39/192], loss=68.3052
	step [40/192], loss=68.9076
	step [41/192], loss=72.1976
	step [42/192], loss=64.4101
	step [43/192], loss=72.9933
	step [44/192], loss=61.7475
	step [45/192], loss=76.9190
	step [46/192], loss=68.3052
	step [47/192], loss=72.8242
	step [48/192], loss=67.3021
	step [49/192], loss=75.3593
	step [50/192], loss=63.2730
	step [51/192], loss=69.9961
	step [52/192], loss=68.4045
	step [53/192], loss=71.0943
	step [54/192], loss=73.6059
	step [55/192], loss=72.3710
	step [56/192], loss=77.4240
	step [57/192], loss=70.7960
	step [58/192], loss=81.5819
	step [59/192], loss=84.3180
	step [60/192], loss=60.3680
	step [61/192], loss=81.7457
	step [62/192], loss=78.0601
	step [63/192], loss=63.2596
	step [64/192], loss=70.3937
	step [65/192], loss=59.4660
	step [66/192], loss=85.1429
	step [67/192], loss=76.6474
	step [68/192], loss=73.9578
	step [69/192], loss=72.0372
	step [70/192], loss=86.2892
	step [71/192], loss=69.1215
	step [72/192], loss=74.3279
	step [73/192], loss=70.1242
	step [74/192], loss=75.8371
	step [75/192], loss=63.3532
	step [76/192], loss=70.5052
	step [77/192], loss=64.4905
	step [78/192], loss=77.5051
	step [79/192], loss=69.7584
	step [80/192], loss=59.1956
	step [81/192], loss=68.4607
	step [82/192], loss=60.7211
	step [83/192], loss=71.0117
	step [84/192], loss=65.3481
	step [85/192], loss=75.2206
	step [86/192], loss=81.0831
	step [87/192], loss=65.1522
	step [88/192], loss=59.2712
	step [89/192], loss=64.4874
	step [90/192], loss=66.1665
	step [91/192], loss=68.6188
	step [92/192], loss=68.2136
	step [93/192], loss=71.4155
	step [94/192], loss=68.5080
	step [95/192], loss=60.2118
	step [96/192], loss=76.2463
	step [97/192], loss=76.4398
	step [98/192], loss=74.1460
	step [99/192], loss=69.8804
	step [100/192], loss=57.8874
	step [101/192], loss=78.6434
	step [102/192], loss=55.9692
	step [103/192], loss=67.3228
	step [104/192], loss=77.0230
	step [105/192], loss=79.0770
	step [106/192], loss=80.0133
	step [107/192], loss=65.3703
	step [108/192], loss=74.6117
	step [109/192], loss=69.8931
	step [110/192], loss=75.3504
	step [111/192], loss=70.4293
	step [112/192], loss=74.9193
	step [113/192], loss=77.5018
	step [114/192], loss=78.3526
	step [115/192], loss=75.0426
	step [116/192], loss=63.4698
	step [117/192], loss=79.5939
	step [118/192], loss=74.8902
	step [119/192], loss=59.5554
	step [120/192], loss=78.3787
	step [121/192], loss=68.9734
	step [122/192], loss=73.5542
	step [123/192], loss=81.9462
	step [124/192], loss=70.7831
	step [125/192], loss=69.7334
	step [126/192], loss=66.1651
	step [127/192], loss=68.7242
	step [128/192], loss=64.4277
	step [129/192], loss=66.5177
	step [130/192], loss=66.4534
	step [131/192], loss=75.9574
	step [132/192], loss=66.2137
	step [133/192], loss=70.4848
	step [134/192], loss=71.9759
	step [135/192], loss=67.3456
	step [136/192], loss=82.5082
	step [137/192], loss=70.9293
	step [138/192], loss=69.0716
	step [139/192], loss=77.6295
	step [140/192], loss=64.9110
	step [141/192], loss=68.6537
	step [142/192], loss=61.9292
	step [143/192], loss=69.0294
	step [144/192], loss=75.0827
	step [145/192], loss=79.6905
	step [146/192], loss=71.3340
	step [147/192], loss=71.4501
	step [148/192], loss=71.9529
	step [149/192], loss=68.8508
	step [150/192], loss=67.2997
	step [151/192], loss=71.3493
	step [152/192], loss=77.9440
	step [153/192], loss=66.2775
	step [154/192], loss=70.3603
	step [155/192], loss=69.7299
	step [156/192], loss=67.8193
	step [157/192], loss=64.8460
	step [158/192], loss=63.8570
	step [159/192], loss=64.4691
	step [160/192], loss=72.8306
	step [161/192], loss=69.8622
	step [162/192], loss=66.8912
	step [163/192], loss=74.8966
	step [164/192], loss=68.5668
	step [165/192], loss=53.5894
	step [166/192], loss=66.0228
	step [167/192], loss=57.5037
	step [168/192], loss=66.2900
	step [169/192], loss=60.8207
	step [170/192], loss=68.0036
	step [171/192], loss=71.6602
	step [172/192], loss=77.9650
	step [173/192], loss=64.4831
	step [174/192], loss=61.8155
	step [175/192], loss=64.4407
	step [176/192], loss=73.4640
	step [177/192], loss=69.5168
	step [178/192], loss=75.8336
	step [179/192], loss=70.2318
	step [180/192], loss=71.1859
	step [181/192], loss=67.8409
	step [182/192], loss=76.9777
	step [183/192], loss=69.0268
	step [184/192], loss=81.0725
	step [185/192], loss=69.2744
	step [186/192], loss=67.3731
	step [187/192], loss=79.8396
	step [188/192], loss=74.1076
	step [189/192], loss=81.9855
	step [190/192], loss=78.6868
	step [191/192], loss=62.2382
	step [192/192], loss=68.7538
	Evaluating
	loss=0.0081, precision=0.3187, recall=0.8783, f1=0.4677
Training epoch 88
	step [1/192], loss=74.2093
	step [2/192], loss=75.4918
	step [3/192], loss=63.7707
	step [4/192], loss=81.8764
	step [5/192], loss=79.1687
	step [6/192], loss=70.6905
	step [7/192], loss=65.8804
	step [8/192], loss=61.9666
	step [9/192], loss=70.3493
	step [10/192], loss=68.3026
	step [11/192], loss=65.6345
	step [12/192], loss=56.8096
	step [13/192], loss=69.1400
	step [14/192], loss=66.4713
	step [15/192], loss=68.8096
	step [16/192], loss=78.1064
	step [17/192], loss=70.0958
	step [18/192], loss=73.0376
	step [19/192], loss=75.3322
	step [20/192], loss=69.6362
	step [21/192], loss=75.8539
	step [22/192], loss=71.8364
	step [23/192], loss=70.5093
	step [24/192], loss=58.0920
	step [25/192], loss=83.7096
	step [26/192], loss=69.4971
	step [27/192], loss=79.3084
	step [28/192], loss=78.5914
	step [29/192], loss=72.9601
	step [30/192], loss=63.6081
	step [31/192], loss=73.6158
	step [32/192], loss=81.3355
	step [33/192], loss=62.3550
	step [34/192], loss=64.0093
	step [35/192], loss=67.2799
	step [36/192], loss=66.2005
	step [37/192], loss=69.5332
	step [38/192], loss=63.8427
	step [39/192], loss=65.1753
	step [40/192], loss=73.7976
	step [41/192], loss=62.7957
	step [42/192], loss=69.0757
	step [43/192], loss=70.3162
	step [44/192], loss=73.4145
	step [45/192], loss=67.4978
	step [46/192], loss=67.2370
	step [47/192], loss=75.1080
	step [48/192], loss=71.5080
	step [49/192], loss=54.8120
	step [50/192], loss=62.2721
	step [51/192], loss=81.9644
	step [52/192], loss=80.2802
	step [53/192], loss=69.9411
	step [54/192], loss=53.6146
	step [55/192], loss=68.8145
	step [56/192], loss=79.0499
	step [57/192], loss=77.6962
	step [58/192], loss=82.9535
	step [59/192], loss=75.1911
	step [60/192], loss=63.7781
	step [61/192], loss=65.3944
	step [62/192], loss=79.1154
	step [63/192], loss=73.3878
	step [64/192], loss=63.3244
	step [65/192], loss=73.2172
	step [66/192], loss=72.0572
	step [67/192], loss=78.6693
	step [68/192], loss=69.2818
	step [69/192], loss=81.6260
	step [70/192], loss=85.8673
	step [71/192], loss=64.6608
	step [72/192], loss=79.4040
	step [73/192], loss=70.5303
	step [74/192], loss=74.0465
	step [75/192], loss=82.4249
	step [76/192], loss=75.7369
	step [77/192], loss=68.8396
	step [78/192], loss=67.9414
	step [79/192], loss=64.6495
	step [80/192], loss=76.7368
	step [81/192], loss=61.8123
	step [82/192], loss=70.5397
	step [83/192], loss=76.6171
	step [84/192], loss=70.5738
	step [85/192], loss=76.3781
	step [86/192], loss=63.2052
	step [87/192], loss=75.0341
	step [88/192], loss=69.2504
	step [89/192], loss=67.3801
	step [90/192], loss=69.0713
	step [91/192], loss=62.7523
	step [92/192], loss=79.8125
	step [93/192], loss=69.9183
	step [94/192], loss=64.3416
	step [95/192], loss=72.5045
	step [96/192], loss=66.1686
	step [97/192], loss=62.0035
	step [98/192], loss=66.9904
	step [99/192], loss=73.9939
	step [100/192], loss=82.5673
	step [101/192], loss=69.5998
	step [102/192], loss=76.0742
	step [103/192], loss=66.3873
	step [104/192], loss=67.4898
	step [105/192], loss=71.8868
	step [106/192], loss=70.1802
	step [107/192], loss=63.5810
	step [108/192], loss=75.0446
	step [109/192], loss=67.6362
	step [110/192], loss=74.3212
	step [111/192], loss=69.8595
	step [112/192], loss=61.6591
	step [113/192], loss=71.2205
	step [114/192], loss=72.4219
	step [115/192], loss=77.6030
	step [116/192], loss=71.4750
	step [117/192], loss=72.1051
	step [118/192], loss=75.5925
	step [119/192], loss=71.4590
	step [120/192], loss=84.9138
	step [121/192], loss=79.5530
	step [122/192], loss=65.5219
	step [123/192], loss=61.8675
	step [124/192], loss=68.6260
	step [125/192], loss=60.4811
	step [126/192], loss=79.7931
	step [127/192], loss=62.5099
	step [128/192], loss=70.9463
	step [129/192], loss=63.8044
	step [130/192], loss=53.9923
	step [131/192], loss=67.7867
	step [132/192], loss=76.8895
	step [133/192], loss=63.3031
	step [134/192], loss=72.3022
	step [135/192], loss=61.6982
	step [136/192], loss=71.6663
	step [137/192], loss=73.1378
	step [138/192], loss=79.5954
	step [139/192], loss=61.6361
	step [140/192], loss=64.5886
	step [141/192], loss=52.3815
	step [142/192], loss=62.4377
	step [143/192], loss=73.5937
	step [144/192], loss=56.9530
	step [145/192], loss=62.5148
	step [146/192], loss=74.7550
	step [147/192], loss=76.9146
	step [148/192], loss=66.7110
	step [149/192], loss=68.7462
	step [150/192], loss=66.6850
	step [151/192], loss=71.5913
	step [152/192], loss=75.9560
	step [153/192], loss=72.8417
	step [154/192], loss=77.0138
	step [155/192], loss=66.9186
	step [156/192], loss=68.0881
	step [157/192], loss=70.1742
	step [158/192], loss=74.0165
	step [159/192], loss=59.8475
	step [160/192], loss=82.3060
	step [161/192], loss=69.9073
	step [162/192], loss=86.0648
	step [163/192], loss=62.1070
	step [164/192], loss=68.2544
	step [165/192], loss=67.8457
	step [166/192], loss=70.3286
	step [167/192], loss=78.8993
	step [168/192], loss=69.1743
	step [169/192], loss=57.6014
	step [170/192], loss=74.1059
	step [171/192], loss=76.4579
	step [172/192], loss=87.8652
	step [173/192], loss=73.7061
	step [174/192], loss=77.8008
	step [175/192], loss=66.9223
	step [176/192], loss=66.3367
	step [177/192], loss=73.3257
	step [178/192], loss=79.6453
	step [179/192], loss=62.6363
	step [180/192], loss=63.4245
	step [181/192], loss=72.3760
	step [182/192], loss=83.6532
	step [183/192], loss=70.7009
	step [184/192], loss=68.2230
	step [185/192], loss=72.4657
	step [186/192], loss=70.3022
	step [187/192], loss=71.1595
	step [188/192], loss=71.2390
	step [189/192], loss=64.8840
	step [190/192], loss=75.8407
	step [191/192], loss=80.4287
	step [192/192], loss=49.7436
	Evaluating
	loss=0.0083, precision=0.3249, recall=0.8839, f1=0.4752
Training epoch 89
	step [1/192], loss=72.1001
	step [2/192], loss=73.9382
	step [3/192], loss=66.0597
	step [4/192], loss=74.5730
	step [5/192], loss=68.7285
	step [6/192], loss=65.5652
	step [7/192], loss=72.2590
	step [8/192], loss=68.1622
	step [9/192], loss=64.1671
	step [10/192], loss=63.4559
	step [11/192], loss=66.7150
	step [12/192], loss=71.9219
	step [13/192], loss=70.0961
	step [14/192], loss=69.5063
	step [15/192], loss=75.9712
	step [16/192], loss=70.5908
	step [17/192], loss=79.7491
	step [18/192], loss=80.3528
	step [19/192], loss=76.7733
	step [20/192], loss=71.7687
	step [21/192], loss=82.6775
	step [22/192], loss=75.3296
	step [23/192], loss=68.2432
	step [24/192], loss=70.5153
	step [25/192], loss=63.3828
	step [26/192], loss=70.6231
	step [27/192], loss=73.4642
	step [28/192], loss=69.7361
	step [29/192], loss=82.3936
	step [30/192], loss=68.5568
	step [31/192], loss=78.4397
	step [32/192], loss=58.6857
	step [33/192], loss=63.3003
	step [34/192], loss=68.5980
	step [35/192], loss=68.0232
	step [36/192], loss=72.1261
	step [37/192], loss=61.6241
	step [38/192], loss=70.7679
	step [39/192], loss=75.8777
	step [40/192], loss=72.3023
	step [41/192], loss=70.0090
	step [42/192], loss=62.1921
	step [43/192], loss=74.8114
	step [44/192], loss=64.5833
	step [45/192], loss=59.2355
	step [46/192], loss=72.2381
	step [47/192], loss=64.4905
	step [48/192], loss=60.4218
	step [49/192], loss=69.4686
	step [50/192], loss=74.9254
	step [51/192], loss=74.6105
	step [52/192], loss=93.6903
	step [53/192], loss=69.7275
	step [54/192], loss=64.2843
	step [55/192], loss=78.2278
	step [56/192], loss=80.8277
	step [57/192], loss=62.6882
	step [58/192], loss=68.1436
	step [59/192], loss=72.9909
	step [60/192], loss=71.6078
	step [61/192], loss=74.4382
	step [62/192], loss=67.2181
	step [63/192], loss=73.6827
	step [64/192], loss=65.7843
	step [65/192], loss=70.8158
	step [66/192], loss=80.7058
	step [67/192], loss=68.9328
	step [68/192], loss=75.4085
	step [69/192], loss=74.6868
	step [70/192], loss=71.9471
	step [71/192], loss=77.0643
	step [72/192], loss=76.7494
	step [73/192], loss=64.3781
	step [74/192], loss=69.1348
	step [75/192], loss=64.3337
	step [76/192], loss=61.7404
	step [77/192], loss=79.9387
	step [78/192], loss=62.9366
	step [79/192], loss=71.0025
	step [80/192], loss=60.3183
	step [81/192], loss=73.7859
	step [82/192], loss=77.0332
	step [83/192], loss=67.6205
	step [84/192], loss=70.6193
	step [85/192], loss=71.6016
	step [86/192], loss=72.1579
	step [87/192], loss=65.0210
	step [88/192], loss=76.1526
	step [89/192], loss=74.4311
	step [90/192], loss=67.6892
	step [91/192], loss=94.4834
	step [92/192], loss=62.4341
	step [93/192], loss=62.6493
	step [94/192], loss=79.8214
	step [95/192], loss=64.8444
	step [96/192], loss=79.9828
	step [97/192], loss=70.7492
	step [98/192], loss=74.5552
	step [99/192], loss=69.0320
	step [100/192], loss=70.0042
	step [101/192], loss=70.0832
	step [102/192], loss=65.4024
	step [103/192], loss=80.9178
	step [104/192], loss=68.8893
	step [105/192], loss=71.6209
	step [106/192], loss=65.8681
	step [107/192], loss=73.2072
	step [108/192], loss=59.6986
	step [109/192], loss=68.4831
	step [110/192], loss=76.7312
	step [111/192], loss=75.2966
	step [112/192], loss=63.1535
	step [113/192], loss=68.0487
	step [114/192], loss=58.7212
	step [115/192], loss=78.4088
	step [116/192], loss=64.6713
	step [117/192], loss=69.5320
	step [118/192], loss=61.9633
	step [119/192], loss=68.7668
	step [120/192], loss=78.3205
	step [121/192], loss=66.3104
	step [122/192], loss=69.0645
	step [123/192], loss=69.2495
	step [124/192], loss=65.8237
	step [125/192], loss=55.1909
	step [126/192], loss=64.3802
	step [127/192], loss=74.8594
	step [128/192], loss=62.7002
	step [129/192], loss=85.3395
	step [130/192], loss=68.3519
	step [131/192], loss=62.6299
	step [132/192], loss=61.9180
	step [133/192], loss=78.4816
	step [134/192], loss=56.3041
	step [135/192], loss=69.5755
	step [136/192], loss=76.4541
	step [137/192], loss=69.2080
	step [138/192], loss=65.3435
	step [139/192], loss=88.3405
	step [140/192], loss=71.4247
	step [141/192], loss=79.9340
	step [142/192], loss=76.0732
	step [143/192], loss=59.3258
	step [144/192], loss=56.7648
	step [145/192], loss=72.0313
	step [146/192], loss=69.1860
	step [147/192], loss=68.5288
	step [148/192], loss=75.4719
	step [149/192], loss=80.2122
	step [150/192], loss=74.1122
	step [151/192], loss=56.7059
	step [152/192], loss=71.1821
	step [153/192], loss=74.7452
	step [154/192], loss=67.7032
	step [155/192], loss=77.6208
	step [156/192], loss=77.3228
	step [157/192], loss=65.4563
	step [158/192], loss=67.1838
	step [159/192], loss=65.3362
	step [160/192], loss=76.1912
	step [161/192], loss=75.7289
	step [162/192], loss=78.0579
	step [163/192], loss=68.1888
	step [164/192], loss=75.8062
	step [165/192], loss=77.2526
	step [166/192], loss=62.8161
	step [167/192], loss=58.8793
	step [168/192], loss=66.5703
	step [169/192], loss=73.9790
	step [170/192], loss=65.8628
	step [171/192], loss=68.3949
	step [172/192], loss=58.5901
	step [173/192], loss=73.0593
	step [174/192], loss=65.3422
	step [175/192], loss=67.4582
	step [176/192], loss=87.7752
	step [177/192], loss=65.3982
	step [178/192], loss=77.7662
	step [179/192], loss=67.4399
	step [180/192], loss=82.1592
	step [181/192], loss=69.9709
	step [182/192], loss=57.4602
	step [183/192], loss=68.2997
	step [184/192], loss=64.9766
	step [185/192], loss=59.3637
	step [186/192], loss=62.7652
	step [187/192], loss=69.1277
	step [188/192], loss=61.8943
	step [189/192], loss=70.5437
	step [190/192], loss=76.5725
	step [191/192], loss=68.2665
	step [192/192], loss=67.0351
	Evaluating
	loss=0.0085, precision=0.3156, recall=0.8775, f1=0.4642
Training epoch 90
	step [1/192], loss=62.4399
	step [2/192], loss=62.7369
	step [3/192], loss=66.4373
	step [4/192], loss=75.9911
	step [5/192], loss=75.0268
	step [6/192], loss=78.8957
	step [7/192], loss=67.8783
	step [8/192], loss=73.5655
	step [9/192], loss=63.9648
	step [10/192], loss=74.3451
	step [11/192], loss=60.4993
	step [12/192], loss=76.3161
	step [13/192], loss=70.3726
	step [14/192], loss=62.0547
	step [15/192], loss=75.3506
	step [16/192], loss=68.1892
	step [17/192], loss=58.6182
	step [18/192], loss=86.9712
	step [19/192], loss=80.7943
	step [20/192], loss=64.2877
	step [21/192], loss=66.3323
	step [22/192], loss=58.8961
	step [23/192], loss=54.8079
	step [24/192], loss=73.8897
	step [25/192], loss=75.7089
	step [26/192], loss=63.6501
	step [27/192], loss=72.6339
	step [28/192], loss=85.6507
	step [29/192], loss=75.4661
	step [30/192], loss=74.1927
	step [31/192], loss=63.1968
	step [32/192], loss=68.0801
	step [33/192], loss=67.6742
	step [34/192], loss=61.1936
	step [35/192], loss=59.5784
	step [36/192], loss=77.0865
	step [37/192], loss=64.8529
	step [38/192], loss=67.5940
	step [39/192], loss=78.7390
	step [40/192], loss=77.2183
	step [41/192], loss=72.2662
	step [42/192], loss=78.7338
	step [43/192], loss=61.5847
	step [44/192], loss=67.3606
	step [45/192], loss=67.2535
	step [46/192], loss=70.5170
	step [47/192], loss=68.7258
	step [48/192], loss=62.0829
	step [49/192], loss=63.0237
	step [50/192], loss=87.2874
	step [51/192], loss=67.5862
	step [52/192], loss=64.0505
	step [53/192], loss=67.4572
	step [54/192], loss=74.6561
	step [55/192], loss=70.5685
	step [56/192], loss=58.2405
	step [57/192], loss=62.5719
	step [58/192], loss=54.0139
	step [59/192], loss=61.7439
	step [60/192], loss=78.8524
	step [61/192], loss=73.6817
	step [62/192], loss=69.7966
	step [63/192], loss=62.6374
	step [64/192], loss=62.5118
	step [65/192], loss=70.4202
	step [66/192], loss=89.0611
	step [67/192], loss=66.8030
	step [68/192], loss=66.4169
	step [69/192], loss=67.1442
	step [70/192], loss=66.9162
	step [71/192], loss=65.5823
	step [72/192], loss=70.3962
	step [73/192], loss=73.6441
	step [74/192], loss=67.6525
	step [75/192], loss=66.8805
	step [76/192], loss=66.3079
	step [77/192], loss=63.4406
	step [78/192], loss=60.3450
	step [79/192], loss=65.2748
	step [80/192], loss=75.9306
	step [81/192], loss=78.4188
	step [82/192], loss=73.7669
	step [83/192], loss=62.7743
	step [84/192], loss=58.0400
	step [85/192], loss=69.8874
	step [86/192], loss=62.7651
	step [87/192], loss=67.7398
	step [88/192], loss=61.7375
	step [89/192], loss=77.5886
	step [90/192], loss=75.9937
	step [91/192], loss=80.7883
	step [92/192], loss=77.3421
	step [93/192], loss=71.2752
	step [94/192], loss=76.2840
	step [95/192], loss=70.4714
	step [96/192], loss=69.5316
	step [97/192], loss=71.0593
	step [98/192], loss=74.4110
	step [99/192], loss=76.7501
	step [100/192], loss=74.4090
	step [101/192], loss=74.3629
	step [102/192], loss=72.7986
	step [103/192], loss=68.2149
	step [104/192], loss=73.6892
	step [105/192], loss=67.0522
	step [106/192], loss=56.9009
	step [107/192], loss=67.4481
	step [108/192], loss=80.6056
	step [109/192], loss=73.3782
	step [110/192], loss=75.1308
	step [111/192], loss=60.3905
	step [112/192], loss=72.5466
	step [113/192], loss=67.1151
	step [114/192], loss=75.1942
	step [115/192], loss=64.8321
	step [116/192], loss=51.7458
	step [117/192], loss=73.2459
	step [118/192], loss=69.8268
	step [119/192], loss=72.9130
	step [120/192], loss=66.3729
	step [121/192], loss=70.9392
	step [122/192], loss=61.0373
	step [123/192], loss=62.5677
	step [124/192], loss=88.3161
	step [125/192], loss=75.5182
	step [126/192], loss=80.8713
	step [127/192], loss=58.2525
	step [128/192], loss=71.3304
	step [129/192], loss=80.9292
	step [130/192], loss=72.2457
	step [131/192], loss=55.6730
	step [132/192], loss=66.3680
	step [133/192], loss=62.8517
	step [134/192], loss=67.5976
	step [135/192], loss=92.8698
	step [136/192], loss=77.4692
	step [137/192], loss=74.0696
	step [138/192], loss=79.2232
	step [139/192], loss=73.5355
	step [140/192], loss=67.3693
	step [141/192], loss=63.1516
	step [142/192], loss=69.4283
	step [143/192], loss=68.8560
	step [144/192], loss=70.8515
	step [145/192], loss=61.6650
	step [146/192], loss=62.4984
	step [147/192], loss=81.9207
	step [148/192], loss=72.5255
	step [149/192], loss=67.7375
	step [150/192], loss=75.1975
	step [151/192], loss=70.3094
	step [152/192], loss=66.1457
	step [153/192], loss=72.6439
	step [154/192], loss=72.3892
	step [155/192], loss=78.1538
	step [156/192], loss=70.1323
	step [157/192], loss=70.1091
	step [158/192], loss=74.0711
	step [159/192], loss=70.4182
	step [160/192], loss=74.4590
	step [161/192], loss=68.7581
	step [162/192], loss=67.6088
	step [163/192], loss=70.6400
	step [164/192], loss=75.6033
	step [165/192], loss=65.2597
	step [166/192], loss=65.0788
	step [167/192], loss=82.2394
	step [168/192], loss=68.6835
	step [169/192], loss=74.1890
	step [170/192], loss=68.8350
	step [171/192], loss=74.2457
	step [172/192], loss=75.5774
	step [173/192], loss=76.9517
	step [174/192], loss=67.5978
	step [175/192], loss=71.1578
	step [176/192], loss=78.8441
	step [177/192], loss=65.2592
	step [178/192], loss=60.5738
	step [179/192], loss=73.1522
	step [180/192], loss=62.0893
	step [181/192], loss=67.1475
	step [182/192], loss=68.0307
	step [183/192], loss=75.1060
	step [184/192], loss=77.4688
	step [185/192], loss=78.9220
	step [186/192], loss=69.5885
	step [187/192], loss=73.7660
	step [188/192], loss=62.3434
	step [189/192], loss=78.9946
	step [190/192], loss=75.9046
	step [191/192], loss=66.4628
	step [192/192], loss=64.2080
	Evaluating
	loss=0.0075, precision=0.3423, recall=0.8657, f1=0.4906
Training epoch 91
	step [1/192], loss=69.1126
	step [2/192], loss=75.4616
	step [3/192], loss=78.0456
	step [4/192], loss=72.5528
	step [5/192], loss=71.3527
	step [6/192], loss=56.8583
	step [7/192], loss=84.3890
	step [8/192], loss=70.7269
	step [9/192], loss=63.2624
	step [10/192], loss=66.8252
	step [11/192], loss=71.5775
	step [12/192], loss=75.3347
	step [13/192], loss=76.7811
	step [14/192], loss=69.4778
	step [15/192], loss=76.9383
	step [16/192], loss=72.5550
	step [17/192], loss=70.5658
	step [18/192], loss=68.2710
	step [19/192], loss=75.3515
	step [20/192], loss=67.5109
	step [21/192], loss=64.1818
	step [22/192], loss=72.6319
	step [23/192], loss=80.7536
	step [24/192], loss=66.8574
	step [25/192], loss=70.0591
	step [26/192], loss=89.4281
	step [27/192], loss=73.1718
	step [28/192], loss=69.7449
	step [29/192], loss=77.7532
	step [30/192], loss=65.4633
	step [31/192], loss=65.6745
	step [32/192], loss=67.0389
	step [33/192], loss=76.4462
	step [34/192], loss=83.6952
	step [35/192], loss=70.4021
	step [36/192], loss=59.4899
	step [37/192], loss=73.6015
	step [38/192], loss=64.4561
	step [39/192], loss=73.4020
	step [40/192], loss=70.6418
	step [41/192], loss=69.0811
	step [42/192], loss=74.8855
	step [43/192], loss=77.3082
	step [44/192], loss=73.9122
	step [45/192], loss=56.6992
	step [46/192], loss=69.1279
	step [47/192], loss=76.3724
	step [48/192], loss=74.6474
	step [49/192], loss=59.8799
	step [50/192], loss=62.0440
	step [51/192], loss=73.5396
	step [52/192], loss=73.5749
	step [53/192], loss=74.5010
	step [54/192], loss=64.0576
	step [55/192], loss=66.5585
	step [56/192], loss=52.7411
	step [57/192], loss=73.5084
	step [58/192], loss=65.6857
	step [59/192], loss=67.8491
	step [60/192], loss=66.8163
	step [61/192], loss=62.9178
	step [62/192], loss=73.2272
	step [63/192], loss=74.4730
	step [64/192], loss=76.1790
	step [65/192], loss=73.7549
	step [66/192], loss=68.8968
	step [67/192], loss=71.0744
	step [68/192], loss=84.0886
	step [69/192], loss=72.4535
	step [70/192], loss=76.9848
	step [71/192], loss=73.6370
	step [72/192], loss=68.0335
	step [73/192], loss=68.8696
	step [74/192], loss=77.2553
	step [75/192], loss=74.6234
	step [76/192], loss=63.3649
	step [77/192], loss=69.4744
	step [78/192], loss=67.6495
	step [79/192], loss=61.1639
	step [80/192], loss=66.5493
	step [81/192], loss=73.1192
	step [82/192], loss=66.8358
	step [83/192], loss=60.1491
	step [84/192], loss=67.6833
	step [85/192], loss=72.8708
	step [86/192], loss=75.1978
	step [87/192], loss=81.3458
	step [88/192], loss=60.3848
	step [89/192], loss=72.6415
	step [90/192], loss=63.0574
	step [91/192], loss=68.3976
	step [92/192], loss=62.7071
	step [93/192], loss=84.4893
	step [94/192], loss=58.0344
	step [95/192], loss=69.6459
	step [96/192], loss=65.5095
	step [97/192], loss=68.4236
	step [98/192], loss=87.2629
	step [99/192], loss=63.7273
	step [100/192], loss=66.9593
	step [101/192], loss=66.7602
	step [102/192], loss=75.5542
	step [103/192], loss=77.2451
	step [104/192], loss=72.5593
	step [105/192], loss=53.7670
	step [106/192], loss=67.9118
	step [107/192], loss=67.5448
	step [108/192], loss=75.8468
	step [109/192], loss=72.1424
	step [110/192], loss=74.3023
	step [111/192], loss=71.9556
	step [112/192], loss=64.1014
	step [113/192], loss=71.4052
	step [114/192], loss=67.4903
	step [115/192], loss=89.8250
	step [116/192], loss=65.6354
	step [117/192], loss=72.6596
	step [118/192], loss=61.3088
	step [119/192], loss=71.2761
	step [120/192], loss=64.3076
	step [121/192], loss=53.6288
	step [122/192], loss=60.9671
	step [123/192], loss=84.9787
	step [124/192], loss=69.2113
	step [125/192], loss=65.8610
	step [126/192], loss=65.0841
	step [127/192], loss=64.7388
	step [128/192], loss=72.1369
	step [129/192], loss=67.2408
	step [130/192], loss=68.3500
	step [131/192], loss=67.0894
	step [132/192], loss=81.8149
	step [133/192], loss=64.4945
	step [134/192], loss=72.8352
	step [135/192], loss=71.7514
	step [136/192], loss=64.2827
	step [137/192], loss=78.3712
	step [138/192], loss=84.0489
	step [139/192], loss=65.0435
	step [140/192], loss=65.1082
	step [141/192], loss=67.8477
	step [142/192], loss=65.3889
	step [143/192], loss=69.3199
	step [144/192], loss=61.9270
	step [145/192], loss=71.4206
	step [146/192], loss=67.6961
	step [147/192], loss=74.5438
	step [148/192], loss=51.7021
	step [149/192], loss=74.9591
	step [150/192], loss=74.2299
	step [151/192], loss=66.5176
	step [152/192], loss=72.3274
	step [153/192], loss=66.7800
	step [154/192], loss=56.1335
	step [155/192], loss=61.6694
	step [156/192], loss=65.4613
	step [157/192], loss=77.8444
	step [158/192], loss=67.9296
	step [159/192], loss=71.7496
	step [160/192], loss=75.3316
	step [161/192], loss=80.9216
	step [162/192], loss=60.9286
	step [163/192], loss=74.3784
	step [164/192], loss=67.1513
	step [165/192], loss=70.2553
	step [166/192], loss=69.7725
	step [167/192], loss=63.1818
	step [168/192], loss=74.7260
	step [169/192], loss=60.6867
	step [170/192], loss=52.8104
	step [171/192], loss=63.5711
	step [172/192], loss=59.8183
	step [173/192], loss=68.8508
	step [174/192], loss=65.9662
	step [175/192], loss=66.6603
	step [176/192], loss=70.5490
	step [177/192], loss=70.2524
	step [178/192], loss=67.1352
	step [179/192], loss=66.3111
	step [180/192], loss=67.2738
	step [181/192], loss=76.4181
	step [182/192], loss=92.6823
	step [183/192], loss=70.9363
	step [184/192], loss=64.8214
	step [185/192], loss=58.2068
	step [186/192], loss=67.6796
	step [187/192], loss=74.7921
	step [188/192], loss=92.2365
	step [189/192], loss=64.0369
	step [190/192], loss=78.3648
	step [191/192], loss=73.7382
	step [192/192], loss=67.0192
	Evaluating
	loss=0.0077, precision=0.3388, recall=0.8694, f1=0.4876
Training epoch 92
	step [1/192], loss=80.6946
	step [2/192], loss=74.3760
	step [3/192], loss=64.6149
	step [4/192], loss=74.7159
	step [5/192], loss=74.4083
	step [6/192], loss=62.2428
	step [7/192], loss=65.7087
	step [8/192], loss=74.5687
	step [9/192], loss=64.8882
	step [10/192], loss=73.0384
	step [11/192], loss=73.1707
	step [12/192], loss=64.9377
	step [13/192], loss=73.8242
	step [14/192], loss=60.6368
	step [15/192], loss=73.8784
	step [16/192], loss=79.4691
	step [17/192], loss=62.4352
	step [18/192], loss=61.5566
	step [19/192], loss=73.9109
	step [20/192], loss=66.9628
	step [21/192], loss=78.9117
	step [22/192], loss=76.4565
	step [23/192], loss=84.2794
	step [24/192], loss=80.5362
	step [25/192], loss=72.4445
	step [26/192], loss=57.2495
	step [27/192], loss=78.5271
	step [28/192], loss=83.4702
	step [29/192], loss=68.9809
	step [30/192], loss=65.4987
	step [31/192], loss=76.3713
	step [32/192], loss=69.4785
	step [33/192], loss=53.1526
	step [34/192], loss=59.7935
	step [35/192], loss=72.3205
	step [36/192], loss=80.7550
	step [37/192], loss=71.3669
	step [38/192], loss=70.0548
	step [39/192], loss=65.2426
	step [40/192], loss=75.1327
	step [41/192], loss=61.8433
	step [42/192], loss=72.8390
	step [43/192], loss=80.0277
	step [44/192], loss=61.3258
	step [45/192], loss=63.8881
	step [46/192], loss=61.3407
	step [47/192], loss=68.5211
	step [48/192], loss=84.7472
	step [49/192], loss=70.9257
	step [50/192], loss=72.8374
	step [51/192], loss=75.6937
	step [52/192], loss=68.2732
	step [53/192], loss=64.8568
	step [54/192], loss=56.7415
	step [55/192], loss=64.9108
	step [56/192], loss=68.6139
	step [57/192], loss=73.1590
	step [58/192], loss=74.3835
	step [59/192], loss=70.2497
	step [60/192], loss=68.0246
	step [61/192], loss=68.8910
	step [62/192], loss=75.8182
	step [63/192], loss=61.7231
	step [64/192], loss=72.1258
	step [65/192], loss=67.2710
	step [66/192], loss=74.3029
	step [67/192], loss=79.0080
	step [68/192], loss=77.0370
	step [69/192], loss=78.0195
	step [70/192], loss=63.4422
	step [71/192], loss=81.8193
	step [72/192], loss=70.4048
	step [73/192], loss=85.3125
	step [74/192], loss=74.0887
	step [75/192], loss=60.5973
	step [76/192], loss=62.3297
	step [77/192], loss=65.0243
	step [78/192], loss=63.9081
	step [79/192], loss=67.4825
	step [80/192], loss=73.2432
	step [81/192], loss=73.5570
	step [82/192], loss=74.2935
	step [83/192], loss=71.7096
	step [84/192], loss=68.5801
	step [85/192], loss=74.2366
	step [86/192], loss=74.2289
	step [87/192], loss=63.5108
	step [88/192], loss=59.6274
	step [89/192], loss=69.0952
	step [90/192], loss=70.6410
	step [91/192], loss=79.3108
	step [92/192], loss=61.2997
	step [93/192], loss=61.3847
	step [94/192], loss=84.0912
	step [95/192], loss=57.5184
	step [96/192], loss=80.4031
	step [97/192], loss=67.6309
	step [98/192], loss=79.0202
	step [99/192], loss=57.1416
	step [100/192], loss=79.5254
	step [101/192], loss=59.8937
	step [102/192], loss=61.5017
	step [103/192], loss=69.8019
	step [104/192], loss=68.4543
	step [105/192], loss=77.2203
	step [106/192], loss=71.7959
	step [107/192], loss=62.7722
	step [108/192], loss=68.1818
	step [109/192], loss=73.0094
	step [110/192], loss=65.3824
	step [111/192], loss=60.5235
	step [112/192], loss=83.6531
	step [113/192], loss=69.5563
	step [114/192], loss=55.2129
	step [115/192], loss=75.5662
	step [116/192], loss=74.1886
	step [117/192], loss=71.3901
	step [118/192], loss=69.2349
	step [119/192], loss=68.7353
	step [120/192], loss=64.2296
	step [121/192], loss=83.6813
	step [122/192], loss=71.0216
	step [123/192], loss=61.9360
	step [124/192], loss=63.3874
	step [125/192], loss=74.1013
	step [126/192], loss=71.5859
	step [127/192], loss=55.2306
	step [128/192], loss=71.6341
	step [129/192], loss=60.6872
	step [130/192], loss=71.6419
	step [131/192], loss=58.8284
	step [132/192], loss=84.2564
	step [133/192], loss=66.5295
	step [134/192], loss=70.1866
	step [135/192], loss=70.7752
	step [136/192], loss=54.9020
	step [137/192], loss=89.1384
	step [138/192], loss=72.7076
	step [139/192], loss=59.5124
	step [140/192], loss=63.6500
	step [141/192], loss=57.5319
	step [142/192], loss=75.1119
	step [143/192], loss=73.8885
	step [144/192], loss=64.2921
	step [145/192], loss=68.5893
	step [146/192], loss=68.1785
	step [147/192], loss=55.6165
	step [148/192], loss=78.1681
	step [149/192], loss=77.5408
	step [150/192], loss=68.8456
	step [151/192], loss=69.8298
	step [152/192], loss=76.3309
	step [153/192], loss=72.6587
	step [154/192], loss=70.0572
	step [155/192], loss=64.2219
	step [156/192], loss=64.9034
	step [157/192], loss=61.9627
	step [158/192], loss=79.6739
	step [159/192], loss=78.4639
	step [160/192], loss=75.5052
	step [161/192], loss=74.9976
	step [162/192], loss=65.5297
	step [163/192], loss=57.9904
	step [164/192], loss=71.0747
	step [165/192], loss=90.5621
	step [166/192], loss=68.3914
	step [167/192], loss=70.3362
	step [168/192], loss=72.1611
	step [169/192], loss=63.4875
	step [170/192], loss=66.1947
	step [171/192], loss=64.4919
	step [172/192], loss=72.1560
	step [173/192], loss=71.9950
	step [174/192], loss=70.5216
	step [175/192], loss=67.4337
	step [176/192], loss=68.6027
	step [177/192], loss=66.3256
	step [178/192], loss=71.9369
	step [179/192], loss=69.0936
	step [180/192], loss=78.3257
	step [181/192], loss=70.5879
	step [182/192], loss=74.1292
	step [183/192], loss=74.1981
	step [184/192], loss=68.4206
	step [185/192], loss=54.8371
	step [186/192], loss=53.9783
	step [187/192], loss=55.0602
	step [188/192], loss=69.0751
	step [189/192], loss=60.1783
	step [190/192], loss=74.7294
	step [191/192], loss=73.5651
	step [192/192], loss=57.1986
	Evaluating
	loss=0.0077, precision=0.3430, recall=0.8830, f1=0.4940
Training epoch 93
	step [1/192], loss=67.1983
	step [2/192], loss=74.5781
	step [3/192], loss=71.9667
	step [4/192], loss=78.0565
	step [5/192], loss=65.6440
	step [6/192], loss=90.5129
	step [7/192], loss=61.9027
	step [8/192], loss=70.9253
	step [9/192], loss=81.8760
	step [10/192], loss=65.1924
	step [11/192], loss=69.4677
	step [12/192], loss=67.3998
	step [13/192], loss=66.9180
	step [14/192], loss=63.1779
	step [15/192], loss=68.2308
	step [16/192], loss=76.0729
	step [17/192], loss=75.8592
	step [18/192], loss=71.6019
	step [19/192], loss=74.7358
	step [20/192], loss=67.4014
	step [21/192], loss=67.9138
	step [22/192], loss=68.3185
	step [23/192], loss=65.6944
	step [24/192], loss=71.1893
	step [25/192], loss=72.5267
	step [26/192], loss=66.1412
	step [27/192], loss=68.0044
	step [28/192], loss=72.7778
	step [29/192], loss=64.7288
	step [30/192], loss=72.8983
	step [31/192], loss=76.0148
	step [32/192], loss=80.4105
	step [33/192], loss=74.3368
	step [34/192], loss=67.3441
	step [35/192], loss=71.5905
	step [36/192], loss=74.5438
	step [37/192], loss=72.5607
	step [38/192], loss=64.6811
	step [39/192], loss=60.9781
	step [40/192], loss=70.2370
	step [41/192], loss=68.0792
	step [42/192], loss=69.2641
	step [43/192], loss=68.5708
	step [44/192], loss=61.0927
	step [45/192], loss=65.2712
	step [46/192], loss=71.6239
	step [47/192], loss=73.8305
	step [48/192], loss=69.9244
	step [49/192], loss=85.7779
	step [50/192], loss=65.9728
	step [51/192], loss=63.9272
	step [52/192], loss=69.2030
	step [53/192], loss=67.1758
	step [54/192], loss=65.3371
	step [55/192], loss=76.6610
	step [56/192], loss=66.0299
	step [57/192], loss=72.2275
	step [58/192], loss=64.6947
	step [59/192], loss=68.6601
	step [60/192], loss=65.9379
	step [61/192], loss=73.2332
	step [62/192], loss=61.1198
	step [63/192], loss=71.4269
	step [64/192], loss=55.0925
	step [65/192], loss=62.3081
	step [66/192], loss=74.2978
	step [67/192], loss=68.3025
	step [68/192], loss=70.4473
	step [69/192], loss=60.5726
	step [70/192], loss=68.1556
	step [71/192], loss=75.0696
	step [72/192], loss=82.4038
	step [73/192], loss=69.7964
	step [74/192], loss=62.4137
	step [75/192], loss=71.0948
	step [76/192], loss=70.2018
	step [77/192], loss=69.3683
	step [78/192], loss=71.5669
	step [79/192], loss=70.3662
	step [80/192], loss=63.7535
	step [81/192], loss=68.2733
	step [82/192], loss=58.4362
	step [83/192], loss=76.8987
	step [84/192], loss=80.3375
	step [85/192], loss=68.4471
	step [86/192], loss=75.4406
	step [87/192], loss=64.1433
	step [88/192], loss=69.2013
	step [89/192], loss=66.7693
	step [90/192], loss=81.7990
	step [91/192], loss=67.4492
	step [92/192], loss=63.9315
	step [93/192], loss=74.8849
	step [94/192], loss=75.4640
	step [95/192], loss=59.9432
	step [96/192], loss=66.9347
	step [97/192], loss=48.8426
	step [98/192], loss=70.7132
	step [99/192], loss=68.9515
	step [100/192], loss=81.1890
	step [101/192], loss=69.2515
	step [102/192], loss=65.0126
	step [103/192], loss=83.3914
	step [104/192], loss=60.7629
	step [105/192], loss=67.2149
	step [106/192], loss=72.3566
	step [107/192], loss=80.2283
	step [108/192], loss=74.6178
	step [109/192], loss=67.3233
	step [110/192], loss=71.1923
	step [111/192], loss=68.8195
	step [112/192], loss=58.3818
	step [113/192], loss=79.9077
	step [114/192], loss=69.7421
	step [115/192], loss=54.5355
	step [116/192], loss=67.3021
	step [117/192], loss=63.1057
	step [118/192], loss=68.4005
	step [119/192], loss=67.7554
	step [120/192], loss=72.0863
	step [121/192], loss=82.2741
	step [122/192], loss=69.8771
	step [123/192], loss=80.7388
	step [124/192], loss=69.3789
	step [125/192], loss=68.4720
	step [126/192], loss=67.4989
	step [127/192], loss=72.5136
	step [128/192], loss=77.4066
	step [129/192], loss=70.4917
	step [130/192], loss=70.4523
	step [131/192], loss=63.5901
	step [132/192], loss=85.9609
	step [133/192], loss=74.7508
	step [134/192], loss=79.6601
	step [135/192], loss=61.3541
	step [136/192], loss=65.5466
	step [137/192], loss=74.8678
	step [138/192], loss=70.5807
	step [139/192], loss=62.4841
	step [140/192], loss=72.1599
	step [141/192], loss=81.2134
	step [142/192], loss=65.3392
	step [143/192], loss=61.6491
	step [144/192], loss=72.1291
	step [145/192], loss=67.0157
	step [146/192], loss=59.4025
	step [147/192], loss=63.6986
	step [148/192], loss=57.1857
	step [149/192], loss=58.3353
	step [150/192], loss=66.9843
	step [151/192], loss=72.7564
	step [152/192], loss=52.8027
	step [153/192], loss=61.9419
	step [154/192], loss=67.2720
	step [155/192], loss=54.3839
	step [156/192], loss=75.9330
	step [157/192], loss=69.1953
	step [158/192], loss=72.3528
	step [159/192], loss=70.2732
	step [160/192], loss=80.0584
	step [161/192], loss=77.4066
	step [162/192], loss=64.0481
	step [163/192], loss=71.9083
	step [164/192], loss=72.6269
	step [165/192], loss=70.1564
	step [166/192], loss=65.8251
	step [167/192], loss=76.4220
	step [168/192], loss=64.9084
	step [169/192], loss=90.7615
	step [170/192], loss=72.3485
	step [171/192], loss=78.3793
	step [172/192], loss=57.5136
	step [173/192], loss=65.2768
	step [174/192], loss=57.0662
	step [175/192], loss=75.0741
	step [176/192], loss=73.0089
	step [177/192], loss=78.1221
	step [178/192], loss=76.0698
	step [179/192], loss=60.8747
	step [180/192], loss=79.0417
	step [181/192], loss=75.2112
	step [182/192], loss=65.6758
	step [183/192], loss=73.1738
	step [184/192], loss=80.4099
	step [185/192], loss=67.1507
	step [186/192], loss=71.7399
	step [187/192], loss=73.9081
	step [188/192], loss=79.1190
	step [189/192], loss=78.7598
	step [190/192], loss=62.9320
	step [191/192], loss=58.0022
	step [192/192], loss=52.6070
	Evaluating
	loss=0.0064, precision=0.4044, recall=0.8738, f1=0.5529
saving model as: 0_saved_model.pth
Training epoch 94
	step [1/192], loss=60.4817
	step [2/192], loss=82.2595
	step [3/192], loss=76.5180
	step [4/192], loss=66.5540
	step [5/192], loss=72.8829
	step [6/192], loss=81.1032
	step [7/192], loss=68.2521
	step [8/192], loss=71.1828
	step [9/192], loss=71.7630
	step [10/192], loss=60.4335
	step [11/192], loss=72.9374
	step [12/192], loss=69.4122
	step [13/192], loss=78.5388
	step [14/192], loss=61.3636
	step [15/192], loss=85.0194
	step [16/192], loss=70.6763
	step [17/192], loss=66.4116
	step [18/192], loss=72.6331
	step [19/192], loss=72.6982
	step [20/192], loss=80.1823
	step [21/192], loss=82.0147
	step [22/192], loss=64.9846
	step [23/192], loss=72.4639
	step [24/192], loss=71.3101
	step [25/192], loss=59.0140
	step [26/192], loss=72.2306
	step [27/192], loss=53.1455
	step [28/192], loss=67.8519
	step [29/192], loss=84.7966
	step [30/192], loss=58.7921
	step [31/192], loss=70.6160
	step [32/192], loss=74.3547
	step [33/192], loss=59.3435
	step [34/192], loss=70.1041
	step [35/192], loss=75.3184
	step [36/192], loss=74.1368
	step [37/192], loss=66.1258
	step [38/192], loss=78.2619
	step [39/192], loss=67.9543
	step [40/192], loss=66.7139
	step [41/192], loss=51.4492
	step [42/192], loss=75.4571
	step [43/192], loss=71.7619
	step [44/192], loss=65.3426
	step [45/192], loss=70.5279
	step [46/192], loss=70.0017
	step [47/192], loss=75.5516
	step [48/192], loss=79.9116
	step [49/192], loss=70.8633
	step [50/192], loss=70.3854
	step [51/192], loss=70.1834
	step [52/192], loss=64.9675
	step [53/192], loss=67.8886
	step [54/192], loss=62.7201
	step [55/192], loss=66.0327
	step [56/192], loss=74.5308
	step [57/192], loss=65.4480
	step [58/192], loss=69.6104
	step [59/192], loss=69.1814
	step [60/192], loss=75.8160
	step [61/192], loss=72.0352
	step [62/192], loss=71.2146
	step [63/192], loss=68.5825
	step [64/192], loss=70.3842
	step [65/192], loss=67.9139
	step [66/192], loss=82.9957
	step [67/192], loss=71.0497
	step [68/192], loss=57.6232
	step [69/192], loss=64.1009
	step [70/192], loss=75.1799
	step [71/192], loss=74.5955
	step [72/192], loss=68.4030
	step [73/192], loss=71.1674
	step [74/192], loss=64.2174
	step [75/192], loss=68.7763
	step [76/192], loss=73.2131
	step [77/192], loss=81.7396
	step [78/192], loss=70.5750
	step [79/192], loss=72.0447
	step [80/192], loss=75.8895
	step [81/192], loss=58.5403
	step [82/192], loss=68.4669
	step [83/192], loss=69.3447
	step [84/192], loss=64.4600
	step [85/192], loss=81.7467
	step [86/192], loss=74.3982
	step [87/192], loss=67.0939
	step [88/192], loss=65.3285
	step [89/192], loss=65.6711
	step [90/192], loss=56.1051
	step [91/192], loss=85.7628
	step [92/192], loss=69.2748
	step [93/192], loss=62.0039
	step [94/192], loss=53.7234
	step [95/192], loss=68.0289
	step [96/192], loss=73.1861
	step [97/192], loss=68.6467
	step [98/192], loss=56.0499
	step [99/192], loss=66.1369
	step [100/192], loss=61.1119
	step [101/192], loss=77.5023
	step [102/192], loss=78.2705
	step [103/192], loss=59.5425
	step [104/192], loss=70.0823
	step [105/192], loss=69.9017
	step [106/192], loss=68.9286
	step [107/192], loss=81.2611
	step [108/192], loss=81.5177
	step [109/192], loss=76.2094
	step [110/192], loss=72.7690
	step [111/192], loss=72.4915
	step [112/192], loss=69.7445
	step [113/192], loss=70.7815
	step [114/192], loss=69.7800
	step [115/192], loss=73.7472
	step [116/192], loss=67.0130
	step [117/192], loss=61.6204
	step [118/192], loss=75.3030
	step [119/192], loss=67.3666
	step [120/192], loss=64.5267
	step [121/192], loss=62.4527
	step [122/192], loss=69.5089
	step [123/192], loss=65.2972
	step [124/192], loss=66.1772
	step [125/192], loss=70.0491
	step [126/192], loss=71.1498
	step [127/192], loss=70.4222
	step [128/192], loss=81.9821
	step [129/192], loss=75.3251
	step [130/192], loss=67.1989
	step [131/192], loss=73.7123
	step [132/192], loss=81.1766
	step [133/192], loss=63.1536
	step [134/192], loss=59.4681
	step [135/192], loss=68.0892
	step [136/192], loss=69.9844
	step [137/192], loss=78.7684
	step [138/192], loss=67.8780
	step [139/192], loss=55.3183
	step [140/192], loss=64.4874
	step [141/192], loss=64.7918
	step [142/192], loss=70.0388
	step [143/192], loss=83.3474
	step [144/192], loss=64.1550
	step [145/192], loss=68.7847
	step [146/192], loss=60.9523
	step [147/192], loss=84.6349
	step [148/192], loss=64.7505
	step [149/192], loss=63.2812
	step [150/192], loss=69.8529
	step [151/192], loss=74.6562
	step [152/192], loss=72.4888
	step [153/192], loss=71.4649
	step [154/192], loss=81.4554
	step [155/192], loss=66.7335
	step [156/192], loss=75.4576
	step [157/192], loss=72.8361
	step [158/192], loss=65.3758
	step [159/192], loss=78.0735
	step [160/192], loss=65.0123
	step [161/192], loss=74.2850
	step [162/192], loss=68.6723
	step [163/192], loss=64.5286
	step [164/192], loss=76.6020
	step [165/192], loss=53.2144
	step [166/192], loss=79.1494
	step [167/192], loss=73.7719
	step [168/192], loss=72.2546
	step [169/192], loss=72.8484
	step [170/192], loss=62.7131
	step [171/192], loss=68.6752
	step [172/192], loss=65.8294
	step [173/192], loss=62.6066
	step [174/192], loss=68.4673
	step [175/192], loss=57.3511
	step [176/192], loss=68.2592
	step [177/192], loss=68.3237
	step [178/192], loss=65.0003
	step [179/192], loss=60.4749
	step [180/192], loss=66.3367
	step [181/192], loss=68.7033
	step [182/192], loss=54.0444
	step [183/192], loss=87.1035
	step [184/192], loss=68.0649
	step [185/192], loss=67.8295
	step [186/192], loss=59.9482
	step [187/192], loss=67.8856
	step [188/192], loss=62.3971
	step [189/192], loss=64.0836
	step [190/192], loss=74.1815
	step [191/192], loss=62.1307
	step [192/192], loss=58.1043
	Evaluating
	loss=0.0068, precision=0.3976, recall=0.8670, f1=0.5452
Training epoch 95
	step [1/192], loss=69.0763
	step [2/192], loss=72.0552
	step [3/192], loss=74.6711
	step [4/192], loss=70.0276
	step [5/192], loss=63.6019
	step [6/192], loss=65.0524
	step [7/192], loss=78.6815
	step [8/192], loss=74.7049
	step [9/192], loss=76.2572
	step [10/192], loss=72.3477
	step [11/192], loss=72.8705
	step [12/192], loss=69.6986
	step [13/192], loss=68.5603
	step [14/192], loss=58.2132
	step [15/192], loss=67.7676
	step [16/192], loss=76.9715
	step [17/192], loss=68.5167
	step [18/192], loss=70.8487
	step [19/192], loss=69.7106
	step [20/192], loss=59.4364
	step [21/192], loss=83.6273
	step [22/192], loss=74.6574
	step [23/192], loss=70.3046
	step [24/192], loss=71.9344
	step [25/192], loss=63.1376
	step [26/192], loss=56.0686
	step [27/192], loss=66.8279
	step [28/192], loss=71.7890
	step [29/192], loss=69.6235
	step [30/192], loss=66.9949
	step [31/192], loss=79.1691
	step [32/192], loss=68.7428
	step [33/192], loss=74.8948
	step [34/192], loss=65.3935
	step [35/192], loss=80.3492
	step [36/192], loss=65.0724
	step [37/192], loss=75.1199
	step [38/192], loss=68.9395
	step [39/192], loss=80.5019
	step [40/192], loss=68.1252
	step [41/192], loss=49.5440
	step [42/192], loss=78.5507
	step [43/192], loss=80.1086
	step [44/192], loss=73.5376
	step [45/192], loss=70.6637
	step [46/192], loss=62.2712
	step [47/192], loss=72.1618
	step [48/192], loss=62.9409
	step [49/192], loss=69.8810
	step [50/192], loss=68.7121
	step [51/192], loss=60.6882
	step [52/192], loss=64.0740
	step [53/192], loss=70.2919
	step [54/192], loss=65.7908
	step [55/192], loss=62.8084
	step [56/192], loss=59.2084
	step [57/192], loss=63.6730
	step [58/192], loss=76.6362
	step [59/192], loss=64.3464
	step [60/192], loss=68.1556
	step [61/192], loss=81.9653
	step [62/192], loss=67.4961
	step [63/192], loss=76.5578
	step [64/192], loss=68.7817
	step [65/192], loss=73.9004
	step [66/192], loss=67.7713
	step [67/192], loss=75.1593
	step [68/192], loss=68.3298
	step [69/192], loss=72.8823
	step [70/192], loss=77.0967
	step [71/192], loss=66.4800
	step [72/192], loss=78.4864
	step [73/192], loss=74.4977
	step [74/192], loss=68.8148
	step [75/192], loss=74.8839
	step [76/192], loss=66.8212
	step [77/192], loss=76.4940
	step [78/192], loss=71.9504
	step [79/192], loss=65.8650
	step [80/192], loss=73.2063
	step [81/192], loss=59.4659
	step [82/192], loss=61.7055
	step [83/192], loss=67.3268
	step [84/192], loss=68.1700
	step [85/192], loss=57.6970
	step [86/192], loss=72.5980
	step [87/192], loss=63.7008
	step [88/192], loss=72.2234
	step [89/192], loss=73.1202
	step [90/192], loss=59.4515
	step [91/192], loss=70.8845
	step [92/192], loss=64.6576
	step [93/192], loss=66.7179
	step [94/192], loss=68.8370
	step [95/192], loss=63.3032
	step [96/192], loss=74.4052
	step [97/192], loss=76.2110
	step [98/192], loss=61.2660
	step [99/192], loss=75.5835
	step [100/192], loss=70.7760
	step [101/192], loss=60.6440
	step [102/192], loss=59.6753
	step [103/192], loss=63.9995
	step [104/192], loss=63.3672
	step [105/192], loss=61.3490
	step [106/192], loss=67.8704
	step [107/192], loss=67.7472
	step [108/192], loss=68.5604
	step [109/192], loss=60.0473
	step [110/192], loss=57.5817
	step [111/192], loss=58.3800
	step [112/192], loss=82.3828
	step [113/192], loss=72.7513
	step [114/192], loss=71.5254
	step [115/192], loss=75.0989
	step [116/192], loss=73.6718
	step [117/192], loss=67.4754
	step [118/192], loss=67.0282
	step [119/192], loss=61.1362
	step [120/192], loss=72.3062
	step [121/192], loss=67.5191
	step [122/192], loss=77.6335
	step [123/192], loss=67.5935
	step [124/192], loss=64.1699
	step [125/192], loss=64.1569
	step [126/192], loss=76.6825
	step [127/192], loss=67.8675
	step [128/192], loss=65.6669
	step [129/192], loss=62.7947
	step [130/192], loss=59.3212
	step [131/192], loss=82.4469
	step [132/192], loss=89.1098
	step [133/192], loss=65.7046
	step [134/192], loss=73.0115
	step [135/192], loss=77.0148
	step [136/192], loss=78.1188
	step [137/192], loss=72.7660
	step [138/192], loss=75.0759
	step [139/192], loss=70.4489
	step [140/192], loss=64.8037
	step [141/192], loss=65.4769
	step [142/192], loss=83.9761
	step [143/192], loss=61.9487
	step [144/192], loss=72.7783
	step [145/192], loss=73.4366
	step [146/192], loss=65.1225
	step [147/192], loss=74.6612
	step [148/192], loss=66.7972
	step [149/192], loss=76.8915
	step [150/192], loss=62.3659
	step [151/192], loss=60.0506
	step [152/192], loss=74.8941
	step [153/192], loss=69.6785
	step [154/192], loss=73.6947
	step [155/192], loss=65.4248
	step [156/192], loss=87.6239
	step [157/192], loss=67.9291
	step [158/192], loss=69.1134
	step [159/192], loss=79.3768
	step [160/192], loss=60.3353
	step [161/192], loss=65.5353
	step [162/192], loss=74.1164
	step [163/192], loss=77.9149
	step [164/192], loss=58.2535
	step [165/192], loss=64.6972
	step [166/192], loss=68.9654
	step [167/192], loss=62.4079
	step [168/192], loss=66.0802
	step [169/192], loss=62.0193
	step [170/192], loss=78.4159
	step [171/192], loss=63.2111
	step [172/192], loss=73.5290
	step [173/192], loss=76.5504
	step [174/192], loss=72.6367
	step [175/192], loss=61.8256
	step [176/192], loss=69.6174
	step [177/192], loss=69.8816
	step [178/192], loss=67.4870
	step [179/192], loss=60.1095
	step [180/192], loss=70.3563
	step [181/192], loss=74.3753
	step [182/192], loss=85.8812
	step [183/192], loss=75.9213
	step [184/192], loss=65.1745
	step [185/192], loss=62.7153
	step [186/192], loss=73.2269
	step [187/192], loss=63.4447
	step [188/192], loss=80.6198
	step [189/192], loss=64.5401
	step [190/192], loss=69.0192
	step [191/192], loss=88.6752
	step [192/192], loss=55.8291
	Evaluating
	loss=0.0077, precision=0.3479, recall=0.8779, f1=0.4983
Training epoch 96
	step [1/192], loss=73.1391
	step [2/192], loss=69.1958
	step [3/192], loss=77.7005
	step [4/192], loss=62.4982
	step [5/192], loss=68.2150
	step [6/192], loss=72.3054
	step [7/192], loss=66.3690
	step [8/192], loss=60.3636
	step [9/192], loss=71.8831
	step [10/192], loss=60.1142
	step [11/192], loss=71.1531
	step [12/192], loss=71.9688
	step [13/192], loss=73.6279
	step [14/192], loss=62.9037
	step [15/192], loss=67.6870
	step [16/192], loss=80.5701
	step [17/192], loss=66.1309
	step [18/192], loss=80.5798
	step [19/192], loss=62.0327
	step [20/192], loss=65.4239
	step [21/192], loss=66.5046
	step [22/192], loss=68.6573
	step [23/192], loss=71.3027
	step [24/192], loss=83.1798
	step [25/192], loss=65.5254
	step [26/192], loss=66.1702
	step [27/192], loss=68.7539
	step [28/192], loss=75.8756
	step [29/192], loss=58.0425
	step [30/192], loss=72.3348
	step [31/192], loss=59.3696
	step [32/192], loss=67.3967
	step [33/192], loss=64.6373
	step [34/192], loss=81.5472
	step [35/192], loss=75.3925
	step [36/192], loss=74.1907
	step [37/192], loss=73.8253
	step [38/192], loss=57.3572
	step [39/192], loss=70.7033
	step [40/192], loss=72.4485
	step [41/192], loss=75.2144
	step [42/192], loss=60.3005
	step [43/192], loss=62.6365
	step [44/192], loss=72.6589
	step [45/192], loss=80.5970
	step [46/192], loss=65.5337
	step [47/192], loss=66.2868
	step [48/192], loss=76.3471
	step [49/192], loss=63.9563
	step [50/192], loss=68.3600
	step [51/192], loss=74.6488
	step [52/192], loss=70.7382
	step [53/192], loss=60.8069
	step [54/192], loss=73.3350
	step [55/192], loss=74.4421
	step [56/192], loss=77.8875
	step [57/192], loss=72.0458
	step [58/192], loss=71.5702
	step [59/192], loss=71.1729
	step [60/192], loss=78.2872
	step [61/192], loss=80.1600
	step [62/192], loss=72.8335
	step [63/192], loss=72.4076
	step [64/192], loss=73.4902
	step [65/192], loss=61.8726
	step [66/192], loss=69.8262
	step [67/192], loss=64.6154
	step [68/192], loss=72.0700
	step [69/192], loss=73.6998
	step [70/192], loss=63.0750
	step [71/192], loss=74.5174
	step [72/192], loss=72.4259
	step [73/192], loss=61.2514
	step [74/192], loss=63.3596
	step [75/192], loss=73.1339
	step [76/192], loss=78.1350
	step [77/192], loss=73.0892
	step [78/192], loss=69.8729
	step [79/192], loss=77.3015
	step [80/192], loss=65.4237
	step [81/192], loss=68.6724
	step [82/192], loss=73.3889
	step [83/192], loss=63.9954
	step [84/192], loss=67.6194
	step [85/192], loss=63.5769
	step [86/192], loss=59.1710
	step [87/192], loss=72.6297
	step [88/192], loss=49.9918
	step [89/192], loss=73.4421
	step [90/192], loss=72.6693
	step [91/192], loss=63.0187
	step [92/192], loss=82.8708
	step [93/192], loss=59.8599
	step [94/192], loss=62.7146
	step [95/192], loss=66.8141
	step [96/192], loss=64.6527
	step [97/192], loss=74.3767
	step [98/192], loss=65.0588
	step [99/192], loss=71.0835
	step [100/192], loss=72.1194
	step [101/192], loss=61.8328
	step [102/192], loss=59.1234
	step [103/192], loss=77.6489
	step [104/192], loss=73.0286
	step [105/192], loss=77.9218
	step [106/192], loss=67.2353
	step [107/192], loss=77.6596
	step [108/192], loss=69.6197
	step [109/192], loss=69.5932
	step [110/192], loss=73.0415
	step [111/192], loss=65.3704
	step [112/192], loss=76.0711
	step [113/192], loss=62.2649
	step [114/192], loss=66.9409
	step [115/192], loss=87.5878
	step [116/192], loss=67.3307
	step [117/192], loss=59.7083
	step [118/192], loss=69.0841
	step [119/192], loss=64.8934
	step [120/192], loss=70.1178
	step [121/192], loss=55.9594
	step [122/192], loss=64.1049
	step [123/192], loss=67.4476
	step [124/192], loss=76.2010
	step [125/192], loss=79.8674
	step [126/192], loss=73.9416
	step [127/192], loss=63.5145
	step [128/192], loss=64.9512
	step [129/192], loss=63.8759
	step [130/192], loss=65.3035
	step [131/192], loss=67.5524
	step [132/192], loss=68.3405
	step [133/192], loss=85.5567
	step [134/192], loss=65.3736
	step [135/192], loss=80.9402
	step [136/192], loss=63.3480
	step [137/192], loss=61.8641
	step [138/192], loss=70.7411
	step [139/192], loss=61.8304
	step [140/192], loss=72.4529
	step [141/192], loss=75.7634
	step [142/192], loss=66.5408
	step [143/192], loss=63.9621
	step [144/192], loss=67.2105
	step [145/192], loss=63.9855
	step [146/192], loss=58.5258
	step [147/192], loss=68.2039
	step [148/192], loss=62.3100
	step [149/192], loss=74.3456
	step [150/192], loss=72.6608
	step [151/192], loss=61.6221
	step [152/192], loss=54.8274
	step [153/192], loss=69.5940
	step [154/192], loss=72.1553
	step [155/192], loss=69.9814
	step [156/192], loss=73.0084
	step [157/192], loss=68.3254
	step [158/192], loss=70.8374
	step [159/192], loss=58.8294
	step [160/192], loss=67.0301
	step [161/192], loss=70.4023
	step [162/192], loss=73.5835
	step [163/192], loss=69.5533
	step [164/192], loss=71.7615
	step [165/192], loss=75.7951
	step [166/192], loss=72.2915
	step [167/192], loss=69.1674
	step [168/192], loss=74.0412
	step [169/192], loss=68.3298
	step [170/192], loss=63.4584
	step [171/192], loss=61.9435
	step [172/192], loss=64.3610
	step [173/192], loss=78.3767
	step [174/192], loss=61.4342
	step [175/192], loss=69.4799
	step [176/192], loss=77.2007
	step [177/192], loss=83.7823
	step [178/192], loss=69.8796
	step [179/192], loss=63.4346
	step [180/192], loss=67.5816
	step [181/192], loss=68.6418
	step [182/192], loss=72.8062
	step [183/192], loss=61.9721
	step [184/192], loss=65.2271
	step [185/192], loss=66.7186
	step [186/192], loss=72.5078
	step [187/192], loss=80.3500
	step [188/192], loss=62.3926
	step [189/192], loss=76.9421
	step [190/192], loss=74.7373
	step [191/192], loss=76.4563
	step [192/192], loss=57.3768
	Evaluating
	loss=0.0077, precision=0.3431, recall=0.8771, f1=0.4932
Training epoch 97
	step [1/192], loss=68.5493
	step [2/192], loss=66.8258
	step [3/192], loss=78.3633
	step [4/192], loss=60.5707
	step [5/192], loss=72.8642
	step [6/192], loss=83.4544
	step [7/192], loss=55.5510
	step [8/192], loss=73.5557
	step [9/192], loss=50.8726
	step [10/192], loss=69.5648
	step [11/192], loss=82.0674
	step [12/192], loss=63.3562
	step [13/192], loss=70.9881
	step [14/192], loss=81.5572
	step [15/192], loss=76.6227
	step [16/192], loss=77.1635
	step [17/192], loss=73.6118
	step [18/192], loss=63.4364
	step [19/192], loss=69.1458
	step [20/192], loss=62.9515
	step [21/192], loss=62.0895
	step [22/192], loss=60.1530
	step [23/192], loss=71.6109
	step [24/192], loss=68.0299
	step [25/192], loss=66.3432
	step [26/192], loss=62.7749
	step [27/192], loss=84.8865
	step [28/192], loss=75.4447
	step [29/192], loss=60.7356
	step [30/192], loss=70.6895
	step [31/192], loss=68.7462
	step [32/192], loss=73.4851
	step [33/192], loss=68.0931
	step [34/192], loss=56.6712
	step [35/192], loss=63.0534
	step [36/192], loss=75.5421
	step [37/192], loss=72.2420
	step [38/192], loss=59.9165
	step [39/192], loss=77.9662
	step [40/192], loss=71.1520
	step [41/192], loss=60.4619
	step [42/192], loss=59.7233
	step [43/192], loss=58.7547
	step [44/192], loss=69.9747
	step [45/192], loss=75.1040
	step [46/192], loss=65.6568
	step [47/192], loss=66.1389
	step [48/192], loss=66.5065
	step [49/192], loss=67.3468
	step [50/192], loss=71.7651
	step [51/192], loss=70.2722
	step [52/192], loss=74.2623
	step [53/192], loss=90.3290
	step [54/192], loss=74.0571
	step [55/192], loss=80.3220
	step [56/192], loss=80.8698
	step [57/192], loss=64.3323
	step [58/192], loss=69.9728
	step [59/192], loss=72.2086
	step [60/192], loss=64.2441
	step [61/192], loss=66.5985
	step [62/192], loss=60.9978
	step [63/192], loss=64.6041
	step [64/192], loss=73.3204
	step [65/192], loss=63.9889
	step [66/192], loss=78.9837
	step [67/192], loss=63.8663
	step [68/192], loss=70.8470
	step [69/192], loss=79.8303
	step [70/192], loss=70.8379
	step [71/192], loss=59.3304
	step [72/192], loss=74.8178
	step [73/192], loss=64.0881
	step [74/192], loss=82.6123
	step [75/192], loss=72.1855
	step [76/192], loss=50.6508
	step [77/192], loss=57.1813
	step [78/192], loss=63.5895
	step [79/192], loss=62.3663
	step [80/192], loss=64.5460
	step [81/192], loss=70.3434
	step [82/192], loss=65.1717
	step [83/192], loss=65.0445
	step [84/192], loss=69.2081
	step [85/192], loss=64.4598
	step [86/192], loss=69.2183
	step [87/192], loss=70.4017
	step [88/192], loss=71.8969
	step [89/192], loss=63.6801
	step [90/192], loss=72.2857
	step [91/192], loss=70.6312
	step [92/192], loss=75.8942
	step [93/192], loss=63.8837
	step [94/192], loss=66.0772
	step [95/192], loss=70.2815
	step [96/192], loss=73.0694
	step [97/192], loss=69.7755
	step [98/192], loss=68.2998
	step [99/192], loss=69.3413
	step [100/192], loss=61.6278
	step [101/192], loss=75.1652
	step [102/192], loss=57.9458
	step [103/192], loss=74.8165
	step [104/192], loss=69.0510
	step [105/192], loss=67.1297
	step [106/192], loss=64.3143
	step [107/192], loss=76.4265
	step [108/192], loss=71.5338
	step [109/192], loss=75.7981
	step [110/192], loss=70.7363
	step [111/192], loss=68.2424
	step [112/192], loss=75.3242
	step [113/192], loss=70.3285
	step [114/192], loss=69.2200
	step [115/192], loss=65.8466
	step [116/192], loss=57.5040
	step [117/192], loss=67.0333
	step [118/192], loss=75.6761
	step [119/192], loss=74.7836
	step [120/192], loss=61.8575
	step [121/192], loss=69.3736
	step [122/192], loss=75.6870
	step [123/192], loss=75.7515
	step [124/192], loss=74.0781
	step [125/192], loss=61.4501
	step [126/192], loss=82.9250
	step [127/192], loss=71.3215
	step [128/192], loss=75.3637
	step [129/192], loss=68.2656
	step [130/192], loss=72.7284
	step [131/192], loss=63.5813
	step [132/192], loss=80.7910
	step [133/192], loss=76.5913
	step [134/192], loss=73.8571
	step [135/192], loss=78.8203
	step [136/192], loss=64.7077
	step [137/192], loss=68.3945
	step [138/192], loss=58.3609
	step [139/192], loss=66.6635
	step [140/192], loss=72.8330
	step [141/192], loss=73.4457
	step [142/192], loss=74.2967
	step [143/192], loss=68.4380
	step [144/192], loss=62.3221
	step [145/192], loss=55.2548
	step [146/192], loss=68.7557
	step [147/192], loss=62.2165
	step [148/192], loss=82.6889
	step [149/192], loss=60.6892
	step [150/192], loss=67.5437
	step [151/192], loss=78.1933
	step [152/192], loss=67.1660
	step [153/192], loss=73.7053
	step [154/192], loss=71.5552
	step [155/192], loss=60.3553
	step [156/192], loss=71.6787
	step [157/192], loss=76.6877
	step [158/192], loss=66.6309
	step [159/192], loss=53.5671
	step [160/192], loss=63.8300
	step [161/192], loss=68.8165
	step [162/192], loss=67.1906
	step [163/192], loss=71.2086
	step [164/192], loss=66.8532
	step [165/192], loss=59.2704
	step [166/192], loss=72.4092
	step [167/192], loss=67.2479
	step [168/192], loss=69.2495
	step [169/192], loss=64.2371
	step [170/192], loss=69.1446
	step [171/192], loss=45.6919
	step [172/192], loss=78.2920
	step [173/192], loss=71.8308
	step [174/192], loss=64.8774
	step [175/192], loss=64.9899
	step [176/192], loss=76.9869
	step [177/192], loss=70.8730
	step [178/192], loss=77.4438
	step [179/192], loss=65.8650
	step [180/192], loss=68.1593
	step [181/192], loss=56.6246
	step [182/192], loss=77.0742
	step [183/192], loss=77.7447
	step [184/192], loss=65.6830
	step [185/192], loss=76.5818
	step [186/192], loss=69.3897
	step [187/192], loss=63.1024
	step [188/192], loss=69.4375
	step [189/192], loss=68.0109
	step [190/192], loss=71.7311
	step [191/192], loss=76.3986
	step [192/192], loss=56.8642
	Evaluating
	loss=0.0086, precision=0.3063, recall=0.8709, f1=0.4532
Training epoch 98
	step [1/192], loss=80.5374
	step [2/192], loss=62.2132
	step [3/192], loss=58.6863
	step [4/192], loss=63.8782
	step [5/192], loss=51.7545
	step [6/192], loss=67.5951
	step [7/192], loss=67.9128
	step [8/192], loss=58.5750
	step [9/192], loss=81.6319
	step [10/192], loss=67.4843
	step [11/192], loss=66.0696
	step [12/192], loss=79.6931
	step [13/192], loss=53.9297
	step [14/192], loss=64.9020
	step [15/192], loss=76.9412
	step [16/192], loss=66.0905
	step [17/192], loss=75.2214
	step [18/192], loss=68.7816
	step [19/192], loss=66.1152
	step [20/192], loss=76.9970
	step [21/192], loss=80.9447
	step [22/192], loss=69.5929
	step [23/192], loss=67.1306
	step [24/192], loss=72.2309
	step [25/192], loss=74.6175
	step [26/192], loss=69.1868
	step [27/192], loss=60.9429
	step [28/192], loss=61.0421
	step [29/192], loss=59.9844
	step [30/192], loss=69.3327
	step [31/192], loss=82.1539
	step [32/192], loss=71.7892
	step [33/192], loss=74.7819
	step [34/192], loss=61.7524
	step [35/192], loss=72.1885
	step [36/192], loss=69.6664
	step [37/192], loss=54.2646
	step [38/192], loss=70.1249
	step [39/192], loss=68.7848
	step [40/192], loss=65.2258
	step [41/192], loss=73.3856
	step [42/192], loss=69.4601
	step [43/192], loss=55.4308
	step [44/192], loss=71.2633
	step [45/192], loss=62.7284
	step [46/192], loss=67.5032
	step [47/192], loss=67.1465
	step [48/192], loss=66.0482
	step [49/192], loss=80.2235
	step [50/192], loss=64.0342
	step [51/192], loss=77.1064
	step [52/192], loss=63.0134
	step [53/192], loss=75.5953
	step [54/192], loss=75.1275
	step [55/192], loss=85.2395
	step [56/192], loss=90.2988
	step [57/192], loss=66.0471
	step [58/192], loss=68.7962
	step [59/192], loss=67.7542
	step [60/192], loss=64.4306
	step [61/192], loss=72.7348
	step [62/192], loss=75.8863
	step [63/192], loss=63.9478
	step [64/192], loss=62.9004
	step [65/192], loss=76.1118
	step [66/192], loss=70.6674
	step [67/192], loss=77.0833
	step [68/192], loss=78.1374
	step [69/192], loss=71.6747
	step [70/192], loss=57.6548
	step [71/192], loss=72.4771
	step [72/192], loss=69.7639
	step [73/192], loss=63.3338
	step [74/192], loss=63.1592
	step [75/192], loss=63.5843
	step [76/192], loss=65.9826
	step [77/192], loss=62.7803
	step [78/192], loss=74.9033
	step [79/192], loss=68.0486
	step [80/192], loss=76.4444
	step [81/192], loss=65.2936
	step [82/192], loss=78.2427
	step [83/192], loss=75.8750
	step [84/192], loss=67.5228
	step [85/192], loss=77.5808
	step [86/192], loss=69.1182
	step [87/192], loss=73.7495
	step [88/192], loss=65.0849
	step [89/192], loss=68.9565
	step [90/192], loss=60.8706
	step [91/192], loss=63.3432
	step [92/192], loss=68.2127
	step [93/192], loss=53.4944
	step [94/192], loss=76.9744
	step [95/192], loss=65.0018
	step [96/192], loss=76.4002
	step [97/192], loss=71.3546
	step [98/192], loss=70.0913
	step [99/192], loss=63.6269
	step [100/192], loss=75.2575
	step [101/192], loss=65.4747
	step [102/192], loss=78.9312
	step [103/192], loss=74.2441
	step [104/192], loss=69.0616
	step [105/192], loss=69.4621
	step [106/192], loss=66.0174
	step [107/192], loss=70.3729
	step [108/192], loss=61.6732
	step [109/192], loss=74.3796
	step [110/192], loss=69.4309
	step [111/192], loss=72.7205
	step [112/192], loss=65.9681
	step [113/192], loss=62.2559
	step [114/192], loss=68.9524
	step [115/192], loss=59.5779
	step [116/192], loss=72.0595
	step [117/192], loss=68.3258
	step [118/192], loss=65.4499
	step [119/192], loss=63.8898
	step [120/192], loss=71.3525
	step [121/192], loss=76.2306
	step [122/192], loss=69.1987
	step [123/192], loss=68.0867
	step [124/192], loss=63.0093
	step [125/192], loss=59.9794
	step [126/192], loss=61.8307
	step [127/192], loss=76.3298
	step [128/192], loss=53.2374
	step [129/192], loss=70.1347
	step [130/192], loss=58.9458
	step [131/192], loss=66.6971
	step [132/192], loss=63.4844
	step [133/192], loss=67.8638
	step [134/192], loss=71.7814
	step [135/192], loss=74.8662
	step [136/192], loss=72.7539
	step [137/192], loss=68.0387
	step [138/192], loss=56.7742
	step [139/192], loss=71.8402
	step [140/192], loss=60.4623
	step [141/192], loss=68.2711
	step [142/192], loss=52.2057
	step [143/192], loss=63.0094
	step [144/192], loss=61.5351
	step [145/192], loss=72.7176
	step [146/192], loss=76.3567
	step [147/192], loss=69.8272
	step [148/192], loss=73.8161
	step [149/192], loss=66.8382
	step [150/192], loss=71.2919
	step [151/192], loss=81.6044
	step [152/192], loss=68.8864
	step [153/192], loss=69.4669
	step [154/192], loss=69.8083
	step [155/192], loss=68.5368
	step [156/192], loss=62.8904
	step [157/192], loss=72.9107
	step [158/192], loss=62.1796
	step [159/192], loss=72.1270
	step [160/192], loss=63.2835
	step [161/192], loss=68.0831
	step [162/192], loss=76.7050
	step [163/192], loss=65.9646
	step [164/192], loss=80.4243
	step [165/192], loss=74.3977
	step [166/192], loss=62.5840
	step [167/192], loss=73.7722
	step [168/192], loss=81.3803
	step [169/192], loss=75.9066
	step [170/192], loss=68.4628
	step [171/192], loss=68.4703
	step [172/192], loss=68.5336
	step [173/192], loss=62.7337
	step [174/192], loss=71.6894
	step [175/192], loss=67.3056
	step [176/192], loss=79.3244
	step [177/192], loss=78.8185
	step [178/192], loss=65.0362
	step [179/192], loss=58.7790
	step [180/192], loss=78.0748
	step [181/192], loss=68.5206
	step [182/192], loss=79.7239
	step [183/192], loss=65.1017
	step [184/192], loss=68.0886
	step [185/192], loss=58.6667
	step [186/192], loss=61.3235
	step [187/192], loss=67.8705
	step [188/192], loss=68.1729
	step [189/192], loss=81.7911
	step [190/192], loss=75.6340
	step [191/192], loss=66.1797
	step [192/192], loss=60.8624
	Evaluating
	loss=0.0085, precision=0.2973, recall=0.8703, f1=0.4432
Training epoch 99
	step [1/192], loss=58.4559
	step [2/192], loss=63.8527
	step [3/192], loss=71.5964
	step [4/192], loss=63.0401
	step [5/192], loss=78.0572
	step [6/192], loss=68.0172
	step [7/192], loss=79.4502
	step [8/192], loss=59.7045
	step [9/192], loss=71.3719
	step [10/192], loss=88.1873
	step [11/192], loss=66.3780
	step [12/192], loss=69.6470
	step [13/192], loss=67.4269
	step [14/192], loss=67.5214
	step [15/192], loss=58.1585
	step [16/192], loss=64.0700
	step [17/192], loss=69.7757
	step [18/192], loss=76.2918
	step [19/192], loss=64.5786
	step [20/192], loss=58.6655
	step [21/192], loss=68.7672
	step [22/192], loss=65.4158
	step [23/192], loss=73.6403
	step [24/192], loss=60.8415
	step [25/192], loss=69.2122
	step [26/192], loss=58.4562
	step [27/192], loss=67.7522
	step [28/192], loss=64.0331
	step [29/192], loss=66.6708
	step [30/192], loss=79.7969
	step [31/192], loss=83.2196
	step [32/192], loss=63.5484
	step [33/192], loss=77.1422
	step [34/192], loss=59.4162
	step [35/192], loss=68.7848
	step [36/192], loss=68.9425
	step [37/192], loss=63.7922
	step [38/192], loss=63.0855
	step [39/192], loss=76.3467
	step [40/192], loss=76.4849
	step [41/192], loss=70.8019
	step [42/192], loss=74.6225
	step [43/192], loss=75.1791
	step [44/192], loss=85.4668
	step [45/192], loss=68.7886
	step [46/192], loss=63.1993
	step [47/192], loss=76.1590
	step [48/192], loss=61.5149
	step [49/192], loss=71.9957
	step [50/192], loss=71.9397
	step [51/192], loss=62.1580
	step [52/192], loss=77.1722
	step [53/192], loss=66.9550
	step [54/192], loss=51.8764
	step [55/192], loss=69.6014
	step [56/192], loss=58.7679
	step [57/192], loss=67.7440
	step [58/192], loss=56.5463
	step [59/192], loss=72.4417
	step [60/192], loss=65.9473
	step [61/192], loss=73.2525
	step [62/192], loss=75.9037
	step [63/192], loss=64.3349
	step [64/192], loss=69.9216
	step [65/192], loss=50.3663
	step [66/192], loss=66.9813
	step [67/192], loss=71.3299
	step [68/192], loss=63.0829
	step [69/192], loss=83.1649
	step [70/192], loss=67.2636
	step [71/192], loss=80.8646
	step [72/192], loss=81.8337
	step [73/192], loss=62.4246
	step [74/192], loss=61.8597
	step [75/192], loss=71.4278
	step [76/192], loss=60.4852
	step [77/192], loss=69.3407
	step [78/192], loss=71.1168
	step [79/192], loss=61.7544
	step [80/192], loss=60.3361
	step [81/192], loss=64.1308
	step [82/192], loss=66.4897
	step [83/192], loss=59.7825
	step [84/192], loss=64.6339
	step [85/192], loss=70.3959
	step [86/192], loss=70.2990
	step [87/192], loss=82.4694
	step [88/192], loss=77.7976
	step [89/192], loss=72.9937
	step [90/192], loss=71.1020
	step [91/192], loss=79.5007
	step [92/192], loss=79.4382
	step [93/192], loss=63.0463
	step [94/192], loss=69.6674
	step [95/192], loss=79.6855
	step [96/192], loss=69.6951
	step [97/192], loss=61.8033
	step [98/192], loss=68.3183
	step [99/192], loss=66.6526
	step [100/192], loss=69.2381
	step [101/192], loss=78.8566
	step [102/192], loss=69.1970
	step [103/192], loss=58.3893
	step [104/192], loss=64.1158
	step [105/192], loss=81.5435
	step [106/192], loss=63.4986
	step [107/192], loss=83.5729
	step [108/192], loss=73.4527
	step [109/192], loss=68.8800
	step [110/192], loss=63.6426
	step [111/192], loss=69.1065
	step [112/192], loss=68.0250
	step [113/192], loss=63.4099
	step [114/192], loss=51.7244
	step [115/192], loss=70.8754
	step [116/192], loss=67.6805
	step [117/192], loss=62.3984
	step [118/192], loss=85.8541
	step [119/192], loss=61.3247
	step [120/192], loss=62.3925
	step [121/192], loss=64.9646
	step [122/192], loss=72.6230
	step [123/192], loss=67.3473
	step [124/192], loss=70.1324
	step [125/192], loss=70.9108
	step [126/192], loss=79.9411
	step [127/192], loss=70.0536
	step [128/192], loss=73.5476
	step [129/192], loss=66.1668
	step [130/192], loss=83.8441
	step [131/192], loss=61.5002
	step [132/192], loss=69.6966
	step [133/192], loss=82.2079
	step [134/192], loss=75.2569
	step [135/192], loss=67.2577
	step [136/192], loss=70.6741
	step [137/192], loss=64.6124
	step [138/192], loss=67.0487
	step [139/192], loss=69.9737
	step [140/192], loss=67.3050
	step [141/192], loss=70.2543
	step [142/192], loss=62.4972
	step [143/192], loss=59.8055
	step [144/192], loss=53.4579
	step [145/192], loss=64.4791
	step [146/192], loss=76.2968
	step [147/192], loss=78.5179
	step [148/192], loss=69.0247
	step [149/192], loss=75.0032
	step [150/192], loss=65.9486
	step [151/192], loss=73.7651
	step [152/192], loss=73.6120
	step [153/192], loss=62.7481
	step [154/192], loss=69.8758
	step [155/192], loss=74.3676
	step [156/192], loss=59.0449
	step [157/192], loss=75.4116
	step [158/192], loss=64.7843
	step [159/192], loss=67.6710
	step [160/192], loss=71.3725
	step [161/192], loss=67.2871
	step [162/192], loss=66.3595
	step [163/192], loss=64.5288
	step [164/192], loss=53.3405
	step [165/192], loss=76.6943
	step [166/192], loss=62.9823
	step [167/192], loss=71.5470
	step [168/192], loss=68.2408
	step [169/192], loss=66.5163
	step [170/192], loss=73.1807
	step [171/192], loss=76.2120
	step [172/192], loss=62.5194
	step [173/192], loss=79.2902
	step [174/192], loss=65.6639
	step [175/192], loss=78.0065
	step [176/192], loss=74.2460
	step [177/192], loss=65.0104
	step [178/192], loss=63.5435
	step [179/192], loss=66.2859
	step [180/192], loss=60.5674
	step [181/192], loss=69.8625
	step [182/192], loss=61.9266
	step [183/192], loss=63.7268
	step [184/192], loss=62.8814
	step [185/192], loss=63.1770
	step [186/192], loss=69.9150
	step [187/192], loss=75.3620
	step [188/192], loss=69.6233
	step [189/192], loss=57.0157
	step [190/192], loss=72.4495
	step [191/192], loss=80.3889
	step [192/192], loss=54.0757
	Evaluating
	loss=0.0064, precision=0.3855, recall=0.8649, f1=0.5333
Training epoch 100
	step [1/192], loss=75.5780
	step [2/192], loss=70.5916
	step [3/192], loss=70.7899
	step [4/192], loss=75.8423
	step [5/192], loss=71.5866
	step [6/192], loss=52.7988
	step [7/192], loss=68.6684
	step [8/192], loss=62.9265
	step [9/192], loss=72.3222
	step [10/192], loss=58.4087
	step [11/192], loss=72.7737
	step [12/192], loss=64.0488
	step [13/192], loss=68.2160
	step [14/192], loss=70.0565
	step [15/192], loss=68.8925
	step [16/192], loss=70.3019
	step [17/192], loss=64.1642
	step [18/192], loss=72.0962
	step [19/192], loss=75.4169
	step [20/192], loss=59.7945
	step [21/192], loss=68.8785
	step [22/192], loss=64.8768
	step [23/192], loss=67.9370
	step [24/192], loss=56.3241
	step [25/192], loss=58.9935
	step [26/192], loss=70.8713
	step [27/192], loss=62.9173
	step [28/192], loss=69.7074
	step [29/192], loss=69.0780
	step [30/192], loss=74.3289
	step [31/192], loss=63.0442
	step [32/192], loss=67.9365
	step [33/192], loss=73.4198
	step [34/192], loss=68.1462
	step [35/192], loss=58.1215
	step [36/192], loss=59.9989
	step [37/192], loss=66.2504
	step [38/192], loss=74.7773
	step [39/192], loss=67.6104
	step [40/192], loss=68.1988
	step [41/192], loss=89.0591
	step [42/192], loss=60.1071
	step [43/192], loss=77.0051
	step [44/192], loss=68.0087
	step [45/192], loss=71.2739
	step [46/192], loss=62.7284
	step [47/192], loss=80.0683
	step [48/192], loss=68.5930
	step [49/192], loss=67.4184
	step [50/192], loss=71.0465
	step [51/192], loss=79.4728
	step [52/192], loss=76.8621
	step [53/192], loss=68.2513
	step [54/192], loss=72.0769
	step [55/192], loss=76.8716
	step [56/192], loss=59.5483
	step [57/192], loss=71.5898
	step [58/192], loss=61.2287
	step [59/192], loss=61.8402
	step [60/192], loss=68.9965
	step [61/192], loss=74.5499
	step [62/192], loss=63.0947
	step [63/192], loss=73.2770
	step [64/192], loss=66.3477
	step [65/192], loss=75.3699
	step [66/192], loss=72.4416
	step [67/192], loss=70.5043
	step [68/192], loss=56.4551
	step [69/192], loss=71.6938
	step [70/192], loss=73.3366
	step [71/192], loss=64.0271
	step [72/192], loss=64.7674
	step [73/192], loss=88.7054
	step [74/192], loss=64.7497
	step [75/192], loss=60.4543
	step [76/192], loss=55.5368
	step [77/192], loss=77.1898
	step [78/192], loss=60.7985
	step [79/192], loss=67.7961
	step [80/192], loss=73.9100
	step [81/192], loss=62.4185
	step [82/192], loss=74.7364
	step [83/192], loss=74.8974
	step [84/192], loss=66.6276
	step [85/192], loss=62.4278
	step [86/192], loss=79.3932
	step [87/192], loss=63.9391
	step [88/192], loss=60.5993
	step [89/192], loss=68.2635
	step [90/192], loss=65.7136
	step [91/192], loss=76.0576
	step [92/192], loss=65.6155
	step [93/192], loss=82.0387
	step [94/192], loss=62.6817
	step [95/192], loss=56.2095
	step [96/192], loss=68.1594
	step [97/192], loss=68.4746
	step [98/192], loss=71.8380
	step [99/192], loss=70.5862
	step [100/192], loss=82.6065
	step [101/192], loss=65.5329
	step [102/192], loss=53.5286
	step [103/192], loss=77.3960
	step [104/192], loss=59.7785
	step [105/192], loss=61.7723
	step [106/192], loss=70.5334
	step [107/192], loss=59.7326
	step [108/192], loss=63.4415
	step [109/192], loss=69.7363
	step [110/192], loss=68.1574
	step [111/192], loss=63.0571
	step [112/192], loss=61.8944
	step [113/192], loss=64.4954
	step [114/192], loss=73.7802
	step [115/192], loss=58.9200
	step [116/192], loss=61.8327
	step [117/192], loss=61.6811
	step [118/192], loss=67.1298
	step [119/192], loss=69.2281
	step [120/192], loss=60.4717
	step [121/192], loss=70.2992
	step [122/192], loss=67.5179
	step [123/192], loss=70.7976
	step [124/192], loss=77.5138
	step [125/192], loss=78.5073
	step [126/192], loss=60.9736
	step [127/192], loss=72.1821
	step [128/192], loss=74.5869
	step [129/192], loss=75.0264
	step [130/192], loss=72.2233
	step [131/192], loss=70.8371
	step [132/192], loss=71.7209
	step [133/192], loss=64.4574
	step [134/192], loss=79.3919
	step [135/192], loss=67.8651
	step [136/192], loss=53.0214
	step [137/192], loss=74.6499
	step [138/192], loss=59.2719
	step [139/192], loss=63.0751
	step [140/192], loss=70.6711
	step [141/192], loss=69.5725
	step [142/192], loss=71.4104
	step [143/192], loss=65.7880
	step [144/192], loss=66.2971
	step [145/192], loss=76.5816
	step [146/192], loss=80.7704
	step [147/192], loss=70.3224
	step [148/192], loss=83.3516
	step [149/192], loss=56.4827
	step [150/192], loss=72.8657
	step [151/192], loss=70.2818
	step [152/192], loss=76.4059
	step [153/192], loss=63.0151
	step [154/192], loss=67.0370
	step [155/192], loss=71.8522
	step [156/192], loss=78.2332
	step [157/192], loss=70.2094
	step [158/192], loss=67.0551
	step [159/192], loss=65.3894
	step [160/192], loss=67.6790
	step [161/192], loss=67.1710
	step [162/192], loss=70.0177
	step [163/192], loss=72.2120
	step [164/192], loss=73.4259
	step [165/192], loss=78.0333
	step [166/192], loss=73.7967
	step [167/192], loss=68.2520
	step [168/192], loss=55.1746
	step [169/192], loss=66.9335
	step [170/192], loss=67.9289
	step [171/192], loss=63.2988
	step [172/192], loss=72.9317
	step [173/192], loss=58.9237
	step [174/192], loss=68.6111
	step [175/192], loss=63.4779
	step [176/192], loss=69.2976
	step [177/192], loss=72.7134
	step [178/192], loss=69.5955
	step [179/192], loss=88.5271
	step [180/192], loss=81.8828
	step [181/192], loss=68.7640
	step [182/192], loss=77.9611
	step [183/192], loss=61.2588
	step [184/192], loss=59.1857
	step [185/192], loss=75.0490
	step [186/192], loss=51.2676
	step [187/192], loss=77.6989
	step [188/192], loss=68.5884
	step [189/192], loss=65.9187
	step [190/192], loss=62.4974
	step [191/192], loss=72.5275
	step [192/192], loss=57.2120
	Evaluating
	loss=0.0064, precision=0.4135, recall=0.8685, f1=0.5603
saving model as: 0_saved_model.pth
Training epoch 101
	step [1/192], loss=67.9773
	step [2/192], loss=66.7345
	step [3/192], loss=64.7865
	step [4/192], loss=70.2924
	step [5/192], loss=81.3058
	step [6/192], loss=65.5453
	step [7/192], loss=81.3633
	step [8/192], loss=71.5231
	step [9/192], loss=64.8992
	step [10/192], loss=56.5026
	step [11/192], loss=62.1373
	step [12/192], loss=74.7154
	step [13/192], loss=69.0370
	step [14/192], loss=68.6770
	step [15/192], loss=61.9414
	step [16/192], loss=73.2445
	step [17/192], loss=62.2059
	step [18/192], loss=75.9377
	step [19/192], loss=66.8086
	step [20/192], loss=67.0542
	step [21/192], loss=75.2146
	step [22/192], loss=70.4983
	step [23/192], loss=70.5142
	step [24/192], loss=65.3550
	step [25/192], loss=55.5372
	step [26/192], loss=70.4713
	step [27/192], loss=63.8050
	step [28/192], loss=60.3459
	step [29/192], loss=61.9881
	step [30/192], loss=71.2197
	step [31/192], loss=76.7919
	step [32/192], loss=77.4522
	step [33/192], loss=67.9501
	step [34/192], loss=69.6595
	step [35/192], loss=77.3067
	step [36/192], loss=65.4054
	step [37/192], loss=73.7663
	step [38/192], loss=68.1336
	step [39/192], loss=56.1982
	step [40/192], loss=56.1703
	step [41/192], loss=51.7807
	step [42/192], loss=60.7655
	step [43/192], loss=53.0465
	step [44/192], loss=78.8401
	step [45/192], loss=75.1888
	step [46/192], loss=71.0588
	step [47/192], loss=79.7958
	step [48/192], loss=61.6498
	step [49/192], loss=62.4334
	step [50/192], loss=65.1329
	step [51/192], loss=66.6004
	step [52/192], loss=61.5834
	step [53/192], loss=57.4944
	step [54/192], loss=74.8205
	step [55/192], loss=76.4076
	step [56/192], loss=83.2749
	step [57/192], loss=71.4058
	step [58/192], loss=71.6209
	step [59/192], loss=70.9051
	step [60/192], loss=75.8705
	step [61/192], loss=69.2949
	step [62/192], loss=76.6172
	step [63/192], loss=63.1013
	step [64/192], loss=81.7239
	step [65/192], loss=67.5524
	step [66/192], loss=66.2517
	step [67/192], loss=65.7404
	step [68/192], loss=68.8548
	step [69/192], loss=66.7381
	step [70/192], loss=65.3977
	step [71/192], loss=82.4374
	step [72/192], loss=68.5771
	step [73/192], loss=72.4122
	step [74/192], loss=67.9422
	step [75/192], loss=73.1000
	step [76/192], loss=63.3015
	step [77/192], loss=70.1489
	step [78/192], loss=68.8894
	step [79/192], loss=70.1248
	step [80/192], loss=64.3370
	step [81/192], loss=72.4465
	step [82/192], loss=52.6034
	step [83/192], loss=63.8393
	step [84/192], loss=69.6379
	step [85/192], loss=69.5546
	step [86/192], loss=65.9334
	step [87/192], loss=65.7100
	step [88/192], loss=59.7355
	step [89/192], loss=67.8141
	step [90/192], loss=56.0176
	step [91/192], loss=73.5413
	step [92/192], loss=64.2817
	step [93/192], loss=69.0693
	step [94/192], loss=68.8291
	step [95/192], loss=70.2932
	step [96/192], loss=73.2892
	step [97/192], loss=72.1307
	step [98/192], loss=66.6379
	step [99/192], loss=59.8125
	step [100/192], loss=79.6096
	step [101/192], loss=66.5027
	step [102/192], loss=58.4520
	step [103/192], loss=71.3136
	step [104/192], loss=70.9634
	step [105/192], loss=67.0800
	step [106/192], loss=54.8606
	step [107/192], loss=74.3799
	step [108/192], loss=66.9611
	step [109/192], loss=70.3325
	step [110/192], loss=53.7197
	step [111/192], loss=73.1574
	step [112/192], loss=71.8732
	step [113/192], loss=67.4174
	step [114/192], loss=77.0859
	step [115/192], loss=75.1372
	step [116/192], loss=72.9411
	step [117/192], loss=65.1663
	step [118/192], loss=67.4071
	step [119/192], loss=84.9378
	step [120/192], loss=74.0801
	step [121/192], loss=54.1638
	step [122/192], loss=71.6891
	step [123/192], loss=67.9686
	step [124/192], loss=60.8620
	step [125/192], loss=55.2660
	step [126/192], loss=77.3416
	step [127/192], loss=76.8862
	step [128/192], loss=58.4896
	step [129/192], loss=63.9601
	step [130/192], loss=68.4515
	step [131/192], loss=67.4682
	step [132/192], loss=73.8268
	step [133/192], loss=75.4214
	step [134/192], loss=71.7788
	step [135/192], loss=67.5768
	step [136/192], loss=72.9548
	step [137/192], loss=79.4720
	step [138/192], loss=72.6850
	step [139/192], loss=77.8602
	step [140/192], loss=82.7979
	step [141/192], loss=72.1533
	step [142/192], loss=67.9701
	step [143/192], loss=67.1231
	step [144/192], loss=65.9906
	step [145/192], loss=66.3136
	step [146/192], loss=59.0889
	step [147/192], loss=65.3820
	step [148/192], loss=74.9700
	step [149/192], loss=69.5700
	step [150/192], loss=71.0607
	step [151/192], loss=64.6584
	step [152/192], loss=66.3070
	step [153/192], loss=62.6312
	step [154/192], loss=73.5999
	step [155/192], loss=57.4672
	step [156/192], loss=74.6315
	step [157/192], loss=63.2684
	step [158/192], loss=67.9375
	step [159/192], loss=68.0820
	step [160/192], loss=67.5885
	step [161/192], loss=72.0025
	step [162/192], loss=61.6019
	step [163/192], loss=77.2659
	step [164/192], loss=65.1932
	step [165/192], loss=76.7647
	step [166/192], loss=57.2222
	step [167/192], loss=60.0735
	step [168/192], loss=69.1114
	step [169/192], loss=73.0369
	step [170/192], loss=61.5509
	step [171/192], loss=77.5957
	step [172/192], loss=60.5573
	step [173/192], loss=85.8624
	step [174/192], loss=63.4523
	step [175/192], loss=67.5488
	step [176/192], loss=62.6112
	step [177/192], loss=75.4227
	step [178/192], loss=69.3028
	step [179/192], loss=57.2166
	step [180/192], loss=69.6514
	step [181/192], loss=63.7510
	step [182/192], loss=78.9201
	step [183/192], loss=71.3084
	step [184/192], loss=64.3304
	step [185/192], loss=55.6534
	step [186/192], loss=72.2466
	step [187/192], loss=67.8224
	step [188/192], loss=71.7699
	step [189/192], loss=59.8293
	step [190/192], loss=59.1059
	step [191/192], loss=77.0582
	step [192/192], loss=64.3964
	Evaluating
	loss=0.0081, precision=0.3176, recall=0.8753, f1=0.4661
Training finished
best_f1: 0.5603082012408274
directing: Z rim_enhanced: True test_id 1
removed wrong scan: weights_Z_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_278_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_276_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_298_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_210_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_262_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_254_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_58_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_293_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_291_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_206_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_252_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_265_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_295_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_221_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_292_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_294_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_297_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_214_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_296_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_225_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_300_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_243_xwqg-A00121_2019-05-15.npy
# all image files: 16489 # all weight files in weight_dir: 12192 # image files with weight 12192
removed wrong scan: weights_Z_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_278_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_276_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_298_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_210_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_262_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_254_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_58_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_293_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_291_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_206_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_252_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_265_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_295_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_221_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_292_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_294_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_297_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_214_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_296_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_225_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_300_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_243_xwqg-A00121_2019-05-15.npy
# all image files: 16489 # all weight files in weight_dir: 3352 # image files with weight 3352
train: /ibex/scratch/projects/c2052/air_tube_seg/training_samples/Z 12192
Using 4 GPUs
Going to train epochs [61-110]
Training epoch 61
	step [1/191], loss=71.3283
	step [2/191], loss=79.1318
	step [3/191], loss=73.6075
	step [4/191], loss=57.6451
	step [5/191], loss=70.9976
	step [6/191], loss=82.5956
	step [7/191], loss=80.6377
	step [8/191], loss=57.1608
	step [9/191], loss=64.1164
	step [10/191], loss=82.7589
	step [11/191], loss=68.0680
	step [12/191], loss=70.9497
	step [13/191], loss=68.6759
	step [14/191], loss=85.7656
	step [15/191], loss=68.1962
	step [16/191], loss=63.2904
	step [17/191], loss=71.4043
	step [18/191], loss=59.6941
	step [19/191], loss=63.7374
	step [20/191], loss=82.1257
	step [21/191], loss=68.4937
	step [22/191], loss=63.9738
	step [23/191], loss=70.7123
	step [24/191], loss=78.1875
	step [25/191], loss=74.5323
	step [26/191], loss=72.3911
	step [27/191], loss=63.2925
	step [28/191], loss=62.5137
	step [29/191], loss=72.2708
	step [30/191], loss=74.7529
	step [31/191], loss=70.2739
	step [32/191], loss=78.9760
	step [33/191], loss=75.9706
	step [34/191], loss=77.7692
	step [35/191], loss=69.7688
	step [36/191], loss=75.8485
	step [37/191], loss=73.8972
	step [38/191], loss=70.0920
	step [39/191], loss=74.9615
	step [40/191], loss=74.9663
	step [41/191], loss=57.0860
	step [42/191], loss=63.0276
	step [43/191], loss=62.8739
	step [44/191], loss=66.8255
	step [45/191], loss=70.1219
	step [46/191], loss=58.3399
	step [47/191], loss=76.9740
	step [48/191], loss=65.7385
	step [49/191], loss=73.7848
	step [50/191], loss=65.5026
	step [51/191], loss=92.0166
	step [52/191], loss=75.2237
	step [53/191], loss=70.4303
	step [54/191], loss=70.4211
	step [55/191], loss=76.0519
	step [56/191], loss=56.3533
	step [57/191], loss=75.9918
	step [58/191], loss=55.4367
	step [59/191], loss=82.0703
	step [60/191], loss=75.1756
	step [61/191], loss=95.4995
	step [62/191], loss=73.0690
	step [63/191], loss=68.7813
	step [64/191], loss=74.8573
	step [65/191], loss=77.1607
	step [66/191], loss=75.3768
	step [67/191], loss=70.4333
	step [68/191], loss=68.8063
	step [69/191], loss=68.8008
	step [70/191], loss=83.6883
	step [71/191], loss=74.0999
	step [72/191], loss=72.1518
	step [73/191], loss=72.6588
	step [74/191], loss=76.8164
	step [75/191], loss=70.6211
	step [76/191], loss=85.9566
	step [77/191], loss=70.2692
	step [78/191], loss=67.1596
	step [79/191], loss=63.3713
	step [80/191], loss=75.3534
	step [81/191], loss=74.5586
	step [82/191], loss=68.1562
	step [83/191], loss=81.0658
	step [84/191], loss=65.4088
	step [85/191], loss=61.6615
	step [86/191], loss=73.3871
	step [87/191], loss=76.0505
	step [88/191], loss=88.1039
	step [89/191], loss=74.8916
	step [90/191], loss=77.4835
	step [91/191], loss=73.0521
	step [92/191], loss=69.9613
	step [93/191], loss=66.0580
	step [94/191], loss=65.9711
	step [95/191], loss=82.9808
	step [96/191], loss=77.3026
	step [97/191], loss=71.7690
	step [98/191], loss=70.6545
	step [99/191], loss=63.7625
	step [100/191], loss=56.8913
	step [101/191], loss=61.9775
	step [102/191], loss=66.3220
	step [103/191], loss=59.0193
	step [104/191], loss=65.4758
	step [105/191], loss=79.1456
	step [106/191], loss=84.0701
	step [107/191], loss=76.6276
	step [108/191], loss=76.0510
	step [109/191], loss=60.1998
	step [110/191], loss=60.6261
	step [111/191], loss=67.7278
	step [112/191], loss=65.4897
	step [113/191], loss=67.0789
	step [114/191], loss=79.3920
	step [115/191], loss=83.6096
	step [116/191], loss=67.7496
	step [117/191], loss=71.1118
	step [118/191], loss=86.2777
	step [119/191], loss=72.6890
	step [120/191], loss=72.0950
	step [121/191], loss=77.3047
	step [122/191], loss=82.1805
	step [123/191], loss=72.7172
	step [124/191], loss=78.7233
	step [125/191], loss=83.4774
	step [126/191], loss=86.2326
	step [127/191], loss=82.6239
	step [128/191], loss=59.8873
	step [129/191], loss=84.1308
	step [130/191], loss=60.0390
	step [131/191], loss=67.1954
	step [132/191], loss=65.5527
	step [133/191], loss=73.3640
	step [134/191], loss=68.1737
	step [135/191], loss=79.2454
	step [136/191], loss=68.3978
	step [137/191], loss=67.0836
	step [138/191], loss=79.8082
	step [139/191], loss=78.1226
	step [140/191], loss=75.7626
	step [141/191], loss=75.1629
	step [142/191], loss=69.4670
	step [143/191], loss=64.7978
	step [144/191], loss=63.8938
	step [145/191], loss=76.7339
	step [146/191], loss=67.4833
	step [147/191], loss=68.1690
	step [148/191], loss=66.9988
	step [149/191], loss=61.2014
	step [150/191], loss=89.9728
	step [151/191], loss=71.0707
	step [152/191], loss=79.1912
	step [153/191], loss=69.7173
	step [154/191], loss=68.2924
	step [155/191], loss=92.7648
	step [156/191], loss=73.5988
	step [157/191], loss=67.0085
	step [158/191], loss=79.7951
	step [159/191], loss=71.2459
	step [160/191], loss=70.4302
	step [161/191], loss=65.5910
	step [162/191], loss=75.3516
	step [163/191], loss=79.0364
	step [164/191], loss=76.8189
	step [165/191], loss=70.4709
	step [166/191], loss=87.7970
	step [167/191], loss=60.2193
	step [168/191], loss=75.6842
	step [169/191], loss=62.4408
	step [170/191], loss=61.1708
	step [171/191], loss=60.3977
	step [172/191], loss=72.5556
	step [173/191], loss=77.6854
	step [174/191], loss=67.2386
	step [175/191], loss=81.6081
	step [176/191], loss=70.1951
	step [177/191], loss=72.4748
	step [178/191], loss=74.2807
	step [179/191], loss=72.7641
	step [180/191], loss=81.5967
	step [181/191], loss=76.0407
	step [182/191], loss=60.4762
	step [183/191], loss=73.5239
	step [184/191], loss=71.6982
	step [185/191], loss=67.1956
	step [186/191], loss=70.0991
	step [187/191], loss=70.6692
	step [188/191], loss=86.4852
	step [189/191], loss=64.3388
	step [190/191], loss=81.4952
	step [191/191], loss=41.0311
	Evaluating
	loss=0.0100, precision=0.3732, recall=0.8793, f1=0.5240
saving model as: 1_saved_model.pth
Training epoch 62
	step [1/191], loss=73.4845
	step [2/191], loss=68.6280
	step [3/191], loss=79.1988
	step [4/191], loss=73.0142
	step [5/191], loss=77.7368
	step [6/191], loss=68.7861
	step [7/191], loss=80.5048
	step [8/191], loss=73.5665
	step [9/191], loss=74.5878
	step [10/191], loss=59.5398
	step [11/191], loss=57.7383
	step [12/191], loss=66.7807
	step [13/191], loss=69.0912
	step [14/191], loss=63.2016
	step [15/191], loss=74.4863
	step [16/191], loss=70.1110
	step [17/191], loss=69.4454
	step [18/191], loss=63.1585
	step [19/191], loss=64.5807
	step [20/191], loss=76.4360
	step [21/191], loss=79.5636
	step [22/191], loss=72.8651
	step [23/191], loss=71.0098
	step [24/191], loss=77.7916
	step [25/191], loss=64.1860
	step [26/191], loss=64.1983
	step [27/191], loss=67.5741
	step [28/191], loss=70.5174
	step [29/191], loss=72.4616
	step [30/191], loss=64.0933
	step [31/191], loss=79.7404
	step [32/191], loss=65.6898
	step [33/191], loss=65.1635
	step [34/191], loss=62.0470
	step [35/191], loss=75.6502
	step [36/191], loss=65.1810
	step [37/191], loss=77.7498
	step [38/191], loss=77.8383
	step [39/191], loss=66.1588
	step [40/191], loss=68.6771
	step [41/191], loss=81.3227
	step [42/191], loss=64.2124
	step [43/191], loss=66.5216
	step [44/191], loss=72.9034
	step [45/191], loss=71.1975
	step [46/191], loss=69.6015
	step [47/191], loss=71.1358
	step [48/191], loss=77.7943
	step [49/191], loss=73.4285
	step [50/191], loss=67.8225
	step [51/191], loss=58.2723
	step [52/191], loss=72.6113
	step [53/191], loss=60.5381
	step [54/191], loss=68.4975
	step [55/191], loss=56.0898
	step [56/191], loss=70.4079
	step [57/191], loss=86.3909
	step [58/191], loss=77.5783
	step [59/191], loss=66.3015
	step [60/191], loss=65.2401
	step [61/191], loss=75.5072
	step [62/191], loss=76.9116
	step [63/191], loss=76.2738
	step [64/191], loss=67.3319
	step [65/191], loss=70.6882
	step [66/191], loss=84.5497
	step [67/191], loss=73.3186
	step [68/191], loss=64.9972
	step [69/191], loss=74.0553
	step [70/191], loss=67.5696
	step [71/191], loss=68.6250
	step [72/191], loss=55.5554
	step [73/191], loss=65.0657
	step [74/191], loss=80.1174
	step [75/191], loss=71.6892
	step [76/191], loss=60.5402
	step [77/191], loss=78.0667
	step [78/191], loss=67.3163
	step [79/191], loss=68.5117
	step [80/191], loss=75.2670
	step [81/191], loss=67.7906
	step [82/191], loss=66.9201
	step [83/191], loss=64.6101
	step [84/191], loss=68.9206
	step [85/191], loss=78.5425
	step [86/191], loss=70.8728
	step [87/191], loss=71.9155
	step [88/191], loss=73.4243
	step [89/191], loss=69.5136
	step [90/191], loss=63.8460
	step [91/191], loss=61.5146
	step [92/191], loss=74.9019
	step [93/191], loss=68.3454
	step [94/191], loss=72.4907
	step [95/191], loss=56.3105
	step [96/191], loss=60.3905
	step [97/191], loss=90.0710
	step [98/191], loss=78.7820
	step [99/191], loss=72.0744
	step [100/191], loss=74.9622
	step [101/191], loss=77.5152
	step [102/191], loss=71.5247
	step [103/191], loss=83.2722
	step [104/191], loss=71.6638
	step [105/191], loss=69.7431
	step [106/191], loss=72.5775
	step [107/191], loss=79.6977
	step [108/191], loss=62.3582
	step [109/191], loss=89.1861
	step [110/191], loss=84.5269
	step [111/191], loss=65.3928
	step [112/191], loss=76.3274
	step [113/191], loss=75.2282
	step [114/191], loss=51.6828
	step [115/191], loss=82.1884
	step [116/191], loss=58.3975
	step [117/191], loss=72.5418
	step [118/191], loss=75.4792
	step [119/191], loss=61.0771
	step [120/191], loss=80.9115
	step [121/191], loss=65.0780
	step [122/191], loss=89.9757
	step [123/191], loss=67.6628
	step [124/191], loss=80.4901
	step [125/191], loss=74.7327
	step [126/191], loss=74.4092
	step [127/191], loss=64.3144
	step [128/191], loss=79.0258
	step [129/191], loss=73.6654
	step [130/191], loss=96.0906
	step [131/191], loss=79.2331
	step [132/191], loss=67.0574
	step [133/191], loss=57.8473
	step [134/191], loss=82.6354
	step [135/191], loss=79.9528
	step [136/191], loss=68.5152
	step [137/191], loss=62.5861
	step [138/191], loss=64.1180
	step [139/191], loss=70.4357
	step [140/191], loss=66.7935
	step [141/191], loss=73.5193
	step [142/191], loss=75.7858
	step [143/191], loss=61.0033
	step [144/191], loss=69.7412
	step [145/191], loss=74.3691
	step [146/191], loss=72.8571
	step [147/191], loss=77.1691
	step [148/191], loss=65.1805
	step [149/191], loss=68.4313
	step [150/191], loss=69.0045
	step [151/191], loss=64.7377
	step [152/191], loss=70.4313
	step [153/191], loss=66.2676
	step [154/191], loss=71.4039
	step [155/191], loss=67.8643
	step [156/191], loss=73.8346
	step [157/191], loss=63.2179
	step [158/191], loss=80.8110
	step [159/191], loss=77.8645
	step [160/191], loss=72.5593
	step [161/191], loss=76.4255
	step [162/191], loss=73.8083
	step [163/191], loss=70.8431
	step [164/191], loss=68.2436
	step [165/191], loss=71.2885
	step [166/191], loss=60.5247
	step [167/191], loss=62.3456
	step [168/191], loss=67.2206
	step [169/191], loss=64.4835
	step [170/191], loss=81.0122
	step [171/191], loss=74.2472
	step [172/191], loss=98.5431
	step [173/191], loss=62.3360
	step [174/191], loss=78.9953
	step [175/191], loss=70.4830
	step [176/191], loss=71.6533
	step [177/191], loss=70.6022
	step [178/191], loss=87.2831
	step [179/191], loss=81.9395
	step [180/191], loss=72.1485
	step [181/191], loss=79.5823
	step [182/191], loss=71.8362
	step [183/191], loss=78.8038
	step [184/191], loss=63.5083
	step [185/191], loss=75.0311
	step [186/191], loss=69.4495
	step [187/191], loss=80.5070
	step [188/191], loss=71.8562
	step [189/191], loss=67.6534
	step [190/191], loss=90.6105
	step [191/191], loss=35.2645
	Evaluating
	loss=0.0090, precision=0.4026, recall=0.8631, f1=0.5491
saving model as: 1_saved_model.pth
Training epoch 63
	step [1/191], loss=79.5996
	step [2/191], loss=76.8144
	step [3/191], loss=59.2332
	step [4/191], loss=74.4474
	step [5/191], loss=71.0744
	step [6/191], loss=69.6722
	step [7/191], loss=77.1926
	step [8/191], loss=79.9078
	step [9/191], loss=86.3486
	step [10/191], loss=70.1218
	step [11/191], loss=80.5774
	step [12/191], loss=66.2789
	step [13/191], loss=68.5983
	step [14/191], loss=75.5946
	step [15/191], loss=61.0642
	step [16/191], loss=62.4167
	step [17/191], loss=73.3128
	step [18/191], loss=77.1899
	step [19/191], loss=68.6782
	step [20/191], loss=66.4792
	step [21/191], loss=82.6109
	step [22/191], loss=69.3775
	step [23/191], loss=69.5586
	step [24/191], loss=70.4132
	step [25/191], loss=71.3888
	step [26/191], loss=79.9140
	step [27/191], loss=67.1709
	step [28/191], loss=65.6071
	step [29/191], loss=81.6308
	step [30/191], loss=77.0625
	step [31/191], loss=74.5143
	step [32/191], loss=67.0348
	step [33/191], loss=75.1091
	step [34/191], loss=74.6083
	step [35/191], loss=65.5920
	step [36/191], loss=74.2132
	step [37/191], loss=53.7402
	step [38/191], loss=76.4887
	step [39/191], loss=65.0998
	step [40/191], loss=67.8615
	step [41/191], loss=71.0033
	step [42/191], loss=75.3275
	step [43/191], loss=80.9470
	step [44/191], loss=69.7374
	step [45/191], loss=84.9871
	step [46/191], loss=65.8865
	step [47/191], loss=76.3373
	step [48/191], loss=68.2115
	step [49/191], loss=70.7520
	step [50/191], loss=65.4837
	step [51/191], loss=65.1187
	step [52/191], loss=72.8253
	step [53/191], loss=66.7781
	step [54/191], loss=58.3235
	step [55/191], loss=71.5413
	step [56/191], loss=64.3955
	step [57/191], loss=73.5722
	step [58/191], loss=66.8843
	step [59/191], loss=83.1640
	step [60/191], loss=79.1564
	step [61/191], loss=72.1526
	step [62/191], loss=88.7649
	step [63/191], loss=69.0414
	step [64/191], loss=66.3561
	step [65/191], loss=87.7115
	step [66/191], loss=68.2859
	step [67/191], loss=74.4373
	step [68/191], loss=65.7666
	step [69/191], loss=65.9336
	step [70/191], loss=69.2558
	step [71/191], loss=70.6422
	step [72/191], loss=75.2909
	step [73/191], loss=68.2781
	step [74/191], loss=72.6111
	step [75/191], loss=65.9566
	step [76/191], loss=75.2528
	step [77/191], loss=87.6821
	step [78/191], loss=71.0335
	step [79/191], loss=76.2582
	step [80/191], loss=74.0821
	step [81/191], loss=79.1857
	step [82/191], loss=61.8140
	step [83/191], loss=59.7067
	step [84/191], loss=65.8906
	step [85/191], loss=62.2125
	step [86/191], loss=64.4459
	step [87/191], loss=76.2184
	step [88/191], loss=88.0878
	step [89/191], loss=70.3747
	step [90/191], loss=71.7033
	step [91/191], loss=65.6751
	step [92/191], loss=75.3325
	step [93/191], loss=62.6064
	step [94/191], loss=74.0012
	step [95/191], loss=56.7507
	step [96/191], loss=68.0415
	step [97/191], loss=83.0843
	step [98/191], loss=72.0854
	step [99/191], loss=70.3127
	step [100/191], loss=68.6282
	step [101/191], loss=69.8889
	step [102/191], loss=66.0629
	step [103/191], loss=61.6478
	step [104/191], loss=77.3243
	step [105/191], loss=70.2519
	step [106/191], loss=75.5586
	step [107/191], loss=73.2610
	step [108/191], loss=70.9153
	step [109/191], loss=71.2082
	step [110/191], loss=69.9642
	step [111/191], loss=70.9869
	step [112/191], loss=70.2177
	step [113/191], loss=74.0883
	step [114/191], loss=58.4018
	step [115/191], loss=74.0922
	step [116/191], loss=66.6632
	step [117/191], loss=70.4124
	step [118/191], loss=67.3260
	step [119/191], loss=77.9355
	step [120/191], loss=74.5207
	step [121/191], loss=67.6438
	step [122/191], loss=66.9292
	step [123/191], loss=57.0550
	step [124/191], loss=66.3704
	step [125/191], loss=61.6772
	step [126/191], loss=74.7836
	step [127/191], loss=63.0195
	step [128/191], loss=66.9300
	step [129/191], loss=68.0506
	step [130/191], loss=70.7305
	step [131/191], loss=76.0607
	step [132/191], loss=69.3998
	step [133/191], loss=67.6043
	step [134/191], loss=69.5442
	step [135/191], loss=65.4494
	step [136/191], loss=54.5381
	step [137/191], loss=80.0464
	step [138/191], loss=63.9701
	step [139/191], loss=69.0811
	step [140/191], loss=78.5953
	step [141/191], loss=58.5283
	step [142/191], loss=77.0277
	step [143/191], loss=76.8130
	step [144/191], loss=68.6389
	step [145/191], loss=67.3508
	step [146/191], loss=81.7799
	step [147/191], loss=62.2121
	step [148/191], loss=75.7274
	step [149/191], loss=72.0161
	step [150/191], loss=59.8909
	step [151/191], loss=74.3635
	step [152/191], loss=80.1340
	step [153/191], loss=76.1702
	step [154/191], loss=63.7499
	step [155/191], loss=82.6267
	step [156/191], loss=95.2991
	step [157/191], loss=69.1837
	step [158/191], loss=72.8118
	step [159/191], loss=74.0462
	step [160/191], loss=71.8021
	step [161/191], loss=79.1772
	step [162/191], loss=59.5498
	step [163/191], loss=62.9245
	step [164/191], loss=68.3114
	step [165/191], loss=70.3706
	step [166/191], loss=80.8258
	step [167/191], loss=72.6351
	step [168/191], loss=75.9163
	step [169/191], loss=72.8964
	step [170/191], loss=79.5625
	step [171/191], loss=68.3713
	step [172/191], loss=81.2675
	step [173/191], loss=71.2263
	step [174/191], loss=82.0632
	step [175/191], loss=77.2790
	step [176/191], loss=74.3774
	step [177/191], loss=76.1531
	step [178/191], loss=66.0248
	step [179/191], loss=61.6110
	step [180/191], loss=76.6885
	step [181/191], loss=70.6317
	step [182/191], loss=83.8532
	step [183/191], loss=63.6626
	step [184/191], loss=70.9937
	step [185/191], loss=65.9576
	step [186/191], loss=72.2124
	step [187/191], loss=67.8812
	step [188/191], loss=63.2527
	step [189/191], loss=67.4281
	step [190/191], loss=73.8116
	step [191/191], loss=43.3870
	Evaluating
	loss=0.0089, precision=0.4113, recall=0.8563, f1=0.5557
saving model as: 1_saved_model.pth
Training epoch 64
	step [1/191], loss=79.9000
	step [2/191], loss=76.7112
	step [3/191], loss=57.4897
	step [4/191], loss=63.6327
	step [5/191], loss=77.8139
	step [6/191], loss=78.1889
	step [7/191], loss=84.4485
	step [8/191], loss=82.9043
	step [9/191], loss=66.9785
	step [10/191], loss=69.0656
	step [11/191], loss=74.9380
	step [12/191], loss=75.2929
	step [13/191], loss=69.6162
	step [14/191], loss=72.6961
	step [15/191], loss=72.6476
	step [16/191], loss=74.3291
	step [17/191], loss=73.4153
	step [18/191], loss=99.2345
	step [19/191], loss=57.2282
	step [20/191], loss=72.9973
	step [21/191], loss=74.9403
	step [22/191], loss=85.2536
	step [23/191], loss=81.6533
	step [24/191], loss=67.8388
	step [25/191], loss=73.4224
	step [26/191], loss=60.3366
	step [27/191], loss=79.2540
	step [28/191], loss=76.7208
	step [29/191], loss=69.7976
	step [30/191], loss=77.0083
	step [31/191], loss=74.2998
	step [32/191], loss=69.2097
	step [33/191], loss=70.6616
	step [34/191], loss=72.7679
	step [35/191], loss=65.5966
	step [36/191], loss=75.5476
	step [37/191], loss=72.6611
	step [38/191], loss=73.2854
	step [39/191], loss=77.4853
	step [40/191], loss=68.1249
	step [41/191], loss=66.0956
	step [42/191], loss=76.3544
	step [43/191], loss=66.4382
	step [44/191], loss=76.0515
	step [45/191], loss=70.2674
	step [46/191], loss=70.1073
	step [47/191], loss=69.1184
	step [48/191], loss=74.4642
	step [49/191], loss=75.2142
	step [50/191], loss=72.8829
	step [51/191], loss=67.3287
	step [52/191], loss=66.9409
	step [53/191], loss=57.0003
	step [54/191], loss=71.3934
	step [55/191], loss=76.2369
	step [56/191], loss=67.7817
	step [57/191], loss=76.6336
	step [58/191], loss=58.0409
	step [59/191], loss=80.4962
	step [60/191], loss=71.8590
	step [61/191], loss=66.4916
	step [62/191], loss=64.3403
	step [63/191], loss=58.2279
	step [64/191], loss=75.4130
	step [65/191], loss=72.4007
	step [66/191], loss=74.6432
	step [67/191], loss=70.3598
	step [68/191], loss=62.3063
	step [69/191], loss=75.1795
	step [70/191], loss=65.7107
	step [71/191], loss=68.4307
	step [72/191], loss=84.4977
	step [73/191], loss=78.9194
	step [74/191], loss=82.0524
	step [75/191], loss=73.2154
	step [76/191], loss=65.5894
	step [77/191], loss=69.7022
	step [78/191], loss=85.1822
	step [79/191], loss=67.2046
	step [80/191], loss=66.0028
	step [81/191], loss=68.3959
	step [82/191], loss=68.9864
	step [83/191], loss=74.9649
	step [84/191], loss=63.5438
	step [85/191], loss=64.2916
	step [86/191], loss=70.9812
	step [87/191], loss=63.8936
	step [88/191], loss=64.2541
	step [89/191], loss=69.3649
	step [90/191], loss=61.8770
	step [91/191], loss=69.3756
	step [92/191], loss=70.7243
	step [93/191], loss=89.5881
	step [94/191], loss=79.3900
	step [95/191], loss=72.0041
	step [96/191], loss=77.4448
	step [97/191], loss=72.0910
	step [98/191], loss=80.3135
	step [99/191], loss=66.3868
	step [100/191], loss=65.1492
	step [101/191], loss=60.2836
	step [102/191], loss=60.0713
	step [103/191], loss=68.8078
	step [104/191], loss=62.4231
	step [105/191], loss=68.1240
	step [106/191], loss=65.9615
	step [107/191], loss=68.9880
	step [108/191], loss=74.5233
	step [109/191], loss=60.3056
	step [110/191], loss=87.1283
	step [111/191], loss=76.2079
	step [112/191], loss=92.1665
	step [113/191], loss=73.0561
	step [114/191], loss=66.5967
	step [115/191], loss=57.1829
	step [116/191], loss=69.0030
	step [117/191], loss=62.0429
	step [118/191], loss=68.3967
	step [119/191], loss=71.6062
	step [120/191], loss=68.0243
	step [121/191], loss=79.4738
	step [122/191], loss=62.5658
	step [123/191], loss=85.4347
	step [124/191], loss=57.4354
	step [125/191], loss=72.4377
	step [126/191], loss=68.9504
	step [127/191], loss=75.1841
	step [128/191], loss=59.8096
	step [129/191], loss=67.9791
	step [130/191], loss=68.3837
	step [131/191], loss=64.6372
	step [132/191], loss=74.7669
	step [133/191], loss=66.5396
	step [134/191], loss=66.1831
	step [135/191], loss=71.3847
	step [136/191], loss=77.3963
	step [137/191], loss=81.3906
	step [138/191], loss=81.3848
	step [139/191], loss=78.9132
	step [140/191], loss=73.8771
	step [141/191], loss=73.5392
	step [142/191], loss=62.8048
	step [143/191], loss=64.8649
	step [144/191], loss=66.3140
	step [145/191], loss=71.3035
	step [146/191], loss=60.2170
	step [147/191], loss=70.8627
	step [148/191], loss=63.3786
	step [149/191], loss=65.4379
	step [150/191], loss=66.9686
	step [151/191], loss=62.6401
	step [152/191], loss=74.5974
	step [153/191], loss=82.8467
	step [154/191], loss=64.7604
	step [155/191], loss=76.8300
	step [156/191], loss=71.5408
	step [157/191], loss=68.7289
	step [158/191], loss=60.1120
	step [159/191], loss=67.5712
	step [160/191], loss=58.3630
	step [161/191], loss=88.4995
	step [162/191], loss=64.3845
	step [163/191], loss=59.4262
	step [164/191], loss=79.5866
	step [165/191], loss=83.3110
	step [166/191], loss=72.8335
	step [167/191], loss=68.9422
	step [168/191], loss=68.9485
	step [169/191], loss=56.6350
	step [170/191], loss=61.9677
	step [171/191], loss=79.8767
	step [172/191], loss=74.8854
	step [173/191], loss=69.0955
	step [174/191], loss=64.1591
	step [175/191], loss=80.3213
	step [176/191], loss=61.1727
	step [177/191], loss=80.4524
	step [178/191], loss=59.4557
	step [179/191], loss=85.5000
	step [180/191], loss=70.8120
	step [181/191], loss=78.7857
	step [182/191], loss=69.5101
	step [183/191], loss=69.4279
	step [184/191], loss=69.6111
	step [185/191], loss=65.6806
	step [186/191], loss=69.7883
	step [187/191], loss=68.2421
	step [188/191], loss=72.8217
	step [189/191], loss=75.0194
	step [190/191], loss=65.3329
	step [191/191], loss=35.2501
	Evaluating
	loss=0.0077, precision=0.4531, recall=0.8577, f1=0.5929
saving model as: 1_saved_model.pth
Training epoch 65
	step [1/191], loss=70.7763
	step [2/191], loss=71.6265
	step [3/191], loss=63.4063
	step [4/191], loss=66.4893
	step [5/191], loss=69.4358
	step [6/191], loss=68.6393
	step [7/191], loss=72.0302
	step [8/191], loss=60.9625
	step [9/191], loss=78.5162
	step [10/191], loss=77.5985
	step [11/191], loss=76.5039
	step [12/191], loss=73.4072
	step [13/191], loss=58.4796
	step [14/191], loss=57.4659
	step [15/191], loss=75.8108
	step [16/191], loss=65.7688
	step [17/191], loss=74.7458
	step [18/191], loss=74.1820
	step [19/191], loss=77.5470
	step [20/191], loss=72.1952
	step [21/191], loss=77.4861
	step [22/191], loss=67.4266
	step [23/191], loss=72.1570
	step [24/191], loss=70.4679
	step [25/191], loss=72.4176
	step [26/191], loss=75.0310
	step [27/191], loss=71.7439
	step [28/191], loss=69.2488
	step [29/191], loss=71.2567
	step [30/191], loss=61.5175
	step [31/191], loss=67.1178
	step [32/191], loss=74.6236
	step [33/191], loss=57.2765
	step [34/191], loss=72.2768
	step [35/191], loss=79.8832
	step [36/191], loss=76.6537
	step [37/191], loss=63.5727
	step [38/191], loss=58.7256
	step [39/191], loss=78.0711
	step [40/191], loss=56.4694
	step [41/191], loss=78.8665
	step [42/191], loss=67.8462
	step [43/191], loss=56.6764
	step [44/191], loss=78.0740
	step [45/191], loss=72.6978
	step [46/191], loss=87.0310
	step [47/191], loss=74.5790
	step [48/191], loss=78.4010
	step [49/191], loss=62.2587
	step [50/191], loss=59.7778
	step [51/191], loss=85.2079
	step [52/191], loss=66.9587
	step [53/191], loss=69.7299
	step [54/191], loss=69.8535
	step [55/191], loss=65.6777
	step [56/191], loss=56.2950
	step [57/191], loss=73.1125
	step [58/191], loss=74.1438
	step [59/191], loss=84.1868
	step [60/191], loss=69.0033
	step [61/191], loss=84.0593
	step [62/191], loss=69.7304
	step [63/191], loss=77.9336
	step [64/191], loss=63.5168
	step [65/191], loss=84.3688
	step [66/191], loss=75.6997
	step [67/191], loss=70.8884
	step [68/191], loss=73.2428
	step [69/191], loss=81.2805
	step [70/191], loss=70.9089
	step [71/191], loss=78.5806
	step [72/191], loss=77.8826
	step [73/191], loss=65.7967
	step [74/191], loss=64.4360
	step [75/191], loss=72.2532
	step [76/191], loss=83.9546
	step [77/191], loss=69.8852
	step [78/191], loss=73.0537
	step [79/191], loss=64.1919
	step [80/191], loss=66.0818
	step [81/191], loss=73.0506
	step [82/191], loss=78.1741
	step [83/191], loss=64.7614
	step [84/191], loss=72.9380
	step [85/191], loss=68.6041
	step [86/191], loss=63.5463
	step [87/191], loss=77.1670
	step [88/191], loss=73.1895
	step [89/191], loss=63.0969
	step [90/191], loss=69.1461
	step [91/191], loss=71.4057
	step [92/191], loss=60.6595
	step [93/191], loss=79.0198
	step [94/191], loss=73.7579
	step [95/191], loss=78.2358
	step [96/191], loss=66.8683
	step [97/191], loss=74.6334
	step [98/191], loss=70.1416
	step [99/191], loss=65.5381
	step [100/191], loss=68.4682
	step [101/191], loss=64.7449
	step [102/191], loss=80.6116
	step [103/191], loss=69.1604
	step [104/191], loss=80.2721
	step [105/191], loss=64.1611
	step [106/191], loss=55.0682
	step [107/191], loss=87.0005
	step [108/191], loss=67.8109
	step [109/191], loss=79.9297
	step [110/191], loss=66.2484
	step [111/191], loss=64.2799
	step [112/191], loss=59.5028
	step [113/191], loss=63.3984
	step [114/191], loss=71.4076
	step [115/191], loss=70.8807
	step [116/191], loss=63.3283
	step [117/191], loss=70.0010
	step [118/191], loss=69.0717
	step [119/191], loss=63.3640
	step [120/191], loss=72.7621
	step [121/191], loss=69.0079
	step [122/191], loss=77.8118
	step [123/191], loss=62.1836
	step [124/191], loss=70.5474
	step [125/191], loss=68.5540
	step [126/191], loss=72.5603
	step [127/191], loss=64.8004
	step [128/191], loss=74.4366
	step [129/191], loss=69.7187
	step [130/191], loss=58.6516
	step [131/191], loss=66.7395
	step [132/191], loss=74.0934
	step [133/191], loss=79.0882
	step [134/191], loss=65.5808
	step [135/191], loss=72.8556
	step [136/191], loss=75.0963
	step [137/191], loss=75.7861
	step [138/191], loss=79.4796
	step [139/191], loss=61.6937
	step [140/191], loss=82.3318
	step [141/191], loss=75.1194
	step [142/191], loss=59.9402
	step [143/191], loss=79.1324
	step [144/191], loss=74.1079
	step [145/191], loss=58.6803
	step [146/191], loss=71.1196
	step [147/191], loss=69.4589
	step [148/191], loss=72.6295
	step [149/191], loss=77.3852
	step [150/191], loss=73.8164
	step [151/191], loss=68.3873
	step [152/191], loss=69.7770
	step [153/191], loss=61.8626
	step [154/191], loss=82.5981
	step [155/191], loss=68.5050
	step [156/191], loss=63.9987
	step [157/191], loss=88.5099
	step [158/191], loss=69.0124
	step [159/191], loss=63.3105
	step [160/191], loss=74.4073
	step [161/191], loss=77.9615
	step [162/191], loss=75.2456
	step [163/191], loss=68.9016
	step [164/191], loss=63.1052
	step [165/191], loss=61.8132
	step [166/191], loss=72.8877
	step [167/191], loss=85.3305
	step [168/191], loss=61.7327
	step [169/191], loss=69.3990
	step [170/191], loss=69.1264
	step [171/191], loss=63.9742
	step [172/191], loss=75.2323
	step [173/191], loss=83.9294
	step [174/191], loss=66.0648
	step [175/191], loss=60.4656
	step [176/191], loss=67.7341
	step [177/191], loss=67.8490
	step [178/191], loss=70.8374
	step [179/191], loss=78.4101
	step [180/191], loss=69.7502
	step [181/191], loss=64.0169
	step [182/191], loss=68.6327
	step [183/191], loss=67.8060
	step [184/191], loss=82.8476
	step [185/191], loss=77.6615
	step [186/191], loss=71.1899
	step [187/191], loss=74.7884
	step [188/191], loss=68.6623
	step [189/191], loss=60.2980
	step [190/191], loss=72.1575
	step [191/191], loss=35.5648
	Evaluating
	loss=0.0078, precision=0.4510, recall=0.8560, f1=0.5907
Training epoch 66
