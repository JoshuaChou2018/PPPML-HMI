Loading anaconda...
...Anaconda env loaded
directing: X rim_enhanced: True test_id 0
removed wrong scan: weights_X_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_22_xwqg-B00034_2020-04-03.npy
# all image files: 12135 # all weight files in weight_dir: 9412 # image files with weight 9372
removed wrong scan: weights_X_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_22_xwqg-B00034_2020-04-03.npy
# all image files: 12135 # all weight files in weight_dir: 2471 # image files with weight 2460
train: /ibex/scratch/projects/c2052/air_tube_seg/training_samples_enhanced_one/X 9372
Using 4 GPUs
Going to train epochs [1-30]
Training epoch 1
	step [1/147], loss=931.3998
	step [2/147], loss=586.3656
	step [3/147], loss=350.2540
	step [4/147], loss=372.4101
	step [5/147], loss=249.7636
	step [6/147], loss=287.5311
	step [7/147], loss=273.4005
	step [8/147], loss=235.6926
	step [9/147], loss=287.5609
	step [10/147], loss=271.0133
	step [11/147], loss=266.0037
	step [12/147], loss=262.8051
	step [13/147], loss=255.3835
	step [14/147], loss=282.8762
	step [15/147], loss=286.6959
	step [16/147], loss=275.0050
	step [17/147], loss=265.5476
	step [18/147], loss=216.9398
	step [19/147], loss=256.7964
	step [20/147], loss=261.2220
	step [21/147], loss=257.4872
	step [22/147], loss=242.8081
	step [23/147], loss=254.8944
	step [24/147], loss=253.4047
	step [25/147], loss=243.7693
	step [26/147], loss=267.0721
	step [27/147], loss=217.6824
	step [28/147], loss=238.2722
	step [29/147], loss=266.6088
	step [30/147], loss=228.8926
	step [31/147], loss=254.0177
	step [32/147], loss=258.5979
	step [33/147], loss=223.3676
	step [34/147], loss=204.0260
	step [35/147], loss=236.4125
	step [36/147], loss=223.5029
	step [37/147], loss=210.8386
	step [38/147], loss=236.2569
	step [39/147], loss=214.4856
	step [40/147], loss=229.2578
	step [41/147], loss=212.4766
	step [42/147], loss=221.4299
	step [43/147], loss=216.2799
	step [44/147], loss=238.0059
	step [45/147], loss=205.5686
	step [46/147], loss=232.4509
	step [47/147], loss=214.6353
	step [48/147], loss=199.4880
	step [49/147], loss=218.5034
	step [50/147], loss=192.2929
	step [51/147], loss=234.9698
	step [52/147], loss=220.5749
	step [53/147], loss=225.3225
	step [54/147], loss=210.8269
	step [55/147], loss=224.5489
	step [56/147], loss=200.0999
	step [57/147], loss=199.8019
	step [58/147], loss=214.7463
	step [59/147], loss=186.3328
	step [60/147], loss=191.0195
	step [61/147], loss=190.3040
	step [62/147], loss=195.9495
	step [63/147], loss=234.1596
	step [64/147], loss=207.0228
	step [65/147], loss=208.6527
	step [66/147], loss=218.2595
	step [67/147], loss=198.3691
	step [68/147], loss=196.5775
	step [69/147], loss=212.4680
	step [70/147], loss=192.3359
	step [71/147], loss=227.3880
	step [72/147], loss=199.3388
	step [73/147], loss=209.6041
	step [74/147], loss=189.7848
	step [75/147], loss=205.0313
	step [76/147], loss=207.8104
	step [77/147], loss=208.1096
	step [78/147], loss=199.2008
	step [79/147], loss=206.4747
	step [80/147], loss=208.9357
	step [81/147], loss=195.9712
	step [82/147], loss=202.5405
	step [83/147], loss=183.0609
	step [84/147], loss=200.4120
	step [85/147], loss=204.2466
	step [86/147], loss=230.5238
	step [87/147], loss=209.2666
	step [88/147], loss=203.2977
	step [89/147], loss=180.6609
	step [90/147], loss=202.5858
	step [91/147], loss=188.2217
	step [92/147], loss=246.8143
	step [93/147], loss=198.4576
	step [94/147], loss=203.7767
	step [95/147], loss=190.5699
	step [96/147], loss=199.0861
	step [97/147], loss=180.6558
	step [98/147], loss=177.2037
	step [99/147], loss=218.4900
	step [100/147], loss=176.2921
	step [101/147], loss=202.1368
	step [102/147], loss=174.1984
	step [103/147], loss=183.5137
	step [104/147], loss=175.8727
	step [105/147], loss=178.1298
	step [106/147], loss=194.6071
	step [107/147], loss=197.2655
	step [108/147], loss=166.6025
	step [109/147], loss=188.3267
	step [110/147], loss=187.9243
	step [111/147], loss=165.9749
	step [112/147], loss=189.3281
	step [113/147], loss=203.4802
	step [114/147], loss=197.7498
	step [115/147], loss=167.9555
	step [116/147], loss=184.9982
	step [117/147], loss=202.4736
	step [118/147], loss=203.8889
	step [119/147], loss=181.9818
	step [120/147], loss=202.0998
	step [121/147], loss=204.8542
	step [122/147], loss=180.4198
	step [123/147], loss=194.6319
	step [124/147], loss=194.8231
	step [125/147], loss=199.8872
	step [126/147], loss=178.1956
	step [127/147], loss=172.1974
	step [128/147], loss=200.2413
	step [129/147], loss=171.6349
	step [130/147], loss=189.8196
	step [131/147], loss=188.9610
	step [132/147], loss=193.2827
	step [133/147], loss=182.7962
	step [134/147], loss=190.7502
	step [135/147], loss=177.3620
	step [136/147], loss=204.0503
	step [137/147], loss=172.8439
	step [138/147], loss=161.7446
	step [139/147], loss=175.7296
	step [140/147], loss=164.9758
	step [141/147], loss=186.8806
	step [142/147], loss=203.3500
	step [143/147], loss=173.4005
	step [144/147], loss=159.1191
	step [145/147], loss=167.8928
	step [146/147], loss=190.8061
	step [147/147], loss=69.6048
	Evaluating
	loss=0.2738, precision=0.2643, recall=0.9156, f1=0.4101
saving model as: 0_saved_model.pth
Training epoch 2
	step [1/147], loss=178.7067
	step [2/147], loss=174.0564
	step [3/147], loss=201.8895
	step [4/147], loss=173.7881
	step [5/147], loss=177.4178
	step [6/147], loss=183.5330
	step [7/147], loss=194.0428
	step [8/147], loss=179.9355
	step [9/147], loss=188.8734
	step [10/147], loss=173.7745
	step [11/147], loss=176.7643
	step [12/147], loss=199.0788
	step [13/147], loss=202.9408
	step [14/147], loss=199.2999
	step [15/147], loss=189.1407
	step [16/147], loss=175.2590
	step [17/147], loss=212.7311
	step [18/147], loss=179.6461
	step [19/147], loss=182.4153
	step [20/147], loss=159.8300
	step [21/147], loss=155.9532
	step [22/147], loss=203.4566
	step [23/147], loss=148.4554
	step [24/147], loss=166.9855
	step [25/147], loss=180.6041
	step [26/147], loss=194.0408
	step [27/147], loss=172.6544
	step [28/147], loss=160.9199
	step [29/147], loss=166.9146
	step [30/147], loss=172.5652
	step [31/147], loss=164.0741
	step [32/147], loss=165.8042
	step [33/147], loss=194.9212
	step [34/147], loss=162.8954
	step [35/147], loss=184.1910
	step [36/147], loss=171.3956
	step [37/147], loss=178.6047
	step [38/147], loss=173.0467
	step [39/147], loss=205.5518
	step [40/147], loss=197.8745
	step [41/147], loss=166.9742
	step [42/147], loss=161.5522
	step [43/147], loss=182.1058
	step [44/147], loss=181.7272
	step [45/147], loss=163.0991
	step [46/147], loss=187.6870
	step [47/147], loss=174.4338
	step [48/147], loss=180.8381
	step [49/147], loss=158.9724
	step [50/147], loss=169.6496
	step [51/147], loss=199.4428
	step [52/147], loss=151.8243
	step [53/147], loss=176.3540
	step [54/147], loss=162.9725
	step [55/147], loss=191.3406
	step [56/147], loss=171.1833
	step [57/147], loss=169.7234
	step [58/147], loss=163.4814
	step [59/147], loss=183.6219
	step [60/147], loss=171.2263
	step [61/147], loss=171.3859
	step [62/147], loss=186.7972
	step [63/147], loss=194.7154
	step [64/147], loss=197.0676
	step [65/147], loss=175.3281
	step [66/147], loss=204.3015
	step [67/147], loss=168.7316
	step [68/147], loss=172.2378
	step [69/147], loss=172.4992
	step [70/147], loss=150.1339
	step [71/147], loss=164.9559
	step [72/147], loss=192.9539
	step [73/147], loss=165.7821
	step [74/147], loss=213.7621
	step [75/147], loss=165.1489
	step [76/147], loss=176.9629
	step [77/147], loss=176.0327
	step [78/147], loss=150.3517
	step [79/147], loss=158.8515
	step [80/147], loss=177.0193
	step [81/147], loss=166.9778
	step [82/147], loss=199.2157
	step [83/147], loss=164.8425
	step [84/147], loss=157.0879
	step [85/147], loss=164.1499
	step [86/147], loss=166.9924
	step [87/147], loss=187.2474
	step [88/147], loss=160.2840
	step [89/147], loss=180.4034
	step [90/147], loss=170.0999
	step [91/147], loss=184.5270
	step [92/147], loss=184.8825
	step [93/147], loss=176.3646
	step [94/147], loss=162.4300
	step [95/147], loss=191.9272
	step [96/147], loss=145.0797
	step [97/147], loss=143.2597
	step [98/147], loss=153.8317
	step [99/147], loss=156.3885
	step [100/147], loss=167.9172
	step [101/147], loss=172.7494
	step [102/147], loss=164.9126
	step [103/147], loss=186.1380
	step [104/147], loss=158.2514
	step [105/147], loss=153.4458
	step [106/147], loss=168.3649
	step [107/147], loss=188.5189
	step [108/147], loss=160.0131
	step [109/147], loss=186.4469
	step [110/147], loss=175.6038
	step [111/147], loss=160.5206
	step [112/147], loss=175.7069
	step [113/147], loss=178.6116
	step [114/147], loss=176.7053
	step [115/147], loss=170.6595
	step [116/147], loss=152.2268
	step [117/147], loss=172.5993
	step [118/147], loss=166.2630
	step [119/147], loss=147.2787
	step [120/147], loss=173.9411
	step [121/147], loss=182.3777
	step [122/147], loss=160.3570
	step [123/147], loss=175.2487
	step [124/147], loss=155.5132
	step [125/147], loss=163.1747
	step [126/147], loss=169.3075
	step [127/147], loss=168.9002
	step [128/147], loss=155.0145
	step [129/147], loss=169.2213
	step [130/147], loss=188.8992
	step [131/147], loss=182.5492
	step [132/147], loss=179.2545
	step [133/147], loss=188.2771
	step [134/147], loss=168.2440
	step [135/147], loss=149.4590
	step [136/147], loss=160.0581
	step [137/147], loss=146.6355
	step [138/147], loss=164.8130
	step [139/147], loss=174.0104
	step [140/147], loss=147.3208
	step [141/147], loss=157.9854
	step [142/147], loss=186.6084
	step [143/147], loss=160.4319
	step [144/147], loss=139.6183
	step [145/147], loss=148.9111
	step [146/147], loss=186.8001
	step [147/147], loss=83.4730
	Evaluating
	loss=0.2134, precision=0.4477, recall=0.9194, f1=0.6022
saving model as: 0_saved_model.pth
Training epoch 3
	step [1/147], loss=155.7945
	step [2/147], loss=168.7800
	step [3/147], loss=143.9407
	step [4/147], loss=181.2322
	step [5/147], loss=142.4755
	step [6/147], loss=192.3453
	step [7/147], loss=157.3018
	step [8/147], loss=150.7900
	step [9/147], loss=154.6371
	step [10/147], loss=151.8927
	step [11/147], loss=160.6380
	step [12/147], loss=163.6348
	step [13/147], loss=152.2076
	step [14/147], loss=148.3957
	step [15/147], loss=166.2939
	step [16/147], loss=157.6002
	step [17/147], loss=177.8676
	step [18/147], loss=177.1588
	step [19/147], loss=170.3122
	step [20/147], loss=168.3976
	step [21/147], loss=183.5989
	step [22/147], loss=170.7700
	step [23/147], loss=161.3468
	step [24/147], loss=170.0964
	step [25/147], loss=171.5018
	step [26/147], loss=160.0423
	step [27/147], loss=159.3133
	step [28/147], loss=171.6728
	step [29/147], loss=165.8871
	step [30/147], loss=180.3396
	step [31/147], loss=172.1681
	step [32/147], loss=193.6240
	step [33/147], loss=147.3539
	step [34/147], loss=155.5882
	step [35/147], loss=175.9351
	step [36/147], loss=173.9999
	step [37/147], loss=160.0611
	step [38/147], loss=169.1014
	step [39/147], loss=168.8804
	step [40/147], loss=143.3850
	step [41/147], loss=161.8450
	step [42/147], loss=165.0836
	step [43/147], loss=154.3952
	step [44/147], loss=155.6316
	step [45/147], loss=174.4082
	step [46/147], loss=170.5385
	step [47/147], loss=171.8716
	step [48/147], loss=174.9364
	step [49/147], loss=163.4604
	step [50/147], loss=166.8792
	step [51/147], loss=147.1565
	step [52/147], loss=162.6364
	step [53/147], loss=180.7238
	step [54/147], loss=151.1508
	step [55/147], loss=165.1535
	step [56/147], loss=134.2681
	step [57/147], loss=156.4765
	step [58/147], loss=161.3063
	step [59/147], loss=176.4181
	step [60/147], loss=175.1059
	step [61/147], loss=149.2341
	step [62/147], loss=177.8768
	step [63/147], loss=162.1965
	step [64/147], loss=154.9689
	step [65/147], loss=167.6899
	step [66/147], loss=175.7782
	step [67/147], loss=185.3474
	step [68/147], loss=157.2858
	step [69/147], loss=191.0150
	step [70/147], loss=151.4770
	step [71/147], loss=142.0675
	step [72/147], loss=154.0688
	step [73/147], loss=164.7471
	step [74/147], loss=165.7555
	step [75/147], loss=154.2023
	step [76/147], loss=156.9099
	step [77/147], loss=174.0151
	step [78/147], loss=151.5865
	step [79/147], loss=160.4169
	step [80/147], loss=141.6790
	step [81/147], loss=181.8339
	step [82/147], loss=154.5734
	step [83/147], loss=145.8901
	step [84/147], loss=159.1716
	step [85/147], loss=153.9822
	step [86/147], loss=148.3333
	step [87/147], loss=145.7195
	step [88/147], loss=163.8859
	step [89/147], loss=145.8991
	step [90/147], loss=160.9621
	step [91/147], loss=149.2735
	step [92/147], loss=148.5552
	step [93/147], loss=174.4567
	step [94/147], loss=168.5646
	step [95/147], loss=140.0682
	step [96/147], loss=129.4928
	step [97/147], loss=141.3979
	step [98/147], loss=149.3635
	step [99/147], loss=178.8153
	step [100/147], loss=171.7995
	step [101/147], loss=164.2792
	step [102/147], loss=155.0806
	step [103/147], loss=167.9521
	step [104/147], loss=161.8026
	step [105/147], loss=135.4968
	step [106/147], loss=183.5438
	step [107/147], loss=175.8046
	step [108/147], loss=150.2467
	step [109/147], loss=154.5103
	step [110/147], loss=134.7598
	step [111/147], loss=159.2598
	step [112/147], loss=131.4308
	step [113/147], loss=136.2372
	step [114/147], loss=150.8628
	step [115/147], loss=148.2297
	step [116/147], loss=162.6663
	step [117/147], loss=123.4918
	step [118/147], loss=169.2724
	step [119/147], loss=134.1763
	step [120/147], loss=159.1065
	step [121/147], loss=179.6117
	step [122/147], loss=162.6580
	step [123/147], loss=147.3438
	step [124/147], loss=156.9239
	step [125/147], loss=149.3695
	step [126/147], loss=140.6739
	step [127/147], loss=142.3064
	step [128/147], loss=137.5592
	step [129/147], loss=141.4732
	step [130/147], loss=152.3746
	step [131/147], loss=187.6016
	step [132/147], loss=186.3555
	step [133/147], loss=165.0435
	step [134/147], loss=174.5033
	step [135/147], loss=152.3033
	step [136/147], loss=142.1405
	step [137/147], loss=164.9059
	step [138/147], loss=177.0343
	step [139/147], loss=152.5560
	step [140/147], loss=156.1138
	step [141/147], loss=151.3021
	step [142/147], loss=138.5856
	step [143/147], loss=166.9372
	step [144/147], loss=154.7953
	step [145/147], loss=174.1288
	step [146/147], loss=163.0136
	step [147/147], loss=70.4060
	Evaluating
	loss=0.1768, precision=0.3896, recall=0.9412, f1=0.5511
Training epoch 4
	step [1/147], loss=143.2485
	step [2/147], loss=164.7917
	step [3/147], loss=129.8996
	step [4/147], loss=158.7974
	step [5/147], loss=146.6402
	step [6/147], loss=155.3737
	step [7/147], loss=167.2610
	step [8/147], loss=153.8557
	step [9/147], loss=137.6568
	step [10/147], loss=136.9268
	step [11/147], loss=150.4622
	step [12/147], loss=170.0781
	step [13/147], loss=152.5213
	step [14/147], loss=183.9160
	step [15/147], loss=166.1710
	step [16/147], loss=174.4514
	step [17/147], loss=147.4493
	step [18/147], loss=150.2473
	step [19/147], loss=187.6746
	step [20/147], loss=153.6412
	step [21/147], loss=150.1936
	step [22/147], loss=183.1094
	step [23/147], loss=156.0413
	step [24/147], loss=125.9144
	step [25/147], loss=157.4018
	step [26/147], loss=143.9198
	step [27/147], loss=162.2508
	step [28/147], loss=139.7965
	step [29/147], loss=151.1643
	step [30/147], loss=143.9252
	step [31/147], loss=157.8093
	step [32/147], loss=146.7901
	step [33/147], loss=152.7978
	step [34/147], loss=157.6129
	step [35/147], loss=137.8454
	step [36/147], loss=137.2711
	step [37/147], loss=162.4563
	step [38/147], loss=158.9086
	step [39/147], loss=164.5416
	step [40/147], loss=160.3419
	step [41/147], loss=150.2758
	step [42/147], loss=154.6512
	step [43/147], loss=166.2688
	step [44/147], loss=161.0956
	step [45/147], loss=147.3749
	step [46/147], loss=180.1891
	step [47/147], loss=158.3179
	step [48/147], loss=153.2931
	step [49/147], loss=184.8446
	step [50/147], loss=157.6553
	step [51/147], loss=131.0072
	step [52/147], loss=154.1235
	step [53/147], loss=173.2346
	step [54/147], loss=159.6236
	step [55/147], loss=151.7416
	step [56/147], loss=147.9426
	step [57/147], loss=141.2199
	step [58/147], loss=141.0118
	step [59/147], loss=129.1383
	step [60/147], loss=148.0401
	step [61/147], loss=143.6741
	step [62/147], loss=176.0717
	step [63/147], loss=146.9599
	step [64/147], loss=133.8193
	step [65/147], loss=151.8939
	step [66/147], loss=168.6380
	step [67/147], loss=156.7059
	step [68/147], loss=156.5405
	step [69/147], loss=139.7696
	step [70/147], loss=138.7685
	step [71/147], loss=168.7351
	step [72/147], loss=149.5645
	step [73/147], loss=161.1628
	step [74/147], loss=133.3860
	step [75/147], loss=161.4128
	step [76/147], loss=147.3157
	step [77/147], loss=147.1384
	step [78/147], loss=143.8282
	step [79/147], loss=148.7466
	step [80/147], loss=143.1545
	step [81/147], loss=155.4657
	step [82/147], loss=147.8640
	step [83/147], loss=158.2747
	step [84/147], loss=162.2827
	step [85/147], loss=139.9915
	step [86/147], loss=164.2520
	step [87/147], loss=150.8744
	step [88/147], loss=142.3236
	step [89/147], loss=169.9342
	step [90/147], loss=144.0507
	step [91/147], loss=160.0654
	step [92/147], loss=143.8152
	step [93/147], loss=151.9312
	step [94/147], loss=156.2530
	step [95/147], loss=197.3779
	step [96/147], loss=156.1921
	step [97/147], loss=160.1301
	step [98/147], loss=142.8703
	step [99/147], loss=166.2611
	step [100/147], loss=175.3356
	step [101/147], loss=142.1273
	step [102/147], loss=154.3257
	step [103/147], loss=123.8178
	step [104/147], loss=156.2518
	step [105/147], loss=136.7659
	step [106/147], loss=139.0234
	step [107/147], loss=160.1807
	step [108/147], loss=143.1238
	step [109/147], loss=137.3416
	step [110/147], loss=142.8141
	step [111/147], loss=158.3183
	step [112/147], loss=148.0564
	step [113/147], loss=173.2050
	step [114/147], loss=144.4861
	step [115/147], loss=158.1225
	step [116/147], loss=154.6227
	step [117/147], loss=153.5985
	step [118/147], loss=160.1413
	step [119/147], loss=141.2083
	step [120/147], loss=148.5280
	step [121/147], loss=131.0982
	step [122/147], loss=140.6975
	step [123/147], loss=125.5454
	step [124/147], loss=143.9358
	step [125/147], loss=136.3763
	step [126/147], loss=149.8429
	step [127/147], loss=143.9819
	step [128/147], loss=151.1745
	step [129/147], loss=168.4127
	step [130/147], loss=154.0938
	step [131/147], loss=141.4857
	step [132/147], loss=149.4557
	step [133/147], loss=137.2661
	step [134/147], loss=132.6187
	step [135/147], loss=141.1949
	step [136/147], loss=142.8923
	step [137/147], loss=140.6621
	step [138/147], loss=158.3021
	step [139/147], loss=143.0212
	step [140/147], loss=184.1651
	step [141/147], loss=135.5578
	step [142/147], loss=139.7077
	step [143/147], loss=152.9553
	step [144/147], loss=139.6942
	step [145/147], loss=153.1349
	step [146/147], loss=128.0648
	step [147/147], loss=50.1192
	Evaluating
	loss=0.1424, precision=0.4217, recall=0.9196, f1=0.5783
Training epoch 5
	step [1/147], loss=146.5901
	step [2/147], loss=160.6625
	step [3/147], loss=136.3402
	step [4/147], loss=145.2873
	step [5/147], loss=152.2617
	step [6/147], loss=143.5269
	step [7/147], loss=167.2555
	step [8/147], loss=136.7491
	step [9/147], loss=151.4308
	step [10/147], loss=164.0763
	step [11/147], loss=133.9646
	step [12/147], loss=149.6596
	step [13/147], loss=155.9275
	step [14/147], loss=119.5087
	step [15/147], loss=136.9169
	step [16/147], loss=140.3494
	step [17/147], loss=147.8447
	step [18/147], loss=137.2472
	step [19/147], loss=145.1474
	step [20/147], loss=148.2186
	step [21/147], loss=155.7163
	step [22/147], loss=141.6935
	step [23/147], loss=153.3063
	step [24/147], loss=145.2170
	step [25/147], loss=147.1622
	step [26/147], loss=143.2956
	step [27/147], loss=159.7976
	step [28/147], loss=144.1882
	step [29/147], loss=148.5006
	step [30/147], loss=155.6551
	step [31/147], loss=139.1151
	step [32/147], loss=143.9395
	step [33/147], loss=139.0189
	step [34/147], loss=125.7495
	step [35/147], loss=146.1420
	step [36/147], loss=150.4436
	step [37/147], loss=153.1319
	step [38/147], loss=140.8585
	step [39/147], loss=152.9118
	step [40/147], loss=142.1140
	step [41/147], loss=157.6822
	step [42/147], loss=149.8385
	step [43/147], loss=119.4124
	step [44/147], loss=166.0218
	step [45/147], loss=134.1404
	step [46/147], loss=149.9674
	step [47/147], loss=129.9045
	step [48/147], loss=157.8396
	step [49/147], loss=153.4865
	step [50/147], loss=135.1082
	step [51/147], loss=128.7926
	step [52/147], loss=141.6273
	step [53/147], loss=147.9566
	step [54/147], loss=142.4972
	step [55/147], loss=162.3268
	step [56/147], loss=139.5795
	step [57/147], loss=130.7411
	step [58/147], loss=133.5871
	step [59/147], loss=156.6205
	step [60/147], loss=152.0336
	step [61/147], loss=129.8609
	step [62/147], loss=152.5524
	step [63/147], loss=145.6127
	step [64/147], loss=140.6042
	step [65/147], loss=139.9328
	step [66/147], loss=146.6926
	step [67/147], loss=131.9873
	step [68/147], loss=156.6057
	step [69/147], loss=150.7342
	step [70/147], loss=131.9900
	step [71/147], loss=183.2165
	step [72/147], loss=168.7463
	step [73/147], loss=132.1579
	step [74/147], loss=151.7309
	step [75/147], loss=143.1813
	step [76/147], loss=149.4264
	step [77/147], loss=155.3689
	step [78/147], loss=139.7000
	step [79/147], loss=138.1113
	step [80/147], loss=139.4112
	step [81/147], loss=134.0805
	step [82/147], loss=139.0687
	step [83/147], loss=154.2025
	step [84/147], loss=128.4069
	step [85/147], loss=127.4646
	step [86/147], loss=144.1660
	step [87/147], loss=134.4548
	step [88/147], loss=136.5219
	step [89/147], loss=147.8856
	step [90/147], loss=162.2692
	step [91/147], loss=153.1994
	step [92/147], loss=152.4898
	step [93/147], loss=133.4530
	step [94/147], loss=125.9090
	step [95/147], loss=162.8661
	step [96/147], loss=144.6275
	step [97/147], loss=144.5457
	step [98/147], loss=132.1846
	step [99/147], loss=129.7610
	step [100/147], loss=162.3881
	step [101/147], loss=140.5191
	step [102/147], loss=151.2632
	step [103/147], loss=127.1845
	step [104/147], loss=148.7689
	step [105/147], loss=162.4635
	step [106/147], loss=146.0436
	step [107/147], loss=106.8417
	step [108/147], loss=141.2485
	step [109/147], loss=134.2310
	step [110/147], loss=139.2700
	step [111/147], loss=144.8443
	step [112/147], loss=164.3054
	step [113/147], loss=133.5768
	step [114/147], loss=153.4811
	step [115/147], loss=150.8740
	step [116/147], loss=157.8139
	step [117/147], loss=135.6399
	step [118/147], loss=139.9469
	step [119/147], loss=149.1093
	step [120/147], loss=151.6983
	step [121/147], loss=158.9616
	step [122/147], loss=165.7121
	step [123/147], loss=160.7604
	step [124/147], loss=152.8950
	step [125/147], loss=144.7622
	step [126/147], loss=133.1792
	step [127/147], loss=127.0129
	step [128/147], loss=149.7381
	step [129/147], loss=149.5422
	step [130/147], loss=163.8040
	step [131/147], loss=145.3944
	step [132/147], loss=144.1246
	step [133/147], loss=162.1998
	step [134/147], loss=145.2294
	step [135/147], loss=154.8916
	step [136/147], loss=145.3607
	step [137/147], loss=128.5390
	step [138/147], loss=140.5825
	step [139/147], loss=123.8519
	step [140/147], loss=122.2711
	step [141/147], loss=133.0560
	step [142/147], loss=163.3821
	step [143/147], loss=152.1551
	step [144/147], loss=138.0192
	step [145/147], loss=144.5583
	step [146/147], loss=165.6305
	step [147/147], loss=66.5375
	Evaluating
	loss=0.1135, precision=0.4888, recall=0.9198, f1=0.6384
saving model as: 0_saved_model.pth
Training epoch 6
	step [1/147], loss=162.8190
	step [2/147], loss=110.4655
	step [3/147], loss=147.3078
	step [4/147], loss=126.7912
	step [5/147], loss=143.5572
	step [6/147], loss=125.9742
	step [7/147], loss=142.4831
	step [8/147], loss=126.5258
	step [9/147], loss=131.3784
	step [10/147], loss=145.4808
	step [11/147], loss=134.0239
	step [12/147], loss=143.5148
	step [13/147], loss=158.1213
	step [14/147], loss=141.4776
	step [15/147], loss=113.0624
	step [16/147], loss=140.4021
	step [17/147], loss=143.5891
	step [18/147], loss=138.7721
	step [19/147], loss=146.5614
	step [20/147], loss=126.8719
	step [21/147], loss=128.9250
	step [22/147], loss=127.1740
	step [23/147], loss=131.0837
	step [24/147], loss=129.5595
	step [25/147], loss=148.7293
	step [26/147], loss=133.3965
	step [27/147], loss=140.1008
	step [28/147], loss=136.2973
	step [29/147], loss=126.5656
	step [30/147], loss=127.3330
	step [31/147], loss=142.0057
	step [32/147], loss=155.7587
	step [33/147], loss=130.4034
	step [34/147], loss=145.3305
	step [35/147], loss=134.9870
	step [36/147], loss=159.5355
	step [37/147], loss=146.5999
	step [38/147], loss=153.2530
	step [39/147], loss=139.0885
	step [40/147], loss=160.6103
	step [41/147], loss=124.3995
	step [42/147], loss=142.0702
	step [43/147], loss=160.6016
	step [44/147], loss=147.9997
	step [45/147], loss=137.3850
	step [46/147], loss=155.7713
	step [47/147], loss=131.8478
	step [48/147], loss=136.5418
	step [49/147], loss=139.3004
	step [50/147], loss=156.7542
	step [51/147], loss=130.7312
	step [52/147], loss=118.8930
	step [53/147], loss=145.4226
	step [54/147], loss=144.2764
	step [55/147], loss=136.7307
	step [56/147], loss=137.7523
	step [57/147], loss=166.0413
	step [58/147], loss=142.8779
	step [59/147], loss=157.5062
	step [60/147], loss=133.0208
	step [61/147], loss=135.9659
	step [62/147], loss=134.5553
	step [63/147], loss=148.8811
	step [64/147], loss=153.3473
	step [65/147], loss=134.4978
	step [66/147], loss=142.2401
	step [67/147], loss=121.3199
	step [68/147], loss=128.3057
	step [69/147], loss=163.6677
	step [70/147], loss=120.8673
	step [71/147], loss=152.4790
	step [72/147], loss=162.8857
	step [73/147], loss=141.4662
	step [74/147], loss=129.1773
	step [75/147], loss=142.8188
	step [76/147], loss=144.1963
	step [77/147], loss=133.5110
	step [78/147], loss=112.3576
	step [79/147], loss=146.0000
	step [80/147], loss=146.6292
	step [81/147], loss=143.1375
	step [82/147], loss=138.0894
	step [83/147], loss=153.2508
	step [84/147], loss=131.8667
	step [85/147], loss=118.3820
	step [86/147], loss=141.9377
	step [87/147], loss=154.6066
	step [88/147], loss=139.7531
	step [89/147], loss=141.9093
	step [90/147], loss=129.4919
	step [91/147], loss=162.4851
	step [92/147], loss=127.8715
	step [93/147], loss=133.5083
	step [94/147], loss=108.7055
	step [95/147], loss=142.7648
	step [96/147], loss=136.8192
	step [97/147], loss=133.3394
	step [98/147], loss=138.0874
	step [99/147], loss=140.7044
	step [100/147], loss=166.7349
	step [101/147], loss=138.7834
	step [102/147], loss=147.3257
	step [103/147], loss=158.3205
	step [104/147], loss=143.6031
	step [105/147], loss=152.9374
	step [106/147], loss=132.6193
	step [107/147], loss=152.3015
	step [108/147], loss=134.1022
	step [109/147], loss=144.7561
	step [110/147], loss=135.9025
	step [111/147], loss=139.9039
	step [112/147], loss=130.5679
	step [113/147], loss=145.0314
	step [114/147], loss=136.3493
	step [115/147], loss=135.1134
	step [116/147], loss=140.0486
	step [117/147], loss=145.0106
	step [118/147], loss=150.9588
	step [119/147], loss=140.2722
	step [120/147], loss=149.6715
	step [121/147], loss=110.5140
	step [122/147], loss=136.9818
	step [123/147], loss=138.7160
	step [124/147], loss=133.4223
	step [125/147], loss=150.2815
	step [126/147], loss=146.1945
	step [127/147], loss=127.0169
	step [128/147], loss=149.0689
	step [129/147], loss=148.3186
	step [130/147], loss=130.5007
	step [131/147], loss=146.1648
	step [132/147], loss=120.3727
	step [133/147], loss=132.0256
	step [134/147], loss=130.3882
	step [135/147], loss=141.4055
	step [136/147], loss=136.9201
	step [137/147], loss=147.2188
	step [138/147], loss=146.8330
	step [139/147], loss=141.8336
	step [140/147], loss=132.4548
	step [141/147], loss=127.9531
	step [142/147], loss=149.1604
	step [143/147], loss=130.4041
	step [144/147], loss=137.7678
	step [145/147], loss=133.3546
	step [146/147], loss=133.5578
	step [147/147], loss=56.6886
	Evaluating
	loss=0.1033, precision=0.4349, recall=0.9078, f1=0.5881
Training epoch 7
	step [1/147], loss=162.6200
	step [2/147], loss=140.9781
	step [3/147], loss=152.5942
	step [4/147], loss=132.5080
	step [5/147], loss=132.8663
	step [6/147], loss=126.1964
	step [7/147], loss=143.8650
	step [8/147], loss=137.0509
	step [9/147], loss=128.3982
	step [10/147], loss=165.1378
	step [11/147], loss=139.4082
	step [12/147], loss=138.7822
	step [13/147], loss=139.6450
	step [14/147], loss=145.1838
	step [15/147], loss=133.1631
	step [16/147], loss=146.4162
	step [17/147], loss=121.4143
	step [18/147], loss=128.1452
	step [19/147], loss=134.1540
	step [20/147], loss=130.0426
	step [21/147], loss=167.0787
	step [22/147], loss=126.7750
	step [23/147], loss=133.8401
	step [24/147], loss=128.1940
	step [25/147], loss=132.3333
	step [26/147], loss=114.6039
	step [27/147], loss=118.8183
	step [28/147], loss=158.0878
	step [29/147], loss=115.7503
	step [30/147], loss=131.4066
	step [31/147], loss=147.9637
	step [32/147], loss=189.2779
	step [33/147], loss=121.9663
	step [34/147], loss=132.2426
	step [35/147], loss=135.6727
	step [36/147], loss=129.5065
	step [37/147], loss=149.7120
	step [38/147], loss=135.3667
	step [39/147], loss=139.5761
	step [40/147], loss=173.0100
	step [41/147], loss=143.7990
	step [42/147], loss=124.1398
	step [43/147], loss=133.4604
	step [44/147], loss=124.6062
	step [45/147], loss=124.2935
	step [46/147], loss=144.5372
	step [47/147], loss=133.4034
	step [48/147], loss=147.4321
	step [49/147], loss=150.6342
	step [50/147], loss=124.7080
	step [51/147], loss=137.1393
	step [52/147], loss=126.9928
	step [53/147], loss=153.5662
	step [54/147], loss=130.8653
	step [55/147], loss=129.4875
	step [56/147], loss=127.5624
	step [57/147], loss=140.8639
	step [58/147], loss=147.6139
	step [59/147], loss=146.6579
	step [60/147], loss=116.8390
	step [61/147], loss=112.8697
	step [62/147], loss=129.6673
	step [63/147], loss=126.9540
	step [64/147], loss=147.4225
	step [65/147], loss=128.7956
	step [66/147], loss=138.2906
	step [67/147], loss=141.6580
	step [68/147], loss=141.5693
	step [69/147], loss=132.6896
	step [70/147], loss=134.1515
	step [71/147], loss=137.2559
	step [72/147], loss=124.9168
	step [73/147], loss=137.6647
	step [74/147], loss=145.5974
	step [75/147], loss=128.2086
	step [76/147], loss=131.9136
	step [77/147], loss=151.7799
	step [78/147], loss=142.1825
	step [79/147], loss=134.5048
	step [80/147], loss=141.5573
	step [81/147], loss=118.5738
	step [82/147], loss=129.9098
	step [83/147], loss=138.1617
	step [84/147], loss=123.5109
	step [85/147], loss=139.6579
	step [86/147], loss=127.3067
	step [87/147], loss=132.6053
	step [88/147], loss=127.5224
	step [89/147], loss=154.5245
	step [90/147], loss=123.6978
	step [91/147], loss=153.0956
	step [92/147], loss=141.0831
	step [93/147], loss=138.5708
	step [94/147], loss=140.2585
	step [95/147], loss=128.4485
	step [96/147], loss=143.5535
	step [97/147], loss=117.0449
	step [98/147], loss=162.6579
	step [99/147], loss=138.1291
	step [100/147], loss=141.4194
	step [101/147], loss=111.8584
	step [102/147], loss=124.5914
	step [103/147], loss=137.3753
	step [104/147], loss=158.9431
	step [105/147], loss=133.0628
	step [106/147], loss=140.3810
	step [107/147], loss=120.0009
	step [108/147], loss=145.8886
	step [109/147], loss=132.7539
	step [110/147], loss=156.7869
	step [111/147], loss=140.8778
	step [112/147], loss=140.1077
	step [113/147], loss=138.9502
	step [114/147], loss=125.5820
	step [115/147], loss=106.3157
	step [116/147], loss=101.9571
	step [117/147], loss=136.2195
	step [118/147], loss=132.6863
	step [119/147], loss=158.0928
	step [120/147], loss=114.1742
	step [121/147], loss=113.1041
	step [122/147], loss=124.3206
	step [123/147], loss=140.4729
	step [124/147], loss=134.4551
	step [125/147], loss=122.6174
	step [126/147], loss=133.6408
	step [127/147], loss=142.0188
	step [128/147], loss=137.5827
	step [129/147], loss=133.3585
	step [130/147], loss=165.0017
	step [131/147], loss=113.9111
	step [132/147], loss=137.5420
	step [133/147], loss=145.9004
	step [134/147], loss=133.8504
	step [135/147], loss=136.0590
	step [136/147], loss=147.2367
	step [137/147], loss=165.4166
	step [138/147], loss=127.1813
	step [139/147], loss=144.6439
	step [140/147], loss=128.3875
	step [141/147], loss=137.4638
	step [142/147], loss=124.7426
	step [143/147], loss=135.3821
	step [144/147], loss=143.6650
	step [145/147], loss=103.7254
	step [146/147], loss=130.8875
	step [147/147], loss=44.9118
	Evaluating
	loss=0.0822, precision=0.5148, recall=0.8948, f1=0.6536
saving model as: 0_saved_model.pth
Training epoch 8
	step [1/147], loss=124.7651
	step [2/147], loss=137.5633
	step [3/147], loss=150.5272
	step [4/147], loss=148.4269
	step [5/147], loss=145.4709
	step [6/147], loss=148.4810
	step [7/147], loss=129.6149
	step [8/147], loss=158.1830
	step [9/147], loss=129.4360
	step [10/147], loss=124.5709
	step [11/147], loss=109.3822
	step [12/147], loss=137.9529
	step [13/147], loss=122.2479
	step [14/147], loss=127.6430
	step [15/147], loss=127.4151
	step [16/147], loss=127.2750
	step [17/147], loss=134.7111
	step [18/147], loss=145.0171
	step [19/147], loss=127.6008
	step [20/147], loss=131.5483
	step [21/147], loss=132.3283
	step [22/147], loss=131.4283
	step [23/147], loss=140.5253
	step [24/147], loss=122.3801
	step [25/147], loss=158.9949
	step [26/147], loss=131.7981
	step [27/147], loss=124.7533
	step [28/147], loss=118.5963
	step [29/147], loss=119.6103
	step [30/147], loss=135.2864
	step [31/147], loss=128.8884
	step [32/147], loss=140.1460
	step [33/147], loss=120.9534
	step [34/147], loss=166.6433
	step [35/147], loss=138.7953
	step [36/147], loss=136.8330
	step [37/147], loss=130.7181
	step [38/147], loss=135.0472
	step [39/147], loss=139.1061
	step [40/147], loss=136.8707
	step [41/147], loss=149.1777
	step [42/147], loss=139.6943
	step [43/147], loss=142.5192
	step [44/147], loss=145.7240
	step [45/147], loss=137.4044
	step [46/147], loss=132.2606
	step [47/147], loss=156.3745
	step [48/147], loss=123.8862
	step [49/147], loss=137.6701
	step [50/147], loss=117.4116
	step [51/147], loss=131.8355
	step [52/147], loss=138.0081
	step [53/147], loss=118.6660
	step [54/147], loss=117.7106
	step [55/147], loss=133.1072
	step [56/147], loss=131.4191
	step [57/147], loss=137.1356
	step [58/147], loss=105.2166
	step [59/147], loss=126.0096
	step [60/147], loss=150.6837
	step [61/147], loss=139.2493
	step [62/147], loss=148.2753
	step [63/147], loss=133.8648
	step [64/147], loss=146.5990
	step [65/147], loss=146.7122
	step [66/147], loss=142.4047
	step [67/147], loss=118.2120
	step [68/147], loss=140.2529
	step [69/147], loss=129.9105
	step [70/147], loss=122.3566
	step [71/147], loss=128.4512
	step [72/147], loss=142.9458
	step [73/147], loss=155.9829
	step [74/147], loss=144.8071
	step [75/147], loss=130.9485
	step [76/147], loss=130.3345
	step [77/147], loss=148.3891
	step [78/147], loss=125.8370
	step [79/147], loss=127.5531
	step [80/147], loss=144.8952
	step [81/147], loss=130.6759
	step [82/147], loss=114.0711
	step [83/147], loss=121.7290
	step [84/147], loss=129.5554
	step [85/147], loss=136.4030
	step [86/147], loss=142.8688
	step [87/147], loss=144.1626
	step [88/147], loss=126.1588
	step [89/147], loss=121.3959
	step [90/147], loss=146.4951
	step [91/147], loss=107.2873
	step [92/147], loss=135.5699
	step [93/147], loss=150.6329
	step [94/147], loss=123.4227
	step [95/147], loss=125.3708
	step [96/147], loss=111.1564
	step [97/147], loss=131.5126
	step [98/147], loss=143.7265
	step [99/147], loss=114.3123
	step [100/147], loss=137.0338
	step [101/147], loss=145.8212
	step [102/147], loss=143.7824
	step [103/147], loss=136.1685
	step [104/147], loss=117.1012
	step [105/147], loss=115.1141
	step [106/147], loss=128.3441
	step [107/147], loss=123.3696
	step [108/147], loss=111.8270
	step [109/147], loss=128.5279
	step [110/147], loss=128.0643
	step [111/147], loss=124.7275
	step [112/147], loss=131.5179
	step [113/147], loss=129.5421
	step [114/147], loss=130.0253
	step [115/147], loss=126.3240
	step [116/147], loss=131.4620
	step [117/147], loss=126.1852
	step [118/147], loss=127.7407
	step [119/147], loss=131.0477
	step [120/147], loss=136.1172
	step [121/147], loss=113.0116
	step [122/147], loss=122.6598
	step [123/147], loss=138.9710
	step [124/147], loss=114.9214
	step [125/147], loss=134.4971
	step [126/147], loss=124.5403
	step [127/147], loss=118.5023
	step [128/147], loss=138.1872
	step [129/147], loss=121.1325
	step [130/147], loss=130.1235
	step [131/147], loss=121.1936
	step [132/147], loss=127.0310
	step [133/147], loss=126.9603
	step [134/147], loss=132.7517
	step [135/147], loss=122.9857
	step [136/147], loss=137.5609
	step [137/147], loss=117.1079
	step [138/147], loss=131.7640
	step [139/147], loss=134.8183
	step [140/147], loss=135.3450
	step [141/147], loss=121.5354
	step [142/147], loss=139.7584
	step [143/147], loss=124.4574
	step [144/147], loss=111.3218
	step [145/147], loss=135.9568
	step [146/147], loss=126.0403
	step [147/147], loss=45.7518
	Evaluating
	loss=0.0726, precision=0.4467, recall=0.9274, f1=0.6030
Training epoch 9
	step [1/147], loss=141.8526
	step [2/147], loss=145.5781
	step [3/147], loss=139.1593
	step [4/147], loss=115.2119
	step [5/147], loss=117.1682
	step [6/147], loss=135.3093
	step [7/147], loss=122.3075
	step [8/147], loss=105.4818
	step [9/147], loss=150.6959
	step [10/147], loss=111.0990
	step [11/147], loss=137.3697
	step [12/147], loss=125.8390
	step [13/147], loss=139.1924
	step [14/147], loss=144.5798
	step [15/147], loss=138.5123
	step [16/147], loss=154.2746
	step [17/147], loss=124.5039
	step [18/147], loss=131.3913
	step [19/147], loss=149.6262
	step [20/147], loss=135.0618
	step [21/147], loss=121.5858
	step [22/147], loss=127.3467
	step [23/147], loss=126.0240
	step [24/147], loss=136.1102
	step [25/147], loss=133.3901
	step [26/147], loss=142.6893
	step [27/147], loss=141.6848
	step [28/147], loss=112.7052
	step [29/147], loss=130.7479
	step [30/147], loss=134.9464
	step [31/147], loss=126.6044
	step [32/147], loss=124.9120
	step [33/147], loss=126.0571
	step [34/147], loss=150.5369
	step [35/147], loss=121.9026
	step [36/147], loss=136.8392
	step [37/147], loss=136.3793
	step [38/147], loss=113.1690
	step [39/147], loss=130.0159
	step [40/147], loss=154.6265
	step [41/147], loss=145.8000
	step [42/147], loss=127.9486
	step [43/147], loss=124.6381
	step [44/147], loss=119.5702
	step [45/147], loss=144.8069
	step [46/147], loss=119.3481
	step [47/147], loss=118.5509
	step [48/147], loss=133.4278
	step [49/147], loss=127.8562
	step [50/147], loss=127.1398
	step [51/147], loss=120.4375
	step [52/147], loss=123.7182
	step [53/147], loss=132.9341
	step [54/147], loss=133.6900
	step [55/147], loss=126.6837
	step [56/147], loss=128.5572
	step [57/147], loss=131.4308
	step [58/147], loss=140.2690
	step [59/147], loss=111.8528
	step [60/147], loss=131.0920
	step [61/147], loss=118.2561
	step [62/147], loss=133.7624
	step [63/147], loss=140.6750
	step [64/147], loss=133.8180
	step [65/147], loss=146.1837
	step [66/147], loss=127.3715
	step [67/147], loss=109.5777
	step [68/147], loss=105.2183
	step [69/147], loss=152.7094
	step [70/147], loss=104.3452
	step [71/147], loss=107.7224
	step [72/147], loss=123.6670
	step [73/147], loss=124.8290
	step [74/147], loss=132.9817
	step [75/147], loss=160.7239
	step [76/147], loss=160.6751
	step [77/147], loss=144.8672
	step [78/147], loss=126.3902
	step [79/147], loss=123.0847
	step [80/147], loss=140.5542
	step [81/147], loss=130.4247
	step [82/147], loss=135.9172
	step [83/147], loss=126.8143
	step [84/147], loss=141.9915
	step [85/147], loss=146.5741
	step [86/147], loss=115.6436
	step [87/147], loss=107.8682
	step [88/147], loss=127.0539
	step [89/147], loss=129.0445
	step [90/147], loss=128.4236
	step [91/147], loss=119.9844
	step [92/147], loss=124.6887
	step [93/147], loss=126.8990
	step [94/147], loss=103.9418
	step [95/147], loss=137.0711
	step [96/147], loss=124.5046
	step [97/147], loss=97.7394
	step [98/147], loss=109.5979
	step [99/147], loss=116.8514
	step [100/147], loss=156.1097
	step [101/147], loss=131.9656
	step [102/147], loss=122.5403
	step [103/147], loss=109.8950
	step [104/147], loss=125.2769
	step [105/147], loss=140.3862
	step [106/147], loss=109.6339
	step [107/147], loss=118.9058
	step [108/147], loss=125.2221
	step [109/147], loss=124.9194
	step [110/147], loss=125.5152
	step [111/147], loss=134.6114
	step [112/147], loss=143.7265
	step [113/147], loss=131.6218
	step [114/147], loss=129.8694
	step [115/147], loss=110.7577
	step [116/147], loss=119.7808
	step [117/147], loss=153.3222
	step [118/147], loss=105.9736
	step [119/147], loss=122.9170
	step [120/147], loss=125.9942
	step [121/147], loss=119.2714
	step [122/147], loss=148.5204
	step [123/147], loss=112.4436
	step [124/147], loss=125.5848
	step [125/147], loss=124.5027
	step [126/147], loss=146.7644
	step [127/147], loss=148.0682
	step [128/147], loss=130.9271
	step [129/147], loss=174.7675
	step [130/147], loss=114.6732
	step [131/147], loss=115.5919
	step [132/147], loss=115.9407
	step [133/147], loss=127.0999
	step [134/147], loss=134.4226
	step [135/147], loss=103.4373
	step [136/147], loss=121.5735
	step [137/147], loss=126.2634
	step [138/147], loss=143.1079
	step [139/147], loss=136.0645
	step [140/147], loss=152.8142
	step [141/147], loss=127.9802
	step [142/147], loss=135.9548
	step [143/147], loss=132.7402
	step [144/147], loss=117.3134
	step [145/147], loss=109.2610
	step [146/147], loss=121.4231
	step [147/147], loss=61.2513
	Evaluating
	loss=0.0657, precision=0.3958, recall=0.9152, f1=0.5526
Training epoch 10
	step [1/147], loss=122.0622
	step [2/147], loss=103.9144
	step [3/147], loss=135.2847
	step [4/147], loss=127.1292
	step [5/147], loss=152.0085
	step [6/147], loss=148.5401
	step [7/147], loss=114.1347
	step [8/147], loss=118.2983
	step [9/147], loss=121.3391
	step [10/147], loss=121.4475
	step [11/147], loss=130.6062
	step [12/147], loss=133.3820
	step [13/147], loss=106.1750
	step [14/147], loss=108.0344
	step [15/147], loss=112.6072
	step [16/147], loss=131.4937
	step [17/147], loss=111.7266
	step [18/147], loss=115.4697
	step [19/147], loss=129.2553
	step [20/147], loss=119.2075
	step [21/147], loss=165.9085
	step [22/147], loss=117.8955
	step [23/147], loss=128.0622
	step [24/147], loss=122.9425
	step [25/147], loss=134.2481
	step [26/147], loss=120.2701
	step [27/147], loss=127.7449
	step [28/147], loss=149.0318
	step [29/147], loss=131.4963
	step [30/147], loss=104.7199
	step [31/147], loss=122.0534
	step [32/147], loss=138.2560
	step [33/147], loss=123.0505
	step [34/147], loss=114.5050
	step [35/147], loss=135.1564
	step [36/147], loss=135.4993
	step [37/147], loss=122.0535
	step [38/147], loss=145.0979
	step [39/147], loss=139.2287
	step [40/147], loss=141.4050
	step [41/147], loss=119.8381
	step [42/147], loss=131.6628
	step [43/147], loss=120.1998
	step [44/147], loss=137.8589
	step [45/147], loss=112.1967
	step [46/147], loss=126.7339
	step [47/147], loss=125.3784
	step [48/147], loss=137.6745
	step [49/147], loss=127.4744
	step [50/147], loss=109.6798
	step [51/147], loss=137.9980
	step [52/147], loss=113.6561
	step [53/147], loss=114.5086
	step [54/147], loss=124.1078
	step [55/147], loss=114.2144
	step [56/147], loss=122.2290
	step [57/147], loss=139.5140
	step [58/147], loss=133.0784
	step [59/147], loss=117.6684
	step [60/147], loss=116.5280
	step [61/147], loss=120.1818
	step [62/147], loss=127.2201
	step [63/147], loss=123.4233
	step [64/147], loss=119.7604
	step [65/147], loss=125.1479
	step [66/147], loss=115.1841
	step [67/147], loss=117.4959
	step [68/147], loss=140.0866
	step [69/147], loss=154.4390
	step [70/147], loss=118.1296
	step [71/147], loss=112.6354
	step [72/147], loss=133.2272
	step [73/147], loss=158.6189
	step [74/147], loss=139.1626
	step [75/147], loss=138.4273
	step [76/147], loss=119.3895
	step [77/147], loss=129.2948
	step [78/147], loss=133.7469
	step [79/147], loss=130.7650
	step [80/147], loss=115.6793
	step [81/147], loss=146.0730
	step [82/147], loss=130.3138
	step [83/147], loss=116.5458
	step [84/147], loss=129.8415
	step [85/147], loss=124.1214
	step [86/147], loss=127.6114
	step [87/147], loss=133.7484
	step [88/147], loss=119.6673
	step [89/147], loss=133.8598
	step [90/147], loss=122.9654
	step [91/147], loss=139.3256
	step [92/147], loss=145.8694
	step [93/147], loss=118.4923
	step [94/147], loss=147.0875
	step [95/147], loss=112.5656
	step [96/147], loss=138.4568
	step [97/147], loss=129.8131
	step [98/147], loss=120.2745
	step [99/147], loss=144.2315
	step [100/147], loss=105.1920
	step [101/147], loss=137.2327
	step [102/147], loss=136.0575
	step [103/147], loss=141.6959
	step [104/147], loss=140.7600
	step [105/147], loss=123.5446
	step [106/147], loss=104.9942
	step [107/147], loss=143.7521
	step [108/147], loss=134.1820
	step [109/147], loss=119.0842
	step [110/147], loss=133.5370
	step [111/147], loss=117.7953
	step [112/147], loss=143.1285
	step [113/147], loss=127.9086
	step [114/147], loss=120.6326
	step [115/147], loss=131.3711
	step [116/147], loss=133.0603
	step [117/147], loss=119.8476
	step [118/147], loss=138.0075
	step [119/147], loss=140.5451
	step [120/147], loss=123.5870
	step [121/147], loss=115.9540
	step [122/147], loss=141.4616
	step [123/147], loss=133.6983
	step [124/147], loss=131.4721
	step [125/147], loss=127.9434
	step [126/147], loss=120.6656
	step [127/147], loss=137.2417
	step [128/147], loss=97.0160
	step [129/147], loss=118.2079
	step [130/147], loss=120.1686
	step [131/147], loss=128.7755
	step [132/147], loss=120.0237
	step [133/147], loss=116.7299
	step [134/147], loss=103.5166
	step [135/147], loss=114.4613
	step [136/147], loss=124.1118
	step [137/147], loss=131.3044
	step [138/147], loss=142.5776
	step [139/147], loss=113.8629
	step [140/147], loss=141.6266
	step [141/147], loss=102.6137
	step [142/147], loss=104.8169
	step [143/147], loss=148.2055
	step [144/147], loss=132.2178
	step [145/147], loss=131.4372
	step [146/147], loss=129.2254
	step [147/147], loss=40.3760
	Evaluating
	loss=0.0600, precision=0.3468, recall=0.9109, f1=0.5023
Training epoch 11
	step [1/147], loss=107.0219
	step [2/147], loss=121.3479
	step [3/147], loss=136.4420
	step [4/147], loss=116.0210
	step [5/147], loss=121.1773
	step [6/147], loss=117.7842
	step [7/147], loss=105.1521
	step [8/147], loss=111.9627
	step [9/147], loss=117.2007
	step [10/147], loss=142.7565
	step [11/147], loss=128.8179
	step [12/147], loss=134.1812
	step [13/147], loss=138.5564
	step [14/147], loss=131.8587
	step [15/147], loss=119.9128
	step [16/147], loss=123.6696
	step [17/147], loss=121.8391
	step [18/147], loss=139.0760
	step [19/147], loss=98.6554
	step [20/147], loss=138.3745
	step [21/147], loss=138.7252
	step [22/147], loss=125.4771
	step [23/147], loss=130.3577
	step [24/147], loss=137.4761
	step [25/147], loss=122.3802
	step [26/147], loss=128.9425
	step [27/147], loss=141.8884
	step [28/147], loss=138.7015
	step [29/147], loss=99.6619
	step [30/147], loss=107.8117
	step [31/147], loss=127.2612
	step [32/147], loss=141.2637
	step [33/147], loss=127.5675
	step [34/147], loss=122.4486
	step [35/147], loss=111.2216
	step [36/147], loss=114.2251
	step [37/147], loss=110.9175
	step [38/147], loss=112.6769
	step [39/147], loss=130.9905
	step [40/147], loss=118.3890
	step [41/147], loss=135.0682
	step [42/147], loss=143.7770
	step [43/147], loss=141.4254
	step [44/147], loss=120.8832
	step [45/147], loss=117.7395
	step [46/147], loss=125.7391
	step [47/147], loss=124.5635
	step [48/147], loss=142.9110
	step [49/147], loss=123.7650
	step [50/147], loss=128.6882
	step [51/147], loss=130.6988
	step [52/147], loss=100.7523
	step [53/147], loss=101.3066
	step [54/147], loss=125.9609
	step [55/147], loss=138.0566
	step [56/147], loss=126.6128
	step [57/147], loss=136.5901
	step [58/147], loss=119.9054
	step [59/147], loss=146.8840
	step [60/147], loss=135.3190
	step [61/147], loss=107.7176
	step [62/147], loss=122.6104
	step [63/147], loss=116.3773
	step [64/147], loss=134.8573
	step [65/147], loss=110.3893
	step [66/147], loss=131.1763
	step [67/147], loss=109.3320
	step [68/147], loss=115.3593
	step [69/147], loss=141.4665
	step [70/147], loss=120.6212
	step [71/147], loss=138.5004
	step [72/147], loss=113.6533
	step [73/147], loss=120.2639
	step [74/147], loss=120.0333
	step [75/147], loss=132.8188
	step [76/147], loss=128.4557
	step [77/147], loss=124.8453
	step [78/147], loss=106.9542
	step [79/147], loss=133.0760
	step [80/147], loss=127.6146
	step [81/147], loss=120.5570
	step [82/147], loss=137.2495
	step [83/147], loss=145.0533
	step [84/147], loss=135.9594
	step [85/147], loss=113.4884
	step [86/147], loss=110.9321
	step [87/147], loss=131.1019
	step [88/147], loss=107.1100
	step [89/147], loss=127.3319
	step [90/147], loss=119.4420
	step [91/147], loss=122.7775
	step [92/147], loss=122.8318
	step [93/147], loss=128.5955
	step [94/147], loss=120.5056
	step [95/147], loss=105.9341
	step [96/147], loss=113.9426
	step [97/147], loss=110.6646
	step [98/147], loss=142.2125
	step [99/147], loss=119.9252
	step [100/147], loss=134.5094
	step [101/147], loss=115.5019
	step [102/147], loss=130.8665
	step [103/147], loss=128.8917
	step [104/147], loss=133.8334
	step [105/147], loss=140.6112
	step [106/147], loss=129.4895
	step [107/147], loss=113.3124
	step [108/147], loss=107.2005
	step [109/147], loss=126.2607
	step [110/147], loss=118.2012
	step [111/147], loss=120.0263
	step [112/147], loss=110.3728
	step [113/147], loss=146.2995
	step [114/147], loss=133.5070
	step [115/147], loss=109.9848
	step [116/147], loss=124.1549
	step [117/147], loss=99.4285
	step [118/147], loss=117.3773
	step [119/147], loss=122.5096
	step [120/147], loss=111.2521
	step [121/147], loss=121.1879
	step [122/147], loss=130.4806
	step [123/147], loss=126.9065
	step [124/147], loss=118.8905
	step [125/147], loss=137.2464
	step [126/147], loss=145.8139
	step [127/147], loss=110.5918
	step [128/147], loss=122.1296
	step [129/147], loss=119.1995
	step [130/147], loss=137.7888
	step [131/147], loss=132.0034
	step [132/147], loss=130.1660
	step [133/147], loss=133.7189
	step [134/147], loss=102.9964
	step [135/147], loss=114.6696
	step [136/147], loss=150.7533
	step [137/147], loss=124.8926
	step [138/147], loss=127.7925
	step [139/147], loss=139.3950
	step [140/147], loss=124.0730
	step [141/147], loss=118.1023
	step [142/147], loss=125.1015
	step [143/147], loss=119.0813
	step [144/147], loss=138.4057
	step [145/147], loss=124.4297
	step [146/147], loss=104.0983
	step [147/147], loss=48.0252
	Evaluating
	loss=0.0525, precision=0.4131, recall=0.8917, f1=0.5646
Training epoch 12
	step [1/147], loss=114.3262
	step [2/147], loss=124.5171
	step [3/147], loss=122.6215
	step [4/147], loss=138.6889
	step [5/147], loss=125.9758
	step [6/147], loss=115.2549
	step [7/147], loss=127.9977
	step [8/147], loss=131.2740
	step [9/147], loss=91.6121
	step [10/147], loss=114.0470
	step [11/147], loss=110.6709
	step [12/147], loss=140.1872
	step [13/147], loss=139.2263
	step [14/147], loss=129.4118
	step [15/147], loss=123.4536
	step [16/147], loss=129.3206
	step [17/147], loss=144.9790
	step [18/147], loss=134.8745
	step [19/147], loss=107.4319
	step [20/147], loss=114.5301
	step [21/147], loss=126.3962
	step [22/147], loss=139.6954
	step [23/147], loss=134.4955
	step [24/147], loss=110.6904
	step [25/147], loss=143.7903
	step [26/147], loss=150.5688
	step [27/147], loss=118.1295
	step [28/147], loss=123.8482
	step [29/147], loss=108.4174
	step [30/147], loss=113.4895
	step [31/147], loss=136.5382
	step [32/147], loss=130.1324
	step [33/147], loss=118.9214
	step [34/147], loss=122.9582
	step [35/147], loss=145.5630
	step [36/147], loss=111.9445
	step [37/147], loss=138.2397
	step [38/147], loss=125.8694
	step [39/147], loss=128.4886
	step [40/147], loss=119.8182
	step [41/147], loss=135.6714
	step [42/147], loss=111.4338
	step [43/147], loss=129.7839
	step [44/147], loss=114.2774
	step [45/147], loss=128.6919
	step [46/147], loss=120.8759
	step [47/147], loss=122.3159
	step [48/147], loss=145.7695
	step [49/147], loss=89.0699
	step [50/147], loss=141.2466
	step [51/147], loss=119.0177
	step [52/147], loss=114.2927
	step [53/147], loss=125.4292
	step [54/147], loss=115.8894
	step [55/147], loss=125.8722
	step [56/147], loss=117.9054
	step [57/147], loss=108.0984
	step [58/147], loss=102.3044
	step [59/147], loss=137.6417
	step [60/147], loss=140.2794
	step [61/147], loss=130.6267
	step [62/147], loss=119.1849
	step [63/147], loss=117.5329
	step [64/147], loss=133.2734
	step [65/147], loss=129.9471
	step [66/147], loss=125.9980
	step [67/147], loss=137.3183
	step [68/147], loss=126.2851
	step [69/147], loss=132.8793
	step [70/147], loss=140.2255
	step [71/147], loss=119.8941
	step [72/147], loss=110.4498
	step [73/147], loss=124.8417
	step [74/147], loss=107.1910
	step [75/147], loss=133.0571
	step [76/147], loss=122.6770
	step [77/147], loss=139.8075
	step [78/147], loss=119.0636
	step [79/147], loss=105.4563
	step [80/147], loss=122.7671
	step [81/147], loss=118.9051
	step [82/147], loss=118.0759
	step [83/147], loss=102.2080
	step [84/147], loss=118.5882
	step [85/147], loss=140.3254
	step [86/147], loss=115.5014
	step [87/147], loss=142.2107
	step [88/147], loss=125.0094
	step [89/147], loss=122.1379
	step [90/147], loss=122.2352
	step [91/147], loss=131.6891
	step [92/147], loss=90.7726
	step [93/147], loss=121.1151
	step [94/147], loss=113.7553
	step [95/147], loss=104.9281
	step [96/147], loss=139.0170
	step [97/147], loss=120.3712
	step [98/147], loss=123.3651
	step [99/147], loss=135.1691
	step [100/147], loss=114.1214
	step [101/147], loss=114.3928
	step [102/147], loss=125.6605
	step [103/147], loss=122.3525
	step [104/147], loss=111.8330
	step [105/147], loss=108.7145
	step [106/147], loss=124.2964
	step [107/147], loss=124.5739
	step [108/147], loss=118.8643
	step [109/147], loss=106.6439
	step [110/147], loss=116.0211
	step [111/147], loss=118.9060
	step [112/147], loss=131.5027
	step [113/147], loss=104.6065
	step [114/147], loss=124.7382
	step [115/147], loss=105.7182
	step [116/147], loss=125.5729
	step [117/147], loss=135.3305
	step [118/147], loss=107.0543
	step [119/147], loss=124.0815
	step [120/147], loss=109.6884
	step [121/147], loss=138.4308
	step [122/147], loss=112.5773
	step [123/147], loss=122.7746
	step [124/147], loss=118.4538
	step [125/147], loss=120.8018
	step [126/147], loss=141.7883
	step [127/147], loss=110.4014
	step [128/147], loss=129.8181
	step [129/147], loss=136.3178
	step [130/147], loss=133.8050
	step [131/147], loss=130.8289
	step [132/147], loss=117.4095
	step [133/147], loss=99.0457
	step [134/147], loss=114.3651
	step [135/147], loss=126.5380
	step [136/147], loss=123.1506
	step [137/147], loss=109.6599
	step [138/147], loss=130.0525
	step [139/147], loss=118.4631
	step [140/147], loss=118.5969
	step [141/147], loss=107.0938
	step [142/147], loss=113.6003
	step [143/147], loss=114.0653
	step [144/147], loss=133.3453
	step [145/147], loss=140.1524
	step [146/147], loss=96.9688
	step [147/147], loss=36.0885
	Evaluating
	loss=0.0482, precision=0.3802, recall=0.9233, f1=0.5386
Training epoch 13
	step [1/147], loss=105.4605
	step [2/147], loss=133.8572
	step [3/147], loss=119.0260
	step [4/147], loss=129.4447
	step [5/147], loss=123.4779
	step [6/147], loss=100.3576
	step [7/147], loss=108.6156
	step [8/147], loss=131.7135
	step [9/147], loss=138.6267
	step [10/147], loss=108.9996
	step [11/147], loss=128.0119
	step [12/147], loss=131.0361
	step [13/147], loss=119.0623
	step [14/147], loss=114.4507
	step [15/147], loss=117.5604
	step [16/147], loss=132.9078
	step [17/147], loss=112.6795
	step [18/147], loss=132.1579
	step [19/147], loss=123.3277
	step [20/147], loss=127.4621
	step [21/147], loss=113.1977
	step [22/147], loss=133.8264
	step [23/147], loss=103.9149
	step [24/147], loss=116.0341
	step [25/147], loss=123.6441
	step [26/147], loss=123.5465
	step [27/147], loss=95.3643
	step [28/147], loss=118.4273
	step [29/147], loss=113.2103
	step [30/147], loss=127.4630
	step [31/147], loss=119.5839
	step [32/147], loss=117.0909
	step [33/147], loss=121.1132
	step [34/147], loss=125.5935
	step [35/147], loss=117.6254
	step [36/147], loss=115.1729
	step [37/147], loss=142.8568
	step [38/147], loss=110.6895
	step [39/147], loss=108.7258
	step [40/147], loss=107.5634
	step [41/147], loss=93.4361
	step [42/147], loss=126.6978
	step [43/147], loss=120.3059
	step [44/147], loss=114.8800
	step [45/147], loss=116.4796
	step [46/147], loss=114.8798
	step [47/147], loss=116.6999
	step [48/147], loss=132.7467
	step [49/147], loss=128.3145
	step [50/147], loss=123.3524
	step [51/147], loss=125.2398
	step [52/147], loss=144.0010
	step [53/147], loss=125.4251
	step [54/147], loss=131.3599
	step [55/147], loss=101.2103
	step [56/147], loss=111.9707
	step [57/147], loss=120.2223
	step [58/147], loss=133.6238
	step [59/147], loss=134.3497
	step [60/147], loss=128.9333
	step [61/147], loss=116.7011
	step [62/147], loss=116.5241
	step [63/147], loss=116.7089
	step [64/147], loss=130.9576
	step [65/147], loss=115.2454
	step [66/147], loss=120.9310
	step [67/147], loss=140.1419
	step [68/147], loss=128.7043
	step [69/147], loss=142.2322
	step [70/147], loss=121.8595
	step [71/147], loss=139.7095
	step [72/147], loss=121.4616
	step [73/147], loss=105.4547
	step [74/147], loss=111.2889
	step [75/147], loss=108.7854
	step [76/147], loss=138.7969
	step [77/147], loss=106.2619
	step [78/147], loss=126.3366
	step [79/147], loss=124.6861
	step [80/147], loss=128.3961
	step [81/147], loss=127.3782
	step [82/147], loss=122.1202
	step [83/147], loss=95.5858
	step [84/147], loss=103.2941
	step [85/147], loss=99.4726
	step [86/147], loss=114.2910
	step [87/147], loss=132.2180
	step [88/147], loss=126.1813
	step [89/147], loss=121.8547
	step [90/147], loss=116.9492
	step [91/147], loss=134.7821
	step [92/147], loss=122.0652
	step [93/147], loss=128.5762
	step [94/147], loss=122.1165
	step [95/147], loss=115.0484
	step [96/147], loss=122.7812
	step [97/147], loss=112.1948
	step [98/147], loss=113.1388
	step [99/147], loss=126.7586
	step [100/147], loss=122.7854
	step [101/147], loss=113.7020
	step [102/147], loss=116.7847
	step [103/147], loss=107.8254
	step [104/147], loss=114.1389
	step [105/147], loss=117.0781
	step [106/147], loss=134.4543
	step [107/147], loss=124.5315
	step [108/147], loss=102.0486
	step [109/147], loss=98.8480
	step [110/147], loss=104.6071
	step [111/147], loss=120.0673
	step [112/147], loss=134.7181
	step [113/147], loss=107.3688
	step [114/147], loss=100.2214
	step [115/147], loss=131.2174
	step [116/147], loss=120.3699
	step [117/147], loss=151.5564
	step [118/147], loss=109.4273
	step [119/147], loss=116.1340
	step [120/147], loss=127.6810
	step [121/147], loss=125.3151
	step [122/147], loss=107.1244
	step [123/147], loss=138.8091
	step [124/147], loss=83.5712
	step [125/147], loss=136.8318
	step [126/147], loss=135.9943
	step [127/147], loss=133.5191
	step [128/147], loss=108.9736
	step [129/147], loss=146.7809
	step [130/147], loss=140.2207
	step [131/147], loss=116.4827
	step [132/147], loss=113.7895
	step [133/147], loss=124.8119
	step [134/147], loss=107.3678
	step [135/147], loss=119.6533
	step [136/147], loss=126.7817
	step [137/147], loss=111.0863
	step [138/147], loss=105.2705
	step [139/147], loss=131.6314
	step [140/147], loss=128.4532
	step [141/147], loss=139.5951
	step [142/147], loss=140.1457
	step [143/147], loss=126.8381
	step [144/147], loss=101.1931
	step [145/147], loss=132.4277
	step [146/147], loss=125.5557
	step [147/147], loss=58.2623
	Evaluating
	loss=0.0574, precision=0.1609, recall=0.9359, f1=0.2746
Training epoch 14
	step [1/147], loss=114.1058
	step [2/147], loss=121.9738
	step [3/147], loss=115.3033
	step [4/147], loss=147.6783
	step [5/147], loss=100.9917
	step [6/147], loss=123.7851
	step [7/147], loss=131.4914
	step [8/147], loss=106.5827
	step [9/147], loss=118.4458
	step [10/147], loss=112.0998
	step [11/147], loss=112.6052
	step [12/147], loss=116.8894
	step [13/147], loss=119.4129
	step [14/147], loss=122.6996
	step [15/147], loss=116.0860
	step [16/147], loss=126.8708
	step [17/147], loss=119.3086
	step [18/147], loss=116.0478
	step [19/147], loss=111.4973
	step [20/147], loss=119.4837
	step [21/147], loss=127.8601
	step [22/147], loss=132.8111
	step [23/147], loss=132.4897
	step [24/147], loss=119.2909
	step [25/147], loss=115.4011
	step [26/147], loss=131.6338
	step [27/147], loss=99.2300
	step [28/147], loss=127.5317
	step [29/147], loss=134.0431
	step [30/147], loss=143.3105
	step [31/147], loss=102.3839
	step [32/147], loss=117.0956
	step [33/147], loss=114.9228
	step [34/147], loss=118.8228
	step [35/147], loss=106.0971
	step [36/147], loss=124.5536
	step [37/147], loss=106.2249
	step [38/147], loss=142.5807
	step [39/147], loss=115.8131
	step [40/147], loss=121.7324
	step [41/147], loss=115.1061
	step [42/147], loss=112.1869
	step [43/147], loss=109.9446
	step [44/147], loss=96.4161
	step [45/147], loss=113.7353
	step [46/147], loss=102.2497
	step [47/147], loss=129.2319
	step [48/147], loss=108.0288
	step [49/147], loss=150.0464
	step [50/147], loss=125.2466
	step [51/147], loss=121.6489
	step [52/147], loss=117.5526
	step [53/147], loss=126.5476
	step [54/147], loss=119.4849
	step [55/147], loss=162.4066
	step [56/147], loss=131.4595
	step [57/147], loss=112.6722
	step [58/147], loss=131.6249
	step [59/147], loss=102.9872
	step [60/147], loss=128.3360
	step [61/147], loss=137.3265
	step [62/147], loss=132.2151
	step [63/147], loss=134.3289
	step [64/147], loss=101.7033
	step [65/147], loss=105.7256
	step [66/147], loss=135.1871
	step [67/147], loss=121.0260
	step [68/147], loss=88.0319
	step [69/147], loss=134.3882
	step [70/147], loss=135.9557
	step [71/147], loss=112.7039
	step [72/147], loss=121.2765
	step [73/147], loss=115.8290
	step [74/147], loss=102.5748
	step [75/147], loss=117.0163
	step [76/147], loss=129.5266
	step [77/147], loss=129.9386
	step [78/147], loss=113.4594
	step [79/147], loss=142.6154
	step [80/147], loss=124.9749
	step [81/147], loss=119.9261
	step [82/147], loss=107.7364
	step [83/147], loss=129.0626
	step [84/147], loss=124.1924
	step [85/147], loss=118.4446
	step [86/147], loss=102.2060
	step [87/147], loss=112.9757
	step [88/147], loss=122.4164
	step [89/147], loss=132.5662
	step [90/147], loss=128.5441
	step [91/147], loss=115.1138
	step [92/147], loss=103.3884
	step [93/147], loss=109.9081
	step [94/147], loss=117.3513
	step [95/147], loss=129.7762
	step [96/147], loss=107.4820
	step [97/147], loss=123.2365
	step [98/147], loss=111.3713
	step [99/147], loss=108.0219
	step [100/147], loss=121.6309
	step [101/147], loss=100.7586
	step [102/147], loss=135.4220
	step [103/147], loss=101.3202
	step [104/147], loss=126.1062
	step [105/147], loss=108.0271
	step [106/147], loss=122.8949
	step [107/147], loss=131.5259
	step [108/147], loss=97.0479
	step [109/147], loss=120.5098
	step [110/147], loss=126.8895
	step [111/147], loss=115.8258
	step [112/147], loss=120.9281
	step [113/147], loss=104.8341
	step [114/147], loss=110.7900
	step [115/147], loss=115.8397
	step [116/147], loss=109.5595
	step [117/147], loss=127.7963
	step [118/147], loss=89.1631
	step [119/147], loss=118.3041
	step [120/147], loss=112.3865
	step [121/147], loss=135.4008
	step [122/147], loss=121.5491
	step [123/147], loss=128.5012
	step [124/147], loss=116.8482
	step [125/147], loss=127.1469
	step [126/147], loss=132.6521
	step [127/147], loss=114.2753
	step [128/147], loss=116.3101
	step [129/147], loss=121.8680
	step [130/147], loss=120.4747
	step [131/147], loss=114.6637
	step [132/147], loss=122.9708
	step [133/147], loss=100.6944
	step [134/147], loss=120.7012
	step [135/147], loss=131.0026
	step [136/147], loss=110.6975
	step [137/147], loss=126.0304
	step [138/147], loss=132.6542
	step [139/147], loss=112.4926
	step [140/147], loss=99.5983
	step [141/147], loss=135.7638
	step [142/147], loss=118.6984
	step [143/147], loss=115.1329
	step [144/147], loss=111.8915
	step [145/147], loss=116.5418
	step [146/147], loss=106.5267
	step [147/147], loss=50.9128
	Evaluating
	loss=0.0484, precision=0.2231, recall=0.9036, f1=0.3578
Training epoch 15
	step [1/147], loss=126.0667
	step [2/147], loss=121.5294
	step [3/147], loss=99.2501
	step [4/147], loss=117.1134
	step [5/147], loss=108.8951
	step [6/147], loss=125.1386
	step [7/147], loss=127.5240
	step [8/147], loss=110.4195
	step [9/147], loss=112.5261
	step [10/147], loss=96.7413
	step [11/147], loss=126.0006
	step [12/147], loss=118.0165
	step [13/147], loss=107.3519
	step [14/147], loss=132.7517
	step [15/147], loss=137.3315
	step [16/147], loss=118.8476
	step [17/147], loss=112.7318
	step [18/147], loss=111.7025
	step [19/147], loss=126.2928
	step [20/147], loss=118.5355
	step [21/147], loss=130.1430
	step [22/147], loss=115.4950
	step [23/147], loss=126.1968
	step [24/147], loss=119.9645
	step [25/147], loss=112.3627
	step [26/147], loss=133.4881
	step [27/147], loss=106.6789
	step [28/147], loss=125.9499
	step [29/147], loss=103.6242
	step [30/147], loss=125.8515
	step [31/147], loss=124.8318
	step [32/147], loss=109.6158
	step [33/147], loss=132.4034
	step [34/147], loss=106.8765
	step [35/147], loss=111.5483
	step [36/147], loss=121.7270
	step [37/147], loss=118.2861
	step [38/147], loss=100.1225
	step [39/147], loss=103.8125
	step [40/147], loss=115.5872
	step [41/147], loss=104.5416
	step [42/147], loss=124.8720
	step [43/147], loss=116.1184
	step [44/147], loss=112.2943
	step [45/147], loss=106.1953
	step [46/147], loss=125.7846
	step [47/147], loss=119.4292
	step [48/147], loss=138.6497
	step [49/147], loss=94.5843
	step [50/147], loss=124.1046
	step [51/147], loss=123.4861
	step [52/147], loss=128.2590
	step [53/147], loss=104.7665
	step [54/147], loss=102.8358
	step [55/147], loss=111.2645
	step [56/147], loss=107.6994
	step [57/147], loss=120.4889
	step [58/147], loss=123.4202
	step [59/147], loss=111.2878
	step [60/147], loss=98.5528
	step [61/147], loss=134.3981
	step [62/147], loss=105.1775
	step [63/147], loss=110.4979
	step [64/147], loss=126.0153
	step [65/147], loss=113.7062
	step [66/147], loss=136.2843
	step [67/147], loss=136.2959
	step [68/147], loss=104.6770
	step [69/147], loss=129.9198
	step [70/147], loss=102.0253
	step [71/147], loss=124.9459
	step [72/147], loss=105.0525
	step [73/147], loss=103.4149
	step [74/147], loss=112.3459
	step [75/147], loss=104.5522
	step [76/147], loss=116.5772
	step [77/147], loss=109.5776
	step [78/147], loss=121.0996
	step [79/147], loss=119.9644
	step [80/147], loss=128.5938
	step [81/147], loss=136.5002
	step [82/147], loss=113.5574
	step [83/147], loss=111.7849
	step [84/147], loss=128.7402
	step [85/147], loss=138.7097
	step [86/147], loss=104.0880
	step [87/147], loss=117.1282
	step [88/147], loss=124.2275
	step [89/147], loss=97.3353
	step [90/147], loss=115.4166
	step [91/147], loss=116.1264
	step [92/147], loss=131.4809
	step [93/147], loss=135.2434
	step [94/147], loss=106.6321
	step [95/147], loss=129.5168
	step [96/147], loss=118.5457
	step [97/147], loss=119.1043
	step [98/147], loss=120.5960
	step [99/147], loss=124.8242
	step [100/147], loss=114.2341
	step [101/147], loss=125.6331
	step [102/147], loss=135.2251
	step [103/147], loss=108.4249
	step [104/147], loss=131.4476
	step [105/147], loss=123.8273
	step [106/147], loss=123.6919
	step [107/147], loss=106.4888
	step [108/147], loss=132.7515
	step [109/147], loss=139.8816
	step [110/147], loss=110.6742
	step [111/147], loss=105.2511
	step [112/147], loss=110.7160
	step [113/147], loss=111.2541
	step [114/147], loss=124.3009
	step [115/147], loss=105.3432
	step [116/147], loss=113.9287
	step [117/147], loss=135.0464
	step [118/147], loss=135.1078
	step [119/147], loss=103.4047
	step [120/147], loss=121.1581
	step [121/147], loss=126.3930
	step [122/147], loss=138.2917
	step [123/147], loss=124.3882
	step [124/147], loss=84.3303
	step [125/147], loss=90.3172
	step [126/147], loss=114.3466
	step [127/147], loss=105.8228
	step [128/147], loss=98.1265
	step [129/147], loss=121.3177
	step [130/147], loss=126.1225
	step [131/147], loss=127.8771
	step [132/147], loss=126.3515
	step [133/147], loss=118.6434
	step [134/147], loss=132.0958
	step [135/147], loss=123.5920
	step [136/147], loss=120.6530
	step [137/147], loss=99.3219
	step [138/147], loss=128.4841
	step [139/147], loss=123.9303
	step [140/147], loss=109.3815
	step [141/147], loss=111.9981
	step [142/147], loss=137.6923
	step [143/147], loss=130.8430
	step [144/147], loss=107.8426
	step [145/147], loss=115.3875
	step [146/147], loss=109.0174
	step [147/147], loss=39.1158
	Evaluating
	loss=0.0299, precision=0.5164, recall=0.8863, f1=0.6525
Training epoch 16
	step [1/147], loss=136.3916
	step [2/147], loss=98.5773
	step [3/147], loss=127.7456
	step [4/147], loss=107.2161
	step [5/147], loss=105.3211
	step [6/147], loss=100.9990
	step [7/147], loss=127.2861
	step [8/147], loss=111.2069
	step [9/147], loss=121.1950
	step [10/147], loss=131.9159
	step [11/147], loss=122.9954
	step [12/147], loss=115.9926
	step [13/147], loss=118.8874
	step [14/147], loss=111.3790
	step [15/147], loss=110.3002
	step [16/147], loss=123.8282
	step [17/147], loss=109.7089
	step [18/147], loss=109.2882
	step [19/147], loss=101.7971
	step [20/147], loss=119.3014
	step [21/147], loss=120.2271
	step [22/147], loss=121.0369
	step [23/147], loss=108.9166
	step [24/147], loss=116.4199
	step [25/147], loss=96.8367
	step [26/147], loss=113.4256
	step [27/147], loss=106.2503
	step [28/147], loss=101.1922
	step [29/147], loss=119.0370
	step [30/147], loss=101.9676
	step [31/147], loss=121.5009
	step [32/147], loss=115.4501
	step [33/147], loss=123.8227
	step [34/147], loss=111.3227
	step [35/147], loss=121.1555
	step [36/147], loss=108.4618
	step [37/147], loss=126.9254
	step [38/147], loss=127.2741
	step [39/147], loss=104.0462
	step [40/147], loss=110.2186
	step [41/147], loss=126.6588
	step [42/147], loss=105.0587
	step [43/147], loss=112.0131
	step [44/147], loss=121.5533
	step [45/147], loss=103.7641
	step [46/147], loss=138.3875
	step [47/147], loss=115.5397
	step [48/147], loss=92.0888
	step [49/147], loss=104.1223
	step [50/147], loss=118.8375
	step [51/147], loss=114.1318
	step [52/147], loss=136.4267
	step [53/147], loss=122.5639
	step [54/147], loss=107.3193
	step [55/147], loss=112.7819
	step [56/147], loss=115.4068
	step [57/147], loss=126.9930
	step [58/147], loss=129.8885
	step [59/147], loss=105.2885
	step [60/147], loss=100.0997
	step [61/147], loss=116.4199
	step [62/147], loss=113.4854
	step [63/147], loss=137.0566
	step [64/147], loss=117.3921
	step [65/147], loss=132.4496
	step [66/147], loss=104.2962
	step [67/147], loss=122.1219
	step [68/147], loss=128.9292
	step [69/147], loss=120.7333
	step [70/147], loss=128.6933
	step [71/147], loss=113.4768
	step [72/147], loss=110.5811
	step [73/147], loss=137.9807
	step [74/147], loss=103.1357
	step [75/147], loss=124.5946
	step [76/147], loss=100.1569
	step [77/147], loss=124.3871
	step [78/147], loss=124.4597
	step [79/147], loss=112.0025
	step [80/147], loss=145.1712
	step [81/147], loss=134.8415
	step [82/147], loss=114.6358
	step [83/147], loss=120.4716
	step [84/147], loss=122.3364
	step [85/147], loss=118.3879
	step [86/147], loss=118.8090
	step [87/147], loss=112.4014
	step [88/147], loss=115.7610
	step [89/147], loss=88.4741
	step [90/147], loss=106.4341
	step [91/147], loss=119.0910
	step [92/147], loss=112.5376
	step [93/147], loss=104.6440
	step [94/147], loss=96.6861
	step [95/147], loss=116.7832
	step [96/147], loss=122.7748
	step [97/147], loss=112.1194
	step [98/147], loss=116.7504
	step [99/147], loss=119.3466
	step [100/147], loss=114.1179
	step [101/147], loss=104.0525
	step [102/147], loss=111.0591
	step [103/147], loss=99.2858
	step [104/147], loss=121.9571
	step [105/147], loss=115.9146
	step [106/147], loss=136.5137
	step [107/147], loss=105.5473
	step [108/147], loss=115.4204
	step [109/147], loss=119.3171
	step [110/147], loss=117.3579
	step [111/147], loss=124.3517
	step [112/147], loss=128.3980
	step [113/147], loss=116.8712
	step [114/147], loss=124.8648
	step [115/147], loss=122.7067
	step [116/147], loss=107.6310
	step [117/147], loss=109.9227
	step [118/147], loss=135.0520
	step [119/147], loss=132.4335
	step [120/147], loss=109.5239
	step [121/147], loss=116.5046
	step [122/147], loss=115.1706
	step [123/147], loss=110.5455
	step [124/147], loss=121.9810
	step [125/147], loss=113.1657
	step [126/147], loss=116.8004
	step [127/147], loss=131.4059
	step [128/147], loss=117.8720
	step [129/147], loss=123.8347
	step [130/147], loss=139.9669
	step [131/147], loss=122.0475
	step [132/147], loss=111.5723
	step [133/147], loss=108.9419
	step [134/147], loss=110.9056
	step [135/147], loss=123.1555
	step [136/147], loss=111.3036
	step [137/147], loss=109.0474
	step [138/147], loss=98.8451
	step [139/147], loss=118.2892
	step [140/147], loss=102.5809
	step [141/147], loss=117.5033
	step [142/147], loss=115.7925
	step [143/147], loss=129.1685
	step [144/147], loss=99.8632
	step [145/147], loss=102.7629
	step [146/147], loss=123.7810
	step [147/147], loss=41.9251
	Evaluating
	loss=0.0304, precision=0.3941, recall=0.9246, f1=0.5526
Training epoch 17
	step [1/147], loss=128.8246
	step [2/147], loss=111.0594
	step [3/147], loss=136.6046
	step [4/147], loss=110.0822
	step [5/147], loss=108.5666
	step [6/147], loss=126.2425
	step [7/147], loss=101.3829
	step [8/147], loss=124.6867
	step [9/147], loss=107.5060
	step [10/147], loss=126.6914
	step [11/147], loss=110.7841
	step [12/147], loss=128.3660
	step [13/147], loss=112.1279
	step [14/147], loss=122.0597
	step [15/147], loss=125.2839
	step [16/147], loss=132.0139
	step [17/147], loss=127.6676
	step [18/147], loss=100.7055
	step [19/147], loss=139.5642
	step [20/147], loss=114.7038
	step [21/147], loss=110.9731
	step [22/147], loss=115.3217
	step [23/147], loss=102.0564
	step [24/147], loss=106.1233
	step [25/147], loss=144.2966
	step [26/147], loss=105.7110
	step [27/147], loss=98.3244
	step [28/147], loss=124.3262
	step [29/147], loss=117.4428
	step [30/147], loss=90.2083
	step [31/147], loss=109.5251
	step [32/147], loss=123.1735
	step [33/147], loss=95.5966
	step [34/147], loss=105.4569
	step [35/147], loss=110.6918
	step [36/147], loss=116.3424
	step [37/147], loss=106.7425
	step [38/147], loss=118.9630
	step [39/147], loss=119.9877
	step [40/147], loss=109.8448
	step [41/147], loss=140.7939
	step [42/147], loss=110.4568
	step [43/147], loss=129.3323
	step [44/147], loss=101.2880
	step [45/147], loss=124.4882
	step [46/147], loss=98.4927
	step [47/147], loss=103.5713
	step [48/147], loss=104.1360
	step [49/147], loss=124.2139
	step [50/147], loss=131.7002
	step [51/147], loss=119.2055
	step [52/147], loss=122.4113
	step [53/147], loss=102.7698
	step [54/147], loss=106.4263
	step [55/147], loss=112.7170
	step [56/147], loss=113.1923
	step [57/147], loss=100.3236
	step [58/147], loss=99.3476
	step [59/147], loss=120.9997
	step [60/147], loss=108.3608
	step [61/147], loss=111.7279
	step [62/147], loss=132.8571
	step [63/147], loss=102.2037
	step [64/147], loss=102.2526
	step [65/147], loss=104.4891
	step [66/147], loss=130.4947
	step [67/147], loss=116.1439
	step [68/147], loss=116.0101
	step [69/147], loss=119.4989
	step [70/147], loss=122.2108
	step [71/147], loss=130.7726
	step [72/147], loss=107.6032
	step [73/147], loss=125.3727
	step [74/147], loss=124.5773
	step [75/147], loss=112.7430
	step [76/147], loss=110.9281
	step [77/147], loss=125.3231
	step [78/147], loss=111.2897
	step [79/147], loss=114.2730
	step [80/147], loss=122.0686
	step [81/147], loss=109.5320
	step [82/147], loss=130.2092
	step [83/147], loss=105.0987
	step [84/147], loss=135.0676
	step [85/147], loss=95.7119
	step [86/147], loss=120.0602
	step [87/147], loss=115.8338
	step [88/147], loss=111.1173
	step [89/147], loss=128.3435
	step [90/147], loss=121.5885
	step [91/147], loss=105.6418
	step [92/147], loss=100.6701
	step [93/147], loss=117.7834
	step [94/147], loss=104.4420
	step [95/147], loss=104.1686
	step [96/147], loss=113.5003
	step [97/147], loss=105.8475
	step [98/147], loss=116.1142
	step [99/147], loss=120.5699
	step [100/147], loss=130.9424
	step [101/147], loss=112.2581
	step [102/147], loss=115.4051
	step [103/147], loss=101.9101
	step [104/147], loss=105.8070
	step [105/147], loss=110.4203
	step [106/147], loss=105.2295
	step [107/147], loss=109.3257
	step [108/147], loss=114.4633
	step [109/147], loss=113.8214
	step [110/147], loss=99.8276
	step [111/147], loss=103.4480
	step [112/147], loss=116.3730
	step [113/147], loss=93.0238
	step [114/147], loss=134.1083
	step [115/147], loss=129.2883
	step [116/147], loss=123.5022
	step [117/147], loss=106.6799
	step [118/147], loss=99.0702
	step [119/147], loss=121.4136
	step [120/147], loss=99.6571
	step [121/147], loss=130.3122
	step [122/147], loss=109.9041
	step [123/147], loss=124.2006
	step [124/147], loss=104.4612
	step [125/147], loss=114.8264
	step [126/147], loss=119.9330
	step [127/147], loss=101.4150
	step [128/147], loss=136.3559
	step [129/147], loss=118.4325
	step [130/147], loss=102.8682
	step [131/147], loss=112.3804
	step [132/147], loss=106.8531
	step [133/147], loss=126.0323
	step [134/147], loss=105.2138
	step [135/147], loss=129.1464
	step [136/147], loss=124.2270
	step [137/147], loss=117.5714
	step [138/147], loss=103.6168
	step [139/147], loss=106.6191
	step [140/147], loss=103.1777
	step [141/147], loss=85.1612
	step [142/147], loss=113.0835
	step [143/147], loss=123.6767
	step [144/147], loss=123.8258
	step [145/147], loss=98.8744
	step [146/147], loss=112.6392
	step [147/147], loss=58.4117
	Evaluating
	loss=0.0281, precision=0.4111, recall=0.9092, f1=0.5662
Training epoch 18
	step [1/147], loss=104.1242
	step [2/147], loss=110.4465
	step [3/147], loss=115.5136
	step [4/147], loss=129.9666
	step [5/147], loss=114.4301
	step [6/147], loss=90.4526
	step [7/147], loss=101.5241
	step [8/147], loss=101.8974
	step [9/147], loss=102.8083
	step [10/147], loss=88.8757
	step [11/147], loss=123.3348
	step [12/147], loss=96.4644
	step [13/147], loss=108.3607
	step [14/147], loss=120.2101
	step [15/147], loss=105.2370
	step [16/147], loss=133.1041
	step [17/147], loss=118.3063
	step [18/147], loss=93.1477
	step [19/147], loss=142.4926
	step [20/147], loss=108.3023
	step [21/147], loss=113.9654
	step [22/147], loss=108.0509
	step [23/147], loss=96.6292
	step [24/147], loss=118.4436
	step [25/147], loss=105.5556
	step [26/147], loss=123.9142
	step [27/147], loss=106.0834
	step [28/147], loss=109.1112
	step [29/147], loss=111.6552
	step [30/147], loss=97.9977
	step [31/147], loss=111.3174
	step [32/147], loss=116.7558
	step [33/147], loss=134.2157
	step [34/147], loss=130.7685
	step [35/147], loss=97.6064
	step [36/147], loss=111.5381
	step [37/147], loss=106.2474
	step [38/147], loss=111.9330
	step [39/147], loss=131.6223
	step [40/147], loss=110.1802
	step [41/147], loss=123.7376
	step [42/147], loss=104.8791
	step [43/147], loss=101.6113
	step [44/147], loss=131.6634
	step [45/147], loss=98.8634
	step [46/147], loss=112.3064
	step [47/147], loss=127.4032
	step [48/147], loss=118.3053
	step [49/147], loss=119.9118
	step [50/147], loss=111.3265
	step [51/147], loss=121.6535
	step [52/147], loss=124.7401
	step [53/147], loss=134.8522
	step [54/147], loss=119.3454
	step [55/147], loss=112.9875
	step [56/147], loss=153.6712
	step [57/147], loss=119.3417
	step [58/147], loss=128.9972
	step [59/147], loss=111.2399
	step [60/147], loss=93.5394
	step [61/147], loss=108.0413
	step [62/147], loss=116.1065
	step [63/147], loss=99.6648
	step [64/147], loss=130.3213
	step [65/147], loss=98.8981
	step [66/147], loss=98.2594
	step [67/147], loss=116.6806
	step [68/147], loss=112.2356
	step [69/147], loss=101.2345
	step [70/147], loss=110.7115
	step [71/147], loss=115.8447
	step [72/147], loss=112.2634
	step [73/147], loss=139.6846
	step [74/147], loss=105.7343
	step [75/147], loss=109.1287
	step [76/147], loss=121.5896
	step [77/147], loss=105.6295
	step [78/147], loss=96.4200
	step [79/147], loss=107.7832
	step [80/147], loss=117.4126
	step [81/147], loss=144.8406
	step [82/147], loss=117.0877
	step [83/147], loss=104.3960
	step [84/147], loss=109.6172
	step [85/147], loss=118.6850
	step [86/147], loss=101.2244
	step [87/147], loss=119.8353
	step [88/147], loss=94.5420
	step [89/147], loss=126.9129
	step [90/147], loss=103.3693
	step [91/147], loss=122.0283
	step [92/147], loss=102.0628
	step [93/147], loss=122.0250
	step [94/147], loss=131.6376
	step [95/147], loss=99.8086
	step [96/147], loss=126.4822
	step [97/147], loss=134.8212
	step [98/147], loss=103.9251
	step [99/147], loss=129.1423
	step [100/147], loss=112.6362
	step [101/147], loss=117.0091
	step [102/147], loss=120.6155
	step [103/147], loss=112.3510
	step [104/147], loss=112.8312
	step [105/147], loss=103.9641
	step [106/147], loss=115.1834
	step [107/147], loss=82.1721
	step [108/147], loss=121.9184
	step [109/147], loss=119.7479
	step [110/147], loss=103.0464
	step [111/147], loss=121.9350
	step [112/147], loss=107.3624
	step [113/147], loss=129.0820
	step [114/147], loss=89.8086
	step [115/147], loss=110.6538
	step [116/147], loss=104.3006
	step [117/147], loss=109.8942
	step [118/147], loss=102.7837
	step [119/147], loss=112.1285
	step [120/147], loss=111.1623
	step [121/147], loss=113.3215
	step [122/147], loss=116.4631
	step [123/147], loss=116.6254
	step [124/147], loss=105.5113
	step [125/147], loss=90.4187
	step [126/147], loss=122.1510
	step [127/147], loss=121.3078
	step [128/147], loss=106.1337
	step [129/147], loss=118.9820
	step [130/147], loss=100.8998
	step [131/147], loss=109.2685
	step [132/147], loss=141.5967
	step [133/147], loss=111.8774
	step [134/147], loss=105.0644
	step [135/147], loss=124.8770
	step [136/147], loss=113.3457
	step [137/147], loss=113.8895
	step [138/147], loss=107.1617
	step [139/147], loss=102.8380
	step [140/147], loss=112.2628
	step [141/147], loss=122.9300
	step [142/147], loss=113.5024
	step [143/147], loss=120.8790
	step [144/147], loss=121.5059
	step [145/147], loss=115.7845
	step [146/147], loss=113.1801
	step [147/147], loss=48.2754
	Evaluating
	loss=0.0314, precision=0.3163, recall=0.8818, f1=0.4656
Training epoch 19
	step [1/147], loss=99.8179
	step [2/147], loss=123.4574
	step [3/147], loss=98.5111
	step [4/147], loss=98.8728
	step [5/147], loss=119.6864
	step [6/147], loss=105.0470
	step [7/147], loss=100.8730
	step [8/147], loss=105.1870
	step [9/147], loss=114.8120
	step [10/147], loss=118.8435
	step [11/147], loss=113.6111
	step [12/147], loss=128.9359
	step [13/147], loss=135.0079
	step [14/147], loss=105.6675
	step [15/147], loss=89.1569
	step [16/147], loss=133.1830
	step [17/147], loss=117.5549
	step [18/147], loss=108.7101
	step [19/147], loss=94.5422
	step [20/147], loss=110.1559
	step [21/147], loss=142.8456
	step [22/147], loss=123.0625
	step [23/147], loss=111.0750
	step [24/147], loss=117.9722
	step [25/147], loss=139.5863
	step [26/147], loss=96.7704
	step [27/147], loss=121.4226
	step [28/147], loss=121.8323
	step [29/147], loss=113.0303
	step [30/147], loss=121.5142
	step [31/147], loss=102.2429
	step [32/147], loss=107.2514
	step [33/147], loss=129.9644
	step [34/147], loss=108.3744
	step [35/147], loss=87.9457
	step [36/147], loss=96.2406
	step [37/147], loss=92.1927
	step [38/147], loss=110.6149
	step [39/147], loss=104.9174
	step [40/147], loss=126.5346
	step [41/147], loss=104.1928
	step [42/147], loss=100.7754
	step [43/147], loss=128.9582
	step [44/147], loss=126.0330
	step [45/147], loss=110.8098
	step [46/147], loss=107.5776
	step [47/147], loss=101.5808
	step [48/147], loss=93.9161
	step [49/147], loss=111.3865
	step [50/147], loss=113.8273
	step [51/147], loss=104.0323
	step [52/147], loss=94.4329
	step [53/147], loss=97.6113
	step [54/147], loss=118.5862
	step [55/147], loss=104.6229
	step [56/147], loss=94.2782
	step [57/147], loss=98.9538
	step [58/147], loss=118.6237
	step [59/147], loss=111.8971
	step [60/147], loss=107.1684
	step [61/147], loss=111.5375
	step [62/147], loss=141.0470
	step [63/147], loss=111.3728
	step [64/147], loss=122.3172
	step [65/147], loss=102.2490
	step [66/147], loss=114.4843
	step [67/147], loss=131.0002
	step [68/147], loss=122.0974
	step [69/147], loss=106.4330
	step [70/147], loss=111.2756
	step [71/147], loss=121.6361
	step [72/147], loss=130.2975
	step [73/147], loss=121.4362
	step [74/147], loss=89.9539
	step [75/147], loss=107.8508
	step [76/147], loss=112.9173
	step [77/147], loss=82.6208
	step [78/147], loss=104.8491
	step [79/147], loss=105.6400
	step [80/147], loss=105.9222
	step [81/147], loss=121.7145
	step [82/147], loss=104.8199
	step [83/147], loss=112.1353
	step [84/147], loss=107.6191
	step [85/147], loss=129.5900
	step [86/147], loss=112.8397
	step [87/147], loss=106.1752
	step [88/147], loss=102.7090
	step [89/147], loss=94.6873
	step [90/147], loss=99.5331
	step [91/147], loss=84.5610
	step [92/147], loss=122.0670
	step [93/147], loss=120.8786
	step [94/147], loss=118.4136
	step [95/147], loss=110.9751
	step [96/147], loss=102.6512
	step [97/147], loss=115.5286
	step [98/147], loss=98.6244
	step [99/147], loss=127.3889
	step [100/147], loss=98.2486
	step [101/147], loss=130.1459
	step [102/147], loss=113.9829
	step [103/147], loss=106.5650
	step [104/147], loss=119.0419
	step [105/147], loss=100.1092
	step [106/147], loss=99.3687
	step [107/147], loss=118.1365
	step [108/147], loss=121.8026
	step [109/147], loss=113.9080
	step [110/147], loss=103.0783
	step [111/147], loss=121.3275
	step [112/147], loss=111.9851
	step [113/147], loss=108.3268
	step [114/147], loss=109.7047
	step [115/147], loss=110.0426
	step [116/147], loss=135.4216
	step [117/147], loss=98.8888
	step [118/147], loss=133.6451
	step [119/147], loss=120.5562
	step [120/147], loss=114.6479
	step [121/147], loss=122.5619
	step [122/147], loss=98.8699
	step [123/147], loss=92.7705
	step [124/147], loss=112.4394
	step [125/147], loss=136.5620
	step [126/147], loss=92.4532
	step [127/147], loss=108.3803
	step [128/147], loss=107.0312
	step [129/147], loss=103.8679
	step [130/147], loss=101.9175
	step [131/147], loss=127.2678
	step [132/147], loss=132.5718
	step [133/147], loss=118.4264
	step [134/147], loss=113.5977
	step [135/147], loss=128.0925
	step [136/147], loss=114.8732
	step [137/147], loss=109.3044
	step [138/147], loss=91.0010
	step [139/147], loss=107.5708
	step [140/147], loss=118.4293
	step [141/147], loss=90.1117
	step [142/147], loss=130.6102
	step [143/147], loss=113.3418
	step [144/147], loss=117.2273
	step [145/147], loss=120.9685
	step [146/147], loss=130.9886
	step [147/147], loss=45.1900
	Evaluating
	loss=0.0257, precision=0.3516, recall=0.9390, f1=0.5116
Training epoch 20
	step [1/147], loss=105.8000
	step [2/147], loss=103.2371
	step [3/147], loss=119.2907
	step [4/147], loss=110.3807
	step [5/147], loss=119.2066
	step [6/147], loss=108.8479
	step [7/147], loss=126.8615
	step [8/147], loss=110.6225
	step [9/147], loss=114.6055
	step [10/147], loss=123.7975
	step [11/147], loss=105.3975
	step [12/147], loss=109.3737
	step [13/147], loss=121.9141
	step [14/147], loss=97.8066
	step [15/147], loss=119.0943
	step [16/147], loss=114.8082
	step [17/147], loss=98.5119
	step [18/147], loss=100.4404
	step [19/147], loss=127.4332
	step [20/147], loss=108.5381
	step [21/147], loss=96.7441
	step [22/147], loss=109.3008
	step [23/147], loss=112.3860
	step [24/147], loss=89.7822
	step [25/147], loss=104.2220
	step [26/147], loss=98.5690
	step [27/147], loss=120.4863
	step [28/147], loss=97.6415
	step [29/147], loss=96.2672
	step [30/147], loss=121.0845
	step [31/147], loss=110.7435
	step [32/147], loss=96.3681
	step [33/147], loss=106.5510
	step [34/147], loss=110.3388
	step [35/147], loss=97.6198
	step [36/147], loss=119.7640
	step [37/147], loss=120.5157
	step [38/147], loss=97.0957
	step [39/147], loss=107.1552
	step [40/147], loss=104.1073
	step [41/147], loss=116.2256
	step [42/147], loss=98.5194
	step [43/147], loss=114.4492
	step [44/147], loss=103.2571
	step [45/147], loss=101.0024
	step [46/147], loss=109.7130
	step [47/147], loss=112.1573
	step [48/147], loss=108.6000
	step [49/147], loss=109.6295
	step [50/147], loss=103.2302
	step [51/147], loss=131.4789
	step [52/147], loss=106.5618
	step [53/147], loss=102.8676
	step [54/147], loss=110.7367
	step [55/147], loss=109.5697
	step [56/147], loss=100.1051
	step [57/147], loss=132.8923
	step [58/147], loss=117.5995
	step [59/147], loss=131.2782
	step [60/147], loss=93.6733
	step [61/147], loss=107.3067
	step [62/147], loss=120.1442
	step [63/147], loss=105.3110
	step [64/147], loss=89.7355
	step [65/147], loss=111.7364
	step [66/147], loss=94.0324
	step [67/147], loss=127.4773
	step [68/147], loss=112.0116
	step [69/147], loss=119.3215
	step [70/147], loss=115.5989
	step [71/147], loss=102.8092
	step [72/147], loss=120.0862
	step [73/147], loss=120.9730
	step [74/147], loss=142.1540
	step [75/147], loss=105.0910
	step [76/147], loss=120.1104
	step [77/147], loss=143.3493
	step [78/147], loss=118.7256
	step [79/147], loss=90.2591
	step [80/147], loss=117.3053
	step [81/147], loss=106.3804
	step [82/147], loss=121.3549
	step [83/147], loss=100.6851
	step [84/147], loss=106.8399
	step [85/147], loss=107.1847
	step [86/147], loss=110.1184
	step [87/147], loss=106.5080
	step [88/147], loss=133.3212
	step [89/147], loss=115.5605
	step [90/147], loss=116.0550
	step [91/147], loss=100.8411
	step [92/147], loss=106.1714
	step [93/147], loss=114.9925
	step [94/147], loss=107.3184
	step [95/147], loss=97.9168
	step [96/147], loss=115.0502
	step [97/147], loss=104.8793
	step [98/147], loss=96.8770
	step [99/147], loss=91.7626
	step [100/147], loss=109.7339
	step [101/147], loss=102.9003
	step [102/147], loss=116.5810
	step [103/147], loss=126.8519
	step [104/147], loss=128.8157
	step [105/147], loss=117.6126
	step [106/147], loss=110.7544
	step [107/147], loss=107.1927
	step [108/147], loss=115.0612
	step [109/147], loss=111.6525
	step [110/147], loss=116.3899
	step [111/147], loss=93.5910
	step [112/147], loss=113.5398
	step [113/147], loss=98.0879
	step [114/147], loss=94.2899
	step [115/147], loss=108.8268
	step [116/147], loss=107.4849
	step [117/147], loss=107.5908
	step [118/147], loss=104.0189
	step [119/147], loss=115.6676
	step [120/147], loss=107.1419
	step [121/147], loss=108.4509
	step [122/147], loss=106.3376
	step [123/147], loss=128.7528
	step [124/147], loss=111.6953
	step [125/147], loss=102.6377
	step [126/147], loss=109.7001
	step [127/147], loss=116.7555
	step [128/147], loss=92.5013
	step [129/147], loss=105.5331
	step [130/147], loss=138.1905
	step [131/147], loss=104.7835
	step [132/147], loss=104.7032
	step [133/147], loss=119.5351
	step [134/147], loss=123.9651
	step [135/147], loss=114.4633
	step [136/147], loss=113.9651
	step [137/147], loss=126.0937
	step [138/147], loss=94.7476
	step [139/147], loss=111.6924
	step [140/147], loss=108.2614
	step [141/147], loss=123.8204
	step [142/147], loss=106.0436
	step [143/147], loss=110.8069
	step [144/147], loss=103.6105
	step [145/147], loss=96.9522
	step [146/147], loss=105.7428
	step [147/147], loss=47.3881
	Evaluating
	loss=0.0248, precision=0.3311, recall=0.9255, f1=0.4877
Training epoch 21
	step [1/147], loss=120.2783
	step [2/147], loss=82.2572
	step [3/147], loss=123.4530
	step [4/147], loss=96.8873
	step [5/147], loss=84.6466
	step [6/147], loss=128.9966
	step [7/147], loss=95.3830
	step [8/147], loss=101.0864
	step [9/147], loss=107.8690
	step [10/147], loss=96.6173
	step [11/147], loss=124.7328
	step [12/147], loss=92.9730
	step [13/147], loss=113.8619
	step [14/147], loss=113.1173
	step [15/147], loss=113.2309
	step [16/147], loss=97.4847
	step [17/147], loss=99.6781
	step [18/147], loss=102.6876
	step [19/147], loss=99.1033
	step [20/147], loss=88.2525
	step [21/147], loss=125.8793
	step [22/147], loss=100.2670
	step [23/147], loss=109.7542
	step [24/147], loss=118.4242
	step [25/147], loss=96.7100
	step [26/147], loss=104.9911
	step [27/147], loss=105.8491
	step [28/147], loss=117.4364
	step [29/147], loss=117.8749
	step [30/147], loss=111.7606
	step [31/147], loss=102.3882
	step [32/147], loss=108.3266
	step [33/147], loss=107.3279
	step [34/147], loss=112.7065
	step [35/147], loss=108.0916
	step [36/147], loss=106.8558
	step [37/147], loss=116.5047
	step [38/147], loss=109.3145
	step [39/147], loss=132.4744
	step [40/147], loss=114.4427
	step [41/147], loss=99.6627
	step [42/147], loss=114.5626
	step [43/147], loss=116.6655
	step [44/147], loss=115.3787
	step [45/147], loss=111.1712
	step [46/147], loss=115.5980
	step [47/147], loss=97.6728
	step [48/147], loss=89.7996
	step [49/147], loss=115.3586
	step [50/147], loss=102.1361
	step [51/147], loss=106.4417
	step [52/147], loss=127.6483
	step [53/147], loss=118.8140
	step [54/147], loss=121.7207
	step [55/147], loss=117.0644
	step [56/147], loss=93.8313
	step [57/147], loss=97.3768
	step [58/147], loss=109.3338
	step [59/147], loss=107.9688
	step [60/147], loss=121.0578
	step [61/147], loss=109.8688
	step [62/147], loss=123.7336
	step [63/147], loss=105.0570
	step [64/147], loss=114.1795
	step [65/147], loss=122.8309
	step [66/147], loss=96.7241
	step [67/147], loss=93.9661
	step [68/147], loss=110.1216
	step [69/147], loss=111.6871
	step [70/147], loss=111.3948
	step [71/147], loss=86.5635
	step [72/147], loss=97.7069
	step [73/147], loss=99.1683
	step [74/147], loss=109.7433
	step [75/147], loss=118.0813
	step [76/147], loss=104.8699
	step [77/147], loss=104.2675
	step [78/147], loss=120.9857
	step [79/147], loss=102.8777
	step [80/147], loss=123.9878
	step [81/147], loss=110.5949
	step [82/147], loss=120.7441
	step [83/147], loss=101.7184
	step [84/147], loss=109.7258
	step [85/147], loss=131.5123
	step [86/147], loss=88.0231
	step [87/147], loss=107.1098
	step [88/147], loss=99.2266
	step [89/147], loss=123.3886
	step [90/147], loss=100.9488
	step [91/147], loss=91.8768
	step [92/147], loss=121.7789
	step [93/147], loss=129.5516
	step [94/147], loss=120.9904
	step [95/147], loss=112.9077
	step [96/147], loss=100.4385
	step [97/147], loss=102.1356
	step [98/147], loss=134.8547
	step [99/147], loss=115.0710
	step [100/147], loss=116.9488
	step [101/147], loss=105.8645
	step [102/147], loss=108.2839
	step [103/147], loss=101.7311
	step [104/147], loss=100.4618
	step [105/147], loss=100.9363
	step [106/147], loss=97.6235
	step [107/147], loss=102.6318
	step [108/147], loss=115.8085
	step [109/147], loss=112.5492
	step [110/147], loss=103.3049
	step [111/147], loss=89.0345
	step [112/147], loss=95.8374
	step [113/147], loss=119.9932
	step [114/147], loss=97.2244
	step [115/147], loss=98.7932
	step [116/147], loss=113.4754
	step [117/147], loss=95.1937
	step [118/147], loss=114.2198
	step [119/147], loss=117.2243
	step [120/147], loss=136.6906
	step [121/147], loss=86.0302
	step [122/147], loss=112.3137
	step [123/147], loss=101.7996
	step [124/147], loss=107.0226
	step [125/147], loss=119.8831
	step [126/147], loss=110.8547
	step [127/147], loss=89.7629
	step [128/147], loss=111.2875
	step [129/147], loss=119.8102
	step [130/147], loss=130.8221
	step [131/147], loss=88.2288
	step [132/147], loss=108.9298
	step [133/147], loss=106.1516
	step [134/147], loss=98.6165
	step [135/147], loss=108.6075
	step [136/147], loss=98.1722
	step [137/147], loss=117.3684
	step [138/147], loss=96.8641
	step [139/147], loss=104.6392
	step [140/147], loss=99.2072
	step [141/147], loss=117.6195
	step [142/147], loss=114.7782
	step [143/147], loss=134.6492
	step [144/147], loss=121.7079
	step [145/147], loss=98.2982
	step [146/147], loss=99.0065
	step [147/147], loss=62.2185
	Evaluating
	loss=0.0214, precision=0.4235, recall=0.9222, f1=0.5804
Training epoch 22
	step [1/147], loss=100.3614
	step [2/147], loss=99.8012
	step [3/147], loss=106.9664
	step [4/147], loss=112.7956
	step [5/147], loss=132.7578
	step [6/147], loss=94.6938
	step [7/147], loss=97.9052
	step [8/147], loss=100.2153
	step [9/147], loss=98.1360
	step [10/147], loss=105.5192
	step [11/147], loss=111.1063
	step [12/147], loss=110.3050
	step [13/147], loss=112.5942
	step [14/147], loss=95.9678
	step [15/147], loss=96.5856
	step [16/147], loss=104.3737
	step [17/147], loss=92.7717
	step [18/147], loss=100.9445
	step [19/147], loss=99.0999
	step [20/147], loss=103.4369
	step [21/147], loss=111.2869
	step [22/147], loss=105.1608
	step [23/147], loss=123.3869
	step [24/147], loss=117.5900
	step [25/147], loss=118.4592
	step [26/147], loss=98.5338
	step [27/147], loss=110.4326
	step [28/147], loss=110.6347
	step [29/147], loss=111.8793
	step [30/147], loss=125.2616
	step [31/147], loss=118.2336
	step [32/147], loss=113.2791
	step [33/147], loss=100.7619
	step [34/147], loss=103.6771
	step [35/147], loss=121.2127
	step [36/147], loss=108.9785
	step [37/147], loss=120.9810
	step [38/147], loss=114.1972
	step [39/147], loss=90.8397
	step [40/147], loss=104.8964
	step [41/147], loss=119.0040
	step [42/147], loss=116.7180
	step [43/147], loss=100.1902
	step [44/147], loss=107.1658
	step [45/147], loss=112.0653
	step [46/147], loss=131.3241
	step [47/147], loss=107.7979
	step [48/147], loss=118.0290
	step [49/147], loss=104.1185
	step [50/147], loss=90.7791
	step [51/147], loss=107.1339
	step [52/147], loss=118.1152
	step [53/147], loss=117.6846
	step [54/147], loss=115.3606
	step [55/147], loss=82.9021
	step [56/147], loss=119.7545
	step [57/147], loss=99.4092
	step [58/147], loss=115.6870
	step [59/147], loss=97.7612
	step [60/147], loss=87.3864
	step [61/147], loss=105.3114
	step [62/147], loss=92.9719
	step [63/147], loss=102.4775
	step [64/147], loss=110.4283
	step [65/147], loss=120.0083
	step [66/147], loss=113.0066
	step [67/147], loss=94.7647
	step [68/147], loss=118.6419
	step [69/147], loss=84.1271
	step [70/147], loss=123.7528
	step [71/147], loss=121.1625
	step [72/147], loss=98.4282
	step [73/147], loss=105.7571
	step [74/147], loss=112.8562
	step [75/147], loss=105.6241
	step [76/147], loss=103.0748
	step [77/147], loss=99.2724
	step [78/147], loss=108.2775
	step [79/147], loss=93.9606
	step [80/147], loss=92.2849
	step [81/147], loss=89.5905
	step [82/147], loss=92.6536
	step [83/147], loss=127.2414
	step [84/147], loss=97.2812
	step [85/147], loss=92.5079
	step [86/147], loss=106.6435
	step [87/147], loss=118.6650
	step [88/147], loss=101.2272
	step [89/147], loss=99.6449
	step [90/147], loss=104.1836
	step [91/147], loss=91.4708
	step [92/147], loss=109.4771
	step [93/147], loss=120.2714
	step [94/147], loss=109.9451
	step [95/147], loss=101.4077
	step [96/147], loss=99.1295
	step [97/147], loss=101.7068
	step [98/147], loss=110.6297
	step [99/147], loss=101.6946
	step [100/147], loss=108.7040
	step [101/147], loss=116.0998
	step [102/147], loss=92.9150
	step [103/147], loss=102.0476
	step [104/147], loss=98.9552
	step [105/147], loss=114.9253
	step [106/147], loss=139.3941
	step [107/147], loss=93.1639
	step [108/147], loss=104.5413
	step [109/147], loss=84.3704
	step [110/147], loss=105.9660
	step [111/147], loss=95.5847
	step [112/147], loss=91.5624
	step [113/147], loss=111.9257
	step [114/147], loss=101.4168
	step [115/147], loss=131.3638
	step [116/147], loss=86.4321
	step [117/147], loss=103.6239
	step [118/147], loss=112.6916
	step [119/147], loss=109.3099
	step [120/147], loss=108.2728
	step [121/147], loss=121.4598
	step [122/147], loss=105.2617
	step [123/147], loss=113.2027
	step [124/147], loss=111.8860
	step [125/147], loss=111.9245
	step [126/147], loss=121.4881
	step [127/147], loss=124.8004
	step [128/147], loss=74.6710
	step [129/147], loss=112.3702
	step [130/147], loss=115.7826
	step [131/147], loss=109.2120
	step [132/147], loss=115.9324
	step [133/147], loss=108.9764
	step [134/147], loss=109.3982
	step [135/147], loss=96.1367
	step [136/147], loss=121.4275
	step [137/147], loss=104.8919
	step [138/147], loss=116.2654
	step [139/147], loss=101.0498
	step [140/147], loss=119.7786
	step [141/147], loss=119.7806
	step [142/147], loss=109.9930
	step [143/147], loss=110.6535
	step [144/147], loss=114.3429
	step [145/147], loss=102.3771
	step [146/147], loss=115.4140
	step [147/147], loss=44.6751
	Evaluating
	loss=0.0205, precision=0.3984, recall=0.8937, f1=0.5511
Training epoch 23
	step [1/147], loss=91.0766
	step [2/147], loss=114.7789
	step [3/147], loss=109.0133
	step [4/147], loss=94.1703
	step [5/147], loss=113.0319
	step [6/147], loss=103.6757
	step [7/147], loss=89.6903
	step [8/147], loss=100.6404
	step [9/147], loss=131.1230
	step [10/147], loss=104.8869
	step [11/147], loss=119.9333
	step [12/147], loss=101.6266
	step [13/147], loss=125.0268
	step [14/147], loss=103.7291
	step [15/147], loss=93.2643
	step [16/147], loss=99.4934
	step [17/147], loss=101.5293
	step [18/147], loss=88.0062
	step [19/147], loss=112.8847
	step [20/147], loss=113.8586
	step [21/147], loss=100.4742
	step [22/147], loss=114.9152
	step [23/147], loss=113.4277
	step [24/147], loss=86.0350
	step [25/147], loss=115.3348
	step [26/147], loss=119.2844
	step [27/147], loss=111.3930
	step [28/147], loss=102.0822
	step [29/147], loss=100.8355
	step [30/147], loss=123.0534
	step [31/147], loss=107.1136
	step [32/147], loss=96.2709
	step [33/147], loss=115.3148
	step [34/147], loss=105.4948
	step [35/147], loss=106.9294
	step [36/147], loss=109.6086
	step [37/147], loss=101.8493
	step [38/147], loss=89.8275
	step [39/147], loss=116.2178
	step [40/147], loss=120.7641
	step [41/147], loss=86.5272
	step [42/147], loss=95.5199
	step [43/147], loss=114.3670
	step [44/147], loss=115.4274
	step [45/147], loss=128.6759
	step [46/147], loss=92.3358
	step [47/147], loss=106.9434
	step [48/147], loss=100.2498
	step [49/147], loss=91.1453
	step [50/147], loss=133.2889
	step [51/147], loss=117.4086
	step [52/147], loss=111.9458
	step [53/147], loss=113.1549
	step [54/147], loss=117.4865
	step [55/147], loss=115.8456
	step [56/147], loss=127.6403
	step [57/147], loss=110.4340
	step [58/147], loss=124.3232
	step [59/147], loss=103.5619
	step [60/147], loss=106.7294
	step [61/147], loss=105.0305
	step [62/147], loss=92.6671
	step [63/147], loss=106.4245
	step [64/147], loss=95.5783
	step [65/147], loss=94.8872
	step [66/147], loss=106.1191
	step [67/147], loss=128.1160
	step [68/147], loss=98.2292
	step [69/147], loss=99.9865
	step [70/147], loss=97.0029
	step [71/147], loss=104.2556
	step [72/147], loss=95.9429
	step [73/147], loss=121.0660
	step [74/147], loss=85.7857
	step [75/147], loss=133.8861
	step [76/147], loss=125.9401
	step [77/147], loss=100.3153
	step [78/147], loss=116.0128
	step [79/147], loss=104.3553
	step [80/147], loss=106.4390
	step [81/147], loss=83.0193
	step [82/147], loss=105.7641
	step [83/147], loss=96.7651
	step [84/147], loss=92.1965
	step [85/147], loss=108.0801
	step [86/147], loss=108.5571
	step [87/147], loss=109.3662
	step [88/147], loss=103.3463
	step [89/147], loss=109.6259
	step [90/147], loss=109.4977
	step [91/147], loss=129.7986
	step [92/147], loss=79.1298
	step [93/147], loss=102.3976
	step [94/147], loss=100.7831
	step [95/147], loss=111.7809
	step [96/147], loss=110.0819
	step [97/147], loss=101.8869
	step [98/147], loss=105.3545
	step [99/147], loss=108.3204
	step [100/147], loss=96.6084
	step [101/147], loss=96.9714
	step [102/147], loss=111.5267
	step [103/147], loss=103.1283
	step [104/147], loss=99.1676
	step [105/147], loss=105.1296
	step [106/147], loss=111.2194
	step [107/147], loss=87.7085
	step [108/147], loss=127.9648
	step [109/147], loss=104.6345
	step [110/147], loss=106.2005
	step [111/147], loss=104.0895
	step [112/147], loss=87.3558
	step [113/147], loss=89.7634
	step [114/147], loss=105.2023
	step [115/147], loss=103.4272
	step [116/147], loss=117.4929
	step [117/147], loss=110.8938
	step [118/147], loss=115.0550
	step [119/147], loss=95.8118
	step [120/147], loss=94.4505
	step [121/147], loss=111.9750
	step [122/147], loss=123.9813
	step [123/147], loss=112.0926
	step [124/147], loss=93.9217
	step [125/147], loss=103.9670
	step [126/147], loss=105.2080
	step [127/147], loss=100.5852
	step [128/147], loss=90.7935
	step [129/147], loss=102.2140
	step [130/147], loss=102.6899
	step [131/147], loss=84.0124
	step [132/147], loss=93.6167
	step [133/147], loss=94.5402
	step [134/147], loss=108.8091
	step [135/147], loss=101.9004
	step [136/147], loss=108.4988
	step [137/147], loss=103.4159
	step [138/147], loss=107.2352
	step [139/147], loss=119.0487
	step [140/147], loss=107.6694
	step [141/147], loss=99.8049
	step [142/147], loss=95.7360
	step [143/147], loss=127.2348
	step [144/147], loss=119.8454
	step [145/147], loss=94.3602
	step [146/147], loss=110.7830
	step [147/147], loss=48.5745
	Evaluating
	loss=0.0192, precision=0.3796, recall=0.9291, f1=0.5390
Training epoch 24
	step [1/147], loss=96.3618
	step [2/147], loss=98.6824
	step [3/147], loss=124.9090
	step [4/147], loss=108.5476
	step [5/147], loss=106.6003
	step [6/147], loss=125.9458
	step [7/147], loss=99.3207
	step [8/147], loss=108.3571
	step [9/147], loss=110.3048
	step [10/147], loss=101.9612
	step [11/147], loss=93.4968
	step [12/147], loss=102.2684
	step [13/147], loss=100.3494
	step [14/147], loss=112.3864
	step [15/147], loss=80.3307
	step [16/147], loss=97.1642
	step [17/147], loss=107.3811
	step [18/147], loss=103.5421
	step [19/147], loss=96.2335
	step [20/147], loss=91.3369
	step [21/147], loss=91.9607
	step [22/147], loss=112.2604
	step [23/147], loss=91.3624
	step [24/147], loss=105.4559
	step [25/147], loss=102.9387
	step [26/147], loss=121.6270
	step [27/147], loss=102.4536
	step [28/147], loss=107.4137
	step [29/147], loss=94.7985
	step [30/147], loss=104.9404
	step [31/147], loss=100.2586
	step [32/147], loss=92.9505
	step [33/147], loss=112.2184
	step [34/147], loss=97.4546
	step [35/147], loss=109.2442
	step [36/147], loss=99.5857
	step [37/147], loss=88.7280
	step [38/147], loss=108.0514
	step [39/147], loss=91.4716
	step [40/147], loss=92.7965
	step [41/147], loss=87.9913
	step [42/147], loss=109.6957
	step [43/147], loss=91.7001
	step [44/147], loss=110.7869
	step [45/147], loss=117.0693
	step [46/147], loss=99.2586
	step [47/147], loss=106.3030
	step [48/147], loss=101.3505
	step [49/147], loss=85.9928
	step [50/147], loss=112.8827
	step [51/147], loss=105.0363
	step [52/147], loss=110.4103
	step [53/147], loss=105.3939
	step [54/147], loss=109.4137
	step [55/147], loss=111.4224
	step [56/147], loss=102.9880
	step [57/147], loss=96.9825
	step [58/147], loss=117.3674
	step [59/147], loss=113.5216
	step [60/147], loss=99.5929
	step [61/147], loss=108.1753
	step [62/147], loss=109.9800
	step [63/147], loss=98.8334
	step [64/147], loss=96.7711
	step [65/147], loss=118.6086
	step [66/147], loss=90.3477
	step [67/147], loss=125.8792
	step [68/147], loss=101.5932
	step [69/147], loss=115.9241
	step [70/147], loss=110.2275
	step [71/147], loss=116.5878
	step [72/147], loss=105.9574
	step [73/147], loss=108.3088
	step [74/147], loss=120.7286
	step [75/147], loss=111.5029
	step [76/147], loss=102.2904
	step [77/147], loss=107.9207
	step [78/147], loss=103.5482
	step [79/147], loss=91.2134
	step [80/147], loss=96.6705
	step [81/147], loss=100.7864
	step [82/147], loss=130.4379
	step [83/147], loss=118.3403
	step [84/147], loss=105.7779
	step [85/147], loss=111.9318
	step [86/147], loss=107.6880
	step [87/147], loss=118.5364
	step [88/147], loss=96.1434
	step [89/147], loss=89.5065
	step [90/147], loss=101.4550
	step [91/147], loss=106.4494
	step [92/147], loss=108.9540
	step [93/147], loss=125.5845
	step [94/147], loss=107.5635
	step [95/147], loss=115.0611
	step [96/147], loss=107.8886
	step [97/147], loss=114.5613
	step [98/147], loss=112.3059
	step [99/147], loss=127.9869
	step [100/147], loss=105.0706
	step [101/147], loss=108.5325
	step [102/147], loss=105.1092
	step [103/147], loss=99.9217
	step [104/147], loss=111.5076
	step [105/147], loss=118.0160
	step [106/147], loss=104.4586
	step [107/147], loss=100.8272
	step [108/147], loss=89.4392
	step [109/147], loss=117.8588
	step [110/147], loss=87.8870
	step [111/147], loss=103.0761
	step [112/147], loss=91.4405
	step [113/147], loss=99.4581
	step [114/147], loss=93.7930
	step [115/147], loss=89.8275
	step [116/147], loss=98.2113
	step [117/147], loss=92.0568
	step [118/147], loss=106.3856
	step [119/147], loss=92.4387
	step [120/147], loss=105.8444
	step [121/147], loss=111.2799
	step [122/147], loss=105.9585
	step [123/147], loss=115.3780
	step [124/147], loss=97.9062
	step [125/147], loss=105.9204
	step [126/147], loss=116.4144
	step [127/147], loss=84.5566
	step [128/147], loss=102.2080
	step [129/147], loss=99.4366
	step [130/147], loss=102.5580
	step [131/147], loss=95.3701
	step [132/147], loss=94.0146
	step [133/147], loss=101.7961
	step [134/147], loss=116.1141
	step [135/147], loss=96.7305
	step [136/147], loss=111.0381
	step [137/147], loss=93.7787
	step [138/147], loss=109.2640
	step [139/147], loss=115.8773
	step [140/147], loss=125.6293
	step [141/147], loss=109.0200
	step [142/147], loss=88.6335
	step [143/147], loss=100.2293
	step [144/147], loss=111.5818
	step [145/147], loss=96.8952
	step [146/147], loss=92.4648
	step [147/147], loss=42.2359
	Evaluating
	loss=0.0188, precision=0.3905, recall=0.8973, f1=0.5442
Training epoch 25
	step [1/147], loss=108.6542
	step [2/147], loss=105.7363
	step [3/147], loss=90.4109
	step [4/147], loss=105.8273
	step [5/147], loss=91.3591
	step [6/147], loss=102.9913
	step [7/147], loss=108.5511
	step [8/147], loss=89.4195
	step [9/147], loss=97.1423
	step [10/147], loss=107.0847
	step [11/147], loss=111.2735
	step [12/147], loss=109.2186
	step [13/147], loss=102.2635
	step [14/147], loss=104.5330
	step [15/147], loss=105.4665
	step [16/147], loss=94.6111
	step [17/147], loss=84.8893
	step [18/147], loss=107.7979
	step [19/147], loss=104.7536
	step [20/147], loss=108.9655
	step [21/147], loss=105.9521
	step [22/147], loss=108.3476
	step [23/147], loss=123.0216
	step [24/147], loss=109.3114
	step [25/147], loss=97.4821
	step [26/147], loss=96.1550
	step [27/147], loss=104.3780
	step [28/147], loss=78.3773
	step [29/147], loss=101.0207
	step [30/147], loss=110.7456
	step [31/147], loss=98.7614
	step [32/147], loss=116.4632
	step [33/147], loss=101.2462
	step [34/147], loss=95.0886
	step [35/147], loss=108.8711
	step [36/147], loss=110.0360
	step [37/147], loss=95.9051
	step [38/147], loss=93.1654
	step [39/147], loss=111.7261
	step [40/147], loss=93.3624
	step [41/147], loss=102.8369
	step [42/147], loss=131.2393
	step [43/147], loss=111.8721
	step [44/147], loss=119.6021
	step [45/147], loss=98.0518
	step [46/147], loss=105.2379
	step [47/147], loss=94.8591
	step [48/147], loss=106.3108
	step [49/147], loss=94.2639
	step [50/147], loss=100.1573
	step [51/147], loss=83.6897
	step [52/147], loss=95.3456
	step [53/147], loss=106.3424
	step [54/147], loss=107.8442
	step [55/147], loss=90.0319
	step [56/147], loss=97.8009
	step [57/147], loss=106.9613
	step [58/147], loss=97.7084
	step [59/147], loss=116.4362
	step [60/147], loss=121.5034
	step [61/147], loss=88.5877
	step [62/147], loss=101.2723
	step [63/147], loss=84.3427
	step [64/147], loss=107.8995
	step [65/147], loss=116.6729
	step [66/147], loss=101.5403
	step [67/147], loss=103.5938
	step [68/147], loss=92.7545
	step [69/147], loss=91.2284
	step [70/147], loss=108.7093
	step [71/147], loss=113.3922
	step [72/147], loss=105.5493
	step [73/147], loss=92.3993
	step [74/147], loss=112.3759
	step [75/147], loss=98.9036
	step [76/147], loss=104.8117
	step [77/147], loss=125.5560
	step [78/147], loss=105.7185
	step [79/147], loss=88.2112
	step [80/147], loss=105.3459
	step [81/147], loss=115.3747
	step [82/147], loss=93.4183
	step [83/147], loss=77.4858
	step [84/147], loss=101.9897
	step [85/147], loss=125.6734
	step [86/147], loss=93.2720
	step [87/147], loss=97.1264
	step [88/147], loss=130.0387
	step [89/147], loss=82.0372
	step [90/147], loss=112.3669
	step [91/147], loss=97.7086
	step [92/147], loss=84.7173
	step [93/147], loss=119.2645
	step [94/147], loss=104.9307
	step [95/147], loss=102.7441
	step [96/147], loss=103.6161
	step [97/147], loss=91.7093
	step [98/147], loss=89.8388
	step [99/147], loss=91.9523
	step [100/147], loss=106.0989
	step [101/147], loss=111.2025
	step [102/147], loss=103.9315
	step [103/147], loss=99.9855
	step [104/147], loss=97.3874
	step [105/147], loss=100.7981
	step [106/147], loss=98.6351
	step [107/147], loss=108.9355
	step [108/147], loss=94.7696
	step [109/147], loss=114.0127
	step [110/147], loss=103.4660
	step [111/147], loss=113.8562
	step [112/147], loss=117.3517
	step [113/147], loss=103.4754
	step [114/147], loss=105.4222
	step [115/147], loss=109.9733
	step [116/147], loss=95.0842
	step [117/147], loss=95.1164
	step [118/147], loss=93.4070
	step [119/147], loss=116.3841
	step [120/147], loss=91.6432
	step [121/147], loss=118.9055
	step [122/147], loss=122.1500
	step [123/147], loss=117.7347
	step [124/147], loss=102.8215
	step [125/147], loss=102.4512
	step [126/147], loss=91.0205
	step [127/147], loss=97.4001
	step [128/147], loss=94.2341
	step [129/147], loss=104.7097
	step [130/147], loss=112.7505
	step [131/147], loss=93.8622
	step [132/147], loss=102.1479
	step [133/147], loss=99.2118
	step [134/147], loss=124.2561
	step [135/147], loss=108.7467
	step [136/147], loss=124.7783
	step [137/147], loss=111.2400
	step [138/147], loss=119.5525
	step [139/147], loss=79.9322
	step [140/147], loss=103.4047
	step [141/147], loss=88.3981
	step [142/147], loss=85.6444
	step [143/147], loss=95.0139
	step [144/147], loss=90.9907
	step [145/147], loss=116.2652
	step [146/147], loss=99.7505
	step [147/147], loss=39.6339
	Evaluating
	loss=0.0154, precision=0.4339, recall=0.9068, f1=0.5869
Training epoch 26
	step [1/147], loss=107.9895
	step [2/147], loss=100.4412
	step [3/147], loss=105.3831
	step [4/147], loss=105.4156
	step [5/147], loss=99.9907
	step [6/147], loss=102.5936
	step [7/147], loss=113.0692
	step [8/147], loss=91.8678
	step [9/147], loss=95.0333
	step [10/147], loss=107.5450
	step [11/147], loss=97.5789
	step [12/147], loss=94.6136
	step [13/147], loss=99.0172
	step [14/147], loss=96.7716
	step [15/147], loss=97.2723
	step [16/147], loss=83.0427
	step [17/147], loss=84.8971
	step [18/147], loss=102.3474
	step [19/147], loss=99.4441
	step [20/147], loss=109.3804
	step [21/147], loss=90.5504
	step [22/147], loss=109.8968
	step [23/147], loss=105.7031
	step [24/147], loss=107.5420
	step [25/147], loss=95.2274
	step [26/147], loss=123.5967
	step [27/147], loss=110.8290
	step [28/147], loss=98.4349
	step [29/147], loss=114.1865
	step [30/147], loss=89.0761
	step [31/147], loss=108.0505
	step [32/147], loss=94.1875
	step [33/147], loss=96.9285
	step [34/147], loss=100.6425
	step [35/147], loss=84.2851
	step [36/147], loss=102.4751
	step [37/147], loss=90.4637
	step [38/147], loss=106.4893
	step [39/147], loss=92.7592
	step [40/147], loss=99.2409
	step [41/147], loss=108.3211
	step [42/147], loss=99.0744
	step [43/147], loss=106.1183
	step [44/147], loss=120.6317
	step [45/147], loss=90.7316
	step [46/147], loss=115.0348
	step [47/147], loss=94.3777
	step [48/147], loss=125.2553
	step [49/147], loss=96.3808
	step [50/147], loss=125.6950
	step [51/147], loss=100.2322
	step [52/147], loss=95.3483
	step [53/147], loss=111.8194
	step [54/147], loss=113.1025
	step [55/147], loss=97.1963
	step [56/147], loss=110.4444
	step [57/147], loss=103.7382
	step [58/147], loss=109.2336
	step [59/147], loss=89.1888
	step [60/147], loss=107.3418
	step [61/147], loss=103.9544
	step [62/147], loss=123.2069
	step [63/147], loss=107.6982
	step [64/147], loss=87.0611
	step [65/147], loss=89.4182
	step [66/147], loss=105.0840
	step [67/147], loss=105.6826
	step [68/147], loss=88.1590
	step [69/147], loss=113.5039
	step [70/147], loss=116.7994
	step [71/147], loss=104.7921
	step [72/147], loss=108.3715
	step [73/147], loss=116.9264
	step [74/147], loss=110.2936
	step [75/147], loss=105.0495
	step [76/147], loss=99.7467
	step [77/147], loss=94.5992
	step [78/147], loss=85.6827
	step [79/147], loss=114.7208
	step [80/147], loss=79.5225
	step [81/147], loss=94.6028
	step [82/147], loss=112.1955
	step [83/147], loss=104.9664
	step [84/147], loss=101.0758
	step [85/147], loss=104.0755
	step [86/147], loss=114.2313
	step [87/147], loss=90.7865
	step [88/147], loss=95.6363
	step [89/147], loss=90.4550
	step [90/147], loss=93.4865
	step [91/147], loss=94.2146
	step [92/147], loss=101.8092
	step [93/147], loss=89.4243
	step [94/147], loss=107.2532
	step [95/147], loss=91.1863
	step [96/147], loss=95.3047
	step [97/147], loss=86.8449
	step [98/147], loss=89.5198
	step [99/147], loss=98.6866
	step [100/147], loss=107.3885
	step [101/147], loss=100.1233
	step [102/147], loss=129.2160
	step [103/147], loss=109.7309
	step [104/147], loss=88.2048
	step [105/147], loss=95.1511
	step [106/147], loss=104.4096
	step [107/147], loss=97.4581
	step [108/147], loss=88.3335
	step [109/147], loss=89.5391
	step [110/147], loss=102.2368
	step [111/147], loss=119.9795
	step [112/147], loss=103.0703
	step [113/147], loss=103.3877
	step [114/147], loss=93.2099
	step [115/147], loss=124.5534
	step [116/147], loss=100.0026
	step [117/147], loss=133.1449
	step [118/147], loss=99.1784
	step [119/147], loss=95.3030
	step [120/147], loss=116.3904
	step [121/147], loss=104.9744
	step [122/147], loss=119.2296
	step [123/147], loss=98.5154
	step [124/147], loss=93.4709
	step [125/147], loss=90.7823
	step [126/147], loss=109.4887
	step [127/147], loss=96.0417
	step [128/147], loss=108.7570
	step [129/147], loss=85.0731
	step [130/147], loss=90.8425
	step [131/147], loss=69.8513
	step [132/147], loss=104.5648
	step [133/147], loss=87.9717
	step [134/147], loss=94.3884
	step [135/147], loss=119.6874
	step [136/147], loss=106.5575
	step [137/147], loss=99.8643
	step [138/147], loss=105.3565
	step [139/147], loss=99.9385
	step [140/147], loss=89.7923
	step [141/147], loss=96.2151
	step [142/147], loss=90.0670
	step [143/147], loss=111.6433
	step [144/147], loss=86.4239
	step [145/147], loss=89.3407
	step [146/147], loss=105.4884
	step [147/147], loss=42.7074
	Evaluating
	loss=0.0157, precision=0.4145, recall=0.9036, f1=0.5683
Training epoch 27
	step [1/147], loss=109.2510
	step [2/147], loss=101.1947
	step [3/147], loss=102.4797
	step [4/147], loss=107.2005
	step [5/147], loss=104.1414
	step [6/147], loss=76.9819
	step [7/147], loss=89.0705
	step [8/147], loss=111.8684
	step [9/147], loss=107.2867
	step [10/147], loss=97.5853
	step [11/147], loss=107.5155
	step [12/147], loss=89.3615
	step [13/147], loss=101.3497
	step [14/147], loss=114.3241
	step [15/147], loss=94.5874
	step [16/147], loss=76.7156
	step [17/147], loss=107.5772
	step [18/147], loss=82.4028
	step [19/147], loss=106.0404
	step [20/147], loss=97.2181
	step [21/147], loss=94.9894
	step [22/147], loss=90.2383
	step [23/147], loss=88.7900
	step [24/147], loss=93.6040
	step [25/147], loss=99.9611
	step [26/147], loss=101.2629
	step [27/147], loss=110.6927
	step [28/147], loss=96.8974
	step [29/147], loss=106.0354
	step [30/147], loss=87.4809
	step [31/147], loss=102.9535
	step [32/147], loss=106.9263
	step [33/147], loss=99.1769
	step [34/147], loss=107.3140
	step [35/147], loss=96.9770
	step [36/147], loss=101.2956
	step [37/147], loss=105.6733
	step [38/147], loss=107.5876
	step [39/147], loss=98.0939
	step [40/147], loss=94.4931
	step [41/147], loss=110.4451
	step [42/147], loss=101.4082
	step [43/147], loss=90.3077
	step [44/147], loss=105.6437
	step [45/147], loss=99.9059
	step [46/147], loss=88.8940
	step [47/147], loss=90.6233
	step [48/147], loss=91.6670
	step [49/147], loss=96.5024
	step [50/147], loss=101.1501
	step [51/147], loss=104.2039
	step [52/147], loss=121.6452
	step [53/147], loss=92.7975
	step [54/147], loss=92.5849
	step [55/147], loss=92.5952
	step [56/147], loss=94.1852
	step [57/147], loss=91.3092
	step [58/147], loss=102.3813
	step [59/147], loss=94.8384
	step [60/147], loss=89.9645
	step [61/147], loss=95.8830
	step [62/147], loss=116.8633
	step [63/147], loss=104.7169
	step [64/147], loss=117.1328
	step [65/147], loss=89.0786
	step [66/147], loss=96.2580
	step [67/147], loss=94.8962
	step [68/147], loss=93.6442
	step [69/147], loss=86.6236
	step [70/147], loss=98.9733
	step [71/147], loss=107.4491
	step [72/147], loss=85.6394
	step [73/147], loss=124.9574
	step [74/147], loss=108.2978
	step [75/147], loss=99.4265
	step [76/147], loss=104.6391
	step [77/147], loss=103.0855
	step [78/147], loss=105.4836
	step [79/147], loss=123.2833
	step [80/147], loss=108.9206
	step [81/147], loss=97.9822
	step [82/147], loss=102.7138
	step [83/147], loss=103.2400
	step [84/147], loss=113.4334
	step [85/147], loss=90.2567
	step [86/147], loss=103.8037
	step [87/147], loss=96.5838
	step [88/147], loss=100.6974
	step [89/147], loss=106.1858
	step [90/147], loss=105.3860
	step [91/147], loss=103.5963
	step [92/147], loss=90.9130
	step [93/147], loss=127.4582
	step [94/147], loss=102.0306
	step [95/147], loss=94.2459
	step [96/147], loss=108.8858
	step [97/147], loss=103.3649
	step [98/147], loss=97.6930
	step [99/147], loss=90.8201
	step [100/147], loss=102.5690
	step [101/147], loss=84.5391
	step [102/147], loss=103.5518
	step [103/147], loss=115.6584
	step [104/147], loss=90.2992
	step [105/147], loss=101.8459
	step [106/147], loss=99.8716
	step [107/147], loss=81.3748
	step [108/147], loss=97.0614
	step [109/147], loss=105.7263
	step [110/147], loss=93.0514
	step [111/147], loss=94.9439
	step [112/147], loss=88.8106
	step [113/147], loss=97.1637
	step [114/147], loss=84.4187
	step [115/147], loss=99.6333
	step [116/147], loss=103.1888
	step [117/147], loss=110.2259
	step [118/147], loss=90.2342
	step [119/147], loss=101.9837
	step [120/147], loss=98.7223
	step [121/147], loss=93.5052
	step [122/147], loss=96.2011
	step [123/147], loss=104.2511
	step [124/147], loss=92.4537
	step [125/147], loss=99.3021
	step [126/147], loss=98.0933
	step [127/147], loss=108.9068
	step [128/147], loss=98.1796
	step [129/147], loss=88.9746
	step [130/147], loss=92.5904
	step [131/147], loss=92.0976
	step [132/147], loss=96.8907
	step [133/147], loss=103.4029
	step [134/147], loss=103.4686
	step [135/147], loss=121.3529
	step [136/147], loss=109.5845
	step [137/147], loss=94.1173
	step [138/147], loss=101.9197
	step [139/147], loss=103.9036
	step [140/147], loss=94.2268
	step [141/147], loss=103.9422
	step [142/147], loss=94.0540
	step [143/147], loss=103.9888
	step [144/147], loss=106.3069
	step [145/147], loss=94.4550
	step [146/147], loss=116.3736
	step [147/147], loss=41.0669
	Evaluating
	loss=0.0144, precision=0.4257, recall=0.8979, f1=0.5776
Training epoch 28
	step [1/147], loss=84.3348
	step [2/147], loss=108.9310
	step [3/147], loss=84.8047
	step [4/147], loss=90.3874
	step [5/147], loss=94.6253
	step [6/147], loss=106.0374
	step [7/147], loss=81.4694
	step [8/147], loss=91.0217
	step [9/147], loss=96.3364
	step [10/147], loss=114.5923
	step [11/147], loss=81.9042
	step [12/147], loss=105.6387
	step [13/147], loss=98.0407
	step [14/147], loss=104.5244
	step [15/147], loss=92.3924
	step [16/147], loss=108.8208
	step [17/147], loss=87.6148
	step [18/147], loss=100.9586
	step [19/147], loss=106.0722
	step [20/147], loss=84.4720
	step [21/147], loss=100.9924
	step [22/147], loss=92.0555
	step [23/147], loss=95.8386
	step [24/147], loss=100.7910
	step [25/147], loss=116.9024
	step [26/147], loss=100.5896
	step [27/147], loss=109.6787
	step [28/147], loss=117.6697
	step [29/147], loss=78.6103
	step [30/147], loss=109.9935
	step [31/147], loss=94.5087
	step [32/147], loss=111.0395
	step [33/147], loss=95.9162
	step [34/147], loss=93.2358
	step [35/147], loss=96.1200
	step [36/147], loss=105.5727
	step [37/147], loss=90.5358
	step [38/147], loss=94.2630
	step [39/147], loss=105.7239
	step [40/147], loss=105.6532
	step [41/147], loss=84.4239
	step [42/147], loss=96.7202
	step [43/147], loss=91.1017
	step [44/147], loss=93.4701
	step [45/147], loss=109.6394
	step [46/147], loss=117.5760
	step [47/147], loss=104.1800
	step [48/147], loss=85.0959
	step [49/147], loss=98.7655
	step [50/147], loss=106.0712
	step [51/147], loss=119.4829
	step [52/147], loss=105.4881
	step [53/147], loss=105.0599
	step [54/147], loss=114.6739
	step [55/147], loss=106.6424
	step [56/147], loss=95.1818
	step [57/147], loss=76.5784
	step [58/147], loss=117.7124
	step [59/147], loss=104.5656
	step [60/147], loss=101.5703
	step [61/147], loss=93.4484
	step [62/147], loss=107.3302
	step [63/147], loss=99.5842
	step [64/147], loss=83.9926
	step [65/147], loss=110.5959
	step [66/147], loss=92.7650
	step [67/147], loss=89.3139
	step [68/147], loss=91.0924
	step [69/147], loss=93.1745
	step [70/147], loss=114.7386
	step [71/147], loss=110.6480
	step [72/147], loss=124.2038
	step [73/147], loss=100.9092
	step [74/147], loss=96.9129
	step [75/147], loss=99.0796
	step [76/147], loss=92.1540
	step [77/147], loss=102.1447
	step [78/147], loss=89.8616
	step [79/147], loss=107.7274
	step [80/147], loss=101.8759
	step [81/147], loss=94.0367
	step [82/147], loss=102.7842
	step [83/147], loss=99.3657
	step [84/147], loss=85.1105
	step [85/147], loss=103.5831
	step [86/147], loss=89.7273
	step [87/147], loss=92.7058
	step [88/147], loss=104.7422
	step [89/147], loss=89.9183
	step [90/147], loss=102.3963
	step [91/147], loss=99.3848
	step [92/147], loss=106.0406
	step [93/147], loss=103.5920
	step [94/147], loss=84.7314
	step [95/147], loss=94.0643
	step [96/147], loss=103.4659
	step [97/147], loss=93.3215
	step [98/147], loss=89.1644
	step [99/147], loss=98.5840
	step [100/147], loss=92.1581
	step [101/147], loss=81.5232
	step [102/147], loss=105.7509
	step [103/147], loss=119.0430
	step [104/147], loss=98.6510
	step [105/147], loss=75.6032
	step [106/147], loss=114.0099
	step [107/147], loss=92.8022
	step [108/147], loss=91.7708
	step [109/147], loss=106.1791
	step [110/147], loss=101.5860
	step [111/147], loss=112.8735
	step [112/147], loss=84.3661
	step [113/147], loss=112.6494
	step [114/147], loss=100.6128
	step [115/147], loss=99.2106
	step [116/147], loss=94.4895
	step [117/147], loss=109.1673
	step [118/147], loss=107.8472
	step [119/147], loss=95.8059
	step [120/147], loss=117.8415
	step [121/147], loss=121.3528
	step [122/147], loss=109.6107
	step [123/147], loss=85.6747
	step [124/147], loss=101.8337
	step [125/147], loss=84.3110
	step [126/147], loss=95.2699
	step [127/147], loss=90.0930
	step [128/147], loss=98.5553
	step [129/147], loss=96.9722
	step [130/147], loss=98.8431
	step [131/147], loss=102.8462
	step [132/147], loss=90.3915
	step [133/147], loss=94.2569
	step [134/147], loss=103.6192
	step [135/147], loss=87.3917
	step [136/147], loss=100.4841
	step [137/147], loss=100.9037
	step [138/147], loss=114.2447
	step [139/147], loss=91.3686
	step [140/147], loss=106.7269
	step [141/147], loss=96.4926
	step [142/147], loss=109.5723
	step [143/147], loss=86.1764
	step [144/147], loss=97.8188
	step [145/147], loss=74.9282
	step [146/147], loss=93.8552
	step [147/147], loss=47.1137
	Evaluating
	loss=0.0162, precision=0.3712, recall=0.9011, f1=0.5258
Training epoch 29
	step [1/147], loss=87.2143
	step [2/147], loss=103.0257
	step [3/147], loss=92.8491
	step [4/147], loss=101.1204
	step [5/147], loss=89.1003
	step [6/147], loss=118.2376
	step [7/147], loss=94.3957
	step [8/147], loss=100.9353
	step [9/147], loss=92.2490
	step [10/147], loss=108.8410
	step [11/147], loss=92.2982
	step [12/147], loss=100.7692
	step [13/147], loss=93.5701
	step [14/147], loss=117.1317
	step [15/147], loss=98.3780
	step [16/147], loss=103.9636
	step [17/147], loss=95.1796
	step [18/147], loss=94.4316
	step [19/147], loss=92.1312
	step [20/147], loss=84.8138
	step [21/147], loss=88.1574
	step [22/147], loss=90.9932
	step [23/147], loss=102.6039
	step [24/147], loss=89.5536
	step [25/147], loss=91.4723
	step [26/147], loss=80.0539
	step [27/147], loss=107.3693
	step [28/147], loss=88.0535
	step [29/147], loss=124.2510
	step [30/147], loss=102.5123
	step [31/147], loss=94.4134
	step [32/147], loss=93.5459
	step [33/147], loss=100.9090
	step [34/147], loss=92.2047
	step [35/147], loss=100.9850
	step [36/147], loss=93.7473
	step [37/147], loss=90.4512
	step [38/147], loss=100.6198
	step [39/147], loss=105.8576
	step [40/147], loss=95.9497
	step [41/147], loss=100.6430
	step [42/147], loss=89.3035
	step [43/147], loss=103.9600
	step [44/147], loss=91.6097
	step [45/147], loss=99.1502
	step [46/147], loss=90.8520
	step [47/147], loss=100.6162
	step [48/147], loss=96.2573
	step [49/147], loss=99.8614
	step [50/147], loss=101.8414
	step [51/147], loss=92.1826
	step [52/147], loss=106.4772
	step [53/147], loss=93.5644
	step [54/147], loss=90.2157
	step [55/147], loss=94.3587
	step [56/147], loss=98.3071
	step [57/147], loss=105.1291
	step [58/147], loss=91.8889
	step [59/147], loss=106.1115
	step [60/147], loss=102.5214
	step [61/147], loss=96.5514
	step [62/147], loss=116.9609
	step [63/147], loss=100.8567
	step [64/147], loss=101.0231
	step [65/147], loss=86.3794
	step [66/147], loss=83.0784
	step [67/147], loss=101.2179
	step [68/147], loss=89.3078
	step [69/147], loss=93.6683
	step [70/147], loss=119.1863
	step [71/147], loss=89.1218
	step [72/147], loss=94.2711
	step [73/147], loss=101.4468
	step [74/147], loss=102.3746
	step [75/147], loss=95.4849
	step [76/147], loss=83.1903
	step [77/147], loss=101.1077
	step [78/147], loss=94.8906
	step [79/147], loss=99.6891
	step [80/147], loss=94.0473
	step [81/147], loss=88.4773
	step [82/147], loss=100.6681
	step [83/147], loss=93.1450
	step [84/147], loss=94.9405
	step [85/147], loss=95.3943
	step [86/147], loss=92.2659
	step [87/147], loss=97.2902
	step [88/147], loss=80.6390
	step [89/147], loss=103.3023
	step [90/147], loss=95.4982
	step [91/147], loss=100.7155
	step [92/147], loss=96.8598
	step [93/147], loss=99.3311
	step [94/147], loss=112.0099
	step [95/147], loss=94.0621
	step [96/147], loss=107.8611
	step [97/147], loss=100.7365
	step [98/147], loss=105.0458
	step [99/147], loss=101.7968
	step [100/147], loss=96.8596
	step [101/147], loss=107.8862
	step [102/147], loss=85.7548
	step [103/147], loss=86.8164
	step [104/147], loss=97.9171
	step [105/147], loss=91.0175
	step [106/147], loss=111.7517
	step [107/147], loss=76.4882
	step [108/147], loss=104.1593
	step [109/147], loss=90.2527
	step [110/147], loss=104.8043
	step [111/147], loss=107.1880
	step [112/147], loss=112.7646
	step [113/147], loss=90.9728
	step [114/147], loss=100.2644
	step [115/147], loss=77.4299
	step [116/147], loss=97.2504
	step [117/147], loss=95.3218
	step [118/147], loss=100.1812
	step [119/147], loss=115.0878
	step [120/147], loss=104.5101
	step [121/147], loss=86.8182
	step [122/147], loss=88.1950
	step [123/147], loss=101.7349
	step [124/147], loss=91.9714
	step [125/147], loss=101.2221
	step [126/147], loss=100.6140
	step [127/147], loss=95.0896
	step [128/147], loss=88.6711
	step [129/147], loss=120.7698
	step [130/147], loss=93.7710
	step [131/147], loss=93.6796
	step [132/147], loss=89.1273
	step [133/147], loss=94.1584
	step [134/147], loss=124.5547
	step [135/147], loss=86.8942
	step [136/147], loss=102.6171
	step [137/147], loss=92.9944
	step [138/147], loss=89.7505
	step [139/147], loss=106.4323
	step [140/147], loss=115.7029
	step [141/147], loss=93.4173
	step [142/147], loss=86.3118
	step [143/147], loss=99.8031
	step [144/147], loss=95.1603
	step [145/147], loss=96.0141
	step [146/147], loss=109.1743
	step [147/147], loss=49.3043
	Evaluating
	loss=0.0147, precision=0.3829, recall=0.9106, f1=0.5391
Training epoch 30
	step [1/147], loss=91.0320
	step [2/147], loss=84.1258
	step [3/147], loss=88.4864
	step [4/147], loss=107.9068
	step [5/147], loss=119.8535
	step [6/147], loss=95.0029
	step [7/147], loss=86.1967
	step [8/147], loss=104.6078
	step [9/147], loss=93.0983
	step [10/147], loss=102.3251
	step [11/147], loss=101.0517
	step [12/147], loss=105.0669
	step [13/147], loss=86.7811
	step [14/147], loss=103.8755
	step [15/147], loss=87.7368
	step [16/147], loss=104.7275
	step [17/147], loss=119.6850
	step [18/147], loss=107.9696
	step [19/147], loss=84.8461
	step [20/147], loss=94.8670
	step [21/147], loss=85.8293
	step [22/147], loss=91.2684
	step [23/147], loss=99.7594
	step [24/147], loss=106.9096
	step [25/147], loss=108.5089
	step [26/147], loss=80.2612
	step [27/147], loss=100.4970
	step [28/147], loss=88.7578
	step [29/147], loss=96.9044
	step [30/147], loss=98.9582
	step [31/147], loss=106.5291
	step [32/147], loss=103.8575
	step [33/147], loss=99.1484
	step [34/147], loss=100.1272
	step [35/147], loss=95.9586
	step [36/147], loss=93.6598
	step [37/147], loss=100.8588
	step [38/147], loss=86.9246
	step [39/147], loss=83.4305
	step [40/147], loss=102.4008
	step [41/147], loss=89.2876
	step [42/147], loss=91.2761
	step [43/147], loss=91.8018
	step [44/147], loss=81.8170
	step [45/147], loss=91.4294
	step [46/147], loss=87.8912
	step [47/147], loss=96.5654
	step [48/147], loss=104.4951
	step [49/147], loss=84.1253
	step [50/147], loss=85.8135
	step [51/147], loss=90.7873
	step [52/147], loss=97.0119
	step [53/147], loss=79.4811
	step [54/147], loss=94.3592
	step [55/147], loss=111.5620
	step [56/147], loss=104.2348
	step [57/147], loss=86.5870
	step [58/147], loss=94.1907
	step [59/147], loss=96.6176
	step [60/147], loss=103.7732
	step [61/147], loss=99.3631
	step [62/147], loss=92.7288
	step [63/147], loss=109.2344
	step [64/147], loss=92.7828
	step [65/147], loss=88.5771
	step [66/147], loss=85.9395
	step [67/147], loss=98.1358
	step [68/147], loss=93.3253
	step [69/147], loss=88.9020
	step [70/147], loss=88.0874
	step [71/147], loss=85.6397
	step [72/147], loss=88.7141
	step [73/147], loss=99.4218
	step [74/147], loss=99.6342
	step [75/147], loss=95.6838
	step [76/147], loss=80.5796
	step [77/147], loss=102.7193
	step [78/147], loss=92.6012
	step [79/147], loss=108.1104
	step [80/147], loss=116.3933
	step [81/147], loss=78.4262
	step [82/147], loss=100.2989
	step [83/147], loss=92.1287
	step [84/147], loss=120.9582
	step [85/147], loss=102.3673
	step [86/147], loss=85.4992
	step [87/147], loss=99.8114
	step [88/147], loss=112.8999
	step [89/147], loss=107.3238
	step [90/147], loss=107.6599
	step [91/147], loss=100.9311
	step [92/147], loss=92.0782
	step [93/147], loss=98.7081
	step [94/147], loss=97.3988
	step [95/147], loss=103.8112
	step [96/147], loss=95.2029
	step [97/147], loss=84.0952
	step [98/147], loss=99.1378
	step [99/147], loss=89.2756
	step [100/147], loss=88.4998
	step [101/147], loss=97.8562
	step [102/147], loss=99.2925
	step [103/147], loss=117.3136
	step [104/147], loss=84.5742
	step [105/147], loss=95.8418
	step [106/147], loss=88.7887
	step [107/147], loss=100.1374
	step [108/147], loss=120.6763
	step [109/147], loss=86.2029
	step [110/147], loss=91.2683
	step [111/147], loss=89.9045
	step [112/147], loss=89.2900
	step [113/147], loss=91.2859
	step [114/147], loss=94.3636
	step [115/147], loss=97.1361
	step [116/147], loss=89.4247
	step [117/147], loss=101.6232
	step [118/147], loss=93.5684
	step [119/147], loss=97.3684
	step [120/147], loss=100.4584
	step [121/147], loss=113.9121
	step [122/147], loss=98.6895
	step [123/147], loss=88.8204
	step [124/147], loss=108.7445
	step [125/147], loss=107.5007
	step [126/147], loss=98.7236
	step [127/147], loss=78.0721
	step [128/147], loss=98.5366
	step [129/147], loss=85.4087
	step [130/147], loss=95.2437
	step [131/147], loss=95.5927
	step [132/147], loss=106.5373
	step [133/147], loss=83.6281
	step [134/147], loss=98.2166
	step [135/147], loss=94.5658
	step [136/147], loss=84.3572
	step [137/147], loss=91.5580
	step [138/147], loss=104.9877
	step [139/147], loss=92.0347
	step [140/147], loss=83.7171
	step [141/147], loss=91.7693
	step [142/147], loss=110.8280
	step [143/147], loss=89.0019
	step [144/147], loss=98.0410
	step [145/147], loss=100.6894
	step [146/147], loss=89.1242
	step [147/147], loss=50.0651
	Evaluating
	loss=0.0121, precision=0.4481, recall=0.8906, f1=0.5962
Training finished
best_f1: 0.6535766559736444
directing: Y rim_enhanced: True test_id 0
removed wrong scan: weights_Y_335_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_383_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_262_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_311_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_260_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_225_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_320_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_192_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_319_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_226_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_346_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_271_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_277_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_290_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_269_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_379_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_375_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_378_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_366_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_309_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_200_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_327_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_193_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_174_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_253_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_220_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_265_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_360_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_303_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_232_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_243_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_240_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_237_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_256_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_254_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_254_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_231_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_359_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_172_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_340_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_263_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_388_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_307_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_278_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_196_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_272_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_314_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_350_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_361_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_194_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_323_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_201_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_299_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_259_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_276_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_167_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_185_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_215_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_268_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_264_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_384_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_261_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_280_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_287_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_385_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_216_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_349_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_348_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_371_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_357_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_234_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_288_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_324_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_229_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_184_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_148_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_168_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_370_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_286_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_302_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_265_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_153_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_274_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_255_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_217_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_355_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_294_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_159_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_338_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_236_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_152_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_292_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_222_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_376_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_183_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_258_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_282_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_278_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_284_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_300_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_163_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_197_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_203_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_243_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_224_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_238_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_374_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_218_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_259_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_282_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_387_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_353_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_386_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_296_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_341_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_206_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_220_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_239_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_231_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_202_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_358_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_246_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_315_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_275_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_169_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_316_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_221_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_188_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_208_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_273_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_251_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_224_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_333_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_263_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_173_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_243_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_241_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_326_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_230_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_236_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_345_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_206_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_258_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_187_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_87_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_154_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_253_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_337_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_232_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_147_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_377_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_362_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_233_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_250_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_297_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_354_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_262_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_225_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_365_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_356_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_242_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_245_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_230_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_318_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_181_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_305_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_255_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_298_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_301_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_293_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_229_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_332_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_257_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_295_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_239_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_260_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_228_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_317_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_364_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_262_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_162_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_310_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_160_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_237_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_254_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_257_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_291_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_279_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_215_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_372_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_218_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_210_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_158_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_156_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_339_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_190_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_373_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_205_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_329_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_380_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_271_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_235_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_244_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_312_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_186_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_308_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_347_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_241_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_270_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_261_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_222_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_189_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_289_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_146_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_252_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_344_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_331_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_313_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_328_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_265_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_283_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_248_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_381_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_211_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_214_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_176_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_221_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_150_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_343_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_249_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_322_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_351_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_234_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_195_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_330_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_334_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_306_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_213_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_325_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_342_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_247_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_352_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_225_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_226_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_256_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_242_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_368_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_382_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_157_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_238_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_267_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_367_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_321_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_233_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_281_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_161_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_336_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_170_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_304_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_44_xwqg-B00034_2020-04-03.npy
# all image files: 20333 # all weight files in weight_dir: 15948 # image files with weight 15917
removed wrong scan: weights_Y_335_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_383_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_262_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_311_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_260_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_225_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_320_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_192_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_319_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_226_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_346_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_271_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_277_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_290_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_269_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_379_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_375_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_378_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_366_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_309_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_200_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_327_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_193_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_174_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_253_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_220_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_265_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_360_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_303_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_232_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_243_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_240_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_237_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_256_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_254_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_254_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_231_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_359_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_172_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_340_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_263_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_388_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_307_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_278_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_196_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_272_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_314_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_350_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_361_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_194_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_323_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_201_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_299_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_259_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_276_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_167_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_185_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_215_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_268_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_264_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_384_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_261_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_280_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_287_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_385_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_216_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_349_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_348_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_371_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_357_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_234_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_288_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_324_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_229_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_184_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_148_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_168_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_370_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_286_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_302_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_265_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_153_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_274_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_255_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_217_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_355_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_294_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_159_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_338_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_236_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_152_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_292_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_222_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_376_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_183_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_258_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_282_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_278_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_284_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_300_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_163_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_197_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_203_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_243_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_224_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_238_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_374_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_218_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_259_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_282_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_387_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_353_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_386_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_296_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_341_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_206_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_220_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_239_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_231_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_202_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_358_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_246_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_315_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_275_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_169_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_316_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_221_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_188_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_208_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_273_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_251_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_224_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_333_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_263_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_173_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_243_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_241_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_326_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_230_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_236_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_345_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_206_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_258_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_187_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_87_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_154_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_253_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_337_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_232_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_147_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_377_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_362_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_233_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_250_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_297_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_354_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_262_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_225_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_365_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_356_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_242_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_245_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_230_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_318_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_181_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_305_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_255_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_298_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_301_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_293_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_229_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_332_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_257_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_295_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_239_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_260_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_228_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_317_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_364_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_262_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_162_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_310_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_160_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_237_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_254_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_257_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_291_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_279_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_215_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_372_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_218_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_210_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_158_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_156_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_339_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_190_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_373_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_205_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_329_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_380_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_271_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_235_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_244_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_312_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_186_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_308_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_347_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_241_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_270_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_261_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_222_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_189_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_289_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_146_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_252_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_344_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_331_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_313_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_328_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_265_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_283_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_248_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_381_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_211_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_214_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_176_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_221_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_150_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_343_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_249_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_322_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_351_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_234_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_195_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_330_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_334_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_306_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_213_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_325_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_342_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_247_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_352_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_225_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_226_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_256_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_242_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_368_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_382_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_157_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_238_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_267_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_367_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_321_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_233_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_281_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_161_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_336_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_170_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_304_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_44_xwqg-B00034_2020-04-03.npy
# all image files: 20333 # all weight files in weight_dir: 4124 # image files with weight 4113
train: /ibex/scratch/projects/c2052/air_tube_seg/training_samples_enhanced_one/Y 15917
Using 4 GPUs
Going to train epochs [1-30]
Training epoch 1
	step [1/249], loss=1012.8359
	step [2/249], loss=631.4645
	step [3/249], loss=426.0962
	step [4/249], loss=354.0238
	step [5/249], loss=330.7578
	step [6/249], loss=365.3126
	step [7/249], loss=318.4941
	step [8/249], loss=343.8186
	step [9/249], loss=369.6432
	step [10/249], loss=311.1158
	step [11/249], loss=321.5833
	step [12/249], loss=292.8914
	step [13/249], loss=254.9250
	step [14/249], loss=287.5102
	step [15/249], loss=289.9642
	step [16/249], loss=287.2602
	step [17/249], loss=272.4612
	step [18/249], loss=284.1377
	step [19/249], loss=260.3440
	step [20/249], loss=255.8448
	step [21/249], loss=268.9884
	step [22/249], loss=271.9995
	step [23/249], loss=248.3544
	step [24/249], loss=260.9802
	step [25/249], loss=282.3326
	step [26/249], loss=245.2296
	step [27/249], loss=236.1900
	step [28/249], loss=239.0053
	step [29/249], loss=262.7108
	step [30/249], loss=237.2207
	step [31/249], loss=261.8982
	step [32/249], loss=238.7637
	step [33/249], loss=218.7411
	step [34/249], loss=230.6728
	step [35/249], loss=236.5724
	step [36/249], loss=243.7509
	step [37/249], loss=226.0577
	step [38/249], loss=235.3093
	step [39/249], loss=215.0631
	step [40/249], loss=217.5647
	step [41/249], loss=220.8076
	step [42/249], loss=218.0094
	step [43/249], loss=223.3590
	step [44/249], loss=225.7889
	step [45/249], loss=207.6387
	step [46/249], loss=232.5653
	step [47/249], loss=237.7246
	step [48/249], loss=223.0980
	step [49/249], loss=192.3292
	step [50/249], loss=203.7024
	step [51/249], loss=190.7992
	step [52/249], loss=230.0466
	step [53/249], loss=235.2684
	step [54/249], loss=217.4483
	step [55/249], loss=233.7808
	step [56/249], loss=226.9034
	step [57/249], loss=208.0538
	step [58/249], loss=203.5498
	step [59/249], loss=220.1239
	step [60/249], loss=214.3440
	step [61/249], loss=195.3221
	step [62/249], loss=205.5379
	step [63/249], loss=218.5260
	step [64/249], loss=225.5565
	step [65/249], loss=206.7509
	step [66/249], loss=209.8807
	step [67/249], loss=222.2695
	step [68/249], loss=216.6024
	step [69/249], loss=209.7363
	step [70/249], loss=213.3798
	step [71/249], loss=211.0800
	step [72/249], loss=198.6702
	step [73/249], loss=184.5724
	step [74/249], loss=203.4296
	step [75/249], loss=195.9824
	step [76/249], loss=223.0903
	step [77/249], loss=176.7756
	step [78/249], loss=204.3690
	step [79/249], loss=207.9993
	step [80/249], loss=243.4747
	step [81/249], loss=222.9367
	step [82/249], loss=206.0472
	step [83/249], loss=199.6185
	step [84/249], loss=190.4353
	step [85/249], loss=214.9567
	step [86/249], loss=216.0898
	step [87/249], loss=176.2194
	step [88/249], loss=204.0797
	step [89/249], loss=209.8760
	step [90/249], loss=205.6079
	step [91/249], loss=191.6235
	step [92/249], loss=199.4200
	step [93/249], loss=214.5996
	step [94/249], loss=217.5475
	step [95/249], loss=212.8670
	step [96/249], loss=206.4670
	step [97/249], loss=199.0826
	step [98/249], loss=191.8349
	step [99/249], loss=202.9483
	step [100/249], loss=191.3977
	step [101/249], loss=218.4255
	step [102/249], loss=224.6882
	step [103/249], loss=189.8409
	step [104/249], loss=205.6295
	step [105/249], loss=200.3458
	step [106/249], loss=190.2219
	step [107/249], loss=175.3672
	step [108/249], loss=209.4948
	step [109/249], loss=203.2862
	step [110/249], loss=226.9023
	step [111/249], loss=188.5915
	step [112/249], loss=181.1744
	step [113/249], loss=195.1701
	step [114/249], loss=186.3199
	step [115/249], loss=170.6622
	step [116/249], loss=211.9750
	step [117/249], loss=187.1138
	step [118/249], loss=198.3761
	step [119/249], loss=188.6211
	step [120/249], loss=204.6715
	step [121/249], loss=209.7871
	step [122/249], loss=189.8484
	step [123/249], loss=199.4997
	step [124/249], loss=200.8101
	step [125/249], loss=194.9465
	step [126/249], loss=196.3019
	step [127/249], loss=188.1580
	step [128/249], loss=199.0394
	step [129/249], loss=192.2701
	step [130/249], loss=183.9385
	step [131/249], loss=170.3865
	step [132/249], loss=180.2745
	step [133/249], loss=185.1057
	step [134/249], loss=182.7032
	step [135/249], loss=194.6972
	step [136/249], loss=191.6298
	step [137/249], loss=193.9512
	step [138/249], loss=204.8544
	step [139/249], loss=187.5893
	step [140/249], loss=202.2074
	step [141/249], loss=186.7752
	step [142/249], loss=202.9458
	step [143/249], loss=173.1556
	step [144/249], loss=178.9034
	step [145/249], loss=193.8443
	step [146/249], loss=205.6060
	step [147/249], loss=200.7626
	step [148/249], loss=188.0339
	step [149/249], loss=179.4229
	step [150/249], loss=187.1687
	step [151/249], loss=199.0492
	step [152/249], loss=195.4628
	step [153/249], loss=167.2155
	step [154/249], loss=208.9956
	step [155/249], loss=204.2825
	step [156/249], loss=172.6125
	step [157/249], loss=180.0683
	step [158/249], loss=174.1455
	step [159/249], loss=177.2829
	step [160/249], loss=190.1530
	step [161/249], loss=197.9592
	step [162/249], loss=202.1782
	step [163/249], loss=161.4000
	step [164/249], loss=183.5613
	step [165/249], loss=196.8257
	step [166/249], loss=176.7679
	step [167/249], loss=190.6712
	step [168/249], loss=156.4046
	step [169/249], loss=215.9525
	step [170/249], loss=169.3204
	step [171/249], loss=191.8708
	step [172/249], loss=170.9801
	step [173/249], loss=188.1440
	step [174/249], loss=193.1213
	step [175/249], loss=159.9381
	step [176/249], loss=181.3101
	step [177/249], loss=169.8949
	step [178/249], loss=199.7144
	step [179/249], loss=189.0539
	step [180/249], loss=165.4266
	step [181/249], loss=179.5100
	step [182/249], loss=172.2054
	step [183/249], loss=184.2873
	step [184/249], loss=183.5475
	step [185/249], loss=175.7807
	step [186/249], loss=193.0800
	step [187/249], loss=165.9447
	step [188/249], loss=200.0154
	step [189/249], loss=201.4373
	step [190/249], loss=166.7645
	step [191/249], loss=185.1598
	step [192/249], loss=203.6610
	step [193/249], loss=183.2257
	step [194/249], loss=172.7058
	step [195/249], loss=174.7802
	step [196/249], loss=186.1620
	step [197/249], loss=181.9908
	step [198/249], loss=160.7134
	step [199/249], loss=201.6163
	step [200/249], loss=168.2538
	step [201/249], loss=180.2749
	step [202/249], loss=182.2813
	step [203/249], loss=186.1360
	step [204/249], loss=167.8485
	step [205/249], loss=154.2144
	step [206/249], loss=185.1781
	step [207/249], loss=183.0418
	step [208/249], loss=171.1325
	step [209/249], loss=188.1721
	step [210/249], loss=212.7101
	step [211/249], loss=173.2498
	step [212/249], loss=208.5656
	step [213/249], loss=175.6976
	step [214/249], loss=169.4443
	step [215/249], loss=180.5795
	step [216/249], loss=158.1976
	step [217/249], loss=169.6872
	step [218/249], loss=170.1028
	step [219/249], loss=184.1430
	step [220/249], loss=170.2103
	step [221/249], loss=170.9173
	step [222/249], loss=170.9960
	step [223/249], loss=161.6574
	step [224/249], loss=177.6345
	step [225/249], loss=182.6524
	step [226/249], loss=182.2354
	step [227/249], loss=158.4284
	step [228/249], loss=177.2991
	step [229/249], loss=181.4984
	step [230/249], loss=178.2843
	step [231/249], loss=193.5743
	step [232/249], loss=182.7823
	step [233/249], loss=170.5363
	step [234/249], loss=179.3571
	step [235/249], loss=192.5793
	step [236/249], loss=178.1787
	step [237/249], loss=179.3831
	step [238/249], loss=154.2690
	step [239/249], loss=173.5102
	step [240/249], loss=191.4432
	step [241/249], loss=181.4015
	step [242/249], loss=190.7135
	step [243/249], loss=169.0322
	step [244/249], loss=175.1494
	step [245/249], loss=176.9879
	step [246/249], loss=161.3568
	step [247/249], loss=172.5010
	step [248/249], loss=195.0599
	step [249/249], loss=113.9437
	Evaluating
	loss=0.2839, precision=0.3341, recall=0.9315, f1=0.4918
saving model as: 0_saved_model.pth
Training epoch 2
	step [1/249], loss=150.2608
	step [2/249], loss=160.1843
	step [3/249], loss=170.1533
	step [4/249], loss=160.7238
	step [5/249], loss=195.1216
	step [6/249], loss=184.2128
	step [7/249], loss=158.4037
	step [8/249], loss=178.1566
	step [9/249], loss=159.4609
	step [10/249], loss=177.5995
	step [11/249], loss=175.3017
	step [12/249], loss=183.3651
	step [13/249], loss=181.4944
	step [14/249], loss=179.3586
	step [15/249], loss=182.7981
	step [16/249], loss=182.9778
	step [17/249], loss=172.4693
	step [18/249], loss=165.6976
	step [19/249], loss=181.4789
	step [20/249], loss=175.5931
	step [21/249], loss=161.9475
	step [22/249], loss=136.5988
	step [23/249], loss=159.8228
	step [24/249], loss=170.8579
	step [25/249], loss=174.4785
	step [26/249], loss=153.0290
	step [27/249], loss=162.0172
	step [28/249], loss=166.3789
	step [29/249], loss=189.7244
	step [30/249], loss=181.6566
	step [31/249], loss=177.5364
	step [32/249], loss=177.2576
	step [33/249], loss=164.5769
	step [34/249], loss=148.7775
	step [35/249], loss=167.7999
	step [36/249], loss=176.1627
	step [37/249], loss=176.9138
	step [38/249], loss=156.4588
	step [39/249], loss=179.8458
	step [40/249], loss=178.2988
	step [41/249], loss=178.9979
	step [42/249], loss=176.0557
	step [43/249], loss=179.3074
	step [44/249], loss=169.1373
	step [45/249], loss=158.5873
	step [46/249], loss=156.3795
	step [47/249], loss=172.1490
	step [48/249], loss=174.3817
	step [49/249], loss=159.2797
	step [50/249], loss=171.9763
	step [51/249], loss=176.5849
	step [52/249], loss=161.0274
	step [53/249], loss=170.8330
	step [54/249], loss=178.9316
	step [55/249], loss=189.3822
	step [56/249], loss=158.3850
	step [57/249], loss=159.7329
	step [58/249], loss=161.9141
	step [59/249], loss=176.2756
	step [60/249], loss=177.3070
	step [61/249], loss=150.7271
	step [62/249], loss=151.3127
	step [63/249], loss=167.1712
	step [64/249], loss=173.0445
	step [65/249], loss=166.1245
	step [66/249], loss=148.5397
	step [67/249], loss=171.2015
	step [68/249], loss=163.8868
	step [69/249], loss=154.4950
	step [70/249], loss=170.3367
	step [71/249], loss=145.7538
	step [72/249], loss=183.9682
	step [73/249], loss=138.3706
	step [74/249], loss=184.0239
	step [75/249], loss=143.4247
	step [76/249], loss=154.1314
	step [77/249], loss=157.5850
	step [78/249], loss=154.5281
	step [79/249], loss=183.1343
	step [80/249], loss=182.9636
	step [81/249], loss=173.6200
	step [82/249], loss=180.2430
	step [83/249], loss=142.7396
	step [84/249], loss=145.0906
	step [85/249], loss=166.8183
	step [86/249], loss=128.5306
	step [87/249], loss=177.6595
	step [88/249], loss=161.6192
	step [89/249], loss=161.7863
	step [90/249], loss=150.9423
	step [91/249], loss=160.8564
	step [92/249], loss=155.3027
	step [93/249], loss=159.8984
	step [94/249], loss=146.1390
	step [95/249], loss=157.9653
	step [96/249], loss=173.2736
	step [97/249], loss=142.8905
	step [98/249], loss=145.9717
	step [99/249], loss=165.0489
	step [100/249], loss=169.7286
	step [101/249], loss=160.2786
	step [102/249], loss=145.8663
	step [103/249], loss=169.3796
	step [104/249], loss=147.4961
	step [105/249], loss=139.8539
	step [106/249], loss=150.7092
	step [107/249], loss=150.9891
	step [108/249], loss=183.0591
	step [109/249], loss=175.7428
	step [110/249], loss=155.8378
	step [111/249], loss=172.7691
	step [112/249], loss=168.1813
	step [113/249], loss=157.2803
	step [114/249], loss=159.9298
	step [115/249], loss=152.8293
	step [116/249], loss=150.1332
	step [117/249], loss=149.3796
	step [118/249], loss=160.1263
	step [119/249], loss=158.7653
	step [120/249], loss=151.6683
	step [121/249], loss=153.4037
	step [122/249], loss=161.0101
	step [123/249], loss=164.5962
	step [124/249], loss=139.6281
	step [125/249], loss=186.3610
	step [126/249], loss=159.9210
	step [127/249], loss=187.5973
	step [128/249], loss=182.1809
	step [129/249], loss=173.0892
	step [130/249], loss=187.7581
	step [131/249], loss=178.2513
	step [132/249], loss=167.1175
	step [133/249], loss=154.4501
	step [134/249], loss=146.4841
	step [135/249], loss=154.1286
	step [136/249], loss=174.2795
	step [137/249], loss=168.6991
	step [138/249], loss=174.4047
	step [139/249], loss=154.4966
	step [140/249], loss=161.8592
	step [141/249], loss=169.9588
	step [142/249], loss=181.6192
	step [143/249], loss=165.2735
	step [144/249], loss=181.1942
	step [145/249], loss=175.6628
	step [146/249], loss=177.4341
	step [147/249], loss=161.8224
	step [148/249], loss=158.8925
	step [149/249], loss=162.5403
	step [150/249], loss=141.4318
	step [151/249], loss=164.1317
	step [152/249], loss=168.6384
	step [153/249], loss=162.2621
	step [154/249], loss=170.5999
	step [155/249], loss=158.6892
	step [156/249], loss=156.7578
	step [157/249], loss=155.8360
	step [158/249], loss=162.9783
	step [159/249], loss=154.6656
	step [160/249], loss=183.1208
	step [161/249], loss=191.2482
	step [162/249], loss=147.1081
	step [163/249], loss=162.4852
	step [164/249], loss=153.1794
	step [165/249], loss=183.9049
	step [166/249], loss=183.1844
	step [167/249], loss=155.7686
	step [168/249], loss=184.5250
	step [169/249], loss=158.9982
	step [170/249], loss=169.8291
	step [171/249], loss=148.2330
	step [172/249], loss=146.6650
	step [173/249], loss=156.6359
	step [174/249], loss=162.0974
	step [175/249], loss=161.2341
	step [176/249], loss=138.4132
	step [177/249], loss=189.0662
	step [178/249], loss=148.0047
	step [179/249], loss=139.1271
	step [180/249], loss=143.9327
	step [181/249], loss=137.7325
	step [182/249], loss=153.7714
	step [183/249], loss=142.4209
	step [184/249], loss=153.2104
	step [185/249], loss=159.2529
	step [186/249], loss=155.2138
	step [187/249], loss=151.4086
	step [188/249], loss=143.5331
	step [189/249], loss=155.4669
	step [190/249], loss=174.1624
	step [191/249], loss=162.3726
	step [192/249], loss=158.4811
	step [193/249], loss=135.8460
	step [194/249], loss=156.1419
	step [195/249], loss=129.8351
	step [196/249], loss=159.4046
	step [197/249], loss=164.8293
	step [198/249], loss=132.7365
	step [199/249], loss=147.7702
	step [200/249], loss=169.6309
	step [201/249], loss=155.9509
	step [202/249], loss=147.8824
	step [203/249], loss=162.2278
	step [204/249], loss=170.0650
	step [205/249], loss=167.1696
	step [206/249], loss=153.9642
	step [207/249], loss=139.2647
	step [208/249], loss=145.1223
	step [209/249], loss=159.0649
	step [210/249], loss=173.0263
	step [211/249], loss=142.7245
	step [212/249], loss=166.6178
	step [213/249], loss=112.1532
	step [214/249], loss=158.9807
	step [215/249], loss=154.4221
	step [216/249], loss=162.1588
	step [217/249], loss=162.9092
	step [218/249], loss=161.0694
	step [219/249], loss=157.7398
	step [220/249], loss=142.2599
	step [221/249], loss=173.1201
	step [222/249], loss=183.5728
	step [223/249], loss=154.2105
	step [224/249], loss=150.7923
	step [225/249], loss=143.5992
	step [226/249], loss=174.8640
	step [227/249], loss=156.0777
	step [228/249], loss=144.9422
	step [229/249], loss=139.9051
	step [230/249], loss=144.5779
	step [231/249], loss=157.2117
	step [232/249], loss=151.1403
	step [233/249], loss=155.5059
	step [234/249], loss=153.6720
	step [235/249], loss=142.3960
	step [236/249], loss=141.8847
	step [237/249], loss=158.2226
	step [238/249], loss=178.3444
	step [239/249], loss=151.2291
	step [240/249], loss=139.1318
	step [241/249], loss=179.1915
	step [242/249], loss=149.9852
	step [243/249], loss=151.0980
	step [244/249], loss=141.4928
	step [245/249], loss=142.9428
	step [246/249], loss=132.5184
	step [247/249], loss=168.0973
	step [248/249], loss=141.6864
	step [249/249], loss=111.1336
	Evaluating
	loss=0.1891, precision=0.4503, recall=0.9135, f1=0.6032
saving model as: 0_saved_model.pth
Training epoch 3
	step [1/249], loss=156.5214
	step [2/249], loss=158.7923
	step [3/249], loss=157.3792
	step [4/249], loss=158.7136
	step [5/249], loss=132.1552
	step [6/249], loss=139.0858
	step [7/249], loss=137.3870
	step [8/249], loss=155.2389
	step [9/249], loss=138.0446
	step [10/249], loss=164.3716
	step [11/249], loss=146.6519
	step [12/249], loss=160.8751
	step [13/249], loss=152.6983
	step [14/249], loss=158.4313
	step [15/249], loss=159.4566
	step [16/249], loss=157.9631
	step [17/249], loss=167.9534
	step [18/249], loss=139.3647
	step [19/249], loss=145.7900
	step [20/249], loss=132.0082
	step [21/249], loss=149.6600
	step [22/249], loss=169.3725
	step [23/249], loss=156.8089
	step [24/249], loss=172.2655
	step [25/249], loss=148.5057
	step [26/249], loss=151.1837
	step [27/249], loss=137.1415
	step [28/249], loss=165.0851
	step [29/249], loss=159.5364
	step [30/249], loss=156.3246
	step [31/249], loss=143.2858
	step [32/249], loss=155.3200
	step [33/249], loss=163.6492
	step [34/249], loss=167.7740
	step [35/249], loss=152.7376
	step [36/249], loss=142.8625
	step [37/249], loss=182.2787
	step [38/249], loss=171.3863
	step [39/249], loss=125.2845
	step [40/249], loss=159.2494
	step [41/249], loss=151.3147
	step [42/249], loss=153.9673
	step [43/249], loss=148.9562
	step [44/249], loss=132.6332
	step [45/249], loss=150.3138
	step [46/249], loss=175.7078
	step [47/249], loss=156.3688
	step [48/249], loss=164.9729
	step [49/249], loss=154.3791
	step [50/249], loss=148.0701
	step [51/249], loss=134.1144
	step [52/249], loss=162.8199
	step [53/249], loss=151.5391
	step [54/249], loss=150.1866
	step [55/249], loss=145.5527
	step [56/249], loss=137.3721
	step [57/249], loss=140.6708
	step [58/249], loss=149.4781
	step [59/249], loss=172.2001
	step [60/249], loss=143.9326
	step [61/249], loss=142.8091
	step [62/249], loss=153.7538
	step [63/249], loss=137.1382
	step [64/249], loss=147.9025
	step [65/249], loss=142.5242
	step [66/249], loss=150.2811
	step [67/249], loss=138.7757
	step [68/249], loss=135.6133
	step [69/249], loss=166.4376
	step [70/249], loss=145.4872
	step [71/249], loss=143.0228
	step [72/249], loss=156.2213
	step [73/249], loss=151.4539
	step [74/249], loss=137.8621
	step [75/249], loss=134.0674
	step [76/249], loss=124.4927
	step [77/249], loss=161.1780
	step [78/249], loss=138.5086
	step [79/249], loss=150.1389
	step [80/249], loss=145.1433
	step [81/249], loss=158.6022
	step [82/249], loss=131.5122
	step [83/249], loss=154.9690
	step [84/249], loss=125.4966
	step [85/249], loss=131.0464
	step [86/249], loss=147.4279
	step [87/249], loss=153.9284
	step [88/249], loss=148.4673
	step [89/249], loss=146.3862
	step [90/249], loss=142.4569
	step [91/249], loss=157.0383
	step [92/249], loss=133.2035
	step [93/249], loss=149.5918
	step [94/249], loss=145.9844
	step [95/249], loss=150.7512
	step [96/249], loss=156.9243
	step [97/249], loss=135.6612
	step [98/249], loss=152.5331
	step [99/249], loss=148.5372
	step [100/249], loss=156.8015
	step [101/249], loss=142.6796
	step [102/249], loss=139.1756
	step [103/249], loss=146.3338
	step [104/249], loss=146.6881
	step [105/249], loss=161.0313
	step [106/249], loss=152.8923
	step [107/249], loss=151.8848
	step [108/249], loss=143.5972
	step [109/249], loss=154.0483
	step [110/249], loss=146.8167
	step [111/249], loss=133.3360
	step [112/249], loss=152.7845
	step [113/249], loss=139.0706
	step [114/249], loss=113.4782
	step [115/249], loss=136.5388
	step [116/249], loss=136.6665
	step [117/249], loss=142.9404
	step [118/249], loss=142.0004
	step [119/249], loss=142.5388
	step [120/249], loss=157.9555
	step [121/249], loss=113.2546
	step [122/249], loss=157.4569
	step [123/249], loss=151.3520
	step [124/249], loss=139.8835
	step [125/249], loss=164.2445
	step [126/249], loss=142.7723
	step [127/249], loss=135.7828
	step [128/249], loss=138.3240
	step [129/249], loss=149.5328
	step [130/249], loss=148.2961
	step [131/249], loss=146.7416
	step [132/249], loss=168.8300
	step [133/249], loss=144.7979
	step [134/249], loss=124.9539
	step [135/249], loss=131.6478
	step [136/249], loss=128.2392
	step [137/249], loss=144.0685
	step [138/249], loss=159.3475
	step [139/249], loss=162.9420
	step [140/249], loss=134.8979
	step [141/249], loss=131.4439
	step [142/249], loss=155.0986
	step [143/249], loss=136.8125
	step [144/249], loss=149.6255
	step [145/249], loss=135.3649
	step [146/249], loss=144.7147
	step [147/249], loss=146.2394
	step [148/249], loss=159.6938
	step [149/249], loss=151.9078
	step [150/249], loss=146.7715
	step [151/249], loss=135.6375
	step [152/249], loss=146.5095
	step [153/249], loss=142.2386
	step [154/249], loss=150.6122
	step [155/249], loss=169.7831
	step [156/249], loss=141.4789
	step [157/249], loss=142.8561
	step [158/249], loss=141.7649
	step [159/249], loss=136.3651
	step [160/249], loss=158.8310
	step [161/249], loss=133.8985
	step [162/249], loss=131.3754
	step [163/249], loss=129.3998
	step [164/249], loss=139.6737
	step [165/249], loss=126.7121
	step [166/249], loss=137.8915
	step [167/249], loss=152.5118
	step [168/249], loss=135.6172
	step [169/249], loss=149.8931
	step [170/249], loss=114.7132
	step [171/249], loss=139.4916
	step [172/249], loss=146.7039
	step [173/249], loss=141.7032
	step [174/249], loss=164.2941
	step [175/249], loss=149.4631
	step [176/249], loss=139.8633
	step [177/249], loss=157.3230
	step [178/249], loss=195.1430
	step [179/249], loss=156.2331
	step [180/249], loss=150.3783
	step [181/249], loss=139.9151
	step [182/249], loss=157.5612
	step [183/249], loss=142.2941
	step [184/249], loss=138.6211
	step [185/249], loss=123.6042
	step [186/249], loss=153.8409
	step [187/249], loss=121.9934
	step [188/249], loss=136.5584
	step [189/249], loss=126.8486
	step [190/249], loss=134.3750
	step [191/249], loss=148.5735
	step [192/249], loss=129.4792
	step [193/249], loss=173.4457
	step [194/249], loss=158.4849
	step [195/249], loss=135.6150
	step [196/249], loss=146.8143
	step [197/249], loss=142.6201
	step [198/249], loss=141.1598
	step [199/249], loss=142.3478
	step [200/249], loss=128.5005
	step [201/249], loss=130.5492
	step [202/249], loss=132.2394
	step [203/249], loss=140.8428
	step [204/249], loss=139.4163
	step [205/249], loss=158.7106
	step [206/249], loss=122.4419
	step [207/249], loss=133.9084
	step [208/249], loss=147.4362
	step [209/249], loss=155.1378
	step [210/249], loss=152.3845
	step [211/249], loss=128.0912
	step [212/249], loss=148.0737
	step [213/249], loss=127.3263
	step [214/249], loss=134.8932
	step [215/249], loss=143.3413
	step [216/249], loss=135.3654
	step [217/249], loss=139.7346
	step [218/249], loss=147.8641
	step [219/249], loss=160.9522
	step [220/249], loss=137.1560
	step [221/249], loss=136.3537
	step [222/249], loss=134.2436
	step [223/249], loss=115.3481
	step [224/249], loss=141.9917
	step [225/249], loss=157.7636
	step [226/249], loss=149.5195
	step [227/249], loss=156.5843
	step [228/249], loss=154.6442
	step [229/249], loss=133.2527
	step [230/249], loss=148.2102
	step [231/249], loss=148.9088
	step [232/249], loss=140.0041
	step [233/249], loss=132.3260
	step [234/249], loss=127.7813
	step [235/249], loss=116.5912
	step [236/249], loss=138.6407
	step [237/249], loss=137.7133
	step [238/249], loss=141.8389
	step [239/249], loss=133.5671
	step [240/249], loss=138.0938
	step [241/249], loss=150.5597
	step [242/249], loss=134.0161
	step [243/249], loss=124.1820
	step [244/249], loss=143.5203
	step [245/249], loss=135.8428
	step [246/249], loss=139.4833
	step [247/249], loss=152.3288
	step [248/249], loss=134.1723
	step [249/249], loss=82.1801
	Evaluating
	loss=0.1356, precision=0.4616, recall=0.8952, f1=0.6091
saving model as: 0_saved_model.pth
Training epoch 4
	step [1/249], loss=134.9752
	step [2/249], loss=132.3587
	step [3/249], loss=115.6300
	step [4/249], loss=136.4357
	step [5/249], loss=123.0118
	step [6/249], loss=158.7514
	step [7/249], loss=135.3164
	step [8/249], loss=114.5706
	step [9/249], loss=116.8886
	step [10/249], loss=127.1763
	step [11/249], loss=128.7297
	step [12/249], loss=151.7810
	step [13/249], loss=140.7711
	step [14/249], loss=116.1440
	step [15/249], loss=134.1656
	step [16/249], loss=123.8938
	step [17/249], loss=116.3897
	step [18/249], loss=162.3068
	step [19/249], loss=161.1828
	step [20/249], loss=147.4400
	step [21/249], loss=137.7457
	step [22/249], loss=139.5975
	step [23/249], loss=141.3802
	step [24/249], loss=134.9243
	step [25/249], loss=126.3290
	step [26/249], loss=153.3325
	step [27/249], loss=137.5970
	step [28/249], loss=143.7604
	step [29/249], loss=140.2787
	step [30/249], loss=142.9646
	step [31/249], loss=137.4845
	step [32/249], loss=138.5116
	step [33/249], loss=152.6917
	step [34/249], loss=160.3918
	step [35/249], loss=137.4432
	step [36/249], loss=143.6885
	step [37/249], loss=165.5308
	step [38/249], loss=142.8259
	step [39/249], loss=144.2495
	step [40/249], loss=186.3900
	step [41/249], loss=137.6776
	step [42/249], loss=129.3373
	step [43/249], loss=141.7898
	step [44/249], loss=126.4129
	step [45/249], loss=124.3111
	step [46/249], loss=141.3749
	step [47/249], loss=126.4984
	step [48/249], loss=131.8365
	step [49/249], loss=142.6211
	step [50/249], loss=132.1402
	step [51/249], loss=134.5798
	step [52/249], loss=142.3727
	step [53/249], loss=131.9995
	step [54/249], loss=148.4163
	step [55/249], loss=157.4573
	step [56/249], loss=125.9785
	step [57/249], loss=140.3744
	step [58/249], loss=136.8405
	step [59/249], loss=135.3971
	step [60/249], loss=139.6354
	step [61/249], loss=125.9079
	step [62/249], loss=122.1629
	step [63/249], loss=146.9530
	step [64/249], loss=146.1357
	step [65/249], loss=134.5266
	step [66/249], loss=143.1314
	step [67/249], loss=123.9313
	step [68/249], loss=146.8195
	step [69/249], loss=152.6865
	step [70/249], loss=126.5022
	step [71/249], loss=144.3466
	step [72/249], loss=151.4421
	step [73/249], loss=136.0444
	step [74/249], loss=131.5153
	step [75/249], loss=136.5387
	step [76/249], loss=124.4093
	step [77/249], loss=140.3344
	step [78/249], loss=109.7913
	step [79/249], loss=137.5758
	step [80/249], loss=154.8470
	step [81/249], loss=145.3972
	step [82/249], loss=133.1296
	step [83/249], loss=129.7212
	step [84/249], loss=149.3672
	step [85/249], loss=127.8769
	step [86/249], loss=140.1580
	step [87/249], loss=126.0760
	step [88/249], loss=158.3289
	step [89/249], loss=137.7237
	step [90/249], loss=171.4520
	step [91/249], loss=138.7631
	step [92/249], loss=126.0126
	step [93/249], loss=116.8270
	step [94/249], loss=157.8755
	step [95/249], loss=130.9626
	step [96/249], loss=121.0204
	step [97/249], loss=136.4062
	step [98/249], loss=148.3696
	step [99/249], loss=139.9208
	step [100/249], loss=130.6867
	step [101/249], loss=146.4218
	step [102/249], loss=141.5696
	step [103/249], loss=131.7100
	step [104/249], loss=139.0005
	step [105/249], loss=159.3364
	step [106/249], loss=148.6060
	step [107/249], loss=154.0184
	step [108/249], loss=126.6328
	step [109/249], loss=122.9995
	step [110/249], loss=135.8819
	step [111/249], loss=141.2776
	step [112/249], loss=128.1514
	step [113/249], loss=119.7907
	step [114/249], loss=131.8371
	step [115/249], loss=160.2805
	step [116/249], loss=148.1786
	step [117/249], loss=132.2646
	step [118/249], loss=133.9382
	step [119/249], loss=129.8546
	step [120/249], loss=139.0520
	step [121/249], loss=130.7159
	step [122/249], loss=156.4624
	step [123/249], loss=144.6786
	step [124/249], loss=142.1225
	step [125/249], loss=130.8175
	step [126/249], loss=128.5367
	step [127/249], loss=136.3781
	step [128/249], loss=152.6104
	step [129/249], loss=141.7938
	step [130/249], loss=132.6568
	step [131/249], loss=118.0421
	step [132/249], loss=108.7202
	step [133/249], loss=137.8772
	step [134/249], loss=124.5342
	step [135/249], loss=115.2726
	step [136/249], loss=130.3115
	step [137/249], loss=118.1836
	step [138/249], loss=128.4290
	step [139/249], loss=138.5382
	step [140/249], loss=131.4841
	step [141/249], loss=148.1779
	step [142/249], loss=159.4309
	step [143/249], loss=144.6117
	step [144/249], loss=128.5292
	step [145/249], loss=153.9859
	step [146/249], loss=148.8036
	step [147/249], loss=124.0927
	step [148/249], loss=154.5925
	step [149/249], loss=143.6255
	step [150/249], loss=106.5476
	step [151/249], loss=124.5266
	step [152/249], loss=114.9436
	step [153/249], loss=126.3920
	step [154/249], loss=120.4529
	step [155/249], loss=115.4642
	step [156/249], loss=150.7289
	step [157/249], loss=122.5850
	step [158/249], loss=144.0769
	step [159/249], loss=113.6706
	step [160/249], loss=131.9246
	step [161/249], loss=143.1141
	step [162/249], loss=138.8655
	step [163/249], loss=138.4741
	step [164/249], loss=146.0667
	step [165/249], loss=128.7781
	step [166/249], loss=129.5451
	step [167/249], loss=140.5646
	step [168/249], loss=118.5181
	step [169/249], loss=121.2620
	step [170/249], loss=126.9315
	step [171/249], loss=153.6093
	step [172/249], loss=113.2923
	step [173/249], loss=143.8676
	step [174/249], loss=133.5165
	step [175/249], loss=122.2041
	step [176/249], loss=130.4069
	step [177/249], loss=165.2879
	step [178/249], loss=116.5147
	step [179/249], loss=139.1713
	step [180/249], loss=123.8824
	step [181/249], loss=109.7415
	step [182/249], loss=144.5345
	step [183/249], loss=134.5722
	step [184/249], loss=128.7635
	step [185/249], loss=141.2945
	step [186/249], loss=146.1008
	step [187/249], loss=127.4153
	step [188/249], loss=151.1165
	step [189/249], loss=138.0505
	step [190/249], loss=135.4077
	step [191/249], loss=121.7850
	step [192/249], loss=138.7478
	step [193/249], loss=127.6541
	step [194/249], loss=113.5680
	step [195/249], loss=134.6976
	step [196/249], loss=113.7173
	step [197/249], loss=137.1008
	step [198/249], loss=120.4566
	step [199/249], loss=120.1134
	step [200/249], loss=134.7164
	step [201/249], loss=132.1865
	step [202/249], loss=124.0263
	step [203/249], loss=140.9170
	step [204/249], loss=116.0067
	step [205/249], loss=124.3344
	step [206/249], loss=118.3508
	step [207/249], loss=113.6925
	step [208/249], loss=125.4054
	step [209/249], loss=151.1300
	step [210/249], loss=121.1548
	step [211/249], loss=139.3713
	step [212/249], loss=120.9905
	step [213/249], loss=125.0683
	step [214/249], loss=136.9159
	step [215/249], loss=117.7173
	step [216/249], loss=142.8455
	step [217/249], loss=145.0981
	step [218/249], loss=149.7668
	step [219/249], loss=122.1350
	step [220/249], loss=133.5088
	step [221/249], loss=122.2719
	step [222/249], loss=145.5959
	step [223/249], loss=141.3418
	step [224/249], loss=129.4005
	step [225/249], loss=118.6658
	step [226/249], loss=126.8539
	step [227/249], loss=175.3412
	step [228/249], loss=127.6522
	step [229/249], loss=111.6345
	step [230/249], loss=111.2221
	step [231/249], loss=133.6433
	step [232/249], loss=117.0332
	step [233/249], loss=141.3192
	step [234/249], loss=132.2470
	step [235/249], loss=159.0439
	step [236/249], loss=114.1081
	step [237/249], loss=140.8010
	step [238/249], loss=160.0982
	step [239/249], loss=153.9972
	step [240/249], loss=132.3308
	step [241/249], loss=130.4508
	step [242/249], loss=138.0390
	step [243/249], loss=136.2442
	step [244/249], loss=110.4110
	step [245/249], loss=122.5483
	step [246/249], loss=129.1227
	step [247/249], loss=141.7802
	step [248/249], loss=127.9166
	step [249/249], loss=99.0224
	Evaluating
	loss=0.0985, precision=0.4650, recall=0.9001, f1=0.6132
saving model as: 0_saved_model.pth
Training epoch 5
	step [1/249], loss=149.4142
	step [2/249], loss=132.6665
	step [3/249], loss=147.3761
	step [4/249], loss=139.8064
	step [5/249], loss=149.8582
	step [6/249], loss=119.9353
	step [7/249], loss=135.8457
	step [8/249], loss=125.6564
	step [9/249], loss=139.0952
	step [10/249], loss=152.0853
	step [11/249], loss=135.7256
	step [12/249], loss=121.4486
	step [13/249], loss=124.4862
	step [14/249], loss=112.3718
	step [15/249], loss=134.3274
	step [16/249], loss=122.5100
	step [17/249], loss=135.4249
	step [18/249], loss=143.9387
	step [19/249], loss=143.0700
	step [20/249], loss=120.8818
	step [21/249], loss=133.1619
	step [22/249], loss=118.3299
	step [23/249], loss=115.1622
	step [24/249], loss=146.5828
	step [25/249], loss=138.5490
	step [26/249], loss=165.1831
	step [27/249], loss=120.9878
	step [28/249], loss=133.1001
	step [29/249], loss=126.8125
	step [30/249], loss=127.7937
	step [31/249], loss=130.0376
	step [32/249], loss=126.9538
	step [33/249], loss=135.1398
	step [34/249], loss=139.7801
	step [35/249], loss=163.9067
	step [36/249], loss=141.0029
	step [37/249], loss=128.6501
	step [38/249], loss=149.6270
	step [39/249], loss=148.2103
	step [40/249], loss=131.3742
	step [41/249], loss=120.3683
	step [42/249], loss=142.0444
	step [43/249], loss=134.4654
	step [44/249], loss=126.5964
	step [45/249], loss=131.1537
	step [46/249], loss=149.4250
	step [47/249], loss=134.5254
	step [48/249], loss=115.8442
	step [49/249], loss=145.7010
	step [50/249], loss=138.5730
	step [51/249], loss=142.0571
	step [52/249], loss=120.7200
	step [53/249], loss=136.2515
	step [54/249], loss=125.3007
	step [55/249], loss=142.7384
	step [56/249], loss=102.9301
	step [57/249], loss=118.4018
	step [58/249], loss=130.3122
	step [59/249], loss=128.4159
	step [60/249], loss=132.7896
	step [61/249], loss=98.1365
	step [62/249], loss=128.0363
	step [63/249], loss=113.8687
	step [64/249], loss=121.6055
	step [65/249], loss=131.5306
	step [66/249], loss=125.1888
	step [67/249], loss=117.6138
	step [68/249], loss=126.3181
	step [69/249], loss=122.5632
	step [70/249], loss=144.0271
	step [71/249], loss=133.5182
	step [72/249], loss=127.6556
	step [73/249], loss=111.9584
	step [74/249], loss=135.0050
	step [75/249], loss=156.9566
	step [76/249], loss=135.3741
	step [77/249], loss=127.3500
	step [78/249], loss=120.8420
	step [79/249], loss=141.6171
	step [80/249], loss=138.3446
	step [81/249], loss=122.6584
	step [82/249], loss=128.7385
	step [83/249], loss=129.5530
	step [84/249], loss=118.3045
	step [85/249], loss=122.1540
	step [86/249], loss=125.7583
	step [87/249], loss=152.4492
	step [88/249], loss=144.1850
	step [89/249], loss=145.3105
	step [90/249], loss=133.7129
	step [91/249], loss=120.4286
	step [92/249], loss=140.3844
	step [93/249], loss=140.6677
	step [94/249], loss=114.6587
	step [95/249], loss=115.0948
	step [96/249], loss=139.2001
	step [97/249], loss=113.9525
	step [98/249], loss=128.6191
	step [99/249], loss=118.8421
	step [100/249], loss=142.9518
	step [101/249], loss=142.0744
	step [102/249], loss=128.0560
	step [103/249], loss=144.2723
	step [104/249], loss=124.1963
	step [105/249], loss=148.0444
	step [106/249], loss=136.3139
	step [107/249], loss=124.7568
	step [108/249], loss=129.2377
	step [109/249], loss=156.0754
	step [110/249], loss=142.8266
	step [111/249], loss=136.2534
	step [112/249], loss=102.8690
	step [113/249], loss=123.6505
	step [114/249], loss=107.9757
	step [115/249], loss=129.4368
	step [116/249], loss=123.6175
	step [117/249], loss=118.6540
	step [118/249], loss=131.8732
	step [119/249], loss=120.0303
	step [120/249], loss=163.6091
	step [121/249], loss=151.0966
	step [122/249], loss=123.4251
	step [123/249], loss=121.6816
	step [124/249], loss=131.0404
	step [125/249], loss=125.1504
	step [126/249], loss=112.3732
	step [127/249], loss=117.2203
	step [128/249], loss=144.8952
	step [129/249], loss=120.7728
	step [130/249], loss=123.5839
	step [131/249], loss=113.2807
	step [132/249], loss=122.7033
	step [133/249], loss=101.6034
	step [134/249], loss=119.0440
	step [135/249], loss=127.5893
	step [136/249], loss=132.1281
	step [137/249], loss=126.7446
	step [138/249], loss=129.3889
	step [139/249], loss=121.5569
	step [140/249], loss=138.4754
	step [141/249], loss=132.5492
	step [142/249], loss=145.8194
	step [143/249], loss=111.6301
	step [144/249], loss=143.1863
	step [145/249], loss=139.8030
	step [146/249], loss=120.6694
	step [147/249], loss=131.8027
	step [148/249], loss=129.7944
	step [149/249], loss=133.6248
	step [150/249], loss=122.5517
	step [151/249], loss=103.8071
	step [152/249], loss=127.3828
	step [153/249], loss=109.3878
	step [154/249], loss=141.4242
	step [155/249], loss=118.7951
	step [156/249], loss=137.5251
	step [157/249], loss=130.9239
	step [158/249], loss=115.8000
	step [159/249], loss=124.9545
	step [160/249], loss=128.5069
	step [161/249], loss=124.8939
	step [162/249], loss=114.1777
	step [163/249], loss=108.1458
	step [164/249], loss=128.6997
	step [165/249], loss=118.7819
	step [166/249], loss=130.1934
	step [167/249], loss=141.1709
	step [168/249], loss=125.4299
	step [169/249], loss=128.2957
	step [170/249], loss=114.4471
	step [171/249], loss=107.4104
	step [172/249], loss=132.4509
	step [173/249], loss=133.6210
	step [174/249], loss=133.2919
	step [175/249], loss=111.3750
	step [176/249], loss=124.9462
	step [177/249], loss=118.8000
	step [178/249], loss=137.2395
	step [179/249], loss=120.6973
	step [180/249], loss=121.0401
	step [181/249], loss=132.0901
	step [182/249], loss=137.9845
	step [183/249], loss=140.4393
	step [184/249], loss=127.7093
	step [185/249], loss=137.0858
	step [186/249], loss=108.0404
	step [187/249], loss=97.9670
	step [188/249], loss=135.3017
	step [189/249], loss=141.9236
	step [190/249], loss=144.3185
	step [191/249], loss=116.6963
	step [192/249], loss=130.4962
	step [193/249], loss=111.9662
	step [194/249], loss=135.2380
	step [195/249], loss=140.3882
	step [196/249], loss=108.9659
	step [197/249], loss=125.0935
	step [198/249], loss=136.1327
	step [199/249], loss=139.8692
	step [200/249], loss=150.4427
	step [201/249], loss=112.0553
	step [202/249], loss=115.5650
	step [203/249], loss=136.3980
	step [204/249], loss=109.2848
	step [205/249], loss=121.4351
	step [206/249], loss=115.1094
	step [207/249], loss=125.8695
	step [208/249], loss=142.4888
	step [209/249], loss=119.5321
	step [210/249], loss=133.9551
	step [211/249], loss=124.4702
	step [212/249], loss=114.5520
	step [213/249], loss=124.0564
	step [214/249], loss=128.4076
	step [215/249], loss=123.9901
	step [216/249], loss=137.4007
	step [217/249], loss=105.9722
	step [218/249], loss=132.8004
	step [219/249], loss=117.1982
	step [220/249], loss=131.4482
	step [221/249], loss=123.5552
	step [222/249], loss=138.9259
	step [223/249], loss=125.6571
	step [224/249], loss=113.4713
	step [225/249], loss=138.4605
	step [226/249], loss=108.9793
	step [227/249], loss=126.2589
	step [228/249], loss=108.4680
	step [229/249], loss=108.8064
	step [230/249], loss=122.2244
	step [231/249], loss=122.5001
	step [232/249], loss=113.8519
	step [233/249], loss=116.9807
	step [234/249], loss=124.1103
	step [235/249], loss=124.4403
	step [236/249], loss=142.0066
	step [237/249], loss=131.0507
	step [238/249], loss=110.6086
	step [239/249], loss=126.5770
	step [240/249], loss=135.8080
	step [241/249], loss=138.1327
	step [242/249], loss=136.6182
	step [243/249], loss=118.7368
	step [244/249], loss=104.9404
	step [245/249], loss=115.7820
	step [246/249], loss=116.4437
	step [247/249], loss=97.1539
	step [248/249], loss=127.7553
	step [249/249], loss=78.1836
	Evaluating
	loss=0.0724, precision=0.4458, recall=0.9239, f1=0.6014
Training epoch 6
	step [1/249], loss=99.1864
	step [2/249], loss=120.8758
	step [3/249], loss=122.8078
	step [4/249], loss=96.7516
	step [5/249], loss=132.0907
	step [6/249], loss=130.5661
	step [7/249], loss=110.1940
	step [8/249], loss=127.9703
	step [9/249], loss=143.5336
	step [10/249], loss=133.7113
	step [11/249], loss=158.2933
	step [12/249], loss=139.3843
	step [13/249], loss=119.5293
	step [14/249], loss=145.1262
	step [15/249], loss=116.0519
	step [16/249], loss=147.2721
	step [17/249], loss=108.8428
	step [18/249], loss=132.6384
	step [19/249], loss=118.5272
	step [20/249], loss=126.4408
	step [21/249], loss=121.9498
	step [22/249], loss=132.6173
	step [23/249], loss=109.2536
	step [24/249], loss=123.1172
	step [25/249], loss=117.4616
	step [26/249], loss=133.0365
	step [27/249], loss=123.3819
	step [28/249], loss=90.8232
	step [29/249], loss=115.6256
	step [30/249], loss=142.7050
	step [31/249], loss=147.9065
	step [32/249], loss=121.9490
	step [33/249], loss=132.6420
	step [34/249], loss=126.6915
	step [35/249], loss=118.8473
	step [36/249], loss=125.9269
	step [37/249], loss=136.1334
	step [38/249], loss=135.8672
	step [39/249], loss=129.2417
	step [40/249], loss=141.3021
	step [41/249], loss=148.3166
	step [42/249], loss=110.9766
	step [43/249], loss=122.4377
	step [44/249], loss=113.5882
	step [45/249], loss=134.2321
	step [46/249], loss=124.6521
	step [47/249], loss=121.4520
	step [48/249], loss=114.1637
	step [49/249], loss=106.3163
	step [50/249], loss=132.6138
	step [51/249], loss=118.0624
	step [52/249], loss=128.1839
	step [53/249], loss=124.8207
	step [54/249], loss=119.9147
	step [55/249], loss=90.3953
	step [56/249], loss=110.3606
	step [57/249], loss=132.5802
	step [58/249], loss=129.4612
	step [59/249], loss=119.7260
	step [60/249], loss=155.3477
	step [61/249], loss=124.8223
	step [62/249], loss=115.5514
	step [63/249], loss=147.8499
	step [64/249], loss=127.9640
	step [65/249], loss=123.1061
	step [66/249], loss=126.1729
	step [67/249], loss=130.6629
	step [68/249], loss=117.4801
	step [69/249], loss=137.1736
	step [70/249], loss=131.8487
	step [71/249], loss=111.8488
	step [72/249], loss=120.2969
	step [73/249], loss=135.7580
	step [74/249], loss=136.0119
	step [75/249], loss=139.9664
	step [76/249], loss=126.9853
	step [77/249], loss=133.5779
	step [78/249], loss=142.0088
	step [79/249], loss=99.4881
	step [80/249], loss=92.2696
	step [81/249], loss=127.3827
	step [82/249], loss=125.6587
	step [83/249], loss=105.2138
	step [84/249], loss=118.6451
	step [85/249], loss=111.0419
	step [86/249], loss=113.1820
	step [87/249], loss=104.9459
	step [88/249], loss=119.0234
	step [89/249], loss=113.7020
	step [90/249], loss=119.1525
	step [91/249], loss=125.0305
	step [92/249], loss=127.5463
	step [93/249], loss=126.1023
	step [94/249], loss=123.3678
	step [95/249], loss=128.2268
	step [96/249], loss=120.1329
	step [97/249], loss=146.3159
	step [98/249], loss=130.4775
	step [99/249], loss=135.1891
	step [100/249], loss=108.6103
	step [101/249], loss=113.0852
	step [102/249], loss=120.5894
	step [103/249], loss=129.3334
	step [104/249], loss=126.8795
	step [105/249], loss=122.8616
	step [106/249], loss=143.7196
	step [107/249], loss=125.8737
	step [108/249], loss=154.2212
	step [109/249], loss=135.6593
	step [110/249], loss=127.0547
	step [111/249], loss=147.6237
	step [112/249], loss=119.9222
	step [113/249], loss=115.8650
	step [114/249], loss=140.0218
	step [115/249], loss=133.4874
	step [116/249], loss=122.1165
	step [117/249], loss=130.8232
	step [118/249], loss=111.2588
	step [119/249], loss=109.0381
	step [120/249], loss=126.2270
	step [121/249], loss=112.3949
	step [122/249], loss=140.8514
	step [123/249], loss=114.7379
	step [124/249], loss=111.9910
	step [125/249], loss=139.1401
	step [126/249], loss=129.9689
	step [127/249], loss=140.9876
	step [128/249], loss=141.5122
	step [129/249], loss=144.0833
	step [130/249], loss=129.1654
	step [131/249], loss=127.4370
	step [132/249], loss=121.3704
	step [133/249], loss=123.9013
	step [134/249], loss=92.0923
	step [135/249], loss=130.2974
	step [136/249], loss=113.4827
	step [137/249], loss=130.4508
	step [138/249], loss=118.4991
	step [139/249], loss=119.2564
	step [140/249], loss=99.8020
	step [141/249], loss=121.7159
	step [142/249], loss=138.6740
	step [143/249], loss=129.0777
	step [144/249], loss=121.3574
	step [145/249], loss=133.2771
	step [146/249], loss=112.4032
	step [147/249], loss=116.9549
	step [148/249], loss=117.4827
	step [149/249], loss=128.1428
	step [150/249], loss=128.5007
	step [151/249], loss=109.2912
	step [152/249], loss=135.4521
	step [153/249], loss=108.4463
	step [154/249], loss=126.0231
	step [155/249], loss=139.0034
	step [156/249], loss=158.7906
	step [157/249], loss=118.3574
	step [158/249], loss=125.0939
	step [159/249], loss=99.1996
	step [160/249], loss=115.7904
	step [161/249], loss=99.9771
	step [162/249], loss=139.2905
	step [163/249], loss=132.5939
	step [164/249], loss=120.7717
	step [165/249], loss=121.3700
	step [166/249], loss=109.5038
	step [167/249], loss=127.7590
	step [168/249], loss=151.8247
	step [169/249], loss=111.5509
	step [170/249], loss=127.8369
	step [171/249], loss=111.5148
	step [172/249], loss=111.5017
	step [173/249], loss=164.6263
	step [174/249], loss=119.4674
	step [175/249], loss=134.1774
	step [176/249], loss=112.8458
	step [177/249], loss=110.2915
	step [178/249], loss=106.1969
	step [179/249], loss=111.1506
	step [180/249], loss=121.4931
	step [181/249], loss=105.8016
	step [182/249], loss=132.1468
	step [183/249], loss=122.5866
	step [184/249], loss=121.5387
	step [185/249], loss=113.7386
	step [186/249], loss=110.7992
	step [187/249], loss=119.7272
	step [188/249], loss=133.9309
	step [189/249], loss=130.0731
	step [190/249], loss=84.4882
	step [191/249], loss=146.3797
	step [192/249], loss=124.6812
	step [193/249], loss=145.0597
	step [194/249], loss=127.6147
	step [195/249], loss=116.5900
	step [196/249], loss=141.5012
	step [197/249], loss=133.1768
	step [198/249], loss=139.7576
	step [199/249], loss=113.4931
	step [200/249], loss=115.7556
	step [201/249], loss=126.5516
	step [202/249], loss=123.6728
	step [203/249], loss=125.4645
	step [204/249], loss=149.9830
	step [205/249], loss=126.8774
	step [206/249], loss=116.3931
	step [207/249], loss=130.8933
	step [208/249], loss=141.0631
	step [209/249], loss=120.9683
	step [210/249], loss=116.0952
	step [211/249], loss=111.3240
	step [212/249], loss=131.5369
	step [213/249], loss=117.3364
	step [214/249], loss=105.0623
	step [215/249], loss=101.6364
	step [216/249], loss=90.5051
	step [217/249], loss=109.3448
	step [218/249], loss=125.0460
	step [219/249], loss=118.7651
	step [220/249], loss=111.9755
	step [221/249], loss=115.8348
	step [222/249], loss=130.4807
	step [223/249], loss=115.6846
	step [224/249], loss=107.0838
	step [225/249], loss=118.0560
	step [226/249], loss=132.9325
	step [227/249], loss=101.6772
	step [228/249], loss=116.3419
	step [229/249], loss=109.8510
	step [230/249], loss=121.6423
	step [231/249], loss=126.1538
	step [232/249], loss=129.1230
	step [233/249], loss=157.1356
	step [234/249], loss=93.2893
	step [235/249], loss=94.8177
	step [236/249], loss=128.1945
	step [237/249], loss=127.3085
	step [238/249], loss=124.6048
	step [239/249], loss=116.1615
	step [240/249], loss=160.7637
	step [241/249], loss=112.2801
	step [242/249], loss=130.4372
	step [243/249], loss=126.4093
	step [244/249], loss=106.1163
	step [245/249], loss=118.6021
	step [246/249], loss=120.9630
	step [247/249], loss=113.3724
	step [248/249], loss=131.5925
	step [249/249], loss=88.9355
	Evaluating
	loss=0.0560, precision=0.4374, recall=0.9232, f1=0.5935
Training epoch 7
	step [1/249], loss=130.1419
	step [2/249], loss=123.6758
	step [3/249], loss=115.9160
	step [4/249], loss=101.5481
	step [5/249], loss=127.5058
	step [6/249], loss=120.6592
	step [7/249], loss=114.1311
	step [8/249], loss=121.7976
	step [9/249], loss=106.2913
	step [10/249], loss=110.7583
	step [11/249], loss=102.6203
	step [12/249], loss=124.4304
	step [13/249], loss=115.7558
	step [14/249], loss=120.4922
	step [15/249], loss=124.6493
	step [16/249], loss=140.8595
	step [17/249], loss=118.5453
	step [18/249], loss=121.7451
	step [19/249], loss=126.5314
	step [20/249], loss=129.8810
	step [21/249], loss=110.2361
	step [22/249], loss=114.4596
	step [23/249], loss=123.6133
	step [24/249], loss=107.3887
	step [25/249], loss=114.6364
	step [26/249], loss=112.1101
	step [27/249], loss=94.0384
	step [28/249], loss=130.7107
	step [29/249], loss=104.5000
	step [30/249], loss=124.2289
	step [31/249], loss=107.5989
	step [32/249], loss=132.6295
	step [33/249], loss=146.4911
	step [34/249], loss=134.8826
	step [35/249], loss=111.6764
	step [36/249], loss=129.8562
	step [37/249], loss=121.2309
	step [38/249], loss=110.3540
	step [39/249], loss=117.9393
	step [40/249], loss=133.4642
	step [41/249], loss=128.6580
	step [42/249], loss=140.9631
	step [43/249], loss=136.4423
	step [44/249], loss=109.1634
	step [45/249], loss=126.9037
	step [46/249], loss=109.4411
	step [47/249], loss=112.0552
	step [48/249], loss=119.1872
	step [49/249], loss=117.5992
	step [50/249], loss=124.3376
	step [51/249], loss=103.9781
	step [52/249], loss=106.1958
	step [53/249], loss=123.1881
	step [54/249], loss=123.2600
	step [55/249], loss=118.8810
	step [56/249], loss=115.4838
	step [57/249], loss=121.7856
	step [58/249], loss=122.6820
	step [59/249], loss=128.6081
	step [60/249], loss=121.5264
	step [61/249], loss=124.0265
	step [62/249], loss=112.0539
	step [63/249], loss=128.8385
	step [64/249], loss=90.9884
	step [65/249], loss=115.1293
	step [66/249], loss=106.5014
	step [67/249], loss=110.3842
	step [68/249], loss=100.2959
	step [69/249], loss=102.5520
	step [70/249], loss=115.2802
	step [71/249], loss=116.3425
	step [72/249], loss=119.9283
	step [73/249], loss=122.5363
	step [74/249], loss=107.9192
	step [75/249], loss=121.7171
	step [76/249], loss=115.8541
	step [77/249], loss=124.5305
	step [78/249], loss=132.4204
	step [79/249], loss=112.8928
	step [80/249], loss=118.2131
	step [81/249], loss=113.2659
	step [82/249], loss=132.5356
	step [83/249], loss=110.4153
	step [84/249], loss=117.9184
	step [85/249], loss=123.6036
	step [86/249], loss=145.9417
	step [87/249], loss=137.4939
	step [88/249], loss=126.0391
	step [89/249], loss=116.0476
	step [90/249], loss=122.1386
	step [91/249], loss=124.1238
	step [92/249], loss=110.4024
	step [93/249], loss=113.8343
	step [94/249], loss=133.3332
	step [95/249], loss=122.9406
	step [96/249], loss=118.4891
	step [97/249], loss=126.6173
	step [98/249], loss=125.7175
	step [99/249], loss=133.3877
	step [100/249], loss=119.5923
	step [101/249], loss=108.4241
	step [102/249], loss=109.4839
	step [103/249], loss=111.1494
	step [104/249], loss=113.1746
	step [105/249], loss=104.6658
	step [106/249], loss=137.7785
	step [107/249], loss=105.8192
	step [108/249], loss=117.1638
	step [109/249], loss=134.2742
	step [110/249], loss=117.3656
	step [111/249], loss=131.7591
	step [112/249], loss=112.2666
	step [113/249], loss=130.0943
	step [114/249], loss=132.2785
	step [115/249], loss=135.3712
	step [116/249], loss=111.6871
	step [117/249], loss=114.8619
	step [118/249], loss=117.0383
	step [119/249], loss=111.5721
	step [120/249], loss=149.5912
	step [121/249], loss=126.7831
	step [122/249], loss=122.5066
	step [123/249], loss=120.7111
	step [124/249], loss=111.0910
	step [125/249], loss=93.1783
	step [126/249], loss=127.4726
	step [127/249], loss=129.2940
	step [128/249], loss=124.7752
	step [129/249], loss=95.5525
	step [130/249], loss=117.4064
	step [131/249], loss=130.5306
	step [132/249], loss=125.7223
	step [133/249], loss=121.3343
	step [134/249], loss=115.9644
	step [135/249], loss=112.8571
	step [136/249], loss=122.1192
	step [137/249], loss=130.9951
	step [138/249], loss=132.7309
	step [139/249], loss=118.6360
	step [140/249], loss=133.5357
	step [141/249], loss=121.1934
	step [142/249], loss=115.2065
	step [143/249], loss=105.7678
	step [144/249], loss=88.5652
	step [145/249], loss=118.2149
	step [146/249], loss=124.3576
	step [147/249], loss=122.7475
	step [148/249], loss=128.3566
	step [149/249], loss=114.8760
	step [150/249], loss=117.4066
	step [151/249], loss=112.3555
	step [152/249], loss=112.7455
	step [153/249], loss=115.7465
	step [154/249], loss=125.1537
	step [155/249], loss=101.9196
	step [156/249], loss=117.9189
	step [157/249], loss=126.6831
	step [158/249], loss=113.3912
	step [159/249], loss=102.1055
	step [160/249], loss=122.1753
	step [161/249], loss=123.7470
	step [162/249], loss=122.3388
	step [163/249], loss=132.1292
	step [164/249], loss=114.9819
	step [165/249], loss=121.9108
	step [166/249], loss=128.9712
	step [167/249], loss=131.6187
	step [168/249], loss=117.0904
	step [169/249], loss=123.3910
	step [170/249], loss=119.7735
	step [171/249], loss=120.2774
	step [172/249], loss=135.3874
	step [173/249], loss=133.3043
	step [174/249], loss=136.2134
	step [175/249], loss=121.4641
	step [176/249], loss=139.8262
	step [177/249], loss=127.8772
	step [178/249], loss=117.4035
	step [179/249], loss=90.2392
	step [180/249], loss=127.2244
	step [181/249], loss=134.5098
	step [182/249], loss=112.5324
	step [183/249], loss=131.7880
	step [184/249], loss=126.0291
	step [185/249], loss=121.1907
	step [186/249], loss=124.0739
	step [187/249], loss=140.1088
	step [188/249], loss=127.5833
	step [189/249], loss=126.4156
	step [190/249], loss=118.3900
	step [191/249], loss=111.4289
	step [192/249], loss=119.7940
	step [193/249], loss=101.4910
	step [194/249], loss=144.6422
	step [195/249], loss=122.5944
	step [196/249], loss=141.4535
	step [197/249], loss=123.4081
	step [198/249], loss=139.8585
	step [199/249], loss=143.1895
	step [200/249], loss=118.0802
	step [201/249], loss=103.1731
	step [202/249], loss=123.1937
	step [203/249], loss=132.6744
	step [204/249], loss=102.7608
	step [205/249], loss=130.6306
	step [206/249], loss=107.5536
	step [207/249], loss=117.6190
	step [208/249], loss=116.1894
	step [209/249], loss=96.4305
	step [210/249], loss=121.8063
	step [211/249], loss=116.6351
	step [212/249], loss=117.2654
	step [213/249], loss=99.0818
	step [214/249], loss=128.0677
	step [215/249], loss=109.7244
	step [216/249], loss=122.3578
	step [217/249], loss=126.6705
	step [218/249], loss=117.2801
	step [219/249], loss=118.8606
	step [220/249], loss=129.2333
	step [221/249], loss=142.2722
	step [222/249], loss=117.3564
	step [223/249], loss=144.6176
	step [224/249], loss=114.3292
	step [225/249], loss=125.3874
	step [226/249], loss=133.0300
	step [227/249], loss=130.1676
	step [228/249], loss=147.0387
	step [229/249], loss=108.4638
	step [230/249], loss=130.6146
	step [231/249], loss=123.5486
	step [232/249], loss=104.0978
	step [233/249], loss=115.6948
	step [234/249], loss=124.7893
	step [235/249], loss=119.2231
	step [236/249], loss=101.6477
	step [237/249], loss=131.9717
	step [238/249], loss=93.4770
	step [239/249], loss=114.5201
	step [240/249], loss=118.3486
	step [241/249], loss=117.4155
	step [242/249], loss=106.6356
	step [243/249], loss=106.3351
	step [244/249], loss=129.1564
	step [245/249], loss=112.3066
	step [246/249], loss=114.5782
	step [247/249], loss=108.3827
	step [248/249], loss=130.4816
	step [249/249], loss=87.3870
	Evaluating
	loss=0.0449, precision=0.4809, recall=0.9042, f1=0.6279
saving model as: 0_saved_model.pth
Training epoch 8
	step [1/249], loss=110.3848
	step [2/249], loss=102.4772
	step [3/249], loss=121.1095
	step [4/249], loss=130.5804
	step [5/249], loss=112.3401
	step [6/249], loss=113.5678
	step [7/249], loss=135.1838
	step [8/249], loss=91.5755
	step [9/249], loss=119.3057
	step [10/249], loss=123.5498
	step [11/249], loss=126.6404
	step [12/249], loss=133.8792
	step [13/249], loss=130.6193
	step [14/249], loss=121.7441
	step [15/249], loss=121.1836
	step [16/249], loss=121.9604
	step [17/249], loss=97.5225
	step [18/249], loss=119.2591
	step [19/249], loss=127.8065
	step [20/249], loss=131.4408
	step [21/249], loss=95.3405
	step [22/249], loss=104.0001
	step [23/249], loss=105.5558
	step [24/249], loss=113.8949
	step [25/249], loss=121.0377
	step [26/249], loss=119.2984
	step [27/249], loss=117.2074
	step [28/249], loss=134.5422
	step [29/249], loss=101.5359
	step [30/249], loss=116.8699
	step [31/249], loss=113.7680
	step [32/249], loss=122.6305
	step [33/249], loss=103.0680
	step [34/249], loss=105.2483
	step [35/249], loss=139.5620
	step [36/249], loss=107.6245
	step [37/249], loss=111.3022
	step [38/249], loss=126.4261
	step [39/249], loss=110.9803
	step [40/249], loss=104.9716
	step [41/249], loss=109.2604
	step [42/249], loss=107.5154
	step [43/249], loss=109.1698
	step [44/249], loss=116.7957
	step [45/249], loss=97.7041
	step [46/249], loss=94.6376
	step [47/249], loss=102.0771
	step [48/249], loss=108.1345
	step [49/249], loss=124.0851
	step [50/249], loss=118.7167
	step [51/249], loss=123.9328
	step [52/249], loss=125.9095
	step [53/249], loss=103.4219
	step [54/249], loss=124.1655
	step [55/249], loss=136.6976
	step [56/249], loss=99.0744
	step [57/249], loss=113.8707
	step [58/249], loss=107.9217
	step [59/249], loss=128.2438
	step [60/249], loss=118.4554
	step [61/249], loss=91.2868
	step [62/249], loss=110.6751
	step [63/249], loss=119.5436
	step [64/249], loss=120.6464
	step [65/249], loss=117.5581
	step [66/249], loss=129.4804
	step [67/249], loss=102.6908
	step [68/249], loss=132.9164
	step [69/249], loss=127.5408
	step [70/249], loss=114.2504
	step [71/249], loss=96.6145
	step [72/249], loss=112.0544
	step [73/249], loss=104.2393
	step [74/249], loss=101.4744
	step [75/249], loss=124.1337
	step [76/249], loss=126.9470
	step [77/249], loss=121.5416
	step [78/249], loss=136.2682
	step [79/249], loss=117.1137
	step [80/249], loss=126.2739
	step [81/249], loss=111.1026
	step [82/249], loss=106.8306
	step [83/249], loss=113.6738
	step [84/249], loss=113.3920
	step [85/249], loss=92.7410
	step [86/249], loss=119.4511
	step [87/249], loss=128.2904
	step [88/249], loss=106.4076
	step [89/249], loss=127.5508
	step [90/249], loss=117.6057
	step [91/249], loss=121.7279
	step [92/249], loss=113.9114
	step [93/249], loss=131.0872
	step [94/249], loss=132.9420
	step [95/249], loss=135.0735
	step [96/249], loss=116.5268
	step [97/249], loss=128.6368
	step [98/249], loss=111.4252
	step [99/249], loss=120.4069
	step [100/249], loss=126.6883
	step [101/249], loss=119.4215
	step [102/249], loss=99.3319
	step [103/249], loss=100.8050
	step [104/249], loss=132.8800
	step [105/249], loss=147.9531
	step [106/249], loss=108.3763
	step [107/249], loss=108.8332
	step [108/249], loss=118.5678
	step [109/249], loss=107.6472
	step [110/249], loss=122.9900
	step [111/249], loss=125.5043
	step [112/249], loss=101.8577
	step [113/249], loss=117.6724
	step [114/249], loss=123.1399
	step [115/249], loss=113.8460
	step [116/249], loss=153.3084
	step [117/249], loss=104.9167
	step [118/249], loss=104.4662
	step [119/249], loss=135.7949
	step [120/249], loss=131.0105
	step [121/249], loss=144.2724
	step [122/249], loss=108.3423
	step [123/249], loss=103.4006
	step [124/249], loss=118.3563
	step [125/249], loss=120.5712
	step [126/249], loss=114.0013
	step [127/249], loss=102.9228
	step [128/249], loss=96.8633
	step [129/249], loss=132.1942
	step [130/249], loss=104.2347
	step [131/249], loss=118.1573
	step [132/249], loss=136.1566
	step [133/249], loss=126.1794
	step [134/249], loss=132.1265
	step [135/249], loss=122.1374
	step [136/249], loss=133.8659
	step [137/249], loss=132.8070
	step [138/249], loss=120.1999
	step [139/249], loss=105.5165
	step [140/249], loss=113.1683
	step [141/249], loss=113.1968
	step [142/249], loss=115.6977
	step [143/249], loss=120.1138
	step [144/249], loss=124.1471
	step [145/249], loss=129.7765
	step [146/249], loss=127.5266
	step [147/249], loss=107.4029
	step [148/249], loss=144.2374
	step [149/249], loss=124.8609
	step [150/249], loss=107.4571
	step [151/249], loss=107.3762
	step [152/249], loss=133.8409
	step [153/249], loss=114.7724
	step [154/249], loss=116.4702
	step [155/249], loss=103.1460
	step [156/249], loss=113.7377
	step [157/249], loss=123.9339
	step [158/249], loss=128.0488
	step [159/249], loss=104.1934
	step [160/249], loss=123.3938
	step [161/249], loss=111.8777
	step [162/249], loss=121.0416
	step [163/249], loss=118.4333
	step [164/249], loss=127.0696
	step [165/249], loss=128.5274
	step [166/249], loss=120.2052
	step [167/249], loss=96.1540
	step [168/249], loss=114.0332
	step [169/249], loss=111.6093
	step [170/249], loss=122.4043
	step [171/249], loss=125.1336
	step [172/249], loss=125.4386
	step [173/249], loss=118.9784
	step [174/249], loss=94.9045
	step [175/249], loss=150.8503
	step [176/249], loss=131.8304
	step [177/249], loss=96.9036
	step [178/249], loss=115.2211
	step [179/249], loss=124.0633
	step [180/249], loss=115.9902
	step [181/249], loss=120.4573
	step [182/249], loss=88.7199
	step [183/249], loss=121.0332
	step [184/249], loss=93.4575
	step [185/249], loss=111.1521
	step [186/249], loss=130.0899
	step [187/249], loss=122.0805
	step [188/249], loss=106.8146
	step [189/249], loss=119.6024
	step [190/249], loss=119.5343
	step [191/249], loss=108.1937
	step [192/249], loss=138.4601
	step [193/249], loss=107.6165
	step [194/249], loss=114.6657
	step [195/249], loss=122.2993
	step [196/249], loss=92.3826
	step [197/249], loss=111.9670
	step [198/249], loss=104.5994
	step [199/249], loss=112.0080
	step [200/249], loss=133.5949
	step [201/249], loss=112.7159
	step [202/249], loss=122.3971
	step [203/249], loss=136.3372
	step [204/249], loss=123.0746
	step [205/249], loss=107.5943
	step [206/249], loss=102.9584
	step [207/249], loss=121.8105
	step [208/249], loss=120.2856
	step [209/249], loss=124.7991
	step [210/249], loss=120.5048
	step [211/249], loss=119.1324
	step [212/249], loss=119.2681
	step [213/249], loss=142.8566
	step [214/249], loss=116.1739
	step [215/249], loss=131.7855
	step [216/249], loss=107.1871
	step [217/249], loss=117.7140
	step [218/249], loss=134.8429
	step [219/249], loss=135.0288
	step [220/249], loss=118.6053
	step [221/249], loss=109.1389
	step [222/249], loss=126.8700
	step [223/249], loss=130.0649
	step [224/249], loss=121.8042
	step [225/249], loss=127.7969
	step [226/249], loss=115.9016
	step [227/249], loss=116.3318
	step [228/249], loss=142.1749
	step [229/249], loss=127.0086
	step [230/249], loss=134.4926
	step [231/249], loss=108.1629
	step [232/249], loss=100.1511
	step [233/249], loss=141.2918
	step [234/249], loss=127.6583
	step [235/249], loss=120.8930
	step [236/249], loss=98.6611
	step [237/249], loss=136.5495
	step [238/249], loss=109.3484
	step [239/249], loss=100.9876
	step [240/249], loss=116.2168
	step [241/249], loss=108.9303
	step [242/249], loss=112.0378
	step [243/249], loss=115.6188
	step [244/249], loss=109.1962
	step [245/249], loss=119.7704
	step [246/249], loss=123.6292
	step [247/249], loss=105.3785
	step [248/249], loss=110.5948
	step [249/249], loss=81.8272
	Evaluating
	loss=0.0404, precision=0.3819, recall=0.9167, f1=0.5392
Training epoch 9
	step [1/249], loss=98.0055
	step [2/249], loss=133.7633
	step [3/249], loss=122.9851
	step [4/249], loss=96.1165
	step [5/249], loss=137.9956
	step [6/249], loss=119.7235
	step [7/249], loss=124.6597
	step [8/249], loss=92.6139
	step [9/249], loss=109.1257
	step [10/249], loss=103.2703
	step [11/249], loss=131.1550
	step [12/249], loss=115.8954
	step [13/249], loss=112.5184
	step [14/249], loss=123.7047
	step [15/249], loss=126.6727
	step [16/249], loss=115.0981
	step [17/249], loss=107.6886
	step [18/249], loss=109.7673
	step [19/249], loss=101.0083
	step [20/249], loss=129.4666
	step [21/249], loss=136.2681
	step [22/249], loss=115.0938
	step [23/249], loss=122.0072
	step [24/249], loss=116.1152
	step [25/249], loss=118.7271
	step [26/249], loss=134.0471
	step [27/249], loss=132.6564
	step [28/249], loss=120.0686
	step [29/249], loss=130.2624
	step [30/249], loss=117.8891
	step [31/249], loss=118.4649
	step [32/249], loss=115.1275
	step [33/249], loss=99.7475
	step [34/249], loss=111.6639
	step [35/249], loss=135.6322
	step [36/249], loss=96.2529
	step [37/249], loss=124.7404
	step [38/249], loss=104.1426
	step [39/249], loss=98.9965
	step [40/249], loss=117.5657
	step [41/249], loss=123.0786
	step [42/249], loss=123.0941
	step [43/249], loss=102.2388
	step [44/249], loss=125.5442
	step [45/249], loss=117.2169
	step [46/249], loss=115.7243
	step [47/249], loss=121.5624
	step [48/249], loss=113.7650
	step [49/249], loss=102.0309
	step [50/249], loss=129.7899
	step [51/249], loss=126.5003
	step [52/249], loss=115.0531
	step [53/249], loss=105.0262
	step [54/249], loss=124.9237
	step [55/249], loss=89.1421
	step [56/249], loss=104.3074
	step [57/249], loss=118.1320
	step [58/249], loss=102.2817
	step [59/249], loss=115.1693
	step [60/249], loss=155.6837
	step [61/249], loss=124.5977
	step [62/249], loss=120.4613
	step [63/249], loss=107.4909
	step [64/249], loss=123.9654
	step [65/249], loss=113.4292
	step [66/249], loss=112.6151
	step [67/249], loss=113.0295
	step [68/249], loss=110.9516
	step [69/249], loss=115.6914
	step [70/249], loss=108.5925
	step [71/249], loss=96.2015
	step [72/249], loss=131.5415
	step [73/249], loss=117.7715
	step [74/249], loss=139.5695
	step [75/249], loss=114.6117
	step [76/249], loss=113.6195
	step [77/249], loss=96.2024
	step [78/249], loss=104.8636
	step [79/249], loss=101.0566
	step [80/249], loss=113.5280
	step [81/249], loss=87.8991
	step [82/249], loss=116.9003
	step [83/249], loss=109.4947
	step [84/249], loss=121.2296
	step [85/249], loss=109.5180
	step [86/249], loss=103.8197
	step [87/249], loss=108.4355
	step [88/249], loss=135.1344
	step [89/249], loss=119.7444
	step [90/249], loss=120.7252
	step [91/249], loss=110.8890
	step [92/249], loss=117.4863
	step [93/249], loss=107.8240
	step [94/249], loss=93.8837
	step [95/249], loss=116.0468
	step [96/249], loss=113.1500
	step [97/249], loss=119.1603
	step [98/249], loss=121.5222
	step [99/249], loss=111.9874
	step [100/249], loss=126.2521
	step [101/249], loss=112.8330
	step [102/249], loss=103.1042
	step [103/249], loss=113.7291
	step [104/249], loss=124.4192
	step [105/249], loss=125.9894
	step [106/249], loss=122.1824
	step [107/249], loss=106.5733
	step [108/249], loss=95.9316
	step [109/249], loss=112.0684
	step [110/249], loss=122.6755
	step [111/249], loss=130.0152
	step [112/249], loss=109.9004
	step [113/249], loss=115.8896
	step [114/249], loss=127.0103
	step [115/249], loss=116.8576
	step [116/249], loss=124.8791
	step [117/249], loss=112.3604
	step [118/249], loss=125.1886
	step [119/249], loss=103.8208
	step [120/249], loss=132.0149
	step [121/249], loss=100.8209
	step [122/249], loss=102.9101
	step [123/249], loss=110.9958
	step [124/249], loss=103.7228
	step [125/249], loss=125.7189
	step [126/249], loss=105.2126
	step [127/249], loss=106.4080
	step [128/249], loss=86.8643
	step [129/249], loss=141.2725
	step [130/249], loss=94.5120
	step [131/249], loss=102.3993
	step [132/249], loss=149.5857
	step [133/249], loss=101.5510
	step [134/249], loss=137.9490
	step [135/249], loss=104.7814
	step [136/249], loss=104.9946
	step [137/249], loss=112.5201
	step [138/249], loss=122.2531
	step [139/249], loss=129.0675
	step [140/249], loss=120.1329
	step [141/249], loss=123.3010
	step [142/249], loss=115.7585
	step [143/249], loss=132.7382
	step [144/249], loss=116.9722
	step [145/249], loss=106.0764
	step [146/249], loss=124.5116
	step [147/249], loss=84.8632
	step [148/249], loss=96.0787
	step [149/249], loss=111.6822
	step [150/249], loss=126.6293
	step [151/249], loss=120.9923
	step [152/249], loss=127.0946
	step [153/249], loss=124.2710
	step [154/249], loss=90.0062
	step [155/249], loss=106.9428
	step [156/249], loss=122.2466
	step [157/249], loss=115.3055
	step [158/249], loss=130.1020
	step [159/249], loss=110.0736
	step [160/249], loss=118.1137
	step [161/249], loss=122.3135
	step [162/249], loss=116.4988
	step [163/249], loss=110.8143
	step [164/249], loss=134.0671
	step [165/249], loss=95.9542
	step [166/249], loss=113.0370
	step [167/249], loss=115.9684
	step [168/249], loss=121.2982
	step [169/249], loss=116.5711
	step [170/249], loss=115.7666
	step [171/249], loss=107.7400
	step [172/249], loss=124.1433
	step [173/249], loss=116.4519
	step [174/249], loss=103.2792
	step [175/249], loss=102.1542
	step [176/249], loss=106.1531
	step [177/249], loss=131.1291
	step [178/249], loss=114.7375
	step [179/249], loss=113.0437
	step [180/249], loss=123.0099
	step [181/249], loss=102.6360
	step [182/249], loss=118.0665
	step [183/249], loss=113.3117
	step [184/249], loss=122.0990
	step [185/249], loss=129.7884
	step [186/249], loss=141.1631
	step [187/249], loss=111.3598
	step [188/249], loss=110.3804
	step [189/249], loss=97.5549
	step [190/249], loss=113.3522
	step [191/249], loss=110.3222
	step [192/249], loss=105.2514
	step [193/249], loss=136.1310
	step [194/249], loss=125.0216
	step [195/249], loss=100.3782
	step [196/249], loss=99.9521
	step [197/249], loss=99.7785
	step [198/249], loss=96.6761
	step [199/249], loss=88.6101
	step [200/249], loss=125.0072
	step [201/249], loss=105.2222
	step [202/249], loss=135.8167
	step [203/249], loss=117.1895
	step [204/249], loss=100.9785
	step [205/249], loss=112.1113
	step [206/249], loss=98.1237
	step [207/249], loss=122.7363
	step [208/249], loss=111.8966
	step [209/249], loss=123.7771
	step [210/249], loss=123.4047
	step [211/249], loss=117.0240
	step [212/249], loss=111.8704
	step [213/249], loss=127.3983
	step [214/249], loss=113.1065
	step [215/249], loss=98.0980
	step [216/249], loss=102.6365
	step [217/249], loss=111.9106
	step [218/249], loss=125.9619
	step [219/249], loss=111.4014
	step [220/249], loss=113.9275
	step [221/249], loss=114.2074
	step [222/249], loss=126.5792
	step [223/249], loss=106.0146
	step [224/249], loss=107.6782
	step [225/249], loss=128.4085
	step [226/249], loss=118.1773
	step [227/249], loss=120.5035
	step [228/249], loss=122.5188
	step [229/249], loss=101.8943
	step [230/249], loss=95.3595
	step [231/249], loss=125.0622
	step [232/249], loss=121.5905
	step [233/249], loss=102.4206
	step [234/249], loss=123.9077
	step [235/249], loss=116.9611
	step [236/249], loss=125.4038
	step [237/249], loss=116.8652
	step [238/249], loss=114.8048
	step [239/249], loss=130.4093
	step [240/249], loss=131.4743
	step [241/249], loss=107.7365
	step [242/249], loss=118.7607
	step [243/249], loss=107.3221
	step [244/249], loss=114.2302
	step [245/249], loss=107.3218
	step [246/249], loss=113.3555
	step [247/249], loss=111.3729
	step [248/249], loss=121.6655
	step [249/249], loss=70.5930
	Evaluating
	loss=0.0326, precision=0.3995, recall=0.9030, f1=0.5539
Training epoch 10
	step [1/249], loss=116.8660
	step [2/249], loss=110.8981
	step [3/249], loss=99.7466
	step [4/249], loss=118.2811
	step [5/249], loss=105.5881
	step [6/249], loss=87.1950
	step [7/249], loss=126.3186
	step [8/249], loss=104.6218
	step [9/249], loss=131.4808
	step [10/249], loss=118.8640
	step [11/249], loss=98.1340
	step [12/249], loss=112.0680
	step [13/249], loss=100.2305
	step [14/249], loss=143.1770
	step [15/249], loss=142.2679
	step [16/249], loss=120.7616
	step [17/249], loss=110.9567
	step [18/249], loss=110.5483
	step [19/249], loss=110.5636
	step [20/249], loss=124.6702
	step [21/249], loss=124.9633
	step [22/249], loss=99.2703
	step [23/249], loss=101.9792
	step [24/249], loss=118.3757
	step [25/249], loss=125.8821
	step [26/249], loss=108.7280
	step [27/249], loss=131.0478
	step [28/249], loss=133.7523
	step [29/249], loss=120.1877
	step [30/249], loss=106.8616
	step [31/249], loss=127.6452
	step [32/249], loss=122.3769
	step [33/249], loss=90.2751
	step [34/249], loss=105.1570
	step [35/249], loss=93.4834
	step [36/249], loss=119.4360
	step [37/249], loss=107.8040
	step [38/249], loss=113.6978
	step [39/249], loss=102.5433
	step [40/249], loss=107.4414
	step [41/249], loss=108.9057
	step [42/249], loss=127.3222
	step [43/249], loss=123.4913
	step [44/249], loss=109.2088
	step [45/249], loss=94.8610
	step [46/249], loss=104.2091
	step [47/249], loss=108.9089
	step [48/249], loss=95.1529
	step [49/249], loss=117.4662
	step [50/249], loss=115.4448
	step [51/249], loss=99.5822
	step [52/249], loss=114.0382
	step [53/249], loss=98.7150
	step [54/249], loss=128.1321
	step [55/249], loss=113.3395
	step [56/249], loss=117.3095
	step [57/249], loss=127.5205
	step [58/249], loss=137.8337
	step [59/249], loss=104.9855
	step [60/249], loss=140.4153
	step [61/249], loss=104.9961
	step [62/249], loss=136.1793
	step [63/249], loss=104.6287
	step [64/249], loss=105.0239
	step [65/249], loss=123.5807
	step [66/249], loss=110.7646
	step [67/249], loss=120.4980
	step [68/249], loss=120.4710
	step [69/249], loss=118.8078
	step [70/249], loss=102.5005
	step [71/249], loss=142.9474
	step [72/249], loss=103.6108
	step [73/249], loss=118.6839
	step [74/249], loss=122.0613
	step [75/249], loss=123.4731
	step [76/249], loss=94.9411
	step [77/249], loss=145.9159
	step [78/249], loss=115.0705
	step [79/249], loss=105.0542
	step [80/249], loss=130.6823
	step [81/249], loss=122.0675
	step [82/249], loss=103.8060
	step [83/249], loss=119.9678
	step [84/249], loss=98.8940
	step [85/249], loss=109.5890
	step [86/249], loss=114.9223
	step [87/249], loss=106.9875
	step [88/249], loss=99.6842
	step [89/249], loss=119.0530
	step [90/249], loss=93.5060
	step [91/249], loss=111.0553
	step [92/249], loss=114.7418
	step [93/249], loss=118.5954
	step [94/249], loss=108.5057
	step [95/249], loss=101.5130
	step [96/249], loss=98.1968
	step [97/249], loss=121.1763
	step [98/249], loss=110.3475
	step [99/249], loss=122.7836
	step [100/249], loss=121.7010
	step [101/249], loss=103.2853
	step [102/249], loss=116.9875
	step [103/249], loss=126.4159
	step [104/249], loss=103.2229
	step [105/249], loss=106.0965
	step [106/249], loss=119.7909
	step [107/249], loss=108.2526
	step [108/249], loss=116.5573
	step [109/249], loss=119.6226
	step [110/249], loss=109.7701
	step [111/249], loss=117.6257
	step [112/249], loss=137.3251
	step [113/249], loss=107.6357
	step [114/249], loss=106.5708
	step [115/249], loss=111.6030
	step [116/249], loss=109.5901
	step [117/249], loss=115.3648
	step [118/249], loss=130.8157
	step [119/249], loss=108.5709
	step [120/249], loss=109.1027
	step [121/249], loss=129.2073
	step [122/249], loss=111.2380
	step [123/249], loss=109.8649
	step [124/249], loss=94.8076
	step [125/249], loss=131.3786
	step [126/249], loss=99.5182
	step [127/249], loss=125.5452
	step [128/249], loss=115.4830
	step [129/249], loss=111.1591
	step [130/249], loss=107.9709
	step [131/249], loss=101.2788
	step [132/249], loss=138.5725
	step [133/249], loss=120.4358
	step [134/249], loss=95.2654
	step [135/249], loss=134.9104
	step [136/249], loss=103.2876
	step [137/249], loss=145.7477
	step [138/249], loss=108.9882
	step [139/249], loss=105.9812
	step [140/249], loss=126.9270
	step [141/249], loss=111.8901
	step [142/249], loss=114.0185
	step [143/249], loss=99.7599
	step [144/249], loss=95.8229
	step [145/249], loss=109.8559
	step [146/249], loss=107.6039
	step [147/249], loss=111.4547
	step [148/249], loss=119.8664
	step [149/249], loss=94.1341
	step [150/249], loss=140.9267
	step [151/249], loss=97.5967
	step [152/249], loss=121.2077
	step [153/249], loss=100.4029
	step [154/249], loss=128.7020
	step [155/249], loss=102.7182
	step [156/249], loss=93.6451
	step [157/249], loss=116.8633
	step [158/249], loss=137.3059
	step [159/249], loss=103.7928
	step [160/249], loss=116.8961
	step [161/249], loss=109.0580
	step [162/249], loss=124.0601
	step [163/249], loss=99.9050
	step [164/249], loss=107.3993
	step [165/249], loss=105.0425
	step [166/249], loss=123.3187
	step [167/249], loss=96.5463
	step [168/249], loss=111.9825
	step [169/249], loss=111.1178
	step [170/249], loss=110.1246
	step [171/249], loss=96.5260
	step [172/249], loss=113.2679
	step [173/249], loss=107.5920
	step [174/249], loss=99.1045
	step [175/249], loss=130.5999
	step [176/249], loss=113.7423
	step [177/249], loss=117.9425
	step [178/249], loss=108.6776
	step [179/249], loss=118.6200
	step [180/249], loss=121.1096
	step [181/249], loss=118.6112
	step [182/249], loss=137.2635
	step [183/249], loss=114.3707
	step [184/249], loss=118.4721
	step [185/249], loss=97.0675
	step [186/249], loss=102.8118
	step [187/249], loss=79.3367
	step [188/249], loss=118.2992
	step [189/249], loss=129.5138
	step [190/249], loss=105.7581
	step [191/249], loss=117.4709
	step [192/249], loss=138.3610
	step [193/249], loss=123.6389
	step [194/249], loss=110.1850
	step [195/249], loss=115.3127
	step [196/249], loss=110.2584
	step [197/249], loss=128.9871
	step [198/249], loss=117.1404
	step [199/249], loss=116.8806
	step [200/249], loss=105.8465
	step [201/249], loss=101.5679
	step [202/249], loss=95.2491
	step [203/249], loss=132.7814
	step [204/249], loss=107.7059
	step [205/249], loss=121.2031
	step [206/249], loss=117.5860
	step [207/249], loss=112.7350
	step [208/249], loss=114.7174
	step [209/249], loss=100.2011
	step [210/249], loss=119.6045
	step [211/249], loss=113.5864
	step [212/249], loss=117.9342
	step [213/249], loss=122.6523
	step [214/249], loss=108.1519
	step [215/249], loss=117.5258
	step [216/249], loss=98.6077
	step [217/249], loss=118.5753
	step [218/249], loss=141.5213
	step [219/249], loss=111.1414
	step [220/249], loss=125.0597
	step [221/249], loss=119.6263
	step [222/249], loss=110.7741
	step [223/249], loss=115.6792
	step [224/249], loss=100.7671
	step [225/249], loss=100.3809
	step [226/249], loss=126.9450
	step [227/249], loss=101.6726
	step [228/249], loss=98.0459
	step [229/249], loss=125.9549
	step [230/249], loss=119.0601
	step [231/249], loss=109.5779
	step [232/249], loss=105.8414
	step [233/249], loss=112.5213
	step [234/249], loss=110.1546
	step [235/249], loss=98.2593
	step [236/249], loss=95.4479
	step [237/249], loss=124.7940
	step [238/249], loss=111.2599
	step [239/249], loss=124.0647
	step [240/249], loss=111.8171
	step [241/249], loss=111.4991
	step [242/249], loss=114.9186
	step [243/249], loss=105.2248
	step [244/249], loss=90.7319
	step [245/249], loss=122.4416
	step [246/249], loss=106.8227
	step [247/249], loss=92.0677
	step [248/249], loss=100.9945
	step [249/249], loss=54.9367
	Evaluating
	loss=0.0270, precision=0.4159, recall=0.9401, f1=0.5766
Training epoch 11
	step [1/249], loss=108.3498
	step [2/249], loss=109.9192
	step [3/249], loss=123.5898
	step [4/249], loss=120.0281
	step [5/249], loss=122.1677
	step [6/249], loss=104.2947
	step [7/249], loss=111.5233
	step [8/249], loss=129.8382
	step [9/249], loss=125.1702
	step [10/249], loss=116.5047
	step [11/249], loss=105.2775
	step [12/249], loss=107.0008
	step [13/249], loss=113.0276
	step [14/249], loss=101.4735
	step [15/249], loss=116.6474
	step [16/249], loss=126.2769
	step [17/249], loss=118.9480
	step [18/249], loss=92.4159
	step [19/249], loss=92.1344
	step [20/249], loss=129.2780
	step [21/249], loss=92.6013
	step [22/249], loss=121.2592
	step [23/249], loss=126.4484
	step [24/249], loss=104.9043
	step [25/249], loss=106.7180
	step [26/249], loss=107.0592
	step [27/249], loss=105.0131
	step [28/249], loss=123.9998
	step [29/249], loss=111.9171
	step [30/249], loss=96.2117
	step [31/249], loss=124.8477
	step [32/249], loss=112.0530
	step [33/249], loss=94.0234
	step [34/249], loss=103.2935
	step [35/249], loss=95.0307
	step [36/249], loss=105.9785
	step [37/249], loss=128.9914
	step [38/249], loss=120.2458
	step [39/249], loss=129.8773
	step [40/249], loss=108.4660
	step [41/249], loss=129.4788
	step [42/249], loss=100.3309
	step [43/249], loss=111.9018
	step [44/249], loss=92.0492
	step [45/249], loss=109.8476
	step [46/249], loss=102.9485
	step [47/249], loss=111.7014
	step [48/249], loss=111.1377
	step [49/249], loss=98.6257
	step [50/249], loss=131.0370
	step [51/249], loss=111.7687
	step [52/249], loss=112.7147
	step [53/249], loss=94.6853
	step [54/249], loss=93.8009
	step [55/249], loss=102.4408
	step [56/249], loss=94.6162
	step [57/249], loss=116.7842
	step [58/249], loss=92.4678
	step [59/249], loss=137.5481
	step [60/249], loss=133.3069
	step [61/249], loss=123.0963
	step [62/249], loss=111.6303
	step [63/249], loss=112.8831
	step [64/249], loss=117.1781
	step [65/249], loss=114.5230
	step [66/249], loss=126.0733
	step [67/249], loss=97.0222
	step [68/249], loss=116.3317
	step [69/249], loss=110.5096
	step [70/249], loss=88.0410
	step [71/249], loss=99.4710
	step [72/249], loss=122.2266
	step [73/249], loss=113.7957
	step [74/249], loss=121.8256
	step [75/249], loss=91.6337
	step [76/249], loss=116.8918
	step [77/249], loss=122.7099
	step [78/249], loss=126.1869
	step [79/249], loss=130.3279
	step [80/249], loss=105.0086
	step [81/249], loss=118.2726
	step [82/249], loss=128.5364
	step [83/249], loss=117.3710
	step [84/249], loss=107.0204
	step [85/249], loss=100.0363
	step [86/249], loss=116.7995
	step [87/249], loss=127.4031
	step [88/249], loss=134.3172
	step [89/249], loss=128.6166
	step [90/249], loss=90.6107
	step [91/249], loss=116.4851
	step [92/249], loss=100.5742
	step [93/249], loss=109.1013
	step [94/249], loss=79.1132
	step [95/249], loss=138.8436
	step [96/249], loss=117.7705
	step [97/249], loss=122.7764
	step [98/249], loss=83.3174
	step [99/249], loss=110.3443
	step [100/249], loss=114.0496
	step [101/249], loss=139.6369
	step [102/249], loss=95.5150
	step [103/249], loss=108.9604
	step [104/249], loss=118.8825
	step [105/249], loss=117.5960
	step [106/249], loss=127.6283
	step [107/249], loss=109.8408
	step [108/249], loss=117.5551
	step [109/249], loss=113.4880
	step [110/249], loss=123.1800
	step [111/249], loss=130.1295
	step [112/249], loss=108.5673
	step [113/249], loss=119.4002
	step [114/249], loss=117.8474
	step [115/249], loss=99.5266
	step [116/249], loss=130.9095
	step [117/249], loss=91.1202
	step [118/249], loss=111.3837
	step [119/249], loss=91.0768
	step [120/249], loss=127.9645
	step [121/249], loss=97.2023
	step [122/249], loss=109.6160
	step [123/249], loss=95.0424
	step [124/249], loss=101.2091
	step [125/249], loss=115.8592
	step [126/249], loss=104.9432
	step [127/249], loss=146.0616
	step [128/249], loss=127.6421
	step [129/249], loss=132.2780
	step [130/249], loss=90.5081
	step [131/249], loss=113.9160
	step [132/249], loss=114.3420
	step [133/249], loss=104.3362
	step [134/249], loss=90.5392
	step [135/249], loss=112.9462
	step [136/249], loss=105.8632
	step [137/249], loss=131.3307
	step [138/249], loss=109.8570
	step [139/249], loss=108.1660
	step [140/249], loss=114.5332
	step [141/249], loss=123.7515
	step [142/249], loss=115.4951
	step [143/249], loss=95.5865
	step [144/249], loss=110.8250
	step [145/249], loss=132.1033
	step [146/249], loss=86.4118
	step [147/249], loss=89.6151
	step [148/249], loss=113.5788
	step [149/249], loss=107.6845
	step [150/249], loss=116.7433
	step [151/249], loss=115.4079
	step [152/249], loss=129.5778
	step [153/249], loss=127.2576
	step [154/249], loss=134.1375
	step [155/249], loss=107.0187
	step [156/249], loss=110.5465
	step [157/249], loss=129.0138
	step [158/249], loss=95.9073
	step [159/249], loss=90.3465
	step [160/249], loss=111.2529
	step [161/249], loss=111.7553
	step [162/249], loss=112.8520
	step [163/249], loss=99.0629
	step [164/249], loss=109.0661
	step [165/249], loss=135.1044
	step [166/249], loss=104.5794
	step [167/249], loss=103.9975
	step [168/249], loss=111.5439
	step [169/249], loss=114.7947
	step [170/249], loss=126.4379
	step [171/249], loss=89.3095
	step [172/249], loss=117.6813
	step [173/249], loss=113.3662
	step [174/249], loss=120.6825
	step [175/249], loss=137.9634
	step [176/249], loss=98.9264
	step [177/249], loss=119.3523
	step [178/249], loss=111.7669
	step [179/249], loss=118.7684
	step [180/249], loss=107.0367
	step [181/249], loss=84.3334
	step [182/249], loss=108.8955
	step [183/249], loss=91.5378
	step [184/249], loss=121.6451
	step [185/249], loss=109.9989
	step [186/249], loss=100.9942
	step [187/249], loss=126.6268
	step [188/249], loss=88.1590
	step [189/249], loss=105.0907
	step [190/249], loss=104.2119
	step [191/249], loss=113.7240
	step [192/249], loss=131.5644
	step [193/249], loss=112.1860
	step [194/249], loss=93.4433
	step [195/249], loss=119.2315
	step [196/249], loss=89.6711
	step [197/249], loss=104.0514
	step [198/249], loss=121.6840
	step [199/249], loss=90.4735
	step [200/249], loss=124.5043
	step [201/249], loss=102.5426
	step [202/249], loss=117.6601
	step [203/249], loss=99.2196
	step [204/249], loss=115.7339
	step [205/249], loss=121.1294
	step [206/249], loss=89.7735
	step [207/249], loss=105.9644
	step [208/249], loss=110.7902
	step [209/249], loss=103.7438
	step [210/249], loss=109.3059
	step [211/249], loss=116.9612
	step [212/249], loss=123.3482
	step [213/249], loss=104.1357
	step [214/249], loss=107.0019
	step [215/249], loss=117.7316
	step [216/249], loss=108.7749
	step [217/249], loss=100.5687
	step [218/249], loss=98.8192
	step [219/249], loss=127.6135
	step [220/249], loss=100.0921
	step [221/249], loss=106.4210
	step [222/249], loss=111.1257
	step [223/249], loss=105.5245
	step [224/249], loss=114.5199
	step [225/249], loss=96.6299
	step [226/249], loss=110.3372
	step [227/249], loss=113.3946
	step [228/249], loss=113.8181
	step [229/249], loss=109.0099
	step [230/249], loss=100.0028
	step [231/249], loss=111.4293
	step [232/249], loss=114.4165
	step [233/249], loss=102.8748
	step [234/249], loss=104.9926
	step [235/249], loss=108.9690
	step [236/249], loss=131.2118
	step [237/249], loss=105.4877
	step [238/249], loss=103.7188
	step [239/249], loss=92.7599
	step [240/249], loss=119.0289
	step [241/249], loss=132.1785
	step [242/249], loss=98.9212
	step [243/249], loss=86.8159
	step [244/249], loss=114.6336
	step [245/249], loss=95.6245
	step [246/249], loss=111.3222
	step [247/249], loss=102.9353
	step [248/249], loss=125.4778
	step [249/249], loss=90.9108
	Evaluating
	loss=0.0250, precision=0.3728, recall=0.9238, f1=0.5312
Training epoch 12
	step [1/249], loss=106.2505
	step [2/249], loss=119.3800
	step [3/249], loss=117.3155
	step [4/249], loss=111.4202
	step [5/249], loss=117.3163
	step [6/249], loss=114.2864
	step [7/249], loss=78.6166
	step [8/249], loss=82.4369
	step [9/249], loss=110.9956
	step [10/249], loss=127.4092
	step [11/249], loss=98.0579
	step [12/249], loss=133.8866
	step [13/249], loss=126.0048
	step [14/249], loss=118.5412
	step [15/249], loss=131.3684
	step [16/249], loss=100.2178
	step [17/249], loss=108.6054
	step [18/249], loss=107.0158
	step [19/249], loss=122.0560
	step [20/249], loss=114.8302
	step [21/249], loss=134.8452
	step [22/249], loss=105.6016
	step [23/249], loss=109.3148
	step [24/249], loss=119.3850
	step [25/249], loss=109.6369
	step [26/249], loss=85.4713
	step [27/249], loss=94.9055
	step [28/249], loss=98.0097
	step [29/249], loss=97.0263
	step [30/249], loss=99.6723
	step [31/249], loss=95.3792
	step [32/249], loss=127.4419
	step [33/249], loss=112.2640
	step [34/249], loss=111.2856
	step [35/249], loss=108.6731
	step [36/249], loss=104.7982
	step [37/249], loss=97.5928
	step [38/249], loss=120.7462
	step [39/249], loss=110.4497
	step [40/249], loss=112.3730
	step [41/249], loss=117.3198
	step [42/249], loss=101.7169
	step [43/249], loss=135.3780
	step [44/249], loss=123.6702
	step [45/249], loss=108.9582
	step [46/249], loss=122.4146
	step [47/249], loss=123.3890
	step [48/249], loss=111.2350
	step [49/249], loss=113.9966
	step [50/249], loss=83.1886
	step [51/249], loss=106.7227
	step [52/249], loss=111.0708
	step [53/249], loss=93.3639
	step [54/249], loss=102.8908
	step [55/249], loss=130.3323
	step [56/249], loss=106.7587
	step [57/249], loss=104.1483
	step [58/249], loss=97.7605
	step [59/249], loss=108.8589
	step [60/249], loss=118.6431
	step [61/249], loss=111.9162
	step [62/249], loss=116.1196
	step [63/249], loss=103.7455
	step [64/249], loss=100.9177
	step [65/249], loss=90.0783
	step [66/249], loss=111.2740
	step [67/249], loss=101.8981
	step [68/249], loss=97.0493
	step [69/249], loss=114.7220
	step [70/249], loss=101.2036
	step [71/249], loss=106.3982
	step [72/249], loss=95.5184
	step [73/249], loss=115.4992
	step [74/249], loss=119.7381
	step [75/249], loss=114.6976
	step [76/249], loss=108.6238
	step [77/249], loss=94.2573
	step [78/249], loss=99.7637
	step [79/249], loss=122.5176
	step [80/249], loss=110.2988
	step [81/249], loss=106.6923
	step [82/249], loss=98.3405
	step [83/249], loss=125.1487
	step [84/249], loss=112.4244
	step [85/249], loss=108.9207
	step [86/249], loss=129.7191
	step [87/249], loss=130.8024
	step [88/249], loss=105.4688
	step [89/249], loss=115.3330
	step [90/249], loss=120.3852
	step [91/249], loss=111.8499
	step [92/249], loss=100.8964
	step [93/249], loss=111.6322
	step [94/249], loss=112.1313
	step [95/249], loss=101.8612
	step [96/249], loss=105.6662
	step [97/249], loss=103.2852
	step [98/249], loss=115.1599
	step [99/249], loss=126.0300
	step [100/249], loss=113.9039
	step [101/249], loss=126.3351
	step [102/249], loss=117.5604
	step [103/249], loss=124.8352
	step [104/249], loss=95.6253
	step [105/249], loss=101.0190
	step [106/249], loss=117.3780
	step [107/249], loss=95.9388
	step [108/249], loss=99.3755
	step [109/249], loss=98.4132
	step [110/249], loss=114.9224
	step [111/249], loss=132.6965
	step [112/249], loss=123.1892
	step [113/249], loss=124.9279
	step [114/249], loss=111.0152
	step [115/249], loss=109.2493
	step [116/249], loss=83.6426
	step [117/249], loss=113.5302
	step [118/249], loss=106.5142
	step [119/249], loss=96.9407
	step [120/249], loss=96.9010
	step [121/249], loss=85.7834
	step [122/249], loss=92.9551
	step [123/249], loss=121.2748
	step [124/249], loss=92.9260
	step [125/249], loss=101.6758
	step [126/249], loss=108.7716
	step [127/249], loss=99.1612
	step [128/249], loss=101.8951
	step [129/249], loss=98.4605
	step [130/249], loss=113.3753
	step [131/249], loss=109.7144
	step [132/249], loss=118.8244
	step [133/249], loss=115.9965
	step [134/249], loss=105.4001
	step [135/249], loss=105.7897
	step [136/249], loss=114.0661
	step [137/249], loss=124.2544
	step [138/249], loss=101.6308
	step [139/249], loss=119.3994
	step [140/249], loss=124.2130
	step [141/249], loss=97.5321
	step [142/249], loss=121.0751
	step [143/249], loss=104.4307
	step [144/249], loss=123.5630
	step [145/249], loss=89.4117
	step [146/249], loss=96.7962
	step [147/249], loss=116.1373
	step [148/249], loss=127.8411
	step [149/249], loss=117.0781
	step [150/249], loss=121.4327
	step [151/249], loss=128.5311
	step [152/249], loss=111.0982
	step [153/249], loss=106.0924
	step [154/249], loss=109.5674
	step [155/249], loss=101.5286
	step [156/249], loss=107.0721
	step [157/249], loss=119.8547
	step [158/249], loss=124.8036
	step [159/249], loss=126.0543
	step [160/249], loss=125.6976
	step [161/249], loss=107.8270
	step [162/249], loss=110.6690
	step [163/249], loss=93.9500
	step [164/249], loss=107.9263
	step [165/249], loss=105.8573
	step [166/249], loss=105.5054
	step [167/249], loss=105.9524
	step [168/249], loss=99.4306
	step [169/249], loss=88.8288
	step [170/249], loss=123.4650
	step [171/249], loss=108.3110
	step [172/249], loss=105.6768
	step [173/249], loss=126.0576
	step [174/249], loss=104.9083
	step [175/249], loss=101.7204
	step [176/249], loss=112.3539
	step [177/249], loss=112.1554
	step [178/249], loss=105.2032
	step [179/249], loss=83.7206
	step [180/249], loss=129.5794
	step [181/249], loss=84.8828
	step [182/249], loss=85.4662
	step [183/249], loss=108.0203
	step [184/249], loss=113.8786
	step [185/249], loss=109.5053
	step [186/249], loss=115.0672
	step [187/249], loss=98.3983
	step [188/249], loss=124.8278
	step [189/249], loss=112.3077
	step [190/249], loss=113.9060
	step [191/249], loss=107.5306
	step [192/249], loss=97.4700
	step [193/249], loss=97.2308
	step [194/249], loss=107.7832
	step [195/249], loss=91.3268
	step [196/249], loss=132.6649
	step [197/249], loss=111.5398
	step [198/249], loss=91.5694
	step [199/249], loss=107.1998
	step [200/249], loss=127.4497
	step [201/249], loss=98.2366
	step [202/249], loss=105.8654
	step [203/249], loss=111.6570
	step [204/249], loss=89.3237
	step [205/249], loss=133.1383
	step [206/249], loss=101.1165
	step [207/249], loss=100.3084
	step [208/249], loss=101.9561
	step [209/249], loss=116.2879
	step [210/249], loss=87.0909
	step [211/249], loss=106.5231
	step [212/249], loss=111.5599
	step [213/249], loss=108.4320
	step [214/249], loss=109.3840
	step [215/249], loss=106.4336
	step [216/249], loss=107.3451
	step [217/249], loss=114.7019
	step [218/249], loss=119.4795
	step [219/249], loss=125.4785
	step [220/249], loss=107.0904
	step [221/249], loss=103.3410
	step [222/249], loss=111.4045
	step [223/249], loss=109.4229
	step [224/249], loss=95.8054
	step [225/249], loss=96.3787
	step [226/249], loss=117.0658
	step [227/249], loss=123.9888
	step [228/249], loss=120.2052
	step [229/249], loss=128.8407
	step [230/249], loss=100.3560
	step [231/249], loss=106.2670
	step [232/249], loss=119.0333
	step [233/249], loss=94.3786
	step [234/249], loss=97.6181
	step [235/249], loss=115.8569
	step [236/249], loss=113.9087
	step [237/249], loss=120.6322
	step [238/249], loss=118.9599
	step [239/249], loss=105.7858
	step [240/249], loss=116.4284
	step [241/249], loss=89.2573
	step [242/249], loss=114.7368
	step [243/249], loss=127.7145
	step [244/249], loss=132.5509
	step [245/249], loss=113.9204
	step [246/249], loss=99.9056
	step [247/249], loss=122.1977
	step [248/249], loss=113.0931
	step [249/249], loss=73.0209
	Evaluating
	loss=0.0220, precision=0.4084, recall=0.8916, f1=0.5602
Training epoch 13
	step [1/249], loss=97.4355
	step [2/249], loss=129.9632
	step [3/249], loss=91.9825
	step [4/249], loss=88.2892
	step [5/249], loss=93.2246
	step [6/249], loss=88.9772
	step [7/249], loss=94.5748
	step [8/249], loss=133.3170
	step [9/249], loss=115.0717
	step [10/249], loss=112.2309
	step [11/249], loss=106.8728
	step [12/249], loss=94.9845
	step [13/249], loss=121.8000
	step [14/249], loss=90.4578
	step [15/249], loss=102.6253
	step [16/249], loss=102.8112
	step [17/249], loss=94.4467
	step [18/249], loss=90.7940
	step [19/249], loss=112.2370
	step [20/249], loss=124.7522
	step [21/249], loss=108.6812
	step [22/249], loss=96.3336
	step [23/249], loss=106.2098
	step [24/249], loss=106.4955
	step [25/249], loss=110.9451
	step [26/249], loss=105.8080
	step [27/249], loss=104.1681
	step [28/249], loss=101.1813
	step [29/249], loss=111.6626
	step [30/249], loss=120.2144
	step [31/249], loss=88.7214
	step [32/249], loss=104.3540
	step [33/249], loss=103.8310
	step [34/249], loss=102.7089
	step [35/249], loss=100.8052
	step [36/249], loss=112.4247
	step [37/249], loss=110.8199
	step [38/249], loss=120.6473
	step [39/249], loss=110.0714
	step [40/249], loss=96.9436
	step [41/249], loss=93.4573
	step [42/249], loss=106.8336
	step [43/249], loss=134.1932
	step [44/249], loss=122.0720
	step [45/249], loss=86.3593
	step [46/249], loss=96.7538
	step [47/249], loss=109.9514
	step [48/249], loss=105.0103
	step [49/249], loss=106.7091
	step [50/249], loss=96.8721
	step [51/249], loss=113.9485
	step [52/249], loss=112.8879
	step [53/249], loss=108.3886
	step [54/249], loss=114.3084
	step [55/249], loss=120.4481
	step [56/249], loss=139.3797
	step [57/249], loss=114.4587
	step [58/249], loss=108.4366
	step [59/249], loss=98.1552
	step [60/249], loss=108.8829
	step [61/249], loss=108.2507
	step [62/249], loss=121.7108
	step [63/249], loss=108.9018
	step [64/249], loss=112.3843
	step [65/249], loss=107.2927
	step [66/249], loss=120.9273
	step [67/249], loss=106.2952
	step [68/249], loss=96.8067
	step [69/249], loss=106.9832
	step [70/249], loss=100.1769
	step [71/249], loss=97.0911
	step [72/249], loss=112.2627
	step [73/249], loss=115.4500
	step [74/249], loss=124.1510
	step [75/249], loss=112.7515
	step [76/249], loss=116.7662
	step [77/249], loss=106.1821
	step [78/249], loss=111.3466
	step [79/249], loss=103.6033
	step [80/249], loss=110.4485
	step [81/249], loss=103.4850
	step [82/249], loss=117.8806
	step [83/249], loss=115.6229
	step [84/249], loss=115.5719
	step [85/249], loss=92.6198
	step [86/249], loss=103.1605
	step [87/249], loss=119.2426
	step [88/249], loss=110.8712
	step [89/249], loss=109.1010
	step [90/249], loss=122.9759
	step [91/249], loss=125.4505
	step [92/249], loss=109.5412
	step [93/249], loss=86.7905
	step [94/249], loss=119.9683
	step [95/249], loss=127.3413
	step [96/249], loss=92.7004
	step [97/249], loss=111.4044
	step [98/249], loss=119.1375
	step [99/249], loss=114.9198
	step [100/249], loss=104.8353
	step [101/249], loss=113.1828
	step [102/249], loss=100.4326
	step [103/249], loss=112.6814
	step [104/249], loss=116.1765
	step [105/249], loss=117.7665
	step [106/249], loss=96.6967
	step [107/249], loss=111.6751
	step [108/249], loss=106.2230
	step [109/249], loss=104.7059
	step [110/249], loss=102.7511
	step [111/249], loss=99.1730
	step [112/249], loss=111.1378
	step [113/249], loss=112.9566
	step [114/249], loss=122.5997
	step [115/249], loss=118.7614
	step [116/249], loss=93.1173
	step [117/249], loss=107.3829
	step [118/249], loss=111.7959
	step [119/249], loss=123.0180
	step [120/249], loss=82.1947
	step [121/249], loss=129.2677
	step [122/249], loss=104.3492
	step [123/249], loss=119.4548
	step [124/249], loss=101.8486
	step [125/249], loss=97.3989
	step [126/249], loss=85.2110
	step [127/249], loss=87.5209
	step [128/249], loss=118.9652
	step [129/249], loss=112.1670
	step [130/249], loss=101.9745
	step [131/249], loss=139.7457
	step [132/249], loss=123.1720
	step [133/249], loss=97.2886
	step [134/249], loss=95.6960
	step [135/249], loss=94.4762
	step [136/249], loss=98.3366
	step [137/249], loss=123.8457
	step [138/249], loss=116.3111
	step [139/249], loss=80.7138
	step [140/249], loss=111.2486
	step [141/249], loss=108.7660
	step [142/249], loss=92.7773
	step [143/249], loss=99.1750
	step [144/249], loss=104.3141
	step [145/249], loss=111.1270
	step [146/249], loss=117.3750
	step [147/249], loss=122.1146
	step [148/249], loss=115.1246
	step [149/249], loss=100.6484
	step [150/249], loss=90.9521
	step [151/249], loss=83.4062
	step [152/249], loss=89.0783
	step [153/249], loss=109.9804
	step [154/249], loss=113.4059
	step [155/249], loss=107.7215
	step [156/249], loss=95.6078
	step [157/249], loss=107.4503
	step [158/249], loss=92.5093
	step [159/249], loss=114.8014
	step [160/249], loss=107.7367
	step [161/249], loss=87.2320
	step [162/249], loss=105.5156
	step [163/249], loss=118.2815
	step [164/249], loss=107.0493
	step [165/249], loss=98.1326
	step [166/249], loss=104.1902
	step [167/249], loss=112.0514
	step [168/249], loss=108.5607
	step [169/249], loss=119.5680
	step [170/249], loss=112.6857
	step [171/249], loss=92.3386
	step [172/249], loss=120.5797
	step [173/249], loss=125.1871
	step [174/249], loss=116.4282
	step [175/249], loss=127.9620
	step [176/249], loss=113.9160
	step [177/249], loss=101.9974
	step [178/249], loss=117.1480
	step [179/249], loss=90.9042
	step [180/249], loss=106.0730
	step [181/249], loss=122.8018
	step [182/249], loss=107.3647
	step [183/249], loss=112.1205
	step [184/249], loss=99.5207
	step [185/249], loss=120.0480
	step [186/249], loss=105.7282
	step [187/249], loss=102.0736
	step [188/249], loss=97.4669
	step [189/249], loss=91.3224
	step [190/249], loss=104.3765
	step [191/249], loss=117.9990
	step [192/249], loss=96.2443
	step [193/249], loss=102.3831
	step [194/249], loss=121.2645
	step [195/249], loss=84.8065
	step [196/249], loss=108.9149
	step [197/249], loss=117.5572
	step [198/249], loss=112.5228
	step [199/249], loss=114.9354
	step [200/249], loss=128.9726
	step [201/249], loss=110.1668
	step [202/249], loss=94.4599
	step [203/249], loss=131.2021
	step [204/249], loss=123.8913
	step [205/249], loss=113.2820
	step [206/249], loss=121.3997
	step [207/249], loss=98.7060
	step [208/249], loss=106.1263
	step [209/249], loss=104.9301
	step [210/249], loss=98.0603
	step [211/249], loss=112.7090
	step [212/249], loss=109.0723
	step [213/249], loss=113.0597
	step [214/249], loss=122.1645
	step [215/249], loss=99.4979
	step [216/249], loss=85.5360
	step [217/249], loss=110.4123
	step [218/249], loss=111.2062
	step [219/249], loss=110.5685
	step [220/249], loss=115.4102
	step [221/249], loss=108.7830
	step [222/249], loss=94.2676
	step [223/249], loss=120.5508
	step [224/249], loss=95.0180
	step [225/249], loss=122.2661
	step [226/249], loss=115.5226
	step [227/249], loss=112.5087
	step [228/249], loss=91.6133
	step [229/249], loss=96.0610
	step [230/249], loss=100.3584
	step [231/249], loss=112.6239
	step [232/249], loss=103.3041
	step [233/249], loss=103.7326
	step [234/249], loss=108.8346
	step [235/249], loss=112.3656
	step [236/249], loss=99.1206
	step [237/249], loss=96.7777
	step [238/249], loss=134.6329
	step [239/249], loss=115.6916
	step [240/249], loss=104.6259
	step [241/249], loss=120.7631
	step [242/249], loss=91.4673
	step [243/249], loss=130.7996
	step [244/249], loss=122.1552
	step [245/249], loss=96.8081
	step [246/249], loss=100.1097
	step [247/249], loss=104.7834
	step [248/249], loss=106.8109
	step [249/249], loss=73.9623
	Evaluating
	loss=0.0173, precision=0.4230, recall=0.9158, f1=0.5787
Training epoch 14
	step [1/249], loss=137.5992
	step [2/249], loss=125.9815
	step [3/249], loss=109.0094
	step [4/249], loss=103.5198
	step [5/249], loss=110.7216
	step [6/249], loss=120.1623
	step [7/249], loss=108.5755
	step [8/249], loss=91.0807
	step [9/249], loss=99.5424
	step [10/249], loss=103.3940
	step [11/249], loss=108.9333
	step [12/249], loss=97.8229
	step [13/249], loss=101.0718
	step [14/249], loss=113.4519
	step [15/249], loss=112.8958
	step [16/249], loss=89.1168
	step [17/249], loss=109.1536
	step [18/249], loss=95.5953
	step [19/249], loss=109.2595
	step [20/249], loss=115.0990
	step [21/249], loss=103.4648
	step [22/249], loss=101.4815
	step [23/249], loss=117.5327
	step [24/249], loss=99.7448
	step [25/249], loss=112.3416
	step [26/249], loss=118.7155
	step [27/249], loss=105.4817
	step [28/249], loss=92.7326
	step [29/249], loss=92.2093
	step [30/249], loss=104.9136
	step [31/249], loss=94.2748
	step [32/249], loss=117.7602
	step [33/249], loss=106.1321
	step [34/249], loss=107.8084
	step [35/249], loss=89.3474
	step [36/249], loss=111.6372
	step [37/249], loss=116.5249
	step [38/249], loss=118.4851
	step [39/249], loss=89.3717
	step [40/249], loss=91.5388
	step [41/249], loss=108.8773
	step [42/249], loss=97.9551
	step [43/249], loss=114.6032
	step [44/249], loss=110.6338
	step [45/249], loss=109.3196
	step [46/249], loss=89.5901
	step [47/249], loss=119.4582
	step [48/249], loss=114.4738
	step [49/249], loss=121.5642
	step [50/249], loss=101.7512
	step [51/249], loss=117.0544
	step [52/249], loss=118.6532
	step [53/249], loss=106.2026
	step [54/249], loss=101.7344
	step [55/249], loss=95.1084
	step [56/249], loss=116.8622
	step [57/249], loss=112.1707
	step [58/249], loss=112.8663
	step [59/249], loss=97.0105
	step [60/249], loss=111.6381
	step [61/249], loss=113.0748
	step [62/249], loss=94.8777
	step [63/249], loss=95.4233
	step [64/249], loss=104.1181
	step [65/249], loss=105.1936
	step [66/249], loss=90.4850
	step [67/249], loss=117.4071
	step [68/249], loss=113.2495
	step [69/249], loss=110.8312
	step [70/249], loss=81.7453
	step [71/249], loss=92.5490
	step [72/249], loss=87.8989
	step [73/249], loss=100.4133
	step [74/249], loss=105.4063
	step [75/249], loss=118.1740
	step [76/249], loss=102.1715
	step [77/249], loss=105.4209
	step [78/249], loss=97.3701
	step [79/249], loss=113.6209
	step [80/249], loss=107.6230
	step [81/249], loss=91.4928
	step [82/249], loss=104.7406
	step [83/249], loss=103.9239
	step [84/249], loss=101.4445
	step [85/249], loss=112.3506
	step [86/249], loss=99.7202
	step [87/249], loss=100.1614
	step [88/249], loss=120.2172
	step [89/249], loss=142.0358
	step [90/249], loss=102.5221
	step [91/249], loss=92.0962
	step [92/249], loss=125.0111
	step [93/249], loss=102.8753
	step [94/249], loss=112.1832
	step [95/249], loss=108.9395
	step [96/249], loss=93.1967
	step [97/249], loss=105.6018
	step [98/249], loss=119.2080
	step [99/249], loss=90.3617
	step [100/249], loss=117.7086
	step [101/249], loss=106.4317
	step [102/249], loss=101.7744
	step [103/249], loss=92.8362
	step [104/249], loss=98.3914
	step [105/249], loss=116.5314
	step [106/249], loss=103.4426
	step [107/249], loss=111.8704
	step [108/249], loss=126.2598
	step [109/249], loss=118.0778
	step [110/249], loss=103.6907
	step [111/249], loss=119.2384
	step [112/249], loss=110.0522
	step [113/249], loss=105.5745
	step [114/249], loss=135.0216
	step [115/249], loss=101.2632
	step [116/249], loss=86.7648
	step [117/249], loss=97.3870
	step [118/249], loss=102.3732
	step [119/249], loss=119.1242
	step [120/249], loss=98.6423
	step [121/249], loss=113.2358
	step [122/249], loss=115.2344
	step [123/249], loss=119.7322
	step [124/249], loss=118.4297
	step [125/249], loss=102.2861
	step [126/249], loss=120.2596
	step [127/249], loss=102.1662
	step [128/249], loss=122.7246
	step [129/249], loss=104.6819
	step [130/249], loss=78.6386
	step [131/249], loss=71.6195
	step [132/249], loss=98.0092
	step [133/249], loss=108.5990
	step [134/249], loss=113.5280
	step [135/249], loss=105.6129
	step [136/249], loss=109.3658
	step [137/249], loss=103.6734
	step [138/249], loss=95.7116
	step [139/249], loss=104.7142
	step [140/249], loss=118.1777
	step [141/249], loss=101.6519
	step [142/249], loss=92.8345
	step [143/249], loss=111.1430
	step [144/249], loss=110.6925
	step [145/249], loss=111.6768
	step [146/249], loss=99.3290
	step [147/249], loss=93.1410
	step [148/249], loss=94.1530
	step [149/249], loss=105.2015
	step [150/249], loss=91.7659
	step [151/249], loss=104.3282
	step [152/249], loss=112.7830
	step [153/249], loss=118.5306
	step [154/249], loss=96.8425
	step [155/249], loss=102.3003
	step [156/249], loss=110.6188
	step [157/249], loss=94.8768
	step [158/249], loss=105.7073
	step [159/249], loss=86.4431
	step [160/249], loss=118.3955
	step [161/249], loss=114.6989
	step [162/249], loss=108.1632
	step [163/249], loss=118.4456
	step [164/249], loss=98.5439
	step [165/249], loss=109.5976
	step [166/249], loss=124.2633
	step [167/249], loss=115.7014
	step [168/249], loss=106.8034
	step [169/249], loss=123.8849
	step [170/249], loss=101.7086
	step [171/249], loss=97.0508
	step [172/249], loss=126.5564
	step [173/249], loss=97.1311
	step [174/249], loss=70.9621
	step [175/249], loss=95.9302
	step [176/249], loss=137.4204
	step [177/249], loss=108.1786
	step [178/249], loss=122.3650
	step [179/249], loss=107.1310
	step [180/249], loss=103.5549
	step [181/249], loss=108.6572
	step [182/249], loss=120.0999
	step [183/249], loss=96.3645
	step [184/249], loss=119.0192
	step [185/249], loss=104.6182
	step [186/249], loss=106.3545
	step [187/249], loss=118.1406
	step [188/249], loss=137.4681
	step [189/249], loss=118.9355
	step [190/249], loss=109.4789
	step [191/249], loss=125.8622
	step [192/249], loss=110.0541
	step [193/249], loss=108.0156
	step [194/249], loss=105.2107
	step [195/249], loss=99.7800
	step [196/249], loss=108.1492
	step [197/249], loss=99.2210
	step [198/249], loss=119.4377
	step [199/249], loss=123.6643
	step [200/249], loss=110.0130
	step [201/249], loss=96.3649
	step [202/249], loss=98.0130
	step [203/249], loss=104.7321
	step [204/249], loss=96.5111
	step [205/249], loss=109.0260
	step [206/249], loss=104.3656
	step [207/249], loss=97.7776
	step [208/249], loss=95.5680
	step [209/249], loss=112.9023
	step [210/249], loss=108.5550
	step [211/249], loss=102.4563
	step [212/249], loss=113.4644
	step [213/249], loss=107.1345
	step [214/249], loss=100.9980
	step [215/249], loss=114.0586
	step [216/249], loss=111.6742
	step [217/249], loss=115.9833
	step [218/249], loss=118.7342
	step [219/249], loss=111.5338
	step [220/249], loss=110.4930
	step [221/249], loss=81.3204
	step [222/249], loss=95.9955
	step [223/249], loss=106.5841
	step [224/249], loss=102.4401
	step [225/249], loss=119.3919
	step [226/249], loss=113.9175
	step [227/249], loss=107.1265
	step [228/249], loss=125.8979
	step [229/249], loss=86.2501
	step [230/249], loss=114.6406
	step [231/249], loss=106.9015
	step [232/249], loss=86.2528
	step [233/249], loss=90.4647
	step [234/249], loss=96.4361
	step [235/249], loss=100.0597
	step [236/249], loss=105.3490
	step [237/249], loss=100.2458
	step [238/249], loss=105.0330
	step [239/249], loss=83.6752
	step [240/249], loss=105.3750
	step [241/249], loss=119.1702
	step [242/249], loss=112.4673
	step [243/249], loss=94.7332
	step [244/249], loss=104.8542
	step [245/249], loss=102.9231
	step [246/249], loss=111.7376
	step [247/249], loss=104.6858
	step [248/249], loss=113.0932
	step [249/249], loss=84.6842
	Evaluating
	loss=0.0177, precision=0.3528, recall=0.9008, f1=0.5070
Training epoch 15
	step [1/249], loss=112.8950
	step [2/249], loss=118.0585
	step [3/249], loss=107.0708
	step [4/249], loss=121.1327
	step [5/249], loss=108.4320
	step [6/249], loss=97.5531
	step [7/249], loss=111.2750
	step [8/249], loss=111.8623
	step [9/249], loss=96.0554
	step [10/249], loss=105.5559
	step [11/249], loss=104.4630
	step [12/249], loss=94.1619
	step [13/249], loss=105.6392
	step [14/249], loss=104.4015
	step [15/249], loss=118.8480
	step [16/249], loss=122.3331
	step [17/249], loss=100.8673
	step [18/249], loss=113.2864
	step [19/249], loss=110.3280
	step [20/249], loss=117.6225
	step [21/249], loss=96.2022
	step [22/249], loss=92.9931
	step [23/249], loss=104.1822
	step [24/249], loss=108.1326
	step [25/249], loss=106.6705
	step [26/249], loss=95.5126
	step [27/249], loss=101.9789
	step [28/249], loss=107.0362
	step [29/249], loss=125.6339
	step [30/249], loss=116.7857
	step [31/249], loss=83.1930
	step [32/249], loss=115.8505
	step [33/249], loss=134.1845
	step [34/249], loss=93.0457
	step [35/249], loss=122.2712
	step [36/249], loss=90.0250
	step [37/249], loss=110.4686
	step [38/249], loss=100.6097
	step [39/249], loss=90.4739
	step [40/249], loss=105.9479
	step [41/249], loss=100.7540
	step [42/249], loss=129.4529
	step [43/249], loss=97.4988
	step [44/249], loss=92.4422
	step [45/249], loss=113.3302
	step [46/249], loss=110.6419
	step [47/249], loss=91.5388
	step [48/249], loss=114.2078
	step [49/249], loss=102.4676
	step [50/249], loss=121.5466
	step [51/249], loss=95.7499
	step [52/249], loss=99.7291
	step [53/249], loss=105.1023
	step [54/249], loss=107.5848
	step [55/249], loss=105.7980
	step [56/249], loss=104.9402
	step [57/249], loss=99.7289
	step [58/249], loss=98.8257
	step [59/249], loss=97.6652
	step [60/249], loss=111.6224
	step [61/249], loss=117.9978
	step [62/249], loss=102.4024
	step [63/249], loss=113.2323
	step [64/249], loss=97.4671
	step [65/249], loss=98.7276
	step [66/249], loss=118.5268
	step [67/249], loss=90.8573
	step [68/249], loss=96.6359
	step [69/249], loss=103.5729
	step [70/249], loss=96.9940
	step [71/249], loss=91.9916
	step [72/249], loss=93.6519
	step [73/249], loss=108.0296
	step [74/249], loss=100.7827
	step [75/249], loss=95.7007
	step [76/249], loss=108.0106
	step [77/249], loss=125.8807
	step [78/249], loss=83.6035
	step [79/249], loss=106.2016
	step [80/249], loss=116.4811
	step [81/249], loss=101.5834
	step [82/249], loss=106.0726
	step [83/249], loss=127.1155
	step [84/249], loss=94.9447
	step [85/249], loss=114.2495
	step [86/249], loss=89.3751
	step [87/249], loss=115.7010
	step [88/249], loss=126.3314
	step [89/249], loss=96.2487
	step [90/249], loss=100.3799
	step [91/249], loss=84.3575
	step [92/249], loss=102.8250
	step [93/249], loss=96.8380
	step [94/249], loss=126.1075
	step [95/249], loss=112.3922
	step [96/249], loss=125.7144
	step [97/249], loss=95.1896
	step [98/249], loss=106.1685
	step [99/249], loss=93.2147
	step [100/249], loss=105.2223
	step [101/249], loss=112.2156
	step [102/249], loss=91.4564
	step [103/249], loss=101.4350
	step [104/249], loss=124.5702
	step [105/249], loss=105.4819
	step [106/249], loss=111.5037
	step [107/249], loss=107.2906
	step [108/249], loss=107.4057
	step [109/249], loss=100.5681
	step [110/249], loss=101.5361
	step [111/249], loss=95.0679
	step [112/249], loss=97.2414
	step [113/249], loss=119.9808
	step [114/249], loss=94.6276
	step [115/249], loss=92.6481
	step [116/249], loss=126.9751
	step [117/249], loss=97.5787
	step [118/249], loss=116.0971
	step [119/249], loss=102.4311
	step [120/249], loss=102.9570
	step [121/249], loss=100.3326
	step [122/249], loss=93.5836
	step [123/249], loss=105.7252
	step [124/249], loss=92.0540
	step [125/249], loss=108.6232
	step [126/249], loss=96.8777
	step [127/249], loss=100.4736
	step [128/249], loss=112.3362
	step [129/249], loss=96.3590
	step [130/249], loss=114.3194
	step [131/249], loss=96.2474
	step [132/249], loss=105.1618
	step [133/249], loss=116.6553
	step [134/249], loss=100.5778
	step [135/249], loss=107.6971
	step [136/249], loss=92.4492
	step [137/249], loss=97.6361
	step [138/249], loss=118.1655
	step [139/249], loss=102.9320
	step [140/249], loss=108.9857
	step [141/249], loss=93.2549
	step [142/249], loss=103.6605
	step [143/249], loss=83.5376
	step [144/249], loss=115.9036
	step [145/249], loss=94.7136
	step [146/249], loss=95.6641
	step [147/249], loss=105.5909
	step [148/249], loss=111.1653
	step [149/249], loss=90.9862
	step [150/249], loss=113.8386
	step [151/249], loss=86.7610
	step [152/249], loss=111.1168
	step [153/249], loss=121.7410
	step [154/249], loss=87.8217
	step [155/249], loss=107.8999
	step [156/249], loss=115.4857
	step [157/249], loss=103.2228
	step [158/249], loss=114.2886
	step [159/249], loss=100.3359
	step [160/249], loss=89.2934
	step [161/249], loss=103.8362
	step [162/249], loss=131.1566
	step [163/249], loss=92.9184
	step [164/249], loss=109.9100
	step [165/249], loss=108.6425
	step [166/249], loss=106.8873
	step [167/249], loss=120.7015
	step [168/249], loss=112.1234
	step [169/249], loss=99.0544
	step [170/249], loss=107.6399
	step [171/249], loss=97.6449
	step [172/249], loss=85.4968
	step [173/249], loss=126.3564
	step [174/249], loss=107.3100
	step [175/249], loss=92.0083
	step [176/249], loss=96.0425
	step [177/249], loss=114.1858
	step [178/249], loss=91.6496
	step [179/249], loss=101.4122
	step [180/249], loss=106.2951
	step [181/249], loss=103.7932
	step [182/249], loss=126.2898
	step [183/249], loss=91.1609
	step [184/249], loss=94.6260
	step [185/249], loss=94.0407
	step [186/249], loss=116.5655
	step [187/249], loss=117.5873
	step [188/249], loss=109.5804
	step [189/249], loss=89.3719
	step [190/249], loss=118.3749
	step [191/249], loss=114.6952
	step [192/249], loss=125.0747
	step [193/249], loss=122.6950
	step [194/249], loss=110.1038
	step [195/249], loss=83.2144
	step [196/249], loss=101.9799
	step [197/249], loss=95.1190
	step [198/249], loss=96.7457
	step [199/249], loss=120.6358
	step [200/249], loss=114.2804
	step [201/249], loss=110.9449
	step [202/249], loss=106.6778
	step [203/249], loss=109.7031
	step [204/249], loss=92.2030
	step [205/249], loss=96.3853
	step [206/249], loss=108.6900
	step [207/249], loss=87.8078
	step [208/249], loss=98.8004
	step [209/249], loss=104.1102
	step [210/249], loss=100.2433
	step [211/249], loss=116.6737
	step [212/249], loss=115.8718
	step [213/249], loss=86.1582
	step [214/249], loss=111.2118
	step [215/249], loss=98.9206
	step [216/249], loss=112.6946
	step [217/249], loss=93.5112
	step [218/249], loss=132.4650
	step [219/249], loss=88.0724
	step [220/249], loss=122.1914
	step [221/249], loss=97.7777
	step [222/249], loss=103.4114
	step [223/249], loss=113.9935
	step [224/249], loss=109.5435
	step [225/249], loss=96.0460
	step [226/249], loss=97.3435
	step [227/249], loss=91.8294
	step [228/249], loss=100.7612
	step [229/249], loss=98.1884
	step [230/249], loss=97.1192
	step [231/249], loss=101.2275
	step [232/249], loss=117.1772
	step [233/249], loss=97.1716
	step [234/249], loss=91.9266
	step [235/249], loss=114.4514
	step [236/249], loss=100.2230
	step [237/249], loss=100.3769
	step [238/249], loss=85.2831
	step [239/249], loss=99.0235
	step [240/249], loss=133.4349
	step [241/249], loss=113.5277
	step [242/249], loss=108.6171
	step [243/249], loss=106.9073
	step [244/249], loss=98.2226
	step [245/249], loss=116.1920
	step [246/249], loss=104.8819
	step [247/249], loss=117.2637
	step [248/249], loss=103.3153
	step [249/249], loss=70.5710
	Evaluating
	loss=0.0153, precision=0.3553, recall=0.9148, f1=0.5118
Training epoch 16
	step [1/249], loss=95.9547
	step [2/249], loss=99.3923
	step [3/249], loss=102.2255
	step [4/249], loss=101.6440
	step [5/249], loss=101.3734
	step [6/249], loss=120.2096
	step [7/249], loss=108.8273
	step [8/249], loss=101.2593
	step [9/249], loss=105.1616
	step [10/249], loss=120.7019
	step [11/249], loss=108.6203
	step [12/249], loss=116.2175
	step [13/249], loss=110.8086
	step [14/249], loss=86.2452
	step [15/249], loss=108.5195
	step [16/249], loss=114.7638
	step [17/249], loss=113.7054
	step [18/249], loss=90.2693
	step [19/249], loss=100.0293
	step [20/249], loss=95.6116
	step [21/249], loss=102.8652
	step [22/249], loss=114.9949
	step [23/249], loss=112.3581
	step [24/249], loss=97.3520
	step [25/249], loss=111.6811
	step [26/249], loss=116.2165
	step [27/249], loss=113.3009
	step [28/249], loss=105.6648
	step [29/249], loss=87.2000
	step [30/249], loss=111.1691
	step [31/249], loss=96.0869
	step [32/249], loss=104.0079
	step [33/249], loss=110.1591
	step [34/249], loss=96.1366
	step [35/249], loss=106.7590
	step [36/249], loss=95.2074
	step [37/249], loss=113.3682
	step [38/249], loss=119.7788
	step [39/249], loss=102.2892
	step [40/249], loss=94.9063
	step [41/249], loss=110.1913
	step [42/249], loss=100.8275
	step [43/249], loss=106.2488
	step [44/249], loss=91.8386
	step [45/249], loss=95.9151
	step [46/249], loss=108.8632
	step [47/249], loss=110.6647
	step [48/249], loss=110.1993
	step [49/249], loss=127.5049
	step [50/249], loss=95.6495
	step [51/249], loss=102.4233
	step [52/249], loss=80.1234
	step [53/249], loss=111.5906
	step [54/249], loss=103.3048
	step [55/249], loss=91.6087
	step [56/249], loss=108.4038
	step [57/249], loss=100.5926
	step [58/249], loss=101.4181
	step [59/249], loss=93.9916
	step [60/249], loss=105.6678
	step [61/249], loss=114.8814
	step [62/249], loss=93.9200
	step [63/249], loss=100.7939
	step [64/249], loss=96.9903
	step [65/249], loss=115.9479
	step [66/249], loss=111.3817
	step [67/249], loss=117.8639
	step [68/249], loss=102.9679
	step [69/249], loss=98.6170
	step [70/249], loss=99.4180
	step [71/249], loss=98.5716
	step [72/249], loss=111.1067
	step [73/249], loss=108.0088
	step [74/249], loss=104.9956
	step [75/249], loss=96.0272
	step [76/249], loss=91.4906
	step [77/249], loss=100.6026
	step [78/249], loss=100.6073
	step [79/249], loss=106.5588
	step [80/249], loss=96.5096
	step [81/249], loss=112.5892
	step [82/249], loss=99.3645
	step [83/249], loss=105.9991
	step [84/249], loss=106.5796
	step [85/249], loss=118.3238
	step [86/249], loss=87.0995
	step [87/249], loss=111.8343
	step [88/249], loss=95.2128
	step [89/249], loss=104.3938
	step [90/249], loss=92.9031
	step [91/249], loss=76.6954
	step [92/249], loss=121.0480
	step [93/249], loss=92.6600
	step [94/249], loss=104.6600
	step [95/249], loss=106.4670
	step [96/249], loss=92.9697
	step [97/249], loss=94.7182
	step [98/249], loss=98.1878
	step [99/249], loss=107.9270
	step [100/249], loss=100.4095
	step [101/249], loss=106.5438
	step [102/249], loss=113.8256
	step [103/249], loss=107.9583
	step [104/249], loss=98.1300
	step [105/249], loss=90.1971
	step [106/249], loss=111.7525
	step [107/249], loss=96.8083
	step [108/249], loss=105.5963
	step [109/249], loss=103.1271
	step [110/249], loss=111.8098
	step [111/249], loss=100.5123
	step [112/249], loss=110.0221
	step [113/249], loss=128.0423
	step [114/249], loss=100.2715
	step [115/249], loss=107.1184
	step [116/249], loss=107.8149
	step [117/249], loss=110.7564
	step [118/249], loss=89.9312
	step [119/249], loss=114.2139
	step [120/249], loss=104.3949
	step [121/249], loss=101.4482
	step [122/249], loss=92.0117
	step [123/249], loss=109.1177
	step [124/249], loss=98.6459
	step [125/249], loss=103.3658
	step [126/249], loss=104.6361
	step [127/249], loss=95.3330
	step [128/249], loss=82.8998
	step [129/249], loss=91.5830
	step [130/249], loss=94.4664
	step [131/249], loss=103.3775
	step [132/249], loss=92.1764
	step [133/249], loss=94.7084
	step [134/249], loss=110.6484
	step [135/249], loss=120.6356
	step [136/249], loss=108.2826
	step [137/249], loss=115.9366
	step [138/249], loss=111.2563
	step [139/249], loss=115.2266
	step [140/249], loss=93.8665
	step [141/249], loss=113.8873
	step [142/249], loss=88.9718
	step [143/249], loss=95.6988
	step [144/249], loss=88.0945
	step [145/249], loss=106.1155
	step [146/249], loss=101.4776
	step [147/249], loss=110.3607
	step [148/249], loss=91.1447
	step [149/249], loss=101.3687
	step [150/249], loss=98.6034
	step [151/249], loss=88.1207
	step [152/249], loss=110.7986
	step [153/249], loss=114.5869
	step [154/249], loss=111.9030
	step [155/249], loss=107.1024
	step [156/249], loss=95.7798
	step [157/249], loss=99.6646
	step [158/249], loss=103.1908
	step [159/249], loss=86.6682
	step [160/249], loss=100.1929
	step [161/249], loss=89.5245
	step [162/249], loss=128.5791
	step [163/249], loss=91.1119
	step [164/249], loss=90.7684
	step [165/249], loss=110.7839
	step [166/249], loss=107.2769
	step [167/249], loss=105.1481
	step [168/249], loss=107.2489
	step [169/249], loss=98.4152
	step [170/249], loss=117.9047
	step [171/249], loss=110.7323
	step [172/249], loss=111.1104
	step [173/249], loss=91.8138
	step [174/249], loss=91.7393
	step [175/249], loss=95.3400
	step [176/249], loss=94.5604
	step [177/249], loss=88.1578
	step [178/249], loss=100.8402
	step [179/249], loss=117.4779
	step [180/249], loss=94.4317
	step [181/249], loss=95.5857
	step [182/249], loss=117.8081
	step [183/249], loss=111.8579
	step [184/249], loss=98.3044
	step [185/249], loss=96.8073
	step [186/249], loss=100.2791
	step [187/249], loss=98.1949
	step [188/249], loss=107.9458
	step [189/249], loss=112.5235
	step [190/249], loss=107.7147
	step [191/249], loss=106.8453
	step [192/249], loss=107.1341
	step [193/249], loss=101.0984
	step [194/249], loss=83.0509
	step [195/249], loss=107.9013
	step [196/249], loss=93.9071
	step [197/249], loss=105.6390
	step [198/249], loss=98.8138
	step [199/249], loss=88.0215
	step [200/249], loss=88.9926
	step [201/249], loss=106.9288
	step [202/249], loss=95.3039
	step [203/249], loss=97.4326
	step [204/249], loss=95.3710
	step [205/249], loss=93.5943
	step [206/249], loss=108.8572
	step [207/249], loss=101.0026
	step [208/249], loss=101.3438
	step [209/249], loss=118.5117
	step [210/249], loss=119.9613
	step [211/249], loss=105.0954
	step [212/249], loss=102.4840
	step [213/249], loss=110.6459
	step [214/249], loss=82.1003
	step [215/249], loss=98.4937
	step [216/249], loss=105.4534
	step [217/249], loss=111.0838
	step [218/249], loss=111.8907
	step [219/249], loss=114.5889
	step [220/249], loss=108.1805
	step [221/249], loss=109.1004
	step [222/249], loss=101.4774
	step [223/249], loss=88.0313
	step [224/249], loss=112.2994
	step [225/249], loss=107.4219
	step [226/249], loss=101.9440
	step [227/249], loss=89.0715
	step [228/249], loss=117.4779
	step [229/249], loss=91.9324
	step [230/249], loss=100.6478
	step [231/249], loss=101.2739
	step [232/249], loss=105.3466
	step [233/249], loss=99.1571
	step [234/249], loss=103.8189
	step [235/249], loss=104.5088
	step [236/249], loss=95.4818
	step [237/249], loss=118.4190
	step [238/249], loss=119.1330
	step [239/249], loss=122.3732
	step [240/249], loss=106.6772
	step [241/249], loss=104.0624
	step [242/249], loss=97.3243
	step [243/249], loss=113.3543
	step [244/249], loss=113.5782
	step [245/249], loss=98.0944
	step [246/249], loss=119.0409
	step [247/249], loss=116.6332
	step [248/249], loss=95.1501
	step [249/249], loss=59.3937
	Evaluating
	loss=0.0151, precision=0.3402, recall=0.9237, f1=0.4973
Training epoch 17
	step [1/249], loss=95.8305
	step [2/249], loss=108.1390
	step [3/249], loss=100.9702
	step [4/249], loss=106.3664
	step [5/249], loss=91.9715
	step [6/249], loss=115.0954
	step [7/249], loss=98.0589
	step [8/249], loss=97.6178
	step [9/249], loss=96.0482
	step [10/249], loss=91.6632
	step [11/249], loss=91.8436
	step [12/249], loss=93.2723
	step [13/249], loss=108.3911
	step [14/249], loss=87.4390
	step [15/249], loss=97.6536
	step [16/249], loss=88.0361
	step [17/249], loss=107.7409
	step [18/249], loss=93.2960
	step [19/249], loss=109.7312
	step [20/249], loss=118.7005
	step [21/249], loss=105.6741
	step [22/249], loss=68.8777
	step [23/249], loss=113.0906
	step [24/249], loss=102.6463
	step [25/249], loss=112.6805
	step [26/249], loss=118.3859
	step [27/249], loss=103.7517
	step [28/249], loss=127.3336
	step [29/249], loss=91.4265
	step [30/249], loss=102.9048
	step [31/249], loss=115.4341
	step [32/249], loss=116.9630
	step [33/249], loss=102.8364
	step [34/249], loss=94.6588
	step [35/249], loss=91.7503
	step [36/249], loss=97.9467
	step [37/249], loss=93.2627
	step [38/249], loss=94.8186
	step [39/249], loss=93.4170
	step [40/249], loss=104.3198
	step [41/249], loss=119.8835
	step [42/249], loss=102.8103
	step [43/249], loss=110.5241
	step [44/249], loss=115.9003
	step [45/249], loss=111.9867
	step [46/249], loss=107.5191
	step [47/249], loss=123.0450
	step [48/249], loss=109.4012
	step [49/249], loss=105.5365
	step [50/249], loss=102.2252
	step [51/249], loss=100.8514
	step [52/249], loss=103.1819
	step [53/249], loss=98.6123
	step [54/249], loss=109.5875
	step [55/249], loss=104.9357
	step [56/249], loss=111.1827
	step [57/249], loss=91.5187
	step [58/249], loss=108.2291
	step [59/249], loss=97.7766
	step [60/249], loss=99.1946
	step [61/249], loss=89.0748
	step [62/249], loss=105.5379
	step [63/249], loss=106.4958
	step [64/249], loss=123.8467
	step [65/249], loss=112.4759
	step [66/249], loss=83.8255
	step [67/249], loss=117.6365
	step [68/249], loss=103.2143
	step [69/249], loss=112.7098
	step [70/249], loss=95.5535
	step [71/249], loss=102.2770
	step [72/249], loss=88.5608
	step [73/249], loss=115.8377
	step [74/249], loss=108.4362
	step [75/249], loss=108.4046
	step [76/249], loss=105.8684
	step [77/249], loss=136.1140
	step [78/249], loss=102.6847
	step [79/249], loss=106.8547
	step [80/249], loss=109.1154
	step [81/249], loss=81.1513
	step [82/249], loss=88.8762
	step [83/249], loss=68.7533
	step [84/249], loss=94.1893
	step [85/249], loss=106.4399
	step [86/249], loss=111.1613
	step [87/249], loss=88.5251
	step [88/249], loss=105.2265
	step [89/249], loss=97.3189
	step [90/249], loss=107.0278
	step [91/249], loss=105.1779
	step [92/249], loss=114.6342
	step [93/249], loss=119.1074
	step [94/249], loss=103.6113
	step [95/249], loss=98.8245
	step [96/249], loss=83.8203
	step [97/249], loss=99.6029
	step [98/249], loss=84.4072
	step [99/249], loss=108.5369
	step [100/249], loss=96.1196
	step [101/249], loss=112.0463
	step [102/249], loss=94.3547
	step [103/249], loss=94.6424
	step [104/249], loss=106.6198
	step [105/249], loss=100.2077
	step [106/249], loss=105.3396
	step [107/249], loss=113.9406
	step [108/249], loss=88.1453
	step [109/249], loss=106.2625
	step [110/249], loss=94.0257
	step [111/249], loss=95.8748
	step [112/249], loss=108.2632
	step [113/249], loss=104.7311
	step [114/249], loss=123.0387
	step [115/249], loss=96.4893
	step [116/249], loss=91.3778
	step [117/249], loss=113.3579
	step [118/249], loss=93.1033
	step [119/249], loss=87.1425
	step [120/249], loss=107.6776
	step [121/249], loss=103.7278
	step [122/249], loss=101.0602
	step [123/249], loss=99.7086
	step [124/249], loss=83.9647
	step [125/249], loss=131.4828
	step [126/249], loss=122.6553
	step [127/249], loss=108.7920
	step [128/249], loss=88.1589
	step [129/249], loss=105.0181
	step [130/249], loss=93.3320
	step [131/249], loss=115.5882
	step [132/249], loss=116.2647
	step [133/249], loss=108.8218
	step [134/249], loss=94.3230
	step [135/249], loss=103.6464
	step [136/249], loss=103.4476
	step [137/249], loss=102.3748
	step [138/249], loss=80.1186
	step [139/249], loss=119.1676
	step [140/249], loss=97.2762
	step [141/249], loss=101.9866
	step [142/249], loss=90.6420
	step [143/249], loss=97.7607
	step [144/249], loss=125.8187
	step [145/249], loss=94.2794
	step [146/249], loss=95.9757
	step [147/249], loss=104.0920
	step [148/249], loss=106.7084
	step [149/249], loss=114.5755
	step [150/249], loss=87.1620
	step [151/249], loss=111.6760
	step [152/249], loss=106.6353
	step [153/249], loss=86.8550
	step [154/249], loss=85.6319
	step [155/249], loss=98.8617
	step [156/249], loss=111.0355
	step [157/249], loss=99.7936
	step [158/249], loss=119.5087
	step [159/249], loss=117.4259
	step [160/249], loss=95.4421
	step [161/249], loss=108.1505
	step [162/249], loss=118.3788
	step [163/249], loss=104.5375
	step [164/249], loss=107.8230
	step [165/249], loss=96.2311
	step [166/249], loss=92.6075
	step [167/249], loss=94.2614
	step [168/249], loss=88.0782
	step [169/249], loss=95.4775
	step [170/249], loss=102.7489
	step [171/249], loss=84.8385
	step [172/249], loss=92.6185
	step [173/249], loss=104.6160
	step [174/249], loss=101.2670
	step [175/249], loss=71.3500
	step [176/249], loss=103.5495
	step [177/249], loss=114.6596
	step [178/249], loss=101.5786
	step [179/249], loss=106.7644
	step [180/249], loss=103.8913
	step [181/249], loss=104.1346
	step [182/249], loss=109.6358
	step [183/249], loss=93.1694
	step [184/249], loss=94.9827
	step [185/249], loss=100.6251
	step [186/249], loss=85.6933
	step [187/249], loss=112.9228
	step [188/249], loss=99.3184
	step [189/249], loss=99.3740
	step [190/249], loss=91.4256
	step [191/249], loss=105.8731
	step [192/249], loss=88.9666
	step [193/249], loss=139.4106
	step [194/249], loss=80.7847
	step [195/249], loss=108.9891
	step [196/249], loss=106.3809
	step [197/249], loss=107.7789
	step [198/249], loss=108.1203
	step [199/249], loss=83.1356
	step [200/249], loss=99.0346
	step [201/249], loss=102.8512
	step [202/249], loss=89.2230
	step [203/249], loss=86.2908
	step [204/249], loss=95.0063
	step [205/249], loss=98.0333
	step [206/249], loss=117.3261
	step [207/249], loss=117.8908
	step [208/249], loss=102.8166
	step [209/249], loss=99.3170
	step [210/249], loss=114.2457
	step [211/249], loss=135.0045
	step [212/249], loss=93.2051
	step [213/249], loss=102.5113
	step [214/249], loss=101.1842
	step [215/249], loss=95.7051
	step [216/249], loss=108.8453
	step [217/249], loss=106.2967
	step [218/249], loss=103.9956
	step [219/249], loss=103.4343
	step [220/249], loss=90.3854
	step [221/249], loss=95.9852
	step [222/249], loss=96.6146
	step [223/249], loss=109.4291
	step [224/249], loss=121.3969
	step [225/249], loss=99.4702
	step [226/249], loss=108.1715
	step [227/249], loss=123.0419
	step [228/249], loss=97.3897
	step [229/249], loss=117.9874
	step [230/249], loss=94.7681
	step [231/249], loss=91.5047
	step [232/249], loss=106.7627
	step [233/249], loss=109.3464
	step [234/249], loss=106.4868
	step [235/249], loss=92.7700
	step [236/249], loss=102.6151
	step [237/249], loss=91.4484
	step [238/249], loss=109.4518
	step [239/249], loss=88.8586
	step [240/249], loss=108.7906
	step [241/249], loss=91.7646
	step [242/249], loss=107.1749
	step [243/249], loss=109.5903
	step [244/249], loss=107.9771
	step [245/249], loss=88.9506
	step [246/249], loss=99.6000
	step [247/249], loss=80.9990
	step [248/249], loss=94.5838
	step [249/249], loss=57.9200
	Evaluating
	loss=0.0141, precision=0.3327, recall=0.8898, f1=0.4843
Training epoch 18
	step [1/249], loss=122.8732
	step [2/249], loss=96.4780
	step [3/249], loss=101.6064
	step [4/249], loss=109.9402
	step [5/249], loss=106.2443
	step [6/249], loss=104.6409
	step [7/249], loss=114.7452
	step [8/249], loss=120.3123
	step [9/249], loss=104.2249
	step [10/249], loss=110.5687
	step [11/249], loss=95.3611
	step [12/249], loss=88.4045
	step [13/249], loss=110.8662
	step [14/249], loss=116.8913
	step [15/249], loss=105.8044
	step [16/249], loss=99.7175
	step [17/249], loss=108.6065
	step [18/249], loss=96.4192
	step [19/249], loss=101.8523
	step [20/249], loss=84.6906
	step [21/249], loss=97.4319
	step [22/249], loss=116.4633
	step [23/249], loss=102.8181
	step [24/249], loss=88.2212
	step [25/249], loss=93.4000
	step [26/249], loss=113.1243
	step [27/249], loss=91.5819
	step [28/249], loss=115.1214
	step [29/249], loss=115.0102
	step [30/249], loss=98.5931
	step [31/249], loss=108.6362
	step [32/249], loss=102.5365
	step [33/249], loss=105.4061
	step [34/249], loss=94.5984
	step [35/249], loss=102.4847
	step [36/249], loss=109.9012
	step [37/249], loss=111.3717
	step [38/249], loss=93.9317
	step [39/249], loss=104.7815
	step [40/249], loss=109.6920
	step [41/249], loss=119.2877
	step [42/249], loss=111.7004
	step [43/249], loss=109.6352
	step [44/249], loss=97.8235
	step [45/249], loss=106.3320
	step [46/249], loss=99.3463
	step [47/249], loss=84.3648
	step [48/249], loss=101.6684
	step [49/249], loss=91.3252
	step [50/249], loss=94.6058
	step [51/249], loss=108.0501
	step [52/249], loss=112.8216
	step [53/249], loss=111.1448
	step [54/249], loss=119.6486
	step [55/249], loss=93.4870
	step [56/249], loss=99.8139
	step [57/249], loss=119.4503
	step [58/249], loss=97.6798
	step [59/249], loss=91.4047
	step [60/249], loss=86.3264
	step [61/249], loss=97.6683
	step [62/249], loss=102.5149
	step [63/249], loss=105.4559
	step [64/249], loss=99.4653
	step [65/249], loss=101.8225
	step [66/249], loss=103.0889
	step [67/249], loss=100.6653
	step [68/249], loss=101.3590
	step [69/249], loss=79.7922
	step [70/249], loss=98.4644
	step [71/249], loss=101.4378
	step [72/249], loss=92.7258
	step [73/249], loss=89.0317
	step [74/249], loss=89.9505
	step [75/249], loss=107.7603
	step [76/249], loss=105.1778
	step [77/249], loss=103.9429
	step [78/249], loss=93.7297
	step [79/249], loss=103.9610
	step [80/249], loss=102.4806
	step [81/249], loss=81.1705
	step [82/249], loss=90.3605
	step [83/249], loss=107.8457
	step [84/249], loss=98.7552
	step [85/249], loss=104.9086
	step [86/249], loss=87.5770
	step [87/249], loss=101.4433
	step [88/249], loss=100.5806
	step [89/249], loss=101.8403
	step [90/249], loss=115.7921
	step [91/249], loss=120.0227
	step [92/249], loss=116.9222
	step [93/249], loss=87.7253
	step [94/249], loss=129.8457
	step [95/249], loss=107.2785
	step [96/249], loss=106.8045
	step [97/249], loss=77.9461
	step [98/249], loss=92.4886
	step [99/249], loss=104.7792
	step [100/249], loss=73.4870
	step [101/249], loss=100.2293
	step [102/249], loss=95.0574
	step [103/249], loss=94.9734
	step [104/249], loss=99.7085
	step [105/249], loss=112.3742
	step [106/249], loss=115.9956
	step [107/249], loss=111.0612
	step [108/249], loss=108.1359
	step [109/249], loss=106.5033
	step [110/249], loss=96.4116
	step [111/249], loss=120.4112
	step [112/249], loss=104.5020
	step [113/249], loss=89.8689
	step [114/249], loss=109.8478
	step [115/249], loss=111.6936
	step [116/249], loss=96.5450
	step [117/249], loss=86.9001
	step [118/249], loss=112.0448
	step [119/249], loss=101.5223
	step [120/249], loss=109.6479
	step [121/249], loss=102.5296
	step [122/249], loss=83.5458
	step [123/249], loss=103.8085
	step [124/249], loss=100.6378
	step [125/249], loss=93.6593
	step [126/249], loss=86.2534
	step [127/249], loss=80.0061
	step [128/249], loss=108.2243
	step [129/249], loss=96.5650
	step [130/249], loss=102.1858
	step [131/249], loss=109.2876
	step [132/249], loss=106.1207
	step [133/249], loss=93.8617
	step [134/249], loss=91.6331
	step [135/249], loss=107.1389
	step [136/249], loss=99.7860
	step [137/249], loss=100.8849
	step [138/249], loss=80.8403
	step [139/249], loss=105.7405
	step [140/249], loss=112.2493
	step [141/249], loss=115.9807
	step [142/249], loss=89.6157
	step [143/249], loss=110.5313
	step [144/249], loss=92.5052
	step [145/249], loss=124.7183
	step [146/249], loss=93.7056
	step [147/249], loss=109.6293
	step [148/249], loss=94.8301
	step [149/249], loss=108.5349
	step [150/249], loss=104.1476
	step [151/249], loss=99.7282
	step [152/249], loss=96.2695
	step [153/249], loss=85.6902
	step [154/249], loss=103.8850
	step [155/249], loss=96.3263
	step [156/249], loss=100.0698
	step [157/249], loss=100.1778
	step [158/249], loss=76.5025
	step [159/249], loss=79.5268
	step [160/249], loss=109.3349
	step [161/249], loss=101.4489
	step [162/249], loss=92.8207
	step [163/249], loss=111.4152
	step [164/249], loss=96.3644
	step [165/249], loss=96.5679
	step [166/249], loss=101.2114
	step [167/249], loss=86.7683
	step [168/249], loss=103.4573
	step [169/249], loss=112.6565
	step [170/249], loss=94.3040
	step [171/249], loss=99.4949
	step [172/249], loss=105.9143
	step [173/249], loss=95.4600
	step [174/249], loss=102.5085
	step [175/249], loss=110.4264
	step [176/249], loss=76.4856
	step [177/249], loss=91.7356
	step [178/249], loss=100.9220
	step [179/249], loss=87.2012
	step [180/249], loss=101.4310
	step [181/249], loss=107.4495
	step [182/249], loss=79.5536
	step [183/249], loss=116.6801
	step [184/249], loss=100.7025
	step [185/249], loss=87.0510
	step [186/249], loss=96.2381
	step [187/249], loss=98.9879
	step [188/249], loss=110.2808
	step [189/249], loss=113.9605
	step [190/249], loss=88.7284
	step [191/249], loss=114.7431
	step [192/249], loss=93.1689
	step [193/249], loss=94.5860
	step [194/249], loss=103.1295
	step [195/249], loss=93.6288
	step [196/249], loss=112.8354
	step [197/249], loss=107.1625
	step [198/249], loss=103.5345
	step [199/249], loss=98.0392
	step [200/249], loss=104.9346
	step [201/249], loss=100.5070
	step [202/249], loss=98.6481
	step [203/249], loss=100.7195
	step [204/249], loss=116.1423
	step [205/249], loss=83.5131
	step [206/249], loss=92.7610
	step [207/249], loss=87.6282
	step [208/249], loss=88.2542
	step [209/249], loss=108.8854
	step [210/249], loss=104.0520
	step [211/249], loss=84.4679
	step [212/249], loss=85.9329
	step [213/249], loss=95.0058
	step [214/249], loss=92.6424
	step [215/249], loss=91.8829
	step [216/249], loss=97.1649
	step [217/249], loss=84.9604
	step [218/249], loss=78.7807
	step [219/249], loss=93.1996
	step [220/249], loss=110.7835
	step [221/249], loss=111.1825
	step [222/249], loss=84.9974
	step [223/249], loss=102.3174
	step [224/249], loss=128.2001
	step [225/249], loss=91.9700
	step [226/249], loss=94.9765
	step [227/249], loss=120.1417
	step [228/249], loss=120.1827
	step [229/249], loss=97.4435
	step [230/249], loss=93.1223
	step [231/249], loss=105.7118
	step [232/249], loss=117.6399
	step [233/249], loss=96.1307
	step [234/249], loss=98.1294
	step [235/249], loss=94.4670
	step [236/249], loss=91.6370
	step [237/249], loss=99.3882
	step [238/249], loss=96.0841
	step [239/249], loss=94.3655
	step [240/249], loss=98.4958
	step [241/249], loss=88.3327
	step [242/249], loss=98.5531
	step [243/249], loss=107.7473
	step [244/249], loss=103.9145
	step [245/249], loss=119.7569
	step [246/249], loss=94.1362
	step [247/249], loss=98.9548
	step [248/249], loss=124.2226
	step [249/249], loss=68.2069
	Evaluating
	loss=0.0130, precision=0.3469, recall=0.8678, f1=0.4957
Training epoch 19
	step [1/249], loss=121.6431
	step [2/249], loss=90.5014
	step [3/249], loss=120.8961
	step [4/249], loss=107.9205
	step [5/249], loss=89.6302
	step [6/249], loss=110.5965
	step [7/249], loss=97.8197
	step [8/249], loss=98.6805
	step [9/249], loss=94.8330
	step [10/249], loss=114.9572
	step [11/249], loss=101.2976
	step [12/249], loss=96.5707
	step [13/249], loss=106.9855
	step [14/249], loss=101.7169
	step [15/249], loss=99.6644
	step [16/249], loss=83.3630
	step [17/249], loss=111.7626
	step [18/249], loss=97.8607
	step [19/249], loss=96.5421
	step [20/249], loss=107.1549
	step [21/249], loss=128.7722
	step [22/249], loss=101.9324
	step [23/249], loss=106.8953
	step [24/249], loss=108.7502
	step [25/249], loss=117.9331
	step [26/249], loss=99.6277
	step [27/249], loss=114.2062
	step [28/249], loss=100.5108
	step [29/249], loss=92.7335
	step [30/249], loss=97.4986
	step [31/249], loss=96.0035
	step [32/249], loss=92.1882
	step [33/249], loss=94.4352
	step [34/249], loss=109.9598
	step [35/249], loss=101.9200
	step [36/249], loss=105.8948
	step [37/249], loss=87.6944
	step [38/249], loss=90.3440
	step [39/249], loss=124.2128
	step [40/249], loss=104.9479
	step [41/249], loss=107.2357
	step [42/249], loss=114.1893
	step [43/249], loss=80.8012
	step [44/249], loss=100.2509
	step [45/249], loss=102.3223
	step [46/249], loss=92.6547
	step [47/249], loss=111.3073
	step [48/249], loss=83.8165
	step [49/249], loss=73.0731
	step [50/249], loss=86.0812
	step [51/249], loss=112.0041
	step [52/249], loss=103.3592
	step [53/249], loss=88.1330
	step [54/249], loss=87.6442
	step [55/249], loss=101.8981
	step [56/249], loss=98.4137
	step [57/249], loss=116.4564
	step [58/249], loss=96.5836
	step [59/249], loss=119.0196
	step [60/249], loss=127.8238
	step [61/249], loss=92.8357
	step [62/249], loss=106.0230
	step [63/249], loss=104.1380
	step [64/249], loss=90.7691
	step [65/249], loss=99.4122
	step [66/249], loss=115.0318
	step [67/249], loss=98.0977
	step [68/249], loss=94.1871
	step [69/249], loss=107.3922
	step [70/249], loss=96.1454
	step [71/249], loss=85.4348
	step [72/249], loss=127.4580
	step [73/249], loss=102.0947
	step [74/249], loss=91.5016
	step [75/249], loss=105.5956
	step [76/249], loss=86.5989
	step [77/249], loss=78.4718
	step [78/249], loss=73.6481
	step [79/249], loss=105.5060
	step [80/249], loss=92.0731
	step [81/249], loss=87.0874
	step [82/249], loss=73.6921
	step [83/249], loss=97.1158
	step [84/249], loss=92.1950
	step [85/249], loss=90.1051
	step [86/249], loss=77.6907
	step [87/249], loss=103.9864
	step [88/249], loss=89.8717
	step [89/249], loss=86.9743
	step [90/249], loss=104.4599
	step [91/249], loss=99.2486
	step [92/249], loss=88.7130
	step [93/249], loss=100.2584
	step [94/249], loss=105.9886
	step [95/249], loss=92.7945
	step [96/249], loss=104.7766
	step [97/249], loss=106.9736
	step [98/249], loss=101.6180
	step [99/249], loss=80.0594
	step [100/249], loss=101.4135
	step [101/249], loss=105.1405
	step [102/249], loss=94.9767
	step [103/249], loss=91.8204
	step [104/249], loss=110.0901
	step [105/249], loss=95.6056
	step [106/249], loss=104.3300
	step [107/249], loss=99.7969
	step [108/249], loss=97.2111
	step [109/249], loss=89.3182
	step [110/249], loss=87.7800
	step [111/249], loss=101.4201
	step [112/249], loss=94.7463
	step [113/249], loss=107.1179
	step [114/249], loss=105.8814
	step [115/249], loss=83.4351
	step [116/249], loss=104.7185
	step [117/249], loss=99.7831
	step [118/249], loss=105.9150
	step [119/249], loss=106.4147
	step [120/249], loss=103.6244
	step [121/249], loss=92.1100
	step [122/249], loss=99.1211
	step [123/249], loss=95.9015
	step [124/249], loss=89.8701
	step [125/249], loss=109.8993
	step [126/249], loss=113.2330
	step [127/249], loss=112.8473
	step [128/249], loss=82.9251
	step [129/249], loss=89.7257
	step [130/249], loss=97.1549
	step [131/249], loss=85.2104
	step [132/249], loss=91.5067
	step [133/249], loss=105.1001
	step [134/249], loss=85.9396
	step [135/249], loss=101.6633
	step [136/249], loss=102.2162
	step [137/249], loss=97.8264
	step [138/249], loss=86.8548
	step [139/249], loss=98.2086
	step [140/249], loss=120.0821
	step [141/249], loss=100.5422
	step [142/249], loss=92.3446
	step [143/249], loss=118.4654
	step [144/249], loss=103.5385
	step [145/249], loss=94.9341
	step [146/249], loss=100.0738
	step [147/249], loss=104.1607
	step [148/249], loss=110.8535
	step [149/249], loss=101.2185
	step [150/249], loss=110.1129
	step [151/249], loss=95.9478
	step [152/249], loss=103.8548
	step [153/249], loss=88.7470
	step [154/249], loss=99.2021
	step [155/249], loss=98.2646
	step [156/249], loss=80.9120
	step [157/249], loss=97.8661
	step [158/249], loss=103.6221
	step [159/249], loss=102.3943
	step [160/249], loss=94.6095
	step [161/249], loss=114.1964
	step [162/249], loss=89.8893
	step [163/249], loss=107.3521
	step [164/249], loss=99.6501
	step [165/249], loss=102.2889
	step [166/249], loss=90.1460
	step [167/249], loss=103.6170
	step [168/249], loss=102.4011
	step [169/249], loss=89.5448
	step [170/249], loss=109.3543
	step [171/249], loss=99.6799
	step [172/249], loss=92.8375
	step [173/249], loss=95.9193
	step [174/249], loss=79.5020
	step [175/249], loss=91.1944
	step [176/249], loss=109.0105
	step [177/249], loss=79.8095
	step [178/249], loss=96.8830
	step [179/249], loss=106.6078
	step [180/249], loss=99.2615
	step [181/249], loss=80.4663
	step [182/249], loss=86.6313
	step [183/249], loss=93.2046
	step [184/249], loss=101.6546
	step [185/249], loss=111.0143
	step [186/249], loss=105.9153
	step [187/249], loss=114.7719
	step [188/249], loss=118.0609
	step [189/249], loss=102.4826
	step [190/249], loss=82.2866
	step [191/249], loss=111.4299
	step [192/249], loss=108.5478
	step [193/249], loss=121.4709
	step [194/249], loss=110.9043
	step [195/249], loss=111.8624
	step [196/249], loss=93.9083
	step [197/249], loss=92.5569
	step [198/249], loss=110.1255
	step [199/249], loss=94.1086
	step [200/249], loss=88.8933
	step [201/249], loss=88.3216
	step [202/249], loss=85.0862
	step [203/249], loss=86.5827
	step [204/249], loss=106.6984
	step [205/249], loss=85.4136
	step [206/249], loss=103.3430
	step [207/249], loss=89.7132
	step [208/249], loss=100.5495
	step [209/249], loss=89.1109
	step [210/249], loss=86.8350
	step [211/249], loss=108.3426
	step [212/249], loss=83.0181
	step [213/249], loss=86.7853
	step [214/249], loss=89.7476
	step [215/249], loss=100.6327
	step [216/249], loss=91.1794
	step [217/249], loss=87.2493
	step [218/249], loss=105.1068
	step [219/249], loss=77.4384
	step [220/249], loss=85.2875
	step [221/249], loss=71.7555
	step [222/249], loss=110.9949
	step [223/249], loss=92.7296
	step [224/249], loss=114.0813
	step [225/249], loss=89.1108
	step [226/249], loss=103.0335
	step [227/249], loss=100.5002
	step [228/249], loss=115.8178
	step [229/249], loss=94.2629
	step [230/249], loss=106.9258
	step [231/249], loss=118.0952
	step [232/249], loss=111.9943
	step [233/249], loss=120.2619
	step [234/249], loss=98.3390
	step [235/249], loss=94.5198
	step [236/249], loss=114.5866
	step [237/249], loss=106.3539
	step [238/249], loss=101.0264
	step [239/249], loss=96.0890
	step [240/249], loss=91.3127
	step [241/249], loss=91.1225
	step [242/249], loss=90.7784
	step [243/249], loss=96.7313
	step [244/249], loss=79.0146
	step [245/249], loss=96.5001
	step [246/249], loss=123.2517
	step [247/249], loss=114.2415
	step [248/249], loss=65.6277
	step [249/249], loss=66.6119
	Evaluating
	loss=0.0119, precision=0.3529, recall=0.9097, f1=0.5085
Training epoch 20
	step [1/249], loss=116.9992
	step [2/249], loss=98.1435
	step [3/249], loss=109.2057
	step [4/249], loss=103.4998
	step [5/249], loss=86.5810
	step [6/249], loss=96.5329
	step [7/249], loss=94.4371
	step [8/249], loss=112.4330
	step [9/249], loss=93.5973
	step [10/249], loss=96.3245
	step [11/249], loss=119.5275
	step [12/249], loss=87.6035
	step [13/249], loss=101.6594
	step [14/249], loss=79.3589
	step [15/249], loss=90.4253
	step [16/249], loss=95.3626
	step [17/249], loss=82.4461
	step [18/249], loss=105.4123
	step [19/249], loss=114.7297
	step [20/249], loss=85.5067
	step [21/249], loss=88.4278
	step [22/249], loss=102.5404
	step [23/249], loss=100.1690
	step [24/249], loss=83.6001
	step [25/249], loss=76.4625
	step [26/249], loss=104.1722
	step [27/249], loss=115.1542
	step [28/249], loss=105.1549
	step [29/249], loss=93.0600
	step [30/249], loss=99.3686
	step [31/249], loss=95.3479
	step [32/249], loss=82.6978
	step [33/249], loss=83.8137
	step [34/249], loss=111.7933
	step [35/249], loss=92.8095
	step [36/249], loss=117.9902
	step [37/249], loss=96.2964
	step [38/249], loss=96.6412
	step [39/249], loss=102.4756
	step [40/249], loss=116.5918
	step [41/249], loss=96.5419
	step [42/249], loss=101.7078
	step [43/249], loss=89.6762
	step [44/249], loss=100.1102
	step [45/249], loss=93.7913
	step [46/249], loss=91.8359
	step [47/249], loss=100.5344
	step [48/249], loss=96.6128
	step [49/249], loss=95.1801
	step [50/249], loss=84.3555
	step [51/249], loss=106.5082
	step [52/249], loss=104.6588
	step [53/249], loss=91.5970
	step [54/249], loss=95.5154
	step [55/249], loss=112.2663
	step [56/249], loss=121.7135
	step [57/249], loss=96.2832
	step [58/249], loss=86.7020
	step [59/249], loss=99.2202
	step [60/249], loss=88.9573
	step [61/249], loss=85.1921
	step [62/249], loss=85.5749
	step [63/249], loss=86.1682
	step [64/249], loss=103.9381
	step [65/249], loss=90.2378
	step [66/249], loss=117.1292
	step [67/249], loss=85.7760
	step [68/249], loss=94.5648
	step [69/249], loss=71.7483
	step [70/249], loss=93.1574
	step [71/249], loss=85.5029
	step [72/249], loss=95.1115
	step [73/249], loss=111.5988
	step [74/249], loss=111.3109
	step [75/249], loss=106.0234
	step [76/249], loss=98.7092
	step [77/249], loss=102.2767
	step [78/249], loss=88.6904
	step [79/249], loss=87.5484
	step [80/249], loss=88.4768
	step [81/249], loss=90.8204
	step [82/249], loss=99.9709
	step [83/249], loss=90.7281
	step [84/249], loss=114.3151
	step [85/249], loss=98.4910
	step [86/249], loss=99.3452
	step [87/249], loss=96.8726
	step [88/249], loss=92.8695
	step [89/249], loss=93.3599
	step [90/249], loss=115.0481
	step [91/249], loss=116.9416
	step [92/249], loss=95.5186
	step [93/249], loss=94.4241
	step [94/249], loss=89.2560
	step [95/249], loss=86.8549
	step [96/249], loss=70.5087
	step [97/249], loss=103.0703
	step [98/249], loss=92.1445
	step [99/249], loss=78.1552
	step [100/249], loss=107.3827
	step [101/249], loss=99.3208
	step [102/249], loss=93.1146
	step [103/249], loss=103.8849
	step [104/249], loss=95.1742
	step [105/249], loss=96.4807
	step [106/249], loss=90.0616
	step [107/249], loss=101.9449
	step [108/249], loss=106.4841
	step [109/249], loss=74.7214
	step [110/249], loss=89.0592
	step [111/249], loss=106.5206
	step [112/249], loss=105.0886
	step [113/249], loss=105.7519
	step [114/249], loss=97.4222
	step [115/249], loss=96.4083
	step [116/249], loss=106.7735
	step [117/249], loss=119.8219
	step [118/249], loss=101.1234
	step [119/249], loss=99.1312
	step [120/249], loss=86.3220
	step [121/249], loss=101.2611
	step [122/249], loss=106.9121
	step [123/249], loss=83.5072
	step [124/249], loss=98.4811
	step [125/249], loss=95.9455
	step [126/249], loss=80.6767
	step [127/249], loss=102.5384
	step [128/249], loss=102.3900
	step [129/249], loss=99.4180
	step [130/249], loss=100.7755
	step [131/249], loss=107.7198
	step [132/249], loss=78.9938
	step [133/249], loss=101.1185
	step [134/249], loss=76.9173
	step [135/249], loss=77.8351
	step [136/249], loss=100.6914
	step [137/249], loss=81.9730
	step [138/249], loss=100.9821
	step [139/249], loss=106.5764
	step [140/249], loss=95.4925
	step [141/249], loss=123.0598
	step [142/249], loss=97.4912
	step [143/249], loss=107.8823
	step [144/249], loss=86.4889
	step [145/249], loss=93.0149
	step [146/249], loss=80.3016
	step [147/249], loss=89.7443
	step [148/249], loss=102.3846
	step [149/249], loss=103.9657
	step [150/249], loss=96.0601
	step [151/249], loss=100.0416
	step [152/249], loss=84.9596
	step [153/249], loss=110.9605
	step [154/249], loss=82.6074
	step [155/249], loss=91.9228
	step [156/249], loss=97.0728
	step [157/249], loss=114.2091
	step [158/249], loss=87.1754
	step [159/249], loss=85.3030
	step [160/249], loss=111.6103
	step [161/249], loss=87.9489
	step [162/249], loss=105.5793
	step [163/249], loss=112.2581
	step [164/249], loss=89.3047
	step [165/249], loss=106.4874
	step [166/249], loss=110.6072
	step [167/249], loss=99.2290
	step [168/249], loss=124.8076
	step [169/249], loss=107.5354
	step [170/249], loss=118.4445
	step [171/249], loss=108.7527
	step [172/249], loss=106.6329
	step [173/249], loss=105.6886
	step [174/249], loss=83.6572
	step [175/249], loss=99.6088
	step [176/249], loss=101.9893
	step [177/249], loss=88.9734
	step [178/249], loss=99.6672
	step [179/249], loss=98.7204
	step [180/249], loss=109.2077
	step [181/249], loss=108.6737
	step [182/249], loss=107.8420
	step [183/249], loss=80.7440
	step [184/249], loss=112.5712
	step [185/249], loss=102.7526
	step [186/249], loss=101.2062
	step [187/249], loss=102.6642
	step [188/249], loss=94.2277
	step [189/249], loss=90.9595
	step [190/249], loss=106.4190
	step [191/249], loss=107.0929
	step [192/249], loss=84.4774
	step [193/249], loss=95.5431
	step [194/249], loss=136.7531
	step [195/249], loss=90.5356
	step [196/249], loss=94.4744
	step [197/249], loss=103.7421
	step [198/249], loss=76.5359
	step [199/249], loss=91.6714
	step [200/249], loss=92.3932
	step [201/249], loss=86.0272
	step [202/249], loss=95.5718
	step [203/249], loss=108.8817
	step [204/249], loss=90.3492
	step [205/249], loss=105.3412
	step [206/249], loss=85.7064
	step [207/249], loss=112.6247
	step [208/249], loss=96.2541
	step [209/249], loss=99.8282
	step [210/249], loss=103.0873
	step [211/249], loss=98.3352
	step [212/249], loss=103.5966
	step [213/249], loss=107.0228
	step [214/249], loss=100.2459
	step [215/249], loss=110.4516
	step [216/249], loss=91.1171
	step [217/249], loss=111.7817
	step [218/249], loss=125.0958
	step [219/249], loss=92.0781
	step [220/249], loss=102.8759
	step [221/249], loss=86.6086
	step [222/249], loss=88.8423
	step [223/249], loss=92.3732
	step [224/249], loss=84.4585
	step [225/249], loss=87.5889
	step [226/249], loss=93.7580
	step [227/249], loss=101.0209
	step [228/249], loss=107.0824
	step [229/249], loss=102.1932
	step [230/249], loss=86.0699
	step [231/249], loss=99.7528
	step [232/249], loss=101.7424
	step [233/249], loss=93.1074
	step [234/249], loss=92.9000
	step [235/249], loss=89.4144
	step [236/249], loss=86.2079
	step [237/249], loss=103.5229
	step [238/249], loss=101.4974
	step [239/249], loss=95.2222
	step [240/249], loss=90.5549
	step [241/249], loss=96.3468
	step [242/249], loss=92.2404
	step [243/249], loss=111.5194
	step [244/249], loss=90.4435
	step [245/249], loss=101.3623
	step [246/249], loss=97.7134
	step [247/249], loss=113.2638
	step [248/249], loss=79.1572
	step [249/249], loss=65.4798
	Evaluating
	loss=0.0105, precision=0.3459, recall=0.9053, f1=0.5006
Training epoch 21
	step [1/249], loss=91.8474
	step [2/249], loss=92.8949
	step [3/249], loss=95.4000
	step [4/249], loss=98.4020
	step [5/249], loss=95.3172
	step [6/249], loss=99.9882
	step [7/249], loss=94.6957
	step [8/249], loss=87.3875
	step [9/249], loss=89.4072
	step [10/249], loss=86.9416
	step [11/249], loss=102.4560
	step [12/249], loss=105.1446
	step [13/249], loss=106.2659
	step [14/249], loss=84.3459
	step [15/249], loss=108.8448
	step [16/249], loss=90.7044
	step [17/249], loss=87.1082
	step [18/249], loss=87.2058
	step [19/249], loss=101.9784
	step [20/249], loss=76.4910
	step [21/249], loss=84.0916
	step [22/249], loss=94.9769
	step [23/249], loss=82.8412
	step [24/249], loss=100.9960
	step [25/249], loss=91.4850
	step [26/249], loss=104.2352
	step [27/249], loss=116.6297
	step [28/249], loss=115.9128
	step [29/249], loss=91.0036
	step [30/249], loss=98.9114
	step [31/249], loss=93.8692
	step [32/249], loss=87.2770
	step [33/249], loss=125.5228
	step [34/249], loss=95.2089
	step [35/249], loss=96.6872
	step [36/249], loss=109.1193
	step [37/249], loss=82.0645
	step [38/249], loss=108.7109
	step [39/249], loss=77.0547
	step [40/249], loss=107.5400
	step [41/249], loss=107.8036
	step [42/249], loss=77.8052
	step [43/249], loss=108.8659
	step [44/249], loss=106.4375
	step [45/249], loss=110.2682
	step [46/249], loss=108.7842
	step [47/249], loss=95.8004
	step [48/249], loss=90.5046
	step [49/249], loss=90.0279
	step [50/249], loss=110.5417
	step [51/249], loss=88.3077
	step [52/249], loss=110.8206
	step [53/249], loss=97.7535
	step [54/249], loss=87.2790
	step [55/249], loss=97.9000
	step [56/249], loss=93.1186
	step [57/249], loss=101.0592
	step [58/249], loss=100.3677
	step [59/249], loss=92.7180
	step [60/249], loss=99.1323
	step [61/249], loss=76.9673
	step [62/249], loss=81.8634
	step [63/249], loss=92.3659
	step [64/249], loss=93.4959
	step [65/249], loss=105.0928
	step [66/249], loss=110.5449
	step [67/249], loss=99.6839
	step [68/249], loss=94.4411
	step [69/249], loss=99.7825
	step [70/249], loss=94.8765
	step [71/249], loss=119.8482
	step [72/249], loss=92.5215
	step [73/249], loss=93.5671
	step [74/249], loss=107.6205
	step [75/249], loss=91.7399
	step [76/249], loss=104.8025
	step [77/249], loss=90.9769
	step [78/249], loss=91.6407
	step [79/249], loss=86.5702
	step [80/249], loss=92.5068
	step [81/249], loss=81.8181
	step [82/249], loss=98.5548
	step [83/249], loss=83.8361
	step [84/249], loss=90.5873
	step [85/249], loss=108.1141
	step [86/249], loss=96.7574
	step [87/249], loss=109.7954
	step [88/249], loss=82.5061
	step [89/249], loss=100.9118
	step [90/249], loss=84.9611
	step [91/249], loss=93.4317
	step [92/249], loss=82.1520
	step [93/249], loss=71.3608
	step [94/249], loss=109.8453
	step [95/249], loss=85.4656
	step [96/249], loss=92.1661
	step [97/249], loss=103.7567
	step [98/249], loss=103.0041
	step [99/249], loss=81.9270
	step [100/249], loss=101.8991
	step [101/249], loss=99.9581
	step [102/249], loss=108.0600
	step [103/249], loss=88.3403
	step [104/249], loss=91.8536
	step [105/249], loss=105.3033
	step [106/249], loss=97.2445
	step [107/249], loss=95.8905
	step [108/249], loss=91.1113
	step [109/249], loss=99.1256
	step [110/249], loss=100.2694
	step [111/249], loss=90.4247
	step [112/249], loss=95.5948
	step [113/249], loss=99.3548
	step [114/249], loss=105.2003
	step [115/249], loss=118.4151
	step [116/249], loss=83.1155
	step [117/249], loss=80.8263
	step [118/249], loss=80.4247
	step [119/249], loss=90.0398
	step [120/249], loss=82.5137
	step [121/249], loss=80.5137
	step [122/249], loss=78.6385
	step [123/249], loss=79.3824
	step [124/249], loss=106.3121
	step [125/249], loss=93.1582
	step [126/249], loss=96.2660
	step [127/249], loss=95.7597
	step [128/249], loss=98.9029
	step [129/249], loss=85.5560
	step [130/249], loss=100.2916
	step [131/249], loss=99.6862
	step [132/249], loss=106.8521
	step [133/249], loss=88.7563
	step [134/249], loss=107.1026
	step [135/249], loss=124.5136
	step [136/249], loss=107.1518
	step [137/249], loss=112.9365
	step [138/249], loss=110.8819
	step [139/249], loss=96.0927
	step [140/249], loss=92.6776
	step [141/249], loss=85.4119
	step [142/249], loss=77.4462
	step [143/249], loss=101.5503
	step [144/249], loss=105.5640
	step [145/249], loss=101.4145
	step [146/249], loss=98.6972
	step [147/249], loss=95.0956
	step [148/249], loss=100.0102
	step [149/249], loss=98.6140
	step [150/249], loss=98.5403
	step [151/249], loss=103.2846
	step [152/249], loss=100.9117
	step [153/249], loss=89.1058
	step [154/249], loss=92.2411
	step [155/249], loss=94.3011
	step [156/249], loss=96.7712
	step [157/249], loss=97.8223
	step [158/249], loss=88.7903
	step [159/249], loss=102.0540
	step [160/249], loss=96.1251
	step [161/249], loss=101.9182
	step [162/249], loss=88.5194
	step [163/249], loss=83.9140
	step [164/249], loss=85.0527
	step [165/249], loss=92.1690
	step [166/249], loss=102.2254
	step [167/249], loss=87.4951
	step [168/249], loss=82.2775
	step [169/249], loss=80.5365
	step [170/249], loss=94.6582
	step [171/249], loss=102.7333
	step [172/249], loss=88.9908
	step [173/249], loss=125.1722
	step [174/249], loss=101.8122
	step [175/249], loss=84.4040
	step [176/249], loss=80.0413
	step [177/249], loss=85.6582
	step [178/249], loss=97.2734
	step [179/249], loss=89.9536
	step [180/249], loss=90.8874
	step [181/249], loss=91.6728
	step [182/249], loss=85.2791
	step [183/249], loss=114.1102
	step [184/249], loss=115.8318
	step [185/249], loss=126.1208
	step [186/249], loss=96.3341
	step [187/249], loss=71.9292
	step [188/249], loss=106.4152
	step [189/249], loss=95.8294
	step [190/249], loss=69.8903
	step [191/249], loss=104.0607
	step [192/249], loss=92.9435
	step [193/249], loss=109.9464
	step [194/249], loss=102.1969
	step [195/249], loss=86.9415
	step [196/249], loss=86.0117
	step [197/249], loss=109.9630
	step [198/249], loss=89.9853
	step [199/249], loss=103.1290
	step [200/249], loss=88.1331
	step [201/249], loss=81.4474
	step [202/249], loss=88.5361
	step [203/249], loss=109.3946
	step [204/249], loss=107.1258
	step [205/249], loss=104.5281
	step [206/249], loss=109.5407
	step [207/249], loss=102.2465
	step [208/249], loss=85.2554
	step [209/249], loss=107.6633
	step [210/249], loss=97.4673
	step [211/249], loss=97.4021
	step [212/249], loss=94.7575
	step [213/249], loss=101.8755
	step [214/249], loss=105.5190
	step [215/249], loss=92.7693
	step [216/249], loss=90.7241
	step [217/249], loss=88.0825
	step [218/249], loss=100.9233
	step [219/249], loss=93.5399
	step [220/249], loss=102.1564
	step [221/249], loss=103.5112
	step [222/249], loss=77.1448
	step [223/249], loss=85.9416
	step [224/249], loss=100.9887
	step [225/249], loss=98.1196
	step [226/249], loss=72.5487
	step [227/249], loss=89.5698
	step [228/249], loss=107.1016
	step [229/249], loss=76.8127
	step [230/249], loss=88.3323
	step [231/249], loss=98.3425
	step [232/249], loss=95.0955
	step [233/249], loss=95.1699
	step [234/249], loss=94.5501
	step [235/249], loss=92.6534
	step [236/249], loss=89.7593
	step [237/249], loss=91.4107
	step [238/249], loss=103.5082
	step [239/249], loss=101.2378
	step [240/249], loss=90.8596
	step [241/249], loss=88.3157
	step [242/249], loss=96.2094
	step [243/249], loss=84.6079
	step [244/249], loss=98.4587
	step [245/249], loss=107.0859
	step [246/249], loss=102.0041
	step [247/249], loss=94.5281
	step [248/249], loss=90.1380
	step [249/249], loss=64.0940
	Evaluating
	loss=0.0098, precision=0.3929, recall=0.8921, f1=0.5455
Training epoch 22
	step [1/249], loss=111.5778
	step [2/249], loss=109.7571
	step [3/249], loss=95.1694
	step [4/249], loss=91.6939
	step [5/249], loss=89.0523
	step [6/249], loss=91.2946
	step [7/249], loss=83.5022
	step [8/249], loss=103.7513
	step [9/249], loss=107.5705
	step [10/249], loss=107.4109
	step [11/249], loss=117.1685
	step [12/249], loss=80.4898
	step [13/249], loss=87.0520
	step [14/249], loss=88.2874
	step [15/249], loss=102.0091
	step [16/249], loss=103.0571
	step [17/249], loss=101.6978
	step [18/249], loss=89.8677
	step [19/249], loss=88.1616
	step [20/249], loss=117.7634
	step [21/249], loss=88.9579
	step [22/249], loss=96.6933
	step [23/249], loss=120.8907
	step [24/249], loss=99.0392
	step [25/249], loss=99.6015
	step [26/249], loss=89.2278
	step [27/249], loss=91.8634
	step [28/249], loss=103.6519
	step [29/249], loss=97.6238
	step [30/249], loss=81.4170
	step [31/249], loss=94.4229
	step [32/249], loss=94.5525
	step [33/249], loss=97.6940
	step [34/249], loss=80.1595
	step [35/249], loss=105.7206
	step [36/249], loss=109.4164
	step [37/249], loss=107.5020
	step [38/249], loss=104.9264
	step [39/249], loss=94.7246
	step [40/249], loss=83.4806
	step [41/249], loss=104.1203
	step [42/249], loss=89.7702
	step [43/249], loss=91.6886
	step [44/249], loss=105.1418
	step [45/249], loss=95.2603
	step [46/249], loss=87.4633
	step [47/249], loss=99.1598
	step [48/249], loss=88.8770
	step [49/249], loss=83.7279
	step [50/249], loss=112.1770
	step [51/249], loss=88.7442
	step [52/249], loss=78.1423
	step [53/249], loss=93.4899
	step [54/249], loss=80.4643
	step [55/249], loss=95.0635
	step [56/249], loss=80.8036
	step [57/249], loss=94.6327
	step [58/249], loss=112.0362
	step [59/249], loss=95.2742
	step [60/249], loss=100.3514
	step [61/249], loss=99.1020
	step [62/249], loss=78.5397
	step [63/249], loss=97.0547
	step [64/249], loss=90.9698
	step [65/249], loss=90.1171
	step [66/249], loss=93.0934
	step [67/249], loss=84.9586
	step [68/249], loss=86.0742
	step [69/249], loss=101.6727
	step [70/249], loss=96.7005
	step [71/249], loss=95.7704
	step [72/249], loss=102.1168
	step [73/249], loss=106.1413
	step [74/249], loss=91.0084
	step [75/249], loss=76.2599
	step [76/249], loss=69.9011
	step [77/249], loss=88.6238
	step [78/249], loss=113.1965
	step [79/249], loss=87.3489
	step [80/249], loss=99.4107
	step [81/249], loss=66.0776
	step [82/249], loss=100.5166
	step [83/249], loss=89.1577
	step [84/249], loss=87.6504
	step [85/249], loss=114.2924
	step [86/249], loss=83.7915
	step [87/249], loss=96.0979
	step [88/249], loss=87.4079
	step [89/249], loss=80.1069
	step [90/249], loss=99.7606
	step [91/249], loss=73.5281
	step [92/249], loss=103.1227
	step [93/249], loss=93.6521
	step [94/249], loss=93.6459
	step [95/249], loss=98.0731
	step [96/249], loss=97.8369
	step [97/249], loss=88.6287
	step [98/249], loss=96.0475
	step [99/249], loss=110.0643
	step [100/249], loss=79.7776
	step [101/249], loss=92.6578
	step [102/249], loss=85.7270
	step [103/249], loss=97.6095
	step [104/249], loss=88.3690
	step [105/249], loss=106.0900
	step [106/249], loss=92.0133
	step [107/249], loss=98.6879
	step [108/249], loss=89.5892
	step [109/249], loss=94.5548
	step [110/249], loss=89.2612
	step [111/249], loss=95.2849
	step [112/249], loss=91.7812
	step [113/249], loss=78.5290
	step [114/249], loss=91.1452
	step [115/249], loss=116.1361
	step [116/249], loss=91.1251
	step [117/249], loss=90.5535
	step [118/249], loss=101.1978
	step [119/249], loss=115.2376
	step [120/249], loss=119.3708
	step [121/249], loss=118.0434
	step [122/249], loss=89.0876
	step [123/249], loss=101.1247
	step [124/249], loss=100.7285
	step [125/249], loss=94.3173
	step [126/249], loss=83.2811
	step [127/249], loss=88.0432
	step [128/249], loss=89.1638
	step [129/249], loss=87.8236
	step [130/249], loss=92.9293
	step [131/249], loss=89.1883
	step [132/249], loss=94.4254
	step [133/249], loss=91.2323
	step [134/249], loss=91.3791
	step [135/249], loss=98.3493
	step [136/249], loss=74.8473
	step [137/249], loss=94.6768
	step [138/249], loss=103.4431
	step [139/249], loss=116.8780
	step [140/249], loss=93.0676
	step [141/249], loss=81.9334
	step [142/249], loss=84.7427
	step [143/249], loss=102.8085
	step [144/249], loss=83.6612
	step [145/249], loss=88.9053
	step [146/249], loss=106.6418
	step [147/249], loss=105.9246
	step [148/249], loss=90.7499
	step [149/249], loss=111.9711
	step [150/249], loss=89.6231
	step [151/249], loss=96.1348
	step [152/249], loss=96.3443
	step [153/249], loss=90.8488
	step [154/249], loss=97.4496
	step [155/249], loss=93.1386
	step [156/249], loss=86.5321
	step [157/249], loss=77.9233
	step [158/249], loss=99.0066
	step [159/249], loss=102.9619
	step [160/249], loss=105.3274
	step [161/249], loss=78.0804
	step [162/249], loss=122.5972
	step [163/249], loss=106.7429
	step [164/249], loss=100.0362
	step [165/249], loss=100.0148
	step [166/249], loss=91.3895
	step [167/249], loss=91.5430
	step [168/249], loss=89.8335
	step [169/249], loss=100.6348
	step [170/249], loss=83.1986
	step [171/249], loss=98.5117
	step [172/249], loss=89.6350
	step [173/249], loss=100.4342
	step [174/249], loss=83.1179
	step [175/249], loss=99.5519
	step [176/249], loss=93.3716
	step [177/249], loss=91.9845
	step [178/249], loss=86.3150
	step [179/249], loss=110.0598
	step [180/249], loss=93.7662
	step [181/249], loss=90.9649
	step [182/249], loss=96.8283
	step [183/249], loss=85.2873
	step [184/249], loss=94.6543
	step [185/249], loss=85.3525
	step [186/249], loss=96.7014
	step [187/249], loss=96.1464
	step [188/249], loss=81.9999
	step [189/249], loss=86.8454
	step [190/249], loss=94.5081
	step [191/249], loss=117.3149
	step [192/249], loss=85.3740
	step [193/249], loss=97.3529
	step [194/249], loss=93.0397
	step [195/249], loss=118.4560
	step [196/249], loss=112.2853
	step [197/249], loss=107.8888
	step [198/249], loss=74.9335
	step [199/249], loss=92.8879
	step [200/249], loss=97.5397
	step [201/249], loss=83.0543
	step [202/249], loss=89.5323
	step [203/249], loss=87.6779
	step [204/249], loss=78.2897
	step [205/249], loss=97.5546
	step [206/249], loss=73.7986
	step [207/249], loss=79.1183
	step [208/249], loss=99.6427
	step [209/249], loss=97.2861
	step [210/249], loss=84.1687
	step [211/249], loss=98.8209
	step [212/249], loss=95.3930
	step [213/249], loss=85.6077
	step [214/249], loss=101.0926
	step [215/249], loss=101.0792
	step [216/249], loss=98.5480
	step [217/249], loss=87.6085
	step [218/249], loss=101.2400
	step [219/249], loss=92.2801
	step [220/249], loss=105.0642
	step [221/249], loss=85.8345
	step [222/249], loss=78.7173
	step [223/249], loss=93.8564
	step [224/249], loss=78.5299
	step [225/249], loss=105.3735
	step [226/249], loss=89.8331
	step [227/249], loss=90.9354
	step [228/249], loss=80.8593
	step [229/249], loss=95.4054
	step [230/249], loss=97.5406
	step [231/249], loss=107.9183
	step [232/249], loss=92.7126
	step [233/249], loss=83.1095
	step [234/249], loss=103.2085
	step [235/249], loss=89.6785
	step [236/249], loss=86.9695
	step [237/249], loss=92.3273
	step [238/249], loss=93.5646
	step [239/249], loss=99.3798
	step [240/249], loss=88.7197
	step [241/249], loss=110.2765
	step [242/249], loss=93.0972
	step [243/249], loss=94.0438
	step [244/249], loss=93.7277
	step [245/249], loss=90.9912
	step [246/249], loss=111.0997
	step [247/249], loss=101.3454
	step [248/249], loss=94.4056
	step [249/249], loss=73.6994
	Evaluating
	loss=0.0108, precision=0.3182, recall=0.8906, f1=0.4689
Training epoch 23
	step [1/249], loss=105.3582
	step [2/249], loss=92.8736
	step [3/249], loss=69.6988
	step [4/249], loss=100.3422
	step [5/249], loss=102.2977
	step [6/249], loss=90.9952
	step [7/249], loss=77.0241
	step [8/249], loss=98.8049
	step [9/249], loss=91.4601
	step [10/249], loss=88.8730
	step [11/249], loss=118.3646
	step [12/249], loss=91.8691
	step [13/249], loss=87.8990
	step [14/249], loss=89.1705
	step [15/249], loss=86.7220
	step [16/249], loss=93.3438
	step [17/249], loss=87.4028
	step [18/249], loss=98.9383
	step [19/249], loss=83.2095
	step [20/249], loss=104.3383
	step [21/249], loss=96.9106
	step [22/249], loss=106.7355
	step [23/249], loss=93.7870
	step [24/249], loss=101.6675
	step [25/249], loss=83.4484
	step [26/249], loss=97.8754
	step [27/249], loss=107.5818
	step [28/249], loss=99.2242
	step [29/249], loss=87.5540
	step [30/249], loss=102.6174
	step [31/249], loss=97.0472
	step [32/249], loss=91.7029
	step [33/249], loss=94.3177
	step [34/249], loss=84.7271
	step [35/249], loss=79.0117
	step [36/249], loss=99.0974
	step [37/249], loss=76.4054
	step [38/249], loss=92.7706
	step [39/249], loss=85.1388
	step [40/249], loss=93.0556
	step [41/249], loss=98.2059
	step [42/249], loss=104.6796
	step [43/249], loss=89.5973
	step [44/249], loss=89.3442
	step [45/249], loss=95.6257
	step [46/249], loss=72.1933
	step [47/249], loss=103.9957
	step [48/249], loss=107.5233
	step [49/249], loss=103.9219
	step [50/249], loss=94.8693
	step [51/249], loss=101.1790
	step [52/249], loss=91.0125
	step [53/249], loss=94.1463
	step [54/249], loss=92.4619
	step [55/249], loss=83.3444
	step [56/249], loss=80.8996
	step [57/249], loss=95.1243
	step [58/249], loss=74.3714
	step [59/249], loss=100.0623
	step [60/249], loss=84.0802
	step [61/249], loss=90.2030
	step [62/249], loss=83.4496
	step [63/249], loss=82.7485
	step [64/249], loss=91.1098
	step [65/249], loss=95.3078
	step [66/249], loss=96.6279
	step [67/249], loss=77.1585
	step [68/249], loss=89.4124
	step [69/249], loss=90.8824
	step [70/249], loss=87.9834
	step [71/249], loss=100.4260
	step [72/249], loss=70.5188
	step [73/249], loss=91.6971
	step [74/249], loss=99.9164
	step [75/249], loss=86.9034
	step [76/249], loss=103.4173
	step [77/249], loss=129.5555
	step [78/249], loss=93.0115
	step [79/249], loss=76.8880
	step [80/249], loss=110.9531
	step [81/249], loss=94.0385
	step [82/249], loss=115.8016
	step [83/249], loss=110.6588
	step [84/249], loss=91.1623
	step [85/249], loss=79.6690
	step [86/249], loss=64.0942
	step [87/249], loss=80.7615
	step [88/249], loss=98.7104
	step [89/249], loss=99.9241
	step [90/249], loss=99.9611
	step [91/249], loss=106.2488
	step [92/249], loss=78.1072
	step [93/249], loss=85.9812
	step [94/249], loss=93.2801
	step [95/249], loss=98.3831
	step [96/249], loss=86.5873
	step [97/249], loss=97.8802
	step [98/249], loss=98.2599
	step [99/249], loss=90.1001
	step [100/249], loss=95.3947
	step [101/249], loss=85.5286
	step [102/249], loss=82.0814
	step [103/249], loss=79.6386
	step [104/249], loss=92.4910
	step [105/249], loss=113.0300
	step [106/249], loss=99.7360
	step [107/249], loss=96.5120
	step [108/249], loss=88.3204
	step [109/249], loss=82.5538
	step [110/249], loss=97.2213
	step [111/249], loss=101.3764
	step [112/249], loss=94.6026
	step [113/249], loss=80.6184
	step [114/249], loss=92.3890
	step [115/249], loss=94.8521
	step [116/249], loss=91.6411
	step [117/249], loss=89.6384
	step [118/249], loss=74.3379
	step [119/249], loss=78.3952
	step [120/249], loss=79.6454
	step [121/249], loss=106.2152
	step [122/249], loss=90.2768
	step [123/249], loss=104.5662
	step [124/249], loss=94.2317
	step [125/249], loss=90.1844
	step [126/249], loss=84.3070
	step [127/249], loss=96.0450
	step [128/249], loss=88.5304
	step [129/249], loss=97.7844
	step [130/249], loss=121.0287
	step [131/249], loss=92.9827
	step [132/249], loss=99.0541
	step [133/249], loss=91.7499
	step [134/249], loss=89.2971
	step [135/249], loss=117.9672
	step [136/249], loss=90.3546
	step [137/249], loss=95.4120
	step [138/249], loss=86.2081
	step [139/249], loss=123.0310
	step [140/249], loss=101.5510
	step [141/249], loss=78.6611
	step [142/249], loss=82.4853
	step [143/249], loss=72.1852
	step [144/249], loss=91.9775
	step [145/249], loss=89.1566
	step [146/249], loss=95.9012
	step [147/249], loss=95.1108
	step [148/249], loss=101.1739
	step [149/249], loss=92.9535
	step [150/249], loss=82.1225
	step [151/249], loss=91.7533
	step [152/249], loss=90.2885
	step [153/249], loss=98.9924
	step [154/249], loss=92.5429
	step [155/249], loss=70.0059
	step [156/249], loss=81.0726
	step [157/249], loss=100.8185
	step [158/249], loss=95.1478
	step [159/249], loss=79.6836
	step [160/249], loss=116.4368
	step [161/249], loss=93.1967
	step [162/249], loss=86.7457
	step [163/249], loss=80.7944
	step [164/249], loss=88.8307
	step [165/249], loss=113.6248
	step [166/249], loss=104.7998
	step [167/249], loss=88.8747
	step [168/249], loss=92.4047
	step [169/249], loss=77.2091
	step [170/249], loss=78.5247
	step [171/249], loss=97.7515
	step [172/249], loss=87.7608
	step [173/249], loss=93.9347
	step [174/249], loss=107.6941
	step [175/249], loss=87.0810
	step [176/249], loss=105.5466
	step [177/249], loss=89.7336
	step [178/249], loss=96.2035
	step [179/249], loss=100.3625
	step [180/249], loss=103.6518
	step [181/249], loss=90.6532
	step [182/249], loss=80.5497
	step [183/249], loss=97.5417
	step [184/249], loss=87.4774
	step [185/249], loss=76.9519
	step [186/249], loss=84.1290
	step [187/249], loss=104.7956
	step [188/249], loss=105.3999
	step [189/249], loss=100.2119
	step [190/249], loss=103.9720
	step [191/249], loss=92.5199
	step [192/249], loss=76.1526
	step [193/249], loss=108.1965
	step [194/249], loss=82.2264
	step [195/249], loss=82.7104
	step [196/249], loss=86.5811
	step [197/249], loss=109.2084
	step [198/249], loss=113.1131
	step [199/249], loss=87.9693
	step [200/249], loss=78.7945
	step [201/249], loss=95.3166
	step [202/249], loss=95.1684
	step [203/249], loss=83.9312
	step [204/249], loss=88.2948
	step [205/249], loss=110.0148
	step [206/249], loss=100.3704
	step [207/249], loss=90.0897
	step [208/249], loss=95.9637
	step [209/249], loss=94.8405
	step [210/249], loss=105.5382
	step [211/249], loss=93.9281
	step [212/249], loss=91.2061
	step [213/249], loss=87.0711
	step [214/249], loss=96.3857
	step [215/249], loss=83.0702
	step [216/249], loss=99.5295
	step [217/249], loss=77.3235
	step [218/249], loss=89.8898
	step [219/249], loss=89.7323
	step [220/249], loss=79.4841
	step [221/249], loss=95.6808
	step [222/249], loss=96.0404
	step [223/249], loss=94.7204
	step [224/249], loss=76.8303
	step [225/249], loss=88.9688
	step [226/249], loss=96.7109
	step [227/249], loss=115.7039
	step [228/249], loss=94.2468
	step [229/249], loss=86.5136
	step [230/249], loss=109.3244
	step [231/249], loss=84.8092
	step [232/249], loss=82.3649
	step [233/249], loss=87.9736
	step [234/249], loss=93.5830
	step [235/249], loss=82.0622
	step [236/249], loss=110.2393
	step [237/249], loss=75.4897
	step [238/249], loss=99.6489
	step [239/249], loss=88.2932
	step [240/249], loss=120.2589
	step [241/249], loss=79.0888
	step [242/249], loss=92.8998
	step [243/249], loss=93.0578
	step [244/249], loss=89.2051
	step [245/249], loss=73.2713
	step [246/249], loss=100.0340
	step [247/249], loss=102.5143
	step [248/249], loss=116.1438
	step [249/249], loss=61.5179
	Evaluating
	loss=0.0106, precision=0.3044, recall=0.8996, f1=0.4549
Training epoch 24
	step [1/249], loss=102.7726
	step [2/249], loss=90.5968
	step [3/249], loss=98.3471
	step [4/249], loss=98.9010
	step [5/249], loss=96.0261
	step [6/249], loss=84.0545
	step [7/249], loss=86.2184
	step [8/249], loss=95.1779
	step [9/249], loss=94.0814
	step [10/249], loss=102.9030
	step [11/249], loss=95.1224
	step [12/249], loss=97.0194
	step [13/249], loss=101.2425
	step [14/249], loss=94.7370
	step [15/249], loss=112.4722
	step [16/249], loss=91.8500
	step [17/249], loss=100.4221
	step [18/249], loss=86.4024
	step [19/249], loss=88.7926
	step [20/249], loss=96.7027
	step [21/249], loss=87.0867
	step [22/249], loss=83.3964
	step [23/249], loss=86.2442
	step [24/249], loss=91.4416
	step [25/249], loss=101.4259
	step [26/249], loss=91.8922
	step [27/249], loss=107.1252
	step [28/249], loss=89.4991
	step [29/249], loss=92.0168
	step [30/249], loss=86.3014
	step [31/249], loss=91.1185
	step [32/249], loss=77.8625
	step [33/249], loss=94.9900
	step [34/249], loss=111.1475
	step [35/249], loss=106.1054
	step [36/249], loss=82.6822
	step [37/249], loss=104.8431
	step [38/249], loss=105.9148
	step [39/249], loss=90.8330
	step [40/249], loss=112.5195
	step [41/249], loss=78.2915
	step [42/249], loss=94.7227
	step [43/249], loss=80.1272
	step [44/249], loss=79.3366
	step [45/249], loss=110.7192
	step [46/249], loss=96.1462
	step [47/249], loss=86.1145
	step [48/249], loss=80.5097
	step [49/249], loss=86.7563
	step [50/249], loss=85.4718
	step [51/249], loss=102.5822
	step [52/249], loss=93.2614
	step [53/249], loss=80.1388
	step [54/249], loss=88.0488
	step [55/249], loss=89.1209
	step [56/249], loss=77.1158
	step [57/249], loss=101.7233
	step [58/249], loss=91.0732
	step [59/249], loss=71.9548
	step [60/249], loss=87.4567
	step [61/249], loss=88.8512
	step [62/249], loss=92.0339
	step [63/249], loss=99.3403
	step [64/249], loss=91.1245
	step [65/249], loss=99.2063
	step [66/249], loss=82.3826
	step [67/249], loss=100.3567
	step [68/249], loss=94.0459
	step [69/249], loss=89.2816
	step [70/249], loss=80.8552
	step [71/249], loss=81.5565
	step [72/249], loss=75.7380
	step [73/249], loss=95.3408
	step [74/249], loss=87.8322
	step [75/249], loss=92.2550
	step [76/249], loss=84.4221
	step [77/249], loss=88.5244
	step [78/249], loss=78.3608
	step [79/249], loss=82.6899
	step [80/249], loss=81.4812
	step [81/249], loss=99.2827
	step [82/249], loss=92.7846
	step [83/249], loss=95.7200
	step [84/249], loss=77.9533
	step [85/249], loss=92.2332
	step [86/249], loss=89.2390
	step [87/249], loss=102.3623
	step [88/249], loss=100.0935
	step [89/249], loss=82.6191
	step [90/249], loss=96.0379
	step [91/249], loss=109.5212
	step [92/249], loss=77.6971
	step [93/249], loss=105.3202
	step [94/249], loss=97.7498
	step [95/249], loss=89.4377
	step [96/249], loss=108.1812
	step [97/249], loss=82.3036
	step [98/249], loss=80.7886
	step [99/249], loss=96.7127
	step [100/249], loss=106.4945
	step [101/249], loss=114.6421
	step [102/249], loss=95.2228
	step [103/249], loss=67.3557
	step [104/249], loss=103.1599
	step [105/249], loss=89.1442
	step [106/249], loss=89.0294
	step [107/249], loss=87.7431
	step [108/249], loss=97.3826
	step [109/249], loss=96.7989
	step [110/249], loss=86.8667
	step [111/249], loss=108.6978
	step [112/249], loss=91.9518
	step [113/249], loss=86.9600
	step [114/249], loss=92.7024
	step [115/249], loss=100.5113
	step [116/249], loss=76.9444
	step [117/249], loss=68.1856
	step [118/249], loss=87.2660
	step [119/249], loss=99.6676
	step [120/249], loss=93.2015
	step [121/249], loss=102.4296
	step [122/249], loss=104.6979
	step [123/249], loss=98.9374
	step [124/249], loss=90.3914
	step [125/249], loss=93.2998
	step [126/249], loss=104.4837
	step [127/249], loss=97.3003
	step [128/249], loss=82.6375
	step [129/249], loss=92.5979
	step [130/249], loss=103.4772
	step [131/249], loss=92.3487
	step [132/249], loss=94.4949
	step [133/249], loss=104.6678
	step [134/249], loss=79.8251
	step [135/249], loss=111.5125
	step [136/249], loss=71.6960
	step [137/249], loss=80.2352
	step [138/249], loss=88.4069
	step [139/249], loss=93.4671
	step [140/249], loss=71.0153
	step [141/249], loss=92.7325
	step [142/249], loss=88.1512
	step [143/249], loss=92.0030
	step [144/249], loss=96.7363
	step [145/249], loss=79.9867
	step [146/249], loss=84.3687
	step [147/249], loss=98.8143
	step [148/249], loss=92.1807
	step [149/249], loss=89.0112
	step [150/249], loss=86.3087
	step [151/249], loss=100.6495
	step [152/249], loss=87.8458
	step [153/249], loss=99.9760
	step [154/249], loss=95.6732
	step [155/249], loss=99.4859
	step [156/249], loss=95.6701
	step [157/249], loss=96.8808
	step [158/249], loss=92.2559
	step [159/249], loss=94.3438
	step [160/249], loss=93.5481
	step [161/249], loss=107.5268
	step [162/249], loss=95.3652
	step [163/249], loss=106.3427
	step [164/249], loss=80.3307
	step [165/249], loss=92.8319
	step [166/249], loss=93.4193
	step [167/249], loss=96.5912
	step [168/249], loss=99.2089
	step [169/249], loss=81.8899
	step [170/249], loss=82.2768
	step [171/249], loss=81.4320
	step [172/249], loss=86.7561
	step [173/249], loss=102.9552
	step [174/249], loss=87.2182
	step [175/249], loss=100.7539
	step [176/249], loss=95.5437
	step [177/249], loss=100.1948
	step [178/249], loss=109.6003
	step [179/249], loss=86.8834
	step [180/249], loss=87.3297
	step [181/249], loss=104.6263
	step [182/249], loss=85.7242
	step [183/249], loss=82.6227
	step [184/249], loss=81.5355
	step [185/249], loss=94.9062
	step [186/249], loss=96.5976
	step [187/249], loss=92.1047
	step [188/249], loss=77.5938
	step [189/249], loss=92.5326
	step [190/249], loss=97.1646
	step [191/249], loss=109.9506
	step [192/249], loss=96.6089
	step [193/249], loss=96.7046
	step [194/249], loss=75.0003
	step [195/249], loss=92.3162
	step [196/249], loss=94.6850
	step [197/249], loss=64.5913
	step [198/249], loss=76.9897
	step [199/249], loss=96.6511
	step [200/249], loss=98.2654
	step [201/249], loss=87.0774
	step [202/249], loss=98.8704
	step [203/249], loss=90.4387
	step [204/249], loss=84.4247
	step [205/249], loss=81.8796
	step [206/249], loss=83.4826
	step [207/249], loss=97.7689
	step [208/249], loss=70.3779
	step [209/249], loss=68.2471
	step [210/249], loss=86.6807
	step [211/249], loss=90.7787
	step [212/249], loss=101.2487
	step [213/249], loss=107.7912
	step [214/249], loss=88.4562
	step [215/249], loss=84.1259
	step [216/249], loss=84.9354
	step [217/249], loss=101.2942
	step [218/249], loss=96.5209
	step [219/249], loss=78.8301
	step [220/249], loss=88.5647
	step [221/249], loss=98.1167
	step [222/249], loss=87.0881
	step [223/249], loss=74.0042
	step [224/249], loss=82.6979
	step [225/249], loss=101.7279
	step [226/249], loss=102.1936
	step [227/249], loss=78.1911
	step [228/249], loss=89.4157
	step [229/249], loss=99.8884
	step [230/249], loss=119.2889
	step [231/249], loss=83.5816
	step [232/249], loss=83.2730
	step [233/249], loss=75.5872
	step [234/249], loss=83.8320
	step [235/249], loss=78.6664
	step [236/249], loss=89.3861
	step [237/249], loss=71.2258
	step [238/249], loss=103.9985
	step [239/249], loss=88.2173
	step [240/249], loss=102.2074
	step [241/249], loss=93.9226
	step [242/249], loss=86.2170
	step [243/249], loss=90.4116
	step [244/249], loss=99.8658
	step [245/249], loss=86.0568
	step [246/249], loss=86.4910
	step [247/249], loss=82.2664
	step [248/249], loss=94.1992
	step [249/249], loss=65.8051
	Evaluating
	loss=0.0085, precision=0.3732, recall=0.8612, f1=0.5207
Training epoch 25
	step [1/249], loss=95.5612
	step [2/249], loss=79.7606
	step [3/249], loss=93.8207
	step [4/249], loss=88.9547
	step [5/249], loss=93.2656
	step [6/249], loss=81.3212
	step [7/249], loss=102.9014
	step [8/249], loss=84.5429
	step [9/249], loss=106.3382
	step [10/249], loss=83.1779
	step [11/249], loss=78.1448
	step [12/249], loss=69.6160
	step [13/249], loss=87.5708
	step [14/249], loss=93.1640
	step [15/249], loss=86.0545
	step [16/249], loss=89.9879
	step [17/249], loss=86.1975
	step [18/249], loss=99.5062
	step [19/249], loss=98.7256
	step [20/249], loss=93.0768
	step [21/249], loss=102.7344
	step [22/249], loss=99.9861
	step [23/249], loss=84.9122
	step [24/249], loss=83.4865
	step [25/249], loss=65.2566
	step [26/249], loss=108.8202
	step [27/249], loss=92.9465
	step [28/249], loss=74.5337
	step [29/249], loss=106.2619
	step [30/249], loss=100.9164
	step [31/249], loss=100.4162
	step [32/249], loss=97.8760
	step [33/249], loss=110.2357
	step [34/249], loss=77.2840
	step [35/249], loss=114.3849
	step [36/249], loss=86.6040
	step [37/249], loss=91.1366
	step [38/249], loss=84.8738
	step [39/249], loss=97.4473
	step [40/249], loss=85.4528
	step [41/249], loss=88.5046
	step [42/249], loss=96.4440
	step [43/249], loss=76.2750
	step [44/249], loss=90.0116
	step [45/249], loss=98.1825
	step [46/249], loss=108.5209
	step [47/249], loss=107.9595
	step [48/249], loss=100.2710
	step [49/249], loss=99.0965
	step [50/249], loss=90.3094
	step [51/249], loss=107.6347
	step [52/249], loss=98.4576
	step [53/249], loss=81.1185
	step [54/249], loss=79.6943
	step [55/249], loss=95.7326
	step [56/249], loss=85.8499
	step [57/249], loss=89.9475
	step [58/249], loss=80.5444
	step [59/249], loss=95.7179
	step [60/249], loss=84.9336
	step [61/249], loss=94.5850
	step [62/249], loss=88.9428
	step [63/249], loss=100.4919
	step [64/249], loss=103.4876
	step [65/249], loss=83.2649
	step [66/249], loss=77.6995
	step [67/249], loss=96.8786
	step [68/249], loss=93.5087
	step [69/249], loss=101.2532
	step [70/249], loss=96.7632
	step [71/249], loss=83.2930
	step [72/249], loss=80.9704
	step [73/249], loss=79.2398
	step [74/249], loss=80.6528
	step [75/249], loss=77.2721
	step [76/249], loss=85.7537
	step [77/249], loss=79.2395
	step [78/249], loss=97.7502
	step [79/249], loss=93.0498
	step [80/249], loss=93.7756
	step [81/249], loss=97.0740
	step [82/249], loss=102.7141
	step [83/249], loss=88.8591
	step [84/249], loss=99.3218
	step [85/249], loss=88.2886
	step [86/249], loss=105.6984
	step [87/249], loss=79.2688
	step [88/249], loss=87.0637
	step [89/249], loss=81.6374
	step [90/249], loss=89.5758
	step [91/249], loss=79.9201
	step [92/249], loss=76.7024
	step [93/249], loss=77.9700
	step [94/249], loss=97.2781
	step [95/249], loss=86.3752
	step [96/249], loss=85.3535
	step [97/249], loss=72.6779
	step [98/249], loss=77.4897
	step [99/249], loss=104.5919
	step [100/249], loss=110.8561
	step [101/249], loss=84.3871
	step [102/249], loss=94.7109
	step [103/249], loss=88.8345
	step [104/249], loss=105.8957
	step [105/249], loss=83.6637
	step [106/249], loss=82.4173
	step [107/249], loss=89.7192
	step [108/249], loss=86.0660
	step [109/249], loss=95.6183
	step [110/249], loss=83.0892
	step [111/249], loss=83.1778
	step [112/249], loss=78.3423
	step [113/249], loss=93.8719
	step [114/249], loss=74.1290
	step [115/249], loss=88.6838
	step [116/249], loss=97.0572
	step [117/249], loss=78.8659
	step [118/249], loss=94.6086
	step [119/249], loss=76.8679
	step [120/249], loss=78.5599
	step [121/249], loss=84.4473
	step [122/249], loss=80.8746
	step [123/249], loss=94.5574
	step [124/249], loss=76.9008
	step [125/249], loss=91.4736
	step [126/249], loss=92.9707
	step [127/249], loss=73.1483
	step [128/249], loss=98.0032
	step [129/249], loss=97.6938
	step [130/249], loss=99.4000
	step [131/249], loss=84.9255
	step [132/249], loss=98.4568
	step [133/249], loss=101.3465
	step [134/249], loss=104.4824
	step [135/249], loss=78.7380
	step [136/249], loss=101.9099
	step [137/249], loss=74.9012
	step [138/249], loss=82.5814
	step [139/249], loss=74.3115
	step [140/249], loss=103.1761
	step [141/249], loss=69.8174
	step [142/249], loss=89.4330
	step [143/249], loss=115.9902
	step [144/249], loss=93.3440
	step [145/249], loss=85.9793
	step [146/249], loss=100.4616
	step [147/249], loss=71.5188
	step [148/249], loss=75.5953
	step [149/249], loss=83.4625
	step [150/249], loss=94.3968
	step [151/249], loss=89.6815
	step [152/249], loss=83.4227
	step [153/249], loss=100.3305
	step [154/249], loss=89.5295
	step [155/249], loss=87.8892
	step [156/249], loss=90.4870
	step [157/249], loss=98.8028
	step [158/249], loss=79.9855
	step [159/249], loss=82.9333
	step [160/249], loss=94.9824
	step [161/249], loss=105.3869
	step [162/249], loss=87.3910
	step [163/249], loss=99.1899
	step [164/249], loss=89.4296
	step [165/249], loss=76.0375
	step [166/249], loss=96.9188
	step [167/249], loss=90.3669
	step [168/249], loss=96.1824
	step [169/249], loss=93.9214
	step [170/249], loss=82.8659
	step [171/249], loss=86.7948
	step [172/249], loss=106.6290
	step [173/249], loss=89.6429
	step [174/249], loss=81.7395
	step [175/249], loss=81.9106
	step [176/249], loss=89.9369
	step [177/249], loss=89.7341
	step [178/249], loss=98.2747
	step [179/249], loss=93.9645
	step [180/249], loss=81.6367
	step [181/249], loss=89.2557
	step [182/249], loss=72.0957
	step [183/249], loss=84.0335
	step [184/249], loss=85.4265
	step [185/249], loss=79.6288
	step [186/249], loss=85.3704
	step [187/249], loss=93.5369
	step [188/249], loss=94.4592
	step [189/249], loss=80.6436
	step [190/249], loss=78.0363
	step [191/249], loss=109.9547
	step [192/249], loss=79.3709
	step [193/249], loss=118.3690
	step [194/249], loss=74.8329
	step [195/249], loss=89.3722
	step [196/249], loss=90.6945
	step [197/249], loss=91.8524
	step [198/249], loss=94.7791
	step [199/249], loss=93.1473
	step [200/249], loss=65.6803
	step [201/249], loss=87.3219
	step [202/249], loss=102.7550
	step [203/249], loss=93.1877
	step [204/249], loss=108.4779
	step [205/249], loss=90.5089
	step [206/249], loss=111.3442
	step [207/249], loss=89.7476
	step [208/249], loss=87.0187
	step [209/249], loss=81.7256
	step [210/249], loss=84.9717
	step [211/249], loss=112.3232
	step [212/249], loss=78.0350
	step [213/249], loss=96.8496
	step [214/249], loss=90.8961
	step [215/249], loss=115.6724
	step [216/249], loss=99.6697
	step [217/249], loss=87.3068
	step [218/249], loss=100.6692
	step [219/249], loss=91.5116
	step [220/249], loss=116.8659
	step [221/249], loss=80.8145
	step [222/249], loss=78.8675
	step [223/249], loss=94.6374
	step [224/249], loss=95.5273
	step [225/249], loss=85.3057
	step [226/249], loss=91.4914
	step [227/249], loss=95.5486
	step [228/249], loss=91.5758
	step [229/249], loss=95.0586
	step [230/249], loss=83.6001
	step [231/249], loss=83.8455
	step [232/249], loss=102.9977
	step [233/249], loss=87.4706
	step [234/249], loss=99.9454
	step [235/249], loss=100.0843
	step [236/249], loss=72.7184
	step [237/249], loss=92.9973
	step [238/249], loss=87.4328
	step [239/249], loss=108.6559
	step [240/249], loss=93.2149
	step [241/249], loss=107.9213
	step [242/249], loss=84.0751
	step [243/249], loss=113.4581
	step [244/249], loss=100.3643
	step [245/249], loss=79.8575
	step [246/249], loss=80.8481
	step [247/249], loss=86.0001
	step [248/249], loss=98.3200
	step [249/249], loss=56.4864
	Evaluating
	loss=0.0087, precision=0.3668, recall=0.9084, f1=0.5226
Training epoch 26
	step [1/249], loss=94.8032
	step [2/249], loss=95.8907
	step [3/249], loss=93.3799
	step [4/249], loss=81.0986
	step [5/249], loss=83.5753
	step [6/249], loss=83.2019
	step [7/249], loss=89.7475
	step [8/249], loss=102.4501
	step [9/249], loss=79.5362
	step [10/249], loss=93.2357
	step [11/249], loss=88.3800
	step [12/249], loss=99.2839
	step [13/249], loss=82.6736
	step [14/249], loss=87.6951
	step [15/249], loss=88.2426
	step [16/249], loss=81.3255
	step [17/249], loss=113.6723
	step [18/249], loss=82.8991
	step [19/249], loss=76.1451
	step [20/249], loss=96.2339
	step [21/249], loss=75.5909
	step [22/249], loss=99.0237
	step [23/249], loss=81.1950
	step [24/249], loss=96.2724
	step [25/249], loss=75.2335
	step [26/249], loss=78.8846
	step [27/249], loss=85.7807
	step [28/249], loss=96.8569
	step [29/249], loss=93.2448
	step [30/249], loss=97.4134
	step [31/249], loss=97.9815
	step [32/249], loss=75.7031
	step [33/249], loss=90.1917
	step [34/249], loss=91.0516
	step [35/249], loss=88.3905
	step [36/249], loss=80.6863
	step [37/249], loss=93.7015
	step [38/249], loss=86.6301
	step [39/249], loss=91.9687
	step [40/249], loss=99.8230
	step [41/249], loss=92.7569
	step [42/249], loss=85.1363
	step [43/249], loss=109.9207
	step [44/249], loss=117.1286
	step [45/249], loss=86.4197
	step [46/249], loss=83.5883
	step [47/249], loss=72.8271
	step [48/249], loss=96.5362
	step [49/249], loss=78.1662
	step [50/249], loss=61.4704
	step [51/249], loss=76.8238
	step [52/249], loss=70.9872
	step [53/249], loss=71.8349
	step [54/249], loss=103.4888
	step [55/249], loss=80.3851
	step [56/249], loss=98.1784
	step [57/249], loss=97.2952
	step [58/249], loss=97.9104
	step [59/249], loss=92.8527
	step [60/249], loss=72.5636
	step [61/249], loss=111.9595
	step [62/249], loss=82.2311
	step [63/249], loss=77.1819
	step [64/249], loss=80.5483
	step [65/249], loss=65.8570
	step [66/249], loss=91.8996
	step [67/249], loss=88.2490
	step [68/249], loss=85.6507
	step [69/249], loss=86.6920
	step [70/249], loss=91.3246
	step [71/249], loss=92.9637
	step [72/249], loss=90.7009
	step [73/249], loss=88.9937
	step [74/249], loss=89.8244
	step [75/249], loss=86.7637
	step [76/249], loss=77.7916
	step [77/249], loss=85.0041
	step [78/249], loss=81.3341
	step [79/249], loss=102.2968
	step [80/249], loss=106.0839
	step [81/249], loss=78.2251
	step [82/249], loss=95.7439
	step [83/249], loss=89.1058
	step [84/249], loss=98.2094
	step [85/249], loss=87.3088
	step [86/249], loss=84.7002
	step [87/249], loss=81.5045
	step [88/249], loss=89.7908
	step [89/249], loss=92.7134
	step [90/249], loss=91.9113
	step [91/249], loss=83.0304
	step [92/249], loss=71.0426
	step [93/249], loss=88.8200
	step [94/249], loss=86.6842
	step [95/249], loss=88.6315
	step [96/249], loss=80.0913
	step [97/249], loss=62.9480
	step [98/249], loss=97.5356
	step [99/249], loss=74.6445
	step [100/249], loss=97.9429
	step [101/249], loss=87.4086
	step [102/249], loss=81.0883
	step [103/249], loss=83.5825
	step [104/249], loss=98.5526
	step [105/249], loss=86.9561
	step [106/249], loss=111.5374
	step [107/249], loss=93.3595
	step [108/249], loss=103.6525
	step [109/249], loss=114.5436
	step [110/249], loss=95.8997
	step [111/249], loss=75.8344
	step [112/249], loss=96.4654
	step [113/249], loss=110.0141
	step [114/249], loss=101.4393
	step [115/249], loss=86.4707
	step [116/249], loss=79.2672
	step [117/249], loss=86.3948
	step [118/249], loss=97.3890
	step [119/249], loss=101.9473
	step [120/249], loss=106.2371
	step [121/249], loss=97.8329
	step [122/249], loss=76.7404
	step [123/249], loss=98.1211
	step [124/249], loss=71.0370
	step [125/249], loss=72.8776
	step [126/249], loss=89.8431
	step [127/249], loss=69.6557
	step [128/249], loss=89.4640
	step [129/249], loss=97.0097
	step [130/249], loss=95.4805
	step [131/249], loss=72.4224
	step [132/249], loss=88.3197
	step [133/249], loss=86.0121
	step [134/249], loss=80.3056
	step [135/249], loss=101.9646
	step [136/249], loss=96.4670
	step [137/249], loss=104.3472
	step [138/249], loss=90.1488
	step [139/249], loss=103.3918
	step [140/249], loss=104.0011
	step [141/249], loss=97.0247
	step [142/249], loss=105.6735
	step [143/249], loss=78.4301
	step [144/249], loss=65.0649
	step [145/249], loss=92.1480
	step [146/249], loss=103.2749
	step [147/249], loss=75.8564
	step [148/249], loss=110.7117
	step [149/249], loss=104.1450
	step [150/249], loss=87.0447
	step [151/249], loss=75.9072
	step [152/249], loss=97.6949
	step [153/249], loss=78.3390
	step [154/249], loss=89.6741
	step [155/249], loss=73.2201
	step [156/249], loss=72.8831
	step [157/249], loss=91.8295
	step [158/249], loss=81.2444
	step [159/249], loss=98.9647
	step [160/249], loss=76.4725
	step [161/249], loss=89.8087
	step [162/249], loss=93.5398
	step [163/249], loss=106.8920
	step [164/249], loss=93.7146
	step [165/249], loss=93.7439
	step [166/249], loss=81.3192
	step [167/249], loss=83.6338
	step [168/249], loss=75.6436
	step [169/249], loss=82.6996
	step [170/249], loss=87.6805
	step [171/249], loss=92.9983
	step [172/249], loss=88.3367
	step [173/249], loss=87.1959
	step [174/249], loss=87.1261
	step [175/249], loss=82.4961
	step [176/249], loss=78.7509
	step [177/249], loss=93.4600
	step [178/249], loss=83.1518
	step [179/249], loss=90.6349
	step [180/249], loss=92.4898
	step [181/249], loss=90.0523
	step [182/249], loss=95.4840
	step [183/249], loss=81.3716
	step [184/249], loss=100.3842
	step [185/249], loss=97.4266
	step [186/249], loss=91.7132
	step [187/249], loss=96.9974
	step [188/249], loss=86.6601
	step [189/249], loss=89.7436
	step [190/249], loss=80.0307
	step [191/249], loss=81.3486
	step [192/249], loss=85.8421
	step [193/249], loss=82.4485
	step [194/249], loss=77.1176
	step [195/249], loss=76.2895
	step [196/249], loss=89.3651
	step [197/249], loss=108.6421
	step [198/249], loss=90.8551
	step [199/249], loss=92.5159
	step [200/249], loss=88.7981
	step [201/249], loss=83.0644
	step [202/249], loss=89.1637
	step [203/249], loss=86.8450
	step [204/249], loss=80.2409
	step [205/249], loss=80.2942
	step [206/249], loss=74.2487
	step [207/249], loss=91.6984
	step [208/249], loss=98.7148
	step [209/249], loss=94.3572
	step [210/249], loss=80.1238
	step [211/249], loss=80.9532
	step [212/249], loss=97.8424
	step [213/249], loss=88.8059
	step [214/249], loss=70.2765
	step [215/249], loss=86.4340
	step [216/249], loss=103.0259
	step [217/249], loss=101.8391
	step [218/249], loss=92.1750
	step [219/249], loss=89.0965
	step [220/249], loss=79.9075
	step [221/249], loss=84.2118
	step [222/249], loss=81.7475
	step [223/249], loss=93.0295
	step [224/249], loss=86.3695
	step [225/249], loss=102.3455
	step [226/249], loss=93.6583
	step [227/249], loss=89.7156
	step [228/249], loss=80.4506
	step [229/249], loss=84.2811
	step [230/249], loss=87.0092
	step [231/249], loss=83.2097
	step [232/249], loss=84.1990
	step [233/249], loss=112.5869
	step [234/249], loss=93.8358
	step [235/249], loss=82.6687
	step [236/249], loss=87.6202
	step [237/249], loss=89.8424
	step [238/249], loss=84.5492
	step [239/249], loss=86.2973
	step [240/249], loss=87.9919
	step [241/249], loss=89.8435
	step [242/249], loss=109.5762
	step [243/249], loss=105.7841
	step [244/249], loss=104.8414
	step [245/249], loss=81.8967
	step [246/249], loss=87.3443
	step [247/249], loss=80.0281
	step [248/249], loss=77.1428
	step [249/249], loss=84.3893
	Evaluating
	loss=0.0084, precision=0.3440, recall=0.8760, f1=0.4940
Training epoch 27
	step [1/249], loss=69.6612
	step [2/249], loss=101.9819
	step [3/249], loss=81.6688
	step [4/249], loss=83.5101
	step [5/249], loss=100.9820
	step [6/249], loss=92.5642
	step [7/249], loss=75.9540
	step [8/249], loss=77.3839
	step [9/249], loss=91.5988
	step [10/249], loss=72.8393
	step [11/249], loss=91.5538
	step [12/249], loss=97.9661
	step [13/249], loss=110.1139
	step [14/249], loss=110.3711
	step [15/249], loss=87.2051
	step [16/249], loss=78.3853
	step [17/249], loss=82.3668
	step [18/249], loss=82.5435
	step [19/249], loss=89.9135
	step [20/249], loss=84.8969
	step [21/249], loss=91.6822
	step [22/249], loss=86.4923
	step [23/249], loss=80.7123
	step [24/249], loss=85.3737
	step [25/249], loss=93.7693
	step [26/249], loss=93.6375
	step [27/249], loss=103.1269
	step [28/249], loss=88.4537
	step [29/249], loss=94.4050
	step [30/249], loss=90.0834
	step [31/249], loss=88.6604
	step [32/249], loss=80.2480
	step [33/249], loss=97.0241
	step [34/249], loss=115.8724
	step [35/249], loss=95.1899
	step [36/249], loss=102.5141
	step [37/249], loss=99.4994
	step [38/249], loss=78.2026
	step [39/249], loss=89.4445
	step [40/249], loss=96.5753
	step [41/249], loss=102.7848
	step [42/249], loss=74.0410
	step [43/249], loss=87.5104
	step [44/249], loss=92.7709
	step [45/249], loss=78.7463
	step [46/249], loss=95.4500
	step [47/249], loss=72.3509
	step [48/249], loss=96.8082
	step [49/249], loss=76.6603
	step [50/249], loss=86.6114
	step [51/249], loss=86.3753
	step [52/249], loss=91.6200
	step [53/249], loss=88.4188
	step [54/249], loss=92.5166
	step [55/249], loss=91.0177
	step [56/249], loss=81.3616
	step [57/249], loss=89.6950
	step [58/249], loss=88.0144
	step [59/249], loss=106.9028
	step [60/249], loss=87.5669
	step [61/249], loss=96.4394
	step [62/249], loss=80.7408
	step [63/249], loss=82.9509
	step [64/249], loss=93.0503
	step [65/249], loss=90.4376
	step [66/249], loss=78.8988
	step [67/249], loss=84.9936
	step [68/249], loss=91.1368
	step [69/249], loss=97.7614
	step [70/249], loss=81.5665
	step [71/249], loss=68.0375
	step [72/249], loss=74.5763
	step [73/249], loss=76.8027
	step [74/249], loss=76.6988
	step [75/249], loss=85.2494
	step [76/249], loss=87.4542
	step [77/249], loss=86.5505
	step [78/249], loss=82.1323
	step [79/249], loss=109.7873
	step [80/249], loss=75.2079
	step [81/249], loss=82.0057
	step [82/249], loss=94.1676
	step [83/249], loss=70.9589
	step [84/249], loss=78.5840
	step [85/249], loss=81.6531
	step [86/249], loss=102.9079
	step [87/249], loss=87.8591
	step [88/249], loss=77.2923
	step [89/249], loss=103.3533
	step [90/249], loss=107.0446
	step [91/249], loss=83.1902
	step [92/249], loss=74.6913
	step [93/249], loss=88.8851
	step [94/249], loss=95.4481
	step [95/249], loss=86.2595
	step [96/249], loss=95.2280
	step [97/249], loss=83.6485
	step [98/249], loss=95.0768
	step [99/249], loss=80.0348
	step [100/249], loss=74.1979
	step [101/249], loss=77.8226
	step [102/249], loss=84.5968
	step [103/249], loss=78.5212
	step [104/249], loss=80.6900
	step [105/249], loss=108.1153
	step [106/249], loss=82.4105
	step [107/249], loss=83.6620
	step [108/249], loss=102.6466
	step [109/249], loss=66.9896
	step [110/249], loss=93.7594
	step [111/249], loss=95.2714
	step [112/249], loss=93.2599
	step [113/249], loss=94.1600
	step [114/249], loss=109.1330
	step [115/249], loss=83.8062
	step [116/249], loss=92.8349
	step [117/249], loss=81.4639
	step [118/249], loss=101.7824
	step [119/249], loss=110.0670
	step [120/249], loss=94.4227
	step [121/249], loss=87.6934
	step [122/249], loss=103.4812
	step [123/249], loss=96.3808
	step [124/249], loss=79.1451
	step [125/249], loss=97.4809
	step [126/249], loss=88.6246
	step [127/249], loss=99.7380
	step [128/249], loss=91.0647
	step [129/249], loss=86.1893
	step [130/249], loss=82.7063
	step [131/249], loss=75.6220
	step [132/249], loss=85.1728
	step [133/249], loss=73.6009
	step [134/249], loss=84.0859
	step [135/249], loss=82.1496
	step [136/249], loss=84.7003
	step [137/249], loss=88.7315
	step [138/249], loss=99.6459
	step [139/249], loss=98.1866
	step [140/249], loss=82.9177
	step [141/249], loss=75.4805
	step [142/249], loss=85.4134
	step [143/249], loss=72.9592
	step [144/249], loss=95.4109
	step [145/249], loss=88.8694
	step [146/249], loss=100.4590
	step [147/249], loss=103.2652
	step [148/249], loss=88.7655
	step [149/249], loss=75.7750
	step [150/249], loss=84.9063
	step [151/249], loss=113.7784
	step [152/249], loss=95.9221
	step [153/249], loss=81.8450
	step [154/249], loss=84.9371
	step [155/249], loss=74.8879
	step [156/249], loss=89.4295
	step [157/249], loss=101.3610
	step [158/249], loss=84.9243
	step [159/249], loss=84.1299
	step [160/249], loss=81.2477
	step [161/249], loss=101.3893
	step [162/249], loss=96.3623
	step [163/249], loss=90.8042
	step [164/249], loss=78.9357
	step [165/249], loss=77.9978
	step [166/249], loss=88.6083
	step [167/249], loss=90.1105
	step [168/249], loss=78.4343
	step [169/249], loss=72.3107
	step [170/249], loss=71.2360
	step [171/249], loss=67.8589
	step [172/249], loss=72.1463
	step [173/249], loss=98.5617
	step [174/249], loss=75.1515
	step [175/249], loss=88.4016
	step [176/249], loss=84.2312
	step [177/249], loss=88.4697
	step [178/249], loss=102.8477
	step [179/249], loss=98.4959
	step [180/249], loss=108.6898
	step [181/249], loss=104.9390
	step [182/249], loss=70.9223
	step [183/249], loss=85.0158
	step [184/249], loss=92.3637
	step [185/249], loss=89.2704
	step [186/249], loss=103.8046
	step [187/249], loss=67.8732
	step [188/249], loss=90.1729
	step [189/249], loss=78.8320
	step [190/249], loss=103.1769
	step [191/249], loss=91.9489
	step [192/249], loss=85.4586
	step [193/249], loss=92.6716
	step [194/249], loss=87.4729
	step [195/249], loss=69.8923
	step [196/249], loss=76.1079
	step [197/249], loss=95.6119
	step [198/249], loss=80.8553
	step [199/249], loss=75.0399
	step [200/249], loss=78.2301
	step [201/249], loss=83.9116
	step [202/249], loss=92.1252
	step [203/249], loss=79.9724
	step [204/249], loss=88.1553
	step [205/249], loss=62.2185
	step [206/249], loss=86.4774
	step [207/249], loss=87.5985
	step [208/249], loss=93.1661
	step [209/249], loss=84.7651
	step [210/249], loss=83.6776
	step [211/249], loss=87.1956
	step [212/249], loss=87.4054
	step [213/249], loss=81.9694
	step [214/249], loss=81.7158
	step [215/249], loss=90.0913
	step [216/249], loss=81.1516
	step [217/249], loss=93.6793
	step [218/249], loss=83.8367
	step [219/249], loss=97.0903
	step [220/249], loss=68.2789
	step [221/249], loss=76.2464
	step [222/249], loss=82.7210
	step [223/249], loss=82.2265
	step [224/249], loss=90.4192
	step [225/249], loss=92.6919
	step [226/249], loss=72.5749
	step [227/249], loss=67.7128
	step [228/249], loss=67.9829
	step [229/249], loss=67.0125
	step [230/249], loss=84.2108
	step [231/249], loss=95.6915
	step [232/249], loss=83.5242
	step [233/249], loss=88.3515
	step [234/249], loss=80.9795
	step [235/249], loss=96.5951
	step [236/249], loss=92.2004
	step [237/249], loss=72.8277
	step [238/249], loss=78.8394
	step [239/249], loss=82.4057
	step [240/249], loss=82.7685
	step [241/249], loss=109.9905
	step [242/249], loss=84.8483
	step [243/249], loss=103.6712
	step [244/249], loss=96.1105
	step [245/249], loss=82.6151
	step [246/249], loss=96.6752
	step [247/249], loss=87.6012
	step [248/249], loss=101.7038
	step [249/249], loss=52.8013
	Evaluating
	loss=0.0081, precision=0.3447, recall=0.8955, f1=0.4978
Training epoch 28
	step [1/249], loss=86.7689
	step [2/249], loss=94.7188
	step [3/249], loss=83.1432
	step [4/249], loss=90.4707
	step [5/249], loss=82.8621
	step [6/249], loss=92.7827
	step [7/249], loss=75.4148
	step [8/249], loss=83.4850
	step [9/249], loss=102.1906
	step [10/249], loss=94.5073
	step [11/249], loss=84.9760
	step [12/249], loss=98.1772
	step [13/249], loss=85.2440
	step [14/249], loss=83.7045
	step [15/249], loss=89.5546
	step [16/249], loss=86.7558
	step [17/249], loss=81.1315
	step [18/249], loss=80.0563
	step [19/249], loss=75.3630
	step [20/249], loss=78.4645
	step [21/249], loss=90.9563
	step [22/249], loss=79.6606
	step [23/249], loss=111.1023
	step [24/249], loss=85.4144
	step [25/249], loss=65.1288
	step [26/249], loss=86.5351
	step [27/249], loss=86.9586
	step [28/249], loss=103.6028
	step [29/249], loss=94.6163
	step [30/249], loss=103.3366
	step [31/249], loss=87.5294
	step [32/249], loss=91.9836
	step [33/249], loss=93.0732
	step [34/249], loss=70.0303
	step [35/249], loss=94.4282
	step [36/249], loss=96.0611
	step [37/249], loss=73.7240
	step [38/249], loss=81.1959
	step [39/249], loss=84.9985
	step [40/249], loss=93.6718
	step [41/249], loss=95.3776
	step [42/249], loss=81.7592
	step [43/249], loss=84.3991
	step [44/249], loss=87.9030
	step [45/249], loss=81.6266
	step [46/249], loss=92.7018
	step [47/249], loss=88.0178
	step [48/249], loss=88.4063
	step [49/249], loss=77.2052
	step [50/249], loss=84.5599
	step [51/249], loss=78.9355
	step [52/249], loss=76.9350
	step [53/249], loss=81.6092
	step [54/249], loss=93.2087
	step [55/249], loss=85.6937
	step [56/249], loss=90.9560
	step [57/249], loss=90.0130
	step [58/249], loss=87.0147
	step [59/249], loss=92.4918
	step [60/249], loss=81.8704
	step [61/249], loss=79.8553
	step [62/249], loss=83.6343
	step [63/249], loss=75.3620
	step [64/249], loss=98.5231
	step [65/249], loss=76.3994
	step [66/249], loss=82.0528
	step [67/249], loss=89.5726
	step [68/249], loss=95.6068
	step [69/249], loss=70.0344
	step [70/249], loss=91.5868
	step [71/249], loss=74.3727
	step [72/249], loss=100.5860
	step [73/249], loss=87.6940
	step [74/249], loss=75.8766
	step [75/249], loss=100.3501
	step [76/249], loss=97.4657
	step [77/249], loss=87.6626
	step [78/249], loss=81.3500
	step [79/249], loss=92.2337
	step [80/249], loss=70.7815
	step [81/249], loss=87.7799
	step [82/249], loss=84.0113
	step [83/249], loss=82.2739
	step [84/249], loss=85.7001
	step [85/249], loss=84.0526
	step [86/249], loss=76.7677
	step [87/249], loss=85.0052
	step [88/249], loss=93.3840
	step [89/249], loss=89.8267
	step [90/249], loss=80.3779
	step [91/249], loss=87.2693
	step [92/249], loss=91.0054
	step [93/249], loss=93.8256
	step [94/249], loss=77.8296
	step [95/249], loss=85.4233
	step [96/249], loss=84.7653
	step [97/249], loss=90.4825
	step [98/249], loss=99.2208
	step [99/249], loss=98.5926
	step [100/249], loss=96.6878
	step [101/249], loss=66.4864
	step [102/249], loss=89.8208
	step [103/249], loss=81.9355
	step [104/249], loss=77.2559
	step [105/249], loss=87.9450
	step [106/249], loss=100.2045
	step [107/249], loss=85.8263
	step [108/249], loss=92.4559
	step [109/249], loss=99.7485
	step [110/249], loss=95.3839
	step [111/249], loss=86.1115
	step [112/249], loss=85.2414
	step [113/249], loss=72.7337
	step [114/249], loss=81.4397
	step [115/249], loss=80.2414
	step [116/249], loss=81.2809
	step [117/249], loss=85.1975
	step [118/249], loss=79.6231
	step [119/249], loss=92.6244
	step [120/249], loss=79.3572
	step [121/249], loss=90.9861
	step [122/249], loss=104.6220
	step [123/249], loss=86.0839
	step [124/249], loss=82.7901
	step [125/249], loss=82.8630
	step [126/249], loss=79.8161
	step [127/249], loss=86.1595
	step [128/249], loss=79.5831
	step [129/249], loss=84.0691
	step [130/249], loss=92.8663
	step [131/249], loss=88.9319
	step [132/249], loss=79.1238
	step [133/249], loss=85.8260
	step [134/249], loss=92.6744
	step [135/249], loss=92.6279
	step [136/249], loss=67.1681
	step [137/249], loss=102.3290
	step [138/249], loss=83.0966
	step [139/249], loss=87.1688
	step [140/249], loss=86.5433
	step [141/249], loss=112.4700
	step [142/249], loss=73.0704
	step [143/249], loss=92.0434
	step [144/249], loss=92.3850
	step [145/249], loss=95.4309
	step [146/249], loss=105.4997
	step [147/249], loss=88.1009
	step [148/249], loss=89.7146
	step [149/249], loss=94.9403
	step [150/249], loss=88.2053
	step [151/249], loss=93.5740
	step [152/249], loss=90.9661
	step [153/249], loss=77.0011
	step [154/249], loss=98.4227
	step [155/249], loss=93.3861
	step [156/249], loss=79.0386
	step [157/249], loss=91.5446
	step [158/249], loss=81.6803
	step [159/249], loss=83.4975
	step [160/249], loss=89.0741
	step [161/249], loss=115.4240
	step [162/249], loss=93.8341
	step [163/249], loss=63.1369
	step [164/249], loss=82.0471
	step [165/249], loss=70.4045
	step [166/249], loss=96.6740
	step [167/249], loss=84.3791
	step [168/249], loss=91.6550
	step [169/249], loss=100.3257
	step [170/249], loss=81.0078
	step [171/249], loss=79.8940
	step [172/249], loss=96.2770
	step [173/249], loss=92.8657
	step [174/249], loss=89.5242
	step [175/249], loss=85.7562
	step [176/249], loss=77.2285
	step [177/249], loss=87.8604
	step [178/249], loss=96.3085
	step [179/249], loss=92.4222
	step [180/249], loss=100.6734
	step [181/249], loss=93.5506
	step [182/249], loss=86.2256
	step [183/249], loss=84.5139
	step [184/249], loss=100.0712
	step [185/249], loss=91.2053
	step [186/249], loss=87.1211
	step [187/249], loss=93.6575
	step [188/249], loss=87.6429
	step [189/249], loss=78.2137
	step [190/249], loss=79.4970
	step [191/249], loss=80.8514
	step [192/249], loss=91.4921
	step [193/249], loss=94.1660
	step [194/249], loss=96.1117
	step [195/249], loss=84.7783
	step [196/249], loss=79.1812
	step [197/249], loss=78.5497
	step [198/249], loss=81.5041
	step [199/249], loss=89.6911
	step [200/249], loss=80.1641
	step [201/249], loss=78.0605
	step [202/249], loss=77.9596
	step [203/249], loss=61.6583
	step [204/249], loss=97.8932
	step [205/249], loss=92.9735
	step [206/249], loss=97.1908
	step [207/249], loss=94.8745
	step [208/249], loss=74.9819
	step [209/249], loss=73.4868
	step [210/249], loss=102.9954
	step [211/249], loss=67.8489
	step [212/249], loss=85.1691
	step [213/249], loss=73.2362
	step [214/249], loss=61.5602
	step [215/249], loss=88.9777
	step [216/249], loss=89.3705
	step [217/249], loss=76.9716
	step [218/249], loss=90.7037
	step [219/249], loss=90.2681
	step [220/249], loss=82.3925
	step [221/249], loss=75.8451
	step [222/249], loss=84.8369
	step [223/249], loss=69.0970
	step [224/249], loss=73.1283
	step [225/249], loss=88.6700
	step [226/249], loss=81.3598
	step [227/249], loss=93.4655
	step [228/249], loss=82.8824
	step [229/249], loss=77.0458
	step [230/249], loss=112.9997
	step [231/249], loss=93.0240
	step [232/249], loss=86.2620
	step [233/249], loss=82.6046
	step [234/249], loss=90.6542
	step [235/249], loss=94.7758
	step [236/249], loss=93.8288
	step [237/249], loss=76.9215
	step [238/249], loss=101.8965
	step [239/249], loss=72.7914
	step [240/249], loss=89.7550
	step [241/249], loss=72.2173
	step [242/249], loss=75.0270
	step [243/249], loss=83.3053
	step [244/249], loss=93.9834
	step [245/249], loss=101.0464
	step [246/249], loss=97.8636
	step [247/249], loss=83.1723
	step [248/249], loss=96.1218
	step [249/249], loss=56.5131
	Evaluating
	loss=0.0072, precision=0.3696, recall=0.8801, f1=0.5206
Training epoch 29
	step [1/249], loss=76.1866
	step [2/249], loss=96.1980
	step [3/249], loss=90.6481
	step [4/249], loss=104.0232
	step [5/249], loss=75.2920
	step [6/249], loss=87.5311
	step [7/249], loss=93.0980
	step [8/249], loss=75.7455
	step [9/249], loss=88.2203
	step [10/249], loss=82.3659
	step [11/249], loss=83.2821
	step [12/249], loss=68.3018
	step [13/249], loss=72.6355
	step [14/249], loss=100.9811
	step [15/249], loss=80.2442
	step [16/249], loss=89.9177
	step [17/249], loss=103.7698
	step [18/249], loss=85.4990
	step [19/249], loss=76.8137
	step [20/249], loss=80.1482
	step [21/249], loss=85.4320
	step [22/249], loss=83.2903
	step [23/249], loss=88.4083
	step [24/249], loss=80.2814
	step [25/249], loss=80.3843
	step [26/249], loss=96.6581
	step [27/249], loss=87.1721
	step [28/249], loss=89.3483
	step [29/249], loss=87.4786
	step [30/249], loss=81.5430
	step [31/249], loss=89.4341
	step [32/249], loss=87.8183
	step [33/249], loss=81.6238
	step [34/249], loss=99.4308
	step [35/249], loss=86.5989
	step [36/249], loss=90.7922
	step [37/249], loss=80.2962
	step [38/249], loss=86.6623
	step [39/249], loss=81.9944
	step [40/249], loss=97.9401
	step [41/249], loss=99.7403
	step [42/249], loss=83.1104
	step [43/249], loss=77.5579
	step [44/249], loss=79.8657
	step [45/249], loss=91.1390
	step [46/249], loss=98.6321
	step [47/249], loss=80.9713
	step [48/249], loss=107.2513
	step [49/249], loss=84.6733
	step [50/249], loss=91.3115
	step [51/249], loss=104.4265
	step [52/249], loss=92.5367
	step [53/249], loss=79.5014
	step [54/249], loss=96.9049
	step [55/249], loss=92.4613
	step [56/249], loss=81.1662
	step [57/249], loss=77.9214
	step [58/249], loss=83.7508
	step [59/249], loss=76.5082
	step [60/249], loss=75.9167
	step [61/249], loss=90.8056
	step [62/249], loss=97.3138
	step [63/249], loss=85.4331
	step [64/249], loss=96.7003
	step [65/249], loss=82.6037
	step [66/249], loss=89.3230
	step [67/249], loss=89.5031
	step [68/249], loss=66.2618
	step [69/249], loss=72.6876
	step [70/249], loss=107.6694
	step [71/249], loss=97.9770
	step [72/249], loss=94.2307
	step [73/249], loss=86.1826
	step [74/249], loss=91.4373
	step [75/249], loss=83.3298
	step [76/249], loss=90.1175
	step [77/249], loss=95.0731
	step [78/249], loss=74.9999
	step [79/249], loss=83.3320
	step [80/249], loss=93.4304
	step [81/249], loss=82.2584
	step [82/249], loss=88.3268
	step [83/249], loss=75.8682
	step [84/249], loss=80.3299
	step [85/249], loss=95.6345
	step [86/249], loss=89.4182
	step [87/249], loss=102.2512
	step [88/249], loss=85.7742
	step [89/249], loss=88.5448
	step [90/249], loss=95.7725
	step [91/249], loss=70.2838
	step [92/249], loss=74.4950
	step [93/249], loss=104.0090
	step [94/249], loss=100.1538
	step [95/249], loss=77.1918
	step [96/249], loss=91.7714
	step [97/249], loss=82.3980
	step [98/249], loss=101.3596
	step [99/249], loss=81.0692
	step [100/249], loss=76.2638
	step [101/249], loss=61.2466
	step [102/249], loss=86.5254
	step [103/249], loss=81.8836
	step [104/249], loss=91.5950
	step [105/249], loss=85.5840
	step [106/249], loss=77.7188
	step [107/249], loss=95.2423
	step [108/249], loss=86.9466
	step [109/249], loss=98.2291
	step [110/249], loss=87.4131
	step [111/249], loss=113.4317
	step [112/249], loss=94.4061
	step [113/249], loss=71.8989
	step [114/249], loss=102.3274
	step [115/249], loss=81.9936
	step [116/249], loss=67.4231
	step [117/249], loss=78.6556
	step [118/249], loss=95.7291
	step [119/249], loss=76.6087
	step [120/249], loss=63.1598
	step [121/249], loss=93.2005
	step [122/249], loss=75.1947
	step [123/249], loss=84.7658
	step [124/249], loss=84.1844
	step [125/249], loss=93.2157
	step [126/249], loss=75.3488
	step [127/249], loss=69.8054
	step [128/249], loss=74.4526
	step [129/249], loss=68.1106
	step [130/249], loss=87.4115
	step [131/249], loss=91.4704
	step [132/249], loss=88.4268
	step [133/249], loss=72.6665
	step [134/249], loss=89.8272
	step [135/249], loss=98.5896
	step [136/249], loss=94.0262
	step [137/249], loss=75.3751
	step [138/249], loss=71.1838
	step [139/249], loss=95.4943
	step [140/249], loss=82.7703
	step [141/249], loss=70.7173
	step [142/249], loss=78.4262
	step [143/249], loss=76.3403
	step [144/249], loss=90.3507
	step [145/249], loss=86.5964
	step [146/249], loss=87.0190
	step [147/249], loss=84.7229
	step [148/249], loss=85.8325
	step [149/249], loss=67.8197
	step [150/249], loss=89.1599
	step [151/249], loss=77.7903
	step [152/249], loss=77.5758
	step [153/249], loss=73.5703
	step [154/249], loss=88.7057
	step [155/249], loss=74.4011
	step [156/249], loss=84.6137
	step [157/249], loss=88.4078
	step [158/249], loss=73.3838
	step [159/249], loss=88.1948
	step [160/249], loss=85.1202
	step [161/249], loss=78.1125
	step [162/249], loss=76.7613
	step [163/249], loss=80.4935
	step [164/249], loss=82.6729
	step [165/249], loss=75.5897
	step [166/249], loss=98.3408
	step [167/249], loss=91.1897
	step [168/249], loss=78.9623
	step [169/249], loss=95.4049
	step [170/249], loss=93.1936
	step [171/249], loss=75.0423
	step [172/249], loss=102.3675
	step [173/249], loss=99.8823
	step [174/249], loss=75.2780
	step [175/249], loss=70.2491
	step [176/249], loss=94.2394
	step [177/249], loss=90.0010
	step [178/249], loss=96.7122
	step [179/249], loss=101.2239
	step [180/249], loss=89.0571
	step [181/249], loss=88.0267
	step [182/249], loss=88.3687
	step [183/249], loss=97.6371
	step [184/249], loss=88.7060
	step [185/249], loss=84.3139
	step [186/249], loss=87.8957
	step [187/249], loss=87.4828
	step [188/249], loss=74.4203
	step [189/249], loss=89.0359
	step [190/249], loss=81.6203
	step [191/249], loss=86.2764
	step [192/249], loss=90.3792
	step [193/249], loss=89.3036
	step [194/249], loss=68.6329
	step [195/249], loss=83.0087
	step [196/249], loss=84.0608
	step [197/249], loss=97.0485
	step [198/249], loss=82.0543
	step [199/249], loss=75.2397
	step [200/249], loss=74.6130
	step [201/249], loss=82.5482
	step [202/249], loss=82.4006
	step [203/249], loss=72.7112
	step [204/249], loss=91.5155
	step [205/249], loss=76.7791
	step [206/249], loss=86.8197
	step [207/249], loss=64.0804
	step [208/249], loss=86.5455
	step [209/249], loss=77.6862
	step [210/249], loss=89.2508
	step [211/249], loss=79.9356
	step [212/249], loss=92.5219
	step [213/249], loss=86.2151
	step [214/249], loss=86.1014
	step [215/249], loss=88.1416
	step [216/249], loss=75.9467
	step [217/249], loss=89.6823
	step [218/249], loss=98.5322
	step [219/249], loss=103.5159
	step [220/249], loss=74.0353
	step [221/249], loss=97.9441
	step [222/249], loss=88.1444
	step [223/249], loss=74.6693
	step [224/249], loss=96.5071
	step [225/249], loss=72.6065
	step [226/249], loss=86.1324
	step [227/249], loss=90.1445
	step [228/249], loss=78.0435
	step [229/249], loss=72.2538
	step [230/249], loss=85.8352
	step [231/249], loss=99.6337
	step [232/249], loss=105.1731
	step [233/249], loss=72.8125
	step [234/249], loss=79.6451
	step [235/249], loss=83.0665
	step [236/249], loss=84.9084
	step [237/249], loss=79.2556
	step [238/249], loss=76.3699
	step [239/249], loss=86.6254
	step [240/249], loss=102.3320
	step [241/249], loss=82.5927
	step [242/249], loss=75.4886
	step [243/249], loss=89.2851
	step [244/249], loss=93.6242
	step [245/249], loss=81.8235
	step [246/249], loss=79.0006
	step [247/249], loss=90.4951
	step [248/249], loss=73.9830
	step [249/249], loss=62.5711
	Evaluating
	loss=0.0080, precision=0.3586, recall=0.8781, f1=0.5092
Training epoch 30
	step [1/249], loss=85.1781
	step [2/249], loss=97.0627
	step [3/249], loss=82.6579
	step [4/249], loss=90.1854
	step [5/249], loss=81.6524
	step [6/249], loss=92.4602
	step [7/249], loss=90.7900
	step [8/249], loss=79.9858
	step [9/249], loss=80.0906
	step [10/249], loss=71.8596
	step [11/249], loss=88.0383
	step [12/249], loss=101.2736
	step [13/249], loss=94.8850
	step [14/249], loss=77.4427
	step [15/249], loss=84.5968
	step [16/249], loss=79.6910
	step [17/249], loss=79.6099
	step [18/249], loss=85.6007
	step [19/249], loss=81.1691
	step [20/249], loss=77.8985
	step [21/249], loss=90.0995
	step [22/249], loss=76.9492
	step [23/249], loss=101.2112
	step [24/249], loss=89.4644
	step [25/249], loss=82.0825
	step [26/249], loss=92.0538
	step [27/249], loss=77.2222
	step [28/249], loss=78.6549
	step [29/249], loss=90.1677
	step [30/249], loss=94.1990
	step [31/249], loss=98.7970
	step [32/249], loss=99.6096
	step [33/249], loss=94.8604
	step [34/249], loss=84.4650
	step [35/249], loss=71.9333
	step [36/249], loss=92.4487
	step [37/249], loss=70.5942
	step [38/249], loss=82.0650
	step [39/249], loss=68.9979
	step [40/249], loss=90.1337
	step [41/249], loss=101.2571
	step [42/249], loss=85.6671
	step [43/249], loss=64.6035
	step [44/249], loss=75.9642
	step [45/249], loss=74.6366
	step [46/249], loss=91.4095
	step [47/249], loss=82.9007
	step [48/249], loss=83.6063
	step [49/249], loss=86.7834
	step [50/249], loss=84.9775
	step [51/249], loss=109.1778
	step [52/249], loss=80.8368
	step [53/249], loss=88.6811
	step [54/249], loss=92.6197
	step [55/249], loss=78.9778
	step [56/249], loss=81.9648
	step [57/249], loss=63.1039
	step [58/249], loss=89.9350
	step [59/249], loss=87.7988
	step [60/249], loss=88.2035
	step [61/249], loss=85.4924
	step [62/249], loss=85.6424
	step [63/249], loss=108.4614
	step [64/249], loss=99.4009
	step [65/249], loss=81.7446
	step [66/249], loss=78.6405
	step [67/249], loss=78.8497
	step [68/249], loss=86.3143
	step [69/249], loss=96.3370
	step [70/249], loss=93.1745
	step [71/249], loss=86.3309
	step [72/249], loss=98.0335
	step [73/249], loss=87.9610
	step [74/249], loss=83.0786
	step [75/249], loss=78.7964
	step [76/249], loss=88.1129
	step [77/249], loss=86.0909
	step [78/249], loss=87.8823
	step [79/249], loss=88.8232
	step [80/249], loss=90.5542
	step [81/249], loss=82.2773
	step [82/249], loss=86.6371
	step [83/249], loss=79.2081
	step [84/249], loss=76.5974
	step [85/249], loss=79.4920
	step [86/249], loss=96.9000
	step [87/249], loss=92.0784
	step [88/249], loss=86.7905
	step [89/249], loss=90.4758
	step [90/249], loss=88.5336
	step [91/249], loss=85.6974
	step [92/249], loss=81.7302
	step [93/249], loss=75.6761
	step [94/249], loss=72.0958
	step [95/249], loss=89.8249
	step [96/249], loss=76.3758
	step [97/249], loss=104.0033
	step [98/249], loss=74.9837
	step [99/249], loss=95.3100
	step [100/249], loss=89.6981
	step [101/249], loss=96.8910
	step [102/249], loss=75.8603
	step [103/249], loss=68.3404
	step [104/249], loss=95.2358
	step [105/249], loss=83.3398
	step [106/249], loss=108.3399
	step [107/249], loss=99.6172
	step [108/249], loss=69.2221
	step [109/249], loss=87.9085
	step [110/249], loss=77.4085
	step [111/249], loss=69.1634
	step [112/249], loss=86.3808
	step [113/249], loss=77.3115
	step [114/249], loss=77.1460
	step [115/249], loss=96.7743
	step [116/249], loss=83.2646
	step [117/249], loss=96.6366
	step [118/249], loss=77.1867
	step [119/249], loss=77.0206
	step [120/249], loss=80.0314
	step [121/249], loss=83.9570
	step [122/249], loss=87.6493
	step [123/249], loss=74.5936
	step [124/249], loss=83.4002
	step [125/249], loss=86.8480
	step [126/249], loss=68.1728
	step [127/249], loss=82.7633
	step [128/249], loss=87.3940
	step [129/249], loss=93.6322
	step [130/249], loss=96.3111
	step [131/249], loss=80.9174
	step [132/249], loss=81.1057
	step [133/249], loss=64.0766
	step [134/249], loss=83.1360
	step [135/249], loss=90.9966
	step [136/249], loss=98.2127
	step [137/249], loss=70.4479
	step [138/249], loss=77.7259
	step [139/249], loss=69.7420
	step [140/249], loss=84.9825
	step [141/249], loss=77.8426
	step [142/249], loss=95.9131
	step [143/249], loss=89.4950
	step [144/249], loss=76.6086
	step [145/249], loss=69.4445
	step [146/249], loss=80.1191
	step [147/249], loss=83.6702
	step [148/249], loss=87.6986
	step [149/249], loss=85.8233
	step [150/249], loss=95.0553
	step [151/249], loss=68.3946
	step [152/249], loss=72.7158
	step [153/249], loss=84.2886
	step [154/249], loss=86.4846
	step [155/249], loss=82.5401
	step [156/249], loss=84.9200
	step [157/249], loss=99.1518
	step [158/249], loss=100.2844
	step [159/249], loss=85.9840
	step [160/249], loss=89.4736
	step [161/249], loss=81.4296
	step [162/249], loss=81.9072
	step [163/249], loss=69.4476
	step [164/249], loss=83.4149
	step [165/249], loss=76.6652
	step [166/249], loss=77.4943
	step [167/249], loss=72.1980
	step [168/249], loss=80.0731
	step [169/249], loss=89.2597
	step [170/249], loss=79.0457
	step [171/249], loss=94.4993
	step [172/249], loss=71.4659
	step [173/249], loss=77.4579
	step [174/249], loss=80.6620
	step [175/249], loss=86.8310
	step [176/249], loss=93.1742
	step [177/249], loss=71.8056
	step [178/249], loss=72.0642
	step [179/249], loss=79.4494
	step [180/249], loss=64.0247
	step [181/249], loss=103.0789
	step [182/249], loss=65.2365
	step [183/249], loss=94.9456
	step [184/249], loss=83.2455
	step [185/249], loss=95.3201
	step [186/249], loss=100.0979
	step [187/249], loss=90.1231
	step [188/249], loss=81.4875
	step [189/249], loss=82.2481
	step [190/249], loss=80.2708
	step [191/249], loss=79.7333
	step [192/249], loss=82.3988
	step [193/249], loss=76.6693
	step [194/249], loss=91.2411
	step [195/249], loss=65.2210
	step [196/249], loss=85.6811
	step [197/249], loss=84.9041
	step [198/249], loss=108.6814
	step [199/249], loss=78.9492
	step [200/249], loss=62.5959
	step [201/249], loss=75.3850
	step [202/249], loss=99.3989
	step [203/249], loss=86.7293
	step [204/249], loss=67.9471
	step [205/249], loss=80.6295
	step [206/249], loss=84.5207
	step [207/249], loss=81.3797
	step [208/249], loss=84.6090
	step [209/249], loss=72.1997
	step [210/249], loss=79.6990
	step [211/249], loss=90.7094
	step [212/249], loss=67.0657
	step [213/249], loss=82.8780
	step [214/249], loss=87.5457
	step [215/249], loss=75.2645
	step [216/249], loss=74.6800
	step [217/249], loss=85.0801
	step [218/249], loss=81.4211
	step [219/249], loss=77.8986
	step [220/249], loss=83.6676
	step [221/249], loss=78.0600
	step [222/249], loss=102.8879
	step [223/249], loss=77.5150
	step [224/249], loss=86.5586
	step [225/249], loss=81.1742
	step [226/249], loss=106.3990
	step [227/249], loss=85.2658
	step [228/249], loss=78.7159
	step [229/249], loss=77.9103
	step [230/249], loss=91.0184
	step [231/249], loss=95.5099
	step [232/249], loss=84.2334
	step [233/249], loss=101.3375
	step [234/249], loss=81.5412
	step [235/249], loss=92.0379
	step [236/249], loss=93.1123
	step [237/249], loss=78.9551
	step [238/249], loss=68.7321
	step [239/249], loss=92.6432
	step [240/249], loss=86.5640
	step [241/249], loss=76.3101
	step [242/249], loss=87.5686
	step [243/249], loss=93.4774
	step [244/249], loss=72.2630
	step [245/249], loss=90.8828
	step [246/249], loss=85.9167
	step [247/249], loss=68.2497
	step [248/249], loss=94.5984
	step [249/249], loss=55.1166
	Evaluating
	loss=0.0085, precision=0.3121, recall=0.8966, f1=0.4630
Training finished
best_f1: 0.6279073647649387
directing: Z rim_enhanced: True test_id 0
removed wrong scan: weights_Z_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_278_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_276_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_298_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_210_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_262_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_254_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_58_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_293_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_291_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_206_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_252_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_265_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_295_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_221_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_292_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_294_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_297_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_214_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_296_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_225_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_300_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_243_xwqg-A00121_2019-05-15.npy
# all image files: 15579 # all weight files in weight_dir: 12281 # image files with weight 12232
removed wrong scan: weights_Z_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_278_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_276_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_298_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_210_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_262_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_254_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_58_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_293_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_291_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_206_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_252_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_265_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_295_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_221_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_292_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_294_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_297_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_214_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_296_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_225_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_300_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_243_xwqg-A00121_2019-05-15.npy
# all image files: 15579 # all weight files in weight_dir: 3263 # image files with weight 3252
train: /ibex/scratch/projects/c2052/air_tube_seg/training_samples_enhanced_one/Z 12232
Using 4 GPUs
Going to train epochs [1-30]
Training epoch 1
	step [1/192], loss=665.3548
	step [2/192], loss=456.6967
	step [3/192], loss=344.4298
	step [4/192], loss=291.2064
	step [5/192], loss=267.8947
	step [6/192], loss=239.1380
	step [7/192], loss=244.1500
	step [8/192], loss=238.5145
	step [9/192], loss=244.6550
	step [10/192], loss=232.3198
	step [11/192], loss=231.9999
	step [12/192], loss=252.8680
	step [13/192], loss=218.5489
	step [14/192], loss=238.1995
	step [15/192], loss=215.9914
	step [16/192], loss=224.7195
	step [17/192], loss=213.9319
	step [18/192], loss=220.7599
	step [19/192], loss=209.7854
	step [20/192], loss=227.2290
	step [21/192], loss=211.2686
	step [22/192], loss=210.1693
	step [23/192], loss=207.8402
	step [24/192], loss=196.0683
	step [25/192], loss=215.8325
	step [26/192], loss=201.7426
	step [27/192], loss=174.7660
	step [28/192], loss=188.5048
	step [29/192], loss=202.4070
	step [30/192], loss=181.0444
	step [31/192], loss=188.3636
	step [32/192], loss=220.1297
	step [33/192], loss=222.7112
	step [34/192], loss=208.7971
	step [35/192], loss=194.7294
	step [36/192], loss=220.7946
	step [37/192], loss=194.8462
	step [38/192], loss=190.6830
	step [39/192], loss=180.3577
	step [40/192], loss=198.9952
	step [41/192], loss=208.0644
	step [42/192], loss=163.4054
	step [43/192], loss=199.6044
	step [44/192], loss=174.1967
	step [45/192], loss=181.8344
	step [46/192], loss=178.4944
	step [47/192], loss=162.2567
	step [48/192], loss=202.0112
	step [49/192], loss=184.9588
	step [50/192], loss=198.3019
	step [51/192], loss=201.6521
	step [52/192], loss=172.6670
	step [53/192], loss=185.8520
	step [54/192], loss=192.8737
	step [55/192], loss=183.6560
	step [56/192], loss=183.7220
	step [57/192], loss=197.4609
	step [58/192], loss=173.1233
	step [59/192], loss=192.0666
	step [60/192], loss=216.9964
	step [61/192], loss=166.3607
	step [62/192], loss=172.9565
	step [63/192], loss=188.1951
	step [64/192], loss=169.2002
	step [65/192], loss=179.5668
	step [66/192], loss=171.4307
	step [67/192], loss=178.6404
	step [68/192], loss=184.8222
	step [69/192], loss=177.4929
	step [70/192], loss=196.0190
	step [71/192], loss=168.5910
	step [72/192], loss=174.3088
	step [73/192], loss=159.1246
	step [74/192], loss=171.3687
	step [75/192], loss=151.7340
	step [76/192], loss=167.9851
	step [77/192], loss=180.7085
	step [78/192], loss=192.4089
	step [79/192], loss=158.5678
	step [80/192], loss=160.7940
	step [81/192], loss=150.5411
	step [82/192], loss=182.0851
	step [83/192], loss=176.4089
	step [84/192], loss=189.0979
	step [85/192], loss=163.2405
	step [86/192], loss=178.3696
	step [87/192], loss=161.9933
	step [88/192], loss=156.5151
	step [89/192], loss=169.2914
	step [90/192], loss=169.9217
	step [91/192], loss=188.0108
	step [92/192], loss=163.2090
	step [93/192], loss=164.3266
	step [94/192], loss=164.1848
	step [95/192], loss=152.7399
	step [96/192], loss=164.4176
	step [97/192], loss=180.9333
	step [98/192], loss=171.5415
	step [99/192], loss=158.6732
	step [100/192], loss=161.8193
	step [101/192], loss=166.3474
	step [102/192], loss=147.7928
	step [103/192], loss=154.0631
	step [104/192], loss=172.5217
	step [105/192], loss=177.4188
	step [106/192], loss=180.6573
	step [107/192], loss=171.6424
	step [108/192], loss=161.9965
	step [109/192], loss=171.5891
	step [110/192], loss=139.2771
	step [111/192], loss=179.6798
	step [112/192], loss=176.9962
	step [113/192], loss=157.1747
	step [114/192], loss=152.7681
	step [115/192], loss=144.3578
	step [116/192], loss=156.7392
	step [117/192], loss=173.4400
	step [118/192], loss=161.7657
	step [119/192], loss=152.7942
	step [120/192], loss=164.4852
	step [121/192], loss=166.4256
	step [122/192], loss=132.7579
	step [123/192], loss=174.4691
	step [124/192], loss=169.2748
	step [125/192], loss=151.1495
	step [126/192], loss=170.0522
	step [127/192], loss=150.3528
	step [128/192], loss=147.1051
	step [129/192], loss=161.5327
	step [130/192], loss=159.1784
	step [131/192], loss=144.3802
	step [132/192], loss=159.2189
	step [133/192], loss=161.8179
	step [134/192], loss=160.4537
	step [135/192], loss=155.9115
	step [136/192], loss=160.9184
	step [137/192], loss=151.8454
	step [138/192], loss=159.9194
	step [139/192], loss=189.5511
	step [140/192], loss=147.9785
	step [141/192], loss=169.2162
	step [142/192], loss=146.5866
	step [143/192], loss=157.0052
	step [144/192], loss=151.7922
	step [145/192], loss=164.6443
	step [146/192], loss=155.4619
	step [147/192], loss=145.7171
	step [148/192], loss=145.2265
	step [149/192], loss=152.3733
	step [150/192], loss=164.0113
	step [151/192], loss=148.7336
	step [152/192], loss=136.3910
	step [153/192], loss=159.0436
	step [154/192], loss=158.8564
	step [155/192], loss=173.3676
	step [156/192], loss=163.0747
	step [157/192], loss=155.8762
	step [158/192], loss=132.6307
	step [159/192], loss=165.0249
	step [160/192], loss=151.2321
	step [161/192], loss=147.6167
	step [162/192], loss=150.6623
	step [163/192], loss=156.9107
	step [164/192], loss=162.8677
	step [165/192], loss=158.8702
	step [166/192], loss=150.8909
	step [167/192], loss=150.7273
	step [168/192], loss=161.5842
	step [169/192], loss=139.2802
	step [170/192], loss=155.8808
	step [171/192], loss=184.4641
	step [172/192], loss=154.7524
	step [173/192], loss=157.1278
	step [174/192], loss=160.7116
	step [175/192], loss=156.7776
	step [176/192], loss=142.6383
	step [177/192], loss=174.4156
	step [178/192], loss=142.7986
	step [179/192], loss=157.5880
	step [180/192], loss=144.0039
	step [181/192], loss=151.6783
	step [182/192], loss=155.1268
	step [183/192], loss=136.6785
	step [184/192], loss=153.4844
	step [185/192], loss=157.0552
	step [186/192], loss=146.3260
	step [187/192], loss=160.4654
	step [188/192], loss=147.0916
	step [189/192], loss=140.6118
	step [190/192], loss=164.2766
	step [191/192], loss=143.5469
	step [192/192], loss=14.5250
	Evaluating
	loss=0.1994, precision=0.3817, recall=0.9284, f1=0.5409
saving model as: 0_saved_model.pth
Training epoch 2
	step [1/192], loss=150.7610
	step [2/192], loss=129.6515
	step [3/192], loss=153.5033
	step [4/192], loss=145.8359
	step [5/192], loss=150.3123
	step [6/192], loss=133.6538
	step [7/192], loss=135.6446
	step [8/192], loss=147.3778
	step [9/192], loss=153.7677
	step [10/192], loss=139.7363
	step [11/192], loss=132.8287
	step [12/192], loss=147.9530
	step [13/192], loss=160.9129
	step [14/192], loss=169.2756
	step [15/192], loss=156.1720
	step [16/192], loss=160.9917
	step [17/192], loss=143.6490
	step [18/192], loss=160.5336
	step [19/192], loss=139.9682
	step [20/192], loss=126.1328
	step [21/192], loss=137.8869
	step [22/192], loss=154.0371
	step [23/192], loss=154.2826
	step [24/192], loss=144.7555
	step [25/192], loss=145.5949
	step [26/192], loss=152.1617
	step [27/192], loss=151.3741
	step [28/192], loss=128.5971
	step [29/192], loss=150.9123
	step [30/192], loss=155.2050
	step [31/192], loss=158.4258
	step [32/192], loss=156.3424
	step [33/192], loss=163.9689
	step [34/192], loss=149.7944
	step [35/192], loss=133.3606
	step [36/192], loss=156.5683
	step [37/192], loss=160.9409
	step [38/192], loss=158.5378
	step [39/192], loss=155.1265
	step [40/192], loss=147.6840
	step [41/192], loss=137.4355
	step [42/192], loss=145.7849
	step [43/192], loss=166.2121
	step [44/192], loss=154.0192
	step [45/192], loss=141.3338
	step [46/192], loss=131.8344
	step [47/192], loss=147.9220
	step [48/192], loss=144.7501
	step [49/192], loss=147.4728
	step [50/192], loss=156.1346
	step [51/192], loss=141.2091
	step [52/192], loss=132.7737
	step [53/192], loss=148.4021
	step [54/192], loss=150.8433
	step [55/192], loss=124.6741
	step [56/192], loss=155.3147
	step [57/192], loss=146.4335
	step [58/192], loss=153.6060
	step [59/192], loss=154.2825
	step [60/192], loss=137.7199
	step [61/192], loss=144.4599
	step [62/192], loss=137.4549
	step [63/192], loss=139.9310
	step [64/192], loss=145.6220
	step [65/192], loss=141.6660
	step [66/192], loss=149.0439
	step [67/192], loss=143.8550
	step [68/192], loss=155.0901
	step [69/192], loss=146.2410
	step [70/192], loss=167.5765
	step [71/192], loss=142.2612
	step [72/192], loss=149.6047
	step [73/192], loss=137.9651
	step [74/192], loss=139.1154
	step [75/192], loss=149.4520
	step [76/192], loss=145.4823
	step [77/192], loss=156.0981
	step [78/192], loss=154.7786
	step [79/192], loss=163.1406
	step [80/192], loss=139.4398
	step [81/192], loss=165.7426
	step [82/192], loss=145.3055
	step [83/192], loss=136.6624
	step [84/192], loss=166.1929
	step [85/192], loss=134.1786
	step [86/192], loss=157.2392
	step [87/192], loss=145.3395
	step [88/192], loss=148.0571
	step [89/192], loss=139.0412
	step [90/192], loss=142.1627
	step [91/192], loss=134.8845
	step [92/192], loss=140.4097
	step [93/192], loss=141.7922
	step [94/192], loss=145.5711
	step [95/192], loss=154.2052
	step [96/192], loss=143.5092
	step [97/192], loss=160.7776
	step [98/192], loss=144.7723
	step [99/192], loss=132.7856
	step [100/192], loss=137.1925
	step [101/192], loss=137.8588
	step [102/192], loss=137.4635
	step [103/192], loss=145.4911
	step [104/192], loss=136.2378
	step [105/192], loss=135.6224
	step [106/192], loss=149.2720
	step [107/192], loss=143.8374
	step [108/192], loss=156.7661
	step [109/192], loss=148.3085
	step [110/192], loss=172.7590
	step [111/192], loss=147.8487
	step [112/192], loss=131.5223
	step [113/192], loss=135.6849
	step [114/192], loss=136.9576
	step [115/192], loss=138.0532
	step [116/192], loss=165.0936
	step [117/192], loss=117.6311
	step [118/192], loss=141.8844
	step [119/192], loss=133.1066
	step [120/192], loss=141.2674
	step [121/192], loss=137.9127
	step [122/192], loss=161.5970
	step [123/192], loss=138.3639
	step [124/192], loss=146.2315
	step [125/192], loss=120.1908
	step [126/192], loss=143.2297
	step [127/192], loss=149.8573
	step [128/192], loss=146.8113
	step [129/192], loss=133.6029
	step [130/192], loss=162.6060
	step [131/192], loss=157.4113
	step [132/192], loss=122.2193
	step [133/192], loss=129.1845
	step [134/192], loss=134.0619
	step [135/192], loss=137.2555
	step [136/192], loss=139.8642
	step [137/192], loss=137.8610
	step [138/192], loss=135.5420
	step [139/192], loss=152.1195
	step [140/192], loss=138.9495
	step [141/192], loss=153.9042
	step [142/192], loss=131.0295
	step [143/192], loss=136.9314
	step [144/192], loss=132.6266
	step [145/192], loss=170.2252
	step [146/192], loss=129.0111
	step [147/192], loss=140.5580
	step [148/192], loss=143.1954
	step [149/192], loss=126.2326
	step [150/192], loss=119.1146
	step [151/192], loss=131.9457
	step [152/192], loss=129.6705
	step [153/192], loss=143.4616
	step [154/192], loss=132.2021
	step [155/192], loss=130.0006
	step [156/192], loss=130.8447
	step [157/192], loss=130.9664
	step [158/192], loss=140.6965
	step [159/192], loss=120.2049
	step [160/192], loss=120.4802
	step [161/192], loss=162.7824
	step [162/192], loss=137.2113
	step [163/192], loss=128.2260
	step [164/192], loss=150.5432
	step [165/192], loss=142.1099
	step [166/192], loss=136.2883
	step [167/192], loss=140.6760
	step [168/192], loss=143.8290
	step [169/192], loss=137.9247
	step [170/192], loss=144.3932
	step [171/192], loss=126.8653
	step [172/192], loss=145.0173
	step [173/192], loss=123.9420
	step [174/192], loss=154.4572
	step [175/192], loss=146.4277
	step [176/192], loss=130.9353
	step [177/192], loss=144.3044
	step [178/192], loss=140.7054
	step [179/192], loss=133.4108
	step [180/192], loss=138.5074
	step [181/192], loss=142.6938
	step [182/192], loss=144.1430
	step [183/192], loss=120.4176
	step [184/192], loss=141.3446
	step [185/192], loss=137.7130
	step [186/192], loss=140.4636
	step [187/192], loss=125.5839
	step [188/192], loss=156.8620
	step [189/192], loss=125.4980
	step [190/192], loss=118.7258
	step [191/192], loss=126.4190
	step [192/192], loss=18.3358
	Evaluating
	loss=0.1350, precision=0.4754, recall=0.9061, f1=0.6236
saving model as: 0_saved_model.pth
Training epoch 3
	step [1/192], loss=136.3410
	step [2/192], loss=133.7908
	step [3/192], loss=125.8061
	step [4/192], loss=133.0709
	step [5/192], loss=143.3733
	step [6/192], loss=130.7780
	step [7/192], loss=124.7799
	step [8/192], loss=153.4507
	step [9/192], loss=138.0747
	step [10/192], loss=139.6246
	step [11/192], loss=122.6630
	step [12/192], loss=142.8622
	step [13/192], loss=128.3965
	step [14/192], loss=128.6602
	step [15/192], loss=132.2482
	step [16/192], loss=152.9819
	step [17/192], loss=134.9195
	step [18/192], loss=131.9651
	step [19/192], loss=121.0233
	step [20/192], loss=122.4079
	step [21/192], loss=141.2641
	step [22/192], loss=140.7400
	step [23/192], loss=144.4736
	step [24/192], loss=125.1902
	step [25/192], loss=127.8780
	step [26/192], loss=145.2572
	step [27/192], loss=139.2313
	step [28/192], loss=142.5412
	step [29/192], loss=146.5719
	step [30/192], loss=129.2776
	step [31/192], loss=147.5081
	step [32/192], loss=130.3404
	step [33/192], loss=159.6052
	step [34/192], loss=128.6034
	step [35/192], loss=139.4826
	step [36/192], loss=127.2524
	step [37/192], loss=126.8348
	step [38/192], loss=125.0855
	step [39/192], loss=138.6980
	step [40/192], loss=128.6102
	step [41/192], loss=135.1774
	step [42/192], loss=125.4930
	step [43/192], loss=122.4523
	step [44/192], loss=134.2063
	step [45/192], loss=141.9137
	step [46/192], loss=124.9481
	step [47/192], loss=136.1938
	step [48/192], loss=128.4455
	step [49/192], loss=112.8174
	step [50/192], loss=133.3505
	step [51/192], loss=151.1964
	step [52/192], loss=125.2582
	step [53/192], loss=122.1345
	step [54/192], loss=156.8217
	step [55/192], loss=139.1277
	step [56/192], loss=129.7258
	step [57/192], loss=149.2364
	step [58/192], loss=135.6264
	step [59/192], loss=119.2703
	step [60/192], loss=140.5054
	step [61/192], loss=141.7267
	step [62/192], loss=147.8440
	step [63/192], loss=125.9490
	step [64/192], loss=139.5803
	step [65/192], loss=126.4556
	step [66/192], loss=122.6352
	step [67/192], loss=126.3049
	step [68/192], loss=121.2326
	step [69/192], loss=147.8306
	step [70/192], loss=122.6702
	step [71/192], loss=126.2121
	step [72/192], loss=161.5407
	step [73/192], loss=144.3070
	step [74/192], loss=114.3240
	step [75/192], loss=151.7467
	step [76/192], loss=125.6340
	step [77/192], loss=133.5056
	step [78/192], loss=147.9795
	step [79/192], loss=140.2524
	step [80/192], loss=125.0873
	step [81/192], loss=139.9785
	step [82/192], loss=128.4295
	step [83/192], loss=132.4805
	step [84/192], loss=126.0650
	step [85/192], loss=125.0831
	step [86/192], loss=149.0025
	step [87/192], loss=127.1977
	step [88/192], loss=125.8284
	step [89/192], loss=140.7795
	step [90/192], loss=121.5359
	step [91/192], loss=116.8204
	step [92/192], loss=124.3815
	step [93/192], loss=113.7983
	step [94/192], loss=133.3639
	step [95/192], loss=139.3344
	step [96/192], loss=127.4296
	step [97/192], loss=126.3251
	step [98/192], loss=132.1580
	step [99/192], loss=139.6066
	step [100/192], loss=121.0151
	step [101/192], loss=137.3410
	step [102/192], loss=118.8597
	step [103/192], loss=140.4881
	step [104/192], loss=130.9725
	step [105/192], loss=126.0365
	step [106/192], loss=143.5320
	step [107/192], loss=139.8915
	step [108/192], loss=135.3070
	step [109/192], loss=135.6808
	step [110/192], loss=139.0204
	step [111/192], loss=137.2470
	step [112/192], loss=128.7402
	step [113/192], loss=119.8181
	step [114/192], loss=141.5043
	step [115/192], loss=129.1913
	step [116/192], loss=118.6237
	step [117/192], loss=139.8537
	step [118/192], loss=123.3524
	step [119/192], loss=136.4518
	step [120/192], loss=129.3999
	step [121/192], loss=135.3758
	step [122/192], loss=149.7469
	step [123/192], loss=120.7360
	step [124/192], loss=142.0640
	step [125/192], loss=138.6019
	step [126/192], loss=121.9258
	step [127/192], loss=133.9759
	step [128/192], loss=128.4829
	step [129/192], loss=133.2646
	step [130/192], loss=125.2146
	step [131/192], loss=104.8094
	step [132/192], loss=124.5271
	step [133/192], loss=119.0313
	step [134/192], loss=137.5635
	step [135/192], loss=140.4939
	step [136/192], loss=127.8640
	step [137/192], loss=136.4595
	step [138/192], loss=133.2962
	step [139/192], loss=123.0838
	step [140/192], loss=151.6945
	step [141/192], loss=135.8956
	step [142/192], loss=127.4133
	step [143/192], loss=122.8383
	step [144/192], loss=132.0635
	step [145/192], loss=147.3161
	step [146/192], loss=129.4356
	step [147/192], loss=124.3774
	step [148/192], loss=128.8441
	step [149/192], loss=129.4919
	step [150/192], loss=119.1538
	step [151/192], loss=129.0350
	step [152/192], loss=134.8491
	step [153/192], loss=113.7079
	step [154/192], loss=140.9962
	step [155/192], loss=112.4338
	step [156/192], loss=140.5633
	step [157/192], loss=136.8494
	step [158/192], loss=141.7136
	step [159/192], loss=122.2379
	step [160/192], loss=135.1856
	step [161/192], loss=128.9938
	step [162/192], loss=128.7017
	step [163/192], loss=140.1615
	step [164/192], loss=116.7391
	step [165/192], loss=127.6420
	step [166/192], loss=116.2924
	step [167/192], loss=110.4855
	step [168/192], loss=117.5716
	step [169/192], loss=128.0660
	step [170/192], loss=132.8298
	step [171/192], loss=122.7238
	step [172/192], loss=116.6742
	step [173/192], loss=122.5792
	step [174/192], loss=138.6532
	step [175/192], loss=150.0986
	step [176/192], loss=128.1541
	step [177/192], loss=122.6286
	step [178/192], loss=130.6422
	step [179/192], loss=116.0473
	step [180/192], loss=127.0843
	step [181/192], loss=143.5484
	step [182/192], loss=124.7319
	step [183/192], loss=142.1297
	step [184/192], loss=127.0500
	step [185/192], loss=133.6851
	step [186/192], loss=127.8796
	step [187/192], loss=143.7344
	step [188/192], loss=124.4575
	step [189/192], loss=136.9359
	step [190/192], loss=134.3360
	step [191/192], loss=118.4895
	step [192/192], loss=14.7445
	Evaluating
	loss=0.0998, precision=0.4675, recall=0.9005, f1=0.6155
Training epoch 4
	step [1/192], loss=125.4350
	step [2/192], loss=119.8829
	step [3/192], loss=135.6361
	step [4/192], loss=116.4158
	step [5/192], loss=124.5496
	step [6/192], loss=132.4635
	step [7/192], loss=136.5459
	step [8/192], loss=129.6840
	step [9/192], loss=147.3490
	step [10/192], loss=125.0577
	step [11/192], loss=126.8038
	step [12/192], loss=130.4038
	step [13/192], loss=124.3147
	step [14/192], loss=118.7600
	step [15/192], loss=129.9608
	step [16/192], loss=142.5319
	step [17/192], loss=119.1276
	step [18/192], loss=127.1419
	step [19/192], loss=111.3089
	step [20/192], loss=138.0194
	step [21/192], loss=121.1962
	step [22/192], loss=132.0879
	step [23/192], loss=113.7143
	step [24/192], loss=146.7470
	step [25/192], loss=123.3424
	step [26/192], loss=102.5560
	step [27/192], loss=135.8267
	step [28/192], loss=128.3925
	step [29/192], loss=117.9314
	step [30/192], loss=137.1586
	step [31/192], loss=127.0550
	step [32/192], loss=132.2096
	step [33/192], loss=144.8242
	step [34/192], loss=113.8356
	step [35/192], loss=115.2968
	step [36/192], loss=131.2648
	step [37/192], loss=109.0771
	step [38/192], loss=126.9979
	step [39/192], loss=126.3443
	step [40/192], loss=135.0355
	step [41/192], loss=115.3652
	step [42/192], loss=120.0502
	step [43/192], loss=141.3608
	step [44/192], loss=118.9446
	step [45/192], loss=124.4406
	step [46/192], loss=112.0353
	step [47/192], loss=119.1408
	step [48/192], loss=111.4159
	step [49/192], loss=143.6479
	step [50/192], loss=129.6824
	step [51/192], loss=119.0043
	step [52/192], loss=144.7590
	step [53/192], loss=123.4719
	step [54/192], loss=136.2904
	step [55/192], loss=137.3834
	step [56/192], loss=118.2140
	step [57/192], loss=124.5589
	step [58/192], loss=128.0496
	step [59/192], loss=141.4503
	step [60/192], loss=130.4841
	step [61/192], loss=122.4841
	step [62/192], loss=121.0324
	step [63/192], loss=140.1394
	step [64/192], loss=129.8662
	step [65/192], loss=136.4551
	step [66/192], loss=128.2840
	step [67/192], loss=112.7824
	step [68/192], loss=113.8542
	step [69/192], loss=137.2370
	step [70/192], loss=123.0099
	step [71/192], loss=121.3992
	step [72/192], loss=99.8174
	step [73/192], loss=130.6925
	step [74/192], loss=131.9647
	step [75/192], loss=127.8737
	step [76/192], loss=106.2256
	step [77/192], loss=123.5420
	step [78/192], loss=114.2979
	step [79/192], loss=113.1980
	step [80/192], loss=122.1587
	step [81/192], loss=130.9942
	step [82/192], loss=132.8949
	step [83/192], loss=136.6898
	step [84/192], loss=131.6850
	step [85/192], loss=122.1575
	step [86/192], loss=121.8334
	step [87/192], loss=120.4391
	step [88/192], loss=118.3553
	step [89/192], loss=123.3755
	step [90/192], loss=133.8836
	step [91/192], loss=119.3941
	step [92/192], loss=114.9207
	step [93/192], loss=124.0158
	step [94/192], loss=131.4622
	step [95/192], loss=123.6362
	step [96/192], loss=113.3208
	step [97/192], loss=127.7429
	step [98/192], loss=117.3733
	step [99/192], loss=110.8461
	step [100/192], loss=127.5124
	step [101/192], loss=104.1609
	step [102/192], loss=124.5810
	step [103/192], loss=114.3562
	step [104/192], loss=117.5068
	step [105/192], loss=126.7358
	step [106/192], loss=105.3769
	step [107/192], loss=128.2609
	step [108/192], loss=138.6212
	step [109/192], loss=111.0116
	step [110/192], loss=119.1555
	step [111/192], loss=113.9167
	step [112/192], loss=140.5401
	step [113/192], loss=110.6459
	step [114/192], loss=116.9705
	step [115/192], loss=117.7077
	step [116/192], loss=138.6292
	step [117/192], loss=119.6446
	step [118/192], loss=120.9337
	step [119/192], loss=112.9670
	step [120/192], loss=123.4520
	step [121/192], loss=129.6374
	step [122/192], loss=136.0728
	step [123/192], loss=130.9502
	step [124/192], loss=117.9750
	step [125/192], loss=118.4741
	step [126/192], loss=146.6053
	step [127/192], loss=112.8302
	step [128/192], loss=127.9402
	step [129/192], loss=133.7045
	step [130/192], loss=125.7255
	step [131/192], loss=112.7751
	step [132/192], loss=116.3307
	step [133/192], loss=106.3353
	step [134/192], loss=104.4726
	step [135/192], loss=129.6948
	step [136/192], loss=123.3360
	step [137/192], loss=139.2804
	step [138/192], loss=120.4981
	step [139/192], loss=132.0032
	step [140/192], loss=123.5803
	step [141/192], loss=119.4577
	step [142/192], loss=135.0344
	step [143/192], loss=123.5704
	step [144/192], loss=126.6726
	step [145/192], loss=132.6263
	step [146/192], loss=120.3730
	step [147/192], loss=121.9665
	step [148/192], loss=128.1017
	step [149/192], loss=135.4661
	step [150/192], loss=123.3078
	step [151/192], loss=113.6610
	step [152/192], loss=133.3085
	step [153/192], loss=118.7243
	step [154/192], loss=128.9500
	step [155/192], loss=133.2912
	step [156/192], loss=120.6710
	step [157/192], loss=124.8652
	step [158/192], loss=125.3773
	step [159/192], loss=118.9026
	step [160/192], loss=123.8558
	step [161/192], loss=143.4501
	step [162/192], loss=125.1042
	step [163/192], loss=116.2454
	step [164/192], loss=137.7172
	step [165/192], loss=128.0970
	step [166/192], loss=132.3640
	step [167/192], loss=123.1182
	step [168/192], loss=143.3939
	step [169/192], loss=126.3103
	step [170/192], loss=118.2164
	step [171/192], loss=130.5943
	step [172/192], loss=133.7483
	step [173/192], loss=125.6475
	step [174/192], loss=119.5825
	step [175/192], loss=109.5445
	step [176/192], loss=110.9984
	step [177/192], loss=125.9715
	step [178/192], loss=127.8129
	step [179/192], loss=113.8449
	step [180/192], loss=125.2028
	step [181/192], loss=127.6626
	step [182/192], loss=128.0675
	step [183/192], loss=111.7650
	step [184/192], loss=121.6780
	step [185/192], loss=119.4355
	step [186/192], loss=119.7290
	step [187/192], loss=117.4352
	step [188/192], loss=110.0150
	step [189/192], loss=120.8852
	step [190/192], loss=121.7156
	step [191/192], loss=117.9005
	step [192/192], loss=18.4399
	Evaluating
	loss=0.0778, precision=0.3729, recall=0.9271, f1=0.5318
Training epoch 5
	step [1/192], loss=124.5020
	step [2/192], loss=137.6418
	step [3/192], loss=128.7903
	step [4/192], loss=126.6168
	step [5/192], loss=121.2217
	step [6/192], loss=108.9573
	step [7/192], loss=121.2858
	step [8/192], loss=119.1042
	step [9/192], loss=120.2864
	step [10/192], loss=132.2110
	step [11/192], loss=107.1103
	step [12/192], loss=123.2800
	step [13/192], loss=128.0571
	step [14/192], loss=122.9664
	step [15/192], loss=127.2108
	step [16/192], loss=112.4590
	step [17/192], loss=123.4311
	step [18/192], loss=131.0209
	step [19/192], loss=107.6988
	step [20/192], loss=113.9935
	step [21/192], loss=121.6790
	step [22/192], loss=123.0052
	step [23/192], loss=115.8672
	step [24/192], loss=114.3021
	step [25/192], loss=116.2768
	step [26/192], loss=115.6981
	step [27/192], loss=120.2922
	step [28/192], loss=119.6246
	step [29/192], loss=111.8897
	step [30/192], loss=125.2423
	step [31/192], loss=121.7771
	step [32/192], loss=120.1856
	step [33/192], loss=105.6541
	step [34/192], loss=123.6345
	step [35/192], loss=115.4688
	step [36/192], loss=127.5563
	step [37/192], loss=109.3867
	step [38/192], loss=125.6341
	step [39/192], loss=119.8879
	step [40/192], loss=131.8948
	step [41/192], loss=116.3857
	step [42/192], loss=124.4910
	step [43/192], loss=130.7272
	step [44/192], loss=109.7987
	step [45/192], loss=131.2013
	step [46/192], loss=113.6857
	step [47/192], loss=116.7360
	step [48/192], loss=121.3578
	step [49/192], loss=103.1139
	step [50/192], loss=126.4719
	step [51/192], loss=128.8029
	step [52/192], loss=127.0730
	step [53/192], loss=120.9659
	step [54/192], loss=118.1815
	step [55/192], loss=129.7676
	step [56/192], loss=128.5429
	step [57/192], loss=112.6191
	step [58/192], loss=110.4435
	step [59/192], loss=113.2887
	step [60/192], loss=110.3086
	step [61/192], loss=115.3630
	step [62/192], loss=140.8831
	step [63/192], loss=126.8903
	step [64/192], loss=124.7437
	step [65/192], loss=143.5120
	step [66/192], loss=109.7898
	step [67/192], loss=108.6440
	step [68/192], loss=112.7685
	step [69/192], loss=112.6136
	step [70/192], loss=127.5984
	step [71/192], loss=110.6627
	step [72/192], loss=143.1714
	step [73/192], loss=124.6500
	step [74/192], loss=106.1436
	step [75/192], loss=107.0511
	step [76/192], loss=119.1192
	step [77/192], loss=104.3258
	step [78/192], loss=128.0335
	step [79/192], loss=135.8589
	step [80/192], loss=105.5487
	step [81/192], loss=111.3228
	step [82/192], loss=116.4089
	step [83/192], loss=114.6331
	step [84/192], loss=101.5639
	step [85/192], loss=118.9179
	step [86/192], loss=107.1505
	step [87/192], loss=113.7135
	step [88/192], loss=139.0652
	step [89/192], loss=142.4229
	step [90/192], loss=111.7437
	step [91/192], loss=108.9247
	step [92/192], loss=125.9291
	step [93/192], loss=103.0619
	step [94/192], loss=92.5406
	step [95/192], loss=120.1433
	step [96/192], loss=121.1158
	step [97/192], loss=101.7152
	step [98/192], loss=110.0836
	step [99/192], loss=117.0102
	step [100/192], loss=126.3692
	step [101/192], loss=131.8255
	step [102/192], loss=114.2462
	step [103/192], loss=132.1986
	step [104/192], loss=117.8058
	step [105/192], loss=122.0985
	step [106/192], loss=120.2518
	step [107/192], loss=114.1450
	step [108/192], loss=127.2264
	step [109/192], loss=110.8486
	step [110/192], loss=122.8713
	step [111/192], loss=138.9137
	step [112/192], loss=118.5340
	step [113/192], loss=129.8980
	step [114/192], loss=112.2682
	step [115/192], loss=118.9662
	step [116/192], loss=105.4727
	step [117/192], loss=114.3413
	step [118/192], loss=117.3272
	step [119/192], loss=109.0092
	step [120/192], loss=108.9462
	step [121/192], loss=117.3730
	step [122/192], loss=115.4249
	step [123/192], loss=125.2322
	step [124/192], loss=114.1089
	step [125/192], loss=137.2386
	step [126/192], loss=112.9793
	step [127/192], loss=120.4786
	step [128/192], loss=111.9117
	step [129/192], loss=133.8793
	step [130/192], loss=112.4670
	step [131/192], loss=127.6512
	step [132/192], loss=125.4527
	step [133/192], loss=113.9852
	step [134/192], loss=105.0570
	step [135/192], loss=133.4182
	step [136/192], loss=126.9451
	step [137/192], loss=111.3966
	step [138/192], loss=114.7328
	step [139/192], loss=119.7955
	step [140/192], loss=123.6194
	step [141/192], loss=111.5058
	step [142/192], loss=121.4373
	step [143/192], loss=114.6141
	step [144/192], loss=138.4070
	step [145/192], loss=123.3143
	step [146/192], loss=115.4609
	step [147/192], loss=110.2738
	step [148/192], loss=128.0246
	step [149/192], loss=123.7899
	step [150/192], loss=131.9286
	step [151/192], loss=115.3062
	step [152/192], loss=109.1407
	step [153/192], loss=120.5900
	step [154/192], loss=108.2120
	step [155/192], loss=110.7837
	step [156/192], loss=122.2671
	step [157/192], loss=117.6153
	step [158/192], loss=106.4057
	step [159/192], loss=120.8390
	step [160/192], loss=131.7787
	step [161/192], loss=104.0528
	step [162/192], loss=128.8214
	step [163/192], loss=119.8390
	step [164/192], loss=109.8304
	step [165/192], loss=116.0163
	step [166/192], loss=116.1725
	step [167/192], loss=123.7574
	step [168/192], loss=100.6024
	step [169/192], loss=126.8305
	step [170/192], loss=105.1710
	step [171/192], loss=121.8378
	step [172/192], loss=108.1200
	step [173/192], loss=96.6302
	step [174/192], loss=99.0097
	step [175/192], loss=135.7057
	step [176/192], loss=114.4054
	step [177/192], loss=129.4146
	step [178/192], loss=121.3828
	step [179/192], loss=113.9323
	step [180/192], loss=123.9870
	step [181/192], loss=140.1108
	step [182/192], loss=113.7077
	step [183/192], loss=117.2144
	step [184/192], loss=117.7004
	step [185/192], loss=138.0870
	step [186/192], loss=125.4603
	step [187/192], loss=138.5245
	step [188/192], loss=124.1174
	step [189/192], loss=125.5299
	step [190/192], loss=109.3087
	step [191/192], loss=133.3484
	step [192/192], loss=19.0351
	Evaluating
	loss=0.0576, precision=0.4703, recall=0.9156, f1=0.6214
Training epoch 6
	step [1/192], loss=123.7284
	step [2/192], loss=110.9319
	step [3/192], loss=117.3044
	step [4/192], loss=106.8490
	step [5/192], loss=112.4274
	step [6/192], loss=118.1220
	step [7/192], loss=105.7532
	step [8/192], loss=123.5967
	step [9/192], loss=117.1820
	step [10/192], loss=120.4041
	step [11/192], loss=111.0310
	step [12/192], loss=106.7215
	step [13/192], loss=127.1169
	step [14/192], loss=110.0326
	step [15/192], loss=127.3188
	step [16/192], loss=135.5874
	step [17/192], loss=124.3848
	step [18/192], loss=123.4251
	step [19/192], loss=128.2866
	step [20/192], loss=122.5776
	step [21/192], loss=112.9210
	step [22/192], loss=103.3760
	step [23/192], loss=107.9243
	step [24/192], loss=116.4570
	step [25/192], loss=96.3754
	step [26/192], loss=111.7566
	step [27/192], loss=118.7104
	step [28/192], loss=123.2673
	step [29/192], loss=114.6855
	step [30/192], loss=113.6135
	step [31/192], loss=116.1440
	step [32/192], loss=119.7921
	step [33/192], loss=112.9532
	step [34/192], loss=124.0289
	step [35/192], loss=109.4728
	step [36/192], loss=119.9811
	step [37/192], loss=111.0144
	step [38/192], loss=112.2251
	step [39/192], loss=114.3728
	step [40/192], loss=127.1202
	step [41/192], loss=115.2450
	step [42/192], loss=116.5802
	step [43/192], loss=117.3668
	step [44/192], loss=121.9466
	step [45/192], loss=111.8165
	step [46/192], loss=120.9050
	step [47/192], loss=116.8504
	step [48/192], loss=134.7059
	step [49/192], loss=116.5055
	step [50/192], loss=100.6122
	step [51/192], loss=111.7780
	step [52/192], loss=110.6270
	step [53/192], loss=109.5214
	step [54/192], loss=124.8148
	step [55/192], loss=120.1278
	step [56/192], loss=122.0043
	step [57/192], loss=118.8021
	step [58/192], loss=118.0855
	step [59/192], loss=123.2470
	step [60/192], loss=111.1825
	step [61/192], loss=84.0657
	step [62/192], loss=116.9150
	step [63/192], loss=105.4851
	step [64/192], loss=140.4592
	step [65/192], loss=102.1629
	step [66/192], loss=109.4733
	step [67/192], loss=115.1805
	step [68/192], loss=117.8434
	step [69/192], loss=111.0109
	step [70/192], loss=111.0687
	step [71/192], loss=103.7591
	step [72/192], loss=121.6332
	step [73/192], loss=112.0418
	step [74/192], loss=110.4111
	step [75/192], loss=115.9479
	step [76/192], loss=127.0347
	step [77/192], loss=116.3756
	step [78/192], loss=103.3553
	step [79/192], loss=115.0877
	step [80/192], loss=131.3523
	step [81/192], loss=104.8137
	step [82/192], loss=134.0576
	step [83/192], loss=118.2225
	step [84/192], loss=103.1947
	step [85/192], loss=123.8743
	step [86/192], loss=134.2070
	step [87/192], loss=102.2541
	step [88/192], loss=132.6654
	step [89/192], loss=125.6159
	step [90/192], loss=121.4890
	step [91/192], loss=114.8906
	step [92/192], loss=115.3027
	step [93/192], loss=128.6484
	step [94/192], loss=119.6505
	step [95/192], loss=121.4963
	step [96/192], loss=110.5228
	step [97/192], loss=108.8398
	step [98/192], loss=113.2175
	step [99/192], loss=119.8632
	step [100/192], loss=135.8487
	step [101/192], loss=130.3559
	step [102/192], loss=116.1385
	step [103/192], loss=118.7397
	step [104/192], loss=90.6191
	step [105/192], loss=121.0955
	step [106/192], loss=128.2039
	step [107/192], loss=112.3611
	step [108/192], loss=110.0153
	step [109/192], loss=118.0311
	step [110/192], loss=105.2509
	step [111/192], loss=115.2855
	step [112/192], loss=104.3615
	step [113/192], loss=100.6179
	step [114/192], loss=111.3920
	step [115/192], loss=110.5186
	step [116/192], loss=103.6448
	step [117/192], loss=111.6092
	step [118/192], loss=114.6857
	step [119/192], loss=104.6667
	step [120/192], loss=106.9868
	step [121/192], loss=108.6900
	step [122/192], loss=121.0895
	step [123/192], loss=111.6046
	step [124/192], loss=114.3091
	step [125/192], loss=125.4969
	step [126/192], loss=126.2042
	step [127/192], loss=122.9518
	step [128/192], loss=116.3835
	step [129/192], loss=114.3152
	step [130/192], loss=118.5985
	step [131/192], loss=123.0487
	step [132/192], loss=129.9158
	step [133/192], loss=112.7788
	step [134/192], loss=123.4809
	step [135/192], loss=113.8311
	step [136/192], loss=116.2340
	step [137/192], loss=105.8035
	step [138/192], loss=112.4745
	step [139/192], loss=112.3508
	step [140/192], loss=115.9506
	step [141/192], loss=112.3988
	step [142/192], loss=108.4853
	step [143/192], loss=129.6979
	step [144/192], loss=126.0170
	step [145/192], loss=114.5767
	step [146/192], loss=103.7905
	step [147/192], loss=112.0994
	step [148/192], loss=114.6103
	step [149/192], loss=100.3020
	step [150/192], loss=142.7403
	step [151/192], loss=105.8615
	step [152/192], loss=118.0702
	step [153/192], loss=99.3983
	step [154/192], loss=95.7334
	step [155/192], loss=103.8849
	step [156/192], loss=137.4794
	step [157/192], loss=121.2560
	step [158/192], loss=100.7335
	step [159/192], loss=118.4060
	step [160/192], loss=108.9885
	step [161/192], loss=101.5545
	step [162/192], loss=114.7684
	step [163/192], loss=129.9779
	step [164/192], loss=104.2115
	step [165/192], loss=117.3397
	step [166/192], loss=123.9392
	step [167/192], loss=120.5440
	step [168/192], loss=130.0587
	step [169/192], loss=120.0674
	step [170/192], loss=112.9950
	step [171/192], loss=112.0208
	step [172/192], loss=122.6639
	step [173/192], loss=121.6962
	step [174/192], loss=97.9143
	step [175/192], loss=131.8304
	step [176/192], loss=124.8311
	step [177/192], loss=125.9104
	step [178/192], loss=130.7017
	step [179/192], loss=134.4138
	step [180/192], loss=92.9727
	step [181/192], loss=94.1553
	step [182/192], loss=93.4428
	step [183/192], loss=109.5027
	step [184/192], loss=103.2319
	step [185/192], loss=116.4685
	step [186/192], loss=109.8774
	step [187/192], loss=115.1331
	step [188/192], loss=111.1741
	step [189/192], loss=107.9619
	step [190/192], loss=139.2974
	step [191/192], loss=114.0103
	step [192/192], loss=16.6974
	Evaluating
	loss=0.0483, precision=0.3964, recall=0.9308, f1=0.5561
Training epoch 7
	step [1/192], loss=122.9120
	step [2/192], loss=114.2139
	step [3/192], loss=121.6237
	step [4/192], loss=103.7394
	step [5/192], loss=110.0352
	step [6/192], loss=103.4399
	step [7/192], loss=113.4054
	step [8/192], loss=116.6798
	step [9/192], loss=111.3120
	step [10/192], loss=108.5944
	step [11/192], loss=104.7149
	step [12/192], loss=113.0525
	step [13/192], loss=126.7997
	step [14/192], loss=110.2675
	step [15/192], loss=110.7935
	step [16/192], loss=111.6610
	step [17/192], loss=100.9260
	step [18/192], loss=131.1417
	step [19/192], loss=105.5144
	step [20/192], loss=101.7462
	step [21/192], loss=105.7097
	step [22/192], loss=102.9712
	step [23/192], loss=112.9176
	step [24/192], loss=111.7886
	step [25/192], loss=122.9830
	step [26/192], loss=120.5118
	step [27/192], loss=116.5879
	step [28/192], loss=109.9132
	step [29/192], loss=111.1492
	step [30/192], loss=114.8072
	step [31/192], loss=93.1750
	step [32/192], loss=109.6896
	step [33/192], loss=107.5636
	step [34/192], loss=117.0022
	step [35/192], loss=106.1957
	step [36/192], loss=113.0869
	step [37/192], loss=122.3644
	step [38/192], loss=103.3560
	step [39/192], loss=100.0649
	step [40/192], loss=115.9946
	step [41/192], loss=118.0810
	step [42/192], loss=113.3534
	step [43/192], loss=131.7215
	step [44/192], loss=134.5419
	step [45/192], loss=107.5931
	step [46/192], loss=112.1604
	step [47/192], loss=129.2306
	step [48/192], loss=97.8864
	step [49/192], loss=133.4691
	step [50/192], loss=118.5210
	step [51/192], loss=107.7320
	step [52/192], loss=105.9239
	step [53/192], loss=124.2844
	step [54/192], loss=106.8245
	step [55/192], loss=100.6546
	step [56/192], loss=100.1835
	step [57/192], loss=119.6059
	step [58/192], loss=110.3651
	step [59/192], loss=109.7087
	step [60/192], loss=128.9917
	step [61/192], loss=120.6298
	step [62/192], loss=115.2868
	step [63/192], loss=111.5503
	step [64/192], loss=110.4851
	step [65/192], loss=111.1406
	step [66/192], loss=105.3742
	step [67/192], loss=108.8578
	step [68/192], loss=120.8268
	step [69/192], loss=103.5971
	step [70/192], loss=107.1003
	step [71/192], loss=112.8833
	step [72/192], loss=113.0493
	step [73/192], loss=116.6987
	step [74/192], loss=98.3793
	step [75/192], loss=127.2145
	step [76/192], loss=123.6915
	step [77/192], loss=123.2998
	step [78/192], loss=103.0175
	step [79/192], loss=123.7459
	step [80/192], loss=108.0407
	step [81/192], loss=107.9765
	step [82/192], loss=125.5907
	step [83/192], loss=105.6254
	step [84/192], loss=111.2414
	step [85/192], loss=110.5175
	step [86/192], loss=107.5228
	step [87/192], loss=118.0061
	step [88/192], loss=107.4422
	step [89/192], loss=117.5523
	step [90/192], loss=112.1945
	step [91/192], loss=125.4549
	step [92/192], loss=110.6135
	step [93/192], loss=99.9206
	step [94/192], loss=97.3631
	step [95/192], loss=117.1393
	step [96/192], loss=116.2751
	step [97/192], loss=105.7152
	step [98/192], loss=101.5387
	step [99/192], loss=113.4461
	step [100/192], loss=115.6185
	step [101/192], loss=106.3882
	step [102/192], loss=112.4183
	step [103/192], loss=108.8524
	step [104/192], loss=119.5778
	step [105/192], loss=114.0605
	step [106/192], loss=104.4971
	step [107/192], loss=116.3140
	step [108/192], loss=106.7239
	step [109/192], loss=103.4257
	step [110/192], loss=122.0096
	step [111/192], loss=114.8476
	step [112/192], loss=104.3603
	step [113/192], loss=124.9763
	step [114/192], loss=107.3884
	step [115/192], loss=127.6184
	step [116/192], loss=112.8386
	step [117/192], loss=97.6969
	step [118/192], loss=112.4147
	step [119/192], loss=103.1148
	step [120/192], loss=107.5767
	step [121/192], loss=104.8819
	step [122/192], loss=104.2223
	step [123/192], loss=102.8159
	step [124/192], loss=111.9510
	step [125/192], loss=110.6020
	step [126/192], loss=121.4540
	step [127/192], loss=128.7290
	step [128/192], loss=124.3692
	step [129/192], loss=124.0119
	step [130/192], loss=106.6660
	step [131/192], loss=114.3769
	step [132/192], loss=110.9059
	step [133/192], loss=98.8394
	step [134/192], loss=103.6064
	step [135/192], loss=100.4644
	step [136/192], loss=133.8429
	step [137/192], loss=100.2410
	step [138/192], loss=102.7166
	step [139/192], loss=126.8158
	step [140/192], loss=111.1239
	step [141/192], loss=105.1765
	step [142/192], loss=107.6046
	step [143/192], loss=129.9377
	step [144/192], loss=119.0174
	step [145/192], loss=105.6412
	step [146/192], loss=115.5032
	step [147/192], loss=111.0177
	step [148/192], loss=112.1105
	step [149/192], loss=111.7054
	step [150/192], loss=113.6316
	step [151/192], loss=111.3273
	step [152/192], loss=105.4584
	step [153/192], loss=106.6162
	step [154/192], loss=121.5469
	step [155/192], loss=101.0068
	step [156/192], loss=115.2585
	step [157/192], loss=110.1196
	step [158/192], loss=103.2233
	step [159/192], loss=116.7899
	step [160/192], loss=107.9847
	step [161/192], loss=107.8442
	step [162/192], loss=130.3780
	step [163/192], loss=118.2110
	step [164/192], loss=121.2916
	step [165/192], loss=111.4613
	step [166/192], loss=109.9187
	step [167/192], loss=121.4285
	step [168/192], loss=119.1205
	step [169/192], loss=121.2071
	step [170/192], loss=116.1384
	step [171/192], loss=112.2173
	step [172/192], loss=91.5013
	step [173/192], loss=105.5820
	step [174/192], loss=102.5589
	step [175/192], loss=121.6735
	step [176/192], loss=108.4159
	step [177/192], loss=122.1176
	step [178/192], loss=109.4622
	step [179/192], loss=108.9792
	step [180/192], loss=120.3319
	step [181/192], loss=119.9273
	step [182/192], loss=108.0454
	step [183/192], loss=103.6296
	step [184/192], loss=114.4016
	step [185/192], loss=115.0159
	step [186/192], loss=103.6655
	step [187/192], loss=117.8702
	step [188/192], loss=102.6379
	step [189/192], loss=109.7164
	step [190/192], loss=116.8229
	step [191/192], loss=105.4394
	step [192/192], loss=12.5895
	Evaluating
	loss=0.0449, precision=0.3500, recall=0.9113, f1=0.5058
Training epoch 8
	step [1/192], loss=130.7546
	step [2/192], loss=110.0891
	step [3/192], loss=110.2376
	step [4/192], loss=98.9688
	step [5/192], loss=121.6297
	step [6/192], loss=108.7620
	step [7/192], loss=103.2509
	step [8/192], loss=124.3387
	step [9/192], loss=114.1977
	step [10/192], loss=118.3787
	step [11/192], loss=137.6823
	step [12/192], loss=108.7507
	step [13/192], loss=107.3528
	step [14/192], loss=109.9107
	step [15/192], loss=101.3606
	step [16/192], loss=102.4773
	step [17/192], loss=111.9115
	step [18/192], loss=114.3357
	step [19/192], loss=115.8161
	step [20/192], loss=88.3837
	step [21/192], loss=116.5801
	step [22/192], loss=126.9879
	step [23/192], loss=127.8805
	step [24/192], loss=108.0398
	step [25/192], loss=121.6764
	step [26/192], loss=92.5489
	step [27/192], loss=100.0922
	step [28/192], loss=127.8303
	step [29/192], loss=99.8098
	step [30/192], loss=103.5504
	step [31/192], loss=106.9164
	step [32/192], loss=108.5929
	step [33/192], loss=110.1860
	step [34/192], loss=113.9102
	step [35/192], loss=126.0550
	step [36/192], loss=108.6978
	step [37/192], loss=108.4564
	step [38/192], loss=138.7878
	step [39/192], loss=109.0015
	step [40/192], loss=101.1777
	step [41/192], loss=102.2037
	step [42/192], loss=109.8266
	step [43/192], loss=102.2147
	step [44/192], loss=112.0055
	step [45/192], loss=95.8308
	step [46/192], loss=100.7058
	step [47/192], loss=124.1647
	step [48/192], loss=114.0046
	step [49/192], loss=122.4817
	step [50/192], loss=100.5260
	step [51/192], loss=112.9713
	step [52/192], loss=118.6236
	step [53/192], loss=98.9268
	step [54/192], loss=135.0090
	step [55/192], loss=119.0258
	step [56/192], loss=120.5224
	step [57/192], loss=101.0527
	step [58/192], loss=108.4099
	step [59/192], loss=105.6280
	step [60/192], loss=91.9401
	step [61/192], loss=106.2739
	step [62/192], loss=104.5800
	step [63/192], loss=100.4520
	step [64/192], loss=135.3160
	step [65/192], loss=100.4631
	step [66/192], loss=112.6409
	step [67/192], loss=92.0728
	step [68/192], loss=113.4919
	step [69/192], loss=111.4021
	step [70/192], loss=124.2263
	step [71/192], loss=115.2490
	step [72/192], loss=120.8087
	step [73/192], loss=88.8314
	step [74/192], loss=110.3580
	step [75/192], loss=96.0799
	step [76/192], loss=127.1919
	step [77/192], loss=99.1904
	step [78/192], loss=110.8794
	step [79/192], loss=110.4419
	step [80/192], loss=99.9312
	step [81/192], loss=100.6032
	step [82/192], loss=97.2181
	step [83/192], loss=111.0656
	step [84/192], loss=126.6662
	step [85/192], loss=94.9226
	step [86/192], loss=107.6439
	step [87/192], loss=105.2158
	step [88/192], loss=119.4620
	step [89/192], loss=126.3761
	step [90/192], loss=95.0257
	step [91/192], loss=106.8070
	step [92/192], loss=112.2820
	step [93/192], loss=100.3256
	step [94/192], loss=109.8926
	step [95/192], loss=114.7776
	step [96/192], loss=110.1411
	step [97/192], loss=106.9151
	step [98/192], loss=105.3659
	step [99/192], loss=99.6572
	step [100/192], loss=131.9296
	step [101/192], loss=101.4153
	step [102/192], loss=136.8293
	step [103/192], loss=106.2693
	step [104/192], loss=107.8918
	step [105/192], loss=118.2599
	step [106/192], loss=94.1786
	step [107/192], loss=118.2766
	step [108/192], loss=103.9771
	step [109/192], loss=127.3150
	step [110/192], loss=118.8787
	step [111/192], loss=118.0654
	step [112/192], loss=99.1042
	step [113/192], loss=109.8520
	step [114/192], loss=101.3643
	step [115/192], loss=99.6517
	step [116/192], loss=106.0910
	step [117/192], loss=111.6875
	step [118/192], loss=133.6334
	step [119/192], loss=114.8001
	step [120/192], loss=109.8871
	step [121/192], loss=86.1243
	step [122/192], loss=97.5882
	step [123/192], loss=97.0167
	step [124/192], loss=120.2420
	step [125/192], loss=121.6488
	step [126/192], loss=97.3271
	step [127/192], loss=101.7741
	step [128/192], loss=114.9606
	step [129/192], loss=111.1547
	step [130/192], loss=115.2674
	step [131/192], loss=119.5155
	step [132/192], loss=95.0348
	step [133/192], loss=119.9430
	step [134/192], loss=106.7240
	step [135/192], loss=111.4754
	step [136/192], loss=105.2555
	step [137/192], loss=111.9645
	step [138/192], loss=98.9871
	step [139/192], loss=99.6158
	step [140/192], loss=105.4223
	step [141/192], loss=102.1880
	step [142/192], loss=123.0987
	step [143/192], loss=121.3315
	step [144/192], loss=117.3935
	step [145/192], loss=118.2965
	step [146/192], loss=110.6721
	step [147/192], loss=101.6494
	step [148/192], loss=114.4529
	step [149/192], loss=114.1811
	step [150/192], loss=115.1391
	step [151/192], loss=101.6257
	step [152/192], loss=100.5279
	step [153/192], loss=118.4835
	step [154/192], loss=111.4698
	step [155/192], loss=97.7248
	step [156/192], loss=114.2000
	step [157/192], loss=103.8029
	step [158/192], loss=102.5225
	step [159/192], loss=104.4779
	step [160/192], loss=88.7620
	step [161/192], loss=106.7326
	step [162/192], loss=93.5908
	step [163/192], loss=95.2552
	step [164/192], loss=113.2456
	step [165/192], loss=123.3647
	step [166/192], loss=117.2151
	step [167/192], loss=106.1233
	step [168/192], loss=107.2262
	step [169/192], loss=110.0105
	step [170/192], loss=113.1635
	step [171/192], loss=104.7871
	step [172/192], loss=112.7787
	step [173/192], loss=93.9816
	step [174/192], loss=107.4236
	step [175/192], loss=113.4187
	step [176/192], loss=95.8506
	step [177/192], loss=141.0153
	step [178/192], loss=108.2115
	step [179/192], loss=101.4679
	step [180/192], loss=108.5862
	step [181/192], loss=100.8446
	step [182/192], loss=110.1543
	step [183/192], loss=103.8910
	step [184/192], loss=120.2701
	step [185/192], loss=101.5453
	step [186/192], loss=95.6489
	step [187/192], loss=100.3100
	step [188/192], loss=108.4484
	step [189/192], loss=115.7171
	step [190/192], loss=107.9890
	step [191/192], loss=103.0774
	step [192/192], loss=13.6415
	Evaluating
	loss=0.0355, precision=0.3593, recall=0.9314, f1=0.5186
Training epoch 9
	step [1/192], loss=94.0446
	step [2/192], loss=101.9867
	step [3/192], loss=111.2509
	step [4/192], loss=104.7902
	step [5/192], loss=113.7297
	step [6/192], loss=104.3890
	step [7/192], loss=111.4392
	step [8/192], loss=106.0749
	step [9/192], loss=115.8020
	step [10/192], loss=102.7858
	step [11/192], loss=112.5281
	step [12/192], loss=102.3941
	step [13/192], loss=117.7539
	step [14/192], loss=115.8257
	step [15/192], loss=106.2935
	step [16/192], loss=119.3866
	step [17/192], loss=92.5525
	step [18/192], loss=124.1583
	step [19/192], loss=98.5514
	step [20/192], loss=128.6719
	step [21/192], loss=97.4342
	step [22/192], loss=92.8086
	step [23/192], loss=98.0927
	step [24/192], loss=99.7694
	step [25/192], loss=94.8379
	step [26/192], loss=100.7285
	step [27/192], loss=112.4156
	step [28/192], loss=102.7040
	step [29/192], loss=93.8281
	step [30/192], loss=126.5879
	step [31/192], loss=106.9371
	step [32/192], loss=96.3619
	step [33/192], loss=111.1237
	step [34/192], loss=104.2179
	step [35/192], loss=117.8237
	step [36/192], loss=98.5726
	step [37/192], loss=108.0232
	step [38/192], loss=100.6460
	step [39/192], loss=107.6340
	step [40/192], loss=98.9590
	step [41/192], loss=112.7585
	step [42/192], loss=121.9894
	step [43/192], loss=110.3353
	step [44/192], loss=103.7163
	step [45/192], loss=120.4145
	step [46/192], loss=108.2483
	step [47/192], loss=106.4895
	step [48/192], loss=95.3380
	step [49/192], loss=101.0956
	step [50/192], loss=119.2546
	step [51/192], loss=111.9962
	step [52/192], loss=107.8951
	step [53/192], loss=95.9893
	step [54/192], loss=100.6689
	step [55/192], loss=106.9045
	step [56/192], loss=110.3278
	step [57/192], loss=102.9802
	step [58/192], loss=107.5186
	step [59/192], loss=109.9016
	step [60/192], loss=109.9236
	step [61/192], loss=95.9613
	step [62/192], loss=109.7803
	step [63/192], loss=100.5050
	step [64/192], loss=108.5200
	step [65/192], loss=102.6337
	step [66/192], loss=96.0425
	step [67/192], loss=105.6231
	step [68/192], loss=113.5393
	step [69/192], loss=115.5251
	step [70/192], loss=109.1957
	step [71/192], loss=90.0345
	step [72/192], loss=126.3555
	step [73/192], loss=105.6959
	step [74/192], loss=97.4531
	step [75/192], loss=120.2009
	step [76/192], loss=106.3950
	step [77/192], loss=109.5025
	step [78/192], loss=115.7069
	step [79/192], loss=110.3183
	step [80/192], loss=113.2494
	step [81/192], loss=115.9714
	step [82/192], loss=110.7285
	step [83/192], loss=108.6154
	step [84/192], loss=118.6904
	step [85/192], loss=104.4292
	step [86/192], loss=116.9715
	step [87/192], loss=112.5873
	step [88/192], loss=129.8337
	step [89/192], loss=105.0768
	step [90/192], loss=104.6042
	step [91/192], loss=106.2267
	step [92/192], loss=130.5501
	step [93/192], loss=105.8388
	step [94/192], loss=96.9066
	step [95/192], loss=92.4483
	step [96/192], loss=95.2650
	step [97/192], loss=115.0979
	step [98/192], loss=97.5695
	step [99/192], loss=99.2480
	step [100/192], loss=93.1110
	step [101/192], loss=113.0455
	step [102/192], loss=108.2512
	step [103/192], loss=122.3279
	step [104/192], loss=99.4147
	step [105/192], loss=108.2186
	step [106/192], loss=108.4386
	step [107/192], loss=101.4657
	step [108/192], loss=103.0249
	step [109/192], loss=93.9150
	step [110/192], loss=131.3972
	step [111/192], loss=129.4450
	step [112/192], loss=108.8392
	step [113/192], loss=110.8848
	step [114/192], loss=104.7038
	step [115/192], loss=115.4633
	step [116/192], loss=108.5961
	step [117/192], loss=100.1237
	step [118/192], loss=120.4477
	step [119/192], loss=110.3416
	step [120/192], loss=114.4133
	step [121/192], loss=96.7117
	step [122/192], loss=93.9945
	step [123/192], loss=90.3565
	step [124/192], loss=113.2102
	step [125/192], loss=106.3060
	step [126/192], loss=104.1357
	step [127/192], loss=111.0095
	step [128/192], loss=97.1982
	step [129/192], loss=115.9936
	step [130/192], loss=131.4792
	step [131/192], loss=105.7657
	step [132/192], loss=85.3041
	step [133/192], loss=107.5597
	step [134/192], loss=103.3345
	step [135/192], loss=97.4198
	step [136/192], loss=103.7082
	step [137/192], loss=118.3020
	step [138/192], loss=104.5756
	step [139/192], loss=94.8901
	step [140/192], loss=94.1765
	step [141/192], loss=112.9028
	step [142/192], loss=99.6960
	step [143/192], loss=105.0620
	step [144/192], loss=104.4822
	step [145/192], loss=116.7872
	step [146/192], loss=113.5382
	step [147/192], loss=108.7657
	step [148/192], loss=92.7798
	step [149/192], loss=97.4129
	step [150/192], loss=103.9616
	step [151/192], loss=93.8462
	step [152/192], loss=128.8936
	step [153/192], loss=114.5924
	step [154/192], loss=89.1852
	step [155/192], loss=106.8430
	step [156/192], loss=104.2253
	step [157/192], loss=102.7381
	step [158/192], loss=102.0332
	step [159/192], loss=111.0333
	step [160/192], loss=119.1980
	step [161/192], loss=112.5261
	step [162/192], loss=102.0325
	step [163/192], loss=110.3876
	step [164/192], loss=108.9906
	step [165/192], loss=126.0025
	step [166/192], loss=118.9257
	step [167/192], loss=107.9844
	step [168/192], loss=104.6930
	step [169/192], loss=108.2257
	step [170/192], loss=107.2141
	step [171/192], loss=108.5209
	step [172/192], loss=107.1817
	step [173/192], loss=111.0940
	step [174/192], loss=95.0431
	step [175/192], loss=124.0540
	step [176/192], loss=108.9737
	step [177/192], loss=97.9168
	step [178/192], loss=99.1955
	step [179/192], loss=103.9903
	step [180/192], loss=105.8571
	step [181/192], loss=97.7717
	step [182/192], loss=112.4394
	step [183/192], loss=104.3685
	step [184/192], loss=115.2572
	step [185/192], loss=95.3585
	step [186/192], loss=84.4819
	step [187/192], loss=110.6269
	step [188/192], loss=112.0213
	step [189/192], loss=107.5594
	step [190/192], loss=122.7400
	step [191/192], loss=129.2748
	step [192/192], loss=10.3860
	Evaluating
	loss=0.0298, precision=0.4184, recall=0.9286, f1=0.5769
Training epoch 10
	step [1/192], loss=93.6545
	step [2/192], loss=104.8262
	step [3/192], loss=96.6436
	step [4/192], loss=112.5144
	step [5/192], loss=114.4015
	step [6/192], loss=109.3631
	step [7/192], loss=114.3646
	step [8/192], loss=111.1517
	step [9/192], loss=123.4365
	step [10/192], loss=87.9748
	step [11/192], loss=106.8383
	step [12/192], loss=97.3146
	step [13/192], loss=110.6175
	step [14/192], loss=93.7621
	step [15/192], loss=116.0398
	step [16/192], loss=113.3626
	step [17/192], loss=101.5382
	step [18/192], loss=114.6750
	step [19/192], loss=104.6486
	step [20/192], loss=96.0191
	step [21/192], loss=119.8837
	step [22/192], loss=96.5312
	step [23/192], loss=112.4993
	step [24/192], loss=111.0913
	step [25/192], loss=99.6880
	step [26/192], loss=94.3657
	step [27/192], loss=94.0018
	step [28/192], loss=115.9452
	step [29/192], loss=106.3077
	step [30/192], loss=105.3982
	step [31/192], loss=90.5477
	step [32/192], loss=100.2042
	step [33/192], loss=106.8641
	step [34/192], loss=109.3417
	step [35/192], loss=108.7170
	step [36/192], loss=117.5569
	step [37/192], loss=108.6674
	step [38/192], loss=128.3861
	step [39/192], loss=88.4444
	step [40/192], loss=106.8341
	step [41/192], loss=102.1084
	step [42/192], loss=111.9811
	step [43/192], loss=126.4860
	step [44/192], loss=126.3122
	step [45/192], loss=107.9327
	step [46/192], loss=103.6802
	step [47/192], loss=96.7904
	step [48/192], loss=113.5473
	step [49/192], loss=104.9339
	step [50/192], loss=93.7012
	step [51/192], loss=103.3139
	step [52/192], loss=98.1032
	step [53/192], loss=116.7885
	step [54/192], loss=121.2552
	step [55/192], loss=117.4254
	step [56/192], loss=104.4882
	step [57/192], loss=94.6232
	step [58/192], loss=99.7249
	step [59/192], loss=94.2484
	step [60/192], loss=98.6275
	step [61/192], loss=98.5067
	step [62/192], loss=101.7685
	step [63/192], loss=111.6193
	step [64/192], loss=105.9971
	step [65/192], loss=107.4255
	step [66/192], loss=111.9215
	step [67/192], loss=106.9255
	step [68/192], loss=90.7744
	step [69/192], loss=110.5007
	step [70/192], loss=102.9942
	step [71/192], loss=107.7078
	step [72/192], loss=106.0942
	step [73/192], loss=94.2569
	step [74/192], loss=113.4335
	step [75/192], loss=104.0295
	step [76/192], loss=100.6053
	step [77/192], loss=110.8945
	step [78/192], loss=106.1559
	step [79/192], loss=101.4875
	step [80/192], loss=116.5888
	step [81/192], loss=89.7640
	step [82/192], loss=96.0430
	step [83/192], loss=116.1485
	step [84/192], loss=100.8059
	step [85/192], loss=100.8551
	step [86/192], loss=96.3679
	step [87/192], loss=91.3832
	step [88/192], loss=99.4689
	step [89/192], loss=117.6454
	step [90/192], loss=117.1126
	step [91/192], loss=98.5778
	step [92/192], loss=96.7030
	step [93/192], loss=107.7716
	step [94/192], loss=119.4689
	step [95/192], loss=90.4452
	step [96/192], loss=92.0048
	step [97/192], loss=100.3367
	step [98/192], loss=103.4903
	step [99/192], loss=107.4036
	step [100/192], loss=112.4418
	step [101/192], loss=104.1231
	step [102/192], loss=112.3962
	step [103/192], loss=118.3074
	step [104/192], loss=99.5023
	step [105/192], loss=104.3393
	step [106/192], loss=96.3381
	step [107/192], loss=99.5423
	step [108/192], loss=114.8513
	step [109/192], loss=118.2753
	step [110/192], loss=114.7326
	step [111/192], loss=112.7131
	step [112/192], loss=101.7603
	step [113/192], loss=101.3830
	step [114/192], loss=117.0261
	step [115/192], loss=114.8209
	step [116/192], loss=102.3395
	step [117/192], loss=92.3915
	step [118/192], loss=110.8476
	step [119/192], loss=103.4157
	step [120/192], loss=105.8089
	step [121/192], loss=116.2335
	step [122/192], loss=114.9818
	step [123/192], loss=99.1690
	step [124/192], loss=109.3512
	step [125/192], loss=93.7818
	step [126/192], loss=91.0529
	step [127/192], loss=111.7423
	step [128/192], loss=102.9742
	step [129/192], loss=79.7880
	step [130/192], loss=107.3453
	step [131/192], loss=120.6117
	step [132/192], loss=100.6466
	step [133/192], loss=84.6348
	step [134/192], loss=105.7408
	step [135/192], loss=101.6858
	step [136/192], loss=96.8165
	step [137/192], loss=106.4550
	step [138/192], loss=103.6749
	step [139/192], loss=111.6229
	step [140/192], loss=113.8027
	step [141/192], loss=115.7344
	step [142/192], loss=102.8765
	step [143/192], loss=90.2075
	step [144/192], loss=104.5191
	step [145/192], loss=104.0167
	step [146/192], loss=113.0669
	step [147/192], loss=89.7890
	step [148/192], loss=111.4888
	step [149/192], loss=103.2586
	step [150/192], loss=113.0088
	step [151/192], loss=114.9169
	step [152/192], loss=113.8031
	step [153/192], loss=89.0365
	step [154/192], loss=91.6745
	step [155/192], loss=95.9799
	step [156/192], loss=109.4319
	step [157/192], loss=100.2172
	step [158/192], loss=106.6588
	step [159/192], loss=110.6063
	step [160/192], loss=103.0969
	step [161/192], loss=120.7523
	step [162/192], loss=107.3516
	step [163/192], loss=121.0527
	step [164/192], loss=105.0589
	step [165/192], loss=109.6190
	step [166/192], loss=101.4204
	step [167/192], loss=113.7419
	step [168/192], loss=95.5922
	step [169/192], loss=97.3919
	step [170/192], loss=95.2479
	step [171/192], loss=107.7486
	step [172/192], loss=112.5916
	step [173/192], loss=99.4102
	step [174/192], loss=105.9109
	step [175/192], loss=109.2240
	step [176/192], loss=110.1650
	step [177/192], loss=91.9935
	step [178/192], loss=94.4389
	step [179/192], loss=105.6917
	step [180/192], loss=108.0638
	step [181/192], loss=103.1311
	step [182/192], loss=101.5045
	step [183/192], loss=89.9163
	step [184/192], loss=118.9520
	step [185/192], loss=122.2322
	step [186/192], loss=104.5656
	step [187/192], loss=95.1307
	step [188/192], loss=98.2642
	step [189/192], loss=95.9598
	step [190/192], loss=116.3646
	step [191/192], loss=110.9428
	step [192/192], loss=11.2851
	Evaluating
	loss=0.0268, precision=0.4088, recall=0.9329, f1=0.5685
Training epoch 11
	step [1/192], loss=107.6285
	step [2/192], loss=113.4805
	step [3/192], loss=90.6418
	step [4/192], loss=106.1460
	step [5/192], loss=107.9468
	step [6/192], loss=83.5188
	step [7/192], loss=105.7462
	step [8/192], loss=102.4121
	step [9/192], loss=91.2799
	step [10/192], loss=86.7028
	step [11/192], loss=107.0165
	step [12/192], loss=103.1430
	step [13/192], loss=100.2448
	step [14/192], loss=117.8402
	step [15/192], loss=82.4954
	step [16/192], loss=100.1938
	step [17/192], loss=99.3755
	step [18/192], loss=97.0519
	step [19/192], loss=113.6961
	step [20/192], loss=113.7841
	step [21/192], loss=100.2075
	step [22/192], loss=107.2928
	step [23/192], loss=113.2373
	step [24/192], loss=99.8171
	step [25/192], loss=99.2419
	step [26/192], loss=88.3940
	step [27/192], loss=95.9537
	step [28/192], loss=102.8388
	step [29/192], loss=100.1727
	step [30/192], loss=89.7043
	step [31/192], loss=108.5854
	step [32/192], loss=120.4491
	step [33/192], loss=104.8560
	step [34/192], loss=111.1660
	step [35/192], loss=97.1998
	step [36/192], loss=107.9265
	step [37/192], loss=114.4488
	step [38/192], loss=123.6520
	step [39/192], loss=122.2858
	step [40/192], loss=109.6590
	step [41/192], loss=98.3260
	step [42/192], loss=118.7865
	step [43/192], loss=101.2710
	step [44/192], loss=107.7025
	step [45/192], loss=113.4371
	step [46/192], loss=101.3837
	step [47/192], loss=105.7135
	step [48/192], loss=115.1817
	step [49/192], loss=116.9071
	step [50/192], loss=93.3306
	step [51/192], loss=89.1462
	step [52/192], loss=111.9160
	step [53/192], loss=107.3638
	step [54/192], loss=100.0529
	step [55/192], loss=95.4029
	step [56/192], loss=108.9444
	step [57/192], loss=101.3499
	step [58/192], loss=102.2441
	step [59/192], loss=100.9039
	step [60/192], loss=118.4819
	step [61/192], loss=99.6140
	step [62/192], loss=114.3074
	step [63/192], loss=120.3870
	step [64/192], loss=97.6777
	step [65/192], loss=89.4854
	step [66/192], loss=111.6348
	step [67/192], loss=91.2169
	step [68/192], loss=98.4580
	step [69/192], loss=98.5435
	step [70/192], loss=109.2945
	step [71/192], loss=95.4852
	step [72/192], loss=93.7572
	step [73/192], loss=110.3871
	step [74/192], loss=101.2525
	step [75/192], loss=99.1022
	step [76/192], loss=111.9514
	step [77/192], loss=88.6481
	step [78/192], loss=84.4985
	step [79/192], loss=103.0748
	step [80/192], loss=110.9645
	step [81/192], loss=94.3777
	step [82/192], loss=108.1128
	step [83/192], loss=105.6597
	step [84/192], loss=113.2732
	step [85/192], loss=101.6764
	step [86/192], loss=109.5000
	step [87/192], loss=106.7673
	step [88/192], loss=118.9888
	step [89/192], loss=102.8864
	step [90/192], loss=97.7396
	step [91/192], loss=97.2089
	step [92/192], loss=105.5076
	step [93/192], loss=106.0511
	step [94/192], loss=89.3782
	step [95/192], loss=99.6678
	step [96/192], loss=88.3300
	step [97/192], loss=114.9370
	step [98/192], loss=115.9486
	step [99/192], loss=95.0447
	step [100/192], loss=108.8182
	step [101/192], loss=109.3384
	step [102/192], loss=117.8316
	step [103/192], loss=108.9799
	step [104/192], loss=83.7189
	step [105/192], loss=105.6369
	step [106/192], loss=123.5958
	step [107/192], loss=103.8878
	step [108/192], loss=91.5541
	step [109/192], loss=109.9939
	step [110/192], loss=104.6816
	step [111/192], loss=99.7361
	step [112/192], loss=124.6092
	step [113/192], loss=98.0941
	step [114/192], loss=99.7138
	step [115/192], loss=102.4793
	step [116/192], loss=104.4003
	step [117/192], loss=99.3935
	step [118/192], loss=110.0485
	step [119/192], loss=103.3178
	step [120/192], loss=107.1074
	step [121/192], loss=116.3236
	step [122/192], loss=103.0472
	step [123/192], loss=105.2757
	step [124/192], loss=111.4030
	step [125/192], loss=114.8182
	step [126/192], loss=110.8405
	step [127/192], loss=107.2718
	step [128/192], loss=96.6084
	step [129/192], loss=96.9312
	step [130/192], loss=88.7320
	step [131/192], loss=94.8449
	step [132/192], loss=107.6288
	step [133/192], loss=103.2984
	step [134/192], loss=91.9231
	step [135/192], loss=107.4681
	step [136/192], loss=93.4234
	step [137/192], loss=104.7732
	step [138/192], loss=107.3366
	step [139/192], loss=100.9350
	step [140/192], loss=90.4465
	step [141/192], loss=95.9131
	step [142/192], loss=107.3493
	step [143/192], loss=89.6475
	step [144/192], loss=93.5726
	step [145/192], loss=100.8223
	step [146/192], loss=107.6919
	step [147/192], loss=91.4263
	step [148/192], loss=92.1926
	step [149/192], loss=96.4976
	step [150/192], loss=100.3219
	step [151/192], loss=99.9594
	step [152/192], loss=83.2555
	step [153/192], loss=134.0416
	step [154/192], loss=95.9232
	step [155/192], loss=95.3616
	step [156/192], loss=99.7848
	step [157/192], loss=107.8025
	step [158/192], loss=98.6928
	step [159/192], loss=87.0136
	step [160/192], loss=109.1791
	step [161/192], loss=99.9750
	step [162/192], loss=107.8465
	step [163/192], loss=93.9297
	step [164/192], loss=98.1509
	step [165/192], loss=97.0798
	step [166/192], loss=90.9258
	step [167/192], loss=116.2430
	step [168/192], loss=98.7073
	step [169/192], loss=119.3163
	step [170/192], loss=110.9875
	step [171/192], loss=103.1337
	step [172/192], loss=112.6172
	step [173/192], loss=103.8900
	step [174/192], loss=109.7720
	step [175/192], loss=99.0891
	step [176/192], loss=98.5489
	step [177/192], loss=102.3712
	step [178/192], loss=105.7580
	step [179/192], loss=101.2691
	step [180/192], loss=99.2147
	step [181/192], loss=108.9519
	step [182/192], loss=104.8984
	step [183/192], loss=100.2059
	step [184/192], loss=112.9735
	step [185/192], loss=94.7269
	step [186/192], loss=107.5229
	step [187/192], loss=113.2415
	step [188/192], loss=120.5216
	step [189/192], loss=104.2526
	step [190/192], loss=88.1940
	step [191/192], loss=112.9742
	step [192/192], loss=10.0856
	Evaluating
	loss=0.0230, precision=0.3986, recall=0.9236, f1=0.5569
Training epoch 12
	step [1/192], loss=99.9990
	step [2/192], loss=101.5670
	step [3/192], loss=110.6373
	step [4/192], loss=108.4350
	step [5/192], loss=91.8227
	step [6/192], loss=100.3900
	step [7/192], loss=94.8066
	step [8/192], loss=98.2604
	step [9/192], loss=102.3038
	step [10/192], loss=94.5645
	step [11/192], loss=97.0246
	step [12/192], loss=92.1479
	step [13/192], loss=98.0480
	step [14/192], loss=115.1742
	step [15/192], loss=108.5483
	step [16/192], loss=97.3523
	step [17/192], loss=106.2856
	step [18/192], loss=114.9302
	step [19/192], loss=109.8147
	step [20/192], loss=92.3862
	step [21/192], loss=107.0430
	step [22/192], loss=101.2614
	step [23/192], loss=117.3040
	step [24/192], loss=88.0275
	step [25/192], loss=110.7640
	step [26/192], loss=98.4706
	step [27/192], loss=107.0215
	step [28/192], loss=103.9904
	step [29/192], loss=88.8330
	step [30/192], loss=99.8799
	step [31/192], loss=92.7350
	step [32/192], loss=100.4447
	step [33/192], loss=107.1003
	step [34/192], loss=100.6779
	step [35/192], loss=104.9208
	step [36/192], loss=97.6315
	step [37/192], loss=99.6171
	step [38/192], loss=105.6322
	step [39/192], loss=99.5255
	step [40/192], loss=98.7486
	step [41/192], loss=80.2135
	step [42/192], loss=94.8456
	step [43/192], loss=106.1548
	step [44/192], loss=106.9662
	step [45/192], loss=104.9649
	step [46/192], loss=89.5166
	step [47/192], loss=109.1210
	step [48/192], loss=98.8980
	step [49/192], loss=107.1590
	step [50/192], loss=101.4416
	step [51/192], loss=102.1928
	step [52/192], loss=122.5460
	step [53/192], loss=102.8635
	step [54/192], loss=99.1152
	step [55/192], loss=110.9129
	step [56/192], loss=91.6200
	step [57/192], loss=100.9094
	step [58/192], loss=90.3116
	step [59/192], loss=96.8919
	step [60/192], loss=84.4574
	step [61/192], loss=111.0922
	step [62/192], loss=104.2213
	step [63/192], loss=97.8718
	step [64/192], loss=103.0030
	step [65/192], loss=107.5009
	step [66/192], loss=113.8135
	step [67/192], loss=78.8659
	step [68/192], loss=105.3739
	step [69/192], loss=92.5337
	step [70/192], loss=111.7199
	step [71/192], loss=119.3945
	step [72/192], loss=98.6631
	step [73/192], loss=113.7716
	step [74/192], loss=106.8050
	step [75/192], loss=110.2043
	step [76/192], loss=98.0570
	step [77/192], loss=97.3201
	step [78/192], loss=93.4160
	step [79/192], loss=88.1804
	step [80/192], loss=102.8499
	step [81/192], loss=111.1198
	step [82/192], loss=103.5535
	step [83/192], loss=100.4153
	step [84/192], loss=110.7529
	step [85/192], loss=107.7836
	step [86/192], loss=104.6555
	step [87/192], loss=107.1277
	step [88/192], loss=107.1933
	step [89/192], loss=112.3916
	step [90/192], loss=90.2010
	step [91/192], loss=112.0445
	step [92/192], loss=94.2016
	step [93/192], loss=89.1455
	step [94/192], loss=96.1039
	step [95/192], loss=92.4987
	step [96/192], loss=94.6777
	step [97/192], loss=102.5688
	step [98/192], loss=109.8021
	step [99/192], loss=92.4012
	step [100/192], loss=98.1264
	step [101/192], loss=88.0930
	step [102/192], loss=97.9513
	step [103/192], loss=92.8669
	step [104/192], loss=105.1447
	step [105/192], loss=117.1585
	step [106/192], loss=99.4101
	step [107/192], loss=100.4458
	step [108/192], loss=105.3668
	step [109/192], loss=106.0927
	step [110/192], loss=95.4845
	step [111/192], loss=96.6319
	step [112/192], loss=118.5196
	step [113/192], loss=105.9432
	step [114/192], loss=97.4349
	step [115/192], loss=95.7839
	step [116/192], loss=94.8493
	step [117/192], loss=82.6437
	step [118/192], loss=104.8301
	step [119/192], loss=93.1132
	step [120/192], loss=96.2722
	step [121/192], loss=105.8022
	step [122/192], loss=99.5483
	step [123/192], loss=87.8209
	step [124/192], loss=107.6017
	step [125/192], loss=97.0448
	step [126/192], loss=97.6562
	step [127/192], loss=101.2722
	step [128/192], loss=102.8696
	step [129/192], loss=95.1870
	step [130/192], loss=99.4877
	step [131/192], loss=95.9193
	step [132/192], loss=100.6967
	step [133/192], loss=117.9655
	step [134/192], loss=125.9708
	step [135/192], loss=92.1711
	step [136/192], loss=97.1004
	step [137/192], loss=94.9617
	step [138/192], loss=107.2523
	step [139/192], loss=102.0791
	step [140/192], loss=99.0559
	step [141/192], loss=114.2036
	step [142/192], loss=101.9309
	step [143/192], loss=95.8605
	step [144/192], loss=110.8511
	step [145/192], loss=96.5448
	step [146/192], loss=103.9219
	step [147/192], loss=100.5710
	step [148/192], loss=111.1852
	step [149/192], loss=103.6149
	step [150/192], loss=108.9569
	step [151/192], loss=120.4861
	step [152/192], loss=92.2450
	step [153/192], loss=98.3737
	step [154/192], loss=106.9949
	step [155/192], loss=84.7410
	step [156/192], loss=105.7260
	step [157/192], loss=104.1165
	step [158/192], loss=86.0413
	step [159/192], loss=105.3497
	step [160/192], loss=132.3446
	step [161/192], loss=109.6007
	step [162/192], loss=104.3607
	step [163/192], loss=89.8824
	step [164/192], loss=93.8938
	step [165/192], loss=95.8751
	step [166/192], loss=104.0058
	step [167/192], loss=102.6713
	step [168/192], loss=92.9211
	step [169/192], loss=99.3302
	step [170/192], loss=102.5965
	step [171/192], loss=101.7870
	step [172/192], loss=97.7454
	step [173/192], loss=95.9968
	step [174/192], loss=110.2549
	step [175/192], loss=116.8019
	step [176/192], loss=101.1149
	step [177/192], loss=103.0216
	step [178/192], loss=95.0906
	step [179/192], loss=99.7266
	step [180/192], loss=104.3915
	step [181/192], loss=97.1347
	step [182/192], loss=106.8292
	step [183/192], loss=101.8620
	step [184/192], loss=92.4289
	step [185/192], loss=116.3857
	step [186/192], loss=104.6623
	step [187/192], loss=93.5915
	step [188/192], loss=99.7887
	step [189/192], loss=90.2275
	step [190/192], loss=101.3670
	step [191/192], loss=99.7634
	step [192/192], loss=17.1372
	Evaluating
	loss=0.0194, precision=0.4348, recall=0.9100, f1=0.5884
Training epoch 13
	step [1/192], loss=97.1961
	step [2/192], loss=102.2820
	step [3/192], loss=104.2118
	step [4/192], loss=107.7489
	step [5/192], loss=93.8344
	step [6/192], loss=94.8660
	step [7/192], loss=96.8188
	step [8/192], loss=97.9399
	step [9/192], loss=112.0413
	step [10/192], loss=100.2238
	step [11/192], loss=89.2813
	step [12/192], loss=98.8325
	step [13/192], loss=113.1890
	step [14/192], loss=96.6673
	step [15/192], loss=107.3074
	step [16/192], loss=102.5396
	step [17/192], loss=116.4176
	step [18/192], loss=92.3342
	step [19/192], loss=98.8206
	step [20/192], loss=105.3854
	step [21/192], loss=98.5327
	step [22/192], loss=99.7634
	step [23/192], loss=105.5786
	step [24/192], loss=107.8107
	step [25/192], loss=95.1669
	step [26/192], loss=89.5970
	step [27/192], loss=98.6680
	step [28/192], loss=115.8314
	step [29/192], loss=91.9672
	step [30/192], loss=106.9625
	step [31/192], loss=104.4507
	step [32/192], loss=90.6868
	step [33/192], loss=102.6541
	step [34/192], loss=87.4813
	step [35/192], loss=101.1598
	step [36/192], loss=97.0523
	step [37/192], loss=125.1802
	step [38/192], loss=99.7075
	step [39/192], loss=102.1191
	step [40/192], loss=108.4844
	step [41/192], loss=98.8305
	step [42/192], loss=120.8694
	step [43/192], loss=107.1490
	step [44/192], loss=106.5567
	step [45/192], loss=113.8942
	step [46/192], loss=109.7437
	step [47/192], loss=92.8080
	step [48/192], loss=102.6364
	step [49/192], loss=107.2968
	step [50/192], loss=91.5583
	step [51/192], loss=84.3520
	step [52/192], loss=108.1091
	step [53/192], loss=102.9177
	step [54/192], loss=93.1599
	step [55/192], loss=97.2548
	step [56/192], loss=104.0467
	step [57/192], loss=102.4302
	step [58/192], loss=90.0807
	step [59/192], loss=102.5398
	step [60/192], loss=104.6733
	step [61/192], loss=86.1807
	step [62/192], loss=107.6564
	step [63/192], loss=99.2660
	step [64/192], loss=95.3576
	step [65/192], loss=111.0162
	step [66/192], loss=84.6260
	step [67/192], loss=107.9225
	step [68/192], loss=114.1716
	step [69/192], loss=109.2157
	step [70/192], loss=88.6900
	step [71/192], loss=100.1545
	step [72/192], loss=112.7541
	step [73/192], loss=104.0368
	step [74/192], loss=93.1756
	step [75/192], loss=93.6516
	step [76/192], loss=89.5637
	step [77/192], loss=113.9505
	step [78/192], loss=107.6696
	step [79/192], loss=91.7135
	step [80/192], loss=87.1761
	step [81/192], loss=83.7328
	step [82/192], loss=106.1254
	step [83/192], loss=95.1663
	step [84/192], loss=97.1146
	step [85/192], loss=105.9659
	step [86/192], loss=101.7828
	step [87/192], loss=87.9840
	step [88/192], loss=103.4799
	step [89/192], loss=103.6715
	step [90/192], loss=88.9975
	step [91/192], loss=98.6006
	step [92/192], loss=89.8976
	step [93/192], loss=91.5991
	step [94/192], loss=100.9548
	step [95/192], loss=96.2198
	step [96/192], loss=109.2175
	step [97/192], loss=97.8131
	step [98/192], loss=94.4462
	step [99/192], loss=92.4642
	step [100/192], loss=115.8884
	step [101/192], loss=95.8902
	step [102/192], loss=103.5948
	step [103/192], loss=95.7947
	step [104/192], loss=82.5906
	step [105/192], loss=111.2958
	step [106/192], loss=85.3748
	step [107/192], loss=94.0269
	step [108/192], loss=103.2125
	step [109/192], loss=108.7273
	step [110/192], loss=105.1636
	step [111/192], loss=99.6066
	step [112/192], loss=102.0102
	step [113/192], loss=109.4377
	step [114/192], loss=89.0915
	step [115/192], loss=86.7312
	step [116/192], loss=107.2406
	step [117/192], loss=79.6142
	step [118/192], loss=99.4690
	step [119/192], loss=88.5036
	step [120/192], loss=104.2783
	step [121/192], loss=99.1136
	step [122/192], loss=92.3138
	step [123/192], loss=114.5290
	step [124/192], loss=92.5221
	step [125/192], loss=105.6232
	step [126/192], loss=103.0925
	step [127/192], loss=90.7943
	step [128/192], loss=100.0398
	step [129/192], loss=99.3020
	step [130/192], loss=108.2827
	step [131/192], loss=97.1721
	step [132/192], loss=87.3231
	step [133/192], loss=86.7320
	step [134/192], loss=86.9627
	step [135/192], loss=108.5774
	step [136/192], loss=93.1355
	step [137/192], loss=95.7370
	step [138/192], loss=103.0747
	step [139/192], loss=93.5543
	step [140/192], loss=107.6102
	step [141/192], loss=111.5314
	step [142/192], loss=93.6188
	step [143/192], loss=88.5516
	step [144/192], loss=98.8673
	step [145/192], loss=105.6791
	step [146/192], loss=109.8177
	step [147/192], loss=119.4286
	step [148/192], loss=95.9912
	step [149/192], loss=109.5793
	step [150/192], loss=99.4960
	step [151/192], loss=110.5610
	step [152/192], loss=107.5710
	step [153/192], loss=81.6979
	step [154/192], loss=98.8720
	step [155/192], loss=93.5853
	step [156/192], loss=91.4440
	step [157/192], loss=111.6077
	step [158/192], loss=97.5523
	step [159/192], loss=103.2532
	step [160/192], loss=114.2198
	step [161/192], loss=98.2198
	step [162/192], loss=103.3408
	step [163/192], loss=95.7241
	step [164/192], loss=99.4534
	step [165/192], loss=100.0427
	step [166/192], loss=88.1011
	step [167/192], loss=120.7636
	step [168/192], loss=103.6831
	step [169/192], loss=102.2442
	step [170/192], loss=89.8676
	step [171/192], loss=97.5062
	step [172/192], loss=107.4889
	step [173/192], loss=97.7049
	step [174/192], loss=97.7946
	step [175/192], loss=91.4844
	step [176/192], loss=113.6651
	step [177/192], loss=110.5828
	step [178/192], loss=92.9404
	step [179/192], loss=93.7620
	step [180/192], loss=87.4045
	step [181/192], loss=99.7154
	step [182/192], loss=94.7528
	step [183/192], loss=96.8754
	step [184/192], loss=93.4540
	step [185/192], loss=117.3961
	step [186/192], loss=98.0265
	step [187/192], loss=110.5076
	step [188/192], loss=92.3421
	step [189/192], loss=103.5819
	step [190/192], loss=91.1667
	step [191/192], loss=92.6016
	step [192/192], loss=19.7349
	Evaluating
	loss=0.0195, precision=0.3972, recall=0.9156, f1=0.5541
Training epoch 14
	step [1/192], loss=110.2088
	step [2/192], loss=91.2642
	step [3/192], loss=102.2580
	step [4/192], loss=102.0162
	step [5/192], loss=95.2662
	step [6/192], loss=100.3848
	step [7/192], loss=112.2013
	step [8/192], loss=80.0453
	step [9/192], loss=102.4690
	step [10/192], loss=95.2598
	step [11/192], loss=97.7062
	step [12/192], loss=88.2949
	step [13/192], loss=97.4412
	step [14/192], loss=117.8717
	step [15/192], loss=109.0232
	step [16/192], loss=89.7342
	step [17/192], loss=91.8308
	step [18/192], loss=96.5813
	step [19/192], loss=103.3546
	step [20/192], loss=93.8041
	step [21/192], loss=94.5148
	step [22/192], loss=92.5134
	step [23/192], loss=91.7782
	step [24/192], loss=89.0305
	step [25/192], loss=83.9750
	step [26/192], loss=108.8919
	step [27/192], loss=93.9247
	step [28/192], loss=105.8371
	step [29/192], loss=113.6388
	step [30/192], loss=76.5399
	step [31/192], loss=107.7984
	step [32/192], loss=110.5095
	step [33/192], loss=98.1147
	step [34/192], loss=83.2940
	step [35/192], loss=102.1910
	step [36/192], loss=105.6849
	step [37/192], loss=100.7364
	step [38/192], loss=91.4758
	step [39/192], loss=106.4499
	step [40/192], loss=88.7448
	step [41/192], loss=105.1266
	step [42/192], loss=112.3734
	step [43/192], loss=99.6235
	step [44/192], loss=93.5540
	step [45/192], loss=105.3775
	step [46/192], loss=91.9459
	step [47/192], loss=100.6899
	step [48/192], loss=86.9480
	step [49/192], loss=122.7408
	step [50/192], loss=83.8610
	step [51/192], loss=100.6233
	step [52/192], loss=82.3725
	step [53/192], loss=94.0840
	step [54/192], loss=105.5123
	step [55/192], loss=94.5014
	step [56/192], loss=103.6828
	step [57/192], loss=91.6674
	step [58/192], loss=83.2789
	step [59/192], loss=94.5511
	step [60/192], loss=108.5451
	step [61/192], loss=100.5553
	step [62/192], loss=122.9706
	step [63/192], loss=99.8740
	step [64/192], loss=91.9369
	step [65/192], loss=95.8509
	step [66/192], loss=90.1716
	step [67/192], loss=116.5205
	step [68/192], loss=91.9854
	step [69/192], loss=108.9671
	step [70/192], loss=100.3234
	step [71/192], loss=94.0021
	step [72/192], loss=103.3939
	step [73/192], loss=113.8567
	step [74/192], loss=104.2434
	step [75/192], loss=79.3680
	step [76/192], loss=100.0008
	step [77/192], loss=108.1270
	step [78/192], loss=95.0380
	step [79/192], loss=101.9116
	step [80/192], loss=84.0159
	step [81/192], loss=92.7771
	step [82/192], loss=101.7480
	step [83/192], loss=93.8519
	step [84/192], loss=103.6842
	step [85/192], loss=103.1687
	step [86/192], loss=104.3510
	step [87/192], loss=89.9699
	step [88/192], loss=103.8323
	step [89/192], loss=105.0797
	step [90/192], loss=108.1682
	step [91/192], loss=102.2760
	step [92/192], loss=104.1262
	step [93/192], loss=111.7039
	step [94/192], loss=95.0680
	step [95/192], loss=103.3753
	step [96/192], loss=108.1083
	step [97/192], loss=100.2122
	step [98/192], loss=77.2719
	step [99/192], loss=106.7184
	step [100/192], loss=97.2557
	step [101/192], loss=119.2797
	step [102/192], loss=101.8828
	step [103/192], loss=92.8497
	step [104/192], loss=98.5893
	step [105/192], loss=104.4801
	step [106/192], loss=107.5231
	step [107/192], loss=96.5668
	step [108/192], loss=102.5094
	step [109/192], loss=106.7562
	step [110/192], loss=85.4909
	step [111/192], loss=93.1899
	step [112/192], loss=96.5709
	step [113/192], loss=104.6515
	step [114/192], loss=103.9284
	step [115/192], loss=91.2877
	step [116/192], loss=81.5244
	step [117/192], loss=97.1323
	step [118/192], loss=94.9792
	step [119/192], loss=96.8724
	step [120/192], loss=102.2694
	step [121/192], loss=97.1517
	step [122/192], loss=89.8677
	step [123/192], loss=102.1300
	step [124/192], loss=100.8666
	step [125/192], loss=101.8729
	step [126/192], loss=100.8869
	step [127/192], loss=96.6224
	step [128/192], loss=85.0989
	step [129/192], loss=104.4111
	step [130/192], loss=96.5925
	step [131/192], loss=96.5141
	step [132/192], loss=85.9738
	step [133/192], loss=89.5562
	step [134/192], loss=92.3297
	step [135/192], loss=85.8289
	step [136/192], loss=92.8912
	step [137/192], loss=100.7263
	step [138/192], loss=90.1380
	step [139/192], loss=93.0096
	step [140/192], loss=77.8483
	step [141/192], loss=103.7626
	step [142/192], loss=89.7911
	step [143/192], loss=92.4607
	step [144/192], loss=87.4973
	step [145/192], loss=100.9376
	step [146/192], loss=108.3697
	step [147/192], loss=88.6015
	step [148/192], loss=85.5463
	step [149/192], loss=87.1877
	step [150/192], loss=97.7070
	step [151/192], loss=97.2939
	step [152/192], loss=101.8040
	step [153/192], loss=90.3256
	step [154/192], loss=104.5135
	step [155/192], loss=99.5612
	step [156/192], loss=104.8882
	step [157/192], loss=99.6333
	step [158/192], loss=100.6421
	step [159/192], loss=105.5246
	step [160/192], loss=100.4498
	step [161/192], loss=102.3720
	step [162/192], loss=105.7583
	step [163/192], loss=93.2325
	step [164/192], loss=110.4135
	step [165/192], loss=92.8387
	step [166/192], loss=115.8009
	step [167/192], loss=96.7635
	step [168/192], loss=78.8627
	step [169/192], loss=109.3598
	step [170/192], loss=99.9289
	step [171/192], loss=93.4887
	step [172/192], loss=103.0261
	step [173/192], loss=101.4763
	step [174/192], loss=87.0918
	step [175/192], loss=97.0267
	step [176/192], loss=74.7955
	step [177/192], loss=95.3780
	step [178/192], loss=101.4223
	step [179/192], loss=107.8779
	step [180/192], loss=103.4677
	step [181/192], loss=95.2624
	step [182/192], loss=100.6579
	step [183/192], loss=96.1438
	step [184/192], loss=95.0626
	step [185/192], loss=96.9569
	step [186/192], loss=101.6108
	step [187/192], loss=85.2920
	step [188/192], loss=99.1685
	step [189/192], loss=96.4288
	step [190/192], loss=108.0950
	step [191/192], loss=101.2178
	step [192/192], loss=17.3135
	Evaluating
	loss=0.0194, precision=0.3176, recall=0.8974, f1=0.4692
Training epoch 15
	step [1/192], loss=99.1434
	step [2/192], loss=102.2060
	step [3/192], loss=98.9561
	step [4/192], loss=88.8237
	step [5/192], loss=117.6945
	step [6/192], loss=86.9918
	step [7/192], loss=103.2664
	step [8/192], loss=101.5320
	step [9/192], loss=82.3720
	step [10/192], loss=88.4648
	step [11/192], loss=99.1451
	step [12/192], loss=90.5679
	step [13/192], loss=97.8798
	step [14/192], loss=91.6717
	step [15/192], loss=98.2479
	step [16/192], loss=92.1801
	step [17/192], loss=74.8245
	step [18/192], loss=103.6768
	step [19/192], loss=95.2492
	step [20/192], loss=112.3641
	step [21/192], loss=86.7692
	step [22/192], loss=91.5116
	step [23/192], loss=101.0165
	step [24/192], loss=92.4672
	step [25/192], loss=102.3983
	step [26/192], loss=94.6057
	step [27/192], loss=105.9871
	step [28/192], loss=94.1134
	step [29/192], loss=94.9109
	step [30/192], loss=95.4161
	step [31/192], loss=94.6281
	step [32/192], loss=102.6619
	step [33/192], loss=92.3294
	step [34/192], loss=88.7788
	step [35/192], loss=110.4572
	step [36/192], loss=89.7382
	step [37/192], loss=98.1668
	step [38/192], loss=115.7427
	step [39/192], loss=84.9682
	step [40/192], loss=95.8105
	step [41/192], loss=101.2560
	step [42/192], loss=90.5192
	step [43/192], loss=90.5971
	step [44/192], loss=112.6163
	step [45/192], loss=106.2979
	step [46/192], loss=107.1902
	step [47/192], loss=95.2934
	step [48/192], loss=96.1717
	step [49/192], loss=88.9015
	step [50/192], loss=97.2311
	step [51/192], loss=98.0730
	step [52/192], loss=106.9813
	step [53/192], loss=92.9929
	step [54/192], loss=94.9019
	step [55/192], loss=87.7373
	step [56/192], loss=76.5831
	step [57/192], loss=98.3605
	step [58/192], loss=99.4605
	step [59/192], loss=108.8502
	step [60/192], loss=92.4672
	step [61/192], loss=98.1491
	step [62/192], loss=92.7750
	step [63/192], loss=89.5688
	step [64/192], loss=109.0830
	step [65/192], loss=86.7022
	step [66/192], loss=98.6866
	step [67/192], loss=89.0935
	step [68/192], loss=105.5841
	step [69/192], loss=98.8338
	step [70/192], loss=99.0417
	step [71/192], loss=100.5883
	step [72/192], loss=103.9807
	step [73/192], loss=105.5681
	step [74/192], loss=99.5169
	step [75/192], loss=93.4393
	step [76/192], loss=87.3995
	step [77/192], loss=100.3120
	step [78/192], loss=96.9476
	step [79/192], loss=94.5897
	step [80/192], loss=85.6510
	step [81/192], loss=83.5987
	step [82/192], loss=114.0607
	step [83/192], loss=92.7626
	step [84/192], loss=105.0828
	step [85/192], loss=72.0076
	step [86/192], loss=88.4780
	step [87/192], loss=97.5423
	step [88/192], loss=80.4816
	step [89/192], loss=101.5434
	step [90/192], loss=96.8142
	step [91/192], loss=82.2860
	step [92/192], loss=86.5286
	step [93/192], loss=92.3478
	step [94/192], loss=81.7104
	step [95/192], loss=89.0172
	step [96/192], loss=108.6083
	step [97/192], loss=101.6687
	step [98/192], loss=105.0107
	step [99/192], loss=113.3247
	step [100/192], loss=98.5036
	step [101/192], loss=88.1970
	step [102/192], loss=100.6253
	step [103/192], loss=99.0053
	step [104/192], loss=110.0801
	step [105/192], loss=100.8026
	step [106/192], loss=101.8449
	step [107/192], loss=99.8695
	step [108/192], loss=90.9649
	step [109/192], loss=102.2728
	step [110/192], loss=99.6887
	step [111/192], loss=106.3494
	step [112/192], loss=102.3718
	step [113/192], loss=96.6495
	step [114/192], loss=91.3566
	step [115/192], loss=97.7289
	step [116/192], loss=97.3966
	step [117/192], loss=89.0362
	step [118/192], loss=90.2173
	step [119/192], loss=84.7126
	step [120/192], loss=100.6643
	step [121/192], loss=108.9423
	step [122/192], loss=105.9801
	step [123/192], loss=105.8871
	step [124/192], loss=110.2113
	step [125/192], loss=98.2081
	step [126/192], loss=88.3781
	step [127/192], loss=95.4850
	step [128/192], loss=105.5041
	step [129/192], loss=99.6478
	step [130/192], loss=83.2059
	step [131/192], loss=91.8999
	step [132/192], loss=89.7041
	step [133/192], loss=111.9926
	step [134/192], loss=81.2074
	step [135/192], loss=112.1979
	step [136/192], loss=91.3143
	step [137/192], loss=100.1283
	step [138/192], loss=94.3869
	step [139/192], loss=77.3477
	step [140/192], loss=92.8318
	step [141/192], loss=100.8511
	step [142/192], loss=96.6479
	step [143/192], loss=85.8992
	step [144/192], loss=86.0528
	step [145/192], loss=99.6583
	step [146/192], loss=111.1823
	step [147/192], loss=93.8338
	step [148/192], loss=110.2077
	step [149/192], loss=108.4831
	step [150/192], loss=78.6019
	step [151/192], loss=93.2642
	step [152/192], loss=93.2987
	step [153/192], loss=89.5472
	step [154/192], loss=101.5582
	step [155/192], loss=98.1954
	step [156/192], loss=90.3926
	step [157/192], loss=90.9495
	step [158/192], loss=97.6784
	step [159/192], loss=87.2536
	step [160/192], loss=79.7376
	step [161/192], loss=102.8113
	step [162/192], loss=87.0536
	step [163/192], loss=98.5521
	step [164/192], loss=111.7632
	step [165/192], loss=101.7480
	step [166/192], loss=94.5462
	step [167/192], loss=97.4336
	step [168/192], loss=102.8165
	step [169/192], loss=92.0067
	step [170/192], loss=90.2656
	step [171/192], loss=97.0458
	step [172/192], loss=111.9170
	step [173/192], loss=92.0545
	step [174/192], loss=91.0244
	step [175/192], loss=114.0356
	step [176/192], loss=80.2868
	step [177/192], loss=96.3608
	step [178/192], loss=87.1871
	step [179/192], loss=100.3260
	step [180/192], loss=88.0524
	step [181/192], loss=117.4671
	step [182/192], loss=91.7865
	step [183/192], loss=98.9390
	step [184/192], loss=93.9649
	step [185/192], loss=103.9276
	step [186/192], loss=86.6988
	step [187/192], loss=94.1535
	step [188/192], loss=104.4801
	step [189/192], loss=104.9790
	step [190/192], loss=106.5398
	step [191/192], loss=109.7152
	step [192/192], loss=12.8500
	Evaluating
	loss=0.0200, precision=0.3166, recall=0.9075, f1=0.4694
Training epoch 16
	step [1/192], loss=115.8900
	step [2/192], loss=85.1771
	step [3/192], loss=88.1237
	step [4/192], loss=99.4255
	step [5/192], loss=95.8090
	step [6/192], loss=98.5234
	step [7/192], loss=97.6720
	step [8/192], loss=106.7368
	step [9/192], loss=103.2618
	step [10/192], loss=85.1644
	step [11/192], loss=107.9138
	step [12/192], loss=102.4215
	step [13/192], loss=101.4454
	step [14/192], loss=95.6527
	step [15/192], loss=72.3508
	step [16/192], loss=103.3810
	step [17/192], loss=89.5369
	step [18/192], loss=106.4644
	step [19/192], loss=95.7768
	step [20/192], loss=90.3466
	step [21/192], loss=105.6311
	step [22/192], loss=106.3735
	step [23/192], loss=100.0356
	step [24/192], loss=79.5729
	step [25/192], loss=82.6881
	step [26/192], loss=90.0813
	step [27/192], loss=85.3587
	step [28/192], loss=89.1044
	step [29/192], loss=105.8830
	step [30/192], loss=102.9368
	step [31/192], loss=91.3339
	step [32/192], loss=92.0117
	step [33/192], loss=112.9195
	step [34/192], loss=113.2160
	step [35/192], loss=86.4858
	step [36/192], loss=85.9455
	step [37/192], loss=79.2401
	step [38/192], loss=86.4639
	step [39/192], loss=92.1058
	step [40/192], loss=101.4862
	step [41/192], loss=103.3094
	step [42/192], loss=83.4521
	step [43/192], loss=85.4315
	step [44/192], loss=84.4825
	step [45/192], loss=107.2941
	step [46/192], loss=96.2286
	step [47/192], loss=103.6790
	step [48/192], loss=100.3772
	step [49/192], loss=98.2934
	step [50/192], loss=102.2455
	step [51/192], loss=97.4464
	step [52/192], loss=96.4783
	step [53/192], loss=101.3806
	step [54/192], loss=92.6279
	step [55/192], loss=85.2699
	step [56/192], loss=96.4491
	step [57/192], loss=88.9752
	step [58/192], loss=90.5829
	step [59/192], loss=81.8122
	step [60/192], loss=80.7997
	step [61/192], loss=96.4095
	step [62/192], loss=85.1921
	step [63/192], loss=110.9395
	step [64/192], loss=99.5880
	step [65/192], loss=99.2827
	step [66/192], loss=98.4869
	step [67/192], loss=96.3429
	step [68/192], loss=87.9267
	step [69/192], loss=106.0632
	step [70/192], loss=96.9414
	step [71/192], loss=82.8646
	step [72/192], loss=95.4930
	step [73/192], loss=96.8104
	step [74/192], loss=88.5530
	step [75/192], loss=93.2253
	step [76/192], loss=104.2749
	step [77/192], loss=99.7404
	step [78/192], loss=113.0275
	step [79/192], loss=91.1704
	step [80/192], loss=94.7768
	step [81/192], loss=86.6627
	step [82/192], loss=96.1192
	step [83/192], loss=85.9160
	step [84/192], loss=97.0793
	step [85/192], loss=111.4520
	step [86/192], loss=95.7705
	step [87/192], loss=108.7818
	step [88/192], loss=87.7977
	step [89/192], loss=90.9392
	step [90/192], loss=97.7452
	step [91/192], loss=86.9143
	step [92/192], loss=107.3950
	step [93/192], loss=89.5505
	step [94/192], loss=94.3424
	step [95/192], loss=112.0324
	step [96/192], loss=108.5249
	step [97/192], loss=97.1451
	step [98/192], loss=98.7234
	step [99/192], loss=84.8381
	step [100/192], loss=105.4050
	step [101/192], loss=88.3997
	step [102/192], loss=109.2576
	step [103/192], loss=92.3862
	step [104/192], loss=89.5261
	step [105/192], loss=97.8187
	step [106/192], loss=107.2150
	step [107/192], loss=96.6741
	step [108/192], loss=94.6681
	step [109/192], loss=84.4795
	step [110/192], loss=101.6754
	step [111/192], loss=99.5589
	step [112/192], loss=98.1116
	step [113/192], loss=90.5087
	step [114/192], loss=102.3006
	step [115/192], loss=90.4530
	step [116/192], loss=97.1794
	step [117/192], loss=94.9943
	step [118/192], loss=94.7954
	step [119/192], loss=83.6738
	step [120/192], loss=102.2197
	step [121/192], loss=93.4135
	step [122/192], loss=92.2156
	step [123/192], loss=94.3133
	step [124/192], loss=106.7913
	step [125/192], loss=70.3546
	step [126/192], loss=91.2829
	step [127/192], loss=96.3540
	step [128/192], loss=94.5981
	step [129/192], loss=87.9513
	step [130/192], loss=104.2879
	step [131/192], loss=80.4761
	step [132/192], loss=109.7045
	step [133/192], loss=79.6188
	step [134/192], loss=101.2740
	step [135/192], loss=94.5844
	step [136/192], loss=91.5574
	step [137/192], loss=94.2748
	step [138/192], loss=93.6470
	step [139/192], loss=97.8527
	step [140/192], loss=108.0476
	step [141/192], loss=105.8799
	step [142/192], loss=104.8426
	step [143/192], loss=88.2023
	step [144/192], loss=94.1789
	step [145/192], loss=99.6665
	step [146/192], loss=90.9948
	step [147/192], loss=109.0897
	step [148/192], loss=83.2508
	step [149/192], loss=84.7475
	step [150/192], loss=94.6046
	step [151/192], loss=109.1843
	step [152/192], loss=95.3229
	step [153/192], loss=94.2833
	step [154/192], loss=99.1810
	step [155/192], loss=92.3703
	step [156/192], loss=82.1545
	step [157/192], loss=84.4585
	step [158/192], loss=92.0718
	step [159/192], loss=92.3573
	step [160/192], loss=85.6552
	step [161/192], loss=90.3407
	step [162/192], loss=71.3355
	step [163/192], loss=92.0181
	step [164/192], loss=111.3115
	step [165/192], loss=89.5993
	step [166/192], loss=105.5690
	step [167/192], loss=100.1413
	step [168/192], loss=88.1447
	step [169/192], loss=94.9283
	step [170/192], loss=96.5047
	step [171/192], loss=101.8566
	step [172/192], loss=75.8218
	step [173/192], loss=89.8443
	step [174/192], loss=96.7723
	step [175/192], loss=95.4955
	step [176/192], loss=88.8821
	step [177/192], loss=83.3614
	step [178/192], loss=92.1325
	step [179/192], loss=100.9944
	step [180/192], loss=82.9493
	step [181/192], loss=87.5115
	step [182/192], loss=96.2664
	step [183/192], loss=93.9031
	step [184/192], loss=103.0342
	step [185/192], loss=98.2632
	step [186/192], loss=91.1819
	step [187/192], loss=87.8557
	step [188/192], loss=96.4215
	step [189/192], loss=106.8934
	step [190/192], loss=86.4972
	step [191/192], loss=99.9437
	step [192/192], loss=13.0418
	Evaluating
	loss=0.0150, precision=0.4004, recall=0.9029, f1=0.5548
Training epoch 17
	step [1/192], loss=78.1632
	step [2/192], loss=97.5676
	step [3/192], loss=99.7496
	step [4/192], loss=92.1294
	step [5/192], loss=93.7008
	step [6/192], loss=88.0660
	step [7/192], loss=94.7484
	step [8/192], loss=97.9422
	step [9/192], loss=94.6551
	step [10/192], loss=92.9100
	step [11/192], loss=103.0577
	step [12/192], loss=95.0152
	step [13/192], loss=95.9487
	step [14/192], loss=105.1389
	step [15/192], loss=101.2765
	step [16/192], loss=93.5384
	step [17/192], loss=91.6494
	step [18/192], loss=103.9747
	step [19/192], loss=104.7796
	step [20/192], loss=99.6868
	step [21/192], loss=83.5837
	step [22/192], loss=101.3312
	step [23/192], loss=97.2759
	step [24/192], loss=92.1979
	step [25/192], loss=100.9069
	step [26/192], loss=95.2426
	step [27/192], loss=84.0777
	step [28/192], loss=97.5303
	step [29/192], loss=93.9719
	step [30/192], loss=109.4998
	step [31/192], loss=91.2453
	step [32/192], loss=98.3120
	step [33/192], loss=90.7723
	step [34/192], loss=92.9589
	step [35/192], loss=87.4841
	step [36/192], loss=106.0856
	step [37/192], loss=99.0179
	step [38/192], loss=112.9560
	step [39/192], loss=103.1936
	step [40/192], loss=99.6596
	step [41/192], loss=93.7003
	step [42/192], loss=91.7500
	step [43/192], loss=109.7039
	step [44/192], loss=82.2541
	step [45/192], loss=90.0519
	step [46/192], loss=97.3468
	step [47/192], loss=101.8063
	step [48/192], loss=72.4017
	step [49/192], loss=93.1440
	step [50/192], loss=96.2240
	step [51/192], loss=94.0541
	step [52/192], loss=87.3722
	step [53/192], loss=102.7203
	step [54/192], loss=92.7727
	step [55/192], loss=96.9725
	step [56/192], loss=88.6160
	step [57/192], loss=104.7993
	step [58/192], loss=94.6516
	step [59/192], loss=101.7599
	step [60/192], loss=84.6017
	step [61/192], loss=106.9841
	step [62/192], loss=95.4643
	step [63/192], loss=79.1767
	step [64/192], loss=96.1607
	step [65/192], loss=89.0199
	step [66/192], loss=85.6149
	step [67/192], loss=104.9182
	step [68/192], loss=104.7377
	step [69/192], loss=102.9147
	step [70/192], loss=105.1028
	step [71/192], loss=85.0523
	step [72/192], loss=90.4815
	step [73/192], loss=92.0165
	step [74/192], loss=101.4099
	step [75/192], loss=82.9477
	step [76/192], loss=83.4633
	step [77/192], loss=95.3168
	step [78/192], loss=89.5093
	step [79/192], loss=85.3775
	step [80/192], loss=91.9395
	step [81/192], loss=97.0698
	step [82/192], loss=89.0273
	step [83/192], loss=88.4465
	step [84/192], loss=114.1400
	step [85/192], loss=92.0015
	step [86/192], loss=101.2474
	step [87/192], loss=91.2135
	step [88/192], loss=98.8530
	step [89/192], loss=87.8900
	step [90/192], loss=87.7923
	step [91/192], loss=80.3528
	step [92/192], loss=91.0482
	step [93/192], loss=112.1249
	step [94/192], loss=103.1849
	step [95/192], loss=85.1187
	step [96/192], loss=95.5659
	step [97/192], loss=98.3706
	step [98/192], loss=104.1854
	step [99/192], loss=71.7339
	step [100/192], loss=105.7505
	step [101/192], loss=92.3610
	step [102/192], loss=91.4259
	step [103/192], loss=82.7889
	step [104/192], loss=107.1235
	step [105/192], loss=98.2296
	step [106/192], loss=95.0833
	step [107/192], loss=100.3222
	step [108/192], loss=95.6117
	step [109/192], loss=86.2331
	step [110/192], loss=108.9004
	step [111/192], loss=78.4077
	step [112/192], loss=78.2217
	step [113/192], loss=94.6047
	step [114/192], loss=91.4806
	step [115/192], loss=102.7202
	step [116/192], loss=85.9815
	step [117/192], loss=96.8317
	step [118/192], loss=95.4989
	step [119/192], loss=84.6516
	step [120/192], loss=105.0202
	step [121/192], loss=103.5542
	step [122/192], loss=83.1612
	step [123/192], loss=91.1428
	step [124/192], loss=87.8079
	step [125/192], loss=85.3023
	step [126/192], loss=75.0493
	step [127/192], loss=102.7510
	step [128/192], loss=87.3554
	step [129/192], loss=96.4614
	step [130/192], loss=85.4776
	step [131/192], loss=122.3947
	step [132/192], loss=87.4662
	step [133/192], loss=86.0965
	step [134/192], loss=81.2121
	step [135/192], loss=101.9848
	step [136/192], loss=86.0290
	step [137/192], loss=102.5085
	step [138/192], loss=82.4038
	step [139/192], loss=96.4555
	step [140/192], loss=92.5653
	step [141/192], loss=96.1299
	step [142/192], loss=83.3864
	step [143/192], loss=100.8654
	step [144/192], loss=98.6553
	step [145/192], loss=88.6474
	step [146/192], loss=93.2163
	step [147/192], loss=93.3513
	step [148/192], loss=95.7515
	step [149/192], loss=84.4317
	step [150/192], loss=108.3451
	step [151/192], loss=93.8276
	step [152/192], loss=104.2236
	step [153/192], loss=88.1695
	step [154/192], loss=93.6485
	step [155/192], loss=97.6326
	step [156/192], loss=87.3578
	step [157/192], loss=103.4306
	step [158/192], loss=88.3291
	step [159/192], loss=100.2872
	step [160/192], loss=79.9635
	step [161/192], loss=79.0242
	step [162/192], loss=96.6804
	step [163/192], loss=86.8569
	step [164/192], loss=92.3576
	step [165/192], loss=104.6644
	step [166/192], loss=78.1601
	step [167/192], loss=95.0368
	step [168/192], loss=96.3395
	step [169/192], loss=92.1734
	step [170/192], loss=95.7330
	step [171/192], loss=93.5366
	step [172/192], loss=100.5649
	step [173/192], loss=89.5367
	step [174/192], loss=87.6633
	step [175/192], loss=97.7404
	step [176/192], loss=106.1308
	step [177/192], loss=94.2256
	step [178/192], loss=88.9528
	step [179/192], loss=85.4072
	step [180/192], loss=88.2636
	step [181/192], loss=80.0859
	step [182/192], loss=94.8124
	step [183/192], loss=83.3440
	step [184/192], loss=95.3959
	step [185/192], loss=92.8232
	step [186/192], loss=92.1864
	step [187/192], loss=92.2976
	step [188/192], loss=93.1334
	step [189/192], loss=86.0889
	step [190/192], loss=91.5957
	step [191/192], loss=91.7141
	step [192/192], loss=11.8140
	Evaluating
	loss=0.0136, precision=0.4307, recall=0.9171, f1=0.5862
Training epoch 18
	step [1/192], loss=75.0185
	step [2/192], loss=97.5494
	step [3/192], loss=95.0227
	step [4/192], loss=101.3601
	step [5/192], loss=99.9880
	step [6/192], loss=106.2644
	step [7/192], loss=97.6809
	step [8/192], loss=96.2083
	step [9/192], loss=74.6243
	step [10/192], loss=72.2833
	step [11/192], loss=95.8062
	step [12/192], loss=81.6724
	step [13/192], loss=89.3365
	step [14/192], loss=90.7855
	step [15/192], loss=93.8479
	step [16/192], loss=85.9523
	step [17/192], loss=92.7730
	step [18/192], loss=105.3667
	step [19/192], loss=93.4673
	step [20/192], loss=92.4701
	step [21/192], loss=86.9830
	step [22/192], loss=97.8002
	step [23/192], loss=83.8319
	step [24/192], loss=93.8568
	step [25/192], loss=91.8797
	step [26/192], loss=101.0119
	step [27/192], loss=113.2132
	step [28/192], loss=90.5589
	step [29/192], loss=101.9966
	step [30/192], loss=92.3761
	step [31/192], loss=76.3183
	step [32/192], loss=103.8616
	step [33/192], loss=97.6242
	step [34/192], loss=87.5416
	step [35/192], loss=101.8441
	step [36/192], loss=86.7872
	step [37/192], loss=77.4218
	step [38/192], loss=85.8546
	step [39/192], loss=86.4433
	step [40/192], loss=95.0743
	step [41/192], loss=94.6855
	step [42/192], loss=86.9932
	step [43/192], loss=84.4087
	step [44/192], loss=96.4795
	step [45/192], loss=97.5954
	step [46/192], loss=97.9037
	step [47/192], loss=89.4085
	step [48/192], loss=88.5619
	step [49/192], loss=103.0765
	step [50/192], loss=85.5661
	step [51/192], loss=96.1953
	step [52/192], loss=91.9877
	step [53/192], loss=92.7079
	step [54/192], loss=94.3848
	step [55/192], loss=88.0430
	step [56/192], loss=90.1436
	step [57/192], loss=86.2047
	step [58/192], loss=93.3599
	step [59/192], loss=86.2637
	step [60/192], loss=99.7561
	step [61/192], loss=97.7338
	step [62/192], loss=97.3742
	step [63/192], loss=85.8747
	step [64/192], loss=93.8631
	step [65/192], loss=102.6205
	step [66/192], loss=83.4651
	step [67/192], loss=103.0963
	step [68/192], loss=91.9177
	step [69/192], loss=92.1663
	step [70/192], loss=84.9571
	step [71/192], loss=89.6669
	step [72/192], loss=97.5981
	step [73/192], loss=97.0328
	step [74/192], loss=85.3686
	step [75/192], loss=90.6584
	step [76/192], loss=94.2527
	step [77/192], loss=88.1813
	step [78/192], loss=99.8094
	step [79/192], loss=94.5026
	step [80/192], loss=88.9220
	step [81/192], loss=91.5473
	step [82/192], loss=81.7386
	step [83/192], loss=116.8389
	step [84/192], loss=117.3039
	step [85/192], loss=87.3921
	step [86/192], loss=108.1944
	step [87/192], loss=107.0726
	step [88/192], loss=76.0482
	step [89/192], loss=89.7083
	step [90/192], loss=94.9754
	step [91/192], loss=83.1829
	step [92/192], loss=88.2152
	step [93/192], loss=99.8362
	step [94/192], loss=99.1187
	step [95/192], loss=96.6723
	step [96/192], loss=87.6483
	step [97/192], loss=100.0476
	step [98/192], loss=93.4847
	step [99/192], loss=101.8989
	step [100/192], loss=86.2110
	step [101/192], loss=108.4242
	step [102/192], loss=91.2634
	step [103/192], loss=87.3993
	step [104/192], loss=81.0126
	step [105/192], loss=100.2434
	step [106/192], loss=85.0051
	step [107/192], loss=103.5660
	step [108/192], loss=86.0817
	step [109/192], loss=79.6231
	step [110/192], loss=87.8224
	step [111/192], loss=95.9822
	step [112/192], loss=106.7877
	step [113/192], loss=78.8002
	step [114/192], loss=86.8354
	step [115/192], loss=103.7063
	step [116/192], loss=110.6910
	step [117/192], loss=95.2754
	step [118/192], loss=95.1290
	step [119/192], loss=82.9585
	step [120/192], loss=99.0580
	step [121/192], loss=95.7101
	step [122/192], loss=92.1824
	step [123/192], loss=83.2974
	step [124/192], loss=77.6230
	step [125/192], loss=82.9528
	step [126/192], loss=97.8657
	step [127/192], loss=80.9178
	step [128/192], loss=95.8946
	step [129/192], loss=86.6816
	step [130/192], loss=88.8449
	step [131/192], loss=98.0474
	step [132/192], loss=95.9423
	step [133/192], loss=92.3899
	step [134/192], loss=91.4590
	step [135/192], loss=76.5473
	step [136/192], loss=95.3913
	step [137/192], loss=85.3633
	step [138/192], loss=82.7303
	step [139/192], loss=90.7935
	step [140/192], loss=90.5757
	step [141/192], loss=92.7788
	step [142/192], loss=111.3308
	step [143/192], loss=82.0362
	step [144/192], loss=95.3109
	step [145/192], loss=89.6880
	step [146/192], loss=97.7009
	step [147/192], loss=93.9281
	step [148/192], loss=105.6585
	step [149/192], loss=91.3886
	step [150/192], loss=95.4941
	step [151/192], loss=95.7964
	step [152/192], loss=110.6837
	step [153/192], loss=98.6138
	step [154/192], loss=88.5078
	step [155/192], loss=93.5139
	step [156/192], loss=84.6881
	step [157/192], loss=103.9674
	step [158/192], loss=90.9877
	step [159/192], loss=75.5394
	step [160/192], loss=92.9884
	step [161/192], loss=91.3648
	step [162/192], loss=81.5586
	step [163/192], loss=102.1380
	step [164/192], loss=94.8995
	step [165/192], loss=88.1739
	step [166/192], loss=81.0170
	step [167/192], loss=101.9832
	step [168/192], loss=81.2262
	step [169/192], loss=92.5501
	step [170/192], loss=106.4282
	step [171/192], loss=104.9711
	step [172/192], loss=87.9350
	step [173/192], loss=100.2439
	step [174/192], loss=99.8035
	step [175/192], loss=102.2030
	step [176/192], loss=91.4637
	step [177/192], loss=81.5857
	step [178/192], loss=79.9264
	step [179/192], loss=85.1453
	step [180/192], loss=80.8991
	step [181/192], loss=101.7804
	step [182/192], loss=81.7298
	step [183/192], loss=100.2605
	step [184/192], loss=96.5048
	step [185/192], loss=84.8837
	step [186/192], loss=85.9163
	step [187/192], loss=81.0358
	step [188/192], loss=88.0316
	step [189/192], loss=97.3466
	step [190/192], loss=79.3528
	step [191/192], loss=97.2330
	step [192/192], loss=11.0991
	Evaluating
	loss=0.0162, precision=0.2957, recall=0.9243, f1=0.4481
Training epoch 19
	step [1/192], loss=101.0805
	step [2/192], loss=94.2643
	step [3/192], loss=100.6482
	step [4/192], loss=94.0639
	step [5/192], loss=83.2290
	step [6/192], loss=95.3441
	step [7/192], loss=86.8245
	step [8/192], loss=82.3720
	step [9/192], loss=87.0276
	step [10/192], loss=100.5302
	step [11/192], loss=87.5377
	step [12/192], loss=96.8517
	step [13/192], loss=104.9924
	step [14/192], loss=80.8774
	step [15/192], loss=91.5326
	step [16/192], loss=72.0674
	step [17/192], loss=90.2002
	step [18/192], loss=86.6555
	step [19/192], loss=93.6013
	step [20/192], loss=105.1881
	step [21/192], loss=95.1036
	step [22/192], loss=94.7897
	step [23/192], loss=102.6633
	step [24/192], loss=93.5574
	step [25/192], loss=80.1603
	step [26/192], loss=103.2133
	step [27/192], loss=76.5114
	step [28/192], loss=91.5867
	step [29/192], loss=116.5252
	step [30/192], loss=86.8552
	step [31/192], loss=104.2532
	step [32/192], loss=85.1721
	step [33/192], loss=75.4389
	step [34/192], loss=94.2996
	step [35/192], loss=83.9343
	step [36/192], loss=102.5759
	step [37/192], loss=93.5286
	step [38/192], loss=85.0388
	step [39/192], loss=88.9254
	step [40/192], loss=98.7059
	step [41/192], loss=92.4667
	step [42/192], loss=101.1287
	step [43/192], loss=83.7540
	step [44/192], loss=89.2937
	step [45/192], loss=87.1036
	step [46/192], loss=78.2690
	step [47/192], loss=86.0560
	step [48/192], loss=92.7141
	step [49/192], loss=83.9929
	step [50/192], loss=90.6229
	step [51/192], loss=102.7410
	step [52/192], loss=85.7312
	step [53/192], loss=75.6920
	step [54/192], loss=102.4882
	step [55/192], loss=101.5034
	step [56/192], loss=96.9438
	step [57/192], loss=82.7842
	step [58/192], loss=106.5286
	step [59/192], loss=82.1034
	step [60/192], loss=97.8687
	step [61/192], loss=112.7372
	step [62/192], loss=81.1065
	step [63/192], loss=113.3812
	step [64/192], loss=98.8980
	step [65/192], loss=94.5589
	step [66/192], loss=83.2295
	step [67/192], loss=93.3071
	step [68/192], loss=83.1216
	step [69/192], loss=87.0493
	step [70/192], loss=93.2781
	step [71/192], loss=84.5979
	step [72/192], loss=88.5281
	step [73/192], loss=87.8118
	step [74/192], loss=100.4327
	step [75/192], loss=91.8317
	step [76/192], loss=78.9250
	step [77/192], loss=91.0760
	step [78/192], loss=95.0303
	step [79/192], loss=79.5158
	step [80/192], loss=97.5430
	step [81/192], loss=83.8287
	step [82/192], loss=88.6302
	step [83/192], loss=83.5639
	step [84/192], loss=102.0207
	step [85/192], loss=102.3878
	step [86/192], loss=84.5868
	step [87/192], loss=104.1239
	step [88/192], loss=94.9640
	step [89/192], loss=92.5712
	step [90/192], loss=89.5350
	step [91/192], loss=120.1154
	step [92/192], loss=90.3993
	step [93/192], loss=83.1042
	step [94/192], loss=98.0677
	step [95/192], loss=84.7249
	step [96/192], loss=102.1764
	step [97/192], loss=84.2966
	step [98/192], loss=100.2443
	step [99/192], loss=88.3091
	step [100/192], loss=83.6123
	step [101/192], loss=76.4408
	step [102/192], loss=88.8153
	step [103/192], loss=105.0386
	step [104/192], loss=96.5570
	step [105/192], loss=104.7050
	step [106/192], loss=91.8718
	step [107/192], loss=106.8596
	step [108/192], loss=88.4095
	step [109/192], loss=78.2409
	step [110/192], loss=74.2378
	step [111/192], loss=94.7953
	step [112/192], loss=99.0999
	step [113/192], loss=81.0979
	step [114/192], loss=88.0327
	step [115/192], loss=83.7040
	step [116/192], loss=80.7869
	step [117/192], loss=93.2643
	step [118/192], loss=85.9686
	step [119/192], loss=103.6426
	step [120/192], loss=88.8107
	step [121/192], loss=81.0379
	step [122/192], loss=78.9107
	step [123/192], loss=112.9555
	step [124/192], loss=101.3770
	step [125/192], loss=109.5938
	step [126/192], loss=94.4525
	step [127/192], loss=105.1655
	step [128/192], loss=94.9127
	step [129/192], loss=83.2898
	step [130/192], loss=99.2080
	step [131/192], loss=100.2685
	step [132/192], loss=90.9145
	step [133/192], loss=91.0885
	step [134/192], loss=81.4269
	step [135/192], loss=105.5326
	step [136/192], loss=85.2153
	step [137/192], loss=84.1065
	step [138/192], loss=83.8478
	step [139/192], loss=91.8587
	step [140/192], loss=79.7072
	step [141/192], loss=80.0804
	step [142/192], loss=85.6240
	step [143/192], loss=92.2597
	step [144/192], loss=90.2259
	step [145/192], loss=80.8494
	step [146/192], loss=89.1102
	step [147/192], loss=90.6283
	step [148/192], loss=99.1581
	step [149/192], loss=81.0532
	step [150/192], loss=111.4115
	step [151/192], loss=77.5290
	step [152/192], loss=97.0228
	step [153/192], loss=88.3305
	step [154/192], loss=78.8838
	step [155/192], loss=93.9452
	step [156/192], loss=93.1158
	step [157/192], loss=83.5552
	step [158/192], loss=81.3701
	step [159/192], loss=91.8578
	step [160/192], loss=94.7909
	step [161/192], loss=80.9750
	step [162/192], loss=92.1316
	step [163/192], loss=81.6413
	step [164/192], loss=93.2957
	step [165/192], loss=87.0138
	step [166/192], loss=100.1502
	step [167/192], loss=80.4485
	step [168/192], loss=81.5110
	step [169/192], loss=92.8499
	step [170/192], loss=81.4064
	step [171/192], loss=89.4822
	step [172/192], loss=91.3612
	step [173/192], loss=95.6087
	step [174/192], loss=97.2756
	step [175/192], loss=91.1199
	step [176/192], loss=83.6066
	step [177/192], loss=86.6291
	step [178/192], loss=81.7722
	step [179/192], loss=86.8633
	step [180/192], loss=104.8902
	step [181/192], loss=90.1503
	step [182/192], loss=93.0624
	step [183/192], loss=94.3337
	step [184/192], loss=99.1982
	step [185/192], loss=72.6881
	step [186/192], loss=87.7715
	step [187/192], loss=106.7107
	step [188/192], loss=73.6537
	step [189/192], loss=92.6236
	step [190/192], loss=88.9777
	step [191/192], loss=91.5017
	step [192/192], loss=12.2554
	Evaluating
	loss=0.0125, precision=0.3806, recall=0.9132, f1=0.5373
Training epoch 20
	step [1/192], loss=90.5467
	step [2/192], loss=89.0061
	step [3/192], loss=78.7350
	step [4/192], loss=71.4131
	step [5/192], loss=83.7272
	step [6/192], loss=93.7601
	step [7/192], loss=85.8442
	step [8/192], loss=95.5331
	step [9/192], loss=97.0603
	step [10/192], loss=87.9940
	step [11/192], loss=85.1323
	step [12/192], loss=87.2370
	step [13/192], loss=94.6216
	step [14/192], loss=86.2670
	step [15/192], loss=105.2651
	step [16/192], loss=102.2989
	step [17/192], loss=91.2887
	step [18/192], loss=91.2531
	step [19/192], loss=84.9400
	step [20/192], loss=97.6636
	step [21/192], loss=88.3577
	step [22/192], loss=89.6914
	step [23/192], loss=94.8450
	step [24/192], loss=77.7891
	step [25/192], loss=87.4661
	step [26/192], loss=89.6347
	step [27/192], loss=104.0009
	step [28/192], loss=99.2994
	step [29/192], loss=82.9888
	step [30/192], loss=77.5592
	step [31/192], loss=79.2022
	step [32/192], loss=90.2397
	step [33/192], loss=95.6013
	step [34/192], loss=83.0749
	step [35/192], loss=88.7735
	step [36/192], loss=97.6404
	step [37/192], loss=93.8561
	step [38/192], loss=91.4273
	step [39/192], loss=95.9574
	step [40/192], loss=97.3760
	step [41/192], loss=98.0990
	step [42/192], loss=80.1521
	step [43/192], loss=92.1198
	step [44/192], loss=91.3310
	step [45/192], loss=93.6544
	step [46/192], loss=105.6379
	step [47/192], loss=96.4619
	step [48/192], loss=95.2296
	step [49/192], loss=109.8627
	step [50/192], loss=72.2839
	step [51/192], loss=85.5565
	step [52/192], loss=90.0595
	step [53/192], loss=107.0785
	step [54/192], loss=87.9548
	step [55/192], loss=78.0422
	step [56/192], loss=89.4372
	step [57/192], loss=93.6668
	step [58/192], loss=86.9057
	step [59/192], loss=85.0652
	step [60/192], loss=93.6224
	step [61/192], loss=92.9633
	step [62/192], loss=87.5312
	step [63/192], loss=89.6263
	step [64/192], loss=86.9935
	step [65/192], loss=95.3600
	step [66/192], loss=86.0193
	step [67/192], loss=98.2099
	step [68/192], loss=116.0563
	step [69/192], loss=86.1993
	step [70/192], loss=94.7111
	step [71/192], loss=91.6281
	step [72/192], loss=88.2932
	step [73/192], loss=75.0887
	step [74/192], loss=90.3158
	step [75/192], loss=83.5400
	step [76/192], loss=93.1748
	step [77/192], loss=96.7652
	step [78/192], loss=96.1536
	step [79/192], loss=82.4585
	step [80/192], loss=79.5478
	step [81/192], loss=88.9974
	step [82/192], loss=85.2230
	step [83/192], loss=94.6337
	step [84/192], loss=78.4997
	step [85/192], loss=81.7009
	step [86/192], loss=91.5969
	step [87/192], loss=95.4319
	step [88/192], loss=92.9393
	step [89/192], loss=109.7090
	step [90/192], loss=76.0049
	step [91/192], loss=94.7032
	step [92/192], loss=80.8290
	step [93/192], loss=98.2773
	step [94/192], loss=83.8158
	step [95/192], loss=83.3060
	step [96/192], loss=68.6383
	step [97/192], loss=71.8908
	step [98/192], loss=87.8501
	step [99/192], loss=98.7845
	step [100/192], loss=87.8282
	step [101/192], loss=90.9853
	step [102/192], loss=94.4416
	step [103/192], loss=84.9006
	step [104/192], loss=85.4513
	step [105/192], loss=95.9921
	step [106/192], loss=91.7519
	step [107/192], loss=97.1120
	step [108/192], loss=93.6187
	step [109/192], loss=90.5642
	step [110/192], loss=94.7282
	step [111/192], loss=99.4702
	step [112/192], loss=108.4094
	step [113/192], loss=83.4560
	step [114/192], loss=93.9805
	step [115/192], loss=87.4837
	step [116/192], loss=96.3074
	step [117/192], loss=89.5910
	step [118/192], loss=74.9437
	step [119/192], loss=76.4083
	step [120/192], loss=82.2338
	step [121/192], loss=90.8039
	step [122/192], loss=94.8136
	step [123/192], loss=89.9661
	step [124/192], loss=86.9668
	step [125/192], loss=94.2616
	step [126/192], loss=93.5764
	step [127/192], loss=80.2919
	step [128/192], loss=76.2499
	step [129/192], loss=89.0227
	step [130/192], loss=88.5875
	step [131/192], loss=91.9538
	step [132/192], loss=91.5084
	step [133/192], loss=114.0216
	step [134/192], loss=80.8551
	step [135/192], loss=87.0171
	step [136/192], loss=103.1664
	step [137/192], loss=90.8262
	step [138/192], loss=99.3699
	step [139/192], loss=100.2209
	step [140/192], loss=97.0747
	step [141/192], loss=97.6275
	step [142/192], loss=86.7658
	step [143/192], loss=92.4026
	step [144/192], loss=93.6455
	step [145/192], loss=90.0356
	step [146/192], loss=95.2121
	step [147/192], loss=79.9757
	step [148/192], loss=81.2945
	step [149/192], loss=90.4788
	step [150/192], loss=96.2615
	step [151/192], loss=82.1013
	step [152/192], loss=102.4132
	step [153/192], loss=79.9449
	step [154/192], loss=96.1199
	step [155/192], loss=81.5403
	step [156/192], loss=93.7074
	step [157/192], loss=96.4925
	step [158/192], loss=92.9689
	step [159/192], loss=92.5260
	step [160/192], loss=88.6980
	step [161/192], loss=95.5720
	step [162/192], loss=101.4910
	step [163/192], loss=89.5577
	step [164/192], loss=86.1981
	step [165/192], loss=104.7055
	step [166/192], loss=83.0170
	step [167/192], loss=78.5735
	step [168/192], loss=89.4397
	step [169/192], loss=95.8797
	step [170/192], loss=81.3185
	step [171/192], loss=83.4051
	step [172/192], loss=90.5932
	step [173/192], loss=92.7494
	step [174/192], loss=89.2594
	step [175/192], loss=80.0046
	step [176/192], loss=73.9548
	step [177/192], loss=96.8127
	step [178/192], loss=86.7370
	step [179/192], loss=91.3719
	step [180/192], loss=80.6845
	step [181/192], loss=85.2345
	step [182/192], loss=83.6539
	step [183/192], loss=84.9317
	step [184/192], loss=82.5153
	step [185/192], loss=88.2477
	step [186/192], loss=92.5936
	step [187/192], loss=81.8806
	step [188/192], loss=90.5922
	step [189/192], loss=78.7186
	step [190/192], loss=90.3063
	step [191/192], loss=82.5085
	step [192/192], loss=7.4513
	Evaluating
	loss=0.0114, precision=0.3778, recall=0.8940, f1=0.5311
Training epoch 21
	step [1/192], loss=89.0174
	step [2/192], loss=85.8367
	step [3/192], loss=88.3509
	step [4/192], loss=78.0130
	step [5/192], loss=90.0620
	step [6/192], loss=81.0820
	step [7/192], loss=80.1206
	step [8/192], loss=80.5243
	step [9/192], loss=81.3834
	step [10/192], loss=86.3678
	step [11/192], loss=92.6805
	step [12/192], loss=99.0373
	step [13/192], loss=97.8754
	step [14/192], loss=93.7235
	step [15/192], loss=87.9103
	step [16/192], loss=92.5726
	step [17/192], loss=88.9087
	step [18/192], loss=97.5569
	step [19/192], loss=95.7459
	step [20/192], loss=85.8171
	step [21/192], loss=94.3544
	step [22/192], loss=76.1060
	step [23/192], loss=91.3168
	step [24/192], loss=99.7785
	step [25/192], loss=96.1392
	step [26/192], loss=72.7092
	step [27/192], loss=90.3606
	step [28/192], loss=93.5854
	step [29/192], loss=87.8833
	step [30/192], loss=108.2807
	step [31/192], loss=82.6867
	step [32/192], loss=89.8160
	step [33/192], loss=83.5202
	step [34/192], loss=89.0446
	step [35/192], loss=105.3286
	step [36/192], loss=105.4276
	step [37/192], loss=78.7351
	step [38/192], loss=95.9793
	step [39/192], loss=87.9340
	step [40/192], loss=84.9899
	step [41/192], loss=86.2406
	step [42/192], loss=76.9584
	step [43/192], loss=85.6419
	step [44/192], loss=87.2150
	step [45/192], loss=85.4909
	step [46/192], loss=94.2792
	step [47/192], loss=88.5828
	step [48/192], loss=91.7997
	step [49/192], loss=96.5434
	step [50/192], loss=92.7192
	step [51/192], loss=104.3372
	step [52/192], loss=87.0631
	step [53/192], loss=79.2155
	step [54/192], loss=89.1070
	step [55/192], loss=83.5761
	step [56/192], loss=69.0955
	step [57/192], loss=83.7409
	step [58/192], loss=91.5510
	step [59/192], loss=93.9803
	step [60/192], loss=84.5792
	step [61/192], loss=88.8451
	step [62/192], loss=93.7689
	step [63/192], loss=83.7003
	step [64/192], loss=77.5397
	step [65/192], loss=89.1014
	step [66/192], loss=97.8808
	step [67/192], loss=90.4446
	step [68/192], loss=82.8768
	step [69/192], loss=83.5082
	step [70/192], loss=84.9424
	step [71/192], loss=81.8645
	step [72/192], loss=92.3277
	step [73/192], loss=86.1317
	step [74/192], loss=93.8214
	step [75/192], loss=93.0455
	step [76/192], loss=91.3611
	step [77/192], loss=75.9115
	step [78/192], loss=99.8475
	step [79/192], loss=102.8143
	step [80/192], loss=90.3533
	step [81/192], loss=87.4566
	step [82/192], loss=69.9711
	step [83/192], loss=94.6042
	step [84/192], loss=95.0615
	step [85/192], loss=101.3457
	step [86/192], loss=83.6840
	step [87/192], loss=85.5729
	step [88/192], loss=93.0474
	step [89/192], loss=79.4150
	step [90/192], loss=90.9744
	step [91/192], loss=87.2584
	step [92/192], loss=94.2289
	step [93/192], loss=77.5267
	step [94/192], loss=79.8844
	step [95/192], loss=86.4766
	step [96/192], loss=99.9881
	step [97/192], loss=89.3563
	step [98/192], loss=96.5173
	step [99/192], loss=90.2952
	step [100/192], loss=90.8823
	step [101/192], loss=85.7701
	step [102/192], loss=85.7537
	step [103/192], loss=90.7722
	step [104/192], loss=89.4113
	step [105/192], loss=92.5085
	step [106/192], loss=85.0744
	step [107/192], loss=80.4041
	step [108/192], loss=82.1596
	step [109/192], loss=96.4448
	step [110/192], loss=113.1872
	step [111/192], loss=91.1188
	step [112/192], loss=91.0610
	step [113/192], loss=71.9403
	step [114/192], loss=102.3326
	step [115/192], loss=99.3543
	step [116/192], loss=86.8829
	step [117/192], loss=92.3064
	step [118/192], loss=78.0741
	step [119/192], loss=81.6860
	step [120/192], loss=79.7644
	step [121/192], loss=82.9889
	step [122/192], loss=92.8999
	step [123/192], loss=83.4500
	step [124/192], loss=83.5785
	step [125/192], loss=98.4095
	step [126/192], loss=86.3568
	step [127/192], loss=84.0163
	step [128/192], loss=91.1183
	step [129/192], loss=95.0650
	step [130/192], loss=80.5195
	step [131/192], loss=80.5500
	step [132/192], loss=96.9187
	step [133/192], loss=89.0206
	step [134/192], loss=87.3821
	step [135/192], loss=90.0804
	step [136/192], loss=81.6502
	step [137/192], loss=91.2880
	step [138/192], loss=88.2934
	step [139/192], loss=75.6342
	step [140/192], loss=84.9812
	step [141/192], loss=81.3805
	step [142/192], loss=82.4359
	step [143/192], loss=94.0894
	step [144/192], loss=99.0068
	step [145/192], loss=82.0364
	step [146/192], loss=80.6194
	step [147/192], loss=89.6506
	step [148/192], loss=82.2203
	step [149/192], loss=104.2977
	step [150/192], loss=87.9270
	step [151/192], loss=94.2889
	step [152/192], loss=92.4171
	step [153/192], loss=81.7244
	step [154/192], loss=93.6450
	step [155/192], loss=80.4271
	step [156/192], loss=94.3559
	step [157/192], loss=90.6899
	step [158/192], loss=84.1017
	step [159/192], loss=97.4355
	step [160/192], loss=94.0877
	step [161/192], loss=107.0423
	step [162/192], loss=96.6923
	step [163/192], loss=92.3508
	step [164/192], loss=79.0291
	step [165/192], loss=88.1043
	step [166/192], loss=79.6126
	step [167/192], loss=88.6456
	step [168/192], loss=88.6192
	step [169/192], loss=92.0584
	step [170/192], loss=81.8611
	step [171/192], loss=94.7816
	step [172/192], loss=92.4729
	step [173/192], loss=101.5822
	step [174/192], loss=97.3790
	step [175/192], loss=76.7102
	step [176/192], loss=82.7145
	step [177/192], loss=76.0443
	step [178/192], loss=116.7249
	step [179/192], loss=75.3526
	step [180/192], loss=78.0076
	step [181/192], loss=82.0105
	step [182/192], loss=83.6487
	step [183/192], loss=91.6021
	step [184/192], loss=89.9678
	step [185/192], loss=83.8874
	step [186/192], loss=78.2516
	step [187/192], loss=94.1632
	step [188/192], loss=87.2250
	step [189/192], loss=89.1689
	step [190/192], loss=74.3921
	step [191/192], loss=89.5375
	step [192/192], loss=13.4298
	Evaluating
	loss=0.0131, precision=0.3436, recall=0.9023, f1=0.4977
Training epoch 22
	step [1/192], loss=88.8519
	step [2/192], loss=78.1791
	step [3/192], loss=92.2734
	step [4/192], loss=100.9369
	step [5/192], loss=87.0305
	step [6/192], loss=94.1467
	step [7/192], loss=85.8968
	step [8/192], loss=83.6181
	step [9/192], loss=84.4265
	step [10/192], loss=86.4044
	step [11/192], loss=87.3357
	step [12/192], loss=88.1551
	step [13/192], loss=77.4318
	step [14/192], loss=81.8689
	step [15/192], loss=96.6933
	step [16/192], loss=85.8757
	step [17/192], loss=96.7603
	step [18/192], loss=83.7870
	step [19/192], loss=87.0924
	step [20/192], loss=85.1345
	step [21/192], loss=85.5971
	step [22/192], loss=86.4517
	step [23/192], loss=75.5544
	step [24/192], loss=92.4628
	step [25/192], loss=80.6084
	step [26/192], loss=84.7095
	step [27/192], loss=83.2296
	step [28/192], loss=84.4715
	step [29/192], loss=90.2347
	step [30/192], loss=89.2825
	step [31/192], loss=81.6081
	step [32/192], loss=94.8358
	step [33/192], loss=84.4822
	step [34/192], loss=90.6533
	step [35/192], loss=90.1698
	step [36/192], loss=84.4257
	step [37/192], loss=93.9669
	step [38/192], loss=96.1479
	step [39/192], loss=99.7585
	step [40/192], loss=93.8327
	step [41/192], loss=99.6790
	step [42/192], loss=83.8710
	step [43/192], loss=79.6187
	step [44/192], loss=88.0485
	step [45/192], loss=85.2058
	step [46/192], loss=91.8831
	step [47/192], loss=106.6797
	step [48/192], loss=90.9023
	step [49/192], loss=82.1022
	step [50/192], loss=90.4416
	step [51/192], loss=95.4628
	step [52/192], loss=86.4222
	step [53/192], loss=75.5818
	step [54/192], loss=99.2239
	step [55/192], loss=89.8524
	step [56/192], loss=94.7086
	step [57/192], loss=74.3431
	step [58/192], loss=89.4101
	step [59/192], loss=85.6280
	step [60/192], loss=82.7979
	step [61/192], loss=99.3138
	step [62/192], loss=76.7500
	step [63/192], loss=80.7096
	step [64/192], loss=99.8010
	step [65/192], loss=81.3952
	step [66/192], loss=85.0983
	step [67/192], loss=91.5222
	step [68/192], loss=95.7857
	step [69/192], loss=90.1959
	step [70/192], loss=86.4394
	step [71/192], loss=75.4459
	step [72/192], loss=94.8578
	step [73/192], loss=91.7926
	step [74/192], loss=78.8110
	step [75/192], loss=80.9735
	step [76/192], loss=86.7741
	step [77/192], loss=89.5847
	step [78/192], loss=89.0730
	step [79/192], loss=94.4985
	step [80/192], loss=78.8688
	step [81/192], loss=88.4300
	step [82/192], loss=90.1983
	step [83/192], loss=71.4496
	step [84/192], loss=94.9984
	step [85/192], loss=92.9156
	step [86/192], loss=70.8601
	step [87/192], loss=78.2436
	step [88/192], loss=90.0648
	step [89/192], loss=88.9247
	step [90/192], loss=82.5921
	step [91/192], loss=92.2821
	step [92/192], loss=82.7173
	step [93/192], loss=81.1765
	step [94/192], loss=75.2545
	step [95/192], loss=95.1828
	step [96/192], loss=83.8171
	step [97/192], loss=86.9987
	step [98/192], loss=95.4324
	step [99/192], loss=99.0898
	step [100/192], loss=112.1964
	step [101/192], loss=91.1320
	step [102/192], loss=82.4493
	step [103/192], loss=96.4848
	step [104/192], loss=88.6654
	step [105/192], loss=86.8817
	step [106/192], loss=80.3806
	step [107/192], loss=79.9241
	step [108/192], loss=85.2886
	step [109/192], loss=79.1808
	step [110/192], loss=91.3241
	step [111/192], loss=89.0669
	step [112/192], loss=82.2722
	step [113/192], loss=86.5637
	step [114/192], loss=73.8100
	step [115/192], loss=80.9546
	step [116/192], loss=78.5196
	step [117/192], loss=84.2968
	step [118/192], loss=90.6094
	step [119/192], loss=88.0837
	step [120/192], loss=72.7096
	step [121/192], loss=92.9314
	step [122/192], loss=81.2797
	step [123/192], loss=93.8889
	step [124/192], loss=77.9776
	step [125/192], loss=92.3438
	step [126/192], loss=78.0175
	step [127/192], loss=84.5453
	step [128/192], loss=100.6983
	step [129/192], loss=81.5476
	step [130/192], loss=82.9648
	step [131/192], loss=100.0856
	step [132/192], loss=76.2765
	step [133/192], loss=86.9941
	step [134/192], loss=92.2835
	step [135/192], loss=101.2125
	step [136/192], loss=75.4743
	step [137/192], loss=83.0685
	step [138/192], loss=97.4117
	step [139/192], loss=97.4155
	step [140/192], loss=77.9593
	step [141/192], loss=90.8295
	step [142/192], loss=91.9909
	step [143/192], loss=90.9759
	step [144/192], loss=88.3199
	step [145/192], loss=80.6089
	step [146/192], loss=67.8405
	step [147/192], loss=103.9859
	step [148/192], loss=79.8100
	step [149/192], loss=85.0203
	step [150/192], loss=83.9541
	step [151/192], loss=105.3720
	step [152/192], loss=89.1293
	step [153/192], loss=86.3175
	step [154/192], loss=96.9834
	step [155/192], loss=85.6600
	step [156/192], loss=87.0223
	step [157/192], loss=91.7642
	step [158/192], loss=92.8887
	step [159/192], loss=83.1988
	step [160/192], loss=86.1327
	step [161/192], loss=86.6539
	step [162/192], loss=84.0743
	step [163/192], loss=81.3111
	step [164/192], loss=67.8498
	step [165/192], loss=95.3616
	step [166/192], loss=86.6217
	step [167/192], loss=78.7041
	step [168/192], loss=108.0661
	step [169/192], loss=73.9293
	step [170/192], loss=73.4022
	step [171/192], loss=71.7995
	step [172/192], loss=94.1953
	step [173/192], loss=106.6411
	step [174/192], loss=91.4169
	step [175/192], loss=85.8146
	step [176/192], loss=90.8740
	step [177/192], loss=89.3932
	step [178/192], loss=86.1457
	step [179/192], loss=83.6766
	step [180/192], loss=103.6823
	step [181/192], loss=89.9809
	step [182/192], loss=86.8027
	step [183/192], loss=95.0818
	step [184/192], loss=92.7854
	step [185/192], loss=92.2095
	step [186/192], loss=97.4845
	step [187/192], loss=103.1253
	step [188/192], loss=87.2793
	step [189/192], loss=97.1813
	step [190/192], loss=95.3211
	step [191/192], loss=77.2793
	step [192/192], loss=12.3949
	Evaluating
	loss=0.0115, precision=0.3459, recall=0.9046, f1=0.5005
Training epoch 23
	step [1/192], loss=86.0515
	step [2/192], loss=83.0691
	step [3/192], loss=89.3169
	step [4/192], loss=81.7344
	step [5/192], loss=83.8375
	step [6/192], loss=104.0088
	step [7/192], loss=103.2543
	step [8/192], loss=90.8114
	step [9/192], loss=85.0433
	step [10/192], loss=87.4840
	step [11/192], loss=85.3795
	step [12/192], loss=84.3902
	step [13/192], loss=78.1221
	step [14/192], loss=102.5509
	step [15/192], loss=89.2921
	step [16/192], loss=86.9511
	step [17/192], loss=84.6929
	step [18/192], loss=92.3069
	step [19/192], loss=97.3991
	step [20/192], loss=72.6242
	step [21/192], loss=76.3497
	step [22/192], loss=89.7868
	step [23/192], loss=76.7702
	step [24/192], loss=87.3196
	step [25/192], loss=79.6571
	step [26/192], loss=83.9273
	step [27/192], loss=81.8585
	step [28/192], loss=84.0379
	step [29/192], loss=92.6130
	step [30/192], loss=73.9212
	step [31/192], loss=90.4991
	step [32/192], loss=90.9960
	step [33/192], loss=85.8034
	step [34/192], loss=87.3898
	step [35/192], loss=94.5121
	step [36/192], loss=73.8533
	step [37/192], loss=85.8397
	step [38/192], loss=95.6647
	step [39/192], loss=89.9833
	step [40/192], loss=88.0372
	step [41/192], loss=94.1225
	step [42/192], loss=88.5978
	step [43/192], loss=87.8022
	step [44/192], loss=81.1941
	step [45/192], loss=80.2276
	step [46/192], loss=88.3280
	step [47/192], loss=101.3544
	step [48/192], loss=102.2353
	step [49/192], loss=82.4193
	step [50/192], loss=67.5488
	step [51/192], loss=93.0484
	step [52/192], loss=98.9525
	step [53/192], loss=89.6306
	step [54/192], loss=83.5386
	step [55/192], loss=70.8541
	step [56/192], loss=78.1165
	step [57/192], loss=78.4668
	step [58/192], loss=108.4179
	step [59/192], loss=86.3785
	step [60/192], loss=92.9516
	step [61/192], loss=85.1784
	step [62/192], loss=80.9271
	step [63/192], loss=90.3729
	step [64/192], loss=68.1248
	step [65/192], loss=94.6681
	step [66/192], loss=81.4430
	step [67/192], loss=86.1602
	step [68/192], loss=92.3326
	step [69/192], loss=78.5414
	step [70/192], loss=90.2900
	step [71/192], loss=86.2676
	step [72/192], loss=83.2404
	step [73/192], loss=74.0572
	step [74/192], loss=88.9654
	step [75/192], loss=102.5874
	step [76/192], loss=81.3048
	step [77/192], loss=88.8067
	step [78/192], loss=81.0922
	step [79/192], loss=88.1906
	step [80/192], loss=83.4033
	step [81/192], loss=87.4856
	step [82/192], loss=96.6539
	step [83/192], loss=87.4933
	step [84/192], loss=91.1246
	step [85/192], loss=77.3829
	step [86/192], loss=92.2361
	step [87/192], loss=89.6141
	step [88/192], loss=96.4062
	step [89/192], loss=81.2304
	step [90/192], loss=92.0994
	step [91/192], loss=86.0166
	step [92/192], loss=83.2987
	step [93/192], loss=98.3204
	step [94/192], loss=86.2249
	step [95/192], loss=95.9272
	step [96/192], loss=93.2548
	step [97/192], loss=77.9567
	step [98/192], loss=85.1929
	step [99/192], loss=77.0392
	step [100/192], loss=92.4795
	step [101/192], loss=92.8539
	step [102/192], loss=84.0267
	step [103/192], loss=81.9343
	step [104/192], loss=94.6082
	step [105/192], loss=72.3111
	step [106/192], loss=81.7708
	step [107/192], loss=85.6538
	step [108/192], loss=84.1042
	step [109/192], loss=89.9225
	step [110/192], loss=92.5505
	step [111/192], loss=89.1951
	step [112/192], loss=88.5499
	step [113/192], loss=72.5775
	step [114/192], loss=97.1070
	step [115/192], loss=83.1541
	step [116/192], loss=84.9688
	step [117/192], loss=85.7636
	step [118/192], loss=87.5269
	step [119/192], loss=81.4819
	step [120/192], loss=87.8947
	step [121/192], loss=89.9724
	step [122/192], loss=88.2102
	step [123/192], loss=89.0136
	step [124/192], loss=98.7684
	step [125/192], loss=83.8492
	step [126/192], loss=84.9545
	step [127/192], loss=70.3517
	step [128/192], loss=80.9494
	step [129/192], loss=82.1393
	step [130/192], loss=89.5262
	step [131/192], loss=83.2324
	step [132/192], loss=70.4475
	step [133/192], loss=97.8040
	step [134/192], loss=92.6425
	step [135/192], loss=73.5211
	step [136/192], loss=86.0728
	step [137/192], loss=77.7940
	step [138/192], loss=96.5006
	step [139/192], loss=84.0329
	step [140/192], loss=103.4843
	step [141/192], loss=90.5840
	step [142/192], loss=82.2034
	step [143/192], loss=89.6786
	step [144/192], loss=84.4661
	step [145/192], loss=76.3477
	step [146/192], loss=86.1808
	step [147/192], loss=92.9598
	step [148/192], loss=87.7659
	step [149/192], loss=88.6749
	step [150/192], loss=77.8088
	step [151/192], loss=74.5037
	step [152/192], loss=78.7155
	step [153/192], loss=80.6656
	step [154/192], loss=84.8303
	step [155/192], loss=90.7148
	step [156/192], loss=85.5352
	step [157/192], loss=80.0303
	step [158/192], loss=69.4849
	step [159/192], loss=72.6050
	step [160/192], loss=92.4556
	step [161/192], loss=96.8502
	step [162/192], loss=83.5651
	step [163/192], loss=78.4311
	step [164/192], loss=94.8804
	step [165/192], loss=102.9706
	step [166/192], loss=82.2930
	step [167/192], loss=95.6006
	step [168/192], loss=100.5591
	step [169/192], loss=95.7160
	step [170/192], loss=87.1787
	step [171/192], loss=73.5381
	step [172/192], loss=96.6967
	step [173/192], loss=85.2726
	step [174/192], loss=79.1557
	step [175/192], loss=98.8151
	step [176/192], loss=101.1618
	step [177/192], loss=94.9097
	step [178/192], loss=77.8517
	step [179/192], loss=76.6556
	step [180/192], loss=73.7181
	step [181/192], loss=110.1438
	step [182/192], loss=77.1487
	step [183/192], loss=74.2705
	step [184/192], loss=95.1687
	step [185/192], loss=91.7241
	step [186/192], loss=87.9367
	step [187/192], loss=90.1121
	step [188/192], loss=78.9542
	step [189/192], loss=96.5636
	step [190/192], loss=91.7606
	step [191/192], loss=84.9585
	step [192/192], loss=8.5571
	Evaluating
	loss=0.0134, precision=0.3112, recall=0.9195, f1=0.4650
Training epoch 24
	step [1/192], loss=80.6056
	step [2/192], loss=91.1654
	step [3/192], loss=104.3943
	step [4/192], loss=102.5805
	step [5/192], loss=99.0248
	step [6/192], loss=84.2099
	step [7/192], loss=81.1428
	step [8/192], loss=84.5494
	step [9/192], loss=80.0306
	step [10/192], loss=80.1704
	step [11/192], loss=72.5784
	step [12/192], loss=75.6599
	step [13/192], loss=79.9408
	step [14/192], loss=92.6216
	step [15/192], loss=108.2699
	step [16/192], loss=85.3544
	step [17/192], loss=95.9759
	step [18/192], loss=91.7525
	step [19/192], loss=91.6488
	step [20/192], loss=81.6335
	step [21/192], loss=93.5904
	step [22/192], loss=92.7243
	step [23/192], loss=86.7319
	step [24/192], loss=87.3248
	step [25/192], loss=91.8611
	step [26/192], loss=80.9537
	step [27/192], loss=88.1366
	step [28/192], loss=81.3474
	step [29/192], loss=74.8223
	step [30/192], loss=87.8583
	step [31/192], loss=86.1548
	step [32/192], loss=89.3797
	step [33/192], loss=79.1085
	step [34/192], loss=82.1652
	step [35/192], loss=85.9900
	step [36/192], loss=84.2601
	step [37/192], loss=86.2900
	step [38/192], loss=70.6488
	step [39/192], loss=82.2138
	step [40/192], loss=80.5977
	step [41/192], loss=83.3301
	step [42/192], loss=61.2082
	step [43/192], loss=87.4321
	step [44/192], loss=92.8094
	step [45/192], loss=83.6174
	step [46/192], loss=92.0082
	step [47/192], loss=81.3055
	step [48/192], loss=80.6305
	step [49/192], loss=72.2255
	step [50/192], loss=90.9002
	step [51/192], loss=72.7837
	step [52/192], loss=91.7970
	step [53/192], loss=84.6171
	step [54/192], loss=92.1303
	step [55/192], loss=78.5451
	step [56/192], loss=87.1856
	step [57/192], loss=63.7011
	step [58/192], loss=84.2841
	step [59/192], loss=78.5056
	step [60/192], loss=86.8499
	step [61/192], loss=93.0230
	step [62/192], loss=80.0112
	step [63/192], loss=84.6352
	step [64/192], loss=97.6003
	step [65/192], loss=66.6188
	step [66/192], loss=83.8871
	step [67/192], loss=90.7736
	step [68/192], loss=89.5251
	step [69/192], loss=79.6634
	step [70/192], loss=82.4803
	step [71/192], loss=95.7178
	step [72/192], loss=78.4817
	step [73/192], loss=89.6212
	step [74/192], loss=87.2318
	step [75/192], loss=84.1691
	step [76/192], loss=84.4602
	step [77/192], loss=84.2992
	step [78/192], loss=92.9791
	step [79/192], loss=95.2703
	step [80/192], loss=76.2975
	step [81/192], loss=83.4783
	step [82/192], loss=83.2435
	step [83/192], loss=79.1518
	step [84/192], loss=81.2775
	step [85/192], loss=84.5437
	step [86/192], loss=75.9373
	step [87/192], loss=89.0544
	step [88/192], loss=84.9405
	step [89/192], loss=102.2963
	step [90/192], loss=83.7405
	step [91/192], loss=93.7408
	step [92/192], loss=87.6375
	step [93/192], loss=91.6339
	step [94/192], loss=91.4161
	step [95/192], loss=89.5495
	step [96/192], loss=88.6089
	step [97/192], loss=79.7971
	step [98/192], loss=77.8944
	step [99/192], loss=90.9573
	step [100/192], loss=84.5548
	step [101/192], loss=79.6710
	step [102/192], loss=89.9338
	step [103/192], loss=87.0743
	step [104/192], loss=103.8035
	step [105/192], loss=89.2346
	step [106/192], loss=91.8432
	step [107/192], loss=81.5556
	step [108/192], loss=81.2377
	step [109/192], loss=88.2818
	step [110/192], loss=87.0739
	step [111/192], loss=91.2730
	step [112/192], loss=90.3767
	step [113/192], loss=88.3455
	step [114/192], loss=81.5846
	step [115/192], loss=81.9797
	step [116/192], loss=87.0525
	step [117/192], loss=81.5096
	step [118/192], loss=86.6670
	step [119/192], loss=80.1821
	step [120/192], loss=80.2511
	step [121/192], loss=89.5584
	step [122/192], loss=77.6170
	step [123/192], loss=84.7288
	step [124/192], loss=90.0579
	step [125/192], loss=74.3736
	step [126/192], loss=86.2845
	step [127/192], loss=82.3106
	step [128/192], loss=95.3655
	step [129/192], loss=85.5807
	step [130/192], loss=80.0660
	step [131/192], loss=87.5031
	step [132/192], loss=77.6799
	step [133/192], loss=90.3490
	step [134/192], loss=87.3814
	step [135/192], loss=68.1724
	step [136/192], loss=87.6278
	step [137/192], loss=80.6488
	step [138/192], loss=78.0950
	step [139/192], loss=85.8448
	step [140/192], loss=83.6956
	step [141/192], loss=94.6914
	step [142/192], loss=90.1141
	step [143/192], loss=97.2458
	step [144/192], loss=89.3114
	step [145/192], loss=75.2132
	step [146/192], loss=92.0750
	step [147/192], loss=81.5779
	step [148/192], loss=86.9871
	step [149/192], loss=87.6195
	step [150/192], loss=85.4110
	step [151/192], loss=82.1136
	step [152/192], loss=87.8292
	step [153/192], loss=82.0658
	step [154/192], loss=88.0258
	step [155/192], loss=98.3314
	step [156/192], loss=78.8353
	step [157/192], loss=71.9800
	step [158/192], loss=87.1118
	step [159/192], loss=85.6280
	step [160/192], loss=87.7728
	step [161/192], loss=80.5828
	step [162/192], loss=85.9879
	step [163/192], loss=93.2568
	step [164/192], loss=104.1002
	step [165/192], loss=94.8344
	step [166/192], loss=77.1330
	step [167/192], loss=91.5947
	step [168/192], loss=68.6430
	step [169/192], loss=89.9554
	step [170/192], loss=78.5849
	step [171/192], loss=69.8184
	step [172/192], loss=70.8551
	step [173/192], loss=84.8415
	step [174/192], loss=84.3371
	step [175/192], loss=92.1546
	step [176/192], loss=84.6222
	step [177/192], loss=82.4690
	step [178/192], loss=94.1027
	step [179/192], loss=71.5216
	step [180/192], loss=83.2301
	step [181/192], loss=94.5385
	step [182/192], loss=95.3364
	step [183/192], loss=95.2415
	step [184/192], loss=85.7538
	step [185/192], loss=83.8039
	step [186/192], loss=87.8747
	step [187/192], loss=86.9124
	step [188/192], loss=84.4578
	step [189/192], loss=95.0745
	step [190/192], loss=83.6821
	step [191/192], loss=80.4354
	step [192/192], loss=12.6505
	Evaluating
	loss=0.0089, precision=0.4286, recall=0.9234, f1=0.5854
Training epoch 25
	step [1/192], loss=81.4991
	step [2/192], loss=86.5885
	step [3/192], loss=74.9161
	step [4/192], loss=93.4902
	step [5/192], loss=89.8152
	step [6/192], loss=86.0582
	step [7/192], loss=90.5604
	step [8/192], loss=79.2342
	step [9/192], loss=96.7153
	step [10/192], loss=85.1816
	step [11/192], loss=95.8755
	step [12/192], loss=90.2979
	step [13/192], loss=79.6418
	step [14/192], loss=98.6254
	step [15/192], loss=80.9169
	step [16/192], loss=86.5688
	step [17/192], loss=82.9437
	step [18/192], loss=69.4256
	step [19/192], loss=76.5291
	step [20/192], loss=70.2943
	step [21/192], loss=91.4672
	step [22/192], loss=78.0755
	step [23/192], loss=85.7296
	step [24/192], loss=77.7011
	step [25/192], loss=86.6732
	step [26/192], loss=86.8486
	step [27/192], loss=65.9684
	step [28/192], loss=76.6453
	step [29/192], loss=73.9742
	step [30/192], loss=82.4925
	step [31/192], loss=86.0373
	step [32/192], loss=79.7174
	step [33/192], loss=103.2967
	step [34/192], loss=78.3248
	step [35/192], loss=82.8456
	step [36/192], loss=79.9628
	step [37/192], loss=92.8848
	step [38/192], loss=83.7055
	step [39/192], loss=73.3978
	step [40/192], loss=79.9978
	step [41/192], loss=75.8015
	step [42/192], loss=74.6095
	step [43/192], loss=82.9519
	step [44/192], loss=77.2129
	step [45/192], loss=91.4532
	step [46/192], loss=79.0234
	step [47/192], loss=71.9332
	step [48/192], loss=103.5900
	step [49/192], loss=81.0406
	step [50/192], loss=74.2195
	step [51/192], loss=76.5294
	step [52/192], loss=81.6975
	step [53/192], loss=93.0093
	step [54/192], loss=78.3491
	step [55/192], loss=87.0807
	step [56/192], loss=95.7274
	step [57/192], loss=83.3706
	step [58/192], loss=72.0508
	step [59/192], loss=61.7140
	step [60/192], loss=80.8773
	step [61/192], loss=84.8357
	step [62/192], loss=86.7325
	step [63/192], loss=69.0818
	step [64/192], loss=90.4931
	step [65/192], loss=83.3507
	step [66/192], loss=84.4743
	step [67/192], loss=84.5052
	step [68/192], loss=74.3973
	step [69/192], loss=93.0144
	step [70/192], loss=73.8753
	step [71/192], loss=92.5344
	step [72/192], loss=86.7018
	step [73/192], loss=95.2124
	step [74/192], loss=65.8959
	step [75/192], loss=93.1427
	step [76/192], loss=77.7595
	step [77/192], loss=78.3952
	step [78/192], loss=84.7878
	step [79/192], loss=78.3457
	step [80/192], loss=91.1611
	step [81/192], loss=100.3636
	step [82/192], loss=102.5060
	step [83/192], loss=69.6662
	step [84/192], loss=78.9269
	step [85/192], loss=84.1854
	step [86/192], loss=78.6785
	step [87/192], loss=86.0288
	step [88/192], loss=92.3854
	step [89/192], loss=74.6469
	step [90/192], loss=87.4596
	step [91/192], loss=80.9625
	step [92/192], loss=85.9033
	step [93/192], loss=83.7950
	step [94/192], loss=71.0585
	step [95/192], loss=82.8831
	step [96/192], loss=103.3904
	step [97/192], loss=82.3505
	step [98/192], loss=100.5808
	step [99/192], loss=95.1224
	step [100/192], loss=88.2214
	step [101/192], loss=82.0423
	step [102/192], loss=80.0785
	step [103/192], loss=73.1068
	step [104/192], loss=79.9087
	step [105/192], loss=84.5576
	step [106/192], loss=81.8292
	step [107/192], loss=81.9369
	step [108/192], loss=78.5859
	step [109/192], loss=95.5156
	step [110/192], loss=91.7592
	step [111/192], loss=82.8340
	step [112/192], loss=86.0717
	step [113/192], loss=86.1750
	step [114/192], loss=83.8990
	step [115/192], loss=104.6263
	step [116/192], loss=90.6824
	step [117/192], loss=87.8269
	step [118/192], loss=90.6602
	step [119/192], loss=75.2564
	step [120/192], loss=78.2637
	step [121/192], loss=65.2208
	step [122/192], loss=83.9342
	step [123/192], loss=88.7569
	step [124/192], loss=71.5063
	step [125/192], loss=91.2221
	step [126/192], loss=96.3004
	step [127/192], loss=82.8627
	step [128/192], loss=92.8894
	step [129/192], loss=81.8102
	step [130/192], loss=99.8705
	step [131/192], loss=73.8142
	step [132/192], loss=73.1607
	step [133/192], loss=98.6639
	step [134/192], loss=98.2787
	step [135/192], loss=95.9105
	step [136/192], loss=93.5898
	step [137/192], loss=73.7125
	step [138/192], loss=86.9183
	step [139/192], loss=73.9043
	step [140/192], loss=65.7106
	step [141/192], loss=87.7388
	step [142/192], loss=75.7497
	step [143/192], loss=101.5278
	step [144/192], loss=78.0290
	step [145/192], loss=81.5360
	step [146/192], loss=77.2585
	step [147/192], loss=84.7823
	step [148/192], loss=95.5353
	step [149/192], loss=87.8580
	step [150/192], loss=83.5605
	step [151/192], loss=97.8223
	step [152/192], loss=97.7140
	step [153/192], loss=83.4616
	step [154/192], loss=85.8432
	step [155/192], loss=85.8064
	step [156/192], loss=78.9004
	step [157/192], loss=87.8373
	step [158/192], loss=83.1049
	step [159/192], loss=79.8239
	step [160/192], loss=95.0252
	step [161/192], loss=101.0149
	step [162/192], loss=85.6034
	step [163/192], loss=99.8940
	step [164/192], loss=81.1327
	step [165/192], loss=95.6226
	step [166/192], loss=85.5665
	step [167/192], loss=78.7650
	step [168/192], loss=95.7005
	step [169/192], loss=75.8097
	step [170/192], loss=84.6754
	step [171/192], loss=86.3059
	step [172/192], loss=89.3356
	step [173/192], loss=91.2233
	step [174/192], loss=72.9369
	step [175/192], loss=82.6073
	step [176/192], loss=87.9090
	step [177/192], loss=120.4481
	step [178/192], loss=87.8883
	step [179/192], loss=80.7656
	step [180/192], loss=99.8118
	step [181/192], loss=82.7701
	step [182/192], loss=88.0916
	step [183/192], loss=68.2521
	step [184/192], loss=73.9605
	step [185/192], loss=82.5748
	step [186/192], loss=86.9101
	step [187/192], loss=65.6620
	step [188/192], loss=86.1874
	step [189/192], loss=78.1824
	step [190/192], loss=94.4124
	step [191/192], loss=89.2402
	step [192/192], loss=7.6866
	Evaluating
	loss=0.0095, precision=0.3687, recall=0.9041, f1=0.5238
Training epoch 26
	step [1/192], loss=81.2637
	step [2/192], loss=82.7845
	step [3/192], loss=87.7239
	step [4/192], loss=83.2272
	step [5/192], loss=78.6555
	step [6/192], loss=83.8867
	step [7/192], loss=96.3007
	step [8/192], loss=82.1930
	step [9/192], loss=75.1347
	step [10/192], loss=76.3654
	step [11/192], loss=103.9835
	step [12/192], loss=96.4606
	step [13/192], loss=88.3185
	step [14/192], loss=80.5714
	step [15/192], loss=84.4453
	step [16/192], loss=84.1147
	step [17/192], loss=103.2687
	step [18/192], loss=86.1980
	step [19/192], loss=76.1109
	step [20/192], loss=65.5095
	step [21/192], loss=90.4790
	step [22/192], loss=75.8842
	step [23/192], loss=75.1098
	step [24/192], loss=85.5298
	step [25/192], loss=96.6396
	step [26/192], loss=98.2014
	step [27/192], loss=80.9332
	step [28/192], loss=88.0121
	step [29/192], loss=80.9518
	step [30/192], loss=79.8959
	step [31/192], loss=90.1631
	step [32/192], loss=104.7622
	step [33/192], loss=96.3804
	step [34/192], loss=77.3900
	step [35/192], loss=80.9747
	step [36/192], loss=95.9213
	step [37/192], loss=73.7511
	step [38/192], loss=74.8042
	step [39/192], loss=65.5200
	step [40/192], loss=89.9494
	step [41/192], loss=78.1771
	step [42/192], loss=90.4242
	step [43/192], loss=94.9908
	step [44/192], loss=83.7435
	step [45/192], loss=83.3774
	step [46/192], loss=72.4179
	step [47/192], loss=80.9716
	step [48/192], loss=102.9399
	step [49/192], loss=85.7721
	step [50/192], loss=83.4204
	step [51/192], loss=72.0151
	step [52/192], loss=81.1857
	step [53/192], loss=80.3008
	step [54/192], loss=91.5986
	step [55/192], loss=102.4159
	step [56/192], loss=97.1822
	step [57/192], loss=79.5776
	step [58/192], loss=88.2214
	step [59/192], loss=89.7514
	step [60/192], loss=70.6672
	step [61/192], loss=85.5525
	step [62/192], loss=92.5340
	step [63/192], loss=83.6066
	step [64/192], loss=82.2903
	step [65/192], loss=97.8016
	step [66/192], loss=85.1458
	step [67/192], loss=73.8347
	step [68/192], loss=103.0764
	step [69/192], loss=90.9811
	step [70/192], loss=78.4246
	step [71/192], loss=86.2724
	step [72/192], loss=91.9967
	step [73/192], loss=87.6434
	step [74/192], loss=87.4761
	step [75/192], loss=73.4386
	step [76/192], loss=92.9060
	step [77/192], loss=74.7393
	step [78/192], loss=83.8966
	step [79/192], loss=92.2758
	step [80/192], loss=81.0443
	step [81/192], loss=67.7138
	step [82/192], loss=74.5875
	step [83/192], loss=93.6304
	step [84/192], loss=79.0820
	step [85/192], loss=96.7761
	step [86/192], loss=83.5826
	step [87/192], loss=84.1984
	step [88/192], loss=73.7826
	step [89/192], loss=72.3069
	step [90/192], loss=80.6404
	step [91/192], loss=83.7649
	step [92/192], loss=80.6160
	step [93/192], loss=78.8788
	step [94/192], loss=60.7650
	step [95/192], loss=84.8806
	step [96/192], loss=74.6441
	step [97/192], loss=81.1562
	step [98/192], loss=81.6535
	step [99/192], loss=76.6951
	step [100/192], loss=81.6686
	step [101/192], loss=82.6504
	step [102/192], loss=76.6066
	step [103/192], loss=83.5313
	step [104/192], loss=79.5024
	step [105/192], loss=68.3724
	step [106/192], loss=91.1844
	step [107/192], loss=85.7518
	step [108/192], loss=74.8182
	step [109/192], loss=82.0150
	step [110/192], loss=89.0829
	step [111/192], loss=76.8610
	step [112/192], loss=83.6238
	step [113/192], loss=91.8870
	step [114/192], loss=90.9847
	step [115/192], loss=85.7561
	step [116/192], loss=91.6736
	step [117/192], loss=93.0882
	step [118/192], loss=72.3455
	step [119/192], loss=68.4658
	step [120/192], loss=68.6245
	step [121/192], loss=69.6839
	step [122/192], loss=86.0151
	step [123/192], loss=85.0426
	step [124/192], loss=95.4700
	step [125/192], loss=87.4601
	step [126/192], loss=81.5779
	step [127/192], loss=78.7751
	step [128/192], loss=95.3884
	step [129/192], loss=80.7660
	step [130/192], loss=73.9174
	step [131/192], loss=93.8547
	step [132/192], loss=81.0247
	step [133/192], loss=84.7892
	step [134/192], loss=81.0114
	step [135/192], loss=92.5505
	step [136/192], loss=76.1439
	step [137/192], loss=82.2752
	step [138/192], loss=83.3166
	step [139/192], loss=80.3034
	step [140/192], loss=77.2348
	step [141/192], loss=92.6020
	step [142/192], loss=80.4327
	step [143/192], loss=70.3076
	step [144/192], loss=79.0618
	step [145/192], loss=84.8422
	step [146/192], loss=91.2888
	step [147/192], loss=89.0770
	step [148/192], loss=75.9162
	step [149/192], loss=85.8427
	step [150/192], loss=73.2053
	step [151/192], loss=93.3492
	step [152/192], loss=79.2564
	step [153/192], loss=83.8262
	step [154/192], loss=71.3953
	step [155/192], loss=70.1337
	step [156/192], loss=77.0642
	step [157/192], loss=88.0611
	step [158/192], loss=102.3183
	step [159/192], loss=80.8010
	step [160/192], loss=76.0898
	step [161/192], loss=88.5083
	step [162/192], loss=82.0560
	step [163/192], loss=79.7493
	step [164/192], loss=80.1636
	step [165/192], loss=89.2982
	step [166/192], loss=84.1460
	step [167/192], loss=83.0212
	step [168/192], loss=80.9777
	step [169/192], loss=88.2798
	step [170/192], loss=92.8829
	step [171/192], loss=80.9366
	step [172/192], loss=81.6453
	step [173/192], loss=69.7816
	step [174/192], loss=85.0320
	step [175/192], loss=76.4362
	step [176/192], loss=79.5975
	step [177/192], loss=69.1316
	step [178/192], loss=64.0627
	step [179/192], loss=80.6601
	step [180/192], loss=98.1031
	step [181/192], loss=81.1435
	step [182/192], loss=80.3745
	step [183/192], loss=87.5041
	step [184/192], loss=85.5951
	step [185/192], loss=95.0172
	step [186/192], loss=84.1756
	step [187/192], loss=101.0238
	step [188/192], loss=90.1523
	step [189/192], loss=90.7323
	step [190/192], loss=90.4408
	step [191/192], loss=79.2223
	step [192/192], loss=12.1305
	Evaluating
	loss=0.0093, precision=0.3695, recall=0.8989, f1=0.5237
Training epoch 27
	step [1/192], loss=103.8414
	step [2/192], loss=96.7239
	step [3/192], loss=73.9449
	step [4/192], loss=80.4243
	step [5/192], loss=86.5686
	step [6/192], loss=82.3364
	step [7/192], loss=86.2931
	step [8/192], loss=75.0410
	step [9/192], loss=67.9182
	step [10/192], loss=83.7471
	step [11/192], loss=74.5969
	step [12/192], loss=92.6919
	step [13/192], loss=88.8594
	step [14/192], loss=73.7912
	step [15/192], loss=81.3388
	step [16/192], loss=80.4644
	step [17/192], loss=85.6387
	step [18/192], loss=73.8808
	step [19/192], loss=74.2037
	step [20/192], loss=77.6242
	step [21/192], loss=72.7962
	step [22/192], loss=81.8002
	step [23/192], loss=102.0271
	step [24/192], loss=80.6821
	step [25/192], loss=80.2746
	step [26/192], loss=78.0276
	step [27/192], loss=94.0122
	step [28/192], loss=80.6072
	step [29/192], loss=93.8820
	step [30/192], loss=77.3908
	step [31/192], loss=85.5361
	step [32/192], loss=83.7152
	step [33/192], loss=65.0915
	step [34/192], loss=74.8097
	step [35/192], loss=91.5145
	step [36/192], loss=90.0030
	step [37/192], loss=69.7952
	step [38/192], loss=86.4742
	step [39/192], loss=84.7739
	step [40/192], loss=91.5712
	step [41/192], loss=80.7501
	step [42/192], loss=81.5685
	step [43/192], loss=81.0938
	step [44/192], loss=76.1466
	step [45/192], loss=79.7755
	step [46/192], loss=89.3263
	step [47/192], loss=84.3160
	step [48/192], loss=78.6057
	step [49/192], loss=79.8449
	step [50/192], loss=81.1864
	step [51/192], loss=90.4654
	step [52/192], loss=72.5511
	step [53/192], loss=70.4959
	step [54/192], loss=75.2598
	step [55/192], loss=73.1246
	step [56/192], loss=90.6624
	step [57/192], loss=83.2319
	step [58/192], loss=68.4903
	step [59/192], loss=96.1950
	step [60/192], loss=83.6183
	step [61/192], loss=77.6822
	step [62/192], loss=79.0259
	step [63/192], loss=88.3922
	step [64/192], loss=70.6442
	step [65/192], loss=72.4706
	step [66/192], loss=74.7181
	step [67/192], loss=82.4339
	step [68/192], loss=92.5893
	step [69/192], loss=76.8246
	step [70/192], loss=82.3045
	step [71/192], loss=88.5540
	step [72/192], loss=69.3402
	step [73/192], loss=105.0163
	step [74/192], loss=83.7504
	step [75/192], loss=85.6531
	step [76/192], loss=83.4729
	step [77/192], loss=105.5126
	step [78/192], loss=75.8859
	step [79/192], loss=86.6576
	step [80/192], loss=91.9697
	step [81/192], loss=84.8325
	step [82/192], loss=86.7062
	step [83/192], loss=86.8494
	step [84/192], loss=78.7378
	step [85/192], loss=80.9580
	step [86/192], loss=92.4655
	step [87/192], loss=89.1073
	step [88/192], loss=83.9472
	step [89/192], loss=84.2907
	step [90/192], loss=83.9428
	step [91/192], loss=78.5754
	step [92/192], loss=89.3739
	step [93/192], loss=78.2086
	step [94/192], loss=92.1380
	step [95/192], loss=82.8247
	step [96/192], loss=68.9275
	step [97/192], loss=67.5228
	step [98/192], loss=75.9301
	step [99/192], loss=74.5596
	step [100/192], loss=92.0402
	step [101/192], loss=90.7218
	step [102/192], loss=77.0233
	step [103/192], loss=75.3083
	step [104/192], loss=76.9888
	step [105/192], loss=80.0303
	step [106/192], loss=70.7287
	step [107/192], loss=100.9920
	step [108/192], loss=75.1453
	step [109/192], loss=80.0180
	step [110/192], loss=86.5379
	step [111/192], loss=104.6769
	step [112/192], loss=74.9102
	step [113/192], loss=78.8704
	step [114/192], loss=82.4786
	step [115/192], loss=85.2278
	step [116/192], loss=80.2886
	step [117/192], loss=97.6839
	step [118/192], loss=84.3087
	step [119/192], loss=61.2416
	step [120/192], loss=70.1654
	step [121/192], loss=94.6776
	step [122/192], loss=78.9960
	step [123/192], loss=90.6042
	step [124/192], loss=79.8763
	step [125/192], loss=87.1044
	step [126/192], loss=68.2431
	step [127/192], loss=95.7257
	step [128/192], loss=91.5768
	step [129/192], loss=80.3219
	step [130/192], loss=87.4131
	step [131/192], loss=72.0099
	step [132/192], loss=78.4831
	step [133/192], loss=70.9720
	step [134/192], loss=76.5761
	step [135/192], loss=80.1959
	step [136/192], loss=88.2402
	step [137/192], loss=86.7715
	step [138/192], loss=74.6387
	step [139/192], loss=79.9968
	step [140/192], loss=93.6265
	step [141/192], loss=95.3213
	step [142/192], loss=83.7441
	step [143/192], loss=80.2482
	step [144/192], loss=79.8273
	step [145/192], loss=74.6682
	step [146/192], loss=91.8211
	step [147/192], loss=74.0988
	step [148/192], loss=79.7893
	step [149/192], loss=81.7781
	step [150/192], loss=92.4176
	step [151/192], loss=85.9927
	step [152/192], loss=90.9053
	step [153/192], loss=81.7616
	step [154/192], loss=81.1930
	step [155/192], loss=92.5386
	step [156/192], loss=77.2679
	step [157/192], loss=77.3633
	step [158/192], loss=100.7128
	step [159/192], loss=89.1094
	step [160/192], loss=94.8424
	step [161/192], loss=70.6525
	step [162/192], loss=81.2432
	step [163/192], loss=86.6694
	step [164/192], loss=77.6836
	step [165/192], loss=100.3134
	step [166/192], loss=100.3300
	step [167/192], loss=61.9967
	step [168/192], loss=74.4571
	step [169/192], loss=75.7028
	step [170/192], loss=77.3660
	step [171/192], loss=94.8161
	step [172/192], loss=84.6683
	step [173/192], loss=75.9859
	step [174/192], loss=82.0523
	step [175/192], loss=85.9252
	step [176/192], loss=75.5465
	step [177/192], loss=81.6907
	step [178/192], loss=78.1886
	step [179/192], loss=72.3516
	step [180/192], loss=84.5999
	step [181/192], loss=92.4012
	step [182/192], loss=90.6456
	step [183/192], loss=83.9442
	step [184/192], loss=83.5116
	step [185/192], loss=81.8226
	step [186/192], loss=85.0442
	step [187/192], loss=70.3318
	step [188/192], loss=85.3296
	step [189/192], loss=84.4249
	step [190/192], loss=96.8170
	step [191/192], loss=81.4525
	step [192/192], loss=8.0102
	Evaluating
	loss=0.0080, precision=0.4488, recall=0.8829, f1=0.5951
Training epoch 28
	step [1/192], loss=73.9657
	step [2/192], loss=81.4619
	step [3/192], loss=79.8343
	step [4/192], loss=81.2123
	step [5/192], loss=98.0699
	step [6/192], loss=94.4947
	step [7/192], loss=84.8033
	step [8/192], loss=94.4596
	step [9/192], loss=71.5163
	step [10/192], loss=94.5669
	step [11/192], loss=88.4573
	step [12/192], loss=84.8818
	step [13/192], loss=85.1796
	step [14/192], loss=76.8716
	step [15/192], loss=73.3215
	step [16/192], loss=74.6048
	step [17/192], loss=74.0341
	step [18/192], loss=85.1480
	step [19/192], loss=72.7218
	step [20/192], loss=88.8210
	step [21/192], loss=77.2974
	step [22/192], loss=76.5819
	step [23/192], loss=82.9256
	step [24/192], loss=90.7070
	step [25/192], loss=92.7288
	step [26/192], loss=93.2541
	step [27/192], loss=82.5237
	step [28/192], loss=93.4901
	step [29/192], loss=73.4830
	step [30/192], loss=89.6856
	step [31/192], loss=74.6601
	step [32/192], loss=81.7712
	step [33/192], loss=80.3408
	step [34/192], loss=85.2558
	step [35/192], loss=79.5107
	step [36/192], loss=76.6740
	step [37/192], loss=89.7094
	step [38/192], loss=76.1054
	step [39/192], loss=82.8691
	step [40/192], loss=90.8123
	step [41/192], loss=80.7792
	step [42/192], loss=72.5126
	step [43/192], loss=79.3000
	step [44/192], loss=71.0502
	step [45/192], loss=78.6710
	step [46/192], loss=81.3662
	step [47/192], loss=69.6704
	step [48/192], loss=82.2865
	step [49/192], loss=94.9011
	step [50/192], loss=93.5466
	step [51/192], loss=75.1085
	step [52/192], loss=97.5889
	step [53/192], loss=71.1028
	step [54/192], loss=75.7844
	step [55/192], loss=79.0972
	step [56/192], loss=80.1719
	step [57/192], loss=84.4146
	step [58/192], loss=73.9398
	step [59/192], loss=91.1420
	step [60/192], loss=70.3445
	step [61/192], loss=79.8578
	step [62/192], loss=92.7547
	step [63/192], loss=93.1427
	step [64/192], loss=81.0175
	step [65/192], loss=81.7371
	step [66/192], loss=79.8323
	step [67/192], loss=89.8049
	step [68/192], loss=76.7902
	step [69/192], loss=90.2072
	step [70/192], loss=88.4492
	step [71/192], loss=68.1705
	step [72/192], loss=79.4159
	step [73/192], loss=82.4034
	step [74/192], loss=67.6480
	step [75/192], loss=85.8129
	step [76/192], loss=78.0121
	step [77/192], loss=75.6830
	step [78/192], loss=61.6306
	step [79/192], loss=74.5006
	step [80/192], loss=84.2528
	step [81/192], loss=96.6387
	step [82/192], loss=75.7638
	step [83/192], loss=72.4747
	step [84/192], loss=87.0599
	step [85/192], loss=96.3857
	step [86/192], loss=83.6890
	step [87/192], loss=96.5218
	step [88/192], loss=74.2235
	step [89/192], loss=84.9941
	step [90/192], loss=89.2227
	step [91/192], loss=74.8547
	step [92/192], loss=77.7169
	step [93/192], loss=83.8288
	step [94/192], loss=79.5878
	step [95/192], loss=85.3096
	step [96/192], loss=76.0194
	step [97/192], loss=82.1563
	step [98/192], loss=79.7146
	step [99/192], loss=78.7695
	step [100/192], loss=68.9100
	step [101/192], loss=79.4784
	step [102/192], loss=80.9065
	step [103/192], loss=83.3677
	step [104/192], loss=90.2533
	step [105/192], loss=78.9073
	step [106/192], loss=75.3255
	step [107/192], loss=77.1373
	step [108/192], loss=82.0824
	step [109/192], loss=82.9910
	step [110/192], loss=82.3314
	step [111/192], loss=68.3333
	step [112/192], loss=69.7988
	step [113/192], loss=86.3572
	step [114/192], loss=79.3317
	step [115/192], loss=84.9028
	step [116/192], loss=95.7975
	step [117/192], loss=78.9261
	step [118/192], loss=78.6682
	step [119/192], loss=90.8049
	step [120/192], loss=77.1053
	step [121/192], loss=76.2865
	step [122/192], loss=81.7715
	step [123/192], loss=70.6363
	step [124/192], loss=91.5592
	step [125/192], loss=62.9453
	step [126/192], loss=69.1269
	step [127/192], loss=91.0946
	step [128/192], loss=85.0901
	step [129/192], loss=80.1746
	step [130/192], loss=71.4880
	step [131/192], loss=78.1546
	step [132/192], loss=69.7959
	step [133/192], loss=96.2685
	step [134/192], loss=79.4019
	step [135/192], loss=90.9832
	step [136/192], loss=92.3362
	step [137/192], loss=71.8431
	step [138/192], loss=79.6102
	step [139/192], loss=85.5182
	step [140/192], loss=77.8681
	step [141/192], loss=91.5165
	step [142/192], loss=70.9966
	step [143/192], loss=85.8314
	step [144/192], loss=77.6615
	step [145/192], loss=79.9595
	step [146/192], loss=82.6421
	step [147/192], loss=69.8213
	step [148/192], loss=71.9957
	step [149/192], loss=70.8582
	step [150/192], loss=90.0288
	step [151/192], loss=81.4227
	step [152/192], loss=83.0059
	step [153/192], loss=78.2879
	step [154/192], loss=86.8481
	step [155/192], loss=74.2664
	step [156/192], loss=66.3067
	step [157/192], loss=102.5108
	step [158/192], loss=81.2846
	step [159/192], loss=71.7402
	step [160/192], loss=95.8904
	step [161/192], loss=89.6176
	step [162/192], loss=72.5935
	step [163/192], loss=84.4336
	step [164/192], loss=73.0027
	step [165/192], loss=78.7795
	step [166/192], loss=73.5205
	step [167/192], loss=84.4227
	step [168/192], loss=83.5520
	step [169/192], loss=79.6250
	step [170/192], loss=78.4356
	step [171/192], loss=79.9561
	step [172/192], loss=84.2185
	step [173/192], loss=70.3934
	step [174/192], loss=86.8766
	step [175/192], loss=85.9677
	step [176/192], loss=86.1331
	step [177/192], loss=80.3835
	step [178/192], loss=79.9217
	step [179/192], loss=89.3575
	step [180/192], loss=78.2379
	step [181/192], loss=87.8694
	step [182/192], loss=86.2602
	step [183/192], loss=82.6924
	step [184/192], loss=89.2059
	step [185/192], loss=72.5708
	step [186/192], loss=65.6747
	step [187/192], loss=87.3982
	step [188/192], loss=90.5361
	step [189/192], loss=88.4653
	step [190/192], loss=88.1090
	step [191/192], loss=72.8056
	step [192/192], loss=9.8573
	Evaluating
	loss=0.0105, precision=0.3266, recall=0.9065, f1=0.4802
Training epoch 29
	step [1/192], loss=97.1463
	step [2/192], loss=91.4042
	step [3/192], loss=82.2834
	step [4/192], loss=68.6567
	step [5/192], loss=85.2772
	step [6/192], loss=84.1246
	step [7/192], loss=82.4472
	step [8/192], loss=81.8058
	step [9/192], loss=77.1574
	step [10/192], loss=82.8828
	step [11/192], loss=77.8034
	step [12/192], loss=88.1401
	step [13/192], loss=84.3162
	step [14/192], loss=74.6770
	step [15/192], loss=90.1586
	step [16/192], loss=84.8384
	step [17/192], loss=86.5768
	step [18/192], loss=69.9845
	step [19/192], loss=80.1627
	step [20/192], loss=77.4864
	step [21/192], loss=72.2924
	step [22/192], loss=91.9242
	step [23/192], loss=84.7274
	step [24/192], loss=98.6655
	step [25/192], loss=94.7435
	step [26/192], loss=90.2995
	step [27/192], loss=77.8862
	step [28/192], loss=93.4302
	step [29/192], loss=80.1392
	step [30/192], loss=88.0046
	step [31/192], loss=90.9923
	step [32/192], loss=80.1879
	step [33/192], loss=74.7904
	step [34/192], loss=84.1062
	step [35/192], loss=75.9260
	step [36/192], loss=75.1371
	step [37/192], loss=92.1822
	step [38/192], loss=81.0958
	step [39/192], loss=82.0819
	step [40/192], loss=73.5964
	step [41/192], loss=70.3005
	step [42/192], loss=77.5028
	step [43/192], loss=81.1510
	step [44/192], loss=84.2059
	step [45/192], loss=84.3710
	step [46/192], loss=82.0890
	step [47/192], loss=63.3492
	step [48/192], loss=77.5641
	step [49/192], loss=75.1749
	step [50/192], loss=74.8366
	step [51/192], loss=78.3814
	step [52/192], loss=89.8205
	step [53/192], loss=88.9401
	step [54/192], loss=88.0999
	step [55/192], loss=88.1273
	step [56/192], loss=80.2243
	step [57/192], loss=86.7068
	step [58/192], loss=75.0242
	step [59/192], loss=85.2723
	step [60/192], loss=80.1414
	step [61/192], loss=76.5841
	step [62/192], loss=82.2756
	step [63/192], loss=76.2721
	step [64/192], loss=60.9693
	step [65/192], loss=63.0135
	step [66/192], loss=81.7085
	step [67/192], loss=81.5817
	step [68/192], loss=78.9945
	step [69/192], loss=95.2530
	step [70/192], loss=81.3881
	step [71/192], loss=90.3769
	step [72/192], loss=79.2668
	step [73/192], loss=75.8325
	step [74/192], loss=82.0919
	step [75/192], loss=84.2983
	step [76/192], loss=81.4875
	step [77/192], loss=73.9577
	step [78/192], loss=77.6749
	step [79/192], loss=83.8805
	step [80/192], loss=79.6729
	step [81/192], loss=80.8877
	step [82/192], loss=83.7417
	step [83/192], loss=79.7269
	step [84/192], loss=87.5436
	step [85/192], loss=71.8956
	step [86/192], loss=93.5739
	step [87/192], loss=79.1410
	step [88/192], loss=82.8862
	step [89/192], loss=73.8910
	step [90/192], loss=82.8651
	step [91/192], loss=79.9363
	step [92/192], loss=79.5184
	step [93/192], loss=87.5479
	step [94/192], loss=79.7879
	step [95/192], loss=76.1295
	step [96/192], loss=82.5907
	step [97/192], loss=79.6493
	step [98/192], loss=84.5887
	step [99/192], loss=96.2772
	step [100/192], loss=73.2958
	step [101/192], loss=75.3815
	step [102/192], loss=69.8072
	step [103/192], loss=90.9193
	step [104/192], loss=78.3305
	step [105/192], loss=83.5033
	step [106/192], loss=76.6293
	step [107/192], loss=102.0080
	step [108/192], loss=71.3780
	step [109/192], loss=82.3764
	step [110/192], loss=73.6889
	step [111/192], loss=72.0359
	step [112/192], loss=81.5377
	step [113/192], loss=84.2193
	step [114/192], loss=75.2050
	step [115/192], loss=65.7033
	step [116/192], loss=86.9022
	step [117/192], loss=82.6539
	step [118/192], loss=75.7848
	step [119/192], loss=73.6943
	step [120/192], loss=85.6446
	step [121/192], loss=74.3918
	step [122/192], loss=76.8609
	step [123/192], loss=99.1065
	step [124/192], loss=91.2882
	step [125/192], loss=75.5719
	step [126/192], loss=78.9961
	step [127/192], loss=82.0366
	step [128/192], loss=79.0115
	step [129/192], loss=75.4598
	step [130/192], loss=72.3578
	step [131/192], loss=79.1232
	step [132/192], loss=92.5992
	step [133/192], loss=81.5220
	step [134/192], loss=70.0968
	step [135/192], loss=59.8617
	step [136/192], loss=67.6768
	step [137/192], loss=75.7551
	step [138/192], loss=72.2893
	step [139/192], loss=85.8530
	step [140/192], loss=102.1223
	step [141/192], loss=90.0641
	step [142/192], loss=64.1440
	step [143/192], loss=87.6856
	step [144/192], loss=70.7122
	step [145/192], loss=80.1294
	step [146/192], loss=82.2383
	step [147/192], loss=78.0590
	step [148/192], loss=86.1735
	step [149/192], loss=88.6708
	step [150/192], loss=81.1075
	step [151/192], loss=82.3570
	step [152/192], loss=75.7848
	step [153/192], loss=69.7056
	step [154/192], loss=81.1584
	step [155/192], loss=72.3341
	step [156/192], loss=90.8550
	step [157/192], loss=72.8077
	step [158/192], loss=77.5026
	step [159/192], loss=78.3634
	step [160/192], loss=100.3584
	step [161/192], loss=90.1658
	step [162/192], loss=82.6299
	step [163/192], loss=72.4969
	step [164/192], loss=84.1165
	step [165/192], loss=87.3072
	step [166/192], loss=67.7023
	step [167/192], loss=67.6824
	step [168/192], loss=94.0468
	step [169/192], loss=70.8048
	step [170/192], loss=81.2828
	step [171/192], loss=67.2170
	step [172/192], loss=94.3890
	step [173/192], loss=67.8485
	step [174/192], loss=81.0537
	step [175/192], loss=75.5719
	step [176/192], loss=73.0099
	step [177/192], loss=88.7984
	step [178/192], loss=89.9209
	step [179/192], loss=90.6221
	step [180/192], loss=93.4045
	step [181/192], loss=72.2391
	step [182/192], loss=77.0911
	step [183/192], loss=77.3005
	step [184/192], loss=82.8724
	step [185/192], loss=89.8473
	step [186/192], loss=89.5131
	step [187/192], loss=72.8964
	step [188/192], loss=67.6709
	step [189/192], loss=85.4352
	step [190/192], loss=69.6138
	step [191/192], loss=71.5288
	step [192/192], loss=8.8833
	Evaluating
	loss=0.0093, precision=0.3452, recall=0.9031, f1=0.4995
Training epoch 30
	step [1/192], loss=74.9040
	step [2/192], loss=72.8366
	step [3/192], loss=77.8087
	step [4/192], loss=71.8196
	step [5/192], loss=81.5560
	step [6/192], loss=92.4050
	step [7/192], loss=76.4507
	step [8/192], loss=77.0619
	step [9/192], loss=91.0159
	step [10/192], loss=71.3220
	step [11/192], loss=85.9793
	step [12/192], loss=84.5873
	step [13/192], loss=81.1743
	step [14/192], loss=81.2839
	step [15/192], loss=75.9918
	step [16/192], loss=67.1274
	step [17/192], loss=61.7343
	step [18/192], loss=76.1859
	step [19/192], loss=77.4354
	step [20/192], loss=75.0938
	step [21/192], loss=90.2495
	step [22/192], loss=82.1583
	step [23/192], loss=81.6842
	step [24/192], loss=88.8190
	step [25/192], loss=74.3362
	step [26/192], loss=92.0923
	step [27/192], loss=74.0884
	step [28/192], loss=75.4858
	step [29/192], loss=70.4975
	step [30/192], loss=73.2317
	step [31/192], loss=74.7287
	step [32/192], loss=75.3190
	step [33/192], loss=82.6800
	step [34/192], loss=74.9714
	step [35/192], loss=71.8316
	step [36/192], loss=70.1032
	step [37/192], loss=78.6028
	step [38/192], loss=74.5292
	step [39/192], loss=85.3656
	step [40/192], loss=73.1273
	step [41/192], loss=83.8441
	step [42/192], loss=77.6085
	step [43/192], loss=86.8253
	step [44/192], loss=90.4769
	step [45/192], loss=84.8919
	step [46/192], loss=77.0625
	step [47/192], loss=90.9492
	step [48/192], loss=91.7387
	step [49/192], loss=74.2476
	step [50/192], loss=78.4884
	step [51/192], loss=61.7063
	step [52/192], loss=92.6089
	step [53/192], loss=79.1315
	step [54/192], loss=73.7618
	step [55/192], loss=81.3203
	step [56/192], loss=87.0310
	step [57/192], loss=94.0946
	step [58/192], loss=82.8039
	step [59/192], loss=80.0541
	step [60/192], loss=82.4568
	step [61/192], loss=79.7973
	step [62/192], loss=87.7016
	step [63/192], loss=72.3374
	step [64/192], loss=77.5985
	step [65/192], loss=83.7530
	step [66/192], loss=68.6563
	step [67/192], loss=81.3746
	step [68/192], loss=80.3515
	step [69/192], loss=88.4049
	step [70/192], loss=74.5033
	step [71/192], loss=70.9233
	step [72/192], loss=81.7060
	step [73/192], loss=85.1813
	step [74/192], loss=81.7768
	step [75/192], loss=69.2726
	step [76/192], loss=71.4922
	step [77/192], loss=79.3966
	step [78/192], loss=78.3735
	step [79/192], loss=89.4605
	step [80/192], loss=72.8328
	step [81/192], loss=86.7936
	step [82/192], loss=82.9489
	step [83/192], loss=79.1063
	step [84/192], loss=85.4737
	step [85/192], loss=98.8815
	step [86/192], loss=84.3999
	step [87/192], loss=84.6185
	step [88/192], loss=81.5310
	step [89/192], loss=95.4568
	step [90/192], loss=80.6454
	step [91/192], loss=76.2624
	step [92/192], loss=75.4711
	step [93/192], loss=88.9707
	step [94/192], loss=84.8805
	step [95/192], loss=104.0156
	step [96/192], loss=76.3818
	step [97/192], loss=72.9631
	step [98/192], loss=72.6157
	step [99/192], loss=83.1256
	step [100/192], loss=91.0516
	step [101/192], loss=69.6142
	step [102/192], loss=81.6165
	step [103/192], loss=77.0713
	step [104/192], loss=85.2738
	step [105/192], loss=78.7820
	step [106/192], loss=72.6409
	step [107/192], loss=78.8205
	step [108/192], loss=82.9688
	step [109/192], loss=83.1253
	step [110/192], loss=61.0240
	step [111/192], loss=70.9896
	step [112/192], loss=90.6099
	step [113/192], loss=85.8626
	step [114/192], loss=80.0684
	step [115/192], loss=81.8896
	step [116/192], loss=76.6027
	step [117/192], loss=81.0951
	step [118/192], loss=92.5211
	step [119/192], loss=82.1090
	step [120/192], loss=81.1118
	step [121/192], loss=84.8431
	step [122/192], loss=74.0827
	step [123/192], loss=70.0247
	step [124/192], loss=86.1697
	step [125/192], loss=82.5542
	step [126/192], loss=77.6648
	step [127/192], loss=79.5569
	step [128/192], loss=67.0603
	step [129/192], loss=84.8366
	step [130/192], loss=78.3738
	step [131/192], loss=87.9520
	step [132/192], loss=93.8959
	step [133/192], loss=65.3227
	step [134/192], loss=84.7198
	step [135/192], loss=79.9711
	step [136/192], loss=86.6375
	step [137/192], loss=85.2656
	step [138/192], loss=79.7291
	step [139/192], loss=81.1731
	step [140/192], loss=94.2908
	step [141/192], loss=69.0661
	step [142/192], loss=68.8551
	step [143/192], loss=76.5535
	step [144/192], loss=75.3620
	step [145/192], loss=67.0180
	step [146/192], loss=95.5242
	step [147/192], loss=87.2959
	step [148/192], loss=79.3175
	step [149/192], loss=73.0342
	step [150/192], loss=71.6348
	step [151/192], loss=83.4039
	step [152/192], loss=78.3041
	step [153/192], loss=78.8917
	step [154/192], loss=74.2304
	step [155/192], loss=79.0822
	step [156/192], loss=84.9550
	step [157/192], loss=78.4259
	step [158/192], loss=73.0450
	step [159/192], loss=67.4513
	step [160/192], loss=84.2759
	step [161/192], loss=86.0339
	step [162/192], loss=73.9042
	step [163/192], loss=92.2363
	step [164/192], loss=79.5150
	step [165/192], loss=81.9248
	step [166/192], loss=73.2566
	step [167/192], loss=75.9254
	step [168/192], loss=88.6364
	step [169/192], loss=83.0911
	step [170/192], loss=67.6556
	step [171/192], loss=80.5440
	step [172/192], loss=85.4078
	step [173/192], loss=76.2054
	step [174/192], loss=85.8066
	step [175/192], loss=94.5941
	step [176/192], loss=90.8860
	step [177/192], loss=80.5825
	step [178/192], loss=81.3829
	step [179/192], loss=76.2053
	step [180/192], loss=77.6834
	step [181/192], loss=93.2151
	step [182/192], loss=85.7686
	step [183/192], loss=54.1716
	step [184/192], loss=70.4525
	step [185/192], loss=73.7643
	step [186/192], loss=78.8322
	step [187/192], loss=83.7673
	step [188/192], loss=75.8905
	step [189/192], loss=78.9276
	step [190/192], loss=71.2023
	step [191/192], loss=77.4346
	step [192/192], loss=11.6014
	Evaluating
	loss=0.0100, precision=0.3260, recall=0.8795, f1=0.4757
Training finished
best_f1: 0.6236496738028742
directing: X rim_enhanced: True test_id 1
removed wrong scan: weights_X_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_22_xwqg-B00034_2020-04-03.npy
# all image files: 12135 # all weight files in weight_dir: 9175 # image files with weight 9141
removed wrong scan: weights_X_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_22_xwqg-B00034_2020-04-03.npy
# all image files: 12135 # all weight files in weight_dir: 2708 # image files with weight 2691
train: /ibex/scratch/projects/c2052/air_tube_seg/training_samples_enhanced_one/X 9141
Using 4 GPUs
Going to train epochs [1-30]
Training epoch 1
	step [1/143], loss=391.9700
	step [2/143], loss=366.6818
	step [3/143], loss=381.0132
	step [4/143], loss=349.5731
	step [5/143], loss=332.5087
	step [6/143], loss=300.1860
	step [7/143], loss=325.5692
	step [8/143], loss=331.7454
	step [9/143], loss=303.6323
	step [10/143], loss=304.9409
	step [11/143], loss=304.5723
	step [12/143], loss=305.4962
	step [13/143], loss=298.9292
	step [14/143], loss=309.8165
	step [15/143], loss=302.9087
	step [16/143], loss=283.3941
	step [17/143], loss=283.2816
	step [18/143], loss=268.2415
	step [19/143], loss=270.4912
	step [20/143], loss=298.4477
	step [21/143], loss=282.9412
	step [22/143], loss=281.1060
	step [23/143], loss=310.0765
	step [24/143], loss=286.0443
	step [25/143], loss=301.1001
	step [26/143], loss=290.8586
	step [27/143], loss=264.2472
	step [28/143], loss=247.4359
	step [29/143], loss=246.9351
	step [30/143], loss=292.0081
	step [31/143], loss=269.8201
	step [32/143], loss=286.9589
	step [33/143], loss=296.2001
	step [34/143], loss=262.6140
	step [35/143], loss=271.8203
	step [36/143], loss=264.4246
	step [37/143], loss=267.7549
	step [38/143], loss=262.8533
	step [39/143], loss=265.2816
	step [40/143], loss=284.7306
	step [41/143], loss=268.1046
	step [42/143], loss=236.1566
	step [43/143], loss=248.9541
	step [44/143], loss=270.7858
	step [45/143], loss=238.2610
	step [46/143], loss=255.7657
	step [47/143], loss=253.7428
	step [48/143], loss=229.4675
	step [49/143], loss=236.6012
	step [50/143], loss=241.6003
	step [51/143], loss=231.3888
	step [52/143], loss=236.2340
	step [53/143], loss=248.8461
	step [54/143], loss=227.2279
	step [55/143], loss=225.0288
	step [56/143], loss=226.3273
	step [57/143], loss=233.8684
	step [58/143], loss=242.1172
	step [59/143], loss=259.5635
	step [60/143], loss=256.1807
	step [61/143], loss=245.3425
	step [62/143], loss=233.9619
	step [63/143], loss=225.8474
	step [64/143], loss=242.8902
	step [65/143], loss=241.7313
	step [66/143], loss=251.8970
	step [67/143], loss=234.9536
	step [68/143], loss=238.0712
	step [69/143], loss=242.8776
	step [70/143], loss=247.1323
	step [71/143], loss=228.1630
	step [72/143], loss=231.8647
	step [73/143], loss=215.9355
	step [74/143], loss=218.3347
	step [75/143], loss=236.0089
	step [76/143], loss=235.8259
	step [77/143], loss=243.0099
	step [78/143], loss=230.2584
	step [79/143], loss=221.2850
	step [80/143], loss=230.4731
	step [81/143], loss=215.9450
	step [82/143], loss=244.9649
	step [83/143], loss=238.1555
	step [84/143], loss=233.6092
	step [85/143], loss=244.0556
	step [86/143], loss=209.6656
	step [87/143], loss=236.7381
	step [88/143], loss=224.9689
	step [89/143], loss=218.4341
	step [90/143], loss=216.4032
	step [91/143], loss=231.6037
	step [92/143], loss=229.6723
	step [93/143], loss=211.3142
	step [94/143], loss=214.0481
	step [95/143], loss=230.7053
	step [96/143], loss=202.8387
	step [97/143], loss=203.9752
	step [98/143], loss=236.4329
	step [99/143], loss=223.7762
	step [100/143], loss=224.7328
	step [101/143], loss=232.1661
	step [102/143], loss=208.0531
	step [103/143], loss=213.2621
	step [104/143], loss=209.1633
	step [105/143], loss=207.5854
	step [106/143], loss=231.2558
	step [107/143], loss=216.8338
	step [108/143], loss=222.6013
	step [109/143], loss=211.5443
	step [110/143], loss=217.1078
	step [111/143], loss=204.1162
	step [112/143], loss=210.4495
	step [113/143], loss=221.5327
	step [114/143], loss=220.8683
	step [115/143], loss=211.3989
	step [116/143], loss=227.6615
	step [117/143], loss=205.0126
	step [118/143], loss=213.7294
	step [119/143], loss=197.1584
	step [120/143], loss=208.1989
	step [121/143], loss=193.7445
	step [122/143], loss=189.4052
	step [123/143], loss=222.7666
	step [124/143], loss=234.8708
	step [125/143], loss=211.8932
	step [126/143], loss=204.5672
	step [127/143], loss=220.8855
	step [128/143], loss=187.5432
	step [129/143], loss=186.8546
	step [130/143], loss=195.7863
	step [131/143], loss=179.1493
	step [132/143], loss=208.2110
	step [133/143], loss=222.0354
	step [134/143], loss=204.8397
	step [135/143], loss=211.2181
	step [136/143], loss=196.3239
	step [137/143], loss=232.6440
	step [138/143], loss=196.3541
	step [139/143], loss=212.3195
	step [140/143], loss=218.8839
	step [141/143], loss=206.9054
	step [142/143], loss=200.1623
	step [143/143], loss=148.7405
	Evaluating
	loss=0.4524, precision=0.0787, recall=0.8888, f1=0.1446
saving model as: 1_saved_model.pth
Training epoch 2
	step [1/143], loss=219.0795
	step [2/143], loss=219.1786
	step [3/143], loss=212.6867
	step [4/143], loss=209.7119
	step [5/143], loss=205.4738
	step [6/143], loss=214.2472
	step [7/143], loss=202.1709
	step [8/143], loss=193.0445
	step [9/143], loss=215.4709
	step [10/143], loss=197.6890
	step [11/143], loss=186.7778
	step [12/143], loss=214.1421
	step [13/143], loss=181.9681
	step [14/143], loss=232.2792
	step [15/143], loss=217.7803
	step [16/143], loss=235.3787
	step [17/143], loss=204.3845
	step [18/143], loss=192.1671
	step [19/143], loss=199.0351
	step [20/143], loss=206.3529
	step [21/143], loss=228.1677
	step [22/143], loss=200.4985
	step [23/143], loss=190.6562
	step [24/143], loss=214.4879
	step [25/143], loss=179.1290
	step [26/143], loss=196.7731
	step [27/143], loss=192.3120
	step [28/143], loss=202.7389
	step [29/143], loss=201.3671
	step [30/143], loss=201.4843
	step [31/143], loss=176.2987
	step [32/143], loss=202.2357
	step [33/143], loss=193.4443
	step [34/143], loss=198.8817
	step [35/143], loss=191.3210
	step [36/143], loss=208.0521
	step [37/143], loss=210.0823
	step [38/143], loss=203.0694
	step [39/143], loss=189.0270
	step [40/143], loss=198.0125
	step [41/143], loss=202.1491
	step [42/143], loss=193.9196
	step [43/143], loss=181.7408
	step [44/143], loss=181.6930
	step [45/143], loss=193.7188
	step [46/143], loss=209.1113
	step [47/143], loss=194.8759
	step [48/143], loss=177.5312
	step [49/143], loss=190.1811
	step [50/143], loss=198.6726
	step [51/143], loss=194.3206
	step [52/143], loss=187.2105
	step [53/143], loss=195.2464
	step [54/143], loss=192.9319
	step [55/143], loss=199.0817
	step [56/143], loss=174.7506
	step [57/143], loss=174.1757
	step [58/143], loss=202.9317
	step [59/143], loss=171.4805
	step [60/143], loss=168.9252
	step [61/143], loss=199.2112
	step [62/143], loss=190.9318
	step [63/143], loss=168.3178
	step [64/143], loss=192.2596
	step [65/143], loss=209.1063
	step [66/143], loss=199.7986
	step [67/143], loss=197.6378
	step [68/143], loss=200.7850
	step [69/143], loss=176.4445
	step [70/143], loss=186.2135
	step [71/143], loss=189.3554
	step [72/143], loss=220.1778
	step [73/143], loss=195.0556
	step [74/143], loss=205.3834
	step [75/143], loss=190.7714
	step [76/143], loss=179.0911
	step [77/143], loss=196.2711
	step [78/143], loss=195.5855
	step [79/143], loss=198.4164
	step [80/143], loss=188.4787
	step [81/143], loss=197.4576
	step [82/143], loss=185.6379
	step [83/143], loss=165.6888
	step [84/143], loss=212.1035
	step [85/143], loss=191.8866
	step [86/143], loss=184.4119
	step [87/143], loss=208.4510
	step [88/143], loss=173.2397
	step [89/143], loss=175.4174
	step [90/143], loss=199.1987
	step [91/143], loss=206.7185
	step [92/143], loss=212.7003
	step [93/143], loss=207.5058
	step [94/143], loss=194.5877
	step [95/143], loss=178.7706
	step [96/143], loss=183.1688
	step [97/143], loss=180.7405
	step [98/143], loss=208.3898
	step [99/143], loss=189.0059
	step [100/143], loss=192.8208
	step [101/143], loss=200.4113
	step [102/143], loss=192.8290
	step [103/143], loss=165.2007
	step [104/143], loss=196.8571
	step [105/143], loss=202.1334
	step [106/143], loss=175.0749
	step [107/143], loss=185.2643
	step [108/143], loss=173.0563
	step [109/143], loss=193.2252
	step [110/143], loss=182.0852
	step [111/143], loss=194.0204
	step [112/143], loss=176.2386
	step [113/143], loss=193.5259
	step [114/143], loss=202.5770
	step [115/143], loss=192.5511
	step [116/143], loss=166.9436
	step [117/143], loss=164.4119
	step [118/143], loss=178.3302
	step [119/143], loss=207.8560
	step [120/143], loss=203.2556
	step [121/143], loss=201.1002
	step [122/143], loss=221.5857
	step [123/143], loss=172.6739
	step [124/143], loss=187.7480
	step [125/143], loss=195.2528
	step [126/143], loss=172.2337
	step [127/143], loss=178.1897
	step [128/143], loss=194.5288
	step [129/143], loss=188.9256
	step [130/143], loss=172.3561
	step [131/143], loss=184.2651
	step [132/143], loss=203.3310
	step [133/143], loss=159.4442
	step [134/143], loss=178.5329
	step [135/143], loss=191.6432
	step [136/143], loss=184.1047
	step [137/143], loss=159.4532
	step [138/143], loss=188.2708
	step [139/143], loss=192.4763
	step [140/143], loss=205.3894
	step [141/143], loss=173.2467
	step [142/143], loss=185.5264
	step [143/143], loss=169.8893
	Evaluating
	loss=0.3429, precision=0.3697, recall=0.9040, f1=0.5248
saving model as: 1_saved_model.pth
Training epoch 3
	step [1/143], loss=185.2960
	step [2/143], loss=192.1219
	step [3/143], loss=159.2797
	step [4/143], loss=178.5264
	step [5/143], loss=175.3990
	step [6/143], loss=185.6927
	step [7/143], loss=189.6364
	step [8/143], loss=168.0239
	step [9/143], loss=173.0681
	step [10/143], loss=210.2480
	step [11/143], loss=180.3302
	step [12/143], loss=186.7969
	step [13/143], loss=191.8366
	step [14/143], loss=176.7020
	step [15/143], loss=204.0265
	step [16/143], loss=197.3898
	step [17/143], loss=198.0428
	step [18/143], loss=212.7072
	step [19/143], loss=177.5327
	step [20/143], loss=215.2702
	step [21/143], loss=210.7282
	step [22/143], loss=183.9986
	step [23/143], loss=169.1332
	step [24/143], loss=177.9164
	step [25/143], loss=180.7674
	step [26/143], loss=186.2187
	step [27/143], loss=172.7584
	step [28/143], loss=175.7090
	step [29/143], loss=178.9788
	step [30/143], loss=165.2864
	step [31/143], loss=185.5943
	step [32/143], loss=201.0376
	step [33/143], loss=192.9037
	step [34/143], loss=151.7939
	step [35/143], loss=179.3471
	step [36/143], loss=156.5491
	step [37/143], loss=156.8254
	step [38/143], loss=169.7077
	step [39/143], loss=184.7990
	step [40/143], loss=178.1713
	step [41/143], loss=168.2237
	step [42/143], loss=190.3829
	step [43/143], loss=172.8063
	step [44/143], loss=179.7324
	step [45/143], loss=176.5119
	step [46/143], loss=182.8193
	step [47/143], loss=167.5308
	step [48/143], loss=150.9936
	step [49/143], loss=174.6783
	step [50/143], loss=163.7460
	step [51/143], loss=157.5634
	step [52/143], loss=173.8473
	step [53/143], loss=175.6479
	step [54/143], loss=183.1608
	step [55/143], loss=174.2510
	step [56/143], loss=185.5574
	step [57/143], loss=170.6591
	step [58/143], loss=162.6454
	step [59/143], loss=167.0551
	step [60/143], loss=171.9129
	step [61/143], loss=171.3322
	step [62/143], loss=153.9476
	step [63/143], loss=160.4918
	step [64/143], loss=178.4049
	step [65/143], loss=163.5087
	step [66/143], loss=192.6544
	step [67/143], loss=169.2621
	step [68/143], loss=175.6964
	step [69/143], loss=157.0865
	step [70/143], loss=204.9885
	step [71/143], loss=178.9055
	step [72/143], loss=179.8875
	step [73/143], loss=153.3396
	step [74/143], loss=196.8475
	step [75/143], loss=188.2489
	step [76/143], loss=175.0415
	step [77/143], loss=194.5632
	step [78/143], loss=177.9283
	step [79/143], loss=173.6868
	step [80/143], loss=180.6489
	step [81/143], loss=164.5769
	step [82/143], loss=203.2630
	step [83/143], loss=195.2836
	step [84/143], loss=186.6965
	step [85/143], loss=156.8767
	step [86/143], loss=176.6138
	step [87/143], loss=194.1308
	step [88/143], loss=149.1344
	step [89/143], loss=155.5234
	step [90/143], loss=161.7844
	step [91/143], loss=142.2998
	step [92/143], loss=183.3850
	step [93/143], loss=177.5920
	step [94/143], loss=171.1146
	step [95/143], loss=183.1042
	step [96/143], loss=164.8511
	step [97/143], loss=171.6147
	step [98/143], loss=179.5431
	step [99/143], loss=179.4311
	step [100/143], loss=176.8931
	step [101/143], loss=171.1941
	step [102/143], loss=155.5515
	step [103/143], loss=163.6233
	step [104/143], loss=180.4695
	step [105/143], loss=154.0008
	step [106/143], loss=168.7049
	step [107/143], loss=188.9945
	step [108/143], loss=181.4466
	step [109/143], loss=170.5035
	step [110/143], loss=171.6012
	step [111/143], loss=148.8772
	step [112/143], loss=169.9207
	step [113/143], loss=168.8419
	step [114/143], loss=180.0919
	step [115/143], loss=162.5349
	step [116/143], loss=155.2933
	step [117/143], loss=159.9416
	step [118/143], loss=173.3283
	step [119/143], loss=172.6671
	step [120/143], loss=160.9561
	step [121/143], loss=183.3374
	step [122/143], loss=153.1842
	step [123/143], loss=157.9671
	step [124/143], loss=188.4551
	step [125/143], loss=161.5517
	step [126/143], loss=165.2564
	step [127/143], loss=184.2150
	step [128/143], loss=168.7459
	step [129/143], loss=171.8390
	step [130/143], loss=179.5899
	step [131/143], loss=159.9531
	step [132/143], loss=177.0309
	step [133/143], loss=172.4759
	step [134/143], loss=165.4786
	step [135/143], loss=166.0671
	step [136/143], loss=162.8157
	step [137/143], loss=174.5344
	step [138/143], loss=156.2054
	step [139/143], loss=176.2872
	step [140/143], loss=173.5466
	step [141/143], loss=202.1085
	step [142/143], loss=163.4967
	step [143/143], loss=127.4254
	Evaluating
	loss=0.2738, precision=0.5198, recall=0.9157, f1=0.6631
saving model as: 1_saved_model.pth
Training epoch 4
	step [1/143], loss=161.4847
	step [2/143], loss=173.6294
	step [3/143], loss=155.1361
	step [4/143], loss=182.3203
	step [5/143], loss=165.4359
	step [6/143], loss=151.9088
	step [7/143], loss=196.9456
	step [8/143], loss=137.2932
	step [9/143], loss=184.3539
	step [10/143], loss=166.7807
	step [11/143], loss=179.5227
	step [12/143], loss=162.5810
	step [13/143], loss=169.5868
	step [14/143], loss=176.2415
	step [15/143], loss=193.6744
	step [16/143], loss=167.5627
	step [17/143], loss=155.2604
	step [18/143], loss=192.3678
	step [19/143], loss=184.0667
	step [20/143], loss=161.4627
	step [21/143], loss=159.4610
	step [22/143], loss=162.2334
	step [23/143], loss=167.0981
	step [24/143], loss=171.0873
	step [25/143], loss=143.1235
	step [26/143], loss=180.7294
	step [27/143], loss=172.0269
	step [28/143], loss=160.8113
	step [29/143], loss=160.0718
	step [30/143], loss=154.3056
	step [31/143], loss=167.8132
	step [32/143], loss=159.8922
	step [33/143], loss=161.6800
	step [34/143], loss=172.0755
	step [35/143], loss=167.0261
	step [36/143], loss=190.6895
	step [37/143], loss=181.8502
	step [38/143], loss=143.8719
	step [39/143], loss=162.6326
	step [40/143], loss=160.2443
	step [41/143], loss=195.1445
	step [42/143], loss=167.2578
	step [43/143], loss=156.8633
	step [44/143], loss=176.4885
	step [45/143], loss=161.5040
	step [46/143], loss=160.6772
	step [47/143], loss=165.7325
	step [48/143], loss=179.5096
	step [49/143], loss=161.2376
	step [50/143], loss=169.0777
	step [51/143], loss=161.5639
	step [52/143], loss=158.0266
	step [53/143], loss=172.0127
	step [54/143], loss=153.3601
	step [55/143], loss=175.2440
	step [56/143], loss=152.7945
	step [57/143], loss=158.5826
	step [58/143], loss=155.3406
	step [59/143], loss=148.8526
	step [60/143], loss=156.0153
	step [61/143], loss=159.8929
	step [62/143], loss=145.0802
	step [63/143], loss=161.8995
	step [64/143], loss=166.5123
	step [65/143], loss=170.2720
	step [66/143], loss=151.1403
	step [67/143], loss=166.6739
	step [68/143], loss=166.3598
	step [69/143], loss=168.1721
	step [70/143], loss=178.1024
	step [71/143], loss=139.6544
	step [72/143], loss=172.7692
	step [73/143], loss=159.0529
	step [74/143], loss=151.1851
	step [75/143], loss=159.4417
	step [76/143], loss=156.8017
	step [77/143], loss=156.8473
	step [78/143], loss=165.2123
	step [79/143], loss=153.9302
	step [80/143], loss=164.5353
	step [81/143], loss=158.6691
	step [82/143], loss=160.0720
	step [83/143], loss=167.5559
	step [84/143], loss=173.0730
	step [85/143], loss=154.7400
	step [86/143], loss=170.9827
	step [87/143], loss=162.7352
	step [88/143], loss=161.0894
	step [89/143], loss=168.3138
	step [90/143], loss=154.2070
	step [91/143], loss=142.4867
	step [92/143], loss=161.5473
	step [93/143], loss=164.0423
	step [94/143], loss=179.9029
	step [95/143], loss=182.4050
	step [96/143], loss=170.8844
	step [97/143], loss=174.5916
	step [98/143], loss=157.8775
	step [99/143], loss=171.7548
	step [100/143], loss=136.3367
	step [101/143], loss=146.9218
	step [102/143], loss=181.4377
	step [103/143], loss=162.2103
	step [104/143], loss=165.4003
	step [105/143], loss=166.3622
	step [106/143], loss=159.8179
	step [107/143], loss=166.0999
	step [108/143], loss=162.8645
	step [109/143], loss=154.9530
	step [110/143], loss=180.2150
	step [111/143], loss=160.9979
	step [112/143], loss=157.4398
	step [113/143], loss=148.7722
	step [114/143], loss=150.5186
	step [115/143], loss=168.6216
	step [116/143], loss=147.4411
	step [117/143], loss=155.8275
	step [118/143], loss=164.2056
	step [119/143], loss=150.4781
	step [120/143], loss=177.5592
	step [121/143], loss=133.9102
	step [122/143], loss=151.0751
	step [123/143], loss=144.9073
	step [124/143], loss=167.1041
	step [125/143], loss=162.9642
	step [126/143], loss=155.6070
	step [127/143], loss=162.1537
	step [128/143], loss=128.2154
	step [129/143], loss=167.7763
	step [130/143], loss=175.0520
	step [131/143], loss=160.1890
	step [132/143], loss=135.2891
	step [133/143], loss=166.9349
	step [134/143], loss=166.8198
	step [135/143], loss=134.5524
	step [136/143], loss=155.7140
	step [137/143], loss=164.7332
	step [138/143], loss=169.4907
	step [139/143], loss=167.7416
	step [140/143], loss=163.6349
	step [141/143], loss=143.3694
	step [142/143], loss=166.5492
	step [143/143], loss=125.9622
	Evaluating
	loss=0.2236, precision=0.4835, recall=0.9065, f1=0.6307
Training epoch 5
	step [1/143], loss=166.6533
	step [2/143], loss=136.8385
	step [3/143], loss=155.7144
	step [4/143], loss=141.3383
	step [5/143], loss=156.8509
	step [6/143], loss=166.4371
	step [7/143], loss=162.7988
	step [8/143], loss=155.8803
	step [9/143], loss=158.4253
	step [10/143], loss=157.1438
	step [11/143], loss=159.1876
	step [12/143], loss=169.8231
	step [13/143], loss=153.3751
	step [14/143], loss=157.1259
	step [15/143], loss=160.0105
	step [16/143], loss=176.6320
	step [17/143], loss=154.5305
	step [18/143], loss=150.9668
	step [19/143], loss=151.1729
	step [20/143], loss=152.5692
	step [21/143], loss=147.2205
	step [22/143], loss=180.0121
	step [23/143], loss=156.7823
	step [24/143], loss=158.4105
	step [25/143], loss=169.9011
	step [26/143], loss=142.2454
	step [27/143], loss=156.5507
	step [28/143], loss=153.9306
	step [29/143], loss=162.3473
	step [30/143], loss=140.3303
	step [31/143], loss=150.3378
	step [32/143], loss=136.9775
	step [33/143], loss=166.8791
	step [34/143], loss=183.3807
	step [35/143], loss=162.1129
	step [36/143], loss=141.0552
	step [37/143], loss=175.4547
	step [38/143], loss=158.5398
	step [39/143], loss=151.4401
	step [40/143], loss=147.7482
	step [41/143], loss=150.0624
	step [42/143], loss=154.3956
	step [43/143], loss=167.9133
	step [44/143], loss=166.7861
	step [45/143], loss=162.4393
	step [46/143], loss=154.2697
	step [47/143], loss=151.2574
	step [48/143], loss=164.3001
	step [49/143], loss=131.2762
	step [50/143], loss=148.8385
	step [51/143], loss=151.0471
	step [52/143], loss=144.0109
	step [53/143], loss=157.2217
	step [54/143], loss=178.9501
	step [55/143], loss=188.3615
	step [56/143], loss=143.8451
	step [57/143], loss=162.8893
	step [58/143], loss=157.1937
	step [59/143], loss=160.4416
	step [60/143], loss=138.8742
	step [61/143], loss=150.5187
	step [62/143], loss=167.3955
	step [63/143], loss=137.0767
	step [64/143], loss=153.2216
	step [65/143], loss=172.3255
	step [66/143], loss=144.7575
	step [67/143], loss=141.7018
	step [68/143], loss=173.2604
	step [69/143], loss=145.3486
	step [70/143], loss=130.8820
	step [71/143], loss=154.2347
	step [72/143], loss=154.3350
	step [73/143], loss=154.1607
	step [74/143], loss=132.8570
	step [75/143], loss=147.3570
	step [76/143], loss=171.8125
	step [77/143], loss=139.0511
	step [78/143], loss=159.6571
	step [79/143], loss=137.3520
	step [80/143], loss=145.0990
	step [81/143], loss=143.0416
	step [82/143], loss=162.1931
	step [83/143], loss=144.1560
	step [84/143], loss=155.1790
	step [85/143], loss=144.5251
	step [86/143], loss=148.0105
	step [87/143], loss=143.8277
	step [88/143], loss=152.3579
	step [89/143], loss=137.4519
	step [90/143], loss=163.3230
	step [91/143], loss=153.7528
	step [92/143], loss=143.3903
	step [93/143], loss=152.4029
	step [94/143], loss=155.7142
	step [95/143], loss=177.3661
	step [96/143], loss=162.1129
	step [97/143], loss=144.1739
	step [98/143], loss=147.8714
	step [99/143], loss=153.7610
	step [100/143], loss=142.9916
	step [101/143], loss=145.0252
	step [102/143], loss=147.0335
	step [103/143], loss=147.4231
	step [104/143], loss=160.1656
	step [105/143], loss=128.1410
	step [106/143], loss=141.5044
	step [107/143], loss=162.2690
	step [108/143], loss=144.6535
	step [109/143], loss=140.2754
	step [110/143], loss=128.4225
	step [111/143], loss=148.3571
	step [112/143], loss=145.8194
	step [113/143], loss=151.3013
	step [114/143], loss=154.3032
	step [115/143], loss=161.1339
	step [116/143], loss=130.7277
	step [117/143], loss=154.3044
	step [118/143], loss=145.9965
	step [119/143], loss=162.3192
	step [120/143], loss=157.2547
	step [121/143], loss=157.3633
	step [122/143], loss=153.1658
	step [123/143], loss=135.6905
	step [124/143], loss=149.9931
	step [125/143], loss=141.2610
	step [126/143], loss=148.4167
	step [127/143], loss=134.7938
	step [128/143], loss=172.1240
	step [129/143], loss=139.1040
	step [130/143], loss=157.3077
	step [131/143], loss=149.1721
	step [132/143], loss=160.9541
	step [133/143], loss=145.6326
	step [134/143], loss=135.9641
	step [135/143], loss=148.9491
	step [136/143], loss=144.8564
	step [137/143], loss=127.8407
	step [138/143], loss=155.3739
	step [139/143], loss=170.9579
	step [140/143], loss=154.2113
	step [141/143], loss=158.8794
	step [142/143], loss=147.4518
	step [143/143], loss=122.3195
	Evaluating
	loss=0.1860, precision=0.5263, recall=0.8783, f1=0.6582
Training epoch 6
	step [1/143], loss=144.2723
	step [2/143], loss=150.1035
	step [3/143], loss=147.1474
	step [4/143], loss=155.0661
	step [5/143], loss=162.2262
	step [6/143], loss=130.1764
	step [7/143], loss=149.0334
	step [8/143], loss=145.5056
	step [9/143], loss=157.1075
	step [10/143], loss=161.2235
	step [11/143], loss=161.0420
	step [12/143], loss=155.3570
	step [13/143], loss=155.2637
	step [14/143], loss=140.4982
	step [15/143], loss=153.4889
	step [16/143], loss=135.1297
	step [17/143], loss=174.5767
	step [18/143], loss=156.4276
	step [19/143], loss=124.4742
	step [20/143], loss=148.7763
	step [21/143], loss=178.7853
	step [22/143], loss=154.1045
	step [23/143], loss=133.4631
	step [24/143], loss=150.9755
	step [25/143], loss=143.3149
	step [26/143], loss=143.6169
	step [27/143], loss=162.8060
	step [28/143], loss=153.3364
	step [29/143], loss=142.3114
	step [30/143], loss=139.2257
	step [31/143], loss=174.4188
	step [32/143], loss=137.1095
	step [33/143], loss=139.5059
	step [34/143], loss=147.9655
	step [35/143], loss=122.2349
	step [36/143], loss=140.4710
	step [37/143], loss=156.7956
	step [38/143], loss=124.1756
	step [39/143], loss=137.9487
	step [40/143], loss=141.9704
	step [41/143], loss=160.7080
	step [42/143], loss=153.3357
	step [43/143], loss=132.3580
	step [44/143], loss=142.7611
	step [45/143], loss=153.5011
	step [46/143], loss=145.8123
	step [47/143], loss=140.2702
	step [48/143], loss=161.9061
	step [49/143], loss=176.0392
	step [50/143], loss=150.3698
	step [51/143], loss=153.8513
	step [52/143], loss=145.5507
	step [53/143], loss=158.9286
	step [54/143], loss=161.7828
	step [55/143], loss=134.5673
	step [56/143], loss=132.0372
	step [57/143], loss=129.0148
	step [58/143], loss=149.1593
	step [59/143], loss=143.9392
	step [60/143], loss=153.1452
	step [61/143], loss=156.9037
	step [62/143], loss=147.8523
	step [63/143], loss=146.0061
	step [64/143], loss=153.8947
	step [65/143], loss=129.3791
	step [66/143], loss=133.0830
	step [67/143], loss=118.4530
	step [68/143], loss=149.9783
	step [69/143], loss=147.8049
	step [70/143], loss=128.5471
	step [71/143], loss=155.3846
	step [72/143], loss=172.2299
	step [73/143], loss=127.3753
	step [74/143], loss=146.8674
	step [75/143], loss=156.6146
	step [76/143], loss=145.7822
	step [77/143], loss=129.6418
	step [78/143], loss=154.4160
	step [79/143], loss=151.1711
	step [80/143], loss=120.2082
	step [81/143], loss=161.0121
	step [82/143], loss=152.2547
	step [83/143], loss=156.0767
	step [84/143], loss=133.3139
	step [85/143], loss=136.0949
	step [86/143], loss=151.5565
	step [87/143], loss=153.9534
	step [88/143], loss=146.0542
	step [89/143], loss=143.1722
	step [90/143], loss=153.9360
	step [91/143], loss=148.0344
	step [92/143], loss=132.7883
	step [93/143], loss=159.1636
	step [94/143], loss=140.3436
	step [95/143], loss=149.4899
	step [96/143], loss=142.8684
	step [97/143], loss=122.4914
	step [98/143], loss=152.3913
	step [99/143], loss=153.7338
	step [100/143], loss=155.2874
	step [101/143], loss=141.8226
	step [102/143], loss=170.5633
	step [103/143], loss=124.2174
	step [104/143], loss=120.1051
	step [105/143], loss=168.5099
	step [106/143], loss=131.1163
	step [107/143], loss=141.4726
	step [108/143], loss=147.0413
	step [109/143], loss=152.4951
	step [110/143], loss=136.8495
	step [111/143], loss=146.4689
	step [112/143], loss=124.8976
	step [113/143], loss=134.3451
	step [114/143], loss=152.1396
	step [115/143], loss=162.5145
	step [116/143], loss=157.8543
	step [117/143], loss=163.4125
	step [118/143], loss=130.7738
	step [119/143], loss=155.7035
	step [120/143], loss=146.0919
	step [121/143], loss=143.2815
	step [122/143], loss=126.5506
	step [123/143], loss=172.0853
	step [124/143], loss=151.9894
	step [125/143], loss=156.2378
	step [126/143], loss=141.1338
	step [127/143], loss=149.7190
	step [128/143], loss=149.3001
	step [129/143], loss=116.1747
	step [130/143], loss=140.0772
	step [131/143], loss=154.4297
	step [132/143], loss=142.6677
	step [133/143], loss=103.1603
	step [134/143], loss=136.5040
	step [135/143], loss=134.8602
	step [136/143], loss=139.6026
	step [137/143], loss=120.3110
	step [138/143], loss=118.1609
	step [139/143], loss=139.1082
	step [140/143], loss=137.3661
	step [141/143], loss=135.4833
	step [142/143], loss=134.6375
	step [143/143], loss=104.6316
	Evaluating
	loss=0.1538, precision=0.5122, recall=0.9048, f1=0.6542
Training epoch 7
	step [1/143], loss=121.2821
	step [2/143], loss=150.2272
	step [3/143], loss=140.0505
	step [4/143], loss=135.5058
	step [5/143], loss=140.8751
	step [6/143], loss=142.2229
	step [7/143], loss=146.1659
	step [8/143], loss=126.6675
	step [9/143], loss=150.4015
	step [10/143], loss=171.7480
	step [11/143], loss=146.0068
	step [12/143], loss=182.8384
	step [13/143], loss=136.4989
	step [14/143], loss=136.2819
	step [15/143], loss=143.8931
	step [16/143], loss=138.8031
	step [17/143], loss=138.4401
	step [18/143], loss=141.5055
	step [19/143], loss=157.6738
	step [20/143], loss=158.1079
	step [21/143], loss=139.9181
	step [22/143], loss=126.5639
	step [23/143], loss=149.2480
	step [24/143], loss=116.9476
	step [25/143], loss=124.2686
	step [26/143], loss=131.7521
	step [27/143], loss=130.6806
	step [28/143], loss=147.1599
	step [29/143], loss=134.9341
	step [30/143], loss=148.6000
	step [31/143], loss=130.6904
	step [32/143], loss=141.3461
	step [33/143], loss=141.7573
	step [34/143], loss=135.1537
	step [35/143], loss=138.9074
	step [36/143], loss=156.2281
	step [37/143], loss=122.3569
	step [38/143], loss=123.4794
	step [39/143], loss=139.3641
	step [40/143], loss=140.2341
	step [41/143], loss=125.2436
	step [42/143], loss=173.7056
	step [43/143], loss=144.1400
	step [44/143], loss=151.3801
	step [45/143], loss=127.9111
	step [46/143], loss=148.5932
	step [47/143], loss=148.0301
	step [48/143], loss=128.1560
	step [49/143], loss=148.8729
	step [50/143], loss=146.8608
	step [51/143], loss=136.1023
	step [52/143], loss=140.6157
	step [53/143], loss=128.6281
	step [54/143], loss=127.4788
	step [55/143], loss=148.3326
	step [56/143], loss=134.7263
	step [57/143], loss=156.3742
	step [58/143], loss=114.5370
	step [59/143], loss=137.6097
	step [60/143], loss=143.4204
	step [61/143], loss=129.7617
	step [62/143], loss=145.9908
	step [63/143], loss=148.3319
	step [64/143], loss=142.7491
	step [65/143], loss=127.2018
	step [66/143], loss=127.8723
	step [67/143], loss=130.4553
	step [68/143], loss=141.4331
	step [69/143], loss=124.6704
	step [70/143], loss=155.1080
	step [71/143], loss=147.4372
	step [72/143], loss=138.6729
	step [73/143], loss=148.7389
	step [74/143], loss=135.5638
	step [75/143], loss=135.9799
	step [76/143], loss=137.1081
	step [77/143], loss=150.2262
	step [78/143], loss=140.8687
	step [79/143], loss=120.8911
	step [80/143], loss=151.4324
	step [81/143], loss=126.5665
	step [82/143], loss=146.8682
	step [83/143], loss=136.0127
	step [84/143], loss=139.2339
	step [85/143], loss=129.1182
	step [86/143], loss=137.9625
	step [87/143], loss=131.6261
	step [88/143], loss=143.3726
	step [89/143], loss=130.9547
	step [90/143], loss=123.9354
	step [91/143], loss=165.2223
	step [92/143], loss=126.0908
	step [93/143], loss=140.5395
	step [94/143], loss=137.0140
	step [95/143], loss=158.1632
	step [96/143], loss=149.0871
	step [97/143], loss=154.0657
	step [98/143], loss=162.7467
	step [99/143], loss=111.5360
	step [100/143], loss=146.8675
	step [101/143], loss=115.5058
	step [102/143], loss=145.2509
	step [103/143], loss=130.1891
	step [104/143], loss=119.2702
	step [105/143], loss=134.5752
	step [106/143], loss=134.8870
	step [107/143], loss=127.2425
	step [108/143], loss=157.4231
	step [109/143], loss=131.3061
	step [110/143], loss=135.7035
	step [111/143], loss=151.2096
	step [112/143], loss=130.8495
	step [113/143], loss=127.1430
	step [114/143], loss=135.3120
	step [115/143], loss=121.7418
	step [116/143], loss=158.1567
	step [117/143], loss=154.4547
	step [118/143], loss=157.8253
	step [119/143], loss=123.2615
	step [120/143], loss=147.2900
	step [121/143], loss=148.1101
	step [122/143], loss=130.9814
	step [123/143], loss=135.0858
	step [124/143], loss=122.5851
	step [125/143], loss=129.4164
	step [126/143], loss=140.2416
	step [127/143], loss=137.5103
	step [128/143], loss=125.0261
	step [129/143], loss=137.7630
	step [130/143], loss=142.1127
	step [131/143], loss=128.3008
	step [132/143], loss=134.6799
	step [133/143], loss=140.1533
	step [134/143], loss=143.7596
	step [135/143], loss=143.7105
	step [136/143], loss=142.8946
	step [137/143], loss=135.1132
	step [138/143], loss=119.8472
	step [139/143], loss=145.3934
	step [140/143], loss=119.8761
	step [141/143], loss=141.9558
	step [142/143], loss=139.5188
	step [143/143], loss=122.5558
	Evaluating
	loss=0.1338, precision=0.4619, recall=0.9022, f1=0.6110
Training epoch 8
	step [1/143], loss=123.1571
	step [2/143], loss=132.6353
	step [3/143], loss=140.9846
	step [4/143], loss=126.9059
	step [5/143], loss=119.1293
	step [6/143], loss=141.4465
	step [7/143], loss=162.1590
	step [8/143], loss=122.3280
	step [9/143], loss=142.2344
	step [10/143], loss=137.4962
	step [11/143], loss=149.1808
	step [12/143], loss=145.3787
	step [13/143], loss=148.7978
	step [14/143], loss=131.9812
	step [15/143], loss=117.0753
	step [16/143], loss=126.8109
	step [17/143], loss=146.2389
	step [18/143], loss=140.6359
	step [19/143], loss=132.1891
	step [20/143], loss=122.2474
	step [21/143], loss=122.0345
	step [22/143], loss=134.1009
	step [23/143], loss=120.0322
	step [24/143], loss=124.8657
	step [25/143], loss=143.2375
	step [26/143], loss=127.3076
	step [27/143], loss=132.9561
	step [28/143], loss=144.2264
	step [29/143], loss=133.7271
	step [30/143], loss=113.0163
	step [31/143], loss=132.1300
	step [32/143], loss=136.0480
	step [33/143], loss=117.6674
	step [34/143], loss=137.5367
	step [35/143], loss=124.9720
	step [36/143], loss=124.6086
	step [37/143], loss=118.4890
	step [38/143], loss=136.6433
	step [39/143], loss=142.4723
	step [40/143], loss=139.0526
	step [41/143], loss=138.0728
	step [42/143], loss=136.3994
	step [43/143], loss=152.2052
	step [44/143], loss=128.1662
	step [45/143], loss=149.6310
	step [46/143], loss=144.3512
	step [47/143], loss=140.8479
	step [48/143], loss=121.5466
	step [49/143], loss=134.0403
	step [50/143], loss=154.1032
	step [51/143], loss=118.3808
	step [52/143], loss=142.1587
	step [53/143], loss=140.2029
	step [54/143], loss=135.8137
	step [55/143], loss=123.5230
	step [56/143], loss=157.7668
	step [57/143], loss=139.8869
	step [58/143], loss=148.2681
	step [59/143], loss=138.3670
	step [60/143], loss=144.6359
	step [61/143], loss=115.3042
	step [62/143], loss=147.9905
	step [63/143], loss=137.1403
	step [64/143], loss=128.0646
	step [65/143], loss=136.0158
	step [66/143], loss=119.1278
	step [67/143], loss=115.1461
	step [68/143], loss=139.5287
	step [69/143], loss=116.7785
	step [70/143], loss=157.7477
	step [71/143], loss=91.5592
	step [72/143], loss=145.7576
	step [73/143], loss=118.5330
	step [74/143], loss=127.3401
	step [75/143], loss=129.7682
	step [76/143], loss=127.4214
	step [77/143], loss=147.2731
	step [78/143], loss=156.7037
	step [79/143], loss=127.1170
	step [80/143], loss=150.6124
	step [81/143], loss=136.3153
	step [82/143], loss=115.5860
	step [83/143], loss=140.2983
	step [84/143], loss=143.0074
	step [85/143], loss=124.4626
	step [86/143], loss=138.5662
	step [87/143], loss=121.5436
	step [88/143], loss=133.7077
	step [89/143], loss=145.2105
	step [90/143], loss=115.5104
	step [91/143], loss=125.6884
	step [92/143], loss=124.8905
	step [93/143], loss=122.9435
	step [94/143], loss=133.4574
	step [95/143], loss=129.1311
	step [96/143], loss=139.0802
	step [97/143], loss=136.2126
	step [98/143], loss=130.6002
	step [99/143], loss=125.2835
	step [100/143], loss=135.2914
	step [101/143], loss=155.8915
	step [102/143], loss=121.9944
	step [103/143], loss=120.7614
	step [104/143], loss=149.8560
	step [105/143], loss=126.7320
	step [106/143], loss=134.3048
	step [107/143], loss=114.6268
	step [108/143], loss=132.8559
	step [109/143], loss=124.6425
	step [110/143], loss=133.5634
	step [111/143], loss=129.6557
	step [112/143], loss=120.1689
	step [113/143], loss=126.2353
	step [114/143], loss=123.5190
	step [115/143], loss=143.7623
	step [116/143], loss=150.8091
	step [117/143], loss=141.9170
	step [118/143], loss=148.9882
	step [119/143], loss=141.9975
	step [120/143], loss=130.1467
	step [121/143], loss=143.8008
	step [122/143], loss=133.1771
	step [123/143], loss=136.9464
	step [124/143], loss=133.3307
	step [125/143], loss=150.9676
	step [126/143], loss=152.9061
	step [127/143], loss=121.8806
	step [128/143], loss=112.6759
	step [129/143], loss=137.7877
	step [130/143], loss=137.0224
	step [131/143], loss=132.4455
	step [132/143], loss=154.6853
	step [133/143], loss=126.0916
	step [134/143], loss=172.3425
	step [135/143], loss=134.8902
	step [136/143], loss=148.6922
	step [137/143], loss=150.8106
	step [138/143], loss=122.2687
	step [139/143], loss=120.2687
	step [140/143], loss=135.1406
	step [141/143], loss=110.7631
	step [142/143], loss=119.3613
	step [143/143], loss=107.5494
	Evaluating
	loss=0.1120, precision=0.4840, recall=0.9051, f1=0.6307
Training epoch 9
	step [1/143], loss=127.4946
	step [2/143], loss=123.6054
	step [3/143], loss=131.6685
	step [4/143], loss=133.3993
	step [5/143], loss=131.2130
	step [6/143], loss=142.9488
	step [7/143], loss=128.6461
	step [8/143], loss=128.3121
	step [9/143], loss=108.3096
	step [10/143], loss=126.5929
	step [11/143], loss=138.2096
	step [12/143], loss=139.0806
	step [13/143], loss=117.8637
	step [14/143], loss=137.0477
	step [15/143], loss=122.7349
	step [16/143], loss=127.0929
	step [17/143], loss=144.7018
	step [18/143], loss=151.8852
	step [19/143], loss=110.9611
	step [20/143], loss=113.1073
	step [21/143], loss=133.0059
	step [22/143], loss=120.4225
	step [23/143], loss=118.2291
	step [24/143], loss=123.7571
	step [25/143], loss=127.6963
	step [26/143], loss=141.5175
	step [27/143], loss=119.4727
	step [28/143], loss=138.4217
	step [29/143], loss=136.0858
	step [30/143], loss=131.1411
	step [31/143], loss=125.4832
	step [32/143], loss=133.4856
	step [33/143], loss=145.7185
	step [34/143], loss=141.7148
	step [35/143], loss=127.7276
	step [36/143], loss=141.3090
	step [37/143], loss=140.3863
	step [38/143], loss=128.0576
	step [39/143], loss=134.7605
	step [40/143], loss=145.1565
	step [41/143], loss=135.3940
	step [42/143], loss=136.0009
	step [43/143], loss=131.9337
	step [44/143], loss=133.8664
	step [45/143], loss=141.9708
	step [46/143], loss=109.6268
	step [47/143], loss=130.1112
	step [48/143], loss=133.0577
	step [49/143], loss=134.8184
	step [50/143], loss=123.6830
	step [51/143], loss=142.9962
	step [52/143], loss=143.7030
	step [53/143], loss=125.3612
	step [54/143], loss=139.7758
	step [55/143], loss=146.0219
	step [56/143], loss=148.7988
	step [57/143], loss=145.5173
	step [58/143], loss=115.5178
	step [59/143], loss=129.1369
	step [60/143], loss=121.1198
	step [61/143], loss=133.4916
	step [62/143], loss=135.4477
	step [63/143], loss=119.4856
	step [64/143], loss=117.0682
	step [65/143], loss=120.6743
	step [66/143], loss=130.8043
	step [67/143], loss=140.6036
	step [68/143], loss=142.8125
	step [69/143], loss=135.7842
	step [70/143], loss=147.7391
	step [71/143], loss=139.3194
	step [72/143], loss=125.0429
	step [73/143], loss=117.8421
	step [74/143], loss=138.7846
	step [75/143], loss=119.5732
	step [76/143], loss=135.4797
	step [77/143], loss=133.6547
	step [78/143], loss=134.9562
	step [79/143], loss=127.3640
	step [80/143], loss=141.9196
	step [81/143], loss=119.6898
	step [82/143], loss=143.4030
	step [83/143], loss=124.7636
	step [84/143], loss=124.1338
	step [85/143], loss=124.7684
	step [86/143], loss=126.3605
	step [87/143], loss=134.5786
	step [88/143], loss=145.2361
	step [89/143], loss=121.0821
	step [90/143], loss=116.5590
	step [91/143], loss=131.1435
	step [92/143], loss=122.0022
	step [93/143], loss=127.7569
	step [94/143], loss=126.1183
	step [95/143], loss=148.6723
	step [96/143], loss=133.8389
	step [97/143], loss=114.6365
	step [98/143], loss=130.1153
	step [99/143], loss=113.0541
	step [100/143], loss=109.9185
	step [101/143], loss=112.6126
	step [102/143], loss=118.7757
	step [103/143], loss=109.1962
	step [104/143], loss=113.9157
	step [105/143], loss=130.8638
	step [106/143], loss=123.2409
	step [107/143], loss=136.1146
	step [108/143], loss=121.3302
	step [109/143], loss=114.5832
	step [110/143], loss=157.3010
	step [111/143], loss=133.1893
	step [112/143], loss=124.5322
	step [113/143], loss=113.8875
	step [114/143], loss=141.4888
	step [115/143], loss=123.3465
	step [116/143], loss=116.6619
	step [117/143], loss=117.4172
	step [118/143], loss=125.7461
	step [119/143], loss=131.7150
	step [120/143], loss=125.1518
	step [121/143], loss=141.9190
	step [122/143], loss=129.7874
	step [123/143], loss=128.7642
	step [124/143], loss=147.0170
	step [125/143], loss=127.2605
	step [126/143], loss=113.8445
	step [127/143], loss=119.2298
	step [128/143], loss=118.7826
	step [129/143], loss=108.9623
	step [130/143], loss=128.3244
	step [131/143], loss=110.2045
	step [132/143], loss=131.0115
	step [133/143], loss=125.1329
	step [134/143], loss=116.3675
	step [135/143], loss=129.1836
	step [136/143], loss=149.0779
	step [137/143], loss=139.9976
	step [138/143], loss=140.2625
	step [139/143], loss=117.3959
	step [140/143], loss=130.5173
	step [141/143], loss=142.7463
	step [142/143], loss=129.1689
	step [143/143], loss=121.8398
	Evaluating
	loss=0.0944, precision=0.5607, recall=0.8836, f1=0.6860
saving model as: 1_saved_model.pth
Training epoch 10
	step [1/143], loss=118.8734
	step [2/143], loss=113.8257
	step [3/143], loss=117.5159
	step [4/143], loss=135.8275
	step [5/143], loss=133.1446
	step [6/143], loss=103.2117
	step [7/143], loss=136.5639
	step [8/143], loss=132.9084
	step [9/143], loss=128.8477
	step [10/143], loss=144.7776
	step [11/143], loss=109.0557
	step [12/143], loss=125.2301
	step [13/143], loss=119.8647
	step [14/143], loss=134.0569
	step [15/143], loss=131.0043
	step [16/143], loss=141.5544
	step [17/143], loss=143.5762
	step [18/143], loss=129.1648
	step [19/143], loss=141.9733
	step [20/143], loss=150.6747
	step [21/143], loss=106.3794
	step [22/143], loss=114.0571
	step [23/143], loss=118.8364
	step [24/143], loss=101.1171
	step [25/143], loss=131.6963
	step [26/143], loss=147.6681
	step [27/143], loss=148.1188
	step [28/143], loss=121.3959
	step [29/143], loss=121.1503
	step [30/143], loss=123.1629
	step [31/143], loss=139.1371
	step [32/143], loss=121.4865
	step [33/143], loss=117.9966
	step [34/143], loss=135.8714
	step [35/143], loss=117.0387
	step [36/143], loss=153.6578
	step [37/143], loss=125.6152
	step [38/143], loss=132.8676
	step [39/143], loss=129.6784
	step [40/143], loss=125.3142
	step [41/143], loss=128.7822
	step [42/143], loss=124.1767
	step [43/143], loss=104.4608
	step [44/143], loss=130.2138
	step [45/143], loss=118.0421
	step [46/143], loss=121.3959
	step [47/143], loss=107.6251
	step [48/143], loss=132.4890
	step [49/143], loss=111.4431
	step [50/143], loss=119.2858
	step [51/143], loss=145.8076
	step [52/143], loss=117.4751
	step [53/143], loss=136.1201
	step [54/143], loss=127.2582
	step [55/143], loss=112.8710
	step [56/143], loss=121.9346
	step [57/143], loss=140.8689
	step [58/143], loss=150.5378
	step [59/143], loss=128.3253
	step [60/143], loss=141.8041
	step [61/143], loss=113.0049
	step [62/143], loss=151.3397
	step [63/143], loss=112.9660
	step [64/143], loss=120.1704
	step [65/143], loss=103.5551
	step [66/143], loss=126.8909
	step [67/143], loss=131.8311
	step [68/143], loss=100.2844
	step [69/143], loss=123.9759
	step [70/143], loss=125.8820
	step [71/143], loss=107.8954
	step [72/143], loss=129.3984
	step [73/143], loss=118.0418
	step [74/143], loss=115.4098
	step [75/143], loss=123.1083
	step [76/143], loss=118.0254
	step [77/143], loss=108.3677
	step [78/143], loss=132.1323
	step [79/143], loss=135.9751
	step [80/143], loss=119.0822
	step [81/143], loss=114.7185
	step [82/143], loss=143.5815
	step [83/143], loss=121.6475
	step [84/143], loss=117.0630
	step [85/143], loss=124.9302
	step [86/143], loss=122.4167
	step [87/143], loss=109.6714
	step [88/143], loss=107.7896
	step [89/143], loss=125.2792
	step [90/143], loss=133.2101
	step [91/143], loss=121.4635
	step [92/143], loss=151.4687
	step [93/143], loss=102.2001
	step [94/143], loss=128.3865
	step [95/143], loss=123.4073
	step [96/143], loss=130.1967
	step [97/143], loss=111.7925
	step [98/143], loss=111.1000
	step [99/143], loss=132.1053
	step [100/143], loss=134.0381
	step [101/143], loss=149.1467
	step [102/143], loss=115.8755
	step [103/143], loss=120.0402
	step [104/143], loss=119.6746
	step [105/143], loss=132.0856
	step [106/143], loss=118.0104
	step [107/143], loss=128.2714
	step [108/143], loss=158.5104
	step [109/143], loss=123.7088
	step [110/143], loss=127.6043
	step [111/143], loss=129.5902
	step [112/143], loss=120.4818
	step [113/143], loss=118.9166
	step [114/143], loss=112.5590
	step [115/143], loss=147.8202
	step [116/143], loss=127.6414
	step [117/143], loss=127.6312
	step [118/143], loss=123.6489
	step [119/143], loss=114.5835
	step [120/143], loss=136.4152
	step [121/143], loss=122.2695
	step [122/143], loss=132.9993
	step [123/143], loss=127.6709
	step [124/143], loss=140.4914
	step [125/143], loss=133.5000
	step [126/143], loss=117.7232
	step [127/143], loss=125.8566
	step [128/143], loss=107.3848
	step [129/143], loss=145.8558
	step [130/143], loss=142.3049
	step [131/143], loss=122.6582
	step [132/143], loss=116.5188
	step [133/143], loss=115.3264
	step [134/143], loss=142.7752
	step [135/143], loss=137.0360
	step [136/143], loss=130.5840
	step [137/143], loss=120.9043
	step [138/143], loss=143.0724
	step [139/143], loss=115.8574
	step [140/143], loss=111.9966
	step [141/143], loss=113.4645
	step [142/143], loss=128.4956
	step [143/143], loss=114.3587
	Evaluating
	loss=0.0912, precision=0.3299, recall=0.8948, f1=0.4821
Training epoch 11
	step [1/143], loss=105.3754
	step [2/143], loss=114.3607
	step [3/143], loss=118.7785
	step [4/143], loss=125.1598
	step [5/143], loss=128.8239
	step [6/143], loss=140.8062
	step [7/143], loss=119.9695
	step [8/143], loss=122.2051
	step [9/143], loss=146.6319
	step [10/143], loss=130.3694
	step [11/143], loss=138.8862
	step [12/143], loss=134.2246
	step [13/143], loss=111.6718
	step [14/143], loss=117.3440
	step [15/143], loss=128.9135
	step [16/143], loss=126.2365
	step [17/143], loss=105.7152
	step [18/143], loss=136.7638
	step [19/143], loss=136.4898
	step [20/143], loss=118.8232
	step [21/143], loss=124.4050
	step [22/143], loss=112.5140
	step [23/143], loss=122.2610
	step [24/143], loss=137.6494
	step [25/143], loss=114.3388
	step [26/143], loss=126.4740
	step [27/143], loss=124.3229
	step [28/143], loss=107.5801
	step [29/143], loss=107.8308
	step [30/143], loss=116.1815
	step [31/143], loss=121.2800
	step [32/143], loss=128.2987
	step [33/143], loss=129.9859
	step [34/143], loss=122.4344
	step [35/143], loss=123.2900
	step [36/143], loss=119.6619
	step [37/143], loss=127.6577
	step [38/143], loss=137.4874
	step [39/143], loss=99.6115
	step [40/143], loss=130.2095
	step [41/143], loss=148.0424
	step [42/143], loss=111.7248
	step [43/143], loss=113.5462
	step [44/143], loss=135.9116
	step [45/143], loss=120.4962
	step [46/143], loss=106.0206
	step [47/143], loss=123.7212
	step [48/143], loss=132.9114
	step [49/143], loss=128.5769
	step [50/143], loss=120.5626
	step [51/143], loss=130.2477
	step [52/143], loss=133.2312
	step [53/143], loss=125.1853
	step [54/143], loss=117.5528
	step [55/143], loss=145.2928
	step [56/143], loss=141.2344
	step [57/143], loss=123.2001
	step [58/143], loss=140.9498
	step [59/143], loss=112.1444
	step [60/143], loss=131.8629
	step [61/143], loss=137.7521
	step [62/143], loss=109.2468
	step [63/143], loss=126.6410
	step [64/143], loss=114.5289
	step [65/143], loss=136.5675
	step [66/143], loss=119.8140
	step [67/143], loss=122.2617
	step [68/143], loss=102.7388
	step [69/143], loss=113.4485
	step [70/143], loss=128.3796
	step [71/143], loss=146.5182
	step [72/143], loss=116.6597
	step [73/143], loss=119.3074
	step [74/143], loss=125.5861
	step [75/143], loss=122.9166
	step [76/143], loss=122.5303
	step [77/143], loss=126.2908
	step [78/143], loss=136.1664
	step [79/143], loss=126.1411
	step [80/143], loss=141.1177
	step [81/143], loss=133.4838
	step [82/143], loss=128.4607
	step [83/143], loss=120.1267
	step [84/143], loss=111.8655
	step [85/143], loss=150.2704
	step [86/143], loss=122.7718
	step [87/143], loss=96.4952
	step [88/143], loss=125.4807
	step [89/143], loss=105.3201
	step [90/143], loss=114.0916
	step [91/143], loss=99.9391
	step [92/143], loss=129.2605
	step [93/143], loss=125.3465
	step [94/143], loss=129.5753
	step [95/143], loss=112.8246
	step [96/143], loss=120.1238
	step [97/143], loss=134.5763
	step [98/143], loss=116.1632
	step [99/143], loss=114.6674
	step [100/143], loss=127.8622
	step [101/143], loss=146.2503
	step [102/143], loss=120.6252
	step [103/143], loss=121.7554
	step [104/143], loss=125.4818
	step [105/143], loss=117.1481
	step [106/143], loss=106.7704
	step [107/143], loss=101.3517
	step [108/143], loss=120.6879
	step [109/143], loss=117.9928
	step [110/143], loss=108.3124
	step [111/143], loss=123.0584
	step [112/143], loss=131.4785
	step [113/143], loss=129.2633
	step [114/143], loss=120.3561
	step [115/143], loss=125.4401
	step [116/143], loss=97.8414
	step [117/143], loss=142.3993
	step [118/143], loss=117.8699
	step [119/143], loss=80.8478
	step [120/143], loss=128.7992
	step [121/143], loss=135.1212
	step [122/143], loss=126.3721
	step [123/143], loss=115.8400
	step [124/143], loss=116.9012
	step [125/143], loss=120.3683
	step [126/143], loss=126.3840
	step [127/143], loss=126.6125
	step [128/143], loss=135.8173
	step [129/143], loss=127.6977
	step [130/143], loss=113.9333
	step [131/143], loss=137.4025
	step [132/143], loss=131.9627
	step [133/143], loss=124.9559
	step [134/143], loss=117.2643
	step [135/143], loss=123.8044
	step [136/143], loss=98.7946
	step [137/143], loss=128.5716
	step [138/143], loss=112.4291
	step [139/143], loss=113.5802
	step [140/143], loss=119.7808
	step [141/143], loss=96.6013
	step [142/143], loss=129.7251
	step [143/143], loss=103.4550
	Evaluating
	loss=0.0766, precision=0.4672, recall=0.8979, f1=0.6146
Training epoch 12
	step [1/143], loss=123.1752
	step [2/143], loss=145.9643
	step [3/143], loss=124.2637
	step [4/143], loss=118.9073
	step [5/143], loss=123.2048
	step [6/143], loss=142.6130
	step [7/143], loss=135.0424
	step [8/143], loss=124.7847
	step [9/143], loss=113.5282
	step [10/143], loss=111.3842
	step [11/143], loss=133.3392
	step [12/143], loss=120.0468
	step [13/143], loss=121.0374
	step [14/143], loss=136.1358
	step [15/143], loss=109.9717
	step [16/143], loss=125.7400
	step [17/143], loss=127.9471
	step [18/143], loss=122.9996
	step [19/143], loss=138.9581
	step [20/143], loss=119.9303
	step [21/143], loss=114.4719
	step [22/143], loss=91.7910
	step [23/143], loss=119.1435
	step [24/143], loss=122.4580
	step [25/143], loss=104.7602
	step [26/143], loss=120.9040
	step [27/143], loss=116.3808
	step [28/143], loss=109.6799
	step [29/143], loss=129.9072
	step [30/143], loss=117.3795
	step [31/143], loss=92.2612
	step [32/143], loss=114.5116
	step [33/143], loss=132.3663
	step [34/143], loss=127.7190
	step [35/143], loss=123.7398
	step [36/143], loss=111.4951
	step [37/143], loss=131.4466
	step [38/143], loss=134.2754
	step [39/143], loss=114.8903
	step [40/143], loss=135.4149
	step [41/143], loss=118.3353
	step [42/143], loss=112.4640
	step [43/143], loss=127.0665
	step [44/143], loss=135.3624
	step [45/143], loss=118.5842
	step [46/143], loss=119.2926
	step [47/143], loss=140.9254
	step [48/143], loss=120.6542
	step [49/143], loss=111.1931
	step [50/143], loss=108.7236
	step [51/143], loss=114.9039
	step [52/143], loss=140.1283
	step [53/143], loss=127.3390
	step [54/143], loss=128.2010
	step [55/143], loss=129.5880
	step [56/143], loss=102.6092
	step [57/143], loss=111.2172
	step [58/143], loss=93.5658
	step [59/143], loss=119.5545
	step [60/143], loss=132.5289
	step [61/143], loss=132.4274
	step [62/143], loss=136.3963
	step [63/143], loss=114.0519
	step [64/143], loss=134.3391
	step [65/143], loss=119.8000
	step [66/143], loss=130.4408
	step [67/143], loss=110.5652
	step [68/143], loss=109.6308
	step [69/143], loss=98.5974
	step [70/143], loss=130.9115
	step [71/143], loss=107.4428
	step [72/143], loss=111.8416
	step [73/143], loss=116.4237
	step [74/143], loss=124.7586
	step [75/143], loss=126.6037
	step [76/143], loss=114.9185
	step [77/143], loss=121.2332
	step [78/143], loss=125.5023
	step [79/143], loss=133.1328
	step [80/143], loss=135.1593
	step [81/143], loss=142.7457
	step [82/143], loss=110.6062
	step [83/143], loss=115.6152
	step [84/143], loss=126.8394
	step [85/143], loss=100.8608
	step [86/143], loss=120.6143
	step [87/143], loss=129.3754
	step [88/143], loss=113.6904
	step [89/143], loss=141.4312
	step [90/143], loss=129.5350
	step [91/143], loss=114.4877
	step [92/143], loss=116.6343
	step [93/143], loss=117.6675
	step [94/143], loss=111.2685
	step [95/143], loss=121.8209
	step [96/143], loss=109.4003
	step [97/143], loss=113.4153
	step [98/143], loss=95.2631
	step [99/143], loss=127.3036
	step [100/143], loss=120.8950
	step [101/143], loss=121.1679
	step [102/143], loss=121.2219
	step [103/143], loss=101.7778
	step [104/143], loss=105.9153
	step [105/143], loss=133.1837
	step [106/143], loss=129.0921
	step [107/143], loss=122.0566
	step [108/143], loss=101.7470
	step [109/143], loss=140.3781
	step [110/143], loss=129.1893
	step [111/143], loss=108.6095
	step [112/143], loss=132.6960
	step [113/143], loss=113.6196
	step [114/143], loss=126.1196
	step [115/143], loss=108.6574
	step [116/143], loss=110.8709
	step [117/143], loss=109.1887
	step [118/143], loss=141.0095
	step [119/143], loss=104.8954
	step [120/143], loss=112.0307
	step [121/143], loss=131.5564
	step [122/143], loss=103.8389
	step [123/143], loss=108.2272
	step [124/143], loss=142.4000
	step [125/143], loss=129.7249
	step [126/143], loss=113.3585
	step [127/143], loss=114.7337
	step [128/143], loss=111.6675
	step [129/143], loss=132.1798
	step [130/143], loss=95.6316
	step [131/143], loss=114.8713
	step [132/143], loss=123.5945
	step [133/143], loss=129.9893
	step [134/143], loss=111.4739
	step [135/143], loss=130.5426
	step [136/143], loss=113.3566
	step [137/143], loss=111.5092
	step [138/143], loss=122.9726
	step [139/143], loss=104.5002
	step [140/143], loss=122.1763
	step [141/143], loss=116.1958
	step [142/143], loss=116.0667
	step [143/143], loss=84.9406
	Evaluating
	loss=0.0642, precision=0.5264, recall=0.8802, f1=0.6588
Training epoch 13
	step [1/143], loss=114.9763
	step [2/143], loss=101.2407
	step [3/143], loss=113.0303
	step [4/143], loss=120.6805
	step [5/143], loss=100.4079
	step [6/143], loss=112.4309
	step [7/143], loss=128.4177
	step [8/143], loss=103.3564
	step [9/143], loss=107.8901
	step [10/143], loss=123.0488
	step [11/143], loss=125.7614
	step [12/143], loss=136.4180
	step [13/143], loss=109.8856
	step [14/143], loss=120.4809
	step [15/143], loss=142.3624
	step [16/143], loss=114.0396
	step [17/143], loss=117.7906
	step [18/143], loss=113.9899
	step [19/143], loss=115.9735
	step [20/143], loss=130.7266
	step [21/143], loss=121.9858
	step [22/143], loss=137.0851
	step [23/143], loss=133.3690
	step [24/143], loss=109.3165
	step [25/143], loss=127.9394
	step [26/143], loss=121.4009
	step [27/143], loss=105.0020
	step [28/143], loss=130.7443
	step [29/143], loss=117.6805
	step [30/143], loss=108.2617
	step [31/143], loss=122.6889
	step [32/143], loss=124.4657
	step [33/143], loss=127.3635
	step [34/143], loss=120.9541
	step [35/143], loss=113.5285
	step [36/143], loss=114.9640
	step [37/143], loss=116.5450
	step [38/143], loss=128.6949
	step [39/143], loss=103.4017
	step [40/143], loss=115.0430
	step [41/143], loss=114.5671
	step [42/143], loss=126.9334
	step [43/143], loss=131.9912
	step [44/143], loss=136.9263
	step [45/143], loss=123.2658
	step [46/143], loss=141.8624
	step [47/143], loss=125.0015
	step [48/143], loss=127.2125
	step [49/143], loss=103.4843
	step [50/143], loss=113.3319
	step [51/143], loss=121.5900
	step [52/143], loss=123.1946
	step [53/143], loss=110.1786
	step [54/143], loss=105.7660
	step [55/143], loss=109.8957
	step [56/143], loss=139.2520
	step [57/143], loss=118.9918
	step [58/143], loss=124.5742
	step [59/143], loss=112.3031
	step [60/143], loss=124.8180
	step [61/143], loss=141.5169
	step [62/143], loss=106.0848
	step [63/143], loss=112.9681
	step [64/143], loss=105.0456
	step [65/143], loss=114.9214
	step [66/143], loss=123.7611
	step [67/143], loss=122.4146
	step [68/143], loss=137.2739
	step [69/143], loss=120.6596
	step [70/143], loss=123.4133
	step [71/143], loss=122.6698
	step [72/143], loss=103.6510
	step [73/143], loss=128.6544
	step [74/143], loss=114.3703
	step [75/143], loss=117.8123
	step [76/143], loss=114.2438
	step [77/143], loss=124.9118
	step [78/143], loss=112.3868
	step [79/143], loss=135.3706
	step [80/143], loss=129.0531
	step [81/143], loss=117.4006
	step [82/143], loss=101.4517
	step [83/143], loss=115.3441
	step [84/143], loss=124.9411
	step [85/143], loss=96.4728
	step [86/143], loss=104.9168
	step [87/143], loss=108.8297
	step [88/143], loss=122.1391
	step [89/143], loss=113.6781
	step [90/143], loss=101.4199
	step [91/143], loss=105.0594
	step [92/143], loss=121.1258
	step [93/143], loss=117.4788
	step [94/143], loss=119.8659
	step [95/143], loss=125.6784
	step [96/143], loss=124.6552
	step [97/143], loss=118.8401
	step [98/143], loss=129.2383
	step [99/143], loss=111.2722
	step [100/143], loss=96.6931
	step [101/143], loss=112.0518
	step [102/143], loss=115.8221
	step [103/143], loss=108.4701
	step [104/143], loss=119.9756
	step [105/143], loss=112.2159
	step [106/143], loss=97.5017
	step [107/143], loss=126.2274
	step [108/143], loss=114.8345
	step [109/143], loss=102.6005
	step [110/143], loss=94.6445
	step [111/143], loss=100.0651
	step [112/143], loss=132.7487
	step [113/143], loss=134.1662
	step [114/143], loss=119.9737
	step [115/143], loss=119.7199
	step [116/143], loss=143.0702
	step [117/143], loss=109.6445
	step [118/143], loss=125.0272
	step [119/143], loss=93.9874
	step [120/143], loss=111.2997
	step [121/143], loss=122.3170
	step [122/143], loss=111.2260
	step [123/143], loss=108.8027
	step [124/143], loss=129.6738
	step [125/143], loss=111.6394
	step [126/143], loss=123.4213
	step [127/143], loss=130.6902
	step [128/143], loss=119.6323
	step [129/143], loss=113.2329
	step [130/143], loss=145.7754
	step [131/143], loss=107.5752
	step [132/143], loss=124.5231
	step [133/143], loss=116.7121
	step [134/143], loss=116.6265
	step [135/143], loss=110.5843
	step [136/143], loss=114.9258
	step [137/143], loss=109.2773
	step [138/143], loss=103.0124
	step [139/143], loss=119.6024
	step [140/143], loss=106.6607
	step [141/143], loss=108.8930
	step [142/143], loss=103.4321
	step [143/143], loss=98.6138
	Evaluating
	loss=0.0593, precision=0.4770, recall=0.8862, f1=0.6202
Training epoch 14
	step [1/143], loss=117.4042
	step [2/143], loss=121.7976
	step [3/143], loss=123.2220
	step [4/143], loss=106.5756
	step [5/143], loss=116.6733
	step [6/143], loss=99.2695
	step [7/143], loss=121.0454
	step [8/143], loss=104.5540
	step [9/143], loss=117.9905
	step [10/143], loss=135.0868
	step [11/143], loss=112.8295
	step [12/143], loss=92.6057
	step [13/143], loss=107.6220
	step [14/143], loss=114.4441
	step [15/143], loss=128.8978
	step [16/143], loss=110.1480
	step [17/143], loss=124.4975
	step [18/143], loss=105.2474
	step [19/143], loss=119.9825
	step [20/143], loss=110.7026
	step [21/143], loss=118.3829
	step [22/143], loss=113.2252
	step [23/143], loss=113.1388
	step [24/143], loss=124.4028
	step [25/143], loss=106.8545
	step [26/143], loss=105.9833
	step [27/143], loss=106.1028
	step [28/143], loss=102.7190
	step [29/143], loss=111.8120
	step [30/143], loss=126.0194
	step [31/143], loss=102.8136
	step [32/143], loss=113.9652
	step [33/143], loss=124.6647
	step [34/143], loss=133.7748
	step [35/143], loss=109.2209
	step [36/143], loss=103.0207
	step [37/143], loss=118.7840
	step [38/143], loss=127.9380
	step [39/143], loss=125.7110
	step [40/143], loss=114.8213
	step [41/143], loss=158.2540
	step [42/143], loss=117.9000
	step [43/143], loss=107.7164
	step [44/143], loss=122.9205
	step [45/143], loss=116.6571
	step [46/143], loss=100.6689
	step [47/143], loss=132.0379
	step [48/143], loss=131.8851
	step [49/143], loss=130.3347
	step [50/143], loss=123.0803
	step [51/143], loss=111.3404
	step [52/143], loss=129.0365
	step [53/143], loss=105.5894
	step [54/143], loss=95.9724
	step [55/143], loss=128.0807
	step [56/143], loss=104.4997
	step [57/143], loss=88.6421
	step [58/143], loss=121.5993
	step [59/143], loss=135.9529
	step [60/143], loss=114.2222
	step [61/143], loss=112.5898
	step [62/143], loss=119.0717
	step [63/143], loss=104.1403
	step [64/143], loss=100.7768
	step [65/143], loss=107.4129
	step [66/143], loss=126.3544
	step [67/143], loss=125.7978
	step [68/143], loss=113.8547
	step [69/143], loss=129.7041
	step [70/143], loss=106.6220
	step [71/143], loss=107.1301
	step [72/143], loss=130.1175
	step [73/143], loss=121.3637
	step [74/143], loss=112.7783
	step [75/143], loss=121.9953
	step [76/143], loss=118.5754
	step [77/143], loss=94.0554
	step [78/143], loss=113.6474
	step [79/143], loss=116.3819
	step [80/143], loss=117.9634
	step [81/143], loss=118.9509
	step [82/143], loss=113.3783
	step [83/143], loss=101.8900
	step [84/143], loss=109.0264
	step [85/143], loss=93.8365
	step [86/143], loss=110.0962
	step [87/143], loss=129.3017
	step [88/143], loss=110.3505
	step [89/143], loss=121.4332
	step [90/143], loss=122.5050
	step [91/143], loss=122.0893
	step [92/143], loss=105.3968
	step [93/143], loss=129.0944
	step [94/143], loss=131.6696
	step [95/143], loss=122.1418
	step [96/143], loss=106.3913
	step [97/143], loss=92.2842
	step [98/143], loss=109.7690
	step [99/143], loss=114.4346
	step [100/143], loss=126.6727
	step [101/143], loss=124.6769
	step [102/143], loss=116.8015
	step [103/143], loss=115.9636
	step [104/143], loss=111.6398
	step [105/143], loss=117.5604
	step [106/143], loss=126.4770
	step [107/143], loss=108.7226
	step [108/143], loss=111.5692
	step [109/143], loss=124.1739
	step [110/143], loss=112.7496
	step [111/143], loss=101.1986
	step [112/143], loss=126.8734
	step [113/143], loss=99.9588
	step [114/143], loss=115.6913
	step [115/143], loss=132.7625
	step [116/143], loss=105.7595
	step [117/143], loss=115.5711
	step [118/143], loss=106.0203
	step [119/143], loss=113.6456
	step [120/143], loss=111.3302
	step [121/143], loss=121.3311
	step [122/143], loss=106.0337
	step [123/143], loss=101.6171
	step [124/143], loss=104.5981
	step [125/143], loss=125.8899
	step [126/143], loss=117.5617
	step [127/143], loss=106.8959
	step [128/143], loss=110.2098
	step [129/143], loss=132.7832
	step [130/143], loss=112.0963
	step [131/143], loss=107.6376
	step [132/143], loss=123.0119
	step [133/143], loss=143.0655
	step [134/143], loss=111.5277
	step [135/143], loss=103.4890
	step [136/143], loss=101.3162
	step [137/143], loss=112.1687
	step [138/143], loss=110.9677
	step [139/143], loss=115.2231
	step [140/143], loss=113.9695
	step [141/143], loss=121.3358
	step [142/143], loss=119.3487
	step [143/143], loss=113.9362
	Evaluating
	loss=0.0480, precision=0.5443, recall=0.8726, f1=0.6704
Training epoch 15
	step [1/143], loss=97.5029
	step [2/143], loss=116.9542
	step [3/143], loss=115.7621
	step [4/143], loss=106.2945
	step [5/143], loss=99.4073
	step [6/143], loss=126.0442
	step [7/143], loss=126.9807
	step [8/143], loss=123.7173
	step [9/143], loss=96.4388
	step [10/143], loss=147.3712
	step [11/143], loss=92.9731
	step [12/143], loss=115.3507
	step [13/143], loss=97.3885
	step [14/143], loss=115.0250
	step [15/143], loss=107.4950
	step [16/143], loss=104.8851
	step [17/143], loss=111.1846
	step [18/143], loss=119.4887
	step [19/143], loss=108.5141
	step [20/143], loss=110.3793
	step [21/143], loss=94.1487
	step [22/143], loss=129.0728
	step [23/143], loss=102.9271
	step [24/143], loss=111.2729
	step [25/143], loss=139.6238
	step [26/143], loss=126.0585
	step [27/143], loss=117.3372
	step [28/143], loss=119.6835
	step [29/143], loss=117.0824
	step [30/143], loss=100.6915
	step [31/143], loss=118.1821
	step [32/143], loss=108.4853
	step [33/143], loss=117.3867
	step [34/143], loss=131.9439
	step [35/143], loss=104.3778
	step [36/143], loss=96.3913
	step [37/143], loss=112.8310
	step [38/143], loss=111.7951
	step [39/143], loss=123.0077
	step [40/143], loss=92.2584
	step [41/143], loss=128.6879
	step [42/143], loss=113.4837
	step [43/143], loss=142.2529
	step [44/143], loss=114.2998
	step [45/143], loss=112.9656
	step [46/143], loss=108.9855
	step [47/143], loss=117.6579
	step [48/143], loss=107.2027
	step [49/143], loss=114.1737
	step [50/143], loss=104.7395
	step [51/143], loss=116.3643
	step [52/143], loss=101.5234
	step [53/143], loss=119.9209
	step [54/143], loss=130.8551
	step [55/143], loss=114.1358
	step [56/143], loss=101.1290
	step [57/143], loss=110.1174
	step [58/143], loss=123.2508
	step [59/143], loss=105.7859
	step [60/143], loss=107.2999
	step [61/143], loss=109.4977
	step [62/143], loss=96.8438
	step [63/143], loss=92.0122
	step [64/143], loss=115.1024
	step [65/143], loss=110.3297
	step [66/143], loss=116.9162
	step [67/143], loss=127.6559
	step [68/143], loss=128.8345
	step [69/143], loss=103.8356
	step [70/143], loss=121.9853
	step [71/143], loss=125.9218
	step [72/143], loss=122.6874
	step [73/143], loss=115.8289
	step [74/143], loss=97.6377
	step [75/143], loss=111.2079
	step [76/143], loss=111.1100
	step [77/143], loss=125.9644
	step [78/143], loss=117.4029
	step [79/143], loss=113.1668
	step [80/143], loss=111.7895
	step [81/143], loss=109.6007
	step [82/143], loss=109.5630
	step [83/143], loss=109.9272
	step [84/143], loss=107.3321
	step [85/143], loss=106.7852
	step [86/143], loss=120.2454
	step [87/143], loss=112.6231
	step [88/143], loss=105.1808
	step [89/143], loss=116.6360
	step [90/143], loss=115.1756
	step [91/143], loss=120.4079
	step [92/143], loss=114.3181
	step [93/143], loss=115.7236
	step [94/143], loss=110.9316
	step [95/143], loss=97.7085
	step [96/143], loss=122.0652
	step [97/143], loss=100.6116
	step [98/143], loss=104.9050
	step [99/143], loss=107.7659
	step [100/143], loss=129.2043
	step [101/143], loss=109.4258
	step [102/143], loss=113.3473
	step [103/143], loss=116.8295
	step [104/143], loss=109.2821
	step [105/143], loss=121.5188
	step [106/143], loss=104.6319
	step [107/143], loss=111.1070
	step [108/143], loss=135.6436
	step [109/143], loss=110.5182
	step [110/143], loss=109.3209
	step [111/143], loss=113.2996
	step [112/143], loss=120.7233
	step [113/143], loss=107.1545
	step [114/143], loss=127.1897
	step [115/143], loss=102.8069
	step [116/143], loss=92.3500
	step [117/143], loss=121.1494
	step [118/143], loss=104.0222
	step [119/143], loss=105.3763
	step [120/143], loss=124.0742
	step [121/143], loss=132.8010
	step [122/143], loss=105.6261
	step [123/143], loss=108.2104
	step [124/143], loss=102.9065
	step [125/143], loss=115.4937
	step [126/143], loss=137.5722
	step [127/143], loss=121.2484
	step [128/143], loss=90.5752
	step [129/143], loss=111.4342
	step [130/143], loss=113.0997
	step [131/143], loss=103.9828
	step [132/143], loss=105.9118
	step [133/143], loss=109.6134
	step [134/143], loss=118.1473
	step [135/143], loss=118.8947
	step [136/143], loss=119.4248
	step [137/143], loss=110.9925
	step [138/143], loss=130.5722
	step [139/143], loss=116.1667
	step [140/143], loss=124.5235
	step [141/143], loss=105.4511
	step [142/143], loss=112.2763
	step [143/143], loss=94.3650
	Evaluating
	loss=0.0461, precision=0.4758, recall=0.8927, f1=0.6207
Training epoch 16
	step [1/143], loss=111.4484
	step [2/143], loss=118.9281
	step [3/143], loss=123.2430
	step [4/143], loss=122.8095
	step [5/143], loss=100.8613
	step [6/143], loss=119.0065
	step [7/143], loss=114.2091
	step [8/143], loss=108.7105
	step [9/143], loss=108.8844
	step [10/143], loss=106.5060
	step [11/143], loss=122.0477
	step [12/143], loss=114.7293
	step [13/143], loss=112.5507
	step [14/143], loss=95.9505
	step [15/143], loss=115.6595
	step [16/143], loss=113.7095
	step [17/143], loss=113.9261
	step [18/143], loss=84.7019
	step [19/143], loss=106.9573
	step [20/143], loss=129.9652
	step [21/143], loss=101.6971
	step [22/143], loss=122.2176
	step [23/143], loss=128.8606
	step [24/143], loss=113.6681
	step [25/143], loss=89.1955
	step [26/143], loss=140.3242
	step [27/143], loss=102.9252
	step [28/143], loss=105.3006
	step [29/143], loss=98.7512
	step [30/143], loss=136.9344
	step [31/143], loss=127.9813
	step [32/143], loss=92.6979
	step [33/143], loss=103.4466
	step [34/143], loss=107.9987
	step [35/143], loss=100.1762
	step [36/143], loss=128.3117
	step [37/143], loss=132.6633
	step [38/143], loss=113.9883
	step [39/143], loss=113.6881
	step [40/143], loss=102.4666
	step [41/143], loss=102.5830
	step [42/143], loss=114.6174
	step [43/143], loss=94.3401
	step [44/143], loss=128.3369
	step [45/143], loss=119.6930
	step [46/143], loss=115.4759
	step [47/143], loss=116.9271
	step [48/143], loss=101.0107
	step [49/143], loss=113.9452
	step [50/143], loss=107.9626
	step [51/143], loss=122.1642
	step [52/143], loss=106.0266
	step [53/143], loss=122.5220
	step [54/143], loss=113.5054
	step [55/143], loss=120.7382
	step [56/143], loss=117.8089
	step [57/143], loss=110.0435
	step [58/143], loss=96.2132
	step [59/143], loss=124.1943
	step [60/143], loss=100.0368
	step [61/143], loss=110.3538
	step [62/143], loss=128.8279
	step [63/143], loss=103.0115
	step [64/143], loss=129.4040
	step [65/143], loss=105.9600
	step [66/143], loss=100.3473
	step [67/143], loss=98.6235
	step [68/143], loss=123.8270
	step [69/143], loss=98.4112
	step [70/143], loss=97.8462
	step [71/143], loss=96.9977
	step [72/143], loss=89.7321
	step [73/143], loss=104.6710
	step [74/143], loss=123.1500
	step [75/143], loss=104.3933
	step [76/143], loss=122.8338
	step [77/143], loss=116.9001
	step [78/143], loss=100.0860
	step [79/143], loss=109.7023
	step [80/143], loss=117.2623
	step [81/143], loss=116.4524
	step [82/143], loss=115.2997
	step [83/143], loss=102.3943
	step [84/143], loss=100.2102
	step [85/143], loss=105.8781
	step [86/143], loss=112.2331
	step [87/143], loss=113.0900
	step [88/143], loss=91.7771
	step [89/143], loss=95.8780
	step [90/143], loss=127.1075
	step [91/143], loss=109.8865
	step [92/143], loss=97.1854
	step [93/143], loss=121.9199
	step [94/143], loss=118.3741
	step [95/143], loss=102.4562
	step [96/143], loss=94.3084
	step [97/143], loss=119.4440
	step [98/143], loss=100.4366
	step [99/143], loss=118.6187
	step [100/143], loss=115.6698
	step [101/143], loss=112.0152
	step [102/143], loss=132.8848
	step [103/143], loss=112.6117
	step [104/143], loss=129.7097
	step [105/143], loss=129.8968
	step [106/143], loss=104.2198
	step [107/143], loss=110.2741
	step [108/143], loss=119.1134
	step [109/143], loss=115.3287
	step [110/143], loss=105.9366
	step [111/143], loss=109.2369
	step [112/143], loss=104.4524
	step [113/143], loss=124.4370
	step [114/143], loss=110.6043
	step [115/143], loss=95.1730
	step [116/143], loss=127.7892
	step [117/143], loss=112.1476
	step [118/143], loss=98.5413
	step [119/143], loss=118.3038
	step [120/143], loss=126.2676
	step [121/143], loss=91.1882
	step [122/143], loss=112.0618
	step [123/143], loss=117.1284
	step [124/143], loss=109.4068
	step [125/143], loss=119.4486
	step [126/143], loss=121.4729
	step [127/143], loss=121.0674
	step [128/143], loss=101.1475
	step [129/143], loss=96.1764
	step [130/143], loss=120.6920
	step [131/143], loss=114.2125
	step [132/143], loss=128.0319
	step [133/143], loss=120.2752
	step [134/143], loss=103.8799
	step [135/143], loss=103.7127
	step [136/143], loss=111.5641
	step [137/143], loss=111.8364
	step [138/143], loss=101.4995
	step [139/143], loss=128.1886
	step [140/143], loss=117.3383
	step [141/143], loss=124.5432
	step [142/143], loss=103.4079
	step [143/143], loss=85.7417
	Evaluating
	loss=0.0428, precision=0.4575, recall=0.8789, f1=0.6017
Training epoch 17
	step [1/143], loss=144.7790
	step [2/143], loss=90.6335
	step [3/143], loss=105.0022
	step [4/143], loss=115.6707
	step [5/143], loss=112.6957
	step [6/143], loss=117.7709
	step [7/143], loss=108.1528
	step [8/143], loss=94.1310
	step [9/143], loss=106.3783
	step [10/143], loss=107.2003
	step [11/143], loss=115.0114
	step [12/143], loss=110.5898
	step [13/143], loss=107.2081
	step [14/143], loss=108.7603
	step [15/143], loss=109.7885
	step [16/143], loss=111.9660
	step [17/143], loss=100.8330
	step [18/143], loss=93.6109
	step [19/143], loss=100.0566
	step [20/143], loss=117.0644
	step [21/143], loss=139.1086
	step [22/143], loss=140.2253
	step [23/143], loss=104.3555
	step [24/143], loss=99.6022
	step [25/143], loss=120.0934
	step [26/143], loss=114.5366
	step [27/143], loss=117.7787
	step [28/143], loss=89.3691
	step [29/143], loss=115.2352
	step [30/143], loss=121.7650
	step [31/143], loss=93.7957
	step [32/143], loss=127.2317
	step [33/143], loss=130.7369
	step [34/143], loss=112.9850
	step [35/143], loss=96.4212
	step [36/143], loss=121.2121
	step [37/143], loss=113.2308
	step [38/143], loss=112.5968
	step [39/143], loss=118.1682
	step [40/143], loss=105.8508
	step [41/143], loss=98.5148
	step [42/143], loss=92.9200
	step [43/143], loss=100.1545
	step [44/143], loss=105.9342
	step [45/143], loss=127.6971
	step [46/143], loss=113.6137
	step [47/143], loss=115.6536
	step [48/143], loss=107.9527
	step [49/143], loss=106.2003
	step [50/143], loss=98.8840
	step [51/143], loss=116.5236
	step [52/143], loss=110.6136
	step [53/143], loss=116.2864
	step [54/143], loss=108.0097
	step [55/143], loss=100.4921
	step [56/143], loss=114.2702
	step [57/143], loss=110.2905
	step [58/143], loss=91.4120
	step [59/143], loss=118.1901
	step [60/143], loss=99.8406
	step [61/143], loss=112.2629
	step [62/143], loss=109.3273
	step [63/143], loss=113.5271
	step [64/143], loss=89.4596
	step [65/143], loss=119.5627
	step [66/143], loss=119.5212
	step [67/143], loss=125.2203
	step [68/143], loss=107.5909
	step [69/143], loss=116.1595
	step [70/143], loss=113.1311
	step [71/143], loss=118.7209
	step [72/143], loss=99.9089
	step [73/143], loss=100.6834
	step [74/143], loss=97.5144
	step [75/143], loss=94.1785
	step [76/143], loss=95.2222
	step [77/143], loss=118.7047
	step [78/143], loss=124.7988
	step [79/143], loss=108.9173
	step [80/143], loss=105.1581
	step [81/143], loss=125.8218
	step [82/143], loss=132.6691
	step [83/143], loss=120.6888
	step [84/143], loss=96.6978
	step [85/143], loss=117.8106
	step [86/143], loss=106.2383
	step [87/143], loss=105.4919
	step [88/143], loss=104.8950
	step [89/143], loss=106.3786
	step [90/143], loss=116.9969
	step [91/143], loss=129.2414
	step [92/143], loss=109.7794
	step [93/143], loss=114.9141
	step [94/143], loss=116.8253
	step [95/143], loss=113.0962
	step [96/143], loss=88.7653
	step [97/143], loss=117.7270
	step [98/143], loss=105.4595
	step [99/143], loss=99.9042
	step [100/143], loss=98.7974
	step [101/143], loss=112.1991
	step [102/143], loss=106.0860
	step [103/143], loss=108.8393
	step [104/143], loss=100.8417
	step [105/143], loss=135.6913
	step [106/143], loss=90.1943
	step [107/143], loss=101.7376
	step [108/143], loss=104.5071
	step [109/143], loss=114.3982
	step [110/143], loss=111.6371
	step [111/143], loss=90.8688
	step [112/143], loss=118.8798
	step [113/143], loss=126.2651
	step [114/143], loss=107.5548
	step [115/143], loss=88.9691
	step [116/143], loss=109.0117
	step [117/143], loss=113.7901
	step [118/143], loss=111.1772
	step [119/143], loss=124.1456
	step [120/143], loss=109.3277
	step [121/143], loss=119.8995
	step [122/143], loss=108.5361
	step [123/143], loss=124.5538
	step [124/143], loss=113.6345
	step [125/143], loss=118.0006
	step [126/143], loss=120.0817
	step [127/143], loss=98.9207
	step [128/143], loss=119.2148
	step [129/143], loss=119.0207
	step [130/143], loss=92.6188
	step [131/143], loss=92.9896
	step [132/143], loss=123.7262
	step [133/143], loss=116.7596
	step [134/143], loss=103.8795
	step [135/143], loss=100.3086
	step [136/143], loss=102.7846
	step [137/143], loss=95.5612
	step [138/143], loss=97.5784
	step [139/143], loss=95.1530
	step [140/143], loss=120.4906
	step [141/143], loss=96.5771
	step [142/143], loss=101.1260
	step [143/143], loss=74.6754
	Evaluating
	loss=0.0369, precision=0.4518, recall=0.9087, f1=0.6035
Training epoch 18
	step [1/143], loss=126.1576
	step [2/143], loss=120.4397
	step [3/143], loss=106.6185
	step [4/143], loss=109.7511
	step [5/143], loss=109.5407
	step [6/143], loss=109.0019
	step [7/143], loss=101.4073
	step [8/143], loss=102.2727
	step [9/143], loss=112.9729
	step [10/143], loss=121.2838
	step [11/143], loss=98.6171
	step [12/143], loss=119.7347
	step [13/143], loss=100.9863
	step [14/143], loss=107.2093
	step [15/143], loss=124.0809
	step [16/143], loss=109.8235
	step [17/143], loss=109.8760
	step [18/143], loss=109.9835
	step [19/143], loss=105.6783
	step [20/143], loss=133.9555
	step [21/143], loss=109.7629
	step [22/143], loss=92.1350
	step [23/143], loss=97.2370
	step [24/143], loss=107.1486
	step [25/143], loss=105.9215
	step [26/143], loss=94.9006
	step [27/143], loss=105.8222
	step [28/143], loss=114.0158
	step [29/143], loss=116.7995
	step [30/143], loss=119.1813
	step [31/143], loss=95.9779
	step [32/143], loss=114.3822
	step [33/143], loss=107.2783
	step [34/143], loss=109.3408
	step [35/143], loss=112.0223
	step [36/143], loss=101.6032
	step [37/143], loss=95.4119
	step [38/143], loss=104.5409
	step [39/143], loss=93.4617
	step [40/143], loss=99.9751
	step [41/143], loss=93.8198
	step [42/143], loss=115.4323
	step [43/143], loss=118.9546
	step [44/143], loss=96.0115
	step [45/143], loss=103.0147
	step [46/143], loss=108.8823
	step [47/143], loss=101.6140
	step [48/143], loss=126.5632
	step [49/143], loss=113.0058
	step [50/143], loss=90.5630
	step [51/143], loss=112.6326
	step [52/143], loss=106.0657
	step [53/143], loss=121.6001
	step [54/143], loss=100.7477
	step [55/143], loss=99.7717
	step [56/143], loss=137.7480
	step [57/143], loss=93.9979
	step [58/143], loss=105.2905
	step [59/143], loss=115.9643
	step [60/143], loss=104.7909
	step [61/143], loss=109.8947
	step [62/143], loss=134.4579
	step [63/143], loss=133.4216
	step [64/143], loss=106.5613
	step [65/143], loss=124.1350
	step [66/143], loss=106.4578
	step [67/143], loss=112.6924
	step [68/143], loss=112.2854
	step [69/143], loss=108.3107
	step [70/143], loss=98.4950
	step [71/143], loss=104.0660
	step [72/143], loss=106.9283
	step [73/143], loss=121.4365
	step [74/143], loss=81.8715
	step [75/143], loss=111.9949
	step [76/143], loss=104.8965
	step [77/143], loss=90.7166
	step [78/143], loss=94.5072
	step [79/143], loss=109.5039
	step [80/143], loss=124.4248
	step [81/143], loss=93.9677
	step [82/143], loss=96.5845
	step [83/143], loss=100.9439
	step [84/143], loss=114.1274
	step [85/143], loss=105.1072
	step [86/143], loss=116.7581
	step [87/143], loss=92.4600
	step [88/143], loss=103.4470
	step [89/143], loss=123.5041
	step [90/143], loss=120.3131
	step [91/143], loss=115.1012
	step [92/143], loss=109.4593
	step [93/143], loss=100.5731
	step [94/143], loss=90.5066
	step [95/143], loss=114.5122
	step [96/143], loss=92.3517
	step [97/143], loss=118.6015
	step [98/143], loss=101.3384
	step [99/143], loss=86.8369
	step [100/143], loss=114.1816
	step [101/143], loss=115.5649
	step [102/143], loss=99.0502
	step [103/143], loss=116.4431
	step [104/143], loss=95.4662
	step [105/143], loss=101.8120
	step [106/143], loss=122.9873
	step [107/143], loss=110.0098
	step [108/143], loss=98.9243
	step [109/143], loss=109.8966
	step [110/143], loss=104.1664
	step [111/143], loss=114.3188
	step [112/143], loss=97.1334
	step [113/143], loss=100.8970
	step [114/143], loss=112.6838
	step [115/143], loss=113.1972
	step [116/143], loss=94.6534
	step [117/143], loss=117.7060
	step [118/143], loss=98.5406
	step [119/143], loss=110.6775
	step [120/143], loss=116.8547
	step [121/143], loss=119.0754
	step [122/143], loss=117.9852
	step [123/143], loss=125.3429
	step [124/143], loss=102.4873
	step [125/143], loss=87.8327
	step [126/143], loss=102.3437
	step [127/143], loss=114.8156
	step [128/143], loss=120.8608
	step [129/143], loss=118.4652
	step [130/143], loss=107.8591
	step [131/143], loss=107.4727
	step [132/143], loss=106.3307
	step [133/143], loss=99.6040
	step [134/143], loss=108.6057
	step [135/143], loss=101.6039
	step [136/143], loss=116.9978
	step [137/143], loss=112.1701
	step [138/143], loss=113.5952
	step [139/143], loss=115.2776
	step [140/143], loss=104.2505
	step [141/143], loss=117.8119
	step [142/143], loss=90.1943
	step [143/143], loss=84.4668
	Evaluating
	loss=0.0355, precision=0.5099, recall=0.8929, f1=0.6491
Training epoch 19
	step [1/143], loss=110.5138
	step [2/143], loss=113.4425
	step [3/143], loss=112.4657
	step [4/143], loss=102.3231
	step [5/143], loss=101.6898
	step [6/143], loss=102.6144
	step [7/143], loss=89.4093
	step [8/143], loss=114.5515
	step [9/143], loss=118.7155
	step [10/143], loss=108.4601
	step [11/143], loss=104.6168
	step [12/143], loss=95.1415
	step [13/143], loss=118.7251
	step [14/143], loss=113.5016
	step [15/143], loss=111.0024
	step [16/143], loss=122.4784
	step [17/143], loss=117.9608
	step [18/143], loss=101.7832
	step [19/143], loss=113.2942
	step [20/143], loss=111.8717
	step [21/143], loss=114.2458
	step [22/143], loss=94.2093
	step [23/143], loss=98.6860
	step [24/143], loss=118.4614
	step [25/143], loss=102.9809
	step [26/143], loss=100.4057
	step [27/143], loss=102.0209
	step [28/143], loss=113.6017
	step [29/143], loss=106.3902
	step [30/143], loss=105.1117
	step [31/143], loss=111.8484
	step [32/143], loss=102.7037
	step [33/143], loss=106.0413
	step [34/143], loss=116.0002
	step [35/143], loss=110.5477
	step [36/143], loss=108.4631
	step [37/143], loss=93.3811
	step [38/143], loss=106.5182
	step [39/143], loss=113.7313
	step [40/143], loss=119.9027
	step [41/143], loss=116.3362
	step [42/143], loss=114.8022
	step [43/143], loss=111.0159
	step [44/143], loss=110.8042
	step [45/143], loss=123.2526
	step [46/143], loss=89.2400
	step [47/143], loss=129.9718
	step [48/143], loss=122.6464
	step [49/143], loss=88.9286
	step [50/143], loss=106.0599
	step [51/143], loss=116.5580
	step [52/143], loss=102.6770
	step [53/143], loss=95.9883
	step [54/143], loss=101.1559
	step [55/143], loss=107.9604
	step [56/143], loss=98.9400
	step [57/143], loss=94.9267
	step [58/143], loss=97.7691
	step [59/143], loss=104.8094
	step [60/143], loss=107.3601
	step [61/143], loss=119.8848
	step [62/143], loss=90.5388
	step [63/143], loss=108.3341
	step [64/143], loss=115.9992
	step [65/143], loss=105.8071
	step [66/143], loss=118.7016
	step [67/143], loss=110.9937
	step [68/143], loss=110.8767
	step [69/143], loss=135.4670
	step [70/143], loss=106.2967
	step [71/143], loss=105.2390
	step [72/143], loss=96.0541
	step [73/143], loss=112.6268
	step [74/143], loss=99.9044
	step [75/143], loss=95.6208
	step [76/143], loss=111.9031
	step [77/143], loss=112.3242
	step [78/143], loss=99.3857
	step [79/143], loss=102.9273
	step [80/143], loss=105.3639
	step [81/143], loss=97.9861
	step [82/143], loss=98.9769
	step [83/143], loss=123.3817
	step [84/143], loss=106.4634
	step [85/143], loss=99.6303
	step [86/143], loss=110.9391
	step [87/143], loss=123.5432
	step [88/143], loss=102.1599
	step [89/143], loss=86.1994
	step [90/143], loss=113.5341
	step [91/143], loss=116.4025
	step [92/143], loss=99.5257
	step [93/143], loss=114.1563
	step [94/143], loss=117.7282
	step [95/143], loss=125.7620
	step [96/143], loss=93.4810
	step [97/143], loss=119.7244
	step [98/143], loss=105.0219
	step [99/143], loss=124.7649
	step [100/143], loss=95.0636
	step [101/143], loss=99.9689
	step [102/143], loss=105.1112
	step [103/143], loss=101.0518
	step [104/143], loss=115.4636
	step [105/143], loss=100.3686
	step [106/143], loss=100.7191
	step [107/143], loss=116.1618
	step [108/143], loss=94.0141
	step [109/143], loss=118.3411
	step [110/143], loss=115.3532
	step [111/143], loss=110.9971
	step [112/143], loss=121.2044
	step [113/143], loss=102.1828
	step [114/143], loss=85.5924
	step [115/143], loss=105.7597
	step [116/143], loss=95.2328
	step [117/143], loss=93.7829
	step [118/143], loss=83.8993
	step [119/143], loss=92.8325
	step [120/143], loss=132.9103
	step [121/143], loss=102.4633
	step [122/143], loss=80.0702
	step [123/143], loss=113.5355
	step [124/143], loss=100.6244
	step [125/143], loss=115.5117
	step [126/143], loss=111.1901
	step [127/143], loss=108.3906
	step [128/143], loss=98.4586
	step [129/143], loss=96.0835
	step [130/143], loss=106.1438
	step [131/143], loss=96.6214
	step [132/143], loss=116.2641
	step [133/143], loss=110.8501
	step [134/143], loss=105.5078
	step [135/143], loss=96.5408
	step [136/143], loss=103.7275
	step [137/143], loss=92.0342
	step [138/143], loss=99.4122
	step [139/143], loss=98.9500
	step [140/143], loss=109.4808
	step [141/143], loss=117.0783
	step [142/143], loss=125.7601
	step [143/143], loss=76.3930
	Evaluating
	loss=0.0325, precision=0.4586, recall=0.8868, f1=0.6045
Training epoch 20
	step [1/143], loss=104.9365
	step [2/143], loss=99.7989
	step [3/143], loss=128.0359
	step [4/143], loss=106.2645
	step [5/143], loss=109.6073
	step [6/143], loss=100.1197
	step [7/143], loss=100.6134
	step [8/143], loss=110.8030
	step [9/143], loss=114.3347
	step [10/143], loss=104.4453
	step [11/143], loss=82.2689
	step [12/143], loss=98.3175
	step [13/143], loss=100.1966
	step [14/143], loss=100.1708
	step [15/143], loss=104.1778
	step [16/143], loss=108.5528
	step [17/143], loss=121.1943
	step [18/143], loss=97.7485
	step [19/143], loss=118.2999
	step [20/143], loss=107.7352
	step [21/143], loss=105.2629
	step [22/143], loss=114.7659
	step [23/143], loss=112.3886
	step [24/143], loss=117.5172
	step [25/143], loss=107.5372
	step [26/143], loss=99.1020
	step [27/143], loss=97.0677
	step [28/143], loss=97.4290
	step [29/143], loss=98.6535
	step [30/143], loss=93.1443
	step [31/143], loss=110.6102
	step [32/143], loss=111.6239
	step [33/143], loss=109.3324
	step [34/143], loss=89.4511
	step [35/143], loss=89.2835
	step [36/143], loss=102.7060
	step [37/143], loss=113.2965
	step [38/143], loss=95.2377
	step [39/143], loss=90.1911
	step [40/143], loss=124.7449
	step [41/143], loss=111.5081
	step [42/143], loss=96.3160
	step [43/143], loss=108.2331
	step [44/143], loss=106.2000
	step [45/143], loss=101.3752
	step [46/143], loss=106.3387
	step [47/143], loss=110.6738
	step [48/143], loss=109.2890
	step [49/143], loss=100.4648
	step [50/143], loss=94.9213
	step [51/143], loss=94.7845
	step [52/143], loss=121.0237
	step [53/143], loss=103.8946
	step [54/143], loss=89.4260
	step [55/143], loss=86.6476
	step [56/143], loss=119.0935
	step [57/143], loss=91.5624
	step [58/143], loss=89.8671
	step [59/143], loss=116.2788
	step [60/143], loss=113.4895
	step [61/143], loss=93.5903
	step [62/143], loss=122.6661
	step [63/143], loss=103.0478
	step [64/143], loss=100.2025
	step [65/143], loss=88.4910
	step [66/143], loss=100.5736
	step [67/143], loss=125.5425
	step [68/143], loss=110.5957
	step [69/143], loss=94.7941
	step [70/143], loss=79.3115
	step [71/143], loss=101.0717
	step [72/143], loss=107.3258
	step [73/143], loss=102.7084
	step [74/143], loss=98.1183
	step [75/143], loss=115.4560
	step [76/143], loss=114.6906
	step [77/143], loss=109.2055
	step [78/143], loss=105.7396
	step [79/143], loss=100.8235
	step [80/143], loss=102.3689
	step [81/143], loss=97.4749
	step [82/143], loss=118.1935
	step [83/143], loss=119.8435
	step [84/143], loss=108.6825
	step [85/143], loss=113.7468
	step [86/143], loss=99.7213
	step [87/143], loss=101.9184
	step [88/143], loss=123.2323
	step [89/143], loss=99.6227
	step [90/143], loss=105.8544
	step [91/143], loss=109.8531
	step [92/143], loss=86.3816
	step [93/143], loss=107.7470
	step [94/143], loss=99.0089
	step [95/143], loss=110.8530
	step [96/143], loss=110.7541
	step [97/143], loss=99.6979
	step [98/143], loss=120.5628
	step [99/143], loss=103.3591
	step [100/143], loss=108.1031
	step [101/143], loss=105.5992
	step [102/143], loss=105.4450
	step [103/143], loss=111.5068
	step [104/143], loss=106.1702
	step [105/143], loss=103.8447
	step [106/143], loss=101.2438
	step [107/143], loss=85.9585
	step [108/143], loss=122.8897
	step [109/143], loss=88.6282
	step [110/143], loss=114.2951
	step [111/143], loss=129.3878
	step [112/143], loss=89.8927
	step [113/143], loss=109.6266
	step [114/143], loss=100.4692
	step [115/143], loss=102.4943
	step [116/143], loss=110.6503
	step [117/143], loss=131.9445
	step [118/143], loss=106.2116
	step [119/143], loss=102.7178
	step [120/143], loss=105.2291
	step [121/143], loss=95.6179
	step [122/143], loss=100.6068
	step [123/143], loss=101.0897
	step [124/143], loss=93.8304
	step [125/143], loss=111.3799
	step [126/143], loss=108.8332
	step [127/143], loss=98.0010
	step [128/143], loss=87.3903
	step [129/143], loss=105.3133
	step [130/143], loss=118.6766
	step [131/143], loss=100.2337
	step [132/143], loss=94.8905
	step [133/143], loss=107.6136
	step [134/143], loss=110.6029
	step [135/143], loss=110.0992
	step [136/143], loss=104.1319
	step [137/143], loss=125.6735
	step [138/143], loss=100.6029
	step [139/143], loss=112.9684
	step [140/143], loss=116.0690
	step [141/143], loss=119.8678
	step [142/143], loss=103.6397
	step [143/143], loss=92.5288
	Evaluating
	loss=0.0306, precision=0.4196, recall=0.8726, f1=0.5667
Training epoch 21
	step [1/143], loss=104.2012
	step [2/143], loss=134.6720
	step [3/143], loss=97.2981
	step [4/143], loss=114.8741
	step [5/143], loss=95.1019
	step [6/143], loss=98.2558
	step [7/143], loss=103.9819
	step [8/143], loss=104.1425
	step [9/143], loss=87.9668
	step [10/143], loss=129.9272
	step [11/143], loss=91.3116
	step [12/143], loss=105.7391
	step [13/143], loss=86.2162
	step [14/143], loss=93.9042
	step [15/143], loss=111.5589
	step [16/143], loss=105.2934
	step [17/143], loss=114.9464
	step [18/143], loss=103.1648
	step [19/143], loss=109.5379
	step [20/143], loss=105.2239
	step [21/143], loss=102.7907
	step [22/143], loss=110.1988
	step [23/143], loss=111.0585
	step [24/143], loss=109.7565
	step [25/143], loss=104.6384
	step [26/143], loss=110.3952
	step [27/143], loss=103.0398
	step [28/143], loss=103.1924
	step [29/143], loss=116.8326
	step [30/143], loss=96.4479
	step [31/143], loss=100.0064
	step [32/143], loss=104.1782
	step [33/143], loss=114.1727
	step [34/143], loss=105.1279
	step [35/143], loss=98.0160
	step [36/143], loss=100.8324
	step [37/143], loss=110.3537
	step [38/143], loss=97.5548
	step [39/143], loss=100.3213
	step [40/143], loss=105.9516
	step [41/143], loss=123.6228
	step [42/143], loss=92.9543
	step [43/143], loss=109.8833
	step [44/143], loss=121.6612
	step [45/143], loss=107.0305
	step [46/143], loss=107.2799
	step [47/143], loss=102.5133
	step [48/143], loss=105.9353
	step [49/143], loss=90.5293
	step [50/143], loss=117.4085
	step [51/143], loss=99.6940
	step [52/143], loss=106.1296
	step [53/143], loss=109.5541
	step [54/143], loss=104.2500
	step [55/143], loss=107.3180
	step [56/143], loss=99.5661
	step [57/143], loss=95.1163
	step [58/143], loss=98.2592
	step [59/143], loss=108.3867
	step [60/143], loss=117.3071
	step [61/143], loss=91.4923
	step [62/143], loss=95.5608
	step [63/143], loss=101.1189
	step [64/143], loss=96.4097
	step [65/143], loss=95.9626
	step [66/143], loss=100.3265
	step [67/143], loss=112.2274
	step [68/143], loss=104.9366
	step [69/143], loss=93.8411
	step [70/143], loss=100.8412
	step [71/143], loss=105.0982
	step [72/143], loss=95.4521
	step [73/143], loss=101.4270
	step [74/143], loss=101.7514
	step [75/143], loss=107.4032
	step [76/143], loss=94.2448
	step [77/143], loss=118.7551
	step [78/143], loss=120.8629
	step [79/143], loss=109.6503
	step [80/143], loss=85.8767
	step [81/143], loss=118.7631
	step [82/143], loss=94.5525
	step [83/143], loss=88.7893
	step [84/143], loss=100.5765
	step [85/143], loss=89.4495
	step [86/143], loss=104.6626
	step [87/143], loss=95.9050
	step [88/143], loss=115.4673
	step [89/143], loss=109.7724
	step [90/143], loss=88.7002
	step [91/143], loss=111.0197
	step [92/143], loss=120.0537
	step [93/143], loss=92.9249
	step [94/143], loss=100.3608
	step [95/143], loss=89.6326
	step [96/143], loss=98.5791
	step [97/143], loss=95.1230
	step [98/143], loss=105.1618
	step [99/143], loss=106.2061
	step [100/143], loss=100.1789
	step [101/143], loss=100.7606
	step [102/143], loss=82.2483
	step [103/143], loss=108.9152
	step [104/143], loss=114.6626
	step [105/143], loss=94.4723
	step [106/143], loss=113.5397
	step [107/143], loss=107.7455
	step [108/143], loss=96.8877
	step [109/143], loss=84.6126
	step [110/143], loss=107.1700
	step [111/143], loss=128.7948
	step [112/143], loss=95.0298
	step [113/143], loss=100.3256
	step [114/143], loss=108.5231
	step [115/143], loss=86.7220
	step [116/143], loss=114.0231
	step [117/143], loss=101.0690
	step [118/143], loss=105.2205
	step [119/143], loss=115.4201
	step [120/143], loss=113.2399
	step [121/143], loss=104.1344
	step [122/143], loss=94.2072
	step [123/143], loss=105.6669
	step [124/143], loss=93.7302
	step [125/143], loss=107.4808
	step [126/143], loss=100.1739
	step [127/143], loss=107.1559
	step [128/143], loss=102.2081
	step [129/143], loss=115.2083
	step [130/143], loss=117.2881
	step [131/143], loss=100.5470
	step [132/143], loss=102.3394
	step [133/143], loss=91.5498
	step [134/143], loss=119.7623
	step [135/143], loss=105.8901
	step [136/143], loss=111.2464
	step [137/143], loss=93.9951
	step [138/143], loss=93.9404
	step [139/143], loss=93.0599
	step [140/143], loss=104.2835
	step [141/143], loss=104.3197
	step [142/143], loss=124.6265
	step [143/143], loss=84.5196
	Evaluating
	loss=0.0258, precision=0.4895, recall=0.8910, f1=0.6318
Training epoch 22
	step [1/143], loss=78.4151
	step [2/143], loss=96.9803
	step [3/143], loss=98.2991
	step [4/143], loss=88.0616
	step [5/143], loss=95.6115
	step [6/143], loss=115.4939
	step [7/143], loss=99.7853
	step [8/143], loss=116.2946
	step [9/143], loss=112.6518
	step [10/143], loss=118.9796
	step [11/143], loss=118.2071
	step [12/143], loss=94.1888
	step [13/143], loss=109.0641
	step [14/143], loss=109.4571
	step [15/143], loss=122.3468
	step [16/143], loss=118.5428
	step [17/143], loss=118.6901
	step [18/143], loss=93.1390
	step [19/143], loss=97.3688
	step [20/143], loss=113.9320
	step [21/143], loss=88.8658
	step [22/143], loss=90.4523
	step [23/143], loss=95.7952
	step [24/143], loss=120.7023
	step [25/143], loss=106.1029
	step [26/143], loss=116.6302
	step [27/143], loss=89.2967
	step [28/143], loss=101.4994
	step [29/143], loss=103.5979
	step [30/143], loss=92.6444
	step [31/143], loss=85.2089
	step [32/143], loss=90.8845
	step [33/143], loss=98.0079
	step [34/143], loss=103.9966
	step [35/143], loss=106.1050
	step [36/143], loss=71.0435
	step [37/143], loss=91.1442
	step [38/143], loss=105.6922
	step [39/143], loss=109.6215
	step [40/143], loss=86.5526
	step [41/143], loss=126.0129
	step [42/143], loss=92.6381
	step [43/143], loss=109.7370
	step [44/143], loss=109.3862
	step [45/143], loss=99.8973
	step [46/143], loss=110.9447
	step [47/143], loss=104.6952
	step [48/143], loss=104.4405
	step [49/143], loss=100.3070
	step [50/143], loss=95.7927
	step [51/143], loss=110.4799
	step [52/143], loss=90.7024
	step [53/143], loss=98.5173
	step [54/143], loss=102.7948
	step [55/143], loss=108.3434
	step [56/143], loss=94.8917
	step [57/143], loss=112.9659
	step [58/143], loss=82.5379
	step [59/143], loss=108.1761
	step [60/143], loss=93.2699
	step [61/143], loss=101.3561
	step [62/143], loss=94.1418
	step [63/143], loss=102.5657
	step [64/143], loss=105.8049
	step [65/143], loss=76.1228
	step [66/143], loss=90.1184
	step [67/143], loss=105.1638
	step [68/143], loss=112.5890
	step [69/143], loss=102.1613
	step [70/143], loss=101.3976
	step [71/143], loss=103.4814
	step [72/143], loss=95.6133
	step [73/143], loss=109.8020
	step [74/143], loss=106.4442
	step [75/143], loss=99.3369
	step [76/143], loss=101.6561
	step [77/143], loss=118.1098
	step [78/143], loss=125.4539
	step [79/143], loss=94.9266
	step [80/143], loss=110.8435
	step [81/143], loss=110.9523
	step [82/143], loss=105.6632
	step [83/143], loss=110.9045
	step [84/143], loss=112.2678
	step [85/143], loss=117.5655
	step [86/143], loss=113.5697
	step [87/143], loss=123.2746
	step [88/143], loss=121.7744
	step [89/143], loss=100.6094
	step [90/143], loss=77.6102
	step [91/143], loss=104.7162
	step [92/143], loss=92.3121
	step [93/143], loss=107.7924
	step [94/143], loss=91.0290
	step [95/143], loss=101.5316
	step [96/143], loss=96.9271
	step [97/143], loss=106.8077
	step [98/143], loss=89.6284
	step [99/143], loss=101.0592
	step [100/143], loss=100.1500
	step [101/143], loss=126.5023
	step [102/143], loss=100.5759
	step [103/143], loss=121.8641
	step [104/143], loss=106.8051
	step [105/143], loss=92.1421
	step [106/143], loss=98.6095
	step [107/143], loss=89.9660
	step [108/143], loss=108.4822
	step [109/143], loss=108.2201
	step [110/143], loss=82.1440
	step [111/143], loss=104.2990
	step [112/143], loss=95.0280
	step [113/143], loss=95.0669
	step [114/143], loss=100.0488
	step [115/143], loss=112.8163
	step [116/143], loss=98.1791
	step [117/143], loss=95.4936
	step [118/143], loss=87.5010
	step [119/143], loss=115.1273
	step [120/143], loss=101.5909
	step [121/143], loss=85.0636
	step [122/143], loss=85.9422
	step [123/143], loss=106.5549
	step [124/143], loss=113.1857
	step [125/143], loss=93.4061
	step [126/143], loss=106.6592
	step [127/143], loss=100.8617
	step [128/143], loss=99.5403
	step [129/143], loss=103.1012
	step [130/143], loss=90.4692
	step [131/143], loss=96.3068
	step [132/143], loss=110.7479
	step [133/143], loss=100.5373
	step [134/143], loss=123.3588
	step [135/143], loss=99.2131
	step [136/143], loss=99.4578
	step [137/143], loss=89.9467
	step [138/143], loss=97.6940
	step [139/143], loss=99.4715
	step [140/143], loss=102.4687
	step [141/143], loss=113.1673
	step [142/143], loss=102.6016
	step [143/143], loss=92.5690
	Evaluating
	loss=0.0270, precision=0.4458, recall=0.8670, f1=0.5888
Training epoch 23
	step [1/143], loss=91.1190
	step [2/143], loss=99.7325
	step [3/143], loss=101.1996
	step [4/143], loss=112.4676
	step [5/143], loss=100.8971
	step [6/143], loss=106.7141
	step [7/143], loss=87.6340
	step [8/143], loss=92.0738
	step [9/143], loss=93.0188
	step [10/143], loss=101.8127
	step [11/143], loss=95.2492
	step [12/143], loss=98.1689
	step [13/143], loss=89.1957
	step [14/143], loss=101.9410
	step [15/143], loss=100.1425
	step [16/143], loss=105.8373
	step [17/143], loss=114.1920
	step [18/143], loss=110.5503
	step [19/143], loss=103.3657
	step [20/143], loss=97.3495
	step [21/143], loss=120.1440
	step [22/143], loss=114.4200
	step [23/143], loss=108.8633
	step [24/143], loss=87.4826
	step [25/143], loss=118.1558
	step [26/143], loss=93.5283
	step [27/143], loss=108.4632
	step [28/143], loss=102.8697
	step [29/143], loss=92.5974
	step [30/143], loss=110.0855
	step [31/143], loss=99.8668
	step [32/143], loss=108.8880
	step [33/143], loss=106.1756
	step [34/143], loss=75.4991
	step [35/143], loss=105.9995
	step [36/143], loss=100.0627
	step [37/143], loss=101.8160
	step [38/143], loss=98.7599
	step [39/143], loss=88.2837
	step [40/143], loss=106.8497
	step [41/143], loss=97.5369
	step [42/143], loss=112.5119
	step [43/143], loss=93.1007
	step [44/143], loss=91.7308
	step [45/143], loss=97.2355
	step [46/143], loss=97.8243
	step [47/143], loss=106.0671
	step [48/143], loss=98.3224
	step [49/143], loss=84.3813
	step [50/143], loss=98.5423
	step [51/143], loss=106.9066
	step [52/143], loss=95.1282
	step [53/143], loss=115.2790
	step [54/143], loss=89.1197
	step [55/143], loss=81.1282
	step [56/143], loss=95.7469
	step [57/143], loss=124.7494
	step [58/143], loss=109.3548
	step [59/143], loss=104.7667
	step [60/143], loss=106.1496
	step [61/143], loss=100.2666
	step [62/143], loss=100.9364
	step [63/143], loss=109.7806
	step [64/143], loss=110.3307
	step [65/143], loss=112.2532
	step [66/143], loss=91.8523
	step [67/143], loss=101.9367
	step [68/143], loss=114.2746
	step [69/143], loss=91.8387
	step [70/143], loss=94.9603
	step [71/143], loss=105.3700
	step [72/143], loss=97.0661
	step [73/143], loss=92.5496
	step [74/143], loss=103.4520
	step [75/143], loss=99.6880
	step [76/143], loss=106.7558
	step [77/143], loss=111.3302
	step [78/143], loss=91.6028
	step [79/143], loss=91.3004
	step [80/143], loss=96.1197
	step [81/143], loss=105.4973
	step [82/143], loss=110.2890
	step [83/143], loss=99.8672
	step [84/143], loss=113.8514
	step [85/143], loss=109.5103
	step [86/143], loss=104.4661
	step [87/143], loss=100.0521
	step [88/143], loss=90.7311
	step [89/143], loss=86.5949
	step [90/143], loss=89.3170
	step [91/143], loss=100.6789
	step [92/143], loss=106.6789
	step [93/143], loss=98.0518
	step [94/143], loss=105.5629
	step [95/143], loss=79.5314
	step [96/143], loss=107.2249
	step [97/143], loss=90.2793
	step [98/143], loss=99.2718
	step [99/143], loss=100.1159
	step [100/143], loss=115.0274
	step [101/143], loss=100.9014
	step [102/143], loss=87.2662
	step [103/143], loss=93.8414
	step [104/143], loss=107.1023
	step [105/143], loss=108.5266
	step [106/143], loss=94.8339
	step [107/143], loss=82.6327
	step [108/143], loss=104.3148
	step [109/143], loss=131.4840
	step [110/143], loss=73.0470
	step [111/143], loss=105.1692
	step [112/143], loss=106.6180
	step [113/143], loss=118.6309
	step [114/143], loss=96.9987
	step [115/143], loss=110.8149
	step [116/143], loss=97.5963
	step [117/143], loss=124.5266
	step [118/143], loss=111.9472
	step [119/143], loss=86.9715
	step [120/143], loss=117.2807
	step [121/143], loss=86.1270
	step [122/143], loss=94.8881
	step [123/143], loss=106.7929
	step [124/143], loss=119.4207
	step [125/143], loss=105.6351
	step [126/143], loss=93.3118
	step [127/143], loss=83.8662
	step [128/143], loss=92.1801
	step [129/143], loss=115.3905
	step [130/143], loss=87.1485
	step [131/143], loss=83.4581
	step [132/143], loss=108.0743
	step [133/143], loss=101.0524
	step [134/143], loss=85.7929
	step [135/143], loss=88.6821
	step [136/143], loss=89.0079
	step [137/143], loss=103.0839
	step [138/143], loss=121.7696
	step [139/143], loss=83.1919
	step [140/143], loss=97.6774
	step [141/143], loss=122.0035
	step [142/143], loss=103.7335
	step [143/143], loss=92.4925
	Evaluating
	loss=0.0227, precision=0.4836, recall=0.8763, f1=0.6232
Training epoch 24
	step [1/143], loss=105.2161
	step [2/143], loss=116.0056
	step [3/143], loss=100.0364
	step [4/143], loss=102.0349
	step [5/143], loss=103.4171
	step [6/143], loss=83.6750
	step [7/143], loss=88.2201
	step [8/143], loss=114.6154
	step [9/143], loss=101.6287
	step [10/143], loss=105.9604
	step [11/143], loss=86.2175
	step [12/143], loss=95.9474
	step [13/143], loss=94.9865
	step [14/143], loss=96.1610
	step [15/143], loss=115.9979
	step [16/143], loss=113.3835
	step [17/143], loss=92.6019
	step [18/143], loss=80.0151
	step [19/143], loss=110.9194
	step [20/143], loss=102.1482
	step [21/143], loss=103.9904
	step [22/143], loss=94.8930
	step [23/143], loss=118.7127
	step [24/143], loss=82.8075
	step [25/143], loss=104.0369
	step [26/143], loss=95.2410
	step [27/143], loss=103.2954
	step [28/143], loss=96.3288
	step [29/143], loss=96.1446
	step [30/143], loss=110.1930
	step [31/143], loss=110.7420
	step [32/143], loss=110.2519
	step [33/143], loss=100.6471
	step [34/143], loss=75.8280
	step [35/143], loss=105.9745
	step [36/143], loss=90.2880
	step [37/143], loss=100.5202
	step [38/143], loss=95.5019
	step [39/143], loss=108.4447
	step [40/143], loss=106.2943
	step [41/143], loss=96.1533
	step [42/143], loss=110.5987
	step [43/143], loss=102.1942
	step [44/143], loss=93.5174
	step [45/143], loss=87.5770
	step [46/143], loss=89.7025
	step [47/143], loss=94.4615
	step [48/143], loss=86.6361
	step [49/143], loss=106.4909
	step [50/143], loss=95.6431
	step [51/143], loss=72.9332
	step [52/143], loss=91.7394
	step [53/143], loss=107.0692
	step [54/143], loss=87.1298
	step [55/143], loss=95.6187
	step [56/143], loss=104.0863
	step [57/143], loss=102.9581
	step [58/143], loss=89.7014
	step [59/143], loss=77.2411
	step [60/143], loss=115.6029
	step [61/143], loss=97.7337
	step [62/143], loss=90.2022
	step [63/143], loss=96.6048
	step [64/143], loss=103.4549
	step [65/143], loss=91.9874
	step [66/143], loss=94.9200
	step [67/143], loss=106.4926
	step [68/143], loss=104.2776
	step [69/143], loss=97.7099
	step [70/143], loss=97.5688
	step [71/143], loss=95.3909
	step [72/143], loss=85.5671
	step [73/143], loss=107.3946
	step [74/143], loss=109.8173
	step [75/143], loss=88.9673
	step [76/143], loss=100.1048
	step [77/143], loss=82.5460
	step [78/143], loss=111.8272
	step [79/143], loss=106.2667
	step [80/143], loss=103.7363
	step [81/143], loss=90.5911
	step [82/143], loss=98.2104
	step [83/143], loss=78.3197
	step [84/143], loss=105.4823
	step [85/143], loss=103.2166
	step [86/143], loss=94.3590
	step [87/143], loss=96.9533
	step [88/143], loss=126.7296
	step [89/143], loss=85.8893
	step [90/143], loss=106.1704
	step [91/143], loss=107.8886
	step [92/143], loss=94.3442
	step [93/143], loss=104.2819
	step [94/143], loss=98.1262
	step [95/143], loss=95.7790
	step [96/143], loss=125.6875
	step [97/143], loss=104.0684
	step [98/143], loss=97.5790
	step [99/143], loss=115.3857
	step [100/143], loss=102.4455
	step [101/143], loss=103.6140
	step [102/143], loss=106.8197
	step [103/143], loss=98.1381
	step [104/143], loss=107.5623
	step [105/143], loss=103.0782
	step [106/143], loss=112.8797
	step [107/143], loss=90.0110
	step [108/143], loss=114.6292
	step [109/143], loss=99.0059
	step [110/143], loss=109.6291
	step [111/143], loss=101.8409
	step [112/143], loss=93.2433
	step [113/143], loss=106.2935
	step [114/143], loss=93.2061
	step [115/143], loss=107.7255
	step [116/143], loss=96.1779
	step [117/143], loss=106.1305
	step [118/143], loss=102.7395
	step [119/143], loss=88.5261
	step [120/143], loss=95.0802
	step [121/143], loss=102.5772
	step [122/143], loss=100.8757
	step [123/143], loss=91.2611
	step [124/143], loss=93.7177
	step [125/143], loss=102.6300
	step [126/143], loss=109.2983
	step [127/143], loss=89.4586
	step [128/143], loss=99.2558
	step [129/143], loss=111.1185
	step [130/143], loss=96.9218
	step [131/143], loss=88.1430
	step [132/143], loss=111.2918
	step [133/143], loss=97.6408
	step [134/143], loss=90.0890
	step [135/143], loss=82.2077
	step [136/143], loss=83.6773
	step [137/143], loss=96.7539
	step [138/143], loss=93.8514
	step [139/143], loss=78.8206
	step [140/143], loss=111.1841
	step [141/143], loss=91.4314
	step [142/143], loss=79.9593
	step [143/143], loss=80.1808
	Evaluating
	loss=0.0198, precision=0.5649, recall=0.8482, f1=0.6782
Training epoch 25
	step [1/143], loss=76.3182
	step [2/143], loss=103.3346
	step [3/143], loss=111.8422
	step [4/143], loss=89.0997
	step [5/143], loss=90.5027
	step [6/143], loss=96.0919
	step [7/143], loss=106.1542
	step [8/143], loss=96.9250
	step [9/143], loss=108.1332
	step [10/143], loss=101.5119
	step [11/143], loss=89.4729
	step [12/143], loss=89.9990
	step [13/143], loss=93.5088
	step [14/143], loss=104.5485
	step [15/143], loss=95.3795
	step [16/143], loss=79.4089
	step [17/143], loss=101.9741
	step [18/143], loss=107.5375
	step [19/143], loss=93.7006
	step [20/143], loss=85.5253
	step [21/143], loss=93.7520
	step [22/143], loss=91.5140
	step [23/143], loss=119.2373
	step [24/143], loss=110.5691
	step [25/143], loss=104.5627
	step [26/143], loss=87.3439
	step [27/143], loss=90.9537
	step [28/143], loss=114.9087
	step [29/143], loss=107.0335
	step [30/143], loss=114.1486
	step [31/143], loss=100.6617
	step [32/143], loss=91.2698
	step [33/143], loss=101.6914
	step [34/143], loss=99.8891
	step [35/143], loss=97.0088
	step [36/143], loss=103.9194
	step [37/143], loss=108.7533
	step [38/143], loss=95.6159
	step [39/143], loss=105.1655
	step [40/143], loss=117.7195
	step [41/143], loss=100.9152
	step [42/143], loss=93.6879
	step [43/143], loss=77.3488
	step [44/143], loss=88.9100
	step [45/143], loss=106.9836
	step [46/143], loss=115.6933
	step [47/143], loss=111.5108
	step [48/143], loss=113.8706
	step [49/143], loss=83.9736
	step [50/143], loss=102.3588
	step [51/143], loss=82.1955
	step [52/143], loss=102.1957
	step [53/143], loss=103.2350
	step [54/143], loss=100.8367
	step [55/143], loss=89.5884
	step [56/143], loss=100.1023
	step [57/143], loss=98.7374
	step [58/143], loss=105.8461
	step [59/143], loss=121.0637
	step [60/143], loss=86.1223
	step [61/143], loss=102.2351
	step [62/143], loss=81.8721
	step [63/143], loss=93.2130
	step [64/143], loss=84.4020
	step [65/143], loss=94.1956
	step [66/143], loss=101.1635
	step [67/143], loss=100.3829
	step [68/143], loss=109.9607
	step [69/143], loss=90.4362
	step [70/143], loss=97.4108
	step [71/143], loss=106.5077
	step [72/143], loss=118.2963
	step [73/143], loss=86.7644
	step [74/143], loss=101.4182
	step [75/143], loss=107.4094
	step [76/143], loss=81.4454
	step [77/143], loss=88.5181
	step [78/143], loss=102.8775
	step [79/143], loss=96.3415
	step [80/143], loss=92.0540
	step [81/143], loss=74.9999
	step [82/143], loss=94.9717
	step [83/143], loss=101.0647
	step [84/143], loss=103.6703
	step [85/143], loss=95.9413
	step [86/143], loss=110.8603
	step [87/143], loss=88.8202
	step [88/143], loss=94.0660
	step [89/143], loss=99.1551
	step [90/143], loss=86.0509
	step [91/143], loss=81.9599
	step [92/143], loss=83.3927
	step [93/143], loss=97.7334
	step [94/143], loss=101.5541
	step [95/143], loss=110.5027
	step [96/143], loss=105.5253
	step [97/143], loss=94.0248
	step [98/143], loss=107.3916
	step [99/143], loss=96.2123
	step [100/143], loss=109.8003
	step [101/143], loss=86.8566
	step [102/143], loss=120.3031
	step [103/143], loss=109.7869
	step [104/143], loss=97.5420
	step [105/143], loss=88.6748
	step [106/143], loss=103.3504
	step [107/143], loss=106.3086
	step [108/143], loss=81.7988
	step [109/143], loss=112.7166
	step [110/143], loss=112.3467
	step [111/143], loss=115.7626
	step [112/143], loss=115.2174
	step [113/143], loss=103.1552
	step [114/143], loss=96.7488
	step [115/143], loss=109.6673
	step [116/143], loss=120.3530
	step [117/143], loss=106.9492
	step [118/143], loss=87.2777
	step [119/143], loss=102.7390
	step [120/143], loss=105.3510
	step [121/143], loss=104.9896
	step [122/143], loss=97.6067
	step [123/143], loss=97.4903
	step [124/143], loss=89.3765
	step [125/143], loss=98.6038
	step [126/143], loss=111.7019
	step [127/143], loss=104.2212
	step [128/143], loss=95.3890
	step [129/143], loss=90.4524
	step [130/143], loss=101.0937
	step [131/143], loss=91.0883
	step [132/143], loss=104.0138
	step [133/143], loss=98.0599
	step [134/143], loss=83.4607
	step [135/143], loss=84.9730
	step [136/143], loss=101.5100
	step [137/143], loss=84.3594
	step [138/143], loss=91.8251
	step [139/143], loss=80.9742
	step [140/143], loss=86.6754
	step [141/143], loss=103.8903
	step [142/143], loss=103.0638
	step [143/143], loss=73.2622
	Evaluating
	loss=0.0191, precision=0.5098, recall=0.8575, f1=0.6395
Training epoch 26
	step [1/143], loss=108.2247
	step [2/143], loss=102.4938
	step [3/143], loss=103.2072
	step [4/143], loss=84.1220
	step [5/143], loss=116.7212
	step [6/143], loss=89.1688
	step [7/143], loss=101.2759
	step [8/143], loss=98.8412
	step [9/143], loss=97.9252
	step [10/143], loss=86.4504
	step [11/143], loss=86.9282
	step [12/143], loss=96.2152
	step [13/143], loss=88.2229
	step [14/143], loss=77.9839
	step [15/143], loss=100.6535
	step [16/143], loss=80.3378
	step [17/143], loss=102.2350
	step [18/143], loss=88.7388
	step [19/143], loss=87.9306
	step [20/143], loss=123.1969
	step [21/143], loss=91.0992
	step [22/143], loss=83.3294
	step [23/143], loss=97.7126
	step [24/143], loss=95.0520
	step [25/143], loss=106.4968
	step [26/143], loss=95.5694
	step [27/143], loss=100.4622
	step [28/143], loss=116.4102
	step [29/143], loss=99.7275
	step [30/143], loss=94.7393
	step [31/143], loss=95.5715
	step [32/143], loss=99.1879
	step [33/143], loss=92.7320
	step [34/143], loss=87.7282
	step [35/143], loss=96.0030
	step [36/143], loss=100.4857
	step [37/143], loss=94.4526
	step [38/143], loss=101.2776
	step [39/143], loss=103.8880
	step [40/143], loss=100.2208
	step [41/143], loss=104.9880
	step [42/143], loss=103.1897
	step [43/143], loss=102.5953
	step [44/143], loss=110.3236
	step [45/143], loss=96.0357
	step [46/143], loss=94.4805
	step [47/143], loss=130.1613
	step [48/143], loss=86.8031
	step [49/143], loss=107.4350
	step [50/143], loss=120.9433
	step [51/143], loss=87.1822
	step [52/143], loss=90.3470
	step [53/143], loss=92.8766
	step [54/143], loss=99.2962
	step [55/143], loss=90.4179
	step [56/143], loss=96.3246
	step [57/143], loss=98.0167
	step [58/143], loss=97.7891
	step [59/143], loss=107.4525
	step [60/143], loss=99.1041
	step [61/143], loss=83.7727
	step [62/143], loss=105.0677
	step [63/143], loss=111.5303
	step [64/143], loss=81.8013
	step [65/143], loss=101.4782
	step [66/143], loss=89.6214
	step [67/143], loss=96.5640
	step [68/143], loss=111.8811
	step [69/143], loss=74.4025
	step [70/143], loss=88.9146
	step [71/143], loss=107.8363
	step [72/143], loss=99.9557
	step [73/143], loss=90.4909
	step [74/143], loss=99.1922
	step [75/143], loss=90.7154
	step [76/143], loss=89.1701
	step [77/143], loss=82.6458
	step [78/143], loss=85.3496
	step [79/143], loss=108.4531
	step [80/143], loss=85.3498
	step [81/143], loss=94.7566
	step [82/143], loss=100.3189
	step [83/143], loss=87.9160
	step [84/143], loss=94.8097
	step [85/143], loss=111.1098
	step [86/143], loss=90.9710
	step [87/143], loss=101.1032
	step [88/143], loss=99.4210
	step [89/143], loss=116.1169
	step [90/143], loss=85.8910
	step [91/143], loss=107.9797
	step [92/143], loss=84.1293
	step [93/143], loss=87.7879
	step [94/143], loss=96.3185
	step [95/143], loss=72.6939
	step [96/143], loss=99.8282
	step [97/143], loss=112.9039
	step [98/143], loss=73.9933
	step [99/143], loss=100.5214
	step [100/143], loss=100.1156
	step [101/143], loss=105.0501
	step [102/143], loss=91.4297
	step [103/143], loss=88.6257
	step [104/143], loss=94.5692
	step [105/143], loss=86.7887
	step [106/143], loss=116.7799
	step [107/143], loss=85.2444
	step [108/143], loss=94.8453
	step [109/143], loss=95.6232
	step [110/143], loss=95.6112
	step [111/143], loss=94.4577
	step [112/143], loss=79.9585
	step [113/143], loss=94.5247
	step [114/143], loss=98.7409
	step [115/143], loss=114.3650
	step [116/143], loss=101.8031
	step [117/143], loss=101.6543
	step [118/143], loss=101.2025
	step [119/143], loss=78.5050
	step [120/143], loss=98.7305
	step [121/143], loss=97.9087
	step [122/143], loss=100.0099
	step [123/143], loss=113.5978
	step [124/143], loss=95.2025
	step [125/143], loss=97.3431
	step [126/143], loss=85.2180
	step [127/143], loss=94.5814
	step [128/143], loss=93.7788
	step [129/143], loss=90.9671
	step [130/143], loss=109.2006
	step [131/143], loss=93.3605
	step [132/143], loss=103.0015
	step [133/143], loss=94.5517
	step [134/143], loss=90.6525
	step [135/143], loss=86.1962
	step [136/143], loss=83.1821
	step [137/143], loss=99.1471
	step [138/143], loss=108.7841
	step [139/143], loss=101.3855
	step [140/143], loss=84.5501
	step [141/143], loss=99.4375
	step [142/143], loss=127.4853
	step [143/143], loss=73.2100
	Evaluating
	loss=0.0188, precision=0.4787, recall=0.8865, f1=0.6217
Training epoch 27
	step [1/143], loss=83.7169
	step [2/143], loss=94.5212
	step [3/143], loss=104.0791
	step [4/143], loss=109.1994
	step [5/143], loss=76.7646
	step [6/143], loss=93.7631
	step [7/143], loss=77.2190
	step [8/143], loss=90.5347
	step [9/143], loss=118.2103
	step [10/143], loss=101.4442
	step [11/143], loss=76.9108
	step [12/143], loss=87.3987
	step [13/143], loss=90.6361
	step [14/143], loss=98.8101
	step [15/143], loss=114.7167
	step [16/143], loss=120.1938
	step [17/143], loss=96.7409
	step [18/143], loss=78.0389
	step [19/143], loss=101.6688
	step [20/143], loss=100.7412
	step [21/143], loss=103.6472
	step [22/143], loss=94.3600
	step [23/143], loss=107.4864
	step [24/143], loss=99.3312
	step [25/143], loss=85.3661
	step [26/143], loss=91.5628
	step [27/143], loss=109.9516
	step [28/143], loss=96.5397
	step [29/143], loss=102.6667
	step [30/143], loss=104.9975
	step [31/143], loss=104.0721
	step [32/143], loss=99.8865
	step [33/143], loss=97.4215
	step [34/143], loss=104.0378
	step [35/143], loss=67.5953
	step [36/143], loss=76.0929
	step [37/143], loss=72.4988
	step [38/143], loss=109.5570
	step [39/143], loss=112.2848
	step [40/143], loss=89.3613
	step [41/143], loss=100.2412
	step [42/143], loss=88.1426
	step [43/143], loss=90.4312
	step [44/143], loss=113.2137
	step [45/143], loss=101.9339
	step [46/143], loss=98.4819
	step [47/143], loss=103.2392
	step [48/143], loss=92.0497
	step [49/143], loss=112.6784
	step [50/143], loss=104.8626
	step [51/143], loss=93.4776
	step [52/143], loss=85.7018
	step [53/143], loss=86.9033
	step [54/143], loss=69.4329
	step [55/143], loss=101.9943
	step [56/143], loss=108.5618
	step [57/143], loss=99.1816
	step [58/143], loss=95.9726
	step [59/143], loss=85.6578
	step [60/143], loss=85.0533
	step [61/143], loss=100.6657
	step [62/143], loss=77.3911
	step [63/143], loss=93.2773
	step [64/143], loss=98.5986
	step [65/143], loss=91.8960
	step [66/143], loss=98.5725
	step [67/143], loss=103.0746
	step [68/143], loss=103.1381
	step [69/143], loss=93.3481
	step [70/143], loss=103.0937
	step [71/143], loss=84.3464
	step [72/143], loss=100.3902
	step [73/143], loss=84.2576
	step [74/143], loss=103.0474
	step [75/143], loss=105.1650
	step [76/143], loss=95.8523
	step [77/143], loss=89.1806
	step [78/143], loss=95.2010
	step [79/143], loss=116.3084
	step [80/143], loss=96.3781
	step [81/143], loss=106.8461
	step [82/143], loss=87.4686
	step [83/143], loss=75.2498
	step [84/143], loss=95.4887
	step [85/143], loss=110.9917
	step [86/143], loss=96.5849
	step [87/143], loss=89.7981
	step [88/143], loss=91.5445
	step [89/143], loss=81.5794
	step [90/143], loss=98.1280
	step [91/143], loss=88.6680
	step [92/143], loss=96.2593
	step [93/143], loss=106.7388
	step [94/143], loss=112.0322
	step [95/143], loss=97.5400
	step [96/143], loss=85.9108
	step [97/143], loss=93.0709
	step [98/143], loss=80.0807
	step [99/143], loss=102.1306
	step [100/143], loss=103.9801
	step [101/143], loss=80.5938
	step [102/143], loss=90.0123
	step [103/143], loss=81.7863
	step [104/143], loss=98.4679
	step [105/143], loss=89.0276
	step [106/143], loss=86.4528
	step [107/143], loss=89.6343
	step [108/143], loss=97.2092
	step [109/143], loss=91.6388
	step [110/143], loss=101.7874
	step [111/143], loss=97.2746
	step [112/143], loss=100.3591
	step [113/143], loss=107.7391
	step [114/143], loss=87.7290
	step [115/143], loss=86.5507
	step [116/143], loss=105.0587
	step [117/143], loss=92.1623
	step [118/143], loss=97.9263
	step [119/143], loss=85.1757
	step [120/143], loss=80.4119
	step [121/143], loss=98.5597
	step [122/143], loss=105.1427
	step [123/143], loss=80.9159
	step [124/143], loss=85.1521
	step [125/143], loss=108.9968
	step [126/143], loss=87.4794
	step [127/143], loss=98.9412
	step [128/143], loss=93.0254
	step [129/143], loss=97.4737
	step [130/143], loss=119.8142
	step [131/143], loss=92.4218
	step [132/143], loss=91.9620
	step [133/143], loss=105.0571
	step [134/143], loss=114.9226
	step [135/143], loss=92.6843
	step [136/143], loss=99.0516
	step [137/143], loss=100.7649
	step [138/143], loss=86.2757
	step [139/143], loss=109.6012
	step [140/143], loss=86.8422
	step [141/143], loss=95.7718
	step [142/143], loss=97.8859
	step [143/143], loss=77.4763
	Evaluating
	loss=0.0192, precision=0.4338, recall=0.8920, f1=0.5838
Training epoch 28
	step [1/143], loss=81.6870
	step [2/143], loss=93.8120
	step [3/143], loss=94.9418
	step [4/143], loss=106.6063
	step [5/143], loss=93.4840
	step [6/143], loss=94.3905
	step [7/143], loss=115.1657
	step [8/143], loss=95.9340
	step [9/143], loss=113.5070
	step [10/143], loss=103.3500
	step [11/143], loss=112.4150
	step [12/143], loss=90.8739
	step [13/143], loss=90.3818
	step [14/143], loss=101.3976
	step [15/143], loss=97.2395
	step [16/143], loss=90.1468
	step [17/143], loss=95.7482
	step [18/143], loss=89.8875
	step [19/143], loss=94.0221
	step [20/143], loss=92.6795
	step [21/143], loss=91.9076
	step [22/143], loss=97.1744
	step [23/143], loss=103.5769
	step [24/143], loss=101.8669
	step [25/143], loss=104.5443
	step [26/143], loss=90.2308
	step [27/143], loss=100.2901
	step [28/143], loss=109.3771
	step [29/143], loss=85.7571
	step [30/143], loss=104.2050
	step [31/143], loss=97.0394
	step [32/143], loss=97.1502
	step [33/143], loss=99.8995
	step [34/143], loss=89.5604
	step [35/143], loss=87.9527
	step [36/143], loss=94.8061
	step [37/143], loss=113.0459
	step [38/143], loss=89.8044
	step [39/143], loss=97.6063
	step [40/143], loss=77.5820
	step [41/143], loss=75.8910
	step [42/143], loss=91.5961
	step [43/143], loss=87.7517
	step [44/143], loss=106.0083
	step [45/143], loss=94.6283
	step [46/143], loss=90.8966
	step [47/143], loss=87.2677
	step [48/143], loss=90.6013
	step [49/143], loss=76.2631
	step [50/143], loss=81.4026
	step [51/143], loss=86.7668
	step [52/143], loss=95.8605
	step [53/143], loss=102.7748
	step [54/143], loss=82.0029
	step [55/143], loss=86.8030
	step [56/143], loss=102.8694
	step [57/143], loss=98.2708
	step [58/143], loss=81.8881
	step [59/143], loss=98.0310
	step [60/143], loss=102.2601
	step [61/143], loss=88.4497
	step [62/143], loss=102.6810
	step [63/143], loss=113.1082
	step [64/143], loss=84.1983
	step [65/143], loss=93.9037
	step [66/143], loss=89.0190
	step [67/143], loss=96.2168
	step [68/143], loss=71.2793
	step [69/143], loss=99.0537
	step [70/143], loss=84.7998
	step [71/143], loss=116.1253
	step [72/143], loss=110.2580
	step [73/143], loss=90.4627
	step [74/143], loss=99.8009
	step [75/143], loss=95.4708
	step [76/143], loss=83.2708
	step [77/143], loss=87.9836
	step [78/143], loss=86.4479
	step [79/143], loss=102.6278
	step [80/143], loss=98.4076
	step [81/143], loss=96.0406
	step [82/143], loss=80.5701
	step [83/143], loss=89.9271
	step [84/143], loss=89.0867
	step [85/143], loss=95.2545
	step [86/143], loss=90.4832
	step [87/143], loss=92.4000
	step [88/143], loss=79.8224
	step [89/143], loss=92.0043
	step [90/143], loss=80.8172
	step [91/143], loss=78.8048
	step [92/143], loss=107.8031
	step [93/143], loss=102.6938
	step [94/143], loss=94.3043
	step [95/143], loss=98.7135
	step [96/143], loss=108.1222
	step [97/143], loss=94.3976
	step [98/143], loss=88.9028
	step [99/143], loss=82.7918
	step [100/143], loss=100.4557
	step [101/143], loss=80.1827
	step [102/143], loss=77.8802
	step [103/143], loss=91.3866
	step [104/143], loss=84.6442
	step [105/143], loss=98.1469
	step [106/143], loss=96.3504
	step [107/143], loss=94.4094
	step [108/143], loss=82.0580
	step [109/143], loss=102.4957
	step [110/143], loss=87.4435
	step [111/143], loss=106.2120
	step [112/143], loss=90.0949
	step [113/143], loss=95.7104
	step [114/143], loss=99.0692
	step [115/143], loss=96.7239
	step [116/143], loss=95.2286
	step [117/143], loss=110.6698
	step [118/143], loss=97.2001
	step [119/143], loss=75.5880
	step [120/143], loss=106.5315
	step [121/143], loss=105.2405
	step [122/143], loss=87.0958
	step [123/143], loss=103.6843
	step [124/143], loss=112.3293
	step [125/143], loss=85.8966
	step [126/143], loss=85.0094
	step [127/143], loss=83.9971
	step [128/143], loss=98.1241
	step [129/143], loss=82.9341
	step [130/143], loss=90.5803
	step [131/143], loss=108.2833
	step [132/143], loss=86.5738
	step [133/143], loss=79.9319
	step [134/143], loss=90.6153
	step [135/143], loss=97.6984
	step [136/143], loss=100.0011
	step [137/143], loss=93.3659
	step [138/143], loss=105.8314
	step [139/143], loss=89.2387
	step [140/143], loss=92.2282
	step [141/143], loss=103.8236
	step [142/143], loss=96.2985
	step [143/143], loss=89.0220
	Evaluating
	loss=0.0197, precision=0.3905, recall=0.8613, f1=0.5374
Training epoch 29
	step [1/143], loss=89.2911
	step [2/143], loss=101.2568
	step [3/143], loss=96.1348
	step [4/143], loss=92.5040
	step [5/143], loss=99.0142
	step [6/143], loss=116.2320
	step [7/143], loss=95.2524
	step [8/143], loss=99.3662
	step [9/143], loss=96.8326
	step [10/143], loss=90.1773
	step [11/143], loss=98.5385
	step [12/143], loss=90.1315
	step [13/143], loss=97.6460
	step [14/143], loss=79.8963
	step [15/143], loss=89.7565
	step [16/143], loss=80.3444
	step [17/143], loss=86.8139
	step [18/143], loss=87.4681
	step [19/143], loss=85.0065
	step [20/143], loss=104.4547
	step [21/143], loss=95.7056
	step [22/143], loss=92.1973
	step [23/143], loss=101.5371
	step [24/143], loss=97.6458
	step [25/143], loss=91.1176
	step [26/143], loss=112.0632
	step [27/143], loss=99.6944
	step [28/143], loss=99.8137
	step [29/143], loss=84.6583
	step [30/143], loss=92.7587
	step [31/143], loss=78.8513
	step [32/143], loss=99.0586
	step [33/143], loss=88.7736
	step [34/143], loss=79.9206
	step [35/143], loss=77.9734
	step [36/143], loss=87.2527
	step [37/143], loss=97.5680
	step [38/143], loss=96.5021
	step [39/143], loss=86.2915
	step [40/143], loss=84.0388
	step [41/143], loss=85.8699
	step [42/143], loss=82.9006
	step [43/143], loss=83.3070
	step [44/143], loss=95.6501
	step [45/143], loss=91.8918
	step [46/143], loss=117.6401
	step [47/143], loss=83.4212
	step [48/143], loss=97.6152
	step [49/143], loss=81.3852
	step [50/143], loss=77.1530
	step [51/143], loss=93.3334
	step [52/143], loss=99.9740
	step [53/143], loss=74.9306
	step [54/143], loss=115.5506
	step [55/143], loss=93.1845
	step [56/143], loss=96.7844
	step [57/143], loss=104.3093
	step [58/143], loss=86.6308
	step [59/143], loss=94.8677
	step [60/143], loss=113.5352
	step [61/143], loss=85.5292
	step [62/143], loss=84.0574
	step [63/143], loss=92.9011
	step [64/143], loss=90.1195
	step [65/143], loss=100.8968
	step [66/143], loss=88.8289
	step [67/143], loss=83.6602
	step [68/143], loss=82.3529
	step [69/143], loss=82.4545
	step [70/143], loss=112.0307
	step [71/143], loss=102.9576
	step [72/143], loss=93.4886
	step [73/143], loss=91.7644
	step [74/143], loss=89.6426
	step [75/143], loss=81.4241
	step [76/143], loss=89.8182
	step [77/143], loss=94.7426
	step [78/143], loss=114.2463
	step [79/143], loss=92.6192
	step [80/143], loss=85.3536
	step [81/143], loss=89.9820
	step [82/143], loss=93.2466
	step [83/143], loss=115.1449
	step [84/143], loss=92.8730
	step [85/143], loss=96.9952
	step [86/143], loss=95.9127
	step [87/143], loss=96.3214
	step [88/143], loss=90.2693
	step [89/143], loss=97.6076
	step [90/143], loss=100.3949
	step [91/143], loss=89.2688
	step [92/143], loss=98.4549
	step [93/143], loss=87.2638
	step [94/143], loss=107.4441
	step [95/143], loss=96.4738
	step [96/143], loss=91.6468
	step [97/143], loss=120.3253
	step [98/143], loss=99.1346
	step [99/143], loss=93.4075
	step [100/143], loss=96.8951
	step [101/143], loss=79.7508
	step [102/143], loss=97.4685
	step [103/143], loss=86.5954
	step [104/143], loss=70.5177
	step [105/143], loss=89.2004
	step [106/143], loss=90.0701
	step [107/143], loss=90.7234
	step [108/143], loss=88.3657
	step [109/143], loss=98.0921
	step [110/143], loss=102.5690
	step [111/143], loss=98.2917
	step [112/143], loss=95.2609
	step [113/143], loss=87.3434
	step [114/143], loss=87.9722
	step [115/143], loss=94.0497
	step [116/143], loss=101.5677
	step [117/143], loss=107.5074
	step [118/143], loss=94.4198
	step [119/143], loss=109.5363
	step [120/143], loss=82.3201
	step [121/143], loss=100.7939
	step [122/143], loss=89.2467
	step [123/143], loss=95.9458
	step [124/143], loss=73.5174
	step [125/143], loss=106.7423
	step [126/143], loss=79.6379
	step [127/143], loss=95.4514
	step [128/143], loss=91.0762
	step [129/143], loss=73.3774
	step [130/143], loss=101.4574
	step [131/143], loss=92.8397
	step [132/143], loss=104.8017
	step [133/143], loss=105.6596
	step [134/143], loss=99.6029
	step [135/143], loss=88.7218
	step [136/143], loss=81.9425
	step [137/143], loss=90.1187
	step [138/143], loss=94.1166
	step [139/143], loss=107.8054
	step [140/143], loss=102.3688
	step [141/143], loss=86.5861
	step [142/143], loss=86.5058
	step [143/143], loss=73.6338
	Evaluating
	loss=0.0155, precision=0.5123, recall=0.8651, f1=0.6435
Training epoch 30
	step [1/143], loss=88.6891
	step [2/143], loss=78.5533
	step [3/143], loss=98.7875
	step [4/143], loss=87.8462
	step [5/143], loss=85.4478
	step [6/143], loss=85.2822
	step [7/143], loss=77.9391
	step [8/143], loss=87.0430
	step [9/143], loss=102.5295
	step [10/143], loss=81.6981
	step [11/143], loss=96.5706
	step [12/143], loss=107.3680
	step [13/143], loss=96.1858
	step [14/143], loss=100.1086
	step [15/143], loss=82.3700
	step [16/143], loss=83.5126
	step [17/143], loss=89.5610
	step [18/143], loss=91.2219
	step [19/143], loss=87.7177
	step [20/143], loss=101.9276
	step [21/143], loss=103.6465
	step [22/143], loss=108.9507
	step [23/143], loss=88.2854
	step [24/143], loss=93.8184
	step [25/143], loss=99.3854
	step [26/143], loss=99.5775
	step [27/143], loss=108.1320
	step [28/143], loss=120.6581
	step [29/143], loss=83.3483
	step [30/143], loss=100.7778
	step [31/143], loss=92.0929
	step [32/143], loss=94.7745
	step [33/143], loss=84.0340
	step [34/143], loss=91.4973
	step [35/143], loss=88.4647
	step [36/143], loss=99.2404
	step [37/143], loss=92.9373
	step [38/143], loss=93.7522
	step [39/143], loss=102.1579
	step [40/143], loss=101.0603
	step [41/143], loss=108.0116
	step [42/143], loss=90.6658
	step [43/143], loss=84.3516
	step [44/143], loss=78.2911
	step [45/143], loss=93.3947
	step [46/143], loss=91.2914
	step [47/143], loss=99.4538
	step [48/143], loss=94.6725
	step [49/143], loss=104.9716
	step [50/143], loss=86.3664
	step [51/143], loss=85.1496
	step [52/143], loss=81.4257
	step [53/143], loss=101.0255
	step [54/143], loss=78.0543
	step [55/143], loss=90.8421
	step [56/143], loss=86.3275
	step [57/143], loss=95.1091
	step [58/143], loss=94.3549
	step [59/143], loss=88.6200
	step [60/143], loss=82.6630
	step [61/143], loss=93.7551
	step [62/143], loss=86.2790
	step [63/143], loss=93.4072
	step [64/143], loss=91.9549
	step [65/143], loss=93.6584
	step [66/143], loss=87.2307
	step [67/143], loss=97.2769
	step [68/143], loss=86.0419
	step [69/143], loss=83.0159
	step [70/143], loss=83.0848
	step [71/143], loss=77.9540
	step [72/143], loss=97.5515
	step [73/143], loss=76.2483
	step [74/143], loss=66.7867
	step [75/143], loss=98.1621
	step [76/143], loss=107.8777
	step [77/143], loss=93.3025
	step [78/143], loss=94.8686
	step [79/143], loss=105.4194
	step [80/143], loss=91.7447
	step [81/143], loss=99.6283
	step [82/143], loss=77.2237
	step [83/143], loss=85.0284
	step [84/143], loss=101.9419
	step [85/143], loss=97.5022
	step [86/143], loss=92.5572
	step [87/143], loss=85.1134
	step [88/143], loss=100.8494
	step [89/143], loss=84.9451
	step [90/143], loss=77.6694
	step [91/143], loss=87.7708
	step [92/143], loss=84.0514
	step [93/143], loss=89.1194
	step [94/143], loss=98.4407
	step [95/143], loss=85.8303
	step [96/143], loss=96.4407
	step [97/143], loss=85.7671
	step [98/143], loss=86.7077
	step [99/143], loss=101.0214
	step [100/143], loss=88.1533
	step [101/143], loss=91.6303
	step [102/143], loss=105.4619
	step [103/143], loss=87.5210
	step [104/143], loss=84.8540
	step [105/143], loss=74.3188
	step [106/143], loss=75.9224
	step [107/143], loss=87.1179
	step [108/143], loss=95.3022
	step [109/143], loss=94.4123
	step [110/143], loss=89.4200
	step [111/143], loss=96.3968
	step [112/143], loss=99.4968
	step [113/143], loss=96.6867
	step [114/143], loss=86.1582
	step [115/143], loss=105.0719
	step [116/143], loss=93.1325
	step [117/143], loss=114.6484
	step [118/143], loss=91.0419
	step [119/143], loss=80.2489
	step [120/143], loss=87.2555
	step [121/143], loss=68.7173
	step [122/143], loss=102.6899
	step [123/143], loss=82.7087
	step [124/143], loss=89.8526
	step [125/143], loss=88.4165
	step [126/143], loss=94.9575
	step [127/143], loss=87.3799
	step [128/143], loss=79.2550
	step [129/143], loss=111.3183
	step [130/143], loss=90.3811
	step [131/143], loss=104.9109
	step [132/143], loss=103.8074
	step [133/143], loss=109.1613
	step [134/143], loss=95.7649
	step [135/143], loss=104.9866
	step [136/143], loss=81.6574
	step [137/143], loss=98.3101
	step [138/143], loss=81.3793
	step [139/143], loss=102.7149
	step [140/143], loss=87.4027
	step [141/143], loss=99.2987
	step [142/143], loss=78.4640
	step [143/143], loss=69.4872
	Evaluating
	loss=0.0171, precision=0.4135, recall=0.8949, f1=0.5656
Training finished
best_f1: 0.6860350183150687
directing: Y rim_enhanced: True test_id 1
removed wrong scan: weights_Y_335_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_383_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_262_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_311_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_260_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_225_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_320_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_192_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_319_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_226_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_346_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_271_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_277_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_290_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_269_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_379_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_375_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_378_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_366_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_309_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_200_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_327_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_193_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_174_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_253_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_220_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_265_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_360_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_303_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_232_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_243_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_240_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_237_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_256_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_254_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_254_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_231_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_359_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_172_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_340_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_263_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_388_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_307_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_278_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_196_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_272_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_314_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_350_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_361_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_194_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_323_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_201_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_299_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_259_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_276_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_167_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_185_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_215_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_268_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_264_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_384_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_261_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_280_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_287_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_385_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_216_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_349_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_348_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_371_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_357_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_234_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_288_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_324_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_229_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_184_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_148_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_168_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_370_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_286_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_302_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_265_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_153_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_274_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_255_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_217_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_355_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_294_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_159_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_338_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_236_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_152_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_292_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_222_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_376_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_183_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_258_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_282_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_278_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_284_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_300_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_163_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_197_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_203_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_243_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_224_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_238_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_374_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_218_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_259_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_282_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_387_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_353_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_386_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_296_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_341_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_206_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_220_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_239_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_231_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_202_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_358_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_246_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_315_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_275_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_169_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_316_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_221_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_188_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_208_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_273_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_251_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_224_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_333_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_263_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_173_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_243_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_241_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_326_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_230_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_236_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_345_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_206_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_258_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_187_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_87_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_154_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_253_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_337_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_232_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_147_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_377_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_362_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_233_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_250_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_297_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_354_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_262_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_225_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_365_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_356_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_242_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_245_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_230_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_318_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_181_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_305_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_255_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_298_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_301_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_293_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_229_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_332_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_257_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_295_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_239_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_260_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_228_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_317_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_364_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_262_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_162_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_310_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_160_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_237_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_254_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_257_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_291_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_279_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_215_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_372_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_218_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_210_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_158_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_156_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_339_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_190_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_373_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_205_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_329_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_380_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_271_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_235_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_244_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_312_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_186_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_308_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_347_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_241_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_270_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_261_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_222_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_189_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_289_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_146_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_252_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_344_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_331_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_313_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_328_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_265_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_283_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_248_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_381_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_211_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_214_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_176_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_221_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_150_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_343_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_249_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_322_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_351_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_234_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_195_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_330_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_334_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_306_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_213_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_325_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_342_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_247_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_352_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_225_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_226_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_256_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_242_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_368_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_382_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_157_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_238_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_267_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_367_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_321_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_233_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_281_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_161_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_336_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_170_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_304_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_44_xwqg-B00034_2020-04-03.npy
# all image files: 20333 # all weight files in weight_dir: 15610 # image files with weight 15579
removed wrong scan: weights_Y_335_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_383_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_262_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_311_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_260_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_225_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_320_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_192_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_319_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_226_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_346_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_271_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_277_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_290_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_269_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_379_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_375_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_378_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_366_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_309_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_200_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_327_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_193_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_174_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_253_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_220_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_265_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_360_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_303_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_232_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_243_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_240_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_237_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_256_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_254_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_254_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_231_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_359_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_172_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_340_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_263_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_388_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_307_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_278_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_196_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_272_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_314_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_350_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_361_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_194_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_323_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_201_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_299_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_259_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_276_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_167_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_185_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_215_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_268_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_264_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_384_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_261_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_280_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_287_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_385_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_216_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_349_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_348_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_371_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_357_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_234_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_288_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_324_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_229_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_184_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_148_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_168_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_370_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_286_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_302_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_265_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_153_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_274_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_255_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_217_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_355_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_294_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_159_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_338_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_236_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_152_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_292_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_222_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_376_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_183_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_258_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_282_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_278_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_284_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_300_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_163_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_197_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_203_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_243_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_224_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_238_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_374_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_218_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_259_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_282_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_387_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_353_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_386_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_296_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_341_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_206_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_220_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_239_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_231_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_202_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_358_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_246_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_315_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_275_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_169_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_316_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_221_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_188_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_208_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_273_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_251_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_224_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_333_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_263_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_173_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_243_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_241_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_326_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_230_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_236_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_345_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_206_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_258_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_187_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_87_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_154_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_253_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_337_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_232_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_147_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_377_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_362_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_233_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_250_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_297_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_354_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_262_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_225_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_365_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_356_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_242_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_245_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_230_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_318_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_181_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_305_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_255_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_298_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_301_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_293_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_229_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_332_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_257_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_295_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_239_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_260_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_228_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_317_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_364_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_262_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_162_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_310_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_160_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_237_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_254_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_257_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_291_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_279_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_215_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_372_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_218_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_210_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_158_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_156_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_339_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_190_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_373_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_205_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_329_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_380_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_271_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_235_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_244_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_312_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_186_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_308_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_347_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_241_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_270_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_261_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_222_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_189_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_289_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_146_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_252_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_344_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_331_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_313_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_328_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_265_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_283_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_248_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_381_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_211_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_214_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_176_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_221_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_150_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_343_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_249_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_322_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_351_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_234_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_195_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_330_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_334_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_306_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_213_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_325_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_342_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_247_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_352_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_225_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_226_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_256_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_242_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_368_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_382_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_157_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_238_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_267_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_367_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_321_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_233_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_281_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_161_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_336_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_170_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_304_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_44_xwqg-B00034_2020-04-03.npy
# all image files: 20333 # all weight files in weight_dir: 4462 # image files with weight 4451
train: /ibex/scratch/projects/c2052/air_tube_seg/training_samples_enhanced_one/Y 15579
Using 4 GPUs
Going to train epochs [1-30]
Training epoch 1
	step [1/244], loss=548.5463
	step [2/244], loss=407.4472
	step [3/244], loss=436.2120
	step [4/244], loss=365.3069
	step [5/244], loss=405.2539
	step [6/244], loss=373.2983
	step [7/244], loss=360.4225
	step [8/244], loss=357.9785
	step [9/244], loss=362.5793
	step [10/244], loss=328.9409
	step [11/244], loss=325.6525
	step [12/244], loss=330.7341
	step [13/244], loss=343.5626
	step [14/244], loss=343.3045
	step [15/244], loss=319.6205
	step [16/244], loss=324.6121
	step [17/244], loss=298.6017
	step [18/244], loss=322.7542
	step [19/244], loss=321.0274
	step [20/244], loss=318.7764
	step [21/244], loss=310.6051
	step [22/244], loss=294.1336
	step [23/244], loss=300.6739
	step [24/244], loss=289.6472
	step [25/244], loss=318.6028
	step [26/244], loss=288.7714
	step [27/244], loss=311.1133
	step [28/244], loss=293.7877
	step [29/244], loss=285.8484
	step [30/244], loss=310.5418
	step [31/244], loss=268.1261
	step [32/244], loss=312.1271
	step [33/244], loss=249.9771
	step [34/244], loss=305.9559
	step [35/244], loss=280.6486
	step [36/244], loss=293.6602
	step [37/244], loss=260.9363
	step [38/244], loss=282.2543
	step [39/244], loss=277.1112
	step [40/244], loss=265.4236
	step [41/244], loss=273.9987
	step [42/244], loss=250.5156
	step [43/244], loss=257.2590
	step [44/244], loss=277.7198
	step [45/244], loss=261.5450
	step [46/244], loss=259.2995
	step [47/244], loss=266.0126
	step [48/244], loss=299.2778
	step [49/244], loss=242.8738
	step [50/244], loss=273.0807
	step [51/244], loss=280.3894
	step [52/244], loss=260.0979
	step [53/244], loss=280.2362
	step [54/244], loss=256.5284
	step [55/244], loss=240.3703
	step [56/244], loss=273.4327
	step [57/244], loss=233.7953
	step [58/244], loss=256.6401
	step [59/244], loss=253.2959
	step [60/244], loss=270.1714
	step [61/244], loss=253.2746
	step [62/244], loss=247.2106
	step [63/244], loss=280.0326
	step [64/244], loss=254.9275
	step [65/244], loss=257.7505
	step [66/244], loss=265.4898
	step [67/244], loss=248.5681
	step [68/244], loss=262.8952
	step [69/244], loss=257.2230
	step [70/244], loss=259.9690
	step [71/244], loss=246.6406
	step [72/244], loss=259.1340
	step [73/244], loss=244.1223
	step [74/244], loss=251.8869
	step [75/244], loss=251.2785
	step [76/244], loss=272.9618
	step [77/244], loss=248.9382
	step [78/244], loss=252.8322
	step [79/244], loss=254.6046
	step [80/244], loss=262.9103
	step [81/244], loss=257.2315
	step [82/244], loss=244.0764
	step [83/244], loss=277.9193
	step [84/244], loss=245.3302
	step [85/244], loss=228.6195
	step [86/244], loss=245.0650
	step [87/244], loss=244.1785
	step [88/244], loss=242.2384
	step [89/244], loss=254.1497
	step [90/244], loss=238.1594
	step [91/244], loss=227.3715
	step [92/244], loss=232.7185
	step [93/244], loss=250.9788
	step [94/244], loss=261.5624
	step [95/244], loss=241.0923
	step [96/244], loss=254.5539
	step [97/244], loss=250.5685
	step [98/244], loss=230.5417
	step [99/244], loss=250.6169
	step [100/244], loss=217.6446
	step [101/244], loss=232.9504
	step [102/244], loss=243.6455
	step [103/244], loss=251.1430
	step [104/244], loss=250.9758
	step [105/244], loss=228.3822
	step [106/244], loss=242.5890
	step [107/244], loss=241.3768
	step [108/244], loss=236.7645
	step [109/244], loss=254.7888
	step [110/244], loss=218.5158
	step [111/244], loss=223.2730
	step [112/244], loss=241.7440
	step [113/244], loss=228.3260
	step [114/244], loss=238.3517
	step [115/244], loss=240.6936
	step [116/244], loss=222.0085
	step [117/244], loss=218.4601
	step [118/244], loss=234.0868
	step [119/244], loss=249.9092
	step [120/244], loss=219.1940
	step [121/244], loss=229.7608
	step [122/244], loss=218.1649
	step [123/244], loss=230.8826
	step [124/244], loss=231.3864
	step [125/244], loss=241.0642
	step [126/244], loss=250.0900
	step [127/244], loss=219.1045
	step [128/244], loss=254.2073
	step [129/244], loss=235.4421
	step [130/244], loss=243.5117
	step [131/244], loss=210.3938
	step [132/244], loss=214.6579
	step [133/244], loss=209.5981
	step [134/244], loss=233.8262
	step [135/244], loss=218.8377
	step [136/244], loss=219.3165
	step [137/244], loss=213.7465
	step [138/244], loss=221.6445
	step [139/244], loss=218.9832
	step [140/244], loss=237.0413
	step [141/244], loss=201.9340
	step [142/244], loss=239.4820
	step [143/244], loss=232.2425
	step [144/244], loss=207.4969
	step [145/244], loss=215.1509
	step [146/244], loss=210.2701
	step [147/244], loss=230.1552
	step [148/244], loss=246.7454
	step [149/244], loss=208.5913
	step [150/244], loss=217.0164
	step [151/244], loss=237.4366
	step [152/244], loss=229.4407
	step [153/244], loss=215.9790
	step [154/244], loss=223.4270
	step [155/244], loss=218.3160
	step [156/244], loss=232.7305
	step [157/244], loss=231.9552
	step [158/244], loss=214.7297
	step [159/244], loss=212.6085
	step [160/244], loss=249.2110
	step [161/244], loss=221.5659
	step [162/244], loss=228.3177
	step [163/244], loss=227.0930
	step [164/244], loss=224.0572
	step [165/244], loss=227.2064
	step [166/244], loss=220.7032
	step [167/244], loss=215.1108
	step [168/244], loss=208.1488
	step [169/244], loss=213.1651
	step [170/244], loss=227.4223
	step [171/244], loss=216.7845
	step [172/244], loss=210.1212
	step [173/244], loss=237.3739
	step [174/244], loss=206.0845
	step [175/244], loss=222.5751
	step [176/244], loss=210.9974
	step [177/244], loss=205.0856
	step [178/244], loss=211.0764
	step [179/244], loss=222.0302
	step [180/244], loss=207.6620
	step [181/244], loss=197.7928
	step [182/244], loss=216.5296
	step [183/244], loss=225.2843
	step [184/244], loss=215.4299
	step [185/244], loss=238.8392
	step [186/244], loss=235.8857
	step [187/244], loss=203.0038
	step [188/244], loss=216.0918
	step [189/244], loss=207.1448
	step [190/244], loss=215.3979
	step [191/244], loss=202.2411
	step [192/244], loss=196.6364
	step [193/244], loss=195.7439
	step [194/244], loss=210.6293
	step [195/244], loss=191.6775
	step [196/244], loss=212.7467
	step [197/244], loss=207.9768
	step [198/244], loss=226.9239
	step [199/244], loss=211.8478
	step [200/244], loss=210.4349
	step [201/244], loss=240.3074
	step [202/244], loss=214.5455
	step [203/244], loss=232.5959
	step [204/244], loss=202.2362
	step [205/244], loss=214.8461
	step [206/244], loss=219.6215
	step [207/244], loss=212.3819
	step [208/244], loss=204.1448
	step [209/244], loss=185.7646
	step [210/244], loss=232.6537
	step [211/244], loss=210.5793
	step [212/244], loss=196.7251
	step [213/244], loss=202.1985
	step [214/244], loss=233.1482
	step [215/244], loss=213.6770
	step [216/244], loss=214.3322
	step [217/244], loss=219.3873
	step [218/244], loss=203.2317
	step [219/244], loss=209.6538
	step [220/244], loss=183.5050
	step [221/244], loss=205.7259
	step [222/244], loss=202.6479
	step [223/244], loss=178.6958
	step [224/244], loss=233.4556
	step [225/244], loss=185.8109
	step [226/244], loss=225.6493
	step [227/244], loss=198.6651
	step [228/244], loss=212.2810
	step [229/244], loss=192.4568
	step [230/244], loss=216.6215
	step [231/244], loss=203.1442
	step [232/244], loss=198.8173
	step [233/244], loss=210.6839
	step [234/244], loss=200.3553
	step [235/244], loss=232.1981
	step [236/244], loss=215.2501
	step [237/244], loss=215.7106
	step [238/244], loss=185.7292
	step [239/244], loss=211.4929
	step [240/244], loss=220.0157
	step [241/244], loss=214.4929
	step [242/244], loss=217.7110
	step [243/244], loss=204.8300
	step [244/244], loss=93.9052
	Evaluating
	loss=0.4854, precision=0.0546, recall=0.8988, f1=0.1030
saving model as: 1_saved_model.pth
Training epoch 2
	step [1/244], loss=204.4223
	step [2/244], loss=216.8661
	step [3/244], loss=186.9107
	step [4/244], loss=216.3996
	step [5/244], loss=198.4685
	step [6/244], loss=197.2618
	step [7/244], loss=206.5659
	step [8/244], loss=196.7134
	step [9/244], loss=215.2921
	step [10/244], loss=207.7492
	step [11/244], loss=206.0357
	step [12/244], loss=204.3639
	step [13/244], loss=204.0025
	step [14/244], loss=187.1858
	step [15/244], loss=219.0684
	step [16/244], loss=205.5145
	step [17/244], loss=209.3554
	step [18/244], loss=193.9176
	step [19/244], loss=206.3986
	step [20/244], loss=218.5455
	step [21/244], loss=213.9818
	step [22/244], loss=224.7044
	step [23/244], loss=220.0003
	step [24/244], loss=191.0874
	step [25/244], loss=204.8963
	step [26/244], loss=191.1930
	step [27/244], loss=192.7262
	step [28/244], loss=205.1719
	step [29/244], loss=202.2005
	step [30/244], loss=205.3912
	step [31/244], loss=197.8002
	step [32/244], loss=209.0774
	step [33/244], loss=212.2242
	step [34/244], loss=175.1712
	step [35/244], loss=178.4406
	step [36/244], loss=192.6385
	step [37/244], loss=217.3396
	step [38/244], loss=180.9257
	step [39/244], loss=210.9205
	step [40/244], loss=202.1929
	step [41/244], loss=199.3807
	step [42/244], loss=214.4040
	step [43/244], loss=185.2186
	step [44/244], loss=195.0283
	step [45/244], loss=191.1096
	step [46/244], loss=191.6028
	step [47/244], loss=181.6479
	step [48/244], loss=186.5245
	step [49/244], loss=189.0676
	step [50/244], loss=216.2493
	step [51/244], loss=229.0227
	step [52/244], loss=187.3221
	step [53/244], loss=219.2295
	step [54/244], loss=189.6577
	step [55/244], loss=215.9805
	step [56/244], loss=189.0276
	step [57/244], loss=206.7955
	step [58/244], loss=197.0176
	step [59/244], loss=175.4226
	step [60/244], loss=201.1510
	step [61/244], loss=207.6533
	step [62/244], loss=179.7429
	step [63/244], loss=175.4189
	step [64/244], loss=167.7990
	step [65/244], loss=183.9249
	step [66/244], loss=170.7384
	step [67/244], loss=200.0177
	step [68/244], loss=193.7484
	step [69/244], loss=180.6644
	step [70/244], loss=197.1340
	step [71/244], loss=182.2972
	step [72/244], loss=219.0645
	step [73/244], loss=185.1311
	step [74/244], loss=193.6168
	step [75/244], loss=160.5280
	step [76/244], loss=180.8997
	step [77/244], loss=201.1780
	step [78/244], loss=191.9570
	step [79/244], loss=207.6397
	step [80/244], loss=180.3263
	step [81/244], loss=206.3648
	step [82/244], loss=183.4850
	step [83/244], loss=192.4619
	step [84/244], loss=161.5364
	step [85/244], loss=181.9583
	step [86/244], loss=186.3924
	step [87/244], loss=219.6053
	step [88/244], loss=201.2252
	step [89/244], loss=169.5574
	step [90/244], loss=196.8400
	step [91/244], loss=205.2521
	step [92/244], loss=191.4521
	step [93/244], loss=195.2401
	step [94/244], loss=186.6204
	step [95/244], loss=208.3832
	step [96/244], loss=183.7027
	step [97/244], loss=193.3264
	step [98/244], loss=185.2013
	step [99/244], loss=200.6849
	step [100/244], loss=187.6201
	step [101/244], loss=195.8741
	step [102/244], loss=192.0009
	step [103/244], loss=189.7045
	step [104/244], loss=184.3429
	step [105/244], loss=207.8225
	step [106/244], loss=188.7750
	step [107/244], loss=202.9160
	step [108/244], loss=205.4124
	step [109/244], loss=202.3538
	step [110/244], loss=193.3990
	step [111/244], loss=189.5120
	step [112/244], loss=210.1369
	step [113/244], loss=179.8516
	step [114/244], loss=193.3497
	step [115/244], loss=186.7267
	step [116/244], loss=179.8670
	step [117/244], loss=180.4571
	step [118/244], loss=207.7560
	step [119/244], loss=191.7645
	step [120/244], loss=196.4928
	step [121/244], loss=190.6253
	step [122/244], loss=185.8549
	step [123/244], loss=195.5048
	step [124/244], loss=195.3591
	step [125/244], loss=174.5031
	step [126/244], loss=173.6180
	step [127/244], loss=183.8594
	step [128/244], loss=194.7862
	step [129/244], loss=185.2885
	step [130/244], loss=164.4414
	step [131/244], loss=181.0186
	step [132/244], loss=187.9102
	step [133/244], loss=171.1928
	step [134/244], loss=163.0900
	step [135/244], loss=185.6830
	step [136/244], loss=207.1900
	step [137/244], loss=195.6706
	step [138/244], loss=174.4673
	step [139/244], loss=198.8833
	step [140/244], loss=174.7569
	step [141/244], loss=197.1341
	step [142/244], loss=167.0463
	step [143/244], loss=169.1707
	step [144/244], loss=172.4934
	step [145/244], loss=172.5788
	step [146/244], loss=195.8540
	step [147/244], loss=197.3364
	step [148/244], loss=210.3949
	step [149/244], loss=186.9855
	step [150/244], loss=204.1205
	step [151/244], loss=194.3243
	step [152/244], loss=184.6844
	step [153/244], loss=180.7911
	step [154/244], loss=191.7511
	step [155/244], loss=196.0936
	step [156/244], loss=173.0236
	step [157/244], loss=178.7462
	step [158/244], loss=151.2790
	step [159/244], loss=168.6605
	step [160/244], loss=202.2558
	step [161/244], loss=189.1574
	step [162/244], loss=175.6287
	step [163/244], loss=198.2848
	step [164/244], loss=215.1872
	step [165/244], loss=172.2651
	step [166/244], loss=179.2765
	step [167/244], loss=190.9418
	step [168/244], loss=190.7421
	step [169/244], loss=179.7385
	step [170/244], loss=193.7335
	step [171/244], loss=189.8449
	step [172/244], loss=186.4380
	step [173/244], loss=195.6546
	step [174/244], loss=179.6516
	step [175/244], loss=160.5502
	step [176/244], loss=185.9693
	step [177/244], loss=156.7518
	step [178/244], loss=167.0331
	step [179/244], loss=178.6475
	step [180/244], loss=186.8534
	step [181/244], loss=198.8353
	step [182/244], loss=180.8286
	step [183/244], loss=187.1843
	step [184/244], loss=189.2201
	step [185/244], loss=197.9087
	step [186/244], loss=183.2485
	step [187/244], loss=183.3508
	step [188/244], loss=209.8320
	step [189/244], loss=194.0627
	step [190/244], loss=181.0980
	step [191/244], loss=171.8267
	step [192/244], loss=194.8207
	step [193/244], loss=193.6214
	step [194/244], loss=187.8198
	step [195/244], loss=161.7526
	step [196/244], loss=178.4886
	step [197/244], loss=186.9045
	step [198/244], loss=167.7254
	step [199/244], loss=189.7654
	step [200/244], loss=170.4493
	step [201/244], loss=178.7058
	step [202/244], loss=156.8747
	step [203/244], loss=180.3354
	step [204/244], loss=177.6001
	step [205/244], loss=172.9474
	step [206/244], loss=188.0435
	step [207/244], loss=171.7677
	step [208/244], loss=173.8440
	step [209/244], loss=156.2240
	step [210/244], loss=183.7272
	step [211/244], loss=180.6788
	step [212/244], loss=156.8441
	step [213/244], loss=160.6639
	step [214/244], loss=183.2121
	step [215/244], loss=172.0809
	step [216/244], loss=162.2074
	step [217/244], loss=174.3048
	step [218/244], loss=196.7343
	step [219/244], loss=180.7195
	step [220/244], loss=198.3885
	step [221/244], loss=171.3726
	step [222/244], loss=173.2465
	step [223/244], loss=193.7192
	step [224/244], loss=157.7947
	step [225/244], loss=148.6958
	step [226/244], loss=172.3604
	step [227/244], loss=179.5213
	step [228/244], loss=186.8477
	step [229/244], loss=187.2281
	step [230/244], loss=163.6718
	step [231/244], loss=146.3607
	step [232/244], loss=175.5714
	step [233/244], loss=187.9484
	step [234/244], loss=173.0945
	step [235/244], loss=171.5255
	step [236/244], loss=202.6524
	step [237/244], loss=202.2979
	step [238/244], loss=176.5929
	step [239/244], loss=185.8795
	step [240/244], loss=171.4636
	step [241/244], loss=176.6562
	step [242/244], loss=195.9703
	step [243/244], loss=160.1129
	step [244/244], loss=80.9918
	Evaluating
	loss=0.3313, precision=0.4443, recall=0.8908, f1=0.5929
saving model as: 1_saved_model.pth
Training epoch 3
	step [1/244], loss=172.8038
	step [2/244], loss=147.9308
	step [3/244], loss=184.8461
	step [4/244], loss=163.0385
	step [5/244], loss=179.5645
	step [6/244], loss=161.1497
	step [7/244], loss=185.7448
	step [8/244], loss=174.1277
	step [9/244], loss=155.8095
	step [10/244], loss=158.1091
	step [11/244], loss=164.1583
	step [12/244], loss=185.0067
	step [13/244], loss=180.2071
	step [14/244], loss=170.5977
	step [15/244], loss=169.1960
	step [16/244], loss=176.4849
	step [17/244], loss=170.3207
	step [18/244], loss=179.7914
	step [19/244], loss=173.4354
	step [20/244], loss=181.9427
	step [21/244], loss=174.4500
	step [22/244], loss=176.6321
	step [23/244], loss=165.8102
	step [24/244], loss=168.9008
	step [25/244], loss=196.6379
	step [26/244], loss=152.4706
	step [27/244], loss=154.0356
	step [28/244], loss=177.8605
	step [29/244], loss=181.2402
	step [30/244], loss=165.5927
	step [31/244], loss=167.3713
	step [32/244], loss=163.0739
	step [33/244], loss=161.3301
	step [34/244], loss=164.0961
	step [35/244], loss=186.8435
	step [36/244], loss=151.5585
	step [37/244], loss=163.9222
	step [38/244], loss=175.4203
	step [39/244], loss=167.6472
	step [40/244], loss=158.3166
	step [41/244], loss=192.2800
	step [42/244], loss=162.6036
	step [43/244], loss=182.7120
	step [44/244], loss=152.6548
	step [45/244], loss=157.2530
	step [46/244], loss=184.5451
	step [47/244], loss=184.2234
	step [48/244], loss=159.4261
	step [49/244], loss=180.8498
	step [50/244], loss=151.1799
	step [51/244], loss=137.2697
	step [52/244], loss=165.1225
	step [53/244], loss=143.7525
	step [54/244], loss=144.5217
	step [55/244], loss=194.9999
	step [56/244], loss=148.0948
	step [57/244], loss=192.2357
	step [58/244], loss=169.4260
	step [59/244], loss=180.1114
	step [60/244], loss=161.2759
	step [61/244], loss=181.1475
	step [62/244], loss=192.0380
	step [63/244], loss=164.2003
	step [64/244], loss=164.2556
	step [65/244], loss=155.7240
	step [66/244], loss=174.1599
	step [67/244], loss=165.9466
	step [68/244], loss=170.3022
	step [69/244], loss=158.3446
	step [70/244], loss=171.8997
	step [71/244], loss=180.2616
	step [72/244], loss=159.3750
	step [73/244], loss=181.3943
	step [74/244], loss=164.0290
	step [75/244], loss=155.2968
	step [76/244], loss=152.0394
	step [77/244], loss=145.6526
	step [78/244], loss=191.7331
	step [79/244], loss=176.5950
	step [80/244], loss=182.3717
	step [81/244], loss=153.8695
	step [82/244], loss=173.8870
	step [83/244], loss=191.8680
	step [84/244], loss=168.4967
	step [85/244], loss=149.6621
	step [86/244], loss=157.6207
	step [87/244], loss=162.1602
	step [88/244], loss=152.1600
	step [89/244], loss=173.7415
	step [90/244], loss=171.2280
	step [91/244], loss=132.0521
	step [92/244], loss=153.6178
	step [93/244], loss=170.7313
	step [94/244], loss=165.8569
	step [95/244], loss=185.2137
	step [96/244], loss=177.9904
	step [97/244], loss=166.4920
	step [98/244], loss=171.9417
	step [99/244], loss=186.3582
	step [100/244], loss=172.2673
	step [101/244], loss=163.1413
	step [102/244], loss=150.0726
	step [103/244], loss=168.0372
	step [104/244], loss=164.0588
	step [105/244], loss=161.7481
	step [106/244], loss=163.0780
	step [107/244], loss=170.7689
	step [108/244], loss=156.8937
	step [109/244], loss=163.8461
	step [110/244], loss=159.8481
	step [111/244], loss=165.0423
	step [112/244], loss=158.9575
	step [113/244], loss=186.1699
	step [114/244], loss=161.9651
	step [115/244], loss=161.3007
	step [116/244], loss=135.7243
	step [117/244], loss=173.6745
	step [118/244], loss=158.5977
	step [119/244], loss=168.7050
	step [120/244], loss=165.2997
	step [121/244], loss=170.6742
	step [122/244], loss=158.1349
	step [123/244], loss=148.1033
	step [124/244], loss=180.3228
	step [125/244], loss=159.5676
	step [126/244], loss=151.1792
	step [127/244], loss=152.6685
	step [128/244], loss=175.4373
	step [129/244], loss=174.0131
	step [130/244], loss=163.7026
	step [131/244], loss=149.6474
	step [132/244], loss=163.3962
	step [133/244], loss=164.2343
	step [134/244], loss=170.2895
	step [135/244], loss=164.9052
	step [136/244], loss=163.8990
	step [137/244], loss=157.2115
	step [138/244], loss=165.1183
	step [139/244], loss=133.8183
	step [140/244], loss=171.0643
	step [141/244], loss=169.9646
	step [142/244], loss=185.5953
	step [143/244], loss=162.7767
	step [144/244], loss=150.8102
	step [145/244], loss=153.6623
	step [146/244], loss=158.2908
	step [147/244], loss=158.2959
	step [148/244], loss=179.2050
	step [149/244], loss=183.3340
	step [150/244], loss=160.8759
	step [151/244], loss=165.7545
	step [152/244], loss=156.4169
	step [153/244], loss=176.2044
	step [154/244], loss=168.7969
	step [155/244], loss=159.2621
	step [156/244], loss=154.1382
	step [157/244], loss=166.2344
	step [158/244], loss=171.8116
	step [159/244], loss=162.1456
	step [160/244], loss=156.7523
	step [161/244], loss=158.4828
	step [162/244], loss=159.7596
	step [163/244], loss=153.7043
	step [164/244], loss=153.7845
	step [165/244], loss=145.4169
	step [166/244], loss=194.2086
	step [167/244], loss=158.9451
	step [168/244], loss=153.9088
	step [169/244], loss=161.3523
	step [170/244], loss=170.2841
	step [171/244], loss=174.5151
	step [172/244], loss=155.1851
	step [173/244], loss=155.8596
	step [174/244], loss=144.6679
	step [175/244], loss=161.3250
	step [176/244], loss=162.2408
	step [177/244], loss=153.2300
	step [178/244], loss=149.7050
	step [179/244], loss=150.0900
	step [180/244], loss=144.8048
	step [181/244], loss=162.7995
	step [182/244], loss=160.9202
	step [183/244], loss=169.1008
	step [184/244], loss=172.1205
	step [185/244], loss=138.2690
	step [186/244], loss=171.1276
	step [187/244], loss=167.1847
	step [188/244], loss=152.3956
	step [189/244], loss=135.5014
	step [190/244], loss=166.9807
	step [191/244], loss=151.0677
	step [192/244], loss=161.5620
	step [193/244], loss=144.8998
	step [194/244], loss=150.6582
	step [195/244], loss=173.1134
	step [196/244], loss=150.0478
	step [197/244], loss=155.8139
	step [198/244], loss=144.0685
	step [199/244], loss=152.8927
	step [200/244], loss=145.1253
	step [201/244], loss=157.4537
	step [202/244], loss=148.9218
	step [203/244], loss=157.1071
	step [204/244], loss=152.4955
	step [205/244], loss=152.6484
	step [206/244], loss=163.1790
	step [207/244], loss=152.8034
	step [208/244], loss=157.2212
	step [209/244], loss=142.3950
	step [210/244], loss=157.2837
	step [211/244], loss=155.5184
	step [212/244], loss=153.9490
	step [213/244], loss=144.2575
	step [214/244], loss=144.6460
	step [215/244], loss=169.1174
	step [216/244], loss=144.2047
	step [217/244], loss=129.2562
	step [218/244], loss=175.3441
	step [219/244], loss=148.7609
	step [220/244], loss=166.4842
	step [221/244], loss=138.9093
	step [222/244], loss=132.4938
	step [223/244], loss=155.5016
	step [224/244], loss=157.3921
	step [225/244], loss=172.5145
	step [226/244], loss=160.8155
	step [227/244], loss=149.3750
	step [228/244], loss=166.0948
	step [229/244], loss=150.9907
	step [230/244], loss=148.9595
	step [231/244], loss=134.2945
	step [232/244], loss=154.5587
	step [233/244], loss=141.8083
	step [234/244], loss=153.0012
	step [235/244], loss=150.0730
	step [236/244], loss=159.4565
	step [237/244], loss=150.2450
	step [238/244], loss=145.4813
	step [239/244], loss=163.2534
	step [240/244], loss=149.3390
	step [241/244], loss=163.0113
	step [242/244], loss=149.4791
	step [243/244], loss=131.5226
	step [244/244], loss=65.7741
	Evaluating
	loss=0.2349, precision=0.5454, recall=0.8785, f1=0.6730
saving model as: 1_saved_model.pth
Training epoch 4
	step [1/244], loss=153.3494
	step [2/244], loss=152.2431
	step [3/244], loss=132.8952
	step [4/244], loss=164.7364
	step [5/244], loss=149.9063
	step [6/244], loss=161.2999
	step [7/244], loss=159.2248
	step [8/244], loss=149.5396
	step [9/244], loss=171.0653
	step [10/244], loss=148.2181
	step [11/244], loss=146.1239
	step [12/244], loss=139.1885
	step [13/244], loss=160.7169
	step [14/244], loss=147.9867
	step [15/244], loss=141.4504
	step [16/244], loss=158.2241
	step [17/244], loss=152.8650
	step [18/244], loss=155.1970
	step [19/244], loss=173.9269
	step [20/244], loss=128.6656
	step [21/244], loss=160.4237
	step [22/244], loss=144.4956
	step [23/244], loss=153.6168
	step [24/244], loss=142.6176
	step [25/244], loss=142.5856
	step [26/244], loss=133.8125
	step [27/244], loss=152.4481
	step [28/244], loss=178.1464
	step [29/244], loss=145.3167
	step [30/244], loss=154.0146
	step [31/244], loss=156.8390
	step [32/244], loss=140.4232
	step [33/244], loss=138.3973
	step [34/244], loss=135.7473
	step [35/244], loss=151.0056
	step [36/244], loss=150.9600
	step [37/244], loss=168.3322
	step [38/244], loss=138.4806
	step [39/244], loss=154.2787
	step [40/244], loss=143.7211
	step [41/244], loss=159.7101
	step [42/244], loss=140.4881
	step [43/244], loss=185.0203
	step [44/244], loss=164.3903
	step [45/244], loss=148.0332
	step [46/244], loss=146.1784
	step [47/244], loss=161.6876
	step [48/244], loss=129.5171
	step [49/244], loss=153.2387
	step [50/244], loss=153.5916
	step [51/244], loss=161.6462
	step [52/244], loss=134.2116
	step [53/244], loss=139.2722
	step [54/244], loss=153.3794
	step [55/244], loss=149.9571
	step [56/244], loss=144.1362
	step [57/244], loss=150.8560
	step [58/244], loss=138.4071
	step [59/244], loss=139.7097
	step [60/244], loss=154.5639
	step [61/244], loss=142.6770
	step [62/244], loss=143.0441
	step [63/244], loss=130.1523
	step [64/244], loss=143.0578
	step [65/244], loss=137.1814
	step [66/244], loss=160.3246
	step [67/244], loss=137.1694
	step [68/244], loss=143.4566
	step [69/244], loss=142.4734
	step [70/244], loss=170.3141
	step [71/244], loss=183.6136
	step [72/244], loss=143.8072
	step [73/244], loss=161.2531
	step [74/244], loss=140.5812
	step [75/244], loss=138.2213
	step [76/244], loss=150.9052
	step [77/244], loss=156.1176
	step [78/244], loss=143.8670
	step [79/244], loss=145.4812
	step [80/244], loss=146.7137
	step [81/244], loss=153.6238
	step [82/244], loss=146.1130
	step [83/244], loss=145.1663
	step [84/244], loss=126.5345
	step [85/244], loss=137.0578
	step [86/244], loss=149.6615
	step [87/244], loss=146.4308
	step [88/244], loss=155.8186
	step [89/244], loss=161.3182
	step [90/244], loss=143.2039
	step [91/244], loss=145.4050
	step [92/244], loss=139.0028
	step [93/244], loss=149.2237
	step [94/244], loss=164.2569
	step [95/244], loss=156.4426
	step [96/244], loss=151.6555
	step [97/244], loss=137.0848
	step [98/244], loss=136.5480
	step [99/244], loss=143.3285
	step [100/244], loss=137.1258
	step [101/244], loss=137.9722
	step [102/244], loss=151.7396
	step [103/244], loss=134.5170
	step [104/244], loss=163.3186
	step [105/244], loss=143.6765
	step [106/244], loss=153.6376
	step [107/244], loss=148.1914
	step [108/244], loss=144.0090
	step [109/244], loss=161.2978
	step [110/244], loss=122.4548
	step [111/244], loss=135.7445
	step [112/244], loss=134.1137
	step [113/244], loss=130.4059
	step [114/244], loss=134.0843
	step [115/244], loss=140.4338
	step [116/244], loss=161.8224
	step [117/244], loss=135.6730
	step [118/244], loss=131.9836
	step [119/244], loss=172.7314
	step [120/244], loss=144.2414
	step [121/244], loss=138.4532
	step [122/244], loss=130.6152
	step [123/244], loss=150.3420
	step [124/244], loss=159.9868
	step [125/244], loss=154.6953
	step [126/244], loss=144.6246
	step [127/244], loss=124.1179
	step [128/244], loss=129.0609
	step [129/244], loss=131.7451
	step [130/244], loss=137.0439
	step [131/244], loss=145.9050
	step [132/244], loss=145.7555
	step [133/244], loss=132.1890
	step [134/244], loss=171.7980
	step [135/244], loss=150.4349
	step [136/244], loss=141.6670
	step [137/244], loss=161.0135
	step [138/244], loss=167.1900
	step [139/244], loss=168.1389
	step [140/244], loss=137.9649
	step [141/244], loss=156.1465
	step [142/244], loss=132.9037
	step [143/244], loss=109.7522
	step [144/244], loss=151.2090
	step [145/244], loss=134.7563
	step [146/244], loss=143.3001
	step [147/244], loss=145.7362
	step [148/244], loss=147.0584
	step [149/244], loss=133.8719
	step [150/244], loss=148.7593
	step [151/244], loss=140.5404
	step [152/244], loss=125.2508
	step [153/244], loss=150.8133
	step [154/244], loss=176.2672
	step [155/244], loss=146.6526
	step [156/244], loss=154.7116
	step [157/244], loss=154.6426
	step [158/244], loss=163.6524
	step [159/244], loss=130.0901
	step [160/244], loss=140.7701
	step [161/244], loss=150.7641
	step [162/244], loss=133.7010
	step [163/244], loss=126.5167
	step [164/244], loss=160.3807
	step [165/244], loss=127.0628
	step [166/244], loss=128.5291
	step [167/244], loss=139.6992
	step [168/244], loss=153.8191
	step [169/244], loss=149.0126
	step [170/244], loss=161.1912
	step [171/244], loss=145.9393
	step [172/244], loss=132.0313
	step [173/244], loss=128.6342
	step [174/244], loss=151.2418
	step [175/244], loss=141.5937
	step [176/244], loss=125.1013
	step [177/244], loss=155.3867
	step [178/244], loss=142.7631
	step [179/244], loss=144.6162
	step [180/244], loss=134.5344
	step [181/244], loss=163.2694
	step [182/244], loss=135.7580
	step [183/244], loss=125.6387
	step [184/244], loss=138.4434
	step [185/244], loss=141.2048
	step [186/244], loss=148.9180
	step [187/244], loss=128.9412
	step [188/244], loss=135.5594
	step [189/244], loss=147.7857
	step [190/244], loss=151.1872
	step [191/244], loss=164.5248
	step [192/244], loss=141.8244
	step [193/244], loss=144.4265
	step [194/244], loss=143.5984
	step [195/244], loss=138.6130
	step [196/244], loss=134.0351
	step [197/244], loss=143.2686
	step [198/244], loss=147.4571
	step [199/244], loss=148.3064
	step [200/244], loss=120.3498
	step [201/244], loss=152.3815
	step [202/244], loss=140.3164
	step [203/244], loss=129.6786
	step [204/244], loss=108.7179
	step [205/244], loss=139.7984
	step [206/244], loss=143.6549
	step [207/244], loss=142.0585
	step [208/244], loss=113.9102
	step [209/244], loss=153.6566
	step [210/244], loss=152.6361
	step [211/244], loss=137.8073
	step [212/244], loss=125.5933
	step [213/244], loss=110.3588
	step [214/244], loss=152.8457
	step [215/244], loss=136.4994
	step [216/244], loss=134.4959
	step [217/244], loss=118.9689
	step [218/244], loss=134.4280
	step [219/244], loss=140.7728
	step [220/244], loss=137.4648
	step [221/244], loss=140.8973
	step [222/244], loss=138.8108
	step [223/244], loss=130.6823
	step [224/244], loss=138.6804
	step [225/244], loss=137.5097
	step [226/244], loss=125.3535
	step [227/244], loss=154.1590
	step [228/244], loss=142.9902
	step [229/244], loss=141.1010
	step [230/244], loss=163.2620
	step [231/244], loss=139.1065
	step [232/244], loss=160.6324
	step [233/244], loss=148.2651
	step [234/244], loss=120.7442
	step [235/244], loss=160.7690
	step [236/244], loss=112.9317
	step [237/244], loss=134.0131
	step [238/244], loss=152.1261
	step [239/244], loss=125.1251
	step [240/244], loss=140.9849
	step [241/244], loss=143.2991
	step [242/244], loss=123.8383
	step [243/244], loss=139.3557
	step [244/244], loss=74.2657
	Evaluating
	loss=0.1655, precision=0.5134, recall=0.9065, f1=0.6555
Training epoch 5
	step [1/244], loss=124.6884
	step [2/244], loss=130.8899
	step [3/244], loss=156.8916
	step [4/244], loss=149.3761
	step [5/244], loss=127.6970
	step [6/244], loss=123.3245
	step [7/244], loss=122.8562
	step [8/244], loss=152.8528
	step [9/244], loss=118.5641
	step [10/244], loss=113.6505
	step [11/244], loss=142.8198
	step [12/244], loss=135.4903
	step [13/244], loss=117.9041
	step [14/244], loss=141.3544
	step [15/244], loss=169.0789
	step [16/244], loss=124.5482
	step [17/244], loss=137.3338
	step [18/244], loss=132.8548
	step [19/244], loss=141.1014
	step [20/244], loss=125.0698
	step [21/244], loss=134.4298
	step [22/244], loss=141.3458
	step [23/244], loss=116.9594
	step [24/244], loss=140.9760
	step [25/244], loss=134.9988
	step [26/244], loss=135.4774
	step [27/244], loss=154.2421
	step [28/244], loss=139.1063
	step [29/244], loss=141.0241
	step [30/244], loss=130.2482
	step [31/244], loss=128.6050
	step [32/244], loss=144.3159
	step [33/244], loss=130.8391
	step [34/244], loss=122.5384
	step [35/244], loss=139.4846
	step [36/244], loss=147.7843
	step [37/244], loss=131.7134
	step [38/244], loss=160.1071
	step [39/244], loss=153.5476
	step [40/244], loss=128.7163
	step [41/244], loss=141.5657
	step [42/244], loss=123.0225
	step [43/244], loss=130.7565
	step [44/244], loss=140.3120
	step [45/244], loss=144.9808
	step [46/244], loss=138.5582
	step [47/244], loss=132.1047
	step [48/244], loss=118.7719
	step [49/244], loss=114.4959
	step [50/244], loss=132.1354
	step [51/244], loss=144.3287
	step [52/244], loss=140.4262
	step [53/244], loss=141.7818
	step [54/244], loss=148.6756
	step [55/244], loss=143.8211
	step [56/244], loss=112.7281
	step [57/244], loss=160.3741
	step [58/244], loss=152.4067
	step [59/244], loss=132.6363
	step [60/244], loss=138.9491
	step [61/244], loss=146.3706
	step [62/244], loss=170.8870
	step [63/244], loss=147.2790
	step [64/244], loss=148.9041
	step [65/244], loss=140.3676
	step [66/244], loss=142.2262
	step [67/244], loss=130.1757
	step [68/244], loss=145.5830
	step [69/244], loss=122.1900
	step [70/244], loss=129.2366
	step [71/244], loss=133.4660
	step [72/244], loss=139.3646
	step [73/244], loss=129.3690
	step [74/244], loss=125.9742
	step [75/244], loss=140.8381
	step [76/244], loss=156.0761
	step [77/244], loss=141.5038
	step [78/244], loss=130.1843
	step [79/244], loss=138.7036
	step [80/244], loss=135.1889
	step [81/244], loss=122.1341
	step [82/244], loss=151.7183
	step [83/244], loss=127.5226
	step [84/244], loss=142.5902
	step [85/244], loss=108.4703
	step [86/244], loss=126.8677
	step [87/244], loss=147.9642
	step [88/244], loss=151.1680
	step [89/244], loss=134.8063
	step [90/244], loss=135.2643
	step [91/244], loss=153.7145
	step [92/244], loss=124.3956
	step [93/244], loss=129.7035
	step [94/244], loss=159.3887
	step [95/244], loss=120.7705
	step [96/244], loss=126.2170
	step [97/244], loss=144.7817
	step [98/244], loss=114.3952
	step [99/244], loss=127.4350
	step [100/244], loss=131.3353
	step [101/244], loss=128.6486
	step [102/244], loss=141.0091
	step [103/244], loss=124.8561
	step [104/244], loss=138.7363
	step [105/244], loss=155.2461
	step [106/244], loss=154.8857
	step [107/244], loss=121.6305
	step [108/244], loss=150.0260
	step [109/244], loss=146.7305
	step [110/244], loss=120.5514
	step [111/244], loss=129.3866
	step [112/244], loss=120.3833
	step [113/244], loss=130.1423
	step [114/244], loss=125.0707
	step [115/244], loss=122.9739
	step [116/244], loss=122.8526
	step [117/244], loss=159.2006
	step [118/244], loss=132.1588
	step [119/244], loss=158.0439
	step [120/244], loss=142.9152
	step [121/244], loss=124.8537
	step [122/244], loss=122.8571
	step [123/244], loss=133.7448
	step [124/244], loss=140.7449
	step [125/244], loss=127.6172
	step [126/244], loss=131.4614
	step [127/244], loss=128.0891
	step [128/244], loss=137.3906
	step [129/244], loss=133.1995
	step [130/244], loss=133.7280
	step [131/244], loss=130.5419
	step [132/244], loss=140.1548
	step [133/244], loss=128.3676
	step [134/244], loss=153.0232
	step [135/244], loss=112.8524
	step [136/244], loss=126.6885
	step [137/244], loss=117.4342
	step [138/244], loss=127.7922
	step [139/244], loss=130.8480
	step [140/244], loss=144.9509
	step [141/244], loss=136.3274
	step [142/244], loss=143.7778
	step [143/244], loss=117.5594
	step [144/244], loss=129.8585
	step [145/244], loss=121.7942
	step [146/244], loss=139.3903
	step [147/244], loss=127.2492
	step [148/244], loss=131.7726
	step [149/244], loss=146.5131
	step [150/244], loss=127.0019
	step [151/244], loss=122.2796
	step [152/244], loss=148.9775
	step [153/244], loss=117.2731
	step [154/244], loss=128.0100
	step [155/244], loss=148.8449
	step [156/244], loss=113.4287
	step [157/244], loss=103.6833
	step [158/244], loss=144.7954
	step [159/244], loss=111.6679
	step [160/244], loss=131.4721
	step [161/244], loss=146.0927
	step [162/244], loss=132.6783
	step [163/244], loss=142.9647
	step [164/244], loss=110.3863
	step [165/244], loss=129.9981
	step [166/244], loss=143.2849
	step [167/244], loss=127.1713
	step [168/244], loss=143.0862
	step [169/244], loss=119.5316
	step [170/244], loss=124.5261
	step [171/244], loss=129.3581
	step [172/244], loss=116.5192
	step [173/244], loss=108.5288
	step [174/244], loss=140.0131
	step [175/244], loss=129.3900
	step [176/244], loss=130.5130
	step [177/244], loss=125.1709
	step [178/244], loss=145.0649
	step [179/244], loss=131.7180
	step [180/244], loss=128.1339
	step [181/244], loss=135.7668
	step [182/244], loss=129.8393
	step [183/244], loss=123.9247
	step [184/244], loss=140.2186
	step [185/244], loss=119.0671
	step [186/244], loss=130.1791
	step [187/244], loss=129.3846
	step [188/244], loss=123.6323
	step [189/244], loss=132.4924
	step [190/244], loss=112.6352
	step [191/244], loss=135.4478
	step [192/244], loss=109.5051
	step [193/244], loss=112.3944
	step [194/244], loss=114.6929
	step [195/244], loss=124.4504
	step [196/244], loss=125.7254
	step [197/244], loss=124.3844
	step [198/244], loss=134.6638
	step [199/244], loss=106.3057
	step [200/244], loss=138.3391
	step [201/244], loss=111.5089
	step [202/244], loss=111.1189
	step [203/244], loss=134.1899
	step [204/244], loss=128.0110
	step [205/244], loss=117.9203
	step [206/244], loss=109.8786
	step [207/244], loss=122.1665
	step [208/244], loss=124.5199
	step [209/244], loss=141.7411
	step [210/244], loss=143.2804
	step [211/244], loss=108.9141
	step [212/244], loss=142.7287
	step [213/244], loss=143.8442
	step [214/244], loss=122.7481
	step [215/244], loss=140.5963
	step [216/244], loss=143.0915
	step [217/244], loss=117.2574
	step [218/244], loss=114.4565
	step [219/244], loss=136.2625
	step [220/244], loss=105.8178
	step [221/244], loss=156.5200
	step [222/244], loss=146.7824
	step [223/244], loss=117.3018
	step [224/244], loss=120.4908
	step [225/244], loss=111.0813
	step [226/244], loss=103.0284
	step [227/244], loss=121.9190
	step [228/244], loss=106.2376
	step [229/244], loss=142.7673
	step [230/244], loss=109.6342
	step [231/244], loss=152.0192
	step [232/244], loss=128.5991
	step [233/244], loss=136.7323
	step [234/244], loss=116.5073
	step [235/244], loss=126.0040
	step [236/244], loss=127.2132
	step [237/244], loss=143.8634
	step [238/244], loss=135.3333
	step [239/244], loss=121.2079
	step [240/244], loss=131.3670
	step [241/244], loss=120.6389
	step [242/244], loss=142.3955
	step [243/244], loss=118.6415
	step [244/244], loss=47.3796
	Evaluating
	loss=0.1223, precision=0.5223, recall=0.8719, f1=0.6533
Training epoch 6
	step [1/244], loss=123.2231
	step [2/244], loss=127.8506
	step [3/244], loss=122.0764
	step [4/244], loss=143.8007
	step [5/244], loss=123.5629
	step [6/244], loss=107.3882
	step [7/244], loss=119.6882
	step [8/244], loss=126.2273
	step [9/244], loss=111.8411
	step [10/244], loss=138.7477
	step [11/244], loss=139.1309
	step [12/244], loss=106.8688
	step [13/244], loss=120.9154
	step [14/244], loss=145.5057
	step [15/244], loss=109.0059
	step [16/244], loss=109.8402
	step [17/244], loss=153.6939
	step [18/244], loss=117.2827
	step [19/244], loss=131.9083
	step [20/244], loss=134.4380
	step [21/244], loss=121.7543
	step [22/244], loss=103.3355
	step [23/244], loss=118.4711
	step [24/244], loss=130.1790
	step [25/244], loss=136.3566
	step [26/244], loss=159.2020
	step [27/244], loss=145.2685
	step [28/244], loss=120.3455
	step [29/244], loss=130.1105
	step [30/244], loss=138.9887
	step [31/244], loss=135.2635
	step [32/244], loss=114.9246
	step [33/244], loss=127.8993
	step [34/244], loss=118.7821
	step [35/244], loss=135.9221
	step [36/244], loss=120.6953
	step [37/244], loss=110.4639
	step [38/244], loss=126.8153
	step [39/244], loss=122.1203
	step [40/244], loss=117.0936
	step [41/244], loss=136.4634
	step [42/244], loss=125.5489
	step [43/244], loss=122.2610
	step [44/244], loss=129.0447
	step [45/244], loss=148.7886
	step [46/244], loss=118.9009
	step [47/244], loss=120.4834
	step [48/244], loss=138.2054
	step [49/244], loss=133.3024
	step [50/244], loss=124.7609
	step [51/244], loss=103.1902
	step [52/244], loss=125.2738
	step [53/244], loss=128.7540
	step [54/244], loss=128.7326
	step [55/244], loss=126.7001
	step [56/244], loss=125.7622
	step [57/244], loss=115.3088
	step [58/244], loss=113.4467
	step [59/244], loss=107.0130
	step [60/244], loss=129.6394
	step [61/244], loss=124.8679
	step [62/244], loss=121.3823
	step [63/244], loss=130.7478
	step [64/244], loss=120.6647
	step [65/244], loss=122.5060
	step [66/244], loss=124.3535
	step [67/244], loss=136.6651
	step [68/244], loss=123.0617
	step [69/244], loss=136.4219
	step [70/244], loss=128.2287
	step [71/244], loss=129.0219
	step [72/244], loss=120.3975
	step [73/244], loss=120.4407
	step [74/244], loss=144.2609
	step [75/244], loss=134.6629
	step [76/244], loss=133.9354
	step [77/244], loss=133.5150
	step [78/244], loss=108.9663
	step [79/244], loss=131.2151
	step [80/244], loss=125.4834
	step [81/244], loss=107.4261
	step [82/244], loss=124.8889
	step [83/244], loss=116.9918
	step [84/244], loss=122.8694
	step [85/244], loss=153.1572
	step [86/244], loss=126.4165
	step [87/244], loss=120.8965
	step [88/244], loss=120.7263
	step [89/244], loss=110.6413
	step [90/244], loss=108.1454
	step [91/244], loss=123.3786
	step [92/244], loss=116.3870
	step [93/244], loss=121.7576
	step [94/244], loss=147.7574
	step [95/244], loss=115.2686
	step [96/244], loss=130.8417
	step [97/244], loss=118.1001
	step [98/244], loss=120.1557
	step [99/244], loss=118.9218
	step [100/244], loss=123.4797
	step [101/244], loss=118.3295
	step [102/244], loss=114.7358
	step [103/244], loss=133.9350
	step [104/244], loss=124.7937
	step [105/244], loss=128.6244
	step [106/244], loss=110.4363
	step [107/244], loss=124.2778
	step [108/244], loss=134.7136
	step [109/244], loss=116.0678
	step [110/244], loss=127.1895
	step [111/244], loss=134.9933
	step [112/244], loss=113.1862
	step [113/244], loss=119.7128
	step [114/244], loss=115.6633
	step [115/244], loss=122.5648
	step [116/244], loss=138.2392
	step [117/244], loss=121.5991
	step [118/244], loss=103.5673
	step [119/244], loss=131.5348
	step [120/244], loss=113.4676
	step [121/244], loss=145.4788
	step [122/244], loss=122.1054
	step [123/244], loss=131.1984
	step [124/244], loss=152.9026
	step [125/244], loss=109.7457
	step [126/244], loss=132.8977
	step [127/244], loss=107.2735
	step [128/244], loss=108.4164
	step [129/244], loss=152.7283
	step [130/244], loss=115.0836
	step [131/244], loss=116.9427
	step [132/244], loss=131.0790
	step [133/244], loss=126.9085
	step [134/244], loss=122.0052
	step [135/244], loss=133.0647
	step [136/244], loss=129.7059
	step [137/244], loss=131.8374
	step [138/244], loss=118.0190
	step [139/244], loss=108.6395
	step [140/244], loss=125.7452
	step [141/244], loss=124.5074
	step [142/244], loss=115.5084
	step [143/244], loss=129.6683
	step [144/244], loss=141.3748
	step [145/244], loss=127.3222
	step [146/244], loss=149.9266
	step [147/244], loss=129.1432
	step [148/244], loss=148.8335
	step [149/244], loss=117.6931
	step [150/244], loss=115.4512
	step [151/244], loss=122.0360
	step [152/244], loss=134.0457
	step [153/244], loss=104.4408
	step [154/244], loss=127.7227
	step [155/244], loss=127.6478
	step [156/244], loss=121.6709
	step [157/244], loss=111.9520
	step [158/244], loss=123.3702
	step [159/244], loss=110.0215
	step [160/244], loss=118.5737
	step [161/244], loss=112.6877
	step [162/244], loss=129.5657
	step [163/244], loss=116.4141
	step [164/244], loss=115.6512
	step [165/244], loss=117.0023
	step [166/244], loss=117.5024
	step [167/244], loss=114.8687
	step [168/244], loss=130.4563
	step [169/244], loss=114.2114
	step [170/244], loss=128.4737
	step [171/244], loss=103.9969
	step [172/244], loss=123.2915
	step [173/244], loss=105.1166
	step [174/244], loss=144.8723
	step [175/244], loss=142.3575
	step [176/244], loss=146.8336
	step [177/244], loss=121.6570
	step [178/244], loss=135.1694
	step [179/244], loss=112.6687
	step [180/244], loss=126.0239
	step [181/244], loss=126.1877
	step [182/244], loss=108.7066
	step [183/244], loss=126.6072
	step [184/244], loss=123.0664
	step [185/244], loss=136.2947
	step [186/244], loss=120.4639
	step [187/244], loss=115.5642
	step [188/244], loss=127.1228
	step [189/244], loss=117.7978
	step [190/244], loss=129.0771
	step [191/244], loss=132.1806
	step [192/244], loss=138.1532
	step [193/244], loss=99.8693
	step [194/244], loss=107.1467
	step [195/244], loss=99.7999
	step [196/244], loss=117.6441
	step [197/244], loss=128.2768
	step [198/244], loss=139.5258
	step [199/244], loss=120.2027
	step [200/244], loss=99.9171
	step [201/244], loss=116.2717
	step [202/244], loss=110.7511
	step [203/244], loss=110.2598
	step [204/244], loss=132.9946
	step [205/244], loss=126.5509
	step [206/244], loss=100.2457
	step [207/244], loss=114.9949
	step [208/244], loss=138.2724
	step [209/244], loss=126.2066
	step [210/244], loss=141.8560
	step [211/244], loss=123.7372
	step [212/244], loss=124.7828
	step [213/244], loss=110.2637
	step [214/244], loss=118.2065
	step [215/244], loss=130.7833
	step [216/244], loss=135.1238
	step [217/244], loss=106.8060
	step [218/244], loss=130.9950
	step [219/244], loss=110.8981
	step [220/244], loss=116.7032
	step [221/244], loss=116.9523
	step [222/244], loss=120.2262
	step [223/244], loss=104.4969
	step [224/244], loss=119.7741
	step [225/244], loss=141.3579
	step [226/244], loss=127.4261
	step [227/244], loss=109.5785
	step [228/244], loss=140.4107
	step [229/244], loss=130.5958
	step [230/244], loss=129.9642
	step [231/244], loss=103.8525
	step [232/244], loss=110.3323
	step [233/244], loss=118.8088
	step [234/244], loss=121.1502
	step [235/244], loss=123.6263
	step [236/244], loss=139.9650
	step [237/244], loss=115.5062
	step [238/244], loss=105.5057
	step [239/244], loss=107.4989
	step [240/244], loss=116.1477
	step [241/244], loss=129.0814
	step [242/244], loss=103.8336
	step [243/244], loss=119.2974
	step [244/244], loss=51.8066
	Evaluating
	loss=0.0862, precision=0.5117, recall=0.8814, f1=0.6475
Training epoch 7
	step [1/244], loss=133.5623
	step [2/244], loss=95.1654
	step [3/244], loss=116.8940
	step [4/244], loss=122.0340
	step [5/244], loss=106.1563
	step [6/244], loss=129.0734
	step [7/244], loss=129.7121
	step [8/244], loss=96.5947
	step [9/244], loss=135.9796
	step [10/244], loss=118.3279
	step [11/244], loss=108.5722
	step [12/244], loss=121.1044
	step [13/244], loss=135.4058
	step [14/244], loss=122.2818
	step [15/244], loss=120.0829
	step [16/244], loss=109.5073
	step [17/244], loss=133.1035
	step [18/244], loss=137.7404
	step [19/244], loss=108.7043
	step [20/244], loss=130.4036
	step [21/244], loss=117.5610
	step [22/244], loss=135.4867
	step [23/244], loss=128.3860
	step [24/244], loss=113.2413
	step [25/244], loss=127.3647
	step [26/244], loss=109.6667
	step [27/244], loss=112.9259
	step [28/244], loss=94.6294
	step [29/244], loss=101.7901
	step [30/244], loss=122.2467
	step [31/244], loss=121.7295
	step [32/244], loss=111.2912
	step [33/244], loss=104.9126
	step [34/244], loss=109.8641
	step [35/244], loss=121.4303
	step [36/244], loss=138.1104
	step [37/244], loss=98.3990
	step [38/244], loss=107.6723
	step [39/244], loss=119.5551
	step [40/244], loss=121.5652
	step [41/244], loss=115.4018
	step [42/244], loss=117.1464
	step [43/244], loss=121.5474
	step [44/244], loss=130.5318
	step [45/244], loss=106.7452
	step [46/244], loss=108.7421
	step [47/244], loss=120.3512
	step [48/244], loss=131.7715
	step [49/244], loss=105.8278
	step [50/244], loss=143.4782
	step [51/244], loss=144.5206
	step [52/244], loss=96.2565
	step [53/244], loss=119.5132
	step [54/244], loss=117.7705
	step [55/244], loss=118.4475
	step [56/244], loss=136.1520
	step [57/244], loss=120.2270
	step [58/244], loss=110.7731
	step [59/244], loss=119.9865
	step [60/244], loss=116.0695
	step [61/244], loss=113.9301
	step [62/244], loss=112.6366
	step [63/244], loss=115.1863
	step [64/244], loss=101.3281
	step [65/244], loss=97.6312
	step [66/244], loss=118.5470
	step [67/244], loss=120.6539
	step [68/244], loss=149.2075
	step [69/244], loss=117.6857
	step [70/244], loss=114.5702
	step [71/244], loss=124.0486
	step [72/244], loss=114.3832
	step [73/244], loss=123.0681
	step [74/244], loss=109.5952
	step [75/244], loss=122.8833
	step [76/244], loss=128.2814
	step [77/244], loss=121.4238
	step [78/244], loss=125.3081
	step [79/244], loss=113.2158
	step [80/244], loss=121.1563
	step [81/244], loss=123.7076
	step [82/244], loss=103.0067
	step [83/244], loss=105.2274
	step [84/244], loss=117.3130
	step [85/244], loss=115.0060
	step [86/244], loss=118.8897
	step [87/244], loss=147.4703
	step [88/244], loss=99.8384
	step [89/244], loss=122.5255
	step [90/244], loss=145.5453
	step [91/244], loss=129.8872
	step [92/244], loss=135.1497
	step [93/244], loss=127.7799
	step [94/244], loss=104.6395
	step [95/244], loss=127.5067
	step [96/244], loss=124.3412
	step [97/244], loss=116.6443
	step [98/244], loss=132.7873
	step [99/244], loss=112.2893
	step [100/244], loss=100.7441
	step [101/244], loss=115.2043
	step [102/244], loss=114.8060
	step [103/244], loss=114.8742
	step [104/244], loss=115.8970
	step [105/244], loss=122.4069
	step [106/244], loss=137.6441
	step [107/244], loss=125.6997
	step [108/244], loss=127.6885
	step [109/244], loss=108.9315
	step [110/244], loss=119.9841
	step [111/244], loss=108.1690
	step [112/244], loss=125.7352
	step [113/244], loss=114.1655
	step [114/244], loss=125.6580
	step [115/244], loss=116.2399
	step [116/244], loss=110.3779
	step [117/244], loss=130.7530
	step [118/244], loss=147.0577
	step [119/244], loss=102.9066
	step [120/244], loss=119.0007
	step [121/244], loss=119.4683
	step [122/244], loss=109.2744
	step [123/244], loss=104.6596
	step [124/244], loss=132.0804
	step [125/244], loss=92.7125
	step [126/244], loss=106.9616
	step [127/244], loss=94.9169
	step [128/244], loss=108.5800
	step [129/244], loss=107.5012
	step [130/244], loss=123.5144
	step [131/244], loss=132.6451
	step [132/244], loss=122.3467
	step [133/244], loss=98.9602
	step [134/244], loss=152.3278
	step [135/244], loss=132.2861
	step [136/244], loss=102.9077
	step [137/244], loss=112.8683
	step [138/244], loss=119.1847
	step [139/244], loss=105.8590
	step [140/244], loss=136.9089
	step [141/244], loss=116.8599
	step [142/244], loss=123.2881
	step [143/244], loss=128.0494
	step [144/244], loss=98.1214
	step [145/244], loss=121.4496
	step [146/244], loss=100.5804
	step [147/244], loss=146.2628
	step [148/244], loss=138.2021
	step [149/244], loss=102.7063
	step [150/244], loss=128.8582
	step [151/244], loss=114.7241
	step [152/244], loss=116.7632
	step [153/244], loss=119.6165
	step [154/244], loss=114.0671
	step [155/244], loss=101.0963
	step [156/244], loss=117.2267
	step [157/244], loss=129.2847
	step [158/244], loss=118.9351
	step [159/244], loss=120.3392
	step [160/244], loss=94.5135
	step [161/244], loss=114.6186
	step [162/244], loss=102.5570
	step [163/244], loss=111.7190
	step [164/244], loss=111.1877
	step [165/244], loss=111.0767
	step [166/244], loss=131.8594
	step [167/244], loss=135.6851
	step [168/244], loss=110.6518
	step [169/244], loss=119.0853
	step [170/244], loss=127.6261
	step [171/244], loss=108.6239
	step [172/244], loss=102.7214
	step [173/244], loss=117.7462
	step [174/244], loss=96.7536
	step [175/244], loss=122.9621
	step [176/244], loss=116.7239
	step [177/244], loss=100.8612
	step [178/244], loss=126.5693
	step [179/244], loss=122.6089
	step [180/244], loss=111.9238
	step [181/244], loss=101.9818
	step [182/244], loss=126.2616
	step [183/244], loss=153.3857
	step [184/244], loss=121.7523
	step [185/244], loss=100.8515
	step [186/244], loss=138.4279
	step [187/244], loss=123.1567
	step [188/244], loss=117.9370
	step [189/244], loss=98.0464
	step [190/244], loss=135.5247
	step [191/244], loss=118.5929
	step [192/244], loss=122.5818
	step [193/244], loss=110.1543
	step [194/244], loss=117.3921
	step [195/244], loss=101.2118
	step [196/244], loss=108.0841
	step [197/244], loss=125.3273
	step [198/244], loss=102.1868
	step [199/244], loss=106.0082
	step [200/244], loss=113.4918
	step [201/244], loss=114.1462
	step [202/244], loss=126.7379
	step [203/244], loss=123.2437
	step [204/244], loss=111.5229
	step [205/244], loss=93.8448
	step [206/244], loss=110.4420
	step [207/244], loss=108.1401
	step [208/244], loss=121.5047
	step [209/244], loss=113.4799
	step [210/244], loss=127.9958
	step [211/244], loss=115.6966
	step [212/244], loss=139.9613
	step [213/244], loss=131.4444
	step [214/244], loss=105.7794
	step [215/244], loss=132.4342
	step [216/244], loss=115.8035
	step [217/244], loss=118.6641
	step [218/244], loss=117.1640
	step [219/244], loss=129.6884
	step [220/244], loss=142.9973
	step [221/244], loss=120.3955
	step [222/244], loss=106.4379
	step [223/244], loss=122.9914
	step [224/244], loss=105.7088
	step [225/244], loss=97.2782
	step [226/244], loss=113.3634
	step [227/244], loss=97.6384
	step [228/244], loss=98.9047
	step [229/244], loss=137.1187
	step [230/244], loss=117.2661
	step [231/244], loss=104.0215
	step [232/244], loss=81.6133
	step [233/244], loss=111.2835
	step [234/244], loss=113.4929
	step [235/244], loss=122.5178
	step [236/244], loss=106.9146
	step [237/244], loss=121.6954
	step [238/244], loss=119.5112
	step [239/244], loss=128.1335
	step [240/244], loss=126.3896
	step [241/244], loss=91.4739
	step [242/244], loss=104.3537
	step [243/244], loss=130.2741
	step [244/244], loss=57.1226
	Evaluating
	loss=0.0710, precision=0.4536, recall=0.8612, f1=0.5942
Training epoch 8
	step [1/244], loss=121.5033
	step [2/244], loss=114.0687
	step [3/244], loss=130.4967
	step [4/244], loss=103.5434
	step [5/244], loss=144.8974
	step [6/244], loss=102.6230
	step [7/244], loss=116.6737
	step [8/244], loss=106.6505
	step [9/244], loss=108.0609
	step [10/244], loss=112.1459
	step [11/244], loss=128.0272
	step [12/244], loss=106.7858
	step [13/244], loss=114.6805
	step [14/244], loss=100.9903
	step [15/244], loss=111.1504
	step [16/244], loss=123.4170
	step [17/244], loss=102.7582
	step [18/244], loss=101.1346
	step [19/244], loss=115.2586
	step [20/244], loss=118.6316
	step [21/244], loss=103.5180
	step [22/244], loss=115.3488
	step [23/244], loss=108.2547
	step [24/244], loss=116.5805
	step [25/244], loss=118.1242
	step [26/244], loss=123.0836
	step [27/244], loss=108.8908
	step [28/244], loss=114.0783
	step [29/244], loss=120.7067
	step [30/244], loss=117.6127
	step [31/244], loss=111.2897
	step [32/244], loss=106.7414
	step [33/244], loss=147.1876
	step [34/244], loss=120.2478
	step [35/244], loss=120.9761
	step [36/244], loss=101.5230
	step [37/244], loss=132.6487
	step [38/244], loss=117.8071
	step [39/244], loss=104.8718
	step [40/244], loss=113.9391
	step [41/244], loss=105.4224
	step [42/244], loss=108.7779
	step [43/244], loss=127.6157
	step [44/244], loss=108.5806
	step [45/244], loss=119.0383
	step [46/244], loss=101.7329
	step [47/244], loss=114.9759
	step [48/244], loss=115.4475
	step [49/244], loss=125.5250
	step [50/244], loss=104.1023
	step [51/244], loss=128.6450
	step [52/244], loss=99.8028
	step [53/244], loss=137.6727
	step [54/244], loss=121.7091
	step [55/244], loss=119.1465
	step [56/244], loss=101.5387
	step [57/244], loss=121.3717
	step [58/244], loss=111.2059
	step [59/244], loss=104.5064
	step [60/244], loss=106.2735
	step [61/244], loss=129.0740
	step [62/244], loss=115.3534
	step [63/244], loss=110.6998
	step [64/244], loss=119.1046
	step [65/244], loss=113.7773
	step [66/244], loss=109.6597
	step [67/244], loss=101.3165
	step [68/244], loss=117.7392
	step [69/244], loss=120.1651
	step [70/244], loss=97.2126
	step [71/244], loss=109.3491
	step [72/244], loss=144.2612
	step [73/244], loss=117.6360
	step [74/244], loss=146.0791
	step [75/244], loss=116.9019
	step [76/244], loss=117.4748
	step [77/244], loss=110.5419
	step [78/244], loss=132.3813
	step [79/244], loss=111.1912
	step [80/244], loss=107.9556
	step [81/244], loss=132.2156
	step [82/244], loss=91.4326
	step [83/244], loss=138.1977
	step [84/244], loss=129.4790
	step [85/244], loss=108.6159
	step [86/244], loss=107.0302
	step [87/244], loss=111.6873
	step [88/244], loss=122.8017
	step [89/244], loss=121.0212
	step [90/244], loss=119.8434
	step [91/244], loss=110.2001
	step [92/244], loss=122.6531
	step [93/244], loss=120.0588
	step [94/244], loss=136.2641
	step [95/244], loss=110.4873
	step [96/244], loss=124.4023
	step [97/244], loss=111.7788
	step [98/244], loss=117.5451
	step [99/244], loss=106.8513
	step [100/244], loss=125.8223
	step [101/244], loss=131.5240
	step [102/244], loss=115.5722
	step [103/244], loss=116.9557
	step [104/244], loss=103.1158
	step [105/244], loss=132.6015
	step [106/244], loss=118.2253
	step [107/244], loss=98.7365
	step [108/244], loss=105.1250
	step [109/244], loss=112.6300
	step [110/244], loss=104.1657
	step [111/244], loss=112.0358
	step [112/244], loss=113.9017
	step [113/244], loss=96.5892
	step [114/244], loss=98.2117
	step [115/244], loss=115.0920
	step [116/244], loss=97.6883
	step [117/244], loss=107.4742
	step [118/244], loss=129.2431
	step [119/244], loss=96.6465
	step [120/244], loss=118.9665
	step [121/244], loss=124.8765
	step [122/244], loss=98.1842
	step [123/244], loss=117.6427
	step [124/244], loss=120.3475
	step [125/244], loss=118.7342
	step [126/244], loss=116.3397
	step [127/244], loss=107.5548
	step [128/244], loss=83.8202
	step [129/244], loss=107.5504
	step [130/244], loss=94.9302
	step [131/244], loss=91.4168
	step [132/244], loss=116.7627
	step [133/244], loss=132.3857
	step [134/244], loss=101.7943
	step [135/244], loss=112.0435
	step [136/244], loss=117.3008
	step [137/244], loss=101.0800
	step [138/244], loss=83.8439
	step [139/244], loss=101.4998
	step [140/244], loss=108.4250
	step [141/244], loss=121.8177
	step [142/244], loss=103.6232
	step [143/244], loss=109.0353
	step [144/244], loss=106.5363
	step [145/244], loss=110.3347
	step [146/244], loss=88.8712
	step [147/244], loss=109.5970
	step [148/244], loss=116.2778
	step [149/244], loss=118.2874
	step [150/244], loss=125.7663
	step [151/244], loss=134.8105
	step [152/244], loss=115.4153
	step [153/244], loss=118.2569
	step [154/244], loss=106.8211
	step [155/244], loss=122.5923
	step [156/244], loss=121.8132
	step [157/244], loss=115.6929
	step [158/244], loss=99.8628
	step [159/244], loss=120.9190
	step [160/244], loss=92.8724
	step [161/244], loss=126.2173
	step [162/244], loss=110.0063
	step [163/244], loss=113.0350
	step [164/244], loss=113.0586
	step [165/244], loss=116.2083
	step [166/244], loss=98.3390
	step [167/244], loss=102.5315
	step [168/244], loss=106.9579
	step [169/244], loss=113.8469
	step [170/244], loss=117.6076
	step [171/244], loss=113.8328
	step [172/244], loss=95.7126
	step [173/244], loss=147.0404
	step [174/244], loss=108.9006
	step [175/244], loss=107.2941
	step [176/244], loss=123.8555
	step [177/244], loss=105.2714
	step [178/244], loss=95.2866
	step [179/244], loss=92.6097
	step [180/244], loss=108.9027
	step [181/244], loss=123.6391
	step [182/244], loss=96.2478
	step [183/244], loss=128.3147
	step [184/244], loss=93.0486
	step [185/244], loss=119.0891
	step [186/244], loss=102.6456
	step [187/244], loss=103.8056
	step [188/244], loss=100.9786
	step [189/244], loss=128.7309
	step [190/244], loss=117.8945
	step [191/244], loss=103.7171
	step [192/244], loss=103.9427
	step [193/244], loss=109.2975
	step [194/244], loss=118.8422
	step [195/244], loss=108.4325
	step [196/244], loss=96.4323
	step [197/244], loss=114.3875
	step [198/244], loss=104.2063
	step [199/244], loss=126.3669
	step [200/244], loss=104.4974
	step [201/244], loss=104.7633
	step [202/244], loss=107.5571
	step [203/244], loss=122.4925
	step [204/244], loss=96.0344
	step [205/244], loss=106.1051
	step [206/244], loss=124.9934
	step [207/244], loss=143.5546
	step [208/244], loss=107.7624
	step [209/244], loss=96.9228
	step [210/244], loss=121.8598
	step [211/244], loss=107.6545
	step [212/244], loss=127.8366
	step [213/244], loss=95.4483
	step [214/244], loss=108.7582
	step [215/244], loss=141.2433
	step [216/244], loss=113.5965
	step [217/244], loss=118.7963
	step [218/244], loss=119.3672
	step [219/244], loss=128.2574
	step [220/244], loss=127.3274
	step [221/244], loss=122.0113
	step [222/244], loss=110.9351
	step [223/244], loss=136.7329
	step [224/244], loss=119.3896
	step [225/244], loss=103.3483
	step [226/244], loss=118.2293
	step [227/244], loss=98.7910
	step [228/244], loss=104.0869
	step [229/244], loss=114.9104
	step [230/244], loss=103.3233
	step [231/244], loss=120.0893
	step [232/244], loss=113.6569
	step [233/244], loss=107.5482
	step [234/244], loss=116.4030
	step [235/244], loss=105.7669
	step [236/244], loss=121.1168
	step [237/244], loss=117.9579
	step [238/244], loss=110.2897
	step [239/244], loss=117.6668
	step [240/244], loss=112.0954
	step [241/244], loss=123.4599
	step [242/244], loss=95.3163
	step [243/244], loss=100.8948
	step [244/244], loss=55.0005
	Evaluating
	loss=0.0529, precision=0.4918, recall=0.8913, f1=0.6338
Training epoch 9
	step [1/244], loss=105.3284
	step [2/244], loss=85.3853
	step [3/244], loss=129.1339
	step [4/244], loss=105.3355
	step [5/244], loss=101.7558
	step [6/244], loss=125.6519
	step [7/244], loss=131.8315
	step [8/244], loss=116.5623
	step [9/244], loss=119.4506
	step [10/244], loss=125.2670
	step [11/244], loss=122.9497
	step [12/244], loss=104.8175
	step [13/244], loss=117.2488
	step [14/244], loss=111.8724
	step [15/244], loss=126.8744
	step [16/244], loss=122.2531
	step [17/244], loss=99.5989
	step [18/244], loss=125.5876
	step [19/244], loss=118.0364
	step [20/244], loss=113.4672
	step [21/244], loss=131.0436
	step [22/244], loss=109.2641
	step [23/244], loss=105.7488
	step [24/244], loss=104.5280
	step [25/244], loss=100.4018
	step [26/244], loss=100.7441
	step [27/244], loss=118.4754
	step [28/244], loss=117.0875
	step [29/244], loss=101.2052
	step [30/244], loss=89.3907
	step [31/244], loss=110.1437
	step [32/244], loss=122.4215
	step [33/244], loss=92.1115
	step [34/244], loss=110.0902
	step [35/244], loss=122.6786
	step [36/244], loss=140.7487
	step [37/244], loss=130.7408
	step [38/244], loss=99.5748
	step [39/244], loss=120.6955
	step [40/244], loss=96.6189
	step [41/244], loss=113.6103
	step [42/244], loss=126.1857
	step [43/244], loss=91.7696
	step [44/244], loss=116.7409
	step [45/244], loss=119.9367
	step [46/244], loss=145.3321
	step [47/244], loss=91.8965
	step [48/244], loss=94.8970
	step [49/244], loss=122.8178
	step [50/244], loss=98.2608
	step [51/244], loss=120.9416
	step [52/244], loss=105.1459
	step [53/244], loss=107.0198
	step [54/244], loss=114.5699
	step [55/244], loss=114.1787
	step [56/244], loss=128.4330
	step [57/244], loss=120.7811
	step [58/244], loss=130.0156
	step [59/244], loss=113.9775
	step [60/244], loss=113.6860
	step [61/244], loss=113.9857
	step [62/244], loss=107.1006
	step [63/244], loss=131.0101
	step [64/244], loss=103.5792
	step [65/244], loss=129.0096
	step [66/244], loss=127.5246
	step [67/244], loss=130.2066
	step [68/244], loss=124.9985
	step [69/244], loss=133.0784
	step [70/244], loss=111.3885
	step [71/244], loss=91.2337
	step [72/244], loss=135.0047
	step [73/244], loss=101.3629
	step [74/244], loss=116.2629
	step [75/244], loss=119.6878
	step [76/244], loss=109.8648
	step [77/244], loss=129.0203
	step [78/244], loss=129.3761
	step [79/244], loss=120.7138
	step [80/244], loss=116.9733
	step [81/244], loss=103.3990
	step [82/244], loss=114.8616
	step [83/244], loss=111.7517
	step [84/244], loss=109.7563
	step [85/244], loss=120.2981
	step [86/244], loss=106.8406
	step [87/244], loss=97.7012
	step [88/244], loss=113.2566
	step [89/244], loss=89.1353
	step [90/244], loss=116.2116
	step [91/244], loss=86.3212
	step [92/244], loss=94.3762
	step [93/244], loss=122.2906
	step [94/244], loss=90.1561
	step [95/244], loss=121.1003
	step [96/244], loss=89.5175
	step [97/244], loss=110.2010
	step [98/244], loss=116.3553
	step [99/244], loss=116.3739
	step [100/244], loss=85.6651
	step [101/244], loss=101.4893
	step [102/244], loss=91.5317
	step [103/244], loss=133.4102
	step [104/244], loss=92.7043
	step [105/244], loss=97.3811
	step [106/244], loss=116.1055
	step [107/244], loss=102.3910
	step [108/244], loss=112.0643
	step [109/244], loss=106.9393
	step [110/244], loss=117.0572
	step [111/244], loss=113.3555
	step [112/244], loss=100.1950
	step [113/244], loss=93.8149
	step [114/244], loss=94.6339
	step [115/244], loss=112.1073
	step [116/244], loss=103.2792
	step [117/244], loss=113.6360
	step [118/244], loss=112.9877
	step [119/244], loss=92.8541
	step [120/244], loss=122.9361
	step [121/244], loss=125.5856
	step [122/244], loss=86.3867
	step [123/244], loss=111.0673
	step [124/244], loss=119.7121
	step [125/244], loss=109.0446
	step [126/244], loss=113.0212
	step [127/244], loss=106.0318
	step [128/244], loss=117.8825
	step [129/244], loss=96.3234
	step [130/244], loss=118.9188
	step [131/244], loss=102.8742
	step [132/244], loss=102.7009
	step [133/244], loss=114.4375
	step [134/244], loss=117.3864
	step [135/244], loss=108.9276
	step [136/244], loss=112.7307
	step [137/244], loss=97.9241
	step [138/244], loss=97.7505
	step [139/244], loss=96.3524
	step [140/244], loss=119.9560
	step [141/244], loss=97.1354
	step [142/244], loss=123.4369
	step [143/244], loss=94.2328
	step [144/244], loss=123.9348
	step [145/244], loss=111.1560
	step [146/244], loss=104.2429
	step [147/244], loss=133.9166
	step [148/244], loss=90.7366
	step [149/244], loss=127.4509
	step [150/244], loss=136.6891
	step [151/244], loss=112.2149
	step [152/244], loss=111.2752
	step [153/244], loss=106.8913
	step [154/244], loss=135.7101
	step [155/244], loss=103.2774
	step [156/244], loss=116.6798
	step [157/244], loss=104.1482
	step [158/244], loss=122.7972
	step [159/244], loss=105.2499
	step [160/244], loss=102.6994
	step [161/244], loss=116.7197
	step [162/244], loss=92.3627
	step [163/244], loss=102.9446
	step [164/244], loss=112.0983
	step [165/244], loss=108.9022
	step [166/244], loss=99.1357
	step [167/244], loss=98.0322
	step [168/244], loss=120.6898
	step [169/244], loss=104.7663
	step [170/244], loss=117.0935
	step [171/244], loss=90.3529
	step [172/244], loss=93.3517
	step [173/244], loss=92.2814
	step [174/244], loss=116.9783
	step [175/244], loss=126.5532
	step [176/244], loss=102.6712
	step [177/244], loss=119.7706
	step [178/244], loss=105.4374
	step [179/244], loss=116.2473
	step [180/244], loss=109.0714
	step [181/244], loss=112.3244
	step [182/244], loss=106.2620
	step [183/244], loss=104.5922
	step [184/244], loss=104.2815
	step [185/244], loss=106.1646
	step [186/244], loss=111.7711
	step [187/244], loss=114.7514
	step [188/244], loss=118.0845
	step [189/244], loss=115.7121
	step [190/244], loss=111.6384
	step [191/244], loss=119.8448
	step [192/244], loss=110.3337
	step [193/244], loss=110.7569
	step [194/244], loss=85.5824
	step [195/244], loss=98.4558
	step [196/244], loss=105.5948
	step [197/244], loss=104.9517
	step [198/244], loss=108.8404
	step [199/244], loss=113.8217
	step [200/244], loss=112.9521
	step [201/244], loss=93.4618
	step [202/244], loss=90.8578
	step [203/244], loss=84.7913
	step [204/244], loss=117.3313
	step [205/244], loss=122.7664
	step [206/244], loss=120.3768
	step [207/244], loss=122.5500
	step [208/244], loss=109.8693
	step [209/244], loss=99.7265
	step [210/244], loss=111.6490
	step [211/244], loss=99.6450
	step [212/244], loss=93.3359
	step [213/244], loss=104.0041
	step [214/244], loss=99.6545
	step [215/244], loss=88.4105
	step [216/244], loss=105.9909
	step [217/244], loss=128.6787
	step [218/244], loss=124.3035
	step [219/244], loss=115.8783
	step [220/244], loss=97.7620
	step [221/244], loss=118.4843
	step [222/244], loss=106.4911
	step [223/244], loss=115.6003
	step [224/244], loss=95.5466
	step [225/244], loss=110.3530
	step [226/244], loss=113.6710
	step [227/244], loss=107.0816
	step [228/244], loss=105.8724
	step [229/244], loss=98.0606
	step [230/244], loss=109.6205
	step [231/244], loss=109.3674
	step [232/244], loss=90.5483
	step [233/244], loss=92.8314
	step [234/244], loss=124.4341
	step [235/244], loss=99.8657
	step [236/244], loss=111.6217
	step [237/244], loss=101.0725
	step [238/244], loss=126.6101
	step [239/244], loss=89.3998
	step [240/244], loss=128.1933
	step [241/244], loss=98.6416
	step [242/244], loss=108.3427
	step [243/244], loss=126.2950
	step [244/244], loss=43.6609
	Evaluating
	loss=0.0472, precision=0.3714, recall=0.8998, f1=0.5258
Training epoch 10
	step [1/244], loss=112.5575
	step [2/244], loss=110.6724
	step [3/244], loss=106.0069
	step [4/244], loss=106.8125
	step [5/244], loss=118.8545
	step [6/244], loss=122.6556
	step [7/244], loss=127.9808
	step [8/244], loss=126.7384
	step [9/244], loss=106.5619
	step [10/244], loss=100.9345
	step [11/244], loss=88.8969
	step [12/244], loss=106.2644
	step [13/244], loss=116.7702
	step [14/244], loss=118.5686
	step [15/244], loss=109.6541
	step [16/244], loss=98.7613
	step [17/244], loss=93.0109
	step [18/244], loss=115.9867
	step [19/244], loss=105.0460
	step [20/244], loss=104.5549
	step [21/244], loss=129.9215
	step [22/244], loss=93.6155
	step [23/244], loss=114.9464
	step [24/244], loss=104.9678
	step [25/244], loss=97.1901
	step [26/244], loss=105.7592
	step [27/244], loss=103.4238
	step [28/244], loss=92.1085
	step [29/244], loss=98.0193
	step [30/244], loss=101.4965
	step [31/244], loss=119.4383
	step [32/244], loss=102.1261
	step [33/244], loss=100.2125
	step [34/244], loss=88.4244
	step [35/244], loss=95.0576
	step [36/244], loss=115.6620
	step [37/244], loss=115.4803
	step [38/244], loss=113.4789
	step [39/244], loss=97.1739
	step [40/244], loss=113.0396
	step [41/244], loss=118.4073
	step [42/244], loss=109.3828
	step [43/244], loss=118.1310
	step [44/244], loss=108.7919
	step [45/244], loss=117.4588
	step [46/244], loss=126.7300
	step [47/244], loss=112.3470
	step [48/244], loss=98.3113
	step [49/244], loss=115.4578
	step [50/244], loss=109.1697
	step [51/244], loss=95.0974
	step [52/244], loss=105.5607
	step [53/244], loss=109.6078
	step [54/244], loss=116.2126
	step [55/244], loss=104.5716
	step [56/244], loss=90.2836
	step [57/244], loss=101.4480
	step [58/244], loss=102.0004
	step [59/244], loss=97.3593
	step [60/244], loss=97.2704
	step [61/244], loss=115.7493
	step [62/244], loss=106.0311
	step [63/244], loss=106.2514
	step [64/244], loss=113.0753
	step [65/244], loss=119.3716
	step [66/244], loss=111.7070
	step [67/244], loss=116.9482
	step [68/244], loss=125.6312
	step [69/244], loss=104.7649
	step [70/244], loss=128.6328
	step [71/244], loss=124.3772
	step [72/244], loss=101.5025
	step [73/244], loss=112.1540
	step [74/244], loss=113.6884
	step [75/244], loss=125.9199
	step [76/244], loss=123.0382
	step [77/244], loss=103.6931
	step [78/244], loss=121.0008
	step [79/244], loss=114.9407
	step [80/244], loss=118.6102
	step [81/244], loss=99.8063
	step [82/244], loss=97.1029
	step [83/244], loss=109.4423
	step [84/244], loss=115.2224
	step [85/244], loss=115.8425
	step [86/244], loss=114.8054
	step [87/244], loss=89.2212
	step [88/244], loss=117.6248
	step [89/244], loss=117.7466
	step [90/244], loss=113.6223
	step [91/244], loss=98.4214
	step [92/244], loss=115.9619
	step [93/244], loss=119.8964
	step [94/244], loss=96.8776
	step [95/244], loss=85.5456
	step [96/244], loss=116.0131
	step [97/244], loss=110.6115
	step [98/244], loss=125.3594
	step [99/244], loss=98.5976
	step [100/244], loss=123.4281
	step [101/244], loss=109.9034
	step [102/244], loss=113.0991
	step [103/244], loss=129.3289
	step [104/244], loss=110.7172
	step [105/244], loss=117.1173
	step [106/244], loss=86.4095
	step [107/244], loss=115.3185
	step [108/244], loss=109.7680
	step [109/244], loss=106.4962
	step [110/244], loss=112.0887
	step [111/244], loss=117.6503
	step [112/244], loss=114.2534
	step [113/244], loss=127.2957
	step [114/244], loss=89.5458
	step [115/244], loss=86.8416
	step [116/244], loss=116.2437
	step [117/244], loss=110.1040
	step [118/244], loss=127.9705
	step [119/244], loss=104.1343
	step [120/244], loss=113.5604
	step [121/244], loss=93.6741
	step [122/244], loss=117.1593
	step [123/244], loss=111.5316
	step [124/244], loss=108.1334
	step [125/244], loss=104.6247
	step [126/244], loss=103.5189
	step [127/244], loss=103.4604
	step [128/244], loss=135.8443
	step [129/244], loss=122.1937
	step [130/244], loss=100.8739
	step [131/244], loss=99.7074
	step [132/244], loss=92.1821
	step [133/244], loss=117.5405
	step [134/244], loss=104.7943
	step [135/244], loss=93.4168
	step [136/244], loss=95.6146
	step [137/244], loss=133.6946
	step [138/244], loss=120.0785
	step [139/244], loss=112.1388
	step [140/244], loss=99.1519
	step [141/244], loss=129.9139
	step [142/244], loss=103.7465
	step [143/244], loss=104.8103
	step [144/244], loss=119.0239
	step [145/244], loss=107.4949
	step [146/244], loss=134.6551
	step [147/244], loss=103.9186
	step [148/244], loss=99.3235
	step [149/244], loss=99.5452
	step [150/244], loss=111.8267
	step [151/244], loss=101.8445
	step [152/244], loss=103.1765
	step [153/244], loss=99.7924
	step [154/244], loss=117.8685
	step [155/244], loss=115.9915
	step [156/244], loss=95.0697
	step [157/244], loss=93.8956
	step [158/244], loss=107.1867
	step [159/244], loss=98.3331
	step [160/244], loss=85.0164
	step [161/244], loss=93.6620
	step [162/244], loss=112.5693
	step [163/244], loss=96.9182
	step [164/244], loss=99.0285
	step [165/244], loss=110.1667
	step [166/244], loss=108.1598
	step [167/244], loss=96.8721
	step [168/244], loss=86.6006
	step [169/244], loss=102.8510
	step [170/244], loss=97.5951
	step [171/244], loss=126.1119
	step [172/244], loss=114.8683
	step [173/244], loss=94.8761
	step [174/244], loss=82.5999
	step [175/244], loss=115.1837
	step [176/244], loss=87.3241
	step [177/244], loss=105.4836
	step [178/244], loss=94.1809
	step [179/244], loss=128.2104
	step [180/244], loss=92.7538
	step [181/244], loss=98.5219
	step [182/244], loss=116.8142
	step [183/244], loss=117.4334
	step [184/244], loss=108.5827
	step [185/244], loss=107.2061
	step [186/244], loss=122.0778
	step [187/244], loss=97.1592
	step [188/244], loss=95.1243
	step [189/244], loss=107.1932
	step [190/244], loss=110.0137
	step [191/244], loss=107.8060
	step [192/244], loss=100.5421
	step [193/244], loss=124.1330
	step [194/244], loss=109.9345
	step [195/244], loss=90.6609
	step [196/244], loss=89.9014
	step [197/244], loss=104.4114
	step [198/244], loss=91.6114
	step [199/244], loss=109.1502
	step [200/244], loss=85.9901
	step [201/244], loss=112.1326
	step [202/244], loss=104.4246
	step [203/244], loss=123.4186
	step [204/244], loss=95.9529
	step [205/244], loss=120.5527
	step [206/244], loss=95.6311
	step [207/244], loss=84.2663
	step [208/244], loss=110.2734
	step [209/244], loss=131.3560
	step [210/244], loss=103.5042
	step [211/244], loss=98.6083
	step [212/244], loss=111.4940
	step [213/244], loss=109.0913
	step [214/244], loss=93.3628
	step [215/244], loss=94.5187
	step [216/244], loss=106.9536
	step [217/244], loss=119.6238
	step [218/244], loss=98.2686
	step [219/244], loss=111.5684
	step [220/244], loss=106.2884
	step [221/244], loss=127.0008
	step [222/244], loss=119.4637
	step [223/244], loss=126.4229
	step [224/244], loss=95.6127
	step [225/244], loss=96.1058
	step [226/244], loss=110.0335
	step [227/244], loss=85.5022
	step [228/244], loss=103.0154
	step [229/244], loss=115.2772
	step [230/244], loss=107.9513
	step [231/244], loss=112.9694
	step [232/244], loss=106.4062
	step [233/244], loss=100.7209
	step [234/244], loss=104.2014
	step [235/244], loss=102.4612
	step [236/244], loss=115.0762
	step [237/244], loss=93.1522
	step [238/244], loss=106.4762
	step [239/244], loss=107.4152
	step [240/244], loss=108.1269
	step [241/244], loss=103.5704
	step [242/244], loss=110.1298
	step [243/244], loss=97.2246
	step [244/244], loss=48.0646
	Evaluating
	loss=0.0388, precision=0.3309, recall=0.9197, f1=0.4867
Training epoch 11
	step [1/244], loss=108.3759
	step [2/244], loss=101.9231
	step [3/244], loss=118.4015
	step [4/244], loss=105.1203
	step [5/244], loss=96.8162
	step [6/244], loss=98.3382
	step [7/244], loss=142.8261
	step [8/244], loss=108.3889
	step [9/244], loss=117.7115
	step [10/244], loss=112.2068
	step [11/244], loss=95.6320
	step [12/244], loss=111.6857
	step [13/244], loss=101.0983
	step [14/244], loss=114.5998
	step [15/244], loss=103.9060
	step [16/244], loss=96.7250
	step [17/244], loss=99.8376
	step [18/244], loss=92.7057
	step [19/244], loss=120.4664
	step [20/244], loss=114.1131
	step [21/244], loss=85.8793
	step [22/244], loss=108.6756
	step [23/244], loss=99.3179
	step [24/244], loss=113.7142
	step [25/244], loss=101.0085
	step [26/244], loss=120.0425
	step [27/244], loss=115.8517
	step [28/244], loss=92.5264
	step [29/244], loss=98.7100
	step [30/244], loss=98.5184
	step [31/244], loss=94.3462
	step [32/244], loss=111.9224
	step [33/244], loss=110.4006
	step [34/244], loss=132.2657
	step [35/244], loss=117.4773
	step [36/244], loss=100.5405
	step [37/244], loss=96.1277
	step [38/244], loss=101.4553
	step [39/244], loss=104.2934
	step [40/244], loss=103.1786
	step [41/244], loss=86.9794
	step [42/244], loss=103.8373
	step [43/244], loss=104.2596
	step [44/244], loss=119.1983
	step [45/244], loss=114.5163
	step [46/244], loss=89.8989
	step [47/244], loss=112.7716
	step [48/244], loss=117.3318
	step [49/244], loss=116.3033
	step [50/244], loss=79.5250
	step [51/244], loss=106.0757
	step [52/244], loss=117.4990
	step [53/244], loss=93.1377
	step [54/244], loss=104.5650
	step [55/244], loss=101.9518
	step [56/244], loss=117.3567
	step [57/244], loss=125.8038
	step [58/244], loss=102.4551
	step [59/244], loss=106.9155
	step [60/244], loss=105.2852
	step [61/244], loss=117.7476
	step [62/244], loss=91.2859
	step [63/244], loss=108.8974
	step [64/244], loss=93.2124
	step [65/244], loss=80.0019
	step [66/244], loss=97.3047
	step [67/244], loss=116.6138
	step [68/244], loss=116.1369
	step [69/244], loss=102.7110
	step [70/244], loss=85.0238
	step [71/244], loss=93.2746
	step [72/244], loss=131.0408
	step [73/244], loss=101.4714
	step [74/244], loss=104.9074
	step [75/244], loss=109.5141
	step [76/244], loss=84.0320
	step [77/244], loss=129.2948
	step [78/244], loss=99.9716
	step [79/244], loss=87.2529
	step [80/244], loss=108.2899
	step [81/244], loss=94.8841
	step [82/244], loss=96.7762
	step [83/244], loss=122.8388
	step [84/244], loss=99.7994
	step [85/244], loss=120.1616
	step [86/244], loss=113.5171
	step [87/244], loss=122.0383
	step [88/244], loss=106.2592
	step [89/244], loss=102.5450
	step [90/244], loss=92.8306
	step [91/244], loss=112.2077
	step [92/244], loss=128.2678
	step [93/244], loss=102.0309
	step [94/244], loss=99.5808
	step [95/244], loss=95.7900
	step [96/244], loss=117.4369
	step [97/244], loss=93.0003
	step [98/244], loss=95.3317
	step [99/244], loss=97.2389
	step [100/244], loss=100.9138
	step [101/244], loss=102.8286
	step [102/244], loss=87.6399
	step [103/244], loss=106.1988
	step [104/244], loss=105.7616
	step [105/244], loss=116.1893
	step [106/244], loss=103.0930
	step [107/244], loss=110.3005
	step [108/244], loss=88.4363
	step [109/244], loss=115.0892
	step [110/244], loss=94.8925
	step [111/244], loss=95.0258
	step [112/244], loss=114.2151
	step [113/244], loss=107.6065
	step [114/244], loss=129.3038
	step [115/244], loss=130.7828
	step [116/244], loss=89.7490
	step [117/244], loss=126.1500
	step [118/244], loss=92.7843
	step [119/244], loss=112.7723
	step [120/244], loss=120.5570
	step [121/244], loss=91.7351
	step [122/244], loss=114.4101
	step [123/244], loss=101.6558
	step [124/244], loss=133.1830
	step [125/244], loss=106.3578
	step [126/244], loss=106.3836
	step [127/244], loss=99.0886
	step [128/244], loss=101.1005
	step [129/244], loss=113.9263
	step [130/244], loss=110.0044
	step [131/244], loss=108.1743
	step [132/244], loss=109.5784
	step [133/244], loss=94.0801
	step [134/244], loss=90.5244
	step [135/244], loss=106.8008
	step [136/244], loss=93.6888
	step [137/244], loss=84.1967
	step [138/244], loss=113.9155
	step [139/244], loss=101.3165
	step [140/244], loss=93.6800
	step [141/244], loss=100.5385
	step [142/244], loss=103.0883
	step [143/244], loss=117.7653
	step [144/244], loss=101.3545
	step [145/244], loss=111.4626
	step [146/244], loss=129.7676
	step [147/244], loss=126.9524
	step [148/244], loss=114.5277
	step [149/244], loss=109.6622
	step [150/244], loss=96.3492
	step [151/244], loss=92.8986
	step [152/244], loss=120.0619
	step [153/244], loss=123.9950
	step [154/244], loss=107.5320
	step [155/244], loss=94.0104
	step [156/244], loss=121.5105
	step [157/244], loss=102.7567
	step [158/244], loss=97.1171
	step [159/244], loss=100.1464
	step [160/244], loss=105.3174
	step [161/244], loss=96.6232
	step [162/244], loss=107.0208
	step [163/244], loss=100.9049
	step [164/244], loss=89.5600
	step [165/244], loss=88.8681
	step [166/244], loss=94.4733
	step [167/244], loss=98.6956
	step [168/244], loss=118.8940
	step [169/244], loss=110.9951
	step [170/244], loss=121.9455
	step [171/244], loss=113.0387
	step [172/244], loss=110.0144
	step [173/244], loss=120.3393
	step [174/244], loss=90.2051
	step [175/244], loss=103.4277
	step [176/244], loss=110.3335
	step [177/244], loss=102.8959
	step [178/244], loss=109.7356
	step [179/244], loss=119.1910
	step [180/244], loss=96.1150
	step [181/244], loss=110.2679
	step [182/244], loss=99.0335
	step [183/244], loss=114.9667
	step [184/244], loss=111.8451
	step [185/244], loss=99.0712
	step [186/244], loss=130.2780
	step [187/244], loss=124.5232
	step [188/244], loss=103.9492
	step [189/244], loss=87.5560
	step [190/244], loss=98.3244
	step [191/244], loss=96.6548
	step [192/244], loss=118.2776
	step [193/244], loss=111.9898
	step [194/244], loss=112.7997
	step [195/244], loss=92.2378
	step [196/244], loss=103.9795
	step [197/244], loss=120.7029
	step [198/244], loss=107.7626
	step [199/244], loss=95.3364
	step [200/244], loss=99.8433
	step [201/244], loss=99.6091
	step [202/244], loss=114.4980
	step [203/244], loss=79.4866
	step [204/244], loss=100.0566
	step [205/244], loss=92.1719
	step [206/244], loss=95.8114
	step [207/244], loss=106.8155
	step [208/244], loss=103.1347
	step [209/244], loss=111.2962
	step [210/244], loss=134.4366
	step [211/244], loss=106.0017
	step [212/244], loss=100.3983
	step [213/244], loss=110.0922
	step [214/244], loss=103.5178
	step [215/244], loss=116.1621
	step [216/244], loss=94.6480
	step [217/244], loss=115.0885
	step [218/244], loss=92.3196
	step [219/244], loss=101.6250
	step [220/244], loss=113.8519
	step [221/244], loss=120.3348
	step [222/244], loss=95.4843
	step [223/244], loss=102.6619
	step [224/244], loss=109.0820
	step [225/244], loss=101.5141
	step [226/244], loss=107.2238
	step [227/244], loss=109.7056
	step [228/244], loss=114.1601
	step [229/244], loss=98.5571
	step [230/244], loss=116.6111
	step [231/244], loss=114.0474
	step [232/244], loss=98.2049
	step [233/244], loss=106.1696
	step [234/244], loss=90.0653
	step [235/244], loss=107.2795
	step [236/244], loss=101.3132
	step [237/244], loss=103.4006
	step [238/244], loss=99.8789
	step [239/244], loss=120.0878
	step [240/244], loss=123.8312
	step [241/244], loss=92.4136
	step [242/244], loss=113.2506
	step [243/244], loss=102.7209
	step [244/244], loss=38.3595
	Evaluating
	loss=0.0300, precision=0.4884, recall=0.8665, f1=0.6247
Training epoch 12
	step [1/244], loss=94.0476
	step [2/244], loss=105.5142
	step [3/244], loss=112.0045
	step [4/244], loss=107.6466
	step [5/244], loss=101.6033
	step [6/244], loss=94.1287
	step [7/244], loss=91.5781
	step [8/244], loss=88.8920
	step [9/244], loss=107.8543
	step [10/244], loss=98.4749
	step [11/244], loss=98.7540
	step [12/244], loss=98.5846
	step [13/244], loss=101.2063
	step [14/244], loss=106.1705
	step [15/244], loss=108.5275
	step [16/244], loss=107.3275
	step [17/244], loss=106.2087
	step [18/244], loss=103.4257
	step [19/244], loss=98.6079
	step [20/244], loss=119.9194
	step [21/244], loss=126.5811
	step [22/244], loss=126.3157
	step [23/244], loss=110.8322
	step [24/244], loss=104.8594
	step [25/244], loss=110.5215
	step [26/244], loss=79.3780
	step [27/244], loss=130.2009
	step [28/244], loss=95.9354
	step [29/244], loss=116.5025
	step [30/244], loss=107.9719
	step [31/244], loss=101.4244
	step [32/244], loss=120.1409
	step [33/244], loss=101.2513
	step [34/244], loss=98.9517
	step [35/244], loss=105.9037
	step [36/244], loss=85.7961
	step [37/244], loss=93.8016
	step [38/244], loss=94.3375
	step [39/244], loss=102.4512
	step [40/244], loss=106.8562
	step [41/244], loss=92.8017
	step [42/244], loss=85.6288
	step [43/244], loss=104.9678
	step [44/244], loss=109.8563
	step [45/244], loss=96.2410
	step [46/244], loss=94.1630
	step [47/244], loss=115.5620
	step [48/244], loss=92.3376
	step [49/244], loss=94.8195
	step [50/244], loss=105.6476
	step [51/244], loss=120.4733
	step [52/244], loss=98.6097
	step [53/244], loss=98.5011
	step [54/244], loss=87.0396
	step [55/244], loss=97.6043
	step [56/244], loss=100.5954
	step [57/244], loss=77.5794
	step [58/244], loss=104.0113
	step [59/244], loss=104.5775
	step [60/244], loss=126.3004
	step [61/244], loss=83.3010
	step [62/244], loss=87.8807
	step [63/244], loss=121.2881
	step [64/244], loss=111.8850
	step [65/244], loss=108.1249
	step [66/244], loss=93.0247
	step [67/244], loss=111.4615
	step [68/244], loss=116.6796
	step [69/244], loss=96.4033
	step [70/244], loss=90.6818
	step [71/244], loss=121.9872
	step [72/244], loss=100.5723
	step [73/244], loss=90.8095
	step [74/244], loss=125.3673
	step [75/244], loss=106.3096
	step [76/244], loss=109.3363
	step [77/244], loss=93.1589
	step [78/244], loss=112.2313
	step [79/244], loss=119.0443
	step [80/244], loss=125.9551
	step [81/244], loss=101.0425
	step [82/244], loss=107.0035
	step [83/244], loss=94.4346
	step [84/244], loss=111.9138
	step [85/244], loss=101.2129
	step [86/244], loss=120.8769
	step [87/244], loss=101.2638
	step [88/244], loss=105.2766
	step [89/244], loss=116.2607
	step [90/244], loss=105.9861
	step [91/244], loss=100.5584
	step [92/244], loss=97.8344
	step [93/244], loss=103.0096
	step [94/244], loss=114.8347
	step [95/244], loss=117.5524
	step [96/244], loss=112.9786
	step [97/244], loss=109.5898
	step [98/244], loss=82.6160
	step [99/244], loss=110.4874
	step [100/244], loss=108.7430
	step [101/244], loss=101.1871
	step [102/244], loss=98.2979
	step [103/244], loss=114.6917
	step [104/244], loss=87.5588
	step [105/244], loss=116.0206
	step [106/244], loss=121.7669
	step [107/244], loss=116.5229
	step [108/244], loss=96.9124
	step [109/244], loss=113.3601
	step [110/244], loss=99.3974
	step [111/244], loss=106.9783
	step [112/244], loss=102.1095
	step [113/244], loss=119.2971
	step [114/244], loss=104.9487
	step [115/244], loss=98.7309
	step [116/244], loss=120.3745
	step [117/244], loss=109.4696
	step [118/244], loss=125.4435
	step [119/244], loss=101.4907
	step [120/244], loss=113.3053
	step [121/244], loss=98.9769
	step [122/244], loss=102.2388
	step [123/244], loss=96.3618
	step [124/244], loss=98.3291
	step [125/244], loss=107.1970
	step [126/244], loss=108.1246
	step [127/244], loss=89.3530
	step [128/244], loss=112.2271
	step [129/244], loss=102.2206
	step [130/244], loss=86.6499
	step [131/244], loss=111.0331
	step [132/244], loss=106.0266
	step [133/244], loss=110.3969
	step [134/244], loss=110.5990
	step [135/244], loss=93.7414
	step [136/244], loss=110.2096
	step [137/244], loss=90.3077
	step [138/244], loss=110.9022
	step [139/244], loss=113.4011
	step [140/244], loss=96.7558
	step [141/244], loss=97.6528
	step [142/244], loss=104.1307
	step [143/244], loss=94.3387
	step [144/244], loss=107.7565
	step [145/244], loss=100.0725
	step [146/244], loss=88.6387
	step [147/244], loss=111.3573
	step [148/244], loss=89.9686
	step [149/244], loss=82.4730
	step [150/244], loss=115.1645
	step [151/244], loss=108.3588
	step [152/244], loss=98.6553
	step [153/244], loss=88.7229
	step [154/244], loss=95.7784
	step [155/244], loss=103.7036
	step [156/244], loss=103.0566
	step [157/244], loss=97.6892
	step [158/244], loss=113.4262
	step [159/244], loss=101.5859
	step [160/244], loss=102.7833
	step [161/244], loss=85.2382
	step [162/244], loss=118.5902
	step [163/244], loss=116.2809
	step [164/244], loss=104.7573
	step [165/244], loss=91.1723
	step [166/244], loss=123.5838
	step [167/244], loss=100.3030
	step [168/244], loss=114.6287
	step [169/244], loss=103.5134
	step [170/244], loss=114.2567
	step [171/244], loss=100.9629
	step [172/244], loss=98.9964
	step [173/244], loss=89.5712
	step [174/244], loss=105.9189
	step [175/244], loss=107.4350
	step [176/244], loss=103.0878
	step [177/244], loss=93.9555
	step [178/244], loss=120.5763
	step [179/244], loss=108.8156
	step [180/244], loss=104.6272
	step [181/244], loss=97.4408
	step [182/244], loss=111.3054
	step [183/244], loss=110.7153
	step [184/244], loss=106.5786
	step [185/244], loss=109.9887
	step [186/244], loss=102.0819
	step [187/244], loss=94.4514
	step [188/244], loss=110.5226
	step [189/244], loss=122.4503
	step [190/244], loss=103.1478
	step [191/244], loss=99.6998
	step [192/244], loss=105.6949
	step [193/244], loss=96.2063
	step [194/244], loss=113.3711
	step [195/244], loss=105.0708
	step [196/244], loss=121.0283
	step [197/244], loss=108.6068
	step [198/244], loss=108.9247
	step [199/244], loss=96.1892
	step [200/244], loss=123.5290
	step [201/244], loss=106.8633
	step [202/244], loss=104.5811
	step [203/244], loss=116.2503
	step [204/244], loss=96.8050
	step [205/244], loss=113.9710
	step [206/244], loss=91.7052
	step [207/244], loss=95.8433
	step [208/244], loss=91.8279
	step [209/244], loss=99.3010
	step [210/244], loss=105.8057
	step [211/244], loss=95.9227
	step [212/244], loss=114.4335
	step [213/244], loss=114.8595
	step [214/244], loss=90.7209
	step [215/244], loss=95.3937
	step [216/244], loss=95.0079
	step [217/244], loss=104.3372
	step [218/244], loss=99.9857
	step [219/244], loss=95.3461
	step [220/244], loss=94.7426
	step [221/244], loss=102.8694
	step [222/244], loss=97.8942
	step [223/244], loss=114.6994
	step [224/244], loss=119.1411
	step [225/244], loss=105.9254
	step [226/244], loss=109.8035
	step [227/244], loss=94.9570
	step [228/244], loss=101.0824
	step [229/244], loss=100.1896
	step [230/244], loss=107.9493
	step [231/244], loss=88.9056
	step [232/244], loss=92.0361
	step [233/244], loss=110.7869
	step [234/244], loss=88.2934
	step [235/244], loss=102.0842
	step [236/244], loss=99.3167
	step [237/244], loss=98.3648
	step [238/244], loss=124.4935
	step [239/244], loss=105.4667
	step [240/244], loss=100.1375
	step [241/244], loss=113.1656
	step [242/244], loss=120.0292
	step [243/244], loss=98.6867
	step [244/244], loss=46.6523
	Evaluating
	loss=0.0291, precision=0.3873, recall=0.8996, f1=0.5414
Training epoch 13
	step [1/244], loss=89.4254
	step [2/244], loss=131.2165
	step [3/244], loss=110.7894
	step [4/244], loss=90.0213
	step [5/244], loss=98.3524
	step [6/244], loss=90.0451
	step [7/244], loss=79.5783
	step [8/244], loss=102.8097
	step [9/244], loss=93.3002
	step [10/244], loss=97.7934
	step [11/244], loss=107.8487
	step [12/244], loss=106.5681
	step [13/244], loss=89.5544
	step [14/244], loss=106.7354
	step [15/244], loss=108.1429
	step [16/244], loss=91.5320
	step [17/244], loss=109.5480
	step [18/244], loss=111.3876
	step [19/244], loss=101.3065
	step [20/244], loss=103.1142
	step [21/244], loss=85.6109
	step [22/244], loss=92.6655
	step [23/244], loss=82.4121
	step [24/244], loss=131.9592
	step [25/244], loss=111.0735
	step [26/244], loss=103.3319
	step [27/244], loss=92.6987
	step [28/244], loss=86.3481
	step [29/244], loss=111.1893
	step [30/244], loss=104.9041
	step [31/244], loss=87.8972
	step [32/244], loss=107.8231
	step [33/244], loss=106.3277
	step [34/244], loss=120.2375
	step [35/244], loss=108.1548
	step [36/244], loss=87.7863
	step [37/244], loss=123.1794
	step [38/244], loss=95.5744
	step [39/244], loss=108.6040
	step [40/244], loss=95.9275
	step [41/244], loss=107.0817
	step [42/244], loss=85.7742
	step [43/244], loss=100.9845
	step [44/244], loss=94.5394
	step [45/244], loss=117.3184
	step [46/244], loss=98.9066
	step [47/244], loss=85.6178
	step [48/244], loss=101.6179
	step [49/244], loss=101.5329
	step [50/244], loss=83.1929
	step [51/244], loss=115.6694
	step [52/244], loss=103.2316
	step [53/244], loss=107.7774
	step [54/244], loss=98.1338
	step [55/244], loss=121.7419
	step [56/244], loss=91.6529
	step [57/244], loss=76.4819
	step [58/244], loss=100.7711
	step [59/244], loss=102.9323
	step [60/244], loss=103.9034
	step [61/244], loss=111.8930
	step [62/244], loss=99.4768
	step [63/244], loss=89.8533
	step [64/244], loss=89.7739
	step [65/244], loss=95.1913
	step [66/244], loss=116.5294
	step [67/244], loss=111.8374
	step [68/244], loss=102.6533
	step [69/244], loss=118.8563
	step [70/244], loss=99.5650
	step [71/244], loss=105.7291
	step [72/244], loss=114.1597
	step [73/244], loss=88.3905
	step [74/244], loss=104.2615
	step [75/244], loss=88.2157
	step [76/244], loss=100.1374
	step [77/244], loss=108.9758
	step [78/244], loss=103.0617
	step [79/244], loss=113.5055
	step [80/244], loss=110.7425
	step [81/244], loss=104.7496
	step [82/244], loss=103.2114
	step [83/244], loss=106.3509
	step [84/244], loss=94.3914
	step [85/244], loss=107.4334
	step [86/244], loss=135.3314
	step [87/244], loss=110.3765
	step [88/244], loss=105.5819
	step [89/244], loss=103.8039
	step [90/244], loss=99.8039
	step [91/244], loss=136.9106
	step [92/244], loss=72.7049
	step [93/244], loss=104.6909
	step [94/244], loss=85.4550
	step [95/244], loss=110.8847
	step [96/244], loss=116.4206
	step [97/244], loss=105.5334
	step [98/244], loss=103.6102
	step [99/244], loss=89.2201
	step [100/244], loss=76.2360
	step [101/244], loss=115.7723
	step [102/244], loss=95.4405
	step [103/244], loss=115.4462
	step [104/244], loss=103.5614
	step [105/244], loss=101.9275
	step [106/244], loss=124.6865
	step [107/244], loss=89.8300
	step [108/244], loss=98.4110
	step [109/244], loss=103.4806
	step [110/244], loss=76.2539
	step [111/244], loss=89.2313
	step [112/244], loss=74.6805
	step [113/244], loss=99.6535
	step [114/244], loss=85.3421
	step [115/244], loss=88.8878
	step [116/244], loss=118.3538
	step [117/244], loss=89.2299
	step [118/244], loss=80.0959
	step [119/244], loss=83.1316
	step [120/244], loss=100.9039
	step [121/244], loss=97.0793
	step [122/244], loss=91.1719
	step [123/244], loss=84.5979
	step [124/244], loss=127.6722
	step [125/244], loss=108.4813
	step [126/244], loss=94.1603
	step [127/244], loss=91.9709
	step [128/244], loss=94.2836
	step [129/244], loss=109.8032
	step [130/244], loss=104.2912
	step [131/244], loss=135.1954
	step [132/244], loss=111.1196
	step [133/244], loss=112.4116
	step [134/244], loss=92.2901
	step [135/244], loss=113.7878
	step [136/244], loss=99.9158
	step [137/244], loss=137.7486
	step [138/244], loss=89.5342
	step [139/244], loss=112.0224
	step [140/244], loss=94.3563
	step [141/244], loss=94.4139
	step [142/244], loss=101.8971
	step [143/244], loss=96.2218
	step [144/244], loss=95.3922
	step [145/244], loss=90.9981
	step [146/244], loss=105.9706
	step [147/244], loss=108.3860
	step [148/244], loss=111.1140
	step [149/244], loss=108.8621
	step [150/244], loss=100.0168
	step [151/244], loss=110.5150
	step [152/244], loss=104.2010
	step [153/244], loss=93.6615
	step [154/244], loss=121.0499
	step [155/244], loss=85.1233
	step [156/244], loss=100.4159
	step [157/244], loss=95.4392
	step [158/244], loss=88.8541
	step [159/244], loss=122.6477
	step [160/244], loss=116.8523
	step [161/244], loss=103.9022
	step [162/244], loss=109.0906
	step [163/244], loss=91.9223
	step [164/244], loss=100.0468
	step [165/244], loss=111.5135
	step [166/244], loss=112.7701
	step [167/244], loss=109.0924
	step [168/244], loss=97.9504
	step [169/244], loss=116.9965
	step [170/244], loss=109.1324
	step [171/244], loss=101.2958
	step [172/244], loss=118.4346
	step [173/244], loss=101.3263
	step [174/244], loss=81.7124
	step [175/244], loss=84.6797
	step [176/244], loss=91.1168
	step [177/244], loss=98.7414
	step [178/244], loss=91.3051
	step [179/244], loss=110.3153
	step [180/244], loss=107.1557
	step [181/244], loss=94.2837
	step [182/244], loss=117.9403
	step [183/244], loss=117.6495
	step [184/244], loss=109.4426
	step [185/244], loss=99.4460
	step [186/244], loss=97.6881
	step [187/244], loss=96.1600
	step [188/244], loss=88.7455
	step [189/244], loss=106.0430
	step [190/244], loss=108.6054
	step [191/244], loss=112.6489
	step [192/244], loss=84.4594
	step [193/244], loss=117.9734
	step [194/244], loss=90.9985
	step [195/244], loss=111.1417
	step [196/244], loss=80.8339
	step [197/244], loss=105.2700
	step [198/244], loss=100.3192
	step [199/244], loss=110.2056
	step [200/244], loss=74.7176
	step [201/244], loss=94.6665
	step [202/244], loss=121.6336
	step [203/244], loss=112.4385
	step [204/244], loss=109.4646
	step [205/244], loss=118.6184
	step [206/244], loss=76.6969
	step [207/244], loss=99.4022
	step [208/244], loss=103.2110
	step [209/244], loss=85.8382
	step [210/244], loss=121.7590
	step [211/244], loss=102.6817
	step [212/244], loss=95.7132
	step [213/244], loss=103.9068
	step [214/244], loss=109.2122
	step [215/244], loss=103.4649
	step [216/244], loss=97.5255
	step [217/244], loss=92.4988
	step [218/244], loss=109.3287
	step [219/244], loss=97.1502
	step [220/244], loss=93.5390
	step [221/244], loss=98.7008
	step [222/244], loss=88.1456
	step [223/244], loss=122.6464
	step [224/244], loss=83.9347
	step [225/244], loss=84.6727
	step [226/244], loss=105.7682
	step [227/244], loss=111.8102
	step [228/244], loss=93.7011
	step [229/244], loss=103.4715
	step [230/244], loss=108.3225
	step [231/244], loss=118.3116
	step [232/244], loss=105.1895
	step [233/244], loss=118.8359
	step [234/244], loss=101.9277
	step [235/244], loss=114.6969
	step [236/244], loss=106.7661
	step [237/244], loss=102.8462
	step [238/244], loss=89.9166
	step [239/244], loss=117.2521
	step [240/244], loss=112.5643
	step [241/244], loss=126.4385
	step [242/244], loss=109.0916
	step [243/244], loss=107.0861
	step [244/244], loss=47.7529
	Evaluating
	loss=0.0235, precision=0.4430, recall=0.8878, f1=0.5911
Training epoch 14
	step [1/244], loss=112.7487
	step [2/244], loss=100.2987
	step [3/244], loss=87.2070
	step [4/244], loss=98.1927
	step [5/244], loss=87.0868
	step [6/244], loss=89.7008
	step [7/244], loss=109.2885
	step [8/244], loss=113.5678
	step [9/244], loss=112.2350
	step [10/244], loss=97.1960
	step [11/244], loss=103.0012
	step [12/244], loss=102.9422
	step [13/244], loss=95.9650
	step [14/244], loss=101.0205
	step [15/244], loss=109.3037
	step [16/244], loss=95.8444
	step [17/244], loss=102.5716
	step [18/244], loss=108.0903
	step [19/244], loss=97.6486
	step [20/244], loss=104.5558
	step [21/244], loss=107.8281
	step [22/244], loss=94.1248
	step [23/244], loss=110.3139
	step [24/244], loss=89.0970
	step [25/244], loss=97.8530
	step [26/244], loss=96.1133
	step [27/244], loss=88.1928
	step [28/244], loss=92.3874
	step [29/244], loss=104.0035
	step [30/244], loss=106.1111
	step [31/244], loss=108.0506
	step [32/244], loss=92.8566
	step [33/244], loss=111.7982
	step [34/244], loss=104.0482
	step [35/244], loss=111.3189
	step [36/244], loss=110.1711
	step [37/244], loss=94.4154
	step [38/244], loss=100.8370
	step [39/244], loss=108.4977
	step [40/244], loss=102.7824
	step [41/244], loss=103.8089
	step [42/244], loss=109.9493
	step [43/244], loss=94.5587
	step [44/244], loss=111.8055
	step [45/244], loss=103.0495
	step [46/244], loss=106.4812
	step [47/244], loss=101.1567
	step [48/244], loss=88.3195
	step [49/244], loss=94.8272
	step [50/244], loss=108.8497
	step [51/244], loss=84.2991
	step [52/244], loss=112.8834
	step [53/244], loss=115.7218
	step [54/244], loss=94.5866
	step [55/244], loss=117.8261
	step [56/244], loss=103.9077
	step [57/244], loss=115.0195
	step [58/244], loss=100.4583
	step [59/244], loss=108.1272
	step [60/244], loss=104.3234
	step [61/244], loss=104.0189
	step [62/244], loss=94.8951
	step [63/244], loss=92.8418
	step [64/244], loss=96.5765
	step [65/244], loss=90.9838
	step [66/244], loss=114.2837
	step [67/244], loss=104.5877
	step [68/244], loss=101.8193
	step [69/244], loss=109.6761
	step [70/244], loss=89.2835
	step [71/244], loss=117.4067
	step [72/244], loss=78.9645
	step [73/244], loss=86.4131
	step [74/244], loss=110.7175
	step [75/244], loss=98.3060
	step [76/244], loss=114.3905
	step [77/244], loss=95.1670
	step [78/244], loss=100.0760
	step [79/244], loss=107.6792
	step [80/244], loss=94.7735
	step [81/244], loss=87.6224
	step [82/244], loss=79.7992
	step [83/244], loss=107.8108
	step [84/244], loss=89.7843
	step [85/244], loss=105.4810
	step [86/244], loss=115.7545
	step [87/244], loss=87.0022
	step [88/244], loss=110.1137
	step [89/244], loss=106.0220
	step [90/244], loss=93.2386
	step [91/244], loss=104.7689
	step [92/244], loss=97.0003
	step [93/244], loss=104.5306
	step [94/244], loss=129.7265
	step [95/244], loss=106.5060
	step [96/244], loss=94.5411
	step [97/244], loss=91.4156
	step [98/244], loss=100.1358
	step [99/244], loss=94.0175
	step [100/244], loss=104.7247
	step [101/244], loss=101.0155
	step [102/244], loss=133.1784
	step [103/244], loss=85.1933
	step [104/244], loss=92.8475
	step [105/244], loss=86.9701
	step [106/244], loss=106.6385
	step [107/244], loss=100.8438
	step [108/244], loss=100.0595
	step [109/244], loss=117.0719
	step [110/244], loss=90.8657
	step [111/244], loss=107.4122
	step [112/244], loss=108.7622
	step [113/244], loss=93.0099
	step [114/244], loss=102.0399
	step [115/244], loss=96.6210
	step [116/244], loss=100.1161
	step [117/244], loss=109.6665
	step [118/244], loss=89.4908
	step [119/244], loss=108.5781
	step [120/244], loss=123.7410
	step [121/244], loss=112.4770
	step [122/244], loss=108.2873
	step [123/244], loss=98.6785
	step [124/244], loss=118.5343
	step [125/244], loss=93.1867
	step [126/244], loss=109.9269
	step [127/244], loss=119.2028
	step [128/244], loss=91.9332
	step [129/244], loss=112.4607
	step [130/244], loss=95.4944
	step [131/244], loss=109.2907
	step [132/244], loss=107.6211
	step [133/244], loss=117.4298
	step [134/244], loss=100.6595
	step [135/244], loss=92.8361
	step [136/244], loss=97.2748
	step [137/244], loss=109.6636
	step [138/244], loss=92.4911
	step [139/244], loss=94.9056
	step [140/244], loss=106.9375
	step [141/244], loss=102.5818
	step [142/244], loss=105.0323
	step [143/244], loss=104.2004
	step [144/244], loss=95.7562
	step [145/244], loss=85.2205
	step [146/244], loss=91.9518
	step [147/244], loss=107.7008
	step [148/244], loss=93.3324
	step [149/244], loss=82.3589
	step [150/244], loss=105.1790
	step [151/244], loss=110.5090
	step [152/244], loss=99.5160
	step [153/244], loss=116.0883
	step [154/244], loss=104.6405
	step [155/244], loss=114.5782
	step [156/244], loss=95.3456
	step [157/244], loss=106.1947
	step [158/244], loss=128.8339
	step [159/244], loss=122.0270
	step [160/244], loss=94.3915
	step [161/244], loss=91.0400
	step [162/244], loss=110.4923
	step [163/244], loss=102.4623
	step [164/244], loss=106.0070
	step [165/244], loss=97.8073
	step [166/244], loss=88.7919
	step [167/244], loss=110.8849
	step [168/244], loss=107.8811
	step [169/244], loss=90.5111
	step [170/244], loss=96.7435
	step [171/244], loss=83.0515
	step [172/244], loss=84.7391
	step [173/244], loss=142.0169
	step [174/244], loss=89.5011
	step [175/244], loss=101.7074
	step [176/244], loss=78.1340
	step [177/244], loss=114.2298
	step [178/244], loss=110.6224
	step [179/244], loss=117.7393
	step [180/244], loss=94.1817
	step [181/244], loss=75.4030
	step [182/244], loss=100.4948
	step [183/244], loss=114.5045
	step [184/244], loss=101.3380
	step [185/244], loss=106.1675
	step [186/244], loss=122.9258
	step [187/244], loss=96.6610
	step [188/244], loss=105.9577
	step [189/244], loss=95.6696
	step [190/244], loss=86.2726
	step [191/244], loss=86.4677
	step [192/244], loss=102.0266
	step [193/244], loss=85.6843
	step [194/244], loss=98.6992
	step [195/244], loss=85.3079
	step [196/244], loss=86.6980
	step [197/244], loss=91.4245
	step [198/244], loss=103.9946
	step [199/244], loss=88.8893
	step [200/244], loss=96.1315
	step [201/244], loss=92.0793
	step [202/244], loss=96.0298
	step [203/244], loss=108.3446
	step [204/244], loss=105.9329
	step [205/244], loss=90.0499
	step [206/244], loss=105.1495
	step [207/244], loss=114.8456
	step [208/244], loss=101.4772
	step [209/244], loss=95.6498
	step [210/244], loss=100.6029
	step [211/244], loss=87.7700
	step [212/244], loss=94.5041
	step [213/244], loss=95.6908
	step [214/244], loss=100.6180
	step [215/244], loss=102.6539
	step [216/244], loss=83.3988
	step [217/244], loss=94.5074
	step [218/244], loss=102.7794
	step [219/244], loss=99.1809
	step [220/244], loss=88.7711
	step [221/244], loss=102.3634
	step [222/244], loss=112.2935
	step [223/244], loss=104.1048
	step [224/244], loss=129.0831
	step [225/244], loss=96.0254
	step [226/244], loss=97.0675
	step [227/244], loss=113.2368
	step [228/244], loss=89.4758
	step [229/244], loss=92.9892
	step [230/244], loss=93.6251
	step [231/244], loss=106.8254
	step [232/244], loss=96.8681
	step [233/244], loss=105.9774
	step [234/244], loss=93.8781
	step [235/244], loss=87.2364
	step [236/244], loss=103.0241
	step [237/244], loss=95.8631
	step [238/244], loss=110.7551
	step [239/244], loss=112.6768
	step [240/244], loss=93.2341
	step [241/244], loss=97.7455
	step [242/244], loss=100.7068
	step [243/244], loss=113.0937
	step [244/244], loss=36.2242
	Evaluating
	loss=0.0208, precision=0.4484, recall=0.8777, f1=0.5935
Training epoch 15
	step [1/244], loss=116.9877
	step [2/244], loss=103.2988
	step [3/244], loss=100.9535
	step [4/244], loss=99.4743
	step [5/244], loss=104.0331
	step [6/244], loss=102.5107
	step [7/244], loss=100.3717
	step [8/244], loss=94.7182
	step [9/244], loss=86.2915
	step [10/244], loss=87.4512
	step [11/244], loss=104.0372
	step [12/244], loss=95.8979
	step [13/244], loss=100.9978
	step [14/244], loss=86.4415
	step [15/244], loss=93.9725
	step [16/244], loss=103.8191
	step [17/244], loss=89.9845
	step [18/244], loss=91.4186
	step [19/244], loss=108.7111
	step [20/244], loss=96.9571
	step [21/244], loss=94.5108
	step [22/244], loss=85.1111
	step [23/244], loss=115.5950
	step [24/244], loss=117.2331
	step [25/244], loss=111.6762
	step [26/244], loss=100.0623
	step [27/244], loss=104.6246
	step [28/244], loss=112.6250
	step [29/244], loss=108.9421
	step [30/244], loss=99.2176
	step [31/244], loss=93.2702
	step [32/244], loss=100.4758
	step [33/244], loss=87.0588
	step [34/244], loss=114.0094
	step [35/244], loss=95.2897
	step [36/244], loss=103.0589
	step [37/244], loss=90.6030
	step [38/244], loss=91.7934
	step [39/244], loss=79.3195
	step [40/244], loss=113.9542
	step [41/244], loss=107.3369
	step [42/244], loss=113.3664
	step [43/244], loss=112.3502
	step [44/244], loss=106.3787
	step [45/244], loss=97.8838
	step [46/244], loss=106.3405
	step [47/244], loss=107.2591
	step [48/244], loss=95.2225
	step [49/244], loss=81.7690
	step [50/244], loss=97.8137
	step [51/244], loss=91.8739
	step [52/244], loss=103.3256
	step [53/244], loss=97.1830
	step [54/244], loss=78.1855
	step [55/244], loss=84.3213
	step [56/244], loss=90.0062
	step [57/244], loss=117.7434
	step [58/244], loss=87.0546
	step [59/244], loss=105.2770
	step [60/244], loss=104.7258
	step [61/244], loss=117.5905
	step [62/244], loss=89.9061
	step [63/244], loss=104.6603
	step [64/244], loss=99.9421
	step [65/244], loss=89.4960
	step [66/244], loss=92.3537
	step [67/244], loss=104.7580
	step [68/244], loss=102.0639
	step [69/244], loss=107.5439
	step [70/244], loss=110.5370
	step [71/244], loss=87.4143
	step [72/244], loss=105.2004
	step [73/244], loss=95.0795
	step [74/244], loss=109.4207
	step [75/244], loss=98.6022
	step [76/244], loss=107.8272
	step [77/244], loss=114.3048
	step [78/244], loss=107.7965
	step [79/244], loss=95.9204
	step [80/244], loss=95.3406
	step [81/244], loss=100.7315
	step [82/244], loss=100.9360
	step [83/244], loss=95.5910
	step [84/244], loss=97.0299
	step [85/244], loss=95.3150
	step [86/244], loss=84.9040
	step [87/244], loss=112.0844
	step [88/244], loss=100.3523
	step [89/244], loss=95.8712
	step [90/244], loss=119.6439
	step [91/244], loss=125.8388
	step [92/244], loss=96.3896
	step [93/244], loss=115.0504
	step [94/244], loss=81.1074
	step [95/244], loss=90.8382
	step [96/244], loss=114.3883
	step [97/244], loss=83.2263
	step [98/244], loss=86.9498
	step [99/244], loss=88.5213
	step [100/244], loss=82.0445
	step [101/244], loss=92.9745
	step [102/244], loss=96.9444
	step [103/244], loss=96.7860
	step [104/244], loss=87.2452
	step [105/244], loss=113.5587
	step [106/244], loss=84.6764
	step [107/244], loss=101.7940
	step [108/244], loss=91.7154
	step [109/244], loss=81.8660
	step [110/244], loss=101.4248
	step [111/244], loss=99.1448
	step [112/244], loss=105.0699
	step [113/244], loss=93.8984
	step [114/244], loss=94.9894
	step [115/244], loss=103.7632
	step [116/244], loss=104.7717
	step [117/244], loss=115.5034
	step [118/244], loss=100.5389
	step [119/244], loss=96.4499
	step [120/244], loss=89.9331
	step [121/244], loss=105.0987
	step [122/244], loss=109.7790
	step [123/244], loss=103.7370
	step [124/244], loss=92.0234
	step [125/244], loss=88.6684
	step [126/244], loss=105.9797
	step [127/244], loss=98.9190
	step [128/244], loss=115.7974
	step [129/244], loss=87.6212
	step [130/244], loss=111.0621
	step [131/244], loss=96.3610
	step [132/244], loss=105.0481
	step [133/244], loss=101.7474
	step [134/244], loss=102.0511
	step [135/244], loss=102.4553
	step [136/244], loss=111.9101
	step [137/244], loss=98.3777
	step [138/244], loss=105.1272
	step [139/244], loss=81.8044
	step [140/244], loss=92.6055
	step [141/244], loss=99.9653
	step [142/244], loss=101.8620
	step [143/244], loss=108.0919
	step [144/244], loss=105.5423
	step [145/244], loss=113.3659
	step [146/244], loss=113.4435
	step [147/244], loss=96.4421
	step [148/244], loss=87.4293
	step [149/244], loss=112.7315
	step [150/244], loss=104.1969
	step [151/244], loss=101.7714
	step [152/244], loss=85.0015
	step [153/244], loss=92.1715
	step [154/244], loss=88.4687
	step [155/244], loss=89.8516
	step [156/244], loss=93.7889
	step [157/244], loss=117.7935
	step [158/244], loss=93.8048
	step [159/244], loss=108.8192
	step [160/244], loss=98.7541
	step [161/244], loss=112.4435
	step [162/244], loss=110.7954
	step [163/244], loss=104.5741
	step [164/244], loss=97.6632
	step [165/244], loss=99.8661
	step [166/244], loss=90.6683
	step [167/244], loss=92.6973
	step [168/244], loss=122.8903
	step [169/244], loss=92.3898
	step [170/244], loss=107.2410
	step [171/244], loss=103.1251
	step [172/244], loss=70.8873
	step [173/244], loss=92.2874
	step [174/244], loss=93.5595
	step [175/244], loss=108.1984
	step [176/244], loss=101.8318
	step [177/244], loss=98.1680
	step [178/244], loss=126.3253
	step [179/244], loss=99.7740
	step [180/244], loss=97.4088
	step [181/244], loss=104.8325
	step [182/244], loss=92.6167
	step [183/244], loss=109.3604
	step [184/244], loss=94.3133
	step [185/244], loss=108.1908
	step [186/244], loss=92.6136
	step [187/244], loss=104.6492
	step [188/244], loss=115.7307
	step [189/244], loss=87.9096
	step [190/244], loss=113.4231
	step [191/244], loss=100.9380
	step [192/244], loss=112.8072
	step [193/244], loss=82.6831
	step [194/244], loss=104.5665
	step [195/244], loss=110.6904
	step [196/244], loss=116.3777
	step [197/244], loss=115.5940
	step [198/244], loss=95.3368
	step [199/244], loss=81.5177
	step [200/244], loss=100.2829
	step [201/244], loss=103.0281
	step [202/244], loss=113.7293
	step [203/244], loss=99.2209
	step [204/244], loss=88.6375
	step [205/244], loss=105.3146
	step [206/244], loss=102.2962
	step [207/244], loss=88.0024
	step [208/244], loss=118.5652
	step [209/244], loss=88.8244
	step [210/244], loss=102.5056
	step [211/244], loss=101.4911
	step [212/244], loss=96.6359
	step [213/244], loss=116.3657
	step [214/244], loss=90.9974
	step [215/244], loss=117.0722
	step [216/244], loss=115.5854
	step [217/244], loss=102.5673
	step [218/244], loss=98.0078
	step [219/244], loss=109.4691
	step [220/244], loss=100.7210
	step [221/244], loss=109.0882
	step [222/244], loss=102.6786
	step [223/244], loss=94.6127
	step [224/244], loss=106.2591
	step [225/244], loss=93.5352
	step [226/244], loss=93.3897
	step [227/244], loss=96.1779
	step [228/244], loss=101.2814
	step [229/244], loss=89.8257
	step [230/244], loss=88.7818
	step [231/244], loss=86.8767
	step [232/244], loss=90.3013
	step [233/244], loss=92.5119
	step [234/244], loss=83.8744
	step [235/244], loss=92.2408
	step [236/244], loss=96.6558
	step [237/244], loss=101.1197
	step [238/244], loss=100.9789
	step [239/244], loss=107.3390
	step [240/244], loss=73.7268
	step [241/244], loss=109.4703
	step [242/244], loss=113.6096
	step [243/244], loss=91.1114
	step [244/244], loss=36.8729
	Evaluating
	loss=0.0194, precision=0.4276, recall=0.8890, f1=0.5774
Training epoch 16
	step [1/244], loss=97.7846
	step [2/244], loss=84.7490
	step [3/244], loss=97.3583
	step [4/244], loss=84.2920
	step [5/244], loss=103.8493
	step [6/244], loss=101.4202
	step [7/244], loss=100.6404
	step [8/244], loss=99.8092
	step [9/244], loss=91.2030
	step [10/244], loss=101.8976
	step [11/244], loss=103.4647
	step [12/244], loss=100.9697
	step [13/244], loss=116.6152
	step [14/244], loss=85.1076
	step [15/244], loss=98.7201
	step [16/244], loss=106.4888
	step [17/244], loss=81.2125
	step [18/244], loss=92.9890
	step [19/244], loss=91.9888
	step [20/244], loss=92.1460
	step [21/244], loss=100.9312
	step [22/244], loss=96.7998
	step [23/244], loss=116.1225
	step [24/244], loss=100.7237
	step [25/244], loss=101.5424
	step [26/244], loss=85.3694
	step [27/244], loss=86.7483
	step [28/244], loss=82.2320
	step [29/244], loss=97.6966
	step [30/244], loss=114.8050
	step [31/244], loss=119.2956
	step [32/244], loss=119.9045
	step [33/244], loss=91.0753
	step [34/244], loss=107.2405
	step [35/244], loss=116.1652
	step [36/244], loss=90.7386
	step [37/244], loss=111.2970
	step [38/244], loss=91.0139
	step [39/244], loss=123.0970
	step [40/244], loss=119.3618
	step [41/244], loss=94.4655
	step [42/244], loss=104.0226
	step [43/244], loss=98.6623
	step [44/244], loss=95.8213
	step [45/244], loss=114.0037
	step [46/244], loss=99.3137
	step [47/244], loss=90.2617
	step [48/244], loss=99.5491
	step [49/244], loss=94.7629
	step [50/244], loss=89.3501
	step [51/244], loss=117.9146
	step [52/244], loss=89.3321
	step [53/244], loss=113.2199
	step [54/244], loss=106.1767
	step [55/244], loss=82.5036
	step [56/244], loss=85.5190
	step [57/244], loss=75.4273
	step [58/244], loss=112.8214
	step [59/244], loss=89.1741
	step [60/244], loss=93.2141
	step [61/244], loss=94.9719
	step [62/244], loss=105.1362
	step [63/244], loss=97.9452
	step [64/244], loss=103.7986
	step [65/244], loss=106.7427
	step [66/244], loss=91.1823
	step [67/244], loss=98.3339
	step [68/244], loss=97.1134
	step [69/244], loss=85.1711
	step [70/244], loss=104.5701
	step [71/244], loss=85.7607
	step [72/244], loss=91.3271
	step [73/244], loss=94.9747
	step [74/244], loss=92.8752
	step [75/244], loss=91.1327
	step [76/244], loss=80.2823
	step [77/244], loss=99.3197
	step [78/244], loss=100.9445
	step [79/244], loss=98.9223
	step [80/244], loss=101.8132
	step [81/244], loss=111.9874
	step [82/244], loss=126.9643
	step [83/244], loss=108.0223
	step [84/244], loss=78.6290
	step [85/244], loss=104.8306
	step [86/244], loss=94.5819
	step [87/244], loss=91.0413
	step [88/244], loss=95.2325
	step [89/244], loss=99.7176
	step [90/244], loss=87.3676
	step [91/244], loss=86.4734
	step [92/244], loss=85.7235
	step [93/244], loss=97.4953
	step [94/244], loss=86.0646
	step [95/244], loss=84.0082
	step [96/244], loss=90.4141
	step [97/244], loss=97.0615
	step [98/244], loss=87.9585
	step [99/244], loss=118.5162
	step [100/244], loss=125.8324
	step [101/244], loss=91.4941
	step [102/244], loss=102.3704
	step [103/244], loss=103.7773
	step [104/244], loss=77.5766
	step [105/244], loss=106.5717
	step [106/244], loss=85.7610
	step [107/244], loss=102.9705
	step [108/244], loss=98.3552
	step [109/244], loss=98.7710
	step [110/244], loss=109.4592
	step [111/244], loss=103.3436
	step [112/244], loss=92.9535
	step [113/244], loss=93.9234
	step [114/244], loss=98.1825
	step [115/244], loss=94.9966
	step [116/244], loss=116.3041
	step [117/244], loss=98.3719
	step [118/244], loss=92.1195
	step [119/244], loss=118.6061
	step [120/244], loss=100.2086
	step [121/244], loss=100.5964
	step [122/244], loss=100.3572
	step [123/244], loss=110.0073
	step [124/244], loss=112.3095
	step [125/244], loss=98.3070
	step [126/244], loss=108.7253
	step [127/244], loss=101.2317
	step [128/244], loss=97.5150
	step [129/244], loss=96.4284
	step [130/244], loss=105.6338
	step [131/244], loss=92.7928
	step [132/244], loss=94.2377
	step [133/244], loss=86.4474
	step [134/244], loss=110.3794
	step [135/244], loss=95.2876
	step [136/244], loss=104.4313
	step [137/244], loss=95.1257
	step [138/244], loss=91.9683
	step [139/244], loss=80.3001
	step [140/244], loss=93.1855
	step [141/244], loss=95.9394
	step [142/244], loss=94.2438
	step [143/244], loss=91.7450
	step [144/244], loss=108.1005
	step [145/244], loss=106.6614
	step [146/244], loss=91.7104
	step [147/244], loss=99.2210
	step [148/244], loss=95.2463
	step [149/244], loss=120.5683
	step [150/244], loss=102.2673
	step [151/244], loss=98.4942
	step [152/244], loss=103.0820
	step [153/244], loss=124.5184
	step [154/244], loss=98.1311
	step [155/244], loss=100.3356
	step [156/244], loss=108.0549
	step [157/244], loss=105.2200
	step [158/244], loss=95.2485
	step [159/244], loss=90.9453
	step [160/244], loss=106.2234
	step [161/244], loss=115.1831
	step [162/244], loss=104.7412
	step [163/244], loss=111.8119
	step [164/244], loss=110.1178
	step [165/244], loss=99.3586
	step [166/244], loss=95.4285
	step [167/244], loss=109.1382
	step [168/244], loss=98.9702
	step [169/244], loss=94.8062
	step [170/244], loss=104.3446
	step [171/244], loss=111.5244
	step [172/244], loss=103.4045
	step [173/244], loss=94.9958
	step [174/244], loss=103.0797
	step [175/244], loss=100.8326
	step [176/244], loss=95.9268
	step [177/244], loss=103.0520
	step [178/244], loss=96.9836
	step [179/244], loss=113.2196
	step [180/244], loss=109.8352
	step [181/244], loss=122.2355
	step [182/244], loss=83.4338
	step [183/244], loss=85.4219
	step [184/244], loss=95.7270
	step [185/244], loss=87.2224
	step [186/244], loss=113.6076
	step [187/244], loss=87.6216
	step [188/244], loss=130.5295
	step [189/244], loss=115.5830
	step [190/244], loss=89.9088
	step [191/244], loss=101.3758
	step [192/244], loss=98.2949
	step [193/244], loss=89.9477
	step [194/244], loss=113.5454
	step [195/244], loss=98.5746
	step [196/244], loss=99.5491
	step [197/244], loss=96.6427
	step [198/244], loss=105.7505
	step [199/244], loss=90.3799
	step [200/244], loss=96.2812
	step [201/244], loss=103.9636
	step [202/244], loss=80.2136
	step [203/244], loss=87.9964
	step [204/244], loss=88.2567
	step [205/244], loss=111.0724
	step [206/244], loss=86.8526
	step [207/244], loss=89.2758
	step [208/244], loss=92.8785
	step [209/244], loss=91.8395
	step [210/244], loss=105.5528
	step [211/244], loss=90.2218
	step [212/244], loss=89.2176
	step [213/244], loss=103.1712
	step [214/244], loss=95.4499
	step [215/244], loss=107.5464
	step [216/244], loss=109.1129
	step [217/244], loss=83.7568
	step [218/244], loss=93.0186
	step [219/244], loss=81.7044
	step [220/244], loss=111.7980
	step [221/244], loss=97.3289
	step [222/244], loss=92.2472
	step [223/244], loss=92.7878
	step [224/244], loss=82.4480
	step [225/244], loss=89.1412
	step [226/244], loss=89.6445
	step [227/244], loss=99.2514
	step [228/244], loss=93.6093
	step [229/244], loss=100.6842
	step [230/244], loss=111.6871
	step [231/244], loss=91.9264
	step [232/244], loss=91.9540
	step [233/244], loss=103.8849
	step [234/244], loss=88.6770
	step [235/244], loss=88.2211
	step [236/244], loss=104.7523
	step [237/244], loss=96.0913
	step [238/244], loss=91.2373
	step [239/244], loss=95.0447
	step [240/244], loss=96.5663
	step [241/244], loss=119.5737
	step [242/244], loss=93.2662
	step [243/244], loss=97.4778
	step [244/244], loss=36.7702
	Evaluating
	loss=0.0155, precision=0.4247, recall=0.8919, f1=0.5754
Training epoch 17
	step [1/244], loss=80.1020
	step [2/244], loss=106.9766
	step [3/244], loss=93.2978
	step [4/244], loss=106.5976
	step [5/244], loss=129.1894
	step [6/244], loss=111.7550
	step [7/244], loss=89.5376
	step [8/244], loss=109.5149
	step [9/244], loss=97.2108
	step [10/244], loss=94.7670
	step [11/244], loss=107.1895
	step [12/244], loss=94.5892
	step [13/244], loss=104.7751
	step [14/244], loss=88.0810
	step [15/244], loss=95.3887
	step [16/244], loss=97.0570
	step [17/244], loss=99.4751
	step [18/244], loss=92.6640
	step [19/244], loss=100.1105
	step [20/244], loss=107.4330
	step [21/244], loss=96.3424
	step [22/244], loss=76.0142
	step [23/244], loss=105.3306
	step [24/244], loss=86.0227
	step [25/244], loss=106.4042
	step [26/244], loss=84.4122
	step [27/244], loss=81.1314
	step [28/244], loss=119.2972
	step [29/244], loss=98.6758
	step [30/244], loss=106.0505
	step [31/244], loss=101.4458
	step [32/244], loss=94.5392
	step [33/244], loss=77.3250
	step [34/244], loss=90.9879
	step [35/244], loss=106.0793
	step [36/244], loss=93.8655
	step [37/244], loss=98.1072
	step [38/244], loss=77.6331
	step [39/244], loss=88.0985
	step [40/244], loss=113.9573
	step [41/244], loss=91.5297
	step [42/244], loss=98.4434
	step [43/244], loss=103.6692
	step [44/244], loss=103.8649
	step [45/244], loss=129.0780
	step [46/244], loss=101.9808
	step [47/244], loss=92.5517
	step [48/244], loss=77.7140
	step [49/244], loss=96.3212
	step [50/244], loss=82.7492
	step [51/244], loss=105.1240
	step [52/244], loss=99.5082
	step [53/244], loss=94.5762
	step [54/244], loss=77.9240
	step [55/244], loss=99.6672
	step [56/244], loss=96.5419
	step [57/244], loss=90.6289
	step [58/244], loss=97.1935
	step [59/244], loss=108.9065
	step [60/244], loss=108.3598
	step [61/244], loss=114.9575
	step [62/244], loss=85.0669
	step [63/244], loss=107.6848
	step [64/244], loss=87.5666
	step [65/244], loss=95.6667
	step [66/244], loss=87.7991
	step [67/244], loss=94.1208
	step [68/244], loss=98.0970
	step [69/244], loss=105.2380
	step [70/244], loss=76.6203
	step [71/244], loss=104.0488
	step [72/244], loss=89.9039
	step [73/244], loss=93.7697
	step [74/244], loss=89.0574
	step [75/244], loss=86.2530
	step [76/244], loss=102.4492
	step [77/244], loss=121.8392
	step [78/244], loss=116.0248
	step [79/244], loss=100.7852
	step [80/244], loss=89.1281
	step [81/244], loss=89.1517
	step [82/244], loss=91.0300
	step [83/244], loss=103.2217
	step [84/244], loss=86.0909
	step [85/244], loss=99.2893
	step [86/244], loss=107.5427
	step [87/244], loss=99.7964
	step [88/244], loss=94.0301
	step [89/244], loss=74.9295
	step [90/244], loss=98.3073
	step [91/244], loss=90.4973
	step [92/244], loss=94.7294
	step [93/244], loss=110.7578
	step [94/244], loss=91.0957
	step [95/244], loss=93.5743
	step [96/244], loss=98.2556
	step [97/244], loss=101.1333
	step [98/244], loss=94.2900
	step [99/244], loss=105.1192
	step [100/244], loss=96.1070
	step [101/244], loss=86.4113
	step [102/244], loss=112.9272
	step [103/244], loss=82.5632
	step [104/244], loss=105.9311
	step [105/244], loss=85.2080
	step [106/244], loss=106.4113
	step [107/244], loss=106.2931
	step [108/244], loss=105.6703
	step [109/244], loss=91.7704
	step [110/244], loss=95.9977
	step [111/244], loss=96.1352
	step [112/244], loss=96.6735
	step [113/244], loss=88.2684
	step [114/244], loss=95.5222
	step [115/244], loss=89.3854
	step [116/244], loss=102.3361
	step [117/244], loss=86.0213
	step [118/244], loss=90.8624
	step [119/244], loss=89.9864
	step [120/244], loss=92.9588
	step [121/244], loss=97.2958
	step [122/244], loss=85.3873
	step [123/244], loss=118.6019
	step [124/244], loss=94.3712
	step [125/244], loss=108.0854
	step [126/244], loss=106.5603
	step [127/244], loss=98.2614
	step [128/244], loss=95.4848
	step [129/244], loss=106.6548
	step [130/244], loss=92.3714
	step [131/244], loss=103.5867
	step [132/244], loss=104.0241
	step [133/244], loss=103.5791
	step [134/244], loss=86.6860
	step [135/244], loss=88.9990
	step [136/244], loss=132.7687
	step [137/244], loss=78.2852
	step [138/244], loss=101.8864
	step [139/244], loss=90.1572
	step [140/244], loss=93.7761
	step [141/244], loss=111.2604
	step [142/244], loss=102.7887
	step [143/244], loss=85.0094
	step [144/244], loss=88.7511
	step [145/244], loss=97.0741
	step [146/244], loss=73.6732
	step [147/244], loss=96.4951
	step [148/244], loss=94.7542
	step [149/244], loss=113.0750
	step [150/244], loss=94.7075
	step [151/244], loss=115.8557
	step [152/244], loss=117.1672
	step [153/244], loss=86.3960
	step [154/244], loss=83.5199
	step [155/244], loss=87.4155
	step [156/244], loss=95.3525
	step [157/244], loss=91.1684
	step [158/244], loss=110.2791
	step [159/244], loss=105.6711
	step [160/244], loss=98.7215
	step [161/244], loss=83.2045
	step [162/244], loss=84.1033
	step [163/244], loss=95.1285
	step [164/244], loss=103.0962
	step [165/244], loss=105.2664
	step [166/244], loss=95.8722
	step [167/244], loss=105.0239
	step [168/244], loss=95.5554
	step [169/244], loss=102.9927
	step [170/244], loss=85.1649
	step [171/244], loss=94.0022
	step [172/244], loss=113.0080
	step [173/244], loss=97.5194
	step [174/244], loss=97.5165
	step [175/244], loss=107.8789
	step [176/244], loss=125.6319
	step [177/244], loss=94.4295
	step [178/244], loss=98.5083
	step [179/244], loss=97.8365
	step [180/244], loss=89.2265
	step [181/244], loss=103.8963
	step [182/244], loss=95.6717
	step [183/244], loss=93.2941
	step [184/244], loss=77.9227
	step [185/244], loss=90.7138
	step [186/244], loss=82.6784
	step [187/244], loss=82.3119
	step [188/244], loss=99.5219
	step [189/244], loss=107.4176
	step [190/244], loss=94.8822
	step [191/244], loss=98.1215
	step [192/244], loss=75.5943
	step [193/244], loss=106.6020
	step [194/244], loss=103.4776
	step [195/244], loss=100.4086
	step [196/244], loss=83.7159
	step [197/244], loss=100.0516
	step [198/244], loss=90.7276
	step [199/244], loss=132.8444
	step [200/244], loss=101.9500
	step [201/244], loss=83.5655
	step [202/244], loss=86.5166
	step [203/244], loss=92.9101
	step [204/244], loss=102.9670
	step [205/244], loss=95.3576
	step [206/244], loss=93.9828
	step [207/244], loss=119.7901
	step [208/244], loss=117.3861
	step [209/244], loss=120.1315
	step [210/244], loss=90.6107
	step [211/244], loss=94.3501
	step [212/244], loss=102.5157
	step [213/244], loss=106.1648
	step [214/244], loss=81.9068
	step [215/244], loss=103.9677
	step [216/244], loss=105.5548
	step [217/244], loss=90.8209
	step [218/244], loss=102.2822
	step [219/244], loss=86.7465
	step [220/244], loss=100.8152
	step [221/244], loss=99.9993
	step [222/244], loss=97.6282
	step [223/244], loss=95.6276
	step [224/244], loss=80.5081
	step [225/244], loss=111.6008
	step [226/244], loss=90.8059
	step [227/244], loss=86.4579
	step [228/244], loss=98.9715
	step [229/244], loss=89.9823
	step [230/244], loss=91.5398
	step [231/244], loss=86.9610
	step [232/244], loss=113.6677
	step [233/244], loss=103.4662
	step [234/244], loss=96.6388
	step [235/244], loss=114.7853
	step [236/244], loss=97.2442
	step [237/244], loss=114.2920
	step [238/244], loss=111.8229
	step [239/244], loss=106.8115
	step [240/244], loss=92.9823
	step [241/244], loss=106.7520
	step [242/244], loss=95.9922
	step [243/244], loss=97.4265
	step [244/244], loss=28.8507
	Evaluating
	loss=0.0166, precision=0.4175, recall=0.8750, f1=0.5653
Training epoch 18
	step [1/244], loss=98.8099
	step [2/244], loss=96.9877
	step [3/244], loss=90.1528
	step [4/244], loss=88.2290
	step [5/244], loss=102.1063
	step [6/244], loss=94.2835
	step [7/244], loss=131.7054
	step [8/244], loss=85.2186
	step [9/244], loss=107.6364
	step [10/244], loss=97.1636
	step [11/244], loss=99.9644
	step [12/244], loss=93.6212
	step [13/244], loss=94.1632
	step [14/244], loss=92.9274
	step [15/244], loss=97.4351
	step [16/244], loss=111.4565
	step [17/244], loss=85.9901
	step [18/244], loss=95.9605
	step [19/244], loss=79.9172
	step [20/244], loss=102.7390
	step [21/244], loss=93.6713
	step [22/244], loss=105.5884
	step [23/244], loss=114.9113
	step [24/244], loss=80.6878
	step [25/244], loss=92.5692
	step [26/244], loss=96.6606
	step [27/244], loss=98.1893
	step [28/244], loss=95.9346
	step [29/244], loss=87.6762
	step [30/244], loss=99.3238
	step [31/244], loss=120.2413
	step [32/244], loss=97.8700
	step [33/244], loss=84.8187
	step [34/244], loss=85.4505
	step [35/244], loss=99.8861
	step [36/244], loss=117.0513
	step [37/244], loss=100.6701
	step [38/244], loss=88.4585
	step [39/244], loss=82.7888
	step [40/244], loss=104.8802
	step [41/244], loss=90.8182
	step [42/244], loss=105.6908
	step [43/244], loss=91.0128
	step [44/244], loss=83.2477
	step [45/244], loss=92.2410
	step [46/244], loss=79.6930
	step [47/244], loss=95.1822
	step [48/244], loss=104.4949
	step [49/244], loss=108.3567
	step [50/244], loss=95.6748
	step [51/244], loss=108.2983
	step [52/244], loss=98.3915
	step [53/244], loss=104.6018
	step [54/244], loss=84.9628
	step [55/244], loss=90.0172
	step [56/244], loss=86.1591
	step [57/244], loss=83.0298
	step [58/244], loss=79.4821
	step [59/244], loss=76.2780
	step [60/244], loss=88.3795
	step [61/244], loss=114.1266
	step [62/244], loss=79.5254
	step [63/244], loss=92.2045
	step [64/244], loss=85.4215
	step [65/244], loss=85.9743
	step [66/244], loss=108.7566
	step [67/244], loss=92.8599
	step [68/244], loss=109.0367
	step [69/244], loss=99.9895
	step [70/244], loss=101.2297
	step [71/244], loss=87.8719
	step [72/244], loss=101.6914
	step [73/244], loss=96.3216
	step [74/244], loss=91.8919
	step [75/244], loss=95.9153
	step [76/244], loss=94.3157
	step [77/244], loss=88.4925
	step [78/244], loss=89.6176
	step [79/244], loss=106.1741
	step [80/244], loss=90.2637
	step [81/244], loss=93.7395
	step [82/244], loss=101.8325
	step [83/244], loss=84.2105
	step [84/244], loss=97.7126
	step [85/244], loss=105.1200
	step [86/244], loss=100.2510
	step [87/244], loss=92.5076
	step [88/244], loss=94.8787
	step [89/244], loss=85.0090
	step [90/244], loss=102.4333
	step [91/244], loss=98.0342
	step [92/244], loss=107.5431
	step [93/244], loss=106.8009
	step [94/244], loss=91.3362
	step [95/244], loss=95.0757
	step [96/244], loss=90.3841
	step [97/244], loss=97.6290
	step [98/244], loss=112.3551
	step [99/244], loss=76.8140
	step [100/244], loss=91.3036
	step [101/244], loss=113.6964
	step [102/244], loss=103.0778
	step [103/244], loss=76.6984
	step [104/244], loss=89.5213
	step [105/244], loss=95.2352
	step [106/244], loss=91.7059
	step [107/244], loss=96.3981
	step [108/244], loss=62.7593
	step [109/244], loss=96.0931
	step [110/244], loss=89.1013
	step [111/244], loss=88.2562
	step [112/244], loss=90.6742
	step [113/244], loss=82.1994
	step [114/244], loss=105.7144
	step [115/244], loss=99.4467
	step [116/244], loss=96.9806
	step [117/244], loss=86.6962
	step [118/244], loss=119.3131
	step [119/244], loss=87.8448
	step [120/244], loss=98.1460
	step [121/244], loss=110.5568
	step [122/244], loss=89.1537
	step [123/244], loss=92.4880
	step [124/244], loss=98.2179
	step [125/244], loss=97.5365
	step [126/244], loss=85.7954
	step [127/244], loss=107.8065
	step [128/244], loss=91.3517
	step [129/244], loss=105.3575
	step [130/244], loss=90.9055
	step [131/244], loss=73.6872
	step [132/244], loss=112.8351
	step [133/244], loss=98.0941
	step [134/244], loss=117.2763
	step [135/244], loss=90.3896
	step [136/244], loss=108.4274
	step [137/244], loss=82.9233
	step [138/244], loss=83.3831
	step [139/244], loss=111.4941
	step [140/244], loss=78.9392
	step [141/244], loss=109.5875
	step [142/244], loss=98.8923
	step [143/244], loss=98.5071
	step [144/244], loss=98.8738
	step [145/244], loss=99.0634
	step [146/244], loss=94.3695
	step [147/244], loss=107.9971
	step [148/244], loss=96.6549
	step [149/244], loss=100.8586
	step [150/244], loss=80.5999
	step [151/244], loss=92.6797
	step [152/244], loss=101.6368
	step [153/244], loss=106.5202
	step [154/244], loss=106.1804
	step [155/244], loss=110.3452
	step [156/244], loss=84.9527
	step [157/244], loss=80.3356
	step [158/244], loss=92.5414
	step [159/244], loss=101.4657
	step [160/244], loss=85.3770
	step [161/244], loss=89.5506
	step [162/244], loss=94.5891
	step [163/244], loss=102.5873
	step [164/244], loss=94.8330
	step [165/244], loss=78.9687
	step [166/244], loss=91.5052
	step [167/244], loss=98.0708
	step [168/244], loss=107.2385
	step [169/244], loss=94.0258
	step [170/244], loss=101.4599
	step [171/244], loss=86.1769
	step [172/244], loss=88.2480
	step [173/244], loss=98.3749
	step [174/244], loss=94.9400
	step [175/244], loss=94.0260
	step [176/244], loss=98.3208
	step [177/244], loss=103.6632
	step [178/244], loss=89.8001
	step [179/244], loss=104.5185
	step [180/244], loss=119.1048
	step [181/244], loss=101.0433
	step [182/244], loss=110.9352
	step [183/244], loss=92.3712
	step [184/244], loss=99.2169
	step [185/244], loss=92.1723
	step [186/244], loss=105.6751
	step [187/244], loss=98.2333
	step [188/244], loss=102.7237
	step [189/244], loss=97.1795
	step [190/244], loss=107.0282
	step [191/244], loss=87.6990
	step [192/244], loss=102.8253
	step [193/244], loss=94.5483
	step [194/244], loss=105.1262
	step [195/244], loss=84.0086
	step [196/244], loss=86.8298
	step [197/244], loss=92.6797
	step [198/244], loss=102.6932
	step [199/244], loss=88.7783
	step [200/244], loss=118.5252
	step [201/244], loss=94.7922
	step [202/244], loss=95.0832
	step [203/244], loss=88.7460
	step [204/244], loss=80.1033
	step [205/244], loss=106.0293
	step [206/244], loss=104.1963
	step [207/244], loss=110.8014
	step [208/244], loss=94.9018
	step [209/244], loss=97.5686
	step [210/244], loss=98.5112
	step [211/244], loss=68.2945
	step [212/244], loss=98.9304
	step [213/244], loss=99.2409
	step [214/244], loss=82.2277
	step [215/244], loss=86.4080
	step [216/244], loss=87.9043
	step [217/244], loss=90.9744
	step [218/244], loss=96.6122
	step [219/244], loss=98.9230
	step [220/244], loss=99.6244
	step [221/244], loss=80.4934
	step [222/244], loss=94.6345
	step [223/244], loss=103.6926
	step [224/244], loss=119.1105
	step [225/244], loss=101.5583
	step [226/244], loss=91.7915
	step [227/244], loss=100.4629
	step [228/244], loss=101.5592
	step [229/244], loss=85.6251
	step [230/244], loss=101.9349
	step [231/244], loss=106.1362
	step [232/244], loss=101.9474
	step [233/244], loss=104.7295
	step [234/244], loss=116.4866
	step [235/244], loss=96.5007
	step [236/244], loss=103.1171
	step [237/244], loss=90.0901
	step [238/244], loss=110.8330
	step [239/244], loss=102.5497
	step [240/244], loss=90.9382
	step [241/244], loss=92.2824
	step [242/244], loss=87.5502
	step [243/244], loss=107.0323
	step [244/244], loss=51.2623
	Evaluating
	loss=0.0124, precision=0.4952, recall=0.8834, f1=0.6347
Training epoch 19
	step [1/244], loss=110.1765
	step [2/244], loss=71.6791
	step [3/244], loss=90.7622
	step [4/244], loss=94.2319
	step [5/244], loss=102.2172
	step [6/244], loss=95.2724
	step [7/244], loss=92.8569
	step [8/244], loss=92.4057
	step [9/244], loss=96.9899
	step [10/244], loss=84.9936
	step [11/244], loss=97.7449
	step [12/244], loss=86.6685
	step [13/244], loss=95.7164
	step [14/244], loss=113.7271
	step [15/244], loss=93.6792
	step [16/244], loss=99.0145
	step [17/244], loss=92.8458
	step [18/244], loss=82.3496
	step [19/244], loss=98.0492
	step [20/244], loss=99.5211
	step [21/244], loss=98.1330
	step [22/244], loss=104.4211
	step [23/244], loss=117.8526
	step [24/244], loss=103.8659
	step [25/244], loss=97.1231
	step [26/244], loss=113.5202
	step [27/244], loss=97.2347
	step [28/244], loss=101.0161
	step [29/244], loss=84.1979
	step [30/244], loss=108.1720
	step [31/244], loss=106.5888
	step [32/244], loss=100.6841
	step [33/244], loss=95.2031
	step [34/244], loss=77.8672
	step [35/244], loss=84.9424
	step [36/244], loss=76.0713
	step [37/244], loss=91.1689
	step [38/244], loss=101.0932
	step [39/244], loss=85.9035
	step [40/244], loss=84.6867
	step [41/244], loss=105.1026
	step [42/244], loss=90.1241
	step [43/244], loss=88.4524
	step [44/244], loss=96.1091
	step [45/244], loss=92.4118
	step [46/244], loss=104.4440
	step [47/244], loss=89.1419
	step [48/244], loss=86.0168
	step [49/244], loss=89.8404
	step [50/244], loss=72.3889
	step [51/244], loss=98.1254
	step [52/244], loss=68.8067
	step [53/244], loss=102.0518
	step [54/244], loss=95.1067
	step [55/244], loss=78.1435
	step [56/244], loss=101.1369
	step [57/244], loss=115.6998
	step [58/244], loss=91.3800
	step [59/244], loss=102.4245
	step [60/244], loss=87.1348
	step [61/244], loss=110.4974
	step [62/244], loss=100.4411
	step [63/244], loss=88.4952
	step [64/244], loss=94.0456
	step [65/244], loss=97.4376
	step [66/244], loss=73.1973
	step [67/244], loss=77.7526
	step [68/244], loss=99.5119
	step [69/244], loss=92.1134
	step [70/244], loss=108.3594
	step [71/244], loss=100.3274
	step [72/244], loss=124.9329
	step [73/244], loss=95.8511
	step [74/244], loss=95.1684
	step [75/244], loss=92.4295
	step [76/244], loss=87.7304
	step [77/244], loss=116.9580
	step [78/244], loss=97.3586
	step [79/244], loss=98.6166
	step [80/244], loss=81.6112
	step [81/244], loss=95.6818
	step [82/244], loss=88.6962
	step [83/244], loss=103.1175
	step [84/244], loss=111.9283
	step [85/244], loss=82.0647
	step [86/244], loss=90.7068
	step [87/244], loss=82.5623
	step [88/244], loss=88.0899
	step [89/244], loss=85.5311
	step [90/244], loss=92.0531
	step [91/244], loss=93.7822
	step [92/244], loss=101.3161
	step [93/244], loss=89.9618
	step [94/244], loss=86.3476
	step [95/244], loss=74.0954
	step [96/244], loss=100.3723
	step [97/244], loss=88.3077
	step [98/244], loss=89.3478
	step [99/244], loss=97.5856
	step [100/244], loss=85.4846
	step [101/244], loss=92.7246
	step [102/244], loss=88.4043
	step [103/244], loss=106.2087
	step [104/244], loss=86.7986
	step [105/244], loss=97.3687
	step [106/244], loss=97.6022
	step [107/244], loss=101.6346
	step [108/244], loss=105.0914
	step [109/244], loss=88.9403
	step [110/244], loss=96.7874
	step [111/244], loss=120.0221
	step [112/244], loss=113.6830
	step [113/244], loss=113.7932
	step [114/244], loss=96.4652
	step [115/244], loss=81.0547
	step [116/244], loss=96.0283
	step [117/244], loss=97.0855
	step [118/244], loss=78.2843
	step [119/244], loss=90.3020
	step [120/244], loss=103.1153
	step [121/244], loss=96.6688
	step [122/244], loss=97.8267
	step [123/244], loss=94.6470
	step [124/244], loss=99.4887
	step [125/244], loss=99.6857
	step [126/244], loss=81.2477
	step [127/244], loss=85.7294
	step [128/244], loss=100.5756
	step [129/244], loss=113.0274
	step [130/244], loss=94.9108
	step [131/244], loss=91.7664
	step [132/244], loss=110.4986
	step [133/244], loss=111.3005
	step [134/244], loss=78.3349
	step [135/244], loss=81.9392
	step [136/244], loss=103.6071
	step [137/244], loss=87.1858
	step [138/244], loss=115.5442
	step [139/244], loss=91.3763
	step [140/244], loss=97.8232
	step [141/244], loss=109.1543
	step [142/244], loss=100.3565
	step [143/244], loss=93.6429
	step [144/244], loss=96.5034
	step [145/244], loss=94.8662
	step [146/244], loss=85.0385
	step [147/244], loss=91.7899
	step [148/244], loss=101.7352
	step [149/244], loss=102.2224
	step [150/244], loss=83.5487
	step [151/244], loss=84.6599
	step [152/244], loss=96.3992
	step [153/244], loss=101.0308
	step [154/244], loss=104.3717
	step [155/244], loss=90.7859
	step [156/244], loss=78.1064
	step [157/244], loss=90.5179
	step [158/244], loss=95.5346
	step [159/244], loss=101.8866
	step [160/244], loss=94.6390
	step [161/244], loss=104.4983
	step [162/244], loss=100.2480
	step [163/244], loss=97.7800
	step [164/244], loss=107.8226
	step [165/244], loss=106.3013
	step [166/244], loss=84.2546
	step [167/244], loss=95.0123
	step [168/244], loss=97.6719
	step [169/244], loss=88.7228
	step [170/244], loss=116.1879
	step [171/244], loss=98.0940
	step [172/244], loss=98.0088
	step [173/244], loss=110.0532
	step [174/244], loss=80.1532
	step [175/244], loss=101.5323
	step [176/244], loss=90.5021
	step [177/244], loss=100.2238
	step [178/244], loss=105.4976
	step [179/244], loss=104.8469
	step [180/244], loss=99.5248
	step [181/244], loss=94.6913
	step [182/244], loss=90.6674
	step [183/244], loss=79.7344
	step [184/244], loss=90.6878
	step [185/244], loss=87.4306
	step [186/244], loss=93.3945
	step [187/244], loss=95.0455
	step [188/244], loss=106.9778
	step [189/244], loss=94.2470
	step [190/244], loss=99.3695
	step [191/244], loss=95.9290
	step [192/244], loss=98.9702
	step [193/244], loss=98.1379
	step [194/244], loss=93.4751
	step [195/244], loss=96.4576
	step [196/244], loss=91.8573
	step [197/244], loss=94.3168
	step [198/244], loss=98.2711
	step [199/244], loss=84.3866
	step [200/244], loss=89.3209
	step [201/244], loss=108.0086
	step [202/244], loss=95.7980
	step [203/244], loss=100.8528
	step [204/244], loss=85.7654
	step [205/244], loss=97.8294
	step [206/244], loss=98.9750
	step [207/244], loss=105.7702
	step [208/244], loss=108.9072
	step [209/244], loss=92.8074
	step [210/244], loss=97.5026
	step [211/244], loss=102.8863
	step [212/244], loss=85.2785
	step [213/244], loss=93.7985
	step [214/244], loss=108.4227
	step [215/244], loss=107.5877
	step [216/244], loss=91.9078
	step [217/244], loss=115.9231
	step [218/244], loss=92.9262
	step [219/244], loss=121.2473
	step [220/244], loss=86.9890
	step [221/244], loss=95.0110
	step [222/244], loss=90.5385
	step [223/244], loss=84.4517
	step [224/244], loss=86.0358
	step [225/244], loss=88.7919
	step [226/244], loss=103.8342
	step [227/244], loss=79.7044
	step [228/244], loss=105.1599
	step [229/244], loss=78.0198
	step [230/244], loss=101.6441
	step [231/244], loss=104.0733
	step [232/244], loss=86.4830
	step [233/244], loss=91.4974
	step [234/244], loss=95.3829
	step [235/244], loss=80.2727
	step [236/244], loss=88.6461
	step [237/244], loss=74.1749
	step [238/244], loss=94.7215
	step [239/244], loss=83.6148
	step [240/244], loss=74.4016
	step [241/244], loss=123.0818
	step [242/244], loss=94.4252
	step [243/244], loss=105.5032
	step [244/244], loss=36.7093
	Evaluating
	loss=0.0153, precision=0.3266, recall=0.8724, f1=0.4753
Training epoch 20
	step [1/244], loss=100.4439
	step [2/244], loss=104.0155
	step [3/244], loss=76.3988
	step [4/244], loss=100.8509
	step [5/244], loss=87.8275
	step [6/244], loss=95.4039
	step [7/244], loss=110.3807
	step [8/244], loss=88.6781
	step [9/244], loss=98.4484
	step [10/244], loss=76.1196
	step [11/244], loss=83.1844
	step [12/244], loss=84.1041
	step [13/244], loss=98.0940
	step [14/244], loss=86.7040
	step [15/244], loss=91.1870
	step [16/244], loss=94.5658
	step [17/244], loss=87.9409
	step [18/244], loss=97.4426
	step [19/244], loss=102.7662
	step [20/244], loss=106.0708
	step [21/244], loss=101.3185
	step [22/244], loss=91.0419
	step [23/244], loss=83.9650
	step [24/244], loss=88.4047
	step [25/244], loss=88.3470
	step [26/244], loss=90.7779
	step [27/244], loss=92.6043
	step [28/244], loss=113.5400
	step [29/244], loss=76.4413
	step [30/244], loss=99.5858
	step [31/244], loss=88.4690
	step [32/244], loss=94.7428
	step [33/244], loss=88.2937
	step [34/244], loss=81.4488
	step [35/244], loss=102.0746
	step [36/244], loss=101.3950
	step [37/244], loss=85.7093
	step [38/244], loss=89.8502
	step [39/244], loss=99.4535
	step [40/244], loss=86.4332
	step [41/244], loss=90.2363
	step [42/244], loss=84.6542
	step [43/244], loss=89.7953
	step [44/244], loss=97.8679
	step [45/244], loss=106.9923
	step [46/244], loss=79.9624
	step [47/244], loss=114.3573
	step [48/244], loss=87.1860
	step [49/244], loss=93.3003
	step [50/244], loss=84.7974
	step [51/244], loss=94.0837
	step [52/244], loss=98.5416
	step [53/244], loss=98.9740
	step [54/244], loss=73.1718
	step [55/244], loss=84.4955
	step [56/244], loss=88.8925
	step [57/244], loss=96.5516
	step [58/244], loss=115.4834
	step [59/244], loss=104.4286
	step [60/244], loss=100.5955
	step [61/244], loss=94.1056
	step [62/244], loss=93.4820
	step [63/244], loss=108.5741
	step [64/244], loss=88.8808
	step [65/244], loss=108.6914
	step [66/244], loss=91.5242
	step [67/244], loss=92.7882
	step [68/244], loss=97.2079
	step [69/244], loss=108.3393
	step [70/244], loss=107.0233
	step [71/244], loss=108.1354
	step [72/244], loss=96.8370
	step [73/244], loss=91.4401
	step [74/244], loss=88.3826
	step [75/244], loss=84.1015
	step [76/244], loss=94.1937
	step [77/244], loss=103.5772
	step [78/244], loss=86.6106
	step [79/244], loss=96.9921
	step [80/244], loss=124.4728
	step [81/244], loss=120.7353
	step [82/244], loss=114.4007
	step [83/244], loss=109.2648
	step [84/244], loss=90.3735
	step [85/244], loss=88.7888
	step [86/244], loss=80.7424
	step [87/244], loss=82.8241
	step [88/244], loss=93.6079
	step [89/244], loss=90.2347
	step [90/244], loss=96.2979
	step [91/244], loss=99.7186
	step [92/244], loss=82.0259
	step [93/244], loss=109.5328
	step [94/244], loss=107.1197
	step [95/244], loss=90.0218
	step [96/244], loss=95.3884
	step [97/244], loss=101.4448
	step [98/244], loss=85.7700
	step [99/244], loss=93.5372
	step [100/244], loss=91.6888
	step [101/244], loss=94.5954
	step [102/244], loss=106.6890
	step [103/244], loss=95.2392
	step [104/244], loss=80.1818
	step [105/244], loss=104.1758
	step [106/244], loss=100.8607
	step [107/244], loss=92.7071
	step [108/244], loss=86.6237
	step [109/244], loss=82.3446
	step [110/244], loss=96.0817
	step [111/244], loss=114.4005
	step [112/244], loss=87.8162
	step [113/244], loss=89.1666
	step [114/244], loss=96.1389
	step [115/244], loss=89.8530
	step [116/244], loss=79.9032
	step [117/244], loss=74.6940
	step [118/244], loss=103.4518
	step [119/244], loss=88.5876
	step [120/244], loss=89.7688
	step [121/244], loss=109.1005
	step [122/244], loss=81.4552
	step [123/244], loss=104.4508
	step [124/244], loss=104.0773
	step [125/244], loss=102.3461
	step [126/244], loss=96.0877
	step [127/244], loss=106.4839
	step [128/244], loss=115.5804
	step [129/244], loss=94.7720
	step [130/244], loss=100.3809
	step [131/244], loss=89.4191
	step [132/244], loss=83.3121
	step [133/244], loss=113.5145
	step [134/244], loss=105.6400
	step [135/244], loss=91.9401
	step [136/244], loss=90.1407
	step [137/244], loss=102.7329
	step [138/244], loss=104.9225
	step [139/244], loss=82.5984
	step [140/244], loss=86.6269
	step [141/244], loss=85.2091
	step [142/244], loss=97.8044
	step [143/244], loss=95.0558
	step [144/244], loss=84.5722
	step [145/244], loss=87.1117
	step [146/244], loss=107.4138
	step [147/244], loss=83.6463
	step [148/244], loss=101.4261
	step [149/244], loss=92.6015
	step [150/244], loss=86.8614
	step [151/244], loss=82.8267
	step [152/244], loss=105.3130
	step [153/244], loss=79.7914
	step [154/244], loss=100.2330
	step [155/244], loss=60.7973
	step [156/244], loss=91.2899
	step [157/244], loss=94.9418
	step [158/244], loss=89.5868
	step [159/244], loss=109.6537
	step [160/244], loss=80.0317
	step [161/244], loss=91.8195
	step [162/244], loss=98.3259
	step [163/244], loss=83.3433
	step [164/244], loss=75.1540
	step [165/244], loss=89.3732
	step [166/244], loss=84.0565
	step [167/244], loss=104.9397
	step [168/244], loss=97.4918
	step [169/244], loss=88.2449
	step [170/244], loss=86.5176
	step [171/244], loss=95.2401
	step [172/244], loss=87.8210
	step [173/244], loss=97.4447
	step [174/244], loss=95.9925
	step [175/244], loss=97.4093
	step [176/244], loss=92.8261
	step [177/244], loss=97.8991
	step [178/244], loss=100.8386
	step [179/244], loss=104.5726
	step [180/244], loss=89.8851
	step [181/244], loss=98.7098
	step [182/244], loss=75.2705
	step [183/244], loss=106.2013
	step [184/244], loss=106.8296
	step [185/244], loss=92.4427
	step [186/244], loss=102.5565
	step [187/244], loss=75.1475
	step [188/244], loss=85.9003
	step [189/244], loss=96.1935
	step [190/244], loss=91.5824
	step [191/244], loss=89.6949
	step [192/244], loss=91.1312
	step [193/244], loss=100.0944
	step [194/244], loss=110.5697
	step [195/244], loss=100.4704
	step [196/244], loss=90.3320
	step [197/244], loss=118.6998
	step [198/244], loss=85.4848
	step [199/244], loss=87.9863
	step [200/244], loss=86.8517
	step [201/244], loss=92.7170
	step [202/244], loss=84.0149
	step [203/244], loss=101.1122
	step [204/244], loss=105.1004
	step [205/244], loss=97.5141
	step [206/244], loss=95.0149
	step [207/244], loss=106.2791
	step [208/244], loss=114.9924
	step [209/244], loss=100.1436
	step [210/244], loss=88.9458
	step [211/244], loss=98.4422
	step [212/244], loss=92.2528
	step [213/244], loss=84.9854
	step [214/244], loss=93.3198
	step [215/244], loss=96.6936
	step [216/244], loss=74.6362
	step [217/244], loss=91.7265
	step [218/244], loss=99.1953
	step [219/244], loss=88.0710
	step [220/244], loss=92.1130
	step [221/244], loss=98.5818
	step [222/244], loss=89.6206
	step [223/244], loss=87.0283
	step [224/244], loss=114.1879
	step [225/244], loss=102.7206
	step [226/244], loss=86.6756
	step [227/244], loss=87.7728
	step [228/244], loss=83.4915
	step [229/244], loss=93.5684
	step [230/244], loss=93.7143
	step [231/244], loss=91.1378
	step [232/244], loss=96.7777
	step [233/244], loss=91.7017
	step [234/244], loss=90.8759
	step [235/244], loss=80.0896
	step [236/244], loss=93.6823
	step [237/244], loss=99.5466
	step [238/244], loss=88.0921
	step [239/244], loss=84.0476
	step [240/244], loss=82.6210
	step [241/244], loss=91.2920
	step [242/244], loss=102.1041
	step [243/244], loss=96.5455
	step [244/244], loss=49.5266
	Evaluating
	loss=0.0116, precision=0.4547, recall=0.9026, f1=0.6048
Training epoch 21
	step [1/244], loss=103.4526
	step [2/244], loss=85.5880
	step [3/244], loss=89.4227
	step [4/244], loss=99.8995
	step [5/244], loss=83.7062
	step [6/244], loss=110.8535
	step [7/244], loss=89.3133
	step [8/244], loss=93.9609
	step [9/244], loss=95.7271
	step [10/244], loss=91.7791
	step [11/244], loss=101.0662
	step [12/244], loss=89.4553
	step [13/244], loss=82.5843
	step [14/244], loss=75.7515
	step [15/244], loss=103.0054
	step [16/244], loss=88.3225
	step [17/244], loss=96.2464
	step [18/244], loss=97.7994
	step [19/244], loss=96.9728
	step [20/244], loss=101.3654
	step [21/244], loss=87.4664
	step [22/244], loss=85.7973
	step [23/244], loss=86.8958
	step [24/244], loss=93.7433
	step [25/244], loss=94.7099
	step [26/244], loss=93.6640
	step [27/244], loss=95.9499
	step [28/244], loss=84.7755
	step [29/244], loss=79.9302
	step [30/244], loss=86.5791
	step [31/244], loss=82.7382
	step [32/244], loss=89.9710
	step [33/244], loss=101.0665
	step [34/244], loss=93.3527
	step [35/244], loss=77.9279
	step [36/244], loss=69.0626
	step [37/244], loss=106.6809
	step [38/244], loss=88.8100
	step [39/244], loss=100.2763
	step [40/244], loss=106.9266
	step [41/244], loss=89.6931
	step [42/244], loss=94.7129
	step [43/244], loss=91.9862
	step [44/244], loss=86.6935
	step [45/244], loss=94.2997
	step [46/244], loss=98.3360
	step [47/244], loss=82.4422
	step [48/244], loss=100.7142
	step [49/244], loss=80.5627
	step [50/244], loss=98.1140
	step [51/244], loss=97.6593
	step [52/244], loss=105.5027
	step [53/244], loss=85.0874
	step [54/244], loss=94.8124
	step [55/244], loss=92.2111
	step [56/244], loss=92.0615
	step [57/244], loss=108.7706
	step [58/244], loss=92.3856
	step [59/244], loss=85.4842
	step [60/244], loss=91.1237
	step [61/244], loss=84.2586
	step [62/244], loss=102.3524
	step [63/244], loss=81.5268
	step [64/244], loss=91.5582
	step [65/244], loss=87.8503
	step [66/244], loss=106.8166
	step [67/244], loss=92.0006
	step [68/244], loss=95.2281
	step [69/244], loss=100.4985
	step [70/244], loss=101.6902
	step [71/244], loss=87.9717
	step [72/244], loss=91.0176
	step [73/244], loss=91.8563
	step [74/244], loss=110.6461
	step [75/244], loss=118.4383
	step [76/244], loss=71.2406
	step [77/244], loss=92.2930
	step [78/244], loss=103.5456
	step [79/244], loss=92.8647
	step [80/244], loss=89.1831
	step [81/244], loss=107.4405
	step [82/244], loss=84.8908
	step [83/244], loss=102.2049
	step [84/244], loss=96.0375
	step [85/244], loss=102.1379
	step [86/244], loss=101.1526
	step [87/244], loss=90.9786
	step [88/244], loss=92.2760
	step [89/244], loss=102.4683
	step [90/244], loss=93.7769
	step [91/244], loss=115.3070
	step [92/244], loss=82.7589
	step [93/244], loss=75.8167
	step [94/244], loss=74.6661
	step [95/244], loss=92.8000
	step [96/244], loss=108.5838
	step [97/244], loss=87.3822
	step [98/244], loss=100.1663
	step [99/244], loss=83.0362
	step [100/244], loss=87.8253
	step [101/244], loss=98.2246
	step [102/244], loss=128.8416
	step [103/244], loss=87.3516
	step [104/244], loss=74.1061
	step [105/244], loss=91.2176
	step [106/244], loss=95.6074
	step [107/244], loss=92.3706
	step [108/244], loss=109.7401
	step [109/244], loss=93.1622
	step [110/244], loss=108.0054
	step [111/244], loss=90.9128
	step [112/244], loss=79.8176
	step [113/244], loss=82.8073
	step [114/244], loss=87.8620
	step [115/244], loss=95.2608
	step [116/244], loss=88.5345
	step [117/244], loss=104.2977
	step [118/244], loss=92.2637
	step [119/244], loss=96.6676
	step [120/244], loss=82.3573
	step [121/244], loss=79.3341
	step [122/244], loss=88.0649
	step [123/244], loss=98.8411
	step [124/244], loss=90.9188
	step [125/244], loss=96.6297
	step [126/244], loss=92.2351
	step [127/244], loss=83.1324
	step [128/244], loss=84.1318
	step [129/244], loss=110.5089
	step [130/244], loss=92.1811
	step [131/244], loss=94.2812
	step [132/244], loss=102.0133
	step [133/244], loss=101.0717
	step [134/244], loss=96.4646
	step [135/244], loss=102.8599
	step [136/244], loss=105.4438
	step [137/244], loss=91.9283
	step [138/244], loss=87.8804
	step [139/244], loss=90.6116
	step [140/244], loss=85.8380
	step [141/244], loss=93.6409
	step [142/244], loss=88.7534
	step [143/244], loss=89.0347
	step [144/244], loss=106.9296
	step [145/244], loss=97.0380
	step [146/244], loss=99.1340
	step [147/244], loss=107.3146
	step [148/244], loss=92.4805
	step [149/244], loss=93.6050
	step [150/244], loss=95.5289
	step [151/244], loss=97.0014
	step [152/244], loss=97.8092
	step [153/244], loss=107.4415
	step [154/244], loss=94.1735
	step [155/244], loss=90.1945
	step [156/244], loss=90.2398
	step [157/244], loss=78.3192
	step [158/244], loss=83.6112
	step [159/244], loss=108.2169
	step [160/244], loss=97.3773
	step [161/244], loss=85.6141
	step [162/244], loss=92.0729
	step [163/244], loss=91.0883
	step [164/244], loss=99.9617
	step [165/244], loss=86.4702
	step [166/244], loss=87.1200
	step [167/244], loss=89.4677
	step [168/244], loss=93.9753
	step [169/244], loss=106.3139
	step [170/244], loss=93.7294
	step [171/244], loss=100.6497
	step [172/244], loss=105.3119
	step [173/244], loss=86.1014
	step [174/244], loss=77.6199
	step [175/244], loss=101.3138
	step [176/244], loss=90.7647
	step [177/244], loss=93.7228
	step [178/244], loss=98.3920
	step [179/244], loss=83.7254
	step [180/244], loss=102.8553
	step [181/244], loss=91.0534
	step [182/244], loss=96.0528
	step [183/244], loss=100.2322
	step [184/244], loss=85.3773
	step [185/244], loss=91.4411
	step [186/244], loss=93.0231
	step [187/244], loss=88.6464
	step [188/244], loss=88.6531
	step [189/244], loss=88.8073
	step [190/244], loss=88.0369
	step [191/244], loss=88.5413
	step [192/244], loss=96.7964
	step [193/244], loss=98.4935
	step [194/244], loss=100.3237
	step [195/244], loss=77.5666
	step [196/244], loss=95.8937
	step [197/244], loss=90.9067
	step [198/244], loss=114.1380
	step [199/244], loss=85.4460
	step [200/244], loss=97.3900
	step [201/244], loss=98.2355
	step [202/244], loss=89.9704
	step [203/244], loss=88.0040
	step [204/244], loss=103.1710
	step [205/244], loss=100.6511
	step [206/244], loss=82.6764
	step [207/244], loss=93.1013
	step [208/244], loss=82.5140
	step [209/244], loss=103.5196
	step [210/244], loss=88.0430
	step [211/244], loss=101.7279
	step [212/244], loss=78.5204
	step [213/244], loss=91.0570
	step [214/244], loss=70.9401
	step [215/244], loss=84.4520
	step [216/244], loss=94.6060
	step [217/244], loss=100.5110
	step [218/244], loss=95.4387
	step [219/244], loss=94.0241
	step [220/244], loss=86.5788
	step [221/244], loss=83.6287
	step [222/244], loss=83.6063
	step [223/244], loss=79.1195
	step [224/244], loss=99.1452
	step [225/244], loss=103.9315
	step [226/244], loss=92.6470
	step [227/244], loss=106.6164
	step [228/244], loss=92.0447
	step [229/244], loss=116.1031
	step [230/244], loss=91.9660
	step [231/244], loss=87.9179
	step [232/244], loss=84.5992
	step [233/244], loss=76.4807
	step [234/244], loss=105.4051
	step [235/244], loss=109.0949
	step [236/244], loss=86.6595
	step [237/244], loss=88.1829
	step [238/244], loss=82.8617
	step [239/244], loss=84.1209
	step [240/244], loss=79.1658
	step [241/244], loss=83.0883
	step [242/244], loss=93.8679
	step [243/244], loss=93.5453
	step [244/244], loss=41.6337
	Evaluating
	loss=0.0107, precision=0.4468, recall=0.8979, f1=0.5967
Training epoch 22
	step [1/244], loss=108.7555
	step [2/244], loss=92.9749
	step [3/244], loss=96.5445
	step [4/244], loss=117.3079
	step [5/244], loss=89.3608
	step [6/244], loss=92.4410
	step [7/244], loss=108.4459
	step [8/244], loss=96.2686
	step [9/244], loss=102.4572
	step [10/244], loss=91.9666
	step [11/244], loss=96.0754
	step [12/244], loss=84.1184
	step [13/244], loss=81.6168
	step [14/244], loss=83.7719
	step [15/244], loss=84.2268
	step [16/244], loss=110.0240
	step [17/244], loss=74.2269
	step [18/244], loss=93.7559
	step [19/244], loss=94.0994
	step [20/244], loss=73.6544
	step [21/244], loss=88.9040
	step [22/244], loss=102.2661
	step [23/244], loss=89.3956
	step [24/244], loss=101.5453
	step [25/244], loss=96.8927
	step [26/244], loss=96.2497
	step [27/244], loss=90.3045
	step [28/244], loss=85.8262
	step [29/244], loss=91.8007
	step [30/244], loss=95.0763
	step [31/244], loss=86.7411
	step [32/244], loss=87.0502
	step [33/244], loss=89.7634
	step [34/244], loss=103.4917
	step [35/244], loss=96.3385
	step [36/244], loss=104.1899
	step [37/244], loss=86.9932
	step [38/244], loss=103.9890
	step [39/244], loss=94.9470
	step [40/244], loss=93.9539
	step [41/244], loss=87.2272
	step [42/244], loss=84.4046
	step [43/244], loss=91.8085
	step [44/244], loss=87.3741
	step [45/244], loss=113.7148
	step [46/244], loss=87.3605
	step [47/244], loss=74.9043
	step [48/244], loss=103.9089
	step [49/244], loss=107.2169
	step [50/244], loss=105.4271
	step [51/244], loss=80.8694
	step [52/244], loss=83.9772
	step [53/244], loss=108.5255
	step [54/244], loss=104.7973
	step [55/244], loss=80.5586
	step [56/244], loss=77.6342
	step [57/244], loss=81.3028
	step [58/244], loss=92.7140
	step [59/244], loss=71.4680
	step [60/244], loss=69.5518
	step [61/244], loss=87.8810
	step [62/244], loss=96.1936
	step [63/244], loss=109.4536
	step [64/244], loss=106.4871
	step [65/244], loss=91.8296
	step [66/244], loss=91.0389
	step [67/244], loss=82.7081
	step [68/244], loss=97.6446
	step [69/244], loss=94.9034
	step [70/244], loss=98.1247
	step [71/244], loss=92.3264
	step [72/244], loss=81.4009
	step [73/244], loss=77.0866
	step [74/244], loss=85.8658
	step [75/244], loss=101.6931
	step [76/244], loss=98.0044
	step [77/244], loss=80.7868
	step [78/244], loss=91.8726
	step [79/244], loss=96.9131
	step [80/244], loss=98.2295
	step [81/244], loss=100.6177
	step [82/244], loss=89.3792
	step [83/244], loss=110.1838
	step [84/244], loss=97.6981
	step [85/244], loss=99.3050
	step [86/244], loss=87.1177
	step [87/244], loss=86.6762
	step [88/244], loss=84.2721
	step [89/244], loss=94.8346
	step [90/244], loss=81.8335
	step [91/244], loss=94.1711
	step [92/244], loss=92.5525
	step [93/244], loss=92.0110
	step [94/244], loss=94.7947
	step [95/244], loss=88.1729
	step [96/244], loss=91.7362
	step [97/244], loss=92.5092
	step [98/244], loss=79.1521
	step [99/244], loss=89.1941
	step [100/244], loss=86.8819
	step [101/244], loss=97.7105
	step [102/244], loss=97.7948
	step [103/244], loss=94.4346
	step [104/244], loss=101.4783
	step [105/244], loss=84.5183
	step [106/244], loss=109.3207
	step [107/244], loss=89.7028
	step [108/244], loss=100.6415
	step [109/244], loss=78.1556
	step [110/244], loss=95.4141
	step [111/244], loss=86.2469
	step [112/244], loss=88.9371
	step [113/244], loss=100.4975
	step [114/244], loss=116.9183
	step [115/244], loss=92.6564
	step [116/244], loss=100.6495
	step [117/244], loss=101.6088
	step [118/244], loss=93.9415
	step [119/244], loss=83.0243
	step [120/244], loss=78.8859
	step [121/244], loss=96.4157
	step [122/244], loss=89.9744
	step [123/244], loss=72.2696
	step [124/244], loss=86.3978
	step [125/244], loss=98.4439
	step [126/244], loss=94.2444
	step [127/244], loss=74.8022
	step [128/244], loss=85.8811
	step [129/244], loss=103.4926
	step [130/244], loss=86.8459
	step [131/244], loss=83.7209
	step [132/244], loss=88.2975
	step [133/244], loss=84.9946
	step [134/244], loss=102.6595
	step [135/244], loss=90.8373
	step [136/244], loss=85.4629
	step [137/244], loss=102.4086
	step [138/244], loss=89.1158
	step [139/244], loss=81.8028
	step [140/244], loss=87.0603
	step [141/244], loss=110.3581
	step [142/244], loss=76.6702
	step [143/244], loss=84.9132
	step [144/244], loss=86.9497
	step [145/244], loss=88.9769
	step [146/244], loss=109.8664
	step [147/244], loss=86.9893
	step [148/244], loss=100.4677
	step [149/244], loss=98.1215
	step [150/244], loss=93.9061
	step [151/244], loss=100.6607
	step [152/244], loss=83.7668
	step [153/244], loss=75.7945
	step [154/244], loss=89.8664
	step [155/244], loss=84.3751
	step [156/244], loss=110.9299
	step [157/244], loss=78.2363
	step [158/244], loss=87.0656
	step [159/244], loss=99.9985
	step [160/244], loss=106.0116
	step [161/244], loss=97.1294
	step [162/244], loss=90.5108
	step [163/244], loss=111.7596
	step [164/244], loss=89.0896
	step [165/244], loss=105.4868
	step [166/244], loss=95.6414
	step [167/244], loss=112.3314
	step [168/244], loss=93.3131
	step [169/244], loss=97.7572
	step [170/244], loss=92.4748
	step [171/244], loss=78.2706
	step [172/244], loss=97.8228
	step [173/244], loss=75.9491
	step [174/244], loss=94.7740
	step [175/244], loss=106.2148
	step [176/244], loss=106.5411
	step [177/244], loss=82.8063
	step [178/244], loss=104.7015
	step [179/244], loss=82.9035
	step [180/244], loss=79.5014
	step [181/244], loss=95.2924
	step [182/244], loss=79.0018
	step [183/244], loss=95.2030
	step [184/244], loss=87.2728
	step [185/244], loss=87.6055
	step [186/244], loss=89.6927
	step [187/244], loss=79.0649
	step [188/244], loss=88.8879
	step [189/244], loss=83.2424
	step [190/244], loss=99.1674
	step [191/244], loss=76.2666
	step [192/244], loss=95.3448
	step [193/244], loss=89.2443
	step [194/244], loss=87.4571
	step [195/244], loss=82.8041
	step [196/244], loss=85.3648
	step [197/244], loss=96.0940
	step [198/244], loss=86.1702
	step [199/244], loss=88.9497
	step [200/244], loss=95.5406
	step [201/244], loss=95.0742
	step [202/244], loss=100.8331
	step [203/244], loss=82.4040
	step [204/244], loss=86.6264
	step [205/244], loss=87.3328
	step [206/244], loss=110.4017
	step [207/244], loss=89.8875
	step [208/244], loss=101.8465
	step [209/244], loss=83.5764
	step [210/244], loss=97.3068
	step [211/244], loss=88.5651
	step [212/244], loss=102.2155
	step [213/244], loss=77.8040
	step [214/244], loss=93.3856
	step [215/244], loss=86.0139
	step [216/244], loss=84.0890
	step [217/244], loss=115.5751
	step [218/244], loss=80.9338
	step [219/244], loss=79.5516
	step [220/244], loss=82.4700
	step [221/244], loss=91.4481
	step [222/244], loss=103.0552
	step [223/244], loss=92.4971
	step [224/244], loss=114.9535
	step [225/244], loss=88.4423
	step [226/244], loss=93.2907
	step [227/244], loss=73.0234
	step [228/244], loss=99.6718
	step [229/244], loss=81.8311
	step [230/244], loss=99.8163
	step [231/244], loss=96.7191
	step [232/244], loss=99.3099
	step [233/244], loss=91.8933
	step [234/244], loss=80.8098
	step [235/244], loss=103.8759
	step [236/244], loss=78.6065
	step [237/244], loss=110.1803
	step [238/244], loss=85.1237
	step [239/244], loss=90.4500
	step [240/244], loss=93.0246
	step [241/244], loss=96.3559
	step [242/244], loss=99.3993
	step [243/244], loss=94.9142
	step [244/244], loss=36.8028
	Evaluating
	loss=0.0126, precision=0.3480, recall=0.8831, f1=0.4993
Training epoch 23
	step [1/244], loss=100.8232
	step [2/244], loss=84.5260
	step [3/244], loss=132.1900
	step [4/244], loss=83.6781
	step [5/244], loss=98.6452
	step [6/244], loss=89.6491
	step [7/244], loss=94.6500
	step [8/244], loss=97.0906
	step [9/244], loss=94.0146
	step [10/244], loss=80.0346
	step [11/244], loss=87.4006
	step [12/244], loss=74.2169
	step [13/244], loss=96.9933
	step [14/244], loss=91.7784
	step [15/244], loss=88.8115
	step [16/244], loss=93.5266
	step [17/244], loss=83.8978
	step [18/244], loss=106.4143
	step [19/244], loss=86.7132
	step [20/244], loss=103.7631
	step [21/244], loss=97.3542
	step [22/244], loss=83.7209
	step [23/244], loss=90.3107
	step [24/244], loss=95.1927
	step [25/244], loss=86.7279
	step [26/244], loss=86.4321
	step [27/244], loss=117.0961
	step [28/244], loss=96.0977
	step [29/244], loss=106.6492
	step [30/244], loss=82.8457
	step [31/244], loss=100.5304
	step [32/244], loss=105.2200
	step [33/244], loss=93.4297
	step [34/244], loss=83.6833
	step [35/244], loss=102.4204
	step [36/244], loss=100.8966
	step [37/244], loss=76.0981
	step [38/244], loss=90.5552
	step [39/244], loss=93.6490
	step [40/244], loss=78.9861
	step [41/244], loss=89.4554
	step [42/244], loss=100.2130
	step [43/244], loss=84.1636
	step [44/244], loss=94.5890
	step [45/244], loss=89.8559
	step [46/244], loss=83.0002
	step [47/244], loss=82.5397
	step [48/244], loss=84.3491
	step [49/244], loss=82.3173
	step [50/244], loss=102.8519
	step [51/244], loss=98.2474
	step [52/244], loss=87.7461
	step [53/244], loss=87.0719
	step [54/244], loss=104.4208
	step [55/244], loss=86.5942
	step [56/244], loss=86.7002
	step [57/244], loss=91.1102
	step [58/244], loss=101.0679
	step [59/244], loss=70.5353
	step [60/244], loss=103.8579
	step [61/244], loss=76.9654
	step [62/244], loss=70.0231
	step [63/244], loss=99.7507
	step [64/244], loss=87.5432
	step [65/244], loss=88.3225
	step [66/244], loss=86.0434
	step [67/244], loss=87.0326
	step [68/244], loss=86.3675
	step [69/244], loss=107.7102
	step [70/244], loss=79.9421
	step [71/244], loss=92.5643
	step [72/244], loss=83.4801
	step [73/244], loss=96.3632
	step [74/244], loss=82.8811
	step [75/244], loss=72.2341
	step [76/244], loss=86.3697
	step [77/244], loss=93.3916
	step [78/244], loss=86.3478
	step [79/244], loss=92.6741
	step [80/244], loss=62.6558
	step [81/244], loss=90.8480
	step [82/244], loss=106.3746
	step [83/244], loss=78.6622
	step [84/244], loss=91.6442
	step [85/244], loss=88.5166
	step [86/244], loss=110.2964
	step [87/244], loss=80.9601
	step [88/244], loss=103.3733
	step [89/244], loss=83.5058
	step [90/244], loss=82.2868
	step [91/244], loss=66.3668
	step [92/244], loss=70.5883
	step [93/244], loss=100.7196
	step [94/244], loss=93.5133
	step [95/244], loss=88.2367
	step [96/244], loss=89.1373
	step [97/244], loss=84.2081
	step [98/244], loss=95.1710
	step [99/244], loss=91.6898
	step [100/244], loss=91.7513
	step [101/244], loss=97.8668
	step [102/244], loss=100.0997
	step [103/244], loss=101.8691
	step [104/244], loss=81.1542
	step [105/244], loss=91.3167
	step [106/244], loss=93.9887
	step [107/244], loss=104.8802
	step [108/244], loss=99.6691
	step [109/244], loss=95.0425
	step [110/244], loss=94.7263
	step [111/244], loss=100.0877
	step [112/244], loss=86.7689
	step [113/244], loss=104.7626
	step [114/244], loss=86.3359
	step [115/244], loss=98.4527
	step [116/244], loss=81.0570
	step [117/244], loss=86.0411
	step [118/244], loss=94.4954
	step [119/244], loss=85.5623
	step [120/244], loss=91.4024
	step [121/244], loss=67.1231
	step [122/244], loss=98.9822
	step [123/244], loss=76.8697
	step [124/244], loss=79.8916
	step [125/244], loss=97.8463
	step [126/244], loss=79.9128
	step [127/244], loss=94.1911
	step [128/244], loss=82.5995
	step [129/244], loss=120.3134
	step [130/244], loss=83.3648
	step [131/244], loss=98.9214
	step [132/244], loss=72.3577
	step [133/244], loss=83.2992
	step [134/244], loss=99.6855
	step [135/244], loss=91.9402
	step [136/244], loss=82.0054
	step [137/244], loss=103.1898
	step [138/244], loss=78.8452
	step [139/244], loss=76.6962
	step [140/244], loss=82.8765
	step [141/244], loss=104.1808
	step [142/244], loss=97.4193
	step [143/244], loss=88.9652
	step [144/244], loss=112.0164
	step [145/244], loss=99.8184
	step [146/244], loss=93.1431
	step [147/244], loss=92.0228
	step [148/244], loss=88.3528
	step [149/244], loss=72.3188
	step [150/244], loss=93.6647
	step [151/244], loss=75.5973
	step [152/244], loss=72.9258
	step [153/244], loss=82.6002
	step [154/244], loss=81.1318
	step [155/244], loss=95.8390
	step [156/244], loss=95.9012
	step [157/244], loss=85.0680
	step [158/244], loss=93.9681
	step [159/244], loss=94.8805
	step [160/244], loss=102.4461
	step [161/244], loss=100.5526
	step [162/244], loss=91.4465
	step [163/244], loss=88.7327
	step [164/244], loss=87.8563
	step [165/244], loss=100.2337
	step [166/244], loss=110.4861
	step [167/244], loss=82.3554
	step [168/244], loss=75.8604
	step [169/244], loss=84.4500
	step [170/244], loss=104.7609
	step [171/244], loss=109.1466
	step [172/244], loss=92.8196
	step [173/244], loss=83.5795
	step [174/244], loss=104.9786
	step [175/244], loss=97.0307
	step [176/244], loss=85.3222
	step [177/244], loss=78.8442
	step [178/244], loss=92.6648
	step [179/244], loss=95.2068
	step [180/244], loss=105.2494
	step [181/244], loss=87.3509
	step [182/244], loss=88.2458
	step [183/244], loss=104.9565
	step [184/244], loss=92.3317
	step [185/244], loss=95.9244
	step [186/244], loss=92.6001
	step [187/244], loss=87.6504
	step [188/244], loss=106.8212
	step [189/244], loss=105.5012
	step [190/244], loss=83.2073
	step [191/244], loss=95.3171
	step [192/244], loss=88.8797
	step [193/244], loss=101.4758
	step [194/244], loss=105.4396
	step [195/244], loss=83.0852
	step [196/244], loss=114.9327
	step [197/244], loss=104.6259
	step [198/244], loss=110.3891
	step [199/244], loss=85.3804
	step [200/244], loss=78.9663
	step [201/244], loss=86.7991
	step [202/244], loss=94.9374
	step [203/244], loss=77.4938
	step [204/244], loss=96.7471
	step [205/244], loss=81.0497
	step [206/244], loss=111.9794
	step [207/244], loss=82.4006
	step [208/244], loss=86.4888
	step [209/244], loss=97.6412
	step [210/244], loss=107.0441
	step [211/244], loss=59.5783
	step [212/244], loss=86.7527
	step [213/244], loss=91.9761
	step [214/244], loss=93.8580
	step [215/244], loss=94.3377
	step [216/244], loss=114.7782
	step [217/244], loss=96.4857
	step [218/244], loss=89.1875
	step [219/244], loss=91.6953
	step [220/244], loss=95.4803
	step [221/244], loss=72.4827
	step [222/244], loss=85.0199
	step [223/244], loss=106.6128
	step [224/244], loss=96.6829
	step [225/244], loss=104.2104
	step [226/244], loss=101.1494
	step [227/244], loss=92.7725
	step [228/244], loss=89.9695
	step [229/244], loss=80.0123
	step [230/244], loss=83.7213
	step [231/244], loss=76.9439
	step [232/244], loss=89.4540
	step [233/244], loss=79.2285
	step [234/244], loss=88.1531
	step [235/244], loss=86.4890
	step [236/244], loss=87.5596
	step [237/244], loss=89.4466
	step [238/244], loss=100.0892
	step [239/244], loss=90.2247
	step [240/244], loss=83.4726
	step [241/244], loss=81.6979
	step [242/244], loss=105.2537
	step [243/244], loss=85.2584
	step [244/244], loss=30.8058
	Evaluating
	loss=0.0105, precision=0.4112, recall=0.9048, f1=0.5654
Training epoch 24
	step [1/244], loss=110.8214
	step [2/244], loss=89.0943
	step [3/244], loss=95.6899
	step [4/244], loss=98.2831
	step [5/244], loss=93.6158
	step [6/244], loss=100.9236
	step [7/244], loss=69.3832
	step [8/244], loss=85.2863
	step [9/244], loss=77.6703
	step [10/244], loss=98.3657
	step [11/244], loss=92.9706
	step [12/244], loss=79.4863
	step [13/244], loss=117.8769
	step [14/244], loss=82.6073
	step [15/244], loss=84.7616
	step [16/244], loss=99.8703
	step [17/244], loss=85.9179
	step [18/244], loss=91.3268
	step [19/244], loss=89.6956
	step [20/244], loss=112.2206
	step [21/244], loss=108.0595
	step [22/244], loss=78.9182
	step [23/244], loss=92.7588
	step [24/244], loss=81.8127
	step [25/244], loss=85.7107
	step [26/244], loss=86.6599
	step [27/244], loss=95.8750
	step [28/244], loss=96.8766
	step [29/244], loss=77.5327
	step [30/244], loss=81.6504
	step [31/244], loss=85.5610
	step [32/244], loss=91.7687
	step [33/244], loss=102.1262
	step [34/244], loss=116.4278
	step [35/244], loss=85.1314
	step [36/244], loss=89.8686
	step [37/244], loss=83.2323
	step [38/244], loss=94.3526
	step [39/244], loss=100.9497
	step [40/244], loss=92.4479
	step [41/244], loss=105.0169
	step [42/244], loss=87.3246
	step [43/244], loss=101.5077
	step [44/244], loss=78.7547
	step [45/244], loss=91.0688
	step [46/244], loss=97.2536
	step [47/244], loss=99.1949
	step [48/244], loss=83.6011
	step [49/244], loss=74.9370
	step [50/244], loss=85.5199
	step [51/244], loss=97.4810
	step [52/244], loss=96.2031
	step [53/244], loss=76.5692
	step [54/244], loss=94.4363
	step [55/244], loss=97.9621
	step [56/244], loss=93.7410
	step [57/244], loss=82.7178
	step [58/244], loss=93.5134
	step [59/244], loss=103.1322
	step [60/244], loss=81.0521
	step [61/244], loss=97.9054
	step [62/244], loss=77.1054
	step [63/244], loss=109.0149
	step [64/244], loss=91.0099
	step [65/244], loss=94.2905
	step [66/244], loss=78.1604
	step [67/244], loss=87.3186
	step [68/244], loss=104.4602
	step [69/244], loss=84.6495
	step [70/244], loss=91.2749
	step [71/244], loss=98.6222
	step [72/244], loss=96.3606
	step [73/244], loss=103.3864
	step [74/244], loss=79.4447
	step [75/244], loss=84.4582
	step [76/244], loss=86.8194
	step [77/244], loss=88.7579
	step [78/244], loss=84.9034
	step [79/244], loss=96.8116
	step [80/244], loss=74.0387
	step [81/244], loss=83.3546
	step [82/244], loss=87.2573
	step [83/244], loss=78.6153
	step [84/244], loss=80.0566
	step [85/244], loss=88.9411
	step [86/244], loss=85.9522
	step [87/244], loss=93.1748
	step [88/244], loss=95.3995
	step [89/244], loss=100.5515
	step [90/244], loss=92.3516
	step [91/244], loss=69.0273
	step [92/244], loss=93.2842
	step [93/244], loss=79.0705
	step [94/244], loss=81.6721
	step [95/244], loss=102.1494
	step [96/244], loss=88.0889
	step [97/244], loss=90.8005
	step [98/244], loss=80.2807
	step [99/244], loss=86.3870
	step [100/244], loss=89.3095
	step [101/244], loss=95.7360
	step [102/244], loss=80.9577
	step [103/244], loss=84.7557
	step [104/244], loss=97.4575
	step [105/244], loss=106.8687
	step [106/244], loss=94.7877
	step [107/244], loss=85.2447
	step [108/244], loss=96.9770
	step [109/244], loss=86.2097
	step [110/244], loss=91.9481
	step [111/244], loss=92.8341
	step [112/244], loss=99.3044
	step [113/244], loss=75.5890
	step [114/244], loss=87.8213
	step [115/244], loss=86.1892
	step [116/244], loss=84.0505
	step [117/244], loss=104.2264
	step [118/244], loss=87.1662
	step [119/244], loss=102.0164
	step [120/244], loss=72.2826
	step [121/244], loss=91.8486
	step [122/244], loss=82.4734
	step [123/244], loss=102.3301
	step [124/244], loss=100.8631
	step [125/244], loss=82.3704
	step [126/244], loss=93.6336
	step [127/244], loss=94.3731
	step [128/244], loss=89.2651
	step [129/244], loss=76.4836
	step [130/244], loss=77.2652
	step [131/244], loss=100.6132
	step [132/244], loss=86.2073
	step [133/244], loss=72.0370
	step [134/244], loss=92.6867
	step [135/244], loss=91.9240
	step [136/244], loss=90.8628
	step [137/244], loss=85.4797
	step [138/244], loss=84.6976
	step [139/244], loss=107.6201
	step [140/244], loss=77.5229
	step [141/244], loss=77.9088
	step [142/244], loss=91.5618
	step [143/244], loss=95.1801
	step [144/244], loss=95.5559
	step [145/244], loss=97.7155
	step [146/244], loss=80.7222
	step [147/244], loss=76.1377
	step [148/244], loss=95.6298
	step [149/244], loss=81.5074
	step [150/244], loss=84.8695
	step [151/244], loss=100.9082
	step [152/244], loss=74.6018
	step [153/244], loss=101.9475
	step [154/244], loss=79.2741
	step [155/244], loss=103.8787
	step [156/244], loss=89.0277
	step [157/244], loss=80.9479
	step [158/244], loss=89.6791
	step [159/244], loss=122.8602
	step [160/244], loss=102.0831
	step [161/244], loss=89.9115
	step [162/244], loss=90.3805
	step [163/244], loss=96.9972
	step [164/244], loss=91.0925
	step [165/244], loss=80.0549
	step [166/244], loss=78.8331
	step [167/244], loss=92.0244
	step [168/244], loss=90.2399
	step [169/244], loss=84.9755
	step [170/244], loss=83.7872
	step [171/244], loss=83.1082
	step [172/244], loss=109.6660
	step [173/244], loss=107.0670
	step [174/244], loss=92.5384
	step [175/244], loss=100.4984
	step [176/244], loss=85.3688
	step [177/244], loss=105.8907
	step [178/244], loss=102.6615
	step [179/244], loss=105.0445
	step [180/244], loss=99.5623
	step [181/244], loss=92.7016
	step [182/244], loss=86.9584
	step [183/244], loss=91.3673
	step [184/244], loss=91.0376
	step [185/244], loss=98.3240
	step [186/244], loss=84.7164
	step [187/244], loss=79.3435
	step [188/244], loss=86.0204
	step [189/244], loss=78.6052
	step [190/244], loss=86.5744
	step [191/244], loss=90.8179
	step [192/244], loss=79.1058
	step [193/244], loss=62.5900
	step [194/244], loss=85.4667
	step [195/244], loss=94.8929
	step [196/244], loss=86.4611
	step [197/244], loss=99.8034
	step [198/244], loss=96.8068
	step [199/244], loss=85.0193
	step [200/244], loss=92.0886
	step [201/244], loss=93.5229
	step [202/244], loss=111.6252
	step [203/244], loss=77.0523
	step [204/244], loss=74.7309
	step [205/244], loss=89.7463
	step [206/244], loss=76.9457
	step [207/244], loss=89.1349
	step [208/244], loss=91.5167
	step [209/244], loss=80.9975
	step [210/244], loss=99.1433
	step [211/244], loss=87.5429
	step [212/244], loss=81.5237
	step [213/244], loss=85.7815
	step [214/244], loss=89.5232
	step [215/244], loss=93.9207
	step [216/244], loss=88.3733
	step [217/244], loss=92.7433
	step [218/244], loss=103.9911
	step [219/244], loss=86.3573
	step [220/244], loss=86.5757
	step [221/244], loss=88.5804
	step [222/244], loss=73.6863
	step [223/244], loss=97.0586
	step [224/244], loss=87.0701
	step [225/244], loss=80.8110
	step [226/244], loss=81.0858
	step [227/244], loss=80.3620
	step [228/244], loss=109.9019
	step [229/244], loss=76.9193
	step [230/244], loss=82.9780
	step [231/244], loss=93.6173
	step [232/244], loss=84.8031
	step [233/244], loss=107.5830
	step [234/244], loss=83.1598
	step [235/244], loss=91.1487
	step [236/244], loss=107.0687
	step [237/244], loss=106.3906
	step [238/244], loss=78.6720
	step [239/244], loss=89.9322
	step [240/244], loss=75.8973
	step [241/244], loss=82.1596
	step [242/244], loss=96.0240
	step [243/244], loss=81.0967
	step [244/244], loss=30.1730
	Evaluating
	loss=0.0109, precision=0.3414, recall=0.8664, f1=0.4898
Training epoch 25
	step [1/244], loss=93.5154
	step [2/244], loss=97.2371
	step [3/244], loss=102.4735
	step [4/244], loss=75.1973
	step [5/244], loss=98.4454
	step [6/244], loss=101.6113
	step [7/244], loss=100.4249
	step [8/244], loss=81.9142
	step [9/244], loss=102.8681
	step [10/244], loss=95.5503
	step [11/244], loss=107.2950
	step [12/244], loss=105.6682
	step [13/244], loss=92.3596
	step [14/244], loss=77.2962
	step [15/244], loss=89.1350
	step [16/244], loss=70.7862
	step [17/244], loss=80.5579
	step [18/244], loss=92.5444
	step [19/244], loss=89.4106
	step [20/244], loss=94.3584
	step [21/244], loss=74.5885
	step [22/244], loss=72.4456
	step [23/244], loss=97.0412
	step [24/244], loss=79.5363
	step [25/244], loss=76.3882
	step [26/244], loss=81.0366
	step [27/244], loss=73.9201
	step [28/244], loss=93.5033
	step [29/244], loss=92.6445
	step [30/244], loss=76.2276
	step [31/244], loss=82.3872
	step [32/244], loss=99.5321
	step [33/244], loss=77.0834
	step [34/244], loss=105.6346
	step [35/244], loss=111.7406
	step [36/244], loss=91.6229
	step [37/244], loss=88.1653
	step [38/244], loss=91.9661
	step [39/244], loss=86.3914
	step [40/244], loss=91.1330
	step [41/244], loss=78.1373
	step [42/244], loss=97.2737
	step [43/244], loss=89.9874
	step [44/244], loss=81.5552
	step [45/244], loss=90.8139
	step [46/244], loss=89.5352
	step [47/244], loss=106.9552
	step [48/244], loss=99.9991
	step [49/244], loss=74.4881
	step [50/244], loss=114.9496
	step [51/244], loss=93.7614
	step [52/244], loss=93.6901
	step [53/244], loss=103.4282
	step [54/244], loss=93.2424
	step [55/244], loss=76.5044
	step [56/244], loss=98.6133
	step [57/244], loss=75.3775
	step [58/244], loss=88.8450
	step [59/244], loss=102.6322
	step [60/244], loss=85.3249
	step [61/244], loss=96.0847
	step [62/244], loss=88.2038
	step [63/244], loss=89.5398
	step [64/244], loss=71.6778
	step [65/244], loss=78.9132
	step [66/244], loss=92.1655
	step [67/244], loss=87.4691
	step [68/244], loss=94.3659
	step [69/244], loss=88.2906
	step [70/244], loss=103.4505
	step [71/244], loss=96.5125
	step [72/244], loss=99.8867
	step [73/244], loss=89.3118
	step [74/244], loss=81.3311
	step [75/244], loss=83.2058
	step [76/244], loss=113.2492
	step [77/244], loss=94.7552
	step [78/244], loss=77.0759
	step [79/244], loss=71.0629
	step [80/244], loss=107.3861
	step [81/244], loss=77.2385
	step [82/244], loss=81.7921
	step [83/244], loss=95.8599
	step [84/244], loss=86.1557
	step [85/244], loss=81.9420
	step [86/244], loss=98.6679
	step [87/244], loss=93.3965
	step [88/244], loss=86.0995
	step [89/244], loss=81.6846
	step [90/244], loss=84.1994
	step [91/244], loss=91.6336
	step [92/244], loss=92.0488
	step [93/244], loss=82.1926
	step [94/244], loss=80.7396
	step [95/244], loss=117.2754
	step [96/244], loss=77.7495
	step [97/244], loss=94.6071
	step [98/244], loss=93.7152
	step [99/244], loss=84.3577
	step [100/244], loss=90.4746
	step [101/244], loss=84.5834
	step [102/244], loss=87.3208
	step [103/244], loss=79.3244
	step [104/244], loss=81.9370
	step [105/244], loss=69.5666
	step [106/244], loss=97.4448
	step [107/244], loss=84.8993
	step [108/244], loss=81.8441
	step [109/244], loss=91.5892
	step [110/244], loss=77.6078
	step [111/244], loss=73.5427
	step [112/244], loss=88.6376
	step [113/244], loss=90.1783
	step [114/244], loss=97.3876
	step [115/244], loss=76.2734
	step [116/244], loss=83.9189
	step [117/244], loss=82.2323
	step [118/244], loss=107.8534
	step [119/244], loss=87.7658
	step [120/244], loss=90.3006
	step [121/244], loss=83.4800
	step [122/244], loss=85.2250
	step [123/244], loss=94.6983
	step [124/244], loss=81.5516
	step [125/244], loss=83.0549
	step [126/244], loss=107.0110
	step [127/244], loss=97.7209
	step [128/244], loss=95.4897
	step [129/244], loss=97.5188
	step [130/244], loss=102.2176
	step [131/244], loss=96.0449
	step [132/244], loss=72.2708
	step [133/244], loss=96.5782
	step [134/244], loss=83.6258
	step [135/244], loss=93.4950
	step [136/244], loss=112.9434
	step [137/244], loss=81.9511
	step [138/244], loss=70.0228
	step [139/244], loss=86.0350
	step [140/244], loss=82.7514
	step [141/244], loss=95.4416
	step [142/244], loss=91.3648
	step [143/244], loss=108.4216
	step [144/244], loss=73.6853
	step [145/244], loss=94.3555
	step [146/244], loss=87.7620
	step [147/244], loss=101.5153
	step [148/244], loss=102.6387
	step [149/244], loss=103.0044
	step [150/244], loss=90.0750
	step [151/244], loss=75.9541
	step [152/244], loss=102.0698
	step [153/244], loss=90.0262
	step [154/244], loss=84.7483
	step [155/244], loss=96.4503
	step [156/244], loss=96.0027
	step [157/244], loss=91.3525
	step [158/244], loss=81.7149
	step [159/244], loss=76.1967
	step [160/244], loss=77.0168
	step [161/244], loss=76.6597
	step [162/244], loss=69.4282
	step [163/244], loss=102.8142
	step [164/244], loss=105.5616
	step [165/244], loss=111.7333
	step [166/244], loss=71.0589
	step [167/244], loss=88.1379
	step [168/244], loss=67.2476
	step [169/244], loss=74.3679
	step [170/244], loss=90.9352
	step [171/244], loss=76.3131
	step [172/244], loss=92.6007
	step [173/244], loss=85.7113
	step [174/244], loss=95.6830
	step [175/244], loss=72.0718
	step [176/244], loss=82.3581
	step [177/244], loss=77.6351
	step [178/244], loss=95.1182
	step [179/244], loss=83.7517
	step [180/244], loss=85.9052
	step [181/244], loss=78.6764
	step [182/244], loss=94.4302
	step [183/244], loss=85.9399
	step [184/244], loss=82.5713
	step [185/244], loss=94.0184
	step [186/244], loss=87.7220
	step [187/244], loss=88.5531
	step [188/244], loss=76.5708
	step [189/244], loss=103.0499
	step [190/244], loss=98.4629
	step [191/244], loss=82.9849
	step [192/244], loss=86.0563
	step [193/244], loss=86.7781
	step [194/244], loss=95.5595
	step [195/244], loss=83.3474
	step [196/244], loss=101.5272
	step [197/244], loss=90.1013
	step [198/244], loss=86.5020
	step [199/244], loss=87.1622
	step [200/244], loss=107.6947
	step [201/244], loss=86.4563
	step [202/244], loss=87.6303
	step [203/244], loss=89.5330
	step [204/244], loss=83.0492
	step [205/244], loss=79.3701
	step [206/244], loss=82.3157
	step [207/244], loss=81.3946
	step [208/244], loss=97.1047
	step [209/244], loss=94.6317
	step [210/244], loss=91.1304
	step [211/244], loss=94.5442
	step [212/244], loss=88.8579
	step [213/244], loss=106.5501
	step [214/244], loss=97.5945
	step [215/244], loss=105.2423
	step [216/244], loss=77.1181
	step [217/244], loss=98.3352
	step [218/244], loss=98.0108
	step [219/244], loss=98.9083
	step [220/244], loss=92.6389
	step [221/244], loss=97.7766
	step [222/244], loss=92.1931
	step [223/244], loss=103.0514
	step [224/244], loss=77.5922
	step [225/244], loss=90.9543
	step [226/244], loss=100.0314
	step [227/244], loss=79.0187
	step [228/244], loss=74.0643
	step [229/244], loss=91.8674
	step [230/244], loss=97.9796
	step [231/244], loss=95.6886
	step [232/244], loss=88.8637
	step [233/244], loss=85.9999
	step [234/244], loss=77.4109
	step [235/244], loss=84.6953
	step [236/244], loss=80.3484
	step [237/244], loss=83.6329
	step [238/244], loss=82.2121
	step [239/244], loss=97.4610
	step [240/244], loss=74.5210
	step [241/244], loss=89.5158
	step [242/244], loss=113.8537
	step [243/244], loss=96.1934
	step [244/244], loss=42.6769
	Evaluating
	loss=0.0099, precision=0.3768, recall=0.8902, f1=0.5295
Training epoch 26
	step [1/244], loss=79.4645
	step [2/244], loss=99.7429
	step [3/244], loss=95.5006
	step [4/244], loss=102.0569
	step [5/244], loss=92.6892
	step [6/244], loss=96.3613
	step [7/244], loss=102.0042
	step [8/244], loss=103.3382
	step [9/244], loss=98.0968
	step [10/244], loss=108.8176
	step [11/244], loss=82.5505
	step [12/244], loss=70.6815
	step [13/244], loss=92.5984
	step [14/244], loss=82.0763
	step [15/244], loss=88.5614
	step [16/244], loss=78.2048
	step [17/244], loss=80.0160
	step [18/244], loss=83.2329
	step [19/244], loss=79.2233
	step [20/244], loss=92.3078
	step [21/244], loss=87.9412
	step [22/244], loss=81.4698
	step [23/244], loss=86.3937
	step [24/244], loss=85.3888
	step [25/244], loss=91.9227
	step [26/244], loss=82.1878
	step [27/244], loss=87.8427
	step [28/244], loss=77.3103
	step [29/244], loss=107.0226
	step [30/244], loss=72.4613
	step [31/244], loss=87.0349
	step [32/244], loss=77.5865
	step [33/244], loss=96.6803
	step [34/244], loss=89.3999
	step [35/244], loss=90.8655
	step [36/244], loss=99.1293
	step [37/244], loss=76.5721
	step [38/244], loss=82.3331
	step [39/244], loss=88.2886
	step [40/244], loss=95.6295
	step [41/244], loss=72.2331
	step [42/244], loss=102.3025
	step [43/244], loss=85.5977
	step [44/244], loss=67.8809
	step [45/244], loss=65.9438
	step [46/244], loss=80.7077
	step [47/244], loss=85.9462
	step [48/244], loss=86.2844
	step [49/244], loss=75.2122
	step [50/244], loss=86.8061
	step [51/244], loss=67.2822
	step [52/244], loss=92.3810
	step [53/244], loss=78.0651
	step [54/244], loss=88.8769
	step [55/244], loss=89.0711
	step [56/244], loss=92.8390
	step [57/244], loss=94.0065
	step [58/244], loss=82.1522
	step [59/244], loss=84.0107
	step [60/244], loss=102.6685
	step [61/244], loss=85.1134
	step [62/244], loss=66.7715
	step [63/244], loss=89.2347
	step [64/244], loss=92.1966
	step [65/244], loss=81.8968
	step [66/244], loss=73.9064
	step [67/244], loss=98.2023
	step [68/244], loss=85.8984
	step [69/244], loss=92.7468
	step [70/244], loss=87.6432
	step [71/244], loss=97.0843
	step [72/244], loss=72.8530
	step [73/244], loss=94.0044
	step [74/244], loss=88.3914
	step [75/244], loss=85.4353
	step [76/244], loss=76.4111
	step [77/244], loss=107.8808
	step [78/244], loss=85.8463
	step [79/244], loss=67.6613
	step [80/244], loss=86.2442
	step [81/244], loss=85.8523
	step [82/244], loss=92.3883
	step [83/244], loss=84.3137
	step [84/244], loss=76.9523
	step [85/244], loss=92.3159
	step [86/244], loss=82.0892
	step [87/244], loss=72.3050
	step [88/244], loss=88.7215
	step [89/244], loss=88.2014
	step [90/244], loss=85.1615
	step [91/244], loss=93.2030
	step [92/244], loss=89.9324
	step [93/244], loss=74.5949
	step [94/244], loss=98.7422
	step [95/244], loss=93.4303
	step [96/244], loss=97.8115
	step [97/244], loss=98.2919
	step [98/244], loss=86.0486
	step [99/244], loss=82.1966
	step [100/244], loss=74.8131
	step [101/244], loss=99.9177
	step [102/244], loss=100.0892
	step [103/244], loss=85.5220
	step [104/244], loss=85.5662
	step [105/244], loss=86.2096
	step [106/244], loss=86.7082
	step [107/244], loss=105.0912
	step [108/244], loss=100.9535
	step [109/244], loss=96.1447
	step [110/244], loss=88.8689
	step [111/244], loss=82.3602
	step [112/244], loss=89.8068
	step [113/244], loss=90.3044
	step [114/244], loss=106.3514
	step [115/244], loss=92.8471
	step [116/244], loss=81.8924
	step [117/244], loss=92.0085
	step [118/244], loss=94.4175
	step [119/244], loss=84.7888
	step [120/244], loss=91.0077
	step [121/244], loss=101.7396
	step [122/244], loss=86.0300
	step [123/244], loss=105.8377
	step [124/244], loss=107.2063
	step [125/244], loss=83.5132
	step [126/244], loss=85.7447
	step [127/244], loss=81.7209
	step [128/244], loss=78.4755
	step [129/244], loss=87.1176
	step [130/244], loss=82.8730
	step [131/244], loss=89.7197
	step [132/244], loss=86.0414
	step [133/244], loss=78.1266
	step [134/244], loss=89.2312
	step [135/244], loss=115.3553
	step [136/244], loss=88.7561
	step [137/244], loss=88.1473
	step [138/244], loss=101.8175
	step [139/244], loss=82.8645
	step [140/244], loss=94.7362
	step [141/244], loss=89.3913
	step [142/244], loss=88.4253
	step [143/244], loss=81.5930
	step [144/244], loss=83.9201
	step [145/244], loss=78.5553
	step [146/244], loss=75.9899
	step [147/244], loss=103.1017
	step [148/244], loss=88.0786
	step [149/244], loss=87.8789
	step [150/244], loss=94.6274
	step [151/244], loss=95.2680
	step [152/244], loss=86.5795
	step [153/244], loss=95.1606
	step [154/244], loss=88.7689
	step [155/244], loss=91.7410
	step [156/244], loss=85.8661
	step [157/244], loss=91.3925
	step [158/244], loss=86.4296
	step [159/244], loss=98.2352
	step [160/244], loss=77.9035
	step [161/244], loss=82.9031
	step [162/244], loss=102.0449
	step [163/244], loss=97.9076
	step [164/244], loss=69.7076
	step [165/244], loss=90.7172
	step [166/244], loss=85.7330
	step [167/244], loss=97.0656
	step [168/244], loss=81.7951
	step [169/244], loss=96.1210
	step [170/244], loss=104.6634
	step [171/244], loss=69.4816
	step [172/244], loss=96.6267
	step [173/244], loss=89.5789
	step [174/244], loss=95.6788
	step [175/244], loss=80.0996
	step [176/244], loss=80.7225
	step [177/244], loss=81.1491
	step [178/244], loss=93.7750
	step [179/244], loss=81.2680
	step [180/244], loss=103.8153
	step [181/244], loss=98.7801
	step [182/244], loss=82.8387
	step [183/244], loss=104.5462
	step [184/244], loss=101.2753
	step [185/244], loss=88.0163
	step [186/244], loss=101.7041
	step [187/244], loss=82.0566
	step [188/244], loss=83.0544
	step [189/244], loss=91.6827
	step [190/244], loss=77.3270
	step [191/244], loss=87.9805
	step [192/244], loss=94.2703
	step [193/244], loss=87.8950
	step [194/244], loss=85.6866
	step [195/244], loss=62.1010
	step [196/244], loss=80.0249
	step [197/244], loss=71.4767
	step [198/244], loss=86.2007
	step [199/244], loss=97.8829
	step [200/244], loss=105.6538
	step [201/244], loss=95.4635
	step [202/244], loss=80.9489
	step [203/244], loss=83.9632
	step [204/244], loss=87.3551
	step [205/244], loss=87.8636
	step [206/244], loss=87.7918
	step [207/244], loss=86.1821
	step [208/244], loss=94.0478
	step [209/244], loss=98.1396
	step [210/244], loss=77.9664
	step [211/244], loss=80.1756
	step [212/244], loss=96.8490
	step [213/244], loss=93.8734
	step [214/244], loss=81.9553
	step [215/244], loss=84.9124
	step [216/244], loss=93.0266
	step [217/244], loss=82.9668
	step [218/244], loss=90.0626
	step [219/244], loss=85.0674
	step [220/244], loss=86.4211
	step [221/244], loss=91.3078
	step [222/244], loss=80.5107
	step [223/244], loss=84.7124
	step [224/244], loss=96.1378
	step [225/244], loss=72.7177
	step [226/244], loss=86.7922
	step [227/244], loss=95.3467
	step [228/244], loss=83.3363
	step [229/244], loss=76.0068
	step [230/244], loss=95.1363
	step [231/244], loss=92.4901
	step [232/244], loss=100.7045
	step [233/244], loss=92.7664
	step [234/244], loss=89.0383
	step [235/244], loss=78.4016
	step [236/244], loss=105.9203
	step [237/244], loss=111.0333
	step [238/244], loss=92.1604
	step [239/244], loss=73.0499
	step [240/244], loss=95.7362
	step [241/244], loss=87.2680
	step [242/244], loss=98.0364
	step [243/244], loss=93.0601
	step [244/244], loss=46.8089
	Evaluating
	loss=0.0108, precision=0.3412, recall=0.9081, f1=0.4960
Training epoch 27
	step [1/244], loss=90.9672
	step [2/244], loss=72.6330
	step [3/244], loss=103.4990
	step [4/244], loss=91.2690
	step [5/244], loss=95.8068
	step [6/244], loss=91.9968
	step [7/244], loss=75.7444
	step [8/244], loss=98.7703
	step [9/244], loss=100.1359
	step [10/244], loss=81.2078
	step [11/244], loss=98.5319
	step [12/244], loss=92.9362
	step [13/244], loss=95.5255
	step [14/244], loss=79.1597
	step [15/244], loss=59.0730
	step [16/244], loss=82.8895
	step [17/244], loss=107.0547
	step [18/244], loss=80.8128
	step [19/244], loss=73.4334
	step [20/244], loss=64.1897
	step [21/244], loss=78.1705
	step [22/244], loss=88.7663
	step [23/244], loss=92.3686
	step [24/244], loss=93.9443
	step [25/244], loss=82.9800
	step [26/244], loss=95.6814
	step [27/244], loss=89.8101
	step [28/244], loss=88.2512
	step [29/244], loss=90.5135
	step [30/244], loss=71.2042
	step [31/244], loss=74.6474
	step [32/244], loss=84.0289
	step [33/244], loss=60.0278
	step [34/244], loss=73.5974
	step [35/244], loss=78.2438
	step [36/244], loss=75.1805
	step [37/244], loss=92.4183
	step [38/244], loss=82.5867
	step [39/244], loss=79.1905
	step [40/244], loss=86.6309
	step [41/244], loss=93.2698
	step [42/244], loss=94.3363
	step [43/244], loss=78.1696
	step [44/244], loss=101.0651
	step [45/244], loss=86.5632
	step [46/244], loss=91.9943
	step [47/244], loss=74.9574
	step [48/244], loss=95.0119
	step [49/244], loss=100.0969
	step [50/244], loss=93.9195
	step [51/244], loss=107.2219
	step [52/244], loss=93.5188
	step [53/244], loss=88.5027
	step [54/244], loss=85.6769
	step [55/244], loss=100.4922
	step [56/244], loss=107.6402
	step [57/244], loss=76.8936
	step [58/244], loss=73.7251
	step [59/244], loss=86.5928
	step [60/244], loss=87.1889
	step [61/244], loss=93.6451
	step [62/244], loss=101.6353
	step [63/244], loss=92.4294
	step [64/244], loss=77.8615
	step [65/244], loss=81.3644
	step [66/244], loss=100.7545
	step [67/244], loss=77.1815
	step [68/244], loss=86.9671
	step [69/244], loss=85.6928
	step [70/244], loss=64.1929
	step [71/244], loss=86.1938
	step [72/244], loss=97.6133
	step [73/244], loss=83.0986
	step [74/244], loss=95.9315
	step [75/244], loss=78.6310
	step [76/244], loss=96.1176
	step [77/244], loss=68.1635
	step [78/244], loss=83.0487
	step [79/244], loss=96.1437
	step [80/244], loss=91.8147
	step [81/244], loss=88.0999
	step [82/244], loss=85.3609
	step [83/244], loss=83.2177
	step [84/244], loss=68.2063
	step [85/244], loss=119.8181
	step [86/244], loss=80.6566
	step [87/244], loss=73.3456
	step [88/244], loss=96.6767
	step [89/244], loss=91.2247
	step [90/244], loss=86.5246
	step [91/244], loss=68.7898
	step [92/244], loss=98.6993
	step [93/244], loss=103.5147
	step [94/244], loss=87.3634
	step [95/244], loss=92.1693
	step [96/244], loss=81.2238
	step [97/244], loss=67.8428
	step [98/244], loss=117.8005
	step [99/244], loss=95.0054
	step [100/244], loss=84.2611
	step [101/244], loss=91.4522
	step [102/244], loss=84.0280
	step [103/244], loss=79.6087
	step [104/244], loss=78.3135
	step [105/244], loss=106.4919
	step [106/244], loss=96.3642
	step [107/244], loss=102.4245
	step [108/244], loss=84.3217
	step [109/244], loss=104.8463
	step [110/244], loss=77.5382
	step [111/244], loss=96.0110
	step [112/244], loss=88.0096
	step [113/244], loss=75.7628
	step [114/244], loss=88.1044
	step [115/244], loss=100.5984
	step [116/244], loss=82.5080
	step [117/244], loss=74.1652
	step [118/244], loss=94.0739
	step [119/244], loss=99.7550
	step [120/244], loss=91.5795
	step [121/244], loss=102.4639
	step [122/244], loss=104.8023
	step [123/244], loss=78.7729
	step [124/244], loss=75.7788
	step [125/244], loss=83.3618
	step [126/244], loss=93.9529
	step [127/244], loss=99.4062
	step [128/244], loss=90.8570
	step [129/244], loss=70.7266
	step [130/244], loss=102.6208
	step [131/244], loss=75.2772
	step [132/244], loss=91.8336
	step [133/244], loss=95.7450
	step [134/244], loss=89.0573
	step [135/244], loss=91.6163
	step [136/244], loss=77.0756
	step [137/244], loss=81.9625
	step [138/244], loss=98.9459
	step [139/244], loss=86.4260
	step [140/244], loss=89.2696
	step [141/244], loss=79.8411
	step [142/244], loss=84.6391
	step [143/244], loss=101.4940
	step [144/244], loss=85.7953
	step [145/244], loss=80.2991
	step [146/244], loss=76.8735
	step [147/244], loss=87.6854
	step [148/244], loss=82.9271
	step [149/244], loss=99.3464
	step [150/244], loss=87.8575
	step [151/244], loss=81.3262
	step [152/244], loss=90.1452
	step [153/244], loss=107.8965
	step [154/244], loss=93.2635
	step [155/244], loss=86.3139
	step [156/244], loss=100.3562
	step [157/244], loss=102.4156
	step [158/244], loss=88.0097
	step [159/244], loss=83.5366
	step [160/244], loss=79.8548
	step [161/244], loss=88.7151
	step [162/244], loss=108.4265
	step [163/244], loss=93.4786
	step [164/244], loss=80.0895
	step [165/244], loss=75.4104
	step [166/244], loss=88.7010
	step [167/244], loss=74.2414
	step [168/244], loss=84.0792
	step [169/244], loss=95.4588
	step [170/244], loss=82.2444
	step [171/244], loss=103.4889
	step [172/244], loss=101.9076
	step [173/244], loss=78.8369
	step [174/244], loss=79.6765
	step [175/244], loss=94.2532
	step [176/244], loss=104.7019
	step [177/244], loss=78.2470
	step [178/244], loss=83.9778
	step [179/244], loss=95.0675
	step [180/244], loss=84.2686
	step [181/244], loss=98.3163
	step [182/244], loss=101.9065
	step [183/244], loss=90.3553
	step [184/244], loss=97.4770
	step [185/244], loss=68.0217
	step [186/244], loss=98.7061
	step [187/244], loss=95.7268
	step [188/244], loss=84.0868
	step [189/244], loss=85.6193
	step [190/244], loss=87.9231
	step [191/244], loss=92.8218
	step [192/244], loss=79.8117
	step [193/244], loss=83.4419
	step [194/244], loss=95.6441
	step [195/244], loss=103.7091
	step [196/244], loss=82.5714
	step [197/244], loss=107.0131
	step [198/244], loss=94.8570
	step [199/244], loss=74.0071
	step [200/244], loss=87.6391
	step [201/244], loss=96.5684
	step [202/244], loss=66.6224
	step [203/244], loss=82.0412
	step [204/244], loss=84.2019
	step [205/244], loss=74.9100
	step [206/244], loss=71.3152
	step [207/244], loss=79.4847
	step [208/244], loss=76.4897
	step [209/244], loss=92.8803
	step [210/244], loss=90.4172
	step [211/244], loss=75.5308
	step [212/244], loss=81.9462
	step [213/244], loss=103.6025
	step [214/244], loss=111.9229
	step [215/244], loss=86.7167
	step [216/244], loss=82.8013
	step [217/244], loss=97.2500
	step [218/244], loss=95.4271
	step [219/244], loss=107.8782
	step [220/244], loss=78.8504
	step [221/244], loss=89.0134
	step [222/244], loss=85.6445
	step [223/244], loss=71.0852
	step [224/244], loss=85.2525
	step [225/244], loss=75.0217
	step [226/244], loss=88.3319
	step [227/244], loss=69.5455
	step [228/244], loss=90.9839
	step [229/244], loss=72.6134
	step [230/244], loss=77.2978
	step [231/244], loss=89.3634
	step [232/244], loss=83.5209
	step [233/244], loss=89.4833
	step [234/244], loss=98.4900
	step [235/244], loss=59.4823
	step [236/244], loss=96.5676
	step [237/244], loss=84.5112
	step [238/244], loss=73.7680
	step [239/244], loss=80.3924
	step [240/244], loss=96.8722
	step [241/244], loss=78.0334
	step [242/244], loss=86.8079
	step [243/244], loss=96.5931
	step [244/244], loss=40.0694
	Evaluating
	loss=0.0086, precision=0.4249, recall=0.8877, f1=0.5748
Training epoch 28
	step [1/244], loss=85.2978
	step [2/244], loss=103.4005
	step [3/244], loss=84.0874
	step [4/244], loss=75.8788
	step [5/244], loss=75.9211
	step [6/244], loss=86.5466
	step [7/244], loss=102.9916
	step [8/244], loss=87.3313
	step [9/244], loss=91.0642
	step [10/244], loss=103.2229
	step [11/244], loss=83.9798
	step [12/244], loss=75.8131
	step [13/244], loss=91.4782
	step [14/244], loss=89.0566
	step [15/244], loss=87.4755
	step [16/244], loss=97.2429
	step [17/244], loss=88.9418
	step [18/244], loss=96.8224
	step [19/244], loss=91.5982
	step [20/244], loss=82.8197
	step [21/244], loss=93.4669
	step [22/244], loss=81.3523
	step [23/244], loss=103.8698
	step [24/244], loss=92.0847
	step [25/244], loss=115.0793
	step [26/244], loss=89.2786
	step [27/244], loss=76.4556
	step [28/244], loss=108.5850
	step [29/244], loss=86.0665
	step [30/244], loss=75.7270
	step [31/244], loss=85.3962
	step [32/244], loss=97.7041
	step [33/244], loss=87.2030
	step [34/244], loss=77.5820
	step [35/244], loss=110.8264
	step [36/244], loss=92.4383
	step [37/244], loss=112.3553
	step [38/244], loss=101.3070
	step [39/244], loss=88.6187
	step [40/244], loss=85.0346
	step [41/244], loss=90.8417
	step [42/244], loss=104.2447
	step [43/244], loss=74.5333
	step [44/244], loss=107.7783
	step [45/244], loss=76.4927
	step [46/244], loss=78.3629
	step [47/244], loss=75.0489
	step [48/244], loss=92.8319
	step [49/244], loss=87.2428
	step [50/244], loss=90.4475
	step [51/244], loss=93.7167
	step [52/244], loss=88.5888
	step [53/244], loss=84.8597
	step [54/244], loss=77.4486
	step [55/244], loss=90.8933
	step [56/244], loss=80.3966
	step [57/244], loss=79.4526
	step [58/244], loss=75.3230
	step [59/244], loss=84.0879
	step [60/244], loss=77.1221
	step [61/244], loss=75.5483
	step [62/244], loss=97.1826
	step [63/244], loss=90.7721
	step [64/244], loss=91.3091
	step [65/244], loss=73.3693
	step [66/244], loss=84.3163
	step [67/244], loss=72.4194
	step [68/244], loss=99.8320
	step [69/244], loss=84.7359
	step [70/244], loss=95.6415
	step [71/244], loss=86.8522
	step [72/244], loss=95.3970
	step [73/244], loss=77.5427
	step [74/244], loss=97.5568
	step [75/244], loss=74.8955
	step [76/244], loss=97.1607
	step [77/244], loss=87.1551
	step [78/244], loss=79.6648
	step [79/244], loss=77.8826
	step [80/244], loss=79.8837
	step [81/244], loss=84.9417
	step [82/244], loss=100.0955
	step [83/244], loss=92.5860
	step [84/244], loss=95.2604
	step [85/244], loss=78.0478
	step [86/244], loss=117.2011
	step [87/244], loss=76.9565
	step [88/244], loss=82.3854
	step [89/244], loss=85.0260
	step [90/244], loss=70.1147
	step [91/244], loss=85.6184
	step [92/244], loss=74.9734
	step [93/244], loss=90.3260
	step [94/244], loss=71.2309
	step [95/244], loss=80.0230
	step [96/244], loss=85.9814
	step [97/244], loss=100.4105
	step [98/244], loss=94.4436
	step [99/244], loss=87.9983
	step [100/244], loss=84.5417
	step [101/244], loss=83.5620
	step [102/244], loss=98.0334
	step [103/244], loss=85.2200
	step [104/244], loss=90.9145
	step [105/244], loss=92.2065
	step [106/244], loss=93.5888
	step [107/244], loss=72.8573
	step [108/244], loss=101.4421
	step [109/244], loss=88.9021
	step [110/244], loss=89.0847
	step [111/244], loss=86.4374
	step [112/244], loss=97.6204
	step [113/244], loss=81.9788
	step [114/244], loss=90.5452
	step [115/244], loss=92.9956
	step [116/244], loss=105.1384
	step [117/244], loss=92.5284
	step [118/244], loss=77.2899
	step [119/244], loss=82.2907
	step [120/244], loss=76.7327
	step [121/244], loss=86.3632
	step [122/244], loss=72.5567
	step [123/244], loss=91.7951
	step [124/244], loss=65.6548
	step [125/244], loss=89.7300
	step [126/244], loss=91.8388
	step [127/244], loss=93.9996
	step [128/244], loss=88.5884
	step [129/244], loss=77.7597
	step [130/244], loss=80.1824
	step [131/244], loss=84.0663
	step [132/244], loss=94.2848
	step [133/244], loss=101.2685
	step [134/244], loss=92.3722
	step [135/244], loss=85.8948
	step [136/244], loss=96.6074
	step [137/244], loss=76.6441
	step [138/244], loss=91.2409
	step [139/244], loss=83.9061
	step [140/244], loss=92.0820
	step [141/244], loss=106.9515
	step [142/244], loss=95.0064
	step [143/244], loss=89.0555
	step [144/244], loss=111.5311
	step [145/244], loss=74.1388
	step [146/244], loss=87.5563
	step [147/244], loss=97.3189
	step [148/244], loss=78.1961
	step [149/244], loss=112.8644
	step [150/244], loss=69.3359
	step [151/244], loss=78.5527
	step [152/244], loss=88.3408
	step [153/244], loss=81.3153
	step [154/244], loss=92.5705
	step [155/244], loss=88.1316
	step [156/244], loss=79.0352
	step [157/244], loss=84.3354
	step [158/244], loss=82.7079
	step [159/244], loss=90.4617
	step [160/244], loss=85.5953
	step [161/244], loss=86.2791
	step [162/244], loss=92.6947
	step [163/244], loss=76.1655
	step [164/244], loss=65.9788
	step [165/244], loss=72.6959
	step [166/244], loss=77.1874
	step [167/244], loss=91.4840
	step [168/244], loss=89.5046
	step [169/244], loss=89.3916
	step [170/244], loss=93.7061
	step [171/244], loss=97.4702
	step [172/244], loss=86.2919
	step [173/244], loss=62.6545
	step [174/244], loss=88.8502
	step [175/244], loss=77.9352
	step [176/244], loss=75.8786
	step [177/244], loss=78.5000
	step [178/244], loss=89.0315
	step [179/244], loss=89.3797
	step [180/244], loss=79.6200
	step [181/244], loss=79.7056
	step [182/244], loss=84.6577
	step [183/244], loss=77.9110
	step [184/244], loss=65.7202
	step [185/244], loss=82.6514
	step [186/244], loss=81.3006
	step [187/244], loss=95.0281
	step [188/244], loss=79.1212
	step [189/244], loss=108.0856
	step [190/244], loss=91.7133
	step [191/244], loss=83.5395
	step [192/244], loss=79.1082
	step [193/244], loss=89.3731
	step [194/244], loss=90.4910
	step [195/244], loss=84.5279
	step [196/244], loss=81.3627
	step [197/244], loss=100.1060
	step [198/244], loss=71.0098
	step [199/244], loss=99.5700
	step [200/244], loss=81.9504
	step [201/244], loss=78.8041
	step [202/244], loss=98.6223
	step [203/244], loss=74.4548
	step [204/244], loss=80.5809
	step [205/244], loss=83.4636
	step [206/244], loss=97.3734
	step [207/244], loss=89.1349
	step [208/244], loss=91.9180
	step [209/244], loss=105.9534
	step [210/244], loss=88.3492
	step [211/244], loss=87.4455
	step [212/244], loss=90.9447
	step [213/244], loss=82.1991
	step [214/244], loss=74.7104
	step [215/244], loss=87.5718
	step [216/244], loss=105.0201
	step [217/244], loss=77.1161
	step [218/244], loss=89.8642
	step [219/244], loss=86.7056
	step [220/244], loss=86.6383
	step [221/244], loss=73.0997
	step [222/244], loss=79.9036
	step [223/244], loss=91.0116
	step [224/244], loss=89.9045
	step [225/244], loss=77.5128
	step [226/244], loss=68.1008
	step [227/244], loss=81.9484
	step [228/244], loss=90.0715
	step [229/244], loss=82.5409
	step [230/244], loss=98.2729
	step [231/244], loss=93.5904
	step [232/244], loss=88.3000
	step [233/244], loss=77.1905
	step [234/244], loss=102.7942
	step [235/244], loss=72.3138
	step [236/244], loss=71.0334
	step [237/244], loss=85.1073
	step [238/244], loss=87.4815
	step [239/244], loss=83.4303
	step [240/244], loss=88.3872
	step [241/244], loss=68.7473
	step [242/244], loss=53.9412
	step [243/244], loss=85.0948
	step [244/244], loss=31.6052
	Evaluating
	loss=0.0069, precision=0.5075, recall=0.8880, f1=0.6458
Training epoch 29
	step [1/244], loss=89.8033
	step [2/244], loss=85.4090
	step [3/244], loss=77.6238
	step [4/244], loss=84.3825
	step [5/244], loss=96.3802
	step [6/244], loss=72.8651
	step [7/244], loss=77.1965
	step [8/244], loss=100.2192
	step [9/244], loss=87.6582
	step [10/244], loss=76.9853
	step [11/244], loss=76.3952
	step [12/244], loss=89.6421
	step [13/244], loss=88.1803
	step [14/244], loss=88.0927
	step [15/244], loss=89.7221
	step [16/244], loss=81.5306
	step [17/244], loss=83.0390
	step [18/244], loss=70.2087
	step [19/244], loss=71.8486
	step [20/244], loss=85.4531
	step [21/244], loss=80.9920
	step [22/244], loss=72.8266
	step [23/244], loss=99.4032
	step [24/244], loss=77.3842
	step [25/244], loss=93.6966
	step [26/244], loss=80.9457
	step [27/244], loss=92.7004
	step [28/244], loss=94.7784
	step [29/244], loss=88.2488
	step [30/244], loss=92.4966
	step [31/244], loss=88.1242
	step [32/244], loss=81.9394
	step [33/244], loss=80.2311
	step [34/244], loss=78.4120
	step [35/244], loss=102.4038
	step [36/244], loss=91.3213
	step [37/244], loss=89.5733
	step [38/244], loss=93.0624
	step [39/244], loss=98.9485
	step [40/244], loss=83.6597
	step [41/244], loss=71.5540
	step [42/244], loss=82.9408
	step [43/244], loss=86.6949
	step [44/244], loss=88.1514
	step [45/244], loss=96.8211
	step [46/244], loss=79.1874
	step [47/244], loss=79.6648
	step [48/244], loss=77.9818
	step [49/244], loss=75.7178
	step [50/244], loss=67.7444
	step [51/244], loss=95.8922
	step [52/244], loss=93.1403
	step [53/244], loss=76.8630
	step [54/244], loss=88.1463
	step [55/244], loss=100.7059
	step [56/244], loss=82.4818
	step [57/244], loss=101.7616
	step [58/244], loss=84.0682
	step [59/244], loss=80.9208
	step [60/244], loss=87.0638
	step [61/244], loss=96.0429
	step [62/244], loss=95.9072
	step [63/244], loss=83.3554
	step [64/244], loss=91.1515
	step [65/244], loss=89.8290
	step [66/244], loss=89.6854
	step [67/244], loss=85.7735
	step [68/244], loss=91.5919
	step [69/244], loss=74.5550
	step [70/244], loss=90.7540
	step [71/244], loss=79.3138
	step [72/244], loss=80.1297
	step [73/244], loss=82.3399
	step [74/244], loss=71.7497
	step [75/244], loss=94.5308
	step [76/244], loss=80.7894
	step [77/244], loss=106.9515
	step [78/244], loss=75.9354
	step [79/244], loss=91.7580
	step [80/244], loss=92.3539
	step [81/244], loss=82.5033
	step [82/244], loss=82.6172
	step [83/244], loss=97.6469
	step [84/244], loss=86.5458
	step [85/244], loss=89.7635
	step [86/244], loss=66.6019
	step [87/244], loss=88.7846
	step [88/244], loss=73.1674
	step [89/244], loss=103.2029
	step [90/244], loss=77.8464
	step [91/244], loss=79.3202
	step [92/244], loss=111.1720
	step [93/244], loss=79.2717
	step [94/244], loss=83.3986
	step [95/244], loss=85.3373
	step [96/244], loss=78.8709
	step [97/244], loss=75.2707
	step [98/244], loss=87.5636
	step [99/244], loss=85.3758
	step [100/244], loss=79.5152
	step [101/244], loss=83.2275
	step [102/244], loss=83.1738
	step [103/244], loss=76.7943
	step [104/244], loss=90.7470
	step [105/244], loss=77.2202
	step [106/244], loss=77.9609
	step [107/244], loss=73.7265
	step [108/244], loss=64.6126
	step [109/244], loss=85.2989
	step [110/244], loss=67.5700
	step [111/244], loss=111.9521
	step [112/244], loss=98.5133
	step [113/244], loss=75.2172
	step [114/244], loss=91.1606
	step [115/244], loss=84.1875
	step [116/244], loss=83.2227
	step [117/244], loss=82.8325
	step [118/244], loss=83.4347
	step [119/244], loss=93.2116
	step [120/244], loss=80.4531
	step [121/244], loss=86.7552
	step [122/244], loss=90.8381
	step [123/244], loss=98.0612
	step [124/244], loss=93.0068
	step [125/244], loss=95.3634
	step [126/244], loss=102.2651
	step [127/244], loss=95.7256
	step [128/244], loss=82.1857
	step [129/244], loss=87.1396
	step [130/244], loss=79.9679
	step [131/244], loss=86.5822
	step [132/244], loss=111.8267
	step [133/244], loss=77.3204
	step [134/244], loss=91.3751
	step [135/244], loss=93.3520
	step [136/244], loss=69.3354
	step [137/244], loss=98.8479
	step [138/244], loss=95.6109
	step [139/244], loss=88.6711
	step [140/244], loss=97.3658
	step [141/244], loss=64.3949
	step [142/244], loss=80.9991
	step [143/244], loss=100.5325
	step [144/244], loss=66.0915
	step [145/244], loss=79.5952
	step [146/244], loss=89.0235
	step [147/244], loss=78.2461
	step [148/244], loss=88.0680
	step [149/244], loss=79.7028
	step [150/244], loss=97.1736
	step [151/244], loss=93.2442
	step [152/244], loss=76.5228
	step [153/244], loss=82.5125
	step [154/244], loss=85.7347
	step [155/244], loss=86.5334
	step [156/244], loss=95.5358
	step [157/244], loss=93.6761
	step [158/244], loss=85.4034
	step [159/244], loss=72.0100
	step [160/244], loss=82.1617
	step [161/244], loss=84.8621
	step [162/244], loss=86.0576
	step [163/244], loss=82.8577
	step [164/244], loss=101.1275
	step [165/244], loss=81.3375
	step [166/244], loss=94.2239
	step [167/244], loss=71.5973
	step [168/244], loss=86.7764
	step [169/244], loss=86.4154
	step [170/244], loss=98.8729
	step [171/244], loss=89.7985
	step [172/244], loss=67.3640
	step [173/244], loss=81.6753
	step [174/244], loss=90.9869
	step [175/244], loss=89.6518
	step [176/244], loss=101.3787
	step [177/244], loss=94.0099
	step [178/244], loss=88.1539
	step [179/244], loss=88.1138
	step [180/244], loss=76.2955
	step [181/244], loss=88.0326
	step [182/244], loss=75.9251
	step [183/244], loss=81.9965
	step [184/244], loss=86.3037
	step [185/244], loss=95.6309
	step [186/244], loss=62.8458
	step [187/244], loss=78.8993
	step [188/244], loss=90.7145
	step [189/244], loss=79.8029
	step [190/244], loss=77.4260
	step [191/244], loss=83.8284
	step [192/244], loss=90.3669
	step [193/244], loss=96.1286
	step [194/244], loss=86.5069
	step [195/244], loss=89.2925
	step [196/244], loss=94.9461
	step [197/244], loss=90.6617
	step [198/244], loss=74.5998
	step [199/244], loss=81.4370
	step [200/244], loss=99.3807
	step [201/244], loss=90.6403
	step [202/244], loss=86.6061
	step [203/244], loss=78.8311
	step [204/244], loss=90.1500
	step [205/244], loss=79.6356
	step [206/244], loss=98.3800
	step [207/244], loss=92.0377
	step [208/244], loss=83.7064
	step [209/244], loss=100.2711
	step [210/244], loss=82.4245
	step [211/244], loss=91.1063
	step [212/244], loss=82.9886
	step [213/244], loss=86.2839
	step [214/244], loss=87.5736
	step [215/244], loss=87.2944
	step [216/244], loss=69.0016
	step [217/244], loss=86.3730
	step [218/244], loss=94.5827
	step [219/244], loss=96.2599
	step [220/244], loss=73.2736
	step [221/244], loss=81.5265
	step [222/244], loss=82.9314
	step [223/244], loss=85.5548
	step [224/244], loss=79.6240
	step [225/244], loss=83.1123
	step [226/244], loss=78.1712
	step [227/244], loss=101.2466
	step [228/244], loss=67.7364
	step [229/244], loss=97.0782
	step [230/244], loss=80.4393
	step [231/244], loss=95.4700
	step [232/244], loss=100.2775
	step [233/244], loss=75.1629
	step [234/244], loss=98.6250
	step [235/244], loss=87.8993
	step [236/244], loss=76.5296
	step [237/244], loss=86.7399
	step [238/244], loss=98.6584
	step [239/244], loss=94.0110
	step [240/244], loss=75.2086
	step [241/244], loss=99.4771
	step [242/244], loss=91.4973
	step [243/244], loss=83.1225
	step [244/244], loss=35.3947
	Evaluating
	loss=0.0088, precision=0.4039, recall=0.8896, f1=0.5556
Training epoch 30
	step [1/244], loss=80.4667
	step [2/244], loss=106.5189
	step [3/244], loss=84.5563
	step [4/244], loss=83.6768
	step [5/244], loss=80.3395
	step [6/244], loss=71.9359
	step [7/244], loss=79.3022
	step [8/244], loss=94.6935
	step [9/244], loss=86.7308
	step [10/244], loss=87.7914
	step [11/244], loss=87.1008
	step [12/244], loss=92.0356
	step [13/244], loss=80.2359
	step [14/244], loss=84.9550
	step [15/244], loss=70.8452
	step [16/244], loss=74.9284
	step [17/244], loss=90.4185
	step [18/244], loss=87.6161
	step [19/244], loss=83.1847
	step [20/244], loss=82.0160
	step [21/244], loss=76.0533
	step [22/244], loss=61.4012
	step [23/244], loss=94.3826
	step [24/244], loss=84.4005
	step [25/244], loss=80.2231
	step [26/244], loss=84.0967
	step [27/244], loss=75.7184
	step [28/244], loss=77.5272
	step [29/244], loss=85.0991
	step [30/244], loss=89.9000
	step [31/244], loss=99.4188
	step [32/244], loss=86.9693
	step [33/244], loss=88.6802
	step [34/244], loss=84.8637
	step [35/244], loss=90.4381
	step [36/244], loss=79.8627
	step [37/244], loss=91.5747
	step [38/244], loss=79.8879
	step [39/244], loss=95.4187
	step [40/244], loss=75.0446
	step [41/244], loss=77.8898
	step [42/244], loss=61.4058
	step [43/244], loss=82.4096
	step [44/244], loss=80.6750
	step [45/244], loss=78.4209
	step [46/244], loss=90.4553
	step [47/244], loss=81.6424
	step [48/244], loss=74.5574
	step [49/244], loss=96.3990
	step [50/244], loss=59.5516
	step [51/244], loss=81.2627
	step [52/244], loss=82.9419
	step [53/244], loss=85.3598
	step [54/244], loss=86.6420
	step [55/244], loss=73.8157
	step [56/244], loss=74.3314
	step [57/244], loss=97.8931
	step [58/244], loss=67.7585
	step [59/244], loss=77.7619
	step [60/244], loss=70.6511
	step [61/244], loss=73.0759
	step [62/244], loss=85.4144
	step [63/244], loss=87.2830
	step [64/244], loss=70.1131
	step [65/244], loss=79.5113
	step [66/244], loss=106.9074
	step [67/244], loss=79.1893
	step [68/244], loss=77.1601
	step [69/244], loss=96.0983
	step [70/244], loss=89.8729
	step [71/244], loss=81.2183
	step [72/244], loss=83.4556
	step [73/244], loss=92.2684
	step [74/244], loss=70.8656
	step [75/244], loss=97.5521
	step [76/244], loss=80.9760
	step [77/244], loss=92.7944
	step [78/244], loss=75.5183
	step [79/244], loss=70.4309
	step [80/244], loss=77.2741
	step [81/244], loss=102.2546
	step [82/244], loss=94.3464
	step [83/244], loss=88.1015
	step [84/244], loss=87.1024
	step [85/244], loss=79.1573
	step [86/244], loss=80.4691
	step [87/244], loss=67.1863
	step [88/244], loss=61.3904
	step [89/244], loss=80.5123
	step [90/244], loss=82.9680
	step [91/244], loss=88.8661
	step [92/244], loss=94.8917
	step [93/244], loss=97.4478
	step [94/244], loss=87.3789
	step [95/244], loss=91.4877
	step [96/244], loss=102.4326
	step [97/244], loss=78.7842
	step [98/244], loss=75.0677
	step [99/244], loss=83.7886
	step [100/244], loss=110.5609
	step [101/244], loss=67.3190
	step [102/244], loss=76.9335
	step [103/244], loss=102.3845
	step [104/244], loss=76.0249
	step [105/244], loss=90.8225
	step [106/244], loss=78.0912
	step [107/244], loss=85.0213
	step [108/244], loss=85.0770
	step [109/244], loss=96.8557
	step [110/244], loss=97.6208
	step [111/244], loss=87.1488
	step [112/244], loss=91.7694
	step [113/244], loss=87.1309
	step [114/244], loss=86.2994
	step [115/244], loss=92.8934
	step [116/244], loss=80.5184
	step [117/244], loss=89.2473
	step [118/244], loss=107.8293
	step [119/244], loss=90.0992
	step [120/244], loss=94.1796
	step [121/244], loss=92.6873
	step [122/244], loss=87.8554
	step [123/244], loss=79.2832
	step [124/244], loss=90.8115
	step [125/244], loss=73.3972
	step [126/244], loss=76.4867
	step [127/244], loss=90.6662
	step [128/244], loss=80.9828
	step [129/244], loss=104.6899
	step [130/244], loss=92.6570
	step [131/244], loss=78.3269
	step [132/244], loss=106.5763
	step [133/244], loss=104.9095
	step [134/244], loss=71.9606
	step [135/244], loss=75.3524
	step [136/244], loss=105.0980
	step [137/244], loss=83.5108
	step [138/244], loss=90.4524
	step [139/244], loss=97.2609
	step [140/244], loss=79.4666
	step [141/244], loss=82.9294
	step [142/244], loss=86.8010
	step [143/244], loss=93.8813
	step [144/244], loss=80.8301
	step [145/244], loss=86.3891
	step [146/244], loss=81.6352
	step [147/244], loss=80.7845
	step [148/244], loss=83.4152
	step [149/244], loss=81.7158
	step [150/244], loss=104.5179
	step [151/244], loss=84.0149
	step [152/244], loss=81.2698
	step [153/244], loss=100.3644
	step [154/244], loss=87.3448
	step [155/244], loss=92.4797
	step [156/244], loss=87.9426
	step [157/244], loss=87.1694
	step [158/244], loss=88.0344
	step [159/244], loss=77.6510
	step [160/244], loss=71.8484
	step [161/244], loss=99.8083
	step [162/244], loss=99.8180
	step [163/244], loss=90.9696
	step [164/244], loss=93.3235
	step [165/244], loss=88.7842
	step [166/244], loss=91.4922
	step [167/244], loss=77.6290
	step [168/244], loss=91.3750
	step [169/244], loss=79.6720
	step [170/244], loss=94.0752
	step [171/244], loss=79.0182
	step [172/244], loss=89.2930
	step [173/244], loss=75.0257
	step [174/244], loss=82.5086
	step [175/244], loss=80.1498
	step [176/244], loss=82.2179
	step [177/244], loss=88.7751
	step [178/244], loss=90.4291
	step [179/244], loss=101.5346
	step [180/244], loss=78.1901
	step [181/244], loss=95.6582
	step [182/244], loss=80.7014
	step [183/244], loss=74.3610
	step [184/244], loss=88.8632
	step [185/244], loss=88.3729
	step [186/244], loss=81.4480
	step [187/244], loss=85.4211
	step [188/244], loss=84.0679
	step [189/244], loss=102.8968
	step [190/244], loss=78.4690
	step [191/244], loss=80.0973
	step [192/244], loss=86.0538
	step [193/244], loss=74.8486
	step [194/244], loss=81.8810
	step [195/244], loss=86.2395
	step [196/244], loss=87.9596
	step [197/244], loss=106.8796
	step [198/244], loss=78.0147
	step [199/244], loss=110.9805
	step [200/244], loss=74.4437
	step [201/244], loss=98.2020
	step [202/244], loss=82.8810
	step [203/244], loss=72.7725
	step [204/244], loss=72.9972
	step [205/244], loss=72.1210
	step [206/244], loss=77.7830
	step [207/244], loss=78.9741
	step [208/244], loss=66.0947
	step [209/244], loss=77.0522
	step [210/244], loss=79.5871
	step [211/244], loss=74.7607
	step [212/244], loss=104.0934
	step [213/244], loss=85.2919
	step [214/244], loss=83.4487
	step [215/244], loss=75.9803
	step [216/244], loss=67.6295
	step [217/244], loss=83.1817
	step [218/244], loss=83.7492
	step [219/244], loss=82.5640
	step [220/244], loss=96.7070
	step [221/244], loss=86.9276
	step [222/244], loss=76.0272
	step [223/244], loss=81.5069
	step [224/244], loss=87.7115
	step [225/244], loss=67.4838
	step [226/244], loss=82.0395
	step [227/244], loss=85.9191
	step [228/244], loss=82.9265
	step [229/244], loss=93.9406
	step [230/244], loss=107.0363
	step [231/244], loss=92.3601
	step [232/244], loss=74.9170
	step [233/244], loss=64.6035
	step [234/244], loss=79.2748
	step [235/244], loss=81.2840
	step [236/244], loss=76.9639
	step [237/244], loss=97.5405
	step [238/244], loss=84.1862
	step [239/244], loss=90.2651
	step [240/244], loss=100.6877
	step [241/244], loss=103.6584
	step [242/244], loss=94.8048
	step [243/244], loss=76.3500
	step [244/244], loss=27.0601
	Evaluating
	loss=0.0092, precision=0.3546, recall=0.8982, f1=0.5085
Training finished
best_f1: 0.6730094682562255
directing: Z rim_enhanced: True test_id 1
removed wrong scan: weights_Z_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_278_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_276_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_298_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_210_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_262_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_254_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_58_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_293_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_291_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_206_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_252_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_265_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_295_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_221_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_292_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_294_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_297_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_214_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_296_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_225_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_300_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_243_xwqg-A00121_2019-05-15.npy
# all image files: 15579 # all weight files in weight_dir: 12192 # image files with weight 12153
removed wrong scan: weights_Z_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_278_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_276_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_298_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_210_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_262_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_254_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_58_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_293_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_291_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_206_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_252_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_265_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_295_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_221_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_292_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_294_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_297_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_214_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_296_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_225_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_300_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_243_xwqg-A00121_2019-05-15.npy
# all image files: 15579 # all weight files in weight_dir: 3352 # image files with weight 3331
train: /ibex/scratch/projects/c2052/air_tube_seg/training_samples_enhanced_one/Z 12153
Using 4 GPUs
Going to train epochs [1-30]
Training epoch 1
	step [1/190], loss=383.0634
	step [2/190], loss=327.5765
	step [3/190], loss=315.5516
	step [4/190], loss=280.9969
	step [5/190], loss=292.9546
	step [6/190], loss=283.3924
	step [7/190], loss=262.5409
	step [8/190], loss=258.8583
	step [9/190], loss=241.7546
	step [10/190], loss=236.4432
	step [11/190], loss=234.6170
	step [12/190], loss=252.1415
	step [13/190], loss=250.4188
	step [14/190], loss=225.5320
	step [15/190], loss=245.5290
	step [16/190], loss=243.0535
	step [17/190], loss=219.0152
	step [18/190], loss=224.8575
	step [19/190], loss=214.4710
	step [20/190], loss=257.7210
	step [21/190], loss=224.3241
	step [22/190], loss=232.7765
	step [23/190], loss=204.2796
	step [24/190], loss=212.5921
	step [25/190], loss=235.4605
	step [26/190], loss=205.5243
	step [27/190], loss=251.8467
	step [28/190], loss=222.5138
	step [29/190], loss=206.7517
	step [30/190], loss=220.0271
	step [31/190], loss=207.0912
	step [32/190], loss=219.9437
	step [33/190], loss=215.5781
	step [34/190], loss=190.9005
	step [35/190], loss=223.8612
	step [36/190], loss=209.0806
	step [37/190], loss=211.3290
	step [38/190], loss=198.1110
	step [39/190], loss=191.2633
	step [40/190], loss=190.1053
	step [41/190], loss=200.6379
	step [42/190], loss=213.4434
	step [43/190], loss=191.9870
	step [44/190], loss=211.4196
	step [45/190], loss=194.7813
	step [46/190], loss=201.1550
	step [47/190], loss=197.2310
	step [48/190], loss=182.2676
	step [49/190], loss=189.9000
	step [50/190], loss=187.2547
	step [51/190], loss=187.1323
	step [52/190], loss=186.9160
	step [53/190], loss=186.5495
	step [54/190], loss=183.6137
	step [55/190], loss=202.5013
	step [56/190], loss=180.4964
	step [57/190], loss=198.9218
	step [58/190], loss=194.3299
	step [59/190], loss=208.5028
	step [60/190], loss=174.8896
	step [61/190], loss=193.8303
	step [62/190], loss=215.3665
	step [63/190], loss=177.5165
	step [64/190], loss=195.9326
	step [65/190], loss=186.7923
	step [66/190], loss=181.2467
	step [67/190], loss=172.8787
	step [68/190], loss=185.1396
	step [69/190], loss=188.7346
	step [70/190], loss=197.8787
	step [71/190], loss=162.2653
	step [72/190], loss=182.6224
	step [73/190], loss=183.7270
	step [74/190], loss=174.1480
	step [75/190], loss=176.3676
	step [76/190], loss=186.5154
	step [77/190], loss=179.1798
	step [78/190], loss=163.4596
	step [79/190], loss=178.1639
	step [80/190], loss=177.8464
	step [81/190], loss=185.7190
	step [82/190], loss=173.3993
	step [83/190], loss=174.4543
	step [84/190], loss=185.6736
	step [85/190], loss=165.1832
	step [86/190], loss=182.0220
	step [87/190], loss=166.7820
	step [88/190], loss=184.6937
	step [89/190], loss=171.4951
	step [90/190], loss=174.1364
	step [91/190], loss=168.3206
	step [92/190], loss=165.4540
	step [93/190], loss=169.2251
	step [94/190], loss=186.0552
	step [95/190], loss=166.8037
	step [96/190], loss=185.4240
	step [97/190], loss=160.3509
	step [98/190], loss=180.4634
	step [99/190], loss=167.2620
	step [100/190], loss=183.1264
	step [101/190], loss=180.2150
	step [102/190], loss=154.3947
	step [103/190], loss=161.2254
	step [104/190], loss=161.3911
	step [105/190], loss=177.6376
	step [106/190], loss=165.2877
	step [107/190], loss=158.5795
	step [108/190], loss=179.7740
	step [109/190], loss=165.8214
	step [110/190], loss=174.0442
	step [111/190], loss=173.6344
	step [112/190], loss=160.7018
	step [113/190], loss=158.4886
	step [114/190], loss=174.1297
	step [115/190], loss=148.5219
	step [116/190], loss=178.5800
	step [117/190], loss=169.6602
	step [118/190], loss=160.3960
	step [119/190], loss=161.8864
	step [120/190], loss=183.8057
	step [121/190], loss=151.3246
	step [122/190], loss=166.1756
	step [123/190], loss=146.0308
	step [124/190], loss=175.9277
	step [125/190], loss=164.7093
	step [126/190], loss=169.7961
	step [127/190], loss=173.6262
	step [128/190], loss=161.7957
	step [129/190], loss=164.3388
	step [130/190], loss=168.1046
	step [131/190], loss=154.4083
	step [132/190], loss=179.3843
	step [133/190], loss=160.6161
	step [134/190], loss=150.5554
	step [135/190], loss=173.2426
	step [136/190], loss=144.3717
	step [137/190], loss=169.5276
	step [138/190], loss=164.3863
	step [139/190], loss=173.7637
	step [140/190], loss=176.1303
	step [141/190], loss=150.8702
	step [142/190], loss=172.4541
	step [143/190], loss=170.9492
	step [144/190], loss=154.4102
	step [145/190], loss=169.5774
	step [146/190], loss=170.1330
	step [147/190], loss=168.6018
	step [148/190], loss=154.3287
	step [149/190], loss=161.1229
	step [150/190], loss=157.3900
	step [151/190], loss=163.8333
	step [152/190], loss=156.5216
	step [153/190], loss=182.1329
	step [154/190], loss=144.8354
	step [155/190], loss=156.0071
	step [156/190], loss=145.9764
	step [157/190], loss=150.9232
	step [158/190], loss=147.5450
	step [159/190], loss=146.4276
	step [160/190], loss=151.6861
	step [161/190], loss=155.1714
	step [162/190], loss=180.0143
	step [163/190], loss=163.4175
	step [164/190], loss=160.2416
	step [165/190], loss=171.0740
	step [166/190], loss=154.7186
	step [167/190], loss=147.9839
	step [168/190], loss=152.4796
	step [169/190], loss=148.6659
	step [170/190], loss=142.8788
	step [171/190], loss=151.4830
	step [172/190], loss=145.0436
	step [173/190], loss=151.4654
	step [174/190], loss=149.0741
	step [175/190], loss=146.6480
	step [176/190], loss=168.9458
	step [177/190], loss=142.0495
	step [178/190], loss=137.4687
	step [179/190], loss=145.3476
	step [180/190], loss=150.4729
	step [181/190], loss=155.7008
	step [182/190], loss=139.1696
	step [183/190], loss=146.8300
	step [184/190], loss=151.0718
	step [185/190], loss=167.6540
	step [186/190], loss=137.8815
	step [187/190], loss=159.2273
	step [188/190], loss=146.2566
	step [189/190], loss=134.4153
	step [190/190], loss=130.7719
	Evaluating
	loss=0.2137, precision=0.4784, recall=0.9052, f1=0.6260
saving model as: 1_saved_model.pth
Training epoch 2
	step [1/190], loss=157.6602
	step [2/190], loss=164.9067
	step [3/190], loss=153.2706
	step [4/190], loss=145.6107
	step [5/190], loss=142.1997
	step [6/190], loss=153.7573
	step [7/190], loss=150.1978
	step [8/190], loss=153.5083
	step [9/190], loss=147.7086
	step [10/190], loss=152.7013
	step [11/190], loss=157.6009
	step [12/190], loss=147.7230
	step [13/190], loss=141.9631
	step [14/190], loss=167.1440
	step [15/190], loss=155.6404
	step [16/190], loss=157.1710
	step [17/190], loss=161.3694
	step [18/190], loss=141.6118
	step [19/190], loss=142.8057
	step [20/190], loss=150.0092
	step [21/190], loss=148.5163
	step [22/190], loss=140.9911
	step [23/190], loss=154.0186
	step [24/190], loss=139.9231
	step [25/190], loss=136.8318
	step [26/190], loss=152.7317
	step [27/190], loss=143.1407
	step [28/190], loss=161.1497
	step [29/190], loss=155.5233
	step [30/190], loss=136.6262
	step [31/190], loss=146.9030
	step [32/190], loss=146.5285
	step [33/190], loss=142.3649
	step [34/190], loss=135.9918
	step [35/190], loss=156.1081
	step [36/190], loss=129.9195
	step [37/190], loss=146.2546
	step [38/190], loss=127.7673
	step [39/190], loss=157.9696
	step [40/190], loss=157.5779
	step [41/190], loss=146.9333
	step [42/190], loss=140.2463
	step [43/190], loss=137.1914
	step [44/190], loss=136.1683
	step [45/190], loss=134.9960
	step [46/190], loss=164.2447
	step [47/190], loss=139.5957
	step [48/190], loss=136.1682
	step [49/190], loss=147.9328
	step [50/190], loss=139.2811
	step [51/190], loss=144.6216
	step [52/190], loss=127.4241
	step [53/190], loss=140.2255
	step [54/190], loss=138.4889
	step [55/190], loss=155.8890
	step [56/190], loss=139.6553
	step [57/190], loss=155.4914
	step [58/190], loss=143.7043
	step [59/190], loss=136.0771
	step [60/190], loss=120.8171
	step [61/190], loss=143.7525
	step [62/190], loss=142.0731
	step [63/190], loss=126.0394
	step [64/190], loss=144.0045
	step [65/190], loss=139.5659
	step [66/190], loss=141.5145
	step [67/190], loss=133.6826
	step [68/190], loss=131.1702
	step [69/190], loss=156.2667
	step [70/190], loss=155.6402
	step [71/190], loss=135.2549
	step [72/190], loss=144.0333
	step [73/190], loss=145.7625
	step [74/190], loss=123.0703
	step [75/190], loss=119.4349
	step [76/190], loss=139.7406
	step [77/190], loss=150.0247
	step [78/190], loss=148.4433
	step [79/190], loss=125.2674
	step [80/190], loss=131.5986
	step [81/190], loss=141.1342
	step [82/190], loss=133.9720
	step [83/190], loss=139.8307
	step [84/190], loss=126.1503
	step [85/190], loss=121.7661
	step [86/190], loss=158.7660
	step [87/190], loss=142.7958
	step [88/190], loss=139.1508
	step [89/190], loss=137.9410
	step [90/190], loss=160.9953
	step [91/190], loss=126.1661
	step [92/190], loss=169.8619
	step [93/190], loss=139.0546
	step [94/190], loss=151.6509
	step [95/190], loss=142.9999
	step [96/190], loss=147.0238
	step [97/190], loss=153.0025
	step [98/190], loss=124.6047
	step [99/190], loss=158.6608
	step [100/190], loss=155.4147
	step [101/190], loss=140.6382
	step [102/190], loss=154.9919
	step [103/190], loss=153.5846
	step [104/190], loss=144.7900
	step [105/190], loss=131.4745
	step [106/190], loss=139.4389
	step [107/190], loss=144.9873
	step [108/190], loss=123.4339
	step [109/190], loss=149.9094
	step [110/190], loss=139.1853
	step [111/190], loss=142.8638
	step [112/190], loss=149.9426
	step [113/190], loss=148.2826
	step [114/190], loss=136.5875
	step [115/190], loss=132.6689
	step [116/190], loss=115.8887
	step [117/190], loss=142.7303
	step [118/190], loss=125.7881
	step [119/190], loss=129.4751
	step [120/190], loss=146.6459
	step [121/190], loss=153.3727
	step [122/190], loss=125.9017
	step [123/190], loss=146.4388
	step [124/190], loss=146.9317
	step [125/190], loss=156.7163
	step [126/190], loss=142.8907
	step [127/190], loss=150.3400
	step [128/190], loss=130.9422
	step [129/190], loss=139.1905
	step [130/190], loss=135.3529
	step [131/190], loss=127.8528
	step [132/190], loss=146.6211
	step [133/190], loss=146.6480
	step [134/190], loss=140.7901
	step [135/190], loss=124.1197
	step [136/190], loss=136.5904
	step [137/190], loss=139.8874
	step [138/190], loss=136.7354
	step [139/190], loss=116.6387
	step [140/190], loss=131.4042
	step [141/190], loss=150.6797
	step [142/190], loss=148.2359
	step [143/190], loss=147.0144
	step [144/190], loss=140.5934
	step [145/190], loss=149.0762
	step [146/190], loss=132.7618
	step [147/190], loss=123.9473
	step [148/190], loss=162.9608
	step [149/190], loss=151.0939
	step [150/190], loss=135.4315
	step [151/190], loss=150.6132
	step [152/190], loss=136.1560
	step [153/190], loss=130.4724
	step [154/190], loss=143.2567
	step [155/190], loss=138.8362
	step [156/190], loss=131.6099
	step [157/190], loss=149.9609
	step [158/190], loss=125.2067
	step [159/190], loss=133.5747
	step [160/190], loss=141.0531
	step [161/190], loss=138.3496
	step [162/190], loss=148.4656
	step [163/190], loss=132.1031
	step [164/190], loss=155.3717
	step [165/190], loss=153.8109
	step [166/190], loss=133.8630
	step [167/190], loss=133.8093
	step [168/190], loss=126.1082
	step [169/190], loss=129.1260
	step [170/190], loss=133.9214
	step [171/190], loss=136.4114
	step [172/190], loss=130.9826
	step [173/190], loss=123.7533
	step [174/190], loss=129.3249
	step [175/190], loss=144.4855
	step [176/190], loss=129.0729
	step [177/190], loss=133.1935
	step [178/190], loss=117.2116
	step [179/190], loss=127.5403
	step [180/190], loss=149.8975
	step [181/190], loss=158.0805
	step [182/190], loss=117.2701
	step [183/190], loss=122.6352
	step [184/190], loss=133.2812
	step [185/190], loss=146.9154
	step [186/190], loss=112.3341
	step [187/190], loss=148.3387
	step [188/190], loss=145.5557
	step [189/190], loss=134.7810
	step [190/190], loss=123.7804
	Evaluating
	loss=0.1529, precision=0.5158, recall=0.9048, f1=0.6570
saving model as: 1_saved_model.pth
Training epoch 3
	step [1/190], loss=132.8727
	step [2/190], loss=131.8942
	step [3/190], loss=137.3641
	step [4/190], loss=133.7777
	step [5/190], loss=135.1916
	step [6/190], loss=143.3421
	step [7/190], loss=133.0217
	step [8/190], loss=146.9159
	step [9/190], loss=118.5095
	step [10/190], loss=144.3387
	step [11/190], loss=126.6748
	step [12/190], loss=126.0371
	step [13/190], loss=145.9048
	step [14/190], loss=128.7433
	step [15/190], loss=136.9025
	step [16/190], loss=126.7884
	step [17/190], loss=146.0051
	step [18/190], loss=135.3913
	step [19/190], loss=133.1436
	step [20/190], loss=148.6314
	step [21/190], loss=132.9125
	step [22/190], loss=140.2545
	step [23/190], loss=140.7603
	step [24/190], loss=126.7609
	step [25/190], loss=125.4551
	step [26/190], loss=160.3400
	step [27/190], loss=118.4426
	step [28/190], loss=129.3784
	step [29/190], loss=139.5176
	step [30/190], loss=122.6604
	step [31/190], loss=123.8078
	step [32/190], loss=147.8889
	step [33/190], loss=133.0250
	step [34/190], loss=130.7710
	step [35/190], loss=142.6566
	step [36/190], loss=140.5751
	step [37/190], loss=152.2511
	step [38/190], loss=132.0189
	step [39/190], loss=127.2427
	step [40/190], loss=120.4622
	step [41/190], loss=138.2273
	step [42/190], loss=117.1354
	step [43/190], loss=128.9142
	step [44/190], loss=132.2589
	step [45/190], loss=138.7827
	step [46/190], loss=133.3262
	step [47/190], loss=116.7998
	step [48/190], loss=144.4132
	step [49/190], loss=133.4392
	step [50/190], loss=124.6834
	step [51/190], loss=136.1843
	step [52/190], loss=132.2683
	step [53/190], loss=132.6281
	step [54/190], loss=127.8949
	step [55/190], loss=135.4194
	step [56/190], loss=134.6784
	step [57/190], loss=122.7473
	step [58/190], loss=129.4750
	step [59/190], loss=150.4680
	step [60/190], loss=123.5274
	step [61/190], loss=132.6981
	step [62/190], loss=125.8030
	step [63/190], loss=121.7101
	step [64/190], loss=136.9069
	step [65/190], loss=145.7899
	step [66/190], loss=133.3962
	step [67/190], loss=117.1142
	step [68/190], loss=123.9150
	step [69/190], loss=124.1322
	step [70/190], loss=127.3197
	step [71/190], loss=123.6301
	step [72/190], loss=136.5251
	step [73/190], loss=149.7831
	step [74/190], loss=129.1962
	step [75/190], loss=147.9218
	step [76/190], loss=130.4411
	step [77/190], loss=121.8762
	step [78/190], loss=122.8859
	step [79/190], loss=133.3253
	step [80/190], loss=133.9926
	step [81/190], loss=160.0241
	step [82/190], loss=129.8992
	step [83/190], loss=142.7491
	step [84/190], loss=120.2587
	step [85/190], loss=129.2327
	step [86/190], loss=118.2708
	step [87/190], loss=132.1702
	step [88/190], loss=150.5173
	step [89/190], loss=133.5420
	step [90/190], loss=133.7774
	step [91/190], loss=117.5115
	step [92/190], loss=128.5613
	step [93/190], loss=132.6658
	step [94/190], loss=130.6468
	step [95/190], loss=128.5324
	step [96/190], loss=128.5750
	step [97/190], loss=130.3410
	step [98/190], loss=127.8689
	step [99/190], loss=133.5853
	step [100/190], loss=134.3105
	step [101/190], loss=131.3887
	step [102/190], loss=128.7116
	step [103/190], loss=110.3896
	step [104/190], loss=137.3859
	step [105/190], loss=137.2603
	step [106/190], loss=122.9859
	step [107/190], loss=142.5555
	step [108/190], loss=122.1833
	step [109/190], loss=133.7152
	step [110/190], loss=123.8475
	step [111/190], loss=125.1812
	step [112/190], loss=125.4166
	step [113/190], loss=126.0382
	step [114/190], loss=106.2435
	step [115/190], loss=125.5709
	step [116/190], loss=125.4592
	step [117/190], loss=136.8391
	step [118/190], loss=124.0616
	step [119/190], loss=126.9359
	step [120/190], loss=126.0582
	step [121/190], loss=127.8594
	step [122/190], loss=134.7455
	step [123/190], loss=139.1797
	step [124/190], loss=108.6326
	step [125/190], loss=139.1518
	step [126/190], loss=125.7567
	step [127/190], loss=140.6355
	step [128/190], loss=128.7340
	step [129/190], loss=117.9799
	step [130/190], loss=124.0916
	step [131/190], loss=132.8549
	step [132/190], loss=122.5336
	step [133/190], loss=127.6833
	step [134/190], loss=122.2310
	step [135/190], loss=130.8187
	step [136/190], loss=130.3559
	step [137/190], loss=130.3995
	step [138/190], loss=132.1754
	step [139/190], loss=112.8838
	step [140/190], loss=131.5083
	step [141/190], loss=136.3398
	step [142/190], loss=125.6186
	step [143/190], loss=130.1460
	step [144/190], loss=123.1085
	step [145/190], loss=125.7979
	step [146/190], loss=113.5757
	step [147/190], loss=125.2871
	step [148/190], loss=115.0304
	step [149/190], loss=138.8526
	step [150/190], loss=141.0857
	step [151/190], loss=122.2030
	step [152/190], loss=127.7758
	step [153/190], loss=139.4076
	step [154/190], loss=110.6013
	step [155/190], loss=127.4406
	step [156/190], loss=131.3300
	step [157/190], loss=138.1170
	step [158/190], loss=108.1001
	step [159/190], loss=114.8098
	step [160/190], loss=115.8758
	step [161/190], loss=130.7142
	step [162/190], loss=118.1861
	step [163/190], loss=141.9905
	step [164/190], loss=124.2245
	step [165/190], loss=133.3255
	step [166/190], loss=132.6934
	step [167/190], loss=137.7023
	step [168/190], loss=135.3404
	step [169/190], loss=112.6692
	step [170/190], loss=131.0397
	step [171/190], loss=150.0906
	step [172/190], loss=129.1956
	step [173/190], loss=126.2446
	step [174/190], loss=128.7136
	step [175/190], loss=132.8560
	step [176/190], loss=109.6863
	step [177/190], loss=116.7195
	step [178/190], loss=129.4122
	step [179/190], loss=132.9874
	step [180/190], loss=117.1711
	step [181/190], loss=142.9301
	step [182/190], loss=122.5848
	step [183/190], loss=127.2364
	step [184/190], loss=153.8923
	step [185/190], loss=122.3653
	step [186/190], loss=112.6089
	step [187/190], loss=119.3415
	step [188/190], loss=113.1025
	step [189/190], loss=122.4910
	step [190/190], loss=100.0113
	Evaluating
	loss=0.1188, precision=0.5753, recall=0.8832, f1=0.6967
saving model as: 1_saved_model.pth
Training epoch 4
	step [1/190], loss=106.8780
	step [2/190], loss=109.2350
	step [3/190], loss=131.3962
	step [4/190], loss=112.7004
	step [5/190], loss=118.8571
	step [6/190], loss=113.4707
	step [7/190], loss=127.7905
	step [8/190], loss=112.8219
	step [9/190], loss=125.5354
	step [10/190], loss=135.9640
	step [11/190], loss=128.2725
	step [12/190], loss=129.9179
	step [13/190], loss=127.2824
	step [14/190], loss=119.2730
	step [15/190], loss=120.1121
	step [16/190], loss=118.6407
	step [17/190], loss=128.1779
	step [18/190], loss=136.0148
	step [19/190], loss=128.1928
	step [20/190], loss=129.6525
	step [21/190], loss=123.0460
	step [22/190], loss=135.1904
	step [23/190], loss=122.8866
	step [24/190], loss=131.7392
	step [25/190], loss=163.8203
	step [26/190], loss=129.1622
	step [27/190], loss=124.1725
	step [28/190], loss=129.1074
	step [29/190], loss=132.8517
	step [30/190], loss=137.2607
	step [31/190], loss=127.3835
	step [32/190], loss=115.7475
	step [33/190], loss=136.9173
	step [34/190], loss=118.3036
	step [35/190], loss=128.5075
	step [36/190], loss=133.7121
	step [37/190], loss=130.2081
	step [38/190], loss=112.2616
	step [39/190], loss=129.5214
	step [40/190], loss=130.3806
	step [41/190], loss=121.1670
	step [42/190], loss=112.5988
	step [43/190], loss=116.1732
	step [44/190], loss=115.7976
	step [45/190], loss=101.7881
	step [46/190], loss=140.9010
	step [47/190], loss=90.4454
	step [48/190], loss=115.9476
	step [49/190], loss=118.1867
	step [50/190], loss=137.7064
	step [51/190], loss=128.9501
	step [52/190], loss=112.7637
	step [53/190], loss=119.6047
	step [54/190], loss=131.2256
	step [55/190], loss=116.5007
	step [56/190], loss=112.6797
	step [57/190], loss=120.0476
	step [58/190], loss=130.5356
	step [59/190], loss=118.1134
	step [60/190], loss=143.6257
	step [61/190], loss=128.0764
	step [62/190], loss=115.8020
	step [63/190], loss=112.1896
	step [64/190], loss=123.3952
	step [65/190], loss=125.0880
	step [66/190], loss=126.6090
	step [67/190], loss=118.5723
	step [68/190], loss=133.8008
	step [69/190], loss=121.4679
	step [70/190], loss=115.0179
	step [71/190], loss=135.4934
	step [72/190], loss=117.4081
	step [73/190], loss=138.9420
	step [74/190], loss=114.5784
	step [75/190], loss=109.7300
	step [76/190], loss=130.4752
	step [77/190], loss=140.6801
	step [78/190], loss=123.3146
	step [79/190], loss=107.8857
	step [80/190], loss=130.6956
	step [81/190], loss=112.6558
	step [82/190], loss=137.6789
	step [83/190], loss=134.8529
	step [84/190], loss=118.6053
	step [85/190], loss=129.3270
	step [86/190], loss=115.3596
	step [87/190], loss=112.2991
	step [88/190], loss=132.6997
	step [89/190], loss=111.4289
	step [90/190], loss=116.2405
	step [91/190], loss=108.3312
	step [92/190], loss=129.4129
	step [93/190], loss=121.8432
	step [94/190], loss=126.2639
	step [95/190], loss=116.1016
	step [96/190], loss=119.6958
	step [97/190], loss=136.1050
	step [98/190], loss=111.3625
	step [99/190], loss=121.5952
	step [100/190], loss=127.4023
	step [101/190], loss=131.7625
	step [102/190], loss=129.9009
	step [103/190], loss=141.9089
	step [104/190], loss=131.0677
	step [105/190], loss=127.9960
	step [106/190], loss=117.4027
	step [107/190], loss=116.8492
	step [108/190], loss=118.5135
	step [109/190], loss=118.6779
	step [110/190], loss=132.2803
	step [111/190], loss=132.7115
	step [112/190], loss=118.6029
	step [113/190], loss=116.6708
	step [114/190], loss=112.1918
	step [115/190], loss=111.1109
	step [116/190], loss=119.9931
	step [117/190], loss=115.4860
	step [118/190], loss=121.8447
	step [119/190], loss=114.9777
	step [120/190], loss=118.1865
	step [121/190], loss=129.5267
	step [122/190], loss=117.6454
	step [123/190], loss=127.1389
	step [124/190], loss=119.2757
	step [125/190], loss=132.1185
	step [126/190], loss=118.7103
	step [127/190], loss=129.4579
	step [128/190], loss=123.4613
	step [129/190], loss=105.6409
	step [130/190], loss=129.5522
	step [131/190], loss=130.8719
	step [132/190], loss=122.6284
	step [133/190], loss=114.3251
	step [134/190], loss=121.0249
	step [135/190], loss=122.6219
	step [136/190], loss=109.4316
	step [137/190], loss=131.0052
	step [138/190], loss=129.7619
	step [139/190], loss=122.9657
	step [140/190], loss=120.6327
	step [141/190], loss=130.4580
	step [142/190], loss=131.7885
	step [143/190], loss=107.9720
	step [144/190], loss=127.8373
	step [145/190], loss=111.2265
	step [146/190], loss=113.8544
	step [147/190], loss=119.3321
	step [148/190], loss=127.8836
	step [149/190], loss=109.7433
	step [150/190], loss=126.6780
	step [151/190], loss=121.7277
	step [152/190], loss=118.0487
	step [153/190], loss=115.2669
	step [154/190], loss=118.9915
	step [155/190], loss=115.0196
	step [156/190], loss=122.0305
	step [157/190], loss=127.7497
	step [158/190], loss=130.5257
	step [159/190], loss=117.0984
	step [160/190], loss=122.3125
	step [161/190], loss=135.9317
	step [162/190], loss=102.3065
	step [163/190], loss=124.9419
	step [164/190], loss=141.3003
	step [165/190], loss=116.5788
	step [166/190], loss=133.9217
	step [167/190], loss=139.0074
	step [168/190], loss=140.5305
	step [169/190], loss=105.0561
	step [170/190], loss=115.1123
	step [171/190], loss=114.2500
	step [172/190], loss=117.1111
	step [173/190], loss=108.9118
	step [174/190], loss=117.7873
	step [175/190], loss=109.1068
	step [176/190], loss=123.1039
	step [177/190], loss=120.5841
	step [178/190], loss=115.4030
	step [179/190], loss=126.3705
	step [180/190], loss=127.5378
	step [181/190], loss=125.2273
	step [182/190], loss=122.2118
	step [183/190], loss=127.1240
	step [184/190], loss=107.1694
	step [185/190], loss=112.2116
	step [186/190], loss=120.6940
	step [187/190], loss=118.0954
	step [188/190], loss=121.6558
	step [189/190], loss=108.3419
	step [190/190], loss=99.7142
	Evaluating
	loss=0.0944, precision=0.5086, recall=0.8966, f1=0.6490
Training epoch 5
	step [1/190], loss=125.5680
	step [2/190], loss=132.4498
	step [3/190], loss=104.8477
	step [4/190], loss=128.1734
	step [5/190], loss=119.3655
	step [6/190], loss=118.6391
	step [7/190], loss=113.1727
	step [8/190], loss=113.8019
	step [9/190], loss=116.9820
	step [10/190], loss=107.6671
	step [11/190], loss=120.4427
	step [12/190], loss=114.3839
	step [13/190], loss=116.1107
	step [14/190], loss=127.3308
	step [15/190], loss=108.8084
	step [16/190], loss=113.7370
	step [17/190], loss=124.8271
	step [18/190], loss=112.4096
	step [19/190], loss=102.1072
	step [20/190], loss=114.5257
	step [21/190], loss=131.3410
	step [22/190], loss=124.3468
	step [23/190], loss=114.2552
	step [24/190], loss=117.4278
	step [25/190], loss=124.6507
	step [26/190], loss=132.6002
	step [27/190], loss=127.6733
	step [28/190], loss=115.7038
	step [29/190], loss=113.3798
	step [30/190], loss=115.7916
	step [31/190], loss=124.9347
	step [32/190], loss=110.9434
	step [33/190], loss=108.3586
	step [34/190], loss=108.3743
	step [35/190], loss=110.7921
	step [36/190], loss=105.8668
	step [37/190], loss=132.0742
	step [38/190], loss=108.3950
	step [39/190], loss=109.5045
	step [40/190], loss=113.1527
	step [41/190], loss=123.2813
	step [42/190], loss=108.6638
	step [43/190], loss=114.5517
	step [44/190], loss=124.6560
	step [45/190], loss=116.0591
	step [46/190], loss=107.3283
	step [47/190], loss=119.4011
	step [48/190], loss=109.6193
	step [49/190], loss=110.0350
	step [50/190], loss=130.6109
	step [51/190], loss=124.2027
	step [52/190], loss=127.6564
	step [53/190], loss=101.3324
	step [54/190], loss=117.5662
	step [55/190], loss=127.8301
	step [56/190], loss=126.7792
	step [57/190], loss=114.6851
	step [58/190], loss=117.1816
	step [59/190], loss=124.4508
	step [60/190], loss=108.1238
	step [61/190], loss=110.0416
	step [62/190], loss=118.6946
	step [63/190], loss=112.1951
	step [64/190], loss=100.6283
	step [65/190], loss=113.3211
	step [66/190], loss=116.5439
	step [67/190], loss=109.1470
	step [68/190], loss=111.0263
	step [69/190], loss=116.3989
	step [70/190], loss=127.2826
	step [71/190], loss=116.2229
	step [72/190], loss=115.7294
	step [73/190], loss=133.7160
	step [74/190], loss=133.2396
	step [75/190], loss=128.6703
	step [76/190], loss=110.0776
	step [77/190], loss=122.7077
	step [78/190], loss=125.4343
	step [79/190], loss=114.2027
	step [80/190], loss=137.7826
	step [81/190], loss=113.3679
	step [82/190], loss=114.1950
	step [83/190], loss=113.3898
	step [84/190], loss=123.4182
	step [85/190], loss=108.3802
	step [86/190], loss=127.4168
	step [87/190], loss=123.6356
	step [88/190], loss=121.0484
	step [89/190], loss=130.7802
	step [90/190], loss=97.8615
	step [91/190], loss=112.2888
	step [92/190], loss=104.5445
	step [93/190], loss=114.2521
	step [94/190], loss=107.5424
	step [95/190], loss=116.8969
	step [96/190], loss=131.5217
	step [97/190], loss=102.3725
	step [98/190], loss=122.9711
	step [99/190], loss=118.0732
	step [100/190], loss=111.4127
	step [101/190], loss=116.0856
	step [102/190], loss=116.8787
	step [103/190], loss=103.9617
	step [104/190], loss=113.8554
	step [105/190], loss=120.9450
	step [106/190], loss=134.0165
	step [107/190], loss=116.9395
	step [108/190], loss=108.0262
	step [109/190], loss=114.0823
	step [110/190], loss=102.2414
	step [111/190], loss=104.1479
	step [112/190], loss=129.4483
	step [113/190], loss=116.3935
	step [114/190], loss=121.6220
	step [115/190], loss=117.1679
	step [116/190], loss=134.4629
	step [117/190], loss=112.9381
	step [118/190], loss=113.5256
	step [119/190], loss=120.2489
	step [120/190], loss=117.5822
	step [121/190], loss=113.2819
	step [122/190], loss=110.6706
	step [123/190], loss=124.0873
	step [124/190], loss=115.7004
	step [125/190], loss=117.4067
	step [126/190], loss=119.6809
	step [127/190], loss=111.4296
	step [128/190], loss=107.3338
	step [129/190], loss=109.8546
	step [130/190], loss=113.2508
	step [131/190], loss=107.9133
	step [132/190], loss=115.7539
	step [133/190], loss=119.6616
	step [134/190], loss=116.3303
	step [135/190], loss=136.3417
	step [136/190], loss=121.1782
	step [137/190], loss=110.9713
	step [138/190], loss=117.7914
	step [139/190], loss=104.6129
	step [140/190], loss=138.2656
	step [141/190], loss=107.4156
	step [142/190], loss=121.7057
	step [143/190], loss=128.4367
	step [144/190], loss=132.0617
	step [145/190], loss=124.5471
	step [146/190], loss=116.7152
	step [147/190], loss=135.0618
	step [148/190], loss=118.4019
	step [149/190], loss=132.3781
	step [150/190], loss=121.9904
	step [151/190], loss=127.3237
	step [152/190], loss=121.4540
	step [153/190], loss=115.7142
	step [154/190], loss=108.8974
	step [155/190], loss=104.8339
	step [156/190], loss=124.9632
	step [157/190], loss=105.8681
	step [158/190], loss=114.6245
	step [159/190], loss=138.7515
	step [160/190], loss=128.3876
	step [161/190], loss=114.0550
	step [162/190], loss=120.5029
	step [163/190], loss=118.9109
	step [164/190], loss=112.1205
	step [165/190], loss=112.5364
	step [166/190], loss=117.4553
	step [167/190], loss=108.6171
	step [168/190], loss=115.9030
	step [169/190], loss=109.4778
	step [170/190], loss=109.5663
	step [171/190], loss=120.3544
	step [172/190], loss=119.9384
	step [173/190], loss=116.6396
	step [174/190], loss=114.5473
	step [175/190], loss=108.9117
	step [176/190], loss=113.7712
	step [177/190], loss=106.4778
	step [178/190], loss=122.4081
	step [179/190], loss=105.3784
	step [180/190], loss=106.8066
	step [181/190], loss=124.4299
	step [182/190], loss=111.7521
	step [183/190], loss=109.9694
	step [184/190], loss=110.6694
	step [185/190], loss=96.2354
	step [186/190], loss=119.1091
	step [187/190], loss=130.4733
	step [188/190], loss=128.2553
	step [189/190], loss=102.5864
	step [190/190], loss=98.5061
	Evaluating
	loss=0.0752, precision=0.5002, recall=0.9053, f1=0.6444
Training epoch 6
	step [1/190], loss=117.9296
	step [2/190], loss=122.0806
	step [3/190], loss=118.6235
	step [4/190], loss=109.4586
	step [5/190], loss=136.4700
	step [6/190], loss=116.8423
	step [7/190], loss=123.0061
	step [8/190], loss=114.2932
	step [9/190], loss=109.0787
	step [10/190], loss=128.6044
	step [11/190], loss=90.4313
	step [12/190], loss=109.3116
	step [13/190], loss=121.5068
	step [14/190], loss=116.2146
	step [15/190], loss=110.7859
	step [16/190], loss=122.1054
	step [17/190], loss=108.7775
	step [18/190], loss=118.0214
	step [19/190], loss=121.8815
	step [20/190], loss=100.6177
	step [21/190], loss=105.9532
	step [22/190], loss=110.6437
	step [23/190], loss=106.7487
	step [24/190], loss=109.1686
	step [25/190], loss=105.1252
	step [26/190], loss=125.5581
	step [27/190], loss=129.6643
	step [28/190], loss=117.2807
	step [29/190], loss=98.3664
	step [30/190], loss=101.3162
	step [31/190], loss=134.2453
	step [32/190], loss=124.4055
	step [33/190], loss=123.3207
	step [34/190], loss=134.0531
	step [35/190], loss=119.3525
	step [36/190], loss=110.7782
	step [37/190], loss=108.7114
	step [38/190], loss=108.3837
	step [39/190], loss=113.7620
	step [40/190], loss=102.9857
	step [41/190], loss=113.5987
	step [42/190], loss=105.4892
	step [43/190], loss=109.5702
	step [44/190], loss=110.9784
	step [45/190], loss=119.9840
	step [46/190], loss=108.4512
	step [47/190], loss=123.0146
	step [48/190], loss=103.1094
	step [49/190], loss=108.9366
	step [50/190], loss=116.9478
	step [51/190], loss=100.0917
	step [52/190], loss=116.1035
	step [53/190], loss=107.0883
	step [54/190], loss=98.3607
	step [55/190], loss=121.0234
	step [56/190], loss=112.7013
	step [57/190], loss=105.9826
	step [58/190], loss=97.6139
	step [59/190], loss=109.2115
	step [60/190], loss=117.4080
	step [61/190], loss=122.8574
	step [62/190], loss=99.1994
	step [63/190], loss=113.2703
	step [64/190], loss=120.1068
	step [65/190], loss=127.1260
	step [66/190], loss=108.7501
	step [67/190], loss=111.5988
	step [68/190], loss=111.6937
	step [69/190], loss=122.6762
	step [70/190], loss=129.1490
	step [71/190], loss=119.2262
	step [72/190], loss=119.2454
	step [73/190], loss=101.8664
	step [74/190], loss=121.4749
	step [75/190], loss=107.1963
	step [76/190], loss=137.6564
	step [77/190], loss=118.3089
	step [78/190], loss=116.8449
	step [79/190], loss=117.0128
	step [80/190], loss=125.7604
	step [81/190], loss=111.5984
	step [82/190], loss=110.8727
	step [83/190], loss=121.1827
	step [84/190], loss=124.2484
	step [85/190], loss=100.9146
	step [86/190], loss=115.3961
	step [87/190], loss=112.7750
	step [88/190], loss=117.5591
	step [89/190], loss=119.4884
	step [90/190], loss=108.3188
	step [91/190], loss=115.4376
	step [92/190], loss=96.6414
	step [93/190], loss=109.0662
	step [94/190], loss=118.5295
	step [95/190], loss=112.4581
	step [96/190], loss=96.0313
	step [97/190], loss=103.5365
	step [98/190], loss=114.5056
	step [99/190], loss=126.6693
	step [100/190], loss=104.0492
	step [101/190], loss=118.7541
	step [102/190], loss=106.4171
	step [103/190], loss=104.6713
	step [104/190], loss=116.9603
	step [105/190], loss=120.1199
	step [106/190], loss=114.0089
	step [107/190], loss=111.5523
	step [108/190], loss=103.6055
	step [109/190], loss=110.8934
	step [110/190], loss=105.8827
	step [111/190], loss=114.7566
	step [112/190], loss=113.9358
	step [113/190], loss=111.5068
	step [114/190], loss=119.3884
	step [115/190], loss=107.3806
	step [116/190], loss=122.5070
	step [117/190], loss=112.4202
	step [118/190], loss=114.8797
	step [119/190], loss=113.8088
	step [120/190], loss=100.5428
	step [121/190], loss=118.9078
	step [122/190], loss=125.7407
	step [123/190], loss=116.9371
	step [124/190], loss=105.9122
	step [125/190], loss=111.1593
	step [126/190], loss=113.3944
	step [127/190], loss=111.6074
	step [128/190], loss=116.5500
	step [129/190], loss=118.4559
	step [130/190], loss=111.8373
	step [131/190], loss=119.0443
	step [132/190], loss=97.9244
	step [133/190], loss=103.8071
	step [134/190], loss=116.0625
	step [135/190], loss=115.5700
	step [136/190], loss=122.4173
	step [137/190], loss=92.6100
	step [138/190], loss=112.3020
	step [139/190], loss=111.7487
	step [140/190], loss=114.6820
	step [141/190], loss=117.8692
	step [142/190], loss=105.6156
	step [143/190], loss=130.2318
	step [144/190], loss=103.5451
	step [145/190], loss=102.4672
	step [146/190], loss=101.3347
	step [147/190], loss=120.0846
	step [148/190], loss=117.4664
	step [149/190], loss=98.6614
	step [150/190], loss=106.2711
	step [151/190], loss=93.3054
	step [152/190], loss=100.3046
	step [153/190], loss=108.0265
	step [154/190], loss=103.2421
	step [155/190], loss=101.6847
	step [156/190], loss=117.1788
	step [157/190], loss=115.7084
	step [158/190], loss=109.5391
	step [159/190], loss=115.3880
	step [160/190], loss=104.3367
	step [161/190], loss=117.6243
	step [162/190], loss=105.9954
	step [163/190], loss=112.8687
	step [164/190], loss=117.2340
	step [165/190], loss=109.4659
	step [166/190], loss=108.3466
	step [167/190], loss=121.2452
	step [168/190], loss=98.2053
	step [169/190], loss=98.4366
	step [170/190], loss=102.9989
	step [171/190], loss=113.1438
	step [172/190], loss=106.6802
	step [173/190], loss=109.9561
	step [174/190], loss=112.6721
	step [175/190], loss=124.3092
	step [176/190], loss=109.3040
	step [177/190], loss=98.6521
	step [178/190], loss=117.8212
	step [179/190], loss=119.4328
	step [180/190], loss=97.9618
	step [181/190], loss=116.2088
	step [182/190], loss=127.6418
	step [183/190], loss=100.3255
	step [184/190], loss=115.0466
	step [185/190], loss=120.5009
	step [186/190], loss=104.7120
	step [187/190], loss=115.3587
	step [188/190], loss=117.0708
	step [189/190], loss=122.8835
	step [190/190], loss=89.4012
	Evaluating
	loss=0.0620, precision=0.4824, recall=0.8949, f1=0.6269
Training epoch 7
	step [1/190], loss=115.7331
	step [2/190], loss=110.7982
	step [3/190], loss=124.6432
	step [4/190], loss=112.2424
	step [5/190], loss=97.2105
	step [6/190], loss=115.6764
	step [7/190], loss=113.3358
	step [8/190], loss=100.9314
	step [9/190], loss=110.2422
	step [10/190], loss=118.7905
	step [11/190], loss=115.6937
	step [12/190], loss=106.0286
	step [13/190], loss=107.1069
	step [14/190], loss=110.9884
	step [15/190], loss=111.1609
	step [16/190], loss=110.7279
	step [17/190], loss=96.0186
	step [18/190], loss=99.4038
	step [19/190], loss=126.7454
	step [20/190], loss=122.2251
	step [21/190], loss=117.6941
	step [22/190], loss=91.2723
	step [23/190], loss=113.1077
	step [24/190], loss=118.6266
	step [25/190], loss=100.8892
	step [26/190], loss=102.1034
	step [27/190], loss=98.5360
	step [28/190], loss=116.0947
	step [29/190], loss=113.0989
	step [30/190], loss=103.8878
	step [31/190], loss=119.5362
	step [32/190], loss=113.6305
	step [33/190], loss=113.7689
	step [34/190], loss=108.7439
	step [35/190], loss=98.8355
	step [36/190], loss=112.2932
	step [37/190], loss=114.1489
	step [38/190], loss=105.2070
	step [39/190], loss=114.9254
	step [40/190], loss=100.7214
	step [41/190], loss=117.6365
	step [42/190], loss=110.4952
	step [43/190], loss=111.4663
	step [44/190], loss=108.8235
	step [45/190], loss=94.6614
	step [46/190], loss=105.6735
	step [47/190], loss=104.6757
	step [48/190], loss=113.1222
	step [49/190], loss=112.1355
	step [50/190], loss=117.9086
	step [51/190], loss=116.2984
	step [52/190], loss=106.7659
	step [53/190], loss=121.9977
	step [54/190], loss=98.8098
	step [55/190], loss=115.2849
	step [56/190], loss=109.9242
	step [57/190], loss=107.5806
	step [58/190], loss=126.2693
	step [59/190], loss=102.1662
	step [60/190], loss=114.7164
	step [61/190], loss=126.6900
	step [62/190], loss=108.2045
	step [63/190], loss=116.7334
	step [64/190], loss=117.7966
	step [65/190], loss=96.1543
	step [66/190], loss=97.7190
	step [67/190], loss=97.3744
	step [68/190], loss=112.1442
	step [69/190], loss=104.3078
	step [70/190], loss=115.1314
	step [71/190], loss=113.8901
	step [72/190], loss=132.2466
	step [73/190], loss=101.3005
	step [74/190], loss=125.3910
	step [75/190], loss=104.2284
	step [76/190], loss=106.7320
	step [77/190], loss=110.8602
	step [78/190], loss=120.8533
	step [79/190], loss=129.0558
	step [80/190], loss=117.5106
	step [81/190], loss=92.2725
	step [82/190], loss=90.9061
	step [83/190], loss=115.1521
	step [84/190], loss=110.0957
	step [85/190], loss=112.7967
	step [86/190], loss=115.3309
	step [87/190], loss=104.2479
	step [88/190], loss=102.0982
	step [89/190], loss=106.1400
	step [90/190], loss=106.0626
	step [91/190], loss=108.4553
	step [92/190], loss=122.5540
	step [93/190], loss=112.0061
	step [94/190], loss=112.5966
	step [95/190], loss=112.1110
	step [96/190], loss=107.7051
	step [97/190], loss=122.3322
	step [98/190], loss=111.1796
	step [99/190], loss=93.4534
	step [100/190], loss=105.6803
	step [101/190], loss=108.8279
	step [102/190], loss=113.8374
	step [103/190], loss=107.7632
	step [104/190], loss=119.8819
	step [105/190], loss=114.9461
	step [106/190], loss=117.8685
	step [107/190], loss=111.9460
	step [108/190], loss=128.0880
	step [109/190], loss=95.8417
	step [110/190], loss=105.0327
	step [111/190], loss=111.4223
	step [112/190], loss=108.1150
	step [113/190], loss=105.8704
	step [114/190], loss=125.2887
	step [115/190], loss=107.7295
	step [116/190], loss=99.2148
	step [117/190], loss=109.3855
	step [118/190], loss=95.3426
	step [119/190], loss=97.9872
	step [120/190], loss=123.6858
	step [121/190], loss=107.5264
	step [122/190], loss=103.9980
	step [123/190], loss=105.2107
	step [124/190], loss=111.2156
	step [125/190], loss=109.7631
	step [126/190], loss=105.9334
	step [127/190], loss=115.2338
	step [128/190], loss=112.2978
	step [129/190], loss=104.5244
	step [130/190], loss=102.6920
	step [131/190], loss=117.3838
	step [132/190], loss=90.4930
	step [133/190], loss=103.2714
	step [134/190], loss=118.1686
	step [135/190], loss=106.9495
	step [136/190], loss=106.1688
	step [137/190], loss=112.7592
	step [138/190], loss=108.8004
	step [139/190], loss=101.3428
	step [140/190], loss=110.1282
	step [141/190], loss=113.9928
	step [142/190], loss=114.6230
	step [143/190], loss=118.4659
	step [144/190], loss=100.3358
	step [145/190], loss=97.4400
	step [146/190], loss=113.1546
	step [147/190], loss=103.7066
	step [148/190], loss=101.0856
	step [149/190], loss=101.9766
	step [150/190], loss=112.8123
	step [151/190], loss=97.3144
	step [152/190], loss=107.4654
	step [153/190], loss=115.8909
	step [154/190], loss=97.6340
	step [155/190], loss=103.3669
	step [156/190], loss=116.2606
	step [157/190], loss=95.8312
	step [158/190], loss=99.0458
	step [159/190], loss=101.3974
	step [160/190], loss=120.2431
	step [161/190], loss=99.9923
	step [162/190], loss=115.0544
	step [163/190], loss=96.5393
	step [164/190], loss=97.4218
	step [165/190], loss=102.9336
	step [166/190], loss=111.2249
	step [167/190], loss=115.6684
	step [168/190], loss=119.3733
	step [169/190], loss=110.1259
	step [170/190], loss=97.0266
	step [171/190], loss=112.6691
	step [172/190], loss=99.7436
	step [173/190], loss=106.2660
	step [174/190], loss=99.5684
	step [175/190], loss=104.8484
	step [176/190], loss=103.6853
	step [177/190], loss=100.8224
	step [178/190], loss=98.5422
	step [179/190], loss=122.5333
	step [180/190], loss=111.3533
	step [181/190], loss=104.8136
	step [182/190], loss=120.4193
	step [183/190], loss=124.5152
	step [184/190], loss=109.3455
	step [185/190], loss=112.9052
	step [186/190], loss=105.5723
	step [187/190], loss=122.3693
	step [188/190], loss=101.4464
	step [189/190], loss=103.0570
	step [190/190], loss=106.6427
	Evaluating
	loss=0.0565, precision=0.5229, recall=0.8911, f1=0.6590
Training epoch 8
	step [1/190], loss=113.1416
	step [2/190], loss=102.5956
	step [3/190], loss=101.6139
	step [4/190], loss=121.0746
	step [5/190], loss=111.1243
	step [6/190], loss=104.5027
	step [7/190], loss=106.5134
	step [8/190], loss=119.2117
	step [9/190], loss=88.6609
	step [10/190], loss=100.0775
	step [11/190], loss=115.4760
	step [12/190], loss=122.2482
	step [13/190], loss=113.1545
	step [14/190], loss=107.4876
	step [15/190], loss=113.0140
	step [16/190], loss=120.9849
	step [17/190], loss=124.6203
	step [18/190], loss=87.7078
	step [19/190], loss=87.8896
	step [20/190], loss=118.2251
	step [21/190], loss=112.2934
	step [22/190], loss=86.7918
	step [23/190], loss=113.8562
	step [24/190], loss=97.6703
	step [25/190], loss=100.3756
	step [26/190], loss=117.8566
	step [27/190], loss=108.6874
	step [28/190], loss=98.3976
	step [29/190], loss=119.4090
	step [30/190], loss=101.6799
	step [31/190], loss=98.0352
	step [32/190], loss=109.4986
	step [33/190], loss=97.2894
	step [34/190], loss=112.4436
	step [35/190], loss=113.6421
	step [36/190], loss=106.6903
	step [37/190], loss=106.2567
	step [38/190], loss=108.2141
	step [39/190], loss=109.3365
	step [40/190], loss=102.7336
	step [41/190], loss=111.5706
	step [42/190], loss=105.3805
	step [43/190], loss=103.8555
	step [44/190], loss=104.0441
	step [45/190], loss=106.6641
	step [46/190], loss=106.7532
	step [47/190], loss=100.3462
	step [48/190], loss=102.6416
	step [49/190], loss=119.2381
	step [50/190], loss=117.1873
	step [51/190], loss=109.6631
	step [52/190], loss=110.2098
	step [53/190], loss=102.2203
	step [54/190], loss=103.2224
	step [55/190], loss=109.0028
	step [56/190], loss=111.9695
	step [57/190], loss=105.5631
	step [58/190], loss=107.5413
	step [59/190], loss=102.1184
	step [60/190], loss=108.2436
	step [61/190], loss=114.9608
	step [62/190], loss=121.5353
	step [63/190], loss=104.0260
	step [64/190], loss=103.6978
	step [65/190], loss=103.8655
	step [66/190], loss=110.5365
	step [67/190], loss=89.7217
	step [68/190], loss=108.8099
	step [69/190], loss=105.8206
	step [70/190], loss=101.0440
	step [71/190], loss=109.5884
	step [72/190], loss=102.8872
	step [73/190], loss=107.8180
	step [74/190], loss=96.9119
	step [75/190], loss=98.7703
	step [76/190], loss=113.3805
	step [77/190], loss=101.5496
	step [78/190], loss=88.5465
	step [79/190], loss=119.4449
	step [80/190], loss=97.8949
	step [81/190], loss=103.5724
	step [82/190], loss=115.0230
	step [83/190], loss=104.8284
	step [84/190], loss=116.6134
	step [85/190], loss=105.0655
	step [86/190], loss=94.0137
	step [87/190], loss=113.4552
	step [88/190], loss=120.1826
	step [89/190], loss=108.7988
	step [90/190], loss=107.9023
	step [91/190], loss=92.3058
	step [92/190], loss=115.0782
	step [93/190], loss=119.0537
	step [94/190], loss=104.0956
	step [95/190], loss=95.1712
	step [96/190], loss=107.0851
	step [97/190], loss=99.2127
	step [98/190], loss=87.0230
	step [99/190], loss=118.2652
	step [100/190], loss=112.9737
	step [101/190], loss=107.4754
	step [102/190], loss=96.8778
	step [103/190], loss=120.0655
	step [104/190], loss=111.4732
	step [105/190], loss=95.5276
	step [106/190], loss=95.1073
	step [107/190], loss=113.6067
	step [108/190], loss=109.6188
	step [109/190], loss=93.6292
	step [110/190], loss=101.1799
	step [111/190], loss=99.8315
	step [112/190], loss=100.5959
	step [113/190], loss=124.3194
	step [114/190], loss=125.8096
	step [115/190], loss=110.5927
	step [116/190], loss=88.1890
	step [117/190], loss=106.1913
	step [118/190], loss=87.2211
	step [119/190], loss=101.4213
	step [120/190], loss=98.8931
	step [121/190], loss=97.9847
	step [122/190], loss=121.9874
	step [123/190], loss=109.4207
	step [124/190], loss=109.3882
	step [125/190], loss=120.2638
	step [126/190], loss=109.3755
	step [127/190], loss=97.2237
	step [128/190], loss=110.1141
	step [129/190], loss=108.5893
	step [130/190], loss=108.0312
	step [131/190], loss=100.1990
	step [132/190], loss=117.4121
	step [133/190], loss=121.8865
	step [134/190], loss=119.2262
	step [135/190], loss=118.0483
	step [136/190], loss=100.4091
	step [137/190], loss=98.6409
	step [138/190], loss=104.6724
	step [139/190], loss=107.3593
	step [140/190], loss=117.4318
	step [141/190], loss=106.9023
	step [142/190], loss=106.3344
	step [143/190], loss=105.9511
	step [144/190], loss=94.9429
	step [145/190], loss=94.3618
	step [146/190], loss=109.3685
	step [147/190], loss=117.5844
	step [148/190], loss=87.1576
	step [149/190], loss=103.9488
	step [150/190], loss=101.0199
	step [151/190], loss=100.0159
	step [152/190], loss=110.3179
	step [153/190], loss=107.0385
	step [154/190], loss=116.0881
	step [155/190], loss=110.3552
	step [156/190], loss=109.0578
	step [157/190], loss=119.4708
	step [158/190], loss=96.4164
	step [159/190], loss=96.5316
	step [160/190], loss=112.0281
	step [161/190], loss=95.8438
	step [162/190], loss=120.2302
	step [163/190], loss=115.2655
	step [164/190], loss=111.2225
	step [165/190], loss=80.0628
	step [166/190], loss=88.8560
	step [167/190], loss=110.0283
	step [168/190], loss=115.6781
	step [169/190], loss=106.2742
	step [170/190], loss=110.4600
	step [171/190], loss=94.7529
	step [172/190], loss=91.6779
	step [173/190], loss=105.2685
	step [174/190], loss=101.0691
	step [175/190], loss=103.2466
	step [176/190], loss=96.3486
	step [177/190], loss=103.9850
	step [178/190], loss=111.2428
	step [179/190], loss=89.4741
	step [180/190], loss=117.2287
	step [181/190], loss=121.1431
	step [182/190], loss=99.1280
	step [183/190], loss=107.6237
	step [184/190], loss=109.9834
	step [185/190], loss=102.6818
	step [186/190], loss=97.2777
	step [187/190], loss=114.6998
	step [188/190], loss=94.0257
	step [189/190], loss=100.4152
	step [190/190], loss=90.3414
	Evaluating
	loss=0.0437, precision=0.5108, recall=0.8902, f1=0.6491
Training epoch 9
	step [1/190], loss=100.4980
	step [2/190], loss=111.7735
	step [3/190], loss=107.4652
	step [4/190], loss=105.7848
	step [5/190], loss=90.9705
	step [6/190], loss=111.3629
	step [7/190], loss=103.4313
	step [8/190], loss=80.6254
	step [9/190], loss=105.4216
	step [10/190], loss=100.0997
	step [11/190], loss=114.6730
	step [12/190], loss=114.1181
	step [13/190], loss=104.0169
	step [14/190], loss=93.0324
	step [15/190], loss=99.9650
	step [16/190], loss=119.7807
	step [17/190], loss=111.7888
	step [18/190], loss=108.6354
	step [19/190], loss=97.1779
	step [20/190], loss=103.4750
	step [21/190], loss=100.3932
	step [22/190], loss=93.8461
	step [23/190], loss=109.7408
	step [24/190], loss=88.1236
	step [25/190], loss=110.6689
	step [26/190], loss=105.3500
	step [27/190], loss=113.1022
	step [28/190], loss=98.8847
	step [29/190], loss=116.0557
	step [30/190], loss=97.6802
	step [31/190], loss=102.4810
	step [32/190], loss=118.1498
	step [33/190], loss=93.5886
	step [34/190], loss=112.8292
	step [35/190], loss=107.3685
	step [36/190], loss=101.8919
	step [37/190], loss=90.3557
	step [38/190], loss=91.2355
	step [39/190], loss=104.6875
	step [40/190], loss=124.7472
	step [41/190], loss=96.2624
	step [42/190], loss=97.2533
	step [43/190], loss=93.8720
	step [44/190], loss=123.4324
	step [45/190], loss=123.1061
	step [46/190], loss=107.3056
	step [47/190], loss=94.1794
	step [48/190], loss=105.0209
	step [49/190], loss=90.5225
	step [50/190], loss=107.4895
	step [51/190], loss=110.1967
	step [52/190], loss=108.7379
	step [53/190], loss=107.6841
	step [54/190], loss=98.4624
	step [55/190], loss=109.7491
	step [56/190], loss=101.6523
	step [57/190], loss=96.4641
	step [58/190], loss=95.3406
	step [59/190], loss=104.0740
	step [60/190], loss=100.2130
	step [61/190], loss=96.5596
	step [62/190], loss=104.9779
	step [63/190], loss=92.1160
	step [64/190], loss=110.6566
	step [65/190], loss=81.9779
	step [66/190], loss=102.0200
	step [67/190], loss=114.9315
	step [68/190], loss=83.2073
	step [69/190], loss=98.5233
	step [70/190], loss=107.2279
	step [71/190], loss=111.2612
	step [72/190], loss=98.2483
	step [73/190], loss=102.4197
	step [74/190], loss=103.7006
	step [75/190], loss=90.9523
	step [76/190], loss=105.4156
	step [77/190], loss=100.9676
	step [78/190], loss=107.6566
	step [79/190], loss=99.8365
	step [80/190], loss=102.1922
	step [81/190], loss=105.0306
	step [82/190], loss=111.9024
	step [83/190], loss=90.1798
	step [84/190], loss=99.0276
	step [85/190], loss=102.3058
	step [86/190], loss=99.4391
	step [87/190], loss=103.2672
	step [88/190], loss=99.2347
	step [89/190], loss=111.1121
	step [90/190], loss=99.5924
	step [91/190], loss=98.6816
	step [92/190], loss=100.8651
	step [93/190], loss=121.6659
	step [94/190], loss=103.7441
	step [95/190], loss=104.9906
	step [96/190], loss=96.6732
	step [97/190], loss=113.1350
	step [98/190], loss=88.8627
	step [99/190], loss=101.0174
	step [100/190], loss=100.2392
	step [101/190], loss=116.6559
	step [102/190], loss=119.8035
	step [103/190], loss=98.2865
	step [104/190], loss=98.9020
	step [105/190], loss=116.1510
	step [106/190], loss=99.8739
	step [107/190], loss=95.4284
	step [108/190], loss=86.1286
	step [109/190], loss=91.7062
	step [110/190], loss=88.5043
	step [111/190], loss=98.5684
	step [112/190], loss=109.1111
	step [113/190], loss=109.0208
	step [114/190], loss=114.2856
	step [115/190], loss=107.1984
	step [116/190], loss=108.9106
	step [117/190], loss=102.1710
	step [118/190], loss=93.6112
	step [119/190], loss=104.1030
	step [120/190], loss=102.9865
	step [121/190], loss=111.3587
	step [122/190], loss=117.7570
	step [123/190], loss=75.3271
	step [124/190], loss=100.9455
	step [125/190], loss=108.7944
	step [126/190], loss=103.8079
	step [127/190], loss=108.3703
	step [128/190], loss=96.9438
	step [129/190], loss=113.6447
	step [130/190], loss=86.5141
	step [131/190], loss=124.0870
	step [132/190], loss=106.4686
	step [133/190], loss=115.6002
	step [134/190], loss=102.9403
	step [135/190], loss=112.5927
	step [136/190], loss=101.9359
	step [137/190], loss=109.7667
	step [138/190], loss=109.3604
	step [139/190], loss=109.1082
	step [140/190], loss=105.7746
	step [141/190], loss=114.6783
	step [142/190], loss=114.9173
	step [143/190], loss=101.1480
	step [144/190], loss=99.4059
	step [145/190], loss=100.1024
	step [146/190], loss=118.9390
	step [147/190], loss=91.3985
	step [148/190], loss=113.0667
	step [149/190], loss=84.5194
	step [150/190], loss=108.9357
	step [151/190], loss=108.5094
	step [152/190], loss=101.7416
	step [153/190], loss=119.5794
	step [154/190], loss=102.1644
	step [155/190], loss=116.1569
	step [156/190], loss=114.1873
	step [157/190], loss=100.1776
	step [158/190], loss=100.6317
	step [159/190], loss=103.0690
	step [160/190], loss=92.7106
	step [161/190], loss=109.6269
	step [162/190], loss=113.9526
	step [163/190], loss=121.3480
	step [164/190], loss=97.7808
	step [165/190], loss=118.2128
	step [166/190], loss=120.0549
	step [167/190], loss=106.1456
	step [168/190], loss=94.9331
	step [169/190], loss=93.1528
	step [170/190], loss=94.1923
	step [171/190], loss=115.2402
	step [172/190], loss=97.0928
	step [173/190], loss=112.7694
	step [174/190], loss=98.7158
	step [175/190], loss=113.3654
	step [176/190], loss=99.4924
	step [177/190], loss=93.3255
	step [178/190], loss=98.2718
	step [179/190], loss=97.4291
	step [180/190], loss=110.8256
	step [181/190], loss=97.3816
	step [182/190], loss=106.8389
	step [183/190], loss=99.9729
	step [184/190], loss=112.4161
	step [185/190], loss=110.0205
	step [186/190], loss=101.9187
	step [187/190], loss=111.1365
	step [188/190], loss=115.2170
	step [189/190], loss=92.5564
	step [190/190], loss=96.6827
	Evaluating
	loss=0.0388, precision=0.4737, recall=0.8994, f1=0.6205
Training epoch 10
	step [1/190], loss=111.7761
	step [2/190], loss=110.4246
	step [3/190], loss=110.6520
	step [4/190], loss=91.2800
	step [5/190], loss=88.9193
	step [6/190], loss=105.1360
	step [7/190], loss=90.4708
	step [8/190], loss=105.8727
	step [9/190], loss=100.5151
	step [10/190], loss=102.0204
	step [11/190], loss=108.2932
	step [12/190], loss=95.1355
	step [13/190], loss=100.7007
	step [14/190], loss=92.0982
	step [15/190], loss=91.3039
	step [16/190], loss=92.5709
	step [17/190], loss=85.7985
	step [18/190], loss=101.2845
	step [19/190], loss=100.3846
	step [20/190], loss=93.7627
	step [21/190], loss=97.4227
	step [22/190], loss=94.3470
	step [23/190], loss=100.4328
	step [24/190], loss=106.6675
	step [25/190], loss=99.9893
	step [26/190], loss=104.2989
	step [27/190], loss=108.7760
	step [28/190], loss=104.7667
	step [29/190], loss=107.0855
	step [30/190], loss=98.8731
	step [31/190], loss=113.7461
	step [32/190], loss=109.1140
	step [33/190], loss=103.0520
	step [34/190], loss=103.0095
	step [35/190], loss=98.8746
	step [36/190], loss=120.5522
	step [37/190], loss=93.7491
	step [38/190], loss=107.7886
	step [39/190], loss=105.2512
	step [40/190], loss=104.9501
	step [41/190], loss=98.3209
	step [42/190], loss=106.0065
	step [43/190], loss=89.2362
	step [44/190], loss=107.5829
	step [45/190], loss=108.8450
	step [46/190], loss=101.2207
	step [47/190], loss=106.9240
	step [48/190], loss=110.2710
	step [49/190], loss=90.4768
	step [50/190], loss=121.5728
	step [51/190], loss=96.2608
	step [52/190], loss=91.0752
	step [53/190], loss=96.2973
	step [54/190], loss=109.9657
	step [55/190], loss=120.2654
	step [56/190], loss=101.4679
	step [57/190], loss=121.1044
	step [58/190], loss=106.9314
	step [59/190], loss=98.4166
	step [60/190], loss=96.6918
	step [61/190], loss=105.6592
	step [62/190], loss=109.9325
	step [63/190], loss=105.3911
	step [64/190], loss=97.2812
	step [65/190], loss=99.2673
	step [66/190], loss=93.3289
	step [67/190], loss=110.7332
	step [68/190], loss=113.8325
	step [69/190], loss=102.4440
	step [70/190], loss=88.5702
	step [71/190], loss=103.7365
	step [72/190], loss=86.4806
	step [73/190], loss=107.6744
	step [74/190], loss=108.4360
	step [75/190], loss=103.5351
	step [76/190], loss=101.1592
	step [77/190], loss=101.8229
	step [78/190], loss=92.9049
	step [79/190], loss=98.9168
	step [80/190], loss=107.7590
	step [81/190], loss=85.5196
	step [82/190], loss=87.5785
	step [83/190], loss=101.6360
	step [84/190], loss=113.7197
	step [85/190], loss=93.3806
	step [86/190], loss=104.6759
	step [87/190], loss=118.4255
	step [88/190], loss=121.4269
	step [89/190], loss=113.2046
	step [90/190], loss=96.6064
	step [91/190], loss=85.8876
	step [92/190], loss=110.3059
	step [93/190], loss=98.8914
	step [94/190], loss=107.9257
	step [95/190], loss=91.1191
	step [96/190], loss=109.8888
	step [97/190], loss=103.9926
	step [98/190], loss=98.0332
	step [99/190], loss=101.5571
	step [100/190], loss=106.3397
	step [101/190], loss=121.1741
	step [102/190], loss=107.4099
	step [103/190], loss=113.1889
	step [104/190], loss=101.7123
	step [105/190], loss=103.6974
	step [106/190], loss=87.1705
	step [107/190], loss=106.7105
	step [108/190], loss=93.7086
	step [109/190], loss=101.8026
	step [110/190], loss=105.3552
	step [111/190], loss=119.1782
	step [112/190], loss=96.8994
	step [113/190], loss=89.1377
	step [114/190], loss=98.7138
	step [115/190], loss=100.3608
	step [116/190], loss=107.0050
	step [117/190], loss=95.6749
	step [118/190], loss=103.6922
	step [119/190], loss=95.5156
	step [120/190], loss=100.7251
	step [121/190], loss=108.3880
	step [122/190], loss=85.8842
	step [123/190], loss=101.5741
	step [124/190], loss=89.4931
	step [125/190], loss=90.8624
	step [126/190], loss=101.8819
	step [127/190], loss=109.3851
	step [128/190], loss=99.4691
	step [129/190], loss=114.6865
	step [130/190], loss=81.1315
	step [131/190], loss=95.5336
	step [132/190], loss=109.9513
	step [133/190], loss=96.1917
	step [134/190], loss=100.8767
	step [135/190], loss=113.8041
	step [136/190], loss=115.6041
	step [137/190], loss=93.6231
	step [138/190], loss=87.8632
	step [139/190], loss=84.1076
	step [140/190], loss=97.3568
	step [141/190], loss=114.1070
	step [142/190], loss=89.9160
	step [143/190], loss=114.9751
	step [144/190], loss=100.1817
	step [145/190], loss=97.0655
	step [146/190], loss=101.1839
	step [147/190], loss=98.5363
	step [148/190], loss=102.6494
	step [149/190], loss=102.0374
	step [150/190], loss=85.7423
	step [151/190], loss=107.9810
	step [152/190], loss=110.0793
	step [153/190], loss=94.1492
	step [154/190], loss=113.2166
	step [155/190], loss=92.2916
	step [156/190], loss=106.2790
	step [157/190], loss=109.0576
	step [158/190], loss=105.3172
	step [159/190], loss=104.2329
	step [160/190], loss=99.3323
	step [161/190], loss=102.9049
	step [162/190], loss=109.3913
	step [163/190], loss=101.5979
	step [164/190], loss=112.2724
	step [165/190], loss=104.9320
	step [166/190], loss=111.2516
	step [167/190], loss=100.5030
	step [168/190], loss=112.3993
	step [169/190], loss=97.1045
	step [170/190], loss=102.4290
	step [171/190], loss=99.7071
	step [172/190], loss=100.8034
	step [173/190], loss=89.8379
	step [174/190], loss=107.7743
	step [175/190], loss=108.2764
	step [176/190], loss=99.4786
	step [177/190], loss=99.1493
	step [178/190], loss=97.3415
	step [179/190], loss=111.7363
	step [180/190], loss=86.5393
	step [181/190], loss=103.0388
	step [182/190], loss=108.7269
	step [183/190], loss=96.5611
	step [184/190], loss=100.1557
	step [185/190], loss=95.9877
	step [186/190], loss=91.7581
	step [187/190], loss=99.0449
	step [188/190], loss=92.8208
	step [189/190], loss=104.7484
	step [190/190], loss=93.3384
	Evaluating
	loss=0.0333, precision=0.5106, recall=0.9044, f1=0.6527
Training epoch 11
	step [1/190], loss=107.4034
	step [2/190], loss=114.0405
	step [3/190], loss=105.0674
	step [4/190], loss=95.6049
	step [5/190], loss=109.7838
	step [6/190], loss=103.3937
	step [7/190], loss=92.1317
	step [8/190], loss=99.2596
	step [9/190], loss=93.9048
	step [10/190], loss=106.2412
	step [11/190], loss=122.1538
	step [12/190], loss=104.7482
	step [13/190], loss=108.0065
	step [14/190], loss=90.0845
	step [15/190], loss=100.1455
	step [16/190], loss=112.4429
	step [17/190], loss=112.4273
	step [18/190], loss=119.0144
	step [19/190], loss=100.5360
	step [20/190], loss=119.0340
	step [21/190], loss=94.5299
	step [22/190], loss=107.2619
	step [23/190], loss=99.9951
	step [24/190], loss=95.4542
	step [25/190], loss=96.5377
	step [26/190], loss=122.3687
	step [27/190], loss=91.5758
	step [28/190], loss=96.6231
	step [29/190], loss=90.6504
	step [30/190], loss=86.0539
	step [31/190], loss=99.0788
	step [32/190], loss=111.3059
	step [33/190], loss=102.1344
	step [34/190], loss=106.5502
	step [35/190], loss=115.4097
	step [36/190], loss=100.2309
	step [37/190], loss=101.7069
	step [38/190], loss=104.1273
	step [39/190], loss=101.1090
	step [40/190], loss=83.5442
	step [41/190], loss=88.2287
	step [42/190], loss=115.9818
	step [43/190], loss=101.9084
	step [44/190], loss=98.9765
	step [45/190], loss=95.8792
	step [46/190], loss=108.4080
	step [47/190], loss=96.9294
	step [48/190], loss=117.8382
	step [49/190], loss=95.7380
	step [50/190], loss=81.1390
	step [51/190], loss=103.4970
	step [52/190], loss=91.6802
	step [53/190], loss=93.2799
	step [54/190], loss=116.0841
	step [55/190], loss=113.3948
	step [56/190], loss=100.8389
	step [57/190], loss=87.2509
	step [58/190], loss=105.8971
	step [59/190], loss=86.3703
	step [60/190], loss=97.2715
	step [61/190], loss=117.6285
	step [62/190], loss=107.2645
	step [63/190], loss=108.9719
	step [64/190], loss=90.8403
	step [65/190], loss=89.7080
	step [66/190], loss=99.0453
	step [67/190], loss=110.1761
	step [68/190], loss=116.2704
	step [69/190], loss=96.8901
	step [70/190], loss=85.6230
	step [71/190], loss=88.6215
	step [72/190], loss=99.4681
	step [73/190], loss=102.7778
	step [74/190], loss=95.1990
	step [75/190], loss=106.0965
	step [76/190], loss=100.3102
	step [77/190], loss=116.1216
	step [78/190], loss=93.5187
	step [79/190], loss=99.4310
	step [80/190], loss=90.7571
	step [81/190], loss=101.0068
	step [82/190], loss=106.5842
	step [83/190], loss=106.3208
	step [84/190], loss=91.5527
	step [85/190], loss=103.9551
	step [86/190], loss=95.9893
	step [87/190], loss=106.1644
	step [88/190], loss=87.6097
	step [89/190], loss=99.7292
	step [90/190], loss=88.2681
	step [91/190], loss=83.3281
	step [92/190], loss=97.9006
	step [93/190], loss=101.1623
	step [94/190], loss=88.7371
	step [95/190], loss=93.7800
	step [96/190], loss=85.7808
	step [97/190], loss=103.0493
	step [98/190], loss=107.7069
	step [99/190], loss=99.8508
	step [100/190], loss=107.0450
	step [101/190], loss=109.5541
	step [102/190], loss=94.1029
	step [103/190], loss=107.5335
	step [104/190], loss=84.0478
	step [105/190], loss=95.4552
	step [106/190], loss=107.3828
	step [107/190], loss=99.9064
	step [108/190], loss=98.4118
	step [109/190], loss=120.4851
	step [110/190], loss=106.6532
	step [111/190], loss=112.7543
	step [112/190], loss=90.1235
	step [113/190], loss=85.1150
	step [114/190], loss=111.9897
	step [115/190], loss=95.1822
	step [116/190], loss=97.2927
	step [117/190], loss=94.7201
	step [118/190], loss=115.4771
	step [119/190], loss=97.6202
	step [120/190], loss=89.8812
	step [121/190], loss=96.4092
	step [122/190], loss=104.2981
	step [123/190], loss=94.3429
	step [124/190], loss=104.0125
	step [125/190], loss=102.7856
	step [126/190], loss=102.7640
	step [127/190], loss=103.8984
	step [128/190], loss=82.9295
	step [129/190], loss=84.8369
	step [130/190], loss=92.1798
	step [131/190], loss=99.5794
	step [132/190], loss=103.7399
	step [133/190], loss=110.8247
	step [134/190], loss=100.5152
	step [135/190], loss=86.7174
	step [136/190], loss=86.1273
	step [137/190], loss=94.8164
	step [138/190], loss=98.4569
	step [139/190], loss=100.4318
	step [140/190], loss=109.5722
	step [141/190], loss=111.4807
	step [142/190], loss=107.3548
	step [143/190], loss=93.0961
	step [144/190], loss=95.1519
	step [145/190], loss=84.6931
	step [146/190], loss=88.2680
	step [147/190], loss=110.3067
	step [148/190], loss=95.7901
	step [149/190], loss=103.3407
	step [150/190], loss=92.3196
	step [151/190], loss=98.2896
	step [152/190], loss=90.4975
	step [153/190], loss=105.0940
	step [154/190], loss=101.0222
	step [155/190], loss=105.6759
	step [156/190], loss=99.2587
	step [157/190], loss=106.8533
	step [158/190], loss=100.9663
	step [159/190], loss=101.4088
	step [160/190], loss=104.5393
	step [161/190], loss=111.3148
	step [162/190], loss=99.3043
	step [163/190], loss=106.4507
	step [164/190], loss=99.8875
	step [165/190], loss=91.6808
	step [166/190], loss=111.1924
	step [167/190], loss=89.3089
	step [168/190], loss=84.6506
	step [169/190], loss=111.0092
	step [170/190], loss=110.0424
	step [171/190], loss=90.8482
	step [172/190], loss=106.4665
	step [173/190], loss=92.7678
	step [174/190], loss=107.0062
	step [175/190], loss=106.6202
	step [176/190], loss=89.6121
	step [177/190], loss=116.7406
	step [178/190], loss=80.5651
	step [179/190], loss=99.0498
	step [180/190], loss=99.9952
	step [181/190], loss=112.6621
	step [182/190], loss=96.7338
	step [183/190], loss=99.5276
	step [184/190], loss=101.6776
	step [185/190], loss=108.3317
	step [186/190], loss=89.5592
	step [187/190], loss=94.5507
	step [188/190], loss=105.0513
	step [189/190], loss=104.9504
	step [190/190], loss=80.3343
	Evaluating
	loss=0.0322, precision=0.4517, recall=0.9072, f1=0.6031
Training epoch 12
	step [1/190], loss=103.7914
	step [2/190], loss=116.6096
	step [3/190], loss=103.7364
	step [4/190], loss=93.7590
	step [5/190], loss=101.3015
	step [6/190], loss=117.6667
	step [7/190], loss=79.4769
	step [8/190], loss=88.1739
	step [9/190], loss=106.2064
	step [10/190], loss=98.5872
	step [11/190], loss=100.3211
	step [12/190], loss=106.1104
	step [13/190], loss=94.8111
	step [14/190], loss=107.3409
	step [15/190], loss=103.2285
	step [16/190], loss=81.7649
	step [17/190], loss=112.9534
	step [18/190], loss=94.1054
	step [19/190], loss=109.1470
	step [20/190], loss=83.6431
	step [21/190], loss=103.2963
	step [22/190], loss=110.7920
	step [23/190], loss=88.2978
	step [24/190], loss=91.4006
	step [25/190], loss=98.3929
	step [26/190], loss=99.9734
	step [27/190], loss=90.8645
	step [28/190], loss=91.0611
	step [29/190], loss=97.3736
	step [30/190], loss=96.2685
	step [31/190], loss=96.9762
	step [32/190], loss=90.3924
	step [33/190], loss=96.9196
	step [34/190], loss=95.8159
	step [35/190], loss=95.2342
	step [36/190], loss=95.1628
	step [37/190], loss=82.2535
	step [38/190], loss=96.0840
	step [39/190], loss=103.6191
	step [40/190], loss=93.6507
	step [41/190], loss=91.5999
	step [42/190], loss=96.8306
	step [43/190], loss=117.8965
	step [44/190], loss=105.3439
	step [45/190], loss=106.5435
	step [46/190], loss=110.2228
	step [47/190], loss=106.1731
	step [48/190], loss=91.8158
	step [49/190], loss=89.7872
	step [50/190], loss=94.5313
	step [51/190], loss=88.2599
	step [52/190], loss=101.5879
	step [53/190], loss=99.4504
	step [54/190], loss=107.0581
	step [55/190], loss=103.5992
	step [56/190], loss=92.0725
	step [57/190], loss=112.0007
	step [58/190], loss=92.8548
	step [59/190], loss=96.2218
	step [60/190], loss=94.3488
	step [61/190], loss=109.7676
	step [62/190], loss=101.9592
	step [63/190], loss=103.3612
	step [64/190], loss=105.8485
	step [65/190], loss=85.2940
	step [66/190], loss=109.9495
	step [67/190], loss=95.6915
	step [68/190], loss=99.0064
	step [69/190], loss=114.2446
	step [70/190], loss=94.3956
	step [71/190], loss=100.4533
	step [72/190], loss=95.1487
	step [73/190], loss=105.4177
	step [74/190], loss=114.2299
	step [75/190], loss=100.6445
	step [76/190], loss=94.8093
	step [77/190], loss=108.2282
	step [78/190], loss=84.9200
	step [79/190], loss=104.5938
	step [80/190], loss=92.6609
	step [81/190], loss=95.8916
	step [82/190], loss=82.1694
	step [83/190], loss=104.2563
	step [84/190], loss=106.4971
	step [85/190], loss=104.5856
	step [86/190], loss=109.8833
	step [87/190], loss=106.7005
	step [88/190], loss=99.0950
	step [89/190], loss=101.3163
	step [90/190], loss=102.7238
	step [91/190], loss=90.0096
	step [92/190], loss=97.5020
	step [93/190], loss=99.9564
	step [94/190], loss=98.2074
	step [95/190], loss=103.2896
	step [96/190], loss=105.4859
	step [97/190], loss=94.4645
	step [98/190], loss=86.0253
	step [99/190], loss=95.0910
	step [100/190], loss=90.7124
	step [101/190], loss=85.9648
	step [102/190], loss=101.3009
	step [103/190], loss=93.4666
	step [104/190], loss=90.2008
	step [105/190], loss=87.0484
	step [106/190], loss=90.9374
	step [107/190], loss=119.1984
	step [108/190], loss=95.7343
	step [109/190], loss=103.1903
	step [110/190], loss=90.6653
	step [111/190], loss=93.7442
	step [112/190], loss=92.3494
	step [113/190], loss=93.3542
	step [114/190], loss=93.4897
	step [115/190], loss=101.7969
	step [116/190], loss=111.0716
	step [117/190], loss=101.5041
	step [118/190], loss=92.6293
	step [119/190], loss=92.3009
	step [120/190], loss=89.0110
	step [121/190], loss=111.9963
	step [122/190], loss=95.3667
	step [123/190], loss=85.0327
	step [124/190], loss=96.9029
	step [125/190], loss=87.9799
	step [126/190], loss=97.4203
	step [127/190], loss=107.0608
	step [128/190], loss=101.0851
	step [129/190], loss=90.9318
	step [130/190], loss=88.7306
	step [131/190], loss=87.6798
	step [132/190], loss=116.6406
	step [133/190], loss=98.3339
	step [134/190], loss=101.5776
	step [135/190], loss=83.6803
	step [136/190], loss=88.5302
	step [137/190], loss=106.0919
	step [138/190], loss=90.9767
	step [139/190], loss=110.2155
	step [140/190], loss=91.0818
	step [141/190], loss=115.8589
	step [142/190], loss=96.7896
	step [143/190], loss=96.7388
	step [144/190], loss=94.7099
	step [145/190], loss=97.2743
	step [146/190], loss=92.5050
	step [147/190], loss=106.7301
	step [148/190], loss=87.9626
	step [149/190], loss=104.1916
	step [150/190], loss=83.6582
	step [151/190], loss=105.6631
	step [152/190], loss=101.7881
	step [153/190], loss=86.8691
	step [154/190], loss=107.8802
	step [155/190], loss=81.7418
	step [156/190], loss=102.6357
	step [157/190], loss=108.8187
	step [158/190], loss=99.6931
	step [159/190], loss=101.1523
	step [160/190], loss=78.0691
	step [161/190], loss=95.8688
	step [162/190], loss=103.4497
	step [163/190], loss=94.9358
	step [164/190], loss=95.1812
	step [165/190], loss=109.6578
	step [166/190], loss=104.4277
	step [167/190], loss=119.1094
	step [168/190], loss=103.3413
	step [169/190], loss=82.9129
	step [170/190], loss=94.0940
	step [171/190], loss=103.3056
	step [172/190], loss=108.9121
	step [173/190], loss=98.0089
	step [174/190], loss=92.3535
	step [175/190], loss=100.0071
	step [176/190], loss=93.5008
	step [177/190], loss=87.4274
	step [178/190], loss=92.6284
	step [179/190], loss=97.3697
	step [180/190], loss=99.4464
	step [181/190], loss=118.3833
	step [182/190], loss=100.9699
	step [183/190], loss=107.9509
	step [184/190], loss=91.9766
	step [185/190], loss=89.9194
	step [186/190], loss=108.3848
	step [187/190], loss=93.3614
	step [188/190], loss=101.4062
	step [189/190], loss=92.0928
	step [190/190], loss=96.0544
	Evaluating
	loss=0.0281, precision=0.4282, recall=0.8952, f1=0.5793
Training epoch 13
	step [1/190], loss=106.7904
	step [2/190], loss=94.9100
	step [3/190], loss=95.4963
	step [4/190], loss=104.4139
	step [5/190], loss=101.0717
	step [6/190], loss=95.0988
	step [7/190], loss=91.9448
	step [8/190], loss=84.0054
	step [9/190], loss=92.5039
	step [10/190], loss=100.8468
	step [11/190], loss=106.0566
	step [12/190], loss=93.1728
	step [13/190], loss=86.3833
	step [14/190], loss=104.0711
	step [15/190], loss=104.9534
	step [16/190], loss=110.1997
	step [17/190], loss=86.5534
	step [18/190], loss=102.8476
	step [19/190], loss=102.0188
	step [20/190], loss=100.0352
	step [21/190], loss=89.0498
	step [22/190], loss=96.0458
	step [23/190], loss=94.6866
	step [24/190], loss=95.0829
	step [25/190], loss=104.4269
	step [26/190], loss=95.2867
	step [27/190], loss=92.1794
	step [28/190], loss=97.1400
	step [29/190], loss=89.7698
	step [30/190], loss=92.7869
	step [31/190], loss=99.5668
	step [32/190], loss=88.2409
	step [33/190], loss=93.7262
	step [34/190], loss=122.5660
	step [35/190], loss=89.4829
	step [36/190], loss=96.9361
	step [37/190], loss=98.6857
	step [38/190], loss=83.9574
	step [39/190], loss=92.5396
	step [40/190], loss=100.9569
	step [41/190], loss=100.6157
	step [42/190], loss=106.6631
	step [43/190], loss=107.4875
	step [44/190], loss=88.5496
	step [45/190], loss=92.3408
	step [46/190], loss=83.0127
	step [47/190], loss=102.4361
	step [48/190], loss=93.9148
	step [49/190], loss=97.9940
	step [50/190], loss=82.5751
	step [51/190], loss=88.6083
	step [52/190], loss=109.6413
	step [53/190], loss=114.1098
	step [54/190], loss=95.1003
	step [55/190], loss=95.4811
	step [56/190], loss=91.2192
	step [57/190], loss=106.0760
	step [58/190], loss=103.9808
	step [59/190], loss=106.8477
	step [60/190], loss=85.2070
	step [61/190], loss=97.7034
	step [62/190], loss=93.5285
	step [63/190], loss=98.9358
	step [64/190], loss=100.9902
	step [65/190], loss=111.2802
	step [66/190], loss=98.2691
	step [67/190], loss=107.1700
	step [68/190], loss=110.0998
	step [69/190], loss=101.6490
	step [70/190], loss=100.7197
	step [71/190], loss=92.4979
	step [72/190], loss=110.4160
	step [73/190], loss=81.3690
	step [74/190], loss=113.9234
	step [75/190], loss=87.6696
	step [76/190], loss=106.0840
	step [77/190], loss=96.9330
	step [78/190], loss=87.5498
	step [79/190], loss=95.9350
	step [80/190], loss=110.9536
	step [81/190], loss=98.6951
	step [82/190], loss=103.1749
	step [83/190], loss=95.9731
	step [84/190], loss=101.6060
	step [85/190], loss=96.8971
	step [86/190], loss=90.1937
	step [87/190], loss=100.1963
	step [88/190], loss=98.1702
	step [89/190], loss=96.2611
	step [90/190], loss=101.7886
	step [91/190], loss=83.6499
	step [92/190], loss=86.7785
	step [93/190], loss=93.2859
	step [94/190], loss=122.6506
	step [95/190], loss=107.8805
	step [96/190], loss=106.7666
	step [97/190], loss=86.0420
	step [98/190], loss=94.9180
	step [99/190], loss=73.8642
	step [100/190], loss=97.7846
	step [101/190], loss=107.8750
	step [102/190], loss=95.8903
	step [103/190], loss=82.9842
	step [104/190], loss=100.2730
	step [105/190], loss=102.2963
	step [106/190], loss=101.9115
	step [107/190], loss=103.0122
	step [108/190], loss=97.9485
	step [109/190], loss=94.2623
	step [110/190], loss=102.0751
	step [111/190], loss=89.9607
	step [112/190], loss=92.0745
	step [113/190], loss=92.9873
	step [114/190], loss=90.8686
	step [115/190], loss=94.2310
	step [116/190], loss=107.6340
	step [117/190], loss=92.1000
	step [118/190], loss=101.0645
	step [119/190], loss=107.3293
	step [120/190], loss=97.7345
	step [121/190], loss=90.6310
	step [122/190], loss=100.0673
	step [123/190], loss=98.2060
	step [124/190], loss=107.3068
	step [125/190], loss=82.3947
	step [126/190], loss=95.8024
	step [127/190], loss=93.9648
	step [128/190], loss=95.0942
	step [129/190], loss=91.5823
	step [130/190], loss=96.8741
	step [131/190], loss=97.7944
	step [132/190], loss=114.2434
	step [133/190], loss=90.9407
	step [134/190], loss=102.8700
	step [135/190], loss=100.8360
	step [136/190], loss=89.4394
	step [137/190], loss=91.8387
	step [138/190], loss=98.4231
	step [139/190], loss=98.6583
	step [140/190], loss=102.4294
	step [141/190], loss=97.5398
	step [142/190], loss=93.5681
	step [143/190], loss=94.3899
	step [144/190], loss=88.8404
	step [145/190], loss=109.4736
	step [146/190], loss=76.2147
	step [147/190], loss=94.5293
	step [148/190], loss=93.8655
	step [149/190], loss=89.7414
	step [150/190], loss=106.3614
	step [151/190], loss=98.0157
	step [152/190], loss=102.4210
	step [153/190], loss=105.6944
	step [154/190], loss=98.0385
	step [155/190], loss=102.5797
	step [156/190], loss=104.6941
	step [157/190], loss=85.4420
	step [158/190], loss=91.5784
	step [159/190], loss=93.5446
	step [160/190], loss=88.0242
	step [161/190], loss=96.1698
	step [162/190], loss=104.9136
	step [163/190], loss=82.9722
	step [164/190], loss=109.4866
	step [165/190], loss=106.5417
	step [166/190], loss=106.2931
	step [167/190], loss=93.5588
	step [168/190], loss=84.7695
	step [169/190], loss=94.9521
	step [170/190], loss=92.1021
	step [171/190], loss=90.6799
	step [172/190], loss=86.3029
	step [173/190], loss=113.8910
	step [174/190], loss=100.1773
	step [175/190], loss=88.8874
	step [176/190], loss=97.1848
	step [177/190], loss=84.4407
	step [178/190], loss=96.1127
	step [179/190], loss=90.7757
	step [180/190], loss=96.9374
	step [181/190], loss=95.9611
	step [182/190], loss=104.4252
	step [183/190], loss=106.2394
	step [184/190], loss=80.9150
	step [185/190], loss=85.1300
	step [186/190], loss=101.5110
	step [187/190], loss=98.9394
	step [188/190], loss=107.3123
	step [189/190], loss=96.5914
	step [190/190], loss=85.9647
	Evaluating
	loss=0.0247, precision=0.4117, recall=0.8829, f1=0.5616
Training epoch 14
	step [1/190], loss=94.2397
	step [2/190], loss=96.9545
	step [3/190], loss=97.2395
	step [4/190], loss=87.3052
	step [5/190], loss=89.0800
	step [6/190], loss=103.7553
	step [7/190], loss=97.3572
	step [8/190], loss=83.6379
	step [9/190], loss=107.4868
	step [10/190], loss=93.6162
	step [11/190], loss=98.2892
	step [12/190], loss=105.9651
	step [13/190], loss=94.6014
	step [14/190], loss=95.9248
	step [15/190], loss=85.3808
	step [16/190], loss=87.4789
	step [17/190], loss=101.7619
	step [18/190], loss=116.8671
	step [19/190], loss=97.7367
	step [20/190], loss=102.7469
	step [21/190], loss=87.2990
	step [22/190], loss=96.8749
	step [23/190], loss=103.4616
	step [24/190], loss=99.1748
	step [25/190], loss=92.2697
	step [26/190], loss=91.6435
	step [27/190], loss=97.5679
	step [28/190], loss=81.6452
	step [29/190], loss=100.7664
	step [30/190], loss=87.5243
	step [31/190], loss=102.0516
	step [32/190], loss=92.7144
	step [33/190], loss=105.1811
	step [34/190], loss=76.5922
	step [35/190], loss=88.7164
	step [36/190], loss=96.1068
	step [37/190], loss=90.3807
	step [38/190], loss=95.9025
	step [39/190], loss=96.0998
	step [40/190], loss=78.7767
	step [41/190], loss=88.5356
	step [42/190], loss=88.0196
	step [43/190], loss=87.2635
	step [44/190], loss=101.0877
	step [45/190], loss=91.9813
	step [46/190], loss=89.4033
	step [47/190], loss=78.9414
	step [48/190], loss=103.6053
	step [49/190], loss=95.2189
	step [50/190], loss=109.3502
	step [51/190], loss=96.8777
	step [52/190], loss=98.0289
	step [53/190], loss=88.2724
	step [54/190], loss=94.9763
	step [55/190], loss=102.3076
	step [56/190], loss=98.8085
	step [57/190], loss=113.9477
	step [58/190], loss=103.4049
	step [59/190], loss=89.9979
	step [60/190], loss=114.2380
	step [61/190], loss=120.9264
	step [62/190], loss=105.5878
	step [63/190], loss=106.0172
	step [64/190], loss=91.0768
	step [65/190], loss=111.6351
	step [66/190], loss=106.7542
	step [67/190], loss=108.7041
	step [68/190], loss=98.8642
	step [69/190], loss=105.9846
	step [70/190], loss=104.1645
	step [71/190], loss=88.6512
	step [72/190], loss=95.2056
	step [73/190], loss=97.3065
	step [74/190], loss=90.5899
	step [75/190], loss=95.6990
	step [76/190], loss=88.4397
	step [77/190], loss=99.4404
	step [78/190], loss=87.7659
	step [79/190], loss=116.7992
	step [80/190], loss=91.2371
	step [81/190], loss=86.9713
	step [82/190], loss=108.2253
	step [83/190], loss=102.4848
	step [84/190], loss=99.5186
	step [85/190], loss=101.5896
	step [86/190], loss=101.2470
	step [87/190], loss=99.3363
	step [88/190], loss=82.3813
	step [89/190], loss=99.0219
	step [90/190], loss=94.8730
	step [91/190], loss=102.8512
	step [92/190], loss=95.2363
	step [93/190], loss=98.7493
	step [94/190], loss=87.0855
	step [95/190], loss=91.7676
	step [96/190], loss=88.7625
	step [97/190], loss=101.2291
	step [98/190], loss=95.5161
	step [99/190], loss=96.6874
	step [100/190], loss=88.6778
	step [101/190], loss=98.6071
	step [102/190], loss=97.5538
	step [103/190], loss=86.6865
	step [104/190], loss=92.6496
	step [105/190], loss=102.3739
	step [106/190], loss=88.5853
	step [107/190], loss=89.5037
	step [108/190], loss=96.5473
	step [109/190], loss=80.0945
	step [110/190], loss=97.1878
	step [111/190], loss=81.5108
	step [112/190], loss=112.7510
	step [113/190], loss=89.4209
	step [114/190], loss=109.2025
	step [115/190], loss=85.2910
	step [116/190], loss=101.3487
	step [117/190], loss=87.4050
	step [118/190], loss=117.2229
	step [119/190], loss=97.2447
	step [120/190], loss=92.2248
	step [121/190], loss=81.2050
	step [122/190], loss=91.6288
	step [123/190], loss=99.7324
	step [124/190], loss=91.4105
	step [125/190], loss=105.0360
	step [126/190], loss=92.2529
	step [127/190], loss=79.3507
	step [128/190], loss=86.7049
	step [129/190], loss=107.3829
	step [130/190], loss=99.8367
	step [131/190], loss=78.7562
	step [132/190], loss=93.3029
	step [133/190], loss=88.9514
	step [134/190], loss=88.1150
	step [135/190], loss=94.0387
	step [136/190], loss=89.6058
	step [137/190], loss=110.4412
	step [138/190], loss=92.9670
	step [139/190], loss=97.7951
	step [140/190], loss=88.5645
	step [141/190], loss=78.4768
	step [142/190], loss=105.2034
	step [143/190], loss=118.4544
	step [144/190], loss=101.9689
	step [145/190], loss=95.7884
	step [146/190], loss=96.8917
	step [147/190], loss=87.2522
	step [148/190], loss=96.4011
	step [149/190], loss=107.8981
	step [150/190], loss=85.7722
	step [151/190], loss=80.7294
	step [152/190], loss=69.5110
	step [153/190], loss=92.6295
	step [154/190], loss=92.3390
	step [155/190], loss=84.5769
	step [156/190], loss=83.3776
	step [157/190], loss=101.0548
	step [158/190], loss=92.8356
	step [159/190], loss=93.1573
	step [160/190], loss=95.2448
	step [161/190], loss=113.6513
	step [162/190], loss=99.4227
	step [163/190], loss=91.6174
	step [164/190], loss=82.4786
	step [165/190], loss=92.8023
	step [166/190], loss=94.9105
	step [167/190], loss=113.3702
	step [168/190], loss=106.2995
	step [169/190], loss=96.5249
	step [170/190], loss=97.6733
	step [171/190], loss=94.8834
	step [172/190], loss=96.0130
	step [173/190], loss=95.2889
	step [174/190], loss=95.8544
	step [175/190], loss=86.1893
	step [176/190], loss=84.5603
	step [177/190], loss=92.7876
	step [178/190], loss=106.2245
	step [179/190], loss=90.7969
	step [180/190], loss=100.4732
	step [181/190], loss=96.0039
	step [182/190], loss=113.1091
	step [183/190], loss=86.5195
	step [184/190], loss=104.5787
	step [185/190], loss=98.7308
	step [186/190], loss=90.8207
	step [187/190], loss=101.8456
	step [188/190], loss=93.5882
	step [189/190], loss=104.1344
	step [190/190], loss=91.2431
	Evaluating
	loss=0.0283, precision=0.3626, recall=0.8886, f1=0.5151
Training epoch 15
	step [1/190], loss=93.3614
	step [2/190], loss=92.2205
	step [3/190], loss=89.0854
	step [4/190], loss=85.5232
	step [5/190], loss=95.3836
	step [6/190], loss=100.5879
	step [7/190], loss=90.8468
	step [8/190], loss=105.3850
	step [9/190], loss=104.6904
	step [10/190], loss=93.4335
	step [11/190], loss=100.0187
	step [12/190], loss=88.7955
	step [13/190], loss=83.0133
	step [14/190], loss=100.4529
	step [15/190], loss=81.6554
	step [16/190], loss=99.7510
	step [17/190], loss=104.8526
	step [18/190], loss=85.0719
	step [19/190], loss=110.8633
	step [20/190], loss=90.5874
	step [21/190], loss=99.3434
	step [22/190], loss=92.7231
	step [23/190], loss=86.9647
	step [24/190], loss=97.0647
	step [25/190], loss=79.9441
	step [26/190], loss=94.9709
	step [27/190], loss=95.5358
	step [28/190], loss=103.7678
	step [29/190], loss=105.2144
	step [30/190], loss=103.1623
	step [31/190], loss=100.8452
	step [32/190], loss=95.9825
	step [33/190], loss=89.2074
	step [34/190], loss=82.3003
	step [35/190], loss=89.7439
	step [36/190], loss=93.7735
	step [37/190], loss=107.7340
	step [38/190], loss=94.0922
	step [39/190], loss=89.1148
	step [40/190], loss=90.5777
	step [41/190], loss=94.7214
	step [42/190], loss=103.8546
	step [43/190], loss=90.7481
	step [44/190], loss=97.7170
	step [45/190], loss=81.7580
	step [46/190], loss=89.9122
	step [47/190], loss=94.8694
	step [48/190], loss=101.9414
	step [49/190], loss=89.2378
	step [50/190], loss=104.5985
	step [51/190], loss=93.8079
	step [52/190], loss=87.3481
	step [53/190], loss=95.2635
	step [54/190], loss=98.3670
	step [55/190], loss=84.2512
	step [56/190], loss=88.2106
	step [57/190], loss=88.0737
	step [58/190], loss=99.7953
	step [59/190], loss=95.6074
	step [60/190], loss=95.2541
	step [61/190], loss=84.8119
	step [62/190], loss=108.6291
	step [63/190], loss=105.6344
	step [64/190], loss=90.5455
	step [65/190], loss=86.7662
	step [66/190], loss=104.2640
	step [67/190], loss=98.0680
	step [68/190], loss=82.5106
	step [69/190], loss=86.3745
	step [70/190], loss=87.4496
	step [71/190], loss=91.1993
	step [72/190], loss=102.2220
	step [73/190], loss=90.1308
	step [74/190], loss=112.5752
	step [75/190], loss=90.0533
	step [76/190], loss=93.3679
	step [77/190], loss=105.3865
	step [78/190], loss=83.7109
	step [79/190], loss=79.6351
	step [80/190], loss=93.5259
	step [81/190], loss=79.0058
	step [82/190], loss=83.6221
	step [83/190], loss=105.7251
	step [84/190], loss=93.5495
	step [85/190], loss=93.5065
	step [86/190], loss=91.1772
	step [87/190], loss=111.2893
	step [88/190], loss=112.7617
	step [89/190], loss=97.7154
	step [90/190], loss=79.6388
	step [91/190], loss=93.5089
	step [92/190], loss=79.0896
	step [93/190], loss=83.5374
	step [94/190], loss=88.3103
	step [95/190], loss=89.8575
	step [96/190], loss=81.9813
	step [97/190], loss=91.9904
	step [98/190], loss=88.3615
	step [99/190], loss=104.1425
	step [100/190], loss=99.5499
	step [101/190], loss=90.0106
	step [102/190], loss=92.8058
	step [103/190], loss=107.1581
	step [104/190], loss=94.8392
	step [105/190], loss=79.1611
	step [106/190], loss=95.7401
	step [107/190], loss=111.7523
	step [108/190], loss=99.7749
	step [109/190], loss=102.4033
	step [110/190], loss=93.9278
	step [111/190], loss=101.2667
	step [112/190], loss=97.4480
	step [113/190], loss=98.6089
	step [114/190], loss=87.7106
	step [115/190], loss=90.5627
	step [116/190], loss=103.1546
	step [117/190], loss=87.3120
	step [118/190], loss=86.9367
	step [119/190], loss=102.9875
	step [120/190], loss=106.5278
	step [121/190], loss=85.1325
	step [122/190], loss=98.5830
	step [123/190], loss=107.0174
	step [124/190], loss=100.1044
	step [125/190], loss=93.1160
	step [126/190], loss=92.1395
	step [127/190], loss=94.4459
	step [128/190], loss=91.7117
	step [129/190], loss=86.2100
	step [130/190], loss=85.0591
	step [131/190], loss=84.8283
	step [132/190], loss=106.7626
	step [133/190], loss=92.2004
	step [134/190], loss=99.7368
	step [135/190], loss=95.0685
	step [136/190], loss=92.7134
	step [137/190], loss=103.7405
	step [138/190], loss=94.1522
	step [139/190], loss=105.8418
	step [140/190], loss=97.7065
	step [141/190], loss=87.2971
	step [142/190], loss=91.8762
	step [143/190], loss=91.3917
	step [144/190], loss=89.5792
	step [145/190], loss=96.9485
	step [146/190], loss=95.4477
	step [147/190], loss=76.7303
	step [148/190], loss=98.6880
	step [149/190], loss=97.8367
	step [150/190], loss=94.7527
	step [151/190], loss=79.3320
	step [152/190], loss=69.4078
	step [153/190], loss=96.3117
	step [154/190], loss=113.7568
	step [155/190], loss=102.0058
	step [156/190], loss=94.6166
	step [157/190], loss=95.0036
	step [158/190], loss=108.1453
	step [159/190], loss=122.2536
	step [160/190], loss=90.4238
	step [161/190], loss=93.6671
	step [162/190], loss=88.8304
	step [163/190], loss=97.6700
	step [164/190], loss=86.1686
	step [165/190], loss=85.2433
	step [166/190], loss=88.6809
	step [167/190], loss=84.7860
	step [168/190], loss=102.3364
	step [169/190], loss=100.6249
	step [170/190], loss=83.3007
	step [171/190], loss=106.7324
	step [172/190], loss=99.2618
	step [173/190], loss=83.0316
	step [174/190], loss=105.1828
	step [175/190], loss=91.2814
	step [176/190], loss=110.6299
	step [177/190], loss=89.5879
	step [178/190], loss=114.1036
	step [179/190], loss=105.6144
	step [180/190], loss=90.4866
	step [181/190], loss=97.3328
	step [182/190], loss=95.0895
	step [183/190], loss=89.6954
	step [184/190], loss=94.5488
	step [185/190], loss=97.1200
	step [186/190], loss=113.1863
	step [187/190], loss=82.7378
	step [188/190], loss=81.2572
	step [189/190], loss=82.8720
	step [190/190], loss=88.1871
	Evaluating
	loss=0.0227, precision=0.4325, recall=0.8893, f1=0.5819
Training epoch 16
	step [1/190], loss=93.9894
	step [2/190], loss=91.0985
	step [3/190], loss=96.7062
	step [4/190], loss=96.3479
	step [5/190], loss=99.7001
	step [6/190], loss=86.7833
	step [7/190], loss=101.6232
	step [8/190], loss=87.5475
	step [9/190], loss=97.5220
	step [10/190], loss=83.3952
	step [11/190], loss=78.6104
	step [12/190], loss=113.8479
	step [13/190], loss=100.2547
	step [14/190], loss=85.3779
	step [15/190], loss=87.7333
	step [16/190], loss=89.0552
	step [17/190], loss=100.2918
	step [18/190], loss=95.5805
	step [19/190], loss=91.7532
	step [20/190], loss=87.6765
	step [21/190], loss=91.4374
	step [22/190], loss=93.2049
	step [23/190], loss=96.6758
	step [24/190], loss=96.5938
	step [25/190], loss=102.6543
	step [26/190], loss=107.7803
	step [27/190], loss=102.3189
	step [28/190], loss=108.4043
	step [29/190], loss=104.0502
	step [30/190], loss=96.0485
	step [31/190], loss=101.7651
	step [32/190], loss=99.9215
	step [33/190], loss=102.6459
	step [34/190], loss=77.4641
	step [35/190], loss=95.3712
	step [36/190], loss=75.2941
	step [37/190], loss=95.5155
	step [38/190], loss=99.5031
	step [39/190], loss=91.5486
	step [40/190], loss=96.1847
	step [41/190], loss=104.3457
	step [42/190], loss=91.7620
	step [43/190], loss=85.0825
	step [44/190], loss=91.1283
	step [45/190], loss=91.1229
	step [46/190], loss=95.5099
	step [47/190], loss=78.8248
	step [48/190], loss=86.9571
	step [49/190], loss=99.7513
	step [50/190], loss=78.2855
	step [51/190], loss=93.6954
	step [52/190], loss=92.1150
	step [53/190], loss=96.6749
	step [54/190], loss=87.5696
	step [55/190], loss=94.1502
	step [56/190], loss=100.8796
	step [57/190], loss=87.4266
	step [58/190], loss=99.5771
	step [59/190], loss=81.7679
	step [60/190], loss=93.4332
	step [61/190], loss=82.4698
	step [62/190], loss=82.2800
	step [63/190], loss=96.7733
	step [64/190], loss=107.5372
	step [65/190], loss=103.8365
	step [66/190], loss=91.3867
	step [67/190], loss=92.0360
	step [68/190], loss=83.6109
	step [69/190], loss=88.5648
	step [70/190], loss=90.3699
	step [71/190], loss=99.0623
	step [72/190], loss=85.2939
	step [73/190], loss=100.9994
	step [74/190], loss=89.9171
	step [75/190], loss=106.6730
	step [76/190], loss=105.5533
	step [77/190], loss=100.2119
	step [78/190], loss=112.7389
	step [79/190], loss=93.7388
	step [80/190], loss=93.1611
	step [81/190], loss=105.3967
	step [82/190], loss=92.2693
	step [83/190], loss=88.6936
	step [84/190], loss=91.6510
	step [85/190], loss=94.9987
	step [86/190], loss=102.2738
	step [87/190], loss=105.3190
	step [88/190], loss=85.7951
	step [89/190], loss=101.8867
	step [90/190], loss=75.8484
	step [91/190], loss=83.9010
	step [92/190], loss=100.0734
	step [93/190], loss=90.8944
	step [94/190], loss=78.3745
	step [95/190], loss=79.8104
	step [96/190], loss=103.2699
	step [97/190], loss=115.4751
	step [98/190], loss=94.1384
	step [99/190], loss=97.5413
	step [100/190], loss=110.6983
	step [101/190], loss=88.1614
	step [102/190], loss=92.0685
	step [103/190], loss=98.4053
	step [104/190], loss=89.7518
	step [105/190], loss=84.6084
	step [106/190], loss=95.4767
	step [107/190], loss=98.1142
	step [108/190], loss=82.8114
	step [109/190], loss=89.3317
	step [110/190], loss=82.7261
	step [111/190], loss=86.8249
	step [112/190], loss=100.4101
	step [113/190], loss=91.8868
	step [114/190], loss=92.6250
	step [115/190], loss=103.6197
	step [116/190], loss=90.1653
	step [117/190], loss=104.0278
	step [118/190], loss=92.2737
	step [119/190], loss=101.4319
	step [120/190], loss=98.3199
	step [121/190], loss=84.5119
	step [122/190], loss=91.3407
	step [123/190], loss=75.3955
	step [124/190], loss=116.7441
	step [125/190], loss=97.6455
	step [126/190], loss=97.4837
	step [127/190], loss=81.0295
	step [128/190], loss=103.9788
	step [129/190], loss=90.8224
	step [130/190], loss=78.9873
	step [131/190], loss=92.6372
	step [132/190], loss=82.2237
	step [133/190], loss=77.1115
	step [134/190], loss=86.3862
	step [135/190], loss=85.1964
	step [136/190], loss=93.5783
	step [137/190], loss=94.1669
	step [138/190], loss=94.2741
	step [139/190], loss=82.0907
	step [140/190], loss=97.0710
	step [141/190], loss=87.4330
	step [142/190], loss=88.1671
	step [143/190], loss=99.9629
	step [144/190], loss=89.8084
	step [145/190], loss=81.9330
	step [146/190], loss=89.5309
	step [147/190], loss=102.2622
	step [148/190], loss=96.4024
	step [149/190], loss=80.3804
	step [150/190], loss=91.3637
	step [151/190], loss=94.0528
	step [152/190], loss=96.0243
	step [153/190], loss=105.0418
	step [154/190], loss=97.4985
	step [155/190], loss=94.3745
	step [156/190], loss=116.8279
	step [157/190], loss=102.9517
	step [158/190], loss=93.8820
	step [159/190], loss=81.6327
	step [160/190], loss=93.7797
	step [161/190], loss=97.0936
	step [162/190], loss=87.4895
	step [163/190], loss=84.7498
	step [164/190], loss=89.4066
	step [165/190], loss=96.8894
	step [166/190], loss=95.6374
	step [167/190], loss=98.4655
	step [168/190], loss=87.8989
	step [169/190], loss=106.2869
	step [170/190], loss=90.6376
	step [171/190], loss=92.1592
	step [172/190], loss=91.9018
	step [173/190], loss=94.8102
	step [174/190], loss=91.3006
	step [175/190], loss=105.8468
	step [176/190], loss=89.1764
	step [177/190], loss=87.4924
	step [178/190], loss=86.4822
	step [179/190], loss=74.4126
	step [180/190], loss=98.5491
	step [181/190], loss=91.4368
	step [182/190], loss=84.0387
	step [183/190], loss=87.0684
	step [184/190], loss=94.7458
	step [185/190], loss=116.8691
	step [186/190], loss=83.4327
	step [187/190], loss=91.5935
	step [188/190], loss=82.8028
	step [189/190], loss=93.0470
	step [190/190], loss=86.6710
	Evaluating
	loss=0.0180, precision=0.5174, recall=0.8902, f1=0.6544
Training epoch 17
	step [1/190], loss=104.7675
	step [2/190], loss=75.3731
	step [3/190], loss=104.0650
	step [4/190], loss=93.8413
	step [5/190], loss=99.1467
	step [6/190], loss=99.4235
	step [7/190], loss=86.0771
	step [8/190], loss=104.7959
	step [9/190], loss=85.5294
	step [10/190], loss=94.9737
	step [11/190], loss=92.9581
	step [12/190], loss=95.5748
	step [13/190], loss=96.9066
	step [14/190], loss=92.0428
	step [15/190], loss=103.8826
	step [16/190], loss=79.5891
	step [17/190], loss=100.3479
	step [18/190], loss=86.6471
	step [19/190], loss=93.3386
	step [20/190], loss=107.9003
	step [21/190], loss=106.6602
	step [22/190], loss=104.7557
	step [23/190], loss=84.7983
	step [24/190], loss=101.9406
	step [25/190], loss=93.3107
	step [26/190], loss=107.3426
	step [27/190], loss=81.0427
	step [28/190], loss=83.5751
	step [29/190], loss=89.4578
	step [30/190], loss=82.6095
	step [31/190], loss=105.2806
	step [32/190], loss=91.5896
	step [33/190], loss=88.7168
	step [34/190], loss=85.3321
	step [35/190], loss=90.5196
	step [36/190], loss=95.1050
	step [37/190], loss=86.4312
	step [38/190], loss=82.4591
	step [39/190], loss=91.9750
	step [40/190], loss=91.5527
	step [41/190], loss=111.0848
	step [42/190], loss=95.0515
	step [43/190], loss=81.8333
	step [44/190], loss=87.2481
	step [45/190], loss=91.9825
	step [46/190], loss=94.4263
	step [47/190], loss=93.0396
	step [48/190], loss=106.0148
	step [49/190], loss=81.1525
	step [50/190], loss=93.2102
	step [51/190], loss=100.3590
	step [52/190], loss=80.2182
	step [53/190], loss=85.9841
	step [54/190], loss=85.1310
	step [55/190], loss=101.5142
	step [56/190], loss=98.0440
	step [57/190], loss=102.8445
	step [58/190], loss=97.7591
	step [59/190], loss=98.0332
	step [60/190], loss=108.2921
	step [61/190], loss=93.4759
	step [62/190], loss=96.8687
	step [63/190], loss=95.8019
	step [64/190], loss=87.3476
	step [65/190], loss=92.1802
	step [66/190], loss=99.3760
	step [67/190], loss=89.6991
	step [68/190], loss=110.2687
	step [69/190], loss=87.6876
	step [70/190], loss=85.3803
	step [71/190], loss=89.4533
	step [72/190], loss=85.3751
	step [73/190], loss=74.3423
	step [74/190], loss=88.5779
	step [75/190], loss=96.0520
	step [76/190], loss=94.0131
	step [77/190], loss=81.6551
	step [78/190], loss=101.1958
	step [79/190], loss=100.2047
	step [80/190], loss=103.6758
	step [81/190], loss=106.0363
	step [82/190], loss=83.8508
	step [83/190], loss=92.6859
	step [84/190], loss=88.5726
	step [85/190], loss=96.7762
	step [86/190], loss=97.6730
	step [87/190], loss=96.8230
	step [88/190], loss=81.6347
	step [89/190], loss=106.7244
	step [90/190], loss=88.0188
	step [91/190], loss=92.3650
	step [92/190], loss=96.1942
	step [93/190], loss=82.3892
	step [94/190], loss=96.0252
	step [95/190], loss=74.9855
	step [96/190], loss=83.8367
	step [97/190], loss=94.1018
	step [98/190], loss=82.4338
	step [99/190], loss=81.8585
	step [100/190], loss=100.1842
	step [101/190], loss=91.9526
	step [102/190], loss=87.9958
	step [103/190], loss=85.9136
	step [104/190], loss=101.7845
	step [105/190], loss=76.8958
	step [106/190], loss=83.5180
	step [107/190], loss=108.6236
	step [108/190], loss=86.5333
	step [109/190], loss=96.0500
	step [110/190], loss=87.6577
	step [111/190], loss=78.9805
	step [112/190], loss=90.1364
	step [113/190], loss=92.7144
	step [114/190], loss=93.8354
	step [115/190], loss=102.4931
	step [116/190], loss=94.0697
	step [117/190], loss=90.1911
	step [118/190], loss=94.9646
	step [119/190], loss=108.2529
	step [120/190], loss=74.2428
	step [121/190], loss=91.2986
	step [122/190], loss=88.9311
	step [123/190], loss=91.1151
	step [124/190], loss=96.6853
	step [125/190], loss=97.4771
	step [126/190], loss=111.1234
	step [127/190], loss=79.7358
	step [128/190], loss=96.9659
	step [129/190], loss=93.0840
	step [130/190], loss=90.0578
	step [131/190], loss=94.7068
	step [132/190], loss=88.9570
	step [133/190], loss=90.4397
	step [134/190], loss=87.8327
	step [135/190], loss=93.0263
	step [136/190], loss=99.3802
	step [137/190], loss=90.7097
	step [138/190], loss=86.6302
	step [139/190], loss=94.9038
	step [140/190], loss=89.6609
	step [141/190], loss=100.2758
	step [142/190], loss=90.4177
	step [143/190], loss=105.2499
	step [144/190], loss=87.6257
	step [145/190], loss=89.3828
	step [146/190], loss=90.0750
	step [147/190], loss=88.9668
	step [148/190], loss=97.5477
	step [149/190], loss=98.9034
	step [150/190], loss=74.1948
	step [151/190], loss=90.8683
	step [152/190], loss=94.1142
	step [153/190], loss=94.6574
	step [154/190], loss=96.6329
	step [155/190], loss=85.0146
	step [156/190], loss=83.2119
	step [157/190], loss=95.6059
	step [158/190], loss=103.7765
	step [159/190], loss=86.0706
	step [160/190], loss=93.7781
	step [161/190], loss=87.7939
	step [162/190], loss=82.3931
	step [163/190], loss=91.2639
	step [164/190], loss=92.6746
	step [165/190], loss=108.4095
	step [166/190], loss=99.6732
	step [167/190], loss=97.3090
	step [168/190], loss=94.1294
	step [169/190], loss=81.0851
	step [170/190], loss=87.6518
	step [171/190], loss=92.4472
	step [172/190], loss=86.8619
	step [173/190], loss=91.1687
	step [174/190], loss=101.6151
	step [175/190], loss=87.6025
	step [176/190], loss=90.2730
	step [177/190], loss=81.3822
	step [178/190], loss=78.5119
	step [179/190], loss=88.1177
	step [180/190], loss=84.7849
	step [181/190], loss=89.0884
	step [182/190], loss=77.8473
	step [183/190], loss=76.1131
	step [184/190], loss=98.3360
	step [185/190], loss=93.7232
	step [186/190], loss=78.1796
	step [187/190], loss=81.4237
	step [188/190], loss=87.2210
	step [189/190], loss=91.3646
	step [190/190], loss=78.9622
	Evaluating
	loss=0.0194, precision=0.4164, recall=0.8982, f1=0.5690
Training epoch 18
	step [1/190], loss=87.3652
	step [2/190], loss=97.1747
	step [3/190], loss=79.3430
	step [4/190], loss=85.9374
	step [5/190], loss=85.6116
	step [6/190], loss=96.0412
	step [7/190], loss=76.6631
	step [8/190], loss=96.5655
	step [9/190], loss=93.2665
	step [10/190], loss=79.7121
	step [11/190], loss=83.6207
	step [12/190], loss=89.1540
	step [13/190], loss=88.6486
	step [14/190], loss=94.1787
	step [15/190], loss=85.5005
	step [16/190], loss=99.8551
	step [17/190], loss=90.2033
	step [18/190], loss=98.6504
	step [19/190], loss=90.1641
	step [20/190], loss=94.6156
	step [21/190], loss=87.8581
	step [22/190], loss=87.9431
	step [23/190], loss=99.8906
	step [24/190], loss=89.1630
	step [25/190], loss=82.2997
	step [26/190], loss=72.3045
	step [27/190], loss=90.0719
	step [28/190], loss=102.1665
	step [29/190], loss=98.6669
	step [30/190], loss=85.1927
	step [31/190], loss=92.0096
	step [32/190], loss=94.6812
	step [33/190], loss=106.7002
	step [34/190], loss=93.6886
	step [35/190], loss=93.8029
	step [36/190], loss=80.1277
	step [37/190], loss=78.7507
	step [38/190], loss=94.1191
	step [39/190], loss=90.6143
	step [40/190], loss=97.5769
	step [41/190], loss=87.5364
	step [42/190], loss=91.5093
	step [43/190], loss=84.7175
	step [44/190], loss=82.9297
	step [45/190], loss=95.4549
	step [46/190], loss=101.7127
	step [47/190], loss=93.8852
	step [48/190], loss=88.6323
	step [49/190], loss=89.2105
	step [50/190], loss=80.3819
	step [51/190], loss=91.8015
	step [52/190], loss=89.9060
	step [53/190], loss=96.6370
	step [54/190], loss=92.1618
	step [55/190], loss=96.7384
	step [56/190], loss=91.0468
	step [57/190], loss=82.8399
	step [58/190], loss=96.5722
	step [59/190], loss=90.9098
	step [60/190], loss=98.4736
	step [61/190], loss=76.4361
	step [62/190], loss=97.7572
	step [63/190], loss=87.2566
	step [64/190], loss=97.7751
	step [65/190], loss=82.3290
	step [66/190], loss=97.2367
	step [67/190], loss=103.1504
	step [68/190], loss=95.2963
	step [69/190], loss=98.3588
	step [70/190], loss=105.9845
	step [71/190], loss=77.7039
	step [72/190], loss=99.7561
	step [73/190], loss=91.3004
	step [74/190], loss=95.8630
	step [75/190], loss=87.6708
	step [76/190], loss=98.3059
	step [77/190], loss=104.8556
	step [78/190], loss=82.8349
	step [79/190], loss=83.0491
	step [80/190], loss=80.5193
	step [81/190], loss=85.6588
	step [82/190], loss=87.9283
	step [83/190], loss=84.7917
	step [84/190], loss=86.0934
	step [85/190], loss=69.6958
	step [86/190], loss=98.3100
	step [87/190], loss=93.3087
	step [88/190], loss=78.3177
	step [89/190], loss=86.5562
	step [90/190], loss=100.1078
	step [91/190], loss=90.0136
	step [92/190], loss=96.1638
	step [93/190], loss=95.9465
	step [94/190], loss=95.3000
	step [95/190], loss=83.9546
	step [96/190], loss=88.3075
	step [97/190], loss=99.1008
	step [98/190], loss=93.7804
	step [99/190], loss=98.5192
	step [100/190], loss=94.3732
	step [101/190], loss=101.1967
	step [102/190], loss=96.1435
	step [103/190], loss=82.6660
	step [104/190], loss=95.5811
	step [105/190], loss=88.7210
	step [106/190], loss=84.0300
	step [107/190], loss=98.4271
	step [108/190], loss=98.6328
	step [109/190], loss=88.3585
	step [110/190], loss=95.7630
	step [111/190], loss=92.2572
	step [112/190], loss=97.5818
	step [113/190], loss=73.0650
	step [114/190], loss=79.8510
	step [115/190], loss=69.6406
	step [116/190], loss=99.1197
	step [117/190], loss=82.0912
	step [118/190], loss=97.0364
	step [119/190], loss=91.4727
	step [120/190], loss=96.5536
	step [121/190], loss=99.7979
	step [122/190], loss=92.4069
	step [123/190], loss=90.0147
	step [124/190], loss=82.9709
	step [125/190], loss=102.2882
	step [126/190], loss=90.9697
	step [127/190], loss=95.0103
	step [128/190], loss=78.6621
	step [129/190], loss=94.7695
	step [130/190], loss=72.9176
	step [131/190], loss=87.5902
	step [132/190], loss=89.9093
	step [133/190], loss=77.2550
	step [134/190], loss=82.2618
	step [135/190], loss=81.9545
	step [136/190], loss=85.0891
	step [137/190], loss=82.3356
	step [138/190], loss=108.5983
	step [139/190], loss=71.4810
	step [140/190], loss=95.0755
	step [141/190], loss=93.3902
	step [142/190], loss=87.8190
	step [143/190], loss=84.8201
	step [144/190], loss=80.8315
	step [145/190], loss=90.7232
	step [146/190], loss=101.5478
	step [147/190], loss=104.0113
	step [148/190], loss=78.4025
	step [149/190], loss=81.0958
	step [150/190], loss=106.2631
	step [151/190], loss=89.6916
	step [152/190], loss=116.3767
	step [153/190], loss=90.5316
	step [154/190], loss=94.9888
	step [155/190], loss=79.8660
	step [156/190], loss=84.7929
	step [157/190], loss=90.2866
	step [158/190], loss=92.4238
	step [159/190], loss=110.8721
	step [160/190], loss=93.5133
	step [161/190], loss=98.3667
	step [162/190], loss=96.0712
	step [163/190], loss=75.5009
	step [164/190], loss=82.3451
	step [165/190], loss=91.3191
	step [166/190], loss=105.3540
	step [167/190], loss=92.6434
	step [168/190], loss=83.4636
	step [169/190], loss=87.3531
	step [170/190], loss=103.9366
	step [171/190], loss=84.9322
	step [172/190], loss=85.0225
	step [173/190], loss=77.8449
	step [174/190], loss=100.9420
	step [175/190], loss=91.4873
	step [176/190], loss=89.7929
	step [177/190], loss=107.9264
	step [178/190], loss=107.6626
	step [179/190], loss=103.4620
	step [180/190], loss=95.8322
	step [181/190], loss=99.9218
	step [182/190], loss=84.4116
	step [183/190], loss=93.5785
	step [184/190], loss=84.6175
	step [185/190], loss=84.4813
	step [186/190], loss=89.4013
	step [187/190], loss=88.5184
	step [188/190], loss=91.6380
	step [189/190], loss=94.8368
	step [190/190], loss=93.5636
	Evaluating
	loss=0.0163, precision=0.4593, recall=0.8913, f1=0.6062
Training epoch 19
	step [1/190], loss=91.6243
	step [2/190], loss=85.4466
	step [3/190], loss=89.3293
	step [4/190], loss=80.1657
	step [5/190], loss=86.6964
	step [6/190], loss=90.7214
	step [7/190], loss=89.6592
	step [8/190], loss=99.8351
	step [9/190], loss=100.4702
	step [10/190], loss=105.1587
	step [11/190], loss=87.8280
	step [12/190], loss=93.3502
	step [13/190], loss=76.6028
	step [14/190], loss=90.6919
	step [15/190], loss=94.6105
	step [16/190], loss=103.2056
	step [17/190], loss=80.8246
	step [18/190], loss=100.0562
	step [19/190], loss=100.0182
	step [20/190], loss=84.7057
	step [21/190], loss=98.5819
	step [22/190], loss=93.0384
	step [23/190], loss=105.9802
	step [24/190], loss=101.4593
	step [25/190], loss=79.9523
	step [26/190], loss=89.6466
	step [27/190], loss=87.6985
	step [28/190], loss=96.0786
	step [29/190], loss=96.5774
	step [30/190], loss=87.7301
	step [31/190], loss=82.9379
	step [32/190], loss=105.8595
	step [33/190], loss=83.3779
	step [34/190], loss=89.4325
	step [35/190], loss=75.9492
	step [36/190], loss=84.0709
	step [37/190], loss=82.4007
	step [38/190], loss=81.4515
	step [39/190], loss=102.0579
	step [40/190], loss=94.7606
	step [41/190], loss=102.3651
	step [42/190], loss=102.1897
	step [43/190], loss=66.0345
	step [44/190], loss=86.4135
	step [45/190], loss=93.3871
	step [46/190], loss=101.5039
	step [47/190], loss=101.7174
	step [48/190], loss=85.4805
	step [49/190], loss=100.8388
	step [50/190], loss=103.4671
	step [51/190], loss=89.3855
	step [52/190], loss=101.0352
	step [53/190], loss=94.4336
	step [54/190], loss=89.1984
	step [55/190], loss=79.9012
	step [56/190], loss=82.1277
	step [57/190], loss=100.3740
	step [58/190], loss=97.8954
	step [59/190], loss=88.0310
	step [60/190], loss=109.5997
	step [61/190], loss=81.6509
	step [62/190], loss=85.3404
	step [63/190], loss=87.0052
	step [64/190], loss=83.4999
	step [65/190], loss=86.8398
	step [66/190], loss=74.9374
	step [67/190], loss=87.5219
	step [68/190], loss=82.4233
	step [69/190], loss=92.2366
	step [70/190], loss=97.8783
	step [71/190], loss=106.3571
	step [72/190], loss=88.0197
	step [73/190], loss=101.4984
	step [74/190], loss=100.3551
	step [75/190], loss=74.7149
	step [76/190], loss=89.2774
	step [77/190], loss=90.5865
	step [78/190], loss=90.8412
	step [79/190], loss=75.3642
	step [80/190], loss=97.4552
	step [81/190], loss=100.0888
	step [82/190], loss=87.9986
	step [83/190], loss=77.0792
	step [84/190], loss=88.1843
	step [85/190], loss=98.9888
	step [86/190], loss=95.3699
	step [87/190], loss=85.9267
	step [88/190], loss=92.6402
	step [89/190], loss=85.5306
	step [90/190], loss=94.3553
	step [91/190], loss=90.6594
	step [92/190], loss=103.0815
	step [93/190], loss=89.5777
	step [94/190], loss=81.4286
	step [95/190], loss=92.1575
	step [96/190], loss=95.0767
	step [97/190], loss=79.2702
	step [98/190], loss=99.6777
	step [99/190], loss=74.4742
	step [100/190], loss=91.5345
	step [101/190], loss=79.8602
	step [102/190], loss=93.8146
	step [103/190], loss=80.1726
	step [104/190], loss=73.5950
	step [105/190], loss=85.5586
	step [106/190], loss=99.5847
	step [107/190], loss=90.4353
	step [108/190], loss=76.1317
	step [109/190], loss=79.1438
	step [110/190], loss=84.7853
	step [111/190], loss=83.3440
	step [112/190], loss=85.8896
	step [113/190], loss=89.6289
	step [114/190], loss=87.1150
	step [115/190], loss=94.5935
	step [116/190], loss=79.0026
	step [117/190], loss=92.4592
	step [118/190], loss=81.5447
	step [119/190], loss=96.3892
	step [120/190], loss=76.4382
	step [121/190], loss=77.2199
	step [122/190], loss=90.0949
	step [123/190], loss=93.2105
	step [124/190], loss=87.7658
	step [125/190], loss=96.0195
	step [126/190], loss=73.6595
	step [127/190], loss=91.7594
	step [128/190], loss=108.9354
	step [129/190], loss=91.8809
	step [130/190], loss=85.6289
	step [131/190], loss=82.9012
	step [132/190], loss=95.1747
	step [133/190], loss=85.8576
	step [134/190], loss=81.4563
	step [135/190], loss=88.7396
	step [136/190], loss=97.4433
	step [137/190], loss=116.5688
	step [138/190], loss=102.6495
	step [139/190], loss=85.5217
	step [140/190], loss=93.2562
	step [141/190], loss=86.6605
	step [142/190], loss=82.2400
	step [143/190], loss=84.1797
	step [144/190], loss=93.4193
	step [145/190], loss=84.0357
	step [146/190], loss=99.7889
	step [147/190], loss=64.3738
	step [148/190], loss=92.7947
	step [149/190], loss=83.2579
	step [150/190], loss=96.8982
	step [151/190], loss=95.0860
	step [152/190], loss=85.4909
	step [153/190], loss=105.8737
	step [154/190], loss=93.3718
	step [155/190], loss=88.6744
	step [156/190], loss=78.7354
	step [157/190], loss=81.6236
	step [158/190], loss=88.1382
	step [159/190], loss=86.1929
	step [160/190], loss=93.9153
	step [161/190], loss=84.6618
	step [162/190], loss=100.7362
	step [163/190], loss=94.0237
	step [164/190], loss=100.4635
	step [165/190], loss=92.5492
	step [166/190], loss=86.0084
	step [167/190], loss=84.0351
	step [168/190], loss=77.2138
	step [169/190], loss=89.1269
	step [170/190], loss=81.6860
	step [171/190], loss=95.7938
	step [172/190], loss=75.2197
	step [173/190], loss=79.7128
	step [174/190], loss=104.8053
	step [175/190], loss=87.6703
	step [176/190], loss=85.1337
	step [177/190], loss=94.6048
	step [178/190], loss=89.5282
	step [179/190], loss=93.2947
	step [180/190], loss=98.6638
	step [181/190], loss=84.2980
	step [182/190], loss=102.8427
	step [183/190], loss=83.2245
	step [184/190], loss=86.0413
	step [185/190], loss=104.8210
	step [186/190], loss=86.5259
	step [187/190], loss=80.8957
	step [188/190], loss=88.7836
	step [189/190], loss=98.3073
	step [190/190], loss=71.0851
	Evaluating
	loss=0.0145, precision=0.4936, recall=0.8854, f1=0.6338
Training epoch 20
	step [1/190], loss=93.1863
	step [2/190], loss=82.1966
	step [3/190], loss=87.5071
	step [4/190], loss=93.2435
	step [5/190], loss=88.5858
	step [6/190], loss=84.8339
	step [7/190], loss=83.5058
	step [8/190], loss=92.9856
	step [9/190], loss=99.3465
	step [10/190], loss=81.2364
	step [11/190], loss=91.1881
	step [12/190], loss=84.1359
	step [13/190], loss=81.3665
	step [14/190], loss=83.1601
	step [15/190], loss=93.6201
	step [16/190], loss=85.6404
	step [17/190], loss=91.9149
	step [18/190], loss=84.7962
	step [19/190], loss=73.6194
	step [20/190], loss=85.6774
	step [21/190], loss=90.7453
	step [22/190], loss=89.1197
	step [23/190], loss=97.2367
	step [24/190], loss=82.8425
	step [25/190], loss=88.5651
	step [26/190], loss=88.9667
	step [27/190], loss=83.7208
	step [28/190], loss=89.4626
	step [29/190], loss=77.0668
	step [30/190], loss=72.8016
	step [31/190], loss=97.1967
	step [32/190], loss=83.9910
	step [33/190], loss=96.2151
	step [34/190], loss=91.3902
	step [35/190], loss=82.9402
	step [36/190], loss=99.3353
	step [37/190], loss=91.2287
	step [38/190], loss=102.8428
	step [39/190], loss=85.9514
	step [40/190], loss=102.5639
	step [41/190], loss=86.7162
	step [42/190], loss=102.7845
	step [43/190], loss=80.4350
	step [44/190], loss=95.7491
	step [45/190], loss=89.5965
	step [46/190], loss=77.8058
	step [47/190], loss=82.1465
	step [48/190], loss=87.2627
	step [49/190], loss=91.5908
	step [50/190], loss=77.3505
	step [51/190], loss=98.3261
	step [52/190], loss=76.4319
	step [53/190], loss=94.2880
	step [54/190], loss=93.8562
	step [55/190], loss=94.2156
	step [56/190], loss=103.4295
	step [57/190], loss=89.8590
	step [58/190], loss=91.0103
	step [59/190], loss=82.0981
	step [60/190], loss=99.0880
	step [61/190], loss=98.0538
	step [62/190], loss=92.8667
	step [63/190], loss=71.8599
	step [64/190], loss=76.1818
	step [65/190], loss=94.3434
	step [66/190], loss=89.9577
	step [67/190], loss=101.0192
	step [68/190], loss=83.9852
	step [69/190], loss=86.5477
	step [70/190], loss=85.4429
	step [71/190], loss=82.8635
	step [72/190], loss=93.8533
	step [73/190], loss=94.5768
	step [74/190], loss=90.5296
	step [75/190], loss=82.3284
	step [76/190], loss=96.7454
	step [77/190], loss=73.3291
	step [78/190], loss=88.1701
	step [79/190], loss=84.7746
	step [80/190], loss=98.4282
	step [81/190], loss=97.0189
	step [82/190], loss=96.6981
	step [83/190], loss=90.8948
	step [84/190], loss=88.3755
	step [85/190], loss=81.8687
	step [86/190], loss=92.8739
	step [87/190], loss=86.1020
	step [88/190], loss=81.5040
	step [89/190], loss=96.1013
	step [90/190], loss=99.7593
	step [91/190], loss=74.1488
	step [92/190], loss=80.0065
	step [93/190], loss=92.3355
	step [94/190], loss=78.9810
	step [95/190], loss=86.3571
	step [96/190], loss=78.6170
	step [97/190], loss=98.4797
	step [98/190], loss=99.8611
	step [99/190], loss=84.2877
	step [100/190], loss=84.3868
	step [101/190], loss=76.6806
	step [102/190], loss=93.9717
	step [103/190], loss=104.1345
	step [104/190], loss=76.5320
	step [105/190], loss=85.8997
	step [106/190], loss=75.7154
	step [107/190], loss=78.7266
	step [108/190], loss=76.4431
	step [109/190], loss=77.1375
	step [110/190], loss=78.0515
	step [111/190], loss=98.9773
	step [112/190], loss=78.4794
	step [113/190], loss=91.9492
	step [114/190], loss=98.8641
	step [115/190], loss=96.6123
	step [116/190], loss=87.7543
	step [117/190], loss=88.4003
	step [118/190], loss=83.3014
	step [119/190], loss=97.3514
	step [120/190], loss=84.9645
	step [121/190], loss=82.5309
	step [122/190], loss=86.3522
	step [123/190], loss=71.3214
	step [124/190], loss=87.9633
	step [125/190], loss=93.0560
	step [126/190], loss=94.7589
	step [127/190], loss=89.3805
	step [128/190], loss=93.0885
	step [129/190], loss=77.2364
	step [130/190], loss=77.9762
	step [131/190], loss=100.0038
	step [132/190], loss=92.2334
	step [133/190], loss=96.6224
	step [134/190], loss=99.4279
	step [135/190], loss=85.7387
	step [136/190], loss=91.0717
	step [137/190], loss=105.0132
	step [138/190], loss=85.5611
	step [139/190], loss=89.7796
	step [140/190], loss=90.6202
	step [141/190], loss=80.0633
	step [142/190], loss=95.9517
	step [143/190], loss=77.3315
	step [144/190], loss=88.5000
	step [145/190], loss=84.9292
	step [146/190], loss=83.8975
	step [147/190], loss=87.3197
	step [148/190], loss=86.0535
	step [149/190], loss=76.6945
	step [150/190], loss=100.4023
	step [151/190], loss=100.9909
	step [152/190], loss=83.4584
	step [153/190], loss=92.6587
	step [154/190], loss=84.7018
	step [155/190], loss=85.6579
	step [156/190], loss=85.0243
	step [157/190], loss=86.9225
	step [158/190], loss=82.5022
	step [159/190], loss=88.0253
	step [160/190], loss=72.6861
	step [161/190], loss=106.1476
	step [162/190], loss=79.9270
	step [163/190], loss=88.1115
	step [164/190], loss=86.3123
	step [165/190], loss=90.6059
	step [166/190], loss=104.4552
	step [167/190], loss=74.3974
	step [168/190], loss=99.8344
	step [169/190], loss=87.8298
	step [170/190], loss=94.8373
	step [171/190], loss=75.7346
	step [172/190], loss=96.3442
	step [173/190], loss=80.8503
	step [174/190], loss=85.6231
	step [175/190], loss=101.9415
	step [176/190], loss=90.2186
	step [177/190], loss=97.0136
	step [178/190], loss=79.1361
	step [179/190], loss=96.0605
	step [180/190], loss=86.0393
	step [181/190], loss=108.4445
	step [182/190], loss=92.9156
	step [183/190], loss=90.8173
	step [184/190], loss=83.1185
	step [185/190], loss=86.1865
	step [186/190], loss=99.4453
	step [187/190], loss=98.2712
	step [188/190], loss=81.0118
	step [189/190], loss=98.5540
	step [190/190], loss=77.8490
	Evaluating
	loss=0.0154, precision=0.4264, recall=0.8906, f1=0.5767
Training epoch 21
	step [1/190], loss=94.3100
	step [2/190], loss=90.7174
	step [3/190], loss=92.5434
	step [4/190], loss=89.7950
	step [5/190], loss=92.2031
	step [6/190], loss=90.1981
	step [7/190], loss=81.9787
	step [8/190], loss=104.3676
	step [9/190], loss=98.9814
	step [10/190], loss=84.7541
	step [11/190], loss=93.9422
	step [12/190], loss=94.0705
	step [13/190], loss=68.1325
	step [14/190], loss=101.1767
	step [15/190], loss=79.2906
	step [16/190], loss=94.1321
	step [17/190], loss=89.2936
	step [18/190], loss=82.2998
	step [19/190], loss=86.8966
	step [20/190], loss=78.3557
	step [21/190], loss=86.2979
	step [22/190], loss=88.7054
	step [23/190], loss=83.6388
	step [24/190], loss=93.0765
	step [25/190], loss=81.9326
	step [26/190], loss=73.0653
	step [27/190], loss=79.0946
	step [28/190], loss=87.8217
	step [29/190], loss=83.2742
	step [30/190], loss=85.8211
	step [31/190], loss=97.3705
	step [32/190], loss=89.5118
	step [33/190], loss=96.3881
	step [34/190], loss=94.1413
	step [35/190], loss=90.4646
	step [36/190], loss=101.4526
	step [37/190], loss=98.4880
	step [38/190], loss=90.6172
	step [39/190], loss=92.9026
	step [40/190], loss=87.6111
	step [41/190], loss=93.8866
	step [42/190], loss=97.5068
	step [43/190], loss=74.6287
	step [44/190], loss=90.0026
	step [45/190], loss=99.1420
	step [46/190], loss=78.0904
	step [47/190], loss=94.3592
	step [48/190], loss=84.3153
	step [49/190], loss=95.9867
	step [50/190], loss=98.7516
	step [51/190], loss=83.8812
	step [52/190], loss=82.6083
	step [53/190], loss=100.1568
	step [54/190], loss=82.9835
	step [55/190], loss=78.6633
	step [56/190], loss=99.0915
	step [57/190], loss=98.4119
	step [58/190], loss=83.3631
	step [59/190], loss=86.8086
	step [60/190], loss=98.3300
	step [61/190], loss=75.7376
	step [62/190], loss=80.6680
	step [63/190], loss=96.1525
	step [64/190], loss=109.3608
	step [65/190], loss=82.0702
	step [66/190], loss=109.0394
	step [67/190], loss=91.8799
	step [68/190], loss=82.3527
	step [69/190], loss=90.7139
	step [70/190], loss=89.3670
	step [71/190], loss=81.1218
	step [72/190], loss=90.7900
	step [73/190], loss=95.6681
	step [74/190], loss=100.4611
	step [75/190], loss=89.0978
	step [76/190], loss=90.5422
	step [77/190], loss=88.0392
	step [78/190], loss=84.0891
	step [79/190], loss=74.6989
	step [80/190], loss=91.6357
	step [81/190], loss=91.1833
	step [82/190], loss=72.7814
	step [83/190], loss=79.9396
	step [84/190], loss=94.8437
	step [85/190], loss=99.7018
	step [86/190], loss=80.6651
	step [87/190], loss=96.9092
	step [88/190], loss=80.5044
	step [89/190], loss=84.0114
	step [90/190], loss=94.4169
	step [91/190], loss=81.1054
	step [92/190], loss=80.6854
	step [93/190], loss=85.1552
	step [94/190], loss=96.8428
	step [95/190], loss=92.8787
	step [96/190], loss=81.1838
	step [97/190], loss=94.6376
	step [98/190], loss=91.2839
	step [99/190], loss=88.7612
	step [100/190], loss=93.6600
	step [101/190], loss=78.5635
	step [102/190], loss=88.3048
	step [103/190], loss=81.1180
	step [104/190], loss=95.6479
	step [105/190], loss=87.2770
	step [106/190], loss=73.0646
	step [107/190], loss=87.4806
	step [108/190], loss=84.7129
	step [109/190], loss=82.8316
	step [110/190], loss=78.1945
	step [111/190], loss=75.2423
	step [112/190], loss=100.2858
	step [113/190], loss=85.6190
	step [114/190], loss=81.4146
	step [115/190], loss=87.4737
	step [116/190], loss=77.9767
	step [117/190], loss=76.1261
	step [118/190], loss=94.9843
	step [119/190], loss=74.8221
	step [120/190], loss=94.1243
	step [121/190], loss=76.8160
	step [122/190], loss=84.5537
	step [123/190], loss=89.6568
	step [124/190], loss=88.4844
	step [125/190], loss=72.8600
	step [126/190], loss=73.3728
	step [127/190], loss=90.8940
	step [128/190], loss=90.7711
	step [129/190], loss=81.2695
	step [130/190], loss=89.8589
	step [131/190], loss=97.2707
	step [132/190], loss=97.5604
	step [133/190], loss=70.4332
	step [134/190], loss=78.2637
	step [135/190], loss=81.4921
	step [136/190], loss=87.2351
	step [137/190], loss=85.0880
	step [138/190], loss=75.4632
	step [139/190], loss=85.0813
	step [140/190], loss=87.3708
	step [141/190], loss=102.4603
	step [142/190], loss=101.3388
	step [143/190], loss=86.2391
	step [144/190], loss=76.9824
	step [145/190], loss=82.7543
	step [146/190], loss=91.9684
	step [147/190], loss=89.1211
	step [148/190], loss=90.6689
	step [149/190], loss=81.3900
	step [150/190], loss=78.3440
	step [151/190], loss=71.7110
	step [152/190], loss=81.4381
	step [153/190], loss=105.7305
	step [154/190], loss=74.1358
	step [155/190], loss=83.1717
	step [156/190], loss=83.1056
	step [157/190], loss=79.5939
	step [158/190], loss=76.0265
	step [159/190], loss=90.9964
	step [160/190], loss=100.4208
	step [161/190], loss=97.2319
	step [162/190], loss=86.0236
	step [163/190], loss=84.5664
	step [164/190], loss=71.1705
	step [165/190], loss=78.3800
	step [166/190], loss=81.3419
	step [167/190], loss=95.6773
	step [168/190], loss=69.4117
	step [169/190], loss=91.1939
	step [170/190], loss=99.1267
	step [171/190], loss=92.5897
	step [172/190], loss=86.0607
	step [173/190], loss=70.2794
	step [174/190], loss=91.2955
	step [175/190], loss=77.2708
	step [176/190], loss=95.7239
	step [177/190], loss=100.8115
	step [178/190], loss=88.0263
	step [179/190], loss=85.2423
	step [180/190], loss=88.1757
	step [181/190], loss=76.6673
	step [182/190], loss=77.8241
	step [183/190], loss=85.4420
	step [184/190], loss=77.5048
	step [185/190], loss=96.8049
	step [186/190], loss=82.6083
	step [187/190], loss=92.0344
	step [188/190], loss=90.1318
	step [189/190], loss=92.9982
	step [190/190], loss=69.0602
	Evaluating
	loss=0.0167, precision=0.3940, recall=0.8851, f1=0.5453
Training epoch 22
	step [1/190], loss=85.9730
	step [2/190], loss=85.3734
	step [3/190], loss=97.7383
	step [4/190], loss=85.2607
	step [5/190], loss=77.2874
	step [6/190], loss=81.0946
	step [7/190], loss=87.5472
	step [8/190], loss=86.3409
	step [9/190], loss=89.2398
	step [10/190], loss=95.8984
	step [11/190], loss=89.9321
	step [12/190], loss=79.8532
	step [13/190], loss=82.3595
	step [14/190], loss=94.5135
	step [15/190], loss=87.8474
	step [16/190], loss=88.4546
	step [17/190], loss=96.1851
	step [18/190], loss=96.6113
	step [19/190], loss=79.0999
	step [20/190], loss=89.5615
	step [21/190], loss=96.4860
	step [22/190], loss=80.3148
	step [23/190], loss=79.9245
	step [24/190], loss=89.5674
	step [25/190], loss=83.5771
	step [26/190], loss=83.2987
	step [27/190], loss=98.6904
	step [28/190], loss=90.1500
	step [29/190], loss=96.5031
	step [30/190], loss=78.1830
	step [31/190], loss=92.2053
	step [32/190], loss=82.1338
	step [33/190], loss=91.9779
	step [34/190], loss=89.7130
	step [35/190], loss=87.2760
	step [36/190], loss=92.5150
	step [37/190], loss=80.0640
	step [38/190], loss=84.0080
	step [39/190], loss=80.8820
	step [40/190], loss=82.9973
	step [41/190], loss=89.1116
	step [42/190], loss=98.6698
	step [43/190], loss=87.8890
	step [44/190], loss=90.6806
	step [45/190], loss=82.7514
	step [46/190], loss=85.6560
	step [47/190], loss=83.1600
	step [48/190], loss=82.4368
	step [49/190], loss=86.2556
	step [50/190], loss=92.7373
	step [51/190], loss=91.4478
	step [52/190], loss=82.7506
	step [53/190], loss=91.7474
	step [54/190], loss=97.0883
	step [55/190], loss=82.8400
	step [56/190], loss=78.0836
	step [57/190], loss=78.2388
	step [58/190], loss=86.6078
	step [59/190], loss=78.6010
	step [60/190], loss=88.4931
	step [61/190], loss=77.3193
	step [62/190], loss=93.3088
	step [63/190], loss=76.8790
	step [64/190], loss=81.6334
	step [65/190], loss=85.8668
	step [66/190], loss=82.9628
	step [67/190], loss=77.4926
	step [68/190], loss=97.3335
	step [69/190], loss=84.9338
	step [70/190], loss=89.5103
	step [71/190], loss=92.3400
	step [72/190], loss=68.5419
	step [73/190], loss=104.8673
	step [74/190], loss=91.7119
	step [75/190], loss=93.9731
	step [76/190], loss=100.4023
	step [77/190], loss=92.1759
	step [78/190], loss=90.1241
	step [79/190], loss=80.6638
	step [80/190], loss=85.3244
	step [81/190], loss=76.4809
	step [82/190], loss=81.9955
	step [83/190], loss=80.4651
	step [84/190], loss=84.1631
	step [85/190], loss=89.3456
	step [86/190], loss=79.7089
	step [87/190], loss=97.5279
	step [88/190], loss=91.1262
	step [89/190], loss=79.0813
	step [90/190], loss=90.4646
	step [91/190], loss=95.6687
	step [92/190], loss=92.7478
	step [93/190], loss=86.5877
	step [94/190], loss=77.5181
	step [95/190], loss=98.9817
	step [96/190], loss=86.0636
	step [97/190], loss=101.4271
	step [98/190], loss=95.1631
	step [99/190], loss=75.3812
	step [100/190], loss=79.9591
	step [101/190], loss=90.2282
	step [102/190], loss=94.5429
	step [103/190], loss=109.0002
	step [104/190], loss=87.4103
	step [105/190], loss=86.8353
	step [106/190], loss=86.3892
	step [107/190], loss=78.6673
	step [108/190], loss=67.0263
	step [109/190], loss=87.0805
	step [110/190], loss=89.8642
	step [111/190], loss=73.4613
	step [112/190], loss=86.8012
	step [113/190], loss=81.4109
	step [114/190], loss=96.4657
	step [115/190], loss=77.6994
	step [116/190], loss=75.5985
	step [117/190], loss=74.3371
	step [118/190], loss=76.2702
	step [119/190], loss=103.8150
	step [120/190], loss=95.0898
	step [121/190], loss=81.3007
	step [122/190], loss=80.1160
	step [123/190], loss=93.5008
	step [124/190], loss=82.8204
	step [125/190], loss=87.0662
	step [126/190], loss=74.9611
	step [127/190], loss=85.8609
	step [128/190], loss=90.7649
	step [129/190], loss=77.9870
	step [130/190], loss=85.9970
	step [131/190], loss=75.4237
	step [132/190], loss=90.5425
	step [133/190], loss=100.3830
	step [134/190], loss=78.5486
	step [135/190], loss=87.4212
	step [136/190], loss=90.7355
	step [137/190], loss=83.0719
	step [138/190], loss=79.0223
	step [139/190], loss=74.3281
	step [140/190], loss=95.9973
	step [141/190], loss=82.4825
	step [142/190], loss=87.6042
	step [143/190], loss=78.8239
	step [144/190], loss=75.2815
	step [145/190], loss=83.8206
	step [146/190], loss=78.0562
	step [147/190], loss=77.2807
	step [148/190], loss=86.2172
	step [149/190], loss=77.3393
	step [150/190], loss=86.4654
	step [151/190], loss=90.9961
	step [152/190], loss=87.5108
	step [153/190], loss=72.4654
	step [154/190], loss=95.5660
	step [155/190], loss=92.0900
	step [156/190], loss=81.0818
	step [157/190], loss=93.3407
	step [158/190], loss=88.0507
	step [159/190], loss=78.5579
	step [160/190], loss=103.3347
	step [161/190], loss=101.1535
	step [162/190], loss=84.1591
	step [163/190], loss=93.3186
	step [164/190], loss=59.1709
	step [165/190], loss=93.6252
	step [166/190], loss=84.8674
	step [167/190], loss=100.8412
	step [168/190], loss=94.7970
	step [169/190], loss=76.2711
	step [170/190], loss=86.8656
	step [171/190], loss=80.2977
	step [172/190], loss=89.5808
	step [173/190], loss=82.9832
	step [174/190], loss=83.3540
	step [175/190], loss=91.5931
	step [176/190], loss=82.2373
	step [177/190], loss=94.6243
	step [178/190], loss=95.5379
	step [179/190], loss=84.1321
	step [180/190], loss=86.9721
	step [181/190], loss=98.4757
	step [182/190], loss=82.9103
	step [183/190], loss=82.4586
	step [184/190], loss=82.7849
	step [185/190], loss=91.5747
	step [186/190], loss=81.5030
	step [187/190], loss=84.6042
	step [188/190], loss=88.3856
	step [189/190], loss=94.5549
	step [190/190], loss=93.5460
	Evaluating
	loss=0.0139, precision=0.4297, recall=0.8728, f1=0.5759
Training epoch 23
	step [1/190], loss=84.3443
	step [2/190], loss=88.1604
	step [3/190], loss=80.6431
	step [4/190], loss=104.2667
	step [5/190], loss=81.2678
	step [6/190], loss=78.7125
	step [7/190], loss=73.7014
	step [8/190], loss=94.8584
	step [9/190], loss=89.6815
	step [10/190], loss=89.4849
	step [11/190], loss=75.8762
	step [12/190], loss=84.1140
	step [13/190], loss=89.5941
	step [14/190], loss=90.5090
	step [15/190], loss=85.0176
	step [16/190], loss=93.0565
	step [17/190], loss=88.0741
	step [18/190], loss=91.5656
	step [19/190], loss=86.3562
	step [20/190], loss=77.0363
	step [21/190], loss=89.1689
	step [22/190], loss=96.4035
	step [23/190], loss=80.1862
	step [24/190], loss=77.5801
	step [25/190], loss=89.4802
	step [26/190], loss=86.7026
	step [27/190], loss=90.5287
	step [28/190], loss=84.5640
	step [29/190], loss=93.7075
	step [30/190], loss=86.9930
	step [31/190], loss=83.8690
	step [32/190], loss=94.8401
	step [33/190], loss=83.5947
	step [34/190], loss=88.8838
	step [35/190], loss=88.7864
	step [36/190], loss=93.9382
	step [37/190], loss=82.1529
	step [38/190], loss=77.5139
	step [39/190], loss=91.2971
	step [40/190], loss=105.0780
	step [41/190], loss=87.6640
	step [42/190], loss=77.9113
	step [43/190], loss=85.9055
	step [44/190], loss=76.8777
	step [45/190], loss=82.9255
	step [46/190], loss=92.3278
	step [47/190], loss=81.9625
	step [48/190], loss=93.9504
	step [49/190], loss=83.4734
	step [50/190], loss=91.1371
	step [51/190], loss=91.4854
	step [52/190], loss=80.6829
	step [53/190], loss=105.4745
	step [54/190], loss=71.5661
	step [55/190], loss=98.6195
	step [56/190], loss=93.3940
	step [57/190], loss=85.9974
	step [58/190], loss=87.9408
	step [59/190], loss=93.3427
	step [60/190], loss=88.6445
	step [61/190], loss=81.2282
	step [62/190], loss=86.8946
	step [63/190], loss=78.1306
	step [64/190], loss=80.8512
	step [65/190], loss=94.7270
	step [66/190], loss=81.9257
	step [67/190], loss=88.7160
	step [68/190], loss=83.7335
	step [69/190], loss=80.6706
	step [70/190], loss=89.1949
	step [71/190], loss=94.1415
	step [72/190], loss=84.4193
	step [73/190], loss=86.2324
	step [74/190], loss=82.0102
	step [75/190], loss=97.8716
	step [76/190], loss=90.5093
	step [77/190], loss=76.6028
	step [78/190], loss=93.9189
	step [79/190], loss=71.6103
	step [80/190], loss=80.8082
	step [81/190], loss=77.1112
	step [82/190], loss=78.6049
	step [83/190], loss=78.8328
	step [84/190], loss=92.8887
	step [85/190], loss=82.1812
	step [86/190], loss=90.5502
	step [87/190], loss=75.0887
	step [88/190], loss=92.5416
	step [89/190], loss=78.8994
	step [90/190], loss=102.5724
	step [91/190], loss=83.5813
	step [92/190], loss=75.1503
	step [93/190], loss=105.6812
	step [94/190], loss=89.7194
	step [95/190], loss=79.5753
	step [96/190], loss=100.1670
	step [97/190], loss=84.4330
	step [98/190], loss=74.2910
	step [99/190], loss=71.6504
	step [100/190], loss=74.7420
	step [101/190], loss=87.3551
	step [102/190], loss=88.5040
	step [103/190], loss=84.5463
	step [104/190], loss=75.2312
	step [105/190], loss=76.9667
	step [106/190], loss=73.3831
	step [107/190], loss=84.6118
	step [108/190], loss=93.4254
	step [109/190], loss=93.6036
	step [110/190], loss=84.9588
	step [111/190], loss=82.3641
	step [112/190], loss=82.5689
	step [113/190], loss=92.1245
	step [114/190], loss=97.4701
	step [115/190], loss=97.0497
	step [116/190], loss=85.1232
	step [117/190], loss=78.7420
	step [118/190], loss=84.4678
	step [119/190], loss=92.9390
	step [120/190], loss=71.9784
	step [121/190], loss=88.0047
	step [122/190], loss=69.6284
	step [123/190], loss=87.3457
	step [124/190], loss=86.5067
	step [125/190], loss=91.9959
	step [126/190], loss=89.2725
	step [127/190], loss=88.1481
	step [128/190], loss=86.9684
	step [129/190], loss=84.3725
	step [130/190], loss=107.3260
	step [131/190], loss=73.9254
	step [132/190], loss=72.7009
	step [133/190], loss=82.7124
	step [134/190], loss=75.6067
	step [135/190], loss=74.7921
	step [136/190], loss=96.4944
	step [137/190], loss=88.5938
	step [138/190], loss=84.4643
	step [139/190], loss=68.7520
	step [140/190], loss=90.0808
	step [141/190], loss=82.8315
	step [142/190], loss=89.6392
	step [143/190], loss=82.2983
	step [144/190], loss=81.8075
	step [145/190], loss=83.5572
	step [146/190], loss=82.2489
	step [147/190], loss=88.5718
	step [148/190], loss=79.7395
	step [149/190], loss=73.7981
	step [150/190], loss=82.0217
	step [151/190], loss=92.4959
	step [152/190], loss=75.8761
	step [153/190], loss=73.1528
	step [154/190], loss=91.8056
	step [155/190], loss=94.5988
	step [156/190], loss=85.3527
	step [157/190], loss=85.5914
	step [158/190], loss=79.2109
	step [159/190], loss=84.9315
	step [160/190], loss=84.7130
	step [161/190], loss=81.3283
	step [162/190], loss=103.7307
	step [163/190], loss=74.5001
	step [164/190], loss=94.6047
	step [165/190], loss=95.9473
	step [166/190], loss=91.6031
	step [167/190], loss=90.4976
	step [168/190], loss=92.1925
	step [169/190], loss=75.4424
	step [170/190], loss=71.8465
	step [171/190], loss=86.2583
	step [172/190], loss=89.8335
	step [173/190], loss=92.9007
	step [174/190], loss=93.4960
	step [175/190], loss=69.6037
	step [176/190], loss=77.8130
	step [177/190], loss=82.3426
	step [178/190], loss=81.7448
	step [179/190], loss=88.9320
	step [180/190], loss=84.5347
	step [181/190], loss=83.4827
	step [182/190], loss=95.5896
	step [183/190], loss=87.2365
	step [184/190], loss=75.4582
	step [185/190], loss=90.7888
	step [186/190], loss=96.4561
	step [187/190], loss=92.1357
	step [188/190], loss=84.2074
	step [189/190], loss=80.4251
	step [190/190], loss=76.6621
	Evaluating
	loss=0.0123, precision=0.4605, recall=0.8867, f1=0.6062
Training epoch 24
	step [1/190], loss=80.7365
	step [2/190], loss=72.4991
	step [3/190], loss=74.8472
	step [4/190], loss=74.8946
	step [5/190], loss=82.4847
	step [6/190], loss=82.7678
	step [7/190], loss=87.5974
	step [8/190], loss=99.7379
	step [9/190], loss=77.3140
	step [10/190], loss=79.9109
	step [11/190], loss=79.4358
	step [12/190], loss=83.7498
	step [13/190], loss=85.1133
	step [14/190], loss=92.6963
	step [15/190], loss=81.3810
	step [16/190], loss=81.1450
	step [17/190], loss=76.5567
	step [18/190], loss=86.0706
	step [19/190], loss=61.0026
	step [20/190], loss=85.3945
	step [21/190], loss=80.1734
	step [22/190], loss=80.5719
	step [23/190], loss=92.2670
	step [24/190], loss=91.7090
	step [25/190], loss=88.9873
	step [26/190], loss=81.4317
	step [27/190], loss=83.1992
	step [28/190], loss=104.6839
	step [29/190], loss=83.3217
	step [30/190], loss=69.6193
	step [31/190], loss=80.7343
	step [32/190], loss=86.5058
	step [33/190], loss=98.0480
	step [34/190], loss=80.5671
	step [35/190], loss=89.2539
	step [36/190], loss=89.9203
	step [37/190], loss=67.7971
	step [38/190], loss=79.7697
	step [39/190], loss=93.9246
	step [40/190], loss=86.3685
	step [41/190], loss=71.0163
	step [42/190], loss=92.9401
	step [43/190], loss=88.8854
	step [44/190], loss=87.7976
	step [45/190], loss=79.3886
	step [46/190], loss=87.7044
	step [47/190], loss=85.4638
	step [48/190], loss=81.3502
	step [49/190], loss=88.5149
	step [50/190], loss=65.2431
	step [51/190], loss=93.5816
	step [52/190], loss=77.7240
	step [53/190], loss=95.3847
	step [54/190], loss=82.9084
	step [55/190], loss=99.2104
	step [56/190], loss=78.2690
	step [57/190], loss=100.4475
	step [58/190], loss=91.1091
	step [59/190], loss=89.0527
	step [60/190], loss=80.5811
	step [61/190], loss=93.2267
	step [62/190], loss=96.0731
	step [63/190], loss=98.3249
	step [64/190], loss=81.2112
	step [65/190], loss=77.5426
	step [66/190], loss=77.8568
	step [67/190], loss=84.6423
	step [68/190], loss=91.9028
	step [69/190], loss=81.0467
	step [70/190], loss=84.0466
	step [71/190], loss=82.6195
	step [72/190], loss=80.2394
	step [73/190], loss=62.4902
	step [74/190], loss=94.6290
	step [75/190], loss=74.3039
	step [76/190], loss=90.2294
	step [77/190], loss=77.9425
	step [78/190], loss=83.6299
	step [79/190], loss=84.2498
	step [80/190], loss=79.9390
	step [81/190], loss=86.5177
	step [82/190], loss=85.0757
	step [83/190], loss=83.6407
	step [84/190], loss=93.1412
	step [85/190], loss=86.1105
	step [86/190], loss=78.4283
	step [87/190], loss=79.8082
	step [88/190], loss=80.1724
	step [89/190], loss=87.4818
	step [90/190], loss=83.3549
	step [91/190], loss=85.8401
	step [92/190], loss=80.4065
	step [93/190], loss=80.4701
	step [94/190], loss=92.0150
	step [95/190], loss=78.3726
	step [96/190], loss=73.4998
	step [97/190], loss=90.4619
	step [98/190], loss=98.0898
	step [99/190], loss=73.1814
	step [100/190], loss=85.7636
	step [101/190], loss=78.5303
	step [102/190], loss=90.4769
	step [103/190], loss=93.4617
	step [104/190], loss=90.4053
	step [105/190], loss=84.7214
	step [106/190], loss=90.9572
	step [107/190], loss=80.4254
	step [108/190], loss=79.9922
	step [109/190], loss=100.7867
	step [110/190], loss=82.1595
	step [111/190], loss=83.9909
	step [112/190], loss=96.4114
	step [113/190], loss=77.8541
	step [114/190], loss=83.9670
	step [115/190], loss=83.0268
	step [116/190], loss=82.9126
	step [117/190], loss=92.2070
	step [118/190], loss=85.1107
	step [119/190], loss=73.3739
	step [120/190], loss=90.5933
	step [121/190], loss=85.4263
	step [122/190], loss=81.2229
	step [123/190], loss=76.0999
	step [124/190], loss=93.6090
	step [125/190], loss=86.0574
	step [126/190], loss=88.3113
	step [127/190], loss=91.8386
	step [128/190], loss=94.4429
	step [129/190], loss=72.6931
	step [130/190], loss=93.8156
	step [131/190], loss=85.2925
	step [132/190], loss=76.0749
	step [133/190], loss=91.3275
	step [134/190], loss=88.5651
	step [135/190], loss=94.7600
	step [136/190], loss=89.2203
	step [137/190], loss=83.4151
	step [138/190], loss=89.2857
	step [139/190], loss=84.4866
	step [140/190], loss=68.7005
	step [141/190], loss=84.1931
	step [142/190], loss=73.4655
	step [143/190], loss=79.5197
	step [144/190], loss=99.6231
	step [145/190], loss=79.5914
	step [146/190], loss=80.9271
	step [147/190], loss=88.1531
	step [148/190], loss=82.6543
	step [149/190], loss=91.0838
	step [150/190], loss=77.3742
	step [151/190], loss=98.6201
	step [152/190], loss=87.0641
	step [153/190], loss=75.5686
	step [154/190], loss=88.1894
	step [155/190], loss=83.3010
	step [156/190], loss=82.8535
	step [157/190], loss=68.7716
	step [158/190], loss=85.2059
	step [159/190], loss=90.7357
	step [160/190], loss=85.1967
	step [161/190], loss=77.7135
	step [162/190], loss=91.4065
	step [163/190], loss=78.7201
	step [164/190], loss=80.8800
	step [165/190], loss=72.1945
	step [166/190], loss=94.6579
	step [167/190], loss=79.3976
	step [168/190], loss=92.7681
	step [169/190], loss=101.2594
	step [170/190], loss=96.3511
	step [171/190], loss=82.5315
	step [172/190], loss=88.7800
	step [173/190], loss=85.3939
	step [174/190], loss=85.0314
	step [175/190], loss=79.0858
	step [176/190], loss=74.2360
	step [177/190], loss=76.3321
	step [178/190], loss=87.0514
	step [179/190], loss=97.1671
	step [180/190], loss=87.3491
	step [181/190], loss=89.5374
	step [182/190], loss=76.3376
	step [183/190], loss=108.9475
	step [184/190], loss=82.1680
	step [185/190], loss=81.0464
	step [186/190], loss=79.8227
	step [187/190], loss=77.6904
	step [188/190], loss=92.1354
	step [189/190], loss=94.1050
	step [190/190], loss=73.4223
	Evaluating
	loss=0.0121, precision=0.4428, recall=0.8894, f1=0.5913
Training epoch 25
	step [1/190], loss=88.8002
	step [2/190], loss=92.4526
	step [3/190], loss=81.3754
	step [4/190], loss=98.5379
	step [5/190], loss=87.3833
	step [6/190], loss=86.9068
	step [7/190], loss=90.6889
	step [8/190], loss=91.7066
	step [9/190], loss=75.2113
	step [10/190], loss=83.4593
	step [11/190], loss=84.9876
	step [12/190], loss=72.5964
	step [13/190], loss=98.1102
	step [14/190], loss=73.0253
	step [15/190], loss=98.7249
	step [16/190], loss=82.1238
	step [17/190], loss=76.7625
	step [18/190], loss=77.8035
	step [19/190], loss=87.4684
	step [20/190], loss=81.3842
	step [21/190], loss=86.8398
	step [22/190], loss=82.4998
	step [23/190], loss=77.6342
	step [24/190], loss=86.3271
	step [25/190], loss=80.7701
	step [26/190], loss=72.7661
	step [27/190], loss=77.4993
	step [28/190], loss=76.0820
	step [29/190], loss=81.7933
	step [30/190], loss=91.2902
	step [31/190], loss=83.0401
	step [32/190], loss=79.7448
	step [33/190], loss=72.5920
	step [34/190], loss=89.3402
	step [35/190], loss=84.3204
	step [36/190], loss=78.6322
	step [37/190], loss=96.7241
	step [38/190], loss=91.3699
	step [39/190], loss=86.8191
	step [40/190], loss=78.9217
	step [41/190], loss=92.3416
	step [42/190], loss=83.3012
	step [43/190], loss=92.4008
	step [44/190], loss=75.8087
	step [45/190], loss=85.1514
	step [46/190], loss=81.5137
	step [47/190], loss=81.4435
	step [48/190], loss=90.5904
	step [49/190], loss=66.9313
	step [50/190], loss=68.5577
	step [51/190], loss=68.1675
	step [52/190], loss=78.8710
	step [53/190], loss=70.2396
	step [54/190], loss=78.8031
	step [55/190], loss=92.6527
	step [56/190], loss=87.5086
	step [57/190], loss=89.2755
	step [58/190], loss=84.7971
	step [59/190], loss=77.8897
	step [60/190], loss=102.2691
	step [61/190], loss=83.8727
	step [62/190], loss=84.1466
	step [63/190], loss=75.8844
	step [64/190], loss=82.6545
	step [65/190], loss=73.0866
	step [66/190], loss=84.8593
	step [67/190], loss=76.2956
	step [68/190], loss=80.8974
	step [69/190], loss=95.6326
	step [70/190], loss=83.2754
	step [71/190], loss=85.4627
	step [72/190], loss=80.1561
	step [73/190], loss=92.4332
	step [74/190], loss=102.5285
	step [75/190], loss=76.2089
	step [76/190], loss=82.2103
	step [77/190], loss=78.4489
	step [78/190], loss=92.3903
	step [79/190], loss=85.2790
	step [80/190], loss=94.3192
	step [81/190], loss=82.9976
	step [82/190], loss=78.9747
	step [83/190], loss=78.5737
	step [84/190], loss=85.2606
	step [85/190], loss=82.8363
	step [86/190], loss=76.8710
	step [87/190], loss=85.9093
	step [88/190], loss=82.4183
	step [89/190], loss=82.3222
	step [90/190], loss=81.4783
	step [91/190], loss=75.8971
	step [92/190], loss=77.3911
	step [93/190], loss=74.0301
	step [94/190], loss=88.2449
	step [95/190], loss=93.1650
	step [96/190], loss=87.6134
	step [97/190], loss=82.8688
	step [98/190], loss=93.1692
	step [99/190], loss=95.3259
	step [100/190], loss=73.7660
	step [101/190], loss=77.0269
	step [102/190], loss=98.1065
	step [103/190], loss=78.5645
	step [104/190], loss=84.4139
	step [105/190], loss=80.9005
	step [106/190], loss=88.3548
	step [107/190], loss=75.3314
	step [108/190], loss=83.9556
	step [109/190], loss=84.7985
	step [110/190], loss=72.3320
	step [111/190], loss=74.0199
	step [112/190], loss=72.2759
	step [113/190], loss=90.8248
	step [114/190], loss=89.2469
	step [115/190], loss=72.5327
	step [116/190], loss=84.2271
	step [117/190], loss=85.2030
	step [118/190], loss=84.3912
	step [119/190], loss=84.8446
	step [120/190], loss=91.1836
	step [121/190], loss=86.3264
	step [122/190], loss=76.0182
	step [123/190], loss=73.1481
	step [124/190], loss=90.2662
	step [125/190], loss=103.4389
	step [126/190], loss=76.6285
	step [127/190], loss=79.8247
	step [128/190], loss=79.8085
	step [129/190], loss=85.2290
	step [130/190], loss=76.5512
	step [131/190], loss=80.4794
	step [132/190], loss=89.6757
	step [133/190], loss=73.7127
	step [134/190], loss=77.5822
	step [135/190], loss=90.0499
	step [136/190], loss=75.8626
	step [137/190], loss=87.6389
	step [138/190], loss=76.9772
	step [139/190], loss=99.3451
	step [140/190], loss=81.9177
	step [141/190], loss=93.8912
	step [142/190], loss=86.0763
	step [143/190], loss=85.7297
	step [144/190], loss=89.1484
	step [145/190], loss=85.0694
	step [146/190], loss=87.9137
	step [147/190], loss=77.7291
	step [148/190], loss=95.3596
	step [149/190], loss=90.0711
	step [150/190], loss=89.9298
	step [151/190], loss=80.9240
	step [152/190], loss=72.4879
	step [153/190], loss=70.1039
	step [154/190], loss=76.8090
	step [155/190], loss=90.5726
	step [156/190], loss=77.9965
	step [157/190], loss=88.1095
	step [158/190], loss=79.1702
	step [159/190], loss=94.0123
	step [160/190], loss=81.0370
	step [161/190], loss=81.4189
	step [162/190], loss=86.9580
	step [163/190], loss=81.3925
	step [164/190], loss=94.0851
	step [165/190], loss=72.4291
	step [166/190], loss=100.9524
	step [167/190], loss=86.2773
	step [168/190], loss=72.3442
	step [169/190], loss=81.9014
	step [170/190], loss=97.1758
	step [171/190], loss=84.8005
	step [172/190], loss=83.1981
	step [173/190], loss=88.5745
	step [174/190], loss=86.2488
	step [175/190], loss=76.6294
	step [176/190], loss=79.1735
	step [177/190], loss=95.2910
	step [178/190], loss=86.3330
	step [179/190], loss=92.7009
	step [180/190], loss=90.4880
	step [181/190], loss=91.0285
	step [182/190], loss=82.3148
	step [183/190], loss=90.5805
	step [184/190], loss=89.3147
	step [185/190], loss=77.9169
	step [186/190], loss=88.1155
	step [187/190], loss=78.0285
	step [188/190], loss=100.8104
	step [189/190], loss=80.3727
	step [190/190], loss=77.1124
	Evaluating
	loss=0.0132, precision=0.4204, recall=0.8758, f1=0.5681
Training epoch 26
	step [1/190], loss=79.8143
	step [2/190], loss=76.3470
	step [3/190], loss=90.7667
	step [4/190], loss=89.5570
	step [5/190], loss=85.3822
	step [6/190], loss=98.8074
	step [7/190], loss=89.2835
	step [8/190], loss=87.5206
	step [9/190], loss=70.7588
	step [10/190], loss=87.2364
	step [11/190], loss=93.1505
	step [12/190], loss=90.0872
	step [13/190], loss=95.5206
	step [14/190], loss=88.5524
	step [15/190], loss=82.7039
	step [16/190], loss=86.4020
	step [17/190], loss=93.6625
	step [18/190], loss=94.0515
	step [19/190], loss=85.3274
	step [20/190], loss=89.4364
	step [21/190], loss=69.6977
	step [22/190], loss=87.9812
	step [23/190], loss=74.0295
	step [24/190], loss=94.6754
	step [25/190], loss=95.6681
	step [26/190], loss=72.1231
	step [27/190], loss=64.4730
	step [28/190], loss=87.3086
	step [29/190], loss=82.2948
	step [30/190], loss=73.8936
	step [31/190], loss=85.9134
	step [32/190], loss=78.1230
	step [33/190], loss=74.9785
	step [34/190], loss=85.0049
	step [35/190], loss=93.5629
	step [36/190], loss=81.5128
	step [37/190], loss=81.7699
	step [38/190], loss=90.3410
	step [39/190], loss=61.8980
	step [40/190], loss=92.9833
	step [41/190], loss=79.8118
	step [42/190], loss=96.3231
	step [43/190], loss=93.8524
	step [44/190], loss=93.7941
	step [45/190], loss=85.0046
	step [46/190], loss=88.0194
	step [47/190], loss=83.6710
	step [48/190], loss=103.1796
	step [49/190], loss=81.7405
	step [50/190], loss=79.3731
	step [51/190], loss=74.4368
	step [52/190], loss=89.3275
	step [53/190], loss=86.6529
	step [54/190], loss=73.1011
	step [55/190], loss=97.1867
	step [56/190], loss=93.0655
	step [57/190], loss=72.5579
	step [58/190], loss=78.8936
	step [59/190], loss=88.3384
	step [60/190], loss=79.5851
	step [61/190], loss=85.7738
	step [62/190], loss=75.6490
	step [63/190], loss=82.2947
	step [64/190], loss=80.0681
	step [65/190], loss=93.5427
	step [66/190], loss=84.3538
	step [67/190], loss=84.4477
	step [68/190], loss=78.0296
	step [69/190], loss=89.7075
	step [70/190], loss=72.0156
	step [71/190], loss=93.3790
	step [72/190], loss=96.0903
	step [73/190], loss=74.2407
	step [74/190], loss=71.3070
	step [75/190], loss=96.7223
	step [76/190], loss=86.9975
	step [77/190], loss=82.2887
	step [78/190], loss=82.6049
	step [79/190], loss=89.4492
	step [80/190], loss=65.1086
	step [81/190], loss=84.6473
	step [82/190], loss=78.1198
	step [83/190], loss=89.3681
	step [84/190], loss=93.5552
	step [85/190], loss=71.6409
	step [86/190], loss=91.5441
	step [87/190], loss=75.1191
	step [88/190], loss=79.1986
	step [89/190], loss=83.5400
	step [90/190], loss=79.0040
	step [91/190], loss=83.9380
	step [92/190], loss=86.6438
	step [93/190], loss=70.4403
	step [94/190], loss=85.7152
	step [95/190], loss=83.4639
	step [96/190], loss=81.5057
	step [97/190], loss=70.4936
	step [98/190], loss=83.2284
	step [99/190], loss=93.1153
	step [100/190], loss=84.9825
	step [101/190], loss=87.0968
	step [102/190], loss=75.1145
	step [103/190], loss=79.5724
	step [104/190], loss=87.9873
	step [105/190], loss=95.1771
	step [106/190], loss=89.2821
	step [107/190], loss=90.7915
	step [108/190], loss=79.8789
	step [109/190], loss=68.6762
	step [110/190], loss=81.4445
	step [111/190], loss=80.1708
	step [112/190], loss=74.6613
	step [113/190], loss=79.7904
	step [114/190], loss=76.3411
	step [115/190], loss=73.8529
	step [116/190], loss=80.9679
	step [117/190], loss=88.8936
	step [118/190], loss=94.5224
	step [119/190], loss=70.2747
	step [120/190], loss=79.5869
	step [121/190], loss=77.3295
	step [122/190], loss=91.6213
	step [123/190], loss=94.2807
	step [124/190], loss=81.7768
	step [125/190], loss=74.9102
	step [126/190], loss=82.1692
	step [127/190], loss=75.5971
	step [128/190], loss=61.1270
	step [129/190], loss=85.7519
	step [130/190], loss=71.8970
	step [131/190], loss=91.5810
	step [132/190], loss=83.9852
	step [133/190], loss=76.8843
	step [134/190], loss=90.8965
	step [135/190], loss=92.4250
	step [136/190], loss=82.2064
	step [137/190], loss=85.0894
	step [138/190], loss=76.5621
	step [139/190], loss=68.7116
	step [140/190], loss=81.7590
	step [141/190], loss=80.1357
	step [142/190], loss=69.8494
	step [143/190], loss=78.0835
	step [144/190], loss=87.7595
	step [145/190], loss=87.5481
	step [146/190], loss=90.6490
	step [147/190], loss=78.0719
	step [148/190], loss=78.6095
	step [149/190], loss=75.5198
	step [150/190], loss=93.3613
	step [151/190], loss=93.4809
	step [152/190], loss=78.3643
	step [153/190], loss=90.4576
	step [154/190], loss=71.9414
	step [155/190], loss=73.2801
	step [156/190], loss=83.9203
	step [157/190], loss=80.8633
	step [158/190], loss=84.7466
	step [159/190], loss=78.5258
	step [160/190], loss=71.6656
	step [161/190], loss=82.4387
	step [162/190], loss=88.6749
	step [163/190], loss=68.7992
	step [164/190], loss=82.5468
	step [165/190], loss=76.1778
	step [166/190], loss=73.9809
	step [167/190], loss=93.9356
	step [168/190], loss=80.3152
	step [169/190], loss=86.9299
	step [170/190], loss=68.1485
	step [171/190], loss=79.1881
	step [172/190], loss=88.9662
	step [173/190], loss=89.8206
	step [174/190], loss=72.2081
	step [175/190], loss=67.6410
	step [176/190], loss=82.7457
	step [177/190], loss=89.3991
	step [178/190], loss=97.9842
	step [179/190], loss=74.4023
	step [180/190], loss=88.5493
	step [181/190], loss=81.8418
	step [182/190], loss=91.1964
	step [183/190], loss=83.8971
	step [184/190], loss=96.4335
	step [185/190], loss=75.7827
	step [186/190], loss=76.4702
	step [187/190], loss=85.7959
	step [188/190], loss=69.0446
	step [189/190], loss=84.4157
	step [190/190], loss=71.1944
	Evaluating
	loss=0.0108, precision=0.4575, recall=0.8904, f1=0.6044
Training epoch 27
	step [1/190], loss=80.7382
	step [2/190], loss=81.5133
	step [3/190], loss=86.7026
	step [4/190], loss=75.5824
	step [5/190], loss=78.9586
	step [6/190], loss=76.5708
	step [7/190], loss=85.0403
	step [8/190], loss=85.9192
	step [9/190], loss=80.0379
	step [10/190], loss=78.3716
	step [11/190], loss=87.9872
	step [12/190], loss=77.5593
	step [13/190], loss=86.1047
	step [14/190], loss=79.5299
	step [15/190], loss=72.3563
	step [16/190], loss=72.1104
	step [17/190], loss=92.7765
	step [18/190], loss=83.0103
	step [19/190], loss=69.9969
	step [20/190], loss=82.0141
	step [21/190], loss=82.8371
	step [22/190], loss=80.4890
	step [23/190], loss=80.7310
	step [24/190], loss=67.1742
	step [25/190], loss=84.2495
	step [26/190], loss=83.6572
	step [27/190], loss=74.3932
	step [28/190], loss=77.8662
	step [29/190], loss=83.8919
	step [30/190], loss=88.7016
	step [31/190], loss=81.7691
	step [32/190], loss=71.1032
	step [33/190], loss=67.1023
	step [34/190], loss=76.0426
	step [35/190], loss=84.8280
	step [36/190], loss=72.0789
	step [37/190], loss=81.9209
	step [38/190], loss=66.2892
	step [39/190], loss=78.6694
	step [40/190], loss=77.7961
	step [41/190], loss=96.1358
	step [42/190], loss=79.6403
	step [43/190], loss=94.8011
	step [44/190], loss=78.2076
	step [45/190], loss=80.4402
	step [46/190], loss=77.3342
	step [47/190], loss=79.2165
	step [48/190], loss=78.0738
	step [49/190], loss=83.5421
	step [50/190], loss=76.2989
	step [51/190], loss=90.0014
	step [52/190], loss=72.4272
	step [53/190], loss=80.6585
	step [54/190], loss=64.4074
	step [55/190], loss=87.4283
	step [56/190], loss=75.3420
	step [57/190], loss=82.8480
	step [58/190], loss=103.5575
	step [59/190], loss=83.1542
	step [60/190], loss=96.2699
	step [61/190], loss=68.3219
	step [62/190], loss=85.3988
	step [63/190], loss=72.2829
	step [64/190], loss=70.2454
	step [65/190], loss=81.4454
	step [66/190], loss=77.1461
	step [67/190], loss=79.4215
	step [68/190], loss=81.1441
	step [69/190], loss=71.4555
	step [70/190], loss=83.0101
	step [71/190], loss=89.6825
	step [72/190], loss=85.0045
	step [73/190], loss=74.6397
	step [74/190], loss=77.2355
	step [75/190], loss=87.2480
	step [76/190], loss=74.2115
	step [77/190], loss=79.0343
	step [78/190], loss=85.6856
	step [79/190], loss=90.1249
	step [80/190], loss=55.7357
	step [81/190], loss=90.4648
	step [82/190], loss=90.5741
	step [83/190], loss=91.3148
	step [84/190], loss=71.3258
	step [85/190], loss=73.6470
	step [86/190], loss=70.5955
	step [87/190], loss=78.7963
	step [88/190], loss=86.5079
	step [89/190], loss=76.0618
	step [90/190], loss=75.2026
	step [91/190], loss=83.0213
	step [92/190], loss=75.3915
	step [93/190], loss=93.7579
	step [94/190], loss=82.9586
	step [95/190], loss=84.7849
	step [96/190], loss=88.9091
	step [97/190], loss=77.9678
	step [98/190], loss=83.1101
	step [99/190], loss=78.8532
	step [100/190], loss=86.2140
	step [101/190], loss=75.3373
	step [102/190], loss=87.1414
	step [103/190], loss=88.7387
	step [104/190], loss=84.2648
	step [105/190], loss=93.0920
	step [106/190], loss=82.4512
	step [107/190], loss=96.2376
	step [108/190], loss=80.0159
	step [109/190], loss=75.4267
	step [110/190], loss=75.1532
	step [111/190], loss=77.4190
	step [112/190], loss=71.9155
	step [113/190], loss=75.7656
	step [114/190], loss=80.5586
	step [115/190], loss=84.5962
	step [116/190], loss=83.5709
	step [117/190], loss=71.3771
	step [118/190], loss=82.0814
	step [119/190], loss=84.4436
	step [120/190], loss=82.6572
	step [121/190], loss=90.2713
	step [122/190], loss=85.0127
	step [123/190], loss=95.4030
	step [124/190], loss=75.6190
	step [125/190], loss=89.5966
	step [126/190], loss=83.4592
	step [127/190], loss=93.3480
	step [128/190], loss=86.5954
	step [129/190], loss=86.9915
	step [130/190], loss=88.8739
	step [131/190], loss=89.4392
	step [132/190], loss=81.0431
	step [133/190], loss=77.2244
	step [134/190], loss=94.0068
	step [135/190], loss=90.7101
	step [136/190], loss=87.0875
	step [137/190], loss=87.1368
	step [138/190], loss=91.0304
	step [139/190], loss=82.3418
	step [140/190], loss=111.1617
	step [141/190], loss=100.1952
	step [142/190], loss=90.8846
	step [143/190], loss=81.5973
	step [144/190], loss=82.6570
	step [145/190], loss=96.3998
	step [146/190], loss=70.7841
	step [147/190], loss=86.0557
	step [148/190], loss=85.9257
	step [149/190], loss=68.0745
	step [150/190], loss=86.8652
	step [151/190], loss=97.1297
	step [152/190], loss=93.8268
	step [153/190], loss=89.6461
	step [154/190], loss=71.1825
	step [155/190], loss=74.4565
	step [156/190], loss=82.0817
	step [157/190], loss=78.2636
	step [158/190], loss=77.4440
	step [159/190], loss=88.0513
	step [160/190], loss=83.6929
	step [161/190], loss=75.8896
	step [162/190], loss=85.6763
	step [163/190], loss=84.0709
	step [164/190], loss=86.6138
	step [165/190], loss=65.1045
	step [166/190], loss=87.4883
	step [167/190], loss=91.4874
	step [168/190], loss=73.4524
	step [169/190], loss=88.2809
	step [170/190], loss=69.4018
	step [171/190], loss=85.8339
	step [172/190], loss=83.1680
	step [173/190], loss=84.1752
	step [174/190], loss=82.8600
	step [175/190], loss=90.8737
	step [176/190], loss=100.4617
	step [177/190], loss=96.5320
	step [178/190], loss=82.0078
	step [179/190], loss=85.0334
	step [180/190], loss=80.1612
	step [181/190], loss=72.8796
	step [182/190], loss=78.8284
	step [183/190], loss=83.4724
	step [184/190], loss=93.8206
	step [185/190], loss=78.7428
	step [186/190], loss=95.5773
	step [187/190], loss=87.2726
	step [188/190], loss=76.1920
	step [189/190], loss=72.0766
	step [190/190], loss=55.5131
	Evaluating
	loss=0.0105, precision=0.4684, recall=0.8738, f1=0.6099
Training epoch 28
	step [1/190], loss=75.3539
	step [2/190], loss=78.3755
	step [3/190], loss=101.8425
	step [4/190], loss=77.4771
	step [5/190], loss=73.9886
	step [6/190], loss=90.7030
	step [7/190], loss=71.2614
	step [8/190], loss=71.6781
	step [9/190], loss=83.4061
	step [10/190], loss=86.5995
	step [11/190], loss=79.0375
	step [12/190], loss=74.4662
	step [13/190], loss=82.2271
	step [14/190], loss=91.6396
	step [15/190], loss=80.3015
	step [16/190], loss=78.9580
	step [17/190], loss=75.9603
	step [18/190], loss=86.1355
	step [19/190], loss=89.3185
	step [20/190], loss=73.3648
	step [21/190], loss=80.7374
	step [22/190], loss=89.3443
	step [23/190], loss=87.9436
	step [24/190], loss=101.3229
	step [25/190], loss=84.9387
	step [26/190], loss=72.6990
	step [27/190], loss=82.0580
	step [28/190], loss=93.0535
	step [29/190], loss=78.2843
	step [30/190], loss=85.3211
	step [31/190], loss=77.9516
	step [32/190], loss=101.5134
	step [33/190], loss=72.1566
	step [34/190], loss=84.9973
	step [35/190], loss=78.2504
	step [36/190], loss=69.3897
	step [37/190], loss=70.9291
	step [38/190], loss=70.1733
	step [39/190], loss=72.8087
	step [40/190], loss=85.4980
	step [41/190], loss=88.6720
	step [42/190], loss=80.3053
	step [43/190], loss=86.1175
	step [44/190], loss=77.7047
	step [45/190], loss=78.6394
	step [46/190], loss=86.3842
	step [47/190], loss=72.9178
	step [48/190], loss=77.6435
	step [49/190], loss=74.2469
	step [50/190], loss=92.5824
	step [51/190], loss=75.8950
	step [52/190], loss=84.8204
	step [53/190], loss=95.7987
	step [54/190], loss=71.3802
	step [55/190], loss=88.0483
	step [56/190], loss=78.0116
	step [57/190], loss=84.8411
	step [58/190], loss=77.0874
	step [59/190], loss=70.2506
	step [60/190], loss=99.2484
	step [61/190], loss=75.1377
	step [62/190], loss=83.1079
	step [63/190], loss=74.5370
	step [64/190], loss=75.8678
	step [65/190], loss=85.2474
	step [66/190], loss=97.5525
	step [67/190], loss=83.3685
	step [68/190], loss=74.0792
	step [69/190], loss=78.2090
	step [70/190], loss=91.3292
	step [71/190], loss=103.9702
	step [72/190], loss=88.0468
	step [73/190], loss=77.3159
	step [74/190], loss=83.5896
	step [75/190], loss=77.5432
	step [76/190], loss=82.1382
	step [77/190], loss=88.5099
	step [78/190], loss=68.5544
	step [79/190], loss=80.7678
	step [80/190], loss=76.7893
	step [81/190], loss=78.3291
	step [82/190], loss=79.2305
	step [83/190], loss=72.3842
	step [84/190], loss=77.0799
	step [85/190], loss=82.8324
	step [86/190], loss=83.5338
	step [87/190], loss=69.7344
	step [88/190], loss=99.7384
	step [89/190], loss=82.9384
	step [90/190], loss=77.5214
	step [91/190], loss=83.3563
	step [92/190], loss=87.8141
	step [93/190], loss=89.7665
	step [94/190], loss=81.8993
	step [95/190], loss=76.4688
	step [96/190], loss=77.6931
	step [97/190], loss=81.8625
	step [98/190], loss=84.1760
	step [99/190], loss=77.4677
	step [100/190], loss=78.8410
	step [101/190], loss=84.5345
	step [102/190], loss=85.2945
	step [103/190], loss=71.4381
	step [104/190], loss=66.0208
	step [105/190], loss=74.0014
	step [106/190], loss=81.3552
	step [107/190], loss=78.4309
	step [108/190], loss=77.7508
	step [109/190], loss=77.4701
	step [110/190], loss=78.2935
	step [111/190], loss=89.3937
	step [112/190], loss=80.1140
	step [113/190], loss=92.7363
	step [114/190], loss=76.0043
	step [115/190], loss=90.0198
	step [116/190], loss=72.2294
	step [117/190], loss=93.2793
	step [118/190], loss=71.3108
	step [119/190], loss=79.5880
	step [120/190], loss=96.7263
	step [121/190], loss=82.8303
	step [122/190], loss=68.1906
	step [123/190], loss=92.5420
	step [124/190], loss=86.9194
	step [125/190], loss=78.4939
	step [126/190], loss=80.5117
	step [127/190], loss=85.3637
	step [128/190], loss=73.9058
	step [129/190], loss=85.1719
	step [130/190], loss=83.8728
	step [131/190], loss=77.5098
	step [132/190], loss=80.8082
	step [133/190], loss=81.7963
	step [134/190], loss=91.6871
	step [135/190], loss=100.1556
	step [136/190], loss=90.0188
	step [137/190], loss=66.1255
	step [138/190], loss=90.5897
	step [139/190], loss=77.0865
	step [140/190], loss=66.6644
	step [141/190], loss=89.7373
	step [142/190], loss=70.2970
	step [143/190], loss=77.5451
	step [144/190], loss=78.3678
	step [145/190], loss=87.1100
	step [146/190], loss=78.4048
	step [147/190], loss=74.7184
	step [148/190], loss=72.6302
	step [149/190], loss=90.3781
	step [150/190], loss=77.1758
	step [151/190], loss=95.0875
	step [152/190], loss=66.6908
	step [153/190], loss=77.9848
	step [154/190], loss=85.7676
	step [155/190], loss=81.1608
	step [156/190], loss=68.2160
	step [157/190], loss=65.7375
	step [158/190], loss=86.8798
	step [159/190], loss=84.7185
	step [160/190], loss=81.1296
	step [161/190], loss=79.6680
	step [162/190], loss=78.4219
	step [163/190], loss=82.9823
	step [164/190], loss=81.5191
	step [165/190], loss=77.5425
	step [166/190], loss=80.8648
	step [167/190], loss=71.3865
	step [168/190], loss=80.4861
	step [169/190], loss=71.7848
	step [170/190], loss=78.5381
	step [171/190], loss=87.6427
	step [172/190], loss=78.7283
	step [173/190], loss=79.9449
	step [174/190], loss=93.6068
	step [175/190], loss=84.0006
	step [176/190], loss=72.9103
	step [177/190], loss=81.9731
	step [178/190], loss=84.9983
	step [179/190], loss=93.4562
	step [180/190], loss=75.8351
	step [181/190], loss=71.9341
	step [182/190], loss=61.4305
	step [183/190], loss=79.3710
	step [184/190], loss=71.4256
	step [185/190], loss=86.5798
	step [186/190], loss=101.6419
	step [187/190], loss=72.8161
	step [188/190], loss=93.1090
	step [189/190], loss=83.2793
	step [190/190], loss=81.6255
	Evaluating
	loss=0.0102, precision=0.4416, recall=0.8638, f1=0.5844
Training epoch 29
	step [1/190], loss=79.0726
	step [2/190], loss=69.9295
	step [3/190], loss=67.5879
	step [4/190], loss=76.7609
	step [5/190], loss=86.9511
	step [6/190], loss=82.2628
	step [7/190], loss=80.1280
	step [8/190], loss=72.8773
	step [9/190], loss=71.4499
	step [10/190], loss=78.7685
	step [11/190], loss=75.2085
	step [12/190], loss=73.5190
	step [13/190], loss=77.2069
	step [14/190], loss=83.8395
	step [15/190], loss=68.1006
	step [16/190], loss=83.7833
	step [17/190], loss=75.5014
	step [18/190], loss=81.9366
	step [19/190], loss=82.5663
	step [20/190], loss=78.5150
	step [21/190], loss=64.0031
	step [22/190], loss=69.9103
	step [23/190], loss=78.6452
	step [24/190], loss=79.1479
	step [25/190], loss=87.1213
	step [26/190], loss=72.5246
	step [27/190], loss=73.8537
	step [28/190], loss=78.9500
	step [29/190], loss=88.7156
	step [30/190], loss=73.3207
	step [31/190], loss=80.7590
	step [32/190], loss=74.9748
	step [33/190], loss=78.1505
	step [34/190], loss=81.9196
	step [35/190], loss=81.4034
	step [36/190], loss=71.8901
	step [37/190], loss=70.6096
	step [38/190], loss=79.0667
	step [39/190], loss=71.5368
	step [40/190], loss=104.5222
	step [41/190], loss=82.3363
	step [42/190], loss=68.9800
	step [43/190], loss=79.6096
	step [44/190], loss=77.4067
	step [45/190], loss=81.5736
	step [46/190], loss=78.9478
	step [47/190], loss=77.8292
	step [48/190], loss=77.3273
	step [49/190], loss=91.2690
	step [50/190], loss=94.2974
	step [51/190], loss=86.9711
	step [52/190], loss=82.8822
	step [53/190], loss=78.1521
	step [54/190], loss=85.1421
	step [55/190], loss=74.9161
	step [56/190], loss=93.1729
	step [57/190], loss=79.3016
	step [58/190], loss=89.4875
	step [59/190], loss=67.8892
	step [60/190], loss=85.6211
	step [61/190], loss=83.4899
	step [62/190], loss=88.1268
	step [63/190], loss=77.5366
	step [64/190], loss=80.6796
	step [65/190], loss=82.8539
	step [66/190], loss=78.4190
	step [67/190], loss=82.1211
	step [68/190], loss=88.4605
	step [69/190], loss=74.0872
	step [70/190], loss=76.9690
	step [71/190], loss=82.0537
	step [72/190], loss=84.5265
	step [73/190], loss=69.0435
	step [74/190], loss=81.6998
	step [75/190], loss=80.0095
	step [76/190], loss=86.3014
	step [77/190], loss=90.8230
	step [78/190], loss=92.8722
	step [79/190], loss=70.8933
	step [80/190], loss=69.5400
	step [81/190], loss=67.0220
	step [82/190], loss=74.5356
	step [83/190], loss=81.2155
	step [84/190], loss=83.0840
	step [85/190], loss=91.3541
	step [86/190], loss=76.4774
	step [87/190], loss=86.9147
	step [88/190], loss=70.4691
	step [89/190], loss=76.3887
	step [90/190], loss=83.8485
	step [91/190], loss=73.7304
	step [92/190], loss=73.9824
	step [93/190], loss=61.7958
	step [94/190], loss=82.2821
	step [95/190], loss=86.5794
	step [96/190], loss=92.5662
	step [97/190], loss=83.1280
	step [98/190], loss=71.5851
	step [99/190], loss=81.7526
	step [100/190], loss=88.3062
	step [101/190], loss=87.9349
	step [102/190], loss=91.8465
	step [103/190], loss=80.4261
	step [104/190], loss=73.0458
	step [105/190], loss=64.4052
	step [106/190], loss=90.1512
	step [107/190], loss=81.2367
	step [108/190], loss=85.5505
	step [109/190], loss=83.1207
	step [110/190], loss=81.7282
	step [111/190], loss=73.1481
	step [112/190], loss=76.6099
	step [113/190], loss=82.6145
	step [114/190], loss=79.0023
	step [115/190], loss=74.0138
	step [116/190], loss=71.4421
	step [117/190], loss=72.9334
	step [118/190], loss=80.4946
	step [119/190], loss=75.6321
	step [120/190], loss=96.7044
	step [121/190], loss=82.8633
	step [122/190], loss=81.3635
	step [123/190], loss=82.7049
	step [124/190], loss=79.0017
	step [125/190], loss=80.9359
	step [126/190], loss=88.4619
	step [127/190], loss=81.0603
	step [128/190], loss=82.5560
	step [129/190], loss=72.6079
	step [130/190], loss=91.6076
	step [131/190], loss=69.2915
	step [132/190], loss=77.0192
	step [133/190], loss=81.8219
	step [134/190], loss=61.0398
	step [135/190], loss=78.9182
	step [136/190], loss=62.6747
	step [137/190], loss=74.8490
	step [138/190], loss=61.4245
	step [139/190], loss=93.2901
	step [140/190], loss=101.2388
	step [141/190], loss=81.6405
	step [142/190], loss=80.1516
	step [143/190], loss=65.9691
	step [144/190], loss=80.4419
	step [145/190], loss=83.1234
	step [146/190], loss=79.8172
	step [147/190], loss=77.2413
	step [148/190], loss=98.0617
	step [149/190], loss=84.0174
	step [150/190], loss=85.0271
	step [151/190], loss=81.4919
	step [152/190], loss=81.8398
	step [153/190], loss=82.3441
	step [154/190], loss=78.8634
	step [155/190], loss=80.5368
	step [156/190], loss=102.0527
	step [157/190], loss=99.2316
	step [158/190], loss=71.9488
	step [159/190], loss=85.6367
	step [160/190], loss=88.7759
	step [161/190], loss=70.5157
	step [162/190], loss=87.1247
	step [163/190], loss=63.7560
	step [164/190], loss=88.1519
	step [165/190], loss=69.1010
	step [166/190], loss=88.6497
	step [167/190], loss=79.9273
	step [168/190], loss=86.8577
	step [169/190], loss=73.9887
	step [170/190], loss=83.0692
	step [171/190], loss=96.3653
	step [172/190], loss=100.2316
	step [173/190], loss=71.2243
	step [174/190], loss=80.4175
	step [175/190], loss=86.0483
	step [176/190], loss=80.6067
	step [177/190], loss=81.5204
	step [178/190], loss=86.0186
	step [179/190], loss=84.4892
	step [180/190], loss=71.8147
	step [181/190], loss=93.3783
	step [182/190], loss=88.5350
	step [183/190], loss=82.3369
	step [184/190], loss=93.3385
	step [185/190], loss=73.0276
	step [186/190], loss=87.7282
	step [187/190], loss=83.3001
	step [188/190], loss=95.9243
	step [189/190], loss=69.1017
	step [190/190], loss=74.1343
	Evaluating
	loss=0.0093, precision=0.4755, recall=0.8895, f1=0.6198
Training epoch 30
	step [1/190], loss=66.4601
	step [2/190], loss=76.8550
	step [3/190], loss=76.0276
	step [4/190], loss=83.5332
	step [5/190], loss=86.9614
	step [6/190], loss=69.2836
	step [7/190], loss=74.7360
	step [8/190], loss=82.8694
	step [9/190], loss=81.8065
	step [10/190], loss=76.4906
	step [11/190], loss=83.3614
	step [12/190], loss=62.4586
	step [13/190], loss=86.9569
	step [14/190], loss=91.6847
	step [15/190], loss=72.0871
	step [16/190], loss=86.0484
	step [17/190], loss=72.9545
	step [18/190], loss=87.3259
	step [19/190], loss=91.4046
	step [20/190], loss=97.1971
	step [21/190], loss=81.1901
	step [22/190], loss=83.1983
	step [23/190], loss=84.6738
	step [24/190], loss=87.7816
	step [25/190], loss=89.7992
	step [26/190], loss=96.0675
	step [27/190], loss=69.6913
	step [28/190], loss=88.3224
	step [29/190], loss=98.8574
	step [30/190], loss=85.5328
	step [31/190], loss=71.6829
	step [32/190], loss=81.1347
	step [33/190], loss=81.2193
	step [34/190], loss=77.6087
	step [35/190], loss=91.0896
	step [36/190], loss=92.0248
	step [37/190], loss=79.0405
	step [38/190], loss=69.0228
	step [39/190], loss=86.7988
	step [40/190], loss=75.0843
	step [41/190], loss=84.3588
	step [42/190], loss=70.8175
	step [43/190], loss=87.7810
	step [44/190], loss=77.4197
	step [45/190], loss=87.2895
	step [46/190], loss=83.8320
	step [47/190], loss=77.9932
	step [48/190], loss=78.9117
	step [49/190], loss=60.0383
	step [50/190], loss=76.1546
	step [51/190], loss=84.3351
	step [52/190], loss=70.7977
	step [53/190], loss=70.7397
	step [54/190], loss=84.8902
	step [55/190], loss=77.6514
	step [56/190], loss=78.4386
	step [57/190], loss=71.6459
	step [58/190], loss=82.5430
	step [59/190], loss=76.4591
	step [60/190], loss=94.8611
	step [61/190], loss=89.1444
	step [62/190], loss=77.2246
	step [63/190], loss=76.9579
	step [64/190], loss=79.3925
	step [65/190], loss=77.7202
	step [66/190], loss=83.5553
	step [67/190], loss=76.9053
	step [68/190], loss=79.3892
	step [69/190], loss=76.9736
	step [70/190], loss=82.7997
	step [71/190], loss=69.4200
	step [72/190], loss=67.9097
	step [73/190], loss=84.7342
	step [74/190], loss=79.5930
	step [75/190], loss=77.7894
	step [76/190], loss=82.9251
	step [77/190], loss=74.1163
	step [78/190], loss=83.7085
	step [79/190], loss=73.0775
	step [80/190], loss=66.9082
	step [81/190], loss=77.1960
	step [82/190], loss=74.2234
	step [83/190], loss=71.7853
	step [84/190], loss=70.9238
	step [85/190], loss=74.1000
	step [86/190], loss=78.8834
	step [87/190], loss=78.8438
	step [88/190], loss=80.6475
	step [89/190], loss=69.2786
	step [90/190], loss=78.7126
	step [91/190], loss=82.3731
	step [92/190], loss=77.3820
	step [93/190], loss=80.9442
	step [94/190], loss=86.8928
	step [95/190], loss=69.5814
	step [96/190], loss=78.8150
	step [97/190], loss=77.8770
	step [98/190], loss=72.1656
	step [99/190], loss=76.9711
	step [100/190], loss=89.0251
	step [101/190], loss=81.5961
	step [102/190], loss=77.5715
	step [103/190], loss=87.7664
	step [104/190], loss=91.1594
	step [105/190], loss=77.9903
	step [106/190], loss=87.1214
	step [107/190], loss=70.4885
	step [108/190], loss=86.1665
	step [109/190], loss=76.2399
	step [110/190], loss=82.2105
	step [111/190], loss=72.1621
	step [112/190], loss=75.3136
	step [113/190], loss=76.0278
	step [114/190], loss=82.7620
	step [115/190], loss=81.2082
	step [116/190], loss=95.5983
	step [117/190], loss=87.6110
	step [118/190], loss=82.2680
	step [119/190], loss=71.6275
	step [120/190], loss=74.5949
	step [121/190], loss=75.9915
	step [122/190], loss=87.4248
	step [123/190], loss=72.3844
	step [124/190], loss=88.3510
	step [125/190], loss=84.1402
	step [126/190], loss=69.4682
	step [127/190], loss=71.5665
	step [128/190], loss=80.4038
	step [129/190], loss=79.5866
	step [130/190], loss=72.8196
	step [131/190], loss=85.0569
	step [132/190], loss=61.9682
	step [133/190], loss=95.3398
	step [134/190], loss=82.2260
	step [135/190], loss=80.4850
	step [136/190], loss=94.3206
	step [137/190], loss=76.8130
	step [138/190], loss=86.0957
	step [139/190], loss=89.8716
	step [140/190], loss=81.2468
	step [141/190], loss=76.1573
	step [142/190], loss=73.4105
	step [143/190], loss=80.6620
	step [144/190], loss=72.0667
	step [145/190], loss=74.9885
	step [146/190], loss=77.6428
	step [147/190], loss=90.5646
	step [148/190], loss=82.0505
	step [149/190], loss=79.4470
	step [150/190], loss=86.9483
	step [151/190], loss=64.4382
	step [152/190], loss=76.3658
	step [153/190], loss=73.3798
	step [154/190], loss=77.8275
	step [155/190], loss=80.1120
	step [156/190], loss=80.4152
	step [157/190], loss=70.9913
	step [158/190], loss=91.3641
	step [159/190], loss=77.9496
	step [160/190], loss=85.2313
	step [161/190], loss=89.5112
	step [162/190], loss=76.2754
	step [163/190], loss=76.9873
	step [164/190], loss=70.4687
	step [165/190], loss=73.7322
	step [166/190], loss=82.5478
	step [167/190], loss=80.8567
	step [168/190], loss=71.7092
	step [169/190], loss=66.6149
	step [170/190], loss=87.0965
	step [171/190], loss=73.9969
	step [172/190], loss=99.4590
	step [173/190], loss=81.8971
	step [174/190], loss=78.5934
	step [175/190], loss=82.0657
	step [176/190], loss=93.1818
	step [177/190], loss=75.9122
	step [178/190], loss=90.0160
	step [179/190], loss=76.3789
	step [180/190], loss=80.6789
	step [181/190], loss=78.1110
	step [182/190], loss=81.4403
	step [183/190], loss=72.7771
	step [184/190], loss=94.4086
	step [185/190], loss=85.5217
	step [186/190], loss=77.2867
	step [187/190], loss=81.2323
	step [188/190], loss=79.0881
	step [189/190], loss=92.4211
	step [190/190], loss=78.7648
	Evaluating
	loss=0.0100, precision=0.4278, recall=0.8655, f1=0.5726
Training finished
best_f1: 0.6967337341345291
directing: X rim_enhanced: True test_id 2
removed wrong scan: weights_X_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_22_xwqg-B00034_2020-04-03.npy
# all image files: 12135 # all weight files in weight_dir: 9358 # image files with weight 9316
removed wrong scan: weights_X_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_22_xwqg-B00034_2020-04-03.npy
# all image files: 12135 # all weight files in weight_dir: 2525 # image files with weight 2516
train: /ibex/scratch/projects/c2052/air_tube_seg/training_samples_enhanced_one/X 9316
Using 4 GPUs
Going to train epochs [1-30]
Training epoch 1
	step [1/146], loss=856.5687
	step [2/146], loss=568.2119
	step [3/146], loss=405.0820
	step [4/146], loss=282.8658
	step [5/146], loss=250.7161
	step [6/146], loss=272.9513
	step [7/146], loss=273.7168
	step [8/146], loss=296.3767
	step [9/146], loss=304.2173
	step [10/146], loss=243.0419
	step [11/146], loss=260.0078
	step [12/146], loss=242.7072
	step [13/146], loss=259.5128
	step [14/146], loss=250.3987
	step [15/146], loss=246.1219
	step [16/146], loss=263.2070
	step [17/146], loss=248.5698
	step [18/146], loss=241.6554
	step [19/146], loss=237.3130
	step [20/146], loss=215.6066
	step [21/146], loss=234.3088
	step [22/146], loss=232.6846
	step [23/146], loss=228.3394
	step [24/146], loss=232.2197
	step [25/146], loss=224.8595
	step [26/146], loss=233.6530
	step [27/146], loss=230.4480
	step [28/146], loss=217.2770
	step [29/146], loss=222.8039
	step [30/146], loss=226.2152
	step [31/146], loss=278.4319
	step [32/146], loss=222.4636
	step [33/146], loss=207.0972
	step [34/146], loss=216.8204
	step [35/146], loss=218.2704
	step [36/146], loss=217.2602
	step [37/146], loss=237.6720
	step [38/146], loss=175.4122
	step [39/146], loss=205.3876
	step [40/146], loss=208.5771
	step [41/146], loss=240.9445
	step [42/146], loss=234.2950
	step [43/146], loss=207.1539
	step [44/146], loss=221.4643
	step [45/146], loss=218.2611
	step [46/146], loss=221.3604
	step [47/146], loss=214.3215
	step [48/146], loss=211.3768
	step [49/146], loss=212.0760
	step [50/146], loss=228.8401
	step [51/146], loss=210.8703
	step [52/146], loss=204.5150
	step [53/146], loss=196.8880
	step [54/146], loss=196.7362
	step [55/146], loss=206.2264
	step [56/146], loss=206.9943
	step [57/146], loss=200.5366
	step [58/146], loss=223.4259
	step [59/146], loss=191.6558
	step [60/146], loss=203.6967
	step [61/146], loss=211.2664
	step [62/146], loss=196.0839
	step [63/146], loss=191.0406
	step [64/146], loss=187.0947
	step [65/146], loss=200.7859
	step [66/146], loss=218.4875
	step [67/146], loss=202.0995
	step [68/146], loss=199.0036
	step [69/146], loss=187.2446
	step [70/146], loss=212.7322
	step [71/146], loss=224.6789
	step [72/146], loss=209.6055
	step [73/146], loss=233.1008
	step [74/146], loss=200.7771
	step [75/146], loss=171.8835
	step [76/146], loss=210.8143
	step [77/146], loss=218.0238
	step [78/146], loss=198.5633
	step [79/146], loss=199.5555
	step [80/146], loss=198.8040
	step [81/146], loss=177.1643
	step [82/146], loss=208.2192
	step [83/146], loss=201.3469
	step [84/146], loss=205.8554
	step [85/146], loss=198.6081
	step [86/146], loss=183.4381
	step [87/146], loss=201.0014
	step [88/146], loss=187.3929
	step [89/146], loss=175.3107
	step [90/146], loss=226.2686
	step [91/146], loss=181.9331
	step [92/146], loss=190.6239
	step [93/146], loss=203.3436
	step [94/146], loss=173.3594
	step [95/146], loss=209.3965
	step [96/146], loss=198.7313
	step [97/146], loss=197.7928
	step [98/146], loss=175.9286
	step [99/146], loss=181.3592
	step [100/146], loss=190.6357
	step [101/146], loss=222.4760
	step [102/146], loss=185.0381
	step [103/146], loss=197.4253
	step [104/146], loss=182.4456
	step [105/146], loss=181.0065
	step [106/146], loss=196.5211
	step [107/146], loss=176.5652
	step [108/146], loss=199.7066
	step [109/146], loss=185.2803
	step [110/146], loss=179.0320
	step [111/146], loss=175.0225
	step [112/146], loss=211.5334
	step [113/146], loss=198.0537
	step [114/146], loss=179.6556
	step [115/146], loss=197.5395
	step [116/146], loss=163.3352
	step [117/146], loss=201.1203
	step [118/146], loss=215.9495
	step [119/146], loss=194.2483
	step [120/146], loss=181.8669
	step [121/146], loss=169.1637
	step [122/146], loss=178.2399
	step [123/146], loss=182.0537
	step [124/146], loss=179.0222
	step [125/146], loss=185.3990
	step [126/146], loss=197.7909
	step [127/146], loss=188.8314
	step [128/146], loss=186.1283
	step [129/146], loss=178.5373
	step [130/146], loss=185.7027
	step [131/146], loss=190.9217
	step [132/146], loss=196.7366
	step [133/146], loss=213.6591
	step [134/146], loss=195.7423
	step [135/146], loss=199.1796
	step [136/146], loss=185.3524
	step [137/146], loss=158.4469
	step [138/146], loss=171.2544
	step [139/146], loss=168.5434
	step [140/146], loss=181.2034
	step [141/146], loss=184.5326
	step [142/146], loss=193.7465
	step [143/146], loss=196.3181
	step [144/146], loss=193.8192
	step [145/146], loss=199.0485
	step [146/146], loss=113.9525
	Evaluating
	loss=0.2852, precision=0.2649, recall=0.9486, f1=0.4141
saving model as: 2_saved_model.pth
Training epoch 2
	step [1/146], loss=186.3924
	step [2/146], loss=185.6532
	step [3/146], loss=176.6678
	step [4/146], loss=187.3740
	step [5/146], loss=188.4949
	step [6/146], loss=183.8263
	step [7/146], loss=179.3872
	step [8/146], loss=169.0808
	step [9/146], loss=175.3367
	step [10/146], loss=178.7360
	step [11/146], loss=184.8382
	step [12/146], loss=180.9153
	step [13/146], loss=178.0154
	step [14/146], loss=188.2610
	step [15/146], loss=178.1999
	step [16/146], loss=191.8882
	step [17/146], loss=193.3149
	step [18/146], loss=197.2186
	step [19/146], loss=170.0037
	step [20/146], loss=183.6503
	step [21/146], loss=191.6801
	step [22/146], loss=162.5809
	step [23/146], loss=159.2124
	step [24/146], loss=186.0600
	step [25/146], loss=185.7057
	step [26/146], loss=166.8307
	step [27/146], loss=182.0917
	step [28/146], loss=174.5443
	step [29/146], loss=179.7521
	step [30/146], loss=180.0547
	step [31/146], loss=172.3287
	step [32/146], loss=159.1009
	step [33/146], loss=164.4187
	step [34/146], loss=202.4489
	step [35/146], loss=200.6164
	step [36/146], loss=193.8570
	step [37/146], loss=153.1877
	step [38/146], loss=176.3456
	step [39/146], loss=177.7028
	step [40/146], loss=177.4579
	step [41/146], loss=171.2225
	step [42/146], loss=174.0876
	step [43/146], loss=199.9294
	step [44/146], loss=164.7789
	step [45/146], loss=192.1617
	step [46/146], loss=180.5284
	step [47/146], loss=168.6172
	step [48/146], loss=158.9828
	step [49/146], loss=144.0400
	step [50/146], loss=180.2221
	step [51/146], loss=173.5809
	step [52/146], loss=159.0935
	step [53/146], loss=192.2471
	step [54/146], loss=151.6023
	step [55/146], loss=194.4038
	step [56/146], loss=184.9245
	step [57/146], loss=171.6070
	step [58/146], loss=177.7040
	step [59/146], loss=143.7465
	step [60/146], loss=184.1680
	step [61/146], loss=157.1974
	step [62/146], loss=156.2256
	step [63/146], loss=183.6116
	step [64/146], loss=165.0979
	step [65/146], loss=152.5943
	step [66/146], loss=148.7710
	step [67/146], loss=178.1839
	step [68/146], loss=159.8119
	step [69/146], loss=186.0915
	step [70/146], loss=172.0048
	step [71/146], loss=148.7633
	step [72/146], loss=160.4480
	step [73/146], loss=157.3597
	step [74/146], loss=181.0322
	step [75/146], loss=168.7253
	step [76/146], loss=155.8007
	step [77/146], loss=208.9405
	step [78/146], loss=179.4503
	step [79/146], loss=160.0672
	step [80/146], loss=151.7658
	step [81/146], loss=161.7219
	step [82/146], loss=155.9610
	step [83/146], loss=161.8317
	step [84/146], loss=192.9422
	step [85/146], loss=168.3710
	step [86/146], loss=161.3398
	step [87/146], loss=175.9920
	step [88/146], loss=164.8034
	step [89/146], loss=143.2602
	step [90/146], loss=180.1615
	step [91/146], loss=175.4077
	step [92/146], loss=175.9942
	step [93/146], loss=186.6488
	step [94/146], loss=169.9455
	step [95/146], loss=158.7893
	step [96/146], loss=161.2190
	step [97/146], loss=170.4795
	step [98/146], loss=174.4654
	step [99/146], loss=184.4637
	step [100/146], loss=150.1406
	step [101/146], loss=190.4108
	step [102/146], loss=162.9598
	step [103/146], loss=181.4162
	step [104/146], loss=150.8114
	step [105/146], loss=186.7545
	step [106/146], loss=149.5098
	step [107/146], loss=153.4005
	step [108/146], loss=169.6221
	step [109/146], loss=168.7295
	step [110/146], loss=182.1391
	step [111/146], loss=156.9235
	step [112/146], loss=178.6287
	step [113/146], loss=151.5920
	step [114/146], loss=205.5965
	step [115/146], loss=168.4360
	step [116/146], loss=174.3229
	step [117/146], loss=148.3540
	step [118/146], loss=178.0339
	step [119/146], loss=183.5406
	step [120/146], loss=174.3094
	step [121/146], loss=170.4368
	step [122/146], loss=165.8116
	step [123/146], loss=168.0811
	step [124/146], loss=176.4045
	step [125/146], loss=152.8519
	step [126/146], loss=150.3117
	step [127/146], loss=169.9401
	step [128/146], loss=161.3416
	step [129/146], loss=163.0354
	step [130/146], loss=161.1972
	step [131/146], loss=154.8820
	step [132/146], loss=162.2445
	step [133/146], loss=157.0418
	step [134/146], loss=160.6454
	step [135/146], loss=127.3995
	step [136/146], loss=150.7757
	step [137/146], loss=178.0350
	step [138/146], loss=152.8387
	step [139/146], loss=166.6363
	step [140/146], loss=155.2790
	step [141/146], loss=161.4413
	step [142/146], loss=145.0424
	step [143/146], loss=157.5052
	step [144/146], loss=163.3826
	step [145/146], loss=182.1698
	step [146/146], loss=89.9917
	Evaluating
	loss=0.2047, precision=0.4313, recall=0.9632, f1=0.5958
saving model as: 2_saved_model.pth
Training epoch 3
	step [1/146], loss=175.5022
	step [2/146], loss=145.0431
	step [3/146], loss=168.5854
	step [4/146], loss=167.0554
	step [5/146], loss=151.2058
	step [6/146], loss=201.8498
	step [7/146], loss=181.1117
	step [8/146], loss=137.8027
	step [9/146], loss=150.1279
	step [10/146], loss=158.0750
	step [11/146], loss=153.2476
	step [12/146], loss=170.0391
	step [13/146], loss=174.7949
	step [14/146], loss=171.0531
	step [15/146], loss=172.1772
	step [16/146], loss=158.7501
	step [17/146], loss=161.4285
	step [18/146], loss=157.5996
	step [19/146], loss=185.0896
	step [20/146], loss=161.7489
	step [21/146], loss=146.4072
	step [22/146], loss=170.8969
	step [23/146], loss=182.2884
	step [24/146], loss=168.7951
	step [25/146], loss=161.1976
	step [26/146], loss=144.4848
	step [27/146], loss=161.9886
	step [28/146], loss=148.0601
	step [29/146], loss=163.4180
	step [30/146], loss=172.6116
	step [31/146], loss=174.4819
	step [32/146], loss=168.6155
	step [33/146], loss=159.3866
	step [34/146], loss=156.8190
	step [35/146], loss=166.7463
	step [36/146], loss=156.0264
	step [37/146], loss=168.2848
	step [38/146], loss=164.1521
	step [39/146], loss=170.6483
	step [40/146], loss=163.3630
	step [41/146], loss=167.3052
	step [42/146], loss=155.2682
	step [43/146], loss=143.0966
	step [44/146], loss=155.9544
	step [45/146], loss=169.3636
	step [46/146], loss=156.7135
	step [47/146], loss=168.0373
	step [48/146], loss=160.0752
	step [49/146], loss=145.1613
	step [50/146], loss=157.0749
	step [51/146], loss=144.0773
	step [52/146], loss=193.6144
	step [53/146], loss=139.0847
	step [54/146], loss=148.5267
	step [55/146], loss=175.0124
	step [56/146], loss=173.6382
	step [57/146], loss=139.8205
	step [58/146], loss=163.7454
	step [59/146], loss=145.1401
	step [60/146], loss=174.6958
	step [61/146], loss=148.8005
	step [62/146], loss=152.0842
	step [63/146], loss=157.6807
	step [64/146], loss=153.8549
	step [65/146], loss=162.7053
	step [66/146], loss=161.4312
	step [67/146], loss=170.7780
	step [68/146], loss=181.3332
	step [69/146], loss=161.8977
	step [70/146], loss=168.6295
	step [71/146], loss=168.5942
	step [72/146], loss=157.2420
	step [73/146], loss=138.0050
	step [74/146], loss=158.6581
	step [75/146], loss=158.2214
	step [76/146], loss=161.5373
	step [77/146], loss=149.4329
	step [78/146], loss=161.7908
	step [79/146], loss=150.1306
	step [80/146], loss=175.3388
	step [81/146], loss=141.9993
	step [82/146], loss=152.2704
	step [83/146], loss=162.9139
	step [84/146], loss=149.6537
	step [85/146], loss=148.4299
	step [86/146], loss=145.8058
	step [87/146], loss=159.8689
	step [88/146], loss=152.3882
	step [89/146], loss=166.3194
	step [90/146], loss=153.6579
	step [91/146], loss=154.1390
	step [92/146], loss=146.3416
	step [93/146], loss=150.6929
	step [94/146], loss=158.3982
	step [95/146], loss=153.9419
	step [96/146], loss=164.2329
	step [97/146], loss=145.2619
	step [98/146], loss=158.8222
	step [99/146], loss=153.0133
	step [100/146], loss=148.0497
	step [101/146], loss=149.3651
	step [102/146], loss=118.1326
	step [103/146], loss=148.2566
	step [104/146], loss=164.5485
	step [105/146], loss=148.8209
	step [106/146], loss=136.3248
	step [107/146], loss=165.2665
	step [108/146], loss=155.8932
	step [109/146], loss=162.5135
	step [110/146], loss=129.4331
	step [111/146], loss=166.8310
	step [112/146], loss=154.2985
	step [113/146], loss=164.3805
	step [114/146], loss=157.8839
	step [115/146], loss=137.5570
	step [116/146], loss=171.6372
	step [117/146], loss=158.4262
	step [118/146], loss=150.8024
	step [119/146], loss=148.2270
	step [120/146], loss=159.1045
	step [121/146], loss=129.5587
	step [122/146], loss=157.5739
	step [123/146], loss=149.7626
	step [124/146], loss=137.8004
	step [125/146], loss=134.4226
	step [126/146], loss=158.8600
	step [127/146], loss=158.0551
	step [128/146], loss=141.9903
	step [129/146], loss=154.2903
	step [130/146], loss=149.4955
	step [131/146], loss=170.0265
	step [132/146], loss=150.7199
	step [133/146], loss=158.1363
	step [134/146], loss=159.1022
	step [135/146], loss=131.3990
	step [136/146], loss=143.6178
	step [137/146], loss=130.9232
	step [138/146], loss=119.6109
	step [139/146], loss=141.7282
	step [140/146], loss=156.7254
	step [141/146], loss=152.2143
	step [142/146], loss=141.8933
	step [143/146], loss=168.7779
	step [144/146], loss=159.6555
	step [145/146], loss=134.9785
	step [146/146], loss=99.3073
	Evaluating
	loss=0.1660, precision=0.4438, recall=0.9456, f1=0.6041
saving model as: 2_saved_model.pth
Training epoch 4
	step [1/146], loss=154.7176
	step [2/146], loss=157.9626
	step [3/146], loss=163.0816
	step [4/146], loss=149.2744
	step [5/146], loss=176.7471
	step [6/146], loss=139.5631
	step [7/146], loss=150.3231
	step [8/146], loss=147.5898
	step [9/146], loss=148.3199
	step [10/146], loss=152.3654
	step [11/146], loss=148.0862
	step [12/146], loss=140.8574
	step [13/146], loss=166.3890
	step [14/146], loss=167.7917
	step [15/146], loss=166.0774
	step [16/146], loss=151.9567
	step [17/146], loss=146.7309
	step [18/146], loss=142.7333
	step [19/146], loss=138.5783
	step [20/146], loss=158.2386
	step [21/146], loss=133.7928
	step [22/146], loss=141.7543
	step [23/146], loss=152.7071
	step [24/146], loss=151.3148
	step [25/146], loss=155.3931
	step [26/146], loss=158.3603
	step [27/146], loss=151.8539
	step [28/146], loss=165.0042
	step [29/146], loss=153.7011
	step [30/146], loss=171.0982
	step [31/146], loss=152.6328
	step [32/146], loss=170.1616
	step [33/146], loss=153.7291
	step [34/146], loss=146.0716
	step [35/146], loss=133.2150
	step [36/146], loss=178.2429
	step [37/146], loss=167.0251
	step [38/146], loss=161.4465
	step [39/146], loss=123.7580
	step [40/146], loss=133.8851
	step [41/146], loss=162.0622
	step [42/146], loss=145.8396
	step [43/146], loss=152.7935
	step [44/146], loss=134.2650
	step [45/146], loss=146.9312
	step [46/146], loss=154.2462
	step [47/146], loss=121.2646
	step [48/146], loss=158.3490
	step [49/146], loss=153.8761
	step [50/146], loss=160.6136
	step [51/146], loss=159.6257
	step [52/146], loss=137.5724
	step [53/146], loss=163.4376
	step [54/146], loss=149.7588
	step [55/146], loss=160.5455
	step [56/146], loss=140.5219
	step [57/146], loss=133.1527
	step [58/146], loss=137.0414
	step [59/146], loss=163.4617
	step [60/146], loss=142.7746
	step [61/146], loss=142.3355
	step [62/146], loss=157.3849
	step [63/146], loss=160.6314
	step [64/146], loss=162.3174
	step [65/146], loss=140.8024
	step [66/146], loss=142.2333
	step [67/146], loss=129.9251
	step [68/146], loss=145.7772
	step [69/146], loss=176.8129
	step [70/146], loss=141.6198
	step [71/146], loss=171.8055
	step [72/146], loss=129.1991
	step [73/146], loss=144.1856
	step [74/146], loss=133.1501
	step [75/146], loss=142.0537
	step [76/146], loss=170.2748
	step [77/146], loss=154.2560
	step [78/146], loss=140.7146
	step [79/146], loss=149.1674
	step [80/146], loss=138.6023
	step [81/146], loss=129.3824
	step [82/146], loss=141.4057
	step [83/146], loss=140.2858
	step [84/146], loss=157.3666
	step [85/146], loss=171.8312
	step [86/146], loss=153.2889
	step [87/146], loss=159.3350
	step [88/146], loss=172.4915
	step [89/146], loss=143.7337
	step [90/146], loss=134.9596
	step [91/146], loss=155.9801
	step [92/146], loss=153.0787
	step [93/146], loss=154.4953
	step [94/146], loss=154.5533
	step [95/146], loss=153.4032
	step [96/146], loss=146.8286
	step [97/146], loss=162.0167
	step [98/146], loss=141.2994
	step [99/146], loss=138.6273
	step [100/146], loss=144.3132
	step [101/146], loss=141.9191
	step [102/146], loss=142.8806
	step [103/146], loss=136.6462
	step [104/146], loss=148.4417
	step [105/146], loss=157.2239
	step [106/146], loss=140.3265
	step [107/146], loss=126.0713
	step [108/146], loss=153.4147
	step [109/146], loss=139.7313
	step [110/146], loss=130.6556
	step [111/146], loss=126.6025
	step [112/146], loss=141.4247
	step [113/146], loss=134.9688
	step [114/146], loss=154.6735
	step [115/146], loss=142.5800
	step [116/146], loss=135.4579
	step [117/146], loss=157.0708
	step [118/146], loss=149.3667
	step [119/146], loss=140.0169
	step [120/146], loss=138.0516
	step [121/146], loss=134.0649
	step [122/146], loss=141.9029
	step [123/146], loss=159.5159
	step [124/146], loss=130.9996
	step [125/146], loss=151.2016
	step [126/146], loss=131.9662
	step [127/146], loss=137.2289
	step [128/146], loss=154.8542
	step [129/146], loss=134.2315
	step [130/146], loss=166.9007
	step [131/146], loss=143.3893
	step [132/146], loss=146.2165
	step [133/146], loss=147.2872
	step [134/146], loss=147.4028
	step [135/146], loss=140.8782
	step [136/146], loss=144.6417
	step [137/146], loss=142.7343
	step [138/146], loss=144.2613
	step [139/146], loss=175.4562
	step [140/146], loss=144.2935
	step [141/146], loss=124.1650
	step [142/146], loss=137.1170
	step [143/146], loss=139.0457
	step [144/146], loss=123.6712
	step [145/146], loss=157.8584
	step [146/146], loss=67.1943
	Evaluating
	loss=0.1336, precision=0.4968, recall=0.9363, f1=0.6492
saving model as: 2_saved_model.pth
Training epoch 5
	step [1/146], loss=131.4185
	step [2/146], loss=147.4546
	step [3/146], loss=151.9402
	step [4/146], loss=137.9315
	step [5/146], loss=132.6668
	step [6/146], loss=133.2056
	step [7/146], loss=137.9274
	step [8/146], loss=137.6454
	step [9/146], loss=143.2383
	step [10/146], loss=141.1767
	step [11/146], loss=154.8048
	step [12/146], loss=144.3095
	step [13/146], loss=140.4119
	step [14/146], loss=149.9446
	step [15/146], loss=136.7687
	step [16/146], loss=139.7780
	step [17/146], loss=156.3514
	step [18/146], loss=141.0441
	step [19/146], loss=149.4550
	step [20/146], loss=150.3532
	step [21/146], loss=142.2977
	step [22/146], loss=159.3528
	step [23/146], loss=148.8188
	step [24/146], loss=142.3178
	step [25/146], loss=140.4001
	step [26/146], loss=141.8971
	step [27/146], loss=144.1952
	step [28/146], loss=143.7356
	step [29/146], loss=145.2546
	step [30/146], loss=159.9114
	step [31/146], loss=146.7899
	step [32/146], loss=150.9136
	step [33/146], loss=157.3130
	step [34/146], loss=141.9816
	step [35/146], loss=132.8713
	step [36/146], loss=121.7339
	step [37/146], loss=156.1747
	step [38/146], loss=123.4160
	step [39/146], loss=141.6332
	step [40/146], loss=150.9371
	step [41/146], loss=167.4713
	step [42/146], loss=136.0248
	step [43/146], loss=123.1837
	step [44/146], loss=144.2084
	step [45/146], loss=118.2871
	step [46/146], loss=153.2043
	step [47/146], loss=132.0846
	step [48/146], loss=152.6137
	step [49/146], loss=148.8310
	step [50/146], loss=123.2996
	step [51/146], loss=127.6056
	step [52/146], loss=140.6160
	step [53/146], loss=152.0052
	step [54/146], loss=148.5890
	step [55/146], loss=147.6700
	step [56/146], loss=150.2691
	step [57/146], loss=140.3499
	step [58/146], loss=137.0022
	step [59/146], loss=142.4705
	step [60/146], loss=141.0616
	step [61/146], loss=136.7288
	step [62/146], loss=137.2899
	step [63/146], loss=125.1030
	step [64/146], loss=137.0354
	step [65/146], loss=134.8795
	step [66/146], loss=139.0512
	step [67/146], loss=157.3724
	step [68/146], loss=139.9376
	step [69/146], loss=133.3119
	step [70/146], loss=152.9289
	step [71/146], loss=152.2082
	step [72/146], loss=131.2793
	step [73/146], loss=145.8026
	step [74/146], loss=150.7257
	step [75/146], loss=134.7766
	step [76/146], loss=143.4349
	step [77/146], loss=136.3339
	step [78/146], loss=164.4341
	step [79/146], loss=149.4004
	step [80/146], loss=152.3213
	step [81/146], loss=128.2429
	step [82/146], loss=136.4796
	step [83/146], loss=135.1039
	step [84/146], loss=131.4287
	step [85/146], loss=158.4368
	step [86/146], loss=145.8087
	step [87/146], loss=139.4787
	step [88/146], loss=153.0875
	step [89/146], loss=134.5886
	step [90/146], loss=161.2643
	step [91/146], loss=136.9274
	step [92/146], loss=144.8199
	step [93/146], loss=125.9473
	step [94/146], loss=119.9893
	step [95/146], loss=125.5261
	step [96/146], loss=140.8899
	step [97/146], loss=146.0289
	step [98/146], loss=142.5393
	step [99/146], loss=125.2022
	step [100/146], loss=143.3196
	step [101/146], loss=138.7600
	step [102/146], loss=129.3314
	step [103/146], loss=133.8442
	step [104/146], loss=164.4073
	step [105/146], loss=169.2543
	step [106/146], loss=159.8740
	step [107/146], loss=138.1963
	step [108/146], loss=132.5255
	step [109/146], loss=143.3764
	step [110/146], loss=120.9226
	step [111/146], loss=132.8155
	step [112/146], loss=143.5916
	step [113/146], loss=130.0642
	step [114/146], loss=146.9231
	step [115/146], loss=142.2967
	step [116/146], loss=123.5810
	step [117/146], loss=165.5769
	step [118/146], loss=160.2448
	step [119/146], loss=127.2049
	step [120/146], loss=125.5019
	step [121/146], loss=120.4115
	step [122/146], loss=127.4701
	step [123/146], loss=136.0938
	step [124/146], loss=152.8279
	step [125/146], loss=136.0337
	step [126/146], loss=138.6502
	step [127/146], loss=161.9754
	step [128/146], loss=130.7235
	step [129/146], loss=118.4442
	step [130/146], loss=149.5857
	step [131/146], loss=138.3183
	step [132/146], loss=154.2386
	step [133/146], loss=141.1742
	step [134/146], loss=141.2043
	step [135/146], loss=148.9659
	step [136/146], loss=123.5495
	step [137/146], loss=147.7602
	step [138/146], loss=131.8738
	step [139/146], loss=133.6368
	step [140/146], loss=153.8799
	step [141/146], loss=134.5157
	step [142/146], loss=121.5021
	step [143/146], loss=130.6351
	step [144/146], loss=171.0185
	step [145/146], loss=148.0994
	step [146/146], loss=73.7809
	Evaluating
	loss=0.1073, precision=0.4564, recall=0.9441, f1=0.6153
Training epoch 6
	step [1/146], loss=139.3010
	step [2/146], loss=125.5036
	step [3/146], loss=135.0874
	step [4/146], loss=142.2804
	step [5/146], loss=141.9415
	step [6/146], loss=132.9668
	step [7/146], loss=139.8266
	step [8/146], loss=126.6505
	step [9/146], loss=160.5879
	step [10/146], loss=143.1431
	step [11/146], loss=140.1570
	step [12/146], loss=116.3827
	step [13/146], loss=118.1800
	step [14/146], loss=139.1281
	step [15/146], loss=135.9911
	step [16/146], loss=124.8940
	step [17/146], loss=125.7049
	step [18/146], loss=125.9141
	step [19/146], loss=130.1504
	step [20/146], loss=146.8812
	step [21/146], loss=128.2945
	step [22/146], loss=147.4888
	step [23/146], loss=138.2252
	step [24/146], loss=135.6742
	step [25/146], loss=133.6628
	step [26/146], loss=132.0304
	step [27/146], loss=152.9956
	step [28/146], loss=155.4877
	step [29/146], loss=146.5512
	step [30/146], loss=149.0503
	step [31/146], loss=132.1957
	step [32/146], loss=148.1480
	step [33/146], loss=142.3599
	step [34/146], loss=133.9877
	step [35/146], loss=136.2066
	step [36/146], loss=121.9636
	step [37/146], loss=121.1384
	step [38/146], loss=124.9256
	step [39/146], loss=158.5279
	step [40/146], loss=151.3234
	step [41/146], loss=149.1964
	step [42/146], loss=142.1198
	step [43/146], loss=127.0037
	step [44/146], loss=114.8805
	step [45/146], loss=129.9224
	step [46/146], loss=149.0176
	step [47/146], loss=125.9867
	step [48/146], loss=123.0583
	step [49/146], loss=151.2831
	step [50/146], loss=144.7268
	step [51/146], loss=138.8366
	step [52/146], loss=137.6685
	step [53/146], loss=142.4959
	step [54/146], loss=125.3856
	step [55/146], loss=141.7173
	step [56/146], loss=145.9965
	step [57/146], loss=127.3346
	step [58/146], loss=136.2502
	step [59/146], loss=140.1837
	step [60/146], loss=139.3785
	step [61/146], loss=147.1588
	step [62/146], loss=133.0719
	step [63/146], loss=145.3979
	step [64/146], loss=136.4146
	step [65/146], loss=121.2974
	step [66/146], loss=127.2327
	step [67/146], loss=139.3363
	step [68/146], loss=142.3284
	step [69/146], loss=134.2896
	step [70/146], loss=162.7084
	step [71/146], loss=109.7176
	step [72/146], loss=145.1670
	step [73/146], loss=129.0408
	step [74/146], loss=159.4661
	step [75/146], loss=124.8200
	step [76/146], loss=141.9047
	step [77/146], loss=123.2363
	step [78/146], loss=136.6642
	step [79/146], loss=126.9921
	step [80/146], loss=114.7598
	step [81/146], loss=150.3015
	step [82/146], loss=140.9026
	step [83/146], loss=137.5012
	step [84/146], loss=126.1064
	step [85/146], loss=113.4078
	step [86/146], loss=128.8287
	step [87/146], loss=144.4022
	step [88/146], loss=136.7073
	step [89/146], loss=164.0156
	step [90/146], loss=100.8590
	step [91/146], loss=135.6283
	step [92/146], loss=135.9251
	step [93/146], loss=113.7004
	step [94/146], loss=147.3352
	step [95/146], loss=161.4397
	step [96/146], loss=128.0091
	step [97/146], loss=136.9159
	step [98/146], loss=124.9978
	step [99/146], loss=127.8246
	step [100/146], loss=147.1864
	step [101/146], loss=144.0219
	step [102/146], loss=134.1539
	step [103/146], loss=164.5150
	step [104/146], loss=136.0102
	step [105/146], loss=134.6128
	step [106/146], loss=121.8488
	step [107/146], loss=132.8315
	step [108/146], loss=111.3904
	step [109/146], loss=125.6513
	step [110/146], loss=134.5512
	step [111/146], loss=124.5703
	step [112/146], loss=148.3412
	step [113/146], loss=127.5319
	step [114/146], loss=129.1537
	step [115/146], loss=148.9865
	step [116/146], loss=154.6284
	step [117/146], loss=128.0079
	step [118/146], loss=136.3088
	step [119/146], loss=146.6867
	step [120/146], loss=134.6996
	step [121/146], loss=135.1257
	step [122/146], loss=132.8298
	step [123/146], loss=157.9399
	step [124/146], loss=130.1638
	step [125/146], loss=125.1015
	step [126/146], loss=137.3607
	step [127/146], loss=163.6850
	step [128/146], loss=132.6133
	step [129/146], loss=134.4603
	step [130/146], loss=147.0543
	step [131/146], loss=141.3007
	step [132/146], loss=133.1281
	step [133/146], loss=149.6623
	step [134/146], loss=128.2431
	step [135/146], loss=141.1923
	step [136/146], loss=150.6028
	step [137/146], loss=127.4370
	step [138/146], loss=126.3874
	step [139/146], loss=142.0442
	step [140/146], loss=139.3130
	step [141/146], loss=137.4508
	step [142/146], loss=138.4901
	step [143/146], loss=122.3672
	step [144/146], loss=124.4095
	step [145/146], loss=119.1396
	step [146/146], loss=88.9297
	Evaluating
	loss=0.0897, precision=0.4687, recall=0.9446, f1=0.6265
Training epoch 7
	step [1/146], loss=119.0995
	step [2/146], loss=125.3851
	step [3/146], loss=136.8009
	step [4/146], loss=131.3888
	step [5/146], loss=154.4601
	step [6/146], loss=128.9739
	step [7/146], loss=153.4077
	step [8/146], loss=128.5172
	step [9/146], loss=129.8905
	step [10/146], loss=129.1216
	step [11/146], loss=113.7586
	step [12/146], loss=127.0988
	step [13/146], loss=130.6987
	step [14/146], loss=137.0098
	step [15/146], loss=154.2376
	step [16/146], loss=135.2508
	step [17/146], loss=156.3470
	step [18/146], loss=132.8092
	step [19/146], loss=137.1973
	step [20/146], loss=133.8922
	step [21/146], loss=133.6624
	step [22/146], loss=131.1161
	step [23/146], loss=126.6101
	step [24/146], loss=130.7629
	step [25/146], loss=140.5772
	step [26/146], loss=137.3133
	step [27/146], loss=136.0674
	step [28/146], loss=163.4248
	step [29/146], loss=148.6502
	step [30/146], loss=130.9060
	step [31/146], loss=108.8267
	step [32/146], loss=127.7875
	step [33/146], loss=132.4278
	step [34/146], loss=137.0795
	step [35/146], loss=129.4396
	step [36/146], loss=132.5211
	step [37/146], loss=115.6752
	step [38/146], loss=154.5659
	step [39/146], loss=141.8522
	step [40/146], loss=143.1029
	step [41/146], loss=128.6818
	step [42/146], loss=129.1684
	step [43/146], loss=109.7014
	step [44/146], loss=129.2919
	step [45/146], loss=108.0488
	step [46/146], loss=147.7026
	step [47/146], loss=127.3113
	step [48/146], loss=132.6460
	step [49/146], loss=123.5942
	step [50/146], loss=120.5134
	step [51/146], loss=119.4640
	step [52/146], loss=159.2789
	step [53/146], loss=164.8066
	step [54/146], loss=139.7092
	step [55/146], loss=154.3882
	step [56/146], loss=134.3334
	step [57/146], loss=155.7092
	step [58/146], loss=132.6223
	step [59/146], loss=141.9183
	step [60/146], loss=129.2127
	step [61/146], loss=133.5024
	step [62/146], loss=122.1404
	step [63/146], loss=128.6236
	step [64/146], loss=128.9996
	step [65/146], loss=118.7642
	step [66/146], loss=134.4801
	step [67/146], loss=119.8887
	step [68/146], loss=137.7758
	step [69/146], loss=133.3689
	step [70/146], loss=135.0910
	step [71/146], loss=139.8799
	step [72/146], loss=125.9841
	step [73/146], loss=137.4251
	step [74/146], loss=140.4421
	step [75/146], loss=101.0172
	step [76/146], loss=127.9019
	step [77/146], loss=168.9262
	step [78/146], loss=113.0247
	step [79/146], loss=124.3133
	step [80/146], loss=125.4298
	step [81/146], loss=133.5529
	step [82/146], loss=142.1960
	step [83/146], loss=142.4798
	step [84/146], loss=133.3976
	step [85/146], loss=140.0453
	step [86/146], loss=133.2473
	step [87/146], loss=116.6030
	step [88/146], loss=164.4914
	step [89/146], loss=159.9323
	step [90/146], loss=129.6676
	step [91/146], loss=142.6453
	step [92/146], loss=147.3160
	step [93/146], loss=106.8851
	step [94/146], loss=130.9550
	step [95/146], loss=125.6869
	step [96/146], loss=150.5361
	step [97/146], loss=141.6839
	step [98/146], loss=146.8170
	step [99/146], loss=118.3931
	step [100/146], loss=151.0805
	step [101/146], loss=136.6696
	step [102/146], loss=127.2141
	step [103/146], loss=139.4129
	step [104/146], loss=133.1277
	step [105/146], loss=146.2563
	step [106/146], loss=133.2659
	step [107/146], loss=125.5497
	step [108/146], loss=125.0930
	step [109/146], loss=133.7002
	step [110/146], loss=133.6848
	step [111/146], loss=128.7727
	step [112/146], loss=102.4077
	step [113/146], loss=127.5884
	step [114/146], loss=130.6460
	step [115/146], loss=117.0112
	step [116/146], loss=144.5175
	step [117/146], loss=131.6964
	step [118/146], loss=108.8997
	step [119/146], loss=126.1587
	step [120/146], loss=128.8676
	step [121/146], loss=120.7822
	step [122/146], loss=115.6555
	step [123/146], loss=137.6674
	step [124/146], loss=137.1107
	step [125/146], loss=132.3133
	step [126/146], loss=131.2326
	step [127/146], loss=121.3113
	step [128/146], loss=119.3631
	step [129/146], loss=147.5746
	step [130/146], loss=127.4158
	step [131/146], loss=148.5842
	step [132/146], loss=108.3620
	step [133/146], loss=114.2821
	step [134/146], loss=114.0616
	step [135/146], loss=146.3457
	step [136/146], loss=127.4002
	step [137/146], loss=117.9003
	step [138/146], loss=126.6975
	step [139/146], loss=127.2705
	step [140/146], loss=124.0800
	step [141/146], loss=126.9711
	step [142/146], loss=139.0621
	step [143/146], loss=149.0988
	step [144/146], loss=133.5829
	step [145/146], loss=120.4716
	step [146/146], loss=62.8100
	Evaluating
	loss=0.0799, precision=0.4721, recall=0.9354, f1=0.6275
Training epoch 8
	step [1/146], loss=121.2813
	step [2/146], loss=137.0578
	step [3/146], loss=161.3196
	step [4/146], loss=123.6930
	step [5/146], loss=119.3563
	step [6/146], loss=125.4427
	step [7/146], loss=129.2719
	step [8/146], loss=125.6169
	step [9/146], loss=143.9152
	step [10/146], loss=146.9306
	step [11/146], loss=125.6765
	step [12/146], loss=147.1763
	step [13/146], loss=136.9464
	step [14/146], loss=125.4500
	step [15/146], loss=121.9665
	step [16/146], loss=142.8131
	step [17/146], loss=122.2975
	step [18/146], loss=142.9356
	step [19/146], loss=131.7686
	step [20/146], loss=129.9424
	step [21/146], loss=116.0483
	step [22/146], loss=127.5971
	step [23/146], loss=126.9363
	step [24/146], loss=131.3227
	step [25/146], loss=129.8910
	step [26/146], loss=151.1838
	step [27/146], loss=118.9542
	step [28/146], loss=116.4408
	step [29/146], loss=124.9097
	step [30/146], loss=126.9063
	step [31/146], loss=131.5288
	step [32/146], loss=132.2342
	step [33/146], loss=146.1476
	step [34/146], loss=126.3994
	step [35/146], loss=138.8121
	step [36/146], loss=120.5456
	step [37/146], loss=132.7364
	step [38/146], loss=138.2507
	step [39/146], loss=151.0638
	step [40/146], loss=110.0205
	step [41/146], loss=138.5850
	step [42/146], loss=108.3054
	step [43/146], loss=111.5595
	step [44/146], loss=138.3242
	step [45/146], loss=123.7754
	step [46/146], loss=138.1809
	step [47/146], loss=139.5486
	step [48/146], loss=119.3522
	step [49/146], loss=110.4246
	step [50/146], loss=124.6563
	step [51/146], loss=138.2700
	step [52/146], loss=160.0137
	step [53/146], loss=110.0381
	step [54/146], loss=129.9219
	step [55/146], loss=154.1750
	step [56/146], loss=142.8261
	step [57/146], loss=119.5785
	step [58/146], loss=136.2156
	step [59/146], loss=136.7754
	step [60/146], loss=132.0153
	step [61/146], loss=127.7530
	step [62/146], loss=137.0194
	step [63/146], loss=116.7170
	step [64/146], loss=129.9545
	step [65/146], loss=129.4457
	step [66/146], loss=124.1664
	step [67/146], loss=110.4821
	step [68/146], loss=123.8144
	step [69/146], loss=128.4155
	step [70/146], loss=111.7761
	step [71/146], loss=136.8179
	step [72/146], loss=129.4619
	step [73/146], loss=123.2154
	step [74/146], loss=109.0194
	step [75/146], loss=125.0948
	step [76/146], loss=122.9805
	step [77/146], loss=125.1401
	step [78/146], loss=131.5470
	step [79/146], loss=141.2477
	step [80/146], loss=138.1984
	step [81/146], loss=138.1733
	step [82/146], loss=128.3077
	step [83/146], loss=138.2757
	step [84/146], loss=131.0689
	step [85/146], loss=134.8707
	step [86/146], loss=133.2772
	step [87/146], loss=122.1193
	step [88/146], loss=130.1839
	step [89/146], loss=122.1212
	step [90/146], loss=112.4623
	step [91/146], loss=142.1014
	step [92/146], loss=121.8700
	step [93/146], loss=120.2541
	step [94/146], loss=116.6800
	step [95/146], loss=120.6258
	step [96/146], loss=137.3590
	step [97/146], loss=132.4347
	step [98/146], loss=125.5083
	step [99/146], loss=133.9587
	step [100/146], loss=141.2388
	step [101/146], loss=142.0191
	step [102/146], loss=143.0896
	step [103/146], loss=128.7449
	step [104/146], loss=126.1318
	step [105/146], loss=120.0336
	step [106/146], loss=127.6654
	step [107/146], loss=115.7767
	step [108/146], loss=108.1880
	step [109/146], loss=129.2350
	step [110/146], loss=121.3137
	step [111/146], loss=131.0088
	step [112/146], loss=128.5611
	step [113/146], loss=110.4995
	step [114/146], loss=147.6581
	step [115/146], loss=127.9175
	step [116/146], loss=114.4286
	step [117/146], loss=129.8488
	step [118/146], loss=119.4866
	step [119/146], loss=138.9756
	step [120/146], loss=115.4267
	step [121/146], loss=139.8297
	step [122/146], loss=111.5050
	step [123/146], loss=136.8463
	step [124/146], loss=114.6881
	step [125/146], loss=140.1763
	step [126/146], loss=147.3460
	step [127/146], loss=131.0744
	step [128/146], loss=143.0369
	step [129/146], loss=123.5817
	step [130/146], loss=133.8085
	step [131/146], loss=135.8817
	step [132/146], loss=123.4938
	step [133/146], loss=119.9020
	step [134/146], loss=129.2732
	step [135/146], loss=124.8512
	step [136/146], loss=128.3060
	step [137/146], loss=132.8877
	step [138/146], loss=119.3252
	step [139/146], loss=122.3449
	step [140/146], loss=126.8314
	step [141/146], loss=118.0818
	step [142/146], loss=135.8472
	step [143/146], loss=129.5844
	step [144/146], loss=105.2041
	step [145/146], loss=143.2014
	step [146/146], loss=71.7105
	Evaluating
	loss=0.0725, precision=0.4256, recall=0.9249, f1=0.5830
Training epoch 9
	step [1/146], loss=117.1251
	step [2/146], loss=131.6834
	step [3/146], loss=110.9663
	step [4/146], loss=123.7650
	step [5/146], loss=107.8323
	step [6/146], loss=140.7419
	step [7/146], loss=126.7764
	step [8/146], loss=115.8212
	step [9/146], loss=126.9071
	step [10/146], loss=129.2691
	step [11/146], loss=129.8840
	step [12/146], loss=152.7673
	step [13/146], loss=135.3064
	step [14/146], loss=129.7053
	step [15/146], loss=127.0525
	step [16/146], loss=144.3732
	step [17/146], loss=119.9113
	step [18/146], loss=116.9134
	step [19/146], loss=145.6861
	step [20/146], loss=137.0430
	step [21/146], loss=138.3817
	step [22/146], loss=126.8495
	step [23/146], loss=128.4120
	step [24/146], loss=109.3183
	step [25/146], loss=131.6198
	step [26/146], loss=113.4085
	step [27/146], loss=136.2061
	step [28/146], loss=111.3174
	step [29/146], loss=124.5338
	step [30/146], loss=106.8949
	step [31/146], loss=130.0797
	step [32/146], loss=139.6606
	step [33/146], loss=118.4948
	step [34/146], loss=142.4548
	step [35/146], loss=109.8163
	step [36/146], loss=123.8609
	step [37/146], loss=124.5488
	step [38/146], loss=116.9728
	step [39/146], loss=128.6355
	step [40/146], loss=147.3064
	step [41/146], loss=112.2962
	step [42/146], loss=133.8645
	step [43/146], loss=130.0321
	step [44/146], loss=137.7871
	step [45/146], loss=125.3033
	step [46/146], loss=146.4885
	step [47/146], loss=109.9023
	step [48/146], loss=154.8781
	step [49/146], loss=109.3657
	step [50/146], loss=119.5821
	step [51/146], loss=134.1210
	step [52/146], loss=103.3508
	step [53/146], loss=128.9756
	step [54/146], loss=130.3544
	step [55/146], loss=116.6342
	step [56/146], loss=128.9287
	step [57/146], loss=124.0271
	step [58/146], loss=111.0375
	step [59/146], loss=113.2758
	step [60/146], loss=119.7001
	step [61/146], loss=114.8079
	step [62/146], loss=116.7695
	step [63/146], loss=114.5323
	step [64/146], loss=131.1409
	step [65/146], loss=127.6051
	step [66/146], loss=132.2391
	step [67/146], loss=140.2468
	step [68/146], loss=124.8898
	step [69/146], loss=143.8009
	step [70/146], loss=129.2131
	step [71/146], loss=113.8854
	step [72/146], loss=137.3669
	step [73/146], loss=144.0477
	step [74/146], loss=131.9237
	step [75/146], loss=137.5406
	step [76/146], loss=108.3017
	step [77/146], loss=147.7426
	step [78/146], loss=132.8148
	step [79/146], loss=125.4170
	step [80/146], loss=128.9966
	step [81/146], loss=147.5555
	step [82/146], loss=134.2206
	step [83/146], loss=125.9915
	step [84/146], loss=113.9102
	step [85/146], loss=108.3881
	step [86/146], loss=122.6209
	step [87/146], loss=134.5455
	step [88/146], loss=121.1841
	step [89/146], loss=128.8017
	step [90/146], loss=127.6022
	step [91/146], loss=121.5304
	step [92/146], loss=113.9628
	step [93/146], loss=134.1931
	step [94/146], loss=135.8810
	step [95/146], loss=113.5486
	step [96/146], loss=133.3856
	step [97/146], loss=114.9460
	step [98/146], loss=135.0939
	step [99/146], loss=132.4604
	step [100/146], loss=125.0632
	step [101/146], loss=121.0049
	step [102/146], loss=137.1530
	step [103/146], loss=120.2857
	step [104/146], loss=119.6121
	step [105/146], loss=137.0845
	step [106/146], loss=109.1785
	step [107/146], loss=128.6434
	step [108/146], loss=112.0637
	step [109/146], loss=147.0478
	step [110/146], loss=97.3325
	step [111/146], loss=114.3005
	step [112/146], loss=103.9496
	step [113/146], loss=121.5902
	step [114/146], loss=141.7709
	step [115/146], loss=115.1069
	step [116/146], loss=142.4927
	step [117/146], loss=122.5544
	step [118/146], loss=126.0516
	step [119/146], loss=105.1856
	step [120/146], loss=135.4232
	step [121/146], loss=119.8453
	step [122/146], loss=99.9173
	step [123/146], loss=153.3326
	step [124/146], loss=133.0103
	step [125/146], loss=118.0458
	step [126/146], loss=114.0087
	step [127/146], loss=134.4895
	step [128/146], loss=99.9375
	step [129/146], loss=115.8919
	step [130/146], loss=122.8169
	step [131/146], loss=124.4719
	step [132/146], loss=116.4375
	step [133/146], loss=132.0312
	step [134/146], loss=112.2632
	step [135/146], loss=129.4850
	step [136/146], loss=122.5640
	step [137/146], loss=131.8528
	step [138/146], loss=146.0069
	step [139/146], loss=141.2642
	step [140/146], loss=145.2609
	step [141/146], loss=134.5350
	step [142/146], loss=126.9771
	step [143/146], loss=144.0190
	step [144/146], loss=123.2740
	step [145/146], loss=128.0562
	step [146/146], loss=65.9297
	Evaluating
	loss=0.0567, precision=0.4815, recall=0.9187, f1=0.6318
Training epoch 10
	step [1/146], loss=117.6700
	step [2/146], loss=127.9321
	step [3/146], loss=121.0800
	step [4/146], loss=120.3973
	step [5/146], loss=111.4356
	step [6/146], loss=111.1921
	step [7/146], loss=149.0428
	step [8/146], loss=130.0446
	step [9/146], loss=113.2282
	step [10/146], loss=131.5020
	step [11/146], loss=134.5789
	step [12/146], loss=182.9966
	step [13/146], loss=109.3272
	step [14/146], loss=122.7593
	step [15/146], loss=124.6754
	step [16/146], loss=134.1602
	step [17/146], loss=141.4285
	step [18/146], loss=132.3946
	step [19/146], loss=127.0164
	step [20/146], loss=118.6548
	step [21/146], loss=132.5651
	step [22/146], loss=123.6835
	step [23/146], loss=122.2288
	step [24/146], loss=108.1756
	step [25/146], loss=135.8986
	step [26/146], loss=110.5483
	step [27/146], loss=147.1429
	step [28/146], loss=118.6603
	step [29/146], loss=123.6435
	step [30/146], loss=118.7020
	step [31/146], loss=115.2061
	step [32/146], loss=129.5597
	step [33/146], loss=129.5986
	step [34/146], loss=122.9195
	step [35/146], loss=131.9507
	step [36/146], loss=107.3665
	step [37/146], loss=109.5652
	step [38/146], loss=125.9515
	step [39/146], loss=122.6614
	step [40/146], loss=117.2197
	step [41/146], loss=122.0562
	step [42/146], loss=116.0821
	step [43/146], loss=113.7696
	step [44/146], loss=124.5534
	step [45/146], loss=125.6610
	step [46/146], loss=131.4048
	step [47/146], loss=129.0565
	step [48/146], loss=117.4974
	step [49/146], loss=130.7516
	step [50/146], loss=106.9761
	step [51/146], loss=120.3822
	step [52/146], loss=126.8774
	step [53/146], loss=128.9513
	step [54/146], loss=122.6228
	step [55/146], loss=144.7852
	step [56/146], loss=138.7794
	step [57/146], loss=137.9302
	step [58/146], loss=132.7808
	step [59/146], loss=111.4476
	step [60/146], loss=137.7111
	step [61/146], loss=105.1396
	step [62/146], loss=130.8099
	step [63/146], loss=147.7876
	step [64/146], loss=117.5962
	step [65/146], loss=137.3277
	step [66/146], loss=123.5656
	step [67/146], loss=135.9279
	step [68/146], loss=101.6305
	step [69/146], loss=127.9993
	step [70/146], loss=123.5544
	step [71/146], loss=96.6551
	step [72/146], loss=115.3273
	step [73/146], loss=128.7841
	step [74/146], loss=120.8252
	step [75/146], loss=122.4963
	step [76/146], loss=125.8675
	step [77/146], loss=141.0952
	step [78/146], loss=123.6324
	step [79/146], loss=137.3566
	step [80/146], loss=150.0003
	step [81/146], loss=104.0524
	step [82/146], loss=99.8302
	step [83/146], loss=145.6667
	step [84/146], loss=131.8891
	step [85/146], loss=116.1433
	step [86/146], loss=145.7094
	step [87/146], loss=126.3780
	step [88/146], loss=107.4311
	step [89/146], loss=117.5874
	step [90/146], loss=116.9506
	step [91/146], loss=116.6499
	step [92/146], loss=127.1805
	step [93/146], loss=113.0577
	step [94/146], loss=118.6219
	step [95/146], loss=114.4105
	step [96/146], loss=130.3553
	step [97/146], loss=130.3832
	step [98/146], loss=123.0353
	step [99/146], loss=133.9583
	step [100/146], loss=123.5143
	step [101/146], loss=106.5322
	step [102/146], loss=113.0119
	step [103/146], loss=136.3826
	step [104/146], loss=95.1261
	step [105/146], loss=112.7235
	step [106/146], loss=126.6392
	step [107/146], loss=139.9214
	step [108/146], loss=113.1860
	step [109/146], loss=116.4167
	step [110/146], loss=128.7155
	step [111/146], loss=132.6549
	step [112/146], loss=125.5248
	step [113/146], loss=98.0850
	step [114/146], loss=85.2879
	step [115/146], loss=115.1198
	step [116/146], loss=132.0188
	step [117/146], loss=122.4803
	step [118/146], loss=133.6291
	step [119/146], loss=128.6072
	step [120/146], loss=106.9434
	step [121/146], loss=113.0175
	step [122/146], loss=115.4283
	step [123/146], loss=113.0322
	step [124/146], loss=138.5771
	step [125/146], loss=143.7809
	step [126/146], loss=125.8558
	step [127/146], loss=120.3375
	step [128/146], loss=108.9774
	step [129/146], loss=146.5517
	step [130/146], loss=105.1531
	step [131/146], loss=113.1214
	step [132/146], loss=133.3585
	step [133/146], loss=135.4506
	step [134/146], loss=121.9116
	step [135/146], loss=118.2893
	step [136/146], loss=114.8907
	step [137/146], loss=128.9790
	step [138/146], loss=140.9448
	step [139/146], loss=134.9352
	step [140/146], loss=119.9910
	step [141/146], loss=117.0522
	step [142/146], loss=129.4944
	step [143/146], loss=118.3016
	step [144/146], loss=121.4442
	step [145/146], loss=135.7744
	step [146/146], loss=80.3840
	Evaluating
	loss=0.0568, precision=0.3790, recall=0.9275, f1=0.5381
Training epoch 11
	step [1/146], loss=101.3386
	step [2/146], loss=126.1114
	step [3/146], loss=119.4337
	step [4/146], loss=130.3029
	step [5/146], loss=124.0344
	step [6/146], loss=109.6848
	step [7/146], loss=122.5221
	step [8/146], loss=117.5175
	step [9/146], loss=133.3452
	step [10/146], loss=135.5793
	step [11/146], loss=122.9758
	step [12/146], loss=100.5737
	step [13/146], loss=134.4567
	step [14/146], loss=133.8120
	step [15/146], loss=113.8674
	step [16/146], loss=135.7439
	step [17/146], loss=117.5358
	step [18/146], loss=116.5466
	step [19/146], loss=117.2171
	step [20/146], loss=110.3964
	step [21/146], loss=111.1535
	step [22/146], loss=89.4387
	step [23/146], loss=125.0692
	step [24/146], loss=130.7145
	step [25/146], loss=146.3794
	step [26/146], loss=107.7545
	step [27/146], loss=133.5804
	step [28/146], loss=115.5746
	step [29/146], loss=135.9273
	step [30/146], loss=112.2383
	step [31/146], loss=126.8641
	step [32/146], loss=146.0598
	step [33/146], loss=123.3796
	step [34/146], loss=112.7401
	step [35/146], loss=126.6436
	step [36/146], loss=110.1775
	step [37/146], loss=103.0990
	step [38/146], loss=128.5487
	step [39/146], loss=114.7167
	step [40/146], loss=138.6387
	step [41/146], loss=117.3839
	step [42/146], loss=113.4866
	step [43/146], loss=132.1248
	step [44/146], loss=116.9695
	step [45/146], loss=122.3514
	step [46/146], loss=112.3735
	step [47/146], loss=107.4433
	step [48/146], loss=137.0357
	step [49/146], loss=123.9014
	step [50/146], loss=104.3864
	step [51/146], loss=114.8297
	step [52/146], loss=117.3610
	step [53/146], loss=125.2925
	step [54/146], loss=123.3781
	step [55/146], loss=110.7709
	step [56/146], loss=123.2820
	step [57/146], loss=112.5705
	step [58/146], loss=127.7460
	step [59/146], loss=101.6708
	step [60/146], loss=131.2970
	step [61/146], loss=118.8757
	step [62/146], loss=149.5412
	step [63/146], loss=131.0162
	step [64/146], loss=117.1519
	step [65/146], loss=116.8690
	step [66/146], loss=116.5596
	step [67/146], loss=142.0921
	step [68/146], loss=116.3643
	step [69/146], loss=116.8101
	step [70/146], loss=111.7973
	step [71/146], loss=94.7229
	step [72/146], loss=118.3604
	step [73/146], loss=103.1569
	step [74/146], loss=136.3120
	step [75/146], loss=134.8683
	step [76/146], loss=110.2550
	step [77/146], loss=130.0245
	step [78/146], loss=143.8903
	step [79/146], loss=122.1112
	step [80/146], loss=109.5739
	step [81/146], loss=130.4543
	step [82/146], loss=121.7630
	step [83/146], loss=131.5092
	step [84/146], loss=142.5770
	step [85/146], loss=115.0065
	step [86/146], loss=118.1377
	step [87/146], loss=106.7785
	step [88/146], loss=147.8164
	step [89/146], loss=107.4127
	step [90/146], loss=134.1259
	step [91/146], loss=102.8111
	step [92/146], loss=116.8769
	step [93/146], loss=124.6884
	step [94/146], loss=103.7096
	step [95/146], loss=109.1364
	step [96/146], loss=109.4636
	step [97/146], loss=112.7346
	step [98/146], loss=121.4062
	step [99/146], loss=134.7620
	step [100/146], loss=131.3809
	step [101/146], loss=148.4656
	step [102/146], loss=127.8907
	step [103/146], loss=139.0979
	step [104/146], loss=101.6267
	step [105/146], loss=125.8825
	step [106/146], loss=138.2731
	step [107/146], loss=122.0960
	step [108/146], loss=122.1947
	step [109/146], loss=149.3694
	step [110/146], loss=106.1730
	step [111/146], loss=122.8116
	step [112/146], loss=122.3267
	step [113/146], loss=128.4285
	step [114/146], loss=136.3377
	step [115/146], loss=92.9678
	step [116/146], loss=97.5172
	step [117/146], loss=130.1295
	step [118/146], loss=106.8510
	step [119/146], loss=132.5205
	step [120/146], loss=122.5837
	step [121/146], loss=138.1378
	step [122/146], loss=110.9188
	step [123/146], loss=124.3537
	step [124/146], loss=130.8612
	step [125/146], loss=140.7583
	step [126/146], loss=133.5655
	step [127/146], loss=126.7827
	step [128/146], loss=120.0245
	step [129/146], loss=110.8658
	step [130/146], loss=83.2365
	step [131/146], loss=122.1943
	step [132/146], loss=132.4241
	step [133/146], loss=113.7930
	step [134/146], loss=116.6938
	step [135/146], loss=122.8581
	step [136/146], loss=130.7380
	step [137/146], loss=119.4886
	step [138/146], loss=130.7554
	step [139/146], loss=138.5460
	step [140/146], loss=125.3902
	step [141/146], loss=117.6757
	step [142/146], loss=144.7859
	step [143/146], loss=103.5049
	step [144/146], loss=102.9670
	step [145/146], loss=128.7853
	step [146/146], loss=59.8551
	Evaluating
	loss=0.0479, precision=0.3723, recall=0.9500, f1=0.5350
Training epoch 12
	step [1/146], loss=128.0080
	step [2/146], loss=116.0066
	step [3/146], loss=137.3603
	step [4/146], loss=133.3222
	step [5/146], loss=108.7228
	step [6/146], loss=116.5577
	step [7/146], loss=122.7187
	step [8/146], loss=130.8045
	step [9/146], loss=124.1948
	step [10/146], loss=124.4169
	step [11/146], loss=105.5051
	step [12/146], loss=111.5550
	step [13/146], loss=118.0054
	step [14/146], loss=103.4400
	step [15/146], loss=124.2318
	step [16/146], loss=122.7415
	step [17/146], loss=126.0712
	step [18/146], loss=114.1559
	step [19/146], loss=135.0778
	step [20/146], loss=124.6342
	step [21/146], loss=115.3888
	step [22/146], loss=128.5852
	step [23/146], loss=126.0144
	step [24/146], loss=134.1113
	step [25/146], loss=113.7144
	step [26/146], loss=125.7146
	step [27/146], loss=124.0427
	step [28/146], loss=132.5563
	step [29/146], loss=113.2565
	step [30/146], loss=128.3264
	step [31/146], loss=127.7270
	step [32/146], loss=138.5098
	step [33/146], loss=119.4187
	step [34/146], loss=107.1444
	step [35/146], loss=118.4675
	step [36/146], loss=112.3534
	step [37/146], loss=124.0749
	step [38/146], loss=131.4746
	step [39/146], loss=133.4039
	step [40/146], loss=119.5471
	step [41/146], loss=116.7049
	step [42/146], loss=118.4424
	step [43/146], loss=117.8072
	step [44/146], loss=144.3849
	step [45/146], loss=138.3048
	step [46/146], loss=131.0128
	step [47/146], loss=103.0036
	step [48/146], loss=128.2197
	step [49/146], loss=118.8772
	step [50/146], loss=113.0984
	step [51/146], loss=97.5874
	step [52/146], loss=116.1483
	step [53/146], loss=96.4509
	step [54/146], loss=130.6587
	step [55/146], loss=102.9328
	step [56/146], loss=128.9889
	step [57/146], loss=126.6542
	step [58/146], loss=124.1166
	step [59/146], loss=129.3893
	step [60/146], loss=120.1060
	step [61/146], loss=129.0553
	step [62/146], loss=126.9517
	step [63/146], loss=124.2471
	step [64/146], loss=119.6156
	step [65/146], loss=135.7918
	step [66/146], loss=129.2413
	step [67/146], loss=106.6500
	step [68/146], loss=118.6987
	step [69/146], loss=116.0450
	step [70/146], loss=123.3171
	step [71/146], loss=130.3344
	step [72/146], loss=127.9805
	step [73/146], loss=108.8336
	step [74/146], loss=104.0417
	step [75/146], loss=123.6026
	step [76/146], loss=117.9812
	step [77/146], loss=97.1130
	step [78/146], loss=132.1565
	step [79/146], loss=123.9871
	step [80/146], loss=111.7527
	step [81/146], loss=122.6078
	step [82/146], loss=104.1549
	step [83/146], loss=115.1292
	step [84/146], loss=116.9797
	step [85/146], loss=119.0751
	step [86/146], loss=108.6337
	step [87/146], loss=109.1288
	step [88/146], loss=123.8454
	step [89/146], loss=106.7106
	step [90/146], loss=112.2566
	step [91/146], loss=138.9481
	step [92/146], loss=140.5861
	step [93/146], loss=99.0648
	step [94/146], loss=108.1278
	step [95/146], loss=128.7805
	step [96/146], loss=115.2160
	step [97/146], loss=112.2415
	step [98/146], loss=120.6101
	step [99/146], loss=134.5384
	step [100/146], loss=117.1148
	step [101/146], loss=116.2031
	step [102/146], loss=134.0758
	step [103/146], loss=116.9323
	step [104/146], loss=122.2930
	step [105/146], loss=106.2912
	step [106/146], loss=128.9391
	step [107/146], loss=117.9464
	step [108/146], loss=124.3800
	step [109/146], loss=109.3564
	step [110/146], loss=112.7431
	step [111/146], loss=125.8451
	step [112/146], loss=137.5627
	step [113/146], loss=114.9785
	step [114/146], loss=122.8710
	step [115/146], loss=117.8281
	step [116/146], loss=116.9658
	step [117/146], loss=122.1877
	step [118/146], loss=140.1717
	step [119/146], loss=101.0963
	step [120/146], loss=117.8174
	step [121/146], loss=125.6312
	step [122/146], loss=121.8400
	step [123/146], loss=97.8245
	step [124/146], loss=127.4594
	step [125/146], loss=115.6428
	step [126/146], loss=98.7670
	step [127/146], loss=110.7360
	step [128/146], loss=98.6429
	step [129/146], loss=129.6586
	step [130/146], loss=128.6404
	step [131/146], loss=109.2714
	step [132/146], loss=100.3552
	step [133/146], loss=112.9198
	step [134/146], loss=130.1969
	step [135/146], loss=102.6283
	step [136/146], loss=119.2765
	step [137/146], loss=124.6134
	step [138/146], loss=139.9223
	step [139/146], loss=131.2873
	step [140/146], loss=137.3686
	step [141/146], loss=109.2485
	step [142/146], loss=95.7491
	step [143/146], loss=128.6230
	step [144/146], loss=105.4557
	step [145/146], loss=118.1977
	step [146/146], loss=75.8169
	Evaluating
	loss=0.0413, precision=0.4321, recall=0.9287, f1=0.5898
Training epoch 13
	step [1/146], loss=113.2651
	step [2/146], loss=109.4372
	step [3/146], loss=127.5259
	step [4/146], loss=122.8320
	step [5/146], loss=125.7342
	step [6/146], loss=123.5746
	step [7/146], loss=107.7038
	step [8/146], loss=112.1528
	step [9/146], loss=103.8695
	step [10/146], loss=143.1267
	step [11/146], loss=120.0869
	step [12/146], loss=127.7378
	step [13/146], loss=128.8907
	step [14/146], loss=147.4603
	step [15/146], loss=124.0067
	step [16/146], loss=136.9657
	step [17/146], loss=123.3348
	step [18/146], loss=119.0595
	step [19/146], loss=123.4985
	step [20/146], loss=105.8567
	step [21/146], loss=107.5031
	step [22/146], loss=130.8250
	step [23/146], loss=104.2782
	step [24/146], loss=120.1050
	step [25/146], loss=123.1568
	step [26/146], loss=101.5988
	step [27/146], loss=109.9460
	step [28/146], loss=126.1382
	step [29/146], loss=109.7914
	step [30/146], loss=116.7853
	step [31/146], loss=114.4310
	step [32/146], loss=116.1514
	step [33/146], loss=128.9371
	step [34/146], loss=124.3569
	step [35/146], loss=143.7710
	step [36/146], loss=138.3076
	step [37/146], loss=128.2256
	step [38/146], loss=108.9012
	step [39/146], loss=111.4737
	step [40/146], loss=97.2370
	step [41/146], loss=129.2510
	step [42/146], loss=105.0689
	step [43/146], loss=128.5511
	step [44/146], loss=124.6521
	step [45/146], loss=97.4456
	step [46/146], loss=130.6702
	step [47/146], loss=98.3142
	step [48/146], loss=113.8199
	step [49/146], loss=115.5699
	step [50/146], loss=104.3664
	step [51/146], loss=115.1176
	step [52/146], loss=94.3869
	step [53/146], loss=86.0218
	step [54/146], loss=128.7262
	step [55/146], loss=126.7090
	step [56/146], loss=111.6828
	step [57/146], loss=97.2014
	step [58/146], loss=108.4095
	step [59/146], loss=112.6678
	step [60/146], loss=116.1298
	step [61/146], loss=118.0592
	step [62/146], loss=120.8775
	step [63/146], loss=118.8049
	step [64/146], loss=119.3344
	step [65/146], loss=123.0762
	step [66/146], loss=124.7507
	step [67/146], loss=135.7745
	step [68/146], loss=91.4624
	step [69/146], loss=115.1279
	step [70/146], loss=132.8418
	step [71/146], loss=98.0000
	step [72/146], loss=134.2243
	step [73/146], loss=129.7267
	step [74/146], loss=125.1525
	step [75/146], loss=129.8536
	step [76/146], loss=126.9261
	step [77/146], loss=120.9312
	step [78/146], loss=124.2581
	step [79/146], loss=114.6758
	step [80/146], loss=107.1868
	step [81/146], loss=131.8076
	step [82/146], loss=118.5184
	step [83/146], loss=125.3789
	step [84/146], loss=100.2542
	step [85/146], loss=125.2738
	step [86/146], loss=129.8845
	step [87/146], loss=125.1644
	step [88/146], loss=101.0293
	step [89/146], loss=102.8796
	step [90/146], loss=116.1324
	step [91/146], loss=144.4686
	step [92/146], loss=128.2128
	step [93/146], loss=130.1016
	step [94/146], loss=123.1961
	step [95/146], loss=118.9603
	step [96/146], loss=107.5510
	step [97/146], loss=128.1645
	step [98/146], loss=107.7093
	step [99/146], loss=125.3533
	step [100/146], loss=154.8703
	step [101/146], loss=107.9790
	step [102/146], loss=107.0253
	step [103/146], loss=127.8514
	step [104/146], loss=113.7481
	step [105/146], loss=130.9971
	step [106/146], loss=112.0807
	step [107/146], loss=112.9891
	step [108/146], loss=101.3445
	step [109/146], loss=127.7678
	step [110/146], loss=139.8314
	step [111/146], loss=102.6366
	step [112/146], loss=106.1587
	step [113/146], loss=111.4727
	step [114/146], loss=96.8624
	step [115/146], loss=102.7395
	step [116/146], loss=122.9976
	step [117/146], loss=111.6403
	step [118/146], loss=121.3761
	step [119/146], loss=120.0865
	step [120/146], loss=124.5776
	step [121/146], loss=134.6983
	step [122/146], loss=102.3226
	step [123/146], loss=102.2089
	step [124/146], loss=121.3274
	step [125/146], loss=140.9318
	step [126/146], loss=134.5283
	step [127/146], loss=97.7619
	step [128/146], loss=131.0530
	step [129/146], loss=130.9863
	step [130/146], loss=146.8684
	step [131/146], loss=111.7854
	step [132/146], loss=113.5571
	step [133/146], loss=100.7630
	step [134/146], loss=103.8696
	step [135/146], loss=108.9733
	step [136/146], loss=105.1614
	step [137/146], loss=108.8643
	step [138/146], loss=116.4966
	step [139/146], loss=121.1032
	step [140/146], loss=116.5303
	step [141/146], loss=104.2962
	step [142/146], loss=115.3087
	step [143/146], loss=85.9195
	step [144/146], loss=138.6050
	step [145/146], loss=122.7079
	step [146/146], loss=73.1256
	Evaluating
	loss=0.0496, precision=0.2567, recall=0.9393, f1=0.4033
Training epoch 14
	step [1/146], loss=139.1271
	step [2/146], loss=117.3185
	step [3/146], loss=114.6947
	step [4/146], loss=127.1503
	step [5/146], loss=115.4854
	step [6/146], loss=122.6901
	step [7/146], loss=149.4892
	step [8/146], loss=98.4832
	step [9/146], loss=103.3567
	step [10/146], loss=97.1582
	step [11/146], loss=107.6249
	step [12/146], loss=132.7716
	step [13/146], loss=126.2312
	step [14/146], loss=101.4411
	step [15/146], loss=112.2641
	step [16/146], loss=136.3565
	step [17/146], loss=139.0830
	step [18/146], loss=128.1556
	step [19/146], loss=113.0457
	step [20/146], loss=119.2736
	step [21/146], loss=115.6975
	step [22/146], loss=125.0796
	step [23/146], loss=107.5657
	step [24/146], loss=115.7439
	step [25/146], loss=120.7655
	step [26/146], loss=118.6645
	step [27/146], loss=106.6548
	step [28/146], loss=139.3640
	step [29/146], loss=118.3216
	step [30/146], loss=129.1368
	step [31/146], loss=119.1843
	step [32/146], loss=114.3931
	step [33/146], loss=114.7956
	step [34/146], loss=107.1664
	step [35/146], loss=105.2377
	step [36/146], loss=126.1725
	step [37/146], loss=108.1221
	step [38/146], loss=108.9916
	step [39/146], loss=121.1327
	step [40/146], loss=104.6724
	step [41/146], loss=104.9257
	step [42/146], loss=112.7289
	step [43/146], loss=122.6897
	step [44/146], loss=106.8035
	step [45/146], loss=125.9100
	step [46/146], loss=130.8891
	step [47/146], loss=109.4719
	step [48/146], loss=120.6593
	step [49/146], loss=116.3823
	step [50/146], loss=102.1236
	step [51/146], loss=120.0729
	step [52/146], loss=120.9852
	step [53/146], loss=110.1757
	step [54/146], loss=130.5303
	step [55/146], loss=118.3614
	step [56/146], loss=126.8172
	step [57/146], loss=102.4650
	step [58/146], loss=114.9926
	step [59/146], loss=132.3510
	step [60/146], loss=117.9377
	step [61/146], loss=120.9138
	step [62/146], loss=129.2438
	step [63/146], loss=96.1525
	step [64/146], loss=110.1991
	step [65/146], loss=124.1572
	step [66/146], loss=98.0949
	step [67/146], loss=91.2227
	step [68/146], loss=97.8293
	step [69/146], loss=127.9910
	step [70/146], loss=123.8098
	step [71/146], loss=119.4018
	step [72/146], loss=117.9764
	step [73/146], loss=119.7926
	step [74/146], loss=120.1726
	step [75/146], loss=128.5938
	step [76/146], loss=115.3689
	step [77/146], loss=120.9197
	step [78/146], loss=106.1701
	step [79/146], loss=138.6396
	step [80/146], loss=121.2949
	step [81/146], loss=129.5850
	step [82/146], loss=121.3608
	step [83/146], loss=127.2785
	step [84/146], loss=103.9429
	step [85/146], loss=121.8917
	step [86/146], loss=101.4107
	step [87/146], loss=113.5103
	step [88/146], loss=122.3960
	step [89/146], loss=92.3402
	step [90/146], loss=109.8991
	step [91/146], loss=112.5864
	step [92/146], loss=113.8930
	step [93/146], loss=101.6540
	step [94/146], loss=146.1089
	step [95/146], loss=119.1239
	step [96/146], loss=112.2132
	step [97/146], loss=122.9319
	step [98/146], loss=140.6530
	step [99/146], loss=99.0566
	step [100/146], loss=112.7203
	step [101/146], loss=115.6725
	step [102/146], loss=118.3659
	step [103/146], loss=112.9580
	step [104/146], loss=116.4072
	step [105/146], loss=117.1029
	step [106/146], loss=113.6160
	step [107/146], loss=121.8887
	step [108/146], loss=116.9361
	step [109/146], loss=105.8941
	step [110/146], loss=107.5530
	step [111/146], loss=126.1582
	step [112/146], loss=113.9498
	step [113/146], loss=133.4602
	step [114/146], loss=97.6179
	step [115/146], loss=112.0440
	step [116/146], loss=110.4917
	step [117/146], loss=129.0362
	step [118/146], loss=105.1482
	step [119/146], loss=106.3952
	step [120/146], loss=108.3032
	step [121/146], loss=113.0741
	step [122/146], loss=130.4503
	step [123/146], loss=96.4374
	step [124/146], loss=104.2711
	step [125/146], loss=137.9495
	step [126/146], loss=137.0964
	step [127/146], loss=116.3178
	step [128/146], loss=123.5502
	step [129/146], loss=114.3576
	step [130/146], loss=108.5724
	step [131/146], loss=136.5751
	step [132/146], loss=115.4860
	step [133/146], loss=126.7450
	step [134/146], loss=114.3700
	step [135/146], loss=120.6926
	step [136/146], loss=121.8030
	step [137/146], loss=112.8731
	step [138/146], loss=118.0476
	step [139/146], loss=117.8833
	step [140/146], loss=107.3172
	step [141/146], loss=127.1993
	step [142/146], loss=106.2985
	step [143/146], loss=113.3554
	step [144/146], loss=88.2185
	step [145/146], loss=115.4255
	step [146/146], loss=53.8115
	Evaluating
	loss=0.0350, precision=0.3946, recall=0.9299, f1=0.5541
Training epoch 15
	step [1/146], loss=81.5388
	step [2/146], loss=103.3466
	step [3/146], loss=117.7641
	step [4/146], loss=124.4986
	step [5/146], loss=119.9443
	step [6/146], loss=127.3308
	step [7/146], loss=103.1239
	step [8/146], loss=112.2157
	step [9/146], loss=121.4505
	step [10/146], loss=113.3354
	step [11/146], loss=121.4111
	step [12/146], loss=110.3187
	step [13/146], loss=117.8598
	step [14/146], loss=109.4680
	step [15/146], loss=108.1046
	step [16/146], loss=122.2014
	step [17/146], loss=130.2337
	step [18/146], loss=125.1163
	step [19/146], loss=118.6767
	step [20/146], loss=115.1971
	step [21/146], loss=125.6797
	step [22/146], loss=110.1554
	step [23/146], loss=111.2070
	step [24/146], loss=114.0056
	step [25/146], loss=94.5249
	step [26/146], loss=100.4219
	step [27/146], loss=103.0500
	step [28/146], loss=103.9949
	step [29/146], loss=114.8163
	step [30/146], loss=107.4956
	step [31/146], loss=123.5189
	step [32/146], loss=143.9624
	step [33/146], loss=98.6097
	step [34/146], loss=134.1449
	step [35/146], loss=115.4498
	step [36/146], loss=109.9519
	step [37/146], loss=94.7364
	step [38/146], loss=98.7256
	step [39/146], loss=130.7298
	step [40/146], loss=113.1026
	step [41/146], loss=103.9774
	step [42/146], loss=133.0555
	step [43/146], loss=123.1266
	step [44/146], loss=125.4591
	step [45/146], loss=128.7686
	step [46/146], loss=107.4058
	step [47/146], loss=103.2493
	step [48/146], loss=98.1709
	step [49/146], loss=114.5178
	step [50/146], loss=114.6992
	step [51/146], loss=107.4844
	step [52/146], loss=125.2109
	step [53/146], loss=103.8560
	step [54/146], loss=108.4560
	step [55/146], loss=109.9003
	step [56/146], loss=119.4473
	step [57/146], loss=124.4717
	step [58/146], loss=100.0409
	step [59/146], loss=100.6372
	step [60/146], loss=114.7611
	step [61/146], loss=111.6070
	step [62/146], loss=119.0160
	step [63/146], loss=124.7232
	step [64/146], loss=143.7105
	step [65/146], loss=118.1361
	step [66/146], loss=107.6962
	step [67/146], loss=122.6368
	step [68/146], loss=103.8813
	step [69/146], loss=104.8050
	step [70/146], loss=100.9048
	step [71/146], loss=106.0841
	step [72/146], loss=97.4242
	step [73/146], loss=126.3449
	step [74/146], loss=113.9407
	step [75/146], loss=113.2931
	step [76/146], loss=131.2141
	step [77/146], loss=137.5381
	step [78/146], loss=114.0865
	step [79/146], loss=116.1152
	step [80/146], loss=128.4597
	step [81/146], loss=116.2494
	step [82/146], loss=116.9524
	step [83/146], loss=110.9983
	step [84/146], loss=112.2707
	step [85/146], loss=106.4181
	step [86/146], loss=106.2870
	step [87/146], loss=122.4670
	step [88/146], loss=113.8473
	step [89/146], loss=118.4576
	step [90/146], loss=130.9021
	step [91/146], loss=136.2749
	step [92/146], loss=119.9881
	step [93/146], loss=126.9186
	step [94/146], loss=121.8288
	step [95/146], loss=132.6956
	step [96/146], loss=140.6320
	step [97/146], loss=108.5590
	step [98/146], loss=102.3800
	step [99/146], loss=134.1129
	step [100/146], loss=117.2618
	step [101/146], loss=126.8144
	step [102/146], loss=114.9170
	step [103/146], loss=95.3491
	step [104/146], loss=118.1660
	step [105/146], loss=104.3833
	step [106/146], loss=124.4400
	step [107/146], loss=96.0054
	step [108/146], loss=122.1419
	step [109/146], loss=121.7793
	step [110/146], loss=108.2690
	step [111/146], loss=92.6670
	step [112/146], loss=109.1432
	step [113/146], loss=101.5602
	step [114/146], loss=138.1779
	step [115/146], loss=131.3270
	step [116/146], loss=123.1216
	step [117/146], loss=111.1240
	step [118/146], loss=124.0134
	step [119/146], loss=125.0145
	step [120/146], loss=100.9284
	step [121/146], loss=115.9734
	step [122/146], loss=100.3300
	step [123/146], loss=114.9714
	step [124/146], loss=136.4198
	step [125/146], loss=113.0870
	step [126/146], loss=110.1771
	step [127/146], loss=117.5412
	step [128/146], loss=107.5848
	step [129/146], loss=108.8558
	step [130/146], loss=90.0566
	step [131/146], loss=88.8708
	step [132/146], loss=102.2110
	step [133/146], loss=115.1195
	step [134/146], loss=98.8564
	step [135/146], loss=98.3917
	step [136/146], loss=99.8718
	step [137/146], loss=125.4452
	step [138/146], loss=129.6760
	step [139/146], loss=102.4370
	step [140/146], loss=135.1332
	step [141/146], loss=144.6057
	step [142/146], loss=116.7220
	step [143/146], loss=107.2896
	step [144/146], loss=102.3754
	step [145/146], loss=123.4059
	step [146/146], loss=56.5138
	Evaluating
	loss=0.0333, precision=0.3691, recall=0.9397, f1=0.5300
Training epoch 16
	step [1/146], loss=112.3785
	step [2/146], loss=94.3622
	step [3/146], loss=130.4740
	step [4/146], loss=103.3529
	step [5/146], loss=81.1711
	step [6/146], loss=108.0256
	step [7/146], loss=102.5752
	step [8/146], loss=104.3484
	step [9/146], loss=136.7530
	step [10/146], loss=111.6822
	step [11/146], loss=100.5761
	step [12/146], loss=107.3412
	step [13/146], loss=114.7624
	step [14/146], loss=107.5895
	step [15/146], loss=138.5499
	step [16/146], loss=106.3150
	step [17/146], loss=116.5916
	step [18/146], loss=126.6865
	step [19/146], loss=112.4334
	step [20/146], loss=131.0533
	step [21/146], loss=130.2698
	step [22/146], loss=134.7013
	step [23/146], loss=114.2670
	step [24/146], loss=127.5857
	step [25/146], loss=89.4024
	step [26/146], loss=110.0119
	step [27/146], loss=92.0632
	step [28/146], loss=143.2201
	step [29/146], loss=109.6234
	step [30/146], loss=113.8587
	step [31/146], loss=138.5045
	step [32/146], loss=98.3544
	step [33/146], loss=105.6998
	step [34/146], loss=120.7784
	step [35/146], loss=113.4853
	step [36/146], loss=116.1615
	step [37/146], loss=126.1265
	step [38/146], loss=113.6415
	step [39/146], loss=106.9727
	step [40/146], loss=116.8746
	step [41/146], loss=121.0403
	step [42/146], loss=120.5226
	step [43/146], loss=105.1113
	step [44/146], loss=122.0488
	step [45/146], loss=118.6901
	step [46/146], loss=113.9660
	step [47/146], loss=126.7478
	step [48/146], loss=104.5409
	step [49/146], loss=117.3355
	step [50/146], loss=116.8354
	step [51/146], loss=106.1527
	step [52/146], loss=128.6079
	step [53/146], loss=152.0930
	step [54/146], loss=92.7399
	step [55/146], loss=116.2571
	step [56/146], loss=106.1189
	step [57/146], loss=100.4208
	step [58/146], loss=112.8509
	step [59/146], loss=109.5184
	step [60/146], loss=88.1115
	step [61/146], loss=108.3690
	step [62/146], loss=108.3316
	step [63/146], loss=108.4857
	step [64/146], loss=117.7268
	step [65/146], loss=111.6174
	step [66/146], loss=112.0723
	step [67/146], loss=115.7142
	step [68/146], loss=114.5494
	step [69/146], loss=121.4188
	step [70/146], loss=110.0391
	step [71/146], loss=126.1866
	step [72/146], loss=110.8010
	step [73/146], loss=105.0756
	step [74/146], loss=113.8389
	step [75/146], loss=111.0304
	step [76/146], loss=105.7912
	step [77/146], loss=115.8105
	step [78/146], loss=114.3936
	step [79/146], loss=135.3928
	step [80/146], loss=121.8463
	step [81/146], loss=116.1319
	step [82/146], loss=105.1479
	step [83/146], loss=103.8085
	step [84/146], loss=112.0074
	step [85/146], loss=109.9499
	step [86/146], loss=100.2631
	step [87/146], loss=105.9903
	step [88/146], loss=115.3168
	step [89/146], loss=100.9841
	step [90/146], loss=105.6064
	step [91/146], loss=97.3306
	step [92/146], loss=106.9058
	step [93/146], loss=101.0745
	step [94/146], loss=104.1074
	step [95/146], loss=113.5436
	step [96/146], loss=97.6403
	step [97/146], loss=106.9383
	step [98/146], loss=110.8406
	step [99/146], loss=101.7596
	step [100/146], loss=115.9047
	step [101/146], loss=111.9015
	step [102/146], loss=94.3009
	step [103/146], loss=124.9793
	step [104/146], loss=129.6698
	step [105/146], loss=124.2056
	step [106/146], loss=117.6751
	step [107/146], loss=133.0935
	step [108/146], loss=100.8584
	step [109/146], loss=102.6262
	step [110/146], loss=109.0658
	step [111/146], loss=132.8413
	step [112/146], loss=109.1181
	step [113/146], loss=106.8195
	step [114/146], loss=108.0866
	step [115/146], loss=115.3332
	step [116/146], loss=114.0406
	step [117/146], loss=107.3775
	step [118/146], loss=139.1263
	step [119/146], loss=126.7432
	step [120/146], loss=122.7700
	step [121/146], loss=125.6614
	step [122/146], loss=127.7445
	step [123/146], loss=96.9331
	step [124/146], loss=125.0053
	step [125/146], loss=123.4312
	step [126/146], loss=115.8713
	step [127/146], loss=96.8335
	step [128/146], loss=90.6625
	step [129/146], loss=93.3787
	step [130/146], loss=113.8832
	step [131/146], loss=103.3237
	step [132/146], loss=135.7673
	step [133/146], loss=109.0706
	step [134/146], loss=116.5629
	step [135/146], loss=117.3632
	step [136/146], loss=136.0729
	step [137/146], loss=118.5253
	step [138/146], loss=119.7090
	step [139/146], loss=91.8806
	step [140/146], loss=103.9863
	step [141/146], loss=127.9660
	step [142/146], loss=122.3695
	step [143/146], loss=100.0441
	step [144/146], loss=133.4295
	step [145/146], loss=118.6763
	step [146/146], loss=87.4977
	Evaluating
	loss=0.0350, precision=0.2469, recall=0.9312, f1=0.3903
Training epoch 17
	step [1/146], loss=124.0841
	step [2/146], loss=128.9123
	step [3/146], loss=100.1166
	step [4/146], loss=121.7685
	step [5/146], loss=105.9774
	step [6/146], loss=114.0174
	step [7/146], loss=93.8457
	step [8/146], loss=115.4861
	step [9/146], loss=110.3084
	step [10/146], loss=119.6930
	step [11/146], loss=115.3949
	step [12/146], loss=110.1491
	step [13/146], loss=123.4072
	step [14/146], loss=110.5063
	step [15/146], loss=98.2704
	step [16/146], loss=112.6973
	step [17/146], loss=129.7996
	step [18/146], loss=148.0780
	step [19/146], loss=107.8317
	step [20/146], loss=120.1470
	step [21/146], loss=121.7285
	step [22/146], loss=105.2214
	step [23/146], loss=119.0418
	step [24/146], loss=116.8090
	step [25/146], loss=110.1947
	step [26/146], loss=103.2226
	step [27/146], loss=123.8191
	step [28/146], loss=116.8659
	step [29/146], loss=100.5842
	step [30/146], loss=103.5717
	step [31/146], loss=109.5163
	step [32/146], loss=113.1360
	step [33/146], loss=102.7338
	step [34/146], loss=100.9326
	step [35/146], loss=111.1778
	step [36/146], loss=126.5312
	step [37/146], loss=107.7142
	step [38/146], loss=118.7448
	step [39/146], loss=103.0748
	step [40/146], loss=100.9674
	step [41/146], loss=116.0090
	step [42/146], loss=113.4078
	step [43/146], loss=127.0141
	step [44/146], loss=105.2722
	step [45/146], loss=108.5110
	step [46/146], loss=96.1506
	step [47/146], loss=106.0475
	step [48/146], loss=120.0908
	step [49/146], loss=137.4722
	step [50/146], loss=139.5052
	step [51/146], loss=126.6561
	step [52/146], loss=110.2908
	step [53/146], loss=116.0848
	step [54/146], loss=125.6229
	step [55/146], loss=97.7147
	step [56/146], loss=112.5077
	step [57/146], loss=108.3229
	step [58/146], loss=88.7311
	step [59/146], loss=116.0688
	step [60/146], loss=139.4369
	step [61/146], loss=117.9390
	step [62/146], loss=113.6471
	step [63/146], loss=87.7228
	step [64/146], loss=95.3898
	step [65/146], loss=108.2374
	step [66/146], loss=109.1874
	step [67/146], loss=114.0122
	step [68/146], loss=116.8083
	step [69/146], loss=110.1673
	step [70/146], loss=107.2056
	step [71/146], loss=109.1406
	step [72/146], loss=114.4376
	step [73/146], loss=98.3422
	step [74/146], loss=96.9627
	step [75/146], loss=117.6782
	step [76/146], loss=102.4847
	step [77/146], loss=104.6295
	step [78/146], loss=95.4558
	step [79/146], loss=115.5618
	step [80/146], loss=120.5715
	step [81/146], loss=108.9655
	step [82/146], loss=93.6742
	step [83/146], loss=111.1689
	step [84/146], loss=114.2922
	step [85/146], loss=124.9491
	step [86/146], loss=107.3937
	step [87/146], loss=122.6280
	step [88/146], loss=126.6397
	step [89/146], loss=131.2173
	step [90/146], loss=111.9175
	step [91/146], loss=93.8695
	step [92/146], loss=111.3745
	step [93/146], loss=126.5961
	step [94/146], loss=101.3507
	step [95/146], loss=89.2243
	step [96/146], loss=100.8427
	step [97/146], loss=111.5904
	step [98/146], loss=104.1043
	step [99/146], loss=101.6277
	step [100/146], loss=123.0322
	step [101/146], loss=110.8237
	step [102/146], loss=136.1487
	step [103/146], loss=104.2497
	step [104/146], loss=103.5321
	step [105/146], loss=118.6077
	step [106/146], loss=140.0748
	step [107/146], loss=102.9659
	step [108/146], loss=120.9468
	step [109/146], loss=110.1134
	step [110/146], loss=124.6369
	step [111/146], loss=119.5277
	step [112/146], loss=120.6301
	step [113/146], loss=108.8746
	step [114/146], loss=114.1728
	step [115/146], loss=125.9976
	step [116/146], loss=124.0489
	step [117/146], loss=109.4756
	step [118/146], loss=108.4849
	step [119/146], loss=99.3984
	step [120/146], loss=104.7060
	step [121/146], loss=119.8994
	step [122/146], loss=128.7327
	step [123/146], loss=94.9398
	step [124/146], loss=111.3253
	step [125/146], loss=91.8490
	step [126/146], loss=110.0150
	step [127/146], loss=117.8125
	step [128/146], loss=121.0196
	step [129/146], loss=110.7510
	step [130/146], loss=108.2739
	step [131/146], loss=111.4029
	step [132/146], loss=101.8856
	step [133/146], loss=102.7743
	step [134/146], loss=124.9462
	step [135/146], loss=117.6595
	step [136/146], loss=111.5507
	step [137/146], loss=93.2825
	step [138/146], loss=120.9632
	step [139/146], loss=121.1687
	step [140/146], loss=104.8408
	step [141/146], loss=111.8539
	step [142/146], loss=106.9021
	step [143/146], loss=104.4721
	step [144/146], loss=106.9400
	step [145/146], loss=96.4960
	step [146/146], loss=60.0712
	Evaluating
	loss=0.0273, precision=0.4598, recall=0.9164, f1=0.6123
Training epoch 18
	step [1/146], loss=108.8822
	step [2/146], loss=118.7480
	step [3/146], loss=103.1609
	step [4/146], loss=139.3590
	step [5/146], loss=111.5276
	step [6/146], loss=114.7561
	step [7/146], loss=103.5087
	step [8/146], loss=117.9417
	step [9/146], loss=104.3717
	step [10/146], loss=117.4124
	step [11/146], loss=111.7978
	step [12/146], loss=102.8259
	step [13/146], loss=93.8406
	step [14/146], loss=110.4873
	step [15/146], loss=103.9443
	step [16/146], loss=118.4773
	step [17/146], loss=108.3418
	step [18/146], loss=110.8273
	step [19/146], loss=122.6167
	step [20/146], loss=107.0821
	step [21/146], loss=120.3771
	step [22/146], loss=95.0703
	step [23/146], loss=105.5646
	step [24/146], loss=100.3378
	step [25/146], loss=123.9912
	step [26/146], loss=93.2758
	step [27/146], loss=89.9385
	step [28/146], loss=105.4770
	step [29/146], loss=132.2143
	step [30/146], loss=116.1257
	step [31/146], loss=120.8150
	step [32/146], loss=107.1910
	step [33/146], loss=102.7519
	step [34/146], loss=114.6418
	step [35/146], loss=108.2867
	step [36/146], loss=105.9201
	step [37/146], loss=104.6756
	step [38/146], loss=106.6886
	step [39/146], loss=124.1159
	step [40/146], loss=95.4318
	step [41/146], loss=115.6360
	step [42/146], loss=122.3796
	step [43/146], loss=89.9378
	step [44/146], loss=121.3519
	step [45/146], loss=100.6416
	step [46/146], loss=115.0719
	step [47/146], loss=97.2581
	step [48/146], loss=90.3134
	step [49/146], loss=120.3452
	step [50/146], loss=104.3110
	step [51/146], loss=108.1313
	step [52/146], loss=110.6488
	step [53/146], loss=126.8698
	step [54/146], loss=104.3126
	step [55/146], loss=131.0283
	step [56/146], loss=104.4873
	step [57/146], loss=103.1396
	step [58/146], loss=106.6047
	step [59/146], loss=105.4988
	step [60/146], loss=117.8070
	step [61/146], loss=105.5042
	step [62/146], loss=104.1568
	step [63/146], loss=96.8783
	step [64/146], loss=111.7997
	step [65/146], loss=118.1471
	step [66/146], loss=113.8116
	step [67/146], loss=102.2026
	step [68/146], loss=107.6703
	step [69/146], loss=98.8626
	step [70/146], loss=120.1856
	step [71/146], loss=108.2937
	step [72/146], loss=89.0237
	step [73/146], loss=98.4047
	step [74/146], loss=98.0761
	step [75/146], loss=108.8550
	step [76/146], loss=105.7735
	step [77/146], loss=106.1842
	step [78/146], loss=112.1492
	step [79/146], loss=109.0424
	step [80/146], loss=100.1436
	step [81/146], loss=123.2607
	step [82/146], loss=111.9089
	step [83/146], loss=111.2007
	step [84/146], loss=98.8606
	step [85/146], loss=118.6985
	step [86/146], loss=111.7233
	step [87/146], loss=126.4422
	step [88/146], loss=123.6633
	step [89/146], loss=128.9376
	step [90/146], loss=116.5843
	step [91/146], loss=112.8485
	step [92/146], loss=111.7336
	step [93/146], loss=116.7066
	step [94/146], loss=112.6813
	step [95/146], loss=138.7879
	step [96/146], loss=106.2668
	step [97/146], loss=99.0081
	step [98/146], loss=101.9750
	step [99/146], loss=123.5227
	step [100/146], loss=107.7394
	step [101/146], loss=120.7717
	step [102/146], loss=104.3133
	step [103/146], loss=113.7989
	step [104/146], loss=111.0359
	step [105/146], loss=113.9904
	step [106/146], loss=131.2289
	step [107/146], loss=126.1489
	step [108/146], loss=125.1715
	step [109/146], loss=94.5265
	step [110/146], loss=144.5576
	step [111/146], loss=135.4297
	step [112/146], loss=93.3883
	step [113/146], loss=108.5727
	step [114/146], loss=114.7947
	step [115/146], loss=114.7158
	step [116/146], loss=95.0627
	step [117/146], loss=108.4327
	step [118/146], loss=125.2829
	step [119/146], loss=104.0970
	step [120/146], loss=109.8946
	step [121/146], loss=119.7249
	step [122/146], loss=106.8648
	step [123/146], loss=96.0111
	step [124/146], loss=110.4894
	step [125/146], loss=110.0889
	step [126/146], loss=107.0533
	step [127/146], loss=119.9022
	step [128/146], loss=107.4699
	step [129/146], loss=115.2440
	step [130/146], loss=106.9159
	step [131/146], loss=101.8796
	step [132/146], loss=100.2616
	step [133/146], loss=106.4822
	step [134/146], loss=127.9373
	step [135/146], loss=103.1708
	step [136/146], loss=125.0339
	step [137/146], loss=94.3290
	step [138/146], loss=101.9995
	step [139/146], loss=93.7910
	step [140/146], loss=103.7759
	step [141/146], loss=118.4654
	step [142/146], loss=113.2780
	step [143/146], loss=104.7486
	step [144/146], loss=121.7181
	step [145/146], loss=133.4553
	step [146/146], loss=60.9051
	Evaluating
	loss=0.0387, precision=0.2119, recall=0.9300, f1=0.3452
Training epoch 19
	step [1/146], loss=94.9203
	step [2/146], loss=114.1854
	step [3/146], loss=125.2332
	step [4/146], loss=108.6746
	step [5/146], loss=95.7004
	step [6/146], loss=114.3967
	step [7/146], loss=104.6552
	step [8/146], loss=91.2476
	step [9/146], loss=120.3917
	step [10/146], loss=94.5856
	step [11/146], loss=106.4947
	step [12/146], loss=116.5679
	step [13/146], loss=121.4100
	step [14/146], loss=108.3819
	step [15/146], loss=121.2059
	step [16/146], loss=102.3620
	step [17/146], loss=116.0900
	step [18/146], loss=114.0601
	step [19/146], loss=100.8732
	step [20/146], loss=112.7817
	step [21/146], loss=110.0800
	step [22/146], loss=100.3875
	step [23/146], loss=137.7105
	step [24/146], loss=113.1726
	step [25/146], loss=117.2312
	step [26/146], loss=107.8932
	step [27/146], loss=106.5017
	step [28/146], loss=102.0448
	step [29/146], loss=106.6578
	step [30/146], loss=102.6838
	step [31/146], loss=103.5685
	step [32/146], loss=119.9514
	step [33/146], loss=97.3723
	step [34/146], loss=110.8778
	step [35/146], loss=117.8108
	step [36/146], loss=129.3799
	step [37/146], loss=92.1614
	step [38/146], loss=104.0264
	step [39/146], loss=117.5287
	step [40/146], loss=90.7920
	step [41/146], loss=100.5129
	step [42/146], loss=101.4707
	step [43/146], loss=129.2038
	step [44/146], loss=102.5462
	step [45/146], loss=105.1330
	step [46/146], loss=103.7638
	step [47/146], loss=134.6180
	step [48/146], loss=92.3775
	step [49/146], loss=92.5983
	step [50/146], loss=105.4277
	step [51/146], loss=103.0388
	step [52/146], loss=129.0061
	step [53/146], loss=112.4700
	step [54/146], loss=135.7235
	step [55/146], loss=106.3788
	step [56/146], loss=102.6486
	step [57/146], loss=100.3179
	step [58/146], loss=130.2632
	step [59/146], loss=125.8983
	step [60/146], loss=104.3925
	step [61/146], loss=93.8106
	step [62/146], loss=106.7499
	step [63/146], loss=110.8674
	step [64/146], loss=101.7091
	step [65/146], loss=104.1714
	step [66/146], loss=94.7597
	step [67/146], loss=115.9781
	step [68/146], loss=90.4557
	step [69/146], loss=113.9104
	step [70/146], loss=111.3092
	step [71/146], loss=114.6250
	step [72/146], loss=84.9954
	step [73/146], loss=123.5850
	step [74/146], loss=99.4278
	step [75/146], loss=115.3320
	step [76/146], loss=108.6669
	step [77/146], loss=142.6302
	step [78/146], loss=96.9224
	step [79/146], loss=104.1204
	step [80/146], loss=133.4211
	step [81/146], loss=109.1420
	step [82/146], loss=119.3953
	step [83/146], loss=95.9634
	step [84/146], loss=118.7432
	step [85/146], loss=122.2013
	step [86/146], loss=119.8394
	step [87/146], loss=95.1734
	step [88/146], loss=135.7780
	step [89/146], loss=96.6972
	step [90/146], loss=106.9877
	step [91/146], loss=105.6400
	step [92/146], loss=109.5583
	step [93/146], loss=113.3699
	step [94/146], loss=122.9429
	step [95/146], loss=141.3289
	step [96/146], loss=81.8473
	step [97/146], loss=113.0029
	step [98/146], loss=92.6467
	step [99/146], loss=100.0271
	step [100/146], loss=90.8590
	step [101/146], loss=126.2505
	step [102/146], loss=118.0719
	step [103/146], loss=105.7511
	step [104/146], loss=107.7443
	step [105/146], loss=109.9694
	step [106/146], loss=114.7448
	step [107/146], loss=125.7626
	step [108/146], loss=86.4675
	step [109/146], loss=115.0821
	step [110/146], loss=133.2134
	step [111/146], loss=112.1349
	step [112/146], loss=104.3768
	step [113/146], loss=110.8097
	step [114/146], loss=103.9044
	step [115/146], loss=114.6358
	step [116/146], loss=109.8686
	step [117/146], loss=115.0889
	step [118/146], loss=126.4549
	step [119/146], loss=88.0230
	step [120/146], loss=94.0085
	step [121/146], loss=111.4090
	step [122/146], loss=99.4859
	step [123/146], loss=119.6398
	step [124/146], loss=117.6776
	step [125/146], loss=108.1148
	step [126/146], loss=103.3058
	step [127/146], loss=133.7336
	step [128/146], loss=108.9733
	step [129/146], loss=104.2877
	step [130/146], loss=104.5371
	step [131/146], loss=120.5191
	step [132/146], loss=106.1217
	step [133/146], loss=96.3820
	step [134/146], loss=102.8216
	step [135/146], loss=119.7827
	step [136/146], loss=106.6301
	step [137/146], loss=121.0576
	step [138/146], loss=94.0919
	step [139/146], loss=105.5367
	step [140/146], loss=108.4307
	step [141/146], loss=80.9320
	step [142/146], loss=111.8734
	step [143/146], loss=112.3339
	step [144/146], loss=107.6057
	step [145/146], loss=120.6201
	step [146/146], loss=53.9810
	Evaluating
	loss=0.0254, precision=0.3967, recall=0.9211, f1=0.5545
Training epoch 20
	step [1/146], loss=93.5858
	step [2/146], loss=104.9061
	step [3/146], loss=113.6733
	step [4/146], loss=108.6629
	step [5/146], loss=118.2374
	step [6/146], loss=97.2688
	step [7/146], loss=99.5562
	step [8/146], loss=98.4278
	step [9/146], loss=99.5682
	step [10/146], loss=120.1052
	step [11/146], loss=104.8797
	step [12/146], loss=91.9660
	step [13/146], loss=116.0760
	step [14/146], loss=105.4528
	step [15/146], loss=98.9829
	step [16/146], loss=111.1800
	step [17/146], loss=94.4784
	step [18/146], loss=112.8477
	step [19/146], loss=98.4291
	step [20/146], loss=112.9087
	step [21/146], loss=92.5677
	step [22/146], loss=107.4621
	step [23/146], loss=107.5415
	step [24/146], loss=87.9897
	step [25/146], loss=103.9152
	step [26/146], loss=119.3096
	step [27/146], loss=121.4944
	step [28/146], loss=106.3109
	step [29/146], loss=105.7077
	step [30/146], loss=124.6700
	step [31/146], loss=124.0384
	step [32/146], loss=109.5432
	step [33/146], loss=106.2544
	step [34/146], loss=111.7634
	step [35/146], loss=124.3478
	step [36/146], loss=141.2509
	step [37/146], loss=121.9448
	step [38/146], loss=94.9925
	step [39/146], loss=108.1998
	step [40/146], loss=98.8435
	step [41/146], loss=114.6832
	step [42/146], loss=107.8671
	step [43/146], loss=111.0419
	step [44/146], loss=126.4432
	step [45/146], loss=121.2591
	step [46/146], loss=122.8999
	step [47/146], loss=121.9734
	step [48/146], loss=116.3673
	step [49/146], loss=89.7789
	step [50/146], loss=128.3331
	step [51/146], loss=87.2233
	step [52/146], loss=94.7361
	step [53/146], loss=96.4655
	step [54/146], loss=100.8066
	step [55/146], loss=110.4588
	step [56/146], loss=110.2365
	step [57/146], loss=115.9563
	step [58/146], loss=119.5545
	step [59/146], loss=122.3778
	step [60/146], loss=102.0118
	step [61/146], loss=110.6942
	step [62/146], loss=104.9005
	step [63/146], loss=108.7501
	step [64/146], loss=106.2891
	step [65/146], loss=95.0290
	step [66/146], loss=91.4832
	step [67/146], loss=109.5323
	step [68/146], loss=105.3838
	step [69/146], loss=116.2275
	step [70/146], loss=100.5411
	step [71/146], loss=119.8855
	step [72/146], loss=113.2838
	step [73/146], loss=105.5399
	step [74/146], loss=118.9743
	step [75/146], loss=105.3732
	step [76/146], loss=107.6561
	step [77/146], loss=122.3266
	step [78/146], loss=123.5939
	step [79/146], loss=102.5256
	step [80/146], loss=113.4065
	step [81/146], loss=90.1367
	step [82/146], loss=103.9372
	step [83/146], loss=94.7865
	step [84/146], loss=113.6795
	step [85/146], loss=104.2438
	step [86/146], loss=106.8315
	step [87/146], loss=94.8154
	step [88/146], loss=112.5720
	step [89/146], loss=90.0145
	step [90/146], loss=110.3380
	step [91/146], loss=98.4054
	step [92/146], loss=91.0864
	step [93/146], loss=98.2020
	step [94/146], loss=105.2265
	step [95/146], loss=101.9713
	step [96/146], loss=87.1809
	step [97/146], loss=100.6090
	step [98/146], loss=103.5951
	step [99/146], loss=106.5608
	step [100/146], loss=104.5430
	step [101/146], loss=96.3595
	step [102/146], loss=104.8012
	step [103/146], loss=124.3503
	step [104/146], loss=108.2017
	step [105/146], loss=104.9344
	step [106/146], loss=91.0145
	step [107/146], loss=111.0515
	step [108/146], loss=125.4142
	step [109/146], loss=110.5232
	step [110/146], loss=99.5173
	step [111/146], loss=131.5670
	step [112/146], loss=108.0591
	step [113/146], loss=91.5357
	step [114/146], loss=106.4810
	step [115/146], loss=107.6664
	step [116/146], loss=113.5471
	step [117/146], loss=108.7771
	step [118/146], loss=101.2738
	step [119/146], loss=124.2658
	step [120/146], loss=118.6407
	step [121/146], loss=108.3505
	step [122/146], loss=101.6867
	step [123/146], loss=115.0353
	step [124/146], loss=78.5406
	step [125/146], loss=105.7937
	step [126/146], loss=101.9026
	step [127/146], loss=132.2113
	step [128/146], loss=117.9996
	step [129/146], loss=109.0502
	step [130/146], loss=111.0825
	step [131/146], loss=117.4622
	step [132/146], loss=112.9157
	step [133/146], loss=98.2581
	step [134/146], loss=114.4168
	step [135/146], loss=95.5202
	step [136/146], loss=109.6029
	step [137/146], loss=100.6757
	step [138/146], loss=116.2238
	step [139/146], loss=114.9996
	step [140/146], loss=98.8259
	step [141/146], loss=118.6796
	step [142/146], loss=89.0694
	step [143/146], loss=121.1187
	step [144/146], loss=118.3090
	step [145/146], loss=119.0293
	step [146/146], loss=69.0140
	Evaluating
	loss=0.0235, precision=0.3831, recall=0.9418, f1=0.5446
Training epoch 21
	step [1/146], loss=98.2295
	step [2/146], loss=112.5113
	step [3/146], loss=110.7590
	step [4/146], loss=127.5241
	step [5/146], loss=93.2272
	step [6/146], loss=119.4290
	step [7/146], loss=132.6967
	step [8/146], loss=105.0650
	step [9/146], loss=92.3236
	step [10/146], loss=120.5746
	step [11/146], loss=100.4239
	step [12/146], loss=86.4180
	step [13/146], loss=102.6714
	step [14/146], loss=107.6723
	step [15/146], loss=109.0839
	step [16/146], loss=112.9672
	step [17/146], loss=108.2598
	step [18/146], loss=127.4088
	step [19/146], loss=110.0294
	step [20/146], loss=101.5859
	step [21/146], loss=120.2613
	step [22/146], loss=97.4530
	step [23/146], loss=117.7679
	step [24/146], loss=114.6931
	step [25/146], loss=101.3502
	step [26/146], loss=112.5261
	step [27/146], loss=95.7934
	step [28/146], loss=125.2798
	step [29/146], loss=102.9144
	step [30/146], loss=123.0108
	step [31/146], loss=110.6570
	step [32/146], loss=90.6237
	step [33/146], loss=110.9467
	step [34/146], loss=79.3650
	step [35/146], loss=115.3056
	step [36/146], loss=101.5439
	step [37/146], loss=107.8628
	step [38/146], loss=118.4743
	step [39/146], loss=94.4329
	step [40/146], loss=121.0766
	step [41/146], loss=94.0380
	step [42/146], loss=103.3090
	step [43/146], loss=109.5786
	step [44/146], loss=92.5387
	step [45/146], loss=90.4128
	step [46/146], loss=107.1934
	step [47/146], loss=91.3500
	step [48/146], loss=114.4663
	step [49/146], loss=104.5304
	step [50/146], loss=105.2982
	step [51/146], loss=100.7480
	step [52/146], loss=123.2903
	step [53/146], loss=93.9950
	step [54/146], loss=108.0078
	step [55/146], loss=103.5867
	step [56/146], loss=88.8562
	step [57/146], loss=102.7140
	step [58/146], loss=111.8098
	step [59/146], loss=116.2630
	step [60/146], loss=110.9266
	step [61/146], loss=109.1882
	step [62/146], loss=107.6176
	step [63/146], loss=114.4773
	step [64/146], loss=89.8835
	step [65/146], loss=124.7018
	step [66/146], loss=118.6481
	step [67/146], loss=94.0217
	step [68/146], loss=102.7283
	step [69/146], loss=112.4133
	step [70/146], loss=98.7034
	step [71/146], loss=113.0745
	step [72/146], loss=104.2550
	step [73/146], loss=113.4113
	step [74/146], loss=120.1009
	step [75/146], loss=133.0190
	step [76/146], loss=122.5165
	step [77/146], loss=131.3115
	step [78/146], loss=96.3665
	step [79/146], loss=116.4674
	step [80/146], loss=109.4572
	step [81/146], loss=114.9716
	step [82/146], loss=105.8067
	step [83/146], loss=104.4810
	step [84/146], loss=124.0821
	step [85/146], loss=102.6042
	step [86/146], loss=94.1216
	step [87/146], loss=97.3722
	step [88/146], loss=109.9583
	step [89/146], loss=103.0314
	step [90/146], loss=91.9565
	step [91/146], loss=100.3345
	step [92/146], loss=107.5637
	step [93/146], loss=130.5288
	step [94/146], loss=108.5710
	step [95/146], loss=82.3218
	step [96/146], loss=105.9429
	step [97/146], loss=118.8761
	step [98/146], loss=98.4841
	step [99/146], loss=112.0137
	step [100/146], loss=84.9125
	step [101/146], loss=105.2111
	step [102/146], loss=109.1371
	step [103/146], loss=107.9877
	step [104/146], loss=129.0802
	step [105/146], loss=91.8589
	step [106/146], loss=99.8076
	step [107/146], loss=90.3532
	step [108/146], loss=109.7209
	step [109/146], loss=105.5438
	step [110/146], loss=108.5810
	step [111/146], loss=122.7127
	step [112/146], loss=104.0947
	step [113/146], loss=95.0965
	step [114/146], loss=106.1801
	step [115/146], loss=113.3442
	step [116/146], loss=109.0910
	step [117/146], loss=105.7275
	step [118/146], loss=96.5140
	step [119/146], loss=103.3654
	step [120/146], loss=96.8449
	step [121/146], loss=107.9405
	step [122/146], loss=95.2578
	step [123/146], loss=96.8147
	step [124/146], loss=86.6167
	step [125/146], loss=97.3815
	step [126/146], loss=107.5467
	step [127/146], loss=109.4802
	step [128/146], loss=116.0068
	step [129/146], loss=116.3554
	step [130/146], loss=120.9411
	step [131/146], loss=90.1814
	step [132/146], loss=107.7423
	step [133/146], loss=109.8832
	step [134/146], loss=89.6823
	step [135/146], loss=98.6746
	step [136/146], loss=96.3686
	step [137/146], loss=95.8453
	step [138/146], loss=96.0618
	step [139/146], loss=111.8069
	step [140/146], loss=105.6852
	step [141/146], loss=111.2437
	step [142/146], loss=119.5052
	step [143/146], loss=100.9601
	step [144/146], loss=117.5140
	step [145/146], loss=103.0215
	step [146/146], loss=71.8958
	Evaluating
	loss=0.0236, precision=0.3367, recall=0.8880, f1=0.4883
Training epoch 22
	step [1/146], loss=96.9348
	step [2/146], loss=107.5275
	step [3/146], loss=100.4830
	step [4/146], loss=104.0152
	step [5/146], loss=95.0246
	step [6/146], loss=103.2353
	step [7/146], loss=112.0520
	step [8/146], loss=119.7550
	step [9/146], loss=101.0237
	step [10/146], loss=105.3172
	step [11/146], loss=108.5628
	step [12/146], loss=105.3725
	step [13/146], loss=95.3862
	step [14/146], loss=118.9666
	step [15/146], loss=119.9412
	step [16/146], loss=90.3195
	step [17/146], loss=106.4941
	step [18/146], loss=113.2277
	step [19/146], loss=100.1316
	step [20/146], loss=91.7562
	step [21/146], loss=119.0988
	step [22/146], loss=97.5783
	step [23/146], loss=93.7256
	step [24/146], loss=102.6052
	step [25/146], loss=112.2296
	step [26/146], loss=109.0912
	step [27/146], loss=92.4801
	step [28/146], loss=103.6710
	step [29/146], loss=90.5140
	step [30/146], loss=109.9484
	step [31/146], loss=97.0133
	step [32/146], loss=97.6777
	step [33/146], loss=118.6973
	step [34/146], loss=115.0896
	step [35/146], loss=91.5436
	step [36/146], loss=124.2596
	step [37/146], loss=116.9354
	step [38/146], loss=111.9689
	step [39/146], loss=112.4519
	step [40/146], loss=85.7998
	step [41/146], loss=108.1576
	step [42/146], loss=93.1019
	step [43/146], loss=115.7309
	step [44/146], loss=89.0527
	step [45/146], loss=103.5548
	step [46/146], loss=98.1854
	step [47/146], loss=115.5593
	step [48/146], loss=113.1190
	step [49/146], loss=121.2441
	step [50/146], loss=95.7833
	step [51/146], loss=115.4680
	step [52/146], loss=145.4691
	step [53/146], loss=97.3720
	step [54/146], loss=95.2934
	step [55/146], loss=87.5308
	step [56/146], loss=99.0842
	step [57/146], loss=102.7712
	step [58/146], loss=127.2703
	step [59/146], loss=107.3282
	step [60/146], loss=119.7503
	step [61/146], loss=112.4482
	step [62/146], loss=106.8421
	step [63/146], loss=91.1689
	step [64/146], loss=102.7730
	step [65/146], loss=98.0065
	step [66/146], loss=104.5012
	step [67/146], loss=110.1766
	step [68/146], loss=94.4790
	step [69/146], loss=116.4688
	step [70/146], loss=108.9513
	step [71/146], loss=93.4586
	step [72/146], loss=98.5643
	step [73/146], loss=129.0844
	step [74/146], loss=118.3253
	step [75/146], loss=105.6584
	step [76/146], loss=106.2441
	step [77/146], loss=101.7603
	step [78/146], loss=104.8068
	step [79/146], loss=121.8183
	step [80/146], loss=105.5658
	step [81/146], loss=111.9311
	step [82/146], loss=115.2666
	step [83/146], loss=104.2356
	step [84/146], loss=86.2294
	step [85/146], loss=99.2532
	step [86/146], loss=118.6621
	step [87/146], loss=108.8410
	step [88/146], loss=95.3728
	step [89/146], loss=104.0219
	step [90/146], loss=85.6159
	step [91/146], loss=112.8506
	step [92/146], loss=104.4715
	step [93/146], loss=101.4341
	step [94/146], loss=84.0129
	step [95/146], loss=115.3583
	step [96/146], loss=101.2973
	step [97/146], loss=104.7901
	step [98/146], loss=116.2046
	step [99/146], loss=106.3970
	step [100/146], loss=105.1016
	step [101/146], loss=115.8660
	step [102/146], loss=103.2622
	step [103/146], loss=88.9326
	step [104/146], loss=88.0806
	step [105/146], loss=87.6881
	step [106/146], loss=102.0085
	step [107/146], loss=106.5288
	step [108/146], loss=118.5005
	step [109/146], loss=99.8741
	step [110/146], loss=95.1692
	step [111/146], loss=124.4232
	step [112/146], loss=97.2335
	step [113/146], loss=110.3020
	step [114/146], loss=120.4196
	step [115/146], loss=103.7696
	step [116/146], loss=110.7650
	step [117/146], loss=119.2711
	step [118/146], loss=84.4783
	step [119/146], loss=84.6772
	step [120/146], loss=104.2824
	step [121/146], loss=97.6813
	step [122/146], loss=103.7489
	step [123/146], loss=107.6014
	step [124/146], loss=96.8447
	step [125/146], loss=113.7619
	step [126/146], loss=109.7137
	step [127/146], loss=139.4377
	step [128/146], loss=108.2933
	step [129/146], loss=106.9564
	step [130/146], loss=93.5550
	step [131/146], loss=107.4018
	step [132/146], loss=117.3061
	step [133/146], loss=113.2304
	step [134/146], loss=97.9680
	step [135/146], loss=101.7961
	step [136/146], loss=110.8030
	step [137/146], loss=110.1141
	step [138/146], loss=96.1217
	step [139/146], loss=95.3375
	step [140/146], loss=115.6559
	step [141/146], loss=119.8813
	step [142/146], loss=116.4469
	step [143/146], loss=119.0074
	step [144/146], loss=103.8887
	step [145/146], loss=89.1227
	step [146/146], loss=48.4481
	Evaluating
	loss=0.0234, precision=0.3353, recall=0.9287, f1=0.4927
Training epoch 23
	step [1/146], loss=89.8947
	step [2/146], loss=121.6663
	step [3/146], loss=107.2369
	step [4/146], loss=113.5063
	step [5/146], loss=104.6502
	step [6/146], loss=101.5472
	step [7/146], loss=117.7578
	step [8/146], loss=118.1738
	step [9/146], loss=108.7195
	step [10/146], loss=115.9827
	step [11/146], loss=106.5719
	step [12/146], loss=93.9465
	step [13/146], loss=91.4873
	step [14/146], loss=83.9317
	step [15/146], loss=93.9046
	step [16/146], loss=88.2712
	step [17/146], loss=95.6733
	step [18/146], loss=107.6481
	step [19/146], loss=106.1950
	step [20/146], loss=96.7761
	step [21/146], loss=105.8766
	step [22/146], loss=101.2009
	step [23/146], loss=115.5976
	step [24/146], loss=115.9728
	step [25/146], loss=109.3501
	step [26/146], loss=112.7145
	step [27/146], loss=98.6010
	step [28/146], loss=104.8726
	step [29/146], loss=96.9523
	step [30/146], loss=102.1442
	step [31/146], loss=99.0480
	step [32/146], loss=121.9181
	step [33/146], loss=86.2318
	step [34/146], loss=116.7837
	step [35/146], loss=104.5795
	step [36/146], loss=92.1410
	step [37/146], loss=99.0654
	step [38/146], loss=108.0088
	step [39/146], loss=110.8279
	step [40/146], loss=104.5938
	step [41/146], loss=85.5766
	step [42/146], loss=108.1450
	step [43/146], loss=84.6066
	step [44/146], loss=100.9331
	step [45/146], loss=92.1500
	step [46/146], loss=100.3950
	step [47/146], loss=121.0748
	step [48/146], loss=105.6407
	step [49/146], loss=112.6734
	step [50/146], loss=106.5098
	step [51/146], loss=129.8803
	step [52/146], loss=97.0630
	step [53/146], loss=88.9491
	step [54/146], loss=103.2146
	step [55/146], loss=97.3289
	step [56/146], loss=114.4770
	step [57/146], loss=95.3880
	step [58/146], loss=102.8148
	step [59/146], loss=103.7822
	step [60/146], loss=110.7542
	step [61/146], loss=93.3291
	step [62/146], loss=87.6742
	step [63/146], loss=102.1331
	step [64/146], loss=94.8001
	step [65/146], loss=112.1746
	step [66/146], loss=105.0079
	step [67/146], loss=122.0834
	step [68/146], loss=115.0942
	step [69/146], loss=101.4361
	step [70/146], loss=87.5427
	step [71/146], loss=110.1712
	step [72/146], loss=101.4621
	step [73/146], loss=107.9065
	step [74/146], loss=108.8980
	step [75/146], loss=92.4107
	step [76/146], loss=103.2453
	step [77/146], loss=119.9043
	step [78/146], loss=118.3245
	step [79/146], loss=105.3029
	step [80/146], loss=98.1002
	step [81/146], loss=97.8360
	step [82/146], loss=95.9164
	step [83/146], loss=97.9813
	step [84/146], loss=109.9845
	step [85/146], loss=103.8526
	step [86/146], loss=103.6438
	step [87/146], loss=90.5423
	step [88/146], loss=115.3218
	step [89/146], loss=104.9551
	step [90/146], loss=103.5333
	step [91/146], loss=98.2190
	step [92/146], loss=118.5695
	step [93/146], loss=124.2276
	step [94/146], loss=106.8928
	step [95/146], loss=108.4883
	step [96/146], loss=94.0111
	step [97/146], loss=85.8589
	step [98/146], loss=112.2937
	step [99/146], loss=99.6826
	step [100/146], loss=107.3886
	step [101/146], loss=104.1392
	step [102/146], loss=99.7551
	step [103/146], loss=94.9515
	step [104/146], loss=109.4663
	step [105/146], loss=107.9602
	step [106/146], loss=116.7998
	step [107/146], loss=103.4389
	step [108/146], loss=91.8434
	step [109/146], loss=105.8885
	step [110/146], loss=74.7689
	step [111/146], loss=104.1072
	step [112/146], loss=98.5193
	step [113/146], loss=90.3558
	step [114/146], loss=117.8480
	step [115/146], loss=115.3447
	step [116/146], loss=91.7829
	step [117/146], loss=107.8958
	step [118/146], loss=111.6536
	step [119/146], loss=90.1577
	step [120/146], loss=88.0875
	step [121/146], loss=110.5607
	step [122/146], loss=105.5190
	step [123/146], loss=109.2965
	step [124/146], loss=110.8530
	step [125/146], loss=102.6488
	step [126/146], loss=107.1839
	step [127/146], loss=94.3895
	step [128/146], loss=90.8803
	step [129/146], loss=119.8445
	step [130/146], loss=88.5689
	step [131/146], loss=121.1827
	step [132/146], loss=113.5777
	step [133/146], loss=127.1790
	step [134/146], loss=115.0748
	step [135/146], loss=117.5901
	step [136/146], loss=99.9237
	step [137/146], loss=94.9243
	step [138/146], loss=96.5297
	step [139/146], loss=95.7186
	step [140/146], loss=110.2919
	step [141/146], loss=107.8248
	step [142/146], loss=106.7533
	step [143/146], loss=103.2829
	step [144/146], loss=83.0478
	step [145/146], loss=110.6232
	step [146/146], loss=47.0945
	Evaluating
	loss=0.0213, precision=0.3597, recall=0.9201, f1=0.5172
Training epoch 24
	step [1/146], loss=108.9141
	step [2/146], loss=105.0668
	step [3/146], loss=107.8409
	step [4/146], loss=87.4798
	step [5/146], loss=102.4219
	step [6/146], loss=100.3067
	step [7/146], loss=105.4082
	step [8/146], loss=101.9773
	step [9/146], loss=122.8202
	step [10/146], loss=101.7940
	step [11/146], loss=88.8686
	step [12/146], loss=105.0716
	step [13/146], loss=106.1754
	step [14/146], loss=109.7261
	step [15/146], loss=98.7693
	step [16/146], loss=112.9847
	step [17/146], loss=109.6555
	step [18/146], loss=89.9830
	step [19/146], loss=96.9554
	step [20/146], loss=103.5279
	step [21/146], loss=92.5208
	step [22/146], loss=114.1180
	step [23/146], loss=99.1668
	step [24/146], loss=87.2436
	step [25/146], loss=105.2871
	step [26/146], loss=105.1314
	step [27/146], loss=117.4523
	step [28/146], loss=112.4123
	step [29/146], loss=103.6164
	step [30/146], loss=105.1756
	step [31/146], loss=92.3589
	step [32/146], loss=111.2043
	step [33/146], loss=118.7815
	step [34/146], loss=99.2810
	step [35/146], loss=116.1581
	step [36/146], loss=103.2089
	step [37/146], loss=111.6559
	step [38/146], loss=108.6688
	step [39/146], loss=104.6676
	step [40/146], loss=109.8603
	step [41/146], loss=99.0057
	step [42/146], loss=107.6245
	step [43/146], loss=90.9064
	step [44/146], loss=98.8446
	step [45/146], loss=96.7036
	step [46/146], loss=108.6717
	step [47/146], loss=117.0050
	step [48/146], loss=95.9504
	step [49/146], loss=100.5037
	step [50/146], loss=101.1910
	step [51/146], loss=107.2052
	step [52/146], loss=88.9023
	step [53/146], loss=93.1130
	step [54/146], loss=112.7931
	step [55/146], loss=115.5156
	step [56/146], loss=91.8848
	step [57/146], loss=99.4512
	step [58/146], loss=112.6414
	step [59/146], loss=121.7759
	step [60/146], loss=100.9889
	step [61/146], loss=69.6324
	step [62/146], loss=105.5715
	step [63/146], loss=95.7257
	step [64/146], loss=111.4954
	step [65/146], loss=118.5139
	step [66/146], loss=93.4039
	step [67/146], loss=103.3499
	step [68/146], loss=94.0924
	step [69/146], loss=109.7877
	step [70/146], loss=119.2566
	step [71/146], loss=79.3772
	step [72/146], loss=112.3048
	step [73/146], loss=112.1559
	step [74/146], loss=107.3619
	step [75/146], loss=94.9365
	step [76/146], loss=119.3472
	step [77/146], loss=84.9264
	step [78/146], loss=82.7432
	step [79/146], loss=119.4205
	step [80/146], loss=87.7841
	step [81/146], loss=105.2157
	step [82/146], loss=112.0334
	step [83/146], loss=88.3854
	step [84/146], loss=97.5385
	step [85/146], loss=104.6024
	step [86/146], loss=99.5569
	step [87/146], loss=114.4471
	step [88/146], loss=100.3485
	step [89/146], loss=91.9530
	step [90/146], loss=95.1631
	step [91/146], loss=108.0243
	step [92/146], loss=110.7785
	step [93/146], loss=101.8395
	step [94/146], loss=102.2363
	step [95/146], loss=106.0719
	step [96/146], loss=91.7000
	step [97/146], loss=93.7030
	step [98/146], loss=107.8305
	step [99/146], loss=101.9000
	step [100/146], loss=105.2472
	step [101/146], loss=102.7123
	step [102/146], loss=113.3439
	step [103/146], loss=105.3286
	step [104/146], loss=103.6725
	step [105/146], loss=100.8400
	step [106/146], loss=109.4167
	step [107/146], loss=103.2404
	step [108/146], loss=103.9181
	step [109/146], loss=86.5103
	step [110/146], loss=114.9492
	step [111/146], loss=98.9421
	step [112/146], loss=101.2830
	step [113/146], loss=90.7507
	step [114/146], loss=103.5875
	step [115/146], loss=107.3577
	step [116/146], loss=113.9314
	step [117/146], loss=97.5692
	step [118/146], loss=111.3838
	step [119/146], loss=106.4832
	step [120/146], loss=87.7054
	step [121/146], loss=107.6568
	step [122/146], loss=100.1725
	step [123/146], loss=82.1808
	step [124/146], loss=88.4503
	step [125/146], loss=102.5368
	step [126/146], loss=89.2746
	step [127/146], loss=114.3453
	step [128/146], loss=96.4171
	step [129/146], loss=103.3615
	step [130/146], loss=89.2031
	step [131/146], loss=118.9642
	step [132/146], loss=118.3171
	step [133/146], loss=98.4472
	step [134/146], loss=93.6645
	step [135/146], loss=107.4389
	step [136/146], loss=87.5905
	step [137/146], loss=112.7086
	step [138/146], loss=109.9610
	step [139/146], loss=118.7037
	step [140/146], loss=120.4941
	step [141/146], loss=107.9990
	step [142/146], loss=84.9360
	step [143/146], loss=92.1225
	step [144/146], loss=90.9515
	step [145/146], loss=89.7248
	step [146/146], loss=55.1746
	Evaluating
	loss=0.0202, precision=0.3910, recall=0.9261, f1=0.5499
Training epoch 25
	step [1/146], loss=98.9400
	step [2/146], loss=93.9432
	step [3/146], loss=80.5317
	step [4/146], loss=87.9386
	step [5/146], loss=113.5093
	step [6/146], loss=103.1050
	step [7/146], loss=82.9056
	step [8/146], loss=104.6832
	step [9/146], loss=99.4240
	step [10/146], loss=117.4762
	step [11/146], loss=105.2387
	step [12/146], loss=110.6497
	step [13/146], loss=108.3852
	step [14/146], loss=87.5195
	step [15/146], loss=96.3371
	step [16/146], loss=109.9090
	step [17/146], loss=105.7920
	step [18/146], loss=113.1651
	step [19/146], loss=118.3151
	step [20/146], loss=102.4266
	step [21/146], loss=113.3902
	step [22/146], loss=103.8928
	step [23/146], loss=122.9897
	step [24/146], loss=96.0386
	step [25/146], loss=105.9232
	step [26/146], loss=85.5908
	step [27/146], loss=99.4302
	step [28/146], loss=123.9984
	step [29/146], loss=95.1752
	step [30/146], loss=106.9641
	step [31/146], loss=95.4759
	step [32/146], loss=105.0019
	step [33/146], loss=103.1434
	step [34/146], loss=105.0878
	step [35/146], loss=111.2601
	step [36/146], loss=103.6307
	step [37/146], loss=88.6921
	step [38/146], loss=113.5373
	step [39/146], loss=101.5677
	step [40/146], loss=102.9033
	step [41/146], loss=111.6387
	step [42/146], loss=105.0097
	step [43/146], loss=116.9495
	step [44/146], loss=101.5529
	step [45/146], loss=103.3669
	step [46/146], loss=102.6012
	step [47/146], loss=115.6960
	step [48/146], loss=81.6182
	step [49/146], loss=115.3941
	step [50/146], loss=109.6911
	step [51/146], loss=95.8659
	step [52/146], loss=89.0954
	step [53/146], loss=118.2671
	step [54/146], loss=97.5909
	step [55/146], loss=107.9919
	step [56/146], loss=101.9843
	step [57/146], loss=114.0288
	step [58/146], loss=101.5134
	step [59/146], loss=84.5455
	step [60/146], loss=123.6934
	step [61/146], loss=107.5274
	step [62/146], loss=105.3026
	step [63/146], loss=102.5467
	step [64/146], loss=94.0770
	step [65/146], loss=97.0724
	step [66/146], loss=109.2138
	step [67/146], loss=122.5448
	step [68/146], loss=112.4227
	step [69/146], loss=95.8957
	step [70/146], loss=99.2578
	step [71/146], loss=125.0397
	step [72/146], loss=104.7520
	step [73/146], loss=83.6956
	step [74/146], loss=93.8420
	step [75/146], loss=125.6385
	step [76/146], loss=99.2904
	step [77/146], loss=63.9724
	step [78/146], loss=100.4041
	step [79/146], loss=79.1006
	step [80/146], loss=95.5650
	step [81/146], loss=100.1879
	step [82/146], loss=107.5028
	step [83/146], loss=124.8932
	step [84/146], loss=99.8879
	step [85/146], loss=99.7325
	step [86/146], loss=105.8650
	step [87/146], loss=105.5422
	step [88/146], loss=93.3939
	step [89/146], loss=98.8724
	step [90/146], loss=114.5243
	step [91/146], loss=100.4130
	step [92/146], loss=87.9564
	step [93/146], loss=109.2796
	step [94/146], loss=100.8548
	step [95/146], loss=77.9209
	step [96/146], loss=96.5049
	step [97/146], loss=89.4711
	step [98/146], loss=99.3372
	step [99/146], loss=101.7947
	step [100/146], loss=106.8670
	step [101/146], loss=97.2513
	step [102/146], loss=98.0493
	step [103/146], loss=113.5598
	step [104/146], loss=110.1831
	step [105/146], loss=87.2237
	step [106/146], loss=99.8416
	step [107/146], loss=109.3822
	step [108/146], loss=98.6648
	step [109/146], loss=110.4053
	step [110/146], loss=80.7754
	step [111/146], loss=94.5052
	step [112/146], loss=95.4053
	step [113/146], loss=95.5140
	step [114/146], loss=97.4592
	step [115/146], loss=105.2254
	step [116/146], loss=96.8209
	step [117/146], loss=113.4409
	step [118/146], loss=121.0590
	step [119/146], loss=89.3371
	step [120/146], loss=123.4359
	step [121/146], loss=106.5299
	step [122/146], loss=93.1803
	step [123/146], loss=102.3819
	step [124/146], loss=92.4286
	step [125/146], loss=97.9681
	step [126/146], loss=77.9212
	step [127/146], loss=102.2377
	step [128/146], loss=101.3282
	step [129/146], loss=119.1883
	step [130/146], loss=86.2983
	step [131/146], loss=109.0403
	step [132/146], loss=94.6835
	step [133/146], loss=109.8556
	step [134/146], loss=105.6952
	step [135/146], loss=113.0521
	step [136/146], loss=87.6588
	step [137/146], loss=114.0134
	step [138/146], loss=90.5293
	step [139/146], loss=99.1679
	step [140/146], loss=97.0053
	step [141/146], loss=100.2590
	step [142/146], loss=101.2614
	step [143/146], loss=89.8180
	step [144/146], loss=92.4793
	step [145/146], loss=112.0430
	step [146/146], loss=54.7793
	Evaluating
	loss=0.0154, precision=0.4778, recall=0.9272, f1=0.6306
Training epoch 26
	step [1/146], loss=123.0087
	step [2/146], loss=101.0442
	step [3/146], loss=97.7840
	step [4/146], loss=109.0778
	step [5/146], loss=94.9762
	step [6/146], loss=103.3131
	step [7/146], loss=107.0861
	step [8/146], loss=98.0882
	step [9/146], loss=109.1838
	step [10/146], loss=94.8433
	step [11/146], loss=109.2165
	step [12/146], loss=105.6835
	step [13/146], loss=108.3848
	step [14/146], loss=108.5913
	step [15/146], loss=97.9946
	step [16/146], loss=86.3838
	step [17/146], loss=97.0321
	step [18/146], loss=96.0521
	step [19/146], loss=110.6093
	step [20/146], loss=102.1646
	step [21/146], loss=93.3027
	step [22/146], loss=119.1666
	step [23/146], loss=106.0560
	step [24/146], loss=98.1825
	step [25/146], loss=103.8255
	step [26/146], loss=85.7202
	step [27/146], loss=105.5084
	step [28/146], loss=93.2945
	step [29/146], loss=92.2342
	step [30/146], loss=111.3157
	step [31/146], loss=92.0329
	step [32/146], loss=90.6138
	step [33/146], loss=98.7440
	step [34/146], loss=96.0502
	step [35/146], loss=107.2250
	step [36/146], loss=128.4621
	step [37/146], loss=100.9582
	step [38/146], loss=107.3487
	step [39/146], loss=85.7309
	step [40/146], loss=107.1210
	step [41/146], loss=101.0180
	step [42/146], loss=83.8649
	step [43/146], loss=98.7452
	step [44/146], loss=99.7652
	step [45/146], loss=104.4515
	step [46/146], loss=106.9917
	step [47/146], loss=96.6462
	step [48/146], loss=97.0500
	step [49/146], loss=106.8293
	step [50/146], loss=82.9108
	step [51/146], loss=110.8141
	step [52/146], loss=114.5182
	step [53/146], loss=88.8222
	step [54/146], loss=92.7282
	step [55/146], loss=99.5571
	step [56/146], loss=102.1202
	step [57/146], loss=88.8443
	step [58/146], loss=97.7686
	step [59/146], loss=77.5039
	step [60/146], loss=91.5572
	step [61/146], loss=101.0765
	step [62/146], loss=88.3447
	step [63/146], loss=97.2753
	step [64/146], loss=106.3288
	step [65/146], loss=96.9236
	step [66/146], loss=98.6639
	step [67/146], loss=110.4293
	step [68/146], loss=107.5549
	step [69/146], loss=96.3980
	step [70/146], loss=68.8846
	step [71/146], loss=104.6443
	step [72/146], loss=98.2277
	step [73/146], loss=112.1850
	step [74/146], loss=80.6079
	step [75/146], loss=98.8489
	step [76/146], loss=115.3863
	step [77/146], loss=95.7327
	step [78/146], loss=91.2466
	step [79/146], loss=105.9034
	step [80/146], loss=96.2021
	step [81/146], loss=96.9141
	step [82/146], loss=92.4734
	step [83/146], loss=99.2994
	step [84/146], loss=106.9776
	step [85/146], loss=97.3062
	step [86/146], loss=91.3441
	step [87/146], loss=117.8574
	step [88/146], loss=109.4493
	step [89/146], loss=98.0925
	step [90/146], loss=108.8275
	step [91/146], loss=118.8474
	step [92/146], loss=118.1343
	step [93/146], loss=99.8242
	step [94/146], loss=93.7514
	step [95/146], loss=88.4870
	step [96/146], loss=101.7900
	step [97/146], loss=103.1765
	step [98/146], loss=96.2865
	step [99/146], loss=125.3997
	step [100/146], loss=93.2604
	step [101/146], loss=97.2796
	step [102/146], loss=107.0995
	step [103/146], loss=108.4516
	step [104/146], loss=87.2611
	step [105/146], loss=98.0861
	step [106/146], loss=97.3764
	step [107/146], loss=79.9120
	step [108/146], loss=97.3502
	step [109/146], loss=85.5469
	step [110/146], loss=110.7192
	step [111/146], loss=106.7815
	step [112/146], loss=82.4934
	step [113/146], loss=120.1194
	step [114/146], loss=114.9947
	step [115/146], loss=90.4808
	step [116/146], loss=93.0443
	step [117/146], loss=117.3432
	step [118/146], loss=99.9950
	step [119/146], loss=102.9910
	step [120/146], loss=107.1454
	step [121/146], loss=103.7756
	step [122/146], loss=119.0955
	step [123/146], loss=99.1840
	step [124/146], loss=103.2495
	step [125/146], loss=95.2785
	step [126/146], loss=104.7801
	step [127/146], loss=118.7743
	step [128/146], loss=83.5774
	step [129/146], loss=96.2384
	step [130/146], loss=105.0103
	step [131/146], loss=102.7101
	step [132/146], loss=85.7195
	step [133/146], loss=104.1748
	step [134/146], loss=113.8293
	step [135/146], loss=99.8833
	step [136/146], loss=119.5331
	step [137/146], loss=100.7533
	step [138/146], loss=99.3047
	step [139/146], loss=106.4016
	step [140/146], loss=95.4852
	step [141/146], loss=101.2047
	step [142/146], loss=88.6528
	step [143/146], loss=92.9203
	step [144/146], loss=106.2625
	step [145/146], loss=120.2448
	step [146/146], loss=51.2955
	Evaluating
	loss=0.0171, precision=0.4228, recall=0.9237, f1=0.5801
Training epoch 27
	step [1/146], loss=98.5728
	step [2/146], loss=95.0534
	step [3/146], loss=106.1219
	step [4/146], loss=96.2109
	step [5/146], loss=101.6883
	step [6/146], loss=112.9207
	step [7/146], loss=109.6094
	step [8/146], loss=104.2431
	step [9/146], loss=83.0522
	step [10/146], loss=90.2177
	step [11/146], loss=99.4233
	step [12/146], loss=121.6986
	step [13/146], loss=108.2393
	step [14/146], loss=107.4109
	step [15/146], loss=82.7384
	step [16/146], loss=117.3657
	step [17/146], loss=108.7661
	step [18/146], loss=95.2914
	step [19/146], loss=102.8055
	step [20/146], loss=109.3368
	step [21/146], loss=92.2876
	step [22/146], loss=84.0264
	step [23/146], loss=109.6525
	step [24/146], loss=74.6831
	step [25/146], loss=101.0916
	step [26/146], loss=79.4555
	step [27/146], loss=97.1040
	step [28/146], loss=102.8137
	step [29/146], loss=97.0277
	step [30/146], loss=102.6074
	step [31/146], loss=102.8725
	step [32/146], loss=103.1582
	step [33/146], loss=97.7330
	step [34/146], loss=85.5942
	step [35/146], loss=113.2139
	step [36/146], loss=100.9732
	step [37/146], loss=114.1029
	step [38/146], loss=90.0589
	step [39/146], loss=97.0745
	step [40/146], loss=89.4181
	step [41/146], loss=104.2841
	step [42/146], loss=97.7779
	step [43/146], loss=104.4899
	step [44/146], loss=89.0459
	step [45/146], loss=106.0034
	step [46/146], loss=92.2594
	step [47/146], loss=96.3146
	step [48/146], loss=99.5360
	step [49/146], loss=110.5652
	step [50/146], loss=121.5944
	step [51/146], loss=98.0569
	step [52/146], loss=102.7210
	step [53/146], loss=90.1343
	step [54/146], loss=102.7062
	step [55/146], loss=91.1126
	step [56/146], loss=98.8039
	step [57/146], loss=94.2766
	step [58/146], loss=93.9082
	step [59/146], loss=108.1846
	step [60/146], loss=106.3658
	step [61/146], loss=77.4419
	step [62/146], loss=95.2186
	step [63/146], loss=88.0293
	step [64/146], loss=96.3287
	step [65/146], loss=107.0000
	step [66/146], loss=84.7360
	step [67/146], loss=100.0043
	step [68/146], loss=105.4336
	step [69/146], loss=110.0636
	step [70/146], loss=107.1799
	step [71/146], loss=105.3169
	step [72/146], loss=111.0964
	step [73/146], loss=101.6088
	step [74/146], loss=110.1173
	step [75/146], loss=95.5905
	step [76/146], loss=107.8068
	step [77/146], loss=88.7369
	step [78/146], loss=104.4728
	step [79/146], loss=101.7994
	step [80/146], loss=111.1779
	step [81/146], loss=100.7552
	step [82/146], loss=85.8965
	step [83/146], loss=94.2699
	step [84/146], loss=100.4814
	step [85/146], loss=94.2807
	step [86/146], loss=94.8889
	step [87/146], loss=112.0193
	step [88/146], loss=92.5978
	step [89/146], loss=93.3505
	step [90/146], loss=73.3436
	step [91/146], loss=106.1581
	step [92/146], loss=110.7094
	step [93/146], loss=93.6666
	step [94/146], loss=114.9425
	step [95/146], loss=98.5169
	step [96/146], loss=88.5083
	step [97/146], loss=95.1231
	step [98/146], loss=110.5040
	step [99/146], loss=94.5061
	step [100/146], loss=100.7329
	step [101/146], loss=98.9306
	step [102/146], loss=97.8071
	step [103/146], loss=95.7094
	step [104/146], loss=95.6311
	step [105/146], loss=101.2820
	step [106/146], loss=111.8070
	step [107/146], loss=95.9695
	step [108/146], loss=85.7603
	step [109/146], loss=77.2581
	step [110/146], loss=86.0554
	step [111/146], loss=117.6656
	step [112/146], loss=111.3587
	step [113/146], loss=109.8309
	step [114/146], loss=92.9550
	step [115/146], loss=88.7289
	step [116/146], loss=98.0207
	step [117/146], loss=112.2457
	step [118/146], loss=92.2908
	step [119/146], loss=104.5062
	step [120/146], loss=95.9632
	step [121/146], loss=112.9180
	step [122/146], loss=97.2779
	step [123/146], loss=94.8000
	step [124/146], loss=88.2374
	step [125/146], loss=97.8820
	step [126/146], loss=108.9585
	step [127/146], loss=98.3366
	step [128/146], loss=85.6905
	step [129/146], loss=109.1879
	step [130/146], loss=95.2244
	step [131/146], loss=83.0860
	step [132/146], loss=91.9758
	step [133/146], loss=100.4540
	step [134/146], loss=103.0376
	step [135/146], loss=105.2823
	step [136/146], loss=107.4464
	step [137/146], loss=108.6132
	step [138/146], loss=102.1049
	step [139/146], loss=95.4427
	step [140/146], loss=109.8350
	step [141/146], loss=85.5059
	step [142/146], loss=99.4753
	step [143/146], loss=112.7891
	step [144/146], loss=92.2445
	step [145/146], loss=91.9224
	step [146/146], loss=47.6978
	Evaluating
	loss=0.0156, precision=0.4539, recall=0.8886, f1=0.6009
Training epoch 28
	step [1/146], loss=104.8625
	step [2/146], loss=91.8500
	step [3/146], loss=118.2034
	step [4/146], loss=90.0132
	step [5/146], loss=103.0427
	step [6/146], loss=93.4115
	step [7/146], loss=106.9664
	step [8/146], loss=99.1031
	step [9/146], loss=81.5727
	step [10/146], loss=88.0809
	step [11/146], loss=107.0899
	step [12/146], loss=105.1475
	step [13/146], loss=125.3908
	step [14/146], loss=102.0586
	step [15/146], loss=94.2110
	step [16/146], loss=97.3693
	step [17/146], loss=89.7082
	step [18/146], loss=97.1784
	step [19/146], loss=106.7640
	step [20/146], loss=98.5281
	step [21/146], loss=100.3656
	step [22/146], loss=78.1960
	step [23/146], loss=91.9805
	step [24/146], loss=80.1721
	step [25/146], loss=96.8578
	step [26/146], loss=94.3624
	step [27/146], loss=96.8867
	step [28/146], loss=102.0953
	step [29/146], loss=106.5905
	step [30/146], loss=81.6450
	step [31/146], loss=102.6120
	step [32/146], loss=117.0339
	step [33/146], loss=89.7054
	step [34/146], loss=99.7732
	step [35/146], loss=87.8080
	step [36/146], loss=105.2793
	step [37/146], loss=99.6167
	step [38/146], loss=101.9433
	step [39/146], loss=109.1728
	step [40/146], loss=87.1919
	step [41/146], loss=84.9703
	step [42/146], loss=102.2561
	step [43/146], loss=92.0945
	step [44/146], loss=93.9769
	step [45/146], loss=122.6357
	step [46/146], loss=99.2536
	step [47/146], loss=93.0922
	step [48/146], loss=107.6087
	step [49/146], loss=85.7153
	step [50/146], loss=98.4662
	step [51/146], loss=104.4243
	step [52/146], loss=96.5378
	step [53/146], loss=101.9591
	step [54/146], loss=100.9216
	step [55/146], loss=87.7677
	step [56/146], loss=93.5178
	step [57/146], loss=98.2299
	step [58/146], loss=99.0231
	step [59/146], loss=85.2195
	step [60/146], loss=90.8776
	step [61/146], loss=104.1333
	step [62/146], loss=107.2327
	step [63/146], loss=100.9433
	step [64/146], loss=90.9584
	step [65/146], loss=108.5002
	step [66/146], loss=105.2539
	step [67/146], loss=95.1447
	step [68/146], loss=102.1393
	step [69/146], loss=97.0368
	step [70/146], loss=96.8769
	step [71/146], loss=99.4046
	step [72/146], loss=101.3150
	step [73/146], loss=121.0630
	step [74/146], loss=95.5129
	step [75/146], loss=95.1293
	step [76/146], loss=110.7434
	step [77/146], loss=93.5148
	step [78/146], loss=97.7640
	step [79/146], loss=99.5817
	step [80/146], loss=103.2060
	step [81/146], loss=93.3850
	step [82/146], loss=99.1810
	step [83/146], loss=83.5682
	step [84/146], loss=95.1578
	step [85/146], loss=113.9648
	step [86/146], loss=105.2380
	step [87/146], loss=78.4249
	step [88/146], loss=97.2719
	step [89/146], loss=74.8526
	step [90/146], loss=102.6204
	step [91/146], loss=96.2601
	step [92/146], loss=92.8706
	step [93/146], loss=91.1586
	step [94/146], loss=88.2555
	step [95/146], loss=101.5472
	step [96/146], loss=99.4128
	step [97/146], loss=90.4995
	step [98/146], loss=91.3130
	step [99/146], loss=107.8510
	step [100/146], loss=111.1204
	step [101/146], loss=83.6366
	step [102/146], loss=111.8998
	step [103/146], loss=95.9267
	step [104/146], loss=110.3261
	step [105/146], loss=95.8661
	step [106/146], loss=102.5297
	step [107/146], loss=100.0036
	step [108/146], loss=114.4651
	step [109/146], loss=88.6037
	step [110/146], loss=99.4388
	step [111/146], loss=107.4284
	step [112/146], loss=89.6434
	step [113/146], loss=105.8589
	step [114/146], loss=82.2342
	step [115/146], loss=107.9454
	step [116/146], loss=91.5527
	step [117/146], loss=88.2681
	step [118/146], loss=117.0467
	step [119/146], loss=103.3403
	step [120/146], loss=105.6605
	step [121/146], loss=94.4537
	step [122/146], loss=102.6596
	step [123/146], loss=107.7699
	step [124/146], loss=110.0260
	step [125/146], loss=98.2999
	step [126/146], loss=86.3025
	step [127/146], loss=81.0898
	step [128/146], loss=119.4975
	step [129/146], loss=89.0636
	step [130/146], loss=91.7615
	step [131/146], loss=97.6027
	step [132/146], loss=94.7703
	step [133/146], loss=107.4836
	step [134/146], loss=112.2375
	step [135/146], loss=90.7355
	step [136/146], loss=78.2128
	step [137/146], loss=101.1814
	step [138/146], loss=99.7810
	step [139/146], loss=76.4439
	step [140/146], loss=89.7770
	step [141/146], loss=98.9731
	step [142/146], loss=105.5065
	step [143/146], loss=85.7233
	step [144/146], loss=79.3708
	step [145/146], loss=107.7253
	step [146/146], loss=58.2991
	Evaluating
	loss=0.0141, precision=0.4521, recall=0.9059, f1=0.6031
Training epoch 29
	step [1/146], loss=105.6535
	step [2/146], loss=89.1829
	step [3/146], loss=113.9737
	step [4/146], loss=120.3933
	step [5/146], loss=104.9150
	step [6/146], loss=97.5715
	step [7/146], loss=101.9986
	step [8/146], loss=111.3634
	step [9/146], loss=94.3918
	step [10/146], loss=100.9048
	step [11/146], loss=101.8156
	step [12/146], loss=87.2663
	step [13/146], loss=131.1698
	step [14/146], loss=81.5616
	step [15/146], loss=87.5261
	step [16/146], loss=84.4855
	step [17/146], loss=96.5963
	step [18/146], loss=87.0489
	step [19/146], loss=113.1730
	step [20/146], loss=95.1164
	step [21/146], loss=106.8210
	step [22/146], loss=88.5328
	step [23/146], loss=88.0325
	step [24/146], loss=111.5075
	step [25/146], loss=127.7656
	step [26/146], loss=88.7762
	step [27/146], loss=96.6691
	step [28/146], loss=108.2417
	step [29/146], loss=106.8987
	step [30/146], loss=94.1765
	step [31/146], loss=96.5073
	step [32/146], loss=101.2388
	step [33/146], loss=89.6248
	step [34/146], loss=113.3350
	step [35/146], loss=89.4257
	step [36/146], loss=83.6416
	step [37/146], loss=105.5360
	step [38/146], loss=91.6865
	step [39/146], loss=93.9676
	step [40/146], loss=77.0310
	step [41/146], loss=86.5350
	step [42/146], loss=93.6144
	step [43/146], loss=92.0260
	step [44/146], loss=94.3547
	step [45/146], loss=89.7432
	step [46/146], loss=95.9774
	step [47/146], loss=91.1403
	step [48/146], loss=116.5089
	step [49/146], loss=81.0588
	step [50/146], loss=76.7374
	step [51/146], loss=96.1620
	step [52/146], loss=95.0974
	step [53/146], loss=93.1589
	step [54/146], loss=84.8691
	step [55/146], loss=99.2181
	step [56/146], loss=97.4656
	step [57/146], loss=110.5796
	step [58/146], loss=88.2286
	step [59/146], loss=105.0124
	step [60/146], loss=93.0851
	step [61/146], loss=100.0605
	step [62/146], loss=102.8232
	step [63/146], loss=80.2659
	step [64/146], loss=111.2114
	step [65/146], loss=119.9301
	step [66/146], loss=89.6420
	step [67/146], loss=76.6685
	step [68/146], loss=96.3786
	step [69/146], loss=98.6699
	step [70/146], loss=97.9546
	step [71/146], loss=98.1815
	step [72/146], loss=89.0213
	step [73/146], loss=94.3109
	step [74/146], loss=98.2557
	step [75/146], loss=83.5303
	step [76/146], loss=109.3584
	step [77/146], loss=97.2331
	step [78/146], loss=85.4872
	step [79/146], loss=111.2989
	step [80/146], loss=94.4015
	step [81/146], loss=111.0574
	step [82/146], loss=98.9092
	step [83/146], loss=102.5017
	step [84/146], loss=105.0004
	step [85/146], loss=98.6389
	step [86/146], loss=105.6233
	step [87/146], loss=106.4828
	step [88/146], loss=98.3545
	step [89/146], loss=89.3114
	step [90/146], loss=87.2427
	step [91/146], loss=94.9098
	step [92/146], loss=85.0796
	step [93/146], loss=101.3313
	step [94/146], loss=90.5666
	step [95/146], loss=97.4348
	step [96/146], loss=109.2086
	step [97/146], loss=95.7443
	step [98/146], loss=95.2255
	step [99/146], loss=105.4200
	step [100/146], loss=80.9337
	step [101/146], loss=105.2786
	step [102/146], loss=100.4970
	step [103/146], loss=96.5255
	step [104/146], loss=107.0001
	step [105/146], loss=86.0722
	step [106/146], loss=79.3318
	step [107/146], loss=79.7706
	step [108/146], loss=81.8654
	step [109/146], loss=100.4830
	step [110/146], loss=97.0262
	step [111/146], loss=93.7437
	step [112/146], loss=107.9702
	step [113/146], loss=110.7083
	step [114/146], loss=93.5918
	step [115/146], loss=89.7721
	step [116/146], loss=103.5330
	step [117/146], loss=96.8854
	step [118/146], loss=91.1503
	step [119/146], loss=84.9152
	step [120/146], loss=101.7664
	step [121/146], loss=97.6425
	step [122/146], loss=97.2126
	step [123/146], loss=91.2343
	step [124/146], loss=80.2968
	step [125/146], loss=107.1395
	step [126/146], loss=78.7744
	step [127/146], loss=90.2631
	step [128/146], loss=101.6257
	step [129/146], loss=94.3982
	step [130/146], loss=120.2559
	step [131/146], loss=94.7972
	step [132/146], loss=92.4228
	step [133/146], loss=115.7641
	step [134/146], loss=95.8627
	step [135/146], loss=107.9486
	step [136/146], loss=91.3830
	step [137/146], loss=99.4089
	step [138/146], loss=88.5375
	step [139/146], loss=101.5738
	step [140/146], loss=91.8985
	step [141/146], loss=102.5027
	step [142/146], loss=100.6691
	step [143/146], loss=101.5573
	step [144/146], loss=96.2710
	step [145/146], loss=99.2057
	step [146/146], loss=47.1266
	Evaluating
	loss=0.0179, precision=0.3535, recall=0.9087, f1=0.5090
Training epoch 30
	step [1/146], loss=109.2883
	step [2/146], loss=92.6303
	step [3/146], loss=92.5458
	step [4/146], loss=109.7262
	step [5/146], loss=89.5118
	step [6/146], loss=111.0569
	step [7/146], loss=95.1826
	step [8/146], loss=99.5369
	step [9/146], loss=89.9122
	step [10/146], loss=95.7739
	step [11/146], loss=96.2314
	step [12/146], loss=82.2895
	step [13/146], loss=105.5875
	step [14/146], loss=90.9471
	step [15/146], loss=90.3707
	step [16/146], loss=91.2586
	step [17/146], loss=106.9376
	step [18/146], loss=95.8541
	step [19/146], loss=107.2236
	step [20/146], loss=104.3079
	step [21/146], loss=113.0051
	step [22/146], loss=96.1753
	step [23/146], loss=98.3288
	step [24/146], loss=112.3013
	step [25/146], loss=98.6018
	step [26/146], loss=91.7403
	step [27/146], loss=87.7041
	step [28/146], loss=101.1845
	step [29/146], loss=106.3596
	step [30/146], loss=94.7534
	step [31/146], loss=103.3186
	step [32/146], loss=92.8035
	step [33/146], loss=112.9480
	step [34/146], loss=95.6894
	step [35/146], loss=82.5282
	step [36/146], loss=81.2712
	step [37/146], loss=109.2947
	step [38/146], loss=89.3749
	step [39/146], loss=118.1955
	step [40/146], loss=88.9326
	step [41/146], loss=85.0005
	step [42/146], loss=92.1772
	step [43/146], loss=108.2924
	step [44/146], loss=95.9105
	step [45/146], loss=90.1223
	step [46/146], loss=98.5888
	step [47/146], loss=66.9723
	step [48/146], loss=84.2415
	step [49/146], loss=93.8757
	step [50/146], loss=93.7984
	step [51/146], loss=96.3819
	step [52/146], loss=84.8058
	step [53/146], loss=105.8883
	step [54/146], loss=96.0245
	step [55/146], loss=85.6915
	step [56/146], loss=90.6971
	step [57/146], loss=99.1042
	step [58/146], loss=106.6444
	step [59/146], loss=97.0640
	step [60/146], loss=94.2055
	step [61/146], loss=102.6028
	step [62/146], loss=86.7311
	step [63/146], loss=96.6282
	step [64/146], loss=93.1426
	step [65/146], loss=92.8020
	step [66/146], loss=78.1292
	step [67/146], loss=97.9721
	step [68/146], loss=96.8714
	step [69/146], loss=83.1479
	step [70/146], loss=92.9186
	step [71/146], loss=96.1234
	step [72/146], loss=89.4721
	step [73/146], loss=77.4658
	step [74/146], loss=113.4075
	step [75/146], loss=103.7121
	step [76/146], loss=99.2540
	step [77/146], loss=119.2613
	step [78/146], loss=105.0740
	step [79/146], loss=103.2914
	step [80/146], loss=113.9483
	step [81/146], loss=75.0039
	step [82/146], loss=80.1116
	step [83/146], loss=107.9498
	step [84/146], loss=104.2582
	step [85/146], loss=92.2862
	step [86/146], loss=97.0008
	step [87/146], loss=97.3091
	step [88/146], loss=113.8842
	step [89/146], loss=96.7850
	step [90/146], loss=90.1873
	step [91/146], loss=88.0973
	step [92/146], loss=97.2191
	step [93/146], loss=88.6809
	step [94/146], loss=96.1009
	step [95/146], loss=99.4339
	step [96/146], loss=106.7782
	step [97/146], loss=96.4481
	step [98/146], loss=80.4056
	step [99/146], loss=82.5655
	step [100/146], loss=102.7799
	step [101/146], loss=80.3679
	step [102/146], loss=91.5028
	step [103/146], loss=100.8456
	step [104/146], loss=89.8016
	step [105/146], loss=88.0444
	step [106/146], loss=111.7034
	step [107/146], loss=101.2808
	step [108/146], loss=96.1983
	step [109/146], loss=89.1049
	step [110/146], loss=95.7142
	step [111/146], loss=103.9556
	step [112/146], loss=86.6074
	step [113/146], loss=89.2719
	step [114/146], loss=101.2898
	step [115/146], loss=106.7916
	step [116/146], loss=92.8648
	step [117/146], loss=118.3283
	step [118/146], loss=88.3424
	step [119/146], loss=105.0346
	step [120/146], loss=95.9448
	step [121/146], loss=94.1596
	step [122/146], loss=82.2213
	step [123/146], loss=83.8161
	step [124/146], loss=83.2691
	step [125/146], loss=108.1609
	step [126/146], loss=104.2017
	step [127/146], loss=79.3005
	step [128/146], loss=96.9467
	step [129/146], loss=82.3720
	step [130/146], loss=88.6380
	step [131/146], loss=94.6765
	step [132/146], loss=106.7220
	step [133/146], loss=98.2999
	step [134/146], loss=84.5335
	step [135/146], loss=110.2631
	step [136/146], loss=94.3824
	step [137/146], loss=88.3008
	step [138/146], loss=85.0529
	step [139/146], loss=92.5108
	step [140/146], loss=105.8585
	step [141/146], loss=85.5002
	step [142/146], loss=107.8171
	step [143/146], loss=92.1823
	step [144/146], loss=91.9306
	step [145/146], loss=89.6698
	step [146/146], loss=53.7759
	Evaluating
	loss=0.0143, precision=0.4180, recall=0.9031, f1=0.5714
Training finished
best_f1: 0.649176087612408
directing: Y rim_enhanced: True test_id 2
removed wrong scan: weights_Y_335_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_383_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_262_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_311_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_260_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_225_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_320_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_192_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_319_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_226_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_346_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_271_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_277_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_290_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_269_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_379_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_375_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_378_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_366_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_309_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_200_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_327_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_193_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_174_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_253_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_220_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_265_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_360_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_303_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_232_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_243_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_240_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_237_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_256_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_254_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_254_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_231_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_359_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_172_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_340_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_263_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_388_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_307_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_278_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_196_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_272_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_314_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_350_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_361_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_194_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_323_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_201_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_299_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_259_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_276_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_167_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_185_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_215_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_268_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_264_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_384_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_261_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_280_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_287_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_385_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_216_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_349_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_348_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_371_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_357_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_234_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_288_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_324_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_229_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_184_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_148_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_168_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_370_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_286_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_302_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_265_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_153_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_274_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_255_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_217_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_355_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_294_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_159_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_338_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_236_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_152_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_292_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_222_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_376_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_183_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_258_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_282_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_278_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_284_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_300_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_163_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_197_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_203_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_243_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_224_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_238_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_374_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_218_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_259_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_282_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_387_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_353_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_386_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_296_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_341_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_206_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_220_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_239_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_231_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_202_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_358_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_246_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_315_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_275_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_169_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_316_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_221_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_188_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_208_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_273_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_251_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_224_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_333_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_263_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_173_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_243_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_241_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_326_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_230_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_236_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_345_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_206_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_258_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_187_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_87_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_154_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_253_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_337_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_232_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_147_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_377_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_362_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_233_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_250_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_297_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_354_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_262_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_225_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_365_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_356_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_242_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_245_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_230_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_318_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_181_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_305_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_255_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_298_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_301_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_293_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_229_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_332_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_257_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_295_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_239_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_260_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_228_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_317_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_364_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_262_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_162_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_310_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_160_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_237_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_254_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_257_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_291_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_279_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_215_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_372_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_218_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_210_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_158_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_156_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_339_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_190_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_373_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_205_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_329_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_380_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_271_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_235_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_244_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_312_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_186_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_308_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_347_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_241_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_270_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_261_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_222_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_189_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_289_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_146_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_252_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_344_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_331_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_313_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_328_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_265_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_283_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_248_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_381_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_211_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_214_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_176_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_221_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_150_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_343_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_249_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_322_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_351_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_234_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_195_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_330_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_334_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_306_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_213_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_325_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_342_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_247_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_352_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_225_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_226_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_256_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_242_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_368_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_382_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_157_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_238_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_267_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_367_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_321_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_233_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_281_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_161_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_336_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_170_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_304_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_44_xwqg-B00034_2020-04-03.npy
# all image files: 20333 # all weight files in weight_dir: 15586 # image files with weight 15554
removed wrong scan: weights_Y_335_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_383_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_262_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_311_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_260_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_225_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_320_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_192_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_319_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_226_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_346_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_271_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_277_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_290_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_269_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_379_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_375_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_378_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_366_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_309_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_200_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_327_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_193_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_174_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_253_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_220_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_265_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_360_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_303_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_232_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_243_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_240_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_237_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_256_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_254_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_254_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_231_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_359_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_172_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_340_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_263_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_388_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_307_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_278_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_196_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_272_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_314_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_350_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_361_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_194_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_323_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_201_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_299_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_259_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_276_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_167_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_185_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_215_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_268_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_264_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_384_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_261_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_280_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_287_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_385_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_216_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_349_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_348_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_371_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_357_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_234_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_288_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_324_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_229_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_184_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_148_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_168_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_370_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_286_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_302_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_265_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_153_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_274_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_255_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_217_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_355_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_294_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_159_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_338_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_236_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_152_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_292_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_222_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_376_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_183_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_258_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_282_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_278_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_284_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_300_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_163_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_197_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_203_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_243_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_224_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_238_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_374_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_218_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_259_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_282_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_387_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_353_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_386_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_296_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_341_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_206_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_220_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_239_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_231_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_202_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_358_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_246_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_315_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_275_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_169_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_316_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_221_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_188_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_208_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_273_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_251_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_224_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_333_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_263_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_173_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_243_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_241_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_326_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_230_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_236_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_345_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_206_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_258_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_187_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_87_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_154_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_253_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_337_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_232_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_147_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_377_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_362_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_233_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_250_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_297_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_354_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_262_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_225_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_365_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_356_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_242_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_245_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_230_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_318_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_181_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_305_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_255_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_298_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_301_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_293_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_229_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_332_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_257_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_295_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_239_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_260_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_228_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_317_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_364_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_262_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_162_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_310_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_160_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_237_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_254_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_257_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_291_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_279_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_215_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_372_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_218_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_210_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_158_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_156_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_339_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_190_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_373_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_205_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_329_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_380_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_271_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_235_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_244_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_312_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_186_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_308_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_347_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_241_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_270_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_261_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_222_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_189_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_289_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_146_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_252_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_344_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_331_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_313_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_328_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_265_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_283_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_248_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_381_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_211_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_214_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_176_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_221_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_150_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_343_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_249_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_322_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_351_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_234_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_195_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_330_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_334_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_306_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_213_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_325_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_342_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_247_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_352_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_225_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_226_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_256_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_242_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_368_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_382_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_157_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_238_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_267_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_367_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_321_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_233_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_281_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_161_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_336_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_170_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_304_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_44_xwqg-B00034_2020-04-03.npy
# all image files: 20333 # all weight files in weight_dir: 4486 # image files with weight 4476
train: /ibex/scratch/projects/c2052/air_tube_seg/training_samples_enhanced_one/Y 15554
Using 4 GPUs
Going to train epochs [1-30]
Training epoch 1
	step [1/244], loss=999.9031
	step [2/244], loss=619.3941
	step [3/244], loss=436.5853
	step [4/244], loss=336.2904
	step [5/244], loss=290.0282
	step [6/244], loss=343.1836
	step [7/244], loss=337.7974
	step [8/244], loss=316.9958
	step [9/244], loss=268.7225
	step [10/244], loss=313.6509
	step [11/244], loss=272.2406
	step [12/244], loss=283.7917
	step [13/244], loss=238.4992
	step [14/244], loss=254.5677
	step [15/244], loss=275.5045
	step [16/244], loss=288.4431
	step [17/244], loss=256.4587
	step [18/244], loss=274.1339
	step [19/244], loss=235.6148
	step [20/244], loss=237.8163
	step [21/244], loss=252.6233
	step [22/244], loss=244.0153
	step [23/244], loss=279.3906
	step [24/244], loss=238.3112
	step [25/244], loss=273.9504
	step [26/244], loss=237.8392
	step [27/244], loss=269.2833
	step [28/244], loss=236.6611
	step [29/244], loss=238.9055
	step [30/244], loss=242.5209
	step [31/244], loss=235.4628
	step [32/244], loss=244.9582
	step [33/244], loss=229.7616
	step [34/244], loss=243.0456
	step [35/244], loss=221.1147
	step [36/244], loss=226.2409
	step [37/244], loss=229.0132
	step [38/244], loss=234.8027
	step [39/244], loss=216.7591
	step [40/244], loss=230.9394
	step [41/244], loss=254.1862
	step [42/244], loss=227.0283
	step [43/244], loss=233.2853
	step [44/244], loss=217.3786
	step [45/244], loss=226.8534
	step [46/244], loss=261.3220
	step [47/244], loss=213.4791
	step [48/244], loss=272.1116
	step [49/244], loss=206.5008
	step [50/244], loss=258.1534
	step [51/244], loss=202.2929
	step [52/244], loss=213.9658
	step [53/244], loss=228.8405
	step [54/244], loss=211.9078
	step [55/244], loss=191.5957
	step [56/244], loss=227.2765
	step [57/244], loss=207.5268
	step [58/244], loss=246.3931
	step [59/244], loss=251.1266
	step [60/244], loss=212.4220
	step [61/244], loss=222.5042
	step [62/244], loss=210.3510
	step [63/244], loss=186.6575
	step [64/244], loss=209.6779
	step [65/244], loss=220.3571
	step [66/244], loss=217.6608
	step [67/244], loss=216.3570
	step [68/244], loss=231.7475
	step [69/244], loss=201.5237
	step [70/244], loss=172.9519
	step [71/244], loss=213.1987
	step [72/244], loss=189.2184
	step [73/244], loss=179.9566
	step [74/244], loss=226.5669
	step [75/244], loss=239.5652
	step [76/244], loss=213.9048
	step [77/244], loss=214.1960
	step [78/244], loss=208.3010
	step [79/244], loss=212.3240
	step [80/244], loss=207.8196
	step [81/244], loss=189.8493
	step [82/244], loss=220.2395
	step [83/244], loss=209.1339
	step [84/244], loss=201.5574
	step [85/244], loss=189.7865
	step [86/244], loss=205.6180
	step [87/244], loss=210.2294
	step [88/244], loss=201.9052
	step [89/244], loss=177.6658
	step [90/244], loss=201.6819
	step [91/244], loss=237.6696
	step [92/244], loss=206.8757
	step [93/244], loss=205.5142
	step [94/244], loss=212.0204
	step [95/244], loss=183.9040
	step [96/244], loss=220.8028
	step [97/244], loss=198.4697
	step [98/244], loss=185.6749
	step [99/244], loss=206.9372
	step [100/244], loss=214.3737
	step [101/244], loss=227.3524
	step [102/244], loss=189.6974
	step [103/244], loss=229.9273
	step [104/244], loss=197.4821
	step [105/244], loss=230.9199
	step [106/244], loss=233.0093
	step [107/244], loss=180.8364
	step [108/244], loss=202.3318
	step [109/244], loss=190.7979
	step [110/244], loss=199.9833
	step [111/244], loss=212.8844
	step [112/244], loss=190.8810
	step [113/244], loss=204.0979
	step [114/244], loss=240.0735
	step [115/244], loss=224.4581
	step [116/244], loss=189.9174
	step [117/244], loss=211.1067
	step [118/244], loss=191.5141
	step [119/244], loss=233.9796
	step [120/244], loss=172.0492
	step [121/244], loss=199.0473
	step [122/244], loss=199.0002
	step [123/244], loss=202.1064
	step [124/244], loss=198.4637
	step [125/244], loss=181.6602
	step [126/244], loss=210.5200
	step [127/244], loss=203.5134
	step [128/244], loss=183.2438
	step [129/244], loss=200.5150
	step [130/244], loss=218.8656
	step [131/244], loss=199.4484
	step [132/244], loss=182.8666
	step [133/244], loss=179.4668
	step [134/244], loss=204.7402
	step [135/244], loss=193.8094
	step [136/244], loss=223.5858
	step [137/244], loss=208.0973
	step [138/244], loss=192.4710
	step [139/244], loss=215.1837
	step [140/244], loss=179.0682
	step [141/244], loss=167.8628
	step [142/244], loss=184.1196
	step [143/244], loss=181.4344
	step [144/244], loss=187.3147
	step [145/244], loss=191.0170
	step [146/244], loss=183.4157
	step [147/244], loss=183.1183
	step [148/244], loss=201.9519
	step [149/244], loss=200.5046
	step [150/244], loss=200.8271
	step [151/244], loss=187.7707
	step [152/244], loss=198.6799
	step [153/244], loss=194.1548
	step [154/244], loss=159.7397
	step [155/244], loss=181.8319
	step [156/244], loss=178.3853
	step [157/244], loss=207.7242
	step [158/244], loss=188.9824
	step [159/244], loss=171.1396
	step [160/244], loss=173.5735
	step [161/244], loss=181.8833
	step [162/244], loss=187.3082
	step [163/244], loss=177.3177
	step [164/244], loss=215.3506
	step [165/244], loss=171.8051
	step [166/244], loss=194.0351
	step [167/244], loss=174.9626
	step [168/244], loss=165.6989
	step [169/244], loss=182.5002
	step [170/244], loss=140.5579
	step [171/244], loss=176.2196
	step [172/244], loss=178.4523
	step [173/244], loss=172.1221
	step [174/244], loss=160.8318
	step [175/244], loss=163.4318
	step [176/244], loss=189.5798
	step [177/244], loss=191.4516
	step [178/244], loss=175.2654
	step [179/244], loss=189.9017
	step [180/244], loss=174.2332
	step [181/244], loss=209.5093
	step [182/244], loss=193.4577
	step [183/244], loss=177.6679
	step [184/244], loss=200.8488
	step [185/244], loss=187.7395
	step [186/244], loss=168.8054
	step [187/244], loss=181.5044
	step [188/244], loss=197.4801
	step [189/244], loss=180.6426
	step [190/244], loss=167.5267
	step [191/244], loss=183.6157
	step [192/244], loss=158.9739
	step [193/244], loss=180.8402
	step [194/244], loss=207.7459
	step [195/244], loss=162.3074
	step [196/244], loss=167.0215
	step [197/244], loss=192.1848
	step [198/244], loss=199.7271
	step [199/244], loss=167.8053
	step [200/244], loss=164.0479
	step [201/244], loss=169.3210
	step [202/244], loss=175.7497
	step [203/244], loss=149.8459
	step [204/244], loss=180.6312
	step [205/244], loss=204.6120
	step [206/244], loss=177.5463
	step [207/244], loss=161.3055
	step [208/244], loss=175.4029
	step [209/244], loss=176.9317
	step [210/244], loss=163.6573
	step [211/244], loss=178.5024
	step [212/244], loss=190.2181
	step [213/244], loss=159.0787
	step [214/244], loss=197.1920
	step [215/244], loss=207.7583
	step [216/244], loss=161.8540
	step [217/244], loss=196.2433
	step [218/244], loss=192.0403
	step [219/244], loss=188.9148
	step [220/244], loss=191.0607
	step [221/244], loss=183.2623
	step [222/244], loss=181.9001
	step [223/244], loss=193.5582
	step [224/244], loss=165.2107
	step [225/244], loss=175.1567
	step [226/244], loss=156.9579
	step [227/244], loss=176.2937
	step [228/244], loss=155.9285
	step [229/244], loss=179.4287
	step [230/244], loss=161.3013
	step [231/244], loss=168.0177
	step [232/244], loss=168.4586
	step [233/244], loss=163.2968
	step [234/244], loss=158.4060
	step [235/244], loss=176.9967
	step [236/244], loss=158.9926
	step [237/244], loss=179.9483
	step [238/244], loss=178.5351
	step [239/244], loss=161.1952
	step [240/244], loss=160.4154
	step [241/244], loss=182.3680
	step [242/244], loss=172.6070
	step [243/244], loss=171.9566
	step [244/244], loss=4.6864
	Evaluating
	loss=0.2767, precision=0.2000, recall=0.9208, f1=0.3287
saving model as: 2_saved_model.pth
Training epoch 2
	step [1/244], loss=185.7439
	step [2/244], loss=174.4508
	step [3/244], loss=166.7770
	step [4/244], loss=152.6551
	step [5/244], loss=178.9689
	step [6/244], loss=174.1253
	step [7/244], loss=176.1629
	step [8/244], loss=151.0314
	step [9/244], loss=161.0262
	step [10/244], loss=189.5242
	step [11/244], loss=166.2184
	step [12/244], loss=149.1806
	step [13/244], loss=167.7648
	step [14/244], loss=158.5089
	step [15/244], loss=188.3767
	step [16/244], loss=145.1522
	step [17/244], loss=163.6840
	step [18/244], loss=178.3889
	step [19/244], loss=162.8381
	step [20/244], loss=158.8938
	step [21/244], loss=180.5229
	step [22/244], loss=187.7796
	step [23/244], loss=166.9964
	step [24/244], loss=169.3555
	step [25/244], loss=191.9731
	step [26/244], loss=176.7666
	step [27/244], loss=165.5590
	step [28/244], loss=170.0623
	step [29/244], loss=151.9284
	step [30/244], loss=163.8258
	step [31/244], loss=198.0926
	step [32/244], loss=195.4687
	step [33/244], loss=190.1432
	step [34/244], loss=195.0776
	step [35/244], loss=195.7233
	step [36/244], loss=178.9865
	step [37/244], loss=187.5697
	step [38/244], loss=157.3129
	step [39/244], loss=162.0408
	step [40/244], loss=182.8514
	step [41/244], loss=152.1548
	step [42/244], loss=163.0551
	step [43/244], loss=161.6016
	step [44/244], loss=167.5052
	step [45/244], loss=139.0761
	step [46/244], loss=158.6399
	step [47/244], loss=160.1659
	step [48/244], loss=171.2957
	step [49/244], loss=170.9639
	step [50/244], loss=162.1025
	step [51/244], loss=164.4413
	step [52/244], loss=187.6411
	step [53/244], loss=168.8736
	step [54/244], loss=146.6879
	step [55/244], loss=182.6145
	step [56/244], loss=170.2823
	step [57/244], loss=176.3819
	step [58/244], loss=183.4725
	step [59/244], loss=153.7381
	step [60/244], loss=159.7019
	step [61/244], loss=149.7117
	step [62/244], loss=175.8099
	step [63/244], loss=164.8456
	step [64/244], loss=165.7128
	step [65/244], loss=190.8734
	step [66/244], loss=156.6450
	step [67/244], loss=180.3210
	step [68/244], loss=172.4564
	step [69/244], loss=169.6703
	step [70/244], loss=161.5230
	step [71/244], loss=184.4683
	step [72/244], loss=159.8003
	step [73/244], loss=188.7318
	step [74/244], loss=160.4114
	step [75/244], loss=168.4803
	step [76/244], loss=120.8964
	step [77/244], loss=191.0856
	step [78/244], loss=160.4119
	step [79/244], loss=161.2147
	step [80/244], loss=172.7160
	step [81/244], loss=186.2054
	step [82/244], loss=162.9104
	step [83/244], loss=153.4580
	step [84/244], loss=161.8716
	step [85/244], loss=149.8722
	step [86/244], loss=190.7577
	step [87/244], loss=152.8769
	step [88/244], loss=155.5583
	step [89/244], loss=161.2271
	step [90/244], loss=147.9534
	step [91/244], loss=197.8536
	step [92/244], loss=156.8527
	step [93/244], loss=163.8076
	step [94/244], loss=138.1024
	step [95/244], loss=186.3753
	step [96/244], loss=152.4125
	step [97/244], loss=141.2873
	step [98/244], loss=163.8856
	step [99/244], loss=156.0811
	step [100/244], loss=177.7865
	step [101/244], loss=172.6115
	step [102/244], loss=158.8930
	step [103/244], loss=162.0369
	step [104/244], loss=140.1302
	step [105/244], loss=167.5467
	step [106/244], loss=168.6925
	step [107/244], loss=151.9549
	step [108/244], loss=153.6516
	step [109/244], loss=136.4708
	step [110/244], loss=129.6908
	step [111/244], loss=152.7254
	step [112/244], loss=136.0315
	step [113/244], loss=134.6624
	step [114/244], loss=158.8220
	step [115/244], loss=169.2749
	step [116/244], loss=136.1980
	step [117/244], loss=162.0755
	step [118/244], loss=159.4613
	step [119/244], loss=177.9185
	step [120/244], loss=153.9482
	step [121/244], loss=156.6989
	step [122/244], loss=154.5071
	step [123/244], loss=185.8102
	step [124/244], loss=182.3619
	step [125/244], loss=175.8814
	step [126/244], loss=169.2251
	step [127/244], loss=160.3226
	step [128/244], loss=167.8332
	step [129/244], loss=153.2139
	step [130/244], loss=147.0779
	step [131/244], loss=147.3797
	step [132/244], loss=146.9637
	step [133/244], loss=163.2677
	step [134/244], loss=181.0855
	step [135/244], loss=146.4310
	step [136/244], loss=152.5671
	step [137/244], loss=165.4609
	step [138/244], loss=151.7673
	step [139/244], loss=167.0913
	step [140/244], loss=139.6455
	step [141/244], loss=150.3279
	step [142/244], loss=146.7665
	step [143/244], loss=145.2724
	step [144/244], loss=152.4808
	step [145/244], loss=173.9528
	step [146/244], loss=178.0759
	step [147/244], loss=154.4697
	step [148/244], loss=155.1726
	step [149/244], loss=139.4294
	step [150/244], loss=145.3023
	step [151/244], loss=145.5456
	step [152/244], loss=151.5370
	step [153/244], loss=168.2111
	step [154/244], loss=162.5747
	step [155/244], loss=168.1678
	step [156/244], loss=130.2654
	step [157/244], loss=166.6786
	step [158/244], loss=160.9748
	step [159/244], loss=175.7971
	step [160/244], loss=189.8142
	step [161/244], loss=174.4761
	step [162/244], loss=141.0172
	step [163/244], loss=159.6579
	step [164/244], loss=136.3134
	step [165/244], loss=151.0385
	step [166/244], loss=130.4066
	step [167/244], loss=142.4363
	step [168/244], loss=169.2447
	step [169/244], loss=162.7377
	step [170/244], loss=124.4980
	step [171/244], loss=171.1944
	step [172/244], loss=153.0533
	step [173/244], loss=138.3881
	step [174/244], loss=168.7267
	step [175/244], loss=157.1478
	step [176/244], loss=174.8058
	step [177/244], loss=154.5946
	step [178/244], loss=146.0314
	step [179/244], loss=149.6666
	step [180/244], loss=159.7159
	step [181/244], loss=141.6166
	step [182/244], loss=122.7402
	step [183/244], loss=149.1038
	step [184/244], loss=154.2311
	step [185/244], loss=142.1018
	step [186/244], loss=160.9667
	step [187/244], loss=149.6689
	step [188/244], loss=137.4745
	step [189/244], loss=155.0621
	step [190/244], loss=141.6687
	step [191/244], loss=154.0234
	step [192/244], loss=141.4944
	step [193/244], loss=154.0179
	step [194/244], loss=146.0634
	step [195/244], loss=140.1545
	step [196/244], loss=153.7248
	step [197/244], loss=155.0630
	step [198/244], loss=135.2391
	step [199/244], loss=179.1407
	step [200/244], loss=139.2765
	step [201/244], loss=138.5605
	step [202/244], loss=149.7661
	step [203/244], loss=172.9268
	step [204/244], loss=167.4995
	step [205/244], loss=160.1083
	step [206/244], loss=137.3297
	step [207/244], loss=164.8927
	step [208/244], loss=153.5990
	step [209/244], loss=150.4864
	step [210/244], loss=149.0338
	step [211/244], loss=158.9978
	step [212/244], loss=151.0866
	step [213/244], loss=172.3094
	step [214/244], loss=147.6860
	step [215/244], loss=168.4577
	step [216/244], loss=160.5818
	step [217/244], loss=149.3930
	step [218/244], loss=158.2678
	step [219/244], loss=134.8119
	step [220/244], loss=154.0461
	step [221/244], loss=154.3109
	step [222/244], loss=147.6385
	step [223/244], loss=137.6205
	step [224/244], loss=141.6900
	step [225/244], loss=142.9839
	step [226/244], loss=143.0287
	step [227/244], loss=132.2901
	step [228/244], loss=150.6158
	step [229/244], loss=161.3784
	step [230/244], loss=171.7579
	step [231/244], loss=143.1218
	step [232/244], loss=144.3862
	step [233/244], loss=167.5539
	step [234/244], loss=148.4603
	step [235/244], loss=134.2888
	step [236/244], loss=137.9924
	step [237/244], loss=154.9423
	step [238/244], loss=130.7384
	step [239/244], loss=132.2797
	step [240/244], loss=158.6547
	step [241/244], loss=147.3625
	step [242/244], loss=151.3265
	step [243/244], loss=154.5274
	step [244/244], loss=1.4164
	Evaluating
	loss=0.1772, precision=0.3932, recall=0.9429, f1=0.5549
saving model as: 2_saved_model.pth
Training epoch 3
	step [1/244], loss=172.3582
	step [2/244], loss=131.3431
	step [3/244], loss=130.6851
	step [4/244], loss=156.5932
	step [5/244], loss=139.3332
	step [6/244], loss=109.4001
	step [7/244], loss=128.1520
	step [8/244], loss=146.2254
	step [9/244], loss=144.9017
	step [10/244], loss=159.1936
	step [11/244], loss=184.4847
	step [12/244], loss=177.5886
	step [13/244], loss=152.7613
	step [14/244], loss=152.4352
	step [15/244], loss=165.8711
	step [16/244], loss=152.6858
	step [17/244], loss=144.8348
	step [18/244], loss=141.9967
	step [19/244], loss=130.5411
	step [20/244], loss=138.7630
	step [21/244], loss=139.0347
	step [22/244], loss=123.2292
	step [23/244], loss=154.1360
	step [24/244], loss=159.6165
	step [25/244], loss=125.3483
	step [26/244], loss=152.5686
	step [27/244], loss=120.3448
	step [28/244], loss=115.2473
	step [29/244], loss=148.2242
	step [30/244], loss=161.0553
	step [31/244], loss=152.6363
	step [32/244], loss=144.7722
	step [33/244], loss=135.7446
	step [34/244], loss=155.0704
	step [35/244], loss=129.6968
	step [36/244], loss=134.6103
	step [37/244], loss=129.2232
	step [38/244], loss=146.8913
	step [39/244], loss=151.8369
	step [40/244], loss=147.8515
	step [41/244], loss=159.2589
	step [42/244], loss=142.8774
	step [43/244], loss=128.2493
	step [44/244], loss=135.8656
	step [45/244], loss=168.3624
	step [46/244], loss=123.2095
	step [47/244], loss=145.0332
	step [48/244], loss=134.5388
	step [49/244], loss=157.1806
	step [50/244], loss=148.6056
	step [51/244], loss=143.4464
	step [52/244], loss=133.3052
	step [53/244], loss=164.9909
	step [54/244], loss=142.5643
	step [55/244], loss=146.9603
	step [56/244], loss=141.9911
	step [57/244], loss=142.1113
	step [58/244], loss=160.9412
	step [59/244], loss=143.5884
	step [60/244], loss=137.9696
	step [61/244], loss=140.4310
	step [62/244], loss=130.6768
	step [63/244], loss=139.5640
	step [64/244], loss=152.9621
	step [65/244], loss=152.5917
	step [66/244], loss=151.3393
	step [67/244], loss=141.6821
	step [68/244], loss=171.5345
	step [69/244], loss=158.3938
	step [70/244], loss=149.1608
	step [71/244], loss=140.2026
	step [72/244], loss=158.3402
	step [73/244], loss=144.9125
	step [74/244], loss=161.5145
	step [75/244], loss=132.7410
	step [76/244], loss=141.8352
	step [77/244], loss=143.4019
	step [78/244], loss=149.0851
	step [79/244], loss=177.1513
	step [80/244], loss=150.0714
	step [81/244], loss=173.8210
	step [82/244], loss=132.8323
	step [83/244], loss=124.0540
	step [84/244], loss=166.2327
	step [85/244], loss=164.0598
	step [86/244], loss=137.9587
	step [87/244], loss=169.3781
	step [88/244], loss=162.5955
	step [89/244], loss=130.2918
	step [90/244], loss=161.8082
	step [91/244], loss=154.3255
	step [92/244], loss=142.9500
	step [93/244], loss=131.1745
	step [94/244], loss=133.7006
	step [95/244], loss=120.0598
	step [96/244], loss=149.4202
	step [97/244], loss=123.3643
	step [98/244], loss=146.2305
	step [99/244], loss=177.4628
	step [100/244], loss=140.3054
	step [101/244], loss=141.0633
	step [102/244], loss=131.3236
	step [103/244], loss=146.9949
	step [104/244], loss=167.0714
	step [105/244], loss=151.2234
	step [106/244], loss=131.3174
	step [107/244], loss=135.2226
	step [108/244], loss=119.0891
	step [109/244], loss=162.1352
	step [110/244], loss=147.5990
	step [111/244], loss=130.6652
	step [112/244], loss=164.0109
	step [113/244], loss=154.1490
	step [114/244], loss=147.0560
	step [115/244], loss=128.0401
	step [116/244], loss=147.3581
	step [117/244], loss=153.9246
	step [118/244], loss=109.2242
	step [119/244], loss=108.0756
	step [120/244], loss=131.4753
	step [121/244], loss=156.8265
	step [122/244], loss=154.0640
	step [123/244], loss=122.8971
	step [124/244], loss=125.5732
	step [125/244], loss=126.0887
	step [126/244], loss=140.5947
	step [127/244], loss=130.7445
	step [128/244], loss=138.9044
	step [129/244], loss=130.1617
	step [130/244], loss=147.3975
	step [131/244], loss=129.9992
	step [132/244], loss=170.0771
	step [133/244], loss=149.2927
	step [134/244], loss=134.3070
	step [135/244], loss=122.4134
	step [136/244], loss=146.0736
	step [137/244], loss=140.5835
	step [138/244], loss=140.9290
	step [139/244], loss=130.8991
	step [140/244], loss=136.4214
	step [141/244], loss=179.3973
	step [142/244], loss=129.6527
	step [143/244], loss=142.2479
	step [144/244], loss=113.8292
	step [145/244], loss=139.6639
	step [146/244], loss=120.5042
	step [147/244], loss=132.2486
	step [148/244], loss=159.8185
	step [149/244], loss=141.8688
	step [150/244], loss=129.7939
	step [151/244], loss=149.5135
	step [152/244], loss=128.9039
	step [153/244], loss=167.5401
	step [154/244], loss=151.1091
	step [155/244], loss=132.7610
	step [156/244], loss=135.6785
	step [157/244], loss=139.4256
	step [158/244], loss=143.1102
	step [159/244], loss=131.8848
	step [160/244], loss=124.1877
	step [161/244], loss=142.5318
	step [162/244], loss=112.6489
	step [163/244], loss=131.6764
	step [164/244], loss=167.9410
	step [165/244], loss=139.3846
	step [166/244], loss=142.7316
	step [167/244], loss=141.5425
	step [168/244], loss=134.5409
	step [169/244], loss=149.3004
	step [170/244], loss=151.1376
	step [171/244], loss=123.3956
	step [172/244], loss=150.7187
	step [173/244], loss=125.5959
	step [174/244], loss=166.4942
	step [175/244], loss=143.9021
	step [176/244], loss=139.7555
	step [177/244], loss=138.9264
	step [178/244], loss=134.7712
	step [179/244], loss=154.9416
	step [180/244], loss=152.8087
	step [181/244], loss=124.1679
	step [182/244], loss=138.9890
	step [183/244], loss=137.9945
	step [184/244], loss=136.4257
	step [185/244], loss=146.1559
	step [186/244], loss=119.1751
	step [187/244], loss=146.6257
	step [188/244], loss=147.7956
	step [189/244], loss=119.1739
	step [190/244], loss=125.1236
	step [191/244], loss=140.2316
	step [192/244], loss=135.7264
	step [193/244], loss=131.3873
	step [194/244], loss=141.6754
	step [195/244], loss=168.4532
	step [196/244], loss=141.3158
	step [197/244], loss=129.8283
	step [198/244], loss=117.2850
	step [199/244], loss=138.0647
	step [200/244], loss=125.2901
	step [201/244], loss=150.7975
	step [202/244], loss=133.4695
	step [203/244], loss=116.9751
	step [204/244], loss=136.3127
	step [205/244], loss=162.9277
	step [206/244], loss=114.9340
	step [207/244], loss=148.5300
	step [208/244], loss=140.8093
	step [209/244], loss=145.7005
	step [210/244], loss=127.6824
	step [211/244], loss=137.5089
	step [212/244], loss=131.8755
	step [213/244], loss=149.2482
	step [214/244], loss=124.4027
	step [215/244], loss=170.6817
	step [216/244], loss=131.7612
	step [217/244], loss=128.0401
	step [218/244], loss=135.9791
	step [219/244], loss=124.6832
	step [220/244], loss=142.6551
	step [221/244], loss=131.7479
	step [222/244], loss=154.8895
	step [223/244], loss=158.0013
	step [224/244], loss=120.5286
	step [225/244], loss=147.8125
	step [226/244], loss=114.3592
	step [227/244], loss=147.2195
	step [228/244], loss=136.5757
	step [229/244], loss=139.0709
	step [230/244], loss=146.2780
	step [231/244], loss=142.3148
	step [232/244], loss=122.3637
	step [233/244], loss=121.0181
	step [234/244], loss=139.8585
	step [235/244], loss=123.8468
	step [236/244], loss=149.8658
	step [237/244], loss=129.0803
	step [238/244], loss=134.5381
	step [239/244], loss=143.6211
	step [240/244], loss=109.7907
	step [241/244], loss=135.9773
	step [242/244], loss=155.4170
	step [243/244], loss=122.8019
	step [244/244], loss=4.9370
	Evaluating
	loss=0.1182, precision=0.4377, recall=0.9217, f1=0.5935
saving model as: 2_saved_model.pth
Training epoch 4
	step [1/244], loss=137.4899
	step [2/244], loss=156.3867
	step [3/244], loss=140.8125
	step [4/244], loss=144.3660
	step [5/244], loss=130.5721
	step [6/244], loss=127.2181
	step [7/244], loss=122.5340
	step [8/244], loss=158.1221
	step [9/244], loss=124.0223
	step [10/244], loss=162.0300
	step [11/244], loss=112.1263
	step [12/244], loss=130.9831
	step [13/244], loss=138.6508
	step [14/244], loss=120.8084
	step [15/244], loss=152.3685
	step [16/244], loss=136.4055
	step [17/244], loss=121.7783
	step [18/244], loss=130.6292
	step [19/244], loss=119.9531
	step [20/244], loss=127.1263
	step [21/244], loss=124.0274
	step [22/244], loss=117.0450
	step [23/244], loss=140.7379
	step [24/244], loss=148.8914
	step [25/244], loss=140.5451
	step [26/244], loss=128.7055
	step [27/244], loss=142.4111
	step [28/244], loss=122.2174
	step [29/244], loss=160.3602
	step [30/244], loss=131.3118
	step [31/244], loss=124.5168
	step [32/244], loss=129.1723
	step [33/244], loss=126.0056
	step [34/244], loss=135.8478
	step [35/244], loss=117.7062
	step [36/244], loss=154.2100
	step [37/244], loss=132.2620
	step [38/244], loss=160.0597
	step [39/244], loss=140.7771
	step [40/244], loss=168.7963
	step [41/244], loss=128.8104
	step [42/244], loss=115.4013
	step [43/244], loss=135.1388
	step [44/244], loss=120.0741
	step [45/244], loss=112.6570
	step [46/244], loss=109.3269
	step [47/244], loss=119.6951
	step [48/244], loss=142.3316
	step [49/244], loss=119.2259
	step [50/244], loss=140.8056
	step [51/244], loss=145.0629
	step [52/244], loss=135.5489
	step [53/244], loss=123.0266
	step [54/244], loss=129.0696
	step [55/244], loss=155.6538
	step [56/244], loss=96.9541
	step [57/244], loss=144.0838
	step [58/244], loss=125.6645
	step [59/244], loss=133.0354
	step [60/244], loss=135.0436
	step [61/244], loss=141.8758
	step [62/244], loss=119.9801
	step [63/244], loss=130.6985
	step [64/244], loss=109.9233
	step [65/244], loss=155.7647
	step [66/244], loss=139.5497
	step [67/244], loss=134.2124
	step [68/244], loss=135.6591
	step [69/244], loss=149.1947
	step [70/244], loss=124.1940
	step [71/244], loss=123.9013
	step [72/244], loss=136.4695
	step [73/244], loss=129.7744
	step [74/244], loss=126.5711
	step [75/244], loss=136.0988
	step [76/244], loss=137.3519
	step [77/244], loss=143.0162
	step [78/244], loss=128.2949
	step [79/244], loss=123.3669
	step [80/244], loss=128.9081
	step [81/244], loss=147.0135
	step [82/244], loss=118.1688
	step [83/244], loss=140.3320
	step [84/244], loss=128.1816
	step [85/244], loss=134.7591
	step [86/244], loss=128.2005
	step [87/244], loss=130.8087
	step [88/244], loss=123.6055
	step [89/244], loss=136.8498
	step [90/244], loss=135.3432
	step [91/244], loss=127.1472
	step [92/244], loss=146.8656
	step [93/244], loss=124.2627
	step [94/244], loss=128.5882
	step [95/244], loss=108.3218
	step [96/244], loss=129.3156
	step [97/244], loss=161.2097
	step [98/244], loss=121.1447
	step [99/244], loss=148.9540
	step [100/244], loss=152.5146
	step [101/244], loss=123.0577
	step [102/244], loss=130.4699
	step [103/244], loss=124.0106
	step [104/244], loss=140.3853
	step [105/244], loss=127.3887
	step [106/244], loss=127.2917
	step [107/244], loss=137.0995
	step [108/244], loss=120.1762
	step [109/244], loss=138.2756
	step [110/244], loss=131.0381
	step [111/244], loss=145.8097
	step [112/244], loss=117.8498
	step [113/244], loss=138.3019
	step [114/244], loss=120.5391
	step [115/244], loss=141.9661
	step [116/244], loss=120.8287
	step [117/244], loss=134.3352
	step [118/244], loss=120.7937
	step [119/244], loss=153.6926
	step [120/244], loss=129.6335
	step [121/244], loss=121.0905
	step [122/244], loss=110.6555
	step [123/244], loss=136.8807
	step [124/244], loss=128.5063
	step [125/244], loss=130.9706
	step [126/244], loss=140.6594
	step [127/244], loss=117.5970
	step [128/244], loss=174.8535
	step [129/244], loss=121.2765
	step [130/244], loss=117.5222
	step [131/244], loss=138.2944
	step [132/244], loss=108.9269
	step [133/244], loss=124.3416
	step [134/244], loss=150.5980
	step [135/244], loss=110.6217
	step [136/244], loss=157.3295
	step [137/244], loss=123.3233
	step [138/244], loss=140.5491
	step [139/244], loss=151.2233
	step [140/244], loss=135.4037
	step [141/244], loss=134.7469
	step [142/244], loss=113.8994
	step [143/244], loss=129.1033
	step [144/244], loss=112.2167
	step [145/244], loss=121.8566
	step [146/244], loss=140.7625
	step [147/244], loss=140.9670
	step [148/244], loss=125.9724
	step [149/244], loss=148.9926
	step [150/244], loss=138.3853
	step [151/244], loss=126.1780
	step [152/244], loss=122.3235
	step [153/244], loss=134.6131
	step [154/244], loss=135.8454
	step [155/244], loss=117.0757
	step [156/244], loss=149.1346
	step [157/244], loss=121.2823
	step [158/244], loss=154.5243
	step [159/244], loss=117.6896
	step [160/244], loss=144.1449
	step [161/244], loss=106.8014
	step [162/244], loss=115.6925
	step [163/244], loss=128.0698
	step [164/244], loss=141.9905
	step [165/244], loss=131.1849
	step [166/244], loss=133.2191
	step [167/244], loss=151.0687
	step [168/244], loss=138.0218
	step [169/244], loss=116.2575
	step [170/244], loss=141.3989
	step [171/244], loss=143.6511
	step [172/244], loss=138.9917
	step [173/244], loss=140.6294
	step [174/244], loss=129.5979
	step [175/244], loss=101.2212
	step [176/244], loss=141.6748
	step [177/244], loss=124.2296
	step [178/244], loss=118.3614
	step [179/244], loss=130.1062
	step [180/244], loss=129.5441
	step [181/244], loss=118.2664
	step [182/244], loss=133.7578
	step [183/244], loss=136.3127
	step [184/244], loss=129.7885
	step [185/244], loss=121.3979
	step [186/244], loss=140.1213
	step [187/244], loss=140.1566
	step [188/244], loss=138.3210
	step [189/244], loss=112.0258
	step [190/244], loss=121.1054
	step [191/244], loss=124.1655
	step [192/244], loss=124.8234
	step [193/244], loss=118.3061
	step [194/244], loss=145.7719
	step [195/244], loss=126.9700
	step [196/244], loss=106.7912
	step [197/244], loss=130.2085
	step [198/244], loss=128.1371
	step [199/244], loss=128.8323
	step [200/244], loss=129.5709
	step [201/244], loss=147.1982
	step [202/244], loss=128.7211
	step [203/244], loss=97.5217
	step [204/244], loss=154.8505
	step [205/244], loss=136.0008
	step [206/244], loss=155.4809
	step [207/244], loss=153.8999
	step [208/244], loss=128.0445
	step [209/244], loss=143.5662
	step [210/244], loss=132.4359
	step [211/244], loss=98.8450
	step [212/244], loss=105.9644
	step [213/244], loss=143.3687
	step [214/244], loss=127.5936
	step [215/244], loss=152.1148
	step [216/244], loss=129.8379
	step [217/244], loss=119.7566
	step [218/244], loss=116.8897
	step [219/244], loss=125.2997
	step [220/244], loss=138.1265
	step [221/244], loss=148.6628
	step [222/244], loss=137.4186
	step [223/244], loss=129.6257
	step [224/244], loss=143.2711
	step [225/244], loss=114.2831
	step [226/244], loss=108.6717
	step [227/244], loss=150.1145
	step [228/244], loss=134.7806
	step [229/244], loss=145.2451
	step [230/244], loss=136.6159
	step [231/244], loss=131.6578
	step [232/244], loss=132.9030
	step [233/244], loss=120.5740
	step [234/244], loss=130.0183
	step [235/244], loss=137.1852
	step [236/244], loss=127.2265
	step [237/244], loss=130.2450
	step [238/244], loss=122.6422
	step [239/244], loss=128.6670
	step [240/244], loss=140.2992
	step [241/244], loss=119.1079
	step [242/244], loss=122.0778
	step [243/244], loss=131.4437
	step [244/244], loss=1.2415
	Evaluating
	loss=0.0855, precision=0.3766, recall=0.9274, f1=0.5356
Training epoch 5
	step [1/244], loss=114.6212
	step [2/244], loss=132.2698
	step [3/244], loss=147.7043
	step [4/244], loss=139.3475
	step [5/244], loss=143.8160
	step [6/244], loss=143.3278
	step [7/244], loss=116.0735
	step [8/244], loss=138.6137
	step [9/244], loss=135.8263
	step [10/244], loss=131.7487
	step [11/244], loss=128.6559
	step [12/244], loss=122.2165
	step [13/244], loss=146.6641
	step [14/244], loss=119.1084
	step [15/244], loss=130.3433
	step [16/244], loss=119.6098
	step [17/244], loss=117.7401
	step [18/244], loss=142.3256
	step [19/244], loss=107.3979
	step [20/244], loss=133.5409
	step [21/244], loss=140.1254
	step [22/244], loss=172.1714
	step [23/244], loss=141.6174
	step [24/244], loss=114.6253
	step [25/244], loss=137.7165
	step [26/244], loss=142.6253
	step [27/244], loss=157.0923
	step [28/244], loss=141.6317
	step [29/244], loss=129.7509
	step [30/244], loss=126.4814
	step [31/244], loss=120.7120
	step [32/244], loss=134.0979
	step [33/244], loss=136.4788
	step [34/244], loss=129.7878
	step [35/244], loss=131.2046
	step [36/244], loss=105.8316
	step [37/244], loss=136.2086
	step [38/244], loss=110.4650
	step [39/244], loss=126.9544
	step [40/244], loss=125.7883
	step [41/244], loss=123.4353
	step [42/244], loss=147.2643
	step [43/244], loss=107.8671
	step [44/244], loss=138.2059
	step [45/244], loss=147.4052
	step [46/244], loss=132.5275
	step [47/244], loss=130.7517
	step [48/244], loss=116.3848
	step [49/244], loss=113.8550
	step [50/244], loss=143.9520
	step [51/244], loss=113.4697
	step [52/244], loss=129.1082
	step [53/244], loss=138.1478
	step [54/244], loss=112.6752
	step [55/244], loss=121.9214
	step [56/244], loss=109.3278
	step [57/244], loss=118.1361
	step [58/244], loss=113.3237
	step [59/244], loss=146.5710
	step [60/244], loss=129.1935
	step [61/244], loss=122.0528
	step [62/244], loss=147.4721
	step [63/244], loss=120.6983
	step [64/244], loss=115.4273
	step [65/244], loss=140.4712
	step [66/244], loss=129.2092
	step [67/244], loss=128.1567
	step [68/244], loss=125.0679
	step [69/244], loss=136.1087
	step [70/244], loss=124.8011
	step [71/244], loss=130.1845
	step [72/244], loss=118.4521
	step [73/244], loss=138.7826
	step [74/244], loss=123.3810
	step [75/244], loss=127.0215
	step [76/244], loss=112.6915
	step [77/244], loss=125.6767
	step [78/244], loss=132.3558
	step [79/244], loss=132.6742
	step [80/244], loss=117.0000
	step [81/244], loss=137.6617
	step [82/244], loss=128.9617
	step [83/244], loss=158.5105
	step [84/244], loss=111.0060
	step [85/244], loss=115.0941
	step [86/244], loss=116.4264
	step [87/244], loss=123.5211
	step [88/244], loss=122.4675
	step [89/244], loss=115.8050
	step [90/244], loss=112.1088
	step [91/244], loss=116.3574
	step [92/244], loss=134.0976
	step [93/244], loss=150.3210
	step [94/244], loss=114.0313
	step [95/244], loss=115.8163
	step [96/244], loss=115.9104
	step [97/244], loss=128.2147
	step [98/244], loss=118.0104
	step [99/244], loss=126.2956
	step [100/244], loss=120.5692
	step [101/244], loss=115.7831
	step [102/244], loss=156.9529
	step [103/244], loss=115.3042
	step [104/244], loss=109.2777
	step [105/244], loss=119.1857
	step [106/244], loss=120.7214
	step [107/244], loss=140.4854
	step [108/244], loss=124.6174
	step [109/244], loss=130.7066
	step [110/244], loss=142.0725
	step [111/244], loss=132.6506
	step [112/244], loss=145.1497
	step [113/244], loss=143.5367
	step [114/244], loss=142.1421
	step [115/244], loss=121.1730
	step [116/244], loss=130.9082
	step [117/244], loss=121.2084
	step [118/244], loss=114.4813
	step [119/244], loss=97.0502
	step [120/244], loss=109.0259
	step [121/244], loss=102.9587
	step [122/244], loss=135.7771
	step [123/244], loss=124.3319
	step [124/244], loss=124.2885
	step [125/244], loss=132.5805
	step [126/244], loss=123.0799
	step [127/244], loss=131.5850
	step [128/244], loss=130.0544
	step [129/244], loss=137.8810
	step [130/244], loss=127.1931
	step [131/244], loss=146.2084
	step [132/244], loss=126.0915
	step [133/244], loss=132.4215
	step [134/244], loss=128.1284
	step [135/244], loss=118.4797
	step [136/244], loss=131.0632
	step [137/244], loss=109.0664
	step [138/244], loss=127.4082
	step [139/244], loss=124.1004
	step [140/244], loss=133.1771
	step [141/244], loss=128.3191
	step [142/244], loss=114.4145
	step [143/244], loss=108.0477
	step [144/244], loss=113.4996
	step [145/244], loss=131.2061
	step [146/244], loss=127.4426
	step [147/244], loss=135.1173
	step [148/244], loss=137.1196
	step [149/244], loss=106.8847
	step [150/244], loss=139.3715
	step [151/244], loss=112.1185
	step [152/244], loss=120.5382
	step [153/244], loss=132.5930
	step [154/244], loss=115.1892
	step [155/244], loss=109.2454
	step [156/244], loss=113.6697
	step [157/244], loss=134.0484
	step [158/244], loss=132.6586
	step [159/244], loss=100.4088
	step [160/244], loss=132.5523
	step [161/244], loss=132.0778
	step [162/244], loss=129.2735
	step [163/244], loss=116.7455
	step [164/244], loss=148.2518
	step [165/244], loss=120.0284
	step [166/244], loss=117.1765
	step [167/244], loss=120.4156
	step [168/244], loss=127.5955
	step [169/244], loss=113.5965
	step [170/244], loss=113.7791
	step [171/244], loss=127.7917
	step [172/244], loss=113.1515
	step [173/244], loss=107.2138
	step [174/244], loss=113.6893
	step [175/244], loss=153.9748
	step [176/244], loss=118.3346
	step [177/244], loss=125.7642
	step [178/244], loss=138.0592
	step [179/244], loss=135.5568
	step [180/244], loss=126.0978
	step [181/244], loss=131.0255
	step [182/244], loss=130.2440
	step [183/244], loss=117.8910
	step [184/244], loss=118.1430
	step [185/244], loss=140.0563
	step [186/244], loss=114.9836
	step [187/244], loss=132.7939
	step [188/244], loss=134.0040
	step [189/244], loss=109.8921
	step [190/244], loss=103.1717
	step [191/244], loss=145.6645
	step [192/244], loss=113.1346
	step [193/244], loss=110.5391
	step [194/244], loss=123.5638
	step [195/244], loss=122.5200
	step [196/244], loss=89.6277
	step [197/244], loss=109.7659
	step [198/244], loss=143.3508
	step [199/244], loss=116.7030
	step [200/244], loss=145.0360
	step [201/244], loss=119.9331
	step [202/244], loss=118.7980
	step [203/244], loss=112.8445
	step [204/244], loss=120.4443
	step [205/244], loss=130.3515
	step [206/244], loss=118.1987
	step [207/244], loss=133.0750
	step [208/244], loss=137.4556
	step [209/244], loss=134.6888
	step [210/244], loss=130.7921
	step [211/244], loss=111.4301
	step [212/244], loss=99.4209
	step [213/244], loss=136.0000
	step [214/244], loss=118.4654
	step [215/244], loss=116.5570
	step [216/244], loss=118.9013
	step [217/244], loss=103.5709
	step [218/244], loss=123.2509
	step [219/244], loss=137.1847
	step [220/244], loss=123.6332
	step [221/244], loss=122.4393
	step [222/244], loss=133.0724
	step [223/244], loss=120.1351
	step [224/244], loss=119.8978
	step [225/244], loss=123.8242
	step [226/244], loss=117.3009
	step [227/244], loss=131.0866
	step [228/244], loss=134.8421
	step [229/244], loss=130.6230
	step [230/244], loss=109.3306
	step [231/244], loss=107.7399
	step [232/244], loss=118.3774
	step [233/244], loss=115.8115
	step [234/244], loss=106.8663
	step [235/244], loss=112.2607
	step [236/244], loss=109.1564
	step [237/244], loss=128.7896
	step [238/244], loss=136.6802
	step [239/244], loss=124.5328
	step [240/244], loss=112.5778
	step [241/244], loss=119.9115
	step [242/244], loss=122.6801
	step [243/244], loss=146.3338
	step [244/244], loss=3.6956
	Evaluating
	loss=0.0662, precision=0.3673, recall=0.9294, f1=0.5266
Training epoch 6
	step [1/244], loss=137.6162
	step [2/244], loss=115.7966
	step [3/244], loss=124.4365
	step [4/244], loss=139.4548
	step [5/244], loss=131.3069
	step [6/244], loss=117.2214
	step [7/244], loss=137.1695
	step [8/244], loss=128.8885
	step [9/244], loss=130.0061
	step [10/244], loss=120.4471
	step [11/244], loss=112.8703
	step [12/244], loss=127.7406
	step [13/244], loss=115.0132
	step [14/244], loss=118.9637
	step [15/244], loss=145.8802
	step [16/244], loss=115.0736
	step [17/244], loss=113.5607
	step [18/244], loss=125.6216
	step [19/244], loss=117.5234
	step [20/244], loss=135.3489
	step [21/244], loss=140.8079
	step [22/244], loss=123.7446
	step [23/244], loss=95.8250
	step [24/244], loss=135.0043
	step [25/244], loss=131.1060
	step [26/244], loss=127.3545
	step [27/244], loss=123.4704
	step [28/244], loss=119.4002
	step [29/244], loss=138.2260
	step [30/244], loss=120.4443
	step [31/244], loss=110.7114
	step [32/244], loss=99.1373
	step [33/244], loss=134.9868
	step [34/244], loss=129.7902
	step [35/244], loss=103.7982
	step [36/244], loss=125.5419
	step [37/244], loss=124.2450
	step [38/244], loss=120.7986
	step [39/244], loss=106.8578
	step [40/244], loss=115.4317
	step [41/244], loss=113.5281
	step [42/244], loss=153.9821
	step [43/244], loss=128.3226
	step [44/244], loss=141.0110
	step [45/244], loss=136.1153
	step [46/244], loss=126.5059
	step [47/244], loss=136.5648
	step [48/244], loss=122.5408
	step [49/244], loss=111.6733
	step [50/244], loss=125.1016
	step [51/244], loss=133.6236
	step [52/244], loss=110.7936
	step [53/244], loss=125.8793
	step [54/244], loss=109.4185
	step [55/244], loss=96.3648
	step [56/244], loss=133.3033
	step [57/244], loss=110.7506
	step [58/244], loss=127.8704
	step [59/244], loss=115.5009
	step [60/244], loss=115.2209
	step [61/244], loss=128.3484
	step [62/244], loss=122.0931
	step [63/244], loss=108.5690
	step [64/244], loss=123.0093
	step [65/244], loss=117.3943
	step [66/244], loss=136.7047
	step [67/244], loss=113.0518
	step [68/244], loss=126.5857
	step [69/244], loss=114.6957
	step [70/244], loss=158.5950
	step [71/244], loss=128.7083
	step [72/244], loss=141.4047
	step [73/244], loss=145.1679
	step [74/244], loss=126.4243
	step [75/244], loss=151.6993
	step [76/244], loss=124.7846
	step [77/244], loss=115.1327
	step [78/244], loss=137.3842
	step [79/244], loss=120.4670
	step [80/244], loss=127.3766
	step [81/244], loss=136.6136
	step [82/244], loss=143.5364
	step [83/244], loss=110.7462
	step [84/244], loss=88.6531
	step [85/244], loss=102.7485
	step [86/244], loss=112.9822
	step [87/244], loss=130.3429
	step [88/244], loss=102.6191
	step [89/244], loss=102.5598
	step [90/244], loss=113.4556
	step [91/244], loss=123.3382
	step [92/244], loss=133.6095
	step [93/244], loss=149.0651
	step [94/244], loss=93.7048
	step [95/244], loss=114.1854
	step [96/244], loss=107.2159
	step [97/244], loss=130.5750
	step [98/244], loss=121.0411
	step [99/244], loss=119.7600
	step [100/244], loss=124.1679
	step [101/244], loss=110.1312
	step [102/244], loss=125.1776
	step [103/244], loss=124.7676
	step [104/244], loss=136.8005
	step [105/244], loss=121.2745
	step [106/244], loss=119.7884
	step [107/244], loss=144.8204
	step [108/244], loss=127.2318
	step [109/244], loss=108.8153
	step [110/244], loss=121.5110
	step [111/244], loss=127.3573
	step [112/244], loss=124.7312
	step [113/244], loss=111.0796
	step [114/244], loss=106.5109
	step [115/244], loss=136.4907
	step [116/244], loss=121.6112
	step [117/244], loss=122.6774
	step [118/244], loss=114.7779
	step [119/244], loss=126.3441
	step [120/244], loss=123.1912
	step [121/244], loss=94.2741
	step [122/244], loss=118.8554
	step [123/244], loss=124.3788
	step [124/244], loss=117.1202
	step [125/244], loss=125.1462
	step [126/244], loss=124.2822
	step [127/244], loss=106.3338
	step [128/244], loss=107.9852
	step [129/244], loss=119.7472
	step [130/244], loss=97.2053
	step [131/244], loss=146.2791
	step [132/244], loss=110.9766
	step [133/244], loss=137.9576
	step [134/244], loss=115.1344
	step [135/244], loss=126.0493
	step [136/244], loss=110.9206
	step [137/244], loss=129.8262
	step [138/244], loss=147.8853
	step [139/244], loss=113.4900
	step [140/244], loss=110.3246
	step [141/244], loss=100.5196
	step [142/244], loss=105.8240
	step [143/244], loss=132.0042
	step [144/244], loss=134.0357
	step [145/244], loss=136.0132
	step [146/244], loss=129.7242
	step [147/244], loss=130.7441
	step [148/244], loss=102.7972
	step [149/244], loss=120.3627
	step [150/244], loss=87.2305
	step [151/244], loss=103.2866
	step [152/244], loss=116.0142
	step [153/244], loss=138.3597
	step [154/244], loss=129.4207
	step [155/244], loss=139.1343
	step [156/244], loss=127.3272
	step [157/244], loss=102.1285
	step [158/244], loss=118.3151
	step [159/244], loss=114.2133
	step [160/244], loss=105.4341
	step [161/244], loss=153.0613
	step [162/244], loss=115.6952
	step [163/244], loss=152.3561
	step [164/244], loss=130.0576
	step [165/244], loss=113.3901
	step [166/244], loss=109.6838
	step [167/244], loss=128.2701
	step [168/244], loss=108.1516
	step [169/244], loss=107.6660
	step [170/244], loss=124.6429
	step [171/244], loss=131.4097
	step [172/244], loss=144.1019
	step [173/244], loss=117.0728
	step [174/244], loss=111.6205
	step [175/244], loss=117.1155
	step [176/244], loss=117.4334
	step [177/244], loss=145.3161
	step [178/244], loss=132.1655
	step [179/244], loss=110.0759
	step [180/244], loss=106.5057
	step [181/244], loss=128.8914
	step [182/244], loss=117.7006
	step [183/244], loss=99.5751
	step [184/244], loss=103.3586
	step [185/244], loss=104.2469
	step [186/244], loss=99.2808
	step [187/244], loss=93.9764
	step [188/244], loss=139.3068
	step [189/244], loss=121.5553
	step [190/244], loss=107.0244
	step [191/244], loss=122.1388
	step [192/244], loss=121.3768
	step [193/244], loss=114.4883
	step [194/244], loss=123.8791
	step [195/244], loss=122.4563
	step [196/244], loss=115.4496
	step [197/244], loss=117.6070
	step [198/244], loss=118.8077
	step [199/244], loss=122.4279
	step [200/244], loss=129.6596
	step [201/244], loss=108.2137
	step [202/244], loss=122.1151
	step [203/244], loss=111.0710
	step [204/244], loss=112.4033
	step [205/244], loss=129.6347
	step [206/244], loss=119.2331
	step [207/244], loss=129.6612
	step [208/244], loss=147.4481
	step [209/244], loss=103.7765
	step [210/244], loss=146.3992
	step [211/244], loss=114.2958
	step [212/244], loss=129.4680
	step [213/244], loss=127.8188
	step [214/244], loss=132.2709
	step [215/244], loss=118.4089
	step [216/244], loss=113.3514
	step [217/244], loss=104.9972
	step [218/244], loss=119.4983
	step [219/244], loss=114.1533
	step [220/244], loss=144.7945
	step [221/244], loss=90.8550
	step [222/244], loss=115.9758
	step [223/244], loss=131.2427
	step [224/244], loss=116.1572
	step [225/244], loss=108.5882
	step [226/244], loss=108.9618
	step [227/244], loss=139.7734
	step [228/244], loss=112.3435
	step [229/244], loss=138.0695
	step [230/244], loss=117.9102
	step [231/244], loss=112.4723
	step [232/244], loss=121.1553
	step [233/244], loss=135.8049
	step [234/244], loss=122.7697
	step [235/244], loss=116.6686
	step [236/244], loss=119.6017
	step [237/244], loss=132.0606
	step [238/244], loss=130.5347
	step [239/244], loss=89.3383
	step [240/244], loss=126.0655
	step [241/244], loss=118.9959
	step [242/244], loss=97.0722
	step [243/244], loss=96.5158
	step [244/244], loss=1.0033
	Evaluating
	loss=0.0531, precision=0.3505, recall=0.9288, f1=0.5090
Training epoch 7
	step [1/244], loss=109.9425
	step [2/244], loss=115.5306
	step [3/244], loss=108.2678
	step [4/244], loss=119.3977
	step [5/244], loss=106.0710
	step [6/244], loss=121.8884
	step [7/244], loss=114.2117
	step [8/244], loss=128.1284
	step [9/244], loss=115.8685
	step [10/244], loss=118.2013
	step [11/244], loss=125.8708
	step [12/244], loss=132.9331
	step [13/244], loss=100.8056
	step [14/244], loss=128.0997
	step [15/244], loss=121.1667
	step [16/244], loss=130.7983
	step [17/244], loss=130.5930
	step [18/244], loss=126.3385
	step [19/244], loss=120.8883
	step [20/244], loss=105.3935
	step [21/244], loss=114.9915
	step [22/244], loss=113.6851
	step [23/244], loss=123.2705
	step [24/244], loss=122.3208
	step [25/244], loss=119.5765
	step [26/244], loss=129.4519
	step [27/244], loss=100.2897
	step [28/244], loss=114.1846
	step [29/244], loss=105.7656
	step [30/244], loss=107.3123
	step [31/244], loss=108.0038
	step [32/244], loss=135.5546
	step [33/244], loss=122.2566
	step [34/244], loss=102.2888
	step [35/244], loss=132.3075
	step [36/244], loss=107.7595
	step [37/244], loss=108.9800
	step [38/244], loss=113.3022
	step [39/244], loss=134.3725
	step [40/244], loss=152.0206
	step [41/244], loss=100.6259
	step [42/244], loss=96.0774
	step [43/244], loss=138.1606
	step [44/244], loss=122.2985
	step [45/244], loss=140.0172
	step [46/244], loss=105.9246
	step [47/244], loss=125.3846
	step [48/244], loss=111.6818
	step [49/244], loss=121.8406
	step [50/244], loss=117.0180
	step [51/244], loss=106.3014
	step [52/244], loss=144.3234
	step [53/244], loss=135.0682
	step [54/244], loss=110.6863
	step [55/244], loss=125.7161
	step [56/244], loss=105.9352
	step [57/244], loss=109.2481
	step [58/244], loss=136.2344
	step [59/244], loss=133.0452
	step [60/244], loss=127.8582
	step [61/244], loss=137.0497
	step [62/244], loss=120.5976
	step [63/244], loss=129.5993
	step [64/244], loss=122.1623
	step [65/244], loss=120.8100
	step [66/244], loss=110.1593
	step [67/244], loss=111.1448
	step [68/244], loss=118.1718
	step [69/244], loss=127.9287
	step [70/244], loss=121.5584
	step [71/244], loss=126.3329
	step [72/244], loss=119.7966
	step [73/244], loss=133.7348
	step [74/244], loss=137.5221
	step [75/244], loss=146.9036
	step [76/244], loss=86.4086
	step [77/244], loss=101.6347
	step [78/244], loss=122.3581
	step [79/244], loss=96.2779
	step [80/244], loss=127.3303
	step [81/244], loss=98.9810
	step [82/244], loss=99.3566
	step [83/244], loss=109.7469
	step [84/244], loss=119.2486
	step [85/244], loss=110.2932
	step [86/244], loss=123.3694
	step [87/244], loss=105.8060
	step [88/244], loss=101.0506
	step [89/244], loss=108.5213
	step [90/244], loss=104.6434
	step [91/244], loss=120.6961
	step [92/244], loss=126.0034
	step [93/244], loss=126.8313
	step [94/244], loss=118.9441
	step [95/244], loss=118.3905
	step [96/244], loss=106.2568
	step [97/244], loss=127.1122
	step [98/244], loss=116.9135
	step [99/244], loss=120.0432
	step [100/244], loss=122.7967
	step [101/244], loss=106.3647
	step [102/244], loss=116.3516
	step [103/244], loss=150.6053
	step [104/244], loss=109.8988
	step [105/244], loss=108.7582
	step [106/244], loss=132.5967
	step [107/244], loss=101.8028
	step [108/244], loss=107.2804
	step [109/244], loss=125.6914
	step [110/244], loss=111.0373
	step [111/244], loss=124.3238
	step [112/244], loss=120.7526
	step [113/244], loss=114.3523
	step [114/244], loss=129.2709
	step [115/244], loss=119.5747
	step [116/244], loss=115.1728
	step [117/244], loss=124.1995
	step [118/244], loss=144.3206
	step [119/244], loss=121.5755
	step [120/244], loss=105.6326
	step [121/244], loss=108.7704
	step [122/244], loss=101.1514
	step [123/244], loss=109.6158
	step [124/244], loss=126.3631
	step [125/244], loss=126.5449
	step [126/244], loss=107.3240
	step [127/244], loss=104.5039
	step [128/244], loss=100.0298
	step [129/244], loss=103.3895
	step [130/244], loss=95.5306
	step [131/244], loss=130.4201
	step [132/244], loss=116.4441
	step [133/244], loss=110.8997
	step [134/244], loss=120.6037
	step [135/244], loss=120.7520
	step [136/244], loss=138.2412
	step [137/244], loss=115.8235
	step [138/244], loss=119.6919
	step [139/244], loss=109.3239
	step [140/244], loss=98.6468
	step [141/244], loss=115.9945
	step [142/244], loss=131.0194
	step [143/244], loss=99.7324
	step [144/244], loss=122.2431
	step [145/244], loss=111.6683
	step [146/244], loss=118.2148
	step [147/244], loss=126.9624
	step [148/244], loss=128.2017
	step [149/244], loss=166.5678
	step [150/244], loss=97.6656
	step [151/244], loss=116.1707
	step [152/244], loss=129.5140
	step [153/244], loss=119.0503
	step [154/244], loss=126.9148
	step [155/244], loss=100.6198
	step [156/244], loss=122.0939
	step [157/244], loss=152.7321
	step [158/244], loss=123.2206
	step [159/244], loss=112.2804
	step [160/244], loss=129.0251
	step [161/244], loss=124.1367
	step [162/244], loss=113.8372
	step [163/244], loss=104.5791
	step [164/244], loss=121.3479
	step [165/244], loss=103.2736
	step [166/244], loss=117.2656
	step [167/244], loss=129.4718
	step [168/244], loss=110.2680
	step [169/244], loss=107.6729
	step [170/244], loss=99.8933
	step [171/244], loss=109.6869
	step [172/244], loss=134.5350
	step [173/244], loss=142.3548
	step [174/244], loss=112.4970
	step [175/244], loss=116.5203
	step [176/244], loss=112.1540
	step [177/244], loss=124.2774
	step [178/244], loss=105.2077
	step [179/244], loss=117.4953
	step [180/244], loss=114.5430
	step [181/244], loss=115.2000
	step [182/244], loss=129.2642
	step [183/244], loss=114.5161
	step [184/244], loss=135.2007
	step [185/244], loss=98.7014
	step [186/244], loss=121.7895
	step [187/244], loss=141.3114
	step [188/244], loss=135.0491
	step [189/244], loss=124.0792
	step [190/244], loss=122.2913
	step [191/244], loss=117.2674
	step [192/244], loss=126.7332
	step [193/244], loss=135.9386
	step [194/244], loss=118.9913
	step [195/244], loss=94.6326
	step [196/244], loss=119.2025
	step [197/244], loss=104.6779
	step [198/244], loss=111.2538
	step [199/244], loss=101.2035
	step [200/244], loss=122.8459
	step [201/244], loss=109.5896
	step [202/244], loss=130.4372
	step [203/244], loss=102.4656
	step [204/244], loss=127.6691
	step [205/244], loss=129.4125
	step [206/244], loss=115.4244
	step [207/244], loss=104.3140
	step [208/244], loss=123.6528
	step [209/244], loss=109.4011
	step [210/244], loss=119.7632
	step [211/244], loss=109.7696
	step [212/244], loss=109.9022
	step [213/244], loss=116.9450
	step [214/244], loss=115.2400
	step [215/244], loss=108.3351
	step [216/244], loss=126.9460
	step [217/244], loss=125.0098
	step [218/244], loss=123.8754
	step [219/244], loss=118.9245
	step [220/244], loss=122.4845
	step [221/244], loss=131.2614
	step [222/244], loss=118.8104
	step [223/244], loss=109.6303
	step [224/244], loss=110.9911
	step [225/244], loss=154.5233
	step [226/244], loss=89.4487
	step [227/244], loss=104.2287
	step [228/244], loss=139.9776
	step [229/244], loss=116.8737
	step [230/244], loss=131.3415
	step [231/244], loss=112.5987
	step [232/244], loss=121.7665
	step [233/244], loss=115.4840
	step [234/244], loss=101.0910
	step [235/244], loss=148.0328
	step [236/244], loss=112.8411
	step [237/244], loss=99.1670
	step [238/244], loss=109.3923
	step [239/244], loss=112.4036
	step [240/244], loss=104.1835
	step [241/244], loss=115.0279
	step [242/244], loss=129.7558
	step [243/244], loss=104.0755
	step [244/244], loss=2.1765
	Evaluating
	loss=0.0444, precision=0.3401, recall=0.9310, f1=0.4981
Training epoch 8
	step [1/244], loss=151.8844
	step [2/244], loss=113.3684
	step [3/244], loss=120.1456
	step [4/244], loss=91.5658
	step [5/244], loss=134.7625
	step [6/244], loss=102.6831
	step [7/244], loss=129.3956
	step [8/244], loss=126.1497
	step [9/244], loss=103.4117
	step [10/244], loss=114.7843
	step [11/244], loss=120.0599
	step [12/244], loss=128.3053
	step [13/244], loss=95.4785
	step [14/244], loss=107.5715
	step [15/244], loss=137.3501
	step [16/244], loss=127.1374
	step [17/244], loss=118.5726
	step [18/244], loss=139.2854
	step [19/244], loss=104.1498
	step [20/244], loss=130.7080
	step [21/244], loss=123.0645
	step [22/244], loss=115.9484
	step [23/244], loss=123.1053
	step [24/244], loss=129.8063
	step [25/244], loss=101.6873
	step [26/244], loss=118.3210
	step [27/244], loss=130.1792
	step [28/244], loss=116.4410
	step [29/244], loss=103.1761
	step [30/244], loss=104.3753
	step [31/244], loss=107.8475
	step [32/244], loss=117.0301
	step [33/244], loss=105.8941
	step [34/244], loss=126.7696
	step [35/244], loss=116.0711
	step [36/244], loss=102.0432
	step [37/244], loss=125.7332
	step [38/244], loss=122.2060
	step [39/244], loss=118.5785
	step [40/244], loss=119.2929
	step [41/244], loss=88.9300
	step [42/244], loss=98.0284
	step [43/244], loss=118.0103
	step [44/244], loss=129.9662
	step [45/244], loss=105.0596
	step [46/244], loss=122.7888
	step [47/244], loss=127.0611
	step [48/244], loss=140.4761
	step [49/244], loss=101.3270
	step [50/244], loss=114.1125
	step [51/244], loss=136.3379
	step [52/244], loss=137.0158
	step [53/244], loss=114.1251
	step [54/244], loss=104.8138
	step [55/244], loss=119.1221
	step [56/244], loss=132.4479
	step [57/244], loss=124.5907
	step [58/244], loss=122.2567
	step [59/244], loss=122.1016
	step [60/244], loss=107.5714
	step [61/244], loss=115.7424
	step [62/244], loss=107.3152
	step [63/244], loss=129.0153
	step [64/244], loss=112.5883
	step [65/244], loss=136.6315
	step [66/244], loss=132.4807
	step [67/244], loss=120.3178
	step [68/244], loss=120.6138
	step [69/244], loss=120.4989
	step [70/244], loss=114.7327
	step [71/244], loss=127.4601
	step [72/244], loss=98.1469
	step [73/244], loss=146.3100
	step [74/244], loss=107.9960
	step [75/244], loss=120.1259
	step [76/244], loss=112.1466
	step [77/244], loss=125.7806
	step [78/244], loss=130.5199
	step [79/244], loss=107.8759
	step [80/244], loss=90.8394
	step [81/244], loss=110.7773
	step [82/244], loss=89.0438
	step [83/244], loss=115.2767
	step [84/244], loss=119.4727
	step [85/244], loss=123.5133
	step [86/244], loss=103.6954
	step [87/244], loss=100.5551
	step [88/244], loss=113.6653
	step [89/244], loss=129.5778
	step [90/244], loss=111.3008
	step [91/244], loss=123.3340
	step [92/244], loss=127.0643
	step [93/244], loss=140.2267
	step [94/244], loss=111.8680
	step [95/244], loss=117.6816
	step [96/244], loss=105.4140
	step [97/244], loss=124.1403
	step [98/244], loss=104.3669
	step [99/244], loss=115.4544
	step [100/244], loss=118.5718
	step [101/244], loss=108.0971
	step [102/244], loss=126.9051
	step [103/244], loss=96.9583
	step [104/244], loss=122.0883
	step [105/244], loss=118.6954
	step [106/244], loss=106.2091
	step [107/244], loss=108.6382
	step [108/244], loss=96.6541
	step [109/244], loss=111.4010
	step [110/244], loss=108.9417
	step [111/244], loss=94.3637
	step [112/244], loss=128.5205
	step [113/244], loss=156.1868
	step [114/244], loss=98.3453
	step [115/244], loss=114.4310
	step [116/244], loss=123.2910
	step [117/244], loss=117.8763
	step [118/244], loss=125.0654
	step [119/244], loss=115.5728
	step [120/244], loss=97.9304
	step [121/244], loss=112.0442
	step [122/244], loss=101.8824
	step [123/244], loss=140.2458
	step [124/244], loss=112.8907
	step [125/244], loss=100.5722
	step [126/244], loss=101.5780
	step [127/244], loss=105.2160
	step [128/244], loss=97.7411
	step [129/244], loss=94.5546
	step [130/244], loss=109.9469
	step [131/244], loss=114.8722
	step [132/244], loss=117.8722
	step [133/244], loss=113.7498
	step [134/244], loss=126.9617
	step [135/244], loss=112.8073
	step [136/244], loss=110.2546
	step [137/244], loss=117.3846
	step [138/244], loss=136.4843
	step [139/244], loss=123.7036
	step [140/244], loss=100.5568
	step [141/244], loss=105.0175
	step [142/244], loss=138.2304
	step [143/244], loss=100.5775
	step [144/244], loss=125.4859
	step [145/244], loss=119.5249
	step [146/244], loss=106.5549
	step [147/244], loss=124.9774
	step [148/244], loss=106.1224
	step [149/244], loss=102.5546
	step [150/244], loss=105.0407
	step [151/244], loss=113.2916
	step [152/244], loss=129.8170
	step [153/244], loss=129.5960
	step [154/244], loss=112.4135
	step [155/244], loss=113.4185
	step [156/244], loss=100.9296
	step [157/244], loss=131.2439
	step [158/244], loss=118.7947
	step [159/244], loss=123.1450
	step [160/244], loss=105.8234
	step [161/244], loss=103.2727
	step [162/244], loss=114.4388
	step [163/244], loss=108.0766
	step [164/244], loss=109.4791
	step [165/244], loss=121.2052
	step [166/244], loss=113.2062
	step [167/244], loss=117.0207
	step [168/244], loss=130.4976
	step [169/244], loss=117.9419
	step [170/244], loss=113.2142
	step [171/244], loss=115.2112
	step [172/244], loss=124.2967
	step [173/244], loss=123.0448
	step [174/244], loss=120.2077
	step [175/244], loss=106.3586
	step [176/244], loss=112.4701
	step [177/244], loss=117.3702
	step [178/244], loss=87.6458
	step [179/244], loss=131.8896
	step [180/244], loss=121.7980
	step [181/244], loss=104.3773
	step [182/244], loss=111.4372
	step [183/244], loss=105.5810
	step [184/244], loss=107.3007
	step [185/244], loss=102.6705
	step [186/244], loss=118.8176
	step [187/244], loss=124.8688
	step [188/244], loss=133.5646
	step [189/244], loss=116.1369
	step [190/244], loss=109.6610
	step [191/244], loss=112.1126
	step [192/244], loss=106.1597
	step [193/244], loss=105.7567
	step [194/244], loss=125.6786
	step [195/244], loss=125.9109
	step [196/244], loss=120.3746
	step [197/244], loss=118.4516
	step [198/244], loss=127.3023
	step [199/244], loss=111.8927
	step [200/244], loss=114.2433
	step [201/244], loss=111.2142
	step [202/244], loss=117.6530
	step [203/244], loss=109.4395
	step [204/244], loss=110.2069
	step [205/244], loss=122.9276
	step [206/244], loss=126.1974
	step [207/244], loss=120.0317
	step [208/244], loss=118.0004
	step [209/244], loss=130.2500
	step [210/244], loss=100.2127
	step [211/244], loss=113.3457
	step [212/244], loss=105.1978
	step [213/244], loss=115.2846
	step [214/244], loss=102.7312
	step [215/244], loss=104.4790
	step [216/244], loss=111.2610
	step [217/244], loss=102.9485
	step [218/244], loss=125.6686
	step [219/244], loss=101.5747
	step [220/244], loss=112.7869
	step [221/244], loss=110.1881
	step [222/244], loss=99.9002
	step [223/244], loss=109.9321
	step [224/244], loss=125.8343
	step [225/244], loss=103.1168
	step [226/244], loss=117.6838
	step [227/244], loss=122.4251
	step [228/244], loss=137.8658
	step [229/244], loss=124.0669
	step [230/244], loss=104.8488
	step [231/244], loss=127.0120
	step [232/244], loss=115.3322
	step [233/244], loss=108.0537
	step [234/244], loss=109.2690
	step [235/244], loss=112.8921
	step [236/244], loss=116.1648
	step [237/244], loss=114.0620
	step [238/244], loss=130.1406
	step [239/244], loss=110.3054
	step [240/244], loss=115.5030
	step [241/244], loss=98.6316
	step [242/244], loss=99.3590
	step [243/244], loss=103.1734
	step [244/244], loss=3.4552
	Evaluating
	loss=0.0330, precision=0.4231, recall=0.9341, f1=0.5824
Training epoch 9
	step [1/244], loss=113.5207
	step [2/244], loss=101.2525
	step [3/244], loss=103.6218
	step [4/244], loss=92.8914
	step [5/244], loss=142.9524
	step [6/244], loss=120.3389
	step [7/244], loss=107.3005
	step [8/244], loss=100.0762
	step [9/244], loss=98.4006
	step [10/244], loss=103.4430
	step [11/244], loss=98.3496
	step [12/244], loss=134.5264
	step [13/244], loss=120.6387
	step [14/244], loss=114.4413
	step [15/244], loss=124.4063
	step [16/244], loss=102.9306
	step [17/244], loss=126.9927
	step [18/244], loss=120.9005
	step [19/244], loss=114.6962
	step [20/244], loss=105.6287
	step [21/244], loss=102.2455
	step [22/244], loss=109.6681
	step [23/244], loss=124.4670
	step [24/244], loss=116.3789
	step [25/244], loss=108.3454
	step [26/244], loss=137.4632
	step [27/244], loss=109.8845
	step [28/244], loss=126.9560
	step [29/244], loss=132.3290
	step [30/244], loss=134.2592
	step [31/244], loss=118.4378
	step [32/244], loss=97.2705
	step [33/244], loss=121.9717
	step [34/244], loss=134.3999
	step [35/244], loss=102.7976
	step [36/244], loss=105.9589
	step [37/244], loss=111.2783
	step [38/244], loss=117.4492
	step [39/244], loss=116.5254
	step [40/244], loss=93.9954
	step [41/244], loss=124.4066
	step [42/244], loss=108.3746
	step [43/244], loss=93.8173
	step [44/244], loss=99.4391
	step [45/244], loss=104.8644
	step [46/244], loss=92.5673
	step [47/244], loss=123.2595
	step [48/244], loss=109.3931
	step [49/244], loss=129.8165
	step [50/244], loss=106.1170
	step [51/244], loss=133.3724
	step [52/244], loss=106.2479
	step [53/244], loss=104.2362
	step [54/244], loss=101.4290
	step [55/244], loss=110.1163
	step [56/244], loss=111.5372
	step [57/244], loss=97.9502
	step [58/244], loss=134.9804
	step [59/244], loss=112.7901
	step [60/244], loss=100.8282
	step [61/244], loss=142.5756
	step [62/244], loss=107.9942
	step [63/244], loss=114.9723
	step [64/244], loss=121.7796
	step [65/244], loss=107.2850
	step [66/244], loss=109.7430
	step [67/244], loss=133.1522
	step [68/244], loss=106.1124
	step [69/244], loss=88.6704
	step [70/244], loss=102.7140
	step [71/244], loss=110.9129
	step [72/244], loss=135.8359
	step [73/244], loss=114.8274
	step [74/244], loss=95.7151
	step [75/244], loss=124.8385
	step [76/244], loss=89.0076
	step [77/244], loss=109.1001
	step [78/244], loss=127.0626
	step [79/244], loss=101.6618
	step [80/244], loss=114.1122
	step [81/244], loss=100.4632
	step [82/244], loss=125.6213
	step [83/244], loss=110.2254
	step [84/244], loss=104.7030
	step [85/244], loss=124.1364
	step [86/244], loss=124.4654
	step [87/244], loss=114.9913
	step [88/244], loss=115.4481
	step [89/244], loss=115.2150
	step [90/244], loss=109.6906
	step [91/244], loss=110.8836
	step [92/244], loss=115.2888
	step [93/244], loss=121.1644
	step [94/244], loss=116.4113
	step [95/244], loss=112.7283
	step [96/244], loss=108.1390
	step [97/244], loss=108.6699
	step [98/244], loss=96.7089
	step [99/244], loss=106.9875
	step [100/244], loss=129.2578
	step [101/244], loss=128.2787
	step [102/244], loss=127.8044
	step [103/244], loss=112.4022
	step [104/244], loss=101.3512
	step [105/244], loss=108.6819
	step [106/244], loss=104.8842
	step [107/244], loss=117.4532
	step [108/244], loss=109.5618
	step [109/244], loss=103.6317
	step [110/244], loss=84.8669
	step [111/244], loss=132.2324
	step [112/244], loss=135.3173
	step [113/244], loss=104.8281
	step [114/244], loss=118.4489
	step [115/244], loss=103.4077
	step [116/244], loss=108.5246
	step [117/244], loss=81.2539
	step [118/244], loss=86.6177
	step [119/244], loss=92.1011
	step [120/244], loss=116.4698
	step [121/244], loss=106.9811
	step [122/244], loss=137.5149
	step [123/244], loss=122.5166
	step [124/244], loss=123.0655
	step [125/244], loss=104.9713
	step [126/244], loss=102.5190
	step [127/244], loss=114.9167
	step [128/244], loss=111.4143
	step [129/244], loss=108.4960
	step [130/244], loss=123.1162
	step [131/244], loss=125.8138
	step [132/244], loss=116.2273
	step [133/244], loss=112.0075
	step [134/244], loss=126.7493
	step [135/244], loss=96.5596
	step [136/244], loss=122.0400
	step [137/244], loss=115.5641
	step [138/244], loss=122.7773
	step [139/244], loss=101.6101
	step [140/244], loss=147.7755
	step [141/244], loss=108.3156
	step [142/244], loss=98.2154
	step [143/244], loss=101.0270
	step [144/244], loss=113.3770
	step [145/244], loss=121.2771
	step [146/244], loss=122.7424
	step [147/244], loss=128.1467
	step [148/244], loss=105.5019
	step [149/244], loss=125.8854
	step [150/244], loss=121.9735
	step [151/244], loss=125.8934
	step [152/244], loss=112.1128
	step [153/244], loss=120.6586
	step [154/244], loss=121.4750
	step [155/244], loss=133.0946
	step [156/244], loss=114.8436
	step [157/244], loss=118.5768
	step [158/244], loss=107.9213
	step [159/244], loss=103.3845
	step [160/244], loss=127.4124
	step [161/244], loss=116.5213
	step [162/244], loss=116.6755
	step [163/244], loss=109.5466
	step [164/244], loss=125.3615
	step [165/244], loss=129.0757
	step [166/244], loss=123.0589
	step [167/244], loss=108.9037
	step [168/244], loss=105.0281
	step [169/244], loss=105.7619
	step [170/244], loss=135.5071
	step [171/244], loss=97.4893
	step [172/244], loss=149.9587
	step [173/244], loss=90.1069
	step [174/244], loss=110.9178
	step [175/244], loss=111.4935
	step [176/244], loss=85.3198
	step [177/244], loss=100.1641
	step [178/244], loss=119.1267
	step [179/244], loss=98.4385
	step [180/244], loss=113.6445
	step [181/244], loss=110.7117
	step [182/244], loss=107.5296
	step [183/244], loss=139.2844
	step [184/244], loss=116.0178
	step [185/244], loss=106.1650
	step [186/244], loss=105.6162
	step [187/244], loss=117.0019
	step [188/244], loss=89.3435
	step [189/244], loss=130.4922
	step [190/244], loss=140.6564
	step [191/244], loss=131.2294
	step [192/244], loss=93.5951
	step [193/244], loss=123.8170
	step [194/244], loss=124.2700
	step [195/244], loss=127.9342
	step [196/244], loss=113.3930
	step [197/244], loss=104.3138
	step [198/244], loss=114.1841
	step [199/244], loss=120.9243
	step [200/244], loss=96.2547
	step [201/244], loss=99.1035
	step [202/244], loss=107.3839
	step [203/244], loss=126.0384
	step [204/244], loss=118.2155
	step [205/244], loss=106.7845
	step [206/244], loss=102.2789
	step [207/244], loss=108.5002
	step [208/244], loss=114.5213
	step [209/244], loss=85.6231
	step [210/244], loss=116.1577
	step [211/244], loss=113.7407
	step [212/244], loss=101.1801
	step [213/244], loss=108.4602
	step [214/244], loss=125.4085
	step [215/244], loss=109.5573
	step [216/244], loss=99.0846
	step [217/244], loss=118.1975
	step [218/244], loss=120.6666
	step [219/244], loss=130.2488
	step [220/244], loss=92.8623
	step [221/244], loss=104.4170
	step [222/244], loss=96.0705
	step [223/244], loss=126.1932
	step [224/244], loss=120.6670
	step [225/244], loss=128.0001
	step [226/244], loss=104.5464
	step [227/244], loss=103.3302
	step [228/244], loss=113.0669
	step [229/244], loss=110.8068
	step [230/244], loss=124.0568
	step [231/244], loss=126.0644
	step [232/244], loss=127.3250
	step [233/244], loss=107.1259
	step [234/244], loss=129.8092
	step [235/244], loss=140.3243
	step [236/244], loss=102.9594
	step [237/244], loss=130.2151
	step [238/244], loss=107.8555
	step [239/244], loss=92.0354
	step [240/244], loss=141.0199
	step [241/244], loss=131.4616
	step [242/244], loss=101.8010
	step [243/244], loss=121.3111
	step [244/244], loss=4.7475
	Evaluating
	loss=0.0287, precision=0.4501, recall=0.9284, f1=0.6063
saving model as: 2_saved_model.pth
Training epoch 10
	step [1/244], loss=132.4120
	step [2/244], loss=113.5573
	step [3/244], loss=131.8779
	step [4/244], loss=93.6344
	step [5/244], loss=99.9906
	step [6/244], loss=95.5484
	step [7/244], loss=99.4124
	step [8/244], loss=104.9604
	step [9/244], loss=115.1062
	step [10/244], loss=115.9395
	step [11/244], loss=124.9075
	step [12/244], loss=126.2151
	step [13/244], loss=108.5955
	step [14/244], loss=112.1631
	step [15/244], loss=114.0047
	step [16/244], loss=113.4978
	step [17/244], loss=109.7284
	step [18/244], loss=115.5524
	step [19/244], loss=121.4187
	step [20/244], loss=110.5766
	step [21/244], loss=133.3347
	step [22/244], loss=93.9360
	step [23/244], loss=130.7209
	step [24/244], loss=100.2381
	step [25/244], loss=110.5241
	step [26/244], loss=113.0779
	step [27/244], loss=103.9171
	step [28/244], loss=98.9333
	step [29/244], loss=105.5110
	step [30/244], loss=110.0115
	step [31/244], loss=125.2113
	step [32/244], loss=121.0921
	step [33/244], loss=112.8147
	step [34/244], loss=101.2139
	step [35/244], loss=112.4467
	step [36/244], loss=112.1990
	step [37/244], loss=102.6776
	step [38/244], loss=110.1664
	step [39/244], loss=92.9658
	step [40/244], loss=135.0917
	step [41/244], loss=117.9708
	step [42/244], loss=111.0932
	step [43/244], loss=112.4712
	step [44/244], loss=95.9008
	step [45/244], loss=112.8416
	step [46/244], loss=122.6952
	step [47/244], loss=127.5028
	step [48/244], loss=121.3804
	step [49/244], loss=107.0802
	step [50/244], loss=130.3684
	step [51/244], loss=122.6861
	step [52/244], loss=115.6101
	step [53/244], loss=116.1932
	step [54/244], loss=118.3586
	step [55/244], loss=91.1130
	step [56/244], loss=108.2979
	step [57/244], loss=139.5906
	step [58/244], loss=92.3988
	step [59/244], loss=99.3276
	step [60/244], loss=113.9866
	step [61/244], loss=116.8341
	step [62/244], loss=117.4097
	step [63/244], loss=102.8777
	step [64/244], loss=102.6459
	step [65/244], loss=89.6723
	step [66/244], loss=107.4253
	step [67/244], loss=117.6614
	step [68/244], loss=112.3990
	step [69/244], loss=106.8623
	step [70/244], loss=123.8098
	step [71/244], loss=128.5626
	step [72/244], loss=112.8460
	step [73/244], loss=124.5113
	step [74/244], loss=104.5739
	step [75/244], loss=107.7136
	step [76/244], loss=129.0374
	step [77/244], loss=97.9368
	step [78/244], loss=114.0576
	step [79/244], loss=107.7215
	step [80/244], loss=107.7461
	step [81/244], loss=99.3857
	step [82/244], loss=104.5932
	step [83/244], loss=124.5192
	step [84/244], loss=137.1099
	step [85/244], loss=114.8048
	step [86/244], loss=106.6376
	step [87/244], loss=114.9182
	step [88/244], loss=104.7607
	step [89/244], loss=117.8430
	step [90/244], loss=100.8240
	step [91/244], loss=104.7805
	step [92/244], loss=113.5970
	step [93/244], loss=109.1488
	step [94/244], loss=108.8399
	step [95/244], loss=112.4314
	step [96/244], loss=93.0168
	step [97/244], loss=105.0875
	step [98/244], loss=110.9159
	step [99/244], loss=121.5020
	step [100/244], loss=119.1716
	step [101/244], loss=106.1802
	step [102/244], loss=111.0143
	step [103/244], loss=116.3517
	step [104/244], loss=121.3725
	step [105/244], loss=126.9066
	step [106/244], loss=104.2439
	step [107/244], loss=109.5453
	step [108/244], loss=103.1850
	step [109/244], loss=132.7935
	step [110/244], loss=114.8798
	step [111/244], loss=112.2686
	step [112/244], loss=125.5411
	step [113/244], loss=124.3443
	step [114/244], loss=116.9763
	step [115/244], loss=108.0529
	step [116/244], loss=122.3884
	step [117/244], loss=127.2445
	step [118/244], loss=80.9249
	step [119/244], loss=104.6882
	step [120/244], loss=134.1146
	step [121/244], loss=82.2386
	step [122/244], loss=111.1629
	step [123/244], loss=97.8418
	step [124/244], loss=119.2372
	step [125/244], loss=100.7031
	step [126/244], loss=113.7660
	step [127/244], loss=93.0433
	step [128/244], loss=117.6010
	step [129/244], loss=114.8856
	step [130/244], loss=87.4283
	step [131/244], loss=116.5321
	step [132/244], loss=109.9615
	step [133/244], loss=114.8103
	step [134/244], loss=109.5391
	step [135/244], loss=115.4111
	step [136/244], loss=99.9029
	step [137/244], loss=115.6117
	step [138/244], loss=109.5450
	step [139/244], loss=104.2641
	step [140/244], loss=102.6211
	step [141/244], loss=100.0661
	step [142/244], loss=131.8743
	step [143/244], loss=120.4329
	step [144/244], loss=101.6099
	step [145/244], loss=90.6919
	step [146/244], loss=111.7582
	step [147/244], loss=115.4873
	step [148/244], loss=94.9432
	step [149/244], loss=109.8550
	step [150/244], loss=108.2091
	step [151/244], loss=122.5084
	step [152/244], loss=92.3626
	step [153/244], loss=105.6140
	step [154/244], loss=109.2257
	step [155/244], loss=124.5686
	step [156/244], loss=110.3127
	step [157/244], loss=91.9628
	step [158/244], loss=89.3931
	step [159/244], loss=108.6630
	step [160/244], loss=125.3783
	step [161/244], loss=98.3291
	step [162/244], loss=94.3239
	step [163/244], loss=118.0467
	step [164/244], loss=106.4559
	step [165/244], loss=109.7115
	step [166/244], loss=110.4673
	step [167/244], loss=93.5103
	step [168/244], loss=122.0849
	step [169/244], loss=95.5170
	step [170/244], loss=128.9308
	step [171/244], loss=124.9165
	step [172/244], loss=133.7969
	step [173/244], loss=111.7279
	step [174/244], loss=95.0310
	step [175/244], loss=112.0539
	step [176/244], loss=91.8158
	step [177/244], loss=134.3062
	step [178/244], loss=95.8965
	step [179/244], loss=96.7894
	step [180/244], loss=135.9639
	step [181/244], loss=102.7063
	step [182/244], loss=91.6225
	step [183/244], loss=77.4107
	step [184/244], loss=96.2839
	step [185/244], loss=122.8783
	step [186/244], loss=110.7568
	step [187/244], loss=99.3074
	step [188/244], loss=113.7405
	step [189/244], loss=111.9804
	step [190/244], loss=125.5603
	step [191/244], loss=118.5849
	step [192/244], loss=122.4115
	step [193/244], loss=128.9163
	step [194/244], loss=133.9934
	step [195/244], loss=121.5685
	step [196/244], loss=109.7661
	step [197/244], loss=99.8658
	step [198/244], loss=122.9130
	step [199/244], loss=119.6419
	step [200/244], loss=130.2911
	step [201/244], loss=103.2185
	step [202/244], loss=103.2490
	step [203/244], loss=97.9789
	step [204/244], loss=113.7557
	step [205/244], loss=100.9701
	step [206/244], loss=104.6215
	step [207/244], loss=116.4237
	step [208/244], loss=108.6154
	step [209/244], loss=143.7795
	step [210/244], loss=123.8715
	step [211/244], loss=116.5824
	step [212/244], loss=110.2253
	step [213/244], loss=104.1410
	step [214/244], loss=123.5295
	step [215/244], loss=99.1373
	step [216/244], loss=92.7661
	step [217/244], loss=124.7241
	step [218/244], loss=112.4846
	step [219/244], loss=119.0550
	step [220/244], loss=107.6330
	step [221/244], loss=112.0795
	step [222/244], loss=128.7575
	step [223/244], loss=119.1077
	step [224/244], loss=137.7171
	step [225/244], loss=110.5329
	step [226/244], loss=129.0889
	step [227/244], loss=113.9145
	step [228/244], loss=114.0106
	step [229/244], loss=133.7582
	step [230/244], loss=138.4452
	step [231/244], loss=97.9386
	step [232/244], loss=92.1803
	step [233/244], loss=113.3178
	step [234/244], loss=110.5961
	step [235/244], loss=127.4908
	step [236/244], loss=97.3731
	step [237/244], loss=123.9781
	step [238/244], loss=120.8527
	step [239/244], loss=112.2341
	step [240/244], loss=101.8773
	step [241/244], loss=106.1888
	step [242/244], loss=122.6389
	step [243/244], loss=113.2942
	step [244/244], loss=8.6794
	Evaluating
	loss=0.0292, precision=0.3212, recall=0.9202, f1=0.4762
Training epoch 11
	step [1/244], loss=137.3831
	step [2/244], loss=121.8090
	step [3/244], loss=111.3761
	step [4/244], loss=107.2137
	step [5/244], loss=93.0621
	step [6/244], loss=134.8790
	step [7/244], loss=103.7921
	step [8/244], loss=99.6887
	step [9/244], loss=123.6301
	step [10/244], loss=112.5444
	step [11/244], loss=106.9281
	step [12/244], loss=108.6583
	step [13/244], loss=108.8722
	step [14/244], loss=102.4817
	step [15/244], loss=104.7729
	step [16/244], loss=104.8090
	step [17/244], loss=108.1070
	step [18/244], loss=113.9724
	step [19/244], loss=105.7725
	step [20/244], loss=115.5307
	step [21/244], loss=108.6963
	step [22/244], loss=99.3237
	step [23/244], loss=114.5406
	step [24/244], loss=117.8207
	step [25/244], loss=93.6325
	step [26/244], loss=122.7779
	step [27/244], loss=132.9667
	step [28/244], loss=130.0954
	step [29/244], loss=93.4134
	step [30/244], loss=107.1463
	step [31/244], loss=126.8136
	step [32/244], loss=114.2997
	step [33/244], loss=111.3746
	step [34/244], loss=136.9479
	step [35/244], loss=105.6992
	step [36/244], loss=113.8774
	step [37/244], loss=130.0584
	step [38/244], loss=126.2610
	step [39/244], loss=123.0411
	step [40/244], loss=94.8684
	step [41/244], loss=100.1898
	step [42/244], loss=122.1242
	step [43/244], loss=106.2265
	step [44/244], loss=111.0536
	step [45/244], loss=94.6967
	step [46/244], loss=106.5750
	step [47/244], loss=104.4962
	step [48/244], loss=103.4713
	step [49/244], loss=103.8746
	step [50/244], loss=98.5409
	step [51/244], loss=105.7817
	step [52/244], loss=117.8623
	step [53/244], loss=107.0863
	step [54/244], loss=113.6695
	step [55/244], loss=88.8921
	step [56/244], loss=131.8219
	step [57/244], loss=101.1758
	step [58/244], loss=117.0413
	step [59/244], loss=102.5390
	step [60/244], loss=105.5571
	step [61/244], loss=120.3910
	step [62/244], loss=137.2366
	step [63/244], loss=102.4501
	step [64/244], loss=116.2016
	step [65/244], loss=96.1623
	step [66/244], loss=108.8324
	step [67/244], loss=109.9694
	step [68/244], loss=109.4587
	step [69/244], loss=113.9389
	step [70/244], loss=126.0123
	step [71/244], loss=103.8345
	step [72/244], loss=103.1137
	step [73/244], loss=97.8901
	step [74/244], loss=95.0533
	step [75/244], loss=121.0075
	step [76/244], loss=105.9133
	step [77/244], loss=125.8203
	step [78/244], loss=112.8356
	step [79/244], loss=109.0744
	step [80/244], loss=116.9346
	step [81/244], loss=119.0024
	step [82/244], loss=94.2054
	step [83/244], loss=125.3543
	step [84/244], loss=104.6823
	step [85/244], loss=120.9421
	step [86/244], loss=130.2430
	step [87/244], loss=106.0756
	step [88/244], loss=97.8757
	step [89/244], loss=116.1180
	step [90/244], loss=95.6313
	step [91/244], loss=111.7058
	step [92/244], loss=127.3001
	step [93/244], loss=105.6306
	step [94/244], loss=120.6924
	step [95/244], loss=112.4990
	step [96/244], loss=112.6467
	step [97/244], loss=115.3120
	step [98/244], loss=124.7167
	step [99/244], loss=119.1055
	step [100/244], loss=108.5202
	step [101/244], loss=103.3756
	step [102/244], loss=99.4482
	step [103/244], loss=107.0735
	step [104/244], loss=110.9817
	step [105/244], loss=127.6440
	step [106/244], loss=121.8537
	step [107/244], loss=130.9612
	step [108/244], loss=91.1683
	step [109/244], loss=112.4668
	step [110/244], loss=94.1523
	step [111/244], loss=110.9395
	step [112/244], loss=101.4376
	step [113/244], loss=107.5110
	step [114/244], loss=113.7935
	step [115/244], loss=112.8546
	step [116/244], loss=124.1308
	step [117/244], loss=124.0107
	step [118/244], loss=102.2138
	step [119/244], loss=107.5939
	step [120/244], loss=123.2115
	step [121/244], loss=119.7425
	step [122/244], loss=98.1948
	step [123/244], loss=109.7373
	step [124/244], loss=114.4989
	step [125/244], loss=108.6591
	step [126/244], loss=102.9422
	step [127/244], loss=126.0822
	step [128/244], loss=117.4741
	step [129/244], loss=88.5276
	step [130/244], loss=103.8627
	step [131/244], loss=122.6731
	step [132/244], loss=101.7385
	step [133/244], loss=120.9376
	step [134/244], loss=119.7407
	step [135/244], loss=120.3354
	step [136/244], loss=101.6297
	step [137/244], loss=102.6614
	step [138/244], loss=119.3284
	step [139/244], loss=104.4665
	step [140/244], loss=99.2175
	step [141/244], loss=94.7212
	step [142/244], loss=107.0653
	step [143/244], loss=105.2738
	step [144/244], loss=101.6421
	step [145/244], loss=117.0012
	step [146/244], loss=94.1057
	step [147/244], loss=91.8761
	step [148/244], loss=119.2708
	step [149/244], loss=108.1127
	step [150/244], loss=113.1739
	step [151/244], loss=113.4933
	step [152/244], loss=118.3486
	step [153/244], loss=101.5748
	step [154/244], loss=125.6914
	step [155/244], loss=109.6139
	step [156/244], loss=104.6512
	step [157/244], loss=86.4936
	step [158/244], loss=105.0876
	step [159/244], loss=108.6907
	step [160/244], loss=102.5900
	step [161/244], loss=114.2702
	step [162/244], loss=117.7529
	step [163/244], loss=98.9874
	step [164/244], loss=108.3944
	step [165/244], loss=118.1546
	step [166/244], loss=120.9105
	step [167/244], loss=114.2772
	step [168/244], loss=75.3225
	step [169/244], loss=125.4267
	step [170/244], loss=124.9707
	step [171/244], loss=121.2372
	step [172/244], loss=118.8401
	step [173/244], loss=92.0353
	step [174/244], loss=103.9829
	step [175/244], loss=122.5369
	step [176/244], loss=103.9886
	step [177/244], loss=108.2173
	step [178/244], loss=84.6260
	step [179/244], loss=116.2221
	step [180/244], loss=99.3147
	step [181/244], loss=115.0061
	step [182/244], loss=102.9799
	step [183/244], loss=102.7961
	step [184/244], loss=103.4370
	step [185/244], loss=108.6528
	step [186/244], loss=126.0591
	step [187/244], loss=102.2135
	step [188/244], loss=110.5639
	step [189/244], loss=122.0285
	step [190/244], loss=118.9532
	step [191/244], loss=103.0200
	step [192/244], loss=123.0412
	step [193/244], loss=117.1149
	step [194/244], loss=127.0841
	step [195/244], loss=112.6645
	step [196/244], loss=116.4825
	step [197/244], loss=89.0710
	step [198/244], loss=109.3384
	step [199/244], loss=114.7351
	step [200/244], loss=110.5978
	step [201/244], loss=100.2656
	step [202/244], loss=95.6761
	step [203/244], loss=131.7780
	step [204/244], loss=82.4267
	step [205/244], loss=117.5334
	step [206/244], loss=112.0700
	step [207/244], loss=100.5179
	step [208/244], loss=138.0047
	step [209/244], loss=135.6618
	step [210/244], loss=114.7902
	step [211/244], loss=93.7973
	step [212/244], loss=104.1875
	step [213/244], loss=86.0480
	step [214/244], loss=111.4449
	step [215/244], loss=114.6614
	step [216/244], loss=87.1548
	step [217/244], loss=110.0757
	step [218/244], loss=90.5214
	step [219/244], loss=120.1736
	step [220/244], loss=106.5249
	step [221/244], loss=96.4566
	step [222/244], loss=121.3878
	step [223/244], loss=121.5646
	step [224/244], loss=131.7413
	step [225/244], loss=101.7391
	step [226/244], loss=105.5581
	step [227/244], loss=104.3542
	step [228/244], loss=101.3369
	step [229/244], loss=116.7028
	step [230/244], loss=105.7195
	step [231/244], loss=125.9381
	step [232/244], loss=92.9529
	step [233/244], loss=102.6038
	step [234/244], loss=110.2142
	step [235/244], loss=114.1960
	step [236/244], loss=92.0410
	step [237/244], loss=113.2822
	step [238/244], loss=111.6512
	step [239/244], loss=102.9056
	step [240/244], loss=102.1202
	step [241/244], loss=116.3167
	step [242/244], loss=115.2802
	step [243/244], loss=107.6842
	step [244/244], loss=1.8057
	Evaluating
	loss=0.0230, precision=0.3763, recall=0.9367, f1=0.5369
Training epoch 12
	step [1/244], loss=132.5889
	step [2/244], loss=107.9134
	step [3/244], loss=112.4689
	step [4/244], loss=101.9813
	step [5/244], loss=140.0826
	step [6/244], loss=102.0543
	step [7/244], loss=118.2711
	step [8/244], loss=95.5225
	step [9/244], loss=80.5809
	step [10/244], loss=103.6583
	step [11/244], loss=122.4640
	step [12/244], loss=126.5333
	step [13/244], loss=125.7070
	step [14/244], loss=104.9613
	step [15/244], loss=109.5897
	step [16/244], loss=99.6474
	step [17/244], loss=121.7691
	step [18/244], loss=93.6551
	step [19/244], loss=111.6085
	step [20/244], loss=97.7235
	step [21/244], loss=90.6984
	step [22/244], loss=93.3995
	step [23/244], loss=89.0091
	step [24/244], loss=101.5928
	step [25/244], loss=102.6738
	step [26/244], loss=109.1010
	step [27/244], loss=103.5114
	step [28/244], loss=106.0700
	step [29/244], loss=125.8013
	step [30/244], loss=108.6075
	step [31/244], loss=109.0816
	step [32/244], loss=115.6311
	step [33/244], loss=94.9117
	step [34/244], loss=86.2096
	step [35/244], loss=105.7115
	step [36/244], loss=103.7494
	step [37/244], loss=109.3183
	step [38/244], loss=95.6774
	step [39/244], loss=108.0705
	step [40/244], loss=94.3747
	step [41/244], loss=108.4770
	step [42/244], loss=110.7134
	step [43/244], loss=101.2403
	step [44/244], loss=97.0124
	step [45/244], loss=117.0825
	step [46/244], loss=107.0416
	step [47/244], loss=89.1175
	step [48/244], loss=91.1640
	step [49/244], loss=117.6302
	step [50/244], loss=115.5073
	step [51/244], loss=104.3873
	step [52/244], loss=115.7335
	step [53/244], loss=106.5701
	step [54/244], loss=135.6481
	step [55/244], loss=87.1254
	step [56/244], loss=121.3716
	step [57/244], loss=98.4105
	step [58/244], loss=120.9998
	step [59/244], loss=121.9937
	step [60/244], loss=113.7135
	step [61/244], loss=110.2119
	step [62/244], loss=96.1587
	step [63/244], loss=94.9744
	step [64/244], loss=111.8991
	step [65/244], loss=102.4730
	step [66/244], loss=115.5420
	step [67/244], loss=99.7947
	step [68/244], loss=112.4458
	step [69/244], loss=104.3064
	step [70/244], loss=105.9856
	step [71/244], loss=106.2808
	step [72/244], loss=124.3466
	step [73/244], loss=109.0962
	step [74/244], loss=113.7297
	step [75/244], loss=110.8764
	step [76/244], loss=111.2449
	step [77/244], loss=118.5979
	step [78/244], loss=95.9969
	step [79/244], loss=82.1207
	step [80/244], loss=118.6266
	step [81/244], loss=95.8193
	step [82/244], loss=108.1664
	step [83/244], loss=110.7838
	step [84/244], loss=86.7345
	step [85/244], loss=113.7474
	step [86/244], loss=122.9506
	step [87/244], loss=122.1545
	step [88/244], loss=95.4169
	step [89/244], loss=113.7284
	step [90/244], loss=100.3236
	step [91/244], loss=92.8176
	step [92/244], loss=120.9890
	step [93/244], loss=106.3767
	step [94/244], loss=114.7344
	step [95/244], loss=95.1514
	step [96/244], loss=107.6716
	step [97/244], loss=109.1732
	step [98/244], loss=121.3728
	step [99/244], loss=103.1808
	step [100/244], loss=122.0482
	step [101/244], loss=114.0883
	step [102/244], loss=123.1869
	step [103/244], loss=99.3976
	step [104/244], loss=130.3368
	step [105/244], loss=125.8546
	step [106/244], loss=122.5421
	step [107/244], loss=109.0387
	step [108/244], loss=108.6406
	step [109/244], loss=130.1290
	step [110/244], loss=122.7747
	step [111/244], loss=127.2254
	step [112/244], loss=116.7069
	step [113/244], loss=93.5752
	step [114/244], loss=94.2311
	step [115/244], loss=90.2291
	step [116/244], loss=106.2401
	step [117/244], loss=98.2568
	step [118/244], loss=99.5356
	step [119/244], loss=108.5678
	step [120/244], loss=105.0096
	step [121/244], loss=102.9737
	step [122/244], loss=111.3102
	step [123/244], loss=108.6118
	step [124/244], loss=99.6116
	step [125/244], loss=117.9079
	step [126/244], loss=111.0826
	step [127/244], loss=87.4327
	step [128/244], loss=118.4730
	step [129/244], loss=94.3605
	step [130/244], loss=129.1797
	step [131/244], loss=101.6075
	step [132/244], loss=88.5405
	step [133/244], loss=115.9672
	step [134/244], loss=110.8756
	step [135/244], loss=115.1043
	step [136/244], loss=115.2168
	step [137/244], loss=128.4738
	step [138/244], loss=114.6763
	step [139/244], loss=120.0508
	step [140/244], loss=96.6468
	step [141/244], loss=122.9347
	step [142/244], loss=105.0076
	step [143/244], loss=120.1661
	step [144/244], loss=102.3821
	step [145/244], loss=97.0176
	step [146/244], loss=108.3463
	step [147/244], loss=91.8389
	step [148/244], loss=91.5367
	step [149/244], loss=101.7749
	step [150/244], loss=128.9529
	step [151/244], loss=116.1904
	step [152/244], loss=107.2679
	step [153/244], loss=124.6505
	step [154/244], loss=120.3559
	step [155/244], loss=112.3151
	step [156/244], loss=112.9525
	step [157/244], loss=105.5128
	step [158/244], loss=115.9188
	step [159/244], loss=113.1799
	step [160/244], loss=103.3535
	step [161/244], loss=114.8517
	step [162/244], loss=101.9194
	step [163/244], loss=113.4427
	step [164/244], loss=116.4680
	step [165/244], loss=93.9551
	step [166/244], loss=119.1156
	step [167/244], loss=123.6978
	step [168/244], loss=126.5691
	step [169/244], loss=122.0526
	step [170/244], loss=109.1728
	step [171/244], loss=93.1924
	step [172/244], loss=93.9831
	step [173/244], loss=119.8021
	step [174/244], loss=103.0378
	step [175/244], loss=119.6456
	step [176/244], loss=114.6492
	step [177/244], loss=105.8602
	step [178/244], loss=98.7278
	step [179/244], loss=96.2107
	step [180/244], loss=91.7104
	step [181/244], loss=109.4359
	step [182/244], loss=89.6232
	step [183/244], loss=117.8010
	step [184/244], loss=105.8246
	step [185/244], loss=107.8787
	step [186/244], loss=107.2252
	step [187/244], loss=113.8482
	step [188/244], loss=117.9533
	step [189/244], loss=138.0689
	step [190/244], loss=106.7906
	step [191/244], loss=115.5976
	step [192/244], loss=84.9484
	step [193/244], loss=111.0204
	step [194/244], loss=124.9829
	step [195/244], loss=123.3745
	step [196/244], loss=85.6453
	step [197/244], loss=104.8639
	step [198/244], loss=111.0594
	step [199/244], loss=96.6001
	step [200/244], loss=98.4997
	step [201/244], loss=99.6293
	step [202/244], loss=113.8732
	step [203/244], loss=102.9466
	step [204/244], loss=112.0783
	step [205/244], loss=119.6593
	step [206/244], loss=84.3301
	step [207/244], loss=110.8907
	step [208/244], loss=100.8261
	step [209/244], loss=96.2900
	step [210/244], loss=119.3537
	step [211/244], loss=109.9586
	step [212/244], loss=119.4547
	step [213/244], loss=115.5967
	step [214/244], loss=122.8292
	step [215/244], loss=111.9056
	step [216/244], loss=117.4458
	step [217/244], loss=115.6024
	step [218/244], loss=120.1817
	step [219/244], loss=109.6445
	step [220/244], loss=140.4690
	step [221/244], loss=102.0832
	step [222/244], loss=106.2065
	step [223/244], loss=97.3519
	step [224/244], loss=102.1096
	step [225/244], loss=120.8559
	step [226/244], loss=111.8390
	step [227/244], loss=85.4610
	step [228/244], loss=108.7055
	step [229/244], loss=119.7608
	step [230/244], loss=103.5469
	step [231/244], loss=106.3895
	step [232/244], loss=101.9228
	step [233/244], loss=95.3208
	step [234/244], loss=124.7013
	step [235/244], loss=110.3806
	step [236/244], loss=94.6677
	step [237/244], loss=114.3339
	step [238/244], loss=97.4673
	step [239/244], loss=121.9712
	step [240/244], loss=115.9995
	step [241/244], loss=134.7363
	step [242/244], loss=109.9818
	step [243/244], loss=97.4310
	step [244/244], loss=6.6503
	Evaluating
	loss=0.0248, precision=0.2287, recall=0.9098, f1=0.3656
Training epoch 13
	step [1/244], loss=103.2718
	step [2/244], loss=109.0461
	step [3/244], loss=99.6107
	step [4/244], loss=113.8765
	step [5/244], loss=130.8958
	step [6/244], loss=110.8519
	step [7/244], loss=100.3937
	step [8/244], loss=98.9789
	step [9/244], loss=106.8842
	step [10/244], loss=105.8785
	step [11/244], loss=100.9851
	step [12/244], loss=93.4544
	step [13/244], loss=120.5948
	step [14/244], loss=102.3333
	step [15/244], loss=100.3654
	step [16/244], loss=114.2112
	step [17/244], loss=90.4036
	step [18/244], loss=104.1405
	step [19/244], loss=100.4010
	step [20/244], loss=103.2070
	step [21/244], loss=86.6231
	step [22/244], loss=102.1633
	step [23/244], loss=105.1880
	step [24/244], loss=95.1959
	step [25/244], loss=94.5987
	step [26/244], loss=105.1818
	step [27/244], loss=102.4792
	step [28/244], loss=95.7402
	step [29/244], loss=131.9161
	step [30/244], loss=93.3724
	step [31/244], loss=109.8215
	step [32/244], loss=100.2756
	step [33/244], loss=90.9966
	step [34/244], loss=118.8159
	step [35/244], loss=80.1007
	step [36/244], loss=119.2355
	step [37/244], loss=102.1494
	step [38/244], loss=109.1974
	step [39/244], loss=108.9363
	step [40/244], loss=97.7204
	step [41/244], loss=95.1082
	step [42/244], loss=134.5240
	step [43/244], loss=90.7304
	step [44/244], loss=111.1063
	step [45/244], loss=111.7141
	step [46/244], loss=101.2871
	step [47/244], loss=109.6937
	step [48/244], loss=105.7063
	step [49/244], loss=93.5458
	step [50/244], loss=105.9209
	step [51/244], loss=119.6572
	step [52/244], loss=114.9643
	step [53/244], loss=112.3542
	step [54/244], loss=117.6084
	step [55/244], loss=119.0570
	step [56/244], loss=114.8055
	step [57/244], loss=108.5191
	step [58/244], loss=91.1073
	step [59/244], loss=125.2409
	step [60/244], loss=110.3052
	step [61/244], loss=109.0256
	step [62/244], loss=92.0110
	step [63/244], loss=109.0690
	step [64/244], loss=116.8750
	step [65/244], loss=95.6051
	step [66/244], loss=114.6826
	step [67/244], loss=103.3891
	step [68/244], loss=111.0081
	step [69/244], loss=120.4286
	step [70/244], loss=134.8534
	step [71/244], loss=92.3462
	step [72/244], loss=108.8643
	step [73/244], loss=124.6653
	step [74/244], loss=97.3696
	step [75/244], loss=109.2462
	step [76/244], loss=108.3127
	step [77/244], loss=101.5217
	step [78/244], loss=124.5508
	step [79/244], loss=106.2874
	step [80/244], loss=93.8350
	step [81/244], loss=104.0193
	step [82/244], loss=127.7595
	step [83/244], loss=102.2587
	step [84/244], loss=113.2315
	step [85/244], loss=102.4425
	step [86/244], loss=105.5563
	step [87/244], loss=101.2779
	step [88/244], loss=86.8637
	step [89/244], loss=105.3837
	step [90/244], loss=88.3362
	step [91/244], loss=107.4032
	step [92/244], loss=94.3360
	step [93/244], loss=95.2389
	step [94/244], loss=91.2934
	step [95/244], loss=87.2619
	step [96/244], loss=105.2265
	step [97/244], loss=98.0608
	step [98/244], loss=143.1537
	step [99/244], loss=108.7079
	step [100/244], loss=97.3803
	step [101/244], loss=107.2271
	step [102/244], loss=116.7973
	step [103/244], loss=114.7568
	step [104/244], loss=100.9069
	step [105/244], loss=95.4768
	step [106/244], loss=125.8949
	step [107/244], loss=110.4283
	step [108/244], loss=112.4017
	step [109/244], loss=135.9402
	step [110/244], loss=120.1298
	step [111/244], loss=98.5972
	step [112/244], loss=91.6842
	step [113/244], loss=106.1080
	step [114/244], loss=124.7682
	step [115/244], loss=130.6292
	step [116/244], loss=127.3232
	step [117/244], loss=99.4116
	step [118/244], loss=107.8016
	step [119/244], loss=82.1927
	step [120/244], loss=101.9204
	step [121/244], loss=103.5365
	step [122/244], loss=130.2086
	step [123/244], loss=121.9104
	step [124/244], loss=119.4107
	step [125/244], loss=98.8721
	step [126/244], loss=93.7938
	step [127/244], loss=94.5257
	step [128/244], loss=122.9768
	step [129/244], loss=99.5453
	step [130/244], loss=101.8129
	step [131/244], loss=133.2082
	step [132/244], loss=95.6559
	step [133/244], loss=96.5451
	step [134/244], loss=111.7841
	step [135/244], loss=110.7392
	step [136/244], loss=106.3628
	step [137/244], loss=115.8140
	step [138/244], loss=116.3187
	step [139/244], loss=85.5699
	step [140/244], loss=104.1415
	step [141/244], loss=126.4787
	step [142/244], loss=109.1608
	step [143/244], loss=91.5647
	step [144/244], loss=116.3964
	step [145/244], loss=104.2260
	step [146/244], loss=99.8657
	step [147/244], loss=94.7132
	step [148/244], loss=120.8936
	step [149/244], loss=93.0084
	step [150/244], loss=110.0888
	step [151/244], loss=122.3395
	step [152/244], loss=100.6041
	step [153/244], loss=118.7299
	step [154/244], loss=122.2409
	step [155/244], loss=126.4960
	step [156/244], loss=87.3060
	step [157/244], loss=122.2599
	step [158/244], loss=78.8303
	step [159/244], loss=114.4977
	step [160/244], loss=112.2165
	step [161/244], loss=97.3205
	step [162/244], loss=138.5774
	step [163/244], loss=89.4384
	step [164/244], loss=109.1353
	step [165/244], loss=101.6889
	step [166/244], loss=107.8749
	step [167/244], loss=135.0011
	step [168/244], loss=109.8785
	step [169/244], loss=95.6469
	step [170/244], loss=107.1736
	step [171/244], loss=114.4479
	step [172/244], loss=137.8141
	step [173/244], loss=95.7519
	step [174/244], loss=94.3928
	step [175/244], loss=121.0710
	step [176/244], loss=103.2750
	step [177/244], loss=107.2781
	step [178/244], loss=96.6643
	step [179/244], loss=122.4204
	step [180/244], loss=103.8606
	step [181/244], loss=92.8724
	step [182/244], loss=113.0374
	step [183/244], loss=99.3096
	step [184/244], loss=110.2104
	step [185/244], loss=112.1395
	step [186/244], loss=92.7934
	step [187/244], loss=108.3942
	step [188/244], loss=115.9041
	step [189/244], loss=123.4473
	step [190/244], loss=95.8567
	step [191/244], loss=107.0164
	step [192/244], loss=119.5110
	step [193/244], loss=119.2204
	step [194/244], loss=95.6063
	step [195/244], loss=100.8016
	step [196/244], loss=93.7894
	step [197/244], loss=119.8723
	step [198/244], loss=129.4958
	step [199/244], loss=102.5506
	step [200/244], loss=91.5009
	step [201/244], loss=95.2878
	step [202/244], loss=96.2205
	step [203/244], loss=112.5385
	step [204/244], loss=108.7654
	step [205/244], loss=98.4785
	step [206/244], loss=99.6421
	step [207/244], loss=107.8179
	step [208/244], loss=97.5709
	step [209/244], loss=130.4992
	step [210/244], loss=104.8377
	step [211/244], loss=120.3079
	step [212/244], loss=124.5759
	step [213/244], loss=111.2573
	step [214/244], loss=92.3417
	step [215/244], loss=104.9059
	step [216/244], loss=91.7249
	step [217/244], loss=114.2708
	step [218/244], loss=112.5848
	step [219/244], loss=115.9460
	step [220/244], loss=107.7998
	step [221/244], loss=105.9477
	step [222/244], loss=126.3086
	step [223/244], loss=109.3022
	step [224/244], loss=119.7221
	step [225/244], loss=108.5988
	step [226/244], loss=113.6610
	step [227/244], loss=111.1143
	step [228/244], loss=123.6216
	step [229/244], loss=87.6053
	step [230/244], loss=109.4601
	step [231/244], loss=95.2244
	step [232/244], loss=90.1867
	step [233/244], loss=126.9651
	step [234/244], loss=119.5087
	step [235/244], loss=103.0896
	step [236/244], loss=96.5022
	step [237/244], loss=98.5729
	step [238/244], loss=97.1216
	step [239/244], loss=129.1442
	step [240/244], loss=82.5469
	step [241/244], loss=113.5645
	step [242/244], loss=122.8356
	step [243/244], loss=111.8900
	step [244/244], loss=1.0672
	Evaluating
	loss=0.0203, precision=0.3234, recall=0.9259, f1=0.4793
Training epoch 14
	step [1/244], loss=102.5783
	step [2/244], loss=112.0145
	step [3/244], loss=97.5673
	step [4/244], loss=99.9054
	step [5/244], loss=139.1277
	step [6/244], loss=110.0977
	step [7/244], loss=114.5657
	step [8/244], loss=125.1305
	step [9/244], loss=105.8015
	step [10/244], loss=94.6265
	step [11/244], loss=141.8128
	step [12/244], loss=133.2303
	step [13/244], loss=91.1448
	step [14/244], loss=105.7112
	step [15/244], loss=115.7077
	step [16/244], loss=98.9321
	step [17/244], loss=101.5780
	step [18/244], loss=104.7641
	step [19/244], loss=97.3638
	step [20/244], loss=110.3518
	step [21/244], loss=96.8628
	step [22/244], loss=100.2274
	step [23/244], loss=83.6339
	step [24/244], loss=104.2193
	step [25/244], loss=104.3823
	step [26/244], loss=98.8875
	step [27/244], loss=79.9739
	step [28/244], loss=94.6586
	step [29/244], loss=85.4389
	step [30/244], loss=120.3555
	step [31/244], loss=99.2473
	step [32/244], loss=123.9284
	step [33/244], loss=101.7673
	step [34/244], loss=107.7883
	step [35/244], loss=92.9725
	step [36/244], loss=95.5850
	step [37/244], loss=115.6565
	step [38/244], loss=104.3753
	step [39/244], loss=97.1414
	step [40/244], loss=133.5474
	step [41/244], loss=108.8156
	step [42/244], loss=110.9936
	step [43/244], loss=113.5446
	step [44/244], loss=122.7687
	step [45/244], loss=91.7371
	step [46/244], loss=92.8402
	step [47/244], loss=115.0967
	step [48/244], loss=110.9137
	step [49/244], loss=116.9701
	step [50/244], loss=93.9686
	step [51/244], loss=108.4420
	step [52/244], loss=131.0784
	step [53/244], loss=102.1691
	step [54/244], loss=89.7593
	step [55/244], loss=112.3431
	step [56/244], loss=119.2795
	step [57/244], loss=107.3358
	step [58/244], loss=116.6111
	step [59/244], loss=125.4505
	step [60/244], loss=89.2652
	step [61/244], loss=121.9635
	step [62/244], loss=93.9242
	step [63/244], loss=104.6249
	step [64/244], loss=116.6604
	step [65/244], loss=91.9238
	step [66/244], loss=105.1761
	step [67/244], loss=98.4774
	step [68/244], loss=76.7978
	step [69/244], loss=99.4964
	step [70/244], loss=118.0940
	step [71/244], loss=83.7762
	step [72/244], loss=109.9843
	step [73/244], loss=105.2540
	step [74/244], loss=115.7561
	step [75/244], loss=105.5778
	step [76/244], loss=90.3332
	step [77/244], loss=103.1609
	step [78/244], loss=117.0340
	step [79/244], loss=102.4572
	step [80/244], loss=124.4183
	step [81/244], loss=108.0299
	step [82/244], loss=104.5056
	step [83/244], loss=100.8264
	step [84/244], loss=106.1360
	step [85/244], loss=121.7216
	step [86/244], loss=112.9050
	step [87/244], loss=93.7614
	step [88/244], loss=87.4264
	step [89/244], loss=120.4553
	step [90/244], loss=89.4725
	step [91/244], loss=101.1766
	step [92/244], loss=96.0903
	step [93/244], loss=91.9380
	step [94/244], loss=111.7393
	step [95/244], loss=89.2083
	step [96/244], loss=101.9465
	step [97/244], loss=105.8281
	step [98/244], loss=108.5573
	step [99/244], loss=124.7217
	step [100/244], loss=108.7305
	step [101/244], loss=96.6818
	step [102/244], loss=98.5301
	step [103/244], loss=115.9764
	step [104/244], loss=106.0063
	step [105/244], loss=109.4554
	step [106/244], loss=106.0851
	step [107/244], loss=127.3493
	step [108/244], loss=81.2687
	step [109/244], loss=102.5022
	step [110/244], loss=97.1828
	step [111/244], loss=104.4061
	step [112/244], loss=115.2523
	step [113/244], loss=82.2464
	step [114/244], loss=100.4036
	step [115/244], loss=103.9663
	step [116/244], loss=93.9279
	step [117/244], loss=115.0499
	step [118/244], loss=88.9883
	step [119/244], loss=105.3152
	step [120/244], loss=114.8741
	step [121/244], loss=101.1524
	step [122/244], loss=106.4197
	step [123/244], loss=90.6422
	step [124/244], loss=89.8937
	step [125/244], loss=103.1716
	step [126/244], loss=126.5027
	step [127/244], loss=108.6964
	step [128/244], loss=103.5021
	step [129/244], loss=102.5094
	step [130/244], loss=120.4847
	step [131/244], loss=111.4461
	step [132/244], loss=95.5454
	step [133/244], loss=120.6475
	step [134/244], loss=123.5330
	step [135/244], loss=91.1415
	step [136/244], loss=98.9521
	step [137/244], loss=123.7930
	step [138/244], loss=109.7875
	step [139/244], loss=89.3063
	step [140/244], loss=109.6269
	step [141/244], loss=108.4289
	step [142/244], loss=94.6216
	step [143/244], loss=111.0510
	step [144/244], loss=108.0823
	step [145/244], loss=116.8681
	step [146/244], loss=113.3807
	step [147/244], loss=106.4857
	step [148/244], loss=83.6899
	step [149/244], loss=97.2885
	step [150/244], loss=109.0204
	step [151/244], loss=98.4466
	step [152/244], loss=104.6316
	step [153/244], loss=113.6924
	step [154/244], loss=104.0086
	step [155/244], loss=91.6842
	step [156/244], loss=96.9354
	step [157/244], loss=121.1715
	step [158/244], loss=111.0666
	step [159/244], loss=89.7831
	step [160/244], loss=126.0580
	step [161/244], loss=114.9608
	step [162/244], loss=124.4682
	step [163/244], loss=105.0480
	step [164/244], loss=126.5731
	step [165/244], loss=98.9833
	step [166/244], loss=132.7709
	step [167/244], loss=98.3674
	step [168/244], loss=126.2039
	step [169/244], loss=92.1788
	step [170/244], loss=136.4902
	step [171/244], loss=102.3668
	step [172/244], loss=120.2776
	step [173/244], loss=112.5536
	step [174/244], loss=109.4662
	step [175/244], loss=106.6311
	step [176/244], loss=131.4934
	step [177/244], loss=111.6472
	step [178/244], loss=111.7384
	step [179/244], loss=108.9700
	step [180/244], loss=104.4142
	step [181/244], loss=103.4775
	step [182/244], loss=119.2150
	step [183/244], loss=97.1211
	step [184/244], loss=118.1891
	step [185/244], loss=100.3568
	step [186/244], loss=111.2406
	step [187/244], loss=106.8719
	step [188/244], loss=88.0208
	step [189/244], loss=108.9189
	step [190/244], loss=104.6251
	step [191/244], loss=99.1691
	step [192/244], loss=107.6266
	step [193/244], loss=105.0300
	step [194/244], loss=109.6389
	step [195/244], loss=106.7553
	step [196/244], loss=105.9787
	step [197/244], loss=89.8598
	step [198/244], loss=111.6015
	step [199/244], loss=104.1733
	step [200/244], loss=91.3711
	step [201/244], loss=121.4174
	step [202/244], loss=100.2491
	step [203/244], loss=110.0213
	step [204/244], loss=117.6406
	step [205/244], loss=87.4972
	step [206/244], loss=96.1018
	step [207/244], loss=113.2976
	step [208/244], loss=109.6595
	step [209/244], loss=93.6499
	step [210/244], loss=101.8282
	step [211/244], loss=100.8248
	step [212/244], loss=126.6247
	step [213/244], loss=82.2690
	step [214/244], loss=94.2650
	step [215/244], loss=99.3565
	step [216/244], loss=107.2884
	step [217/244], loss=131.8425
	step [218/244], loss=115.2283
	step [219/244], loss=115.1979
	step [220/244], loss=98.8293
	step [221/244], loss=95.0615
	step [222/244], loss=92.8319
	step [223/244], loss=119.0775
	step [224/244], loss=86.2697
	step [225/244], loss=108.1982
	step [226/244], loss=116.1135
	step [227/244], loss=113.2804
	step [228/244], loss=124.4004
	step [229/244], loss=100.6974
	step [230/244], loss=108.3244
	step [231/244], loss=104.5989
	step [232/244], loss=101.7596
	step [233/244], loss=113.9219
	step [234/244], loss=109.6712
	step [235/244], loss=128.7075
	step [236/244], loss=106.8659
	step [237/244], loss=108.9753
	step [238/244], loss=101.4035
	step [239/244], loss=95.1116
	step [240/244], loss=115.1065
	step [241/244], loss=101.7321
	step [242/244], loss=102.4303
	step [243/244], loss=101.9916
	step [244/244], loss=5.9059
	Evaluating
	loss=0.0194, precision=0.3236, recall=0.9178, f1=0.4785
Training epoch 15
	step [1/244], loss=118.8682
	step [2/244], loss=108.0635
	step [3/244], loss=120.1937
	step [4/244], loss=107.2635
	step [5/244], loss=86.8133
	step [6/244], loss=70.2265
	step [7/244], loss=106.9663
	step [8/244], loss=93.1601
	step [9/244], loss=79.1309
	step [10/244], loss=111.3159
	step [11/244], loss=93.9976
	step [12/244], loss=103.5160
	step [13/244], loss=108.8503
	step [14/244], loss=105.2141
	step [15/244], loss=106.5516
	step [16/244], loss=101.9420
	step [17/244], loss=98.9687
	step [18/244], loss=78.9750
	step [19/244], loss=132.1707
	step [20/244], loss=112.6232
	step [21/244], loss=105.9696
	step [22/244], loss=108.5634
	step [23/244], loss=99.8752
	step [24/244], loss=96.5463
	step [25/244], loss=117.3608
	step [26/244], loss=89.4757
	step [27/244], loss=108.4849
	step [28/244], loss=121.6153
	step [29/244], loss=97.2451
	step [30/244], loss=95.1026
	step [31/244], loss=103.2933
	step [32/244], loss=103.9964
	step [33/244], loss=119.2782
	step [34/244], loss=115.3870
	step [35/244], loss=90.2569
	step [36/244], loss=123.2385
	step [37/244], loss=106.2606
	step [38/244], loss=105.4431
	step [39/244], loss=130.7143
	step [40/244], loss=84.6540
	step [41/244], loss=112.7376
	step [42/244], loss=94.0688
	step [43/244], loss=105.7230
	step [44/244], loss=122.1136
	step [45/244], loss=136.2034
	step [46/244], loss=92.5755
	step [47/244], loss=99.2688
	step [48/244], loss=95.1596
	step [49/244], loss=107.6266
	step [50/244], loss=109.3358
	step [51/244], loss=97.0103
	step [52/244], loss=101.5512
	step [53/244], loss=113.2728
	step [54/244], loss=92.4958
	step [55/244], loss=108.2655
	step [56/244], loss=98.1214
	step [57/244], loss=96.9462
	step [58/244], loss=117.5743
	step [59/244], loss=118.2472
	step [60/244], loss=83.9450
	step [61/244], loss=91.1242
	step [62/244], loss=108.9032
	step [63/244], loss=91.4125
	step [64/244], loss=116.5020
	step [65/244], loss=117.8039
	step [66/244], loss=118.3198
	step [67/244], loss=94.0609
	step [68/244], loss=102.5602
	step [69/244], loss=99.0354
	step [70/244], loss=82.1806
	step [71/244], loss=111.4792
	step [72/244], loss=101.5136
	step [73/244], loss=97.7503
	step [74/244], loss=89.2282
	step [75/244], loss=96.3932
	step [76/244], loss=85.1232
	step [77/244], loss=92.9566
	step [78/244], loss=96.2797
	step [79/244], loss=115.2882
	step [80/244], loss=105.9714
	step [81/244], loss=104.5844
	step [82/244], loss=101.9107
	step [83/244], loss=115.1228
	step [84/244], loss=108.0016
	step [85/244], loss=98.1286
	step [86/244], loss=106.6010
	step [87/244], loss=87.0191
	step [88/244], loss=96.1997
	step [89/244], loss=94.8598
	step [90/244], loss=95.9432
	step [91/244], loss=102.0610
	step [92/244], loss=99.0495
	step [93/244], loss=97.2071
	step [94/244], loss=103.3146
	step [95/244], loss=102.2572
	step [96/244], loss=112.7835
	step [97/244], loss=89.7428
	step [98/244], loss=139.7124
	step [99/244], loss=102.5297
	step [100/244], loss=111.4000
	step [101/244], loss=92.1766
	step [102/244], loss=98.8792
	step [103/244], loss=89.3658
	step [104/244], loss=90.5962
	step [105/244], loss=96.4271
	step [106/244], loss=114.4023
	step [107/244], loss=142.7910
	step [108/244], loss=115.3919
	step [109/244], loss=112.6785
	step [110/244], loss=95.1996
	step [111/244], loss=106.8129
	step [112/244], loss=102.7371
	step [113/244], loss=77.1819
	step [114/244], loss=98.9643
	step [115/244], loss=95.8031
	step [116/244], loss=101.6686
	step [117/244], loss=95.4021
	step [118/244], loss=130.2916
	step [119/244], loss=103.7571
	step [120/244], loss=102.5357
	step [121/244], loss=110.2913
	step [122/244], loss=113.6800
	step [123/244], loss=116.8703
	step [124/244], loss=87.2125
	step [125/244], loss=96.0528
	step [126/244], loss=96.9740
	step [127/244], loss=108.6402
	step [128/244], loss=93.4135
	step [129/244], loss=112.0163
	step [130/244], loss=100.1090
	step [131/244], loss=94.3708
	step [132/244], loss=128.1619
	step [133/244], loss=98.2641
	step [134/244], loss=102.0901
	step [135/244], loss=93.8002
	step [136/244], loss=97.9843
	step [137/244], loss=104.2660
	step [138/244], loss=106.6552
	step [139/244], loss=107.3504
	step [140/244], loss=95.0988
	step [141/244], loss=102.9597
	step [142/244], loss=98.2914
	step [143/244], loss=98.9507
	step [144/244], loss=102.8122
	step [145/244], loss=132.2360
	step [146/244], loss=100.6081
	step [147/244], loss=109.4628
	step [148/244], loss=117.3525
	step [149/244], loss=94.6784
	step [150/244], loss=129.0094
	step [151/244], loss=87.6558
	step [152/244], loss=111.1261
	step [153/244], loss=101.2866
	step [154/244], loss=106.0701
	step [155/244], loss=103.4285
	step [156/244], loss=91.0977
	step [157/244], loss=125.0799
	step [158/244], loss=112.7177
	step [159/244], loss=110.8450
	step [160/244], loss=96.8899
	step [161/244], loss=113.7713
	step [162/244], loss=102.1037
	step [163/244], loss=96.3270
	step [164/244], loss=102.9380
	step [165/244], loss=98.4857
	step [166/244], loss=115.0619
	step [167/244], loss=108.9720
	step [168/244], loss=113.8906
	step [169/244], loss=104.2926
	step [170/244], loss=97.4367
	step [171/244], loss=102.7211
	step [172/244], loss=100.0942
	step [173/244], loss=134.2357
	step [174/244], loss=102.4846
	step [175/244], loss=104.8304
	step [176/244], loss=111.5696
	step [177/244], loss=116.9135
	step [178/244], loss=97.2338
	step [179/244], loss=107.6653
	step [180/244], loss=103.0462
	step [181/244], loss=103.6250
	step [182/244], loss=100.2480
	step [183/244], loss=107.4481
	step [184/244], loss=107.3007
	step [185/244], loss=112.4769
	step [186/244], loss=98.5878
	step [187/244], loss=129.4276
	step [188/244], loss=129.6817
	step [189/244], loss=117.2226
	step [190/244], loss=99.1457
	step [191/244], loss=108.9116
	step [192/244], loss=105.3543
	step [193/244], loss=97.6349
	step [194/244], loss=113.7272
	step [195/244], loss=101.4750
	step [196/244], loss=104.2929
	step [197/244], loss=92.6297
	step [198/244], loss=124.3871
	step [199/244], loss=113.6920
	step [200/244], loss=125.5645
	step [201/244], loss=113.0167
	step [202/244], loss=102.4789
	step [203/244], loss=94.1083
	step [204/244], loss=103.7165
	step [205/244], loss=138.1317
	step [206/244], loss=106.3854
	step [207/244], loss=112.1111
	step [208/244], loss=98.2884
	step [209/244], loss=127.8997
	step [210/244], loss=94.0920
	step [211/244], loss=101.3398
	step [212/244], loss=91.2722
	step [213/244], loss=105.6824
	step [214/244], loss=102.7073
	step [215/244], loss=112.8274
	step [216/244], loss=120.0964
	step [217/244], loss=83.3143
	step [218/244], loss=104.3292
	step [219/244], loss=117.4938
	step [220/244], loss=115.3024
	step [221/244], loss=102.1154
	step [222/244], loss=111.3914
	step [223/244], loss=106.9628
	step [224/244], loss=104.2012
	step [225/244], loss=101.7759
	step [226/244], loss=85.1032
	step [227/244], loss=95.1716
	step [228/244], loss=108.4113
	step [229/244], loss=98.1937
	step [230/244], loss=109.1809
	step [231/244], loss=91.9873
	step [232/244], loss=116.4766
	step [233/244], loss=101.4569
	step [234/244], loss=86.9593
	step [235/244], loss=133.5784
	step [236/244], loss=114.5185
	step [237/244], loss=102.2928
	step [238/244], loss=101.1976
	step [239/244], loss=107.8838
	step [240/244], loss=92.0279
	step [241/244], loss=115.2004
	step [242/244], loss=99.6139
	step [243/244], loss=116.6626
	step [244/244], loss=0.6741
	Evaluating
	loss=0.0157, precision=0.3819, recall=0.9193, f1=0.5397
Training epoch 16
	step [1/244], loss=83.5064
	step [2/244], loss=105.3658
	step [3/244], loss=104.5719
	step [4/244], loss=115.1615
	step [5/244], loss=119.8595
	step [6/244], loss=97.7366
	step [7/244], loss=105.5133
	step [8/244], loss=99.0564
	step [9/244], loss=92.7511
	step [10/244], loss=91.7988
	step [11/244], loss=125.9704
	step [12/244], loss=102.1873
	step [13/244], loss=117.0030
	step [14/244], loss=100.9168
	step [15/244], loss=99.6494
	step [16/244], loss=94.6565
	step [17/244], loss=71.0114
	step [18/244], loss=79.0388
	step [19/244], loss=89.7918
	step [20/244], loss=79.6095
	step [21/244], loss=123.6293
	step [22/244], loss=94.6227
	step [23/244], loss=122.6589
	step [24/244], loss=115.0781
	step [25/244], loss=95.0887
	step [26/244], loss=105.1714
	step [27/244], loss=111.7236
	step [28/244], loss=99.5375
	step [29/244], loss=119.9796
	step [30/244], loss=90.9596
	step [31/244], loss=122.7662
	step [32/244], loss=121.8814
	step [33/244], loss=102.2543
	step [34/244], loss=132.0284
	step [35/244], loss=117.8656
	step [36/244], loss=109.7850
	step [37/244], loss=101.2791
	step [38/244], loss=103.5234
	step [39/244], loss=100.8566
	step [40/244], loss=85.5278
	step [41/244], loss=111.3973
	step [42/244], loss=82.9019
	step [43/244], loss=99.6751
	step [44/244], loss=98.1488
	step [45/244], loss=108.1976
	step [46/244], loss=99.5947
	step [47/244], loss=111.7486
	step [48/244], loss=91.6046
	step [49/244], loss=114.2660
	step [50/244], loss=115.9403
	step [51/244], loss=107.2366
	step [52/244], loss=98.9244
	step [53/244], loss=99.1991
	step [54/244], loss=110.5750
	step [55/244], loss=108.4984
	step [56/244], loss=112.5164
	step [57/244], loss=108.6455
	step [58/244], loss=119.7772
	step [59/244], loss=77.4927
	step [60/244], loss=74.8443
	step [61/244], loss=94.5790
	step [62/244], loss=111.3215
	step [63/244], loss=108.3950
	step [64/244], loss=99.2709
	step [65/244], loss=101.2465
	step [66/244], loss=88.7021
	step [67/244], loss=116.6785
	step [68/244], loss=89.1158
	step [69/244], loss=94.3269
	step [70/244], loss=98.5620
	step [71/244], loss=87.5363
	step [72/244], loss=112.8638
	step [73/244], loss=101.3255
	step [74/244], loss=100.9567
	step [75/244], loss=98.6010
	step [76/244], loss=113.9560
	step [77/244], loss=116.1322
	step [78/244], loss=106.7589
	step [79/244], loss=86.1829
	step [80/244], loss=79.4686
	step [81/244], loss=96.5920
	step [82/244], loss=96.1544
	step [83/244], loss=129.6583
	step [84/244], loss=110.4679
	step [85/244], loss=107.6757
	step [86/244], loss=110.7735
	step [87/244], loss=90.1656
	step [88/244], loss=113.6855
	step [89/244], loss=116.2001
	step [90/244], loss=97.0713
	step [91/244], loss=100.8788
	step [92/244], loss=102.0411
	step [93/244], loss=89.9265
	step [94/244], loss=112.6614
	step [95/244], loss=99.8873
	step [96/244], loss=120.9931
	step [97/244], loss=102.5956
	step [98/244], loss=86.7622
	step [99/244], loss=80.0624
	step [100/244], loss=109.4085
	step [101/244], loss=96.6360
	step [102/244], loss=127.1388
	step [103/244], loss=123.5368
	step [104/244], loss=86.6593
	step [105/244], loss=92.6711
	step [106/244], loss=124.5206
	step [107/244], loss=94.1539
	step [108/244], loss=84.4061
	step [109/244], loss=94.8382
	step [110/244], loss=105.5872
	step [111/244], loss=110.3384
	step [112/244], loss=85.8188
	step [113/244], loss=80.7916
	step [114/244], loss=116.7695
	step [115/244], loss=98.8174
	step [116/244], loss=98.5169
	step [117/244], loss=118.0685
	step [118/244], loss=109.1109
	step [119/244], loss=93.7089
	step [120/244], loss=98.2127
	step [121/244], loss=106.3037
	step [122/244], loss=125.1681
	step [123/244], loss=94.1600
	step [124/244], loss=96.4618
	step [125/244], loss=99.9668
	step [126/244], loss=107.0468
	step [127/244], loss=105.5114
	step [128/244], loss=104.1633
	step [129/244], loss=102.8116
	step [130/244], loss=117.1344
	step [131/244], loss=104.9174
	step [132/244], loss=103.1206
	step [133/244], loss=100.0772
	step [134/244], loss=106.8405
	step [135/244], loss=111.7402
	step [136/244], loss=109.5912
	step [137/244], loss=108.0575
	step [138/244], loss=108.2862
	step [139/244], loss=88.4829
	step [140/244], loss=126.4038
	step [141/244], loss=96.7480
	step [142/244], loss=82.5845
	step [143/244], loss=114.0741
	step [144/244], loss=110.6081
	step [145/244], loss=107.3124
	step [146/244], loss=134.3684
	step [147/244], loss=112.7138
	step [148/244], loss=105.3727
	step [149/244], loss=105.4026
	step [150/244], loss=97.3152
	step [151/244], loss=111.2624
	step [152/244], loss=101.6745
	step [153/244], loss=115.5913
	step [154/244], loss=126.3928
	step [155/244], loss=96.9589
	step [156/244], loss=119.8804
	step [157/244], loss=105.2907
	step [158/244], loss=111.8326
	step [159/244], loss=100.2639
	step [160/244], loss=116.0718
	step [161/244], loss=90.3707
	step [162/244], loss=113.7040
	step [163/244], loss=94.0702
	step [164/244], loss=105.7161
	step [165/244], loss=102.1314
	step [166/244], loss=75.7748
	step [167/244], loss=98.3627
	step [168/244], loss=99.3776
	step [169/244], loss=105.2907
	step [170/244], loss=102.0582
	step [171/244], loss=109.8183
	step [172/244], loss=110.0831
	step [173/244], loss=124.3313
	step [174/244], loss=115.8484
	step [175/244], loss=114.4340
	step [176/244], loss=96.3039
	step [177/244], loss=105.2355
	step [178/244], loss=107.2510
	step [179/244], loss=114.8709
	step [180/244], loss=107.2364
	step [181/244], loss=114.8282
	step [182/244], loss=103.7105
	step [183/244], loss=109.6006
	step [184/244], loss=114.5113
	step [185/244], loss=91.2759
	step [186/244], loss=105.8436
	step [187/244], loss=81.2404
	step [188/244], loss=126.8325
	step [189/244], loss=82.0225
	step [190/244], loss=92.3032
	step [191/244], loss=131.2583
	step [192/244], loss=91.8134
	step [193/244], loss=114.7229
	step [194/244], loss=116.0873
	step [195/244], loss=106.4550
	step [196/244], loss=94.5529
	step [197/244], loss=84.6073
	step [198/244], loss=88.8014
	step [199/244], loss=107.1608
	step [200/244], loss=109.0165
	step [201/244], loss=127.4249
	step [202/244], loss=92.0907
	step [203/244], loss=96.2865
	step [204/244], loss=119.7334
	step [205/244], loss=109.4850
	step [206/244], loss=116.2090
	step [207/244], loss=107.4747
	step [208/244], loss=115.2815
	step [209/244], loss=99.4066
	step [210/244], loss=119.9740
	step [211/244], loss=117.7136
	step [212/244], loss=108.0151
	step [213/244], loss=129.0813
	step [214/244], loss=117.2338
	step [215/244], loss=101.0645
	step [216/244], loss=86.8083
	step [217/244], loss=107.9550
	step [218/244], loss=130.5349
	step [219/244], loss=100.1893
	step [220/244], loss=101.5992
	step [221/244], loss=86.3738
	step [222/244], loss=116.3082
	step [223/244], loss=94.4636
	step [224/244], loss=114.9581
	step [225/244], loss=114.6971
	step [226/244], loss=101.1866
	step [227/244], loss=90.5515
	step [228/244], loss=116.6398
	step [229/244], loss=115.3812
	step [230/244], loss=103.0494
	step [231/244], loss=88.7637
	step [232/244], loss=97.8733
	step [233/244], loss=106.6391
	step [234/244], loss=118.1366
	step [235/244], loss=107.5979
	step [236/244], loss=126.8151
	step [237/244], loss=94.0565
	step [238/244], loss=88.8171
	step [239/244], loss=106.1679
	step [240/244], loss=118.3409
	step [241/244], loss=110.1624
	step [242/244], loss=99.4263
	step [243/244], loss=104.7188
	step [244/244], loss=1.8468
	Evaluating
	loss=0.0145, precision=0.3535, recall=0.9220, f1=0.5110
Training epoch 17
	step [1/244], loss=74.4732
	step [2/244], loss=99.0995
	step [3/244], loss=104.8825
	step [4/244], loss=106.2454
	step [5/244], loss=100.5978
	step [6/244], loss=110.0948
	step [7/244], loss=98.7555
	step [8/244], loss=88.4827
	step [9/244], loss=88.9770
	step [10/244], loss=106.1893
	step [11/244], loss=110.4281
	step [12/244], loss=107.8045
	step [13/244], loss=111.5438
	step [14/244], loss=109.2214
	step [15/244], loss=106.9450
	step [16/244], loss=102.2385
	step [17/244], loss=110.5039
	step [18/244], loss=86.2187
	step [19/244], loss=100.2202
	step [20/244], loss=112.0578
	step [21/244], loss=106.1347
	step [22/244], loss=82.2978
	step [23/244], loss=94.5735
	step [24/244], loss=91.2922
	step [25/244], loss=101.9379
	step [26/244], loss=108.1178
	step [27/244], loss=100.5544
	step [28/244], loss=132.8602
	step [29/244], loss=104.3525
	step [30/244], loss=95.5112
	step [31/244], loss=106.1042
	step [32/244], loss=123.5633
	step [33/244], loss=94.8730
	step [34/244], loss=96.8171
	step [35/244], loss=81.4665
	step [36/244], loss=108.2119
	step [37/244], loss=103.4539
	step [38/244], loss=85.5647
	step [39/244], loss=91.5561
	step [40/244], loss=103.4752
	step [41/244], loss=88.2429
	step [42/244], loss=109.5642
	step [43/244], loss=79.2930
	step [44/244], loss=96.7346
	step [45/244], loss=89.3688
	step [46/244], loss=111.9188
	step [47/244], loss=114.1535
	step [48/244], loss=106.3486
	step [49/244], loss=100.2118
	step [50/244], loss=108.3221
	step [51/244], loss=108.1540
	step [52/244], loss=111.6252
	step [53/244], loss=112.3251
	step [54/244], loss=111.0282
	step [55/244], loss=96.9677
	step [56/244], loss=111.1428
	step [57/244], loss=122.9076
	step [58/244], loss=86.1042
	step [59/244], loss=73.1690
	step [60/244], loss=115.4608
	step [61/244], loss=113.9484
	step [62/244], loss=100.8770
	step [63/244], loss=114.1137
	step [64/244], loss=101.4830
	step [65/244], loss=109.9862
	step [66/244], loss=94.7308
	step [67/244], loss=97.9071
	step [68/244], loss=113.5264
	step [69/244], loss=103.2746
	step [70/244], loss=86.4535
	step [71/244], loss=116.2896
	step [72/244], loss=80.6337
	step [73/244], loss=95.5500
	step [74/244], loss=112.8754
	step [75/244], loss=99.9381
	step [76/244], loss=107.3020
	step [77/244], loss=119.7331
	step [78/244], loss=112.5128
	step [79/244], loss=103.5595
	step [80/244], loss=82.9863
	step [81/244], loss=100.2423
	step [82/244], loss=115.4604
	step [83/244], loss=94.6646
	step [84/244], loss=88.3299
	step [85/244], loss=116.7250
	step [86/244], loss=89.4553
	step [87/244], loss=109.6676
	step [88/244], loss=113.0216
	step [89/244], loss=121.8758
	step [90/244], loss=107.9297
	step [91/244], loss=112.1897
	step [92/244], loss=106.6863
	step [93/244], loss=95.2970
	step [94/244], loss=96.2230
	step [95/244], loss=99.5293
	step [96/244], loss=122.0958
	step [97/244], loss=128.8043
	step [98/244], loss=84.8102
	step [99/244], loss=96.5423
	step [100/244], loss=102.5231
	step [101/244], loss=102.9898
	step [102/244], loss=102.2717
	step [103/244], loss=110.3229
	step [104/244], loss=119.7511
	step [105/244], loss=123.3691
	step [106/244], loss=97.3393
	step [107/244], loss=99.2336
	step [108/244], loss=84.8894
	step [109/244], loss=135.6526
	step [110/244], loss=116.6861
	step [111/244], loss=102.8292
	step [112/244], loss=106.8591
	step [113/244], loss=97.7460
	step [114/244], loss=102.5041
	step [115/244], loss=91.0969
	step [116/244], loss=108.1738
	step [117/244], loss=107.5527
	step [118/244], loss=106.3089
	step [119/244], loss=99.5064
	step [120/244], loss=110.8450
	step [121/244], loss=97.8568
	step [122/244], loss=111.8268
	step [123/244], loss=94.4841
	step [124/244], loss=85.9342
	step [125/244], loss=99.6020
	step [126/244], loss=103.8784
	step [127/244], loss=109.3533
	step [128/244], loss=96.6347
	step [129/244], loss=77.4393
	step [130/244], loss=117.5283
	step [131/244], loss=109.0691
	step [132/244], loss=104.5177
	step [133/244], loss=103.4961
	step [134/244], loss=113.0541
	step [135/244], loss=102.7916
	step [136/244], loss=104.6727
	step [137/244], loss=108.6685
	step [138/244], loss=118.4368
	step [139/244], loss=110.1591
	step [140/244], loss=98.6498
	step [141/244], loss=95.1124
	step [142/244], loss=113.4522
	step [143/244], loss=113.0586
	step [144/244], loss=97.9571
	step [145/244], loss=100.1789
	step [146/244], loss=109.1819
	step [147/244], loss=104.4088
	step [148/244], loss=112.6829
	step [149/244], loss=109.1328
	step [150/244], loss=90.8725
	step [151/244], loss=87.9110
	step [152/244], loss=104.1211
	step [153/244], loss=93.3154
	step [154/244], loss=88.2191
	step [155/244], loss=91.3723
	step [156/244], loss=110.9252
	step [157/244], loss=116.2886
	step [158/244], loss=93.0852
	step [159/244], loss=97.8127
	step [160/244], loss=113.1749
	step [161/244], loss=90.0697
	step [162/244], loss=89.7702
	step [163/244], loss=123.4232
	step [164/244], loss=102.0613
	step [165/244], loss=118.1322
	step [166/244], loss=112.9401
	step [167/244], loss=79.6533
	step [168/244], loss=115.9268
	step [169/244], loss=112.8236
	step [170/244], loss=107.9789
	step [171/244], loss=101.9253
	step [172/244], loss=116.5262
	step [173/244], loss=101.8574
	step [174/244], loss=91.6278
	step [175/244], loss=91.7941
	step [176/244], loss=107.3910
	step [177/244], loss=110.2025
	step [178/244], loss=95.1785
	step [179/244], loss=85.3066
	step [180/244], loss=105.6151
	step [181/244], loss=89.2927
	step [182/244], loss=86.7623
	step [183/244], loss=102.5188
	step [184/244], loss=88.7698
	step [185/244], loss=108.9215
	step [186/244], loss=97.5125
	step [187/244], loss=106.0541
	step [188/244], loss=112.1951
	step [189/244], loss=123.1483
	step [190/244], loss=100.2167
	step [191/244], loss=107.8904
	step [192/244], loss=103.8596
	step [193/244], loss=102.5826
	step [194/244], loss=97.8367
	step [195/244], loss=97.7969
	step [196/244], loss=86.7005
	step [197/244], loss=93.2493
	step [198/244], loss=93.8039
	step [199/244], loss=119.5018
	step [200/244], loss=81.4588
	step [201/244], loss=98.3402
	step [202/244], loss=106.7691
	step [203/244], loss=99.8867
	step [204/244], loss=109.6901
	step [205/244], loss=95.0462
	step [206/244], loss=106.2263
	step [207/244], loss=123.7225
	step [208/244], loss=106.9535
	step [209/244], loss=93.5442
	step [210/244], loss=130.5433
	step [211/244], loss=90.1236
	step [212/244], loss=96.3498
	step [213/244], loss=94.5801
	step [214/244], loss=103.0705
	step [215/244], loss=80.4636
	step [216/244], loss=117.5558
	step [217/244], loss=116.2373
	step [218/244], loss=105.0825
	step [219/244], loss=103.6742
	step [220/244], loss=84.9865
	step [221/244], loss=99.1966
	step [222/244], loss=104.1716
	step [223/244], loss=110.3576
	step [224/244], loss=133.3049
	step [225/244], loss=88.6445
	step [226/244], loss=87.3379
	step [227/244], loss=91.1877
	step [228/244], loss=95.1371
	step [229/244], loss=106.8307
	step [230/244], loss=109.6830
	step [231/244], loss=102.0087
	step [232/244], loss=98.8364
	step [233/244], loss=112.6275
	step [234/244], loss=105.7455
	step [235/244], loss=82.1567
	step [236/244], loss=86.2112
	step [237/244], loss=102.3906
	step [238/244], loss=102.4966
	step [239/244], loss=94.3941
	step [240/244], loss=97.5876
	step [241/244], loss=97.8592
	step [242/244], loss=89.5435
	step [243/244], loss=119.0242
	step [244/244], loss=4.1516
	Evaluating
	loss=0.0142, precision=0.3476, recall=0.9357, f1=0.5069
Training epoch 18
	step [1/244], loss=100.0731
	step [2/244], loss=97.8559
	step [3/244], loss=109.2065
	step [4/244], loss=97.5866
	step [5/244], loss=100.6740
	step [6/244], loss=100.1566
	step [7/244], loss=72.4594
	step [8/244], loss=89.5775
	step [9/244], loss=95.9175
	step [10/244], loss=107.6750
	step [11/244], loss=117.0881
	step [12/244], loss=98.0494
	step [13/244], loss=88.7372
	step [14/244], loss=99.2276
	step [15/244], loss=75.4705
	step [16/244], loss=106.4995
	step [17/244], loss=105.5354
	step [18/244], loss=99.9459
	step [19/244], loss=90.0627
	step [20/244], loss=106.8812
	step [21/244], loss=94.3098
	step [22/244], loss=115.1966
	step [23/244], loss=96.1022
	step [24/244], loss=99.2844
	step [25/244], loss=86.8648
	step [26/244], loss=100.7137
	step [27/244], loss=92.8586
	step [28/244], loss=113.3927
	step [29/244], loss=105.0460
	step [30/244], loss=91.9670
	step [31/244], loss=97.1997
	step [32/244], loss=94.0334
	step [33/244], loss=118.3844
	step [34/244], loss=95.2248
	step [35/244], loss=93.1677
	step [36/244], loss=91.5164
	step [37/244], loss=110.2976
	step [38/244], loss=92.2997
	step [39/244], loss=117.3073
	step [40/244], loss=110.9428
	step [41/244], loss=106.3051
	step [42/244], loss=92.6610
	step [43/244], loss=83.0432
	step [44/244], loss=85.0572
	step [45/244], loss=101.3735
	step [46/244], loss=95.6997
	step [47/244], loss=101.8671
	step [48/244], loss=107.2823
	step [49/244], loss=112.7556
	step [50/244], loss=113.8858
	step [51/244], loss=98.8021
	step [52/244], loss=94.0332
	step [53/244], loss=97.0664
	step [54/244], loss=90.8504
	step [55/244], loss=99.7717
	step [56/244], loss=106.7158
	step [57/244], loss=102.5259
	step [58/244], loss=94.3340
	step [59/244], loss=84.9916
	step [60/244], loss=94.4681
	step [61/244], loss=104.0609
	step [62/244], loss=115.7293
	step [63/244], loss=102.4079
	step [64/244], loss=114.0367
	step [65/244], loss=82.5401
	step [66/244], loss=100.7523
	step [67/244], loss=97.3056
	step [68/244], loss=120.4077
	step [69/244], loss=97.8449
	step [70/244], loss=83.5516
	step [71/244], loss=87.0667
	step [72/244], loss=114.7518
	step [73/244], loss=91.0888
	step [74/244], loss=103.4599
	step [75/244], loss=83.0750
	step [76/244], loss=91.2629
	step [77/244], loss=93.7197
	step [78/244], loss=105.9930
	step [79/244], loss=103.3514
	step [80/244], loss=84.7830
	step [81/244], loss=104.7395
	step [82/244], loss=98.7155
	step [83/244], loss=101.4201
	step [84/244], loss=112.6041
	step [85/244], loss=114.9249
	step [86/244], loss=111.1710
	step [87/244], loss=88.7360
	step [88/244], loss=95.3025
	step [89/244], loss=92.4132
	step [90/244], loss=102.0136
	step [91/244], loss=119.0050
	step [92/244], loss=122.8790
	step [93/244], loss=78.9136
	step [94/244], loss=94.1418
	step [95/244], loss=96.8937
	step [96/244], loss=95.8492
	step [97/244], loss=106.1091
	step [98/244], loss=113.3152
	step [99/244], loss=132.5882
	step [100/244], loss=105.0364
	step [101/244], loss=89.3855
	step [102/244], loss=105.2227
	step [103/244], loss=89.5971
	step [104/244], loss=94.2154
	step [105/244], loss=99.5682
	step [106/244], loss=112.6237
	step [107/244], loss=107.4413
	step [108/244], loss=89.0454
	step [109/244], loss=100.4805
	step [110/244], loss=108.5172
	step [111/244], loss=114.5888
	step [112/244], loss=100.7085
	step [113/244], loss=109.1397
	step [114/244], loss=128.5131
	step [115/244], loss=98.4137
	step [116/244], loss=88.5729
	step [117/244], loss=108.7931
	step [118/244], loss=107.7170
	step [119/244], loss=106.1690
	step [120/244], loss=96.7198
	step [121/244], loss=98.6862
	step [122/244], loss=104.2421
	step [123/244], loss=111.4205
	step [124/244], loss=88.6196
	step [125/244], loss=100.7642
	step [126/244], loss=116.6602
	step [127/244], loss=107.8154
	step [128/244], loss=97.5591
	step [129/244], loss=122.8434
	step [130/244], loss=107.9548
	step [131/244], loss=102.5222
	step [132/244], loss=99.9026
	step [133/244], loss=112.0377
	step [134/244], loss=98.0267
	step [135/244], loss=103.6906
	step [136/244], loss=125.6861
	step [137/244], loss=102.1218
	step [138/244], loss=101.5768
	step [139/244], loss=87.9139
	step [140/244], loss=89.8114
	step [141/244], loss=106.5449
	step [142/244], loss=104.2825
	step [143/244], loss=103.1489
	step [144/244], loss=101.3118
	step [145/244], loss=78.4024
	step [146/244], loss=115.0050
	step [147/244], loss=96.0219
	step [148/244], loss=95.4967
	step [149/244], loss=129.0524
	step [150/244], loss=99.8598
	step [151/244], loss=101.8626
	step [152/244], loss=88.5245
	step [153/244], loss=100.6302
	step [154/244], loss=109.5616
	step [155/244], loss=116.9820
	step [156/244], loss=105.1927
	step [157/244], loss=92.5066
	step [158/244], loss=99.4329
	step [159/244], loss=91.6828
	step [160/244], loss=105.4227
	step [161/244], loss=97.4988
	step [162/244], loss=129.6608
	step [163/244], loss=104.1858
	step [164/244], loss=96.1889
	step [165/244], loss=107.4861
	step [166/244], loss=109.7997
	step [167/244], loss=114.3779
	step [168/244], loss=108.8887
	step [169/244], loss=103.4645
	step [170/244], loss=112.0088
	step [171/244], loss=102.7329
	step [172/244], loss=101.4343
	step [173/244], loss=111.4786
	step [174/244], loss=110.1419
	step [175/244], loss=97.3497
	step [176/244], loss=96.6298
	step [177/244], loss=104.9855
	step [178/244], loss=89.6560
	step [179/244], loss=112.0069
	step [180/244], loss=93.2941
	step [181/244], loss=111.3267
	step [182/244], loss=111.9185
	step [183/244], loss=89.2141
	step [184/244], loss=88.3144
	step [185/244], loss=104.9970
	step [186/244], loss=108.8959
	step [187/244], loss=117.9575
	step [188/244], loss=99.8573
	step [189/244], loss=113.6963
	step [190/244], loss=101.4055
	step [191/244], loss=84.6983
	step [192/244], loss=98.9819
	step [193/244], loss=99.4962
	step [194/244], loss=110.2563
	step [195/244], loss=92.7979
	step [196/244], loss=123.0690
	step [197/244], loss=91.7082
	step [198/244], loss=95.0338
	step [199/244], loss=93.3369
	step [200/244], loss=113.7469
	step [201/244], loss=103.2209
	step [202/244], loss=96.1069
	step [203/244], loss=113.7444
	step [204/244], loss=97.7351
	step [205/244], loss=110.1250
	step [206/244], loss=96.3616
	step [207/244], loss=102.6909
	step [208/244], loss=114.9245
	step [209/244], loss=100.7005
	step [210/244], loss=77.5829
	step [211/244], loss=103.0689
	step [212/244], loss=101.9950
	step [213/244], loss=95.4873
	step [214/244], loss=81.2239
	step [215/244], loss=110.6524
	step [216/244], loss=122.7597
	step [217/244], loss=89.4719
	step [218/244], loss=78.2025
	step [219/244], loss=95.9324
	step [220/244], loss=86.1082
	step [221/244], loss=102.7820
	step [222/244], loss=109.2172
	step [223/244], loss=123.2285
	step [224/244], loss=102.8610
	step [225/244], loss=97.5380
	step [226/244], loss=91.5858
	step [227/244], loss=94.5466
	step [228/244], loss=120.9486
	step [229/244], loss=99.8669
	step [230/244], loss=107.9817
	step [231/244], loss=92.6311
	step [232/244], loss=93.5952
	step [233/244], loss=112.5282
	step [234/244], loss=108.7723
	step [235/244], loss=98.1855
	step [236/244], loss=119.8759
	step [237/244], loss=101.5020
	step [238/244], loss=108.3565
	step [239/244], loss=109.5469
	step [240/244], loss=114.4642
	step [241/244], loss=131.8733
	step [242/244], loss=106.0473
	step [243/244], loss=95.2025
	step [244/244], loss=5.5090
	Evaluating
	loss=0.0130, precision=0.3121, recall=0.9312, f1=0.4675
Training epoch 19
	step [1/244], loss=98.1165
	step [2/244], loss=103.3274
	step [3/244], loss=116.2385
	step [4/244], loss=102.6344
	step [5/244], loss=102.2623
	step [6/244], loss=108.0109
	step [7/244], loss=92.2318
	step [8/244], loss=112.0027
	step [9/244], loss=82.5401
	step [10/244], loss=118.4178
	step [11/244], loss=104.8322
	step [12/244], loss=101.7854
	step [13/244], loss=92.3683
	step [14/244], loss=104.8647
	step [15/244], loss=98.8897
	step [16/244], loss=109.0772
	step [17/244], loss=108.2792
	step [18/244], loss=105.2435
	step [19/244], loss=98.3508
	step [20/244], loss=103.3783
	step [21/244], loss=85.8221
	step [22/244], loss=99.6886
	step [23/244], loss=89.3048
	step [24/244], loss=94.6097
	step [25/244], loss=91.3569
	step [26/244], loss=117.6902
	step [27/244], loss=111.5758
	step [28/244], loss=94.6752
	step [29/244], loss=87.8415
	step [30/244], loss=102.9498
	step [31/244], loss=103.8239
	step [32/244], loss=89.6080
	step [33/244], loss=115.7498
	step [34/244], loss=92.9703
	step [35/244], loss=104.9996
	step [36/244], loss=72.8229
	step [37/244], loss=101.4843
	step [38/244], loss=106.6169
	step [39/244], loss=104.0914
	step [40/244], loss=92.3189
	step [41/244], loss=114.4141
	step [42/244], loss=112.9546
	step [43/244], loss=94.1556
	step [44/244], loss=88.8468
	step [45/244], loss=102.5014
	step [46/244], loss=112.5268
	step [47/244], loss=97.4061
	step [48/244], loss=104.4220
	step [49/244], loss=92.2113
	step [50/244], loss=89.1934
	step [51/244], loss=107.3638
	step [52/244], loss=117.8050
	step [53/244], loss=117.5417
	step [54/244], loss=73.9813
	step [55/244], loss=111.2098
	step [56/244], loss=103.5492
	step [57/244], loss=83.7935
	step [58/244], loss=89.7952
	step [59/244], loss=99.9585
	step [60/244], loss=111.5479
	step [61/244], loss=83.2096
	step [62/244], loss=110.5546
	step [63/244], loss=95.5453
	step [64/244], loss=104.9346
	step [65/244], loss=108.5963
	step [66/244], loss=125.1570
	step [67/244], loss=114.2430
	step [68/244], loss=79.0810
	step [69/244], loss=99.2710
	step [70/244], loss=85.5917
	step [71/244], loss=103.8924
	step [72/244], loss=83.0482
	step [73/244], loss=102.1821
	step [74/244], loss=100.9259
	step [75/244], loss=108.7099
	step [76/244], loss=105.4512
	step [77/244], loss=101.9125
	step [78/244], loss=95.7066
	step [79/244], loss=88.9574
	step [80/244], loss=81.7399
	step [81/244], loss=91.1814
	step [82/244], loss=105.8235
	step [83/244], loss=90.8682
	step [84/244], loss=97.1659
	step [85/244], loss=115.2090
	step [86/244], loss=77.4293
	step [87/244], loss=105.0392
	step [88/244], loss=101.5142
	step [89/244], loss=91.5025
	step [90/244], loss=110.2490
	step [91/244], loss=106.3554
	step [92/244], loss=116.4674
	step [93/244], loss=85.9555
	step [94/244], loss=98.9093
	step [95/244], loss=107.9394
	step [96/244], loss=101.5028
	step [97/244], loss=99.4888
	step [98/244], loss=115.1801
	step [99/244], loss=99.6368
	step [100/244], loss=90.6551
	step [101/244], loss=112.3941
	step [102/244], loss=84.6873
	step [103/244], loss=89.6510
	step [104/244], loss=98.5047
	step [105/244], loss=90.2178
	step [106/244], loss=115.0310
	step [107/244], loss=83.6026
	step [108/244], loss=91.2609
	step [109/244], loss=107.7650
	step [110/244], loss=87.6362
	step [111/244], loss=92.5046
	step [112/244], loss=95.3895
	step [113/244], loss=98.8026
	step [114/244], loss=102.9507
	step [115/244], loss=85.0786
	step [116/244], loss=95.5172
	step [117/244], loss=93.9045
	step [118/244], loss=108.7504
	step [119/244], loss=90.1356
	step [120/244], loss=109.6091
	step [121/244], loss=119.1185
	step [122/244], loss=106.9964
	step [123/244], loss=87.6723
	step [124/244], loss=100.7586
	step [125/244], loss=102.2404
	step [126/244], loss=103.5550
	step [127/244], loss=111.9231
	step [128/244], loss=87.4134
	step [129/244], loss=83.0092
	step [130/244], loss=89.2846
	step [131/244], loss=101.9375
	step [132/244], loss=115.6065
	step [133/244], loss=89.1958
	step [134/244], loss=92.7112
	step [135/244], loss=116.3081
	step [136/244], loss=104.7731
	step [137/244], loss=94.7615
	step [138/244], loss=109.9220
	step [139/244], loss=81.9775
	step [140/244], loss=97.7376
	step [141/244], loss=120.0982
	step [142/244], loss=110.1933
	step [143/244], loss=100.6278
	step [144/244], loss=103.3432
	step [145/244], loss=104.9172
	step [146/244], loss=84.6210
	step [147/244], loss=107.2996
	step [148/244], loss=101.0605
	step [149/244], loss=87.5164
	step [150/244], loss=94.5969
	step [151/244], loss=90.6307
	step [152/244], loss=101.2766
	step [153/244], loss=96.3014
	step [154/244], loss=101.0042
	step [155/244], loss=100.1808
	step [156/244], loss=104.0713
	step [157/244], loss=91.7525
	step [158/244], loss=78.2120
	step [159/244], loss=118.5780
	step [160/244], loss=115.7530
	step [161/244], loss=101.8275
	step [162/244], loss=102.5062
	step [163/244], loss=100.4550
	step [164/244], loss=107.5474
	step [165/244], loss=113.9977
	step [166/244], loss=88.5767
	step [167/244], loss=100.0738
	step [168/244], loss=100.5955
	step [169/244], loss=122.1789
	step [170/244], loss=110.1099
	step [171/244], loss=100.5913
	step [172/244], loss=96.3819
	step [173/244], loss=99.3007
	step [174/244], loss=105.1327
	step [175/244], loss=102.4923
	step [176/244], loss=94.3464
	step [177/244], loss=101.8497
	step [178/244], loss=97.9561
	step [179/244], loss=97.6320
	step [180/244], loss=104.9732
	step [181/244], loss=98.3225
	step [182/244], loss=118.7792
	step [183/244], loss=99.5785
	step [184/244], loss=111.6120
	step [185/244], loss=121.8848
	step [186/244], loss=92.1125
	step [187/244], loss=97.7479
	step [188/244], loss=115.1263
	step [189/244], loss=111.2908
	step [190/244], loss=119.0921
	step [191/244], loss=96.5587
	step [192/244], loss=122.7061
	step [193/244], loss=111.2097
	step [194/244], loss=91.4482
	step [195/244], loss=90.6873
	step [196/244], loss=93.1879
	step [197/244], loss=116.5277
	step [198/244], loss=89.9063
	step [199/244], loss=88.8143
	step [200/244], loss=101.9037
	step [201/244], loss=103.3860
	step [202/244], loss=79.6414
	step [203/244], loss=88.0897
	step [204/244], loss=121.5063
	step [205/244], loss=107.7222
	step [206/244], loss=103.0771
	step [207/244], loss=98.4056
	step [208/244], loss=121.5686
	step [209/244], loss=89.1330
	step [210/244], loss=108.0705
	step [211/244], loss=110.2114
	step [212/244], loss=83.3539
	step [213/244], loss=101.2237
	step [214/244], loss=102.4091
	step [215/244], loss=105.1237
	step [216/244], loss=104.6507
	step [217/244], loss=97.1105
	step [218/244], loss=94.9024
	step [219/244], loss=121.8232
	step [220/244], loss=87.1762
	step [221/244], loss=80.3647
	step [222/244], loss=110.8744
	step [223/244], loss=86.7613
	step [224/244], loss=100.5410
	step [225/244], loss=94.5811
	step [226/244], loss=126.1520
	step [227/244], loss=97.2473
	step [228/244], loss=108.5165
	step [229/244], loss=107.7520
	step [230/244], loss=100.7572
	step [231/244], loss=101.1155
	step [232/244], loss=100.8188
	step [233/244], loss=102.2031
	step [234/244], loss=115.4658
	step [235/244], loss=115.8117
	step [236/244], loss=107.8312
	step [237/244], loss=89.4880
	step [238/244], loss=81.0239
	step [239/244], loss=86.6947
	step [240/244], loss=113.5670
	step [241/244], loss=89.9392
	step [242/244], loss=102.1107
	step [243/244], loss=114.2951
	step [244/244], loss=4.0454
	Evaluating
	loss=0.0124, precision=0.3556, recall=0.9244, f1=0.5136
Training epoch 20
	step [1/244], loss=96.6532
	step [2/244], loss=108.2150
	step [3/244], loss=105.0101
	step [4/244], loss=84.1976
	step [5/244], loss=101.5214
	step [6/244], loss=102.9658
	step [7/244], loss=91.3751
	step [8/244], loss=107.2905
	step [9/244], loss=100.2734
	step [10/244], loss=93.9223
	step [11/244], loss=89.9491
	step [12/244], loss=116.5070
	step [13/244], loss=104.3537
	step [14/244], loss=105.0154
	step [15/244], loss=101.3527
	step [16/244], loss=86.1258
	step [17/244], loss=95.4669
	step [18/244], loss=103.1947
	step [19/244], loss=86.2374
	step [20/244], loss=98.8491
	step [21/244], loss=76.4121
	step [22/244], loss=90.5020
	step [23/244], loss=96.6016
	step [24/244], loss=89.8033
	step [25/244], loss=109.5374
	step [26/244], loss=81.8220
	step [27/244], loss=108.0855
	step [28/244], loss=116.5503
	step [29/244], loss=97.3923
	step [30/244], loss=105.5740
	step [31/244], loss=108.6214
	step [32/244], loss=94.1683
	step [33/244], loss=92.7895
	step [34/244], loss=100.9903
	step [35/244], loss=110.7188
	step [36/244], loss=111.2579
	step [37/244], loss=89.5172
	step [38/244], loss=95.8084
	step [39/244], loss=111.9152
	step [40/244], loss=110.4969
	step [41/244], loss=97.9269
	step [42/244], loss=107.1412
	step [43/244], loss=81.8762
	step [44/244], loss=106.7207
	step [45/244], loss=119.5732
	step [46/244], loss=98.5654
	step [47/244], loss=86.3762
	step [48/244], loss=88.4835
	step [49/244], loss=67.6377
	step [50/244], loss=110.1306
	step [51/244], loss=90.2235
	step [52/244], loss=109.9605
	step [53/244], loss=88.1885
	step [54/244], loss=101.0547
	step [55/244], loss=98.2224
	step [56/244], loss=107.2638
	step [57/244], loss=93.9108
	step [58/244], loss=79.5396
	step [59/244], loss=102.2149
	step [60/244], loss=106.3342
	step [61/244], loss=127.6650
	step [62/244], loss=112.7109
	step [63/244], loss=83.3973
	step [64/244], loss=103.6978
	step [65/244], loss=101.0686
	step [66/244], loss=116.8600
	step [67/244], loss=93.7234
	step [68/244], loss=103.5690
	step [69/244], loss=106.6213
	step [70/244], loss=95.6557
	step [71/244], loss=95.9314
	step [72/244], loss=109.4842
	step [73/244], loss=92.6632
	step [74/244], loss=98.7803
	step [75/244], loss=114.3518
	step [76/244], loss=109.1523
	step [77/244], loss=95.0332
	step [78/244], loss=108.5654
	step [79/244], loss=103.7836
	step [80/244], loss=75.4167
	step [81/244], loss=92.6629
	step [82/244], loss=84.9251
	step [83/244], loss=107.1290
	step [84/244], loss=112.0628
	step [85/244], loss=106.3814
	step [86/244], loss=120.4221
	step [87/244], loss=90.0391
	step [88/244], loss=87.6380
	step [89/244], loss=109.6546
	step [90/244], loss=99.4719
	step [91/244], loss=91.7051
	step [92/244], loss=82.4627
	step [93/244], loss=89.7759
	step [94/244], loss=95.3521
	step [95/244], loss=108.7562
	step [96/244], loss=110.2371
	step [97/244], loss=98.1586
	step [98/244], loss=109.5228
	step [99/244], loss=102.7457
	step [100/244], loss=87.4936
	step [101/244], loss=88.8324
	step [102/244], loss=89.4020
	step [103/244], loss=99.5133
	step [104/244], loss=111.9709
	step [105/244], loss=97.9440
	step [106/244], loss=103.8265
	step [107/244], loss=111.3947
	step [108/244], loss=100.2054
	step [109/244], loss=86.9654
	step [110/244], loss=92.6817
	step [111/244], loss=95.9698
	step [112/244], loss=104.1299
	step [113/244], loss=97.5247
	step [114/244], loss=93.2967
	step [115/244], loss=101.7560
	step [116/244], loss=117.5212
	step [117/244], loss=106.7746
	step [118/244], loss=99.2030
	step [119/244], loss=91.4181
	step [120/244], loss=100.4790
	step [121/244], loss=113.3871
	step [122/244], loss=104.6249
	step [123/244], loss=102.4952
	step [124/244], loss=99.1703
	step [125/244], loss=84.3090
	step [126/244], loss=100.1396
	step [127/244], loss=88.0947
	step [128/244], loss=75.4480
	step [129/244], loss=103.0323
	step [130/244], loss=97.8292
	step [131/244], loss=102.9743
	step [132/244], loss=101.7825
	step [133/244], loss=95.4015
	step [134/244], loss=104.7888
	step [135/244], loss=93.0445
	step [136/244], loss=92.1837
	step [137/244], loss=81.5836
	step [138/244], loss=100.9886
	step [139/244], loss=105.6584
	step [140/244], loss=94.0028
	step [141/244], loss=98.9184
	step [142/244], loss=102.6133
	step [143/244], loss=111.0809
	step [144/244], loss=118.9803
	step [145/244], loss=99.6571
	step [146/244], loss=106.5363
	step [147/244], loss=93.9394
	step [148/244], loss=122.5523
	step [149/244], loss=131.6230
	step [150/244], loss=95.5767
	step [151/244], loss=88.7503
	step [152/244], loss=109.2799
	step [153/244], loss=86.5572
	step [154/244], loss=97.2965
	step [155/244], loss=107.5488
	step [156/244], loss=98.6768
	step [157/244], loss=113.0844
	step [158/244], loss=74.8946
	step [159/244], loss=104.9832
	step [160/244], loss=88.1091
	step [161/244], loss=94.4936
	step [162/244], loss=101.3467
	step [163/244], loss=97.8213
	step [164/244], loss=87.0595
	step [165/244], loss=107.9840
	step [166/244], loss=90.8392
	step [167/244], loss=96.1946
	step [168/244], loss=76.8131
	step [169/244], loss=102.5053
	step [170/244], loss=104.8422
	step [171/244], loss=104.1756
	step [172/244], loss=79.9634
	step [173/244], loss=93.4400
	step [174/244], loss=87.0977
	step [175/244], loss=75.1077
	step [176/244], loss=106.2402
	step [177/244], loss=86.7332
	step [178/244], loss=100.1406
	step [179/244], loss=79.4434
	step [180/244], loss=87.9466
	step [181/244], loss=89.4866
	step [182/244], loss=91.4797
	step [183/244], loss=106.6993
	step [184/244], loss=86.5836
	step [185/244], loss=115.8645
	step [186/244], loss=113.2743
	step [187/244], loss=112.6690
	step [188/244], loss=117.0635
	step [189/244], loss=102.4099
	step [190/244], loss=91.8176
	step [191/244], loss=97.5449
	step [192/244], loss=101.5703
	step [193/244], loss=109.1467
	step [194/244], loss=98.6014
	step [195/244], loss=96.5476
	step [196/244], loss=122.4144
	step [197/244], loss=88.7513
	step [198/244], loss=98.2959
	step [199/244], loss=97.5219
	step [200/244], loss=83.2392
	step [201/244], loss=91.8278
	step [202/244], loss=92.4895
	step [203/244], loss=99.9465
	step [204/244], loss=91.9954
	step [205/244], loss=97.7132
	step [206/244], loss=92.1444
	step [207/244], loss=110.9662
	step [208/244], loss=95.5199
	step [209/244], loss=103.1844
	step [210/244], loss=101.7006
	step [211/244], loss=81.8569
	step [212/244], loss=117.7658
	step [213/244], loss=96.2294
	step [214/244], loss=102.2962
	step [215/244], loss=88.1339
	step [216/244], loss=95.1551
	step [217/244], loss=105.8888
	step [218/244], loss=109.5614
	step [219/244], loss=98.7764
	step [220/244], loss=89.8004
	step [221/244], loss=94.4798
	step [222/244], loss=120.1711
	step [223/244], loss=96.9574
	step [224/244], loss=92.6795
	step [225/244], loss=96.3404
	step [226/244], loss=92.5726
	step [227/244], loss=119.1989
	step [228/244], loss=114.5431
	step [229/244], loss=112.4850
	step [230/244], loss=105.6482
	step [231/244], loss=108.4808
	step [232/244], loss=108.4879
	step [233/244], loss=116.0764
	step [234/244], loss=93.9924
	step [235/244], loss=103.6833
	step [236/244], loss=88.4881
	step [237/244], loss=90.1101
	step [238/244], loss=104.0265
	step [239/244], loss=133.9394
	step [240/244], loss=83.8757
	step [241/244], loss=100.5905
	step [242/244], loss=100.7029
	step [243/244], loss=106.2833
	step [244/244], loss=5.6901
	Evaluating
	loss=0.0123, precision=0.3173, recall=0.9120, f1=0.4708
Training epoch 21
	step [1/244], loss=122.7807
	step [2/244], loss=97.6345
	step [3/244], loss=88.1554
	step [4/244], loss=99.8936
	step [5/244], loss=72.5012
	step [6/244], loss=98.6065
	step [7/244], loss=89.6336
	step [8/244], loss=80.3694
	step [9/244], loss=111.2367
	step [10/244], loss=90.5403
	step [11/244], loss=107.2568
	step [12/244], loss=99.5484
	step [13/244], loss=110.4384
	step [14/244], loss=100.0829
	step [15/244], loss=103.6743
	step [16/244], loss=100.5394
	step [17/244], loss=89.6143
	step [18/244], loss=106.4033
	step [19/244], loss=123.1636
	step [20/244], loss=90.7337
	step [21/244], loss=108.0578
	step [22/244], loss=94.6653
	step [23/244], loss=88.9277
	step [24/244], loss=87.7556
	step [25/244], loss=99.6144
	step [26/244], loss=106.0134
	step [27/244], loss=115.8546
	step [28/244], loss=91.7673
	step [29/244], loss=110.0875
	step [30/244], loss=93.4018
	step [31/244], loss=99.2742
	step [32/244], loss=88.4400
	step [33/244], loss=93.9339
	step [34/244], loss=97.8254
	step [35/244], loss=99.9116
	step [36/244], loss=86.0677
	step [37/244], loss=92.5053
	step [38/244], loss=86.4429
	step [39/244], loss=118.7524
	step [40/244], loss=103.4244
	step [41/244], loss=95.6782
	step [42/244], loss=113.2154
	step [43/244], loss=99.2680
	step [44/244], loss=108.6478
	step [45/244], loss=84.9868
	step [46/244], loss=97.0983
	step [47/244], loss=108.6432
	step [48/244], loss=90.4841
	step [49/244], loss=98.1988
	step [50/244], loss=108.3362
	step [51/244], loss=117.7665
	step [52/244], loss=114.1676
	step [53/244], loss=105.3121
	step [54/244], loss=98.7443
	step [55/244], loss=101.5599
	step [56/244], loss=102.7079
	step [57/244], loss=101.6268
	step [58/244], loss=101.7301
	step [59/244], loss=79.3152
	step [60/244], loss=109.2101
	step [61/244], loss=86.6860
	step [62/244], loss=89.7252
	step [63/244], loss=107.4051
	step [64/244], loss=107.7224
	step [65/244], loss=91.8460
	step [66/244], loss=93.5110
	step [67/244], loss=95.1650
	step [68/244], loss=117.4131
	step [69/244], loss=125.4603
	step [70/244], loss=103.4778
	step [71/244], loss=93.9567
	step [72/244], loss=110.1444
	step [73/244], loss=89.0801
	step [74/244], loss=104.8014
	step [75/244], loss=89.6362
	step [76/244], loss=112.4684
	step [77/244], loss=96.5167
	step [78/244], loss=85.3527
	step [79/244], loss=99.8033
	step [80/244], loss=98.5988
	step [81/244], loss=88.9882
	step [82/244], loss=133.7773
	step [83/244], loss=98.8934
	step [84/244], loss=113.4838
	step [85/244], loss=101.6707
	step [86/244], loss=94.7961
	step [87/244], loss=96.6069
	step [88/244], loss=93.4981
	step [89/244], loss=96.7084
	step [90/244], loss=111.1966
	step [91/244], loss=109.3017
	step [92/244], loss=102.8466
	step [93/244], loss=86.0882
	step [94/244], loss=92.5479
	step [95/244], loss=119.0706
	step [96/244], loss=89.5251
	step [97/244], loss=79.2934
	step [98/244], loss=84.9448
	step [99/244], loss=97.5845
	step [100/244], loss=95.6738
	step [101/244], loss=109.3458
	step [102/244], loss=99.8328
	step [103/244], loss=104.2352
	step [104/244], loss=86.2637
	step [105/244], loss=102.6009
	step [106/244], loss=94.7788
	step [107/244], loss=118.0134
	step [108/244], loss=99.8791
	step [109/244], loss=103.3925
	step [110/244], loss=118.2434
	step [111/244], loss=96.6037
	step [112/244], loss=100.7752
	step [113/244], loss=94.1879
	step [114/244], loss=103.1744
	step [115/244], loss=97.6569
	step [116/244], loss=82.0190
	step [117/244], loss=87.2424
	step [118/244], loss=86.6758
	step [119/244], loss=81.0048
	step [120/244], loss=103.9302
	step [121/244], loss=102.8636
	step [122/244], loss=84.7586
	step [123/244], loss=98.9948
	step [124/244], loss=107.2713
	step [125/244], loss=84.7585
	step [126/244], loss=88.0885
	step [127/244], loss=108.9853
	step [128/244], loss=109.3934
	step [129/244], loss=103.5371
	step [130/244], loss=96.5546
	step [131/244], loss=117.4120
	step [132/244], loss=101.8724
	step [133/244], loss=102.8722
	step [134/244], loss=89.0573
	step [135/244], loss=111.0043
	step [136/244], loss=86.2570
	step [137/244], loss=101.1270
	step [138/244], loss=93.9353
	step [139/244], loss=84.9408
	step [140/244], loss=81.6166
	step [141/244], loss=77.7706
	step [142/244], loss=102.3167
	step [143/244], loss=105.0081
	step [144/244], loss=109.9603
	step [145/244], loss=121.2600
	step [146/244], loss=78.1679
	step [147/244], loss=99.9413
	step [148/244], loss=97.9875
	step [149/244], loss=130.8489
	step [150/244], loss=102.1399
	step [151/244], loss=104.7976
	step [152/244], loss=93.1451
	step [153/244], loss=102.5234
	step [154/244], loss=76.8148
	step [155/244], loss=80.7551
	step [156/244], loss=98.9589
	step [157/244], loss=103.8891
	step [158/244], loss=87.1053
	step [159/244], loss=85.3641
	step [160/244], loss=92.6600
	step [161/244], loss=92.0316
	step [162/244], loss=111.5476
	step [163/244], loss=109.0601
	step [164/244], loss=100.6677
	step [165/244], loss=101.3589
	step [166/244], loss=112.6075
	step [167/244], loss=90.4899
	step [168/244], loss=91.8306
	step [169/244], loss=93.3567
	step [170/244], loss=84.6754
	step [171/244], loss=101.0890
	step [172/244], loss=87.1385
	step [173/244], loss=91.2461
	step [174/244], loss=102.9086
	step [175/244], loss=85.6172
	step [176/244], loss=91.5042
	step [177/244], loss=107.0334
	step [178/244], loss=87.9716
	step [179/244], loss=99.0190
	step [180/244], loss=97.8096
	step [181/244], loss=99.6156
	step [182/244], loss=90.1428
	step [183/244], loss=98.6948
	step [184/244], loss=100.2666
	step [185/244], loss=99.7923
	step [186/244], loss=94.7746
	step [187/244], loss=102.9092
	step [188/244], loss=91.0043
	step [189/244], loss=99.8932
	step [190/244], loss=82.4154
	step [191/244], loss=90.1615
	step [192/244], loss=111.7238
	step [193/244], loss=97.9540
	step [194/244], loss=105.1124
	step [195/244], loss=103.8165
	step [196/244], loss=97.6649
	step [197/244], loss=125.6808
	step [198/244], loss=101.4631
	step [199/244], loss=83.2618
	step [200/244], loss=95.9183
	step [201/244], loss=108.1501
	step [202/244], loss=109.4633
	step [203/244], loss=79.5937
	step [204/244], loss=88.4559
	step [205/244], loss=90.7609
	step [206/244], loss=106.2062
	step [207/244], loss=81.2215
	step [208/244], loss=86.9680
	step [209/244], loss=97.4285
	step [210/244], loss=92.7617
	step [211/244], loss=85.5249
	step [212/244], loss=102.0441
	step [213/244], loss=108.6918
	step [214/244], loss=98.8065
	step [215/244], loss=95.9424
	step [216/244], loss=97.2909
	step [217/244], loss=91.2758
	step [218/244], loss=104.4246
	step [219/244], loss=103.1523
	step [220/244], loss=83.4387
	step [221/244], loss=109.5294
	step [222/244], loss=101.4253
	step [223/244], loss=95.6724
	step [224/244], loss=109.7043
	step [225/244], loss=108.3937
	step [226/244], loss=92.3318
	step [227/244], loss=95.6807
	step [228/244], loss=99.2998
	step [229/244], loss=97.3695
	step [230/244], loss=78.8992
	step [231/244], loss=116.3783
	step [232/244], loss=104.0203
	step [233/244], loss=75.7370
	step [234/244], loss=90.0650
	step [235/244], loss=102.8334
	step [236/244], loss=107.4158
	step [237/244], loss=99.7122
	step [238/244], loss=97.8492
	step [239/244], loss=116.6453
	step [240/244], loss=101.5562
	step [241/244], loss=91.8358
	step [242/244], loss=92.3814
	step [243/244], loss=97.8819
	step [244/244], loss=1.1249
	Evaluating
	loss=0.0137, precision=0.2836, recall=0.9057, f1=0.4320
Training epoch 22
	step [1/244], loss=115.1473
	step [2/244], loss=105.8832
	step [3/244], loss=110.1724
	step [4/244], loss=110.1088
	step [5/244], loss=92.6330
	step [6/244], loss=98.5008
	step [7/244], loss=87.3409
	step [8/244], loss=106.8827
	step [9/244], loss=87.3424
	step [10/244], loss=95.0724
	step [11/244], loss=114.9361
	step [12/244], loss=100.7079
	step [13/244], loss=85.4461
	step [14/244], loss=82.2514
	step [15/244], loss=83.5039
	step [16/244], loss=98.7415
	step [17/244], loss=102.2099
	step [18/244], loss=86.5970
	step [19/244], loss=126.0409
	step [20/244], loss=94.6335
	step [21/244], loss=113.0664
	step [22/244], loss=98.0870
	step [23/244], loss=107.9483
	step [24/244], loss=98.2959
	step [25/244], loss=88.9955
	step [26/244], loss=103.7975
	step [27/244], loss=122.5755
	step [28/244], loss=98.0344
	step [29/244], loss=96.5656
	step [30/244], loss=126.2971
	step [31/244], loss=79.7312
	step [32/244], loss=96.0093
	step [33/244], loss=99.4982
	step [34/244], loss=97.4246
	step [35/244], loss=84.5243
	step [36/244], loss=105.7193
	step [37/244], loss=90.2882
	step [38/244], loss=102.5105
	step [39/244], loss=88.2671
	step [40/244], loss=112.7620
	step [41/244], loss=126.4493
	step [42/244], loss=111.3489
	step [43/244], loss=106.5292
	step [44/244], loss=98.5658
	step [45/244], loss=92.4975
	step [46/244], loss=98.3826
	step [47/244], loss=81.5192
	step [48/244], loss=104.4472
	step [49/244], loss=88.9165
	step [50/244], loss=91.6196
	step [51/244], loss=90.2137
	step [52/244], loss=109.7347
	step [53/244], loss=94.9763
	step [54/244], loss=95.0364
	step [55/244], loss=120.1539
	step [56/244], loss=104.0432
	step [57/244], loss=97.9000
	step [58/244], loss=108.5230
	step [59/244], loss=90.9116
	step [60/244], loss=109.3716
	step [61/244], loss=109.9669
	step [62/244], loss=109.5108
	step [63/244], loss=110.3547
	step [64/244], loss=92.9599
	step [65/244], loss=93.9654
	step [66/244], loss=75.7700
	step [67/244], loss=90.9033
	step [68/244], loss=92.7532
	step [69/244], loss=96.5846
	step [70/244], loss=88.3230
	step [71/244], loss=116.9845
	step [72/244], loss=100.3309
	step [73/244], loss=86.0846
	step [74/244], loss=91.2054
	step [75/244], loss=94.1585
	step [76/244], loss=79.0639
	step [77/244], loss=95.5128
	step [78/244], loss=107.1552
	step [79/244], loss=116.4594
	step [80/244], loss=86.1903
	step [81/244], loss=93.7377
	step [82/244], loss=101.7579
	step [83/244], loss=103.1088
	step [84/244], loss=85.6781
	step [85/244], loss=82.4059
	step [86/244], loss=92.7516
	step [87/244], loss=83.9664
	step [88/244], loss=84.4913
	step [89/244], loss=86.9292
	step [90/244], loss=117.1555
	step [91/244], loss=82.7949
	step [92/244], loss=98.1943
	step [93/244], loss=100.3054
	step [94/244], loss=94.4964
	step [95/244], loss=113.5903
	step [96/244], loss=103.2116
	step [97/244], loss=87.9340
	step [98/244], loss=98.6986
	step [99/244], loss=95.9184
	step [100/244], loss=100.0333
	step [101/244], loss=110.6983
	step [102/244], loss=103.7132
	step [103/244], loss=78.7722
	step [104/244], loss=106.6766
	step [105/244], loss=109.2678
	step [106/244], loss=95.7121
	step [107/244], loss=95.3259
	step [108/244], loss=95.7711
	step [109/244], loss=87.1887
	step [110/244], loss=97.3846
	step [111/244], loss=90.6403
	step [112/244], loss=94.6631
	step [113/244], loss=94.2465
	step [114/244], loss=76.7571
	step [115/244], loss=73.3004
	step [116/244], loss=94.9780
	step [117/244], loss=105.8013
	step [118/244], loss=96.0020
	step [119/244], loss=118.1887
	step [120/244], loss=110.5888
	step [121/244], loss=85.4722
	step [122/244], loss=102.0058
	step [123/244], loss=106.5864
	step [124/244], loss=86.5040
	step [125/244], loss=89.5304
	step [126/244], loss=97.4163
	step [127/244], loss=96.6892
	step [128/244], loss=84.3193
	step [129/244], loss=94.9393
	step [130/244], loss=92.7608
	step [131/244], loss=75.7529
	step [132/244], loss=97.9885
	step [133/244], loss=84.7927
	step [134/244], loss=99.8692
	step [135/244], loss=87.0726
	step [136/244], loss=115.9782
	step [137/244], loss=108.9878
	step [138/244], loss=96.8964
	step [139/244], loss=94.9806
	step [140/244], loss=95.9778
	step [141/244], loss=97.9905
	step [142/244], loss=118.6888
	step [143/244], loss=95.8325
	step [144/244], loss=103.9244
	step [145/244], loss=112.5888
	step [146/244], loss=99.6136
	step [147/244], loss=89.4136
	step [148/244], loss=80.8143
	step [149/244], loss=99.2298
	step [150/244], loss=106.3283
	step [151/244], loss=99.4828
	step [152/244], loss=115.0681
	step [153/244], loss=118.4506
	step [154/244], loss=112.1209
	step [155/244], loss=94.8307
	step [156/244], loss=97.5620
	step [157/244], loss=99.9611
	step [158/244], loss=89.5366
	step [159/244], loss=116.8850
	step [160/244], loss=104.4785
	step [161/244], loss=95.1678
	step [162/244], loss=82.9214
	step [163/244], loss=95.3231
	step [164/244], loss=106.7397
	step [165/244], loss=104.7307
	step [166/244], loss=123.1109
	step [167/244], loss=97.7812
	step [168/244], loss=89.1779
	step [169/244], loss=94.2465
	step [170/244], loss=98.0875
	step [171/244], loss=114.9190
	step [172/244], loss=94.0296
	step [173/244], loss=109.3955
	step [174/244], loss=112.1718
	step [175/244], loss=99.3745
	step [176/244], loss=92.3597
	step [177/244], loss=85.9228
	step [178/244], loss=84.4248
	step [179/244], loss=107.7947
	step [180/244], loss=116.6877
	step [181/244], loss=94.4285
	step [182/244], loss=103.4867
	step [183/244], loss=87.7455
	step [184/244], loss=95.9240
	step [185/244], loss=101.0237
	step [186/244], loss=103.0780
	step [187/244], loss=99.8604
	step [188/244], loss=96.3686
	step [189/244], loss=88.6720
	step [190/244], loss=95.6143
	step [191/244], loss=97.1392
	step [192/244], loss=117.1704
	step [193/244], loss=99.7044
	step [194/244], loss=93.5967
	step [195/244], loss=91.7477
	step [196/244], loss=83.4175
	step [197/244], loss=118.4633
	step [198/244], loss=76.9507
	step [199/244], loss=89.8637
	step [200/244], loss=79.0697
	step [201/244], loss=91.8316
	step [202/244], loss=118.8977
	step [203/244], loss=101.2466
	step [204/244], loss=90.5334
	step [205/244], loss=94.4859
	step [206/244], loss=78.7521
	step [207/244], loss=88.3625
	step [208/244], loss=84.7442
	step [209/244], loss=107.9650
	step [210/244], loss=85.2300
	step [211/244], loss=103.1802
	step [212/244], loss=100.1620
	step [213/244], loss=103.3248
	step [214/244], loss=84.1365
	step [215/244], loss=90.4218
	step [216/244], loss=96.7928
	step [217/244], loss=96.9464
	step [218/244], loss=101.1549
	step [219/244], loss=117.9639
	step [220/244], loss=80.0225
	step [221/244], loss=95.9487
	step [222/244], loss=77.3532
	step [223/244], loss=80.4833
	step [224/244], loss=90.4361
	step [225/244], loss=82.7448
	step [226/244], loss=95.2367
	step [227/244], loss=96.7240
	step [228/244], loss=84.4237
	step [229/244], loss=94.9486
	step [230/244], loss=93.2865
	step [231/244], loss=87.8756
	step [232/244], loss=90.7158
	step [233/244], loss=88.1484
	step [234/244], loss=88.3489
	step [235/244], loss=107.6153
	step [236/244], loss=112.1103
	step [237/244], loss=94.8307
	step [238/244], loss=107.9179
	step [239/244], loss=94.7513
	step [240/244], loss=96.2337
	step [241/244], loss=67.5213
	step [242/244], loss=76.6839
	step [243/244], loss=110.2498
	step [244/244], loss=3.0606
	Evaluating
	loss=0.0131, precision=0.3033, recall=0.9026, f1=0.4540
Training epoch 23
	step [1/244], loss=99.4500
	step [2/244], loss=74.6244
	step [3/244], loss=97.2372
	step [4/244], loss=108.9453
	step [5/244], loss=134.0537
	step [6/244], loss=97.5015
	step [7/244], loss=102.4244
	step [8/244], loss=94.6914
	step [9/244], loss=99.4576
	step [10/244], loss=76.3752
	step [11/244], loss=108.1454
	step [12/244], loss=100.9052
	step [13/244], loss=88.3809
	step [14/244], loss=83.7344
	step [15/244], loss=91.9311
	step [16/244], loss=87.0849
	step [17/244], loss=100.2037
	step [18/244], loss=89.0153
	step [19/244], loss=96.4355
	step [20/244], loss=96.1038
	step [21/244], loss=92.8933
	step [22/244], loss=104.0971
	step [23/244], loss=107.4753
	step [24/244], loss=106.3602
	step [25/244], loss=99.8949
	step [26/244], loss=91.5422
	step [27/244], loss=105.1378
	step [28/244], loss=97.2578
	step [29/244], loss=103.5516
	step [30/244], loss=95.9393
	step [31/244], loss=93.9298
	step [32/244], loss=111.2583
	step [33/244], loss=107.4940
	step [34/244], loss=96.9222
	step [35/244], loss=91.8485
	step [36/244], loss=96.2856
	step [37/244], loss=96.5814
	step [38/244], loss=83.1653
	step [39/244], loss=106.7713
	step [40/244], loss=99.3342
	step [41/244], loss=89.0662
	step [42/244], loss=105.0616
	step [43/244], loss=94.7630
	step [44/244], loss=108.1809
	step [45/244], loss=95.3093
	step [46/244], loss=106.4428
	step [47/244], loss=85.9193
	step [48/244], loss=104.0960
	step [49/244], loss=89.9272
	step [50/244], loss=103.3526
	step [51/244], loss=71.8893
	step [52/244], loss=105.9092
	step [53/244], loss=85.7608
	step [54/244], loss=76.8522
	step [55/244], loss=99.5507
	step [56/244], loss=91.8439
	step [57/244], loss=101.7418
	step [58/244], loss=108.4295
	step [59/244], loss=85.5732
	step [60/244], loss=100.9983
	step [61/244], loss=120.6521
	step [62/244], loss=94.7667
	step [63/244], loss=100.4001
	step [64/244], loss=89.7562
	step [65/244], loss=88.0482
	step [66/244], loss=93.8118
	step [67/244], loss=102.2126
	step [68/244], loss=113.3480
	step [69/244], loss=94.9146
	step [70/244], loss=120.6423
	step [71/244], loss=87.5216
	step [72/244], loss=83.5082
	step [73/244], loss=88.4234
	step [74/244], loss=104.4404
	step [75/244], loss=90.4901
	step [76/244], loss=80.7720
	step [77/244], loss=108.7920
	step [78/244], loss=88.1207
	step [79/244], loss=65.6330
	step [80/244], loss=107.5195
	step [81/244], loss=93.8525
	step [82/244], loss=83.5774
	step [83/244], loss=73.7196
	step [84/244], loss=113.9163
	step [85/244], loss=96.9792
	step [86/244], loss=100.3815
	step [87/244], loss=91.6963
	step [88/244], loss=103.2118
	step [89/244], loss=118.0306
	step [90/244], loss=95.5870
	step [91/244], loss=89.4376
	step [92/244], loss=84.5909
	step [93/244], loss=99.5903
	step [94/244], loss=87.7239
	step [95/244], loss=118.9980
	step [96/244], loss=88.4292
	step [97/244], loss=78.6246
	step [98/244], loss=108.5625
	step [99/244], loss=85.5874
	step [100/244], loss=103.6992
	step [101/244], loss=85.9303
	step [102/244], loss=97.2757
	step [103/244], loss=88.8334
	step [104/244], loss=88.2570
	step [105/244], loss=94.0482
	step [106/244], loss=89.8355
	step [107/244], loss=75.6824
	step [108/244], loss=95.4250
	step [109/244], loss=101.4965
	step [110/244], loss=90.5748
	step [111/244], loss=98.9352
	step [112/244], loss=80.8975
	step [113/244], loss=104.0977
	step [114/244], loss=108.4351
	step [115/244], loss=108.7310
	step [116/244], loss=80.1540
	step [117/244], loss=87.0741
	step [118/244], loss=85.9426
	step [119/244], loss=91.6320
	step [120/244], loss=92.0055
	step [121/244], loss=97.8318
	step [122/244], loss=103.3366
	step [123/244], loss=90.3261
	step [124/244], loss=98.1671
	step [125/244], loss=89.5952
	step [126/244], loss=89.0937
	step [127/244], loss=112.9266
	step [128/244], loss=95.8850
	step [129/244], loss=88.5939
	step [130/244], loss=100.2841
	step [131/244], loss=95.4906
	step [132/244], loss=99.7225
	step [133/244], loss=82.0724
	step [134/244], loss=99.6364
	step [135/244], loss=104.9468
	step [136/244], loss=107.6022
	step [137/244], loss=103.5955
	step [138/244], loss=100.6498
	step [139/244], loss=99.2815
	step [140/244], loss=103.9775
	step [141/244], loss=101.0010
	step [142/244], loss=84.8234
	step [143/244], loss=95.6760
	step [144/244], loss=103.9320
	step [145/244], loss=96.6699
	step [146/244], loss=75.9899
	step [147/244], loss=100.0270
	step [148/244], loss=99.1794
	step [149/244], loss=75.5627
	step [150/244], loss=90.4176
	step [151/244], loss=90.3100
	step [152/244], loss=82.1003
	step [153/244], loss=88.9688
	step [154/244], loss=79.3944
	step [155/244], loss=72.4987
	step [156/244], loss=104.4304
	step [157/244], loss=109.0446
	step [158/244], loss=86.7439
	step [159/244], loss=99.8523
	step [160/244], loss=96.0358
	step [161/244], loss=94.3336
	step [162/244], loss=86.2590
	step [163/244], loss=85.7453
	step [164/244], loss=94.1215
	step [165/244], loss=118.1477
	step [166/244], loss=117.0153
	step [167/244], loss=116.6807
	step [168/244], loss=97.4104
	step [169/244], loss=83.6927
	step [170/244], loss=105.1223
	step [171/244], loss=73.5759
	step [172/244], loss=89.7703
	step [173/244], loss=72.5041
	step [174/244], loss=95.8740
	step [175/244], loss=106.1463
	step [176/244], loss=111.0514
	step [177/244], loss=113.6749
	step [178/244], loss=97.7870
	step [179/244], loss=73.6225
	step [180/244], loss=106.6442
	step [181/244], loss=98.7335
	step [182/244], loss=82.6384
	step [183/244], loss=87.8318
	step [184/244], loss=90.0674
	step [185/244], loss=85.4490
	step [186/244], loss=91.0514
	step [187/244], loss=88.1085
	step [188/244], loss=104.9972
	step [189/244], loss=97.8253
	step [190/244], loss=85.3852
	step [191/244], loss=85.9284
	step [192/244], loss=100.1133
	step [193/244], loss=111.1196
	step [194/244], loss=91.8634
	step [195/244], loss=111.4468
	step [196/244], loss=103.4217
	step [197/244], loss=100.4570
	step [198/244], loss=103.5202
	step [199/244], loss=78.2905
	step [200/244], loss=103.4343
	step [201/244], loss=84.4924
	step [202/244], loss=110.8271
	step [203/244], loss=102.3297
	step [204/244], loss=107.9395
	step [205/244], loss=90.0973
	step [206/244], loss=97.4338
	step [207/244], loss=112.9389
	step [208/244], loss=95.1863
	step [209/244], loss=105.3335
	step [210/244], loss=89.1784
	step [211/244], loss=91.0629
	step [212/244], loss=100.8880
	step [213/244], loss=79.4959
	step [214/244], loss=115.6467
	step [215/244], loss=91.0944
	step [216/244], loss=105.7776
	step [217/244], loss=97.2739
	step [218/244], loss=94.9731
	step [219/244], loss=95.1887
	step [220/244], loss=90.1470
	step [221/244], loss=85.7110
	step [222/244], loss=99.8536
	step [223/244], loss=100.4750
	step [224/244], loss=93.3862
	step [225/244], loss=94.6475
	step [226/244], loss=91.6894
	step [227/244], loss=104.9756
	step [228/244], loss=87.9260
	step [229/244], loss=111.3197
	step [230/244], loss=87.9361
	step [231/244], loss=85.1190
	step [232/244], loss=88.7881
	step [233/244], loss=86.8027
	step [234/244], loss=99.4946
	step [235/244], loss=87.6753
	step [236/244], loss=104.7863
	step [237/244], loss=105.1677
	step [238/244], loss=98.8031
	step [239/244], loss=100.5398
	step [240/244], loss=103.4004
	step [241/244], loss=101.3934
	step [242/244], loss=108.8928
	step [243/244], loss=96.0438
	step [244/244], loss=3.4271
	Evaluating
	loss=0.0120, precision=0.3381, recall=0.9176, f1=0.4941
Training epoch 24
	step [1/244], loss=96.4209
	step [2/244], loss=91.7403
	step [3/244], loss=105.0013
	step [4/244], loss=117.2295
	step [5/244], loss=77.9328
	step [6/244], loss=102.5805
	step [7/244], loss=83.1617
	step [8/244], loss=102.9424
	step [9/244], loss=98.6899
	step [10/244], loss=91.8852
	step [11/244], loss=89.5150
	step [12/244], loss=101.9277
	step [13/244], loss=87.0591
	step [14/244], loss=93.9500
	step [15/244], loss=123.6649
	step [16/244], loss=96.1352
	step [17/244], loss=93.4825
	step [18/244], loss=92.9500
	step [19/244], loss=96.7760
	step [20/244], loss=86.4794
	step [21/244], loss=106.5418
	step [22/244], loss=109.7104
	step [23/244], loss=85.9849
	step [24/244], loss=94.5640
	step [25/244], loss=83.7489
	step [26/244], loss=93.1625
	step [27/244], loss=114.7580
	step [28/244], loss=101.9469
	step [29/244], loss=88.4133
	step [30/244], loss=88.2639
	step [31/244], loss=97.1482
	step [32/244], loss=103.2184
	step [33/244], loss=71.7841
	step [34/244], loss=114.6439
	step [35/244], loss=122.6004
	step [36/244], loss=96.6087
	step [37/244], loss=90.7762
	step [38/244], loss=87.6873
	step [39/244], loss=77.6529
	step [40/244], loss=85.4454
	step [41/244], loss=78.5061
	step [42/244], loss=83.7746
	step [43/244], loss=118.7004
	step [44/244], loss=93.0641
	step [45/244], loss=93.9585
	step [46/244], loss=90.3385
	step [47/244], loss=106.8333
	step [48/244], loss=86.6849
	step [49/244], loss=103.3942
	step [50/244], loss=86.0368
	step [51/244], loss=117.0275
	step [52/244], loss=113.4317
	step [53/244], loss=97.4426
	step [54/244], loss=96.1321
	step [55/244], loss=108.2281
	step [56/244], loss=75.7606
	step [57/244], loss=93.3039
	step [58/244], loss=81.6173
	step [59/244], loss=118.9140
	step [60/244], loss=106.4366
	step [61/244], loss=96.3414
	step [62/244], loss=83.8315
	step [63/244], loss=104.9417
	step [64/244], loss=86.4230
	step [65/244], loss=96.0143
	step [66/244], loss=97.3099
	step [67/244], loss=98.2250
	step [68/244], loss=88.4660
	step [69/244], loss=91.3765
	step [70/244], loss=93.1767
	step [71/244], loss=88.7313
	step [72/244], loss=91.1861
	step [73/244], loss=89.5136
	step [74/244], loss=90.5786
	step [75/244], loss=80.5109
	step [76/244], loss=81.7278
	step [77/244], loss=94.2434
	step [78/244], loss=90.4476
	step [79/244], loss=95.7553
	step [80/244], loss=97.0607
	step [81/244], loss=106.5940
	step [82/244], loss=102.5005
	step [83/244], loss=96.6835
	step [84/244], loss=92.8520
	step [85/244], loss=100.7622
	step [86/244], loss=105.8343
	step [87/244], loss=100.3256
	step [88/244], loss=93.8854
	step [89/244], loss=126.5721
	step [90/244], loss=104.2724
	step [91/244], loss=100.2227
	step [92/244], loss=99.7921
	step [93/244], loss=81.2363
	step [94/244], loss=102.1135
	step [95/244], loss=92.2352
	step [96/244], loss=96.0114
	step [97/244], loss=88.1814
	step [98/244], loss=104.6700
	step [99/244], loss=87.1135
	step [100/244], loss=97.6442
	step [101/244], loss=103.3362
	step [102/244], loss=91.9828
	step [103/244], loss=89.4110
	step [104/244], loss=99.1833
	step [105/244], loss=89.6235
	step [106/244], loss=109.0557
	step [107/244], loss=85.7426
	step [108/244], loss=104.8586
	step [109/244], loss=99.1060
	step [110/244], loss=92.3870
	step [111/244], loss=83.0110
	step [112/244], loss=82.4702
	step [113/244], loss=89.3778
	step [114/244], loss=89.0390
	step [115/244], loss=87.8084
	step [116/244], loss=99.1356
	step [117/244], loss=101.4929
	step [118/244], loss=92.2519
	step [119/244], loss=76.0535
	step [120/244], loss=115.6169
	step [121/244], loss=110.7827
	step [122/244], loss=93.4158
	step [123/244], loss=99.9365
	step [124/244], loss=95.2225
	step [125/244], loss=94.8428
	step [126/244], loss=93.4712
	step [127/244], loss=107.2945
	step [128/244], loss=67.4470
	step [129/244], loss=108.4283
	step [130/244], loss=96.1999
	step [131/244], loss=79.8334
	step [132/244], loss=83.4903
	step [133/244], loss=101.0667
	step [134/244], loss=100.9655
	step [135/244], loss=78.9303
	step [136/244], loss=88.5056
	step [137/244], loss=69.5172
	step [138/244], loss=90.8568
	step [139/244], loss=100.6343
	step [140/244], loss=97.8189
	step [141/244], loss=100.7045
	step [142/244], loss=97.8407
	step [143/244], loss=99.8915
	step [144/244], loss=75.1108
	step [145/244], loss=113.8015
	step [146/244], loss=81.6979
	step [147/244], loss=83.3604
	step [148/244], loss=85.4882
	step [149/244], loss=83.4354
	step [150/244], loss=92.0369
	step [151/244], loss=101.7978
	step [152/244], loss=82.7302
	step [153/244], loss=109.6989
	step [154/244], loss=116.8183
	step [155/244], loss=106.1428
	step [156/244], loss=79.4913
	step [157/244], loss=91.6912
	step [158/244], loss=97.0861
	step [159/244], loss=86.7409
	step [160/244], loss=96.2869
	step [161/244], loss=97.0655
	step [162/244], loss=110.5949
	step [163/244], loss=92.9934
	step [164/244], loss=110.7934
	step [165/244], loss=90.7344
	step [166/244], loss=118.4460
	step [167/244], loss=99.5019
	step [168/244], loss=82.3222
	step [169/244], loss=115.8223
	step [170/244], loss=110.6395
	step [171/244], loss=79.9424
	step [172/244], loss=101.5870
	step [173/244], loss=99.8474
	step [174/244], loss=100.7909
	step [175/244], loss=102.8646
	step [176/244], loss=112.1497
	step [177/244], loss=89.7998
	step [178/244], loss=82.8134
	step [179/244], loss=117.5595
	step [180/244], loss=91.7302
	step [181/244], loss=101.6330
	step [182/244], loss=76.3934
	step [183/244], loss=104.0980
	step [184/244], loss=108.8234
	step [185/244], loss=82.2273
	step [186/244], loss=90.9916
	step [187/244], loss=94.9617
	step [188/244], loss=69.1176
	step [189/244], loss=91.0700
	step [190/244], loss=105.5226
	step [191/244], loss=85.9760
	step [192/244], loss=77.9611
	step [193/244], loss=102.4001
	step [194/244], loss=95.0846
	step [195/244], loss=105.1537
	step [196/244], loss=100.8031
	step [197/244], loss=101.1902
	step [198/244], loss=110.1107
	step [199/244], loss=93.5759
	step [200/244], loss=100.3124
	step [201/244], loss=93.5287
	step [202/244], loss=80.9956
	step [203/244], loss=88.7713
	step [204/244], loss=111.7332
	step [205/244], loss=90.1185
	step [206/244], loss=93.8990
	step [207/244], loss=98.3765
	step [208/244], loss=95.8247
	step [209/244], loss=89.5220
	step [210/244], loss=91.0259
	step [211/244], loss=83.2409
	step [212/244], loss=91.3103
	step [213/244], loss=93.0027
	step [214/244], loss=92.5600
	step [215/244], loss=93.1521
	step [216/244], loss=91.3668
	step [217/244], loss=90.8877
	step [218/244], loss=86.8640
	step [219/244], loss=82.0134
	step [220/244], loss=82.2133
	step [221/244], loss=77.8555
	step [222/244], loss=84.1918
	step [223/244], loss=102.6349
	step [224/244], loss=93.5666
	step [225/244], loss=76.3010
	step [226/244], loss=95.1827
	step [227/244], loss=80.8911
	step [228/244], loss=109.3595
	step [229/244], loss=92.2249
	step [230/244], loss=112.8853
	step [231/244], loss=99.7756
	step [232/244], loss=108.2684
	step [233/244], loss=85.2757
	step [234/244], loss=85.4184
	step [235/244], loss=96.2234
	step [236/244], loss=100.5464
	step [237/244], loss=87.0764
	step [238/244], loss=104.1957
	step [239/244], loss=92.8892
	step [240/244], loss=103.3115
	step [241/244], loss=100.0802
	step [242/244], loss=91.2607
	step [243/244], loss=86.4325
	step [244/244], loss=1.1166
	Evaluating
	loss=0.0141, precision=0.2570, recall=0.9065, f1=0.4005
Training epoch 25
	step [1/244], loss=82.4850
	step [2/244], loss=75.5251
	step [3/244], loss=104.9370
	step [4/244], loss=93.9699
	step [5/244], loss=89.1736
	step [6/244], loss=89.2675
	step [7/244], loss=98.6628
	step [8/244], loss=92.8883
	step [9/244], loss=102.4418
	step [10/244], loss=109.5992
	step [11/244], loss=96.2626
	step [12/244], loss=105.5339
	step [13/244], loss=105.2323
	step [14/244], loss=80.5738
	step [15/244], loss=86.4382
	step [16/244], loss=92.7009
	step [17/244], loss=93.8341
	step [18/244], loss=89.6383
	step [19/244], loss=93.1891
	step [20/244], loss=90.0182
	step [21/244], loss=93.6880
	step [22/244], loss=87.7401
	step [23/244], loss=87.3835
	step [24/244], loss=86.6034
	step [25/244], loss=94.4099
	step [26/244], loss=97.0382
	step [27/244], loss=88.8951
	step [28/244], loss=91.3265
	step [29/244], loss=82.0625
	step [30/244], loss=81.9834
	step [31/244], loss=98.5589
	step [32/244], loss=87.0278
	step [33/244], loss=118.9388
	step [34/244], loss=95.2976
	step [35/244], loss=107.2911
	step [36/244], loss=90.0080
	step [37/244], loss=92.5628
	step [38/244], loss=108.1942
	step [39/244], loss=76.7657
	step [40/244], loss=98.3194
	step [41/244], loss=100.2176
	step [42/244], loss=90.8686
	step [43/244], loss=104.3420
	step [44/244], loss=84.2124
	step [45/244], loss=98.2220
	step [46/244], loss=102.3975
	step [47/244], loss=130.8866
	step [48/244], loss=84.8358
	step [49/244], loss=93.0735
	step [50/244], loss=94.0808
	step [51/244], loss=94.5615
	step [52/244], loss=84.0731
	step [53/244], loss=94.8564
	step [54/244], loss=86.0886
	step [55/244], loss=84.0279
	step [56/244], loss=83.2200
	step [57/244], loss=84.9453
	step [58/244], loss=100.5515
	step [59/244], loss=90.7826
	step [60/244], loss=87.6578
	step [61/244], loss=94.2597
	step [62/244], loss=82.0378
	step [63/244], loss=94.1600
	step [64/244], loss=83.0413
	step [65/244], loss=109.3863
	step [66/244], loss=93.2205
	step [67/244], loss=99.5651
	step [68/244], loss=93.8889
	step [69/244], loss=117.0107
	step [70/244], loss=102.8631
	step [71/244], loss=105.5737
	step [72/244], loss=92.9325
	step [73/244], loss=97.7142
	step [74/244], loss=88.1698
	step [75/244], loss=100.4942
	step [76/244], loss=82.8352
	step [77/244], loss=88.5028
	step [78/244], loss=97.3361
	step [79/244], loss=91.0947
	step [80/244], loss=96.2231
	step [81/244], loss=81.3945
	step [82/244], loss=88.4711
	step [83/244], loss=93.0578
	step [84/244], loss=68.5430
	step [85/244], loss=98.0392
	step [86/244], loss=86.1708
	step [87/244], loss=72.3301
	step [88/244], loss=72.2026
	step [89/244], loss=89.4369
	step [90/244], loss=86.7181
	step [91/244], loss=90.3212
	step [92/244], loss=76.5064
	step [93/244], loss=82.1102
	step [94/244], loss=88.3437
	step [95/244], loss=93.7342
	step [96/244], loss=86.8861
	step [97/244], loss=85.4678
	step [98/244], loss=85.8326
	step [99/244], loss=100.2441
	step [100/244], loss=94.2302
	step [101/244], loss=84.3768
	step [102/244], loss=82.9749
	step [103/244], loss=88.2318
	step [104/244], loss=103.4055
	step [105/244], loss=88.4476
	step [106/244], loss=111.1715
	step [107/244], loss=90.4814
	step [108/244], loss=102.3793
	step [109/244], loss=94.9688
	step [110/244], loss=86.9625
	step [111/244], loss=86.4466
	step [112/244], loss=93.6131
	step [113/244], loss=93.3342
	step [114/244], loss=101.9846
	step [115/244], loss=94.3436
	step [116/244], loss=104.6455
	step [117/244], loss=88.7396
	step [118/244], loss=121.0180
	step [119/244], loss=103.1192
	step [120/244], loss=113.7863
	step [121/244], loss=74.1748
	step [122/244], loss=98.8018
	step [123/244], loss=85.6673
	step [124/244], loss=110.0015
	step [125/244], loss=120.2441
	step [126/244], loss=88.1654
	step [127/244], loss=89.2262
	step [128/244], loss=87.4523
	step [129/244], loss=97.9966
	step [130/244], loss=112.3817
	step [131/244], loss=79.6612
	step [132/244], loss=82.2543
	step [133/244], loss=80.7851
	step [134/244], loss=85.1784
	step [135/244], loss=91.2092
	step [136/244], loss=99.4041
	step [137/244], loss=89.5301
	step [138/244], loss=101.3802
	step [139/244], loss=100.8862
	step [140/244], loss=85.1314
	step [141/244], loss=105.2379
	step [142/244], loss=81.1053
	step [143/244], loss=95.4229
	step [144/244], loss=109.6082
	step [145/244], loss=108.7319
	step [146/244], loss=92.2211
	step [147/244], loss=105.2432
	step [148/244], loss=105.1035
	step [149/244], loss=86.9054
	step [150/244], loss=91.2730
	step [151/244], loss=95.3215
	step [152/244], loss=95.8272
	step [153/244], loss=99.1828
	step [154/244], loss=92.9337
	step [155/244], loss=115.2535
	step [156/244], loss=95.3441
	step [157/244], loss=87.0246
	step [158/244], loss=90.4352
	step [159/244], loss=84.2887
	step [160/244], loss=90.7653
	step [161/244], loss=105.5637
	step [162/244], loss=81.6313
	step [163/244], loss=86.8063
	step [164/244], loss=85.4054
	step [165/244], loss=110.9155
	step [166/244], loss=77.1159
	step [167/244], loss=102.5937
	step [168/244], loss=107.8263
	step [169/244], loss=76.5321
	step [170/244], loss=86.7470
	step [171/244], loss=101.8323
	step [172/244], loss=99.8079
	step [173/244], loss=84.1216
	step [174/244], loss=99.3463
	step [175/244], loss=87.1986
	step [176/244], loss=110.3039
	step [177/244], loss=67.8115
	step [178/244], loss=90.3954
	step [179/244], loss=127.2182
	step [180/244], loss=84.7415
	step [181/244], loss=76.3991
	step [182/244], loss=116.6945
	step [183/244], loss=104.9456
	step [184/244], loss=99.7940
	step [185/244], loss=89.7897
	step [186/244], loss=92.5495
	step [187/244], loss=80.9947
	step [188/244], loss=99.9597
	step [189/244], loss=105.2827
	step [190/244], loss=104.3196
	step [191/244], loss=104.8065
	step [192/244], loss=86.5874
	step [193/244], loss=93.8769
	step [194/244], loss=95.0174
	step [195/244], loss=120.5755
	step [196/244], loss=109.8869
	step [197/244], loss=87.5248
	step [198/244], loss=99.2265
	step [199/244], loss=89.9233
	step [200/244], loss=86.9809
	step [201/244], loss=75.1012
	step [202/244], loss=121.3251
	step [203/244], loss=89.4623
	step [204/244], loss=86.2291
	step [205/244], loss=99.2468
	step [206/244], loss=96.7302
	step [207/244], loss=91.5002
	step [208/244], loss=98.6969
	step [209/244], loss=102.8905
	step [210/244], loss=97.5679
	step [211/244], loss=106.3096
	step [212/244], loss=89.9236
	step [213/244], loss=100.1848
	step [214/244], loss=76.8984
	step [215/244], loss=99.0930
	step [216/244], loss=96.8661
	step [217/244], loss=104.3276
	step [218/244], loss=74.0186
	step [219/244], loss=88.4222
	step [220/244], loss=107.3910
	step [221/244], loss=81.8666
	step [222/244], loss=105.9372
	step [223/244], loss=105.6054
	step [224/244], loss=90.9690
	step [225/244], loss=90.2575
	step [226/244], loss=113.2687
	step [227/244], loss=111.3065
	step [228/244], loss=81.6939
	step [229/244], loss=101.6992
	step [230/244], loss=93.3451
	step [231/244], loss=96.0871
	step [232/244], loss=88.6432
	step [233/244], loss=98.5300
	step [234/244], loss=92.0739
	step [235/244], loss=83.4572
	step [236/244], loss=104.2344
	step [237/244], loss=88.3057
	step [238/244], loss=78.1543
	step [239/244], loss=99.1024
	step [240/244], loss=89.2775
	step [241/244], loss=94.1938
	step [242/244], loss=105.1752
	step [243/244], loss=79.3980
	step [244/244], loss=2.5442
	Evaluating
	loss=0.0113, precision=0.3078, recall=0.9008, f1=0.4588
Training epoch 26
	step [1/244], loss=89.2913
	step [2/244], loss=89.5251
	step [3/244], loss=88.8360
	step [4/244], loss=97.7660
	step [5/244], loss=83.8104
	step [6/244], loss=112.5135
	step [7/244], loss=102.0135
	step [8/244], loss=91.1366
	step [9/244], loss=89.9213
	step [10/244], loss=103.4915
	step [11/244], loss=102.3465
	step [12/244], loss=90.8316
	step [13/244], loss=74.0099
	step [14/244], loss=75.1279
	step [15/244], loss=95.3974
	step [16/244], loss=113.1918
	step [17/244], loss=111.0950
	step [18/244], loss=89.4691
	step [19/244], loss=93.3707
	step [20/244], loss=91.6967
	step [21/244], loss=119.0686
	step [22/244], loss=113.3867
	step [23/244], loss=82.0537
	step [24/244], loss=110.9317
	step [25/244], loss=95.0837
	step [26/244], loss=99.5322
	step [27/244], loss=85.5591
	step [28/244], loss=79.1090
	step [29/244], loss=92.0048
	step [30/244], loss=90.4500
	step [31/244], loss=97.0634
	step [32/244], loss=87.9149
	step [33/244], loss=84.5806
	step [34/244], loss=84.3942
	step [35/244], loss=108.7526
	step [36/244], loss=98.1124
	step [37/244], loss=67.9632
	step [38/244], loss=111.2998
	step [39/244], loss=78.7909
	step [40/244], loss=102.7644
	step [41/244], loss=104.3714
	step [42/244], loss=70.4964
	step [43/244], loss=102.8424
	step [44/244], loss=88.9530
	step [45/244], loss=82.9458
	step [46/244], loss=92.5838
	step [47/244], loss=99.7523
	step [48/244], loss=94.4842
	step [49/244], loss=108.8042
	step [50/244], loss=86.1061
	step [51/244], loss=107.1150
	step [52/244], loss=78.6344
	step [53/244], loss=75.0832
	step [54/244], loss=100.8758
	step [55/244], loss=93.9230
	step [56/244], loss=100.1535
	step [57/244], loss=84.9811
	step [58/244], loss=91.9695
	step [59/244], loss=97.1346
	step [60/244], loss=68.0574
	step [61/244], loss=101.9189
	step [62/244], loss=89.9028
	step [63/244], loss=86.8458
	step [64/244], loss=86.0235
	step [65/244], loss=98.0819
	step [66/244], loss=82.8988
	step [67/244], loss=97.4628
	step [68/244], loss=111.1776
	step [69/244], loss=74.5568
	step [70/244], loss=88.4199
	step [71/244], loss=89.6884
	step [72/244], loss=100.6515
	step [73/244], loss=88.7422
	step [74/244], loss=87.5569
	step [75/244], loss=116.8428
	step [76/244], loss=104.4858
	step [77/244], loss=86.7141
	step [78/244], loss=89.2811
	step [79/244], loss=105.3560
	step [80/244], loss=94.8519
	step [81/244], loss=104.4595
	step [82/244], loss=99.2123
	step [83/244], loss=82.4871
	step [84/244], loss=93.1234
	step [85/244], loss=104.8945
	step [86/244], loss=96.1774
	step [87/244], loss=104.1147
	step [88/244], loss=85.7460
	step [89/244], loss=102.2694
	step [90/244], loss=91.3519
	step [91/244], loss=107.7631
	step [92/244], loss=84.1483
	step [93/244], loss=83.3015
	step [94/244], loss=91.4101
	step [95/244], loss=109.0158
	step [96/244], loss=75.8773
	step [97/244], loss=94.6010
	step [98/244], loss=82.7525
	step [99/244], loss=82.7696
	step [100/244], loss=101.3686
	step [101/244], loss=79.0626
	step [102/244], loss=98.7855
	step [103/244], loss=80.7264
	step [104/244], loss=87.9939
	step [105/244], loss=93.9518
	step [106/244], loss=103.6130
	step [107/244], loss=91.5965
	step [108/244], loss=102.6967
	step [109/244], loss=81.5220
	step [110/244], loss=91.5352
	step [111/244], loss=90.2312
	step [112/244], loss=93.0631
	step [113/244], loss=94.2499
	step [114/244], loss=97.3431
	step [115/244], loss=85.9638
	step [116/244], loss=88.2595
	step [117/244], loss=92.5875
	step [118/244], loss=81.7180
	step [119/244], loss=81.3733
	step [120/244], loss=79.1335
	step [121/244], loss=82.2209
	step [122/244], loss=104.7183
	step [123/244], loss=94.7151
	step [124/244], loss=118.1671
	step [125/244], loss=68.1113
	step [126/244], loss=101.1876
	step [127/244], loss=95.9047
	step [128/244], loss=71.6236
	step [129/244], loss=105.4412
	step [130/244], loss=90.5561
	step [131/244], loss=97.6730
	step [132/244], loss=84.5959
	step [133/244], loss=100.0253
	step [134/244], loss=85.8520
	step [135/244], loss=112.8806
	step [136/244], loss=100.5757
	step [137/244], loss=111.3195
	step [138/244], loss=93.1743
	step [139/244], loss=92.5993
	step [140/244], loss=94.8824
	step [141/244], loss=89.0920
	step [142/244], loss=99.8205
	step [143/244], loss=109.2240
	step [144/244], loss=91.2825
	step [145/244], loss=102.7015
	step [146/244], loss=95.5308
	step [147/244], loss=95.1716
	step [148/244], loss=80.6415
	step [149/244], loss=94.7602
	step [150/244], loss=103.9718
	step [151/244], loss=117.6368
	step [152/244], loss=80.0138
	step [153/244], loss=113.0479
	step [154/244], loss=67.4315
	step [155/244], loss=109.1260
	step [156/244], loss=106.8624
	step [157/244], loss=90.6847
	step [158/244], loss=97.2211
	step [159/244], loss=90.7955
	step [160/244], loss=106.7578
	step [161/244], loss=98.1947
	step [162/244], loss=98.4704
	step [163/244], loss=81.7513
	step [164/244], loss=114.0184
	step [165/244], loss=90.8232
	step [166/244], loss=90.9383
	step [167/244], loss=86.6780
	step [168/244], loss=91.1798
	step [169/244], loss=69.5613
	step [170/244], loss=90.8336
	step [171/244], loss=77.7587
	step [172/244], loss=80.0097
	step [173/244], loss=86.5518
	step [174/244], loss=106.9831
	step [175/244], loss=95.0301
	step [176/244], loss=83.8315
	step [177/244], loss=80.4691
	step [178/244], loss=106.6359
	step [179/244], loss=107.8434
	step [180/244], loss=95.3792
	step [181/244], loss=92.0008
	step [182/244], loss=81.7017
	step [183/244], loss=124.3646
	step [184/244], loss=73.8470
	step [185/244], loss=89.4844
	step [186/244], loss=93.1440
	step [187/244], loss=90.7878
	step [188/244], loss=91.7082
	step [189/244], loss=73.6183
	step [190/244], loss=91.9696
	step [191/244], loss=82.6846
	step [192/244], loss=98.5109
	step [193/244], loss=98.8171
	step [194/244], loss=82.5254
	step [195/244], loss=83.1706
	step [196/244], loss=81.4191
	step [197/244], loss=96.6294
	step [198/244], loss=79.2033
	step [199/244], loss=83.5193
	step [200/244], loss=90.7253
	step [201/244], loss=93.2594
	step [202/244], loss=70.0679
	step [203/244], loss=98.6399
	step [204/244], loss=85.7620
	step [205/244], loss=112.9307
	step [206/244], loss=92.1136
	step [207/244], loss=97.4535
	step [208/244], loss=97.1137
	step [209/244], loss=92.2542
	step [210/244], loss=88.0808
	step [211/244], loss=106.5813
	step [212/244], loss=99.2849
	step [213/244], loss=93.5854
	step [214/244], loss=87.6715
	step [215/244], loss=92.3743
	step [216/244], loss=93.4350
	step [217/244], loss=68.0017
	step [218/244], loss=88.7375
	step [219/244], loss=85.7959
	step [220/244], loss=89.1387
	step [221/244], loss=79.9915
	step [222/244], loss=97.1164
	step [223/244], loss=78.1639
	step [224/244], loss=89.0409
	step [225/244], loss=94.4165
	step [226/244], loss=90.3375
	step [227/244], loss=99.8739
	step [228/244], loss=81.7815
	step [229/244], loss=100.7281
	step [230/244], loss=88.9925
	step [231/244], loss=81.9267
	step [232/244], loss=99.1941
	step [233/244], loss=106.7287
	step [234/244], loss=105.4752
	step [235/244], loss=102.3888
	step [236/244], loss=73.4270
	step [237/244], loss=85.7406
	step [238/244], loss=88.6827
	step [239/244], loss=104.2135
	step [240/244], loss=94.5827
	step [241/244], loss=85.5596
	step [242/244], loss=85.0648
	step [243/244], loss=105.8127
	step [244/244], loss=1.5461
	Evaluating
	loss=0.0106, precision=0.3601, recall=0.8908, f1=0.5129
Training epoch 27
	step [1/244], loss=112.0053
	step [2/244], loss=99.6195
	step [3/244], loss=102.7725
	step [4/244], loss=89.9761
	step [5/244], loss=92.6701
	step [6/244], loss=92.2850
	step [7/244], loss=93.3548
	step [8/244], loss=94.3288
	step [9/244], loss=84.0749
	step [10/244], loss=96.0273
	step [11/244], loss=99.2262
	step [12/244], loss=90.6499
	step [13/244], loss=88.6039
	step [14/244], loss=88.8284
	step [15/244], loss=78.8340
	step [16/244], loss=99.7543
	step [17/244], loss=83.0381
	step [18/244], loss=93.3349
	step [19/244], loss=84.0448
	step [20/244], loss=96.5311
	step [21/244], loss=99.6946
	step [22/244], loss=91.6051
	step [23/244], loss=87.5490
	step [24/244], loss=74.3655
	step [25/244], loss=109.3337
	step [26/244], loss=103.7180
	step [27/244], loss=89.1762
	step [28/244], loss=76.3564
	step [29/244], loss=69.3239
	step [30/244], loss=87.2731
	step [31/244], loss=98.3362
	step [32/244], loss=83.7860
	step [33/244], loss=67.5483
	step [34/244], loss=111.8696
	step [35/244], loss=108.8168
	step [36/244], loss=86.2304
	step [37/244], loss=94.2309
	step [38/244], loss=101.0652
	step [39/244], loss=103.5927
	step [40/244], loss=81.3643
	step [41/244], loss=78.4317
	step [42/244], loss=96.5800
	step [43/244], loss=91.6620
	step [44/244], loss=108.1911
	step [45/244], loss=68.0680
	step [46/244], loss=94.3615
	step [47/244], loss=109.6649
	step [48/244], loss=79.4032
	step [49/244], loss=82.5395
	step [50/244], loss=93.3461
	step [51/244], loss=83.0436
	step [52/244], loss=96.4517
	step [53/244], loss=67.5106
	step [54/244], loss=104.5024
	step [55/244], loss=94.0777
	step [56/244], loss=88.0773
	step [57/244], loss=79.5392
	step [58/244], loss=83.1239
	step [59/244], loss=94.3225
	step [60/244], loss=82.1705
	step [61/244], loss=108.0548
	step [62/244], loss=113.5600
	step [63/244], loss=103.1043
	step [64/244], loss=98.5662
	step [65/244], loss=89.1156
	step [66/244], loss=84.3463
	step [67/244], loss=88.1331
	step [68/244], loss=91.3651
	step [69/244], loss=80.7456
	step [70/244], loss=93.2063
	step [71/244], loss=102.3493
	step [72/244], loss=93.2398
	step [73/244], loss=100.6360
	step [74/244], loss=88.4539
	step [75/244], loss=121.3599
	step [76/244], loss=85.7761
	step [77/244], loss=91.4022
	step [78/244], loss=98.8668
	step [79/244], loss=94.9985
	step [80/244], loss=67.7696
	step [81/244], loss=111.9557
	step [82/244], loss=95.6538
	step [83/244], loss=94.2065
	step [84/244], loss=82.6055
	step [85/244], loss=85.5810
	step [86/244], loss=97.6480
	step [87/244], loss=100.2870
	step [88/244], loss=112.4403
	step [89/244], loss=105.9942
	step [90/244], loss=105.7517
	step [91/244], loss=115.0996
	step [92/244], loss=92.5662
	step [93/244], loss=103.2786
	step [94/244], loss=96.1255
	step [95/244], loss=86.6338
	step [96/244], loss=95.9094
	step [97/244], loss=85.1440
	step [98/244], loss=64.4159
	step [99/244], loss=96.9498
	step [100/244], loss=97.1951
	step [101/244], loss=98.9994
	step [102/244], loss=92.5271
	step [103/244], loss=71.4538
	step [104/244], loss=81.1649
	step [105/244], loss=78.7109
	step [106/244], loss=110.6321
	step [107/244], loss=100.2596
	step [108/244], loss=96.9625
	step [109/244], loss=89.2890
	step [110/244], loss=78.5960
	step [111/244], loss=100.8095
	step [112/244], loss=99.2986
	step [113/244], loss=101.1249
	step [114/244], loss=77.7463
	step [115/244], loss=92.8998
	step [116/244], loss=105.0820
	step [117/244], loss=87.7115
	step [118/244], loss=84.5228
	step [119/244], loss=90.9501
	step [120/244], loss=102.3389
	step [121/244], loss=97.5814
	step [122/244], loss=106.2306
	step [123/244], loss=99.8489
	step [124/244], loss=106.4919
	step [125/244], loss=96.1283
	step [126/244], loss=123.1993
	step [127/244], loss=87.0561
	step [128/244], loss=99.9008
	step [129/244], loss=94.1727
	step [130/244], loss=88.2085
	step [131/244], loss=96.3663
	step [132/244], loss=90.2707
	step [133/244], loss=85.6734
	step [134/244], loss=102.8551
	step [135/244], loss=92.6762
	step [136/244], loss=91.6212
	step [137/244], loss=88.3543
	step [138/244], loss=81.9435
	step [139/244], loss=112.1279
	step [140/244], loss=123.7294
	step [141/244], loss=109.6844
	step [142/244], loss=82.9686
	step [143/244], loss=85.1309
	step [144/244], loss=87.2015
	step [145/244], loss=87.5983
	step [146/244], loss=109.1256
	step [147/244], loss=88.2471
	step [148/244], loss=89.3045
	step [149/244], loss=89.1971
	step [150/244], loss=92.2907
	step [151/244], loss=95.0059
	step [152/244], loss=83.5627
	step [153/244], loss=106.7755
	step [154/244], loss=89.7394
	step [155/244], loss=94.5251
	step [156/244], loss=86.3912
	step [157/244], loss=99.8211
	step [158/244], loss=92.2108
	step [159/244], loss=88.7604
	step [160/244], loss=84.3000
	step [161/244], loss=95.3182
	step [162/244], loss=90.3214
	step [163/244], loss=100.6005
	step [164/244], loss=83.4009
	step [165/244], loss=114.8459
	step [166/244], loss=102.5092
	step [167/244], loss=69.4294
	step [168/244], loss=93.5781
	step [169/244], loss=100.9574
	step [170/244], loss=91.8119
	step [171/244], loss=91.7019
	step [172/244], loss=105.3745
	step [173/244], loss=103.7610
	step [174/244], loss=95.9653
	step [175/244], loss=89.4946
	step [176/244], loss=95.2584
	step [177/244], loss=77.4254
	step [178/244], loss=80.3137
	step [179/244], loss=91.4799
	step [180/244], loss=83.0367
	step [181/244], loss=81.1813
	step [182/244], loss=93.0349
	step [183/244], loss=83.0315
	step [184/244], loss=86.3182
	step [185/244], loss=80.9084
	step [186/244], loss=100.5611
	step [187/244], loss=82.0520
	step [188/244], loss=95.5488
	step [189/244], loss=81.1782
	step [190/244], loss=108.6200
	step [191/244], loss=77.1295
	step [192/244], loss=69.5222
	step [193/244], loss=78.5132
	step [194/244], loss=90.8478
	step [195/244], loss=85.4971
	step [196/244], loss=78.2634
	step [197/244], loss=80.2357
	step [198/244], loss=111.3467
	step [199/244], loss=94.5317
	step [200/244], loss=78.8660
	step [201/244], loss=91.1147
	step [202/244], loss=72.1631
	step [203/244], loss=84.9504
	step [204/244], loss=84.6415
	step [205/244], loss=89.2018
	step [206/244], loss=98.4395
	step [207/244], loss=92.4076
	step [208/244], loss=75.2074
	step [209/244], loss=90.1379
	step [210/244], loss=93.5636
	step [211/244], loss=85.4557
	step [212/244], loss=105.4518
	step [213/244], loss=102.8089
	step [214/244], loss=89.8514
	step [215/244], loss=89.0415
	step [216/244], loss=113.4326
	step [217/244], loss=88.3446
	step [218/244], loss=90.4088
	step [219/244], loss=87.0097
	step [220/244], loss=91.9084
	step [221/244], loss=82.7253
	step [222/244], loss=82.4445
	step [223/244], loss=90.4276
	step [224/244], loss=87.3571
	step [225/244], loss=94.1529
	step [226/244], loss=89.5172
	step [227/244], loss=93.8389
	step [228/244], loss=79.8164
	step [229/244], loss=105.2313
	step [230/244], loss=107.5283
	step [231/244], loss=92.6668
	step [232/244], loss=92.8680
	step [233/244], loss=104.6355
	step [234/244], loss=85.0162
	step [235/244], loss=82.7955
	step [236/244], loss=83.7394
	step [237/244], loss=83.9378
	step [238/244], loss=85.7296
	step [239/244], loss=89.2961
	step [240/244], loss=101.2621
	step [241/244], loss=98.8804
	step [242/244], loss=85.6746
	step [243/244], loss=77.9332
	step [244/244], loss=4.9263
	Evaluating
	loss=0.0116, precision=0.2738, recall=0.9054, f1=0.4204
Training epoch 28
	step [1/244], loss=98.3639
	step [2/244], loss=88.1491
	step [3/244], loss=92.6253
	step [4/244], loss=69.6171
	step [5/244], loss=76.9555
	step [6/244], loss=95.0199
	step [7/244], loss=102.5210
	step [8/244], loss=90.3130
	step [9/244], loss=83.1530
	step [10/244], loss=81.6016
	step [11/244], loss=85.8033
	step [12/244], loss=84.2873
	step [13/244], loss=91.4642
	step [14/244], loss=109.8537
	step [15/244], loss=98.8115
	step [16/244], loss=98.3379
	step [17/244], loss=109.8413
	step [18/244], loss=79.8401
	step [19/244], loss=84.8200
	step [20/244], loss=83.9523
	step [21/244], loss=92.3642
	step [22/244], loss=85.6667
	step [23/244], loss=97.2512
	step [24/244], loss=82.1470
	step [25/244], loss=79.5586
	step [26/244], loss=118.8167
	step [27/244], loss=104.4350
	step [28/244], loss=79.5344
	step [29/244], loss=108.7505
	step [30/244], loss=84.5432
	step [31/244], loss=91.7869
	step [32/244], loss=90.4388
	step [33/244], loss=81.1727
	step [34/244], loss=82.6400
	step [35/244], loss=101.9591
	step [36/244], loss=83.4565
	step [37/244], loss=108.8980
	step [38/244], loss=82.4958
	step [39/244], loss=90.3618
	step [40/244], loss=78.9850
	step [41/244], loss=92.3832
	step [42/244], loss=97.5884
	step [43/244], loss=91.2922
	step [44/244], loss=88.4865
	step [45/244], loss=106.1621
	step [46/244], loss=106.1953
	step [47/244], loss=89.9994
	step [48/244], loss=73.1864
	step [49/244], loss=88.3695
	step [50/244], loss=92.4012
	step [51/244], loss=97.3670
	step [52/244], loss=84.9213
	step [53/244], loss=111.4008
	step [54/244], loss=82.2625
	step [55/244], loss=105.2347
	step [56/244], loss=79.0851
	step [57/244], loss=88.5458
	step [58/244], loss=84.4627
	step [59/244], loss=96.6992
	step [60/244], loss=98.2609
	step [61/244], loss=87.1455
	step [62/244], loss=87.3950
	step [63/244], loss=96.0680
	step [64/244], loss=98.4154
	step [65/244], loss=96.1208
	step [66/244], loss=90.5344
	step [67/244], loss=97.6111
	step [68/244], loss=96.6243
	step [69/244], loss=106.7226
	step [70/244], loss=91.4336
	step [71/244], loss=77.6525
	step [72/244], loss=79.4377
	step [73/244], loss=83.8814
	step [74/244], loss=85.5839
	step [75/244], loss=86.8184
	step [76/244], loss=77.3017
	step [77/244], loss=112.0412
	step [78/244], loss=89.0919
	step [79/244], loss=93.7080
	step [80/244], loss=101.8971
	step [81/244], loss=97.0876
	step [82/244], loss=77.8964
	step [83/244], loss=75.2123
	step [84/244], loss=76.1102
	step [85/244], loss=109.2022
	step [86/244], loss=99.9328
	step [87/244], loss=91.4708
	step [88/244], loss=100.1884
	step [89/244], loss=95.7330
	step [90/244], loss=110.4384
	step [91/244], loss=92.7186
	step [92/244], loss=84.3029
	step [93/244], loss=67.6135
	step [94/244], loss=72.1604
	step [95/244], loss=96.0406
	step [96/244], loss=80.5086
	step [97/244], loss=106.8282
	step [98/244], loss=92.3542
	step [99/244], loss=97.1712
	step [100/244], loss=87.2215
	step [101/244], loss=74.6782
	step [102/244], loss=79.9603
	step [103/244], loss=91.1747
	step [104/244], loss=96.4313
	step [105/244], loss=86.3788
	step [106/244], loss=94.4277
	step [107/244], loss=88.6550
	step [108/244], loss=86.7282
	step [109/244], loss=102.8368
	step [110/244], loss=91.2961
	step [111/244], loss=93.0436
	step [112/244], loss=93.0219
	step [113/244], loss=95.7255
	step [114/244], loss=83.2650
	step [115/244], loss=114.2886
	step [116/244], loss=81.1759
	step [117/244], loss=103.2168
	step [118/244], loss=91.3722
	step [119/244], loss=87.7960
	step [120/244], loss=83.3067
	step [121/244], loss=101.8056
	step [122/244], loss=82.1935
	step [123/244], loss=80.7355
	step [124/244], loss=90.8100
	step [125/244], loss=83.3740
	step [126/244], loss=82.4414
	step [127/244], loss=81.8677
	step [128/244], loss=100.0470
	step [129/244], loss=97.6068
	step [130/244], loss=103.5353
	step [131/244], loss=86.8573
	step [132/244], loss=104.4977
	step [133/244], loss=90.6076
	step [134/244], loss=105.2782
	step [135/244], loss=82.4067
	step [136/244], loss=96.6458
	step [137/244], loss=96.9782
	step [138/244], loss=96.1714
	step [139/244], loss=100.5840
	step [140/244], loss=94.1040
	step [141/244], loss=98.5480
	step [142/244], loss=84.2969
	step [143/244], loss=97.5952
	step [144/244], loss=71.4515
	step [145/244], loss=86.4863
	step [146/244], loss=93.2966
	step [147/244], loss=93.1080
	step [148/244], loss=76.0881
	step [149/244], loss=82.9436
	step [150/244], loss=87.5629
	step [151/244], loss=69.2031
	step [152/244], loss=97.7286
	step [153/244], loss=76.7894
	step [154/244], loss=91.3369
	step [155/244], loss=105.9405
	step [156/244], loss=99.0110
	step [157/244], loss=78.4203
	step [158/244], loss=93.4698
	step [159/244], loss=98.7052
	step [160/244], loss=99.9860
	step [161/244], loss=92.5060
	step [162/244], loss=90.3468
	step [163/244], loss=80.0799
	step [164/244], loss=86.3430
	step [165/244], loss=101.0409
	step [166/244], loss=104.3592
	step [167/244], loss=100.0200
	step [168/244], loss=100.0026
	step [169/244], loss=86.9062
	step [170/244], loss=87.8366
	step [171/244], loss=104.1569
	step [172/244], loss=109.9829
	step [173/244], loss=90.2326
	step [174/244], loss=84.4403
	step [175/244], loss=88.6451
	step [176/244], loss=100.6054
	step [177/244], loss=84.2315
	step [178/244], loss=97.0571
	step [179/244], loss=87.3321
	step [180/244], loss=87.4590
	step [181/244], loss=81.1028
	step [182/244], loss=86.6747
	step [183/244], loss=91.6516
	step [184/244], loss=87.4635
	step [185/244], loss=84.5647
	step [186/244], loss=90.8842
	step [187/244], loss=94.5967
	step [188/244], loss=88.0839
	step [189/244], loss=92.0932
	step [190/244], loss=82.6544
	step [191/244], loss=92.9970
	step [192/244], loss=90.2800
	step [193/244], loss=103.1758
	step [194/244], loss=94.8017
	step [195/244], loss=87.2112
	step [196/244], loss=86.0466
	step [197/244], loss=98.8174
	step [198/244], loss=90.5440
	step [199/244], loss=103.8256
	step [200/244], loss=85.9728
	step [201/244], loss=97.1750
	step [202/244], loss=81.8296
	step [203/244], loss=83.3308
	step [204/244], loss=88.1791
	step [205/244], loss=79.9804
	step [206/244], loss=101.3791
	step [207/244], loss=90.9985
	step [208/244], loss=87.4480
	step [209/244], loss=77.1928
	step [210/244], loss=90.8453
	step [211/244], loss=94.8866
	step [212/244], loss=95.2891
	step [213/244], loss=106.1192
	step [214/244], loss=91.8497
	step [215/244], loss=82.3159
	step [216/244], loss=96.4092
	step [217/244], loss=93.4145
	step [218/244], loss=88.6942
	step [219/244], loss=102.4247
	step [220/244], loss=87.9136
	step [221/244], loss=93.8666
	step [222/244], loss=94.9817
	step [223/244], loss=88.9847
	step [224/244], loss=99.7165
	step [225/244], loss=81.7927
	step [226/244], loss=88.3594
	step [227/244], loss=95.9782
	step [228/244], loss=100.5417
	step [229/244], loss=89.0839
	step [230/244], loss=94.3375
	step [231/244], loss=97.4061
	step [232/244], loss=106.7871
	step [233/244], loss=62.1905
	step [234/244], loss=102.2302
	step [235/244], loss=84.6183
	step [236/244], loss=94.0574
	step [237/244], loss=88.0304
	step [238/244], loss=95.1183
	step [239/244], loss=94.4722
	step [240/244], loss=96.8515
	step [241/244], loss=99.4450
	step [242/244], loss=85.7137
	step [243/244], loss=87.3082
	step [244/244], loss=4.7716
	Evaluating
	loss=0.0101, precision=0.3219, recall=0.8943, f1=0.4734
Training epoch 29
	step [1/244], loss=95.2826
	step [2/244], loss=96.3556
	step [3/244], loss=97.5908
	step [4/244], loss=81.8411
	step [5/244], loss=79.4037
	step [6/244], loss=99.6290
	step [7/244], loss=103.3860
	step [8/244], loss=85.7534
	step [9/244], loss=84.9673
	step [10/244], loss=84.1738
	step [11/244], loss=95.3831
	step [12/244], loss=87.1018
	step [13/244], loss=86.7410
	step [14/244], loss=106.1501
	step [15/244], loss=95.0540
	step [16/244], loss=89.3355
	step [17/244], loss=83.5032
	step [18/244], loss=93.7802
	step [19/244], loss=108.2207
	step [20/244], loss=92.6325
	step [21/244], loss=77.8111
	step [22/244], loss=86.0076
	step [23/244], loss=79.0107
	step [24/244], loss=101.5026
	step [25/244], loss=102.3153
	step [26/244], loss=90.1869
	step [27/244], loss=85.8498
	step [28/244], loss=102.5731
	step [29/244], loss=70.3042
	step [30/244], loss=102.8051
	step [31/244], loss=83.7496
	step [32/244], loss=84.9867
	step [33/244], loss=89.2719
	step [34/244], loss=97.5543
	step [35/244], loss=71.7995
	step [36/244], loss=89.7034
	step [37/244], loss=61.3049
	step [38/244], loss=100.4505
	step [39/244], loss=88.9053
	step [40/244], loss=99.9574
	step [41/244], loss=95.1576
	step [42/244], loss=82.9162
	step [43/244], loss=91.1767
	step [44/244], loss=98.5392
	step [45/244], loss=96.8149
	step [46/244], loss=85.6012
	step [47/244], loss=101.1305
	step [48/244], loss=78.2797
	step [49/244], loss=96.9207
	step [50/244], loss=98.8799
	step [51/244], loss=94.3166
	step [52/244], loss=91.3774
	step [53/244], loss=95.8182
	step [54/244], loss=85.9746
	step [55/244], loss=74.7104
	step [56/244], loss=86.8446
	step [57/244], loss=79.1712
	step [58/244], loss=85.9104
	step [59/244], loss=74.6481
	step [60/244], loss=95.4271
	step [61/244], loss=79.6985
	step [62/244], loss=91.6271
	step [63/244], loss=102.8212
	step [64/244], loss=118.5821
	step [65/244], loss=95.5160
	step [66/244], loss=95.5631
	step [67/244], loss=69.1174
	step [68/244], loss=90.9309
	step [69/244], loss=97.8030
	step [70/244], loss=81.5989
	step [71/244], loss=92.1528
	step [72/244], loss=104.9979
	step [73/244], loss=85.2124
	step [74/244], loss=85.4206
	step [75/244], loss=91.2031
	step [76/244], loss=93.1192
	step [77/244], loss=98.0479
	step [78/244], loss=85.1679
	step [79/244], loss=74.5557
	step [80/244], loss=86.6130
	step [81/244], loss=91.6783
	step [82/244], loss=81.6004
	step [83/244], loss=100.3552
	step [84/244], loss=89.9818
	step [85/244], loss=93.0180
	step [86/244], loss=92.9118
	step [87/244], loss=78.8599
	step [88/244], loss=85.4862
	step [89/244], loss=95.9296
	step [90/244], loss=81.2613
	step [91/244], loss=103.0488
	step [92/244], loss=103.6028
	step [93/244], loss=81.3432
	step [94/244], loss=102.9264
	step [95/244], loss=83.3830
	step [96/244], loss=85.3974
	step [97/244], loss=115.1248
	step [98/244], loss=96.2619
	step [99/244], loss=103.1088
	step [100/244], loss=104.3035
	step [101/244], loss=90.2377
	step [102/244], loss=84.5747
	step [103/244], loss=79.0353
	step [104/244], loss=89.9113
	step [105/244], loss=73.8878
	step [106/244], loss=75.8107
	step [107/244], loss=76.2192
	step [108/244], loss=81.6137
	step [109/244], loss=77.0370
	step [110/244], loss=89.3464
	step [111/244], loss=82.0683
	step [112/244], loss=80.3444
	step [113/244], loss=73.6228
	step [114/244], loss=86.2919
	step [115/244], loss=99.0472
	step [116/244], loss=81.3080
	step [117/244], loss=88.9090
	step [118/244], loss=100.6030
	step [119/244], loss=94.0264
	step [120/244], loss=102.4209
	step [121/244], loss=90.0371
	step [122/244], loss=96.2842
	step [123/244], loss=103.1490
	step [124/244], loss=117.7723
	step [125/244], loss=90.4073
	step [126/244], loss=104.9457
	step [127/244], loss=104.9818
	step [128/244], loss=84.3837
	step [129/244], loss=97.2011
	step [130/244], loss=95.0863
	step [131/244], loss=93.6363
	step [132/244], loss=90.1285
	step [133/244], loss=85.8985
	step [134/244], loss=104.2156
	step [135/244], loss=94.8774
	step [136/244], loss=92.9313
	step [137/244], loss=88.8720
	step [138/244], loss=82.6355
	step [139/244], loss=94.2099
	step [140/244], loss=83.0105
	step [141/244], loss=97.5741
	step [142/244], loss=91.3773
	step [143/244], loss=82.6238
	step [144/244], loss=73.7310
	step [145/244], loss=97.8387
	step [146/244], loss=94.0536
	step [147/244], loss=93.3624
	step [148/244], loss=91.7444
	step [149/244], loss=81.5744
	step [150/244], loss=74.9109
	step [151/244], loss=106.2366
	step [152/244], loss=85.8620
	step [153/244], loss=85.3031
	step [154/244], loss=97.4729
	step [155/244], loss=101.4886
	step [156/244], loss=92.1926
	step [157/244], loss=81.0258
	step [158/244], loss=102.2817
	step [159/244], loss=81.6081
	step [160/244], loss=85.9375
	step [161/244], loss=103.4688
	step [162/244], loss=75.4353
	step [163/244], loss=78.5556
	step [164/244], loss=92.9258
	step [165/244], loss=89.1819
	step [166/244], loss=72.9366
	step [167/244], loss=93.2760
	step [168/244], loss=98.3859
	step [169/244], loss=88.9572
	step [170/244], loss=77.4566
	step [171/244], loss=96.8443
	step [172/244], loss=92.4293
	step [173/244], loss=77.6683
	step [174/244], loss=70.2914
	step [175/244], loss=83.5749
	step [176/244], loss=83.9138
	step [177/244], loss=131.1537
	step [178/244], loss=109.9396
	step [179/244], loss=87.0890
	step [180/244], loss=87.9946
	step [181/244], loss=100.9287
	step [182/244], loss=99.6907
	step [183/244], loss=89.0046
	step [184/244], loss=78.7668
	step [185/244], loss=89.8241
	step [186/244], loss=86.2596
	step [187/244], loss=98.3123
	step [188/244], loss=85.8963
	step [189/244], loss=101.5432
	step [190/244], loss=86.8573
	step [191/244], loss=100.8626
	step [192/244], loss=98.8083
	step [193/244], loss=83.6228
	step [194/244], loss=89.7475
	step [195/244], loss=69.9534
	step [196/244], loss=109.3824
	step [197/244], loss=89.6225
	step [198/244], loss=104.4498
	step [199/244], loss=64.6377
	step [200/244], loss=69.6184
	step [201/244], loss=83.1387
	step [202/244], loss=86.2517
	step [203/244], loss=98.3904
	step [204/244], loss=92.0857
	step [205/244], loss=116.0510
	step [206/244], loss=90.2119
	step [207/244], loss=88.2868
	step [208/244], loss=73.5315
	step [209/244], loss=80.8926
	step [210/244], loss=84.7545
	step [211/244], loss=91.0774
	step [212/244], loss=110.9562
	step [213/244], loss=92.0810
	step [214/244], loss=81.3671
	step [215/244], loss=106.0173
	step [216/244], loss=79.3927
	step [217/244], loss=78.7220
	step [218/244], loss=91.9468
	step [219/244], loss=96.0842
	step [220/244], loss=69.8679
	step [221/244], loss=78.9369
	step [222/244], loss=96.2877
	step [223/244], loss=87.7238
	step [224/244], loss=98.1544
	step [225/244], loss=84.3313
	step [226/244], loss=104.0012
	step [227/244], loss=85.8716
	step [228/244], loss=61.7605
	step [229/244], loss=86.7076
	step [230/244], loss=79.2263
	step [231/244], loss=97.9774
	step [232/244], loss=91.1694
	step [233/244], loss=93.0979
	step [234/244], loss=72.6526
	step [235/244], loss=85.3956
	step [236/244], loss=92.7739
	step [237/244], loss=90.0357
	step [238/244], loss=92.6342
	step [239/244], loss=98.3946
	step [240/244], loss=104.5286
	step [241/244], loss=93.3494
	step [242/244], loss=95.9877
	step [243/244], loss=85.9224
	step [244/244], loss=5.7883
	Evaluating
	loss=0.0138, precision=0.2433, recall=0.8985, f1=0.3829
Training epoch 30
	step [1/244], loss=80.2413
	step [2/244], loss=98.5256
	step [3/244], loss=103.0982
	step [4/244], loss=85.7693
	step [5/244], loss=92.8860
	step [6/244], loss=93.7699
	step [7/244], loss=84.1676
	step [8/244], loss=111.2801
	step [9/244], loss=105.0014
	step [10/244], loss=93.8289
	step [11/244], loss=92.9116
	step [12/244], loss=93.6374
	step [13/244], loss=87.0449
	step [14/244], loss=73.4129
	step [15/244], loss=69.9096
	step [16/244], loss=83.6818
	step [17/244], loss=104.2607
	step [18/244], loss=94.5674
	step [19/244], loss=91.0550
	step [20/244], loss=82.0801
	step [21/244], loss=82.8194
	step [22/244], loss=101.7897
	step [23/244], loss=74.0277
	step [24/244], loss=77.2128
	step [25/244], loss=95.5928
	step [26/244], loss=86.8715
	step [27/244], loss=83.3706
	step [28/244], loss=94.4149
	step [29/244], loss=85.2995
	step [30/244], loss=110.2853
	step [31/244], loss=91.9022
	step [32/244], loss=84.7979
	step [33/244], loss=73.7193
	step [34/244], loss=88.5929
	step [35/244], loss=82.8589
	step [36/244], loss=73.7782
	step [37/244], loss=99.8059
	step [38/244], loss=74.1464
	step [39/244], loss=97.1537
	step [40/244], loss=77.1048
	step [41/244], loss=77.4114
	step [42/244], loss=88.8939
	step [43/244], loss=87.5692
	step [44/244], loss=88.9885
	step [45/244], loss=100.1274
	step [46/244], loss=97.4763
	step [47/244], loss=72.9878
	step [48/244], loss=83.9425
	step [49/244], loss=80.1669
	step [50/244], loss=98.2214
	step [51/244], loss=81.5787
	step [52/244], loss=85.1929
	step [53/244], loss=80.9521
	step [54/244], loss=85.6751
	step [55/244], loss=97.9662
	step [56/244], loss=83.1538
	step [57/244], loss=74.1775
	step [58/244], loss=102.5502
	step [59/244], loss=76.3290
	step [60/244], loss=95.4465
	step [61/244], loss=84.6200
	step [62/244], loss=87.5763
	step [63/244], loss=89.1472
	step [64/244], loss=89.5038
	step [65/244], loss=78.3048
	step [66/244], loss=86.4299
	step [67/244], loss=79.2838
	step [68/244], loss=87.1890
	step [69/244], loss=91.1682
	step [70/244], loss=76.6457
	step [71/244], loss=90.8101
	step [72/244], loss=82.1008
	step [73/244], loss=81.4324
	step [74/244], loss=75.6162
	step [75/244], loss=83.5489
	step [76/244], loss=92.0480
	step [77/244], loss=84.4130
	step [78/244], loss=78.6152
	step [79/244], loss=110.0117
	step [80/244], loss=89.4553
	step [81/244], loss=93.6692
	step [82/244], loss=93.1841
	step [83/244], loss=95.4868
	step [84/244], loss=103.2307
	step [85/244], loss=103.2415
	step [86/244], loss=82.0074
	step [87/244], loss=91.5148
	step [88/244], loss=104.8538
	step [89/244], loss=89.6761
	step [90/244], loss=89.1078
	step [91/244], loss=88.9863
	step [92/244], loss=84.1534
	step [93/244], loss=81.0071
	step [94/244], loss=96.3282
	step [95/244], loss=83.1240
	step [96/244], loss=112.7990
	step [97/244], loss=80.9250
	step [98/244], loss=78.8786
	step [99/244], loss=91.6444
	step [100/244], loss=93.5073
	step [101/244], loss=91.9313
	step [102/244], loss=94.8909
	step [103/244], loss=87.9680
	step [104/244], loss=86.0766
	step [105/244], loss=76.9711
	step [106/244], loss=78.0131
	step [107/244], loss=86.7622
	step [108/244], loss=86.3126
	step [109/244], loss=86.5464
	step [110/244], loss=114.6126
	step [111/244], loss=94.3276
	step [112/244], loss=65.9845
	step [113/244], loss=96.3499
	step [114/244], loss=90.5385
	step [115/244], loss=88.2012
	step [116/244], loss=84.4949
	step [117/244], loss=81.0621
	step [118/244], loss=98.8209
	step [119/244], loss=69.3923
	step [120/244], loss=83.1975
	step [121/244], loss=91.1738
	step [122/244], loss=98.5218
	step [123/244], loss=78.7986
	step [124/244], loss=86.2370
	step [125/244], loss=78.6251
	step [126/244], loss=108.8310
	step [127/244], loss=93.0163
	step [128/244], loss=88.0228
	step [129/244], loss=83.9523
	step [130/244], loss=102.5347
	step [131/244], loss=84.6221
	step [132/244], loss=94.0016
	step [133/244], loss=89.5817
	step [134/244], loss=85.6409
	step [135/244], loss=87.1426
	step [136/244], loss=84.3298
	step [137/244], loss=102.6859
	step [138/244], loss=88.2924
	step [139/244], loss=86.1285
	step [140/244], loss=99.6782
	step [141/244], loss=101.2657
	step [142/244], loss=91.6055
	step [143/244], loss=97.3129
	step [144/244], loss=95.6470
	step [145/244], loss=80.0668
	step [146/244], loss=92.6575
	step [147/244], loss=82.1167
	step [148/244], loss=92.8264
	step [149/244], loss=84.8862
	step [150/244], loss=98.8876
	step [151/244], loss=98.3829
	step [152/244], loss=102.5178
	step [153/244], loss=97.0505
	step [154/244], loss=78.5621
	step [155/244], loss=99.1878
	step [156/244], loss=82.5671
	step [157/244], loss=87.3897
	step [158/244], loss=121.1723
	step [159/244], loss=92.2658
	step [160/244], loss=84.6002
	step [161/244], loss=87.0846
	step [162/244], loss=92.5987
	step [163/244], loss=89.9140
	step [164/244], loss=83.6958
	step [165/244], loss=93.7332
	step [166/244], loss=95.5178
	step [167/244], loss=94.3038
	step [168/244], loss=82.0102
	step [169/244], loss=78.2471
	step [170/244], loss=87.2977
	step [171/244], loss=83.0714
	step [172/244], loss=79.1907
	step [173/244], loss=85.2827
	step [174/244], loss=97.6765
	step [175/244], loss=94.9128
	step [176/244], loss=77.4569
	step [177/244], loss=95.6783
	step [178/244], loss=79.9034
	step [179/244], loss=91.9827
	step [180/244], loss=83.8705
	step [181/244], loss=97.6025
	step [182/244], loss=84.5760
	step [183/244], loss=92.0781
	step [184/244], loss=110.8982
	step [185/244], loss=90.7047
	step [186/244], loss=85.2630
	step [187/244], loss=87.2343
	step [188/244], loss=75.0611
	step [189/244], loss=88.0279
	step [190/244], loss=99.0779
	step [191/244], loss=94.6167
	step [192/244], loss=94.1964
	step [193/244], loss=86.7336
	step [194/244], loss=93.9798
	step [195/244], loss=98.2522
	step [196/244], loss=86.7206
	step [197/244], loss=91.6797
	step [198/244], loss=78.5674
	step [199/244], loss=104.1361
	step [200/244], loss=94.4849
	step [201/244], loss=82.8085
	step [202/244], loss=91.8012
	step [203/244], loss=96.6640
	step [204/244], loss=87.1592
	step [205/244], loss=108.9604
	step [206/244], loss=106.5892
	step [207/244], loss=79.1351
	step [208/244], loss=96.0627
	step [209/244], loss=91.5686
	step [210/244], loss=91.0200
	step [211/244], loss=83.9728
	step [212/244], loss=76.0451
	step [213/244], loss=79.8475
	step [214/244], loss=92.7569
	step [215/244], loss=89.7402
	step [216/244], loss=72.9180
	step [217/244], loss=86.8542
	step [218/244], loss=105.0584
	step [219/244], loss=92.5270
	step [220/244], loss=108.0214
	step [221/244], loss=95.8673
	step [222/244], loss=91.8838
	step [223/244], loss=101.4283
	step [224/244], loss=87.9010
	step [225/244], loss=84.6748
	step [226/244], loss=78.9740
	step [227/244], loss=72.9699
	step [228/244], loss=93.8922
	step [229/244], loss=87.4332
	step [230/244], loss=83.2906
	step [231/244], loss=73.0326
	step [232/244], loss=91.3652
	step [233/244], loss=103.4242
	step [234/244], loss=81.6370
	step [235/244], loss=75.9988
	step [236/244], loss=91.5388
	step [237/244], loss=88.0840
	step [238/244], loss=85.2736
	step [239/244], loss=82.3313
	step [240/244], loss=83.9622
	step [241/244], loss=86.0545
	step [242/244], loss=89.2805
	step [243/244], loss=75.8751
	step [244/244], loss=1.5803
	Evaluating
	loss=0.0099, precision=0.3311, recall=0.8966, f1=0.4837
Training finished
best_f1: 0.6062855515807685
directing: Z rim_enhanced: True test_id 2
removed wrong scan: weights_Z_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_278_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_276_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_298_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_210_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_262_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_254_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_58_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_293_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_291_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_206_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_252_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_265_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_295_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_221_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_292_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_294_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_297_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_214_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_296_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_225_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_300_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_243_xwqg-A00121_2019-05-15.npy
# all image files: 15579 # all weight files in weight_dir: 12056 # image files with weight 12007
removed wrong scan: weights_Z_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_278_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_276_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_298_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_210_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_262_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_254_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_58_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_293_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_291_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_206_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_252_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_265_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_295_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_221_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_292_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_294_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_297_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_214_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_296_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_225_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_300_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_243_xwqg-A00121_2019-05-15.npy
# all image files: 15579 # all weight files in weight_dir: 3488 # image files with weight 3477
train: /ibex/scratch/projects/c2052/air_tube_seg/training_samples_enhanced_one/Z 12007
Using 4 GPUs
Going to train epochs [1-30]
Training epoch 1
	step [1/188], loss=606.1696
	step [2/188], loss=406.2025
	step [3/188], loss=324.4907
	step [4/188], loss=323.5540
	step [5/188], loss=328.9497
	step [6/188], loss=329.4983
	step [7/188], loss=328.5729
	step [8/188], loss=309.1156
	step [9/188], loss=273.3726
	step [10/188], loss=268.3532
	step [11/188], loss=272.1439
	step [12/188], loss=276.3716
	step [13/188], loss=285.0831
	step [14/188], loss=302.7709
	step [15/188], loss=282.7143
	step [16/188], loss=264.5898
	step [17/188], loss=251.0281
	step [18/188], loss=249.8366
	step [19/188], loss=260.2193
	step [20/188], loss=240.7094
	step [21/188], loss=251.4351
	step [22/188], loss=261.5246
	step [23/188], loss=240.1571
	step [24/188], loss=231.8036
	step [25/188], loss=229.0049
	step [26/188], loss=250.4772
	step [27/188], loss=226.2831
	step [28/188], loss=248.7558
	step [29/188], loss=237.3318
	step [30/188], loss=245.8121
	step [31/188], loss=246.2637
	step [32/188], loss=246.4518
	step [33/188], loss=226.9411
	step [34/188], loss=231.9020
	step [35/188], loss=265.7303
	step [36/188], loss=241.5343
	step [37/188], loss=224.9251
	step [38/188], loss=227.0289
	step [39/188], loss=242.4185
	step [40/188], loss=222.7847
	step [41/188], loss=249.0744
	step [42/188], loss=215.5276
	step [43/188], loss=234.2688
	step [44/188], loss=193.8287
	step [45/188], loss=202.4456
	step [46/188], loss=214.5243
	step [47/188], loss=227.3100
	step [48/188], loss=221.0496
	step [49/188], loss=217.4728
	step [50/188], loss=246.7462
	step [51/188], loss=218.3139
	step [52/188], loss=211.8032
	step [53/188], loss=207.2020
	step [54/188], loss=236.8033
	step [55/188], loss=191.8773
	step [56/188], loss=226.7062
	step [57/188], loss=203.8900
	step [58/188], loss=218.3489
	step [59/188], loss=217.5090
	step [60/188], loss=232.5768
	step [61/188], loss=190.3749
	step [62/188], loss=204.8673
	step [63/188], loss=220.7562
	step [64/188], loss=215.1651
	step [65/188], loss=207.1849
	step [66/188], loss=204.0617
	step [67/188], loss=215.7143
	step [68/188], loss=214.2605
	step [69/188], loss=223.4635
	step [70/188], loss=215.1107
	step [71/188], loss=221.2610
	step [72/188], loss=210.4633
	step [73/188], loss=201.7309
	step [74/188], loss=199.0289
	step [75/188], loss=195.0723
	step [76/188], loss=199.6425
	step [77/188], loss=212.1049
	step [78/188], loss=184.2404
	step [79/188], loss=195.8217
	step [80/188], loss=197.1466
	step [81/188], loss=195.3341
	step [82/188], loss=196.8525
	step [83/188], loss=190.6308
	step [84/188], loss=189.1688
	step [85/188], loss=199.1735
	step [86/188], loss=181.6518
	step [87/188], loss=208.6180
	step [88/188], loss=198.2591
	step [89/188], loss=213.7553
	step [90/188], loss=218.6640
	step [91/188], loss=210.3808
	step [92/188], loss=216.9833
	step [93/188], loss=186.1254
	step [94/188], loss=203.4116
	step [95/188], loss=190.3577
	step [96/188], loss=188.9798
	step [97/188], loss=195.0591
	step [98/188], loss=200.3043
	step [99/188], loss=210.8296
	step [100/188], loss=183.8166
	step [101/188], loss=209.7422
	step [102/188], loss=200.7287
	step [103/188], loss=202.8569
	step [104/188], loss=192.1733
	step [105/188], loss=193.0574
	step [106/188], loss=186.0388
	step [107/188], loss=197.1617
	step [108/188], loss=202.7624
	step [109/188], loss=199.5687
	step [110/188], loss=208.2932
	step [111/188], loss=187.3101
	step [112/188], loss=194.5523
	step [113/188], loss=187.4248
	step [114/188], loss=185.2310
	step [115/188], loss=199.7261
	step [116/188], loss=186.6540
	step [117/188], loss=181.9249
	step [118/188], loss=174.5986
	step [119/188], loss=193.6927
	step [120/188], loss=200.8878
	step [121/188], loss=192.6867
	step [122/188], loss=172.1118
	step [123/188], loss=226.7888
	step [124/188], loss=206.5581
	step [125/188], loss=187.2527
	step [126/188], loss=179.2145
	step [127/188], loss=194.1952
	step [128/188], loss=194.9483
	step [129/188], loss=194.9099
	step [130/188], loss=203.0723
	step [131/188], loss=163.3229
	step [132/188], loss=186.3990
	step [133/188], loss=181.9910
	step [134/188], loss=200.7796
	step [135/188], loss=208.9024
	step [136/188], loss=204.2832
	step [137/188], loss=181.4450
	step [138/188], loss=181.6625
	step [139/188], loss=178.3358
	step [140/188], loss=190.9831
	step [141/188], loss=196.9566
	step [142/188], loss=186.2029
	step [143/188], loss=202.3504
	step [144/188], loss=189.6546
	step [145/188], loss=204.2263
	step [146/188], loss=186.0817
	step [147/188], loss=176.7021
	step [148/188], loss=169.6427
	step [149/188], loss=173.7347
	step [150/188], loss=179.6685
	step [151/188], loss=195.9685
	step [152/188], loss=183.4063
	step [153/188], loss=165.3961
	step [154/188], loss=196.1986
	step [155/188], loss=196.7361
	step [156/188], loss=177.3572
	step [157/188], loss=176.3874
	step [158/188], loss=180.6492
	step [159/188], loss=169.6073
	step [160/188], loss=195.3096
	step [161/188], loss=187.4710
	step [162/188], loss=189.1621
	step [163/188], loss=182.2668
	step [164/188], loss=184.3389
	step [165/188], loss=162.1713
	step [166/188], loss=174.3107
	step [167/188], loss=190.7587
	step [168/188], loss=179.5873
	step [169/188], loss=193.8484
	step [170/188], loss=177.9011
	step [171/188], loss=184.4620
	step [172/188], loss=180.6818
	step [173/188], loss=174.6749
	step [174/188], loss=172.3230
	step [175/188], loss=182.7241
	step [176/188], loss=184.0651
	step [177/188], loss=179.3637
	step [178/188], loss=173.4468
	step [179/188], loss=174.9638
	step [180/188], loss=178.7982
	step [181/188], loss=171.5916
	step [182/188], loss=171.3466
	step [183/188], loss=200.6770
	step [184/188], loss=183.3728
	step [185/188], loss=185.3726
	step [186/188], loss=178.3802
	step [187/188], loss=181.5387
	step [188/188], loss=106.9621
	Evaluating
	loss=0.3367, precision=0.1791, recall=0.9362, f1=0.3007
saving model as: 2_saved_model.pth
Training epoch 2
	step [1/188], loss=170.6324
	step [2/188], loss=169.0380
	step [3/188], loss=171.0822
	step [4/188], loss=170.1956
	step [5/188], loss=167.0373
	step [6/188], loss=164.0173
	step [7/188], loss=180.4256
	step [8/188], loss=164.8190
	step [9/188], loss=180.5852
	step [10/188], loss=165.7227
	step [11/188], loss=165.1489
	step [12/188], loss=191.2444
	step [13/188], loss=185.6635
	step [14/188], loss=185.1197
	step [15/188], loss=185.7014
	step [16/188], loss=155.4598
	step [17/188], loss=173.2643
	step [18/188], loss=168.5171
	step [19/188], loss=179.6104
	step [20/188], loss=159.9723
	step [21/188], loss=156.6263
	step [22/188], loss=177.4130
	step [23/188], loss=174.3813
	step [24/188], loss=164.6888
	step [25/188], loss=191.3602
	step [26/188], loss=170.9046
	step [27/188], loss=153.2361
	step [28/188], loss=172.3067
	step [29/188], loss=183.8460
	step [30/188], loss=160.2257
	step [31/188], loss=157.7901
	step [32/188], loss=173.9409
	step [33/188], loss=167.4548
	step [34/188], loss=151.2461
	step [35/188], loss=175.4854
	step [36/188], loss=172.4706
	step [37/188], loss=144.3595
	step [38/188], loss=166.7141
	step [39/188], loss=179.7449
	step [40/188], loss=173.3912
	step [41/188], loss=170.0741
	step [42/188], loss=148.7483
	step [43/188], loss=158.0198
	step [44/188], loss=165.8082
	step [45/188], loss=179.2774
	step [46/188], loss=181.5660
	step [47/188], loss=179.6899
	step [48/188], loss=169.2615
	step [49/188], loss=176.1386
	step [50/188], loss=172.0551
	step [51/188], loss=144.6274
	step [52/188], loss=180.4066
	step [53/188], loss=157.4488
	step [54/188], loss=170.9891
	step [55/188], loss=165.8503
	step [56/188], loss=155.3580
	step [57/188], loss=188.4564
	step [58/188], loss=158.6140
	step [59/188], loss=181.5506
	step [60/188], loss=167.1426
	step [61/188], loss=173.5630
	step [62/188], loss=165.1459
	step [63/188], loss=165.6448
	step [64/188], loss=154.4188
	step [65/188], loss=150.8044
	step [66/188], loss=177.8200
	step [67/188], loss=169.8782
	step [68/188], loss=171.6139
	step [69/188], loss=158.8944
	step [70/188], loss=171.4505
	step [71/188], loss=177.3248
	step [72/188], loss=175.0032
	step [73/188], loss=169.1733
	step [74/188], loss=158.9635
	step [75/188], loss=159.7396
	step [76/188], loss=175.7828
	step [77/188], loss=154.1223
	step [78/188], loss=163.1311
	step [79/188], loss=147.2439
	step [80/188], loss=153.6776
	step [81/188], loss=171.3363
	step [82/188], loss=168.7425
	step [83/188], loss=154.1183
	step [84/188], loss=181.7208
	step [85/188], loss=186.6624
	step [86/188], loss=166.1536
	step [87/188], loss=173.1389
	step [88/188], loss=168.5346
	step [89/188], loss=145.7256
	step [90/188], loss=160.8062
	step [91/188], loss=153.4389
	step [92/188], loss=181.6402
	step [93/188], loss=174.8650
	step [94/188], loss=169.2861
	step [95/188], loss=161.8092
	step [96/188], loss=154.5765
	step [97/188], loss=144.1699
	step [98/188], loss=154.8005
	step [99/188], loss=171.7951
	step [100/188], loss=200.3118
	step [101/188], loss=165.5506
	step [102/188], loss=150.2993
	step [103/188], loss=168.7312
	step [104/188], loss=153.6565
	step [105/188], loss=160.7587
	step [106/188], loss=153.6311
	step [107/188], loss=158.7353
	step [108/188], loss=172.2701
	step [109/188], loss=169.6920
	step [110/188], loss=165.6294
	step [111/188], loss=185.4748
	step [112/188], loss=160.4936
	step [113/188], loss=154.5337
	step [114/188], loss=148.9234
	step [115/188], loss=155.4609
	step [116/188], loss=132.5909
	step [117/188], loss=154.1953
	step [118/188], loss=185.3590
	step [119/188], loss=167.4480
	step [120/188], loss=162.1004
	step [121/188], loss=151.8392
	step [122/188], loss=157.2421
	step [123/188], loss=174.2497
	step [124/188], loss=166.9352
	step [125/188], loss=190.7367
	step [126/188], loss=165.9030
	step [127/188], loss=165.2148
	step [128/188], loss=156.4983
	step [129/188], loss=153.8332
	step [130/188], loss=161.2804
	step [131/188], loss=166.0469
	step [132/188], loss=161.3431
	step [133/188], loss=150.5194
	step [134/188], loss=153.2646
	step [135/188], loss=152.8332
	step [136/188], loss=158.7058
	step [137/188], loss=150.6579
	step [138/188], loss=159.0456
	step [139/188], loss=158.2290
	step [140/188], loss=147.5298
	step [141/188], loss=165.2147
	step [142/188], loss=172.3320
	step [143/188], loss=160.9398
	step [144/188], loss=140.7475
	step [145/188], loss=149.9358
	step [146/188], loss=146.3008
	step [147/188], loss=153.5573
	step [148/188], loss=169.2992
	step [149/188], loss=163.3242
	step [150/188], loss=160.0280
	step [151/188], loss=162.1235
	step [152/188], loss=170.6805
	step [153/188], loss=167.4542
	step [154/188], loss=154.7274
	step [155/188], loss=167.1006
	step [156/188], loss=146.9005
	step [157/188], loss=139.4476
	step [158/188], loss=171.5833
	step [159/188], loss=138.2557
	step [160/188], loss=174.1306
	step [161/188], loss=177.6926
	step [162/188], loss=159.6052
	step [163/188], loss=162.0720
	step [164/188], loss=155.6225
	step [165/188], loss=140.2824
	step [166/188], loss=146.5727
	step [167/188], loss=161.3087
	step [168/188], loss=167.9292
	step [169/188], loss=143.7432
	step [170/188], loss=159.8678
	step [171/188], loss=175.4974
	step [172/188], loss=154.5126
	step [173/188], loss=158.5258
	step [174/188], loss=151.2848
	step [175/188], loss=174.5509
	step [176/188], loss=147.4866
	step [177/188], loss=159.9039
	step [178/188], loss=152.6544
	step [179/188], loss=161.4032
	step [180/188], loss=152.2019
	step [181/188], loss=147.9299
	step [182/188], loss=161.6603
	step [183/188], loss=149.0830
	step [184/188], loss=158.1945
	step [185/188], loss=160.4644
	step [186/188], loss=147.1487
	step [187/188], loss=159.8506
	step [188/188], loss=97.9613
	Evaluating
	loss=0.2253, precision=0.4266, recall=0.9363, f1=0.5861
saving model as: 2_saved_model.pth
Training epoch 3
	step [1/188], loss=151.0471
	step [2/188], loss=151.6208
	step [3/188], loss=152.7496
	step [4/188], loss=162.5040
	step [5/188], loss=160.4475
	step [6/188], loss=135.8338
	step [7/188], loss=139.2169
	step [8/188], loss=148.1773
	step [9/188], loss=148.5641
	step [10/188], loss=153.1298
	step [11/188], loss=153.3021
	step [12/188], loss=153.7829
	step [13/188], loss=162.8491
	step [14/188], loss=141.0310
	step [15/188], loss=151.4923
	step [16/188], loss=161.5375
	step [17/188], loss=153.2007
	step [18/188], loss=174.5965
	step [19/188], loss=155.7910
	step [20/188], loss=151.9128
	step [21/188], loss=137.9349
	step [22/188], loss=143.8185
	step [23/188], loss=157.0396
	step [24/188], loss=158.4593
	step [25/188], loss=171.4173
	step [26/188], loss=170.4755
	step [27/188], loss=165.3188
	step [28/188], loss=157.2181
	step [29/188], loss=155.7037
	step [30/188], loss=156.5741
	step [31/188], loss=161.4689
	step [32/188], loss=145.6450
	step [33/188], loss=156.0615
	step [34/188], loss=147.6889
	step [35/188], loss=137.1809
	step [36/188], loss=148.2663
	step [37/188], loss=154.5468
	step [38/188], loss=167.1980
	step [39/188], loss=158.3404
	step [40/188], loss=148.4667
	step [41/188], loss=157.3633
	step [42/188], loss=146.2623
	step [43/188], loss=156.5147
	step [44/188], loss=154.6560
	step [45/188], loss=147.4861
	step [46/188], loss=155.3067
	step [47/188], loss=153.3257
	step [48/188], loss=176.3220
	step [49/188], loss=150.2097
	step [50/188], loss=136.2153
	step [51/188], loss=152.4598
	step [52/188], loss=150.9990
	step [53/188], loss=151.8344
	step [54/188], loss=165.5033
	step [55/188], loss=138.3256
	step [56/188], loss=140.6579
	step [57/188], loss=120.5111
	step [58/188], loss=144.9211
	step [59/188], loss=148.2865
	step [60/188], loss=141.5172
	step [61/188], loss=157.9912
	step [62/188], loss=152.0481
	step [63/188], loss=138.8071
	step [64/188], loss=158.4221
	step [65/188], loss=135.5285
	step [66/188], loss=145.3182
	step [67/188], loss=142.6889
	step [68/188], loss=146.4347
	step [69/188], loss=150.3245
	step [70/188], loss=153.1796
	step [71/188], loss=130.5454
	step [72/188], loss=157.0831
	step [73/188], loss=135.3320
	step [74/188], loss=150.0470
	step [75/188], loss=155.8176
	step [76/188], loss=151.4454
	step [77/188], loss=133.5156
	step [78/188], loss=148.7134
	step [79/188], loss=148.0490
	step [80/188], loss=157.5658
	step [81/188], loss=156.7031
	step [82/188], loss=156.9791
	step [83/188], loss=164.6579
	step [84/188], loss=164.3984
	step [85/188], loss=142.8395
	step [86/188], loss=156.8191
	step [87/188], loss=148.9471
	step [88/188], loss=140.5528
	step [89/188], loss=162.3561
	step [90/188], loss=153.5589
	step [91/188], loss=148.6872
	step [92/188], loss=128.2926
	step [93/188], loss=159.8518
	step [94/188], loss=149.1658
	step [95/188], loss=148.2073
	step [96/188], loss=130.8000
	step [97/188], loss=157.3608
	step [98/188], loss=155.8780
	step [99/188], loss=150.0916
	step [100/188], loss=160.2951
	step [101/188], loss=156.8956
	step [102/188], loss=146.6139
	step [103/188], loss=152.7593
	step [104/188], loss=142.0737
	step [105/188], loss=131.8236
	step [106/188], loss=143.7707
	step [107/188], loss=162.3287
	step [108/188], loss=158.6377
	step [109/188], loss=153.4672
	step [110/188], loss=149.6778
	step [111/188], loss=153.6430
	step [112/188], loss=157.3278
	step [113/188], loss=138.3400
	step [114/188], loss=151.0893
	step [115/188], loss=155.4071
	step [116/188], loss=137.4947
	step [117/188], loss=145.5732
	step [118/188], loss=146.5045
	step [119/188], loss=131.0200
	step [120/188], loss=139.7415
	step [121/188], loss=122.1003
	step [122/188], loss=152.1290
	step [123/188], loss=171.9298
	step [124/188], loss=149.2957
	step [125/188], loss=144.7713
	step [126/188], loss=140.1675
	step [127/188], loss=165.8698
	step [128/188], loss=162.2171
	step [129/188], loss=154.7711
	step [130/188], loss=158.4220
	step [131/188], loss=133.0763
	step [132/188], loss=159.9631
	step [133/188], loss=145.3149
	step [134/188], loss=154.5607
	step [135/188], loss=146.4170
	step [136/188], loss=162.2119
	step [137/188], loss=152.3284
	step [138/188], loss=153.9605
	step [139/188], loss=156.6511
	step [140/188], loss=138.3009
	step [141/188], loss=151.8775
	step [142/188], loss=150.3780
	step [143/188], loss=134.8933
	step [144/188], loss=169.0966
	step [145/188], loss=145.0536
	step [146/188], loss=151.9824
	step [147/188], loss=134.6965
	step [148/188], loss=149.9754
	step [149/188], loss=152.0035
	step [150/188], loss=149.7843
	step [151/188], loss=143.0019
	step [152/188], loss=136.0652
	step [153/188], loss=144.4883
	step [154/188], loss=137.1409
	step [155/188], loss=145.0270
	step [156/188], loss=137.2945
	step [157/188], loss=125.3019
	step [158/188], loss=147.6429
	step [159/188], loss=143.3962
	step [160/188], loss=157.5587
	step [161/188], loss=172.6203
	step [162/188], loss=174.0690
	step [163/188], loss=159.6110
	step [164/188], loss=166.1197
	step [165/188], loss=140.3368
	step [166/188], loss=147.6599
	step [167/188], loss=144.2867
	step [168/188], loss=167.3575
	step [169/188], loss=150.8841
	step [170/188], loss=141.9267
	step [171/188], loss=152.7928
	step [172/188], loss=128.0339
	step [173/188], loss=135.4339
	step [174/188], loss=122.0607
	step [175/188], loss=136.1949
	step [176/188], loss=158.0596
	step [177/188], loss=145.3354
	step [178/188], loss=127.2094
	step [179/188], loss=149.1466
	step [180/188], loss=137.8324
	step [181/188], loss=162.6684
	step [182/188], loss=140.0421
	step [183/188], loss=135.4282
	step [184/188], loss=139.1397
	step [185/188], loss=141.5164
	step [186/188], loss=151.5793
	step [187/188], loss=142.2814
	step [188/188], loss=92.3364
	Evaluating
	loss=0.1765, precision=0.4354, recall=0.9368, f1=0.5945
saving model as: 2_saved_model.pth
Training epoch 4
	step [1/188], loss=137.2170
	step [2/188], loss=172.1223
	step [3/188], loss=145.8574
	step [4/188], loss=158.9289
	step [5/188], loss=151.6575
	step [6/188], loss=144.3559
	step [7/188], loss=145.6806
	step [8/188], loss=128.5570
	step [9/188], loss=160.5064
	step [10/188], loss=140.8916
	step [11/188], loss=139.3508
	step [12/188], loss=139.1927
	step [13/188], loss=145.7573
	step [14/188], loss=147.0460
	step [15/188], loss=147.4358
	step [16/188], loss=153.7053
	step [17/188], loss=143.5663
	step [18/188], loss=152.9732
	step [19/188], loss=125.2442
	step [20/188], loss=142.3911
	step [21/188], loss=132.6568
	step [22/188], loss=155.1349
	step [23/188], loss=133.5624
	step [24/188], loss=145.6596
	step [25/188], loss=141.4998
	step [26/188], loss=149.9802
	step [27/188], loss=135.6861
	step [28/188], loss=126.2011
	step [29/188], loss=146.0694
	step [30/188], loss=143.8566
	step [31/188], loss=146.0666
	step [32/188], loss=153.5099
	step [33/188], loss=167.1494
	step [34/188], loss=140.4124
	step [35/188], loss=137.5161
	step [36/188], loss=152.5868
	step [37/188], loss=145.7377
	step [38/188], loss=123.5181
	step [39/188], loss=137.8975
	step [40/188], loss=143.6930
	step [41/188], loss=159.4849
	step [42/188], loss=139.3166
	step [43/188], loss=136.0341
	step [44/188], loss=136.5568
	step [45/188], loss=162.6924
	step [46/188], loss=146.7586
	step [47/188], loss=152.0668
	step [48/188], loss=141.8323
	step [49/188], loss=148.1014
	step [50/188], loss=151.4145
	step [51/188], loss=137.7494
	step [52/188], loss=130.9276
	step [53/188], loss=136.6648
	step [54/188], loss=142.9764
	step [55/188], loss=139.9236
	step [56/188], loss=144.0466
	step [57/188], loss=135.6560
	step [58/188], loss=134.6259
	step [59/188], loss=138.4922
	step [60/188], loss=141.7907
	step [61/188], loss=146.6701
	step [62/188], loss=144.7214
	step [63/188], loss=129.6565
	step [64/188], loss=126.5375
	step [65/188], loss=143.8799
	step [66/188], loss=144.2110
	step [67/188], loss=145.0289
	step [68/188], loss=134.4487
	step [69/188], loss=135.5535
	step [70/188], loss=140.5709
	step [71/188], loss=147.2677
	step [72/188], loss=130.1063
	step [73/188], loss=148.2648
	step [74/188], loss=154.1643
	step [75/188], loss=149.9289
	step [76/188], loss=144.4346
	step [77/188], loss=137.4594
	step [78/188], loss=137.8640
	step [79/188], loss=135.2186
	step [80/188], loss=146.6150
	step [81/188], loss=141.4150
	step [82/188], loss=142.3718
	step [83/188], loss=114.6040
	step [84/188], loss=130.8674
	step [85/188], loss=122.1152
	step [86/188], loss=140.6878
	step [87/188], loss=125.6496
	step [88/188], loss=131.6722
	step [89/188], loss=144.5909
	step [90/188], loss=146.4082
	step [91/188], loss=151.0786
	step [92/188], loss=138.0900
	step [93/188], loss=145.7383
	step [94/188], loss=130.9475
	step [95/188], loss=157.6522
	step [96/188], loss=160.2105
	step [97/188], loss=148.1590
	step [98/188], loss=135.3875
	step [99/188], loss=124.9227
	step [100/188], loss=138.6559
	step [101/188], loss=142.0858
	step [102/188], loss=151.9084
	step [103/188], loss=148.7129
	step [104/188], loss=139.2279
	step [105/188], loss=127.7895
	step [106/188], loss=130.7906
	step [107/188], loss=132.2124
	step [108/188], loss=137.5134
	step [109/188], loss=128.8086
	step [110/188], loss=140.6379
	step [111/188], loss=131.5748
	step [112/188], loss=129.6494
	step [113/188], loss=148.1997
	step [114/188], loss=134.7844
	step [115/188], loss=150.0471
	step [116/188], loss=134.0918
	step [117/188], loss=132.7694
	step [118/188], loss=134.8856
	step [119/188], loss=150.5883
	step [120/188], loss=124.8300
	step [121/188], loss=142.3505
	step [122/188], loss=143.4487
	step [123/188], loss=167.7322
	step [124/188], loss=145.8430
	step [125/188], loss=144.1221
	step [126/188], loss=133.8538
	step [127/188], loss=140.6578
	step [128/188], loss=149.3248
	step [129/188], loss=136.2384
	step [130/188], loss=136.3304
	step [131/188], loss=126.6058
	step [132/188], loss=145.0499
	step [133/188], loss=111.6708
	step [134/188], loss=130.2651
	step [135/188], loss=168.6869
	step [136/188], loss=134.7309
	step [137/188], loss=125.7510
	step [138/188], loss=150.1011
	step [139/188], loss=137.7663
	step [140/188], loss=120.3887
	step [141/188], loss=149.1794
	step [142/188], loss=130.9881
	step [143/188], loss=131.8143
	step [144/188], loss=148.6858
	step [145/188], loss=142.8374
	step [146/188], loss=114.6342
	step [147/188], loss=130.5402
	step [148/188], loss=143.7242
	step [149/188], loss=154.2581
	step [150/188], loss=135.3964
	step [151/188], loss=122.4567
	step [152/188], loss=144.3486
	step [153/188], loss=132.4848
	step [154/188], loss=128.9700
	step [155/188], loss=133.4141
	step [156/188], loss=149.2724
	step [157/188], loss=137.0228
	step [158/188], loss=146.0058
	step [159/188], loss=148.8214
	step [160/188], loss=133.9406
	step [161/188], loss=140.2579
	step [162/188], loss=124.7714
	step [163/188], loss=136.8950
	step [164/188], loss=138.2648
	step [165/188], loss=134.8661
	step [166/188], loss=128.2028
	step [167/188], loss=152.5637
	step [168/188], loss=120.3434
	step [169/188], loss=123.2017
	step [170/188], loss=144.8207
	step [171/188], loss=148.0988
	step [172/188], loss=124.8959
	step [173/188], loss=117.5186
	step [174/188], loss=122.1426
	step [175/188], loss=151.2294
	step [176/188], loss=145.0736
	step [177/188], loss=115.1282
	step [178/188], loss=142.6965
	step [179/188], loss=140.5066
	step [180/188], loss=140.9666
	step [181/188], loss=135.9744
	step [182/188], loss=131.5247
	step [183/188], loss=126.0776
	step [184/188], loss=133.7464
	step [185/188], loss=129.2328
	step [186/188], loss=142.2311
	step [187/188], loss=152.3265
	step [188/188], loss=73.8415
	Evaluating
	loss=0.1394, precision=0.4742, recall=0.9247, f1=0.6269
saving model as: 2_saved_model.pth
Training epoch 5
	step [1/188], loss=133.6841
	step [2/188], loss=116.5523
	step [3/188], loss=134.3147
	step [4/188], loss=126.0090
	step [5/188], loss=148.8173
	step [6/188], loss=136.9860
	step [7/188], loss=118.8032
	step [8/188], loss=133.4059
	step [9/188], loss=142.9177
	step [10/188], loss=155.0495
	step [11/188], loss=140.9334
	step [12/188], loss=139.8831
	step [13/188], loss=138.9895
	step [14/188], loss=139.9755
	step [15/188], loss=141.5641
	step [16/188], loss=140.4876
	step [17/188], loss=136.2541
	step [18/188], loss=147.5824
	step [19/188], loss=130.2540
	step [20/188], loss=141.6996
	step [21/188], loss=137.2545
	step [22/188], loss=157.1546
	step [23/188], loss=126.7535
	step [24/188], loss=116.4178
	step [25/188], loss=136.7140
	step [26/188], loss=128.0618
	step [27/188], loss=121.3196
	step [28/188], loss=135.8527
	step [29/188], loss=137.8072
	step [30/188], loss=124.2250
	step [31/188], loss=115.0653
	step [32/188], loss=140.1205
	step [33/188], loss=124.0850
	step [34/188], loss=139.4809
	step [35/188], loss=151.9186
	step [36/188], loss=142.8184
	step [37/188], loss=143.0213
	step [38/188], loss=138.0320
	step [39/188], loss=120.7158
	step [40/188], loss=121.7016
	step [41/188], loss=135.5485
	step [42/188], loss=134.4949
	step [43/188], loss=140.7011
	step [44/188], loss=149.0847
	step [45/188], loss=136.8390
	step [46/188], loss=131.1709
	step [47/188], loss=119.2056
	step [48/188], loss=131.4059
	step [49/188], loss=142.4431
	step [50/188], loss=137.9978
	step [51/188], loss=123.4898
	step [52/188], loss=130.1700
	step [53/188], loss=129.6362
	step [54/188], loss=142.7308
	step [55/188], loss=126.2246
	step [56/188], loss=130.0331
	step [57/188], loss=132.2292
	step [58/188], loss=116.3964
	step [59/188], loss=118.3237
	step [60/188], loss=133.4202
	step [61/188], loss=141.0595
	step [62/188], loss=119.3691
	step [63/188], loss=129.4689
	step [64/188], loss=140.8054
	step [65/188], loss=132.2623
	step [66/188], loss=139.7236
	step [67/188], loss=115.7188
	step [68/188], loss=124.0705
	step [69/188], loss=120.6576
	step [70/188], loss=114.9084
	step [71/188], loss=117.6453
	step [72/188], loss=115.2086
	step [73/188], loss=144.6196
	step [74/188], loss=137.7193
	step [75/188], loss=128.8600
	step [76/188], loss=131.3327
	step [77/188], loss=148.4173
	step [78/188], loss=128.2024
	step [79/188], loss=167.7702
	step [80/188], loss=131.4942
	step [81/188], loss=127.2046
	step [82/188], loss=129.2874
	step [83/188], loss=147.2932
	step [84/188], loss=131.7289
	step [85/188], loss=137.8580
	step [86/188], loss=128.2820
	step [87/188], loss=127.6274
	step [88/188], loss=149.9962
	step [89/188], loss=129.7311
	step [90/188], loss=124.9415
	step [91/188], loss=138.7301
	step [92/188], loss=126.0876
	step [93/188], loss=129.3607
	step [94/188], loss=131.9230
	step [95/188], loss=138.5339
	step [96/188], loss=126.9831
	step [97/188], loss=126.1811
	step [98/188], loss=130.4619
	step [99/188], loss=128.6505
	step [100/188], loss=124.8130
	step [101/188], loss=136.6945
	step [102/188], loss=125.5985
	step [103/188], loss=133.8725
	step [104/188], loss=138.5698
	step [105/188], loss=141.6622
	step [106/188], loss=138.9812
	step [107/188], loss=118.6617
	step [108/188], loss=124.4256
	step [109/188], loss=125.3412
	step [110/188], loss=129.3795
	step [111/188], loss=117.9578
	step [112/188], loss=120.8474
	step [113/188], loss=147.7277
	step [114/188], loss=114.4680
	step [115/188], loss=133.5005
	step [116/188], loss=116.5188
	step [117/188], loss=140.0318
	step [118/188], loss=123.1269
	step [119/188], loss=119.6689
	step [120/188], loss=120.7237
	step [121/188], loss=128.0926
	step [122/188], loss=133.1749
	step [123/188], loss=146.8171
	step [124/188], loss=116.6669
	step [125/188], loss=126.5955
	step [126/188], loss=131.2096
	step [127/188], loss=138.4959
	step [128/188], loss=132.8727
	step [129/188], loss=137.9668
	step [130/188], loss=130.7867
	step [131/188], loss=131.7452
	step [132/188], loss=121.8267
	step [133/188], loss=141.7199
	step [134/188], loss=131.4985
	step [135/188], loss=145.5937
	step [136/188], loss=137.9042
	step [137/188], loss=134.3732
	step [138/188], loss=130.1089
	step [139/188], loss=145.1925
	step [140/188], loss=148.6946
	step [141/188], loss=107.3723
	step [142/188], loss=129.8859
	step [143/188], loss=134.1480
	step [144/188], loss=134.6827
	step [145/188], loss=129.5838
	step [146/188], loss=129.7379
	step [147/188], loss=113.1370
	step [148/188], loss=120.0454
	step [149/188], loss=132.8069
	step [150/188], loss=136.4622
	step [151/188], loss=116.4741
	step [152/188], loss=122.8875
	step [153/188], loss=133.3832
	step [154/188], loss=138.3227
	step [155/188], loss=127.4903
	step [156/188], loss=123.4783
	step [157/188], loss=130.6679
	step [158/188], loss=123.1633
	step [159/188], loss=127.7594
	step [160/188], loss=130.4751
	step [161/188], loss=126.1494
	step [162/188], loss=127.8481
	step [163/188], loss=123.3786
	step [164/188], loss=142.2487
	step [165/188], loss=126.9014
	step [166/188], loss=124.3429
	step [167/188], loss=138.2632
	step [168/188], loss=131.4607
	step [169/188], loss=130.2670
	step [170/188], loss=162.1447
	step [171/188], loss=122.2255
	step [172/188], loss=130.4365
	step [173/188], loss=118.0507
	step [174/188], loss=122.8968
	step [175/188], loss=136.5312
	step [176/188], loss=131.4750
	step [177/188], loss=118.9259
	step [178/188], loss=117.4017
	step [179/188], loss=131.9773
	step [180/188], loss=108.2434
	step [181/188], loss=130.9330
	step [182/188], loss=129.5061
	step [183/188], loss=132.9330
	step [184/188], loss=148.3570
	step [185/188], loss=122.6083
	step [186/188], loss=130.9921
	step [187/188], loss=149.4324
	step [188/188], loss=72.1185
	Evaluating
	loss=0.1107, precision=0.4766, recall=0.9256, f1=0.6292
saving model as: 2_saved_model.pth
Training epoch 6
	step [1/188], loss=120.6382
	step [2/188], loss=126.0347
	step [3/188], loss=131.8228
	step [4/188], loss=108.1131
	step [5/188], loss=120.3789
	step [6/188], loss=120.2711
	step [7/188], loss=134.3350
	step [8/188], loss=117.5927
	step [9/188], loss=114.1973
	step [10/188], loss=131.2549
	step [11/188], loss=118.8910
	step [12/188], loss=130.3860
	step [13/188], loss=110.7614
	step [14/188], loss=132.4854
	step [15/188], loss=120.8873
	step [16/188], loss=121.4288
	step [17/188], loss=128.3779
	step [18/188], loss=125.9358
	step [19/188], loss=141.0664
	step [20/188], loss=112.4686
	step [21/188], loss=125.5362
	step [22/188], loss=117.0996
	step [23/188], loss=124.4954
	step [24/188], loss=124.9808
	step [25/188], loss=116.8925
	step [26/188], loss=129.7269
	step [27/188], loss=112.7901
	step [28/188], loss=127.8575
	step [29/188], loss=118.7922
	step [30/188], loss=122.2459
	step [31/188], loss=125.1021
	step [32/188], loss=138.0806
	step [33/188], loss=141.2007
	step [34/188], loss=125.5061
	step [35/188], loss=124.1429
	step [36/188], loss=117.5242
	step [37/188], loss=129.9624
	step [38/188], loss=119.5440
	step [39/188], loss=125.1521
	step [40/188], loss=137.6210
	step [41/188], loss=126.2984
	step [42/188], loss=125.8617
	step [43/188], loss=119.3761
	step [44/188], loss=126.9490
	step [45/188], loss=118.7854
	step [46/188], loss=127.3107
	step [47/188], loss=129.5428
	step [48/188], loss=134.5615
	step [49/188], loss=119.0483
	step [50/188], loss=133.5721
	step [51/188], loss=127.4188
	step [52/188], loss=127.5928
	step [53/188], loss=137.0017
	step [54/188], loss=128.2212
	step [55/188], loss=147.3991
	step [56/188], loss=110.5537
	step [57/188], loss=133.7112
	step [58/188], loss=139.3171
	step [59/188], loss=120.9327
	step [60/188], loss=131.0203
	step [61/188], loss=122.5884
	step [62/188], loss=117.2465
	step [63/188], loss=134.8405
	step [64/188], loss=115.5625
	step [65/188], loss=139.6211
	step [66/188], loss=136.6429
	step [67/188], loss=127.9414
	step [68/188], loss=117.9073
	step [69/188], loss=118.1720
	step [70/188], loss=129.4449
	step [71/188], loss=126.7481
	step [72/188], loss=112.8782
	step [73/188], loss=118.6428
	step [74/188], loss=143.6132
	step [75/188], loss=141.2328
	step [76/188], loss=116.1176
	step [77/188], loss=147.6967
	step [78/188], loss=142.7653
	step [79/188], loss=133.2912
	step [80/188], loss=114.1761
	step [81/188], loss=118.2669
	step [82/188], loss=119.5669
	step [83/188], loss=136.8580
	step [84/188], loss=129.2674
	step [85/188], loss=126.1636
	step [86/188], loss=134.5090
	step [87/188], loss=133.6313
	step [88/188], loss=137.7916
	step [89/188], loss=110.9175
	step [90/188], loss=130.2059
	step [91/188], loss=126.1050
	step [92/188], loss=128.6711
	step [93/188], loss=124.6504
	step [94/188], loss=136.4342
	step [95/188], loss=122.1500
	step [96/188], loss=126.3158
	step [97/188], loss=128.0439
	step [98/188], loss=138.3314
	step [99/188], loss=128.0049
	step [100/188], loss=124.1340
	step [101/188], loss=121.1486
	step [102/188], loss=128.4092
	step [103/188], loss=122.7850
	step [104/188], loss=118.7390
	step [105/188], loss=144.5611
	step [106/188], loss=111.3552
	step [107/188], loss=127.2193
	step [108/188], loss=128.9175
	step [109/188], loss=136.8530
	step [110/188], loss=123.4709
	step [111/188], loss=109.2196
	step [112/188], loss=128.4525
	step [113/188], loss=139.2643
	step [114/188], loss=127.4948
	step [115/188], loss=130.1760
	step [116/188], loss=114.2844
	step [117/188], loss=128.0984
	step [118/188], loss=117.8164
	step [119/188], loss=134.0808
	step [120/188], loss=117.0923
	step [121/188], loss=133.7092
	step [122/188], loss=124.8279
	step [123/188], loss=132.4511
	step [124/188], loss=118.8483
	step [125/188], loss=110.9891
	step [126/188], loss=125.1378
	step [127/188], loss=126.5355
	step [128/188], loss=148.2026
	step [129/188], loss=138.4460
	step [130/188], loss=141.0783
	step [131/188], loss=127.0867
	step [132/188], loss=125.9263
	step [133/188], loss=104.7508
	step [134/188], loss=126.7281
	step [135/188], loss=135.0059
	step [136/188], loss=127.0309
	step [137/188], loss=113.8573
	step [138/188], loss=111.4224
	step [139/188], loss=117.8668
	step [140/188], loss=140.4725
	step [141/188], loss=125.2885
	step [142/188], loss=115.9913
	step [143/188], loss=121.7637
	step [144/188], loss=123.8460
	step [145/188], loss=105.6852
	step [146/188], loss=111.7344
	step [147/188], loss=113.7834
	step [148/188], loss=142.0430
	step [149/188], loss=116.3415
	step [150/188], loss=123.0210
	step [151/188], loss=136.4125
	step [152/188], loss=130.6862
	step [153/188], loss=130.9984
	step [154/188], loss=121.3459
	step [155/188], loss=106.5957
	step [156/188], loss=135.7408
	step [157/188], loss=118.0574
	step [158/188], loss=132.1430
	step [159/188], loss=124.2245
	step [160/188], loss=121.5414
	step [161/188], loss=135.0683
	step [162/188], loss=128.3820
	step [163/188], loss=104.8942
	step [164/188], loss=153.4547
	step [165/188], loss=112.9789
	step [166/188], loss=118.8831
	step [167/188], loss=117.6217
	step [168/188], loss=128.4332
	step [169/188], loss=100.9668
	step [170/188], loss=127.6221
	step [171/188], loss=138.9247
	step [172/188], loss=133.2227
	step [173/188], loss=123.0551
	step [174/188], loss=132.3734
	step [175/188], loss=108.6968
	step [176/188], loss=126.2126
	step [177/188], loss=110.0693
	step [178/188], loss=104.6154
	step [179/188], loss=118.5170
	step [180/188], loss=114.3573
	step [181/188], loss=120.3055
	step [182/188], loss=136.1212
	step [183/188], loss=122.2514
	step [184/188], loss=127.0071
	step [185/188], loss=129.7639
	step [186/188], loss=120.2402
	step [187/188], loss=119.8586
	step [188/188], loss=67.6035
	Evaluating
	loss=0.0938, precision=0.4482, recall=0.9260, f1=0.6040
Training epoch 7
	step [1/188], loss=118.9340
	step [2/188], loss=113.2274
	step [3/188], loss=113.7231
	step [4/188], loss=139.4568
	step [5/188], loss=93.9366
	step [6/188], loss=127.8640
	step [7/188], loss=115.0206
	step [8/188], loss=161.3969
	step [9/188], loss=111.2961
	step [10/188], loss=111.7791
	step [11/188], loss=124.5524
	step [12/188], loss=115.6768
	step [13/188], loss=127.8601
	step [14/188], loss=128.6970
	step [15/188], loss=110.4544
	step [16/188], loss=132.6223
	step [17/188], loss=136.3168
	step [18/188], loss=116.6268
	step [19/188], loss=120.9652
	step [20/188], loss=131.8551
	step [21/188], loss=127.4298
	step [22/188], loss=121.8936
	step [23/188], loss=112.3332
	step [24/188], loss=131.9101
	step [25/188], loss=125.7927
	step [26/188], loss=121.8631
	step [27/188], loss=121.8797
	step [28/188], loss=118.8332
	step [29/188], loss=127.5358
	step [30/188], loss=118.9230
	step [31/188], loss=132.0078
	step [32/188], loss=126.5869
	step [33/188], loss=131.0951
	step [34/188], loss=98.7903
	step [35/188], loss=137.7031
	step [36/188], loss=121.1939
	step [37/188], loss=124.7840
	step [38/188], loss=119.5771
	step [39/188], loss=117.9142
	step [40/188], loss=112.5236
	step [41/188], loss=122.6881
	step [42/188], loss=114.2325
	step [43/188], loss=107.5800
	step [44/188], loss=118.9539
	step [45/188], loss=109.3171
	step [46/188], loss=117.1234
	step [47/188], loss=119.8320
	step [48/188], loss=114.7205
	step [49/188], loss=126.9803
	step [50/188], loss=129.5038
	step [51/188], loss=126.9511
	step [52/188], loss=115.5455
	step [53/188], loss=131.5175
	step [54/188], loss=122.4672
	step [55/188], loss=120.7675
	step [56/188], loss=120.8034
	step [57/188], loss=102.5713
	step [58/188], loss=124.2540
	step [59/188], loss=123.4766
	step [60/188], loss=125.5902
	step [61/188], loss=105.5619
	step [62/188], loss=109.8833
	step [63/188], loss=133.4380
	step [64/188], loss=126.6996
	step [65/188], loss=118.0466
	step [66/188], loss=115.8397
	step [67/188], loss=111.8566
	step [68/188], loss=127.1507
	step [69/188], loss=134.4038
	step [70/188], loss=112.0081
	step [71/188], loss=119.2531
	step [72/188], loss=113.6332
	step [73/188], loss=127.3459
	step [74/188], loss=128.9861
	step [75/188], loss=134.3432
	step [76/188], loss=123.0165
	step [77/188], loss=127.8807
	step [78/188], loss=135.3266
	step [79/188], loss=121.6099
	step [80/188], loss=115.1542
	step [81/188], loss=112.2396
	step [82/188], loss=130.6196
	step [83/188], loss=114.4963
	step [84/188], loss=131.7873
	step [85/188], loss=134.5297
	step [86/188], loss=135.4246
	step [87/188], loss=129.2253
	step [88/188], loss=132.2587
	step [89/188], loss=135.4103
	step [90/188], loss=123.4587
	step [91/188], loss=143.9624
	step [92/188], loss=104.9307
	step [93/188], loss=118.1806
	step [94/188], loss=98.1515
	step [95/188], loss=120.7744
	step [96/188], loss=120.5906
	step [97/188], loss=127.8860
	step [98/188], loss=122.4316
	step [99/188], loss=112.8036
	step [100/188], loss=111.4342
	step [101/188], loss=129.5412
	step [102/188], loss=131.0923
	step [103/188], loss=127.0335
	step [104/188], loss=99.4413
	step [105/188], loss=124.4336
	step [106/188], loss=126.1256
	step [107/188], loss=129.6657
	step [108/188], loss=124.6304
	step [109/188], loss=121.5223
	step [110/188], loss=109.9780
	step [111/188], loss=124.4999
	step [112/188], loss=106.1054
	step [113/188], loss=123.6096
	step [114/188], loss=123.5998
	step [115/188], loss=114.4264
	step [116/188], loss=115.6195
	step [117/188], loss=105.7893
	step [118/188], loss=129.2846
	step [119/188], loss=116.0070
	step [120/188], loss=119.2380
	step [121/188], loss=98.5995
	step [122/188], loss=133.3517
	step [123/188], loss=131.2956
	step [124/188], loss=127.7386
	step [125/188], loss=123.5620
	step [126/188], loss=116.4255
	step [127/188], loss=114.7922
	step [128/188], loss=109.7442
	step [129/188], loss=118.0196
	step [130/188], loss=123.5409
	step [131/188], loss=122.4088
	step [132/188], loss=126.2881
	step [133/188], loss=113.3811
	step [134/188], loss=115.7253
	step [135/188], loss=109.7505
	step [136/188], loss=110.3482
	step [137/188], loss=112.9265
	step [138/188], loss=109.9432
	step [139/188], loss=116.8250
	step [140/188], loss=121.6266
	step [141/188], loss=118.4052
	step [142/188], loss=111.2775
	step [143/188], loss=122.3638
	step [144/188], loss=107.2531
	step [145/188], loss=129.3072
	step [146/188], loss=106.3241
	step [147/188], loss=129.9082
	step [148/188], loss=121.6329
	step [149/188], loss=128.4771
	step [150/188], loss=115.5970
	step [151/188], loss=104.3907
	step [152/188], loss=122.0494
	step [153/188], loss=123.4488
	step [154/188], loss=117.9888
	step [155/188], loss=110.6651
	step [156/188], loss=115.1464
	step [157/188], loss=115.0783
	step [158/188], loss=124.0384
	step [159/188], loss=131.9435
	step [160/188], loss=134.1835
	step [161/188], loss=123.9374
	step [162/188], loss=116.2816
	step [163/188], loss=123.2460
	step [164/188], loss=145.4714
	step [165/188], loss=130.9695
	step [166/188], loss=115.2186
	step [167/188], loss=117.6014
	step [168/188], loss=114.3466
	step [169/188], loss=123.2956
	step [170/188], loss=127.3869
	step [171/188], loss=122.9854
	step [172/188], loss=125.9965
	step [173/188], loss=122.3682
	step [174/188], loss=130.6599
	step [175/188], loss=120.9589
	step [176/188], loss=122.5886
	step [177/188], loss=110.1169
	step [178/188], loss=108.8449
	step [179/188], loss=100.5513
	step [180/188], loss=137.4268
	step [181/188], loss=128.7935
	step [182/188], loss=104.2718
	step [183/188], loss=124.9484
	step [184/188], loss=129.6557
	step [185/188], loss=99.5188
	step [186/188], loss=122.1283
	step [187/188], loss=120.0207
	step [188/188], loss=72.8673
	Evaluating
	loss=0.0800, precision=0.3923, recall=0.9156, f1=0.5492
Training epoch 8
	step [1/188], loss=140.0413
	step [2/188], loss=116.5610
	step [3/188], loss=115.0689
	step [4/188], loss=104.9730
	step [5/188], loss=108.2068
	step [6/188], loss=106.8553
	step [7/188], loss=126.0307
	step [8/188], loss=98.9035
	step [9/188], loss=113.2587
	step [10/188], loss=118.4626
	step [11/188], loss=115.0481
	step [12/188], loss=112.3512
	step [13/188], loss=105.7666
	step [14/188], loss=109.6120
	step [15/188], loss=116.4461
	step [16/188], loss=122.8704
	step [17/188], loss=112.3667
	step [18/188], loss=127.6815
	step [19/188], loss=108.6529
	step [20/188], loss=116.9075
	step [21/188], loss=99.6259
	step [22/188], loss=106.6745
	step [23/188], loss=122.7993
	step [24/188], loss=116.6794
	step [25/188], loss=112.7216
	step [26/188], loss=122.5708
	step [27/188], loss=114.3716
	step [28/188], loss=121.6111
	step [29/188], loss=119.8528
	step [30/188], loss=117.2782
	step [31/188], loss=138.9427
	step [32/188], loss=106.4974
	step [33/188], loss=110.2549
	step [34/188], loss=120.2947
	step [35/188], loss=104.6638
	step [36/188], loss=121.8128
	step [37/188], loss=123.3408
	step [38/188], loss=115.9203
	step [39/188], loss=119.1073
	step [40/188], loss=111.9435
	step [41/188], loss=92.7802
	step [42/188], loss=130.7368
	step [43/188], loss=122.4311
	step [44/188], loss=127.7340
	step [45/188], loss=106.4490
	step [46/188], loss=131.3922
	step [47/188], loss=120.9964
	step [48/188], loss=107.7400
	step [49/188], loss=121.3020
	step [50/188], loss=119.4775
	step [51/188], loss=120.7299
	step [52/188], loss=108.3697
	step [53/188], loss=106.5029
	step [54/188], loss=110.1884
	step [55/188], loss=120.0257
	step [56/188], loss=96.0590
	step [57/188], loss=114.4501
	step [58/188], loss=105.7294
	step [59/188], loss=126.5319
	step [60/188], loss=104.8458
	step [61/188], loss=118.5821
	step [62/188], loss=124.7564
	step [63/188], loss=124.7361
	step [64/188], loss=111.2903
	step [65/188], loss=112.0073
	step [66/188], loss=114.1849
	step [67/188], loss=120.2891
	step [68/188], loss=121.9644
	step [69/188], loss=119.8925
	step [70/188], loss=114.0048
	step [71/188], loss=130.6219
	step [72/188], loss=117.5873
	step [73/188], loss=113.6193
	step [74/188], loss=114.8050
	step [75/188], loss=134.0521
	step [76/188], loss=109.8454
	step [77/188], loss=116.0840
	step [78/188], loss=84.4245
	step [79/188], loss=119.9488
	step [80/188], loss=109.8076
	step [81/188], loss=153.9121
	step [82/188], loss=105.2266
	step [83/188], loss=114.9483
	step [84/188], loss=113.0543
	step [85/188], loss=133.1568
	step [86/188], loss=128.3392
	step [87/188], loss=117.2272
	step [88/188], loss=132.1119
	step [89/188], loss=121.0601
	step [90/188], loss=109.8468
	step [91/188], loss=135.1333
	step [92/188], loss=125.7977
	step [93/188], loss=125.5795
	step [94/188], loss=122.2388
	step [95/188], loss=118.0228
	step [96/188], loss=106.0818
	step [97/188], loss=114.0390
	step [98/188], loss=113.4500
	step [99/188], loss=109.0756
	step [100/188], loss=110.9501
	step [101/188], loss=114.2601
	step [102/188], loss=113.8012
	step [103/188], loss=120.7643
	step [104/188], loss=123.3766
	step [105/188], loss=134.0881
	step [106/188], loss=114.7741
	step [107/188], loss=116.0108
	step [108/188], loss=130.8924
	step [109/188], loss=113.0346
	step [110/188], loss=126.7421
	step [111/188], loss=129.9318
	step [112/188], loss=121.2663
	step [113/188], loss=104.2152
	step [114/188], loss=144.0493
	step [115/188], loss=121.2035
	step [116/188], loss=117.4175
	step [117/188], loss=117.3414
	step [118/188], loss=107.9273
	step [119/188], loss=118.0365
	step [120/188], loss=119.4699
	step [121/188], loss=114.5903
	step [122/188], loss=119.0361
	step [123/188], loss=105.6097
	step [124/188], loss=119.7795
	step [125/188], loss=119.1993
	step [126/188], loss=117.7866
	step [127/188], loss=112.6731
	step [128/188], loss=121.7269
	step [129/188], loss=116.1575
	step [130/188], loss=110.8712
	step [131/188], loss=123.3090
	step [132/188], loss=96.5374
	step [133/188], loss=108.3485
	step [134/188], loss=122.5789
	step [135/188], loss=121.6779
	step [136/188], loss=105.6587
	step [137/188], loss=124.8763
	step [138/188], loss=113.5285
	step [139/188], loss=120.0570
	step [140/188], loss=115.7686
	step [141/188], loss=103.4390
	step [142/188], loss=134.7120
	step [143/188], loss=119.8792
	step [144/188], loss=114.8633
	step [145/188], loss=115.0293
	step [146/188], loss=109.2880
	step [147/188], loss=121.7782
	step [148/188], loss=113.9025
	step [149/188], loss=98.1454
	step [150/188], loss=132.4053
	step [151/188], loss=116.3714
	step [152/188], loss=110.2171
	step [153/188], loss=108.9530
	step [154/188], loss=106.9444
	step [155/188], loss=114.5948
	step [156/188], loss=127.1693
	step [157/188], loss=117.5464
	step [158/188], loss=125.2562
	step [159/188], loss=133.6893
	step [160/188], loss=103.0804
	step [161/188], loss=128.1411
	step [162/188], loss=117.1947
	step [163/188], loss=116.7818
	step [164/188], loss=114.0997
	step [165/188], loss=122.0912
	step [166/188], loss=110.8305
	step [167/188], loss=126.5464
	step [168/188], loss=109.6508
	step [169/188], loss=128.8469
	step [170/188], loss=106.8594
	step [171/188], loss=127.6468
	step [172/188], loss=121.9930
	step [173/188], loss=129.5882
	step [174/188], loss=117.0892
	step [175/188], loss=119.3455
	step [176/188], loss=119.4383
	step [177/188], loss=118.9671
	step [178/188], loss=116.3391
	step [179/188], loss=105.4603
	step [180/188], loss=118.9083
	step [181/188], loss=128.0883
	step [182/188], loss=110.0431
	step [183/188], loss=117.7169
	step [184/188], loss=108.7544
	step [185/188], loss=118.4579
	step [186/188], loss=118.4473
	step [187/188], loss=118.5758
	step [188/188], loss=67.0610
	Evaluating
	loss=0.0633, precision=0.4184, recall=0.9290, f1=0.5770
Training epoch 9
	step [1/188], loss=122.0926
	step [2/188], loss=123.2446
	step [3/188], loss=99.6966
	step [4/188], loss=110.3494
	step [5/188], loss=115.8060
	step [6/188], loss=128.8844
	step [7/188], loss=103.6797
	step [8/188], loss=128.1744
	step [9/188], loss=107.7345
	step [10/188], loss=97.1210
	step [11/188], loss=129.8252
	step [12/188], loss=120.0591
	step [13/188], loss=129.7052
	step [14/188], loss=117.5570
	step [15/188], loss=118.8107
	step [16/188], loss=126.4137
	step [17/188], loss=117.2955
	step [18/188], loss=106.5905
	step [19/188], loss=107.9725
	step [20/188], loss=112.8507
	step [21/188], loss=115.1312
	step [22/188], loss=123.3126
	step [23/188], loss=117.1409
	step [24/188], loss=103.2445
	step [25/188], loss=102.8150
	step [26/188], loss=109.4391
	step [27/188], loss=109.2792
	step [28/188], loss=105.7886
	step [29/188], loss=124.4501
	step [30/188], loss=114.9680
	step [31/188], loss=107.3524
	step [32/188], loss=135.9010
	step [33/188], loss=95.9363
	step [34/188], loss=93.9182
	step [35/188], loss=115.8367
	step [36/188], loss=138.0332
	step [37/188], loss=101.8468
	step [38/188], loss=123.3687
	step [39/188], loss=106.9897
	step [40/188], loss=113.6079
	step [41/188], loss=114.1569
	step [42/188], loss=117.3268
	step [43/188], loss=112.1311
	step [44/188], loss=120.3410
	step [45/188], loss=123.8790
	step [46/188], loss=110.6195
	step [47/188], loss=105.8929
	step [48/188], loss=109.3531
	step [49/188], loss=120.0978
	step [50/188], loss=101.6986
	step [51/188], loss=116.8852
	step [52/188], loss=110.7462
	step [53/188], loss=110.0551
	step [54/188], loss=111.2031
	step [55/188], loss=114.6839
	step [56/188], loss=112.0785
	step [57/188], loss=134.4859
	step [58/188], loss=130.2932
	step [59/188], loss=125.4227
	step [60/188], loss=120.3768
	step [61/188], loss=102.8207
	step [62/188], loss=97.3568
	step [63/188], loss=119.2404
	step [64/188], loss=120.9895
	step [65/188], loss=121.3190
	step [66/188], loss=115.6283
	step [67/188], loss=107.3023
	step [68/188], loss=115.3015
	step [69/188], loss=107.9498
	step [70/188], loss=121.8911
	step [71/188], loss=97.6097
	step [72/188], loss=104.0293
	step [73/188], loss=109.0896
	step [74/188], loss=110.1836
	step [75/188], loss=120.0840
	step [76/188], loss=121.2198
	step [77/188], loss=114.5618
	step [78/188], loss=110.2160
	step [79/188], loss=128.0638
	step [80/188], loss=99.3677
	step [81/188], loss=117.8211
	step [82/188], loss=112.8263
	step [83/188], loss=118.4066
	step [84/188], loss=108.6849
	step [85/188], loss=118.3660
	step [86/188], loss=114.3991
	step [87/188], loss=125.3102
	step [88/188], loss=117.7811
	step [89/188], loss=121.8426
	step [90/188], loss=110.1657
	step [91/188], loss=111.3617
	step [92/188], loss=121.5728
	step [93/188], loss=110.3709
	step [94/188], loss=117.9130
	step [95/188], loss=113.4560
	step [96/188], loss=118.6693
	step [97/188], loss=111.6322
	step [98/188], loss=131.0790
	step [99/188], loss=110.6902
	step [100/188], loss=100.3019
	step [101/188], loss=108.1028
	step [102/188], loss=125.7189
	step [103/188], loss=103.9304
	step [104/188], loss=109.4485
	step [105/188], loss=130.4165
	step [106/188], loss=112.2091
	step [107/188], loss=132.3480
	step [108/188], loss=117.1673
	step [109/188], loss=110.9284
	step [110/188], loss=104.6841
	step [111/188], loss=119.1271
	step [112/188], loss=96.3887
	step [113/188], loss=124.6742
	step [114/188], loss=127.5749
	step [115/188], loss=110.0390
	step [116/188], loss=120.9709
	step [117/188], loss=113.3577
	step [118/188], loss=107.4669
	step [119/188], loss=112.3330
	step [120/188], loss=115.2163
	step [121/188], loss=111.3155
	step [122/188], loss=121.7442
	step [123/188], loss=110.0771
	step [124/188], loss=112.3183
	step [125/188], loss=110.9482
	step [126/188], loss=109.0995
	step [127/188], loss=114.4267
	step [128/188], loss=102.5826
	step [129/188], loss=121.1582
	step [130/188], loss=118.9462
	step [131/188], loss=105.0780
	step [132/188], loss=119.4153
	step [133/188], loss=105.6633
	step [134/188], loss=111.1908
	step [135/188], loss=111.7557
	step [136/188], loss=107.1801
	step [137/188], loss=116.5208
	step [138/188], loss=116.3167
	step [139/188], loss=110.3125
	step [140/188], loss=97.1539
	step [141/188], loss=106.3218
	step [142/188], loss=108.6944
	step [143/188], loss=119.0070
	step [144/188], loss=106.3442
	step [145/188], loss=117.8774
	step [146/188], loss=111.2130
	step [147/188], loss=114.0126
	step [148/188], loss=117.0122
	step [149/188], loss=140.8425
	step [150/188], loss=105.7273
	step [151/188], loss=120.5879
	step [152/188], loss=107.3430
	step [153/188], loss=117.3457
	step [154/188], loss=129.4570
	step [155/188], loss=123.9993
	step [156/188], loss=116.6936
	step [157/188], loss=117.5046
	step [158/188], loss=119.7251
	step [159/188], loss=109.7111
	step [160/188], loss=134.4462
	step [161/188], loss=95.8902
	step [162/188], loss=136.3761
	step [163/188], loss=113.4430
	step [164/188], loss=113.7708
	step [165/188], loss=112.7000
	step [166/188], loss=124.7112
	step [167/188], loss=104.8072
	step [168/188], loss=107.7831
	step [169/188], loss=112.3242
	step [170/188], loss=108.8363
	step [171/188], loss=116.5250
	step [172/188], loss=114.0633
	step [173/188], loss=100.6030
	step [174/188], loss=114.2076
	step [175/188], loss=107.8306
	step [176/188], loss=121.5970
	step [177/188], loss=127.2895
	step [178/188], loss=108.0980
	step [179/188], loss=106.8038
	step [180/188], loss=117.1893
	step [181/188], loss=143.5675
	step [182/188], loss=104.4388
	step [183/188], loss=99.8666
	step [184/188], loss=108.4451
	step [185/188], loss=116.1978
	step [186/188], loss=104.9393
	step [187/188], loss=107.4688
	step [188/188], loss=73.1285
	Evaluating
	loss=0.0557, precision=0.4234, recall=0.9184, f1=0.5796
Training epoch 10
	step [1/188], loss=107.3602
	step [2/188], loss=113.4331
	step [3/188], loss=127.5444
	step [4/188], loss=110.0188
	step [5/188], loss=116.7842
	step [6/188], loss=125.3805
	step [7/188], loss=107.1313
	step [8/188], loss=107.0398
	step [9/188], loss=120.8093
	step [10/188], loss=93.9819
	step [11/188], loss=121.8011
	step [12/188], loss=114.8318
	step [13/188], loss=113.1696
	step [14/188], loss=102.3539
	step [15/188], loss=108.4633
	step [16/188], loss=113.2931
	step [17/188], loss=119.0015
	step [18/188], loss=110.0226
	step [19/188], loss=130.6561
	step [20/188], loss=118.3397
	step [21/188], loss=96.3375
	step [22/188], loss=108.6011
	step [23/188], loss=115.9119
	step [24/188], loss=130.3799
	step [25/188], loss=110.4313
	step [26/188], loss=134.3409
	step [27/188], loss=105.8841
	step [28/188], loss=130.8183
	step [29/188], loss=112.9834
	step [30/188], loss=113.5623
	step [31/188], loss=111.6402
	step [32/188], loss=126.2540
	step [33/188], loss=111.5540
	step [34/188], loss=115.6652
	step [35/188], loss=117.7300
	step [36/188], loss=116.7580
	step [37/188], loss=109.3459
	step [38/188], loss=96.9018
	step [39/188], loss=103.9368
	step [40/188], loss=94.2664
	step [41/188], loss=100.1464
	step [42/188], loss=105.2558
	step [43/188], loss=109.9880
	step [44/188], loss=101.3162
	step [45/188], loss=130.3164
	step [46/188], loss=106.2829
	step [47/188], loss=115.5509
	step [48/188], loss=108.2378
	step [49/188], loss=112.7195
	step [50/188], loss=119.2557
	step [51/188], loss=87.7098
	step [52/188], loss=125.5870
	step [53/188], loss=95.1714
	step [54/188], loss=103.6077
	step [55/188], loss=115.5643
	step [56/188], loss=110.9079
	step [57/188], loss=119.6135
	step [58/188], loss=119.4083
	step [59/188], loss=107.4034
	step [60/188], loss=110.0660
	step [61/188], loss=107.8114
	step [62/188], loss=132.8507
	step [63/188], loss=127.5056
	step [64/188], loss=100.6667
	step [65/188], loss=118.4287
	step [66/188], loss=122.0813
	step [67/188], loss=105.2743
	step [68/188], loss=100.9456
	step [69/188], loss=112.6605
	step [70/188], loss=109.3638
	step [71/188], loss=97.5936
	step [72/188], loss=125.1840
	step [73/188], loss=115.3359
	step [74/188], loss=114.0864
	step [75/188], loss=124.4538
	step [76/188], loss=102.1751
	step [77/188], loss=111.7337
	step [78/188], loss=104.1351
	step [79/188], loss=115.2121
	step [80/188], loss=108.6943
	step [81/188], loss=107.9028
	step [82/188], loss=109.7728
	step [83/188], loss=114.4078
	step [84/188], loss=102.3970
	step [85/188], loss=121.7169
	step [86/188], loss=120.3338
	step [87/188], loss=112.4389
	step [88/188], loss=113.4048
	step [89/188], loss=102.9192
	step [90/188], loss=119.3765
	step [91/188], loss=105.9803
	step [92/188], loss=114.8138
	step [93/188], loss=125.2755
	step [94/188], loss=100.9132
	step [95/188], loss=98.4044
	step [96/188], loss=105.9369
	step [97/188], loss=118.4944
	step [98/188], loss=116.1992
	step [99/188], loss=103.5407
	step [100/188], loss=110.7271
	step [101/188], loss=117.3372
	step [102/188], loss=113.8832
	step [103/188], loss=104.2447
	step [104/188], loss=117.6182
	step [105/188], loss=116.4409
	step [106/188], loss=112.7373
	step [107/188], loss=94.4763
	step [108/188], loss=110.3978
	step [109/188], loss=109.7698
	step [110/188], loss=128.3536
	step [111/188], loss=118.4140
	step [112/188], loss=105.1432
	step [113/188], loss=103.2951
	step [114/188], loss=100.1644
	step [115/188], loss=95.6147
	step [116/188], loss=115.6240
	step [117/188], loss=109.7956
	step [118/188], loss=111.6372
	step [119/188], loss=102.0840
	step [120/188], loss=101.6043
	step [121/188], loss=94.6312
	step [122/188], loss=108.6295
	step [123/188], loss=114.4283
	step [124/188], loss=119.5606
	step [125/188], loss=115.7776
	step [126/188], loss=113.3022
	step [127/188], loss=127.3076
	step [128/188], loss=115.0089
	step [129/188], loss=118.6943
	step [130/188], loss=115.0855
	step [131/188], loss=111.5768
	step [132/188], loss=103.0684
	step [133/188], loss=117.1562
	step [134/188], loss=96.5109
	step [135/188], loss=101.1957
	step [136/188], loss=112.3454
	step [137/188], loss=101.4379
	step [138/188], loss=142.1900
	step [139/188], loss=121.1645
	step [140/188], loss=118.6849
	step [141/188], loss=122.9773
	step [142/188], loss=108.6279
	step [143/188], loss=96.1021
	step [144/188], loss=115.1508
	step [145/188], loss=123.2384
	step [146/188], loss=113.5042
	step [147/188], loss=115.4131
	step [148/188], loss=110.5009
	step [149/188], loss=118.3579
	step [150/188], loss=126.6570
	step [151/188], loss=91.2379
	step [152/188], loss=102.4411
	step [153/188], loss=106.2258
	step [154/188], loss=125.6111
	step [155/188], loss=115.5074
	step [156/188], loss=111.1765
	step [157/188], loss=109.1264
	step [158/188], loss=106.9340
	step [159/188], loss=110.9710
	step [160/188], loss=101.2136
	step [161/188], loss=110.3443
	step [162/188], loss=104.6364
	step [163/188], loss=110.7777
	step [164/188], loss=107.6288
	step [165/188], loss=122.3154
	step [166/188], loss=120.3999
	step [167/188], loss=119.7202
	step [168/188], loss=106.8263
	step [169/188], loss=115.7974
	step [170/188], loss=121.1823
	step [171/188], loss=127.3766
	step [172/188], loss=125.2366
	step [173/188], loss=105.9233
	step [174/188], loss=111.0292
	step [175/188], loss=112.2183
	step [176/188], loss=90.2093
	step [177/188], loss=125.6707
	step [178/188], loss=115.7606
	step [179/188], loss=94.8075
	step [180/188], loss=108.1299
	step [181/188], loss=112.7625
	step [182/188], loss=126.1354
	step [183/188], loss=98.7738
	step [184/188], loss=109.3938
	step [185/188], loss=113.5153
	step [186/188], loss=116.7763
	step [187/188], loss=96.2677
	step [188/188], loss=73.9615
	Evaluating
	loss=0.0474, precision=0.4071, recall=0.9292, f1=0.5661
Training epoch 11
	step [1/188], loss=114.9567
	step [2/188], loss=108.0278
	step [3/188], loss=101.7990
	step [4/188], loss=107.0024
	step [5/188], loss=94.4383
	step [6/188], loss=114.2291
	step [7/188], loss=110.5886
	step [8/188], loss=102.1979
	step [9/188], loss=111.1227
	step [10/188], loss=111.4090
	step [11/188], loss=116.5822
	step [12/188], loss=97.8940
	step [13/188], loss=93.6500
	step [14/188], loss=101.8379
	step [15/188], loss=107.7956
	step [16/188], loss=81.3239
	step [17/188], loss=119.3183
	step [18/188], loss=116.7684
	step [19/188], loss=100.1765
	step [20/188], loss=105.7255
	step [21/188], loss=123.0904
	step [22/188], loss=116.5597
	step [23/188], loss=126.7535
	step [24/188], loss=112.2871
	step [25/188], loss=110.4476
	step [26/188], loss=116.0267
	step [27/188], loss=112.5328
	step [28/188], loss=119.7550
	step [29/188], loss=107.1553
	step [30/188], loss=105.8846
	step [31/188], loss=129.1863
	step [32/188], loss=112.3691
	step [33/188], loss=108.2631
	step [34/188], loss=108.3764
	step [35/188], loss=103.4769
	step [36/188], loss=124.7356
	step [37/188], loss=99.9982
	step [38/188], loss=131.1318
	step [39/188], loss=102.8069
	step [40/188], loss=99.5746
	step [41/188], loss=115.9690
	step [42/188], loss=97.6592
	step [43/188], loss=109.7882
	step [44/188], loss=120.7483
	step [45/188], loss=114.5393
	step [46/188], loss=114.7727
	step [47/188], loss=106.1501
	step [48/188], loss=114.2004
	step [49/188], loss=115.1240
	step [50/188], loss=111.4983
	step [51/188], loss=127.9067
	step [52/188], loss=107.1656
	step [53/188], loss=122.8289
	step [54/188], loss=92.2401
	step [55/188], loss=94.1733
	step [56/188], loss=118.6933
	step [57/188], loss=98.3982
	step [58/188], loss=116.6956
	step [59/188], loss=119.7867
	step [60/188], loss=103.8545
	step [61/188], loss=100.2390
	step [62/188], loss=107.0271
	step [63/188], loss=116.4701
	step [64/188], loss=118.4040
	step [65/188], loss=91.0290
	step [66/188], loss=94.7924
	step [67/188], loss=108.5842
	step [68/188], loss=105.4403
	step [69/188], loss=101.8879
	step [70/188], loss=100.6746
	step [71/188], loss=104.9198
	step [72/188], loss=114.9909
	step [73/188], loss=106.8260
	step [74/188], loss=121.5779
	step [75/188], loss=112.5744
	step [76/188], loss=98.5890
	step [77/188], loss=98.7755
	step [78/188], loss=101.1433
	step [79/188], loss=102.8451
	step [80/188], loss=103.4339
	step [81/188], loss=101.9349
	step [82/188], loss=124.1060
	step [83/188], loss=118.0711
	step [84/188], loss=113.9886
	step [85/188], loss=117.3215
	step [86/188], loss=121.0126
	step [87/188], loss=118.6140
	step [88/188], loss=94.5409
	step [89/188], loss=95.1093
	step [90/188], loss=106.8018
	step [91/188], loss=108.2159
	step [92/188], loss=107.4459
	step [93/188], loss=96.5787
	step [94/188], loss=104.7087
	step [95/188], loss=104.1433
	step [96/188], loss=105.4612
	step [97/188], loss=131.5961
	step [98/188], loss=100.5739
	step [99/188], loss=104.9778
	step [100/188], loss=108.5955
	step [101/188], loss=92.6509
	step [102/188], loss=111.1871
	step [103/188], loss=115.8650
	step [104/188], loss=126.8464
	step [105/188], loss=111.0576
	step [106/188], loss=97.6114
	step [107/188], loss=108.3089
	step [108/188], loss=121.6569
	step [109/188], loss=119.8539
	step [110/188], loss=97.6554
	step [111/188], loss=100.9060
	step [112/188], loss=117.6867
	step [113/188], loss=117.3065
	step [114/188], loss=109.5978
	step [115/188], loss=117.6526
	step [116/188], loss=104.5967
	step [117/188], loss=101.3297
	step [118/188], loss=89.4648
	step [119/188], loss=115.2008
	step [120/188], loss=109.9544
	step [121/188], loss=109.1007
	step [122/188], loss=107.3762
	step [123/188], loss=111.3525
	step [124/188], loss=116.2087
	step [125/188], loss=118.6461
	step [126/188], loss=91.1284
	step [127/188], loss=111.4359
	step [128/188], loss=115.2966
	step [129/188], loss=111.0704
	step [130/188], loss=124.3530
	step [131/188], loss=122.3176
	step [132/188], loss=111.9248
	step [133/188], loss=110.7472
	step [134/188], loss=110.2028
	step [135/188], loss=106.5826
	step [136/188], loss=115.3651
	step [137/188], loss=100.6242
	step [138/188], loss=118.0170
	step [139/188], loss=126.3198
	step [140/188], loss=113.6933
	step [141/188], loss=98.3790
	step [142/188], loss=103.8845
	step [143/188], loss=117.5839
	step [144/188], loss=106.6597
	step [145/188], loss=116.3272
	step [146/188], loss=116.2979
	step [147/188], loss=110.2092
	step [148/188], loss=99.9079
	step [149/188], loss=102.2446
	step [150/188], loss=124.1908
	step [151/188], loss=104.1153
	step [152/188], loss=99.1199
	step [153/188], loss=114.0318
	step [154/188], loss=115.2183
	step [155/188], loss=123.6233
	step [156/188], loss=96.6868
	step [157/188], loss=130.1172
	step [158/188], loss=108.1790
	step [159/188], loss=100.6637
	step [160/188], loss=102.6582
	step [161/188], loss=114.3536
	step [162/188], loss=99.8869
	step [163/188], loss=114.9480
	step [164/188], loss=99.6235
	step [165/188], loss=122.7234
	step [166/188], loss=97.0434
	step [167/188], loss=109.4127
	step [168/188], loss=91.2312
	step [169/188], loss=106.9314
	step [170/188], loss=100.7836
	step [171/188], loss=102.3636
	step [172/188], loss=107.0232
	step [173/188], loss=113.0452
	step [174/188], loss=110.1017
	step [175/188], loss=116.0736
	step [176/188], loss=115.8257
	step [177/188], loss=117.1429
	step [178/188], loss=103.7759
	step [179/188], loss=112.5262
	step [180/188], loss=109.8387
	step [181/188], loss=118.8086
	step [182/188], loss=132.6954
	step [183/188], loss=122.4338
	step [184/188], loss=113.8714
	step [185/188], loss=119.1922
	step [186/188], loss=113.1691
	step [187/188], loss=118.1207
	step [188/188], loss=74.2561
	Evaluating
	loss=0.0399, precision=0.3742, recall=0.9270, f1=0.5332
Training epoch 12
	step [1/188], loss=103.0757
	step [2/188], loss=113.4610
	step [3/188], loss=108.1379
	step [4/188], loss=112.7385
	step [5/188], loss=96.8529
	step [6/188], loss=109.5889
	step [7/188], loss=101.9998
	step [8/188], loss=104.1772
	step [9/188], loss=114.0771
	step [10/188], loss=126.1538
	step [11/188], loss=105.4102
	step [12/188], loss=114.7543
	step [13/188], loss=105.6716
	step [14/188], loss=94.0995
	step [15/188], loss=99.2014
	step [16/188], loss=104.0388
	step [17/188], loss=114.8458
	step [18/188], loss=88.7644
	step [19/188], loss=102.2647
	step [20/188], loss=111.0480
	step [21/188], loss=107.8461
	step [22/188], loss=116.0104
	step [23/188], loss=112.3017
	step [24/188], loss=110.8134
	step [25/188], loss=124.0139
	step [26/188], loss=115.4014
	step [27/188], loss=108.2935
	step [28/188], loss=98.3928
	step [29/188], loss=110.0309
	step [30/188], loss=114.6324
	step [31/188], loss=116.1915
	step [32/188], loss=98.2451
	step [33/188], loss=87.6788
	step [34/188], loss=126.1640
	step [35/188], loss=118.7810
	step [36/188], loss=104.6614
	step [37/188], loss=106.3333
	step [38/188], loss=109.5062
	step [39/188], loss=102.5500
	step [40/188], loss=101.9627
	step [41/188], loss=122.7827
	step [42/188], loss=126.3051
	step [43/188], loss=97.2209
	step [44/188], loss=107.9911
	step [45/188], loss=120.4670
	step [46/188], loss=122.9623
	step [47/188], loss=108.4857
	step [48/188], loss=126.8651
	step [49/188], loss=115.4814
	step [50/188], loss=115.0381
	step [51/188], loss=99.1847
	step [52/188], loss=96.9636
	step [53/188], loss=112.4273
	step [54/188], loss=103.7423
	step [55/188], loss=113.2953
	step [56/188], loss=107.4109
	step [57/188], loss=121.7724
	step [58/188], loss=106.2634
	step [59/188], loss=96.3882
	step [60/188], loss=96.2863
	step [61/188], loss=110.9794
	step [62/188], loss=111.0085
	step [63/188], loss=98.9074
	step [64/188], loss=93.4649
	step [65/188], loss=107.2575
	step [66/188], loss=112.3346
	step [67/188], loss=129.0316
	step [68/188], loss=99.9411
	step [69/188], loss=123.2866
	step [70/188], loss=124.0614
	step [71/188], loss=104.6346
	step [72/188], loss=118.4170
	step [73/188], loss=101.8623
	step [74/188], loss=115.0285
	step [75/188], loss=93.7008
	step [76/188], loss=111.9564
	step [77/188], loss=121.3485
	step [78/188], loss=93.1273
	step [79/188], loss=96.8503
	step [80/188], loss=110.7585
	step [81/188], loss=110.2728
	step [82/188], loss=120.6181
	step [83/188], loss=95.3642
	step [84/188], loss=97.8115
	step [85/188], loss=92.9442
	step [86/188], loss=117.9427
	step [87/188], loss=117.7935
	step [88/188], loss=117.3398
	step [89/188], loss=115.1405
	step [90/188], loss=104.8713
	step [91/188], loss=99.8454
	step [92/188], loss=117.9945
	step [93/188], loss=106.7465
	step [94/188], loss=83.9709
	step [95/188], loss=107.1285
	step [96/188], loss=108.6002
	step [97/188], loss=122.7889
	step [98/188], loss=98.1629
	step [99/188], loss=103.1223
	step [100/188], loss=98.9028
	step [101/188], loss=94.9904
	step [102/188], loss=98.5799
	step [103/188], loss=115.6292
	step [104/188], loss=107.0764
	step [105/188], loss=105.3665
	step [106/188], loss=95.6747
	step [107/188], loss=112.9054
	step [108/188], loss=96.4589
	step [109/188], loss=106.8357
	step [110/188], loss=121.5412
	step [111/188], loss=106.8713
	step [112/188], loss=119.7630
	step [113/188], loss=87.8220
	step [114/188], loss=117.8286
	step [115/188], loss=96.1684
	step [116/188], loss=115.9441
	step [117/188], loss=125.4508
	step [118/188], loss=109.1877
	step [119/188], loss=97.0822
	step [120/188], loss=99.8581
	step [121/188], loss=112.9470
	step [122/188], loss=112.6333
	step [123/188], loss=108.8577
	step [124/188], loss=100.0620
	step [125/188], loss=101.5341
	step [126/188], loss=104.1055
	step [127/188], loss=95.2793
	step [128/188], loss=100.4283
	step [129/188], loss=125.4171
	step [130/188], loss=111.2267
	step [131/188], loss=119.4946
	step [132/188], loss=116.4214
	step [133/188], loss=92.6868
	step [134/188], loss=119.5328
	step [135/188], loss=117.1082
	step [136/188], loss=119.0553
	step [137/188], loss=101.2694
	step [138/188], loss=103.4285
	step [139/188], loss=108.0372
	step [140/188], loss=107.5956
	step [141/188], loss=109.9161
	step [142/188], loss=107.6850
	step [143/188], loss=111.6644
	step [144/188], loss=116.9833
	step [145/188], loss=103.2292
	step [146/188], loss=116.6760
	step [147/188], loss=103.9981
	step [148/188], loss=107.9212
	step [149/188], loss=113.6459
	step [150/188], loss=112.0807
	step [151/188], loss=108.5102
	step [152/188], loss=100.0193
	step [153/188], loss=107.1755
	step [154/188], loss=96.9024
	step [155/188], loss=101.0125
	step [156/188], loss=98.6505
	step [157/188], loss=129.6638
	step [158/188], loss=111.3130
	step [159/188], loss=121.1200
	step [160/188], loss=104.9827
	step [161/188], loss=120.7977
	step [162/188], loss=102.7010
	step [163/188], loss=111.4205
	step [164/188], loss=130.9299
	step [165/188], loss=99.9379
	step [166/188], loss=94.3540
	step [167/188], loss=97.4061
	step [168/188], loss=102.5832
	step [169/188], loss=115.7732
	step [170/188], loss=109.6245
	step [171/188], loss=118.8149
	step [172/188], loss=115.8418
	step [173/188], loss=90.0651
	step [174/188], loss=110.1919
	step [175/188], loss=101.3649
	step [176/188], loss=104.0201
	step [177/188], loss=94.2894
	step [178/188], loss=96.2387
	step [179/188], loss=122.9772
	step [180/188], loss=110.4256
	step [181/188], loss=105.0675
	step [182/188], loss=100.4400
	step [183/188], loss=118.0951
	step [184/188], loss=122.5621
	step [185/188], loss=100.6365
	step [186/188], loss=113.1374
	step [187/188], loss=95.5958
	step [188/188], loss=55.6065
	Evaluating
	loss=0.0356, precision=0.3911, recall=0.9424, f1=0.5528
Training epoch 13
	step [1/188], loss=123.1553
	step [2/188], loss=117.1221
	step [3/188], loss=106.9196
	step [4/188], loss=110.5611
	step [5/188], loss=104.6984
	step [6/188], loss=125.3056
	step [7/188], loss=105.4764
	step [8/188], loss=118.7113
	step [9/188], loss=107.5082
	step [10/188], loss=110.7894
	step [11/188], loss=106.9628
	step [12/188], loss=95.4908
	step [13/188], loss=103.6568
	step [14/188], loss=101.9289
	step [15/188], loss=90.9279
	step [16/188], loss=105.3028
	step [17/188], loss=116.8825
	step [18/188], loss=102.2820
	step [19/188], loss=91.9610
	step [20/188], loss=113.7946
	step [21/188], loss=117.1590
	step [22/188], loss=124.3654
	step [23/188], loss=101.9637
	step [24/188], loss=110.9285
	step [25/188], loss=106.0434
	step [26/188], loss=103.2818
	step [27/188], loss=88.5694
	step [28/188], loss=93.0690
	step [29/188], loss=114.5928
	step [30/188], loss=112.8310
	step [31/188], loss=115.4351
	step [32/188], loss=106.8238
	step [33/188], loss=96.4444
	step [34/188], loss=95.4230
	step [35/188], loss=101.0567
	step [36/188], loss=107.4143
	step [37/188], loss=113.4140
	step [38/188], loss=107.4391
	step [39/188], loss=95.2469
	step [40/188], loss=101.2941
	step [41/188], loss=122.8233
	step [42/188], loss=109.6637
	step [43/188], loss=113.1177
	step [44/188], loss=120.2436
	step [45/188], loss=109.2697
	step [46/188], loss=103.2930
	step [47/188], loss=108.8145
	step [48/188], loss=110.9385
	step [49/188], loss=101.0707
	step [50/188], loss=106.5005
	step [51/188], loss=109.5142
	step [52/188], loss=107.4547
	step [53/188], loss=100.0075
	step [54/188], loss=109.4731
	step [55/188], loss=119.6765
	step [56/188], loss=121.0636
	step [57/188], loss=116.0563
	step [58/188], loss=131.8683
	step [59/188], loss=110.8314
	step [60/188], loss=118.3774
	step [61/188], loss=102.0314
	step [62/188], loss=107.4206
	step [63/188], loss=91.8221
	step [64/188], loss=100.2622
	step [65/188], loss=121.0306
	step [66/188], loss=106.4703
	step [67/188], loss=103.4806
	step [68/188], loss=110.2591
	step [69/188], loss=110.1672
	step [70/188], loss=100.0484
	step [71/188], loss=102.2389
	step [72/188], loss=96.2088
	step [73/188], loss=96.4652
	step [74/188], loss=111.5653
	step [75/188], loss=108.4727
	step [76/188], loss=91.8092
	step [77/188], loss=98.6341
	step [78/188], loss=104.6359
	step [79/188], loss=111.3330
	step [80/188], loss=94.2693
	step [81/188], loss=127.4782
	step [82/188], loss=96.0067
	step [83/188], loss=130.2490
	step [84/188], loss=113.8251
	step [85/188], loss=124.7303
	step [86/188], loss=94.2167
	step [87/188], loss=93.5185
	step [88/188], loss=113.2635
	step [89/188], loss=102.4892
	step [90/188], loss=92.6162
	step [91/188], loss=116.5606
	step [92/188], loss=95.4080
	step [93/188], loss=111.7098
	step [94/188], loss=87.4841
	step [95/188], loss=107.7713
	step [96/188], loss=122.3465
	step [97/188], loss=122.6456
	step [98/188], loss=110.5761
	step [99/188], loss=102.9047
	step [100/188], loss=87.8688
	step [101/188], loss=104.3848
	step [102/188], loss=97.5598
	step [103/188], loss=96.4197
	step [104/188], loss=106.2269
	step [105/188], loss=97.4167
	step [106/188], loss=109.6408
	step [107/188], loss=113.2193
	step [108/188], loss=100.6897
	step [109/188], loss=93.4616
	step [110/188], loss=117.8441
	step [111/188], loss=112.7593
	step [112/188], loss=124.4014
	step [113/188], loss=128.0643
	step [114/188], loss=100.3658
	step [115/188], loss=97.8911
	step [116/188], loss=97.6263
	step [117/188], loss=95.9646
	step [118/188], loss=95.8314
	step [119/188], loss=101.3550
	step [120/188], loss=87.1853
	step [121/188], loss=116.8398
	step [122/188], loss=98.3830
	step [123/188], loss=103.8928
	step [124/188], loss=97.0573
	step [125/188], loss=105.2429
	step [126/188], loss=112.2882
	step [127/188], loss=107.1506
	step [128/188], loss=92.4714
	step [129/188], loss=101.3014
	step [130/188], loss=120.1799
	step [131/188], loss=107.4190
	step [132/188], loss=88.6541
	step [133/188], loss=96.1902
	step [134/188], loss=109.6973
	step [135/188], loss=113.6950
	step [136/188], loss=94.9868
	step [137/188], loss=101.7001
	step [138/188], loss=89.3875
	step [139/188], loss=106.1792
	step [140/188], loss=92.1097
	step [141/188], loss=86.3535
	step [142/188], loss=104.5968
	step [143/188], loss=106.2338
	step [144/188], loss=107.8075
	step [145/188], loss=93.3571
	step [146/188], loss=97.8110
	step [147/188], loss=102.9273
	step [148/188], loss=120.0624
	step [149/188], loss=109.8171
	step [150/188], loss=109.3080
	step [151/188], loss=106.4014
	step [152/188], loss=95.7005
	step [153/188], loss=117.3494
	step [154/188], loss=115.9125
	step [155/188], loss=109.6098
	step [156/188], loss=106.5550
	step [157/188], loss=96.2533
	step [158/188], loss=108.4252
	step [159/188], loss=101.5152
	step [160/188], loss=122.8958
	step [161/188], loss=111.9655
	step [162/188], loss=111.3445
	step [163/188], loss=105.4331
	step [164/188], loss=111.1500
	step [165/188], loss=115.3785
	step [166/188], loss=131.1104
	step [167/188], loss=106.0964
	step [168/188], loss=106.3397
	step [169/188], loss=118.8882
	step [170/188], loss=115.0090
	step [171/188], loss=107.7040
	step [172/188], loss=103.5454
	step [173/188], loss=99.6592
	step [174/188], loss=115.4055
	step [175/188], loss=102.6928
	step [176/188], loss=104.4824
	step [177/188], loss=126.5146
	step [178/188], loss=99.1564
	step [179/188], loss=107.1184
	step [180/188], loss=105.0631
	step [181/188], loss=108.3511
	step [182/188], loss=100.6765
	step [183/188], loss=112.3747
	step [184/188], loss=107.6033
	step [185/188], loss=114.0624
	step [186/188], loss=105.4724
	step [187/188], loss=105.2883
	step [188/188], loss=64.3772
	Evaluating
	loss=0.0354, precision=0.3407, recall=0.9159, f1=0.4967
Training epoch 14
	step [1/188], loss=103.8204
	step [2/188], loss=115.7766
	step [3/188], loss=102.7197
	step [4/188], loss=104.2445
	step [5/188], loss=117.7651
	step [6/188], loss=114.8954
	step [7/188], loss=102.6395
	step [8/188], loss=107.7724
	step [9/188], loss=89.8798
	step [10/188], loss=113.3144
	step [11/188], loss=110.0806
	step [12/188], loss=86.1361
	step [13/188], loss=100.1457
	step [14/188], loss=106.7725
	step [15/188], loss=98.6722
	step [16/188], loss=125.0451
	step [17/188], loss=104.9926
	step [18/188], loss=108.8604
	step [19/188], loss=86.9552
	step [20/188], loss=99.4000
	step [21/188], loss=108.2161
	step [22/188], loss=108.5842
	step [23/188], loss=99.5657
	step [24/188], loss=107.7411
	step [25/188], loss=106.9763
	step [26/188], loss=101.4533
	step [27/188], loss=101.2940
	step [28/188], loss=130.5304
	step [29/188], loss=101.3882
	step [30/188], loss=97.0065
	step [31/188], loss=104.8360
	step [32/188], loss=112.2317
	step [33/188], loss=105.4535
	step [34/188], loss=118.5925
	step [35/188], loss=102.3616
	step [36/188], loss=105.5821
	step [37/188], loss=105.4281
	step [38/188], loss=111.2216
	step [39/188], loss=108.8237
	step [40/188], loss=115.4112
	step [41/188], loss=98.5425
	step [42/188], loss=107.5813
	step [43/188], loss=91.1755
	step [44/188], loss=111.6637
	step [45/188], loss=104.1296
	step [46/188], loss=95.1347
	step [47/188], loss=115.7894
	step [48/188], loss=96.7055
	step [49/188], loss=111.7318
	step [50/188], loss=96.9123
	step [51/188], loss=109.6184
	step [52/188], loss=113.6418
	step [53/188], loss=97.9547
	step [54/188], loss=126.3193
	step [55/188], loss=98.9486
	step [56/188], loss=97.4490
	step [57/188], loss=111.1191
	step [58/188], loss=118.8266
	step [59/188], loss=123.6408
	step [60/188], loss=93.5922
	step [61/188], loss=106.2000
	step [62/188], loss=107.8294
	step [63/188], loss=97.6923
	step [64/188], loss=98.4951
	step [65/188], loss=95.5501
	step [66/188], loss=127.2319
	step [67/188], loss=94.6377
	step [68/188], loss=112.5619
	step [69/188], loss=117.3164
	step [70/188], loss=108.6752
	step [71/188], loss=112.0742
	step [72/188], loss=108.5703
	step [73/188], loss=104.0119
	step [74/188], loss=87.4400
	step [75/188], loss=97.0486
	step [76/188], loss=98.5271
	step [77/188], loss=119.8525
	step [78/188], loss=109.2391
	step [79/188], loss=117.1054
	step [80/188], loss=96.1634
	step [81/188], loss=95.3222
	step [82/188], loss=115.3136
	step [83/188], loss=114.0403
	step [84/188], loss=110.4688
	step [85/188], loss=104.5673
	step [86/188], loss=101.6634
	step [87/188], loss=103.3890
	step [88/188], loss=97.2851
	step [89/188], loss=108.7655
	step [90/188], loss=97.2117
	step [91/188], loss=97.1532
	step [92/188], loss=108.8379
	step [93/188], loss=111.1316
	step [94/188], loss=117.2103
	step [95/188], loss=94.9744
	step [96/188], loss=102.1602
	step [97/188], loss=108.4144
	step [98/188], loss=116.7484
	step [99/188], loss=115.1573
	step [100/188], loss=105.1394
	step [101/188], loss=101.1068
	step [102/188], loss=104.1392
	step [103/188], loss=107.4515
	step [104/188], loss=99.5060
	step [105/188], loss=90.4771
	step [106/188], loss=113.5526
	step [107/188], loss=93.0370
	step [108/188], loss=91.1417
	step [109/188], loss=110.7420
	step [110/188], loss=104.0217
	step [111/188], loss=114.2539
	step [112/188], loss=107.1350
	step [113/188], loss=95.2277
	step [114/188], loss=110.6872
	step [115/188], loss=100.3156
	step [116/188], loss=91.9225
	step [117/188], loss=93.8565
	step [118/188], loss=123.7068
	step [119/188], loss=115.6279
	step [120/188], loss=102.9645
	step [121/188], loss=115.5081
	step [122/188], loss=99.2883
	step [123/188], loss=111.3483
	step [124/188], loss=96.8857
	step [125/188], loss=109.0196
	step [126/188], loss=79.6117
	step [127/188], loss=97.5668
	step [128/188], loss=106.8870
	step [129/188], loss=100.4040
	step [130/188], loss=101.2845
	step [131/188], loss=114.8844
	step [132/188], loss=103.3282
	step [133/188], loss=95.9323
	step [134/188], loss=118.3601
	step [135/188], loss=116.4070
	step [136/188], loss=107.5982
	step [137/188], loss=102.7496
	step [138/188], loss=99.4954
	step [139/188], loss=126.8092
	step [140/188], loss=112.2633
	step [141/188], loss=90.7359
	step [142/188], loss=106.8398
	step [143/188], loss=103.5215
	step [144/188], loss=89.8132
	step [145/188], loss=104.7923
	step [146/188], loss=95.9909
	step [147/188], loss=109.0373
	step [148/188], loss=96.0406
	step [149/188], loss=91.7770
	step [150/188], loss=92.1767
	step [151/188], loss=112.1944
	step [152/188], loss=94.5212
	step [153/188], loss=102.5965
	step [154/188], loss=89.1587
	step [155/188], loss=97.9906
	step [156/188], loss=116.7420
	step [157/188], loss=101.3112
	step [158/188], loss=103.3923
	step [159/188], loss=109.5113
	step [160/188], loss=99.9510
	step [161/188], loss=108.5840
	step [162/188], loss=105.2038
	step [163/188], loss=111.1147
	step [164/188], loss=95.6697
	step [165/188], loss=102.5213
	step [166/188], loss=108.6950
	step [167/188], loss=104.2395
	step [168/188], loss=115.7241
	step [169/188], loss=99.3999
	step [170/188], loss=97.3797
	step [171/188], loss=105.3018
	step [172/188], loss=125.7085
	step [173/188], loss=100.0141
	step [174/188], loss=113.6371
	step [175/188], loss=97.4513
	step [176/188], loss=102.1349
	step [177/188], loss=99.6262
	step [178/188], loss=99.4693
	step [179/188], loss=95.1031
	step [180/188], loss=105.6448
	step [181/188], loss=99.9677
	step [182/188], loss=100.7025
	step [183/188], loss=99.1870
	step [184/188], loss=87.5278
	step [185/188], loss=122.8448
	step [186/188], loss=120.0829
	step [187/188], loss=104.8464
	step [188/188], loss=66.6726
	Evaluating
	loss=0.0337, precision=0.3302, recall=0.9220, f1=0.4863
Training epoch 15
	step [1/188], loss=100.8585
	step [2/188], loss=97.4932
	step [3/188], loss=114.5725
	step [4/188], loss=107.6428
	step [5/188], loss=108.6968
	step [6/188], loss=107.2169
	step [7/188], loss=94.5231
	step [8/188], loss=122.4888
	step [9/188], loss=108.8994
	step [10/188], loss=117.2586
	step [11/188], loss=101.5064
	step [12/188], loss=100.6396
	step [13/188], loss=110.3774
	step [14/188], loss=114.0633
	step [15/188], loss=104.0500
	step [16/188], loss=97.1546
	step [17/188], loss=105.0780
	step [18/188], loss=109.0973
	step [19/188], loss=109.4117
	step [20/188], loss=89.1118
	step [21/188], loss=104.8310
	step [22/188], loss=105.3181
	step [23/188], loss=96.9605
	step [24/188], loss=103.0784
	step [25/188], loss=97.9375
	step [26/188], loss=102.3126
	step [27/188], loss=110.7367
	step [28/188], loss=93.2373
	step [29/188], loss=107.9727
	step [30/188], loss=117.0400
	step [31/188], loss=90.6479
	step [32/188], loss=99.3512
	step [33/188], loss=109.3798
	step [34/188], loss=113.5007
	step [35/188], loss=90.9226
	step [36/188], loss=106.5290
	step [37/188], loss=111.9701
	step [38/188], loss=104.4176
	step [39/188], loss=105.1891
	step [40/188], loss=108.2305
	step [41/188], loss=101.9167
	step [42/188], loss=100.6855
	step [43/188], loss=86.2810
	step [44/188], loss=106.3116
	step [45/188], loss=110.0853
	step [46/188], loss=112.6830
	step [47/188], loss=108.8024
	step [48/188], loss=126.2551
	step [49/188], loss=113.9097
	step [50/188], loss=110.9220
	step [51/188], loss=93.2109
	step [52/188], loss=101.1250
	step [53/188], loss=106.2479
	step [54/188], loss=110.8291
	step [55/188], loss=101.5376
	step [56/188], loss=110.2635
	step [57/188], loss=91.4302
	step [58/188], loss=101.8435
	step [59/188], loss=92.1165
	step [60/188], loss=112.0462
	step [61/188], loss=93.6021
	step [62/188], loss=95.8602
	step [63/188], loss=98.3337
	step [64/188], loss=94.6769
	step [65/188], loss=92.7038
	step [66/188], loss=104.9797
	step [67/188], loss=122.6193
	step [68/188], loss=98.4870
	step [69/188], loss=99.8934
	step [70/188], loss=123.0761
	step [71/188], loss=98.9359
	step [72/188], loss=106.7520
	step [73/188], loss=106.0118
	step [74/188], loss=114.5604
	step [75/188], loss=106.7142
	step [76/188], loss=94.6865
	step [77/188], loss=99.0116
	step [78/188], loss=119.9799
	step [79/188], loss=103.5424
	step [80/188], loss=108.4330
	step [81/188], loss=91.4356
	step [82/188], loss=105.6663
	step [83/188], loss=112.2296
	step [84/188], loss=82.4806
	step [85/188], loss=117.0170
	step [86/188], loss=100.9471
	step [87/188], loss=112.4082
	step [88/188], loss=109.0948
	step [89/188], loss=124.9999
	step [90/188], loss=98.4396
	step [91/188], loss=99.8863
	step [92/188], loss=115.4570
	step [93/188], loss=93.4176
	step [94/188], loss=105.8073
	step [95/188], loss=83.7386
	step [96/188], loss=102.3873
	step [97/188], loss=117.7878
	step [98/188], loss=103.8959
	step [99/188], loss=115.7643
	step [100/188], loss=89.9487
	step [101/188], loss=93.9429
	step [102/188], loss=106.3216
	step [103/188], loss=115.5996
	step [104/188], loss=93.9804
	step [105/188], loss=96.4992
	step [106/188], loss=109.4889
	step [107/188], loss=89.0565
	step [108/188], loss=87.6908
	step [109/188], loss=100.3019
	step [110/188], loss=111.9942
	step [111/188], loss=106.2892
	step [112/188], loss=93.8516
	step [113/188], loss=94.3674
	step [114/188], loss=99.5785
	step [115/188], loss=83.0626
	step [116/188], loss=96.5585
	step [117/188], loss=113.7982
	step [118/188], loss=118.3776
	step [119/188], loss=91.3475
	step [120/188], loss=98.4246
	step [121/188], loss=106.8630
	step [122/188], loss=101.5730
	step [123/188], loss=95.7170
	step [124/188], loss=96.7354
	step [125/188], loss=95.6795
	step [126/188], loss=111.4106
	step [127/188], loss=111.9510
	step [128/188], loss=123.0817
	step [129/188], loss=108.7831
	step [130/188], loss=93.8891
	step [131/188], loss=89.0367
	step [132/188], loss=108.6730
	step [133/188], loss=105.1565
	step [134/188], loss=91.3407
	step [135/188], loss=104.4478
	step [136/188], loss=114.6940
	step [137/188], loss=103.8009
	step [138/188], loss=115.5960
	step [139/188], loss=102.3042
	step [140/188], loss=112.6434
	step [141/188], loss=104.6357
	step [142/188], loss=109.0692
	step [143/188], loss=109.2766
	step [144/188], loss=104.1506
	step [145/188], loss=98.7298
	step [146/188], loss=99.6302
	step [147/188], loss=113.4958
	step [148/188], loss=85.5249
	step [149/188], loss=107.8467
	step [150/188], loss=103.4377
	step [151/188], loss=105.0181
	step [152/188], loss=91.8008
	step [153/188], loss=88.9390
	step [154/188], loss=109.2430
	step [155/188], loss=105.6692
	step [156/188], loss=97.2418
	step [157/188], loss=100.6413
	step [158/188], loss=98.4599
	step [159/188], loss=94.5867
	step [160/188], loss=108.4338
	step [161/188], loss=88.7983
	step [162/188], loss=103.1003
	step [163/188], loss=103.8787
	step [164/188], loss=113.6181
	step [165/188], loss=103.5038
	step [166/188], loss=93.0803
	step [167/188], loss=98.4097
	step [168/188], loss=105.9391
	step [169/188], loss=106.7500
	step [170/188], loss=95.2782
	step [171/188], loss=103.4170
	step [172/188], loss=91.8469
	step [173/188], loss=101.5393
	step [174/188], loss=99.9749
	step [175/188], loss=111.2199
	step [176/188], loss=93.9764
	step [177/188], loss=98.4957
	step [178/188], loss=108.3110
	step [179/188], loss=101.3158
	step [180/188], loss=106.6403
	step [181/188], loss=98.6752
	step [182/188], loss=99.2798
	step [183/188], loss=108.0003
	step [184/188], loss=103.1830
	step [185/188], loss=113.6826
	step [186/188], loss=113.7852
	step [187/188], loss=92.2683
	step [188/188], loss=62.9753
	Evaluating
	loss=0.0266, precision=0.4067, recall=0.9119, f1=0.5625
Training epoch 16
	step [1/188], loss=109.8552
	step [2/188], loss=85.3595
	step [3/188], loss=92.4299
	step [4/188], loss=95.5646
	step [5/188], loss=113.4989
	step [6/188], loss=96.5275
	step [7/188], loss=101.1454
	step [8/188], loss=109.5038
	step [9/188], loss=106.7845
	step [10/188], loss=90.3982
	step [11/188], loss=95.9965
	step [12/188], loss=85.0368
	step [13/188], loss=108.0849
	step [14/188], loss=92.5055
	step [15/188], loss=108.2054
	step [16/188], loss=99.1590
	step [17/188], loss=86.4838
	step [18/188], loss=94.7774
	step [19/188], loss=109.9364
	step [20/188], loss=113.0368
	step [21/188], loss=106.3795
	step [22/188], loss=97.5281
	step [23/188], loss=101.1749
	step [24/188], loss=95.7168
	step [25/188], loss=96.0447
	step [26/188], loss=105.9587
	step [27/188], loss=109.9883
	step [28/188], loss=86.0380
	step [29/188], loss=84.9042
	step [30/188], loss=85.6961
	step [31/188], loss=106.8583
	step [32/188], loss=109.9991
	step [33/188], loss=99.8262
	step [34/188], loss=101.1283
	step [35/188], loss=100.2433
	step [36/188], loss=110.3929
	step [37/188], loss=97.1550
	step [38/188], loss=92.7514
	step [39/188], loss=105.8564
	step [40/188], loss=112.0513
	step [41/188], loss=94.1369
	step [42/188], loss=101.3721
	step [43/188], loss=104.7256
	step [44/188], loss=125.0428
	step [45/188], loss=95.6242
	step [46/188], loss=92.5497
	step [47/188], loss=111.2701
	step [48/188], loss=97.2135
	step [49/188], loss=109.1366
	step [50/188], loss=119.7103
	step [51/188], loss=90.0623
	step [52/188], loss=101.1902
	step [53/188], loss=98.6662
	step [54/188], loss=88.5808
	step [55/188], loss=104.7843
	step [56/188], loss=101.6701
	step [57/188], loss=101.3756
	step [58/188], loss=102.6419
	step [59/188], loss=118.1977
	step [60/188], loss=105.1565
	step [61/188], loss=93.6376
	step [62/188], loss=90.0052
	step [63/188], loss=92.4099
	step [64/188], loss=97.9425
	step [65/188], loss=110.3504
	step [66/188], loss=99.1353
	step [67/188], loss=102.5892
	step [68/188], loss=99.2431
	step [69/188], loss=100.6616
	step [70/188], loss=95.6868
	step [71/188], loss=104.0117
	step [72/188], loss=111.3562
	step [73/188], loss=108.0379
	step [74/188], loss=113.1565
	step [75/188], loss=102.7548
	step [76/188], loss=96.1076
	step [77/188], loss=124.7922
	step [78/188], loss=100.4445
	step [79/188], loss=103.5517
	step [80/188], loss=93.0222
	step [81/188], loss=93.2498
	step [82/188], loss=88.1390
	step [83/188], loss=102.7716
	step [84/188], loss=116.1762
	step [85/188], loss=109.2104
	step [86/188], loss=101.5745
	step [87/188], loss=109.7529
	step [88/188], loss=96.5157
	step [89/188], loss=87.9618
	step [90/188], loss=87.6324
	step [91/188], loss=103.5351
	step [92/188], loss=106.4684
	step [93/188], loss=110.1644
	step [94/188], loss=109.5113
	step [95/188], loss=107.1752
	step [96/188], loss=96.5625
	step [97/188], loss=121.4704
	step [98/188], loss=105.6886
	step [99/188], loss=108.9958
	step [100/188], loss=102.9570
	step [101/188], loss=125.2455
	step [102/188], loss=114.5869
	step [103/188], loss=119.3696
	step [104/188], loss=94.0551
	step [105/188], loss=100.4559
	step [106/188], loss=114.6310
	step [107/188], loss=101.1994
	step [108/188], loss=97.1973
	step [109/188], loss=105.1405
	step [110/188], loss=101.1652
	step [111/188], loss=100.9893
	step [112/188], loss=102.1181
	step [113/188], loss=91.6924
	step [114/188], loss=122.5430
	step [115/188], loss=92.7351
	step [116/188], loss=95.6088
	step [117/188], loss=107.0138
	step [118/188], loss=112.5361
	step [119/188], loss=115.1525
	step [120/188], loss=102.0942
	step [121/188], loss=104.1003
	step [122/188], loss=84.4208
	step [123/188], loss=101.3353
	step [124/188], loss=101.6109
	step [125/188], loss=87.9668
	step [126/188], loss=101.8877
	step [127/188], loss=117.9478
	step [128/188], loss=91.5680
	step [129/188], loss=90.7431
	step [130/188], loss=102.8080
	step [131/188], loss=99.1001
	step [132/188], loss=99.2889
	step [133/188], loss=96.2791
	step [134/188], loss=103.3120
	step [135/188], loss=107.2975
	step [136/188], loss=104.3667
	step [137/188], loss=104.3910
	step [138/188], loss=104.5144
	step [139/188], loss=112.3023
	step [140/188], loss=101.8155
	step [141/188], loss=114.9332
	step [142/188], loss=102.8312
	step [143/188], loss=107.6827
	step [144/188], loss=105.2337
	step [145/188], loss=93.1626
	step [146/188], loss=124.6500
	step [147/188], loss=108.2959
	step [148/188], loss=91.4136
	step [149/188], loss=92.9694
	step [150/188], loss=107.3499
	step [151/188], loss=103.8638
	step [152/188], loss=105.8626
	step [153/188], loss=109.0350
	step [154/188], loss=106.9044
	step [155/188], loss=114.5991
	step [156/188], loss=98.2443
	step [157/188], loss=104.6652
	step [158/188], loss=95.7025
	step [159/188], loss=104.2541
	step [160/188], loss=107.9423
	step [161/188], loss=89.5941
	step [162/188], loss=94.6037
	step [163/188], loss=96.5268
	step [164/188], loss=98.2378
	step [165/188], loss=92.0910
	step [166/188], loss=94.1266
	step [167/188], loss=92.7657
	step [168/188], loss=104.0303
	step [169/188], loss=99.5754
	step [170/188], loss=105.1981
	step [171/188], loss=122.7758
	step [172/188], loss=103.5922
	step [173/188], loss=102.4985
	step [174/188], loss=95.3045
	step [175/188], loss=113.9485
	step [176/188], loss=97.7281
	step [177/188], loss=85.0447
	step [178/188], loss=107.2548
	step [179/188], loss=89.3518
	step [180/188], loss=100.6088
	step [181/188], loss=81.5244
	step [182/188], loss=107.5823
	step [183/188], loss=107.6184
	step [184/188], loss=99.1104
	step [185/188], loss=97.2953
	step [186/188], loss=110.9319
	step [187/188], loss=112.2442
	step [188/188], loss=53.2271
	Evaluating
	loss=0.0254, precision=0.3839, recall=0.9371, f1=0.5447
Training epoch 17
	step [1/188], loss=95.8035
	step [2/188], loss=102.9226
	step [3/188], loss=102.2362
	step [4/188], loss=105.6753
	step [5/188], loss=99.9995
	step [6/188], loss=101.7455
	step [7/188], loss=93.8398
	step [8/188], loss=110.6530
	step [9/188], loss=102.0564
	step [10/188], loss=94.5537
	step [11/188], loss=94.9315
	step [12/188], loss=81.4524
	step [13/188], loss=102.8473
	step [14/188], loss=103.9968
	step [15/188], loss=109.7967
	step [16/188], loss=117.5109
	step [17/188], loss=81.1044
	step [18/188], loss=94.6967
	step [19/188], loss=88.8170
	step [20/188], loss=115.0497
	step [21/188], loss=96.5424
	step [22/188], loss=99.9081
	step [23/188], loss=115.8111
	step [24/188], loss=107.1016
	step [25/188], loss=101.4266
	step [26/188], loss=101.7715
	step [27/188], loss=74.7835
	step [28/188], loss=106.2307
	step [29/188], loss=111.5421
	step [30/188], loss=127.4704
	step [31/188], loss=102.6634
	step [32/188], loss=70.5402
	step [33/188], loss=104.9517
	step [34/188], loss=119.9661
	step [35/188], loss=108.5475
	step [36/188], loss=87.8272
	step [37/188], loss=90.2163
	step [38/188], loss=94.6086
	step [39/188], loss=96.9881
	step [40/188], loss=98.7735
	step [41/188], loss=105.6250
	step [42/188], loss=79.3469
	step [43/188], loss=106.8636
	step [44/188], loss=90.5423
	step [45/188], loss=115.5015
	step [46/188], loss=103.7385
	step [47/188], loss=96.0120
	step [48/188], loss=106.3023
	step [49/188], loss=106.5534
	step [50/188], loss=97.4224
	step [51/188], loss=95.6087
	step [52/188], loss=103.9633
	step [53/188], loss=93.4935
	step [54/188], loss=100.5885
	step [55/188], loss=100.5767
	step [56/188], loss=111.1117
	step [57/188], loss=111.4371
	step [58/188], loss=100.0808
	step [59/188], loss=91.8888
	step [60/188], loss=111.0254
	step [61/188], loss=101.9953
	step [62/188], loss=93.0223
	step [63/188], loss=106.4203
	step [64/188], loss=91.5413
	step [65/188], loss=110.9973
	step [66/188], loss=92.3436
	step [67/188], loss=105.6197
	step [68/188], loss=126.7271
	step [69/188], loss=93.4979
	step [70/188], loss=98.7260
	step [71/188], loss=87.9657
	step [72/188], loss=82.5702
	step [73/188], loss=102.7691
	step [74/188], loss=100.4429
	step [75/188], loss=111.0611
	step [76/188], loss=112.0015
	step [77/188], loss=100.0699
	step [78/188], loss=106.4825
	step [79/188], loss=97.1325
	step [80/188], loss=99.1411
	step [81/188], loss=102.0372
	step [82/188], loss=103.3190
	step [83/188], loss=99.9450
	step [84/188], loss=82.4524
	step [85/188], loss=101.4370
	step [86/188], loss=107.7763
	step [87/188], loss=105.0860
	step [88/188], loss=115.8347
	step [89/188], loss=116.6600
	step [90/188], loss=109.5090
	step [91/188], loss=101.6399
	step [92/188], loss=91.7449
	step [93/188], loss=98.5689
	step [94/188], loss=84.8835
	step [95/188], loss=99.8443
	step [96/188], loss=90.1427
	step [97/188], loss=80.5455
	step [98/188], loss=98.2903
	step [99/188], loss=104.0539
	step [100/188], loss=102.8981
	step [101/188], loss=90.1862
	step [102/188], loss=99.9334
	step [103/188], loss=99.5148
	step [104/188], loss=97.6642
	step [105/188], loss=111.8975
	step [106/188], loss=85.9451
	step [107/188], loss=96.3361
	step [108/188], loss=118.6070
	step [109/188], loss=99.8842
	step [110/188], loss=92.8463
	step [111/188], loss=88.3009
	step [112/188], loss=107.4680
	step [113/188], loss=106.9113
	step [114/188], loss=85.1823
	step [115/188], loss=94.6641
	step [116/188], loss=89.2240
	step [117/188], loss=103.1786
	step [118/188], loss=115.0264
	step [119/188], loss=103.6572
	step [120/188], loss=106.0119
	step [121/188], loss=112.8699
	step [122/188], loss=91.0268
	step [123/188], loss=98.1752
	step [124/188], loss=112.0048
	step [125/188], loss=101.7752
	step [126/188], loss=99.5851
	step [127/188], loss=97.8220
	step [128/188], loss=102.1878
	step [129/188], loss=122.1074
	step [130/188], loss=92.0566
	step [131/188], loss=113.2992
	step [132/188], loss=102.1417
	step [133/188], loss=94.2627
	step [134/188], loss=105.3190
	step [135/188], loss=83.6157
	step [136/188], loss=100.2628
	step [137/188], loss=101.7154
	step [138/188], loss=95.6973
	step [139/188], loss=85.9232
	step [140/188], loss=104.5208
	step [141/188], loss=98.5613
	step [142/188], loss=104.2299
	step [143/188], loss=106.6740
	step [144/188], loss=102.6012
	step [145/188], loss=95.2376
	step [146/188], loss=101.6432
	step [147/188], loss=113.5570
	step [148/188], loss=106.2806
	step [149/188], loss=96.0254
	step [150/188], loss=87.0965
	step [151/188], loss=104.3965
	step [152/188], loss=101.7029
	step [153/188], loss=95.7653
	step [154/188], loss=99.4632
	step [155/188], loss=94.3266
	step [156/188], loss=96.7700
	step [157/188], loss=105.6893
	step [158/188], loss=112.3890
	step [159/188], loss=87.1691
	step [160/188], loss=96.7223
	step [161/188], loss=89.0434
	step [162/188], loss=103.9768
	step [163/188], loss=113.3800
	step [164/188], loss=109.6226
	step [165/188], loss=96.7616
	step [166/188], loss=91.7478
	step [167/188], loss=97.9709
	step [168/188], loss=103.2481
	step [169/188], loss=97.6897
	step [170/188], loss=102.4122
	step [171/188], loss=113.9106
	step [172/188], loss=92.5957
	step [173/188], loss=113.8218
	step [174/188], loss=111.8217
	step [175/188], loss=105.9840
	step [176/188], loss=127.9496
	step [177/188], loss=99.0600
	step [178/188], loss=93.8153
	step [179/188], loss=93.9182
	step [180/188], loss=116.2148
	step [181/188], loss=88.3433
	step [182/188], loss=97.5344
	step [183/188], loss=99.6509
	step [184/188], loss=98.2279
	step [185/188], loss=93.5576
	step [186/188], loss=97.0815
	step [187/188], loss=97.7383
	step [188/188], loss=49.9132
	Evaluating
	loss=0.0212, precision=0.4065, recall=0.9256, f1=0.5649
Training epoch 18
	step [1/188], loss=94.2668
	step [2/188], loss=95.0869
	step [3/188], loss=108.5047
	step [4/188], loss=98.1540
	step [5/188], loss=94.6676
	step [6/188], loss=93.0508
	step [7/188], loss=97.9245
	step [8/188], loss=103.1341
	step [9/188], loss=104.1057
	step [10/188], loss=109.0849
	step [11/188], loss=85.3741
	step [12/188], loss=103.0412
	step [13/188], loss=90.7236
	step [14/188], loss=93.7448
	step [15/188], loss=109.5505
	step [16/188], loss=94.8390
	step [17/188], loss=91.6208
	step [18/188], loss=91.7509
	step [19/188], loss=115.6989
	step [20/188], loss=98.6072
	step [21/188], loss=81.5916
	step [22/188], loss=92.5549
	step [23/188], loss=101.4469
	step [24/188], loss=104.3528
	step [25/188], loss=101.6292
	step [26/188], loss=78.7747
	step [27/188], loss=97.8693
	step [28/188], loss=104.0168
	step [29/188], loss=105.0149
	step [30/188], loss=88.5102
	step [31/188], loss=89.2409
	step [32/188], loss=99.3343
	step [33/188], loss=114.2965
	step [34/188], loss=94.9371
	step [35/188], loss=89.3151
	step [36/188], loss=112.4186
	step [37/188], loss=111.4902
	step [38/188], loss=101.4293
	step [39/188], loss=94.5606
	step [40/188], loss=100.1299
	step [41/188], loss=83.4262
	step [42/188], loss=88.6685
	step [43/188], loss=94.2072
	step [44/188], loss=104.3751
	step [45/188], loss=106.3704
	step [46/188], loss=105.4275
	step [47/188], loss=98.5120
	step [48/188], loss=99.5470
	step [49/188], loss=93.3220
	step [50/188], loss=95.4894
	step [51/188], loss=93.8659
	step [52/188], loss=99.9163
	step [53/188], loss=86.6628
	step [54/188], loss=94.2122
	step [55/188], loss=100.1981
	step [56/188], loss=94.8408
	step [57/188], loss=112.8795
	step [58/188], loss=100.5657
	step [59/188], loss=89.7316
	step [60/188], loss=102.6790
	step [61/188], loss=97.2965
	step [62/188], loss=101.9560
	step [63/188], loss=94.2863
	step [64/188], loss=101.3754
	step [65/188], loss=78.2424
	step [66/188], loss=111.1314
	step [67/188], loss=91.3349
	step [68/188], loss=112.5927
	step [69/188], loss=88.4346
	step [70/188], loss=114.7906
	step [71/188], loss=79.2962
	step [72/188], loss=88.7784
	step [73/188], loss=108.1203
	step [74/188], loss=108.9578
	step [75/188], loss=96.7746
	step [76/188], loss=107.3200
	step [77/188], loss=117.1397
	step [78/188], loss=102.8237
	step [79/188], loss=95.0995
	step [80/188], loss=100.4021
	step [81/188], loss=102.3583
	step [82/188], loss=106.8596
	step [83/188], loss=92.1750
	step [84/188], loss=101.4873
	step [85/188], loss=105.2287
	step [86/188], loss=110.2798
	step [87/188], loss=100.2503
	step [88/188], loss=103.4010
	step [89/188], loss=103.0237
	step [90/188], loss=93.5277
	step [91/188], loss=87.0536
	step [92/188], loss=83.2602
	step [93/188], loss=94.5383
	step [94/188], loss=112.4952
	step [95/188], loss=115.3236
	step [96/188], loss=87.6644
	step [97/188], loss=100.3074
	step [98/188], loss=88.4775
	step [99/188], loss=95.8433
	step [100/188], loss=96.4077
	step [101/188], loss=102.1631
	step [102/188], loss=102.6658
	step [103/188], loss=114.8913
	step [104/188], loss=113.8075
	step [105/188], loss=97.6999
	step [106/188], loss=102.2645
	step [107/188], loss=107.0171
	step [108/188], loss=96.1751
	step [109/188], loss=94.6440
	step [110/188], loss=79.9825
	step [111/188], loss=97.0966
	step [112/188], loss=97.8562
	step [113/188], loss=83.6318
	step [114/188], loss=97.7585
	step [115/188], loss=96.3205
	step [116/188], loss=101.6027
	step [117/188], loss=120.5588
	step [118/188], loss=91.1668
	step [119/188], loss=89.9928
	step [120/188], loss=104.9818
	step [121/188], loss=117.0157
	step [122/188], loss=104.5131
	step [123/188], loss=90.4087
	step [124/188], loss=108.1430
	step [125/188], loss=98.1653
	step [126/188], loss=100.4536
	step [127/188], loss=98.2405
	step [128/188], loss=95.9853
	step [129/188], loss=109.6955
	step [130/188], loss=87.5742
	step [131/188], loss=89.3821
	step [132/188], loss=97.6012
	step [133/188], loss=114.9968
	step [134/188], loss=95.1086
	step [135/188], loss=89.9208
	step [136/188], loss=113.0175
	step [137/188], loss=94.2975
	step [138/188], loss=101.2472
	step [139/188], loss=90.9559
	step [140/188], loss=110.7906
	step [141/188], loss=91.5914
	step [142/188], loss=99.3923
	step [143/188], loss=83.9138
	step [144/188], loss=88.9217
	step [145/188], loss=98.0193
	step [146/188], loss=115.3007
	step [147/188], loss=87.7296
	step [148/188], loss=112.9944
	step [149/188], loss=96.9938
	step [150/188], loss=105.0719
	step [151/188], loss=100.9099
	step [152/188], loss=96.0331
	step [153/188], loss=106.5488
	step [154/188], loss=98.3821
	step [155/188], loss=102.5695
	step [156/188], loss=118.7542
	step [157/188], loss=112.3631
	step [158/188], loss=91.4346
	step [159/188], loss=112.5849
	step [160/188], loss=95.5613
	step [161/188], loss=101.6970
	step [162/188], loss=100.0045
	step [163/188], loss=103.9908
	step [164/188], loss=102.4837
	step [165/188], loss=120.2299
	step [166/188], loss=93.4018
	step [167/188], loss=82.3769
	step [168/188], loss=95.1286
	step [169/188], loss=118.7027
	step [170/188], loss=91.1286
	step [171/188], loss=91.3325
	step [172/188], loss=118.8969
	step [173/188], loss=90.1671
	step [174/188], loss=108.3006
	step [175/188], loss=105.6683
	step [176/188], loss=106.0151
	step [177/188], loss=101.2353
	step [178/188], loss=86.2355
	step [179/188], loss=103.4017
	step [180/188], loss=118.8320
	step [181/188], loss=87.9847
	step [182/188], loss=106.1823
	step [183/188], loss=81.0331
	step [184/188], loss=96.6918
	step [185/188], loss=94.0249
	step [186/188], loss=89.1625
	step [187/188], loss=103.2733
	step [188/188], loss=43.7121
	Evaluating
	loss=0.0201, precision=0.4063, recall=0.9426, f1=0.5678
Training epoch 19
	step [1/188], loss=94.8869
	step [2/188], loss=85.8331
	step [3/188], loss=105.4738
	step [4/188], loss=90.9732
	step [5/188], loss=98.5559
	step [6/188], loss=90.9685
	step [7/188], loss=95.4074
	step [8/188], loss=111.6174
	step [9/188], loss=102.8397
	step [10/188], loss=91.4717
	step [11/188], loss=96.1591
	step [12/188], loss=98.0448
	step [13/188], loss=87.0247
	step [14/188], loss=82.5402
	step [15/188], loss=85.7981
	step [16/188], loss=95.4086
	step [17/188], loss=93.3072
	step [18/188], loss=88.5124
	step [19/188], loss=98.7314
	step [20/188], loss=95.1071
	step [21/188], loss=99.8927
	step [22/188], loss=92.0574
	step [23/188], loss=108.0808
	step [24/188], loss=93.5047
	step [25/188], loss=99.2952
	step [26/188], loss=96.4134
	step [27/188], loss=98.2980
	step [28/188], loss=87.6684
	step [29/188], loss=97.3549
	step [30/188], loss=101.6393
	step [31/188], loss=92.6280
	step [32/188], loss=105.7911
	step [33/188], loss=100.4484
	step [34/188], loss=93.8631
	step [35/188], loss=92.1467
	step [36/188], loss=100.4442
	step [37/188], loss=86.1571
	step [38/188], loss=97.3214
	step [39/188], loss=108.7744
	step [40/188], loss=102.1053
	step [41/188], loss=104.1560
	step [42/188], loss=118.4535
	step [43/188], loss=96.3112
	step [44/188], loss=94.2543
	step [45/188], loss=76.4531
	step [46/188], loss=111.2206
	step [47/188], loss=109.0561
	step [48/188], loss=108.5096
	step [49/188], loss=97.8573
	step [50/188], loss=108.5648
	step [51/188], loss=109.3549
	step [52/188], loss=92.0349
	step [53/188], loss=105.5026
	step [54/188], loss=99.3260
	step [55/188], loss=103.6198
	step [56/188], loss=97.0126
	step [57/188], loss=75.9983
	step [58/188], loss=108.5131
	step [59/188], loss=104.3310
	step [60/188], loss=108.5577
	step [61/188], loss=92.4869
	step [62/188], loss=96.4011
	step [63/188], loss=88.2877
	step [64/188], loss=114.3190
	step [65/188], loss=88.6917
	step [66/188], loss=94.3800
	step [67/188], loss=93.5529
	step [68/188], loss=97.7275
	step [69/188], loss=102.4322
	step [70/188], loss=94.8001
	step [71/188], loss=101.1350
	step [72/188], loss=103.5734
	step [73/188], loss=91.0415
	step [74/188], loss=88.4451
	step [75/188], loss=107.6446
	step [76/188], loss=104.9725
	step [77/188], loss=108.5071
	step [78/188], loss=89.4170
	step [79/188], loss=103.9109
	step [80/188], loss=108.4809
	step [81/188], loss=82.1988
	step [82/188], loss=91.9558
	step [83/188], loss=96.0446
	step [84/188], loss=99.8708
	step [85/188], loss=106.9413
	step [86/188], loss=110.8342
	step [87/188], loss=86.4053
	step [88/188], loss=92.7368
	step [89/188], loss=84.9743
	step [90/188], loss=106.8064
	step [91/188], loss=83.6778
	step [92/188], loss=83.8724
	step [93/188], loss=111.0847
	step [94/188], loss=98.3096
	step [95/188], loss=116.1967
	step [96/188], loss=97.2427
	step [97/188], loss=96.7543
	step [98/188], loss=90.6369
	step [99/188], loss=94.3729
	step [100/188], loss=91.8781
	step [101/188], loss=88.0083
	step [102/188], loss=88.4239
	step [103/188], loss=106.6916
	step [104/188], loss=107.2771
	step [105/188], loss=86.2773
	step [106/188], loss=88.0252
	step [107/188], loss=96.0076
	step [108/188], loss=94.4039
	step [109/188], loss=105.4991
	step [110/188], loss=90.3603
	step [111/188], loss=99.2398
	step [112/188], loss=109.1187
	step [113/188], loss=104.8606
	step [114/188], loss=95.6483
	step [115/188], loss=99.1332
	step [116/188], loss=94.2980
	step [117/188], loss=112.0806
	step [118/188], loss=111.7356
	step [119/188], loss=92.5124
	step [120/188], loss=112.6534
	step [121/188], loss=83.0372
	step [122/188], loss=99.1665
	step [123/188], loss=96.6907
	step [124/188], loss=77.0285
	step [125/188], loss=104.6662
	step [126/188], loss=102.4040
	step [127/188], loss=110.2500
	step [128/188], loss=85.1243
	step [129/188], loss=90.0724
	step [130/188], loss=93.1555
	step [131/188], loss=101.3963
	step [132/188], loss=94.7120
	step [133/188], loss=98.4052
	step [134/188], loss=88.9324
	step [135/188], loss=111.7132
	step [136/188], loss=116.5968
	step [137/188], loss=104.2791
	step [138/188], loss=90.6689
	step [139/188], loss=95.6461
	step [140/188], loss=101.3666
	step [141/188], loss=95.8980
	step [142/188], loss=96.5334
	step [143/188], loss=99.0273
	step [144/188], loss=94.3084
	step [145/188], loss=96.0165
	step [146/188], loss=92.0189
	step [147/188], loss=98.5386
	step [148/188], loss=108.6762
	step [149/188], loss=90.3237
	step [150/188], loss=106.3020
	step [151/188], loss=105.3798
	step [152/188], loss=120.6849
	step [153/188], loss=96.9033
	step [154/188], loss=113.0668
	step [155/188], loss=94.3162
	step [156/188], loss=112.0764
	step [157/188], loss=90.8289
	step [158/188], loss=93.3919
	step [159/188], loss=101.5911
	step [160/188], loss=83.7533
	step [161/188], loss=96.6014
	step [162/188], loss=103.4381
	step [163/188], loss=96.1946
	step [164/188], loss=95.9655
	step [165/188], loss=124.7956
	step [166/188], loss=105.0859
	step [167/188], loss=98.2361
	step [168/188], loss=85.4053
	step [169/188], loss=80.0888
	step [170/188], loss=110.5145
	step [171/188], loss=83.5298
	step [172/188], loss=102.4361
	step [173/188], loss=93.1642
	step [174/188], loss=89.5321
	step [175/188], loss=103.7047
	step [176/188], loss=97.5597
	step [177/188], loss=97.7272
	step [178/188], loss=98.4638
	step [179/188], loss=92.2228
	step [180/188], loss=96.9066
	step [181/188], loss=101.1304
	step [182/188], loss=91.3433
	step [183/188], loss=98.2722
	step [184/188], loss=94.9079
	step [185/188], loss=112.9017
	step [186/188], loss=96.0141
	step [187/188], loss=100.2904
	step [188/188], loss=63.0811
	Evaluating
	loss=0.0252, precision=0.2667, recall=0.9173, f1=0.4133
Training epoch 20
	step [1/188], loss=103.3793
	step [2/188], loss=86.4966
	step [3/188], loss=90.3666
	step [4/188], loss=99.4130
	step [5/188], loss=98.8635
	step [6/188], loss=108.0926
	step [7/188], loss=98.9351
	step [8/188], loss=88.8688
	step [9/188], loss=80.0756
	step [10/188], loss=89.8062
	step [11/188], loss=86.0789
	step [12/188], loss=87.3455
	step [13/188], loss=97.7767
	step [14/188], loss=116.8521
	step [15/188], loss=94.8886
	step [16/188], loss=96.7722
	step [17/188], loss=111.8057
	step [18/188], loss=98.0728
	step [19/188], loss=96.7126
	step [20/188], loss=99.4346
	step [21/188], loss=93.2636
	step [22/188], loss=82.8946
	step [23/188], loss=94.6172
	step [24/188], loss=87.3114
	step [25/188], loss=106.5399
	step [26/188], loss=107.1166
	step [27/188], loss=100.7755
	step [28/188], loss=83.0940
	step [29/188], loss=97.8833
	step [30/188], loss=110.1262
	step [31/188], loss=89.7728
	step [32/188], loss=84.6336
	step [33/188], loss=97.4446
	step [34/188], loss=92.2157
	step [35/188], loss=93.9085
	step [36/188], loss=97.7629
	step [37/188], loss=95.5188
	step [38/188], loss=104.8798
	step [39/188], loss=105.9874
	step [40/188], loss=92.3698
	step [41/188], loss=97.5597
	step [42/188], loss=100.1000
	step [43/188], loss=88.5726
	step [44/188], loss=98.6583
	step [45/188], loss=95.9949
	step [46/188], loss=88.0585
	step [47/188], loss=94.5652
	step [48/188], loss=103.6644
	step [49/188], loss=90.1991
	step [50/188], loss=107.6871
	step [51/188], loss=94.0346
	step [52/188], loss=107.1115
	step [53/188], loss=100.9080
	step [54/188], loss=105.4909
	step [55/188], loss=102.3466
	step [56/188], loss=91.0979
	step [57/188], loss=111.5343
	step [58/188], loss=99.6214
	step [59/188], loss=85.8422
	step [60/188], loss=100.7328
	step [61/188], loss=77.1820
	step [62/188], loss=119.7919
	step [63/188], loss=95.0203
	step [64/188], loss=97.4444
	step [65/188], loss=100.7310
	step [66/188], loss=102.4446
	step [67/188], loss=93.3483
	step [68/188], loss=97.4724
	step [69/188], loss=94.5439
	step [70/188], loss=92.2646
	step [71/188], loss=93.2014
	step [72/188], loss=96.8355
	step [73/188], loss=96.3549
	step [74/188], loss=89.1900
	step [75/188], loss=86.1360
	step [76/188], loss=93.0522
	step [77/188], loss=92.0867
	step [78/188], loss=91.3679
	step [79/188], loss=94.2472
	step [80/188], loss=95.9695
	step [81/188], loss=85.2885
	step [82/188], loss=98.2836
	step [83/188], loss=77.7407
	step [84/188], loss=91.6317
	step [85/188], loss=98.9139
	step [86/188], loss=95.2886
	step [87/188], loss=104.0416
	step [88/188], loss=93.0672
	step [89/188], loss=94.1574
	step [90/188], loss=96.2900
	step [91/188], loss=96.5185
	step [92/188], loss=92.0286
	step [93/188], loss=96.5402
	step [94/188], loss=100.0143
	step [95/188], loss=113.8584
	step [96/188], loss=101.4823
	step [97/188], loss=98.0473
	step [98/188], loss=101.4794
	step [99/188], loss=104.2469
	step [100/188], loss=101.1407
	step [101/188], loss=99.3344
	step [102/188], loss=94.9680
	step [103/188], loss=86.3813
	step [104/188], loss=112.3925
	step [105/188], loss=89.5220
	step [106/188], loss=110.7106
	step [107/188], loss=102.1812
	step [108/188], loss=97.2961
	step [109/188], loss=105.9541
	step [110/188], loss=113.1082
	step [111/188], loss=103.5246
	step [112/188], loss=106.3548
	step [113/188], loss=96.2686
	step [114/188], loss=87.2962
	step [115/188], loss=100.0818
	step [116/188], loss=84.7400
	step [117/188], loss=85.7040
	step [118/188], loss=92.8606
	step [119/188], loss=95.1909
	step [120/188], loss=98.7140
	step [121/188], loss=82.1427
	step [122/188], loss=105.0505
	step [123/188], loss=92.9344
	step [124/188], loss=92.9446
	step [125/188], loss=103.7196
	step [126/188], loss=102.7314
	step [127/188], loss=78.2013
	step [128/188], loss=94.9449
	step [129/188], loss=82.9874
	step [130/188], loss=122.9626
	step [131/188], loss=84.0871
	step [132/188], loss=101.4514
	step [133/188], loss=96.0709
	step [134/188], loss=94.5713
	step [135/188], loss=99.5432
	step [136/188], loss=86.1606
	step [137/188], loss=86.9774
	step [138/188], loss=100.0042
	step [139/188], loss=118.1640
	step [140/188], loss=95.1896
	step [141/188], loss=88.5580
	step [142/188], loss=93.1836
	step [143/188], loss=104.7605
	step [144/188], loss=94.5829
	step [145/188], loss=109.1760
	step [146/188], loss=84.7170
	step [147/188], loss=101.1005
	step [148/188], loss=106.8606
	step [149/188], loss=95.4405
	step [150/188], loss=102.9045
	step [151/188], loss=93.2264
	step [152/188], loss=93.2606
	step [153/188], loss=101.6736
	step [154/188], loss=103.9802
	step [155/188], loss=93.4465
	step [156/188], loss=87.8879
	step [157/188], loss=93.3108
	step [158/188], loss=116.7086
	step [159/188], loss=92.4912
	step [160/188], loss=78.1171
	step [161/188], loss=103.0522
	step [162/188], loss=74.4490
	step [163/188], loss=83.2153
	step [164/188], loss=98.7737
	step [165/188], loss=94.1475
	step [166/188], loss=111.7363
	step [167/188], loss=95.5483
	step [168/188], loss=86.1548
	step [169/188], loss=93.9598
	step [170/188], loss=106.2836
	step [171/188], loss=94.7006
	step [172/188], loss=97.1008
	step [173/188], loss=95.8426
	step [174/188], loss=94.9265
	step [175/188], loss=84.9957
	step [176/188], loss=96.8617
	step [177/188], loss=85.2269
	step [178/188], loss=98.8792
	step [179/188], loss=101.4950
	step [180/188], loss=105.4838
	step [181/188], loss=100.3798
	step [182/188], loss=91.4389
	step [183/188], loss=90.7504
	step [184/188], loss=94.8542
	step [185/188], loss=101.4815
	step [186/188], loss=106.3488
	step [187/188], loss=106.7148
	step [188/188], loss=62.3192
	Evaluating
	loss=0.0190, precision=0.3995, recall=0.9355, f1=0.5599
Training epoch 21
	step [1/188], loss=81.5836
	step [2/188], loss=82.5410
	step [3/188], loss=98.5340
	step [4/188], loss=90.7783
	step [5/188], loss=92.7385
	step [6/188], loss=107.2896
	step [7/188], loss=93.1895
	step [8/188], loss=91.0800
	step [9/188], loss=104.7029
	step [10/188], loss=94.0822
	step [11/188], loss=97.8809
	step [12/188], loss=85.6005
	step [13/188], loss=116.0222
	step [14/188], loss=92.2807
	step [15/188], loss=103.2381
	step [16/188], loss=90.3051
	step [17/188], loss=113.4605
	step [18/188], loss=105.3030
	step [19/188], loss=102.3958
	step [20/188], loss=93.9621
	step [21/188], loss=95.5087
	step [22/188], loss=88.8721
	step [23/188], loss=83.6261
	step [24/188], loss=108.6247
	step [25/188], loss=100.5837
	step [26/188], loss=94.5787
	step [27/188], loss=100.8340
	step [28/188], loss=116.6846
	step [29/188], loss=97.6694
	step [30/188], loss=97.8825
	step [31/188], loss=100.2375
	step [32/188], loss=87.6287
	step [33/188], loss=107.0997
	step [34/188], loss=80.7526
	step [35/188], loss=89.8415
	step [36/188], loss=101.2096
	step [37/188], loss=96.7325
	step [38/188], loss=103.6841
	step [39/188], loss=99.5511
	step [40/188], loss=95.7033
	step [41/188], loss=102.3323
	step [42/188], loss=88.7577
	step [43/188], loss=101.2233
	step [44/188], loss=107.7777
	step [45/188], loss=80.2939
	step [46/188], loss=102.0759
	step [47/188], loss=101.0972
	step [48/188], loss=104.1848
	step [49/188], loss=86.7184
	step [50/188], loss=102.8456
	step [51/188], loss=99.5254
	step [52/188], loss=89.0724
	step [53/188], loss=105.0919
	step [54/188], loss=113.3307
	step [55/188], loss=100.3822
	step [56/188], loss=88.7571
	step [57/188], loss=96.8995
	step [58/188], loss=77.8700
	step [59/188], loss=115.3247
	step [60/188], loss=84.7305
	step [61/188], loss=85.0877
	step [62/188], loss=90.5470
	step [63/188], loss=95.9116
	step [64/188], loss=90.9059
	step [65/188], loss=93.7392
	step [66/188], loss=89.0730
	step [67/188], loss=107.8131
	step [68/188], loss=105.5327
	step [69/188], loss=79.9542
	step [70/188], loss=110.4728
	step [71/188], loss=92.5255
	step [72/188], loss=76.7915
	step [73/188], loss=92.7545
	step [74/188], loss=89.6145
	step [75/188], loss=101.0426
	step [76/188], loss=93.7047
	step [77/188], loss=83.0394
	step [78/188], loss=104.1050
	step [79/188], loss=93.9228
	step [80/188], loss=112.4835
	step [81/188], loss=104.6812
	step [82/188], loss=91.0814
	step [83/188], loss=80.0138
	step [84/188], loss=88.7276
	step [85/188], loss=91.2205
	step [86/188], loss=101.8198
	step [87/188], loss=98.3465
	step [88/188], loss=72.3845
	step [89/188], loss=83.7112
	step [90/188], loss=92.3713
	step [91/188], loss=98.2726
	step [92/188], loss=100.3579
	step [93/188], loss=78.7845
	step [94/188], loss=98.3847
	step [95/188], loss=91.2809
	step [96/188], loss=92.9165
	step [97/188], loss=100.1691
	step [98/188], loss=112.7045
	step [99/188], loss=86.8079
	step [100/188], loss=98.2413
	step [101/188], loss=96.3687
	step [102/188], loss=101.4523
	step [103/188], loss=91.8388
	step [104/188], loss=99.3760
	step [105/188], loss=96.9565
	step [106/188], loss=88.2434
	step [107/188], loss=81.8977
	step [108/188], loss=80.8316
	step [109/188], loss=94.5417
	step [110/188], loss=86.1869
	step [111/188], loss=102.1407
	step [112/188], loss=89.7144
	step [113/188], loss=92.0767
	step [114/188], loss=79.2030
	step [115/188], loss=90.9176
	step [116/188], loss=89.6608
	step [117/188], loss=91.5199
	step [118/188], loss=89.7117
	step [119/188], loss=110.0043
	step [120/188], loss=92.1783
	step [121/188], loss=117.2968
	step [122/188], loss=95.7535
	step [123/188], loss=102.2140
	step [124/188], loss=103.2852
	step [125/188], loss=100.5253
	step [126/188], loss=85.4129
	step [127/188], loss=87.3977
	step [128/188], loss=83.3141
	step [129/188], loss=110.7875
	step [130/188], loss=92.1375
	step [131/188], loss=90.9590
	step [132/188], loss=95.4905
	step [133/188], loss=79.1345
	step [134/188], loss=100.2622
	step [135/188], loss=100.5718
	step [136/188], loss=89.0388
	step [137/188], loss=96.5528
	step [138/188], loss=84.6237
	step [139/188], loss=92.2249
	step [140/188], loss=84.9127
	step [141/188], loss=115.4328
	step [142/188], loss=98.5403
	step [143/188], loss=94.4095
	step [144/188], loss=88.5585
	step [145/188], loss=101.0608
	step [146/188], loss=88.8239
	step [147/188], loss=112.3085
	step [148/188], loss=80.9922
	step [149/188], loss=89.7148
	step [150/188], loss=76.9458
	step [151/188], loss=94.9261
	step [152/188], loss=91.3950
	step [153/188], loss=97.3689
	step [154/188], loss=100.1214
	step [155/188], loss=78.2987
	step [156/188], loss=96.9955
	step [157/188], loss=119.3114
	step [158/188], loss=96.9422
	step [159/188], loss=98.2158
	step [160/188], loss=101.3440
	step [161/188], loss=98.3830
	step [162/188], loss=119.6804
	step [163/188], loss=102.9423
	step [164/188], loss=96.4760
	step [165/188], loss=86.4819
	step [166/188], loss=101.3857
	step [167/188], loss=92.7562
	step [168/188], loss=82.4578
	step [169/188], loss=93.8932
	step [170/188], loss=96.9871
	step [171/188], loss=106.6448
	step [172/188], loss=99.8256
	step [173/188], loss=96.2486
	step [174/188], loss=94.7588
	step [175/188], loss=100.7633
	step [176/188], loss=94.1970
	step [177/188], loss=87.1582
	step [178/188], loss=109.2183
	step [179/188], loss=80.0873
	step [180/188], loss=78.6169
	step [181/188], loss=89.6894
	step [182/188], loss=116.6102
	step [183/188], loss=102.5100
	step [184/188], loss=84.2537
	step [185/188], loss=96.5949
	step [186/188], loss=88.9313
	step [187/188], loss=79.3787
	step [188/188], loss=52.6491
	Evaluating
	loss=0.0193, precision=0.3378, recall=0.9290, f1=0.4955
Training epoch 22
	step [1/188], loss=89.0939
	step [2/188], loss=92.0618
	step [3/188], loss=80.1231
	step [4/188], loss=96.5359
	step [5/188], loss=101.6507
	step [6/188], loss=97.8171
	step [7/188], loss=103.8395
	step [8/188], loss=92.2542
	step [9/188], loss=90.3744
	step [10/188], loss=84.9877
	step [11/188], loss=100.2509
	step [12/188], loss=91.4272
	step [13/188], loss=95.6009
	step [14/188], loss=106.7097
	step [15/188], loss=78.5757
	step [16/188], loss=101.9310
	step [17/188], loss=84.6531
	step [18/188], loss=106.5973
	step [19/188], loss=86.0297
	step [20/188], loss=99.6625
	step [21/188], loss=86.4413
	step [22/188], loss=88.1169
	step [23/188], loss=84.9247
	step [24/188], loss=85.3136
	step [25/188], loss=90.6933
	step [26/188], loss=86.1413
	step [27/188], loss=92.0932
	step [28/188], loss=85.9783
	step [29/188], loss=88.0037
	step [30/188], loss=95.4290
	step [31/188], loss=88.9064
	step [32/188], loss=85.9561
	step [33/188], loss=89.1562
	step [34/188], loss=91.8163
	step [35/188], loss=86.5232
	step [36/188], loss=95.3212
	step [37/188], loss=86.2300
	step [38/188], loss=94.6248
	step [39/188], loss=96.2381
	step [40/188], loss=93.9939
	step [41/188], loss=101.1274
	step [42/188], loss=80.5086
	step [43/188], loss=95.5461
	step [44/188], loss=91.7641
	step [45/188], loss=106.9477
	step [46/188], loss=97.9135
	step [47/188], loss=93.6953
	step [48/188], loss=97.8749
	step [49/188], loss=86.2346
	step [50/188], loss=87.4383
	step [51/188], loss=91.7463
	step [52/188], loss=82.2367
	step [53/188], loss=109.3332
	step [54/188], loss=87.4543
	step [55/188], loss=78.4542
	step [56/188], loss=88.1073
	step [57/188], loss=90.7299
	step [58/188], loss=93.2687
	step [59/188], loss=87.1388
	step [60/188], loss=103.4127
	step [61/188], loss=98.9203
	step [62/188], loss=94.8600
	step [63/188], loss=92.6178
	step [64/188], loss=95.5273
	step [65/188], loss=96.9460
	step [66/188], loss=98.8038
	step [67/188], loss=95.6745
	step [68/188], loss=107.4259
	step [69/188], loss=94.5011
	step [70/188], loss=104.6655
	step [71/188], loss=89.0130
	step [72/188], loss=81.6050
	step [73/188], loss=95.0014
	step [74/188], loss=95.3258
	step [75/188], loss=90.4642
	step [76/188], loss=93.6746
	step [77/188], loss=104.4668
	step [78/188], loss=76.9026
	step [79/188], loss=95.3724
	step [80/188], loss=107.1258
	step [81/188], loss=103.0771
	step [82/188], loss=97.7634
	step [83/188], loss=96.6241
	step [84/188], loss=88.2591
	step [85/188], loss=95.0311
	step [86/188], loss=87.9319
	step [87/188], loss=100.6975
	step [88/188], loss=102.6251
	step [89/188], loss=95.9820
	step [90/188], loss=85.8202
	step [91/188], loss=90.2876
	step [92/188], loss=91.2905
	step [93/188], loss=89.4882
	step [94/188], loss=108.9878
	step [95/188], loss=86.8194
	step [96/188], loss=85.9048
	step [97/188], loss=89.1947
	step [98/188], loss=91.1585
	step [99/188], loss=103.6496
	step [100/188], loss=100.8957
	step [101/188], loss=94.6554
	step [102/188], loss=103.0786
	step [103/188], loss=96.7965
	step [104/188], loss=86.9931
	step [105/188], loss=95.4478
	step [106/188], loss=101.6632
	step [107/188], loss=93.1673
	step [108/188], loss=92.9805
	step [109/188], loss=84.3602
	step [110/188], loss=85.4617
	step [111/188], loss=94.5836
	step [112/188], loss=84.6976
	step [113/188], loss=89.1835
	step [114/188], loss=95.5147
	step [115/188], loss=115.2547
	step [116/188], loss=91.0627
	step [117/188], loss=108.5012
	step [118/188], loss=88.5826
	step [119/188], loss=101.4650
	step [120/188], loss=90.7771
	step [121/188], loss=102.1961
	step [122/188], loss=85.5827
	step [123/188], loss=95.9724
	step [124/188], loss=93.1023
	step [125/188], loss=88.7547
	step [126/188], loss=86.9002
	step [127/188], loss=103.2323
	step [128/188], loss=126.6999
	step [129/188], loss=103.4438
	step [130/188], loss=80.6383
	step [131/188], loss=90.3293
	step [132/188], loss=92.3314
	step [133/188], loss=103.8133
	step [134/188], loss=95.3779
	step [135/188], loss=84.6983
	step [136/188], loss=107.7563
	step [137/188], loss=97.3024
	step [138/188], loss=104.2154
	step [139/188], loss=85.1084
	step [140/188], loss=96.8936
	step [141/188], loss=87.8756
	step [142/188], loss=105.0388
	step [143/188], loss=102.8062
	step [144/188], loss=88.2155
	step [145/188], loss=96.6278
	step [146/188], loss=102.0944
	step [147/188], loss=89.8119
	step [148/188], loss=94.3831
	step [149/188], loss=110.4985
	step [150/188], loss=116.2421
	step [151/188], loss=99.6685
	step [152/188], loss=116.7269
	step [153/188], loss=88.5943
	step [154/188], loss=82.6876
	step [155/188], loss=79.2200
	step [156/188], loss=101.8530
	step [157/188], loss=111.5951
	step [158/188], loss=101.8526
	step [159/188], loss=107.6404
	step [160/188], loss=85.1939
	step [161/188], loss=101.0370
	step [162/188], loss=92.2454
	step [163/188], loss=83.7036
	step [164/188], loss=96.0988
	step [165/188], loss=76.5533
	step [166/188], loss=85.1464
	step [167/188], loss=94.7859
	step [168/188], loss=107.4012
	step [169/188], loss=97.2954
	step [170/188], loss=88.2204
	step [171/188], loss=92.7801
	step [172/188], loss=86.9797
	step [173/188], loss=101.4081
	step [174/188], loss=91.8034
	step [175/188], loss=90.4452
	step [176/188], loss=109.4906
	step [177/188], loss=90.1817
	step [178/188], loss=88.7681
	step [179/188], loss=91.3113
	step [180/188], loss=81.3166
	step [181/188], loss=86.4623
	step [182/188], loss=83.2897
	step [183/188], loss=92.4872
	step [184/188], loss=101.7307
	step [185/188], loss=98.3369
	step [186/188], loss=90.5528
	step [187/188], loss=96.6648
	step [188/188], loss=69.1565
	Evaluating
	loss=0.0186, precision=0.3366, recall=0.9229, f1=0.4933
Training epoch 23
	step [1/188], loss=91.6415
	step [2/188], loss=101.9948
	step [3/188], loss=101.8160
	step [4/188], loss=91.5190
	step [5/188], loss=79.7664
	step [6/188], loss=86.0960
	step [7/188], loss=96.3675
	step [8/188], loss=92.9483
	step [9/188], loss=96.8129
	step [10/188], loss=86.9259
	step [11/188], loss=80.0554
	step [12/188], loss=105.3146
	step [13/188], loss=95.5271
	step [14/188], loss=93.2023
	step [15/188], loss=89.3270
	step [16/188], loss=109.0699
	step [17/188], loss=104.1135
	step [18/188], loss=99.5858
	step [19/188], loss=77.5486
	step [20/188], loss=98.8751
	step [21/188], loss=85.4010
	step [22/188], loss=78.8404
	step [23/188], loss=104.4957
	step [24/188], loss=103.6010
	step [25/188], loss=99.3776
	step [26/188], loss=104.7806
	step [27/188], loss=98.9670
	step [28/188], loss=85.4025
	step [29/188], loss=91.5734
	step [30/188], loss=85.1151
	step [31/188], loss=79.2971
	step [32/188], loss=101.5898
	step [33/188], loss=89.5547
	step [34/188], loss=90.9318
	step [35/188], loss=92.0430
	step [36/188], loss=95.5587
	step [37/188], loss=90.5427
	step [38/188], loss=89.1260
	step [39/188], loss=96.9563
	step [40/188], loss=107.3903
	step [41/188], loss=82.2508
	step [42/188], loss=104.0378
	step [43/188], loss=91.4406
	step [44/188], loss=98.1838
	step [45/188], loss=97.7817
	step [46/188], loss=98.4188
	step [47/188], loss=91.0750
	step [48/188], loss=98.1680
	step [49/188], loss=96.5680
	step [50/188], loss=95.1583
	step [51/188], loss=108.8058
	step [52/188], loss=82.4658
	step [53/188], loss=93.9864
	step [54/188], loss=92.3372
	step [55/188], loss=91.7686
	step [56/188], loss=88.8504
	step [57/188], loss=102.7740
	step [58/188], loss=83.5624
	step [59/188], loss=88.9620
	step [60/188], loss=100.3360
	step [61/188], loss=89.1817
	step [62/188], loss=96.7971
	step [63/188], loss=87.7710
	step [64/188], loss=92.6376
	step [65/188], loss=114.9686
	step [66/188], loss=113.0970
	step [67/188], loss=87.3849
	step [68/188], loss=90.0779
	step [69/188], loss=94.8844
	step [70/188], loss=91.9490
	step [71/188], loss=89.2959
	step [72/188], loss=84.2237
	step [73/188], loss=108.5790
	step [74/188], loss=96.3846
	step [75/188], loss=89.2284
	step [76/188], loss=96.3068
	step [77/188], loss=84.5549
	step [78/188], loss=81.4092
	step [79/188], loss=94.4606
	step [80/188], loss=93.0149
	step [81/188], loss=95.4761
	step [82/188], loss=85.4914
	step [83/188], loss=82.5986
	step [84/188], loss=95.7359
	step [85/188], loss=113.5525
	step [86/188], loss=100.8817
	step [87/188], loss=95.5773
	step [88/188], loss=77.4875
	step [89/188], loss=87.5525
	step [90/188], loss=87.4095
	step [91/188], loss=84.1886
	step [92/188], loss=89.6647
	step [93/188], loss=96.3076
	step [94/188], loss=108.8920
	step [95/188], loss=103.3163
	step [96/188], loss=79.0655
	step [97/188], loss=89.1173
	step [98/188], loss=86.1661
	step [99/188], loss=95.7773
	step [100/188], loss=80.2908
	step [101/188], loss=104.1545
	step [102/188], loss=81.4300
	step [103/188], loss=93.0275
	step [104/188], loss=88.8711
	step [105/188], loss=90.3631
	step [106/188], loss=100.3154
	step [107/188], loss=85.7982
	step [108/188], loss=106.2026
	step [109/188], loss=116.6150
	step [110/188], loss=82.9019
	step [111/188], loss=104.1343
	step [112/188], loss=80.4719
	step [113/188], loss=86.1682
	step [114/188], loss=101.5775
	step [115/188], loss=104.5587
	step [116/188], loss=102.7009
	step [117/188], loss=105.1480
	step [118/188], loss=97.2319
	step [119/188], loss=91.4370
	step [120/188], loss=85.9184
	step [121/188], loss=100.3211
	step [122/188], loss=103.0617
	step [123/188], loss=83.6630
	step [124/188], loss=98.3275
	step [125/188], loss=91.0335
	step [126/188], loss=84.8266
	step [127/188], loss=103.5079
	step [128/188], loss=93.6314
	step [129/188], loss=102.7951
	step [130/188], loss=97.3712
	step [131/188], loss=100.6324
	step [132/188], loss=85.0720
	step [133/188], loss=95.5273
	step [134/188], loss=88.1739
	step [135/188], loss=74.1784
	step [136/188], loss=91.1814
	step [137/188], loss=102.8068
	step [138/188], loss=86.9857
	step [139/188], loss=94.3853
	step [140/188], loss=99.5882
	step [141/188], loss=90.3036
	step [142/188], loss=104.9311
	step [143/188], loss=83.0626
	step [144/188], loss=88.7402
	step [145/188], loss=95.0483
	step [146/188], loss=91.5422
	step [147/188], loss=75.2035
	step [148/188], loss=79.9195
	step [149/188], loss=89.9870
	step [150/188], loss=99.4935
	step [151/188], loss=84.4867
	step [152/188], loss=104.4322
	step [153/188], loss=92.9095
	step [154/188], loss=94.4711
	step [155/188], loss=87.4149
	step [156/188], loss=78.9272
	step [157/188], loss=81.2802
	step [158/188], loss=80.1800
	step [159/188], loss=92.5365
	step [160/188], loss=73.5161
	step [161/188], loss=96.4399
	step [162/188], loss=102.0118
	step [163/188], loss=75.0639
	step [164/188], loss=95.1681
	step [165/188], loss=90.4229
	step [166/188], loss=95.6657
	step [167/188], loss=100.8017
	step [168/188], loss=100.4665
	step [169/188], loss=83.9950
	step [170/188], loss=88.5139
	step [171/188], loss=94.7915
	step [172/188], loss=87.9650
	step [173/188], loss=99.2717
	step [174/188], loss=105.9166
	step [175/188], loss=86.5587
	step [176/188], loss=96.6869
	step [177/188], loss=84.0637
	step [178/188], loss=83.8461
	step [179/188], loss=98.9939
	step [180/188], loss=86.8261
	step [181/188], loss=81.8649
	step [182/188], loss=92.0779
	step [183/188], loss=87.2430
	step [184/188], loss=77.0484
	step [185/188], loss=95.1010
	step [186/188], loss=84.8955
	step [187/188], loss=110.3791
	step [188/188], loss=43.6128
	Evaluating
	loss=0.0172, precision=0.3389, recall=0.9129, f1=0.4943
Training epoch 24
	step [1/188], loss=89.8305
	step [2/188], loss=87.3400
	step [3/188], loss=89.4144
	step [4/188], loss=96.3524
	step [5/188], loss=95.2724
	step [6/188], loss=89.5597
	step [7/188], loss=98.3985
	step [8/188], loss=94.7822
	step [9/188], loss=99.8323
	step [10/188], loss=93.4488
	step [11/188], loss=85.7453
	step [12/188], loss=94.7235
	step [13/188], loss=94.0193
	step [14/188], loss=89.6949
	step [15/188], loss=78.8798
	step [16/188], loss=84.2255
	step [17/188], loss=93.4888
	step [18/188], loss=91.8772
	step [19/188], loss=86.1623
	step [20/188], loss=107.8793
	step [21/188], loss=82.9103
	step [22/188], loss=103.8228
	step [23/188], loss=97.6483
	step [24/188], loss=100.2113
	step [25/188], loss=92.2834
	step [26/188], loss=92.9213
	step [27/188], loss=83.1322
	step [28/188], loss=97.1435
	step [29/188], loss=98.6741
	step [30/188], loss=91.1096
	step [31/188], loss=89.5267
	step [32/188], loss=96.0364
	step [33/188], loss=92.6135
	step [34/188], loss=86.4882
	step [35/188], loss=86.9360
	step [36/188], loss=88.5495
	step [37/188], loss=83.7631
	step [38/188], loss=75.9761
	step [39/188], loss=86.9445
	step [40/188], loss=109.7818
	step [41/188], loss=85.5334
	step [42/188], loss=85.6585
	step [43/188], loss=106.8048
	step [44/188], loss=85.4852
	step [45/188], loss=97.8560
	step [46/188], loss=78.3796
	step [47/188], loss=85.9402
	step [48/188], loss=88.3112
	step [49/188], loss=86.6450
	step [50/188], loss=102.2597
	step [51/188], loss=81.0557
	step [52/188], loss=79.3191
	step [53/188], loss=105.5468
	step [54/188], loss=91.1541
	step [55/188], loss=75.9893
	step [56/188], loss=105.1173
	step [57/188], loss=80.5555
	step [58/188], loss=93.5964
	step [59/188], loss=90.7293
	step [60/188], loss=86.7999
	step [61/188], loss=95.6021
	step [62/188], loss=79.4280
	step [63/188], loss=97.0696
	step [64/188], loss=100.5333
	step [65/188], loss=102.1087
	step [66/188], loss=92.8767
	step [67/188], loss=103.3156
	step [68/188], loss=90.6951
	step [69/188], loss=105.3670
	step [70/188], loss=100.8072
	step [71/188], loss=100.5186
	step [72/188], loss=99.5336
	step [73/188], loss=96.1054
	step [74/188], loss=79.0672
	step [75/188], loss=94.3504
	step [76/188], loss=87.8040
	step [77/188], loss=90.4444
	step [78/188], loss=84.4528
	step [79/188], loss=94.0829
	step [80/188], loss=89.8886
	step [81/188], loss=79.9707
	step [82/188], loss=106.1644
	step [83/188], loss=77.4562
	step [84/188], loss=88.7622
	step [85/188], loss=74.3327
	step [86/188], loss=87.5386
	step [87/188], loss=110.7250
	step [88/188], loss=84.4982
	step [89/188], loss=84.2982
	step [90/188], loss=79.8483
	step [91/188], loss=97.4758
	step [92/188], loss=82.6960
	step [93/188], loss=84.7034
	step [94/188], loss=91.8045
	step [95/188], loss=74.5721
	step [96/188], loss=99.6004
	step [97/188], loss=90.6984
	step [98/188], loss=104.8908
	step [99/188], loss=81.1861
	step [100/188], loss=96.9080
	step [101/188], loss=102.5821
	step [102/188], loss=79.2851
	step [103/188], loss=88.7082
	step [104/188], loss=97.3467
	step [105/188], loss=100.3900
	step [106/188], loss=87.7285
	step [107/188], loss=98.4336
	step [108/188], loss=96.6814
	step [109/188], loss=86.8932
	step [110/188], loss=87.4274
	step [111/188], loss=73.8930
	step [112/188], loss=110.2972
	step [113/188], loss=96.5957
	step [114/188], loss=104.7876
	step [115/188], loss=81.0885
	step [116/188], loss=74.7576
	step [117/188], loss=96.1064
	step [118/188], loss=81.0882
	step [119/188], loss=89.9007
	step [120/188], loss=97.0488
	step [121/188], loss=100.6147
	step [122/188], loss=101.9479
	step [123/188], loss=94.1782
	step [124/188], loss=83.6919
	step [125/188], loss=85.6750
	step [126/188], loss=89.1992
	step [127/188], loss=91.0916
	step [128/188], loss=83.7858
	step [129/188], loss=101.6995
	step [130/188], loss=92.4190
	step [131/188], loss=110.1029
	step [132/188], loss=89.8462
	step [133/188], loss=93.7819
	step [134/188], loss=87.1646
	step [135/188], loss=110.7534
	step [136/188], loss=93.8058
	step [137/188], loss=72.4292
	step [138/188], loss=101.5358
	step [139/188], loss=107.2194
	step [140/188], loss=104.9699
	step [141/188], loss=102.7306
	step [142/188], loss=87.5480
	step [143/188], loss=82.4146
	step [144/188], loss=80.8352
	step [145/188], loss=88.9774
	step [146/188], loss=84.2318
	step [147/188], loss=95.9197
	step [148/188], loss=96.6181
	step [149/188], loss=87.3888
	step [150/188], loss=87.0202
	step [151/188], loss=101.1048
	step [152/188], loss=84.8544
	step [153/188], loss=85.5323
	step [154/188], loss=76.6347
	step [155/188], loss=92.8610
	step [156/188], loss=91.2247
	step [157/188], loss=76.3031
	step [158/188], loss=95.2890
	step [159/188], loss=87.3468
	step [160/188], loss=89.6863
	step [161/188], loss=96.1001
	step [162/188], loss=88.0717
	step [163/188], loss=97.9897
	step [164/188], loss=88.9059
	step [165/188], loss=84.9799
	step [166/188], loss=92.3782
	step [167/188], loss=87.4618
	step [168/188], loss=82.5022
	step [169/188], loss=97.9096
	step [170/188], loss=84.7059
	step [171/188], loss=102.2717
	step [172/188], loss=102.4636
	step [173/188], loss=87.6674
	step [174/188], loss=88.6689
	step [175/188], loss=98.8765
	step [176/188], loss=90.6655
	step [177/188], loss=95.2078
	step [178/188], loss=84.9864
	step [179/188], loss=83.9030
	step [180/188], loss=96.2075
	step [181/188], loss=101.5354
	step [182/188], loss=105.1977
	step [183/188], loss=93.3736
	step [184/188], loss=90.9988
	step [185/188], loss=93.3302
	step [186/188], loss=98.8721
	step [187/188], loss=99.2604
	step [188/188], loss=50.8482
	Evaluating
	loss=0.0159, precision=0.3283, recall=0.9115, f1=0.4827
Training epoch 25
	step [1/188], loss=66.4047
	step [2/188], loss=86.8256
	step [3/188], loss=84.1310
	step [4/188], loss=104.8454
	step [5/188], loss=97.5655
	step [6/188], loss=88.7649
	step [7/188], loss=94.9883
	step [8/188], loss=95.4534
	step [9/188], loss=92.6375
	step [10/188], loss=102.3768
	step [11/188], loss=106.9382
	step [12/188], loss=91.2958
	step [13/188], loss=93.0792
	step [14/188], loss=88.3243
	step [15/188], loss=85.9296
	step [16/188], loss=94.0148
	step [17/188], loss=92.9356
	step [18/188], loss=85.6542
	step [19/188], loss=84.1544
	step [20/188], loss=106.4253
	step [21/188], loss=112.6238
	step [22/188], loss=90.7527
	step [23/188], loss=83.8428
	step [24/188], loss=73.1867
	step [25/188], loss=88.0534
	step [26/188], loss=91.2979
	step [27/188], loss=100.6859
	step [28/188], loss=77.7597
	step [29/188], loss=98.6053
	step [30/188], loss=76.4414
	step [31/188], loss=86.5188
	step [32/188], loss=92.4755
	step [33/188], loss=90.8186
	step [34/188], loss=94.7063
	step [35/188], loss=86.7852
	step [36/188], loss=100.8175
	step [37/188], loss=81.8139
	step [38/188], loss=75.8270
	step [39/188], loss=91.1554
	step [40/188], loss=105.1441
	step [41/188], loss=85.4418
	step [42/188], loss=94.3378
	step [43/188], loss=94.1979
	step [44/188], loss=87.6951
	step [45/188], loss=86.4993
	step [46/188], loss=90.9560
	step [47/188], loss=100.7036
	step [48/188], loss=89.4276
	step [49/188], loss=86.6261
	step [50/188], loss=89.6924
	step [51/188], loss=95.4498
	step [52/188], loss=73.0205
	step [53/188], loss=89.1058
	step [54/188], loss=90.1127
	step [55/188], loss=91.1137
	step [56/188], loss=98.8239
	step [57/188], loss=79.8945
	step [58/188], loss=85.3695
	step [59/188], loss=82.1842
	step [60/188], loss=100.9210
	step [61/188], loss=72.6366
	step [62/188], loss=94.9087
	step [63/188], loss=93.8901
	step [64/188], loss=77.0224
	step [65/188], loss=92.9741
	step [66/188], loss=90.7127
	step [67/188], loss=86.4928
	step [68/188], loss=99.0556
	step [69/188], loss=101.8263
	step [70/188], loss=87.7062
	step [71/188], loss=90.0010
	step [72/188], loss=81.8734
	step [73/188], loss=92.8451
	step [74/188], loss=78.7617
	step [75/188], loss=99.4925
	step [76/188], loss=92.0361
	step [77/188], loss=107.9278
	step [78/188], loss=92.5836
	step [79/188], loss=97.0062
	step [80/188], loss=70.7826
	step [81/188], loss=88.1058
	step [82/188], loss=86.0359
	step [83/188], loss=100.5211
	step [84/188], loss=86.7903
	step [85/188], loss=93.8131
	step [86/188], loss=86.7758
	step [87/188], loss=87.0053
	step [88/188], loss=100.6208
	step [89/188], loss=96.1594
	step [90/188], loss=95.1187
	step [91/188], loss=94.8270
	step [92/188], loss=82.2964
	step [93/188], loss=89.0621
	step [94/188], loss=101.0006
	step [95/188], loss=89.4287
	step [96/188], loss=97.4810
	step [97/188], loss=99.3814
	step [98/188], loss=95.6912
	step [99/188], loss=79.9960
	step [100/188], loss=91.5919
	step [101/188], loss=91.2794
	step [102/188], loss=93.0169
	step [103/188], loss=82.6326
	step [104/188], loss=103.4537
	step [105/188], loss=80.4303
	step [106/188], loss=90.3185
	step [107/188], loss=105.0952
	step [108/188], loss=97.0008
	step [109/188], loss=88.2921
	step [110/188], loss=92.5108
	step [111/188], loss=94.3943
	step [112/188], loss=85.3318
	step [113/188], loss=77.4310
	step [114/188], loss=84.6588
	step [115/188], loss=98.1338
	step [116/188], loss=85.7221
	step [117/188], loss=86.3461
	step [118/188], loss=85.4067
	step [119/188], loss=93.1248
	step [120/188], loss=95.5295
	step [121/188], loss=105.5028
	step [122/188], loss=83.0106
	step [123/188], loss=86.8312
	step [124/188], loss=86.4389
	step [125/188], loss=82.4807
	step [126/188], loss=88.4355
	step [127/188], loss=80.4336
	step [128/188], loss=83.5454
	step [129/188], loss=83.9989
	step [130/188], loss=82.3783
	step [131/188], loss=88.3074
	step [132/188], loss=81.1432
	step [133/188], loss=98.8249
	step [134/188], loss=86.9297
	step [135/188], loss=89.2407
	step [136/188], loss=88.5143
	step [137/188], loss=73.7859
	step [138/188], loss=87.1106
	step [139/188], loss=91.4263
	step [140/188], loss=78.4277
	step [141/188], loss=86.0524
	step [142/188], loss=98.9217
	step [143/188], loss=82.6098
	step [144/188], loss=80.2728
	step [145/188], loss=100.7520
	step [146/188], loss=86.9101
	step [147/188], loss=106.6767
	step [148/188], loss=111.5407
	step [149/188], loss=91.0230
	step [150/188], loss=93.9571
	step [151/188], loss=97.5318
	step [152/188], loss=102.5699
	step [153/188], loss=105.5727
	step [154/188], loss=99.7602
	step [155/188], loss=78.8242
	step [156/188], loss=98.9441
	step [157/188], loss=93.8458
	step [158/188], loss=94.3913
	step [159/188], loss=93.3534
	step [160/188], loss=91.0416
	step [161/188], loss=84.1243
	step [162/188], loss=92.3153
	step [163/188], loss=102.9687
	step [164/188], loss=94.3558
	step [165/188], loss=92.8294
	step [166/188], loss=97.1076
	step [167/188], loss=73.8506
	step [168/188], loss=97.9112
	step [169/188], loss=91.0133
	step [170/188], loss=89.9425
	step [171/188], loss=93.5339
	step [172/188], loss=92.5728
	step [173/188], loss=84.6691
	step [174/188], loss=97.7900
	step [175/188], loss=92.3865
	step [176/188], loss=95.0618
	step [177/188], loss=93.1149
	step [178/188], loss=98.8681
	step [179/188], loss=84.7297
	step [180/188], loss=90.0618
	step [181/188], loss=91.6759
	step [182/188], loss=90.6003
	step [183/188], loss=93.3611
	step [184/188], loss=97.4599
	step [185/188], loss=103.3187
	step [186/188], loss=87.9860
	step [187/188], loss=93.2652
	step [188/188], loss=55.4068
	Evaluating
	loss=0.0149, precision=0.3352, recall=0.9359, f1=0.4937
Training epoch 26
	step [1/188], loss=85.5581
	step [2/188], loss=95.0174
	step [3/188], loss=70.7167
	step [4/188], loss=104.3934
	step [5/188], loss=85.8979
	step [6/188], loss=91.6394
	step [7/188], loss=99.5623
	step [8/188], loss=88.2847
	step [9/188], loss=91.3463
	step [10/188], loss=96.3642
	step [11/188], loss=84.6834
	step [12/188], loss=87.1843
	step [13/188], loss=86.6464
	step [14/188], loss=72.8640
	step [15/188], loss=89.6441
	step [16/188], loss=97.2186
	step [17/188], loss=95.1202
	step [18/188], loss=109.8739
	step [19/188], loss=101.9960
	step [20/188], loss=85.5238
	step [21/188], loss=99.0860
	step [22/188], loss=93.7146
	step [23/188], loss=91.5799
	step [24/188], loss=91.4448
	step [25/188], loss=77.9366
	step [26/188], loss=96.5668
	step [27/188], loss=89.3653
	step [28/188], loss=90.8340
	step [29/188], loss=89.8717
	step [30/188], loss=95.3638
	step [31/188], loss=94.6521
	step [32/188], loss=90.4005
	step [33/188], loss=84.2389
	step [34/188], loss=91.3566
	step [35/188], loss=73.8990
	step [36/188], loss=73.1366
	step [37/188], loss=102.5634
	step [38/188], loss=74.2308
	step [39/188], loss=91.8849
	step [40/188], loss=90.6128
	step [41/188], loss=88.1127
	step [42/188], loss=84.6499
	step [43/188], loss=88.3183
	step [44/188], loss=83.5806
	step [45/188], loss=92.8052
	step [46/188], loss=95.2782
	step [47/188], loss=82.6103
	step [48/188], loss=105.8701
	step [49/188], loss=90.3326
	step [50/188], loss=92.0040
	step [51/188], loss=91.0038
	step [52/188], loss=97.1124
	step [53/188], loss=95.2829
	step [54/188], loss=101.8928
	step [55/188], loss=77.4581
	step [56/188], loss=82.5752
	step [57/188], loss=79.7314
	step [58/188], loss=119.0731
	step [59/188], loss=92.1298
	step [60/188], loss=79.3125
	step [61/188], loss=76.4157
	step [62/188], loss=89.8537
	step [63/188], loss=83.8607
	step [64/188], loss=98.3977
	step [65/188], loss=92.5519
	step [66/188], loss=89.7262
	step [67/188], loss=88.9736
	step [68/188], loss=87.5811
	step [69/188], loss=91.5554
	step [70/188], loss=101.3781
	step [71/188], loss=70.4028
	step [72/188], loss=73.3416
	step [73/188], loss=85.0530
	step [74/188], loss=102.6865
	step [75/188], loss=94.1548
	step [76/188], loss=70.0084
	step [77/188], loss=87.3609
	step [78/188], loss=79.9238
	step [79/188], loss=82.4357
	step [80/188], loss=97.9568
	step [81/188], loss=95.7259
	step [82/188], loss=82.8095
	step [83/188], loss=91.5207
	step [84/188], loss=89.2390
	step [85/188], loss=96.2085
	step [86/188], loss=86.2089
	step [87/188], loss=88.7625
	step [88/188], loss=83.9139
	step [89/188], loss=91.2708
	step [90/188], loss=90.7346
	step [91/188], loss=85.2557
	step [92/188], loss=86.0490
	step [93/188], loss=91.6470
	step [94/188], loss=87.7451
	step [95/188], loss=87.5669
	step [96/188], loss=81.1740
	step [97/188], loss=89.9537
	step [98/188], loss=85.2594
	step [99/188], loss=100.4467
	step [100/188], loss=83.5612
	step [101/188], loss=114.8793
	step [102/188], loss=105.0093
	step [103/188], loss=81.9026
	step [104/188], loss=83.5864
	step [105/188], loss=79.0633
	step [106/188], loss=95.4044
	step [107/188], loss=87.4287
	step [108/188], loss=87.5870
	step [109/188], loss=75.4267
	step [110/188], loss=98.1529
	step [111/188], loss=83.1356
	step [112/188], loss=94.0207
	step [113/188], loss=83.5659
	step [114/188], loss=90.3822
	step [115/188], loss=96.2338
	step [116/188], loss=86.2622
	step [117/188], loss=99.7185
	step [118/188], loss=100.6550
	step [119/188], loss=88.4014
	step [120/188], loss=93.5130
	step [121/188], loss=91.9921
	step [122/188], loss=95.1371
	step [123/188], loss=95.5092
	step [124/188], loss=88.3255
	step [125/188], loss=79.1154
	step [126/188], loss=101.0280
	step [127/188], loss=89.0842
	step [128/188], loss=88.4079
	step [129/188], loss=89.8429
	step [130/188], loss=89.4432
	step [131/188], loss=84.4178
	step [132/188], loss=95.8585
	step [133/188], loss=102.8003
	step [134/188], loss=81.0902
	step [135/188], loss=88.5753
	step [136/188], loss=74.9033
	step [137/188], loss=95.5382
	step [138/188], loss=93.0514
	step [139/188], loss=96.2409
	step [140/188], loss=95.6192
	step [141/188], loss=96.3852
	step [142/188], loss=87.6522
	step [143/188], loss=77.1864
	step [144/188], loss=92.8082
	step [145/188], loss=99.5500
	step [146/188], loss=74.6767
	step [147/188], loss=102.6122
	step [148/188], loss=68.9584
	step [149/188], loss=80.5986
	step [150/188], loss=95.0284
	step [151/188], loss=106.4225
	step [152/188], loss=95.4185
	step [153/188], loss=89.8838
	step [154/188], loss=97.0941
	step [155/188], loss=88.2804
	step [156/188], loss=98.4374
	step [157/188], loss=92.9411
	step [158/188], loss=86.5323
	step [159/188], loss=81.6932
	step [160/188], loss=78.3873
	step [161/188], loss=101.1903
	step [162/188], loss=89.7631
	step [163/188], loss=96.0712
	step [164/188], loss=85.0939
	step [165/188], loss=73.8873
	step [166/188], loss=98.2193
	step [167/188], loss=94.6065
	step [168/188], loss=92.4833
	step [169/188], loss=100.4174
	step [170/188], loss=87.4646
	step [171/188], loss=81.2839
	step [172/188], loss=88.9858
	step [173/188], loss=85.0221
	step [174/188], loss=84.3727
	step [175/188], loss=85.1292
	step [176/188], loss=79.9669
	step [177/188], loss=102.4444
	step [178/188], loss=87.2495
	step [179/188], loss=94.3604
	step [180/188], loss=96.7035
	step [181/188], loss=90.5777
	step [182/188], loss=84.2596
	step [183/188], loss=86.2327
	step [184/188], loss=99.8248
	step [185/188], loss=88.5461
	step [186/188], loss=87.7695
	step [187/188], loss=92.3279
	step [188/188], loss=48.2389
	Evaluating
	loss=0.0142, precision=0.3716, recall=0.9196, f1=0.5293
Training epoch 27
	step [1/188], loss=89.4720
	step [2/188], loss=95.8195
	step [3/188], loss=84.7183
	step [4/188], loss=95.3085
	step [5/188], loss=113.4721
	step [6/188], loss=92.3105
	step [7/188], loss=101.5757
	step [8/188], loss=79.8633
	step [9/188], loss=94.5710
	step [10/188], loss=94.4437
	step [11/188], loss=84.9044
	step [12/188], loss=94.4308
	step [13/188], loss=101.6184
	step [14/188], loss=79.6275
	step [15/188], loss=84.7728
	step [16/188], loss=86.3070
	step [17/188], loss=80.3758
	step [18/188], loss=91.5844
	step [19/188], loss=81.0012
	step [20/188], loss=100.9227
	step [21/188], loss=92.7107
	step [22/188], loss=95.3626
	step [23/188], loss=90.2141
	step [24/188], loss=70.6574
	step [25/188], loss=91.4977
	step [26/188], loss=96.7643
	step [27/188], loss=93.3258
	step [28/188], loss=91.9913
	step [29/188], loss=97.1716
	step [30/188], loss=82.4433
	step [31/188], loss=101.7421
	step [32/188], loss=99.1107
	step [33/188], loss=100.2477
	step [34/188], loss=82.0200
	step [35/188], loss=85.8961
	step [36/188], loss=87.9227
	step [37/188], loss=91.8960
	step [38/188], loss=82.3923
	step [39/188], loss=91.8046
	step [40/188], loss=94.3689
	step [41/188], loss=92.9787
	step [42/188], loss=103.6695
	step [43/188], loss=94.4120
	step [44/188], loss=80.3913
	step [45/188], loss=88.2007
	step [46/188], loss=82.8666
	step [47/188], loss=81.8513
	step [48/188], loss=86.5431
	step [49/188], loss=69.8844
	step [50/188], loss=98.9611
	step [51/188], loss=90.3613
	step [52/188], loss=99.1861
	step [53/188], loss=104.3103
	step [54/188], loss=81.6260
	step [55/188], loss=86.5280
	step [56/188], loss=78.0389
	step [57/188], loss=85.8599
	step [58/188], loss=94.1050
	step [59/188], loss=80.4721
	step [60/188], loss=78.1912
	step [61/188], loss=77.5358
	step [62/188], loss=74.8885
	step [63/188], loss=74.9498
	step [64/188], loss=81.4261
	step [65/188], loss=74.6827
	step [66/188], loss=87.9069
	step [67/188], loss=82.7641
	step [68/188], loss=82.7446
	step [69/188], loss=87.7865
	step [70/188], loss=79.3307
	step [71/188], loss=88.2631
	step [72/188], loss=83.5723
	step [73/188], loss=84.5536
	step [74/188], loss=101.2255
	step [75/188], loss=81.2873
	step [76/188], loss=75.7506
	step [77/188], loss=84.9009
	step [78/188], loss=85.0821
	step [79/188], loss=78.9145
	step [80/188], loss=76.3046
	step [81/188], loss=88.3293
	step [82/188], loss=79.7496
	step [83/188], loss=97.3695
	step [84/188], loss=82.0740
	step [85/188], loss=91.8124
	step [86/188], loss=94.8845
	step [87/188], loss=93.2635
	step [88/188], loss=90.2698
	step [89/188], loss=91.9949
	step [90/188], loss=101.0914
	step [91/188], loss=85.9635
	step [92/188], loss=92.9208
	step [93/188], loss=88.2115
	step [94/188], loss=87.6814
	step [95/188], loss=88.6515
	step [96/188], loss=82.6259
	step [97/188], loss=76.4871
	step [98/188], loss=86.6997
	step [99/188], loss=108.1089
	step [100/188], loss=78.6730
	step [101/188], loss=97.7081
	step [102/188], loss=100.1683
	step [103/188], loss=90.7438
	step [104/188], loss=95.4211
	step [105/188], loss=79.3022
	step [106/188], loss=108.4689
	step [107/188], loss=82.8428
	step [108/188], loss=88.8310
	step [109/188], loss=92.3906
	step [110/188], loss=97.1509
	step [111/188], loss=82.2046
	step [112/188], loss=92.9088
	step [113/188], loss=80.2579
	step [114/188], loss=86.8691
	step [115/188], loss=76.5604
	step [116/188], loss=83.5894
	step [117/188], loss=87.7569
	step [118/188], loss=77.7468
	step [119/188], loss=85.5723
	step [120/188], loss=81.5899
	step [121/188], loss=81.6932
	step [122/188], loss=90.0081
	step [123/188], loss=101.6105
	step [124/188], loss=72.5744
	step [125/188], loss=81.6844
	step [126/188], loss=77.7200
	step [127/188], loss=87.1702
	step [128/188], loss=97.1763
	step [129/188], loss=79.2664
	step [130/188], loss=77.5417
	step [131/188], loss=97.2094
	step [132/188], loss=85.9264
	step [133/188], loss=101.1191
	step [134/188], loss=99.0121
	step [135/188], loss=92.2157
	step [136/188], loss=92.1912
	step [137/188], loss=78.0590
	step [138/188], loss=85.8504
	step [139/188], loss=84.2781
	step [140/188], loss=84.8886
	step [141/188], loss=86.5730
	step [142/188], loss=89.2324
	step [143/188], loss=77.3793
	step [144/188], loss=92.3037
	step [145/188], loss=86.8299
	step [146/188], loss=76.7433
	step [147/188], loss=87.9436
	step [148/188], loss=93.4060
	step [149/188], loss=88.6148
	step [150/188], loss=98.6877
	step [151/188], loss=83.3721
	step [152/188], loss=95.4494
	step [153/188], loss=87.7867
	step [154/188], loss=94.9066
	step [155/188], loss=84.3743
	step [156/188], loss=82.9791
	step [157/188], loss=92.5264
	step [158/188], loss=87.1791
	step [159/188], loss=92.4848
	step [160/188], loss=84.2056
	step [161/188], loss=100.9136
	step [162/188], loss=91.4476
	step [163/188], loss=95.4798
	step [164/188], loss=94.3844
	step [165/188], loss=79.2437
	step [166/188], loss=81.4207
	step [167/188], loss=91.0713
	step [168/188], loss=100.5176
	step [169/188], loss=83.1530
	step [170/188], loss=105.7032
	step [171/188], loss=82.5422
	step [172/188], loss=74.9218
	step [173/188], loss=90.0370
	step [174/188], loss=86.2420
	step [175/188], loss=86.6985
	step [176/188], loss=85.1884
	step [177/188], loss=95.1322
	step [178/188], loss=96.2874
	step [179/188], loss=97.3406
	step [180/188], loss=80.0266
	step [181/188], loss=94.9704
	step [182/188], loss=89.3213
	step [183/188], loss=90.9438
	step [184/188], loss=96.6198
	step [185/188], loss=102.0149
	step [186/188], loss=88.4268
	step [187/188], loss=83.5914
	step [188/188], loss=45.5268
	Evaluating
	loss=0.0153, precision=0.3317, recall=0.9187, f1=0.4874
Training epoch 28
	step [1/188], loss=76.5685
	step [2/188], loss=102.1129
	step [3/188], loss=88.9600
	step [4/188], loss=85.3470
	step [5/188], loss=90.6047
	step [6/188], loss=89.4765
	step [7/188], loss=86.8766
	step [8/188], loss=95.3965
	step [9/188], loss=95.8666
	step [10/188], loss=100.5515
	step [11/188], loss=92.9604
	step [12/188], loss=88.2745
	step [13/188], loss=82.1112
	step [14/188], loss=93.8261
	step [15/188], loss=94.0096
	step [16/188], loss=105.0418
	step [17/188], loss=85.2838
	step [18/188], loss=85.1845
	step [19/188], loss=86.7980
	step [20/188], loss=86.2063
	step [21/188], loss=82.8653
	step [22/188], loss=93.4149
	step [23/188], loss=90.1981
	step [24/188], loss=73.8475
	step [25/188], loss=83.4158
	step [26/188], loss=90.8657
	step [27/188], loss=118.0133
	step [28/188], loss=74.5701
	step [29/188], loss=67.7170
	step [30/188], loss=92.9465
	step [31/188], loss=91.2759
	step [32/188], loss=85.2056
	step [33/188], loss=91.2835
	step [34/188], loss=92.9208
	step [35/188], loss=92.0775
	step [36/188], loss=87.2108
	step [37/188], loss=97.5838
	step [38/188], loss=84.8599
	step [39/188], loss=85.7846
	step [40/188], loss=79.4405
	step [41/188], loss=80.8439
	step [42/188], loss=78.3642
	step [43/188], loss=84.8176
	step [44/188], loss=78.6459
	step [45/188], loss=96.6312
	step [46/188], loss=92.1862
	step [47/188], loss=85.3286
	step [48/188], loss=73.3836
	step [49/188], loss=90.9342
	step [50/188], loss=87.1252
	step [51/188], loss=73.6303
	step [52/188], loss=69.6310
	step [53/188], loss=74.5487
	step [54/188], loss=86.2067
	step [55/188], loss=86.5025
	step [56/188], loss=98.8500
	step [57/188], loss=107.5300
	step [58/188], loss=95.0173
	step [59/188], loss=82.3781
	step [60/188], loss=89.7667
	step [61/188], loss=83.5588
	step [62/188], loss=84.3773
	step [63/188], loss=83.8545
	step [64/188], loss=82.2916
	step [65/188], loss=83.5975
	step [66/188], loss=68.3828
	step [67/188], loss=77.6929
	step [68/188], loss=83.8530
	step [69/188], loss=86.2027
	step [70/188], loss=81.1685
	step [71/188], loss=79.0884
	step [72/188], loss=60.5690
	step [73/188], loss=111.4280
	step [74/188], loss=79.3416
	step [75/188], loss=86.9085
	step [76/188], loss=79.0209
	step [77/188], loss=78.2332
	step [78/188], loss=90.7533
	step [79/188], loss=96.4553
	step [80/188], loss=78.0638
	step [81/188], loss=83.1753
	step [82/188], loss=93.2205
	step [83/188], loss=96.3990
	step [84/188], loss=87.1923
	step [85/188], loss=101.0066
	step [86/188], loss=91.4218
	step [87/188], loss=93.7123
	step [88/188], loss=93.8862
	step [89/188], loss=94.2216
	step [90/188], loss=80.3342
	step [91/188], loss=98.1518
	step [92/188], loss=96.3985
	step [93/188], loss=82.2352
	step [94/188], loss=95.5564
	step [95/188], loss=80.9159
	step [96/188], loss=81.4231
	step [97/188], loss=81.9378
	step [98/188], loss=82.1347
	step [99/188], loss=89.9575
	step [100/188], loss=96.8887
	step [101/188], loss=84.8326
	step [102/188], loss=107.5937
	step [103/188], loss=89.4295
	step [104/188], loss=78.7674
	step [105/188], loss=82.7108
	step [106/188], loss=80.3483
	step [107/188], loss=96.4005
	step [108/188], loss=78.3830
	step [109/188], loss=86.8950
	step [110/188], loss=100.3613
	step [111/188], loss=85.9564
	step [112/188], loss=76.2233
	step [113/188], loss=90.9074
	step [114/188], loss=85.1749
	step [115/188], loss=72.8733
	step [116/188], loss=82.1506
	step [117/188], loss=95.8634
	step [118/188], loss=103.4469
	step [119/188], loss=92.0159
	step [120/188], loss=91.3631
	step [121/188], loss=90.0036
	step [122/188], loss=107.3163
	step [123/188], loss=87.0495
	step [124/188], loss=87.0444
	step [125/188], loss=91.7599
	step [126/188], loss=74.7684
	step [127/188], loss=83.3448
	step [128/188], loss=79.9680
	step [129/188], loss=81.4546
	step [130/188], loss=81.9367
	step [131/188], loss=105.4119
	step [132/188], loss=100.0334
	step [133/188], loss=84.3314
	step [134/188], loss=85.5712
	step [135/188], loss=87.1889
	step [136/188], loss=87.3792
	step [137/188], loss=84.6925
	step [138/188], loss=96.2587
	step [139/188], loss=84.8985
	step [140/188], loss=94.8564
	step [141/188], loss=91.7640
	step [142/188], loss=88.5517
	step [143/188], loss=79.6542
	step [144/188], loss=90.1699
	step [145/188], loss=88.4941
	step [146/188], loss=81.6347
	step [147/188], loss=91.3150
	step [148/188], loss=103.3538
	step [149/188], loss=83.5346
	step [150/188], loss=106.4082
	step [151/188], loss=74.8473
	step [152/188], loss=89.6046
	step [153/188], loss=95.6711
	step [154/188], loss=94.8522
	step [155/188], loss=106.4226
	step [156/188], loss=81.7443
	step [157/188], loss=75.9200
	step [158/188], loss=81.7656
	step [159/188], loss=86.8189
	step [160/188], loss=83.4967
	step [161/188], loss=90.7826
	step [162/188], loss=83.5140
	step [163/188], loss=93.2448
	step [164/188], loss=89.7385
	step [165/188], loss=82.6081
	step [166/188], loss=86.5559
	step [167/188], loss=90.1196
	step [168/188], loss=86.8262
	step [169/188], loss=104.8504
	step [170/188], loss=87.1544
	step [171/188], loss=89.6514
	step [172/188], loss=74.1872
	step [173/188], loss=74.0463
	step [174/188], loss=85.8299
	step [175/188], loss=79.5062
	step [176/188], loss=100.2682
	step [177/188], loss=91.6724
	step [178/188], loss=92.2094
	step [179/188], loss=90.0154
	step [180/188], loss=99.4077
	step [181/188], loss=90.8095
	step [182/188], loss=91.5309
	step [183/188], loss=93.2609
	step [184/188], loss=96.3532
	step [185/188], loss=77.8481
	step [186/188], loss=99.4157
	step [187/188], loss=84.6751
	step [188/188], loss=45.2074
	Evaluating
	loss=0.0125, precision=0.3927, recall=0.9132, f1=0.5492
Training epoch 29
	step [1/188], loss=87.0397
	step [2/188], loss=94.7783
	step [3/188], loss=92.6782
	step [4/188], loss=90.5329
	step [5/188], loss=90.4320
	step [6/188], loss=84.1407
	step [7/188], loss=79.2781
	step [8/188], loss=79.1570
	step [9/188], loss=86.8081
	step [10/188], loss=90.7984
	step [11/188], loss=86.8618
	step [12/188], loss=95.8409
	step [13/188], loss=95.1755
	step [14/188], loss=102.4539
	step [15/188], loss=95.7789
	step [16/188], loss=88.4360
	step [17/188], loss=84.4106
	step [18/188], loss=82.9815
	step [19/188], loss=88.8328
	step [20/188], loss=83.0755
	step [21/188], loss=78.2429
	step [22/188], loss=74.1919
	step [23/188], loss=108.7097
	step [24/188], loss=81.6790
	step [25/188], loss=93.9714
	step [26/188], loss=88.9492
	step [27/188], loss=88.6262
	step [28/188], loss=90.9429
	step [29/188], loss=84.2825
	step [30/188], loss=79.4739
	step [31/188], loss=85.8641
	step [32/188], loss=83.4360
	step [33/188], loss=96.7361
	step [34/188], loss=96.7142
	step [35/188], loss=95.2542
	step [36/188], loss=74.5370
	step [37/188], loss=82.2691
	step [38/188], loss=89.8024
	step [39/188], loss=82.0475
	step [40/188], loss=82.1494
	step [41/188], loss=83.0044
	step [42/188], loss=92.3683
	step [43/188], loss=100.2809
	step [44/188], loss=79.0890
	step [45/188], loss=83.8328
	step [46/188], loss=78.2954
	step [47/188], loss=91.6202
	step [48/188], loss=90.1012
	step [49/188], loss=83.1275
	step [50/188], loss=82.6255
	step [51/188], loss=100.4586
	step [52/188], loss=88.9335
	step [53/188], loss=94.4206
	step [54/188], loss=83.0739
	step [55/188], loss=77.2897
	step [56/188], loss=81.8155
	step [57/188], loss=93.0988
	step [58/188], loss=83.3978
	step [59/188], loss=91.6147
	step [60/188], loss=84.4065
	step [61/188], loss=73.7916
	step [62/188], loss=79.5945
	step [63/188], loss=93.1308
	step [64/188], loss=73.1187
	step [65/188], loss=83.2725
	step [66/188], loss=85.2824
	step [67/188], loss=90.2337
	step [68/188], loss=84.6096
	step [69/188], loss=87.2171
	step [70/188], loss=95.7775
	step [71/188], loss=76.0723
	step [72/188], loss=85.2975
	step [73/188], loss=89.3194
	step [74/188], loss=92.2022
	step [75/188], loss=101.9468
	step [76/188], loss=86.4906
	step [77/188], loss=82.1613
	step [78/188], loss=88.8468
	step [79/188], loss=77.4179
	step [80/188], loss=84.5093
	step [81/188], loss=78.4801
	step [82/188], loss=76.7410
	step [83/188], loss=82.3519
	step [84/188], loss=74.5851
	step [85/188], loss=95.1567
	step [86/188], loss=102.9362
	step [87/188], loss=81.8285
	step [88/188], loss=80.4323
	step [89/188], loss=99.4430
	step [90/188], loss=96.7292
	step [91/188], loss=92.9824
	step [92/188], loss=102.4964
	step [93/188], loss=82.5252
	step [94/188], loss=102.6840
	step [95/188], loss=89.3447
	step [96/188], loss=91.7018
	step [97/188], loss=77.1193
	step [98/188], loss=87.9114
	step [99/188], loss=88.8078
	step [100/188], loss=73.5872
	step [101/188], loss=93.6783
	step [102/188], loss=86.4735
	step [103/188], loss=89.6160
	step [104/188], loss=84.9693
	step [105/188], loss=80.1676
	step [106/188], loss=80.9587
	step [107/188], loss=85.9441
	step [108/188], loss=77.6487
	step [109/188], loss=84.0680
	step [110/188], loss=78.6390
	step [111/188], loss=88.5589
	step [112/188], loss=84.1045
	step [113/188], loss=91.4034
	step [114/188], loss=90.4662
	step [115/188], loss=75.7758
	step [116/188], loss=91.7551
	step [117/188], loss=88.6365
	step [118/188], loss=78.7556
	step [119/188], loss=100.4424
	step [120/188], loss=91.4582
	step [121/188], loss=78.3346
	step [122/188], loss=79.4062
	step [123/188], loss=91.4478
	step [124/188], loss=103.9281
	step [125/188], loss=79.8966
	step [126/188], loss=84.8365
	step [127/188], loss=96.5557
	step [128/188], loss=80.8378
	step [129/188], loss=90.5547
	step [130/188], loss=78.2353
	step [131/188], loss=74.7888
	step [132/188], loss=91.9535
	step [133/188], loss=79.2275
	step [134/188], loss=82.6162
	step [135/188], loss=93.9045
	step [136/188], loss=85.2683
	step [137/188], loss=85.5365
	step [138/188], loss=80.2958
	step [139/188], loss=89.9543
	step [140/188], loss=83.2910
	step [141/188], loss=97.3251
	step [142/188], loss=97.4882
	step [143/188], loss=91.7540
	step [144/188], loss=78.4308
	step [145/188], loss=80.6879
	step [146/188], loss=94.6708
	step [147/188], loss=96.1755
	step [148/188], loss=80.9436
	step [149/188], loss=98.5137
	step [150/188], loss=81.0282
	step [151/188], loss=86.3806
	step [152/188], loss=90.0927
	step [153/188], loss=81.6224
	step [154/188], loss=94.9952
	step [155/188], loss=89.1205
	step [156/188], loss=84.4747
	step [157/188], loss=73.0501
	step [158/188], loss=86.6270
	step [159/188], loss=89.5865
	step [160/188], loss=70.2747
	step [161/188], loss=86.9979
	step [162/188], loss=94.3319
	step [163/188], loss=89.4256
	step [164/188], loss=77.0556
	step [165/188], loss=93.3143
	step [166/188], loss=83.2339
	step [167/188], loss=92.2252
	step [168/188], loss=93.3074
	step [169/188], loss=78.0903
	step [170/188], loss=83.2884
	step [171/188], loss=89.7780
	step [172/188], loss=83.5786
	step [173/188], loss=81.1767
	step [174/188], loss=86.3795
	step [175/188], loss=91.8516
	step [176/188], loss=102.1550
	step [177/188], loss=87.4608
	step [178/188], loss=84.4227
	step [179/188], loss=65.6280
	step [180/188], loss=84.6031
	step [181/188], loss=93.3758
	step [182/188], loss=85.5201
	step [183/188], loss=77.1625
	step [184/188], loss=88.3407
	step [185/188], loss=88.0845
	step [186/188], loss=84.4832
	step [187/188], loss=82.6734
	step [188/188], loss=55.0604
	Evaluating
	loss=0.0117, precision=0.3976, recall=0.9015, f1=0.5518
Training epoch 30
	step [1/188], loss=80.0395
	step [2/188], loss=87.4718
	step [3/188], loss=86.0749
	step [4/188], loss=104.1752
	step [5/188], loss=91.5538
	step [6/188], loss=86.5442
	step [7/188], loss=92.8915
	step [8/188], loss=79.4689
	step [9/188], loss=84.4047
	step [10/188], loss=89.8372
	step [11/188], loss=88.9811
	step [12/188], loss=73.0774
	step [13/188], loss=107.9547
	step [14/188], loss=92.6550
	step [15/188], loss=73.5394
	step [16/188], loss=99.4043
	step [17/188], loss=76.4700
	step [18/188], loss=67.4212
	step [19/188], loss=77.4459
	step [20/188], loss=94.3478
	step [21/188], loss=97.9600
	step [22/188], loss=91.0975
	step [23/188], loss=89.7620
	step [24/188], loss=87.0341
	step [25/188], loss=92.8526
	step [26/188], loss=92.4122
	step [27/188], loss=97.2248
	step [28/188], loss=81.0486
	step [29/188], loss=75.3508
	step [30/188], loss=76.7941
	step [31/188], loss=81.4937
	step [32/188], loss=88.6477
	step [33/188], loss=88.9308
	step [34/188], loss=97.7855
	step [35/188], loss=85.3611
	step [36/188], loss=93.4123
	step [37/188], loss=88.8836
	step [38/188], loss=83.1820
	step [39/188], loss=90.2843
	step [40/188], loss=83.5276
	step [41/188], loss=95.6941
	step [42/188], loss=90.9155
	step [43/188], loss=62.9146
	step [44/188], loss=84.8650
	step [45/188], loss=97.4071
	step [46/188], loss=97.9436
	step [47/188], loss=81.2089
	step [48/188], loss=85.1007
	step [49/188], loss=87.0231
	step [50/188], loss=82.3592
	step [51/188], loss=79.3758
	step [52/188], loss=89.2462
	step [53/188], loss=76.1451
	step [54/188], loss=75.9137
	step [55/188], loss=81.0564
	step [56/188], loss=78.6709
	step [57/188], loss=72.0877
	step [58/188], loss=91.9928
	step [59/188], loss=94.8438
	step [60/188], loss=76.1488
	step [61/188], loss=79.5132
	step [62/188], loss=90.5919
	step [63/188], loss=70.6607
	step [64/188], loss=90.5667
	step [65/188], loss=85.6869
	step [66/188], loss=69.0769
	step [67/188], loss=76.2290
	step [68/188], loss=78.1378
	step [69/188], loss=90.8003
	step [70/188], loss=82.1071
	step [71/188], loss=87.7215
	step [72/188], loss=88.9557
	step [73/188], loss=95.8669
	step [74/188], loss=94.8494
	step [75/188], loss=93.5631
	step [76/188], loss=95.1557
	step [77/188], loss=80.2622
	step [78/188], loss=79.9229
	step [79/188], loss=101.0607
	step [80/188], loss=79.2959
	step [81/188], loss=77.1528
	step [82/188], loss=100.2680
	step [83/188], loss=90.7127
	step [84/188], loss=80.0899
	step [85/188], loss=82.1406
	step [86/188], loss=82.9305
	step [87/188], loss=73.6996
	step [88/188], loss=96.8426
	step [89/188], loss=86.6559
	step [90/188], loss=106.8222
	step [91/188], loss=85.6596
	step [92/188], loss=76.4901
	step [93/188], loss=96.4589
	step [94/188], loss=91.6853
	step [95/188], loss=73.7692
	step [96/188], loss=93.9816
	step [97/188], loss=72.5274
	step [98/188], loss=93.0132
	step [99/188], loss=92.9820
	step [100/188], loss=86.8424
	step [101/188], loss=89.1634
	step [102/188], loss=77.7551
	step [103/188], loss=84.3999
	step [104/188], loss=89.8476
	step [105/188], loss=78.7616
	step [106/188], loss=86.7093
	step [107/188], loss=80.0995
	step [108/188], loss=83.5192
	step [109/188], loss=84.8468
	step [110/188], loss=85.7044
	step [111/188], loss=90.1928
	step [112/188], loss=96.9377
	step [113/188], loss=83.8818
	step [114/188], loss=82.2469
	step [115/188], loss=92.8377
	step [116/188], loss=86.3596
	step [117/188], loss=65.7532
	step [118/188], loss=96.9944
	step [119/188], loss=70.2205
	step [120/188], loss=72.9516
	step [121/188], loss=87.6278
	step [122/188], loss=94.9774
	step [123/188], loss=89.4078
	step [124/188], loss=87.3183
	step [125/188], loss=85.9021
	step [126/188], loss=84.2077
	step [127/188], loss=89.1779
	step [128/188], loss=108.9363
	step [129/188], loss=107.4330
	step [130/188], loss=91.3141
	step [131/188], loss=87.7066
	step [132/188], loss=83.1904
	step [133/188], loss=68.0902
	step [134/188], loss=81.5576
	step [135/188], loss=78.0554
	step [136/188], loss=92.2981
	step [137/188], loss=78.0616
	step [138/188], loss=87.3752
	step [139/188], loss=93.2348
	step [140/188], loss=76.4135
	step [141/188], loss=88.7408
	step [142/188], loss=66.8475
	step [143/188], loss=86.0285
	step [144/188], loss=76.0780
	step [145/188], loss=66.4624
	step [146/188], loss=104.8767
	step [147/188], loss=94.9255
	step [148/188], loss=86.1579
	step [149/188], loss=97.4632
	step [150/188], loss=83.8566
	step [151/188], loss=85.3951
	step [152/188], loss=85.0807
	step [153/188], loss=78.1653
	step [154/188], loss=92.0734
	step [155/188], loss=89.3671
	step [156/188], loss=89.0692
	step [157/188], loss=84.3590
	step [158/188], loss=88.3610
	step [159/188], loss=78.5382
	step [160/188], loss=95.5970
	step [161/188], loss=80.8872
	step [162/188], loss=92.5429
	step [163/188], loss=91.7063
	step [164/188], loss=93.9295
	step [165/188], loss=97.0115
	step [166/188], loss=83.8641
	step [167/188], loss=83.2353
	step [168/188], loss=97.7937
	step [169/188], loss=85.6973
	step [170/188], loss=84.8033
	step [171/188], loss=85.7211
	step [172/188], loss=72.2598
	step [173/188], loss=83.5409
	step [174/188], loss=81.7065
	step [175/188], loss=74.8181
	step [176/188], loss=81.3383
	step [177/188], loss=82.5395
	step [178/188], loss=82.8868
	step [179/188], loss=96.3864
	step [180/188], loss=93.5126
	step [181/188], loss=87.1244
	step [182/188], loss=91.5011
	step [183/188], loss=88.5191
	step [184/188], loss=82.5648
	step [185/188], loss=78.5405
	step [186/188], loss=98.7148
	step [187/188], loss=93.4691
	step [188/188], loss=54.9687
	Evaluating
	loss=0.0111, precision=0.4156, recall=0.9065, f1=0.5700
Training finished
best_f1: 0.6292440195901237
directing: X rim_enhanced: True test_id 3
removed wrong scan: weights_X_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_22_xwqg-B00034_2020-04-03.npy
# all image files: 12135 # all weight files in weight_dir: 9361 # image files with weight 9310
removed wrong scan: weights_X_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_22_xwqg-B00034_2020-04-03.npy
# all image files: 12135 # all weight files in weight_dir: 2522 # image files with weight 2522
train: /ibex/scratch/projects/c2052/air_tube_seg/training_samples_enhanced_one/X 9310
Using 4 GPUs
Going to train epochs [1-30]
Training epoch 1
	step [1/146], loss=428.3256
	step [2/146], loss=310.3395
	step [3/146], loss=296.5118
	step [4/146], loss=281.0111
	step [5/146], loss=321.7977
	step [6/146], loss=322.7580
	step [7/146], loss=253.8826
	step [8/146], loss=301.6069
	step [9/146], loss=288.5364
	step [10/146], loss=317.5077
	step [11/146], loss=244.2775
	step [12/146], loss=263.7118
	step [13/146], loss=264.6200
	step [14/146], loss=246.5871
	step [15/146], loss=250.5736
	step [16/146], loss=252.0138
	step [17/146], loss=250.8964
	step [18/146], loss=265.1992
	step [19/146], loss=248.3476
	step [20/146], loss=261.3655
	step [21/146], loss=251.7196
	step [22/146], loss=243.8083
	step [23/146], loss=239.7224
	step [24/146], loss=255.8468
	step [25/146], loss=282.6694
	step [26/146], loss=251.6453
	step [27/146], loss=228.5444
	step [28/146], loss=237.9258
	step [29/146], loss=235.7105
	step [30/146], loss=226.6069
	step [31/146], loss=219.4192
	step [32/146], loss=269.1088
	step [33/146], loss=259.4700
	step [34/146], loss=206.4299
	step [35/146], loss=241.3154
	step [36/146], loss=230.8009
	step [37/146], loss=236.8671
	step [38/146], loss=240.8035
	step [39/146], loss=213.6395
	step [40/146], loss=233.9109
	step [41/146], loss=238.9194
	step [42/146], loss=215.9428
	step [43/146], loss=252.0020
	step [44/146], loss=240.8332
	step [45/146], loss=233.4120
	step [46/146], loss=251.5906
	step [47/146], loss=220.3362
	step [48/146], loss=245.5869
	step [49/146], loss=212.2572
	step [50/146], loss=229.1502
	step [51/146], loss=197.1008
	step [52/146], loss=225.0231
	step [53/146], loss=225.8935
	step [54/146], loss=197.8852
	step [55/146], loss=245.8019
	step [56/146], loss=238.5096
	step [57/146], loss=190.8016
	step [58/146], loss=208.7091
	step [59/146], loss=214.8382
	step [60/146], loss=205.6097
	step [61/146], loss=212.9987
	step [62/146], loss=222.5354
	step [63/146], loss=216.0381
	step [64/146], loss=215.8080
	step [65/146], loss=209.6678
	step [66/146], loss=186.9759
	step [67/146], loss=189.3370
	step [68/146], loss=220.3610
	step [69/146], loss=203.4515
	step [70/146], loss=206.6092
	step [71/146], loss=189.8063
	step [72/146], loss=189.4466
	step [73/146], loss=218.9184
	step [74/146], loss=194.4714
	step [75/146], loss=226.5280
	step [76/146], loss=222.4201
	step [77/146], loss=231.9735
	step [78/146], loss=186.6213
	step [79/146], loss=233.5315
	step [80/146], loss=208.6889
	step [81/146], loss=208.9513
	step [82/146], loss=186.0252
	step [83/146], loss=193.0364
	step [84/146], loss=190.1110
	step [85/146], loss=195.5925
	step [86/146], loss=196.2215
	step [87/146], loss=216.9846
	step [88/146], loss=176.6033
	step [89/146], loss=194.5431
	step [90/146], loss=217.6498
	step [91/146], loss=172.0827
	step [92/146], loss=204.0363
	step [93/146], loss=168.0267
	step [94/146], loss=216.9211
	step [95/146], loss=192.4200
	step [96/146], loss=168.5667
	step [97/146], loss=171.9395
	step [98/146], loss=198.3832
	step [99/146], loss=213.2578
	step [100/146], loss=179.3317
	step [101/146], loss=214.9005
	step [102/146], loss=193.4851
	step [103/146], loss=199.8010
	step [104/146], loss=193.7029
	step [105/146], loss=184.4960
	step [106/146], loss=200.1872
	step [107/146], loss=173.4776
	step [108/146], loss=178.7741
	step [109/146], loss=205.1136
	step [110/146], loss=211.8436
	step [111/146], loss=210.7538
	step [112/146], loss=206.5421
	step [113/146], loss=205.1746
	step [114/146], loss=179.8460
	step [115/146], loss=182.6279
	step [116/146], loss=180.2949
	step [117/146], loss=185.3929
	step [118/146], loss=187.9091
	step [119/146], loss=165.5144
	step [120/146], loss=189.8317
	step [121/146], loss=214.5148
	step [122/146], loss=183.9416
	step [123/146], loss=196.7030
	step [124/146], loss=200.8287
	step [125/146], loss=175.1260
	step [126/146], loss=188.0296
	step [127/146], loss=177.5299
	step [128/146], loss=188.3716
	step [129/146], loss=183.2327
	step [130/146], loss=166.2136
	step [131/146], loss=193.4104
	step [132/146], loss=203.7778
	step [133/146], loss=189.8988
	step [134/146], loss=192.9583
	step [135/146], loss=183.0146
	step [136/146], loss=203.7307
	step [137/146], loss=179.8845
	step [138/146], loss=200.6467
	step [139/146], loss=198.1645
	step [140/146], loss=181.0758
	step [141/146], loss=177.3848
	step [142/146], loss=167.9760
	step [143/146], loss=168.8173
	step [144/146], loss=189.4925
	step [145/146], loss=173.7649
	step [146/146], loss=82.4025
	Evaluating
	loss=0.3115, precision=0.1931, recall=0.9305, f1=0.3198
saving model as: 3_saved_model.pth
Training epoch 2
	step [1/146], loss=199.5036
	step [2/146], loss=178.7190
	step [3/146], loss=183.3232
	step [4/146], loss=196.3048
	step [5/146], loss=202.3521
	step [6/146], loss=186.3998
	step [7/146], loss=159.8815
	step [8/146], loss=179.7283
	step [9/146], loss=191.2124
	step [10/146], loss=167.3970
	step [11/146], loss=210.0875
	step [12/146], loss=161.2333
	step [13/146], loss=187.3699
	step [14/146], loss=174.9706
	step [15/146], loss=189.2422
	step [16/146], loss=207.4902
	step [17/146], loss=169.0665
	step [18/146], loss=156.1672
	step [19/146], loss=165.9617
	step [20/146], loss=170.2231
	step [21/146], loss=184.5504
	step [22/146], loss=162.3397
	step [23/146], loss=165.9463
	step [24/146], loss=192.2549
	step [25/146], loss=175.0193
	step [26/146], loss=182.5903
	step [27/146], loss=204.7286
	step [28/146], loss=191.0806
	step [29/146], loss=179.8100
	step [30/146], loss=204.3461
	step [31/146], loss=179.5984
	step [32/146], loss=177.1174
	step [33/146], loss=173.6168
	step [34/146], loss=186.3314
	step [35/146], loss=164.8455
	step [36/146], loss=182.5385
	step [37/146], loss=181.1371
	step [38/146], loss=182.7524
	step [39/146], loss=182.0968
	step [40/146], loss=182.8636
	step [41/146], loss=194.5337
	step [42/146], loss=167.2854
	step [43/146], loss=183.2611
	step [44/146], loss=177.4266
	step [45/146], loss=178.5247
	step [46/146], loss=165.9086
	step [47/146], loss=158.2830
	step [48/146], loss=189.8390
	step [49/146], loss=170.7656
	step [50/146], loss=162.7600
	step [51/146], loss=182.8834
	step [52/146], loss=191.4830
	step [53/146], loss=177.7716
	step [54/146], loss=201.8367
	step [55/146], loss=157.4174
	step [56/146], loss=179.0372
	step [57/146], loss=169.2768
	step [58/146], loss=177.5661
	step [59/146], loss=189.1119
	step [60/146], loss=178.4423
	step [61/146], loss=196.0313
	step [62/146], loss=190.8403
	step [63/146], loss=190.6523
	step [64/146], loss=180.8727
	step [65/146], loss=181.4711
	step [66/146], loss=164.2986
	step [67/146], loss=186.7306
	step [68/146], loss=172.8019
	step [69/146], loss=186.7015
	step [70/146], loss=167.0011
	step [71/146], loss=163.6332
	step [72/146], loss=182.7205
	step [73/146], loss=153.1699
	step [74/146], loss=173.6244
	step [75/146], loss=173.1974
	step [76/146], loss=156.9729
	step [77/146], loss=193.9337
	step [78/146], loss=164.8649
	step [79/146], loss=174.9402
	step [80/146], loss=170.2104
	step [81/146], loss=165.7196
	step [82/146], loss=166.0953
	step [83/146], loss=170.7906
	step [84/146], loss=159.2077
	step [85/146], loss=151.7397
	step [86/146], loss=168.7423
	step [87/146], loss=157.2989
	step [88/146], loss=186.5388
	step [89/146], loss=158.1394
	step [90/146], loss=172.3993
	step [91/146], loss=174.6438
	step [92/146], loss=161.7882
	step [93/146], loss=157.5112
	step [94/146], loss=194.8460
	step [95/146], loss=183.6782
	step [96/146], loss=133.8656
	step [97/146], loss=166.4936
	step [98/146], loss=159.3206
	step [99/146], loss=199.0556
	step [100/146], loss=171.3869
	step [101/146], loss=166.7524
	step [102/146], loss=178.5882
	step [103/146], loss=184.4697
	step [104/146], loss=166.3651
	step [105/146], loss=167.3946
	step [106/146], loss=170.8863
	step [107/146], loss=180.0708
	step [108/146], loss=171.8965
	step [109/146], loss=163.5068
	step [110/146], loss=171.2972
	step [111/146], loss=165.6247
	step [112/146], loss=201.1815
	step [113/146], loss=182.1849
	step [114/146], loss=193.7604
	step [115/146], loss=169.3818
	step [116/146], loss=172.1638
	step [117/146], loss=164.7796
	step [118/146], loss=167.7928
	step [119/146], loss=173.8295
	step [120/146], loss=164.8840
	step [121/146], loss=171.6325
	step [122/146], loss=178.5618
	step [123/146], loss=171.1100
	step [124/146], loss=169.4817
	step [125/146], loss=210.4601
	step [126/146], loss=175.4095
	step [127/146], loss=169.3077
	step [128/146], loss=175.0446
	step [129/146], loss=150.9810
	step [130/146], loss=175.8876
	step [131/146], loss=173.5313
	step [132/146], loss=163.3717
	step [133/146], loss=150.8402
	step [134/146], loss=187.9788
	step [135/146], loss=166.4367
	step [136/146], loss=152.4747
	step [137/146], loss=168.9286
	step [138/146], loss=174.9838
	step [139/146], loss=205.0524
	step [140/146], loss=164.5448
	step [141/146], loss=170.6680
	step [142/146], loss=147.8300
	step [143/146], loss=169.8332
	step [144/146], loss=162.7050
	step [145/146], loss=186.5627
	step [146/146], loss=76.5268
	Evaluating
	loss=0.2345, precision=0.4617, recall=0.9117, f1=0.6129
saving model as: 3_saved_model.pth
Training epoch 3
	step [1/146], loss=151.1393
	step [2/146], loss=160.2733
	step [3/146], loss=158.6229
	step [4/146], loss=153.1253
	step [5/146], loss=159.7566
	step [6/146], loss=161.4729
	step [7/146], loss=193.4814
	step [8/146], loss=172.9213
	step [9/146], loss=189.9760
	step [10/146], loss=149.5659
	step [11/146], loss=166.5346
	step [12/146], loss=153.4177
	step [13/146], loss=179.6733
	step [14/146], loss=170.3292
	step [15/146], loss=182.7542
	step [16/146], loss=184.1240
	step [17/146], loss=176.6372
	step [18/146], loss=147.5744
	step [19/146], loss=166.5492
	step [20/146], loss=153.1674
	step [21/146], loss=156.6403
	step [22/146], loss=166.1397
	step [23/146], loss=168.6501
	step [24/146], loss=154.4938
	step [25/146], loss=186.9530
	step [26/146], loss=160.9537
	step [27/146], loss=176.0780
	step [28/146], loss=147.6392
	step [29/146], loss=162.3868
	step [30/146], loss=166.0116
	step [31/146], loss=175.9975
	step [32/146], loss=174.0176
	step [33/146], loss=165.7731
	step [34/146], loss=164.2380
	step [35/146], loss=182.3543
	step [36/146], loss=175.7851
	step [37/146], loss=154.5563
	step [38/146], loss=149.2647
	step [39/146], loss=165.1328
	step [40/146], loss=178.7252
	step [41/146], loss=186.1554
	step [42/146], loss=150.7563
	step [43/146], loss=180.4135
	step [44/146], loss=155.9222
	step [45/146], loss=164.2100
	step [46/146], loss=153.8726
	step [47/146], loss=159.2217
	step [48/146], loss=175.6454
	step [49/146], loss=160.3665
	step [50/146], loss=188.8563
	step [51/146], loss=163.0446
	step [52/146], loss=154.2850
	step [53/146], loss=151.6507
	step [54/146], loss=157.4580
	step [55/146], loss=177.9888
	step [56/146], loss=177.3789
	step [57/146], loss=156.1444
	step [58/146], loss=159.2767
	step [59/146], loss=147.1628
	step [60/146], loss=151.2710
	step [61/146], loss=183.9195
	step [62/146], loss=174.8260
	step [63/146], loss=129.4001
	step [64/146], loss=163.7202
	step [65/146], loss=152.1758
	step [66/146], loss=180.2678
	step [67/146], loss=181.8100
	step [68/146], loss=172.1729
	step [69/146], loss=169.8672
	step [70/146], loss=165.9143
	step [71/146], loss=162.8396
	step [72/146], loss=165.7977
	step [73/146], loss=183.6361
	step [74/146], loss=137.3309
	step [75/146], loss=157.5748
	step [76/146], loss=156.1519
	step [77/146], loss=159.3430
	step [78/146], loss=167.4451
	step [79/146], loss=173.0340
	step [80/146], loss=156.8742
	step [81/146], loss=170.7325
	step [82/146], loss=171.6158
	step [83/146], loss=165.3621
	step [84/146], loss=130.9370
	step [85/146], loss=151.4588
	step [86/146], loss=162.7789
	step [87/146], loss=167.0563
	step [88/146], loss=151.2210
	step [89/146], loss=173.2392
	step [90/146], loss=151.7658
	step [91/146], loss=160.1602
	step [92/146], loss=183.5974
	step [93/146], loss=165.0802
	step [94/146], loss=151.8092
	step [95/146], loss=154.4168
	step [96/146], loss=171.5759
	step [97/146], loss=146.5686
	step [98/146], loss=165.0869
	step [99/146], loss=135.9046
	step [100/146], loss=163.7951
	step [101/146], loss=143.5114
	step [102/146], loss=152.6270
	step [103/146], loss=151.8734
	step [104/146], loss=162.1853
	step [105/146], loss=156.1861
	step [106/146], loss=184.6489
	step [107/146], loss=148.4241
	step [108/146], loss=155.7216
	step [109/146], loss=156.7663
	step [110/146], loss=158.7578
	step [111/146], loss=185.5775
	step [112/146], loss=187.4718
	step [113/146], loss=162.4776
	step [114/146], loss=152.4988
	step [115/146], loss=146.9691
	step [116/146], loss=157.7245
	step [117/146], loss=158.2399
	step [118/146], loss=136.8555
	step [119/146], loss=171.8260
	step [120/146], loss=150.8499
	step [121/146], loss=145.4411
	step [122/146], loss=152.7258
	step [123/146], loss=152.1662
	step [124/146], loss=169.9224
	step [125/146], loss=179.9153
	step [126/146], loss=165.4908
	step [127/146], loss=164.3292
	step [128/146], loss=150.6633
	step [129/146], loss=164.4095
	step [130/146], loss=147.3879
	step [131/146], loss=184.4687
	step [132/146], loss=156.8876
	step [133/146], loss=143.7549
	step [134/146], loss=134.4833
	step [135/146], loss=140.9569
	step [136/146], loss=158.5825
	step [137/146], loss=160.7308
	step [138/146], loss=168.3324
	step [139/146], loss=140.3614
	step [140/146], loss=170.6671
	step [141/146], loss=152.9479
	step [142/146], loss=169.6953
	step [143/146], loss=177.2705
	step [144/146], loss=154.0471
	step [145/146], loss=132.1618
	step [146/146], loss=66.2724
	Evaluating
	loss=0.1882, precision=0.5334, recall=0.8929, f1=0.6679
saving model as: 3_saved_model.pth
Training epoch 4
	step [1/146], loss=169.8321
	step [2/146], loss=161.2509
	step [3/146], loss=164.2539
	step [4/146], loss=171.4097
	step [5/146], loss=143.5043
	step [6/146], loss=190.9369
	step [7/146], loss=148.4125
	step [8/146], loss=167.2190
	step [9/146], loss=151.2277
	step [10/146], loss=160.3894
	step [11/146], loss=173.7180
	step [12/146], loss=146.5936
	step [13/146], loss=145.5295
	step [14/146], loss=153.6281
	step [15/146], loss=143.9888
	step [16/146], loss=141.9548
	step [17/146], loss=152.5063
	step [18/146], loss=139.9429
	step [19/146], loss=151.1001
	step [20/146], loss=150.4195
	step [21/146], loss=166.0897
	step [22/146], loss=142.0514
	step [23/146], loss=153.1541
	step [24/146], loss=144.2924
	step [25/146], loss=112.9163
	step [26/146], loss=147.1498
	step [27/146], loss=157.4699
	step [28/146], loss=146.3742
	step [29/146], loss=135.4248
	step [30/146], loss=134.0627
	step [31/146], loss=154.9215
	step [32/146], loss=144.7249
	step [33/146], loss=164.7364
	step [34/146], loss=156.2687
	step [35/146], loss=148.2054
	step [36/146], loss=168.0382
	step [37/146], loss=143.5855
	step [38/146], loss=141.6735
	step [39/146], loss=153.0306
	step [40/146], loss=142.0246
	step [41/146], loss=139.0556
	step [42/146], loss=144.5638
	step [43/146], loss=182.4442
	step [44/146], loss=156.4823
	step [45/146], loss=148.1256
	step [46/146], loss=157.2795
	step [47/146], loss=161.8592
	step [48/146], loss=153.7783
	step [49/146], loss=160.5241
	step [50/146], loss=143.3158
	step [51/146], loss=163.8128
	step [52/146], loss=146.8416
	step [53/146], loss=150.0105
	step [54/146], loss=140.7606
	step [55/146], loss=164.0513
	step [56/146], loss=146.1511
	step [57/146], loss=158.6273
	step [58/146], loss=148.3773
	step [59/146], loss=174.2815
	step [60/146], loss=144.2848
	step [61/146], loss=150.5014
	step [62/146], loss=155.9121
	step [63/146], loss=154.4250
	step [64/146], loss=171.3885
	step [65/146], loss=154.1010
	step [66/146], loss=178.7177
	step [67/146], loss=152.6092
	step [68/146], loss=154.8940
	step [69/146], loss=161.9238
	step [70/146], loss=143.6647
	step [71/146], loss=148.7638
	step [72/146], loss=161.5700
	step [73/146], loss=153.0807
	step [74/146], loss=145.3887
	step [75/146], loss=143.6698
	step [76/146], loss=152.6781
	step [77/146], loss=155.8521
	step [78/146], loss=173.9212
	step [79/146], loss=136.9384
	step [80/146], loss=151.8734
	step [81/146], loss=150.3756
	step [82/146], loss=149.9310
	step [83/146], loss=154.4979
	step [84/146], loss=129.8377
	step [85/146], loss=136.1111
	step [86/146], loss=171.4054
	step [87/146], loss=149.1599
	step [88/146], loss=154.5937
	step [89/146], loss=173.3125
	step [90/146], loss=161.0480
	step [91/146], loss=152.6777
	step [92/146], loss=150.2896
	step [93/146], loss=147.7665
	step [94/146], loss=162.9461
	step [95/146], loss=150.5661
	step [96/146], loss=159.7748
	step [97/146], loss=143.1040
	step [98/146], loss=141.7740
	step [99/146], loss=127.1531
	step [100/146], loss=151.0501
	step [101/146], loss=154.3509
	step [102/146], loss=127.0822
	step [103/146], loss=142.0802
	step [104/146], loss=143.4046
	step [105/146], loss=158.9823
	step [106/146], loss=152.9711
	step [107/146], loss=167.1583
	step [108/146], loss=145.9470
	step [109/146], loss=163.6157
	step [110/146], loss=148.9261
	step [111/146], loss=147.5880
	step [112/146], loss=155.3789
	step [113/146], loss=150.7804
	step [114/146], loss=143.8410
	step [115/146], loss=153.4164
	step [116/146], loss=150.8604
	step [117/146], loss=140.2899
	step [118/146], loss=157.1384
	step [119/146], loss=172.6213
	step [120/146], loss=177.9207
	step [121/146], loss=161.4804
	step [122/146], loss=157.9094
	step [123/146], loss=152.3019
	step [124/146], loss=136.7224
	step [125/146], loss=154.6561
	step [126/146], loss=168.3426
	step [127/146], loss=158.7193
	step [128/146], loss=135.8665
	step [129/146], loss=138.7783
	step [130/146], loss=159.5306
	step [131/146], loss=137.5434
	step [132/146], loss=177.0652
	step [133/146], loss=169.0613
	step [134/146], loss=153.6686
	step [135/146], loss=175.5213
	step [136/146], loss=148.1863
	step [137/146], loss=135.1870
	step [138/146], loss=159.3780
	step [139/146], loss=146.3363
	step [140/146], loss=139.0246
	step [141/146], loss=130.0712
	step [142/146], loss=162.9774
	step [143/146], loss=146.6782
	step [144/146], loss=156.6358
	step [145/146], loss=142.9646
	step [146/146], loss=72.7496
	Evaluating
	loss=0.1550, precision=0.4411, recall=0.9073, f1=0.5936
Training epoch 5
	step [1/146], loss=153.2744
	step [2/146], loss=137.8310
	step [3/146], loss=143.7745
	step [4/146], loss=151.3330
	step [5/146], loss=125.2134
	step [6/146], loss=150.5758
	step [7/146], loss=169.2750
	step [8/146], loss=140.4068
	step [9/146], loss=146.6716
	step [10/146], loss=149.6614
	step [11/146], loss=141.6344
	step [12/146], loss=152.1654
	step [13/146], loss=166.9612
	step [14/146], loss=160.8421
	step [15/146], loss=159.1456
	step [16/146], loss=154.2647
	step [17/146], loss=151.6972
	step [18/146], loss=159.6013
	step [19/146], loss=127.3102
	step [20/146], loss=141.6190
	step [21/146], loss=128.4158
	step [22/146], loss=170.9581
	step [23/146], loss=139.6549
	step [24/146], loss=183.4949
	step [25/146], loss=168.4509
	step [26/146], loss=158.9552
	step [27/146], loss=158.7620
	step [28/146], loss=154.6079
	step [29/146], loss=138.9360
	step [30/146], loss=156.6967
	step [31/146], loss=134.8340
	step [32/146], loss=135.8611
	step [33/146], loss=165.5179
	step [34/146], loss=129.5577
	step [35/146], loss=159.8961
	step [36/146], loss=157.8752
	step [37/146], loss=131.7092
	step [38/146], loss=130.9259
	step [39/146], loss=172.8224
	step [40/146], loss=166.1429
	step [41/146], loss=155.6992
	step [42/146], loss=128.1592
	step [43/146], loss=155.9949
	step [44/146], loss=139.1024
	step [45/146], loss=162.4641
	step [46/146], loss=137.1967
	step [47/146], loss=138.5193
	step [48/146], loss=149.8523
	step [49/146], loss=133.2116
	step [50/146], loss=148.5568
	step [51/146], loss=162.6823
	step [52/146], loss=153.7319
	step [53/146], loss=135.5919
	step [54/146], loss=136.1116
	step [55/146], loss=155.0108
	step [56/146], loss=112.7711
	step [57/146], loss=156.0257
	step [58/146], loss=163.3358
	step [59/146], loss=142.4712
	step [60/146], loss=144.6058
	step [61/146], loss=157.0448
	step [62/146], loss=140.4942
	step [63/146], loss=162.0120
	step [64/146], loss=161.7784
	step [65/146], loss=140.8788
	step [66/146], loss=150.1931
	step [67/146], loss=128.4233
	step [68/146], loss=143.6675
	step [69/146], loss=154.4999
	step [70/146], loss=149.6154
	step [71/146], loss=146.7649
	step [72/146], loss=138.5043
	step [73/146], loss=153.5522
	step [74/146], loss=159.5781
	step [75/146], loss=143.4306
	step [76/146], loss=122.3944
	step [77/146], loss=143.3207
	step [78/146], loss=160.3164
	step [79/146], loss=143.5707
	step [80/146], loss=138.1604
	step [81/146], loss=163.0497
	step [82/146], loss=133.3435
	step [83/146], loss=148.3759
	step [84/146], loss=153.2817
	step [85/146], loss=158.7897
	step [86/146], loss=145.0293
	step [87/146], loss=152.3246
	step [88/146], loss=118.0635
	step [89/146], loss=166.8468
	step [90/146], loss=133.3457
	step [91/146], loss=144.1660
	step [92/146], loss=141.3694
	step [93/146], loss=155.1568
	step [94/146], loss=143.7725
	step [95/146], loss=117.9784
	step [96/146], loss=149.3264
	step [97/146], loss=145.0316
	step [98/146], loss=143.8827
	step [99/146], loss=125.7510
	step [100/146], loss=129.6148
	step [101/146], loss=155.3343
	step [102/146], loss=127.8076
	step [103/146], loss=133.1028
	step [104/146], loss=145.1503
	step [105/146], loss=160.9100
	step [106/146], loss=171.9688
	step [107/146], loss=145.5608
	step [108/146], loss=128.2656
	step [109/146], loss=175.3369
	step [110/146], loss=113.5522
	step [111/146], loss=123.0935
	step [112/146], loss=131.9763
	step [113/146], loss=132.8100
	step [114/146], loss=159.3294
	step [115/146], loss=145.3681
	step [116/146], loss=123.6111
	step [117/146], loss=146.3508
	step [118/146], loss=131.7719
	step [119/146], loss=143.1345
	step [120/146], loss=110.7672
	step [121/146], loss=145.5920
	step [122/146], loss=139.2611
	step [123/146], loss=143.4060
	step [124/146], loss=136.4182
	step [125/146], loss=116.0628
	step [126/146], loss=143.0829
	step [127/146], loss=160.1308
	step [128/146], loss=131.3282
	step [129/146], loss=169.0940
	step [130/146], loss=149.5298
	step [131/146], loss=147.7169
	step [132/146], loss=157.9258
	step [133/146], loss=144.6562
	step [134/146], loss=140.1876
	step [135/146], loss=143.0951
	step [136/146], loss=143.0020
	step [137/146], loss=153.5628
	step [138/146], loss=143.7361
	step [139/146], loss=142.9821
	step [140/146], loss=136.3068
	step [141/146], loss=137.4681
	step [142/146], loss=127.1827
	step [143/146], loss=136.1384
	step [144/146], loss=137.7585
	step [145/146], loss=134.8387
	step [146/146], loss=69.3778
	Evaluating
	loss=0.1324, precision=0.4813, recall=0.9063, f1=0.6287
Training epoch 6
	step [1/146], loss=135.6652
	step [2/146], loss=161.6970
	step [3/146], loss=145.6277
	step [4/146], loss=130.4959
	step [5/146], loss=143.4078
	step [6/146], loss=148.8008
	step [7/146], loss=116.8138
	step [8/146], loss=116.7608
	step [9/146], loss=150.6639
	step [10/146], loss=149.6047
	step [11/146], loss=133.4648
	step [12/146], loss=163.2699
	step [13/146], loss=136.3328
	step [14/146], loss=131.7923
	step [15/146], loss=143.5479
	step [16/146], loss=145.9943
	step [17/146], loss=140.8957
	step [18/146], loss=133.9183
	step [19/146], loss=134.9064
	step [20/146], loss=138.9655
	step [21/146], loss=131.8550
	step [22/146], loss=128.4953
	step [23/146], loss=157.1395
	step [24/146], loss=121.1519
	step [25/146], loss=133.3432
	step [26/146], loss=146.2923
	step [27/146], loss=129.0970
	step [28/146], loss=155.3175
	step [29/146], loss=158.9002
	step [30/146], loss=117.1454
	step [31/146], loss=152.6083
	step [32/146], loss=142.7610
	step [33/146], loss=143.5265
	step [34/146], loss=132.1619
	step [35/146], loss=122.5650
	step [36/146], loss=149.4346
	step [37/146], loss=157.1225
	step [38/146], loss=137.5144
	step [39/146], loss=132.6238
	step [40/146], loss=138.9990
	step [41/146], loss=135.9280
	step [42/146], loss=158.9908
	step [43/146], loss=141.1836
	step [44/146], loss=159.4917
	step [45/146], loss=132.5579
	step [46/146], loss=150.8366
	step [47/146], loss=141.6480
	step [48/146], loss=150.5653
	step [49/146], loss=138.2525
	step [50/146], loss=126.7551
	step [51/146], loss=167.2905
	step [52/146], loss=128.4897
	step [53/146], loss=146.3761
	step [54/146], loss=129.6313
	step [55/146], loss=142.2054
	step [56/146], loss=131.8840
	step [57/146], loss=139.8866
	step [58/146], loss=148.6248
	step [59/146], loss=135.8563
	step [60/146], loss=144.9778
	step [61/146], loss=135.7457
	step [62/146], loss=144.4226
	step [63/146], loss=140.8256
	step [64/146], loss=144.6039
	step [65/146], loss=123.9301
	step [66/146], loss=155.9706
	step [67/146], loss=128.2359
	step [68/146], loss=146.0386
	step [69/146], loss=141.9510
	step [70/146], loss=129.4811
	step [71/146], loss=156.9228
	step [72/146], loss=124.3820
	step [73/146], loss=141.6178
	step [74/146], loss=121.9051
	step [75/146], loss=133.0828
	step [76/146], loss=133.3784
	step [77/146], loss=125.8318
	step [78/146], loss=134.6879
	step [79/146], loss=138.1378
	step [80/146], loss=110.5889
	step [81/146], loss=146.9827
	step [82/146], loss=151.0198
	step [83/146], loss=134.2553
	step [84/146], loss=129.7605
	step [85/146], loss=171.0564
	step [86/146], loss=131.4804
	step [87/146], loss=132.3766
	step [88/146], loss=138.5986
	step [89/146], loss=138.6804
	step [90/146], loss=140.4294
	step [91/146], loss=145.5370
	step [92/146], loss=122.1692
	step [93/146], loss=123.1085
	step [94/146], loss=139.7473
	step [95/146], loss=161.0504
	step [96/146], loss=152.9713
	step [97/146], loss=115.2162
	step [98/146], loss=128.1489
	step [99/146], loss=145.3544
	step [100/146], loss=122.7571
	step [101/146], loss=138.6944
	step [102/146], loss=176.2717
	step [103/146], loss=156.4306
	step [104/146], loss=134.1365
	step [105/146], loss=127.2032
	step [106/146], loss=125.6376
	step [107/146], loss=156.5184
	step [108/146], loss=139.6333
	step [109/146], loss=161.7759
	step [110/146], loss=149.2022
	step [111/146], loss=139.5297
	step [112/146], loss=135.9721
	step [113/146], loss=121.1558
	step [114/146], loss=155.4836
	step [115/146], loss=152.3133
	step [116/146], loss=136.1385
	step [117/146], loss=146.3134
	step [118/146], loss=142.1625
	step [119/146], loss=136.7080
	step [120/146], loss=148.9115
	step [121/146], loss=153.8903
	step [122/146], loss=151.0698
	step [123/146], loss=131.6796
	step [124/146], loss=141.9517
	step [125/146], loss=130.2657
	step [126/146], loss=107.3987
	step [127/146], loss=131.8919
	step [128/146], loss=126.7095
	step [129/146], loss=141.1659
	step [130/146], loss=139.3164
	step [131/146], loss=137.3127
	step [132/146], loss=127.8710
	step [133/146], loss=164.2270
	step [134/146], loss=150.5418
	step [135/146], loss=127.3914
	step [136/146], loss=139.6131
	step [137/146], loss=170.0980
	step [138/146], loss=126.3905
	step [139/146], loss=145.6505
	step [140/146], loss=134.5996
	step [141/146], loss=134.3615
	step [142/146], loss=161.5490
	step [143/146], loss=119.5421
	step [144/146], loss=127.6601
	step [145/146], loss=142.0230
	step [146/146], loss=71.5002
	Evaluating
	loss=0.1083, precision=0.5216, recall=0.9003, f1=0.6605
Training epoch 7
	step [1/146], loss=147.5283
	step [2/146], loss=158.1599
	step [3/146], loss=131.3441
	step [4/146], loss=119.4175
	step [5/146], loss=131.2614
	step [6/146], loss=155.0475
	step [7/146], loss=136.5771
	step [8/146], loss=130.0411
	step [9/146], loss=138.8916
	step [10/146], loss=125.8040
	step [11/146], loss=140.5738
	step [12/146], loss=115.2560
	step [13/146], loss=139.5355
	step [14/146], loss=162.6287
	step [15/146], loss=140.2896
	step [16/146], loss=130.3032
	step [17/146], loss=144.4384
	step [18/146], loss=122.1102
	step [19/146], loss=165.7489
	step [20/146], loss=140.0526
	step [21/146], loss=145.2698
	step [22/146], loss=117.5339
	step [23/146], loss=156.9529
	step [24/146], loss=137.4705
	step [25/146], loss=144.6762
	step [26/146], loss=118.3106
	step [27/146], loss=131.2271
	step [28/146], loss=138.9050
	step [29/146], loss=147.9771
	step [30/146], loss=152.6537
	step [31/146], loss=142.7384
	step [32/146], loss=115.3340
	step [33/146], loss=123.3044
	step [34/146], loss=145.0904
	step [35/146], loss=109.2974
	step [36/146], loss=128.6113
	step [37/146], loss=128.8157
	step [38/146], loss=124.6146
	step [39/146], loss=130.1192
	step [40/146], loss=126.9773
	step [41/146], loss=163.8493
	step [42/146], loss=126.0844
	step [43/146], loss=141.4795
	step [44/146], loss=136.1538
	step [45/146], loss=147.8944
	step [46/146], loss=138.7443
	step [47/146], loss=154.6290
	step [48/146], loss=137.7073
	step [49/146], loss=126.9426
	step [50/146], loss=163.9420
	step [51/146], loss=115.0345
	step [52/146], loss=140.7969
	step [53/146], loss=139.6174
	step [54/146], loss=116.2746
	step [55/146], loss=145.3031
	step [56/146], loss=135.4290
	step [57/146], loss=118.7147
	step [58/146], loss=118.2322
	step [59/146], loss=154.5531
	step [60/146], loss=133.4734
	step [61/146], loss=124.0153
	step [62/146], loss=134.3176
	step [63/146], loss=140.4228
	step [64/146], loss=129.2742
	step [65/146], loss=116.6709
	step [66/146], loss=132.6826
	step [67/146], loss=102.3458
	step [68/146], loss=149.1980
	step [69/146], loss=148.0434
	step [70/146], loss=127.2707
	step [71/146], loss=132.3016
	step [72/146], loss=142.1662
	step [73/146], loss=146.4121
	step [74/146], loss=119.4920
	step [75/146], loss=126.0688
	step [76/146], loss=150.0192
	step [77/146], loss=132.3551
	step [78/146], loss=160.9498
	step [79/146], loss=126.0179
	step [80/146], loss=133.5816
	step [81/146], loss=151.9959
	step [82/146], loss=135.9996
	step [83/146], loss=147.8630
	step [84/146], loss=122.9050
	step [85/146], loss=137.2543
	step [86/146], loss=130.9397
	step [87/146], loss=130.5938
	step [88/146], loss=141.7528
	step [89/146], loss=135.9857
	step [90/146], loss=138.9570
	step [91/146], loss=144.7781
	step [92/146], loss=145.9137
	step [93/146], loss=121.4460
	step [94/146], loss=141.2264
	step [95/146], loss=147.4211
	step [96/146], loss=100.0176
	step [97/146], loss=122.5619
	step [98/146], loss=155.8225
	step [99/146], loss=125.3820
	step [100/146], loss=145.2883
	step [101/146], loss=123.3660
	step [102/146], loss=138.4197
	step [103/146], loss=118.3666
	step [104/146], loss=146.3391
	step [105/146], loss=144.3459
	step [106/146], loss=145.7212
	step [107/146], loss=132.3840
	step [108/146], loss=145.8280
	step [109/146], loss=111.7247
	step [110/146], loss=135.2833
	step [111/146], loss=136.8314
	step [112/146], loss=144.6197
	step [113/146], loss=125.2197
	step [114/146], loss=143.5560
	step [115/146], loss=121.7733
	step [116/146], loss=135.7036
	step [117/146], loss=140.5400
	step [118/146], loss=141.8355
	step [119/146], loss=133.4604
	step [120/146], loss=144.3503
	step [121/146], loss=128.1244
	step [122/146], loss=113.1980
	step [123/146], loss=111.2051
	step [124/146], loss=139.7333
	step [125/146], loss=125.7572
	step [126/146], loss=176.5339
	step [127/146], loss=140.4214
	step [128/146], loss=142.2624
	step [129/146], loss=144.9022
	step [130/146], loss=142.7583
	step [131/146], loss=141.3754
	step [132/146], loss=148.7424
	step [133/146], loss=139.7635
	step [134/146], loss=116.7983
	step [135/146], loss=120.9665
	step [136/146], loss=126.3768
	step [137/146], loss=117.6033
	step [138/146], loss=146.6944
	step [139/146], loss=129.0495
	step [140/146], loss=111.5976
	step [141/146], loss=142.9389
	step [142/146], loss=114.9651
	step [143/146], loss=120.7589
	step [144/146], loss=148.1227
	step [145/146], loss=125.8679
	step [146/146], loss=63.7063
	Evaluating
	loss=0.0903, precision=0.5119, recall=0.8922, f1=0.6505
Training epoch 8
	step [1/146], loss=141.9742
	step [2/146], loss=140.7469
	step [3/146], loss=145.2389
	step [4/146], loss=109.1502
	step [5/146], loss=124.8896
	step [6/146], loss=148.7820
	step [7/146], loss=138.0466
	step [8/146], loss=130.6690
	step [9/146], loss=131.3104
	step [10/146], loss=146.6094
	step [11/146], loss=130.1129
	step [12/146], loss=133.5367
	step [13/146], loss=136.1332
	step [14/146], loss=149.1868
	step [15/146], loss=127.0967
	step [16/146], loss=137.2331
	step [17/146], loss=108.4141
	step [18/146], loss=120.8346
	step [19/146], loss=125.8387
	step [20/146], loss=139.7242
	step [21/146], loss=116.6176
	step [22/146], loss=153.1999
	step [23/146], loss=114.6184
	step [24/146], loss=120.2795
	step [25/146], loss=145.7600
	step [26/146], loss=126.0978
	step [27/146], loss=121.4540
	step [28/146], loss=125.8846
	step [29/146], loss=146.6128
	step [30/146], loss=123.6297
	step [31/146], loss=141.3410
	step [32/146], loss=123.5283
	step [33/146], loss=137.3468
	step [34/146], loss=120.5246
	step [35/146], loss=137.6120
	step [36/146], loss=127.2415
	step [37/146], loss=126.7259
	step [38/146], loss=133.1182
	step [39/146], loss=125.6396
	step [40/146], loss=140.4699
	step [41/146], loss=110.7011
	step [42/146], loss=130.6660
	step [43/146], loss=117.9877
	step [44/146], loss=121.0906
	step [45/146], loss=133.9457
	step [46/146], loss=124.3983
	step [47/146], loss=139.3055
	step [48/146], loss=137.2405
	step [49/146], loss=144.3469
	step [50/146], loss=136.5867
	step [51/146], loss=153.6945
	step [52/146], loss=154.1143
	step [53/146], loss=125.4681
	step [54/146], loss=143.4731
	step [55/146], loss=135.2171
	step [56/146], loss=132.8123
	step [57/146], loss=161.0882
	step [58/146], loss=146.4228
	step [59/146], loss=124.8936
	step [60/146], loss=131.9155
	step [61/146], loss=131.6151
	step [62/146], loss=125.3856
	step [63/146], loss=163.2205
	step [64/146], loss=118.0665
	step [65/146], loss=119.1487
	step [66/146], loss=126.4716
	step [67/146], loss=143.6407
	step [68/146], loss=137.5057
	step [69/146], loss=140.7016
	step [70/146], loss=133.5871
	step [71/146], loss=120.7201
	step [72/146], loss=117.7735
	step [73/146], loss=136.4558
	step [74/146], loss=145.8962
	step [75/146], loss=122.9245
	step [76/146], loss=141.1735
	step [77/146], loss=132.6252
	step [78/146], loss=125.2190
	step [79/146], loss=133.2385
	step [80/146], loss=119.7625
	step [81/146], loss=126.4811
	step [82/146], loss=164.1825
	step [83/146], loss=120.4624
	step [84/146], loss=124.9442
	step [85/146], loss=131.5389
	step [86/146], loss=138.1275
	step [87/146], loss=130.1501
	step [88/146], loss=137.2447
	step [89/146], loss=120.3489
	step [90/146], loss=134.7741
	step [91/146], loss=142.2132
	step [92/146], loss=126.7291
	step [93/146], loss=127.0325
	step [94/146], loss=121.5925
	step [95/146], loss=123.9471
	step [96/146], loss=138.8989
	step [97/146], loss=127.4227
	step [98/146], loss=127.5094
	step [99/146], loss=129.5302
	step [100/146], loss=114.6940
	step [101/146], loss=137.9693
	step [102/146], loss=131.1543
	step [103/146], loss=117.5091
	step [104/146], loss=116.7167
	step [105/146], loss=139.2568
	step [106/146], loss=127.5757
	step [107/146], loss=117.8930
	step [108/146], loss=125.1644
	step [109/146], loss=117.1682
	step [110/146], loss=142.3798
	step [111/146], loss=115.7484
	step [112/146], loss=111.4528
	step [113/146], loss=138.5030
	step [114/146], loss=147.6654
	step [115/146], loss=122.2810
	step [116/146], loss=122.9996
	step [117/146], loss=145.8907
	step [118/146], loss=119.1138
	step [119/146], loss=128.4633
	step [120/146], loss=158.6577
	step [121/146], loss=141.9113
	step [122/146], loss=124.7906
	step [123/146], loss=112.3964
	step [124/146], loss=132.6511
	step [125/146], loss=128.6590
	step [126/146], loss=127.7104
	step [127/146], loss=123.1714
	step [128/146], loss=131.6576
	step [129/146], loss=113.7145
	step [130/146], loss=157.6461
	step [131/146], loss=127.0941
	step [132/146], loss=158.0047
	step [133/146], loss=141.0287
	step [134/146], loss=131.6308
	step [135/146], loss=127.6050
	step [136/146], loss=119.4978
	step [137/146], loss=117.9799
	step [138/146], loss=157.1896
	step [139/146], loss=137.0375
	step [140/146], loss=108.7357
	step [141/146], loss=157.3185
	step [142/146], loss=126.6040
	step [143/146], loss=146.1402
	step [144/146], loss=156.8167
	step [145/146], loss=130.7336
	step [146/146], loss=65.5376
	Evaluating
	loss=0.0755, precision=0.5159, recall=0.9090, f1=0.6582
Training epoch 9
	step [1/146], loss=153.5543
	step [2/146], loss=118.3836
	step [3/146], loss=110.3812
	step [4/146], loss=124.4893
	step [5/146], loss=141.3888
	step [6/146], loss=121.5116
	step [7/146], loss=129.5303
	step [8/146], loss=125.7536
	step [9/146], loss=109.6697
	step [10/146], loss=126.3643
	step [11/146], loss=130.6683
	step [12/146], loss=130.8847
	step [13/146], loss=131.9622
	step [14/146], loss=137.9033
	step [15/146], loss=127.8038
	step [16/146], loss=123.3455
	step [17/146], loss=129.1209
	step [18/146], loss=130.9453
	step [19/146], loss=135.0510
	step [20/146], loss=127.8302
	step [21/146], loss=121.3171
	step [22/146], loss=117.2875
	step [23/146], loss=160.2896
	step [24/146], loss=103.1116
	step [25/146], loss=143.2705
	step [26/146], loss=124.6208
	step [27/146], loss=142.7285
	step [28/146], loss=133.4382
	step [29/146], loss=119.3743
	step [30/146], loss=131.6877
	step [31/146], loss=121.9200
	step [32/146], loss=128.2962
	step [33/146], loss=115.4910
	step [34/146], loss=150.2537
	step [35/146], loss=142.2492
	step [36/146], loss=145.5027
	step [37/146], loss=124.8340
	step [38/146], loss=148.7074
	step [39/146], loss=141.4212
	step [40/146], loss=117.7373
	step [41/146], loss=157.5316
	step [42/146], loss=128.8808
	step [43/146], loss=116.0381
	step [44/146], loss=116.5907
	step [45/146], loss=128.3583
	step [46/146], loss=134.5601
	step [47/146], loss=122.2427
	step [48/146], loss=127.8234
	step [49/146], loss=142.4548
	step [50/146], loss=122.5003
	step [51/146], loss=137.4636
	step [52/146], loss=114.0639
	step [53/146], loss=117.2106
	step [54/146], loss=116.0569
	step [55/146], loss=126.3316
	step [56/146], loss=121.0360
	step [57/146], loss=100.3592
	step [58/146], loss=108.1094
	step [59/146], loss=145.7120
	step [60/146], loss=137.1369
	step [61/146], loss=130.3796
	step [62/146], loss=137.8055
	step [63/146], loss=128.6833
	step [64/146], loss=116.9468
	step [65/146], loss=119.7888
	step [66/146], loss=110.4893
	step [67/146], loss=137.0167
	step [68/146], loss=125.1680
	step [69/146], loss=112.2316
	step [70/146], loss=125.9736
	step [71/146], loss=129.0099
	step [72/146], loss=125.1320
	step [73/146], loss=122.4759
	step [74/146], loss=139.4412
	step [75/146], loss=129.8052
	step [76/146], loss=141.5777
	step [77/146], loss=135.4523
	step [78/146], loss=122.5765
	step [79/146], loss=130.4820
	step [80/146], loss=122.7441
	step [81/146], loss=138.1151
	step [82/146], loss=151.5950
	step [83/146], loss=125.5371
	step [84/146], loss=142.3920
	step [85/146], loss=123.9183
	step [86/146], loss=141.3186
	step [87/146], loss=117.7994
	step [88/146], loss=139.8544
	step [89/146], loss=136.8516
	step [90/146], loss=152.5774
	step [91/146], loss=134.3746
	step [92/146], loss=110.1310
	step [93/146], loss=141.1011
	step [94/146], loss=119.3882
	step [95/146], loss=123.9315
	step [96/146], loss=129.9431
	step [97/146], loss=121.1466
	step [98/146], loss=96.8801
	step [99/146], loss=123.9824
	step [100/146], loss=123.0195
	step [101/146], loss=117.1095
	step [102/146], loss=132.7682
	step [103/146], loss=130.4639
	step [104/146], loss=127.3686
	step [105/146], loss=129.2798
	step [106/146], loss=124.4666
	step [107/146], loss=129.2925
	step [108/146], loss=131.1773
	step [109/146], loss=120.4885
	step [110/146], loss=114.5666
	step [111/146], loss=108.2965
	step [112/146], loss=139.8086
	step [113/146], loss=140.1082
	step [114/146], loss=134.7271
	step [115/146], loss=123.7392
	step [116/146], loss=123.2229
	step [117/146], loss=136.4810
	step [118/146], loss=131.3541
	step [119/146], loss=126.2477
	step [120/146], loss=123.9238
	step [121/146], loss=131.8694
	step [122/146], loss=152.1317
	step [123/146], loss=126.6898
	step [124/146], loss=130.2543
	step [125/146], loss=109.4859
	step [126/146], loss=120.8169
	step [127/146], loss=116.7782
	step [128/146], loss=123.4700
	step [129/146], loss=112.6705
	step [130/146], loss=163.9357
	step [131/146], loss=150.1384
	step [132/146], loss=106.5310
	step [133/146], loss=129.9948
	step [134/146], loss=123.8248
	step [135/146], loss=117.9903
	step [136/146], loss=126.0809
	step [137/146], loss=136.7408
	step [138/146], loss=138.0826
	step [139/146], loss=124.4910
	step [140/146], loss=156.0785
	step [141/146], loss=151.1819
	step [142/146], loss=122.9345
	step [143/146], loss=125.1212
	step [144/146], loss=107.8118
	step [145/146], loss=139.6925
	step [146/146], loss=64.2417
	Evaluating
	loss=0.0699, precision=0.4156, recall=0.9047, f1=0.5696
Training epoch 10
	step [1/146], loss=140.6286
	step [2/146], loss=118.8902
	step [3/146], loss=126.4380
	step [4/146], loss=115.3916
	step [5/146], loss=134.0858
	step [6/146], loss=135.0162
	step [7/146], loss=120.0808
	step [8/146], loss=133.6504
	step [9/146], loss=119.7135
	step [10/146], loss=120.0279
	step [11/146], loss=146.7750
	step [12/146], loss=124.5875
	step [13/146], loss=116.7211
	step [14/146], loss=126.0154
	step [15/146], loss=110.0878
	step [16/146], loss=118.0292
	step [17/146], loss=141.4059
	step [18/146], loss=132.3553
	step [19/146], loss=121.2221
	step [20/146], loss=137.4268
	step [21/146], loss=108.1874
	step [22/146], loss=127.0523
	step [23/146], loss=131.5581
	step [24/146], loss=145.9533
	step [25/146], loss=148.1571
	step [26/146], loss=135.7864
	step [27/146], loss=141.7289
	step [28/146], loss=112.3322
	step [29/146], loss=130.5160
	step [30/146], loss=137.9753
	step [31/146], loss=124.9787
	step [32/146], loss=141.1012
	step [33/146], loss=134.4913
	step [34/146], loss=108.1549
	step [35/146], loss=124.0845
	step [36/146], loss=108.4626
	step [37/146], loss=125.6837
	step [38/146], loss=121.4428
	step [39/146], loss=137.8395
	step [40/146], loss=125.3893
	step [41/146], loss=119.5242
	step [42/146], loss=126.4479
	step [43/146], loss=122.7690
	step [44/146], loss=129.6878
	step [45/146], loss=126.3018
	step [46/146], loss=139.3720
	step [47/146], loss=125.1934
	step [48/146], loss=116.6944
	step [49/146], loss=123.0763
	step [50/146], loss=114.4462
	step [51/146], loss=122.6077
	step [52/146], loss=122.0451
	step [53/146], loss=123.8030
	step [54/146], loss=127.7478
	step [55/146], loss=125.7187
	step [56/146], loss=141.4616
	step [57/146], loss=114.7506
	step [58/146], loss=134.4925
	step [59/146], loss=125.4181
	step [60/146], loss=115.1421
	step [61/146], loss=127.7830
	step [62/146], loss=135.5699
	step [63/146], loss=131.6932
	step [64/146], loss=149.7596
	step [65/146], loss=128.5903
	step [66/146], loss=121.2590
	step [67/146], loss=126.5132
	step [68/146], loss=114.8480
	step [69/146], loss=115.0606
	step [70/146], loss=113.6145
	step [71/146], loss=109.5342
	step [72/146], loss=105.0604
	step [73/146], loss=129.8192
	step [74/146], loss=109.9658
	step [75/146], loss=123.0459
	step [76/146], loss=136.1036
	step [77/146], loss=130.3055
	step [78/146], loss=137.8963
	step [79/146], loss=134.9501
	step [80/146], loss=120.9042
	step [81/146], loss=127.2697
	step [82/146], loss=123.9890
	step [83/146], loss=121.2976
	step [84/146], loss=141.9453
	step [85/146], loss=121.6756
	step [86/146], loss=121.6671
	step [87/146], loss=135.5607
	step [88/146], loss=127.0763
	step [89/146], loss=127.8704
	step [90/146], loss=136.9977
	step [91/146], loss=126.0473
	step [92/146], loss=111.6168
	step [93/146], loss=112.7638
	step [94/146], loss=116.3079
	step [95/146], loss=113.3797
	step [96/146], loss=111.8367
	step [97/146], loss=113.9249
	step [98/146], loss=135.5001
	step [99/146], loss=132.9018
	step [100/146], loss=155.4420
	step [101/146], loss=124.6085
	step [102/146], loss=123.0703
	step [103/146], loss=127.4006
	step [104/146], loss=115.4764
	step [105/146], loss=134.6030
	step [106/146], loss=121.4801
	step [107/146], loss=119.2009
	step [108/146], loss=134.5567
	step [109/146], loss=122.6517
	step [110/146], loss=112.9952
	step [111/146], loss=150.4736
	step [112/146], loss=127.7048
	step [113/146], loss=129.8288
	step [114/146], loss=117.8114
	step [115/146], loss=114.7860
	step [116/146], loss=134.9610
	step [117/146], loss=134.5696
	step [118/146], loss=120.4954
	step [119/146], loss=109.1287
	step [120/146], loss=138.2575
	step [121/146], loss=122.9070
	step [122/146], loss=152.8242
	step [123/146], loss=136.6091
	step [124/146], loss=113.9415
	step [125/146], loss=117.9379
	step [126/146], loss=122.4508
	step [127/146], loss=119.7997
	step [128/146], loss=124.2776
	step [129/146], loss=128.4658
	step [130/146], loss=131.3109
	step [131/146], loss=117.0566
	step [132/146], loss=114.3664
	step [133/146], loss=122.4870
	step [134/146], loss=115.6309
	step [135/146], loss=119.5512
	step [136/146], loss=113.4249
	step [137/146], loss=158.6238
	step [138/146], loss=123.8670
	step [139/146], loss=139.7602
	step [140/146], loss=119.3816
	step [141/146], loss=130.9300
	step [142/146], loss=113.0551
	step [143/146], loss=119.2631
	step [144/146], loss=105.7270
	step [145/146], loss=144.9133
	step [146/146], loss=68.4731
	Evaluating
	loss=0.0644, precision=0.3850, recall=0.8998, f1=0.5392
Training epoch 11
	step [1/146], loss=107.8500
	step [2/146], loss=115.9432
	step [3/146], loss=141.6767
	step [4/146], loss=136.7634
	step [5/146], loss=110.9310
	step [6/146], loss=115.2706
	step [7/146], loss=130.3590
	step [8/146], loss=127.0836
	step [9/146], loss=126.7848
	step [10/146], loss=126.0929
	step [11/146], loss=137.5211
	step [12/146], loss=134.1366
	step [13/146], loss=123.3579
	step [14/146], loss=125.4688
	step [15/146], loss=120.6016
	step [16/146], loss=115.6952
	step [17/146], loss=144.3877
	step [18/146], loss=110.9773
	step [19/146], loss=151.4317
	step [20/146], loss=103.5706
	step [21/146], loss=124.4112
	step [22/146], loss=108.7061
	step [23/146], loss=125.8221
	step [24/146], loss=127.0124
	step [25/146], loss=121.6142
	step [26/146], loss=124.7753
	step [27/146], loss=123.1319
	step [28/146], loss=139.3341
	step [29/146], loss=109.8002
	step [30/146], loss=108.1260
	step [31/146], loss=127.0097
	step [32/146], loss=111.7991
	step [33/146], loss=125.5926
	step [34/146], loss=140.1756
	step [35/146], loss=127.8616
	step [36/146], loss=138.7034
	step [37/146], loss=113.2866
	step [38/146], loss=117.5504
	step [39/146], loss=124.7717
	step [40/146], loss=129.9307
	step [41/146], loss=111.9455
	step [42/146], loss=119.7555
	step [43/146], loss=123.2076
	step [44/146], loss=134.9686
	step [45/146], loss=126.3180
	step [46/146], loss=119.8786
	step [47/146], loss=131.9200
	step [48/146], loss=92.4649
	step [49/146], loss=129.5357
	step [50/146], loss=133.6537
	step [51/146], loss=133.5121
	step [52/146], loss=117.7039
	step [53/146], loss=139.8366
	step [54/146], loss=132.7979
	step [55/146], loss=106.0354
	step [56/146], loss=115.0844
	step [57/146], loss=115.7847
	step [58/146], loss=120.7398
	step [59/146], loss=128.6017
	step [60/146], loss=138.6179
	step [61/146], loss=118.6357
	step [62/146], loss=128.2393
	step [63/146], loss=113.3554
	step [64/146], loss=116.9636
	step [65/146], loss=132.8005
	step [66/146], loss=117.9776
	step [67/146], loss=120.6649
	step [68/146], loss=122.5770
	step [69/146], loss=137.6830
	step [70/146], loss=116.6159
	step [71/146], loss=109.9243
	step [72/146], loss=111.0321
	step [73/146], loss=113.5990
	step [74/146], loss=125.6622
	step [75/146], loss=105.7484
	step [76/146], loss=121.9362
	step [77/146], loss=127.2159
	step [78/146], loss=122.0967
	step [79/146], loss=149.2695
	step [80/146], loss=121.5406
	step [81/146], loss=127.4730
	step [82/146], loss=104.0369
	step [83/146], loss=122.9997
	step [84/146], loss=143.5246
	step [85/146], loss=119.7527
	step [86/146], loss=117.0718
	step [87/146], loss=149.5333
	step [88/146], loss=116.5566
	step [89/146], loss=135.5677
	step [90/146], loss=135.1993
	step [91/146], loss=137.1632
	step [92/146], loss=131.0442
	step [93/146], loss=124.9501
	step [94/146], loss=137.6902
	step [95/146], loss=102.6520
	step [96/146], loss=132.5595
	step [97/146], loss=136.3206
	step [98/146], loss=125.6449
	step [99/146], loss=120.3175
	step [100/146], loss=133.7440
	step [101/146], loss=114.5981
	step [102/146], loss=121.9500
	step [103/146], loss=111.5387
	step [104/146], loss=110.2886
	step [105/146], loss=137.5915
	step [106/146], loss=128.8451
	step [107/146], loss=113.2009
	step [108/146], loss=133.4252
	step [109/146], loss=112.6352
	step [110/146], loss=120.0499
	step [111/146], loss=106.5638
	step [112/146], loss=118.3116
	step [113/146], loss=138.2249
	step [114/146], loss=111.5958
	step [115/146], loss=132.7065
	step [116/146], loss=110.4587
	step [117/146], loss=132.9787
	step [118/146], loss=121.2637
	step [119/146], loss=114.0797
	step [120/146], loss=118.8986
	step [121/146], loss=110.2093
	step [122/146], loss=134.2081
	step [123/146], loss=115.7435
	step [124/146], loss=110.7814
	step [125/146], loss=134.3300
	step [126/146], loss=128.4362
	step [127/146], loss=112.2078
	step [128/146], loss=159.0904
	step [129/146], loss=138.4301
	step [130/146], loss=128.7902
	step [131/146], loss=113.3538
	step [132/146], loss=128.6278
	step [133/146], loss=104.0798
	step [134/146], loss=124.1172
	step [135/146], loss=123.0023
	step [136/146], loss=118.8856
	step [137/146], loss=130.2113
	step [138/146], loss=134.0869
	step [139/146], loss=124.9328
	step [140/146], loss=124.6899
	step [141/146], loss=104.4604
	step [142/146], loss=122.1181
	step [143/146], loss=118.4430
	step [144/146], loss=102.2414
	step [145/146], loss=120.1501
	step [146/146], loss=62.8456
	Evaluating
	loss=0.0544, precision=0.4238, recall=0.8863, f1=0.5734
Training epoch 12
	step [1/146], loss=118.7370
	step [2/146], loss=128.2646
	step [3/146], loss=120.6266
	step [4/146], loss=141.8828
	step [5/146], loss=107.9986
	step [6/146], loss=112.8256
	step [7/146], loss=102.9086
	step [8/146], loss=127.2212
	step [9/146], loss=102.2117
	step [10/146], loss=117.0394
	step [11/146], loss=125.8035
	step [12/146], loss=127.5380
	step [13/146], loss=132.5605
	step [14/146], loss=118.6472
	step [15/146], loss=108.3758
	step [16/146], loss=131.7287
	step [17/146], loss=133.4968
	step [18/146], loss=126.2320
	step [19/146], loss=129.3461
	step [20/146], loss=100.8434
	step [21/146], loss=109.7522
	step [22/146], loss=112.2724
	step [23/146], loss=143.8950
	step [24/146], loss=104.1339
	step [25/146], loss=104.8548
	step [26/146], loss=150.7851
	step [27/146], loss=133.3253
	step [28/146], loss=103.4666
	step [29/146], loss=137.4182
	step [30/146], loss=118.0412
	step [31/146], loss=141.3493
	step [32/146], loss=127.8865
	step [33/146], loss=123.5847
	step [34/146], loss=125.6293
	step [35/146], loss=117.5648
	step [36/146], loss=132.9237
	step [37/146], loss=124.6083
	step [38/146], loss=123.7770
	step [39/146], loss=104.3663
	step [40/146], loss=161.8347
	step [41/146], loss=117.8961
	step [42/146], loss=128.8849
	step [43/146], loss=124.8935
	step [44/146], loss=117.7862
	step [45/146], loss=135.4826
	step [46/146], loss=111.8807
	step [47/146], loss=120.5844
	step [48/146], loss=119.3883
	step [49/146], loss=113.3600
	step [50/146], loss=119.9832
	step [51/146], loss=133.7151
	step [52/146], loss=128.7156
	step [53/146], loss=142.5007
	step [54/146], loss=143.2895
	step [55/146], loss=144.9813
	step [56/146], loss=123.7610
	step [57/146], loss=132.3680
	step [58/146], loss=123.5488
	step [59/146], loss=118.4987
	step [60/146], loss=103.6451
	step [61/146], loss=109.4530
	step [62/146], loss=131.3295
	step [63/146], loss=129.3273
	step [64/146], loss=126.1691
	step [65/146], loss=139.2776
	step [66/146], loss=112.2462
	step [67/146], loss=112.7825
	step [68/146], loss=110.6142
	step [69/146], loss=117.2270
	step [70/146], loss=132.2914
	step [71/146], loss=136.5135
	step [72/146], loss=110.2133
	step [73/146], loss=138.6878
	step [74/146], loss=113.9932
	step [75/146], loss=110.6073
	step [76/146], loss=126.7047
	step [77/146], loss=132.7073
	step [78/146], loss=115.0625
	step [79/146], loss=124.2308
	step [80/146], loss=133.1358
	step [81/146], loss=123.6710
	step [82/146], loss=109.3963
	step [83/146], loss=125.1909
	step [84/146], loss=126.5147
	step [85/146], loss=120.1148
	step [86/146], loss=130.3389
	step [87/146], loss=136.5925
	step [88/146], loss=111.4854
	step [89/146], loss=113.2759
	step [90/146], loss=117.1021
	step [91/146], loss=117.5452
	step [92/146], loss=141.1352
	step [93/146], loss=124.8251
	step [94/146], loss=112.0754
	step [95/146], loss=124.9277
	step [96/146], loss=114.8733
	step [97/146], loss=119.0074
	step [98/146], loss=113.8142
	step [99/146], loss=119.3596
	step [100/146], loss=110.1628
	step [101/146], loss=109.6602
	step [102/146], loss=98.1217
	step [103/146], loss=108.4566
	step [104/146], loss=152.0845
	step [105/146], loss=130.6542
	step [106/146], loss=147.8636
	step [107/146], loss=119.8940
	step [108/146], loss=121.9706
	step [109/146], loss=116.3591
	step [110/146], loss=105.1890
	step [111/146], loss=105.2070
	step [112/146], loss=106.9577
	step [113/146], loss=104.6310
	step [114/146], loss=104.9601
	step [115/146], loss=128.8817
	step [116/146], loss=118.9090
	step [117/146], loss=114.2558
	step [118/146], loss=109.3850
	step [119/146], loss=119.2732
	step [120/146], loss=100.0623
	step [121/146], loss=106.5046
	step [122/146], loss=120.7219
	step [123/146], loss=122.1087
	step [124/146], loss=117.4554
	step [125/146], loss=136.8144
	step [126/146], loss=127.8656
	step [127/146], loss=101.2270
	step [128/146], loss=132.5640
	step [129/146], loss=124.7057
	step [130/146], loss=111.8281
	step [131/146], loss=117.8795
	step [132/146], loss=124.8724
	step [133/146], loss=122.9458
	step [134/146], loss=121.8774
	step [135/146], loss=116.3317
	step [136/146], loss=110.1387
	step [137/146], loss=118.4167
	step [138/146], loss=112.5017
	step [139/146], loss=125.9101
	step [140/146], loss=120.2850
	step [141/146], loss=126.9133
	step [142/146], loss=130.3857
	step [143/146], loss=110.1804
	step [144/146], loss=130.0749
	step [145/146], loss=135.5994
	step [146/146], loss=47.8497
	Evaluating
	loss=0.0458, precision=0.5307, recall=0.8842, f1=0.6633
Training epoch 13
	step [1/146], loss=115.0468
	step [2/146], loss=106.2836
	step [3/146], loss=124.1252
	step [4/146], loss=127.4565
	step [5/146], loss=122.9641
	step [6/146], loss=115.1445
	step [7/146], loss=107.6505
	step [8/146], loss=124.2840
	step [9/146], loss=125.6236
	step [10/146], loss=144.5074
	step [11/146], loss=117.6139
	step [12/146], loss=116.0382
	step [13/146], loss=136.6668
	step [14/146], loss=96.1435
	step [15/146], loss=118.3984
	step [16/146], loss=108.3168
	step [17/146], loss=133.5797
	step [18/146], loss=115.7603
	step [19/146], loss=108.5876
	step [20/146], loss=135.0889
	step [21/146], loss=120.9228
	step [22/146], loss=112.2710
	step [23/146], loss=117.1099
	step [24/146], loss=117.2960
	step [25/146], loss=125.3893
	step [26/146], loss=139.4641
	step [27/146], loss=110.1847
	step [28/146], loss=110.6873
	step [29/146], loss=124.5995
	step [30/146], loss=113.5776
	step [31/146], loss=119.5076
	step [32/146], loss=120.9408
	step [33/146], loss=118.0571
	step [34/146], loss=119.2075
	step [35/146], loss=121.1247
	step [36/146], loss=106.9141
	step [37/146], loss=123.8917
	step [38/146], loss=111.7307
	step [39/146], loss=121.7817
	step [40/146], loss=113.4720
	step [41/146], loss=102.1883
	step [42/146], loss=151.6655
	step [43/146], loss=133.1599
	step [44/146], loss=108.9739
	step [45/146], loss=133.1523
	step [46/146], loss=114.3972
	step [47/146], loss=107.9499
	step [48/146], loss=112.5203
	step [49/146], loss=128.1664
	step [50/146], loss=123.6996
	step [51/146], loss=125.5958
	step [52/146], loss=105.6201
	step [53/146], loss=110.1925
	step [54/146], loss=113.4669
	step [55/146], loss=124.7115
	step [56/146], loss=124.9926
	step [57/146], loss=109.1688
	step [58/146], loss=130.9267
	step [59/146], loss=139.9495
	step [60/146], loss=128.3647
	step [61/146], loss=98.0665
	step [62/146], loss=118.3297
	step [63/146], loss=110.9810
	step [64/146], loss=110.1766
	step [65/146], loss=118.7748
	step [66/146], loss=115.7585
	step [67/146], loss=124.6490
	step [68/146], loss=113.6558
	step [69/146], loss=121.6082
	step [70/146], loss=108.7445
	step [71/146], loss=115.6341
	step [72/146], loss=147.2577
	step [73/146], loss=131.0430
	step [74/146], loss=122.2486
	step [75/146], loss=99.3717
	step [76/146], loss=103.0567
	step [77/146], loss=114.6790
	step [78/146], loss=132.6932
	step [79/146], loss=127.4722
	step [80/146], loss=129.2882
	step [81/146], loss=124.1296
	step [82/146], loss=119.1412
	step [83/146], loss=116.6871
	step [84/146], loss=136.3541
	step [85/146], loss=123.2941
	step [86/146], loss=107.0898
	step [87/146], loss=112.8858
	step [88/146], loss=125.4716
	step [89/146], loss=110.6073
	step [90/146], loss=117.2732
	step [91/146], loss=118.7720
	step [92/146], loss=128.6167
	step [93/146], loss=132.2347
	step [94/146], loss=124.7635
	step [95/146], loss=116.9134
	step [96/146], loss=100.5900
	step [97/146], loss=118.6741
	step [98/146], loss=101.7818
	step [99/146], loss=117.1517
	step [100/146], loss=137.2551
	step [101/146], loss=104.9675
	step [102/146], loss=106.8233
	step [103/146], loss=117.0871
	step [104/146], loss=120.1498
	step [105/146], loss=114.0003
	step [106/146], loss=107.7376
	step [107/146], loss=123.6290
	step [108/146], loss=100.5564
	step [109/146], loss=153.3361
	step [110/146], loss=126.5467
	step [111/146], loss=109.7857
	step [112/146], loss=130.2061
	step [113/146], loss=115.8448
	step [114/146], loss=137.4758
	step [115/146], loss=117.4898
	step [116/146], loss=116.2928
	step [117/146], loss=139.6275
	step [118/146], loss=122.9373
	step [119/146], loss=128.4993
	step [120/146], loss=116.6607
	step [121/146], loss=150.9521
	step [122/146], loss=103.6145
	step [123/146], loss=121.8654
	step [124/146], loss=100.4504
	step [125/146], loss=107.8236
	step [126/146], loss=144.4523
	step [127/146], loss=124.4807
	step [128/146], loss=133.1882
	step [129/146], loss=151.5964
	step [130/146], loss=127.5775
	step [131/146], loss=115.5944
	step [132/146], loss=100.8029
	step [133/146], loss=132.4948
	step [134/146], loss=123.9993
	step [135/146], loss=123.4034
	step [136/146], loss=113.8537
	step [137/146], loss=121.8824
	step [138/146], loss=133.9223
	step [139/146], loss=103.0884
	step [140/146], loss=116.3280
	step [141/146], loss=111.5628
	step [142/146], loss=120.1341
	step [143/146], loss=132.4316
	step [144/146], loss=103.5979
	step [145/146], loss=117.2902
	step [146/146], loss=68.0697
	Evaluating
	loss=0.0463, precision=0.3982, recall=0.8691, f1=0.5462
Training epoch 14
	step [1/146], loss=114.1199
	step [2/146], loss=113.7052
	step [3/146], loss=113.6440
	step [4/146], loss=139.0067
	step [5/146], loss=133.1222
	step [6/146], loss=129.3673
	step [7/146], loss=112.6204
	step [8/146], loss=117.4760
	step [9/146], loss=110.9500
	step [10/146], loss=127.0046
	step [11/146], loss=119.9156
	step [12/146], loss=125.5533
	step [13/146], loss=134.6611
	step [14/146], loss=115.2557
	step [15/146], loss=118.8326
	step [16/146], loss=128.5027
	step [17/146], loss=100.4702
	step [18/146], loss=122.6630
	step [19/146], loss=119.4906
	step [20/146], loss=125.4325
	step [21/146], loss=100.7060
	step [22/146], loss=132.8615
	step [23/146], loss=117.7295
	step [24/146], loss=107.0055
	step [25/146], loss=115.1745
	step [26/146], loss=132.6611
	step [27/146], loss=109.3996
	step [28/146], loss=129.1520
	step [29/146], loss=108.5101
	step [30/146], loss=113.8753
	step [31/146], loss=129.0068
	step [32/146], loss=127.4335
	step [33/146], loss=131.1052
	step [34/146], loss=121.8586
	step [35/146], loss=93.5695
	step [36/146], loss=124.1615
	step [37/146], loss=117.8738
	step [38/146], loss=118.6477
	step [39/146], loss=109.4415
	step [40/146], loss=113.1748
	step [41/146], loss=114.3536
	step [42/146], loss=121.8169
	step [43/146], loss=118.1135
	step [44/146], loss=125.9276
	step [45/146], loss=131.0468
	step [46/146], loss=115.5050
	step [47/146], loss=101.4223
	step [48/146], loss=97.8550
	step [49/146], loss=102.0527
	step [50/146], loss=143.4915
	step [51/146], loss=147.5329
	step [52/146], loss=110.1436
	step [53/146], loss=123.0414
	step [54/146], loss=115.2452
	step [55/146], loss=114.0044
	step [56/146], loss=114.6483
	step [57/146], loss=120.6983
	step [58/146], loss=102.9027
	step [59/146], loss=142.2249
	step [60/146], loss=126.9475
	step [61/146], loss=118.4298
	step [62/146], loss=126.8416
	step [63/146], loss=144.2052
	step [64/146], loss=137.3683
	step [65/146], loss=125.8226
	step [66/146], loss=126.4895
	step [67/146], loss=109.3505
	step [68/146], loss=133.6721
	step [69/146], loss=105.1824
	step [70/146], loss=128.8655
	step [71/146], loss=124.1001
	step [72/146], loss=107.9989
	step [73/146], loss=116.6905
	step [74/146], loss=111.8315
	step [75/146], loss=112.9277
	step [76/146], loss=115.6023
	step [77/146], loss=129.5832
	step [78/146], loss=123.4388
	step [79/146], loss=104.5788
	step [80/146], loss=91.4854
	step [81/146], loss=119.8260
	step [82/146], loss=119.2668
	step [83/146], loss=121.4183
	step [84/146], loss=124.6490
	step [85/146], loss=105.3451
	step [86/146], loss=115.3514
	step [87/146], loss=127.0123
	step [88/146], loss=104.7966
	step [89/146], loss=123.6987
	step [90/146], loss=117.8290
	step [91/146], loss=124.0490
	step [92/146], loss=124.2186
	step [93/146], loss=118.3212
	step [94/146], loss=138.5150
	step [95/146], loss=100.2891
	step [96/146], loss=148.5089
	step [97/146], loss=91.4318
	step [98/146], loss=113.1005
	step [99/146], loss=122.7544
	step [100/146], loss=109.1580
	step [101/146], loss=115.5791
	step [102/146], loss=120.1550
	step [103/146], loss=112.5200
	step [104/146], loss=125.2404
	step [105/146], loss=114.7320
	step [106/146], loss=92.4175
	step [107/146], loss=106.9968
	step [108/146], loss=121.5779
	step [109/146], loss=111.4674
	step [110/146], loss=104.9148
	step [111/146], loss=116.2214
	step [112/146], loss=125.1214
	step [113/146], loss=120.8873
	step [114/146], loss=127.6357
	step [115/146], loss=100.0823
	step [116/146], loss=130.4046
	step [117/146], loss=130.8795
	step [118/146], loss=122.1197
	step [119/146], loss=124.4131
	step [120/146], loss=131.6332
	step [121/146], loss=117.1429
	step [122/146], loss=104.8064
	step [123/146], loss=123.9234
	step [124/146], loss=116.5665
	step [125/146], loss=126.9879
	step [126/146], loss=113.7932
	step [127/146], loss=115.0113
	step [128/146], loss=117.5968
	step [129/146], loss=140.2917
	step [130/146], loss=105.9011
	step [131/146], loss=130.9134
	step [132/146], loss=112.0706
	step [133/146], loss=97.0255
	step [134/146], loss=118.7872
	step [135/146], loss=110.0516
	step [136/146], loss=129.4763
	step [137/146], loss=111.4502
	step [138/146], loss=134.1440
	step [139/146], loss=116.4655
	step [140/146], loss=98.2118
	step [141/146], loss=121.0668
	step [142/146], loss=121.6293
	step [143/146], loss=125.3010
	step [144/146], loss=126.7498
	step [145/146], loss=123.0953
	step [146/146], loss=45.6548
	Evaluating
	loss=0.0353, precision=0.5686, recall=0.8800, f1=0.6909
saving model as: 3_saved_model.pth
Training epoch 15
	step [1/146], loss=100.4785
	step [2/146], loss=113.0348
	step [3/146], loss=101.5663
	step [4/146], loss=123.9524
	step [5/146], loss=116.9544
	step [6/146], loss=114.1467
	step [7/146], loss=131.9337
	step [8/146], loss=99.4726
	step [9/146], loss=93.8874
	step [10/146], loss=112.2163
	step [11/146], loss=129.5988
	step [12/146], loss=110.6438
	step [13/146], loss=118.0051
	step [14/146], loss=124.4087
	step [15/146], loss=114.4869
	step [16/146], loss=95.3063
	step [17/146], loss=94.3266
	step [18/146], loss=128.6284
	step [19/146], loss=116.3199
	step [20/146], loss=118.4780
	step [21/146], loss=103.9872
	step [22/146], loss=127.3858
	step [23/146], loss=111.2828
	step [24/146], loss=101.8536
	step [25/146], loss=99.7051
	step [26/146], loss=121.5923
	step [27/146], loss=117.5499
	step [28/146], loss=119.1704
	step [29/146], loss=111.5860
	step [30/146], loss=123.7535
	step [31/146], loss=135.0233
	step [32/146], loss=113.2377
	step [33/146], loss=96.8411
	step [34/146], loss=117.4533
	step [35/146], loss=111.5431
	step [36/146], loss=114.8334
	step [37/146], loss=119.8788
	step [38/146], loss=137.8769
	step [39/146], loss=117.7016
	step [40/146], loss=123.9408
	step [41/146], loss=134.6857
	step [42/146], loss=125.4049
	step [43/146], loss=113.4462
	step [44/146], loss=125.0812
	step [45/146], loss=140.1048
	step [46/146], loss=92.1841
	step [47/146], loss=120.5064
	step [48/146], loss=130.6990
	step [49/146], loss=119.9061
	step [50/146], loss=124.1519
	step [51/146], loss=112.8331
	step [52/146], loss=131.4553
	step [53/146], loss=116.4507
	step [54/146], loss=134.4853
	step [55/146], loss=131.5616
	step [56/146], loss=127.4128
	step [57/146], loss=102.4674
	step [58/146], loss=123.8432
	step [59/146], loss=109.1606
	step [60/146], loss=109.1392
	step [61/146], loss=106.0110
	step [62/146], loss=115.3213
	step [63/146], loss=100.1418
	step [64/146], loss=127.1782
	step [65/146], loss=131.5330
	step [66/146], loss=112.6730
	step [67/146], loss=131.2376
	step [68/146], loss=112.6891
	step [69/146], loss=100.5985
	step [70/146], loss=120.5123
	step [71/146], loss=104.9547
	step [72/146], loss=96.9516
	step [73/146], loss=130.4173
	step [74/146], loss=109.8466
	step [75/146], loss=103.5703
	step [76/146], loss=129.6105
	step [77/146], loss=122.1357
	step [78/146], loss=120.5713
	step [79/146], loss=118.5284
	step [80/146], loss=116.8276
	step [81/146], loss=141.2032
	step [82/146], loss=112.8467
	step [83/146], loss=122.1507
	step [84/146], loss=97.6252
	step [85/146], loss=128.1324
	step [86/146], loss=124.4458
	step [87/146], loss=115.4022
	step [88/146], loss=108.4926
	step [89/146], loss=105.7568
	step [90/146], loss=137.6597
	step [91/146], loss=109.5011
	step [92/146], loss=133.3413
	step [93/146], loss=113.2586
	step [94/146], loss=93.6406
	step [95/146], loss=109.8424
	step [96/146], loss=119.2298
	step [97/146], loss=99.8378
	step [98/146], loss=108.3527
	step [99/146], loss=118.2143
	step [100/146], loss=112.4072
	step [101/146], loss=128.6269
	step [102/146], loss=129.3805
	step [103/146], loss=119.5814
	step [104/146], loss=120.0647
	step [105/146], loss=132.8393
	step [106/146], loss=122.1907
	step [107/146], loss=126.6204
	step [108/146], loss=109.1776
	step [109/146], loss=131.1669
	step [110/146], loss=120.2259
	step [111/146], loss=112.5825
	step [112/146], loss=113.1405
	step [113/146], loss=109.9331
	step [114/146], loss=124.6646
	step [115/146], loss=124.4791
	step [116/146], loss=103.0863
	step [117/146], loss=140.9296
	step [118/146], loss=103.6879
	step [119/146], loss=132.9141
	step [120/146], loss=123.8218
	step [121/146], loss=131.6428
	step [122/146], loss=90.2373
	step [123/146], loss=97.8709
	step [124/146], loss=121.8557
	step [125/146], loss=111.1702
	step [126/146], loss=128.5816
	step [127/146], loss=117.9370
	step [128/146], loss=120.0700
	step [129/146], loss=126.3864
	step [130/146], loss=130.7807
	step [131/146], loss=103.1882
	step [132/146], loss=112.4878
	step [133/146], loss=103.5074
	step [134/146], loss=116.4606
	step [135/146], loss=96.0334
	step [136/146], loss=133.4373
	step [137/146], loss=116.2684
	step [138/146], loss=113.0318
	step [139/146], loss=116.7183
	step [140/146], loss=109.2041
	step [141/146], loss=114.8114
	step [142/146], loss=130.0420
	step [143/146], loss=132.7685
	step [144/146], loss=115.5925
	step [145/146], loss=126.6529
	step [146/146], loss=58.7962
	Evaluating
	loss=0.0339, precision=0.4337, recall=0.9082, f1=0.5870
Training epoch 16
	step [1/146], loss=118.5162
	step [2/146], loss=120.2823
	step [3/146], loss=113.6377
	step [4/146], loss=115.6339
	step [5/146], loss=122.8531
	step [6/146], loss=121.4335
	step [7/146], loss=118.3149
	step [8/146], loss=128.4895
	step [9/146], loss=125.4971
	step [10/146], loss=129.3370
	step [11/146], loss=113.7508
	step [12/146], loss=111.9181
	step [13/146], loss=105.9945
	step [14/146], loss=115.9350
	step [15/146], loss=123.9022
	step [16/146], loss=123.3196
	step [17/146], loss=119.4743
	step [18/146], loss=119.8373
	step [19/146], loss=113.7957
	step [20/146], loss=120.9372
	step [21/146], loss=129.1913
	step [22/146], loss=128.9320
	step [23/146], loss=112.0339
	step [24/146], loss=103.9385
	step [25/146], loss=105.9169
	step [26/146], loss=110.9722
	step [27/146], loss=112.8735
	step [28/146], loss=119.1748
	step [29/146], loss=101.5416
	step [30/146], loss=112.9290
	step [31/146], loss=118.4161
	step [32/146], loss=119.1338
	step [33/146], loss=126.4646
	step [34/146], loss=106.4081
	step [35/146], loss=105.4266
	step [36/146], loss=120.4898
	step [37/146], loss=114.3959
	step [38/146], loss=131.0447
	step [39/146], loss=108.7589
	step [40/146], loss=134.5251
	step [41/146], loss=126.6939
	step [42/146], loss=120.0121
	step [43/146], loss=111.7922
	step [44/146], loss=109.7803
	step [45/146], loss=92.8622
	step [46/146], loss=113.8170
	step [47/146], loss=97.3288
	step [48/146], loss=118.2549
	step [49/146], loss=116.9229
	step [50/146], loss=118.6217
	step [51/146], loss=113.6009
	step [52/146], loss=117.3445
	step [53/146], loss=122.9835
	step [54/146], loss=111.8514
	step [55/146], loss=113.4858
	step [56/146], loss=92.6470
	step [57/146], loss=104.4558
	step [58/146], loss=129.1686
	step [59/146], loss=105.2199
	step [60/146], loss=102.7538
	step [61/146], loss=123.0710
	step [62/146], loss=114.2977
	step [63/146], loss=101.6886
	step [64/146], loss=102.8139
	step [65/146], loss=129.3420
	step [66/146], loss=119.6256
	step [67/146], loss=141.4394
	step [68/146], loss=111.9931
	step [69/146], loss=136.6017
	step [70/146], loss=107.1389
	step [71/146], loss=109.4562
	step [72/146], loss=106.8943
	step [73/146], loss=105.9278
	step [74/146], loss=94.5542
	step [75/146], loss=106.7117
	step [76/146], loss=132.2390
	step [77/146], loss=104.5110
	step [78/146], loss=109.9948
	step [79/146], loss=126.2277
	step [80/146], loss=117.0621
	step [81/146], loss=122.9212
	step [82/146], loss=127.1420
	step [83/146], loss=98.0285
	step [84/146], loss=110.9316
	step [85/146], loss=139.0011
	step [86/146], loss=110.5873
	step [87/146], loss=121.6070
	step [88/146], loss=97.8535
	step [89/146], loss=114.1700
	step [90/146], loss=114.8606
	step [91/146], loss=108.8815
	step [92/146], loss=103.6844
	step [93/146], loss=113.6604
	step [94/146], loss=131.8537
	step [95/146], loss=109.2698
	step [96/146], loss=110.2678
	step [97/146], loss=113.3475
	step [98/146], loss=109.8761
	step [99/146], loss=87.4463
	step [100/146], loss=127.7195
	step [101/146], loss=134.5957
	step [102/146], loss=130.2319
	step [103/146], loss=121.3683
	step [104/146], loss=112.2695
	step [105/146], loss=120.8402
	step [106/146], loss=111.4167
	step [107/146], loss=119.4082
	step [108/146], loss=124.0869
	step [109/146], loss=109.4024
	step [110/146], loss=120.4494
	step [111/146], loss=110.3178
	step [112/146], loss=137.1267
	step [113/146], loss=113.4918
	step [114/146], loss=102.4953
	step [115/146], loss=123.7327
	step [116/146], loss=105.7306
	step [117/146], loss=116.2519
	step [118/146], loss=123.0938
	step [119/146], loss=108.6232
	step [120/146], loss=116.1590
	step [121/146], loss=117.6352
	step [122/146], loss=120.5038
	step [123/146], loss=112.1564
	step [124/146], loss=103.0587
	step [125/146], loss=105.1532
	step [126/146], loss=121.4671
	step [127/146], loss=95.5974
	step [128/146], loss=110.7928
	step [129/146], loss=105.3138
	step [130/146], loss=110.2341
	step [131/146], loss=96.0317
	step [132/146], loss=110.6696
	step [133/146], loss=103.4126
	step [134/146], loss=99.9212
	step [135/146], loss=117.0420
	step [136/146], loss=113.9113
	step [137/146], loss=126.0110
	step [138/146], loss=119.6455
	step [139/146], loss=119.0191
	step [140/146], loss=123.2445
	step [141/146], loss=109.9632
	step [142/146], loss=89.0128
	step [143/146], loss=134.4646
	step [144/146], loss=138.0222
	step [145/146], loss=107.4923
	step [146/146], loss=50.8393
	Evaluating
	loss=0.0334, precision=0.4681, recall=0.8717, f1=0.6091
Training epoch 17
	step [1/146], loss=137.3113
	step [2/146], loss=106.3618
	step [3/146], loss=100.5458
	step [4/146], loss=109.7386
	step [5/146], loss=125.3097
	step [6/146], loss=115.3714
	step [7/146], loss=110.6616
	step [8/146], loss=126.3116
	step [9/146], loss=107.1457
	step [10/146], loss=124.7712
	step [11/146], loss=108.3036
	step [12/146], loss=120.3551
	step [13/146], loss=117.9229
	step [14/146], loss=128.7475
	step [15/146], loss=119.2260
	step [16/146], loss=116.2118
	step [17/146], loss=109.6920
	step [18/146], loss=112.0492
	step [19/146], loss=127.2640
	step [20/146], loss=114.4935
	step [21/146], loss=93.5601
	step [22/146], loss=106.8416
	step [23/146], loss=122.0287
	step [24/146], loss=134.4821
	step [25/146], loss=116.7685
	step [26/146], loss=125.0644
	step [27/146], loss=114.2238
	step [28/146], loss=98.0934
	step [29/146], loss=109.9202
	step [30/146], loss=109.5930
	step [31/146], loss=125.0493
	step [32/146], loss=135.5023
	step [33/146], loss=133.2213
	step [34/146], loss=103.4586
	step [35/146], loss=116.3643
	step [36/146], loss=127.2822
	step [37/146], loss=138.2744
	step [38/146], loss=130.1303
	step [39/146], loss=106.6701
	step [40/146], loss=131.5027
	step [41/146], loss=120.7649
	step [42/146], loss=108.9766
	step [43/146], loss=113.8903
	step [44/146], loss=127.1593
	step [45/146], loss=130.4782
	step [46/146], loss=90.8270
	step [47/146], loss=108.1437
	step [48/146], loss=95.9924
	step [49/146], loss=110.4726
	step [50/146], loss=112.5250
	step [51/146], loss=108.5009
	step [52/146], loss=105.3731
	step [53/146], loss=115.2520
	step [54/146], loss=110.8838
	step [55/146], loss=107.3822
	step [56/146], loss=108.5440
	step [57/146], loss=102.2470
	step [58/146], loss=106.1982
	step [59/146], loss=114.7997
	step [60/146], loss=114.4734
	step [61/146], loss=116.2257
	step [62/146], loss=110.6619
	step [63/146], loss=122.6527
	step [64/146], loss=126.1830
	step [65/146], loss=120.5588
	step [66/146], loss=127.2059
	step [67/146], loss=110.2071
	step [68/146], loss=102.5500
	step [69/146], loss=100.3515
	step [70/146], loss=109.9293
	step [71/146], loss=99.9940
	step [72/146], loss=128.1977
	step [73/146], loss=115.7613
	step [74/146], loss=111.5475
	step [75/146], loss=112.3375
	step [76/146], loss=97.2302
	step [77/146], loss=108.6256
	step [78/146], loss=97.7311
	step [79/146], loss=115.4071
	step [80/146], loss=102.2857
	step [81/146], loss=113.2845
	step [82/146], loss=110.3896
	step [83/146], loss=106.4337
	step [84/146], loss=108.4698
	step [85/146], loss=115.5678
	step [86/146], loss=129.1263
	step [87/146], loss=124.4456
	step [88/146], loss=108.0059
	step [89/146], loss=118.2908
	step [90/146], loss=117.9299
	step [91/146], loss=146.8118
	step [92/146], loss=137.4383
	step [93/146], loss=112.9674
	step [94/146], loss=113.0870
	step [95/146], loss=116.8430
	step [96/146], loss=103.3606
	step [97/146], loss=110.4960
	step [98/146], loss=118.3813
	step [99/146], loss=83.5696
	step [100/146], loss=121.1013
	step [101/146], loss=100.2004
	step [102/146], loss=93.5994
	step [103/146], loss=115.7646
	step [104/146], loss=123.2160
	step [105/146], loss=121.1838
	step [106/146], loss=110.9486
	step [107/146], loss=98.6544
	step [108/146], loss=111.6830
	step [109/146], loss=108.7177
	step [110/146], loss=95.1617
	step [111/146], loss=103.4705
	step [112/146], loss=93.9536
	step [113/146], loss=112.9751
	step [114/146], loss=107.8196
	step [115/146], loss=94.8430
	step [116/146], loss=98.3329
	step [117/146], loss=131.4780
	step [118/146], loss=102.5059
	step [119/146], loss=141.0165
	step [120/146], loss=130.8541
	step [121/146], loss=113.4408
	step [122/146], loss=119.3041
	step [123/146], loss=101.5382
	step [124/146], loss=106.0469
	step [125/146], loss=101.2909
	step [126/146], loss=112.1297
	step [127/146], loss=129.5695
	step [128/146], loss=119.7223
	step [129/146], loss=116.3010
	step [130/146], loss=113.7812
	step [131/146], loss=117.5714
	step [132/146], loss=125.5889
	step [133/146], loss=120.6371
	step [134/146], loss=110.9378
	step [135/146], loss=106.4166
	step [136/146], loss=117.4334
	step [137/146], loss=131.0463
	step [138/146], loss=101.5841
	step [139/146], loss=122.2063
	step [140/146], loss=101.7148
	step [141/146], loss=117.0774
	step [142/146], loss=108.3318
	step [143/146], loss=102.3126
	step [144/146], loss=123.4771
	step [145/146], loss=122.6931
	step [146/146], loss=55.8920
	Evaluating
	loss=0.0266, precision=0.5389, recall=0.8848, f1=0.6698
Training epoch 18
	step [1/146], loss=122.4699
	step [2/146], loss=118.7771
	step [3/146], loss=117.8529
	step [4/146], loss=111.0841
	step [5/146], loss=119.4019
	step [6/146], loss=105.5030
	step [7/146], loss=102.4312
	step [8/146], loss=119.7805
	step [9/146], loss=132.4583
	step [10/146], loss=109.7316
	step [11/146], loss=125.9297
	step [12/146], loss=126.6537
	step [13/146], loss=90.8521
	step [14/146], loss=104.3646
	step [15/146], loss=133.7299
	step [16/146], loss=117.9013
	step [17/146], loss=125.2237
	step [18/146], loss=107.4536
	step [19/146], loss=127.3883
	step [20/146], loss=103.1036
	step [21/146], loss=108.9172
	step [22/146], loss=111.4367
	step [23/146], loss=107.1703
	step [24/146], loss=100.0126
	step [25/146], loss=92.9780
	step [26/146], loss=108.3965
	step [27/146], loss=111.4476
	step [28/146], loss=130.9475
	step [29/146], loss=109.4121
	step [30/146], loss=108.0306
	step [31/146], loss=96.9644
	step [32/146], loss=128.3750
	step [33/146], loss=96.4621
	step [34/146], loss=111.7415
	step [35/146], loss=113.2499
	step [36/146], loss=106.2701
	step [37/146], loss=106.3827
	step [38/146], loss=103.6199
	step [39/146], loss=105.8944
	step [40/146], loss=101.2106
	step [41/146], loss=115.3619
	step [42/146], loss=113.8642
	step [43/146], loss=102.2655
	step [44/146], loss=91.5863
	step [45/146], loss=107.9999
	step [46/146], loss=114.5134
	step [47/146], loss=103.7570
	step [48/146], loss=117.9292
	step [49/146], loss=102.7980
	step [50/146], loss=117.2437
	step [51/146], loss=112.1022
	step [52/146], loss=115.9033
	step [53/146], loss=114.3509
	step [54/146], loss=124.0657
	step [55/146], loss=113.7860
	step [56/146], loss=121.1940
	step [57/146], loss=109.7131
	step [58/146], loss=97.5341
	step [59/146], loss=115.7691
	step [60/146], loss=119.6508
	step [61/146], loss=113.2482
	step [62/146], loss=104.0275
	step [63/146], loss=124.4034
	step [64/146], loss=117.7506
	step [65/146], loss=110.2871
	step [66/146], loss=123.3081
	step [67/146], loss=119.7038
	step [68/146], loss=119.0160
	step [69/146], loss=110.4769
	step [70/146], loss=117.9814
	step [71/146], loss=109.8812
	step [72/146], loss=112.0333
	step [73/146], loss=116.2034
	step [74/146], loss=113.1590
	step [75/146], loss=109.4159
	step [76/146], loss=110.5436
	step [77/146], loss=103.2252
	step [78/146], loss=91.0912
	step [79/146], loss=113.7382
	step [80/146], loss=102.5686
	step [81/146], loss=116.1031
	step [82/146], loss=106.3484
	step [83/146], loss=95.5694
	step [84/146], loss=117.8201
	step [85/146], loss=103.2182
	step [86/146], loss=117.4461
	step [87/146], loss=122.1222
	step [88/146], loss=106.3269
	step [89/146], loss=94.0506
	step [90/146], loss=93.7293
	step [91/146], loss=111.2456
	step [92/146], loss=100.6893
	step [93/146], loss=91.5581
	step [94/146], loss=117.1776
	step [95/146], loss=116.3845
	step [96/146], loss=109.1679
	step [97/146], loss=125.6185
	step [98/146], loss=114.1135
	step [99/146], loss=97.1599
	step [100/146], loss=116.0319
	step [101/146], loss=123.5324
	step [102/146], loss=105.8258
	step [103/146], loss=129.1596
	step [104/146], loss=138.2300
	step [105/146], loss=137.1296
	step [106/146], loss=114.2094
	step [107/146], loss=110.6932
	step [108/146], loss=130.1586
	step [109/146], loss=128.6328
	step [110/146], loss=107.6956
	step [111/146], loss=106.1399
	step [112/146], loss=102.2363
	step [113/146], loss=102.4421
	step [114/146], loss=118.1109
	step [115/146], loss=116.2025
	step [116/146], loss=122.1384
	step [117/146], loss=149.8735
	step [118/146], loss=121.6838
	step [119/146], loss=125.8066
	step [120/146], loss=129.1422
	step [121/146], loss=114.0848
	step [122/146], loss=93.3416
	step [123/146], loss=107.9649
	step [124/146], loss=97.2864
	step [125/146], loss=122.8547
	step [126/146], loss=117.4483
	step [127/146], loss=97.2279
	step [128/146], loss=141.8959
	step [129/146], loss=117.1892
	step [130/146], loss=125.2484
	step [131/146], loss=110.8464
	step [132/146], loss=128.2461
	step [133/146], loss=114.8182
	step [134/146], loss=110.7471
	step [135/146], loss=107.0609
	step [136/146], loss=109.7077
	step [137/146], loss=115.3111
	step [138/146], loss=114.8527
	step [139/146], loss=118.6680
	step [140/146], loss=113.5532
	step [141/146], loss=118.1307
	step [142/146], loss=109.9372
	step [143/146], loss=122.7038
	step [144/146], loss=114.5263
	step [145/146], loss=112.3704
	step [146/146], loss=55.3776
	Evaluating
	loss=0.0270, precision=0.4370, recall=0.9153, f1=0.5916
Training epoch 19
	step [1/146], loss=129.2549
	step [2/146], loss=125.9296
	step [3/146], loss=111.8640
	step [4/146], loss=109.2109
	step [5/146], loss=109.0712
	step [6/146], loss=120.1604
	step [7/146], loss=103.5654
	step [8/146], loss=117.4813
	step [9/146], loss=101.9451
	step [10/146], loss=116.7490
	step [11/146], loss=121.6461
	step [12/146], loss=119.9894
	step [13/146], loss=105.9959
	step [14/146], loss=109.4458
	step [15/146], loss=123.6921
	step [16/146], loss=93.7128
	step [17/146], loss=109.4875
	step [18/146], loss=104.7148
	step [19/146], loss=109.7648
	step [20/146], loss=117.8396
	step [21/146], loss=117.0230
	step [22/146], loss=116.1330
	step [23/146], loss=115.0819
	step [24/146], loss=98.8027
	step [25/146], loss=110.6494
	step [26/146], loss=97.0238
	step [27/146], loss=109.9526
	step [28/146], loss=108.2309
	step [29/146], loss=98.5889
	step [30/146], loss=127.8686
	step [31/146], loss=100.0873
	step [32/146], loss=107.1068
	step [33/146], loss=98.7381
	step [34/146], loss=116.7449
	step [35/146], loss=109.2132
	step [36/146], loss=85.2113
	step [37/146], loss=98.4860
	step [38/146], loss=107.0857
	step [39/146], loss=131.3911
	step [40/146], loss=96.6146
	step [41/146], loss=98.9479
	step [42/146], loss=119.2849
	step [43/146], loss=124.2433
	step [44/146], loss=119.4193
	step [45/146], loss=106.1293
	step [46/146], loss=104.4641
	step [47/146], loss=116.9689
	step [48/146], loss=111.3172
	step [49/146], loss=101.0195
	step [50/146], loss=92.7596
	step [51/146], loss=128.3791
	step [52/146], loss=98.5409
	step [53/146], loss=110.0063
	step [54/146], loss=106.6352
	step [55/146], loss=127.1743
	step [56/146], loss=98.4652
	step [57/146], loss=111.7261
	step [58/146], loss=115.7243
	step [59/146], loss=96.2417
	step [60/146], loss=117.7748
	step [61/146], loss=123.7063
	step [62/146], loss=104.5291
	step [63/146], loss=113.4160
	step [64/146], loss=120.6584
	step [65/146], loss=93.0216
	step [66/146], loss=128.3077
	step [67/146], loss=120.1475
	step [68/146], loss=100.0573
	step [69/146], loss=113.5193
	step [70/146], loss=101.2931
	step [71/146], loss=109.7774
	step [72/146], loss=102.1488
	step [73/146], loss=108.0795
	step [74/146], loss=105.5685
	step [75/146], loss=117.6696
	step [76/146], loss=100.9195
	step [77/146], loss=123.8124
	step [78/146], loss=99.3099
	step [79/146], loss=111.4885
	step [80/146], loss=117.7063
	step [81/146], loss=120.5019
	step [82/146], loss=127.5205
	step [83/146], loss=123.1286
	step [84/146], loss=100.9004
	step [85/146], loss=119.5340
	step [86/146], loss=102.0610
	step [87/146], loss=107.4879
	step [88/146], loss=125.5999
	step [89/146], loss=93.7629
	step [90/146], loss=100.9083
	step [91/146], loss=121.1588
	step [92/146], loss=135.3403
	step [93/146], loss=123.1458
	step [94/146], loss=128.9309
	step [95/146], loss=133.0920
	step [96/146], loss=102.6241
	step [97/146], loss=87.3761
	step [98/146], loss=106.7736
	step [99/146], loss=110.6995
	step [100/146], loss=102.8934
	step [101/146], loss=110.3926
	step [102/146], loss=113.1472
	step [103/146], loss=96.7720
	step [104/146], loss=115.1912
	step [105/146], loss=102.3387
	step [106/146], loss=111.8806
	step [107/146], loss=114.0065
	step [108/146], loss=132.2168
	step [109/146], loss=118.9877
	step [110/146], loss=117.5515
	step [111/146], loss=127.1538
	step [112/146], loss=99.4532
	step [113/146], loss=104.5665
	step [114/146], loss=106.0739
	step [115/146], loss=126.0363
	step [116/146], loss=95.4423
	step [117/146], loss=107.1142
	step [118/146], loss=92.8809
	step [119/146], loss=133.5854
	step [120/146], loss=104.5784
	step [121/146], loss=101.5030
	step [122/146], loss=114.9667
	step [123/146], loss=98.9681
	step [124/146], loss=114.9181
	step [125/146], loss=127.0829
	step [126/146], loss=104.0355
	step [127/146], loss=117.3953
	step [128/146], loss=120.4660
	step [129/146], loss=114.3084
	step [130/146], loss=134.5471
	step [131/146], loss=111.1944
	step [132/146], loss=124.4898
	step [133/146], loss=107.9176
	step [134/146], loss=113.4747
	step [135/146], loss=98.8797
	step [136/146], loss=121.8128
	step [137/146], loss=101.3945
	step [138/146], loss=134.1581
	step [139/146], loss=108.0723
	step [140/146], loss=107.0508
	step [141/146], loss=113.0241
	step [142/146], loss=99.7245
	step [143/146], loss=109.7245
	step [144/146], loss=124.8177
	step [145/146], loss=116.6679
	step [146/146], loss=42.7362
	Evaluating
	loss=0.0240, precision=0.4797, recall=0.8867, f1=0.6226
Training epoch 20
	step [1/146], loss=93.2108
	step [2/146], loss=99.4206
	step [3/146], loss=97.7328
	step [4/146], loss=95.7746
	step [5/146], loss=84.2569
	step [6/146], loss=108.7840
	step [7/146], loss=104.3708
	step [8/146], loss=114.0005
	step [9/146], loss=116.7856
	step [10/146], loss=108.4777
	step [11/146], loss=99.8749
	step [12/146], loss=111.5850
	step [13/146], loss=115.9173
	step [14/146], loss=116.4675
	step [15/146], loss=113.9316
	step [16/146], loss=129.3224
	step [17/146], loss=91.2893
	step [18/146], loss=97.2831
	step [19/146], loss=116.5615
	step [20/146], loss=120.9377
	step [21/146], loss=101.5653
	step [22/146], loss=109.9766
	step [23/146], loss=108.0284
	step [24/146], loss=101.8725
	step [25/146], loss=110.8973
	step [26/146], loss=111.1879
	step [27/146], loss=112.7181
	step [28/146], loss=94.4717
	step [29/146], loss=101.8426
	step [30/146], loss=118.1340
	step [31/146], loss=138.9394
	step [32/146], loss=103.7736
	step [33/146], loss=100.0522
	step [34/146], loss=118.1024
	step [35/146], loss=106.1869
	step [36/146], loss=116.7938
	step [37/146], loss=101.4479
	step [38/146], loss=83.0830
	step [39/146], loss=124.3839
	step [40/146], loss=99.2381
	step [41/146], loss=105.1075
	step [42/146], loss=95.0650
	step [43/146], loss=109.0732
	step [44/146], loss=120.7273
	step [45/146], loss=120.0957
	step [46/146], loss=97.2246
	step [47/146], loss=120.7160
	step [48/146], loss=130.0945
	step [49/146], loss=109.7023
	step [50/146], loss=124.0863
	step [51/146], loss=106.5843
	step [52/146], loss=121.3015
	step [53/146], loss=110.5672
	step [54/146], loss=106.8809
	step [55/146], loss=99.2446
	step [56/146], loss=98.0442
	step [57/146], loss=95.4351
	step [58/146], loss=101.9101
	step [59/146], loss=137.5252
	step [60/146], loss=132.3091
	step [61/146], loss=110.7893
	step [62/146], loss=106.3835
	step [63/146], loss=99.1743
	step [64/146], loss=121.8781
	step [65/146], loss=122.4004
	step [66/146], loss=121.1801
	step [67/146], loss=109.6425
	step [68/146], loss=110.8300
	step [69/146], loss=106.6780
	step [70/146], loss=126.5698
	step [71/146], loss=104.5996
	step [72/146], loss=97.6055
	step [73/146], loss=142.8817
	step [74/146], loss=103.8290
	step [75/146], loss=119.7864
	step [76/146], loss=122.6790
	step [77/146], loss=114.2427
	step [78/146], loss=115.6842
	step [79/146], loss=106.5711
	step [80/146], loss=108.8522
	step [81/146], loss=106.5953
	step [82/146], loss=87.7405
	step [83/146], loss=117.6955
	step [84/146], loss=119.5999
	step [85/146], loss=107.0551
	step [86/146], loss=93.0582
	step [87/146], loss=98.8561
	step [88/146], loss=118.2726
	step [89/146], loss=123.0656
	step [90/146], loss=108.4254
	step [91/146], loss=104.5194
	step [92/146], loss=104.5543
	step [93/146], loss=95.6824
	step [94/146], loss=105.7488
	step [95/146], loss=104.7202
	step [96/146], loss=111.0148
	step [97/146], loss=119.3089
	step [98/146], loss=98.2734
	step [99/146], loss=108.1371
	step [100/146], loss=124.2648
	step [101/146], loss=112.2500
	step [102/146], loss=121.0393
	step [103/146], loss=109.9258
	step [104/146], loss=103.4125
	step [105/146], loss=127.7878
	step [106/146], loss=98.6912
	step [107/146], loss=116.9806
	step [108/146], loss=113.3743
	step [109/146], loss=101.6754
	step [110/146], loss=101.3660
	step [111/146], loss=103.0814
	step [112/146], loss=101.2658
	step [113/146], loss=107.7160
	step [114/146], loss=118.2121
	step [115/146], loss=103.1557
	step [116/146], loss=100.3761
	step [117/146], loss=128.9985
	step [118/146], loss=119.6226
	step [119/146], loss=107.1492
	step [120/146], loss=113.9084
	step [121/146], loss=105.0851
	step [122/146], loss=108.6534
	step [123/146], loss=116.8947
	step [124/146], loss=127.4360
	step [125/146], loss=119.1571
	step [126/146], loss=97.3192
	step [127/146], loss=105.8245
	step [128/146], loss=105.0129
	step [129/146], loss=114.1207
	step [130/146], loss=115.3377
	step [131/146], loss=125.8862
	step [132/146], loss=111.6750
	step [133/146], loss=112.8316
	step [134/146], loss=100.2153
	step [135/146], loss=124.4377
	step [136/146], loss=91.6945
	step [137/146], loss=107.8471
	step [138/146], loss=114.1186
	step [139/146], loss=122.4008
	step [140/146], loss=120.6899
	step [141/146], loss=120.9462
	step [142/146], loss=100.9783
	step [143/146], loss=124.4274
	step [144/146], loss=109.2385
	step [145/146], loss=144.0092
	step [146/146], loss=46.5250
	Evaluating
	loss=0.0246, precision=0.5553, recall=0.8443, f1=0.6700
Training epoch 21
	step [1/146], loss=95.3454
	step [2/146], loss=102.4092
	step [3/146], loss=120.5196
	step [4/146], loss=108.7868
	step [5/146], loss=96.0884
	step [6/146], loss=125.9611
	step [7/146], loss=102.9367
	step [8/146], loss=99.2049
	step [9/146], loss=128.2098
	step [10/146], loss=122.5628
	step [11/146], loss=108.1009
	step [12/146], loss=107.2747
	step [13/146], loss=128.9071
	step [14/146], loss=95.2417
	step [15/146], loss=115.4572
	step [16/146], loss=96.6612
	step [17/146], loss=102.9869
	step [18/146], loss=98.8687
	step [19/146], loss=109.2934
	step [20/146], loss=107.0689
	step [21/146], loss=111.7097
	step [22/146], loss=122.5567
	step [23/146], loss=112.5063
	step [24/146], loss=101.9151
	step [25/146], loss=107.5902
	step [26/146], loss=115.2291
	step [27/146], loss=117.7883
	step [28/146], loss=79.6687
	step [29/146], loss=107.7595
	step [30/146], loss=103.9610
	step [31/146], loss=111.8292
	step [32/146], loss=110.8407
	step [33/146], loss=103.4902
	step [34/146], loss=118.5824
	step [35/146], loss=103.1213
	step [36/146], loss=102.4574
	step [37/146], loss=106.5003
	step [38/146], loss=112.2433
	step [39/146], loss=97.3712
	step [40/146], loss=86.5397
	step [41/146], loss=117.1733
	step [42/146], loss=109.1856
	step [43/146], loss=116.3890
	step [44/146], loss=110.7685
	step [45/146], loss=89.2325
	step [46/146], loss=101.8634
	step [47/146], loss=92.1164
	step [48/146], loss=119.8843
	step [49/146], loss=97.1318
	step [50/146], loss=99.4318
	step [51/146], loss=111.2106
	step [52/146], loss=113.7230
	step [53/146], loss=109.4893
	step [54/146], loss=98.2943
	step [55/146], loss=122.9154
	step [56/146], loss=115.9175
	step [57/146], loss=93.8199
	step [58/146], loss=102.9499
	step [59/146], loss=98.5087
	step [60/146], loss=123.1699
	step [61/146], loss=96.3630
	step [62/146], loss=117.8368
	step [63/146], loss=130.4667
	step [64/146], loss=85.5630
	step [65/146], loss=110.7782
	step [66/146], loss=116.4218
	step [67/146], loss=126.7885
	step [68/146], loss=95.5642
	step [69/146], loss=95.3270
	step [70/146], loss=129.5755
	step [71/146], loss=110.8035
	step [72/146], loss=101.2371
	step [73/146], loss=103.2023
	step [74/146], loss=91.7308
	step [75/146], loss=107.7968
	step [76/146], loss=118.0887
	step [77/146], loss=119.0427
	step [78/146], loss=101.0959
	step [79/146], loss=128.3064
	step [80/146], loss=114.2307
	step [81/146], loss=117.0646
	step [82/146], loss=108.1355
	step [83/146], loss=97.2587
	step [84/146], loss=99.6922
	step [85/146], loss=136.0500
	step [86/146], loss=108.4170
	step [87/146], loss=107.5007
	step [88/146], loss=128.8159
	step [89/146], loss=116.5341
	step [90/146], loss=122.9095
	step [91/146], loss=91.9706
	step [92/146], loss=145.5431
	step [93/146], loss=106.2807
	step [94/146], loss=111.6698
	step [95/146], loss=102.2766
	step [96/146], loss=108.0322
	step [97/146], loss=110.6145
	step [98/146], loss=101.2880
	step [99/146], loss=120.2535
	step [100/146], loss=108.9836
	step [101/146], loss=96.3751
	step [102/146], loss=111.6101
	step [103/146], loss=107.3208
	step [104/146], loss=91.6561
	step [105/146], loss=120.8059
	step [106/146], loss=105.2972
	step [107/146], loss=129.9049
	step [108/146], loss=116.0167
	step [109/146], loss=102.6652
	step [110/146], loss=114.7992
	step [111/146], loss=112.9827
	step [112/146], loss=99.6283
	step [113/146], loss=121.0114
	step [114/146], loss=97.9286
	step [115/146], loss=91.0650
	step [116/146], loss=102.5807
	step [117/146], loss=98.5808
	step [118/146], loss=136.2520
	step [119/146], loss=104.0098
	step [120/146], loss=87.1127
	step [121/146], loss=116.4487
	step [122/146], loss=105.3795
	step [123/146], loss=123.2452
	step [124/146], loss=108.4282
	step [125/146], loss=129.0214
	step [126/146], loss=102.5101
	step [127/146], loss=120.9463
	step [128/146], loss=126.2145
	step [129/146], loss=99.6634
	step [130/146], loss=128.8500
	step [131/146], loss=107.1106
	step [132/146], loss=124.2855
	step [133/146], loss=104.3517
	step [134/146], loss=90.3166
	step [135/146], loss=111.6623
	step [136/146], loss=104.2504
	step [137/146], loss=120.2499
	step [138/146], loss=99.3257
	step [139/146], loss=108.1607
	step [140/146], loss=129.1634
	step [141/146], loss=110.6151
	step [142/146], loss=104.3261
	step [143/146], loss=105.0601
	step [144/146], loss=110.0700
	step [145/146], loss=113.6977
	step [146/146], loss=47.2657
	Evaluating
	loss=0.0185, precision=0.5288, recall=0.8777, f1=0.6600
Training epoch 22
	step [1/146], loss=134.1291
	step [2/146], loss=119.7483
	step [3/146], loss=116.7763
	step [4/146], loss=97.7363
	step [5/146], loss=108.3177
	step [6/146], loss=111.4167
	step [7/146], loss=117.7140
	step [8/146], loss=96.6157
	step [9/146], loss=99.0398
	step [10/146], loss=116.2368
	step [11/146], loss=109.9898
	step [12/146], loss=92.2545
	step [13/146], loss=102.5715
	step [14/146], loss=104.2607
	step [15/146], loss=135.3000
	step [16/146], loss=104.5021
	step [17/146], loss=122.4436
	step [18/146], loss=97.2040
	step [19/146], loss=111.9433
	step [20/146], loss=123.4064
	step [21/146], loss=114.2006
	step [22/146], loss=109.7300
	step [23/146], loss=101.8302
	step [24/146], loss=116.1329
	step [25/146], loss=107.5947
	step [26/146], loss=118.0553
	step [27/146], loss=95.8167
	step [28/146], loss=111.7727
	step [29/146], loss=107.1871
	step [30/146], loss=117.3567
	step [31/146], loss=100.1312
	step [32/146], loss=96.8431
	step [33/146], loss=91.9231
	step [34/146], loss=89.4519
	step [35/146], loss=120.1145
	step [36/146], loss=108.0991
	step [37/146], loss=96.4423
	step [38/146], loss=102.7251
	step [39/146], loss=116.8136
	step [40/146], loss=108.2809
	step [41/146], loss=109.2710
	step [42/146], loss=96.8803
	step [43/146], loss=109.0570
	step [44/146], loss=124.9148
	step [45/146], loss=119.8158
	step [46/146], loss=106.9657
	step [47/146], loss=116.7187
	step [48/146], loss=99.2857
	step [49/146], loss=118.0904
	step [50/146], loss=92.2491
	step [51/146], loss=103.9865
	step [52/146], loss=102.6470
	step [53/146], loss=95.4160
	step [54/146], loss=115.9201
	step [55/146], loss=95.2350
	step [56/146], loss=106.5861
	step [57/146], loss=109.5502
	step [58/146], loss=93.5304
	step [59/146], loss=117.1200
	step [60/146], loss=110.3182
	step [61/146], loss=129.5416
	step [62/146], loss=103.3180
	step [63/146], loss=104.0529
	step [64/146], loss=98.5142
	step [65/146], loss=122.0103
	step [66/146], loss=91.5638
	step [67/146], loss=115.6294
	step [68/146], loss=119.4670
	step [69/146], loss=85.4319
	step [70/146], loss=106.8078
	step [71/146], loss=120.0782
	step [72/146], loss=112.1998
	step [73/146], loss=111.0659
	step [74/146], loss=113.7510
	step [75/146], loss=114.5342
	step [76/146], loss=98.6072
	step [77/146], loss=118.5862
	step [78/146], loss=113.6063
	step [79/146], loss=104.8819
	step [80/146], loss=88.8137
	step [81/146], loss=110.5704
	step [82/146], loss=105.9841
	step [83/146], loss=105.2915
	step [84/146], loss=119.5077
	step [85/146], loss=86.2643
	step [86/146], loss=91.5536
	step [87/146], loss=116.0015
	step [88/146], loss=109.1275
	step [89/146], loss=94.2056
	step [90/146], loss=106.2872
	step [91/146], loss=95.4398
	step [92/146], loss=105.4007
	step [93/146], loss=106.5166
	step [94/146], loss=119.8845
	step [95/146], loss=117.7550
	step [96/146], loss=115.5891
	step [97/146], loss=104.1582
	step [98/146], loss=110.4383
	step [99/146], loss=112.4152
	step [100/146], loss=105.3766
	step [101/146], loss=122.6916
	step [102/146], loss=102.8120
	step [103/146], loss=119.9450
	step [104/146], loss=119.0723
	step [105/146], loss=99.3152
	step [106/146], loss=102.7062
	step [107/146], loss=113.6304
	step [108/146], loss=120.1693
	step [109/146], loss=94.0112
	step [110/146], loss=101.3954
	step [111/146], loss=101.7062
	step [112/146], loss=107.8804
	step [113/146], loss=119.8874
	step [114/146], loss=99.3317
	step [115/146], loss=105.3017
	step [116/146], loss=110.6510
	step [117/146], loss=107.2241
	step [118/146], loss=116.3482
	step [119/146], loss=105.7114
	step [120/146], loss=104.2404
	step [121/146], loss=109.1787
	step [122/146], loss=127.5853
	step [123/146], loss=102.3357
	step [124/146], loss=115.7205
	step [125/146], loss=128.0039
	step [126/146], loss=113.3898
	step [127/146], loss=102.6002
	step [128/146], loss=92.3989
	step [129/146], loss=114.0192
	step [130/146], loss=123.8420
	step [131/146], loss=98.1472
	step [132/146], loss=100.2008
	step [133/146], loss=104.8514
	step [134/146], loss=123.5135
	step [135/146], loss=112.4682
	step [136/146], loss=87.9141
	step [137/146], loss=62.3091
	step [138/146], loss=100.9479
	step [139/146], loss=101.0245
	step [140/146], loss=115.4331
	step [141/146], loss=114.3162
	step [142/146], loss=107.2528
	step [143/146], loss=108.1424
	step [144/146], loss=109.4803
	step [145/146], loss=110.6671
	step [146/146], loss=50.6177
	Evaluating
	loss=0.0212, precision=0.4617, recall=0.8951, f1=0.6091
Training epoch 23
	step [1/146], loss=104.4878
	step [2/146], loss=119.2375
	step [3/146], loss=94.0287
	step [4/146], loss=89.9548
	step [5/146], loss=132.0279
	step [6/146], loss=112.2728
	step [7/146], loss=95.3686
	step [8/146], loss=107.9656
	step [9/146], loss=94.8034
	step [10/146], loss=118.1747
	step [11/146], loss=120.4576
	step [12/146], loss=111.1003
	step [13/146], loss=111.0635
	step [14/146], loss=108.2927
	step [15/146], loss=103.6266
	step [16/146], loss=93.6162
	step [17/146], loss=104.6944
	step [18/146], loss=108.7545
	step [19/146], loss=110.3061
	step [20/146], loss=83.2516
	step [21/146], loss=118.2820
	step [22/146], loss=117.5988
	step [23/146], loss=98.3477
	step [24/146], loss=92.3480
	step [25/146], loss=116.6416
	step [26/146], loss=120.6418
	step [27/146], loss=110.8630
	step [28/146], loss=109.5260
	step [29/146], loss=93.4674
	step [30/146], loss=99.8067
	step [31/146], loss=110.4571
	step [32/146], loss=124.4477
	step [33/146], loss=109.2229
	step [34/146], loss=106.7424
	step [35/146], loss=103.3655
	step [36/146], loss=98.0206
	step [37/146], loss=118.9578
	step [38/146], loss=93.8782
	step [39/146], loss=103.5641
	step [40/146], loss=96.1696
	step [41/146], loss=109.4171
	step [42/146], loss=112.7440
	step [43/146], loss=95.9539
	step [44/146], loss=102.7039
	step [45/146], loss=112.2912
	step [46/146], loss=119.2498
	step [47/146], loss=103.3240
	step [48/146], loss=106.6739
	step [49/146], loss=104.7177
	step [50/146], loss=128.8898
	step [51/146], loss=95.4681
	step [52/146], loss=116.0048
	step [53/146], loss=86.9307
	step [54/146], loss=108.8172
	step [55/146], loss=123.9840
	step [56/146], loss=98.2608
	step [57/146], loss=96.0932
	step [58/146], loss=92.3036
	step [59/146], loss=114.0002
	step [60/146], loss=96.3958
	step [61/146], loss=85.6072
	step [62/146], loss=103.5033
	step [63/146], loss=100.5312
	step [64/146], loss=117.2941
	step [65/146], loss=108.9460
	step [66/146], loss=86.3096
	step [67/146], loss=111.2060
	step [68/146], loss=104.0769
	step [69/146], loss=104.5118
	step [70/146], loss=112.6156
	step [71/146], loss=119.7565
	step [72/146], loss=104.3090
	step [73/146], loss=106.3824
	step [74/146], loss=119.4025
	step [75/146], loss=98.1766
	step [76/146], loss=97.8893
	step [77/146], loss=109.9423
	step [78/146], loss=123.3643
	step [79/146], loss=102.1412
	step [80/146], loss=82.4012
	step [81/146], loss=109.9501
	step [82/146], loss=99.5583
	step [83/146], loss=113.7858
	step [84/146], loss=118.0746
	step [85/146], loss=104.0682
	step [86/146], loss=121.7519
	step [87/146], loss=116.2991
	step [88/146], loss=91.2386
	step [89/146], loss=98.3041
	step [90/146], loss=89.5371
	step [91/146], loss=107.6185
	step [92/146], loss=103.3885
	step [93/146], loss=97.3082
	step [94/146], loss=102.1359
	step [95/146], loss=105.4240
	step [96/146], loss=93.4332
	step [97/146], loss=110.5837
	step [98/146], loss=104.6992
	step [99/146], loss=109.8616
	step [100/146], loss=112.6082
	step [101/146], loss=116.1478
	step [102/146], loss=127.9845
	step [103/146], loss=107.1263
	step [104/146], loss=119.1226
	step [105/146], loss=98.2804
	step [106/146], loss=111.6568
	step [107/146], loss=111.5798
	step [108/146], loss=126.5167
	step [109/146], loss=104.7109
	step [110/146], loss=124.7679
	step [111/146], loss=99.2798
	step [112/146], loss=119.2779
	step [113/146], loss=92.2826
	step [114/146], loss=111.9151
	step [115/146], loss=117.2895
	step [116/146], loss=151.1152
	step [117/146], loss=127.0516
	step [118/146], loss=105.9481
	step [119/146], loss=115.9004
	step [120/146], loss=109.6526
	step [121/146], loss=103.1076
	step [122/146], loss=98.7327
	step [123/146], loss=104.4211
	step [124/146], loss=98.5612
	step [125/146], loss=115.0276
	step [126/146], loss=103.5219
	step [127/146], loss=85.0775
	step [128/146], loss=117.5944
	step [129/146], loss=83.7941
	step [130/146], loss=100.8895
	step [131/146], loss=99.6337
	step [132/146], loss=96.2520
	step [133/146], loss=102.3903
	step [134/146], loss=127.9757
	step [135/146], loss=104.8911
	step [136/146], loss=95.4895
	step [137/146], loss=97.4572
	step [138/146], loss=105.0986
	step [139/146], loss=92.4140
	step [140/146], loss=110.5098
	step [141/146], loss=116.8892
	step [142/146], loss=105.1744
	step [143/146], loss=104.1785
	step [144/146], loss=104.5960
	step [145/146], loss=139.1434
	step [146/146], loss=45.4501
	Evaluating
	loss=0.0205, precision=0.4090, recall=0.8704, f1=0.5565
Training epoch 24
	step [1/146], loss=102.0872
	step [2/146], loss=107.5385
	step [3/146], loss=98.7416
	step [4/146], loss=77.7066
	step [5/146], loss=111.4061
	step [6/146], loss=100.4496
	step [7/146], loss=101.4883
	step [8/146], loss=133.9783
	step [9/146], loss=111.2445
	step [10/146], loss=103.8140
	step [11/146], loss=109.6923
	step [12/146], loss=110.3595
	step [13/146], loss=93.5624
	step [14/146], loss=89.8712
	step [15/146], loss=99.7984
	step [16/146], loss=104.1492
	step [17/146], loss=113.7522
	step [18/146], loss=112.7521
	step [19/146], loss=87.0914
	step [20/146], loss=127.1852
	step [21/146], loss=88.9008
	step [22/146], loss=104.0259
	step [23/146], loss=118.1031
	step [24/146], loss=117.8856
	step [25/146], loss=100.8621
	step [26/146], loss=97.9944
	step [27/146], loss=99.1575
	step [28/146], loss=112.7701
	step [29/146], loss=97.6237
	step [30/146], loss=125.7416
	step [31/146], loss=130.3593
	step [32/146], loss=104.2085
	step [33/146], loss=112.7788
	step [34/146], loss=107.8618
	step [35/146], loss=100.8342
	step [36/146], loss=103.4261
	step [37/146], loss=86.7907
	step [38/146], loss=117.9337
	step [39/146], loss=114.8964
	step [40/146], loss=103.8834
	step [41/146], loss=103.4048
	step [42/146], loss=109.6746
	step [43/146], loss=97.6038
	step [44/146], loss=114.3591
	step [45/146], loss=104.0124
	step [46/146], loss=117.5097
	step [47/146], loss=105.5609
	step [48/146], loss=98.5213
	step [49/146], loss=110.2662
	step [50/146], loss=115.7352
	step [51/146], loss=100.6325
	step [52/146], loss=102.9181
	step [53/146], loss=101.6505
	step [54/146], loss=106.5084
	step [55/146], loss=115.1547
	step [56/146], loss=101.5208
	step [57/146], loss=101.2188
	step [58/146], loss=108.7540
	step [59/146], loss=124.7281
	step [60/146], loss=110.1224
	step [61/146], loss=99.0448
	step [62/146], loss=100.6512
	step [63/146], loss=99.7815
	step [64/146], loss=111.3760
	step [65/146], loss=118.9980
	step [66/146], loss=114.8882
	step [67/146], loss=104.1301
	step [68/146], loss=119.9500
	step [69/146], loss=112.2148
	step [70/146], loss=98.2023
	step [71/146], loss=101.4177
	step [72/146], loss=101.2106
	step [73/146], loss=101.3325
	step [74/146], loss=94.6904
	step [75/146], loss=115.1216
	step [76/146], loss=96.6188
	step [77/146], loss=88.2914
	step [78/146], loss=108.3506
	step [79/146], loss=112.7876
	step [80/146], loss=98.2926
	step [81/146], loss=106.6558
	step [82/146], loss=99.4805
	step [83/146], loss=110.5936
	step [84/146], loss=96.3991
	step [85/146], loss=109.0227
	step [86/146], loss=117.9714
	step [87/146], loss=115.7024
	step [88/146], loss=103.1882
	step [89/146], loss=111.6139
	step [90/146], loss=101.9248
	step [91/146], loss=115.2550
	step [92/146], loss=121.2764
	step [93/146], loss=102.1424
	step [94/146], loss=109.6752
	step [95/146], loss=103.7929
	step [96/146], loss=108.5672
	step [97/146], loss=97.6889
	step [98/146], loss=107.5548
	step [99/146], loss=110.9625
	step [100/146], loss=92.3068
	step [101/146], loss=93.2666
	step [102/146], loss=109.2365
	step [103/146], loss=112.5063
	step [104/146], loss=107.9014
	step [105/146], loss=94.2409
	step [106/146], loss=93.1312
	step [107/146], loss=105.9490
	step [108/146], loss=115.5805
	step [109/146], loss=100.6813
	step [110/146], loss=98.6324
	step [111/146], loss=97.0713
	step [112/146], loss=93.3090
	step [113/146], loss=123.9794
	step [114/146], loss=101.9824
	step [115/146], loss=104.6617
	step [116/146], loss=119.7464
	step [117/146], loss=94.4031
	step [118/146], loss=94.9742
	step [119/146], loss=108.6277
	step [120/146], loss=102.3959
	step [121/146], loss=102.4087
	step [122/146], loss=106.2024
	step [123/146], loss=111.4334
	step [124/146], loss=100.3499
	step [125/146], loss=96.0610
	step [126/146], loss=114.9884
	step [127/146], loss=104.7335
	step [128/146], loss=107.1146
	step [129/146], loss=110.3996
	step [130/146], loss=92.9068
	step [131/146], loss=110.2705
	step [132/146], loss=95.2110
	step [133/146], loss=94.0920
	step [134/146], loss=112.9646
	step [135/146], loss=122.2315
	step [136/146], loss=102.8078
	step [137/146], loss=117.2534
	step [138/146], loss=122.8898
	step [139/146], loss=137.4263
	step [140/146], loss=96.1651
	step [141/146], loss=94.5817
	step [142/146], loss=123.4242
	step [143/146], loss=78.6714
	step [144/146], loss=118.9578
	step [145/146], loss=116.0397
	step [146/146], loss=46.3103
	Evaluating
	loss=0.0156, precision=0.5080, recall=0.8686, f1=0.6411
Training epoch 25
	step [1/146], loss=108.5877
	step [2/146], loss=97.9034
	step [3/146], loss=110.3056
	step [4/146], loss=99.0653
	step [5/146], loss=101.1016
	step [6/146], loss=109.4402
	step [7/146], loss=108.5397
	step [8/146], loss=103.0153
	step [9/146], loss=116.0273
	step [10/146], loss=76.8651
	step [11/146], loss=91.2820
	step [12/146], loss=93.2641
	step [13/146], loss=104.4866
	step [14/146], loss=121.7607
	step [15/146], loss=112.5515
	step [16/146], loss=119.6477
	step [17/146], loss=96.6177
	step [18/146], loss=101.7859
	step [19/146], loss=98.9123
	step [20/146], loss=112.6334
	step [21/146], loss=92.9924
	step [22/146], loss=96.9199
	step [23/146], loss=105.6103
	step [24/146], loss=105.5529
	step [25/146], loss=106.8491
	step [26/146], loss=100.7781
	step [27/146], loss=123.2867
	step [28/146], loss=102.2716
	step [29/146], loss=115.8699
	step [30/146], loss=115.2838
	step [31/146], loss=101.9061
	step [32/146], loss=111.3611
	step [33/146], loss=92.6839
	step [34/146], loss=102.8887
	step [35/146], loss=94.8408
	step [36/146], loss=112.6882
	step [37/146], loss=108.0039
	step [38/146], loss=139.3990
	step [39/146], loss=104.8925
	step [40/146], loss=111.1890
	step [41/146], loss=102.4633
	step [42/146], loss=102.6332
	step [43/146], loss=121.9918
	step [44/146], loss=104.0063
	step [45/146], loss=110.7502
	step [46/146], loss=88.6486
	step [47/146], loss=105.1344
	step [48/146], loss=89.8272
	step [49/146], loss=110.3947
	step [50/146], loss=110.4682
	step [51/146], loss=107.4389
	step [52/146], loss=112.2396
	step [53/146], loss=88.4078
	step [54/146], loss=85.8804
	step [55/146], loss=113.8125
	step [56/146], loss=119.8907
	step [57/146], loss=109.1475
	step [58/146], loss=78.4904
	step [59/146], loss=101.3534
	step [60/146], loss=123.5267
	step [61/146], loss=114.2722
	step [62/146], loss=105.3568
	step [63/146], loss=104.9017
	step [64/146], loss=102.5063
	step [65/146], loss=105.2872
	step [66/146], loss=113.1419
	step [67/146], loss=105.0766
	step [68/146], loss=115.0911
	step [69/146], loss=83.1812
	step [70/146], loss=94.3455
	step [71/146], loss=110.0657
	step [72/146], loss=122.5224
	step [73/146], loss=117.6349
	step [74/146], loss=107.9788
	step [75/146], loss=111.4707
	step [76/146], loss=96.0342
	step [77/146], loss=99.9049
	step [78/146], loss=98.1786
	step [79/146], loss=120.2959
	step [80/146], loss=112.4464
	step [81/146], loss=124.2711
	step [82/146], loss=107.4426
	step [83/146], loss=93.3261
	step [84/146], loss=120.1290
	step [85/146], loss=116.6678
	step [86/146], loss=102.8943
	step [87/146], loss=98.3073
	step [88/146], loss=113.2384
	step [89/146], loss=106.1126
	step [90/146], loss=101.4478
	step [91/146], loss=88.3587
	step [92/146], loss=105.8878
	step [93/146], loss=85.6036
	step [94/146], loss=94.6555
	step [95/146], loss=92.1780
	step [96/146], loss=104.6828
	step [97/146], loss=107.2146
	step [98/146], loss=104.1471
	step [99/146], loss=113.3501
	step [100/146], loss=89.8799
	step [101/146], loss=101.0127
	step [102/146], loss=136.5831
	step [103/146], loss=103.8169
	step [104/146], loss=134.9501
	step [105/146], loss=94.6224
	step [106/146], loss=112.2480
	step [107/146], loss=115.6582
	step [108/146], loss=107.2431
	step [109/146], loss=96.8780
	step [110/146], loss=111.0459
	step [111/146], loss=100.4116
	step [112/146], loss=103.1638
	step [113/146], loss=89.5236
	step [114/146], loss=110.0801
	step [115/146], loss=99.2977
	step [116/146], loss=103.2645
	step [117/146], loss=94.9067
	step [118/146], loss=108.9915
	step [119/146], loss=88.5327
	step [120/146], loss=103.7887
	step [121/146], loss=98.9958
	step [122/146], loss=101.6168
	step [123/146], loss=106.9563
	step [124/146], loss=106.6736
	step [125/146], loss=102.9098
	step [126/146], loss=99.7046
	step [127/146], loss=92.1414
	step [128/146], loss=127.5443
	step [129/146], loss=128.4178
	step [130/146], loss=101.1442
	step [131/146], loss=105.0259
	step [132/146], loss=116.6642
	step [133/146], loss=92.2802
	step [134/146], loss=92.4455
	step [135/146], loss=84.7213
	step [136/146], loss=98.7348
	step [137/146], loss=116.2519
	step [138/146], loss=95.4683
	step [139/146], loss=82.3981
	step [140/146], loss=97.1940
	step [141/146], loss=112.9172
	step [142/146], loss=107.8728
	step [143/146], loss=98.9358
	step [144/146], loss=92.6094
	step [145/146], loss=98.2528
	step [146/146], loss=57.5333
	Evaluating
	loss=0.0151, precision=0.5234, recall=0.8670, f1=0.6528
Training epoch 26
	step [1/146], loss=79.0351
	step [2/146], loss=114.0237
	step [3/146], loss=106.9886
	step [4/146], loss=98.3626
	step [5/146], loss=91.9395
	step [6/146], loss=129.5542
	step [7/146], loss=98.8692
	step [8/146], loss=95.8152
	step [9/146], loss=78.1291
	step [10/146], loss=101.4323
	step [11/146], loss=113.3763
	step [12/146], loss=92.8817
	step [13/146], loss=90.0452
	step [14/146], loss=102.3055
	step [15/146], loss=89.9412
	step [16/146], loss=106.7000
	step [17/146], loss=106.9877
	step [18/146], loss=113.8718
	step [19/146], loss=111.1044
	step [20/146], loss=89.7109
	step [21/146], loss=105.8401
	step [22/146], loss=88.6492
	step [23/146], loss=105.6109
	step [24/146], loss=114.0563
	step [25/146], loss=101.8189
	step [26/146], loss=92.9096
	step [27/146], loss=101.3535
	step [28/146], loss=122.5409
	step [29/146], loss=103.4665
	step [30/146], loss=93.2733
	step [31/146], loss=121.7543
	step [32/146], loss=101.7556
	step [33/146], loss=107.4106
	step [34/146], loss=107.0556
	step [35/146], loss=96.2958
	step [36/146], loss=99.7240
	step [37/146], loss=107.0111
	step [38/146], loss=110.6336
	step [39/146], loss=100.1220
	step [40/146], loss=83.6830
	step [41/146], loss=117.0094
	step [42/146], loss=109.8410
	step [43/146], loss=90.3952
	step [44/146], loss=102.5646
	step [45/146], loss=100.6786
	step [46/146], loss=121.4777
	step [47/146], loss=110.2508
	step [48/146], loss=87.8555
	step [49/146], loss=114.5775
	step [50/146], loss=114.9260
	step [51/146], loss=110.9006
	step [52/146], loss=104.1878
	step [53/146], loss=112.9223
	step [54/146], loss=112.5129
	step [55/146], loss=84.4720
	step [56/146], loss=116.5057
	step [57/146], loss=98.7266
	step [58/146], loss=106.3972
	step [59/146], loss=105.4508
	step [60/146], loss=95.2348
	step [61/146], loss=86.4154
	step [62/146], loss=117.9825
	step [63/146], loss=110.3406
	step [64/146], loss=106.8737
	step [65/146], loss=114.0859
	step [66/146], loss=93.3645
	step [67/146], loss=117.8511
	step [68/146], loss=100.5602
	step [69/146], loss=97.9006
	step [70/146], loss=93.5697
	step [71/146], loss=104.5515
	step [72/146], loss=116.1265
	step [73/146], loss=106.0272
	step [74/146], loss=103.9657
	step [75/146], loss=107.7427
	step [76/146], loss=100.1981
	step [77/146], loss=118.0021
	step [78/146], loss=113.5438
	step [79/146], loss=110.9202
	step [80/146], loss=117.6602
	step [81/146], loss=109.2687
	step [82/146], loss=114.0470
	step [83/146], loss=99.4706
	step [84/146], loss=95.3102
	step [85/146], loss=125.3776
	step [86/146], loss=95.6550
	step [87/146], loss=104.3324
	step [88/146], loss=84.1032
	step [89/146], loss=108.4138
	step [90/146], loss=102.7434
	step [91/146], loss=114.9248
	step [92/146], loss=87.2065
	step [93/146], loss=98.0861
	step [94/146], loss=101.5546
	step [95/146], loss=105.6748
	step [96/146], loss=115.8051
	step [97/146], loss=96.8894
	step [98/146], loss=85.6506
	step [99/146], loss=123.9411
	step [100/146], loss=105.8873
	step [101/146], loss=97.4780
	step [102/146], loss=98.6143
	step [103/146], loss=94.5924
	step [104/146], loss=101.1233
	step [105/146], loss=101.5850
	step [106/146], loss=82.1935
	step [107/146], loss=92.7361
	step [108/146], loss=102.6901
	step [109/146], loss=113.0433
	step [110/146], loss=93.3386
	step [111/146], loss=115.4645
	step [112/146], loss=95.7487
	step [113/146], loss=125.7504
	step [114/146], loss=80.9630
	step [115/146], loss=116.6441
	step [116/146], loss=100.7963
	step [117/146], loss=91.0511
	step [118/146], loss=94.5341
	step [119/146], loss=110.4262
	step [120/146], loss=85.9363
	step [121/146], loss=97.8852
	step [122/146], loss=116.2218
	step [123/146], loss=111.4714
	step [124/146], loss=103.4684
	step [125/146], loss=110.2816
	step [126/146], loss=97.9079
	step [127/146], loss=110.0667
	step [128/146], loss=95.2767
	step [129/146], loss=102.2178
	step [130/146], loss=93.0463
	step [131/146], loss=111.0814
	step [132/146], loss=114.8464
	step [133/146], loss=109.0171
	step [134/146], loss=126.4693
	step [135/146], loss=113.3580
	step [136/146], loss=112.2813
	step [137/146], loss=94.4230
	step [138/146], loss=119.2626
	step [139/146], loss=105.2348
	step [140/146], loss=91.9251
	step [141/146], loss=118.7855
	step [142/146], loss=87.4325
	step [143/146], loss=98.3430
	step [144/146], loss=122.9435
	step [145/146], loss=101.0235
	step [146/146], loss=59.6063
	Evaluating
	loss=0.0213, precision=0.3224, recall=0.8607, f1=0.4691
Training epoch 27
	step [1/146], loss=99.2953
	step [2/146], loss=106.7977
	step [3/146], loss=82.9498
	step [4/146], loss=86.4700
	step [5/146], loss=98.6296
	step [6/146], loss=101.3697
	step [7/146], loss=92.4910
	step [8/146], loss=107.0581
	step [9/146], loss=97.1785
	step [10/146], loss=106.2459
	step [11/146], loss=99.7383
	step [12/146], loss=108.6952
	step [13/146], loss=102.8431
	step [14/146], loss=115.2918
	step [15/146], loss=106.5223
	step [16/146], loss=101.5995
	step [17/146], loss=103.5519
	step [18/146], loss=100.2916
	step [19/146], loss=93.5804
	step [20/146], loss=105.5358
	step [21/146], loss=100.0715
	step [22/146], loss=102.2955
	step [23/146], loss=84.6888
	step [24/146], loss=82.8247
	step [25/146], loss=102.3311
	step [26/146], loss=92.2717
	step [27/146], loss=115.7983
	step [28/146], loss=131.7522
	step [29/146], loss=87.7771
	step [30/146], loss=110.3175
	step [31/146], loss=112.4421
	step [32/146], loss=106.8326
	step [33/146], loss=99.3147
	step [34/146], loss=111.3745
	step [35/146], loss=114.2641
	step [36/146], loss=123.1253
	step [37/146], loss=95.9309
	step [38/146], loss=108.2307
	step [39/146], loss=92.6839
	step [40/146], loss=87.1553
	step [41/146], loss=90.5917
	step [42/146], loss=96.5742
	step [43/146], loss=109.6953
	step [44/146], loss=104.5439
	step [45/146], loss=96.9177
	step [46/146], loss=116.4228
	step [47/146], loss=96.5657
	step [48/146], loss=102.9760
	step [49/146], loss=91.1976
	step [50/146], loss=95.0170
	step [51/146], loss=106.0023
	step [52/146], loss=96.5243
	step [53/146], loss=117.4814
	step [54/146], loss=118.5526
	step [55/146], loss=115.8659
	step [56/146], loss=102.0284
	step [57/146], loss=103.7938
	step [58/146], loss=100.4106
	step [59/146], loss=113.2212
	step [60/146], loss=113.9763
	step [61/146], loss=103.4158
	step [62/146], loss=78.6620
	step [63/146], loss=101.0138
	step [64/146], loss=108.1476
	step [65/146], loss=104.8763
	step [66/146], loss=92.3993
	step [67/146], loss=127.6171
	step [68/146], loss=105.5805
	step [69/146], loss=113.8525
	step [70/146], loss=111.1624
	step [71/146], loss=112.5718
	step [72/146], loss=123.2531
	step [73/146], loss=112.4948
	step [74/146], loss=110.7443
	step [75/146], loss=99.3738
	step [76/146], loss=102.1154
	step [77/146], loss=109.2014
	step [78/146], loss=96.5463
	step [79/146], loss=108.1361
	step [80/146], loss=110.7622
	step [81/146], loss=103.8484
	step [82/146], loss=110.6488
	step [83/146], loss=102.7328
	step [84/146], loss=121.8761
	step [85/146], loss=111.7821
	step [86/146], loss=112.0528
	step [87/146], loss=108.2235
	step [88/146], loss=106.4907
	step [89/146], loss=88.2173
	step [90/146], loss=93.8346
	step [91/146], loss=91.4851
	step [92/146], loss=108.4599
	step [93/146], loss=97.3884
	step [94/146], loss=85.6151
	step [95/146], loss=104.7938
	step [96/146], loss=103.1269
	step [97/146], loss=111.9720
	step [98/146], loss=122.6948
	step [99/146], loss=111.0303
	step [100/146], loss=99.3540
	step [101/146], loss=105.6048
	step [102/146], loss=116.8213
	step [103/146], loss=96.4314
	step [104/146], loss=107.4167
	step [105/146], loss=101.9700
	step [106/146], loss=104.6431
	step [107/146], loss=97.8364
	step [108/146], loss=80.0015
	step [109/146], loss=102.6516
	step [110/146], loss=104.1254
	step [111/146], loss=106.4201
	step [112/146], loss=99.4438
	step [113/146], loss=105.5451
	step [114/146], loss=86.0256
	step [115/146], loss=92.7785
	step [116/146], loss=110.0498
	step [117/146], loss=100.1361
	step [118/146], loss=104.7266
	step [119/146], loss=90.1961
	step [120/146], loss=119.4137
	step [121/146], loss=107.6180
	step [122/146], loss=81.7824
	step [123/146], loss=92.4319
	step [124/146], loss=94.4002
	step [125/146], loss=90.2804
	step [126/146], loss=105.1323
	step [127/146], loss=90.8224
	step [128/146], loss=101.1978
	step [129/146], loss=95.0487
	step [130/146], loss=110.8704
	step [131/146], loss=96.0223
	step [132/146], loss=105.5123
	step [133/146], loss=103.8747
	step [134/146], loss=96.4997
	step [135/146], loss=110.4355
	step [136/146], loss=87.1041
	step [137/146], loss=98.3142
	step [138/146], loss=114.0207
	step [139/146], loss=98.9987
	step [140/146], loss=91.9425
	step [141/146], loss=112.6600
	step [142/146], loss=109.6887
	step [143/146], loss=103.4349
	step [144/146], loss=96.2919
	step [145/146], loss=109.5014
	step [146/146], loss=52.1019
	Evaluating
	loss=0.0167, precision=0.4234, recall=0.8891, f1=0.5736
Training epoch 28
	step [1/146], loss=113.7011
	step [2/146], loss=87.8813
	step [3/146], loss=103.4436
	step [4/146], loss=112.6473
	step [5/146], loss=109.8540
	step [6/146], loss=115.2131
	step [7/146], loss=91.9286
	step [8/146], loss=104.4111
	step [9/146], loss=123.7068
	step [10/146], loss=105.4175
	step [11/146], loss=123.7548
	step [12/146], loss=101.9887
	step [13/146], loss=120.5118
	step [14/146], loss=101.2735
	step [15/146], loss=108.9323
	step [16/146], loss=110.2408
	step [17/146], loss=79.4638
	step [18/146], loss=108.3719
	step [19/146], loss=100.2199
	step [20/146], loss=100.9706
	step [21/146], loss=98.1651
	step [22/146], loss=76.0719
	step [23/146], loss=99.7650
	step [24/146], loss=109.7986
	step [25/146], loss=108.5287
	step [26/146], loss=102.6582
	step [27/146], loss=94.9482
	step [28/146], loss=117.1945
	step [29/146], loss=91.2533
	step [30/146], loss=114.2430
	step [31/146], loss=94.1318
	step [32/146], loss=91.8950
	step [33/146], loss=89.5347
	step [34/146], loss=88.6141
	step [35/146], loss=95.2566
	step [36/146], loss=111.8880
	step [37/146], loss=84.0902
	step [38/146], loss=104.7075
	step [39/146], loss=97.9599
	step [40/146], loss=119.3766
	step [41/146], loss=101.6628
	step [42/146], loss=120.4134
	step [43/146], loss=87.4385
	step [44/146], loss=98.6154
	step [45/146], loss=107.9423
	step [46/146], loss=92.0243
	step [47/146], loss=122.8921
	step [48/146], loss=126.3777
	step [49/146], loss=123.5315
	step [50/146], loss=105.9491
	step [51/146], loss=121.1121
	step [52/146], loss=86.1557
	step [53/146], loss=93.5736
	step [54/146], loss=93.0546
	step [55/146], loss=84.6903
	step [56/146], loss=103.7412
	step [57/146], loss=114.2032
	step [58/146], loss=96.5677
	step [59/146], loss=92.5936
	step [60/146], loss=98.2411
	step [61/146], loss=89.4701
	step [62/146], loss=96.6001
	step [63/146], loss=103.4850
	step [64/146], loss=95.8230
	step [65/146], loss=83.7134
	step [66/146], loss=90.2259
	step [67/146], loss=114.0177
	step [68/146], loss=102.7026
	step [69/146], loss=113.0233
	step [70/146], loss=83.9297
	step [71/146], loss=112.6516
	step [72/146], loss=97.0702
	step [73/146], loss=116.0820
	step [74/146], loss=92.8927
	step [75/146], loss=115.9675
	step [76/146], loss=90.0970
	step [77/146], loss=88.1421
	step [78/146], loss=98.0874
	step [79/146], loss=103.7520
	step [80/146], loss=103.2706
	step [81/146], loss=95.9031
	step [82/146], loss=102.9032
	step [83/146], loss=102.5339
	step [84/146], loss=88.5506
	step [85/146], loss=101.2334
	step [86/146], loss=115.8770
	step [87/146], loss=92.7274
	step [88/146], loss=96.1576
	step [89/146], loss=103.9756
	step [90/146], loss=100.6408
	step [91/146], loss=92.8733
	step [92/146], loss=103.8877
	step [93/146], loss=104.9784
	step [94/146], loss=95.1226
	step [95/146], loss=94.4241
	step [96/146], loss=123.7492
	step [97/146], loss=86.2636
	step [98/146], loss=99.7027
	step [99/146], loss=117.3526
	step [100/146], loss=114.6204
	step [101/146], loss=99.5575
	step [102/146], loss=114.5745
	step [103/146], loss=109.5205
	step [104/146], loss=99.9760
	step [105/146], loss=99.7243
	step [106/146], loss=111.9648
	step [107/146], loss=92.5677
	step [108/146], loss=83.8394
	step [109/146], loss=109.6741
	step [110/146], loss=117.2324
	step [111/146], loss=97.1994
	step [112/146], loss=93.8372
	step [113/146], loss=74.9131
	step [114/146], loss=85.6509
	step [115/146], loss=91.6768
	step [116/146], loss=93.8150
	step [117/146], loss=87.1073
	step [118/146], loss=107.6159
	step [119/146], loss=118.1105
	step [120/146], loss=110.2728
	step [121/146], loss=104.3410
	step [122/146], loss=126.7866
	step [123/146], loss=93.4240
	step [124/146], loss=99.6192
	step [125/146], loss=103.8500
	step [126/146], loss=116.5432
	step [127/146], loss=116.4082
	step [128/146], loss=75.7045
	step [129/146], loss=110.7963
	step [130/146], loss=113.2840
	step [131/146], loss=103.1839
	step [132/146], loss=90.5443
	step [133/146], loss=103.0860
	step [134/146], loss=84.8250
	step [135/146], loss=101.8216
	step [136/146], loss=98.9352
	step [137/146], loss=102.1583
	step [138/146], loss=112.3927
	step [139/146], loss=109.1181
	step [140/146], loss=109.3415
	step [141/146], loss=101.2047
	step [142/146], loss=82.5734
	step [143/146], loss=108.2272
	step [144/146], loss=98.0374
	step [145/146], loss=102.1999
	step [146/146], loss=49.7921
	Evaluating
	loss=0.0117, precision=0.5652, recall=0.8670, f1=0.6843
Training epoch 29
	step [1/146], loss=99.4299
	step [2/146], loss=94.9158
	step [3/146], loss=118.4373
	step [4/146], loss=96.1232
	step [5/146], loss=94.6712
	step [6/146], loss=102.2002
	step [7/146], loss=96.9018
	step [8/146], loss=104.1332
	step [9/146], loss=86.0422
	step [10/146], loss=97.9179
	step [11/146], loss=92.3191
	step [12/146], loss=102.3722
	step [13/146], loss=95.2138
	step [14/146], loss=102.2271
	step [15/146], loss=81.2909
	step [16/146], loss=88.7361
	step [17/146], loss=95.4888
	step [18/146], loss=98.3319
	step [19/146], loss=104.7295
	step [20/146], loss=100.0614
	step [21/146], loss=111.2090
	step [22/146], loss=100.8301
	step [23/146], loss=84.1135
	step [24/146], loss=126.1642
	step [25/146], loss=102.8359
	step [26/146], loss=125.3069
	step [27/146], loss=98.0476
	step [28/146], loss=90.8986
	step [29/146], loss=93.3524
	step [30/146], loss=92.9370
	step [31/146], loss=94.6021
	step [32/146], loss=103.5516
	step [33/146], loss=98.8096
	step [34/146], loss=91.2091
	step [35/146], loss=87.3740
	step [36/146], loss=120.1898
	step [37/146], loss=112.2555
	step [38/146], loss=81.3832
	step [39/146], loss=109.3415
	step [40/146], loss=108.6086
	step [41/146], loss=89.5958
	step [42/146], loss=95.3200
	step [43/146], loss=106.8902
	step [44/146], loss=98.5168
	step [45/146], loss=121.7734
	step [46/146], loss=120.0447
	step [47/146], loss=103.5528
	step [48/146], loss=99.2795
	step [49/146], loss=111.2764
	step [50/146], loss=111.1863
	step [51/146], loss=99.2290
	step [52/146], loss=97.9577
	step [53/146], loss=111.9973
	step [54/146], loss=101.1452
	step [55/146], loss=91.3154
	step [56/146], loss=79.2128
	step [57/146], loss=108.5504
	step [58/146], loss=110.3386
	step [59/146], loss=93.3349
	step [60/146], loss=96.5142
	step [61/146], loss=89.4390
	step [62/146], loss=115.5762
	step [63/146], loss=116.2403
	step [64/146], loss=99.6669
	step [65/146], loss=85.5666
	step [66/146], loss=102.8042
	step [67/146], loss=89.2253
	step [68/146], loss=87.9959
	step [69/146], loss=101.3632
	step [70/146], loss=97.5071
	step [71/146], loss=100.2781
	step [72/146], loss=96.7847
	step [73/146], loss=99.6081
	step [74/146], loss=104.7899
	step [75/146], loss=95.6797
	step [76/146], loss=109.2287
	step [77/146], loss=100.8499
	step [78/146], loss=116.2033
	step [79/146], loss=94.3659
	step [80/146], loss=107.5940
	step [81/146], loss=100.1832
	step [82/146], loss=105.0328
	step [83/146], loss=104.5580
	step [84/146], loss=104.6672
	step [85/146], loss=127.8713
	step [86/146], loss=82.2316
	step [87/146], loss=109.4763
	step [88/146], loss=109.8352
	step [89/146], loss=94.5645
	step [90/146], loss=112.6289
	step [91/146], loss=88.8148
	step [92/146], loss=97.0122
	step [93/146], loss=111.7201
	step [94/146], loss=71.5598
	step [95/146], loss=104.0456
	step [96/146], loss=98.8645
	step [97/146], loss=107.5800
	step [98/146], loss=93.9613
	step [99/146], loss=111.3061
	step [100/146], loss=95.1667
	step [101/146], loss=109.4906
	step [102/146], loss=86.4081
	step [103/146], loss=100.5799
	step [104/146], loss=96.1144
	step [105/146], loss=108.1972
	step [106/146], loss=109.1614
	step [107/146], loss=105.1584
	step [108/146], loss=85.7866
	step [109/146], loss=96.7189
	step [110/146], loss=88.1669
	step [111/146], loss=88.3390
	step [112/146], loss=101.5271
	step [113/146], loss=96.5566
	step [114/146], loss=114.2141
	step [115/146], loss=91.2284
	step [116/146], loss=108.5549
	step [117/146], loss=99.9427
	step [118/146], loss=93.1643
	step [119/146], loss=99.9783
	step [120/146], loss=118.2917
	step [121/146], loss=93.8427
	step [122/146], loss=85.8167
	step [123/146], loss=104.7135
	step [124/146], loss=101.1934
	step [125/146], loss=94.6919
	step [126/146], loss=108.3179
	step [127/146], loss=100.1210
	step [128/146], loss=93.0638
	step [129/146], loss=100.2060
	step [130/146], loss=107.1228
	step [131/146], loss=110.2691
	step [132/146], loss=101.1394
	step [133/146], loss=101.3298
	step [134/146], loss=93.3196
	step [135/146], loss=110.5811
	step [136/146], loss=91.8773
	step [137/146], loss=104.5549
	step [138/146], loss=101.9630
	step [139/146], loss=118.0151
	step [140/146], loss=89.7484
	step [141/146], loss=108.3068
	step [142/146], loss=104.4135
	step [143/146], loss=94.0319
	step [144/146], loss=97.8613
	step [145/146], loss=94.3907
	step [146/146], loss=44.5682
	Evaluating
	loss=0.0154, precision=0.4256, recall=0.9000, f1=0.5780
Training epoch 30
	step [1/146], loss=88.2187
	step [2/146], loss=139.0737
	step [3/146], loss=106.1503
	step [4/146], loss=124.2777
	step [5/146], loss=99.4158
	step [6/146], loss=84.8886
	step [7/146], loss=96.0780
	step [8/146], loss=104.5997
	step [9/146], loss=97.3866
	step [10/146], loss=112.5112
	step [11/146], loss=110.1594
	step [12/146], loss=98.3934
	step [13/146], loss=102.5460
	step [14/146], loss=75.7907
	step [15/146], loss=108.0589
	step [16/146], loss=101.4922
	step [17/146], loss=81.2664
	step [18/146], loss=101.5293
	step [19/146], loss=97.3323
	step [20/146], loss=82.6906
	step [21/146], loss=104.2531
	step [22/146], loss=97.5432
	step [23/146], loss=99.4135
	step [24/146], loss=84.9435
	step [25/146], loss=99.2327
	step [26/146], loss=97.9010
	step [27/146], loss=95.8819
	step [28/146], loss=106.3168
	step [29/146], loss=82.6611
	step [30/146], loss=88.4525
	step [31/146], loss=94.3686
	step [32/146], loss=94.6754
	step [33/146], loss=107.2548
	step [34/146], loss=87.3045
	step [35/146], loss=85.5880
	step [36/146], loss=98.9108
	step [37/146], loss=99.7475
	step [38/146], loss=106.3524
	step [39/146], loss=104.8104
	step [40/146], loss=107.1082
	step [41/146], loss=91.7817
	step [42/146], loss=90.4242
	step [43/146], loss=113.4848
	step [44/146], loss=105.6124
	step [45/146], loss=102.2972
	step [46/146], loss=89.8225
	step [47/146], loss=89.8859
	step [48/146], loss=90.2801
	step [49/146], loss=98.5344
	step [50/146], loss=114.6709
	step [51/146], loss=108.8448
	step [52/146], loss=100.1913
	step [53/146], loss=103.3614
	step [54/146], loss=83.3093
	step [55/146], loss=96.5722
	step [56/146], loss=97.6546
	step [57/146], loss=86.9753
	step [58/146], loss=90.8588
	step [59/146], loss=105.1100
	step [60/146], loss=108.9343
	step [61/146], loss=105.0607
	step [62/146], loss=110.5738
	step [63/146], loss=102.5780
	step [64/146], loss=103.8157
	step [65/146], loss=105.5436
	step [66/146], loss=94.1407
	step [67/146], loss=106.4749
	step [68/146], loss=106.6631
	step [69/146], loss=104.2525
	step [70/146], loss=99.7357
	step [71/146], loss=87.1875
	step [72/146], loss=102.8569
	step [73/146], loss=85.8555
	step [74/146], loss=111.3630
	step [75/146], loss=126.5769
	step [76/146], loss=118.2456
	step [77/146], loss=103.6416
	step [78/146], loss=96.1641
	step [79/146], loss=110.3343
	step [80/146], loss=121.9860
	step [81/146], loss=85.6932
	step [82/146], loss=95.1076
	step [83/146], loss=95.1973
	step [84/146], loss=98.7213
	step [85/146], loss=98.0007
	step [86/146], loss=100.3788
	step [87/146], loss=102.0589
	step [88/146], loss=103.7974
	step [89/146], loss=97.8955
	step [90/146], loss=120.7189
	step [91/146], loss=95.0955
	step [92/146], loss=105.3376
	step [93/146], loss=106.7504
	step [94/146], loss=84.2421
	step [95/146], loss=116.6325
	step [96/146], loss=93.8750
	step [97/146], loss=89.1874
	step [98/146], loss=115.7963
	step [99/146], loss=94.0896
	step [100/146], loss=101.1449
	step [101/146], loss=81.5942
	step [102/146], loss=104.4386
	step [103/146], loss=122.9044
	step [104/146], loss=89.4880
	step [105/146], loss=105.4880
	step [106/146], loss=82.8983
	step [107/146], loss=86.5118
	step [108/146], loss=101.9734
	step [109/146], loss=114.4404
	step [110/146], loss=97.3552
	step [111/146], loss=103.9392
	step [112/146], loss=106.7565
	step [113/146], loss=91.4012
	step [114/146], loss=83.8455
	step [115/146], loss=84.3466
	step [116/146], loss=90.5005
	step [117/146], loss=95.8037
	step [118/146], loss=93.0038
	step [119/146], loss=87.4559
	step [120/146], loss=95.0424
	step [121/146], loss=110.6076
	step [122/146], loss=99.9290
	step [123/146], loss=94.9207
	step [124/146], loss=94.0049
	step [125/146], loss=109.9093
	step [126/146], loss=101.0362
	step [127/146], loss=97.8632
	step [128/146], loss=99.7853
	step [129/146], loss=107.2820
	step [130/146], loss=99.4422
	step [131/146], loss=96.8281
	step [132/146], loss=96.2749
	step [133/146], loss=100.5423
	step [134/146], loss=91.5997
	step [135/146], loss=81.4377
	step [136/146], loss=90.1827
	step [137/146], loss=108.7214
	step [138/146], loss=123.9123
	step [139/146], loss=113.0351
	step [140/146], loss=100.7453
	step [141/146], loss=92.2746
	step [142/146], loss=106.4123
	step [143/146], loss=95.2338
	step [144/146], loss=102.4789
	step [145/146], loss=105.8258
	step [146/146], loss=44.5550
	Evaluating
	loss=0.0142, precision=0.4270, recall=0.8828, f1=0.5756
Training finished
best_f1: 0.6908599017622081
directing: Y rim_enhanced: True test_id 3
removed wrong scan: weights_Y_335_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_383_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_262_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_311_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_260_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_225_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_320_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_192_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_319_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_226_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_346_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_271_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_277_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_290_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_269_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_379_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_375_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_378_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_366_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_309_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_200_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_327_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_193_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_174_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_253_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_220_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_265_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_360_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_303_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_232_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_243_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_240_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_237_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_256_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_254_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_254_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_231_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_359_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_172_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_340_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_263_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_388_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_307_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_278_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_196_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_272_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_314_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_350_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_361_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_194_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_323_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_201_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_299_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_259_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_276_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_167_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_185_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_215_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_268_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_264_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_384_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_261_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_280_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_287_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_385_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_216_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_349_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_348_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_371_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_357_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_234_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_288_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_324_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_229_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_184_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_148_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_168_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_370_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_286_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_302_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_265_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_153_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_274_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_255_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_217_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_355_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_294_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_159_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_338_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_236_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_152_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_292_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_222_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_376_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_183_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_258_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_282_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_278_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_284_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_300_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_163_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_197_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_203_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_243_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_224_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_238_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_374_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_218_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_259_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_282_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_387_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_353_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_386_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_296_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_341_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_206_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_220_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_239_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_231_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_202_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_358_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_246_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_315_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_275_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_169_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_316_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_221_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_188_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_208_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_273_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_251_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_224_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_333_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_263_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_173_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_243_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_241_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_326_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_230_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_236_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_345_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_206_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_258_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_187_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_87_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_154_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_253_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_337_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_232_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_147_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_377_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_362_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_233_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_250_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_297_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_354_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_262_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_225_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_365_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_356_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_242_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_245_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_230_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_318_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_181_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_305_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_255_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_298_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_301_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_293_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_229_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_332_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_257_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_295_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_239_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_260_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_228_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_317_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_364_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_262_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_162_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_310_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_160_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_237_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_254_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_257_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_291_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_279_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_215_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_372_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_218_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_210_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_158_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_156_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_339_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_190_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_373_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_205_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_329_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_380_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_271_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_235_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_244_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_312_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_186_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_308_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_347_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_241_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_270_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_261_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_222_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_189_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_289_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_146_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_252_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_344_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_331_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_313_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_328_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_265_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_283_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_248_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_381_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_211_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_214_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_176_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_221_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_150_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_343_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_249_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_322_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_351_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_234_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_195_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_330_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_334_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_306_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_213_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_325_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_342_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_247_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_352_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_225_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_226_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_256_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_242_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_368_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_382_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_157_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_238_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_267_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_367_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_321_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_233_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_281_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_161_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_336_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_170_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_304_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_44_xwqg-B00034_2020-04-03.npy
# all image files: 20333 # all weight files in weight_dir: 15917 # image files with weight 15875
removed wrong scan: weights_Y_335_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_383_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_262_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_311_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_260_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_225_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_320_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_192_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_319_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_226_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_346_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_271_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_277_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_290_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_269_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_379_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_375_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_378_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_366_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_309_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_200_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_327_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_193_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_174_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_253_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_220_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_265_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_360_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_303_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_232_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_243_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_240_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_237_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_256_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_254_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_254_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_231_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_359_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_172_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_340_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_263_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_388_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_307_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_278_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_196_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_272_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_314_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_350_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_361_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_194_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_323_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_201_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_299_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_259_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_276_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_167_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_185_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_215_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_268_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_264_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_384_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_261_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_280_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_287_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_385_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_216_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_349_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_348_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_371_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_357_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_234_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_288_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_324_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_229_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_184_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_148_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_168_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_370_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_286_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_302_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_265_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_153_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_274_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_255_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_217_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_355_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_294_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_159_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_338_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_236_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_152_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_292_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_222_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_376_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_183_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_258_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_282_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_278_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_284_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_300_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_163_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_197_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_203_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_243_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_224_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_238_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_374_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_218_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_259_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_282_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_387_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_353_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_386_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_296_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_341_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_206_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_220_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_239_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_231_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_202_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_358_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_246_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_315_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_275_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_169_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_316_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_221_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_188_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_208_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_273_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_251_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_224_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_333_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_263_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_173_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_243_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_241_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_326_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_230_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_236_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_345_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_206_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_258_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_187_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_87_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_154_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_253_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_337_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_232_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_147_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_377_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_362_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_233_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_250_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_297_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_354_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_262_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_225_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_365_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_356_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_242_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_245_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_230_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_318_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_181_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_305_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_255_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_298_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_301_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_293_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_229_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_332_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_257_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_295_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_239_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_260_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_228_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_317_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_364_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_262_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_162_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_310_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_160_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_237_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_254_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_257_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_291_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_279_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_215_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_372_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_218_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_210_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_158_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_156_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_339_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_190_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_373_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_205_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_329_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_380_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_271_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_235_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_244_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_312_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_186_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_308_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_347_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_241_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_270_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_261_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_222_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_189_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_289_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_146_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_252_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_344_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_331_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_313_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_328_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_265_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_283_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_248_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_381_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_211_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_214_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_176_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_221_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_150_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_343_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_249_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_322_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_351_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_234_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_195_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_330_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_334_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_306_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_213_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_325_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_342_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_247_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_352_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_225_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_226_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_256_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_242_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_368_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_382_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_157_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_238_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_267_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_367_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_321_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_233_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_281_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_161_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_336_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_170_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_304_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_44_xwqg-B00034_2020-04-03.npy
# all image files: 20333 # all weight files in weight_dir: 4155 # image files with weight 4155
train: /ibex/scratch/projects/c2052/air_tube_seg/training_samples_enhanced_one/Y 15875
Using 4 GPUs
Going to train epochs [1-30]
Training epoch 1
	step [1/249], loss=500.9880
	step [2/249], loss=431.1078
	step [3/249], loss=293.2653
	step [4/249], loss=325.5945
	step [5/249], loss=285.4794
	step [6/249], loss=292.2337
	step [7/249], loss=334.0694
	step [8/249], loss=295.0095
	step [9/249], loss=314.7724
	step [10/249], loss=287.3242
	step [11/249], loss=275.8899
	step [12/249], loss=260.9296
	step [13/249], loss=284.1183
	step [14/249], loss=296.3521
	step [15/249], loss=263.8409
	step [16/249], loss=301.1236
	step [17/249], loss=272.2505
	step [18/249], loss=279.6194
	step [19/249], loss=261.6883
	step [20/249], loss=272.7362
	step [21/249], loss=276.9388
	step [22/249], loss=242.2449
	step [23/249], loss=254.8134
	step [24/249], loss=269.0864
	step [25/249], loss=278.6799
	step [26/249], loss=259.0231
	step [27/249], loss=261.7746
	step [28/249], loss=248.1269
	step [29/249], loss=241.5142
	step [30/249], loss=238.4699
	step [31/249], loss=227.1605
	step [32/249], loss=223.3363
	step [33/249], loss=264.7986
	step [34/249], loss=257.2082
	step [35/249], loss=253.2658
	step [36/249], loss=236.0948
	step [37/249], loss=228.7624
	step [38/249], loss=241.3326
	step [39/249], loss=230.9552
	step [40/249], loss=249.8451
	step [41/249], loss=233.9751
	step [42/249], loss=214.5687
	step [43/249], loss=224.7665
	step [44/249], loss=218.8749
	step [45/249], loss=211.7804
	step [46/249], loss=231.5053
	step [47/249], loss=231.5939
	step [48/249], loss=234.5596
	step [49/249], loss=212.3783
	step [50/249], loss=250.7128
	step [51/249], loss=233.6511
	step [52/249], loss=219.1171
	step [53/249], loss=227.2976
	step [54/249], loss=193.5313
	step [55/249], loss=214.5368
	step [56/249], loss=236.4354
	step [57/249], loss=226.2815
	step [58/249], loss=228.0449
	step [59/249], loss=225.7263
	step [60/249], loss=216.9782
	step [61/249], loss=221.4943
	step [62/249], loss=193.2978
	step [63/249], loss=232.3619
	step [64/249], loss=219.8345
	step [65/249], loss=220.8875
	step [66/249], loss=224.1599
	step [67/249], loss=246.9129
	step [68/249], loss=224.3023
	step [69/249], loss=204.6321
	step [70/249], loss=212.8115
	step [71/249], loss=204.2947
	step [72/249], loss=192.6510
	step [73/249], loss=232.3363
	step [74/249], loss=233.5256
	step [75/249], loss=236.4605
	step [76/249], loss=198.0502
	step [77/249], loss=217.5308
	step [78/249], loss=191.9190
	step [79/249], loss=221.7626
	step [80/249], loss=201.0478
	step [81/249], loss=212.3049
	step [82/249], loss=209.6255
	step [83/249], loss=213.3163
	step [84/249], loss=219.5504
	step [85/249], loss=204.0555
	step [86/249], loss=184.4375
	step [87/249], loss=225.6472
	step [88/249], loss=209.7782
	step [89/249], loss=207.3808
	step [90/249], loss=179.2351
	step [91/249], loss=201.6575
	step [92/249], loss=204.6931
	step [93/249], loss=219.5423
	step [94/249], loss=186.6409
	step [95/249], loss=206.2717
	step [96/249], loss=209.2890
	step [97/249], loss=202.0961
	step [98/249], loss=215.8038
	step [99/249], loss=194.9506
	step [100/249], loss=222.9216
	step [101/249], loss=241.4998
	step [102/249], loss=207.3627
	step [103/249], loss=202.3814
	step [104/249], loss=208.6421
	step [105/249], loss=205.7593
	step [106/249], loss=193.7618
	step [107/249], loss=193.0823
	step [108/249], loss=174.0065
	step [109/249], loss=194.2818
	step [110/249], loss=186.3322
	step [111/249], loss=191.8090
	step [112/249], loss=211.9019
	step [113/249], loss=235.5845
	step [114/249], loss=198.6498
	step [115/249], loss=223.1978
	step [116/249], loss=209.4709
	step [117/249], loss=217.6391
	step [118/249], loss=208.7633
	step [119/249], loss=175.4249
	step [120/249], loss=208.6086
	step [121/249], loss=226.7604
	step [122/249], loss=177.4430
	step [123/249], loss=201.9140
	step [124/249], loss=207.1698
	step [125/249], loss=209.7727
	step [126/249], loss=224.9689
	step [127/249], loss=208.5936
	step [128/249], loss=202.0457
	step [129/249], loss=210.0746
	step [130/249], loss=208.0920
	step [131/249], loss=176.8168
	step [132/249], loss=192.7422
	step [133/249], loss=190.8372
	step [134/249], loss=210.5176
	step [135/249], loss=190.0947
	step [136/249], loss=197.5194
	step [137/249], loss=195.5336
	step [138/249], loss=205.3143
	step [139/249], loss=209.0406
	step [140/249], loss=227.0759
	step [141/249], loss=188.1213
	step [142/249], loss=200.1146
	step [143/249], loss=185.2962
	step [144/249], loss=208.5735
	step [145/249], loss=183.3022
	step [146/249], loss=220.2014
	step [147/249], loss=188.8880
	step [148/249], loss=169.4266
	step [149/249], loss=217.2148
	step [150/249], loss=190.0358
	step [151/249], loss=172.5661
	step [152/249], loss=189.4195
	step [153/249], loss=191.2595
	step [154/249], loss=171.6055
	step [155/249], loss=192.3814
	step [156/249], loss=186.6348
	step [157/249], loss=184.6131
	step [158/249], loss=184.0556
	step [159/249], loss=207.8277
	step [160/249], loss=202.0765
	step [161/249], loss=177.6566
	step [162/249], loss=180.6489
	step [163/249], loss=221.9376
	step [164/249], loss=182.3670
	step [165/249], loss=189.6547
	step [166/249], loss=203.6133
	step [167/249], loss=190.0356
	step [168/249], loss=193.8080
	step [169/249], loss=189.2555
	step [170/249], loss=186.4751
	step [171/249], loss=175.6978
	step [172/249], loss=183.0248
	step [173/249], loss=218.3571
	step [174/249], loss=203.9118
	step [175/249], loss=203.8274
	step [176/249], loss=167.2922
	step [177/249], loss=199.5559
	step [178/249], loss=180.2231
	step [179/249], loss=181.8206
	step [180/249], loss=192.6801
	step [181/249], loss=199.7232
	step [182/249], loss=198.3271
	step [183/249], loss=185.5970
	step [184/249], loss=162.3184
	step [185/249], loss=175.9296
	step [186/249], loss=173.0118
	step [187/249], loss=170.4990
	step [188/249], loss=204.1121
	step [189/249], loss=187.3263
	step [190/249], loss=196.4186
	step [191/249], loss=171.1105
	step [192/249], loss=175.3138
	step [193/249], loss=165.0712
	step [194/249], loss=182.5376
	step [195/249], loss=219.8738
	step [196/249], loss=192.2364
	step [197/249], loss=190.6026
	step [198/249], loss=208.6814
	step [199/249], loss=176.6891
	step [200/249], loss=181.3802
	step [201/249], loss=195.9544
	step [202/249], loss=182.8160
	step [203/249], loss=168.5533
	step [204/249], loss=192.6795
	step [205/249], loss=190.5305
	step [206/249], loss=195.1924
	step [207/249], loss=183.6351
	step [208/249], loss=182.6780
	step [209/249], loss=170.9995
	step [210/249], loss=176.0922
	step [211/249], loss=196.4046
	step [212/249], loss=180.6316
	step [213/249], loss=206.0742
	step [214/249], loss=193.2023
	step [215/249], loss=177.4222
	step [216/249], loss=210.3992
	step [217/249], loss=182.6433
	step [218/249], loss=219.7766
	step [219/249], loss=188.9853
	step [220/249], loss=217.2562
	step [221/249], loss=179.8613
	step [222/249], loss=174.1154
	step [223/249], loss=194.9549
	step [224/249], loss=188.9556
	step [225/249], loss=183.3450
	step [226/249], loss=171.4430
	step [227/249], loss=182.6447
	step [228/249], loss=174.6417
	step [229/249], loss=179.9082
	step [230/249], loss=179.6506
	step [231/249], loss=175.3794
	step [232/249], loss=157.2663
	step [233/249], loss=171.4315
	step [234/249], loss=191.4894
	step [235/249], loss=172.3460
	step [236/249], loss=185.5653
	step [237/249], loss=186.0986
	step [238/249], loss=188.6591
	step [239/249], loss=189.2615
	step [240/249], loss=180.5417
	step [241/249], loss=157.8100
	step [242/249], loss=165.7493
	step [243/249], loss=179.5547
	step [244/249], loss=165.1765
	step [245/249], loss=166.2889
	step [246/249], loss=169.9016
	step [247/249], loss=172.4733
	step [248/249], loss=179.1302
	step [249/249], loss=7.3169
	Evaluating
	loss=0.3213, precision=0.3060, recall=0.9212, f1=0.4594
saving model as: 3_saved_model.pth
Training epoch 2
	step [1/249], loss=169.6652
	step [2/249], loss=173.0452
	step [3/249], loss=181.3138
	step [4/249], loss=166.8280
	step [5/249], loss=190.9410
	step [6/249], loss=165.8413
	step [7/249], loss=175.1506
	step [8/249], loss=163.4541
	step [9/249], loss=154.5630
	step [10/249], loss=187.7941
	step [11/249], loss=190.5662
	step [12/249], loss=169.2737
	step [13/249], loss=176.2448
	step [14/249], loss=188.1162
	step [15/249], loss=175.7812
	step [16/249], loss=202.6311
	step [17/249], loss=183.9670
	step [18/249], loss=186.6693
	step [19/249], loss=183.7160
	step [20/249], loss=164.1907
	step [21/249], loss=159.8212
	step [22/249], loss=177.7187
	step [23/249], loss=174.2350
	step [24/249], loss=163.0788
	step [25/249], loss=176.0228
	step [26/249], loss=174.2183
	step [27/249], loss=162.8950
	step [28/249], loss=180.0988
	step [29/249], loss=162.4890
	step [30/249], loss=169.9904
	step [31/249], loss=183.2116
	step [32/249], loss=176.2738
	step [33/249], loss=172.7350
	step [34/249], loss=187.0615
	step [35/249], loss=198.9655
	step [36/249], loss=155.3811
	step [37/249], loss=166.9824
	step [38/249], loss=187.2906
	step [39/249], loss=160.3069
	step [40/249], loss=181.4932
	step [41/249], loss=175.7244
	step [42/249], loss=166.5361
	step [43/249], loss=157.5855
	step [44/249], loss=179.0214
	step [45/249], loss=174.0805
	step [46/249], loss=186.0221
	step [47/249], loss=166.1130
	step [48/249], loss=174.1388
	step [49/249], loss=174.9955
	step [50/249], loss=196.6667
	step [51/249], loss=171.9659
	step [52/249], loss=156.9342
	step [53/249], loss=157.7755
	step [54/249], loss=160.8116
	step [55/249], loss=167.9442
	step [56/249], loss=167.1554
	step [57/249], loss=171.4694
	step [58/249], loss=169.4979
	step [59/249], loss=174.6653
	step [60/249], loss=153.3756
	step [61/249], loss=149.9736
	step [62/249], loss=176.5741
	step [63/249], loss=180.5892
	step [64/249], loss=164.1505
	step [65/249], loss=189.5127
	step [66/249], loss=147.1192
	step [67/249], loss=184.4461
	step [68/249], loss=176.5245
	step [69/249], loss=176.6116
	step [70/249], loss=161.5992
	step [71/249], loss=169.3152
	step [72/249], loss=173.3857
	step [73/249], loss=151.0868
	step [74/249], loss=184.4189
	step [75/249], loss=194.6045
	step [76/249], loss=158.1327
	step [77/249], loss=148.6183
	step [78/249], loss=165.0290
	step [79/249], loss=163.2678
	step [80/249], loss=161.4884
	step [81/249], loss=167.6659
	step [82/249], loss=165.0387
	step [83/249], loss=144.9068
	step [84/249], loss=147.2398
	step [85/249], loss=170.8117
	step [86/249], loss=175.8248
	step [87/249], loss=174.4752
	step [88/249], loss=165.2914
	step [89/249], loss=173.4294
	step [90/249], loss=163.7353
	step [91/249], loss=150.2711
	step [92/249], loss=172.5540
	step [93/249], loss=184.1043
	step [94/249], loss=155.6360
	step [95/249], loss=172.2963
	step [96/249], loss=168.3478
	step [97/249], loss=167.6749
	step [98/249], loss=172.1865
	step [99/249], loss=160.3421
	step [100/249], loss=160.0635
	step [101/249], loss=203.3307
	step [102/249], loss=160.7877
	step [103/249], loss=169.5415
	step [104/249], loss=193.0567
	step [105/249], loss=157.9051
	step [106/249], loss=172.3492
	step [107/249], loss=175.3710
	step [108/249], loss=177.2512
	step [109/249], loss=173.2377
	step [110/249], loss=158.6157
	step [111/249], loss=152.3430
	step [112/249], loss=164.7621
	step [113/249], loss=164.8366
	step [114/249], loss=157.0798
	step [115/249], loss=176.4251
	step [116/249], loss=184.8131
	step [117/249], loss=162.7075
	step [118/249], loss=150.9156
	step [119/249], loss=188.4254
	step [120/249], loss=149.3353
	step [121/249], loss=153.5020
	step [122/249], loss=151.1919
	step [123/249], loss=154.9917
	step [124/249], loss=146.9313
	step [125/249], loss=165.6942
	step [126/249], loss=163.4863
	step [127/249], loss=171.4410
	step [128/249], loss=155.3154
	step [129/249], loss=148.1058
	step [130/249], loss=150.0501
	step [131/249], loss=163.7836
	step [132/249], loss=163.6826
	step [133/249], loss=169.7693
	step [134/249], loss=155.0485
	step [135/249], loss=176.6237
	step [136/249], loss=159.3891
	step [137/249], loss=161.2681
	step [138/249], loss=167.2039
	step [139/249], loss=128.8847
	step [140/249], loss=144.4033
	step [141/249], loss=156.6293
	step [142/249], loss=150.6917
	step [143/249], loss=147.0509
	step [144/249], loss=138.5385
	step [145/249], loss=162.6129
	step [146/249], loss=150.5627
	step [147/249], loss=148.4980
	step [148/249], loss=161.3550
	step [149/249], loss=185.2670
	step [150/249], loss=176.1188
	step [151/249], loss=155.3862
	step [152/249], loss=167.2620
	step [153/249], loss=148.5488
	step [154/249], loss=173.3193
	step [155/249], loss=139.4078
	step [156/249], loss=163.5578
	step [157/249], loss=160.7518
	step [158/249], loss=168.8711
	step [159/249], loss=160.8965
	step [160/249], loss=170.5004
	step [161/249], loss=143.0315
	step [162/249], loss=169.6074
	step [163/249], loss=169.8483
	step [164/249], loss=176.8397
	step [165/249], loss=149.8978
	step [166/249], loss=157.6146
	step [167/249], loss=149.2637
	step [168/249], loss=160.4358
	step [169/249], loss=152.7744
	step [170/249], loss=155.3539
	step [171/249], loss=174.7878
	step [172/249], loss=179.9987
	step [173/249], loss=147.5965
	step [174/249], loss=170.6422
	step [175/249], loss=174.4683
	step [176/249], loss=158.4460
	step [177/249], loss=159.2462
	step [178/249], loss=167.9358
	step [179/249], loss=158.5909
	step [180/249], loss=187.0529
	step [181/249], loss=193.6132
	step [182/249], loss=182.0187
	step [183/249], loss=152.4447
	step [184/249], loss=171.9276
	step [185/249], loss=152.0212
	step [186/249], loss=143.4284
	step [187/249], loss=185.8016
	step [188/249], loss=168.6902
	step [189/249], loss=164.8752
	step [190/249], loss=155.3001
	step [191/249], loss=147.5039
	step [192/249], loss=187.2988
	step [193/249], loss=157.4629
	step [194/249], loss=161.6576
	step [195/249], loss=172.4850
	step [196/249], loss=169.8009
	step [197/249], loss=137.5103
	step [198/249], loss=154.0145
	step [199/249], loss=144.6628
	step [200/249], loss=151.7156
	step [201/249], loss=157.5747
	step [202/249], loss=151.8479
	step [203/249], loss=149.4567
	step [204/249], loss=157.7479
	step [205/249], loss=151.5513
	step [206/249], loss=164.9819
	step [207/249], loss=137.0019
	step [208/249], loss=178.8171
	step [209/249], loss=156.7276
	step [210/249], loss=168.6205
	step [211/249], loss=159.2438
	step [212/249], loss=164.7930
	step [213/249], loss=144.1778
	step [214/249], loss=124.2702
	step [215/249], loss=147.7103
	step [216/249], loss=150.8864
	step [217/249], loss=141.7048
	step [218/249], loss=153.9449
	step [219/249], loss=171.8979
	step [220/249], loss=175.4838
	step [221/249], loss=148.2947
	step [222/249], loss=142.8551
	step [223/249], loss=175.6512
	step [224/249], loss=157.4250
	step [225/249], loss=162.9586
	step [226/249], loss=135.1928
	step [227/249], loss=153.4330
	step [228/249], loss=168.9831
	step [229/249], loss=151.8580
	step [230/249], loss=130.2954
	step [231/249], loss=147.3199
	step [232/249], loss=165.8596
	step [233/249], loss=174.7461
	step [234/249], loss=153.9426
	step [235/249], loss=161.8227
	step [236/249], loss=166.1578
	step [237/249], loss=140.1205
	step [238/249], loss=173.7572
	step [239/249], loss=155.1548
	step [240/249], loss=147.8421
	step [241/249], loss=133.5262
	step [242/249], loss=177.5478
	step [243/249], loss=185.0096
	step [244/249], loss=151.9287
	step [245/249], loss=161.1843
	step [246/249], loss=144.1638
	step [247/249], loss=148.5017
	step [248/249], loss=133.1968
	step [249/249], loss=6.6926
	Evaluating
	loss=0.1992, precision=0.5803, recall=0.8421, f1=0.6871
saving model as: 3_saved_model.pth
Training epoch 3
	step [1/249], loss=132.4525
	step [2/249], loss=144.3329
	step [3/249], loss=153.9449
	step [4/249], loss=165.0412
	step [5/249], loss=149.6175
	step [6/249], loss=130.4749
	step [7/249], loss=171.6768
	step [8/249], loss=148.2786
	step [9/249], loss=180.9707
	step [10/249], loss=149.5833
	step [11/249], loss=129.9683
	step [12/249], loss=160.2545
	step [13/249], loss=133.7314
	step [14/249], loss=159.3273
	step [15/249], loss=157.9535
	step [16/249], loss=145.1640
	step [17/249], loss=152.1325
	step [18/249], loss=175.5015
	step [19/249], loss=148.9359
	step [20/249], loss=129.9120
	step [21/249], loss=161.6458
	step [22/249], loss=119.0037
	step [23/249], loss=145.9396
	step [24/249], loss=141.7244
	step [25/249], loss=158.1450
	step [26/249], loss=158.1836
	step [27/249], loss=169.5752
	step [28/249], loss=136.9833
	step [29/249], loss=147.7500
	step [30/249], loss=131.1923
	step [31/249], loss=134.5405
	step [32/249], loss=130.8713
	step [33/249], loss=128.4075
	step [34/249], loss=157.7025
	step [35/249], loss=148.0665
	step [36/249], loss=134.0758
	step [37/249], loss=162.5043
	step [38/249], loss=137.8123
	step [39/249], loss=163.4865
	step [40/249], loss=131.9194
	step [41/249], loss=152.0576
	step [42/249], loss=134.6752
	step [43/249], loss=152.9143
	step [44/249], loss=164.8537
	step [45/249], loss=126.4433
	step [46/249], loss=146.4332
	step [47/249], loss=151.7378
	step [48/249], loss=149.4366
	step [49/249], loss=171.6068
	step [50/249], loss=144.0311
	step [51/249], loss=158.7560
	step [52/249], loss=112.9730
	step [53/249], loss=144.8503
	step [54/249], loss=142.9225
	step [55/249], loss=134.9293
	step [56/249], loss=164.0963
	step [57/249], loss=145.6357
	step [58/249], loss=148.9155
	step [59/249], loss=134.0655
	step [60/249], loss=158.9930
	step [61/249], loss=141.5665
	step [62/249], loss=144.3014
	step [63/249], loss=131.8476
	step [64/249], loss=136.1818
	step [65/249], loss=158.9049
	step [66/249], loss=157.9410
	step [67/249], loss=154.1784
	step [68/249], loss=148.5952
	step [69/249], loss=134.8358
	step [70/249], loss=147.6094
	step [71/249], loss=154.0150
	step [72/249], loss=146.6657
	step [73/249], loss=141.4858
	step [74/249], loss=143.7793
	step [75/249], loss=161.9493
	step [76/249], loss=154.0137
	step [77/249], loss=161.1438
	step [78/249], loss=147.4430
	step [79/249], loss=146.6011
	step [80/249], loss=160.7731
	step [81/249], loss=151.3657
	step [82/249], loss=161.8394
	step [83/249], loss=143.9437
	step [84/249], loss=121.5248
	step [85/249], loss=156.6490
	step [86/249], loss=158.7669
	step [87/249], loss=172.7639
	step [88/249], loss=156.1595
	step [89/249], loss=130.4900
	step [90/249], loss=146.2176
	step [91/249], loss=165.8835
	step [92/249], loss=139.3346
	step [93/249], loss=143.9194
	step [94/249], loss=170.5142
	step [95/249], loss=151.5512
	step [96/249], loss=126.9665
	step [97/249], loss=153.9558
	step [98/249], loss=144.6309
	step [99/249], loss=140.2696
	step [100/249], loss=124.8615
	step [101/249], loss=130.4195
	step [102/249], loss=122.1418
	step [103/249], loss=133.0492
	step [104/249], loss=146.6368
	step [105/249], loss=140.6402
	step [106/249], loss=118.8965
	step [107/249], loss=146.9048
	step [108/249], loss=148.4021
	step [109/249], loss=148.1942
	step [110/249], loss=123.8632
	step [111/249], loss=150.2255
	step [112/249], loss=145.4271
	step [113/249], loss=152.9799
	step [114/249], loss=141.8629
	step [115/249], loss=146.7452
	step [116/249], loss=152.1841
	step [117/249], loss=154.6085
	step [118/249], loss=154.4240
	step [119/249], loss=143.5172
	step [120/249], loss=152.1474
	step [121/249], loss=149.5193
	step [122/249], loss=149.2608
	step [123/249], loss=157.0701
	step [124/249], loss=171.3048
	step [125/249], loss=152.7412
	step [126/249], loss=131.9182
	step [127/249], loss=133.5269
	step [128/249], loss=123.1951
	step [129/249], loss=145.0615
	step [130/249], loss=145.3944
	step [131/249], loss=180.1497
	step [132/249], loss=124.2029
	step [133/249], loss=131.8464
	step [134/249], loss=156.2892
	step [135/249], loss=146.6023
	step [136/249], loss=162.3614
	step [137/249], loss=129.2914
	step [138/249], loss=130.7027
	step [139/249], loss=144.2045
	step [140/249], loss=129.6135
	step [141/249], loss=150.9140
	step [142/249], loss=143.8650
	step [143/249], loss=126.5477
	step [144/249], loss=128.1790
	step [145/249], loss=127.4053
	step [146/249], loss=154.9515
	step [147/249], loss=123.2423
	step [148/249], loss=124.7016
	step [149/249], loss=171.7381
	step [150/249], loss=121.7776
	step [151/249], loss=145.1908
	step [152/249], loss=121.4051
	step [153/249], loss=121.7971
	step [154/249], loss=135.0453
	step [155/249], loss=177.4462
	step [156/249], loss=165.9463
	step [157/249], loss=149.1004
	step [158/249], loss=131.1829
	step [159/249], loss=148.9017
	step [160/249], loss=140.1116
	step [161/249], loss=135.5578
	step [162/249], loss=153.8189
	step [163/249], loss=146.1821
	step [164/249], loss=152.1838
	step [165/249], loss=145.4164
	step [166/249], loss=148.6842
	step [167/249], loss=154.3259
	step [168/249], loss=141.3775
	step [169/249], loss=175.3471
	step [170/249], loss=156.4788
	step [171/249], loss=159.4405
	step [172/249], loss=150.2230
	step [173/249], loss=130.7742
	step [174/249], loss=141.3982
	step [175/249], loss=135.5589
	step [176/249], loss=137.2288
	step [177/249], loss=130.4696
	step [178/249], loss=131.5073
	step [179/249], loss=127.2289
	step [180/249], loss=142.1218
	step [181/249], loss=143.6320
	step [182/249], loss=120.3944
	step [183/249], loss=141.7212
	step [184/249], loss=138.3283
	step [185/249], loss=134.2571
	step [186/249], loss=152.1221
	step [187/249], loss=136.5189
	step [188/249], loss=139.1543
	step [189/249], loss=137.8400
	step [190/249], loss=149.1319
	step [191/249], loss=116.9994
	step [192/249], loss=126.3473
	step [193/249], loss=152.9199
	step [194/249], loss=144.1842
	step [195/249], loss=153.0876
	step [196/249], loss=143.7739
	step [197/249], loss=153.9604
	step [198/249], loss=125.9805
	step [199/249], loss=149.1906
	step [200/249], loss=151.1978
	step [201/249], loss=121.2328
	step [202/249], loss=140.2421
	step [203/249], loss=127.9439
	step [204/249], loss=155.3724
	step [205/249], loss=131.1723
	step [206/249], loss=144.4365
	step [207/249], loss=137.3045
	step [208/249], loss=134.2014
	step [209/249], loss=133.3898
	step [210/249], loss=121.2350
	step [211/249], loss=129.6957
	step [212/249], loss=139.7306
	step [213/249], loss=120.6392
	step [214/249], loss=158.3406
	step [215/249], loss=145.1946
	step [216/249], loss=126.2836
	step [217/249], loss=144.7485
	step [218/249], loss=154.3764
	step [219/249], loss=123.4873
	step [220/249], loss=129.2494
	step [221/249], loss=137.3610
	step [222/249], loss=157.0231
	step [223/249], loss=123.6644
	step [224/249], loss=122.5870
	step [225/249], loss=131.9969
	step [226/249], loss=128.4978
	step [227/249], loss=134.3740
	step [228/249], loss=137.6966
	step [229/249], loss=141.3011
	step [230/249], loss=143.5952
	step [231/249], loss=141.5052
	step [232/249], loss=128.4918
	step [233/249], loss=152.1989
	step [234/249], loss=153.4937
	step [235/249], loss=130.5414
	step [236/249], loss=135.4671
	step [237/249], loss=141.4272
	step [238/249], loss=157.4498
	step [239/249], loss=143.5500
	step [240/249], loss=159.7569
	step [241/249], loss=136.0615
	step [242/249], loss=134.3138
	step [243/249], loss=154.7234
	step [244/249], loss=134.8058
	step [245/249], loss=137.2663
	step [246/249], loss=141.5973
	step [247/249], loss=152.0484
	step [248/249], loss=136.2211
	step [249/249], loss=5.2352
	Evaluating
	loss=0.1295, precision=0.5410, recall=0.8974, f1=0.6751
Training epoch 4
	step [1/249], loss=128.2764
	step [2/249], loss=156.5355
	step [3/249], loss=133.5942
	step [4/249], loss=136.7709
	step [5/249], loss=131.8482
	step [6/249], loss=125.5302
	step [7/249], loss=141.6857
	step [8/249], loss=131.3673
	step [9/249], loss=114.9158
	step [10/249], loss=119.0816
	step [11/249], loss=122.6338
	step [12/249], loss=129.9818
	step [13/249], loss=113.1950
	step [14/249], loss=144.7975
	step [15/249], loss=154.5957
	step [16/249], loss=136.5242
	step [17/249], loss=127.1547
	step [18/249], loss=146.2194
	step [19/249], loss=135.4719
	step [20/249], loss=127.3944
	step [21/249], loss=143.1105
	step [22/249], loss=151.5314
	step [23/249], loss=142.2496
	step [24/249], loss=144.5309
	step [25/249], loss=147.3141
	step [26/249], loss=138.8468
	step [27/249], loss=119.2253
	step [28/249], loss=160.5955
	step [29/249], loss=130.7817
	step [30/249], loss=139.2758
	step [31/249], loss=144.2858
	step [32/249], loss=161.1109
	step [33/249], loss=141.4398
	step [34/249], loss=136.1699
	step [35/249], loss=117.7958
	step [36/249], loss=158.1078
	step [37/249], loss=146.5890
	step [38/249], loss=146.1224
	step [39/249], loss=146.5849
	step [40/249], loss=122.8686
	step [41/249], loss=129.9980
	step [42/249], loss=140.0035
	step [43/249], loss=122.9271
	step [44/249], loss=132.6781
	step [45/249], loss=143.6263
	step [46/249], loss=143.5623
	step [47/249], loss=136.5988
	step [48/249], loss=143.4553
	step [49/249], loss=151.3525
	step [50/249], loss=123.2374
	step [51/249], loss=121.0396
	step [52/249], loss=143.6026
	step [53/249], loss=132.4450
	step [54/249], loss=136.9468
	step [55/249], loss=150.1104
	step [56/249], loss=130.9894
	step [57/249], loss=131.9385
	step [58/249], loss=138.4934
	step [59/249], loss=125.6866
	step [60/249], loss=131.9076
	step [61/249], loss=139.4226
	step [62/249], loss=100.9991
	step [63/249], loss=156.5948
	step [64/249], loss=104.9741
	step [65/249], loss=127.8643
	step [66/249], loss=155.4077
	step [67/249], loss=145.0113
	step [68/249], loss=127.5582
	step [69/249], loss=159.6258
	step [70/249], loss=150.1794
	step [71/249], loss=130.0046
	step [72/249], loss=115.6072
	step [73/249], loss=136.0531
	step [74/249], loss=138.0597
	step [75/249], loss=132.8414
	step [76/249], loss=124.5400
	step [77/249], loss=136.2145
	step [78/249], loss=127.9480
	step [79/249], loss=126.5892
	step [80/249], loss=135.4984
	step [81/249], loss=107.7405
	step [82/249], loss=137.2572
	step [83/249], loss=136.1632
	step [84/249], loss=149.8561
	step [85/249], loss=138.0830
	step [86/249], loss=133.7966
	step [87/249], loss=145.7469
	step [88/249], loss=156.4323
	step [89/249], loss=127.7632
	step [90/249], loss=123.8415
	step [91/249], loss=130.1286
	step [92/249], loss=125.4785
	step [93/249], loss=136.4853
	step [94/249], loss=102.0616
	step [95/249], loss=146.8586
	step [96/249], loss=160.2699
	step [97/249], loss=118.9378
	step [98/249], loss=144.6250
	step [99/249], loss=131.9060
	step [100/249], loss=148.1156
	step [101/249], loss=129.2361
	step [102/249], loss=127.0693
	step [103/249], loss=137.7599
	step [104/249], loss=132.7013
	step [105/249], loss=121.0909
	step [106/249], loss=132.7107
	step [107/249], loss=129.9480
	step [108/249], loss=135.5701
	step [109/249], loss=134.1549
	step [110/249], loss=114.4829
	step [111/249], loss=124.9521
	step [112/249], loss=135.4583
	step [113/249], loss=116.0478
	step [114/249], loss=140.2499
	step [115/249], loss=130.2683
	step [116/249], loss=112.3151
	step [117/249], loss=123.8860
	step [118/249], loss=138.4290
	step [119/249], loss=115.7873
	step [120/249], loss=125.2072
	step [121/249], loss=116.4720
	step [122/249], loss=144.2802
	step [123/249], loss=125.3892
	step [124/249], loss=129.4709
	step [125/249], loss=133.3925
	step [126/249], loss=140.8490
	step [127/249], loss=135.1660
	step [128/249], loss=135.1935
	step [129/249], loss=154.3628
	step [130/249], loss=134.0265
	step [131/249], loss=130.2139
	step [132/249], loss=151.2766
	step [133/249], loss=119.1889
	step [134/249], loss=127.0738
	step [135/249], loss=136.7903
	step [136/249], loss=135.3089
	step [137/249], loss=146.1741
	step [138/249], loss=125.4262
	step [139/249], loss=134.1627
	step [140/249], loss=133.6274
	step [141/249], loss=120.1966
	step [142/249], loss=118.0192
	step [143/249], loss=114.4156
	step [144/249], loss=121.4330
	step [145/249], loss=113.9873
	step [146/249], loss=134.7874
	step [147/249], loss=132.5659
	step [148/249], loss=126.4996
	step [149/249], loss=112.9581
	step [150/249], loss=123.9471
	step [151/249], loss=142.2459
	step [152/249], loss=118.0630
	step [153/249], loss=134.1714
	step [154/249], loss=126.6986
	step [155/249], loss=145.6389
	step [156/249], loss=118.3759
	step [157/249], loss=110.4287
	step [158/249], loss=159.9638
	step [159/249], loss=113.4670
	step [160/249], loss=154.3961
	step [161/249], loss=132.3029
	step [162/249], loss=118.2386
	step [163/249], loss=129.7381
	step [164/249], loss=148.0071
	step [165/249], loss=153.7615
	step [166/249], loss=151.7641
	step [167/249], loss=129.6804
	step [168/249], loss=112.1527
	step [169/249], loss=110.3051
	step [170/249], loss=131.3736
	step [171/249], loss=120.4492
	step [172/249], loss=120.4041
	step [173/249], loss=133.0066
	step [174/249], loss=124.5038
	step [175/249], loss=127.4406
	step [176/249], loss=142.2561
	step [177/249], loss=120.8375
	step [178/249], loss=118.7511
	step [179/249], loss=138.0391
	step [180/249], loss=124.1536
	step [181/249], loss=106.1466
	step [182/249], loss=148.9783
	step [183/249], loss=92.4746
	step [184/249], loss=122.0498
	step [185/249], loss=160.9204
	step [186/249], loss=151.0995
	step [187/249], loss=116.8867
	step [188/249], loss=140.7726
	step [189/249], loss=143.8208
	step [190/249], loss=150.3837
	step [191/249], loss=116.4367
	step [192/249], loss=120.5696
	step [193/249], loss=136.7848
	step [194/249], loss=123.7075
	step [195/249], loss=163.6011
	step [196/249], loss=120.9734
	step [197/249], loss=115.8240
	step [198/249], loss=115.5113
	step [199/249], loss=148.7347
	step [200/249], loss=136.8748
	step [201/249], loss=129.1105
	step [202/249], loss=111.9124
	step [203/249], loss=142.6600
	step [204/249], loss=104.9335
	step [205/249], loss=112.3604
	step [206/249], loss=116.0125
	step [207/249], loss=110.8145
	step [208/249], loss=132.3695
	step [209/249], loss=107.0329
	step [210/249], loss=130.0685
	step [211/249], loss=121.6932
	step [212/249], loss=153.1967
	step [213/249], loss=138.7385
	step [214/249], loss=119.8149
	step [215/249], loss=106.7748
	step [216/249], loss=128.7107
	step [217/249], loss=119.2978
	step [218/249], loss=130.8017
	step [219/249], loss=116.5595
	step [220/249], loss=135.9139
	step [221/249], loss=130.2498
	step [222/249], loss=136.6274
	step [223/249], loss=155.2347
	step [224/249], loss=141.7336
	step [225/249], loss=137.6048
	step [226/249], loss=103.7908
	step [227/249], loss=144.8859
	step [228/249], loss=119.6861
	step [229/249], loss=145.5103
	step [230/249], loss=117.0019
	step [231/249], loss=113.1533
	step [232/249], loss=117.4527
	step [233/249], loss=135.7230
	step [234/249], loss=140.2644
	step [235/249], loss=141.8779
	step [236/249], loss=147.8796
	step [237/249], loss=152.6599
	step [238/249], loss=114.6829
	step [239/249], loss=138.9169
	step [240/249], loss=117.2072
	step [241/249], loss=139.2413
	step [242/249], loss=96.6276
	step [243/249], loss=122.2566
	step [244/249], loss=127.8511
	step [245/249], loss=117.8074
	step [246/249], loss=104.3899
	step [247/249], loss=129.5992
	step [248/249], loss=123.8003
	step [249/249], loss=7.9096
	Evaluating
	loss=0.0911, precision=0.5245, recall=0.8912, f1=0.6604
Training epoch 5
	step [1/249], loss=133.8315
	step [2/249], loss=135.1604
	step [3/249], loss=129.6106
	step [4/249], loss=110.6739
	step [5/249], loss=116.6472
	step [6/249], loss=138.0428
	step [7/249], loss=115.8933
	step [8/249], loss=119.5637
	step [9/249], loss=127.1686
	step [10/249], loss=108.2909
	step [11/249], loss=140.9310
	step [12/249], loss=100.0805
	step [13/249], loss=126.7083
	step [14/249], loss=134.7787
	step [15/249], loss=111.7411
	step [16/249], loss=134.1953
	step [17/249], loss=102.6554
	step [18/249], loss=124.3249
	step [19/249], loss=126.0991
	step [20/249], loss=139.4634
	step [21/249], loss=110.1463
	step [22/249], loss=141.0562
	step [23/249], loss=125.1602
	step [24/249], loss=129.9981
	step [25/249], loss=122.1428
	step [26/249], loss=132.2144
	step [27/249], loss=120.2023
	step [28/249], loss=114.8884
	step [29/249], loss=125.9131
	step [30/249], loss=109.4997
	step [31/249], loss=156.0771
	step [32/249], loss=116.4701
	step [33/249], loss=108.5470
	step [34/249], loss=106.1277
	step [35/249], loss=112.3596
	step [36/249], loss=123.8079
	step [37/249], loss=126.2962
	step [38/249], loss=114.8526
	step [39/249], loss=120.5629
	step [40/249], loss=133.4029
	step [41/249], loss=132.1700
	step [42/249], loss=122.0201
	step [43/249], loss=136.1890
	step [44/249], loss=112.6480
	step [45/249], loss=148.0743
	step [46/249], loss=119.9584
	step [47/249], loss=121.8260
	step [48/249], loss=115.5666
	step [49/249], loss=139.1677
	step [50/249], loss=111.5172
	step [51/249], loss=159.6768
	step [52/249], loss=118.2955
	step [53/249], loss=145.8328
	step [54/249], loss=132.6228
	step [55/249], loss=97.7460
	step [56/249], loss=126.8302
	step [57/249], loss=126.1220
	step [58/249], loss=125.8161
	step [59/249], loss=130.0761
	step [60/249], loss=123.1203
	step [61/249], loss=143.4674
	step [62/249], loss=108.4703
	step [63/249], loss=140.7419
	step [64/249], loss=152.3768
	step [65/249], loss=137.4882
	step [66/249], loss=122.4723
	step [67/249], loss=113.7130
	step [68/249], loss=128.4492
	step [69/249], loss=115.9904
	step [70/249], loss=115.8224
	step [71/249], loss=127.3069
	step [72/249], loss=124.9631
	step [73/249], loss=116.6007
	step [74/249], loss=150.3578
	step [75/249], loss=131.5596
	step [76/249], loss=141.4145
	step [77/249], loss=140.6353
	step [78/249], loss=127.0559
	step [79/249], loss=141.2094
	step [80/249], loss=113.7476
	step [81/249], loss=155.2375
	step [82/249], loss=121.2600
	step [83/249], loss=139.9254
	step [84/249], loss=125.4900
	step [85/249], loss=108.4316
	step [86/249], loss=121.5763
	step [87/249], loss=124.4254
	step [88/249], loss=129.7017
	step [89/249], loss=133.3042
	step [90/249], loss=136.9952
	step [91/249], loss=131.4563
	step [92/249], loss=123.9040
	step [93/249], loss=102.9739
	step [94/249], loss=144.8743
	step [95/249], loss=120.5262
	step [96/249], loss=110.4479
	step [97/249], loss=147.4749
	step [98/249], loss=126.3944
	step [99/249], loss=136.2247
	step [100/249], loss=126.3397
	step [101/249], loss=140.6435
	step [102/249], loss=116.8043
	step [103/249], loss=125.1065
	step [104/249], loss=143.7123
	step [105/249], loss=115.8844
	step [106/249], loss=109.0655
	step [107/249], loss=126.1686
	step [108/249], loss=109.5475
	step [109/249], loss=113.1748
	step [110/249], loss=107.3838
	step [111/249], loss=103.0389
	step [112/249], loss=118.4258
	step [113/249], loss=118.3435
	step [114/249], loss=134.1517
	step [115/249], loss=121.3110
	step [116/249], loss=123.8662
	step [117/249], loss=127.8884
	step [118/249], loss=119.8451
	step [119/249], loss=128.0313
	step [120/249], loss=130.2081
	step [121/249], loss=127.6347
	step [122/249], loss=124.5912
	step [123/249], loss=132.6772
	step [124/249], loss=145.9222
	step [125/249], loss=109.2765
	step [126/249], loss=118.6488
	step [127/249], loss=141.2166
	step [128/249], loss=108.5525
	step [129/249], loss=128.1553
	step [130/249], loss=114.1063
	step [131/249], loss=126.2754
	step [132/249], loss=127.0878
	step [133/249], loss=102.2603
	step [134/249], loss=140.0724
	step [135/249], loss=120.2175
	step [136/249], loss=141.3891
	step [137/249], loss=102.7156
	step [138/249], loss=137.5144
	step [139/249], loss=115.2378
	step [140/249], loss=128.5473
	step [141/249], loss=117.5629
	step [142/249], loss=135.2342
	step [143/249], loss=116.9030
	step [144/249], loss=121.5498
	step [145/249], loss=119.0472
	step [146/249], loss=126.4043
	step [147/249], loss=111.8040
	step [148/249], loss=135.4452
	step [149/249], loss=140.5742
	step [150/249], loss=127.0245
	step [151/249], loss=128.7198
	step [152/249], loss=117.7642
	step [153/249], loss=111.2024
	step [154/249], loss=127.3284
	step [155/249], loss=122.8888
	step [156/249], loss=131.2960
	step [157/249], loss=125.3804
	step [158/249], loss=106.1587
	step [159/249], loss=112.8088
	step [160/249], loss=113.8385
	step [161/249], loss=138.3447
	step [162/249], loss=120.9442
	step [163/249], loss=140.0701
	step [164/249], loss=111.6030
	step [165/249], loss=112.1026
	step [166/249], loss=117.8059
	step [167/249], loss=117.3994
	step [168/249], loss=118.3257
	step [169/249], loss=126.5856
	step [170/249], loss=129.3440
	step [171/249], loss=108.7562
	step [172/249], loss=153.3320
	step [173/249], loss=115.5821
	step [174/249], loss=122.4291
	step [175/249], loss=125.4549
	step [176/249], loss=116.1780
	step [177/249], loss=116.3594
	step [178/249], loss=97.3448
	step [179/249], loss=123.3603
	step [180/249], loss=141.1762
	step [181/249], loss=124.3672
	step [182/249], loss=122.8151
	step [183/249], loss=111.4919
	step [184/249], loss=107.5931
	step [185/249], loss=147.3890
	step [186/249], loss=124.7369
	step [187/249], loss=142.9529
	step [188/249], loss=137.1407
	step [189/249], loss=116.0253
	step [190/249], loss=134.2802
	step [191/249], loss=124.1427
	step [192/249], loss=101.7447
	step [193/249], loss=103.0548
	step [194/249], loss=102.2923
	step [195/249], loss=141.8853
	step [196/249], loss=110.4160
	step [197/249], loss=118.0347
	step [198/249], loss=133.0702
	step [199/249], loss=131.9245
	step [200/249], loss=133.9185
	step [201/249], loss=126.4364
	step [202/249], loss=150.9343
	step [203/249], loss=132.8676
	step [204/249], loss=98.1252
	step [205/249], loss=133.3727
	step [206/249], loss=117.8307
	step [207/249], loss=112.4111
	step [208/249], loss=98.7794
	step [209/249], loss=119.7418
	step [210/249], loss=130.5777
	step [211/249], loss=146.5567
	step [212/249], loss=150.3043
	step [213/249], loss=115.4216
	step [214/249], loss=134.1629
	step [215/249], loss=118.5417
	step [216/249], loss=130.7966
	step [217/249], loss=132.9425
	step [218/249], loss=122.7294
	step [219/249], loss=107.1217
	step [220/249], loss=121.8868
	step [221/249], loss=96.8143
	step [222/249], loss=133.4875
	step [223/249], loss=106.6212
	step [224/249], loss=139.4691
	step [225/249], loss=117.5521
	step [226/249], loss=120.9086
	step [227/249], loss=118.4999
	step [228/249], loss=129.4313
	step [229/249], loss=112.8242
	step [230/249], loss=115.7508
	step [231/249], loss=143.7289
	step [232/249], loss=121.7453
	step [233/249], loss=137.6211
	step [234/249], loss=123.8810
	step [235/249], loss=131.9274
	step [236/249], loss=116.5037
	step [237/249], loss=131.6450
	step [238/249], loss=111.0392
	step [239/249], loss=107.9239
	step [240/249], loss=135.5295
	step [241/249], loss=130.3729
	step [242/249], loss=100.6415
	step [243/249], loss=115.5283
	step [244/249], loss=107.3679
	step [245/249], loss=94.6120
	step [246/249], loss=128.3431
	step [247/249], loss=135.3905
	step [248/249], loss=109.6463
	step [249/249], loss=9.4095
	Evaluating
	loss=0.0604, precision=0.5087, recall=0.8682, f1=0.6415
Training epoch 6
	step [1/249], loss=133.7923
	step [2/249], loss=116.1923
	step [3/249], loss=119.9868
	step [4/249], loss=109.3259
	step [5/249], loss=113.4324
	step [6/249], loss=123.7487
	step [7/249], loss=136.8637
	step [8/249], loss=122.5767
	step [9/249], loss=114.0077
	step [10/249], loss=134.3188
	step [11/249], loss=116.6760
	step [12/249], loss=119.7698
	step [13/249], loss=106.9280
	step [14/249], loss=96.7212
	step [15/249], loss=127.1396
	step [16/249], loss=101.6433
	step [17/249], loss=111.4499
	step [18/249], loss=111.7304
	step [19/249], loss=145.3353
	step [20/249], loss=116.6152
	step [21/249], loss=136.9062
	step [22/249], loss=115.8816
	step [23/249], loss=114.3105
	step [24/249], loss=133.9162
	step [25/249], loss=134.4892
	step [26/249], loss=110.1003
	step [27/249], loss=127.6084
	step [28/249], loss=118.0346
	step [29/249], loss=114.8532
	step [30/249], loss=126.1585
	step [31/249], loss=127.0064
	step [32/249], loss=126.7260
	step [33/249], loss=114.5532
	step [34/249], loss=130.3859
	step [35/249], loss=119.4211
	step [36/249], loss=128.7791
	step [37/249], loss=102.9517
	step [38/249], loss=129.8943
	step [39/249], loss=137.2248
	step [40/249], loss=116.9625
	step [41/249], loss=106.8920
	step [42/249], loss=123.6893
	step [43/249], loss=127.1299
	step [44/249], loss=109.2600
	step [45/249], loss=106.6460
	step [46/249], loss=124.0444
	step [47/249], loss=119.8824
	step [48/249], loss=134.0524
	step [49/249], loss=106.4014
	step [50/249], loss=117.6967
	step [51/249], loss=112.9430
	step [52/249], loss=128.3648
	step [53/249], loss=137.0230
	step [54/249], loss=114.2317
	step [55/249], loss=99.3764
	step [56/249], loss=123.4782
	step [57/249], loss=127.5889
	step [58/249], loss=129.5807
	step [59/249], loss=132.4545
	step [60/249], loss=130.6909
	step [61/249], loss=121.3845
	step [62/249], loss=130.0160
	step [63/249], loss=125.7449
	step [64/249], loss=133.3059
	step [65/249], loss=142.9079
	step [66/249], loss=115.2617
	step [67/249], loss=130.2885
	step [68/249], loss=108.2709
	step [69/249], loss=100.2927
	step [70/249], loss=97.3854
	step [71/249], loss=116.5287
	step [72/249], loss=114.2770
	step [73/249], loss=115.1510
	step [74/249], loss=103.2946
	step [75/249], loss=113.1452
	step [76/249], loss=125.1149
	step [77/249], loss=107.9805
	step [78/249], loss=130.6129
	step [79/249], loss=109.1175
	step [80/249], loss=110.0650
	step [81/249], loss=120.2512
	step [82/249], loss=116.3923
	step [83/249], loss=119.3665
	step [84/249], loss=120.8740
	step [85/249], loss=129.7647
	step [86/249], loss=115.4607
	step [87/249], loss=130.5037
	step [88/249], loss=107.5968
	step [89/249], loss=118.2683
	step [90/249], loss=146.3306
	step [91/249], loss=105.7318
	step [92/249], loss=119.8980
	step [93/249], loss=135.6972
	step [94/249], loss=140.7897
	step [95/249], loss=104.5128
	step [96/249], loss=118.6418
	step [97/249], loss=111.9126
	step [98/249], loss=107.3263
	step [99/249], loss=122.7664
	step [100/249], loss=111.1773
	step [101/249], loss=130.0829
	step [102/249], loss=98.5008
	step [103/249], loss=136.6446
	step [104/249], loss=109.5559
	step [105/249], loss=131.7119
	step [106/249], loss=125.2327
	step [107/249], loss=102.5664
	step [108/249], loss=124.4965
	step [109/249], loss=133.1456
	step [110/249], loss=111.6293
	step [111/249], loss=121.6077
	step [112/249], loss=115.9474
	step [113/249], loss=106.8199
	step [114/249], loss=121.5589
	step [115/249], loss=100.7355
	step [116/249], loss=144.7173
	step [117/249], loss=118.3485
	step [118/249], loss=114.9477
	step [119/249], loss=112.0980
	step [120/249], loss=111.4997
	step [121/249], loss=120.1760
	step [122/249], loss=115.5183
	step [123/249], loss=119.6306
	step [124/249], loss=107.8868
	step [125/249], loss=119.0649
	step [126/249], loss=137.8394
	step [127/249], loss=124.0104
	step [128/249], loss=135.5379
	step [129/249], loss=127.6720
	step [130/249], loss=110.4113
	step [131/249], loss=119.1154
	step [132/249], loss=119.3690
	step [133/249], loss=124.1379
	step [134/249], loss=133.3003
	step [135/249], loss=117.3479
	step [136/249], loss=115.2017
	step [137/249], loss=116.2096
	step [138/249], loss=97.6204
	step [139/249], loss=124.4011
	step [140/249], loss=137.2742
	step [141/249], loss=100.2379
	step [142/249], loss=114.1742
	step [143/249], loss=125.6992
	step [144/249], loss=106.3874
	step [145/249], loss=108.3343
	step [146/249], loss=126.7642
	step [147/249], loss=128.0571
	step [148/249], loss=108.7044
	step [149/249], loss=125.6764
	step [150/249], loss=130.2550
	step [151/249], loss=130.7182
	step [152/249], loss=118.7102
	step [153/249], loss=108.1849
	step [154/249], loss=135.0607
	step [155/249], loss=119.9147
	step [156/249], loss=132.1084
	step [157/249], loss=97.2705
	step [158/249], loss=129.2281
	step [159/249], loss=105.4262
	step [160/249], loss=130.9872
	step [161/249], loss=119.0391
	step [162/249], loss=123.4871
	step [163/249], loss=126.0647
	step [164/249], loss=118.7762
	step [165/249], loss=112.4296
	step [166/249], loss=112.9489
	step [167/249], loss=106.5400
	step [168/249], loss=122.3098
	step [169/249], loss=115.3954
	step [170/249], loss=136.1226
	step [171/249], loss=106.0492
	step [172/249], loss=147.2985
	step [173/249], loss=98.2670
	step [174/249], loss=133.3665
	step [175/249], loss=109.4333
	step [176/249], loss=107.7731
	step [177/249], loss=110.7873
	step [178/249], loss=119.5564
	step [179/249], loss=129.9129
	step [180/249], loss=113.7443
	step [181/249], loss=102.6404
	step [182/249], loss=136.3850
	step [183/249], loss=101.7831
	step [184/249], loss=120.4304
	step [185/249], loss=118.6691
	step [186/249], loss=115.5218
	step [187/249], loss=91.8666
	step [188/249], loss=115.7668
	step [189/249], loss=135.1800
	step [190/249], loss=103.3187
	step [191/249], loss=121.1612
	step [192/249], loss=129.5550
	step [193/249], loss=119.7012
	step [194/249], loss=127.8271
	step [195/249], loss=112.7438
	step [196/249], loss=117.3173
	step [197/249], loss=114.7626
	step [198/249], loss=103.0398
	step [199/249], loss=110.4914
	step [200/249], loss=127.3552
	step [201/249], loss=121.2656
	step [202/249], loss=124.0262
	step [203/249], loss=120.9101
	step [204/249], loss=113.1562
	step [205/249], loss=103.0029
	step [206/249], loss=127.0414
	step [207/249], loss=108.9675
	step [208/249], loss=121.7480
	step [209/249], loss=123.9377
	step [210/249], loss=129.3683
	step [211/249], loss=122.6736
	step [212/249], loss=114.8088
	step [213/249], loss=110.7839
	step [214/249], loss=124.8318
	step [215/249], loss=133.1610
	step [216/249], loss=125.9730
	step [217/249], loss=115.7694
	step [218/249], loss=130.6502
	step [219/249], loss=138.3276
	step [220/249], loss=94.7004
	step [221/249], loss=137.7530
	step [222/249], loss=125.3485
	step [223/249], loss=98.6492
	step [224/249], loss=135.2298
	step [225/249], loss=116.6492
	step [226/249], loss=132.9109
	step [227/249], loss=120.1603
	step [228/249], loss=117.2768
	step [229/249], loss=120.3605
	step [230/249], loss=117.6912
	step [231/249], loss=120.6043
	step [232/249], loss=110.3200
	step [233/249], loss=121.5518
	step [234/249], loss=130.2097
	step [235/249], loss=97.8362
	step [236/249], loss=131.4538
	step [237/249], loss=116.5333
	step [238/249], loss=121.6529
	step [239/249], loss=101.8060
	step [240/249], loss=113.3863
	step [241/249], loss=105.8507
	step [242/249], loss=128.1716
	step [243/249], loss=106.2756
	step [244/249], loss=126.1590
	step [245/249], loss=150.7868
	step [246/249], loss=111.3906
	step [247/249], loss=129.9120
	step [248/249], loss=101.3694
	step [249/249], loss=5.6856
	Evaluating
	loss=0.0480, precision=0.5090, recall=0.8839, f1=0.6460
Training epoch 7
	step [1/249], loss=107.8023
	step [2/249], loss=97.8983
	step [3/249], loss=121.3484
	step [4/249], loss=111.5393
	step [5/249], loss=113.1727
	step [6/249], loss=114.7068
	step [7/249], loss=128.5862
	step [8/249], loss=134.3586
	step [9/249], loss=110.7307
	step [10/249], loss=123.4411
	step [11/249], loss=123.9924
	step [12/249], loss=107.0284
	step [13/249], loss=111.6736
	step [14/249], loss=101.2070
	step [15/249], loss=116.3197
	step [16/249], loss=111.5983
	step [17/249], loss=143.6264
	step [18/249], loss=103.1336
	step [19/249], loss=125.1220
	step [20/249], loss=123.1997
	step [21/249], loss=104.7431
	step [22/249], loss=111.3755
	step [23/249], loss=105.2166
	step [24/249], loss=131.7115
	step [25/249], loss=109.5630
	step [26/249], loss=118.2038
	step [27/249], loss=131.1954
	step [28/249], loss=105.8160
	step [29/249], loss=140.8488
	step [30/249], loss=130.2801
	step [31/249], loss=94.6832
	step [32/249], loss=113.1802
	step [33/249], loss=136.0058
	step [34/249], loss=118.2575
	step [35/249], loss=124.9209
	step [36/249], loss=120.3031
	step [37/249], loss=122.2690
	step [38/249], loss=125.7324
	step [39/249], loss=111.3494
	step [40/249], loss=103.0363
	step [41/249], loss=109.2558
	step [42/249], loss=134.6954
	step [43/249], loss=102.2517
	step [44/249], loss=99.7457
	step [45/249], loss=95.6747
	step [46/249], loss=105.5276
	step [47/249], loss=104.9523
	step [48/249], loss=134.6666
	step [49/249], loss=109.1469
	step [50/249], loss=110.5368
	step [51/249], loss=97.3638
	step [52/249], loss=113.0590
	step [53/249], loss=121.5651
	step [54/249], loss=130.5061
	step [55/249], loss=126.0843
	step [56/249], loss=113.3888
	step [57/249], loss=93.3229
	step [58/249], loss=122.3932
	step [59/249], loss=139.7021
	step [60/249], loss=126.8636
	step [61/249], loss=111.7228
	step [62/249], loss=122.2841
	step [63/249], loss=124.8736
	step [64/249], loss=115.6257
	step [65/249], loss=123.8868
	step [66/249], loss=117.6018
	step [67/249], loss=112.8802
	step [68/249], loss=117.8723
	step [69/249], loss=104.2446
	step [70/249], loss=122.4095
	step [71/249], loss=108.3351
	step [72/249], loss=119.5806
	step [73/249], loss=102.0254
	step [74/249], loss=129.0204
	step [75/249], loss=92.3630
	step [76/249], loss=129.0252
	step [77/249], loss=95.4886
	step [78/249], loss=111.7800
	step [79/249], loss=136.3403
	step [80/249], loss=104.4669
	step [81/249], loss=116.0781
	step [82/249], loss=123.8559
	step [83/249], loss=112.0151
	step [84/249], loss=119.1934
	step [85/249], loss=130.2346
	step [86/249], loss=127.0682
	step [87/249], loss=125.2748
	step [88/249], loss=114.8437
	step [89/249], loss=146.1391
	step [90/249], loss=108.4123
	step [91/249], loss=107.5370
	step [92/249], loss=121.6929
	step [93/249], loss=143.5375
	step [94/249], loss=99.0488
	step [95/249], loss=123.3178
	step [96/249], loss=147.1565
	step [97/249], loss=114.6279
	step [98/249], loss=118.9149
	step [99/249], loss=104.7058
	step [100/249], loss=125.9743
	step [101/249], loss=99.1123
	step [102/249], loss=121.6290
	step [103/249], loss=126.6573
	step [104/249], loss=130.9836
	step [105/249], loss=111.4869
	step [106/249], loss=128.2600
	step [107/249], loss=122.6202
	step [108/249], loss=106.0269
	step [109/249], loss=110.7593
	step [110/249], loss=91.2975
	step [111/249], loss=122.7001
	step [112/249], loss=112.8798
	step [113/249], loss=126.0994
	step [114/249], loss=94.4797
	step [115/249], loss=102.7820
	step [116/249], loss=126.2883
	step [117/249], loss=123.7771
	step [118/249], loss=115.9436
	step [119/249], loss=125.0584
	step [120/249], loss=120.2324
	step [121/249], loss=91.5568
	step [122/249], loss=104.7631
	step [123/249], loss=122.0113
	step [124/249], loss=105.5573
	step [125/249], loss=103.2331
	step [126/249], loss=98.7082
	step [127/249], loss=136.0373
	step [128/249], loss=123.8787
	step [129/249], loss=129.2374
	step [130/249], loss=97.3397
	step [131/249], loss=99.4476
	step [132/249], loss=104.0002
	step [133/249], loss=101.2586
	step [134/249], loss=123.2110
	step [135/249], loss=122.7190
	step [136/249], loss=113.1199
	step [137/249], loss=91.9975
	step [138/249], loss=118.9646
	step [139/249], loss=117.5627
	step [140/249], loss=116.6278
	step [141/249], loss=126.1894
	step [142/249], loss=112.1726
	step [143/249], loss=91.7798
	step [144/249], loss=155.9280
	step [145/249], loss=108.4087
	step [146/249], loss=130.0036
	step [147/249], loss=114.1549
	step [148/249], loss=108.7466
	step [149/249], loss=111.1077
	step [150/249], loss=112.4086
	step [151/249], loss=105.1805
	step [152/249], loss=99.3069
	step [153/249], loss=106.6504
	step [154/249], loss=109.3616
	step [155/249], loss=142.4021
	step [156/249], loss=95.3121
	step [157/249], loss=91.4343
	step [158/249], loss=126.5971
	step [159/249], loss=119.2410
	step [160/249], loss=124.9315
	step [161/249], loss=126.2828
	step [162/249], loss=145.8748
	step [163/249], loss=132.1289
	step [164/249], loss=107.2467
	step [165/249], loss=120.0141
	step [166/249], loss=125.4843
	step [167/249], loss=136.9612
	step [168/249], loss=131.8627
	step [169/249], loss=95.4740
	step [170/249], loss=97.1381
	step [171/249], loss=95.8997
	step [172/249], loss=120.4176
	step [173/249], loss=108.3729
	step [174/249], loss=121.4171
	step [175/249], loss=109.9954
	step [176/249], loss=92.5672
	step [177/249], loss=99.8086
	step [178/249], loss=122.4014
	step [179/249], loss=113.6362
	step [180/249], loss=117.4099
	step [181/249], loss=117.8510
	step [182/249], loss=110.7883
	step [183/249], loss=124.5610
	step [184/249], loss=117.2722
	step [185/249], loss=125.1413
	step [186/249], loss=120.7950
	step [187/249], loss=135.2819
	step [188/249], loss=112.1555
	step [189/249], loss=115.5617
	step [190/249], loss=110.6895
	step [191/249], loss=113.2941
	step [192/249], loss=130.3479
	step [193/249], loss=112.0660
	step [194/249], loss=95.6956
	step [195/249], loss=152.5450
	step [196/249], loss=127.7014
	step [197/249], loss=112.2997
	step [198/249], loss=118.5948
	step [199/249], loss=116.9131
	step [200/249], loss=100.5427
	step [201/249], loss=116.9883
	step [202/249], loss=98.8458
	step [203/249], loss=106.4191
	step [204/249], loss=123.8890
	step [205/249], loss=124.2351
	step [206/249], loss=106.4875
	step [207/249], loss=120.0601
	step [208/249], loss=111.1602
	step [209/249], loss=114.9251
	step [210/249], loss=108.9272
	step [211/249], loss=90.4617
	step [212/249], loss=91.5432
	step [213/249], loss=126.5557
	step [214/249], loss=137.0555
	step [215/249], loss=119.9646
	step [216/249], loss=112.3273
	step [217/249], loss=108.7992
	step [218/249], loss=128.6316
	step [219/249], loss=118.8259
	step [220/249], loss=117.6160
	step [221/249], loss=117.3505
	step [222/249], loss=100.9130
	step [223/249], loss=115.8309
	step [224/249], loss=109.5404
	step [225/249], loss=105.3195
	step [226/249], loss=115.4900
	step [227/249], loss=113.7697
	step [228/249], loss=135.0957
	step [229/249], loss=109.7211
	step [230/249], loss=105.7554
	step [231/249], loss=127.6932
	step [232/249], loss=111.9832
	step [233/249], loss=122.8798
	step [234/249], loss=105.8745
	step [235/249], loss=121.8139
	step [236/249], loss=117.9584
	step [237/249], loss=115.1091
	step [238/249], loss=99.3335
	step [239/249], loss=120.9548
	step [240/249], loss=129.1675
	step [241/249], loss=111.7455
	step [242/249], loss=115.7771
	step [243/249], loss=115.0155
	step [244/249], loss=106.4927
	step [245/249], loss=118.8538
	step [246/249], loss=113.2235
	step [247/249], loss=137.6712
	step [248/249], loss=111.9849
	step [249/249], loss=7.8493
	Evaluating
	loss=0.0359, precision=0.4394, recall=0.9130, f1=0.5933
Training epoch 8
	step [1/249], loss=115.4327
	step [2/249], loss=121.7749
	step [3/249], loss=96.3580
	step [4/249], loss=99.7285
	step [5/249], loss=128.3616
	step [6/249], loss=116.7365
	step [7/249], loss=118.8312
	step [8/249], loss=120.7968
	step [9/249], loss=97.1783
	step [10/249], loss=121.4884
	step [11/249], loss=125.3636
	step [12/249], loss=116.4214
	step [13/249], loss=116.2078
	step [14/249], loss=127.1972
	step [15/249], loss=130.2064
	step [16/249], loss=100.7903
	step [17/249], loss=104.8017
	step [18/249], loss=127.7818
	step [19/249], loss=110.2353
	step [20/249], loss=111.8471
	step [21/249], loss=112.6631
	step [22/249], loss=101.7724
	step [23/249], loss=125.8976
	step [24/249], loss=83.1809
	step [25/249], loss=86.5006
	step [26/249], loss=130.5408
	step [27/249], loss=108.6627
	step [28/249], loss=99.4040
	step [29/249], loss=108.5653
	step [30/249], loss=117.9387
	step [31/249], loss=118.9465
	step [32/249], loss=112.7975
	step [33/249], loss=97.9694
	step [34/249], loss=111.5682
	step [35/249], loss=116.4782
	step [36/249], loss=109.1950
	step [37/249], loss=144.5320
	step [38/249], loss=133.9212
	step [39/249], loss=105.4537
	step [40/249], loss=111.7552
	step [41/249], loss=117.9704
	step [42/249], loss=119.3471
	step [43/249], loss=113.7420
	step [44/249], loss=109.6622
	step [45/249], loss=109.0748
	step [46/249], loss=103.1066
	step [47/249], loss=102.8781
	step [48/249], loss=99.8441
	step [49/249], loss=119.0961
	step [50/249], loss=106.0844
	step [51/249], loss=106.9569
	step [52/249], loss=124.6963
	step [53/249], loss=85.3980
	step [54/249], loss=130.0673
	step [55/249], loss=138.1001
	step [56/249], loss=105.2646
	step [57/249], loss=135.1580
	step [58/249], loss=94.9523
	step [59/249], loss=110.3432
	step [60/249], loss=125.6330
	step [61/249], loss=112.3106
	step [62/249], loss=120.4757
	step [63/249], loss=107.5714
	step [64/249], loss=129.5665
	step [65/249], loss=118.9129
	step [66/249], loss=130.1228
	step [67/249], loss=126.4558
	step [68/249], loss=98.4618
	step [69/249], loss=104.7185
	step [70/249], loss=90.9185
	step [71/249], loss=125.6566
	step [72/249], loss=104.5082
	step [73/249], loss=106.3524
	step [74/249], loss=117.1160
	step [75/249], loss=132.4263
	step [76/249], loss=109.4720
	step [77/249], loss=111.6413
	step [78/249], loss=86.0747
	step [79/249], loss=121.7287
	step [80/249], loss=121.1520
	step [81/249], loss=120.8310
	step [82/249], loss=127.6015
	step [83/249], loss=107.2498
	step [84/249], loss=124.6269
	step [85/249], loss=103.0724
	step [86/249], loss=110.7244
	step [87/249], loss=126.7565
	step [88/249], loss=93.6297
	step [89/249], loss=116.7351
	step [90/249], loss=105.6264
	step [91/249], loss=117.2823
	step [92/249], loss=122.4409
	step [93/249], loss=95.7790
	step [94/249], loss=102.0958
	step [95/249], loss=134.2604
	step [96/249], loss=112.5175
	step [97/249], loss=112.2618
	step [98/249], loss=115.0022
	step [99/249], loss=134.6000
	step [100/249], loss=114.1115
	step [101/249], loss=133.8183
	step [102/249], loss=117.9018
	step [103/249], loss=102.6905
	step [104/249], loss=99.8642
	step [105/249], loss=105.8872
	step [106/249], loss=107.0662
	step [107/249], loss=121.7669
	step [108/249], loss=135.5834
	step [109/249], loss=116.9593
	step [110/249], loss=126.5527
	step [111/249], loss=129.9153
	step [112/249], loss=124.3724
	step [113/249], loss=126.8403
	step [114/249], loss=116.0576
	step [115/249], loss=114.0114
	step [116/249], loss=92.7841
	step [117/249], loss=119.9101
	step [118/249], loss=97.3056
	step [119/249], loss=105.4969
	step [120/249], loss=113.0196
	step [121/249], loss=105.6538
	step [122/249], loss=120.7442
	step [123/249], loss=93.8362
	step [124/249], loss=125.2901
	step [125/249], loss=97.6205
	step [126/249], loss=129.1234
	step [127/249], loss=98.2264
	step [128/249], loss=112.1050
	step [129/249], loss=116.1278
	step [130/249], loss=115.5387
	step [131/249], loss=115.2195
	step [132/249], loss=111.7647
	step [133/249], loss=120.5944
	step [134/249], loss=109.0565
	step [135/249], loss=116.1449
	step [136/249], loss=102.8893
	step [137/249], loss=112.4272
	step [138/249], loss=123.2989
	step [139/249], loss=106.5199
	step [140/249], loss=125.0234
	step [141/249], loss=113.4993
	step [142/249], loss=109.3248
	step [143/249], loss=127.6620
	step [144/249], loss=126.4525
	step [145/249], loss=116.6059
	step [146/249], loss=136.7962
	step [147/249], loss=104.9278
	step [148/249], loss=113.4579
	step [149/249], loss=109.5090
	step [150/249], loss=123.6833
	step [151/249], loss=107.1038
	step [152/249], loss=96.8303
	step [153/249], loss=131.8544
	step [154/249], loss=128.6658
	step [155/249], loss=118.0653
	step [156/249], loss=117.5085
	step [157/249], loss=111.6478
	step [158/249], loss=102.7872
	step [159/249], loss=102.0142
	step [160/249], loss=134.7873
	step [161/249], loss=109.5759
	step [162/249], loss=115.9996
	step [163/249], loss=110.7259
	step [164/249], loss=109.7946
	step [165/249], loss=104.5087
	step [166/249], loss=131.3567
	step [167/249], loss=113.3893
	step [168/249], loss=114.4684
	step [169/249], loss=120.5374
	step [170/249], loss=103.6202
	step [171/249], loss=125.1259
	step [172/249], loss=116.6494
	step [173/249], loss=114.1319
	step [174/249], loss=110.6284
	step [175/249], loss=107.3242
	step [176/249], loss=111.1618
	step [177/249], loss=103.5345
	step [178/249], loss=117.5405
	step [179/249], loss=107.5188
	step [180/249], loss=93.9375
	step [181/249], loss=93.0519
	step [182/249], loss=114.1227
	step [183/249], loss=119.8170
	step [184/249], loss=118.5501
	step [185/249], loss=122.6625
	step [186/249], loss=116.5549
	step [187/249], loss=102.9734
	step [188/249], loss=101.6341
	step [189/249], loss=131.0046
	step [190/249], loss=83.2930
	step [191/249], loss=121.9848
	step [192/249], loss=102.9596
	step [193/249], loss=126.5841
	step [194/249], loss=126.9251
	step [195/249], loss=107.9204
	step [196/249], loss=97.1993
	step [197/249], loss=111.1749
	step [198/249], loss=116.0809
	step [199/249], loss=133.1449
	step [200/249], loss=101.2289
	step [201/249], loss=119.9417
	step [202/249], loss=101.6573
	step [203/249], loss=115.7565
	step [204/249], loss=123.6954
	step [205/249], loss=104.4254
	step [206/249], loss=117.7747
	step [207/249], loss=115.2973
	step [208/249], loss=119.6929
	step [209/249], loss=121.4548
	step [210/249], loss=116.5695
	step [211/249], loss=104.2487
	step [212/249], loss=120.9367
	step [213/249], loss=117.4381
	step [214/249], loss=140.9808
	step [215/249], loss=104.3983
	step [216/249], loss=112.5861
	step [217/249], loss=115.6707
	step [218/249], loss=99.2061
	step [219/249], loss=113.2406
	step [220/249], loss=111.4596
	step [221/249], loss=93.6495
	step [222/249], loss=101.7678
	step [223/249], loss=96.5509
	step [224/249], loss=111.3922
	step [225/249], loss=110.7336
	step [226/249], loss=110.7803
	step [227/249], loss=130.9897
	step [228/249], loss=133.3997
	step [229/249], loss=97.7980
	step [230/249], loss=123.2232
	step [231/249], loss=93.1103
	step [232/249], loss=109.2164
	step [233/249], loss=133.2433
	step [234/249], loss=108.3100
	step [235/249], loss=106.3346
	step [236/249], loss=112.6498
	step [237/249], loss=101.6328
	step [238/249], loss=108.5945
	step [239/249], loss=91.7571
	step [240/249], loss=121.8538
	step [241/249], loss=110.0951
	step [242/249], loss=103.8487
	step [243/249], loss=111.7400
	step [244/249], loss=103.0633
	step [245/249], loss=107.1146
	step [246/249], loss=90.5832
	step [247/249], loss=136.2694
	step [248/249], loss=102.9720
	step [249/249], loss=5.7776
	Evaluating
	loss=0.0283, precision=0.4890, recall=0.9031, f1=0.6344
Training epoch 9
	step [1/249], loss=121.5439
	step [2/249], loss=117.8317
	step [3/249], loss=112.8795
	step [4/249], loss=107.1272
	step [5/249], loss=118.4171
	step [6/249], loss=128.1108
	step [7/249], loss=108.6885
	step [8/249], loss=121.0837
	step [9/249], loss=119.3613
	step [10/249], loss=131.1479
	step [11/249], loss=102.7027
	step [12/249], loss=130.5529
	step [13/249], loss=125.5993
	step [14/249], loss=112.2346
	step [15/249], loss=115.7106
	step [16/249], loss=120.6654
	step [17/249], loss=112.4680
	step [18/249], loss=101.2231
	step [19/249], loss=107.3908
	step [20/249], loss=118.7141
	step [21/249], loss=100.7262
	step [22/249], loss=112.0280
	step [23/249], loss=125.2241
	step [24/249], loss=87.6943
	step [25/249], loss=104.5241
	step [26/249], loss=140.0645
	step [27/249], loss=145.0472
	step [28/249], loss=107.4652
	step [29/249], loss=105.6912
	step [30/249], loss=117.5478
	step [31/249], loss=104.0076
	step [32/249], loss=104.2578
	step [33/249], loss=111.9768
	step [34/249], loss=117.3837
	step [35/249], loss=103.2943
	step [36/249], loss=118.8981
	step [37/249], loss=113.4531
	step [38/249], loss=113.8396
	step [39/249], loss=139.5036
	step [40/249], loss=117.5608
	step [41/249], loss=128.8857
	step [42/249], loss=110.7832
	step [43/249], loss=116.2201
	step [44/249], loss=116.3392
	step [45/249], loss=116.8433
	step [46/249], loss=108.1236
	step [47/249], loss=121.8095
	step [48/249], loss=132.2519
	step [49/249], loss=121.0526
	step [50/249], loss=100.9734
	step [51/249], loss=115.0968
	step [52/249], loss=94.2018
	step [53/249], loss=109.5967
	step [54/249], loss=107.2432
	step [55/249], loss=140.8086
	step [56/249], loss=108.1207
	step [57/249], loss=91.0998
	step [58/249], loss=94.4185
	step [59/249], loss=98.7246
	step [60/249], loss=117.2236
	step [61/249], loss=109.9447
	step [62/249], loss=97.9627
	step [63/249], loss=113.7007
	step [64/249], loss=117.8302
	step [65/249], loss=114.9892
	step [66/249], loss=106.9812
	step [67/249], loss=107.0741
	step [68/249], loss=111.6438
	step [69/249], loss=107.4054
	step [70/249], loss=110.2231
	step [71/249], loss=128.3742
	step [72/249], loss=100.6803
	step [73/249], loss=114.0916
	step [74/249], loss=109.2510
	step [75/249], loss=116.5590
	step [76/249], loss=101.5639
	step [77/249], loss=99.0273
	step [78/249], loss=105.2057
	step [79/249], loss=110.2587
	step [80/249], loss=103.3464
	step [81/249], loss=120.1606
	step [82/249], loss=109.2004
	step [83/249], loss=112.3167
	step [84/249], loss=99.5100
	step [85/249], loss=113.5925
	step [86/249], loss=101.1422
	step [87/249], loss=115.0639
	step [88/249], loss=93.3500
	step [89/249], loss=102.7826
	step [90/249], loss=136.6787
	step [91/249], loss=102.0376
	step [92/249], loss=106.9902
	step [93/249], loss=120.0365
	step [94/249], loss=104.2919
	step [95/249], loss=113.4924
	step [96/249], loss=109.6780
	step [97/249], loss=114.1549
	step [98/249], loss=95.0394
	step [99/249], loss=96.8338
	step [100/249], loss=126.4173
	step [101/249], loss=116.6159
	step [102/249], loss=99.2846
	step [103/249], loss=110.1013
	step [104/249], loss=122.3834
	step [105/249], loss=125.3585
	step [106/249], loss=123.0339
	step [107/249], loss=106.4812
	step [108/249], loss=115.7817
	step [109/249], loss=108.7169
	step [110/249], loss=112.4794
	step [111/249], loss=121.4070
	step [112/249], loss=101.8916
	step [113/249], loss=114.9476
	step [114/249], loss=112.7680
	step [115/249], loss=107.2933
	step [116/249], loss=117.4628
	step [117/249], loss=118.0792
	step [118/249], loss=110.8785
	step [119/249], loss=96.3408
	step [120/249], loss=122.7576
	step [121/249], loss=103.3465
	step [122/249], loss=115.7614
	step [123/249], loss=98.7614
	step [124/249], loss=121.4008
	step [125/249], loss=117.0669
	step [126/249], loss=96.2094
	step [127/249], loss=116.4879
	step [128/249], loss=113.7300
	step [129/249], loss=103.7113
	step [130/249], loss=109.0433
	step [131/249], loss=117.8511
	step [132/249], loss=112.0294
	step [133/249], loss=85.9250
	step [134/249], loss=104.2219
	step [135/249], loss=113.6575
	step [136/249], loss=126.1701
	step [137/249], loss=114.0132
	step [138/249], loss=126.9255
	step [139/249], loss=106.7407
	step [140/249], loss=119.0880
	step [141/249], loss=107.3278
	step [142/249], loss=107.3695
	step [143/249], loss=106.9130
	step [144/249], loss=134.0719
	step [145/249], loss=87.4578
	step [146/249], loss=107.8986
	step [147/249], loss=125.5521
	step [148/249], loss=106.5835
	step [149/249], loss=105.2112
	step [150/249], loss=120.8729
	step [151/249], loss=83.8397
	step [152/249], loss=107.2056
	step [153/249], loss=95.9334
	step [154/249], loss=101.5394
	step [155/249], loss=110.2251
	step [156/249], loss=111.9592
	step [157/249], loss=109.8394
	step [158/249], loss=103.1567
	step [159/249], loss=112.4154
	step [160/249], loss=115.5954
	step [161/249], loss=101.1557
	step [162/249], loss=114.4052
	step [163/249], loss=109.8554
	step [164/249], loss=116.9807
	step [165/249], loss=105.5040
	step [166/249], loss=110.6632
	step [167/249], loss=103.1574
	step [168/249], loss=114.1543
	step [169/249], loss=98.7919
	step [170/249], loss=97.4126
	step [171/249], loss=103.8606
	step [172/249], loss=120.8455
	step [173/249], loss=129.0532
	step [174/249], loss=96.8840
	step [175/249], loss=115.7076
	step [176/249], loss=119.7298
	step [177/249], loss=112.2810
	step [178/249], loss=121.8125
	step [179/249], loss=120.9781
	step [180/249], loss=118.4335
	step [181/249], loss=116.4469
	step [182/249], loss=117.2382
	step [183/249], loss=117.7467
	step [184/249], loss=91.1994
	step [185/249], loss=131.6675
	step [186/249], loss=140.0835
	step [187/249], loss=93.9541
	step [188/249], loss=114.7906
	step [189/249], loss=108.5791
	step [190/249], loss=104.4920
	step [191/249], loss=80.1613
	step [192/249], loss=104.8385
	step [193/249], loss=146.6149
	step [194/249], loss=113.9547
	step [195/249], loss=121.6174
	step [196/249], loss=124.4683
	step [197/249], loss=114.0710
	step [198/249], loss=112.3369
	step [199/249], loss=133.3297
	step [200/249], loss=108.0293
	step [201/249], loss=89.9387
	step [202/249], loss=107.3630
	step [203/249], loss=119.5315
	step [204/249], loss=111.9964
	step [205/249], loss=114.5115
	step [206/249], loss=95.7719
	step [207/249], loss=117.0480
	step [208/249], loss=107.5819
	step [209/249], loss=117.3109
	step [210/249], loss=85.7842
	step [211/249], loss=100.1489
	step [212/249], loss=104.4169
	step [213/249], loss=95.6472
	step [214/249], loss=120.1734
	step [215/249], loss=92.6472
	step [216/249], loss=119.0387
	step [217/249], loss=102.8861
	step [218/249], loss=86.3693
	step [219/249], loss=115.9665
	step [220/249], loss=100.7773
	step [221/249], loss=114.7715
	step [222/249], loss=103.9822
	step [223/249], loss=126.3245
	step [224/249], loss=119.3605
	step [225/249], loss=106.9653
	step [226/249], loss=97.5916
	step [227/249], loss=118.3166
	step [228/249], loss=100.8446
	step [229/249], loss=136.5282
	step [230/249], loss=104.5746
	step [231/249], loss=110.4897
	step [232/249], loss=98.4550
	step [233/249], loss=119.7266
	step [234/249], loss=117.2423
	step [235/249], loss=142.1787
	step [236/249], loss=111.1118
	step [237/249], loss=102.5319
	step [238/249], loss=107.5609
	step [239/249], loss=113.5543
	step [240/249], loss=102.8679
	step [241/249], loss=118.1914
	step [242/249], loss=104.8416
	step [243/249], loss=112.4792
	step [244/249], loss=93.8700
	step [245/249], loss=103.4354
	step [246/249], loss=104.7636
	step [247/249], loss=112.4319
	step [248/249], loss=91.2998
	step [249/249], loss=4.9736
	Evaluating
	loss=0.0256, precision=0.4275, recall=0.8927, f1=0.5782
Training epoch 10
	step [1/249], loss=106.7804
	step [2/249], loss=110.2176
	step [3/249], loss=124.1713
	step [4/249], loss=95.5825
	step [5/249], loss=118.6230
	step [6/249], loss=116.5790
	step [7/249], loss=94.8616
	step [8/249], loss=100.9586
	step [9/249], loss=116.4661
	step [10/249], loss=114.7594
	step [11/249], loss=111.9997
	step [12/249], loss=135.6357
	step [13/249], loss=96.6758
	step [14/249], loss=97.8609
	step [15/249], loss=116.9047
	step [16/249], loss=115.9945
	step [17/249], loss=125.1194
	step [18/249], loss=119.4686
	step [19/249], loss=94.4260
	step [20/249], loss=115.9633
	step [21/249], loss=119.2755
	step [22/249], loss=119.6889
	step [23/249], loss=80.8327
	step [24/249], loss=122.6032
	step [25/249], loss=112.6472
	step [26/249], loss=98.8496
	step [27/249], loss=101.9197
	step [28/249], loss=122.1512
	step [29/249], loss=103.4360
	step [30/249], loss=123.5021
	step [31/249], loss=105.8974
	step [32/249], loss=101.7611
	step [33/249], loss=108.1564
	step [34/249], loss=103.6884
	step [35/249], loss=108.0427
	step [36/249], loss=131.3598
	step [37/249], loss=113.1554
	step [38/249], loss=100.7159
	step [39/249], loss=90.7165
	step [40/249], loss=124.9371
	step [41/249], loss=111.4804
	step [42/249], loss=120.3210
	step [43/249], loss=115.6914
	step [44/249], loss=114.0843
	step [45/249], loss=131.2702
	step [46/249], loss=104.9024
	step [47/249], loss=116.0939
	step [48/249], loss=106.3938
	step [49/249], loss=114.3775
	step [50/249], loss=91.4995
	step [51/249], loss=108.5125
	step [52/249], loss=125.7553
	step [53/249], loss=108.7018
	step [54/249], loss=106.8263
	step [55/249], loss=97.9188
	step [56/249], loss=108.2679
	step [57/249], loss=102.3009
	step [58/249], loss=96.2435
	step [59/249], loss=126.7838
	step [60/249], loss=137.8901
	step [61/249], loss=94.0268
	step [62/249], loss=112.9257
	step [63/249], loss=115.5294
	step [64/249], loss=98.9783
	step [65/249], loss=135.1859
	step [66/249], loss=89.5286
	step [67/249], loss=126.7733
	step [68/249], loss=105.1560
	step [69/249], loss=96.7290
	step [70/249], loss=89.7852
	step [71/249], loss=106.4730
	step [72/249], loss=126.2964
	step [73/249], loss=112.3477
	step [74/249], loss=93.4182
	step [75/249], loss=105.5266
	step [76/249], loss=103.1557
	step [77/249], loss=98.6352
	step [78/249], loss=113.6723
	step [79/249], loss=118.4896
	step [80/249], loss=96.9700
	step [81/249], loss=101.9801
	step [82/249], loss=109.7618
	step [83/249], loss=109.7074
	step [84/249], loss=118.6501
	step [85/249], loss=113.4477
	step [86/249], loss=103.3296
	step [87/249], loss=109.3353
	step [88/249], loss=104.9255
	step [89/249], loss=117.8228
	step [90/249], loss=97.2793
	step [91/249], loss=103.5044
	step [92/249], loss=98.6104
	step [93/249], loss=96.0383
	step [94/249], loss=101.6791
	step [95/249], loss=97.0711
	step [96/249], loss=132.0623
	step [97/249], loss=107.2901
	step [98/249], loss=96.4743
	step [99/249], loss=105.9633
	step [100/249], loss=111.3880
	step [101/249], loss=124.0865
	step [102/249], loss=104.7613
	step [103/249], loss=94.4354
	step [104/249], loss=114.8171
	step [105/249], loss=114.5937
	step [106/249], loss=97.9538
	step [107/249], loss=125.1642
	step [108/249], loss=121.2755
	step [109/249], loss=121.3219
	step [110/249], loss=82.5624
	step [111/249], loss=112.0708
	step [112/249], loss=102.8071
	step [113/249], loss=112.8552
	step [114/249], loss=111.4951
	step [115/249], loss=105.8341
	step [116/249], loss=116.1885
	step [117/249], loss=108.0751
	step [118/249], loss=118.7407
	step [119/249], loss=100.4258
	step [120/249], loss=113.1396
	step [121/249], loss=97.4020
	step [122/249], loss=130.8344
	step [123/249], loss=99.1698
	step [124/249], loss=124.8007
	step [125/249], loss=104.0655
	step [126/249], loss=119.3307
	step [127/249], loss=128.7188
	step [128/249], loss=103.6322
	step [129/249], loss=115.0941
	step [130/249], loss=114.6993
	step [131/249], loss=111.1262
	step [132/249], loss=109.3414
	step [133/249], loss=100.6336
	step [134/249], loss=105.3157
	step [135/249], loss=114.7351
	step [136/249], loss=106.8463
	step [137/249], loss=105.7813
	step [138/249], loss=129.0009
	step [139/249], loss=87.8253
	step [140/249], loss=110.1969
	step [141/249], loss=112.3843
	step [142/249], loss=105.3649
	step [143/249], loss=98.7305
	step [144/249], loss=82.8200
	step [145/249], loss=103.7388
	step [146/249], loss=100.4297
	step [147/249], loss=80.5293
	step [148/249], loss=103.6186
	step [149/249], loss=116.1912
	step [150/249], loss=112.1567
	step [151/249], loss=101.4894
	step [152/249], loss=107.4212
	step [153/249], loss=100.6076
	step [154/249], loss=113.1905
	step [155/249], loss=131.1588
	step [156/249], loss=108.1011
	step [157/249], loss=136.6024
	step [158/249], loss=116.7886
	step [159/249], loss=112.1830
	step [160/249], loss=104.4588
	step [161/249], loss=108.6584
	step [162/249], loss=98.8326
	step [163/249], loss=91.4008
	step [164/249], loss=97.3439
	step [165/249], loss=99.9622
	step [166/249], loss=77.3664
	step [167/249], loss=89.0632
	step [168/249], loss=97.9802
	step [169/249], loss=108.9269
	step [170/249], loss=108.7601
	step [171/249], loss=129.4834
	step [172/249], loss=108.3539
	step [173/249], loss=141.5820
	step [174/249], loss=102.3736
	step [175/249], loss=124.4749
	step [176/249], loss=110.7902
	step [177/249], loss=97.4784
	step [178/249], loss=120.0672
	step [179/249], loss=106.4391
	step [180/249], loss=85.9377
	step [181/249], loss=94.5377
	step [182/249], loss=103.9225
	step [183/249], loss=145.3897
	step [184/249], loss=122.7379
	step [185/249], loss=117.2044
	step [186/249], loss=103.3353
	step [187/249], loss=91.8059
	step [188/249], loss=104.0394
	step [189/249], loss=100.6598
	step [190/249], loss=110.8063
	step [191/249], loss=115.7907
	step [192/249], loss=95.1879
	step [193/249], loss=102.4239
	step [194/249], loss=109.6996
	step [195/249], loss=108.3083
	step [196/249], loss=114.2458
	step [197/249], loss=115.5239
	step [198/249], loss=99.1758
	step [199/249], loss=120.0896
	step [200/249], loss=98.6145
	step [201/249], loss=125.5291
	step [202/249], loss=104.4197
	step [203/249], loss=94.0291
	step [204/249], loss=111.7186
	step [205/249], loss=123.5394
	step [206/249], loss=115.5936
	step [207/249], loss=104.7430
	step [208/249], loss=100.1405
	step [209/249], loss=94.0022
	step [210/249], loss=124.1687
	step [211/249], loss=104.7858
	step [212/249], loss=102.2805
	step [213/249], loss=114.9903
	step [214/249], loss=116.5898
	step [215/249], loss=101.3824
	step [216/249], loss=123.7836
	step [217/249], loss=102.3919
	step [218/249], loss=125.4471
	step [219/249], loss=115.3377
	step [220/249], loss=115.5842
	step [221/249], loss=102.7889
	step [222/249], loss=102.5732
	step [223/249], loss=104.3068
	step [224/249], loss=131.3157
	step [225/249], loss=105.5502
	step [226/249], loss=130.6709
	step [227/249], loss=133.2535
	step [228/249], loss=110.4182
	step [229/249], loss=101.5334
	step [230/249], loss=102.7356
	step [231/249], loss=111.9572
	step [232/249], loss=125.4566
	step [233/249], loss=108.1613
	step [234/249], loss=102.4904
	step [235/249], loss=115.9528
	step [236/249], loss=106.6365
	step [237/249], loss=128.4974
	step [238/249], loss=105.1551
	step [239/249], loss=112.8117
	step [240/249], loss=137.3525
	step [241/249], loss=143.4973
	step [242/249], loss=108.5993
	step [243/249], loss=102.3214
	step [244/249], loss=98.1409
	step [245/249], loss=97.5038
	step [246/249], loss=117.1069
	step [247/249], loss=129.1783
	step [248/249], loss=107.6163
	step [249/249], loss=3.1483
	Evaluating
	loss=0.0216, precision=0.4302, recall=0.9145, f1=0.5851
Training epoch 11
	step [1/249], loss=92.1803
	step [2/249], loss=101.7153
	step [3/249], loss=110.8369
	step [4/249], loss=117.6887
	step [5/249], loss=92.1796
	step [6/249], loss=114.9066
	step [7/249], loss=102.6404
	step [8/249], loss=98.2772
	step [9/249], loss=123.9154
	step [10/249], loss=102.0709
	step [11/249], loss=117.1216
	step [12/249], loss=110.6817
	step [13/249], loss=100.2117
	step [14/249], loss=121.5955
	step [15/249], loss=109.8182
	step [16/249], loss=90.1513
	step [17/249], loss=114.1092
	step [18/249], loss=110.5250
	step [19/249], loss=96.1501
	step [20/249], loss=98.9363
	step [21/249], loss=135.0387
	step [22/249], loss=100.9992
	step [23/249], loss=102.8333
	step [24/249], loss=88.5044
	step [25/249], loss=91.6277
	step [26/249], loss=103.7849
	step [27/249], loss=112.8438
	step [28/249], loss=111.8309
	step [29/249], loss=95.8511
	step [30/249], loss=108.7278
	step [31/249], loss=127.7819
	step [32/249], loss=93.6751
	step [33/249], loss=99.9148
	step [34/249], loss=97.2705
	step [35/249], loss=103.2399
	step [36/249], loss=107.7551
	step [37/249], loss=110.1145
	step [38/249], loss=117.0125
	step [39/249], loss=116.4010
	step [40/249], loss=116.2543
	step [41/249], loss=94.4196
	step [42/249], loss=116.6435
	step [43/249], loss=113.7950
	step [44/249], loss=120.3708
	step [45/249], loss=127.1331
	step [46/249], loss=104.4204
	step [47/249], loss=113.6008
	step [48/249], loss=110.2705
	step [49/249], loss=107.6159
	step [50/249], loss=101.8602
	step [51/249], loss=123.5981
	step [52/249], loss=116.3813
	step [53/249], loss=83.0914
	step [54/249], loss=132.6491
	step [55/249], loss=130.8871
	step [56/249], loss=121.4157
	step [57/249], loss=102.7457
	step [58/249], loss=118.1169
	step [59/249], loss=113.9026
	step [60/249], loss=91.2383
	step [61/249], loss=125.5107
	step [62/249], loss=118.1574
	step [63/249], loss=106.9271
	step [64/249], loss=111.5795
	step [65/249], loss=122.9868
	step [66/249], loss=107.8315
	step [67/249], loss=94.7093
	step [68/249], loss=110.8501
	step [69/249], loss=109.4179
	step [70/249], loss=114.1916
	step [71/249], loss=95.3694
	step [72/249], loss=111.7507
	step [73/249], loss=132.0493
	step [74/249], loss=87.0413
	step [75/249], loss=109.3573
	step [76/249], loss=102.6602
	step [77/249], loss=124.4666
	step [78/249], loss=80.6440
	step [79/249], loss=114.0979
	step [80/249], loss=112.0932
	step [81/249], loss=101.8555
	step [82/249], loss=90.5807
	step [83/249], loss=114.9111
	step [84/249], loss=89.4328
	step [85/249], loss=98.6140
	step [86/249], loss=115.9235
	step [87/249], loss=132.2437
	step [88/249], loss=105.7310
	step [89/249], loss=106.6348
	step [90/249], loss=111.1179
	step [91/249], loss=119.2388
	step [92/249], loss=87.6813
	step [93/249], loss=109.4128
	step [94/249], loss=96.8580
	step [95/249], loss=110.4753
	step [96/249], loss=108.9190
	step [97/249], loss=127.9674
	step [98/249], loss=101.3039
	step [99/249], loss=100.6127
	step [100/249], loss=104.0610
	step [101/249], loss=105.4311
	step [102/249], loss=83.6180
	step [103/249], loss=98.4007
	step [104/249], loss=111.6649
	step [105/249], loss=94.7395
	step [106/249], loss=114.0896
	step [107/249], loss=84.6006
	step [108/249], loss=101.4962
	step [109/249], loss=94.6383
	step [110/249], loss=135.5827
	step [111/249], loss=140.0362
	step [112/249], loss=88.2225
	step [113/249], loss=103.9146
	step [114/249], loss=114.3417
	step [115/249], loss=106.3265
	step [116/249], loss=116.9227
	step [117/249], loss=115.9007
	step [118/249], loss=107.4027
	step [119/249], loss=109.0083
	step [120/249], loss=123.8349
	step [121/249], loss=101.8571
	step [122/249], loss=140.3338
	step [123/249], loss=119.5541
	step [124/249], loss=133.0453
	step [125/249], loss=106.7184
	step [126/249], loss=105.2291
	step [127/249], loss=102.5812
	step [128/249], loss=117.0164
	step [129/249], loss=105.2169
	step [130/249], loss=101.8994
	step [131/249], loss=108.9458
	step [132/249], loss=106.2926
	step [133/249], loss=95.6838
	step [134/249], loss=124.3552
	step [135/249], loss=118.6987
	step [136/249], loss=97.9898
	step [137/249], loss=108.3392
	step [138/249], loss=99.3268
	step [139/249], loss=122.7141
	step [140/249], loss=84.3691
	step [141/249], loss=122.0530
	step [142/249], loss=119.7652
	step [143/249], loss=96.6573
	step [144/249], loss=96.0349
	step [145/249], loss=113.9938
	step [146/249], loss=127.9302
	step [147/249], loss=100.5437
	step [148/249], loss=96.9255
	step [149/249], loss=122.3878
	step [150/249], loss=112.9165
	step [151/249], loss=88.5678
	step [152/249], loss=95.2314
	step [153/249], loss=107.4117
	step [154/249], loss=106.7524
	step [155/249], loss=111.6742
	step [156/249], loss=115.5241
	step [157/249], loss=105.4752
	step [158/249], loss=117.4043
	step [159/249], loss=98.4552
	step [160/249], loss=85.0316
	step [161/249], loss=113.0841
	step [162/249], loss=108.6890
	step [163/249], loss=95.2237
	step [164/249], loss=120.9986
	step [165/249], loss=92.8383
	step [166/249], loss=124.3138
	step [167/249], loss=123.5402
	step [168/249], loss=110.3801
	step [169/249], loss=115.3338
	step [170/249], loss=107.7042
	step [171/249], loss=110.1075
	step [172/249], loss=110.2256
	step [173/249], loss=99.3461
	step [174/249], loss=93.7552
	step [175/249], loss=105.5826
	step [176/249], loss=114.0656
	step [177/249], loss=78.7621
	step [178/249], loss=102.3285
	step [179/249], loss=104.1279
	step [180/249], loss=96.9212
	step [181/249], loss=107.8740
	step [182/249], loss=118.4550
	step [183/249], loss=94.1923
	step [184/249], loss=115.4023
	step [185/249], loss=114.8994
	step [186/249], loss=94.0302
	step [187/249], loss=98.9331
	step [188/249], loss=110.3043
	step [189/249], loss=140.8738
	step [190/249], loss=104.5111
	step [191/249], loss=96.5466
	step [192/249], loss=102.5249
	step [193/249], loss=98.5554
	step [194/249], loss=119.8032
	step [195/249], loss=127.4189
	step [196/249], loss=103.7486
	step [197/249], loss=112.9630
	step [198/249], loss=135.6649
	step [199/249], loss=108.1959
	step [200/249], loss=117.5154
	step [201/249], loss=88.3823
	step [202/249], loss=99.6639
	step [203/249], loss=112.2629
	step [204/249], loss=119.0665
	step [205/249], loss=115.3239
	step [206/249], loss=98.4167
	step [207/249], loss=104.4855
	step [208/249], loss=105.3186
	step [209/249], loss=114.4647
	step [210/249], loss=140.1160
	step [211/249], loss=95.2748
	step [212/249], loss=97.7682
	step [213/249], loss=93.3330
	step [214/249], loss=96.7659
	step [215/249], loss=106.1270
	step [216/249], loss=113.8789
	step [217/249], loss=97.0338
	step [218/249], loss=111.6327
	step [219/249], loss=101.5630
	step [220/249], loss=103.9611
	step [221/249], loss=119.3482
	step [222/249], loss=121.0493
	step [223/249], loss=108.0483
	step [224/249], loss=110.8227
	step [225/249], loss=78.4098
	step [226/249], loss=107.9173
	step [227/249], loss=115.5802
	step [228/249], loss=88.0608
	step [229/249], loss=98.4661
	step [230/249], loss=108.4963
	step [231/249], loss=94.9843
	step [232/249], loss=105.2032
	step [233/249], loss=99.4473
	step [234/249], loss=118.2434
	step [235/249], loss=119.6950
	step [236/249], loss=86.0447
	step [237/249], loss=101.7559
	step [238/249], loss=111.6693
	step [239/249], loss=93.7243
	step [240/249], loss=118.9533
	step [241/249], loss=107.2721
	step [242/249], loss=112.8848
	step [243/249], loss=106.9175
	step [244/249], loss=101.5297
	step [245/249], loss=113.7787
	step [246/249], loss=109.5668
	step [247/249], loss=121.2397
	step [248/249], loss=100.3139
	step [249/249], loss=1.5075
	Evaluating
	loss=0.0196, precision=0.4589, recall=0.8871, f1=0.6049
Training epoch 12
	step [1/249], loss=124.2264
	step [2/249], loss=103.9315
	step [3/249], loss=109.4222
	step [4/249], loss=129.6537
	step [5/249], loss=93.2908
	step [6/249], loss=110.8492
	step [7/249], loss=112.8165
	step [8/249], loss=114.4727
	step [9/249], loss=112.9543
	step [10/249], loss=94.6949
	step [11/249], loss=103.2407
	step [12/249], loss=108.8302
	step [13/249], loss=113.6485
	step [14/249], loss=97.1721
	step [15/249], loss=94.9586
	step [16/249], loss=90.4659
	step [17/249], loss=107.7902
	step [18/249], loss=103.4370
	step [19/249], loss=119.9517
	step [20/249], loss=109.8897
	step [21/249], loss=99.2289
	step [22/249], loss=113.9456
	step [23/249], loss=95.1311
	step [24/249], loss=107.9219
	step [25/249], loss=90.0370
	step [26/249], loss=117.8431
	step [27/249], loss=100.5212
	step [28/249], loss=102.8050
	step [29/249], loss=113.4333
	step [30/249], loss=94.4623
	step [31/249], loss=111.4940
	step [32/249], loss=112.3913
	step [33/249], loss=130.0331
	step [34/249], loss=115.5447
	step [35/249], loss=96.2189
	step [36/249], loss=110.4985
	step [37/249], loss=91.7174
	step [38/249], loss=106.7308
	step [39/249], loss=103.1533
	step [40/249], loss=104.5504
	step [41/249], loss=139.9742
	step [42/249], loss=122.2227
	step [43/249], loss=100.4933
	step [44/249], loss=97.4937
	step [45/249], loss=105.1290
	step [46/249], loss=99.7941
	step [47/249], loss=115.3557
	step [48/249], loss=118.5596
	step [49/249], loss=102.8242
	step [50/249], loss=110.9284
	step [51/249], loss=97.8716
	step [52/249], loss=123.1748
	step [53/249], loss=96.8566
	step [54/249], loss=104.4116
	step [55/249], loss=86.9561
	step [56/249], loss=96.3633
	step [57/249], loss=105.1269
	step [58/249], loss=104.5483
	step [59/249], loss=109.1244
	step [60/249], loss=106.3125
	step [61/249], loss=102.9575
	step [62/249], loss=114.5461
	step [63/249], loss=96.5800
	step [64/249], loss=112.4644
	step [65/249], loss=119.0697
	step [66/249], loss=90.3878
	step [67/249], loss=105.7632
	step [68/249], loss=114.2618
	step [69/249], loss=98.9285
	step [70/249], loss=104.2111
	step [71/249], loss=117.9118
	step [72/249], loss=118.1622
	step [73/249], loss=90.8524
	step [74/249], loss=111.8197
	step [75/249], loss=108.5250
	step [76/249], loss=88.2617
	step [77/249], loss=98.7850
	step [78/249], loss=102.4352
	step [79/249], loss=112.1361
	step [80/249], loss=112.1774
	step [81/249], loss=87.0018
	step [82/249], loss=102.6046
	step [83/249], loss=111.4421
	step [84/249], loss=123.3473
	step [85/249], loss=118.7503
	step [86/249], loss=95.2049
	step [87/249], loss=116.4296
	step [88/249], loss=121.5696
	step [89/249], loss=102.3885
	step [90/249], loss=100.1164
	step [91/249], loss=113.9732
	step [92/249], loss=120.7085
	step [93/249], loss=117.5518
	step [94/249], loss=97.1940
	step [95/249], loss=112.6124
	step [96/249], loss=106.1083
	step [97/249], loss=91.5691
	step [98/249], loss=118.2233
	step [99/249], loss=97.6212
	step [100/249], loss=92.2025
	step [101/249], loss=112.8630
	step [102/249], loss=92.8010
	step [103/249], loss=110.2797
	step [104/249], loss=111.3188
	step [105/249], loss=118.6460
	step [106/249], loss=115.7231
	step [107/249], loss=113.0403
	step [108/249], loss=141.7167
	step [109/249], loss=101.8808
	step [110/249], loss=116.2405
	step [111/249], loss=105.0884
	step [112/249], loss=96.2040
	step [113/249], loss=91.4915
	step [114/249], loss=100.1268
	step [115/249], loss=104.7479
	step [116/249], loss=88.3662
	step [117/249], loss=101.5991
	step [118/249], loss=113.8811
	step [119/249], loss=96.3117
	step [120/249], loss=92.5583
	step [121/249], loss=99.3596
	step [122/249], loss=86.6531
	step [123/249], loss=119.7481
	step [124/249], loss=89.0843
	step [125/249], loss=112.0919
	step [126/249], loss=90.6804
	step [127/249], loss=100.3608
	step [128/249], loss=111.5049
	step [129/249], loss=96.5874
	step [130/249], loss=91.8171
	step [131/249], loss=107.8653
	step [132/249], loss=94.8420
	step [133/249], loss=108.9496
	step [134/249], loss=99.7217
	step [135/249], loss=101.5742
	step [136/249], loss=114.9059
	step [137/249], loss=109.3695
	step [138/249], loss=114.0661
	step [139/249], loss=106.6401
	step [140/249], loss=114.0153
	step [141/249], loss=108.2798
	step [142/249], loss=99.3830
	step [143/249], loss=109.4797
	step [144/249], loss=93.2632
	step [145/249], loss=93.0350
	step [146/249], loss=113.6411
	step [147/249], loss=86.5658
	step [148/249], loss=97.1255
	step [149/249], loss=88.7324
	step [150/249], loss=129.2905
	step [151/249], loss=109.4515
	step [152/249], loss=106.0621
	step [153/249], loss=103.4629
	step [154/249], loss=104.0933
	step [155/249], loss=107.4327
	step [156/249], loss=99.1007
	step [157/249], loss=111.9955
	step [158/249], loss=95.2711
	step [159/249], loss=108.1121
	step [160/249], loss=98.8316
	step [161/249], loss=98.4376
	step [162/249], loss=102.7169
	step [163/249], loss=87.9708
	step [164/249], loss=98.4978
	step [165/249], loss=114.9415
	step [166/249], loss=83.3065
	step [167/249], loss=88.6313
	step [168/249], loss=109.3548
	step [169/249], loss=127.7807
	step [170/249], loss=113.5024
	step [171/249], loss=110.9235
	step [172/249], loss=95.0037
	step [173/249], loss=122.1786
	step [174/249], loss=113.7528
	step [175/249], loss=101.3535
	step [176/249], loss=111.7791
	step [177/249], loss=85.2065
	step [178/249], loss=108.3433
	step [179/249], loss=86.3351
	step [180/249], loss=106.5431
	step [181/249], loss=96.9042
	step [182/249], loss=113.0257
	step [183/249], loss=88.6798
	step [184/249], loss=117.6428
	step [185/249], loss=100.5731
	step [186/249], loss=109.4983
	step [187/249], loss=109.0392
	step [188/249], loss=101.3725
	step [189/249], loss=150.3083
	step [190/249], loss=114.0007
	step [191/249], loss=93.4615
	step [192/249], loss=114.7409
	step [193/249], loss=100.9461
	step [194/249], loss=117.1730
	step [195/249], loss=105.2630
	step [196/249], loss=101.7688
	step [197/249], loss=102.5001
	step [198/249], loss=112.5530
	step [199/249], loss=113.1634
	step [200/249], loss=95.6924
	step [201/249], loss=90.4850
	step [202/249], loss=89.0254
	step [203/249], loss=114.3770
	step [204/249], loss=94.2797
	step [205/249], loss=120.0132
	step [206/249], loss=86.7081
	step [207/249], loss=112.0486
	step [208/249], loss=97.9914
	step [209/249], loss=112.2987
	step [210/249], loss=107.1302
	step [211/249], loss=104.3588
	step [212/249], loss=102.8791
	step [213/249], loss=85.7698
	step [214/249], loss=100.4199
	step [215/249], loss=116.0459
	step [216/249], loss=108.9590
	step [217/249], loss=111.0368
	step [218/249], loss=119.5801
	step [219/249], loss=119.4732
	step [220/249], loss=115.0396
	step [221/249], loss=105.4826
	step [222/249], loss=103.6776
	step [223/249], loss=122.8188
	step [224/249], loss=110.4069
	step [225/249], loss=76.9598
	step [226/249], loss=87.5906
	step [227/249], loss=123.1860
	step [228/249], loss=106.2537
	step [229/249], loss=152.7394
	step [230/249], loss=141.1189
	step [231/249], loss=95.1743
	step [232/249], loss=109.2625
	step [233/249], loss=109.8164
	step [234/249], loss=131.0467
	step [235/249], loss=104.7439
	step [236/249], loss=92.4104
	step [237/249], loss=115.5390
	step [238/249], loss=134.3760
	step [239/249], loss=108.3962
	step [240/249], loss=101.0854
	step [241/249], loss=97.1840
	step [242/249], loss=121.1535
	step [243/249], loss=108.2415
	step [244/249], loss=101.0457
	step [245/249], loss=108.3409
	step [246/249], loss=119.7993
	step [247/249], loss=114.6096
	step [248/249], loss=100.5946
	step [249/249], loss=6.2909
	Evaluating
	loss=0.0179, precision=0.4188, recall=0.9149, f1=0.5746
Training epoch 13
	step [1/249], loss=97.2290
	step [2/249], loss=111.2432
	step [3/249], loss=97.4087
	step [4/249], loss=115.3053
	step [5/249], loss=102.2155
	step [6/249], loss=109.0435
	step [7/249], loss=109.9691
	step [8/249], loss=111.6666
	step [9/249], loss=94.6066
	step [10/249], loss=120.6737
	step [11/249], loss=100.9446
	step [12/249], loss=97.0659
	step [13/249], loss=100.8112
	step [14/249], loss=111.9096
	step [15/249], loss=98.8166
	step [16/249], loss=103.1441
	step [17/249], loss=104.7648
	step [18/249], loss=110.1093
	step [19/249], loss=98.6583
	step [20/249], loss=102.1090
	step [21/249], loss=105.4768
	step [22/249], loss=132.0776
	step [23/249], loss=113.8198
	step [24/249], loss=103.8259
	step [25/249], loss=123.3513
	step [26/249], loss=98.8499
	step [27/249], loss=86.0706
	step [28/249], loss=97.1525
	step [29/249], loss=102.9752
	step [30/249], loss=106.6499
	step [31/249], loss=103.7932
	step [32/249], loss=106.2949
	step [33/249], loss=99.6174
	step [34/249], loss=98.6386
	step [35/249], loss=115.7479
	step [36/249], loss=111.4102
	step [37/249], loss=123.6287
	step [38/249], loss=96.2235
	step [39/249], loss=106.3670
	step [40/249], loss=96.0905
	step [41/249], loss=98.9098
	step [42/249], loss=107.4740
	step [43/249], loss=101.4137
	step [44/249], loss=97.4085
	step [45/249], loss=113.6709
	step [46/249], loss=90.1753
	step [47/249], loss=105.9912
	step [48/249], loss=98.6615
	step [49/249], loss=100.7048
	step [50/249], loss=106.5100
	step [51/249], loss=92.9792
	step [52/249], loss=104.1153
	step [53/249], loss=110.9966
	step [54/249], loss=95.8257
	step [55/249], loss=107.2637
	step [56/249], loss=74.2783
	step [57/249], loss=91.9067
	step [58/249], loss=100.4931
	step [59/249], loss=108.1904
	step [60/249], loss=96.8388
	step [61/249], loss=100.4421
	step [62/249], loss=133.8152
	step [63/249], loss=101.5700
	step [64/249], loss=101.8637
	step [65/249], loss=96.1927
	step [66/249], loss=112.6925
	step [67/249], loss=108.4001
	step [68/249], loss=103.9679
	step [69/249], loss=110.0704
	step [70/249], loss=97.5489
	step [71/249], loss=121.3050
	step [72/249], loss=102.3659
	step [73/249], loss=122.9272
	step [74/249], loss=113.3742
	step [75/249], loss=115.2891
	step [76/249], loss=82.0072
	step [77/249], loss=103.3172
	step [78/249], loss=103.3132
	step [79/249], loss=124.6798
	step [80/249], loss=90.9666
	step [81/249], loss=126.5169
	step [82/249], loss=106.9499
	step [83/249], loss=101.8000
	step [84/249], loss=116.0301
	step [85/249], loss=114.5186
	step [86/249], loss=109.8544
	step [87/249], loss=107.5507
	step [88/249], loss=95.2559
	step [89/249], loss=105.4474
	step [90/249], loss=104.5380
	step [91/249], loss=104.5531
	step [92/249], loss=127.2029
	step [93/249], loss=104.0417
	step [94/249], loss=95.7184
	step [95/249], loss=96.9227
	step [96/249], loss=102.6750
	step [97/249], loss=93.0739
	step [98/249], loss=107.6754
	step [99/249], loss=105.1131
	step [100/249], loss=101.7449
	step [101/249], loss=102.6117
	step [102/249], loss=115.3232
	step [103/249], loss=111.4918
	step [104/249], loss=119.4395
	step [105/249], loss=118.8031
	step [106/249], loss=134.5972
	step [107/249], loss=107.1748
	step [108/249], loss=100.0240
	step [109/249], loss=98.7179
	step [110/249], loss=103.3900
	step [111/249], loss=124.6764
	step [112/249], loss=109.4981
	step [113/249], loss=100.5329
	step [114/249], loss=117.6872
	step [115/249], loss=92.0649
	step [116/249], loss=79.4709
	step [117/249], loss=106.5933
	step [118/249], loss=100.9641
	step [119/249], loss=108.9330
	step [120/249], loss=86.9267
	step [121/249], loss=85.6983
	step [122/249], loss=122.3841
	step [123/249], loss=114.6156
	step [124/249], loss=126.3264
	step [125/249], loss=98.9129
	step [126/249], loss=106.9079
	step [127/249], loss=99.7796
	step [128/249], loss=111.1837
	step [129/249], loss=108.0027
	step [130/249], loss=97.7903
	step [131/249], loss=113.4793
	step [132/249], loss=111.9662
	step [133/249], loss=92.9893
	step [134/249], loss=106.6557
	step [135/249], loss=89.6979
	step [136/249], loss=120.0028
	step [137/249], loss=102.5364
	step [138/249], loss=100.3213
	step [139/249], loss=113.6843
	step [140/249], loss=92.8760
	step [141/249], loss=101.0185
	step [142/249], loss=98.8067
	step [143/249], loss=104.2761
	step [144/249], loss=113.9333
	step [145/249], loss=100.2184
	step [146/249], loss=127.2519
	step [147/249], loss=99.7225
	step [148/249], loss=108.9267
	step [149/249], loss=113.9960
	step [150/249], loss=102.7376
	step [151/249], loss=99.0458
	step [152/249], loss=94.3454
	step [153/249], loss=105.2012
	step [154/249], loss=103.5037
	step [155/249], loss=89.9185
	step [156/249], loss=102.1499
	step [157/249], loss=101.0801
	step [158/249], loss=92.2091
	step [159/249], loss=115.3323
	step [160/249], loss=114.2228
	step [161/249], loss=102.3202
	step [162/249], loss=113.1154
	step [163/249], loss=99.7893
	step [164/249], loss=95.5044
	step [165/249], loss=94.1761
	step [166/249], loss=88.5116
	step [167/249], loss=78.6007
	step [168/249], loss=114.5134
	step [169/249], loss=117.0556
	step [170/249], loss=96.8260
	step [171/249], loss=119.3878
	step [172/249], loss=97.2740
	step [173/249], loss=97.3495
	step [174/249], loss=106.3168
	step [175/249], loss=102.5415
	step [176/249], loss=102.3834
	step [177/249], loss=104.1985
	step [178/249], loss=118.9033
	step [179/249], loss=132.2813
	step [180/249], loss=104.2666
	step [181/249], loss=105.4685
	step [182/249], loss=118.9092
	step [183/249], loss=95.4131
	step [184/249], loss=93.0115
	step [185/249], loss=118.3780
	step [186/249], loss=102.2490
	step [187/249], loss=123.6913
	step [188/249], loss=113.3594
	step [189/249], loss=104.0510
	step [190/249], loss=104.5134
	step [191/249], loss=107.2722
	step [192/249], loss=103.4874
	step [193/249], loss=113.8022
	step [194/249], loss=100.3556
	step [195/249], loss=83.0424
	step [196/249], loss=113.8408
	step [197/249], loss=89.9386
	step [198/249], loss=109.7847
	step [199/249], loss=96.3704
	step [200/249], loss=95.3826
	step [201/249], loss=89.9680
	step [202/249], loss=109.1593
	step [203/249], loss=106.9511
	step [204/249], loss=100.9855
	step [205/249], loss=94.7628
	step [206/249], loss=99.0352
	step [207/249], loss=104.7090
	step [208/249], loss=91.6099
	step [209/249], loss=73.0883
	step [210/249], loss=124.4255
	step [211/249], loss=126.9132
	step [212/249], loss=106.9031
	step [213/249], loss=92.8044
	step [214/249], loss=105.3736
	step [215/249], loss=100.7995
	step [216/249], loss=107.8235
	step [217/249], loss=106.2193
	step [218/249], loss=111.8877
	step [219/249], loss=103.0753
	step [220/249], loss=112.9442
	step [221/249], loss=109.3527
	step [222/249], loss=122.6259
	step [223/249], loss=107.7267
	step [224/249], loss=111.0135
	step [225/249], loss=94.8927
	step [226/249], loss=98.5179
	step [227/249], loss=118.9851
	step [228/249], loss=118.8946
	step [229/249], loss=118.5803
	step [230/249], loss=117.5485
	step [231/249], loss=95.2978
	step [232/249], loss=100.6171
	step [233/249], loss=86.6629
	step [234/249], loss=142.9905
	step [235/249], loss=101.0224
	step [236/249], loss=92.3102
	step [237/249], loss=109.0249
	step [238/249], loss=93.1398
	step [239/249], loss=98.3741
	step [240/249], loss=103.0039
	step [241/249], loss=111.6068
	step [242/249], loss=91.4700
	step [243/249], loss=123.9221
	step [244/249], loss=110.2489
	step [245/249], loss=94.5424
	step [246/249], loss=102.1116
	step [247/249], loss=105.1923
	step [248/249], loss=106.5261
	step [249/249], loss=3.7266
	Evaluating
	loss=0.0153, precision=0.3837, recall=0.9137, f1=0.5405
Training epoch 14
	step [1/249], loss=105.0821
	step [2/249], loss=112.3475
	step [3/249], loss=111.3895
	step [4/249], loss=111.2945
	step [5/249], loss=112.5729
	step [6/249], loss=85.2485
	step [7/249], loss=124.4953
	step [8/249], loss=102.2886
	step [9/249], loss=110.9888
	step [10/249], loss=121.9848
	step [11/249], loss=98.9177
	step [12/249], loss=101.2605
	step [13/249], loss=110.1944
	step [14/249], loss=102.8864
	step [15/249], loss=104.8558
	step [16/249], loss=97.6911
	step [17/249], loss=110.8802
	step [18/249], loss=108.9661
	step [19/249], loss=91.4608
	step [20/249], loss=120.5645
	step [21/249], loss=110.5334
	step [22/249], loss=127.6268
	step [23/249], loss=97.9119
	step [24/249], loss=114.7086
	step [25/249], loss=77.3330
	step [26/249], loss=91.3098
	step [27/249], loss=100.4518
	step [28/249], loss=103.0794
	step [29/249], loss=117.3795
	step [30/249], loss=109.2281
	step [31/249], loss=109.8186
	step [32/249], loss=96.1924
	step [33/249], loss=108.2732
	step [34/249], loss=92.0714
	step [35/249], loss=113.7991
	step [36/249], loss=105.3652
	step [37/249], loss=83.6532
	step [38/249], loss=97.6167
	step [39/249], loss=121.0773
	step [40/249], loss=104.9907
	step [41/249], loss=102.5927
	step [42/249], loss=104.9956
	step [43/249], loss=101.5063
	step [44/249], loss=84.0536
	step [45/249], loss=94.1292
	step [46/249], loss=96.6571
	step [47/249], loss=89.1770
	step [48/249], loss=86.4020
	step [49/249], loss=107.5431
	step [50/249], loss=90.9652
	step [51/249], loss=94.6037
	step [52/249], loss=98.5399
	step [53/249], loss=132.9522
	step [54/249], loss=119.5279
	step [55/249], loss=106.5413
	step [56/249], loss=136.1070
	step [57/249], loss=96.3268
	step [58/249], loss=111.5056
	step [59/249], loss=108.2359
	step [60/249], loss=109.0018
	step [61/249], loss=122.2214
	step [62/249], loss=114.6780
	step [63/249], loss=84.8044
	step [64/249], loss=120.0547
	step [65/249], loss=113.9681
	step [66/249], loss=82.5428
	step [67/249], loss=116.4753
	step [68/249], loss=98.6156
	step [69/249], loss=93.3707
	step [70/249], loss=115.6228
	step [71/249], loss=108.2078
	step [72/249], loss=126.6835
	step [73/249], loss=110.1942
	step [74/249], loss=109.4073
	step [75/249], loss=91.7682
	step [76/249], loss=106.9441
	step [77/249], loss=100.3092
	step [78/249], loss=112.5021
	step [79/249], loss=100.3757
	step [80/249], loss=94.7023
	step [81/249], loss=121.8147
	step [82/249], loss=106.8183
	step [83/249], loss=127.3096
	step [84/249], loss=85.5982
	step [85/249], loss=111.4987
	step [86/249], loss=97.1540
	step [87/249], loss=149.0882
	step [88/249], loss=83.9223
	step [89/249], loss=113.8630
	step [90/249], loss=106.8441
	step [91/249], loss=90.7050
	step [92/249], loss=98.0626
	step [93/249], loss=96.7245
	step [94/249], loss=95.6273
	step [95/249], loss=92.5900
	step [96/249], loss=101.1111
	step [97/249], loss=125.7871
	step [98/249], loss=112.2251
	step [99/249], loss=75.2161
	step [100/249], loss=97.1460
	step [101/249], loss=111.3299
	step [102/249], loss=114.5530
	step [103/249], loss=95.5216
	step [104/249], loss=100.5777
	step [105/249], loss=100.8849
	step [106/249], loss=109.0879
	step [107/249], loss=107.5730
	step [108/249], loss=106.3630
	step [109/249], loss=109.9820
	step [110/249], loss=109.2777
	step [111/249], loss=132.0905
	step [112/249], loss=97.7116
	step [113/249], loss=112.5437
	step [114/249], loss=98.4838
	step [115/249], loss=118.4936
	step [116/249], loss=111.9373
	step [117/249], loss=105.5359
	step [118/249], loss=113.6447
	step [119/249], loss=93.6226
	step [120/249], loss=93.6279
	step [121/249], loss=112.9272
	step [122/249], loss=99.4786
	step [123/249], loss=104.9752
	step [124/249], loss=100.4651
	step [125/249], loss=106.3684
	step [126/249], loss=91.8383
	step [127/249], loss=90.8338
	step [128/249], loss=105.1946
	step [129/249], loss=104.2725
	step [130/249], loss=110.6587
	step [131/249], loss=103.9643
	step [132/249], loss=106.3168
	step [133/249], loss=118.5870
	step [134/249], loss=105.8724
	step [135/249], loss=109.0314
	step [136/249], loss=102.6972
	step [137/249], loss=92.5200
	step [138/249], loss=113.3947
	step [139/249], loss=93.5017
	step [140/249], loss=109.4121
	step [141/249], loss=104.9261
	step [142/249], loss=102.4562
	step [143/249], loss=87.7767
	step [144/249], loss=86.1284
	step [145/249], loss=80.5101
	step [146/249], loss=89.9703
	step [147/249], loss=96.4183
	step [148/249], loss=88.7171
	step [149/249], loss=88.4742
	step [150/249], loss=106.1504
	step [151/249], loss=92.2926
	step [152/249], loss=99.8950
	step [153/249], loss=109.5935
	step [154/249], loss=87.0238
	step [155/249], loss=104.3829
	step [156/249], loss=104.9346
	step [157/249], loss=101.1423
	step [158/249], loss=111.9235
	step [159/249], loss=84.7714
	step [160/249], loss=83.4642
	step [161/249], loss=93.0009
	step [162/249], loss=118.5548
	step [163/249], loss=113.2277
	step [164/249], loss=119.5271
	step [165/249], loss=103.9761
	step [166/249], loss=117.5992
	step [167/249], loss=93.2277
	step [168/249], loss=90.9231
	step [169/249], loss=127.1227
	step [170/249], loss=110.0051
	step [171/249], loss=111.9194
	step [172/249], loss=85.7333
	step [173/249], loss=121.8214
	step [174/249], loss=108.4485
	step [175/249], loss=94.3665
	step [176/249], loss=108.3488
	step [177/249], loss=92.0725
	step [178/249], loss=114.9485
	step [179/249], loss=93.3336
	step [180/249], loss=117.3186
	step [181/249], loss=110.6959
	step [182/249], loss=110.4174
	step [183/249], loss=109.5190
	step [184/249], loss=95.9661
	step [185/249], loss=96.0140
	step [186/249], loss=91.2234
	step [187/249], loss=109.9384
	step [188/249], loss=83.0464
	step [189/249], loss=84.9187
	step [190/249], loss=116.9527
	step [191/249], loss=111.3258
	step [192/249], loss=101.5182
	step [193/249], loss=101.4191
	step [194/249], loss=95.8513
	step [195/249], loss=88.1241
	step [196/249], loss=99.7245
	step [197/249], loss=105.9872
	step [198/249], loss=97.5643
	step [199/249], loss=84.8122
	step [200/249], loss=77.7133
	step [201/249], loss=98.1345
	step [202/249], loss=112.3749
	step [203/249], loss=98.0606
	step [204/249], loss=110.5513
	step [205/249], loss=100.3377
	step [206/249], loss=102.7981
	step [207/249], loss=103.6429
	step [208/249], loss=87.8166
	step [209/249], loss=108.5796
	step [210/249], loss=101.7602
	step [211/249], loss=94.9626
	step [212/249], loss=101.3753
	step [213/249], loss=98.4235
	step [214/249], loss=89.2735
	step [215/249], loss=112.5426
	step [216/249], loss=101.2720
	step [217/249], loss=104.3559
	step [218/249], loss=95.2192
	step [219/249], loss=92.0125
	step [220/249], loss=92.3791
	step [221/249], loss=113.0116
	step [222/249], loss=91.4114
	step [223/249], loss=101.9294
	step [224/249], loss=103.8838
	step [225/249], loss=107.9186
	step [226/249], loss=106.0898
	step [227/249], loss=116.0410
	step [228/249], loss=111.0112
	step [229/249], loss=132.4174
	step [230/249], loss=118.1749
	step [231/249], loss=87.3654
	step [232/249], loss=101.7915
	step [233/249], loss=113.5763
	step [234/249], loss=95.9734
	step [235/249], loss=94.5229
	step [236/249], loss=115.2850
	step [237/249], loss=128.6398
	step [238/249], loss=98.5865
	step [239/249], loss=107.5874
	step [240/249], loss=105.8753
	step [241/249], loss=113.1269
	step [242/249], loss=115.8929
	step [243/249], loss=102.7386
	step [244/249], loss=121.8337
	step [245/249], loss=124.4787
	step [246/249], loss=108.2470
	step [247/249], loss=108.8817
	step [248/249], loss=95.0424
	step [249/249], loss=5.2900
	Evaluating
	loss=0.0181, precision=0.3406, recall=0.8979, f1=0.4939
Training epoch 15
	step [1/249], loss=101.9690
	step [2/249], loss=96.5431
	step [3/249], loss=118.1298
	step [4/249], loss=76.3301
	step [5/249], loss=108.3918
	step [6/249], loss=113.5365
	step [7/249], loss=104.4335
	step [8/249], loss=94.8495
	step [9/249], loss=119.3813
	step [10/249], loss=114.2054
	step [11/249], loss=105.1194
	step [12/249], loss=107.2164
	step [13/249], loss=111.1619
	step [14/249], loss=91.5313
	step [15/249], loss=108.9265
	step [16/249], loss=108.7477
	step [17/249], loss=93.6249
	step [18/249], loss=83.7932
	step [19/249], loss=107.8670
	step [20/249], loss=118.7456
	step [21/249], loss=94.4591
	step [22/249], loss=106.0460
	step [23/249], loss=100.7610
	step [24/249], loss=107.8722
	step [25/249], loss=103.1051
	step [26/249], loss=105.0721
	step [27/249], loss=102.6112
	step [28/249], loss=94.2056
	step [29/249], loss=110.1986
	step [30/249], loss=116.7408
	step [31/249], loss=111.0183
	step [32/249], loss=100.1332
	step [33/249], loss=101.1828
	step [34/249], loss=104.4989
	step [35/249], loss=108.1086
	step [36/249], loss=121.4181
	step [37/249], loss=121.0827
	step [38/249], loss=88.5845
	step [39/249], loss=77.6966
	step [40/249], loss=101.1231
	step [41/249], loss=108.1673
	step [42/249], loss=118.9376
	step [43/249], loss=100.9115
	step [44/249], loss=93.7029
	step [45/249], loss=118.6905
	step [46/249], loss=111.8679
	step [47/249], loss=103.0294
	step [48/249], loss=85.6802
	step [49/249], loss=101.2529
	step [50/249], loss=113.5960
	step [51/249], loss=82.4647
	step [52/249], loss=108.7223
	step [53/249], loss=113.3978
	step [54/249], loss=98.2640
	step [55/249], loss=96.0885
	step [56/249], loss=99.1750
	step [57/249], loss=95.3731
	step [58/249], loss=89.7812
	step [59/249], loss=109.2408
	step [60/249], loss=110.3514
	step [61/249], loss=92.9452
	step [62/249], loss=115.8704
	step [63/249], loss=96.2806
	step [64/249], loss=101.7020
	step [65/249], loss=107.1155
	step [66/249], loss=95.0234
	step [67/249], loss=114.7742
	step [68/249], loss=117.3896
	step [69/249], loss=109.6365
	step [70/249], loss=95.2655
	step [71/249], loss=96.9211
	step [72/249], loss=98.4781
	step [73/249], loss=90.6208
	step [74/249], loss=87.1161
	step [75/249], loss=97.2229
	step [76/249], loss=83.0764
	step [77/249], loss=108.0796
	step [78/249], loss=107.2898
	step [79/249], loss=102.1704
	step [80/249], loss=91.9909
	step [81/249], loss=105.7774
	step [82/249], loss=110.9925
	step [83/249], loss=120.3157
	step [84/249], loss=98.3707
	step [85/249], loss=97.1260
	step [86/249], loss=93.6050
	step [87/249], loss=116.9067
	step [88/249], loss=99.2104
	step [89/249], loss=109.8302
	step [90/249], loss=96.4728
	step [91/249], loss=103.5043
	step [92/249], loss=88.4000
	step [93/249], loss=117.6700
	step [94/249], loss=105.1636
	step [95/249], loss=100.0378
	step [96/249], loss=106.0000
	step [97/249], loss=87.4874
	step [98/249], loss=118.4441
	step [99/249], loss=96.1424
	step [100/249], loss=110.6451
	step [101/249], loss=99.3158
	step [102/249], loss=99.6551
	step [103/249], loss=113.9907
	step [104/249], loss=107.5871
	step [105/249], loss=107.7859
	step [106/249], loss=94.6822
	step [107/249], loss=105.6610
	step [108/249], loss=107.6694
	step [109/249], loss=112.0265
	step [110/249], loss=84.2921
	step [111/249], loss=94.3362
	step [112/249], loss=129.8968
	step [113/249], loss=111.5681
	step [114/249], loss=103.6077
	step [115/249], loss=91.0090
	step [116/249], loss=91.3885
	step [117/249], loss=92.7187
	step [118/249], loss=116.8988
	step [119/249], loss=95.9076
	step [120/249], loss=102.1037
	step [121/249], loss=108.6216
	step [122/249], loss=93.0722
	step [123/249], loss=106.6806
	step [124/249], loss=109.2467
	step [125/249], loss=97.2528
	step [126/249], loss=107.1036
	step [127/249], loss=120.0204
	step [128/249], loss=113.5983
	step [129/249], loss=91.9612
	step [130/249], loss=105.9030
	step [131/249], loss=91.5282
	step [132/249], loss=124.6426
	step [133/249], loss=111.2366
	step [134/249], loss=121.1925
	step [135/249], loss=112.8442
	step [136/249], loss=109.8587
	step [137/249], loss=94.9243
	step [138/249], loss=102.6230
	step [139/249], loss=91.6598
	step [140/249], loss=106.6392
	step [141/249], loss=109.4998
	step [142/249], loss=81.0921
	step [143/249], loss=96.2009
	step [144/249], loss=113.8671
	step [145/249], loss=84.3785
	step [146/249], loss=96.7743
	step [147/249], loss=100.4880
	step [148/249], loss=108.7945
	step [149/249], loss=117.7967
	step [150/249], loss=92.8248
	step [151/249], loss=96.7790
	step [152/249], loss=100.5695
	step [153/249], loss=97.8403
	step [154/249], loss=103.8203
	step [155/249], loss=102.0214
	step [156/249], loss=98.8397
	step [157/249], loss=88.3837
	step [158/249], loss=108.5135
	step [159/249], loss=88.8505
	step [160/249], loss=108.9787
	step [161/249], loss=106.2227
	step [162/249], loss=100.3072
	step [163/249], loss=110.2363
	step [164/249], loss=113.9840
	step [165/249], loss=94.7307
	step [166/249], loss=107.4205
	step [167/249], loss=89.6152
	step [168/249], loss=88.4441
	step [169/249], loss=115.0710
	step [170/249], loss=91.4714
	step [171/249], loss=98.5326
	step [172/249], loss=98.3695
	step [173/249], loss=100.1324
	step [174/249], loss=128.4895
	step [175/249], loss=105.1727
	step [176/249], loss=99.5431
	step [177/249], loss=112.6897
	step [178/249], loss=113.2955
	step [179/249], loss=88.3402
	step [180/249], loss=117.0716
	step [181/249], loss=105.5500
	step [182/249], loss=121.4115
	step [183/249], loss=77.7037
	step [184/249], loss=100.2990
	step [185/249], loss=93.7192
	step [186/249], loss=107.4354
	step [187/249], loss=105.8864
	step [188/249], loss=96.2772
	step [189/249], loss=94.6841
	step [190/249], loss=124.7141
	step [191/249], loss=119.3745
	step [192/249], loss=101.1305
	step [193/249], loss=99.2143
	step [194/249], loss=97.2720
	step [195/249], loss=93.9790
	step [196/249], loss=110.8299
	step [197/249], loss=119.7516
	step [198/249], loss=104.6428
	step [199/249], loss=116.8051
	step [200/249], loss=112.4382
	step [201/249], loss=101.9328
	step [202/249], loss=95.2579
	step [203/249], loss=91.5385
	step [204/249], loss=113.8570
	step [205/249], loss=120.7297
	step [206/249], loss=89.4371
	step [207/249], loss=101.4290
	step [208/249], loss=95.8481
	step [209/249], loss=102.6927
	step [210/249], loss=80.9784
	step [211/249], loss=93.9206
	step [212/249], loss=111.1984
	step [213/249], loss=88.6961
	step [214/249], loss=94.0734
	step [215/249], loss=85.1452
	step [216/249], loss=115.9343
	step [217/249], loss=108.6379
	step [218/249], loss=88.4315
	step [219/249], loss=112.3484
	step [220/249], loss=102.5003
	step [221/249], loss=114.6778
	step [222/249], loss=93.6444
	step [223/249], loss=102.4559
	step [224/249], loss=100.9877
	step [225/249], loss=98.7030
	step [226/249], loss=107.3190
	step [227/249], loss=112.9355
	step [228/249], loss=104.0487
	step [229/249], loss=98.3685
	step [230/249], loss=115.0548
	step [231/249], loss=91.4776
	step [232/249], loss=105.3503
	step [233/249], loss=99.2431
	step [234/249], loss=91.0101
	step [235/249], loss=95.8141
	step [236/249], loss=85.6285
	step [237/249], loss=100.9679
	step [238/249], loss=101.8368
	step [239/249], loss=99.2897
	step [240/249], loss=113.6497
	step [241/249], loss=108.4594
	step [242/249], loss=92.3418
	step [243/249], loss=114.6530
	step [244/249], loss=113.0229
	step [245/249], loss=97.1932
	step [246/249], loss=93.5617
	step [247/249], loss=90.8632
	step [248/249], loss=102.9328
	step [249/249], loss=3.6738
	Evaluating
	loss=0.0120, precision=0.4165, recall=0.8945, f1=0.5684
Training epoch 16
	step [1/249], loss=95.4590
	step [2/249], loss=108.6475
	step [3/249], loss=104.4060
	step [4/249], loss=97.4571
	step [5/249], loss=100.8216
	step [6/249], loss=101.0202
	step [7/249], loss=97.5902
	step [8/249], loss=103.2825
	step [9/249], loss=88.5753
	step [10/249], loss=104.0064
	step [11/249], loss=103.8882
	step [12/249], loss=112.0909
	step [13/249], loss=101.4045
	step [14/249], loss=98.3146
	step [15/249], loss=93.6373
	step [16/249], loss=123.2666
	step [17/249], loss=92.7647
	step [18/249], loss=123.5207
	step [19/249], loss=111.8217
	step [20/249], loss=88.1041
	step [21/249], loss=92.0322
	step [22/249], loss=90.2212
	step [23/249], loss=95.4641
	step [24/249], loss=93.6739
	step [25/249], loss=84.5514
	step [26/249], loss=115.9083
	step [27/249], loss=92.4557
	step [28/249], loss=101.1315
	step [29/249], loss=109.7022
	step [30/249], loss=109.4969
	step [31/249], loss=93.8682
	step [32/249], loss=114.1704
	step [33/249], loss=93.7060
	step [34/249], loss=102.7439
	step [35/249], loss=105.1494
	step [36/249], loss=121.1626
	step [37/249], loss=81.0564
	step [38/249], loss=102.2239
	step [39/249], loss=88.3946
	step [40/249], loss=89.2313
	step [41/249], loss=105.0224
	step [42/249], loss=107.4622
	step [43/249], loss=82.4624
	step [44/249], loss=106.4830
	step [45/249], loss=100.5191
	step [46/249], loss=128.0987
	step [47/249], loss=104.4468
	step [48/249], loss=105.9562
	step [49/249], loss=101.6698
	step [50/249], loss=82.8400
	step [51/249], loss=92.2882
	step [52/249], loss=101.7248
	step [53/249], loss=110.7631
	step [54/249], loss=96.6092
	step [55/249], loss=96.0008
	step [56/249], loss=96.1690
	step [57/249], loss=106.2440
	step [58/249], loss=114.9973
	step [59/249], loss=100.6884
	step [60/249], loss=107.1854
	step [61/249], loss=100.7947
	step [62/249], loss=103.5932
	step [63/249], loss=101.6734
	step [64/249], loss=104.7292
	step [65/249], loss=90.4106
	step [66/249], loss=98.1710
	step [67/249], loss=98.4078
	step [68/249], loss=124.0181
	step [69/249], loss=95.9416
	step [70/249], loss=109.4202
	step [71/249], loss=91.3651
	step [72/249], loss=89.1348
	step [73/249], loss=102.0259
	step [74/249], loss=102.7097
	step [75/249], loss=122.9116
	step [76/249], loss=112.6023
	step [77/249], loss=103.6907
	step [78/249], loss=100.3840
	step [79/249], loss=90.9497
	step [80/249], loss=110.2084
	step [81/249], loss=106.4789
	step [82/249], loss=94.0912
	step [83/249], loss=87.1603
	step [84/249], loss=109.2060
	step [85/249], loss=117.3300
	step [86/249], loss=112.1752
	step [87/249], loss=101.4897
	step [88/249], loss=88.5878
	step [89/249], loss=98.7432
	step [90/249], loss=93.4961
	step [91/249], loss=109.7368
	step [92/249], loss=113.9436
	step [93/249], loss=113.0891
	step [94/249], loss=100.8621
	step [95/249], loss=101.2477
	step [96/249], loss=115.9283
	step [97/249], loss=107.7453
	step [98/249], loss=120.4848
	step [99/249], loss=114.4315
	step [100/249], loss=95.2464
	step [101/249], loss=76.5265
	step [102/249], loss=98.7166
	step [103/249], loss=105.7887
	step [104/249], loss=113.8646
	step [105/249], loss=88.2644
	step [106/249], loss=96.3735
	step [107/249], loss=112.8956
	step [108/249], loss=116.4939
	step [109/249], loss=97.9717
	step [110/249], loss=95.0637
	step [111/249], loss=113.8056
	step [112/249], loss=94.0799
	step [113/249], loss=84.5390
	step [114/249], loss=102.8804
	step [115/249], loss=105.3491
	step [116/249], loss=99.2500
	step [117/249], loss=106.2635
	step [118/249], loss=106.0105
	step [119/249], loss=92.6217
	step [120/249], loss=106.7691
	step [121/249], loss=113.8732
	step [122/249], loss=102.2616
	step [123/249], loss=82.2111
	step [124/249], loss=114.0217
	step [125/249], loss=98.1776
	step [126/249], loss=97.3600
	step [127/249], loss=104.2950
	step [128/249], loss=88.7078
	step [129/249], loss=104.6599
	step [130/249], loss=103.7044
	step [131/249], loss=92.3073
	step [132/249], loss=111.7565
	step [133/249], loss=86.7496
	step [134/249], loss=84.9467
	step [135/249], loss=92.8124
	step [136/249], loss=100.5310
	step [137/249], loss=119.1639
	step [138/249], loss=102.1243
	step [139/249], loss=115.3724
	step [140/249], loss=97.8359
	step [141/249], loss=105.4180
	step [142/249], loss=110.5733
	step [143/249], loss=107.6423
	step [144/249], loss=118.8523
	step [145/249], loss=115.9943
	step [146/249], loss=122.9954
	step [147/249], loss=100.9518
	step [148/249], loss=104.7568
	step [149/249], loss=93.5730
	step [150/249], loss=97.9268
	step [151/249], loss=97.4790
	step [152/249], loss=102.6591
	step [153/249], loss=95.4267
	step [154/249], loss=95.8269
	step [155/249], loss=122.6281
	step [156/249], loss=93.5178
	step [157/249], loss=93.4491
	step [158/249], loss=99.5371
	step [159/249], loss=99.1961
	step [160/249], loss=111.1589
	step [161/249], loss=102.1491
	step [162/249], loss=96.4801
	step [163/249], loss=92.3453
	step [164/249], loss=107.0146
	step [165/249], loss=94.0095
	step [166/249], loss=113.8181
	step [167/249], loss=97.2558
	step [168/249], loss=101.2578
	step [169/249], loss=92.7140
	step [170/249], loss=109.4968
	step [171/249], loss=121.8661
	step [172/249], loss=111.0036
	step [173/249], loss=114.0516
	step [174/249], loss=111.2505
	step [175/249], loss=91.7954
	step [176/249], loss=107.4165
	step [177/249], loss=99.4573
	step [178/249], loss=99.4895
	step [179/249], loss=92.4398
	step [180/249], loss=93.1931
	step [181/249], loss=77.0022
	step [182/249], loss=100.0983
	step [183/249], loss=109.4316
	step [184/249], loss=95.3250
	step [185/249], loss=99.0337
	step [186/249], loss=109.3983
	step [187/249], loss=108.8328
	step [188/249], loss=102.0811
	step [189/249], loss=97.3855
	step [190/249], loss=87.5166
	step [191/249], loss=85.8696
	step [192/249], loss=104.9301
	step [193/249], loss=91.5195
	step [194/249], loss=120.4677
	step [195/249], loss=102.5061
	step [196/249], loss=124.6588
	step [197/249], loss=109.0184
	step [198/249], loss=102.8547
	step [199/249], loss=102.2408
	step [200/249], loss=95.2395
	step [201/249], loss=110.9619
	step [202/249], loss=106.6680
	step [203/249], loss=94.9058
	step [204/249], loss=112.6532
	step [205/249], loss=125.3994
	step [206/249], loss=105.5682
	step [207/249], loss=101.2872
	step [208/249], loss=100.6119
	step [209/249], loss=103.7082
	step [210/249], loss=109.0616
	step [211/249], loss=91.1386
	step [212/249], loss=111.1539
	step [213/249], loss=87.9192
	step [214/249], loss=96.8682
	step [215/249], loss=113.3309
	step [216/249], loss=81.7189
	step [217/249], loss=95.3144
	step [218/249], loss=92.2276
	step [219/249], loss=100.6748
	step [220/249], loss=92.4251
	step [221/249], loss=72.0017
	step [222/249], loss=94.4886
	step [223/249], loss=114.6348
	step [224/249], loss=99.9178
	step [225/249], loss=85.8404
	step [226/249], loss=89.1729
	step [227/249], loss=88.8607
	step [228/249], loss=101.7137
	step [229/249], loss=106.5252
	step [230/249], loss=99.3431
	step [231/249], loss=90.8125
	step [232/249], loss=118.1091
	step [233/249], loss=95.6767
	step [234/249], loss=100.4106
	step [235/249], loss=114.9831
	step [236/249], loss=93.7998
	step [237/249], loss=85.2201
	step [238/249], loss=84.8902
	step [239/249], loss=96.5839
	step [240/249], loss=108.0832
	step [241/249], loss=112.5127
	step [242/249], loss=78.6396
	step [243/249], loss=113.7989
	step [244/249], loss=102.9131
	step [245/249], loss=111.4879
	step [246/249], loss=90.7378
	step [247/249], loss=118.8328
	step [248/249], loss=89.2635
	step [249/249], loss=3.6857
	Evaluating
	loss=0.0117, precision=0.4322, recall=0.8827, f1=0.5803
Training epoch 17
	step [1/249], loss=102.8971
	step [2/249], loss=95.5749
	step [3/249], loss=73.4701
	step [4/249], loss=111.0926
	step [5/249], loss=100.0169
	step [6/249], loss=115.5053
	step [7/249], loss=95.9249
	step [8/249], loss=94.5284
	step [9/249], loss=113.4780
	step [10/249], loss=96.8105
	step [11/249], loss=86.4027
	step [12/249], loss=110.0752
	step [13/249], loss=105.8803
	step [14/249], loss=107.7901
	step [15/249], loss=97.6366
	step [16/249], loss=95.7243
	step [17/249], loss=95.8650
	step [18/249], loss=81.9060
	step [19/249], loss=99.9709
	step [20/249], loss=100.9078
	step [21/249], loss=102.9717
	step [22/249], loss=108.3033
	step [23/249], loss=85.0767
	step [24/249], loss=105.1525
	step [25/249], loss=108.8875
	step [26/249], loss=92.8360
	step [27/249], loss=108.1572
	step [28/249], loss=93.7146
	step [29/249], loss=109.9964
	step [30/249], loss=104.5229
	step [31/249], loss=122.0534
	step [32/249], loss=122.0106
	step [33/249], loss=96.3490
	step [34/249], loss=92.5228
	step [35/249], loss=99.0502
	step [36/249], loss=104.5607
	step [37/249], loss=93.2272
	step [38/249], loss=103.7186
	step [39/249], loss=111.6033
	step [40/249], loss=86.0458
	step [41/249], loss=109.6488
	step [42/249], loss=95.0457
	step [43/249], loss=78.7569
	step [44/249], loss=108.9472
	step [45/249], loss=93.1432
	step [46/249], loss=83.0787
	step [47/249], loss=101.8865
	step [48/249], loss=104.3107
	step [49/249], loss=91.9247
	step [50/249], loss=93.6179
	step [51/249], loss=92.8181
	step [52/249], loss=103.5867
	step [53/249], loss=121.2093
	step [54/249], loss=97.6302
	step [55/249], loss=102.3773
	step [56/249], loss=99.5328
	step [57/249], loss=93.6414
	step [58/249], loss=90.7480
	step [59/249], loss=106.8481
	step [60/249], loss=93.4927
	step [61/249], loss=91.6134
	step [62/249], loss=99.0862
	step [63/249], loss=83.7044
	step [64/249], loss=100.6901
	step [65/249], loss=95.5353
	step [66/249], loss=85.4289
	step [67/249], loss=85.8641
	step [68/249], loss=117.1023
	step [69/249], loss=125.7241
	step [70/249], loss=105.5636
	step [71/249], loss=89.9884
	step [72/249], loss=95.3900
	step [73/249], loss=107.9195
	step [74/249], loss=91.8220
	step [75/249], loss=103.8783
	step [76/249], loss=115.9641
	step [77/249], loss=95.7271
	step [78/249], loss=105.5912
	step [79/249], loss=98.7280
	step [80/249], loss=86.5429
	step [81/249], loss=122.1143
	step [82/249], loss=107.7124
	step [83/249], loss=97.2824
	step [84/249], loss=108.3702
	step [85/249], loss=101.2113
	step [86/249], loss=105.6071
	step [87/249], loss=93.8836
	step [88/249], loss=101.2226
	step [89/249], loss=119.2222
	step [90/249], loss=99.5396
	step [91/249], loss=100.1001
	step [92/249], loss=93.7521
	step [93/249], loss=93.7936
	step [94/249], loss=138.1849
	step [95/249], loss=68.4562
	step [96/249], loss=110.8421
	step [97/249], loss=105.6564
	step [98/249], loss=86.2759
	step [99/249], loss=92.4805
	step [100/249], loss=93.8490
	step [101/249], loss=90.7757
	step [102/249], loss=113.2034
	step [103/249], loss=104.8665
	step [104/249], loss=103.9795
	step [105/249], loss=109.2682
	step [106/249], loss=88.1595
	step [107/249], loss=99.6583
	step [108/249], loss=90.7060
	step [109/249], loss=88.6942
	step [110/249], loss=94.5088
	step [111/249], loss=86.2640
	step [112/249], loss=103.7686
	step [113/249], loss=97.4483
	step [114/249], loss=122.0219
	step [115/249], loss=121.8188
	step [116/249], loss=92.4276
	step [117/249], loss=103.4866
	step [118/249], loss=100.4009
	step [119/249], loss=121.7238
	step [120/249], loss=101.3458
	step [121/249], loss=100.5344
	step [122/249], loss=107.6044
	step [123/249], loss=124.6515
	step [124/249], loss=128.7208
	step [125/249], loss=126.1701
	step [126/249], loss=96.3463
	step [127/249], loss=88.6951
	step [128/249], loss=84.9065
	step [129/249], loss=92.3284
	step [130/249], loss=89.4138
	step [131/249], loss=93.7939
	step [132/249], loss=81.8483
	step [133/249], loss=109.3903
	step [134/249], loss=89.6698
	step [135/249], loss=105.6936
	step [136/249], loss=89.5571
	step [137/249], loss=101.7126
	step [138/249], loss=108.0872
	step [139/249], loss=95.9393
	step [140/249], loss=89.8873
	step [141/249], loss=85.8810
	step [142/249], loss=108.6934
	step [143/249], loss=93.2696
	step [144/249], loss=102.6390
	step [145/249], loss=106.3558
	step [146/249], loss=103.4282
	step [147/249], loss=119.0624
	step [148/249], loss=96.8215
	step [149/249], loss=86.5137
	step [150/249], loss=107.0979
	step [151/249], loss=95.5720
	step [152/249], loss=110.9772
	step [153/249], loss=106.9569
	step [154/249], loss=85.4706
	step [155/249], loss=106.6635
	step [156/249], loss=81.4091
	step [157/249], loss=87.0601
	step [158/249], loss=120.1506
	step [159/249], loss=94.1540
	step [160/249], loss=95.1843
	step [161/249], loss=99.0627
	step [162/249], loss=81.3675
	step [163/249], loss=96.7961
	step [164/249], loss=84.1821
	step [165/249], loss=88.0781
	step [166/249], loss=114.2732
	step [167/249], loss=76.6231
	step [168/249], loss=100.5135
	step [169/249], loss=91.8514
	step [170/249], loss=101.3682
	step [171/249], loss=98.3746
	step [172/249], loss=97.3563
	step [173/249], loss=90.2779
	step [174/249], loss=94.3606
	step [175/249], loss=106.1419
	step [176/249], loss=94.3118
	step [177/249], loss=106.3053
	step [178/249], loss=113.9098
	step [179/249], loss=88.9791
	step [180/249], loss=101.8691
	step [181/249], loss=105.2356
	step [182/249], loss=114.8215
	step [183/249], loss=96.4659
	step [184/249], loss=110.3270
	step [185/249], loss=104.4391
	step [186/249], loss=103.8831
	step [187/249], loss=114.5526
	step [188/249], loss=115.1749
	step [189/249], loss=92.2209
	step [190/249], loss=109.6951
	step [191/249], loss=97.3634
	step [192/249], loss=105.1971
	step [193/249], loss=110.7628
	step [194/249], loss=97.8547
	step [195/249], loss=120.9717
	step [196/249], loss=99.1907
	step [197/249], loss=95.3935
	step [198/249], loss=102.0225
	step [199/249], loss=100.6794
	step [200/249], loss=96.0881
	step [201/249], loss=108.0741
	step [202/249], loss=99.8407
	step [203/249], loss=97.7439
	step [204/249], loss=99.7135
	step [205/249], loss=93.1387
	step [206/249], loss=96.2543
	step [207/249], loss=90.1851
	step [208/249], loss=106.3908
	step [209/249], loss=94.3687
	step [210/249], loss=108.8616
	step [211/249], loss=82.8772
	step [212/249], loss=103.5938
	step [213/249], loss=110.0198
	step [214/249], loss=108.9431
	step [215/249], loss=117.2705
	step [216/249], loss=107.1348
	step [217/249], loss=107.8835
	step [218/249], loss=104.6701
	step [219/249], loss=94.8142
	step [220/249], loss=97.2896
	step [221/249], loss=88.3008
	step [222/249], loss=99.3890
	step [223/249], loss=129.8192
	step [224/249], loss=74.6781
	step [225/249], loss=82.3441
	step [226/249], loss=82.6111
	step [227/249], loss=114.1978
	step [228/249], loss=85.2760
	step [229/249], loss=112.5832
	step [230/249], loss=100.7610
	step [231/249], loss=95.0362
	step [232/249], loss=106.8877
	step [233/249], loss=111.2786
	step [234/249], loss=104.8145
	step [235/249], loss=89.3112
	step [236/249], loss=99.6288
	step [237/249], loss=111.0497
	step [238/249], loss=117.1856
	step [239/249], loss=100.7611
	step [240/249], loss=120.7306
	step [241/249], loss=94.4249
	step [242/249], loss=94.3038
	step [243/249], loss=97.4247
	step [244/249], loss=96.7631
	step [245/249], loss=85.8057
	step [246/249], loss=115.3129
	step [247/249], loss=104.9541
	step [248/249], loss=93.6529
	step [249/249], loss=6.4355
	Evaluating
	loss=0.0102, precision=0.5188, recall=0.8859, f1=0.6544
Training epoch 18
	step [1/249], loss=100.4865
	step [2/249], loss=112.3563
	step [3/249], loss=92.1992
	step [4/249], loss=119.1034
	step [5/249], loss=93.0034
	step [6/249], loss=87.6161
	step [7/249], loss=96.1264
	step [8/249], loss=100.5029
	step [9/249], loss=96.3604
	step [10/249], loss=99.1137
	step [11/249], loss=108.8161
	step [12/249], loss=101.6647
	step [13/249], loss=89.8032
	step [14/249], loss=99.9404
	step [15/249], loss=92.5484
	step [16/249], loss=82.0296
	step [17/249], loss=97.4262
	step [18/249], loss=75.3209
	step [19/249], loss=99.5136
	step [20/249], loss=90.7898
	step [21/249], loss=99.1609
	step [22/249], loss=76.8786
	step [23/249], loss=95.1451
	step [24/249], loss=102.5349
	step [25/249], loss=110.9557
	step [26/249], loss=114.7730
	step [27/249], loss=111.8690
	step [28/249], loss=103.5945
	step [29/249], loss=91.0440
	step [30/249], loss=90.7205
	step [31/249], loss=91.8471
	step [32/249], loss=77.3543
	step [33/249], loss=95.4671
	step [34/249], loss=89.0816
	step [35/249], loss=81.5440
	step [36/249], loss=97.4633
	step [37/249], loss=94.2857
	step [38/249], loss=98.5786
	step [39/249], loss=114.9100
	step [40/249], loss=89.9854
	step [41/249], loss=115.2041
	step [42/249], loss=119.6063
	step [43/249], loss=90.9415
	step [44/249], loss=99.2939
	step [45/249], loss=101.5328
	step [46/249], loss=87.3270
	step [47/249], loss=93.9586
	step [48/249], loss=109.4252
	step [49/249], loss=97.4385
	step [50/249], loss=102.5990
	step [51/249], loss=89.1136
	step [52/249], loss=106.0126
	step [53/249], loss=101.3417
	step [54/249], loss=87.1454
	step [55/249], loss=117.0555
	step [56/249], loss=108.9420
	step [57/249], loss=90.8700
	step [58/249], loss=107.6711
	step [59/249], loss=101.8377
	step [60/249], loss=100.2603
	step [61/249], loss=92.2585
	step [62/249], loss=131.9951
	step [63/249], loss=100.7857
	step [64/249], loss=104.6009
	step [65/249], loss=94.1913
	step [66/249], loss=105.1875
	step [67/249], loss=98.1839
	step [68/249], loss=107.0778
	step [69/249], loss=86.2676
	step [70/249], loss=122.2339
	step [71/249], loss=97.0048
	step [72/249], loss=98.2329
	step [73/249], loss=85.7236
	step [74/249], loss=105.5135
	step [75/249], loss=125.5103
	step [76/249], loss=98.8668
	step [77/249], loss=120.3764
	step [78/249], loss=87.5951
	step [79/249], loss=98.0954
	step [80/249], loss=95.3976
	step [81/249], loss=92.1560
	step [82/249], loss=117.9895
	step [83/249], loss=107.0726
	step [84/249], loss=113.9924
	step [85/249], loss=101.4125
	step [86/249], loss=93.3019
	step [87/249], loss=92.0123
	step [88/249], loss=91.8144
	step [89/249], loss=105.6571
	step [90/249], loss=100.9174
	step [91/249], loss=102.2377
	step [92/249], loss=101.3871
	step [93/249], loss=90.2732
	step [94/249], loss=91.3533
	step [95/249], loss=86.9106
	step [96/249], loss=106.6198
	step [97/249], loss=116.2201
	step [98/249], loss=94.4251
	step [99/249], loss=115.9622
	step [100/249], loss=112.5010
	step [101/249], loss=98.4084
	step [102/249], loss=104.5713
	step [103/249], loss=100.4025
	step [104/249], loss=99.9798
	step [105/249], loss=85.9707
	step [106/249], loss=108.7563
	step [107/249], loss=98.6980
	step [108/249], loss=100.6417
	step [109/249], loss=116.5869
	step [110/249], loss=124.0125
	step [111/249], loss=98.3781
	step [112/249], loss=91.6588
	step [113/249], loss=105.5359
	step [114/249], loss=98.3907
	step [115/249], loss=105.8038
	step [116/249], loss=91.8718
	step [117/249], loss=90.5982
	step [118/249], loss=91.8164
	step [119/249], loss=72.2621
	step [120/249], loss=107.7057
	step [121/249], loss=94.0515
	step [122/249], loss=98.3413
	step [123/249], loss=85.0777
	step [124/249], loss=112.2044
	step [125/249], loss=106.4189
	step [126/249], loss=97.3451
	step [127/249], loss=91.1158
	step [128/249], loss=117.3051
	step [129/249], loss=97.3273
	step [130/249], loss=92.5376
	step [131/249], loss=93.6663
	step [132/249], loss=91.6083
	step [133/249], loss=100.1733
	step [134/249], loss=93.8027
	step [135/249], loss=103.9922
	step [136/249], loss=101.8542
	step [137/249], loss=94.0908
	step [138/249], loss=107.0250
	step [139/249], loss=101.8194
	step [140/249], loss=113.9260
	step [141/249], loss=99.0552
	step [142/249], loss=108.4175
	step [143/249], loss=104.6895
	step [144/249], loss=87.5364
	step [145/249], loss=98.5606
	step [146/249], loss=100.1995
	step [147/249], loss=105.1162
	step [148/249], loss=99.2545
	step [149/249], loss=96.7744
	step [150/249], loss=94.7822
	step [151/249], loss=119.8804
	step [152/249], loss=118.1782
	step [153/249], loss=101.7850
	step [154/249], loss=102.3883
	step [155/249], loss=115.8606
	step [156/249], loss=103.3839
	step [157/249], loss=99.9120
	step [158/249], loss=101.6881
	step [159/249], loss=94.1131
	step [160/249], loss=106.3236
	step [161/249], loss=102.7977
	step [162/249], loss=96.8653
	step [163/249], loss=98.1816
	step [164/249], loss=110.7662
	step [165/249], loss=102.9088
	step [166/249], loss=80.3107
	step [167/249], loss=102.3333
	step [168/249], loss=89.0326
	step [169/249], loss=104.1827
	step [170/249], loss=83.4718
	step [171/249], loss=101.6389
	step [172/249], loss=107.5679
	step [173/249], loss=86.5398
	step [174/249], loss=102.7185
	step [175/249], loss=100.7926
	step [176/249], loss=92.7395
	step [177/249], loss=81.5607
	step [178/249], loss=92.4756
	step [179/249], loss=108.8102
	step [180/249], loss=92.7790
	step [181/249], loss=105.9185
	step [182/249], loss=78.6948
	step [183/249], loss=102.6249
	step [184/249], loss=78.2215
	step [185/249], loss=83.3764
	step [186/249], loss=118.5905
	step [187/249], loss=90.2205
	step [188/249], loss=108.7863
	step [189/249], loss=86.5159
	step [190/249], loss=98.6651
	step [191/249], loss=102.0461
	step [192/249], loss=91.7988
	step [193/249], loss=106.6854
	step [194/249], loss=90.9753
	step [195/249], loss=115.0306
	step [196/249], loss=88.4719
	step [197/249], loss=109.9562
	step [198/249], loss=96.8287
	step [199/249], loss=91.2872
	step [200/249], loss=85.2063
	step [201/249], loss=102.9347
	step [202/249], loss=96.7804
	step [203/249], loss=86.3626
	step [204/249], loss=98.3836
	step [205/249], loss=92.2779
	step [206/249], loss=86.2879
	step [207/249], loss=104.1452
	step [208/249], loss=87.0575
	step [209/249], loss=98.1439
	step [210/249], loss=100.8855
	step [211/249], loss=76.3180
	step [212/249], loss=99.1727
	step [213/249], loss=110.0129
	step [214/249], loss=110.4254
	step [215/249], loss=90.8148
	step [216/249], loss=91.7139
	step [217/249], loss=101.0636
	step [218/249], loss=115.9066
	step [219/249], loss=97.8776
	step [220/249], loss=107.9051
	step [221/249], loss=92.9436
	step [222/249], loss=101.9522
	step [223/249], loss=84.8437
	step [224/249], loss=94.9739
	step [225/249], loss=108.9216
	step [226/249], loss=101.0937
	step [227/249], loss=86.9868
	step [228/249], loss=98.9057
	step [229/249], loss=97.9428
	step [230/249], loss=119.1554
	step [231/249], loss=108.2793
	step [232/249], loss=95.8827
	step [233/249], loss=112.6884
	step [234/249], loss=92.8951
	step [235/249], loss=92.1013
	step [236/249], loss=96.4968
	step [237/249], loss=102.7735
	step [238/249], loss=111.0009
	step [239/249], loss=103.5115
	step [240/249], loss=97.3697
	step [241/249], loss=84.7655
	step [242/249], loss=78.8483
	step [243/249], loss=110.6077
	step [244/249], loss=96.1725
	step [245/249], loss=84.7523
	step [246/249], loss=95.2377
	step [247/249], loss=96.1524
	step [248/249], loss=81.6173
	step [249/249], loss=3.2197
	Evaluating
	loss=0.0101, precision=0.4192, recall=0.9015, f1=0.5723
Training epoch 19
	step [1/249], loss=120.3920
	step [2/249], loss=94.7154
	step [3/249], loss=81.4760
	step [4/249], loss=80.8858
	step [5/249], loss=91.3312
	step [6/249], loss=86.7579
	step [7/249], loss=114.2325
	step [8/249], loss=110.9940
	step [9/249], loss=106.6743
	step [10/249], loss=128.8228
	step [11/249], loss=91.7952
	step [12/249], loss=102.8183
	step [13/249], loss=96.2553
	step [14/249], loss=100.4246
	step [15/249], loss=103.3074
	step [16/249], loss=92.4287
	step [17/249], loss=95.9155
	step [18/249], loss=119.5169
	step [19/249], loss=91.8382
	step [20/249], loss=76.5727
	step [21/249], loss=93.2095
	step [22/249], loss=78.4316
	step [23/249], loss=99.8553
	step [24/249], loss=92.2014
	step [25/249], loss=68.0054
	step [26/249], loss=129.2261
	step [27/249], loss=97.4956
	step [28/249], loss=114.0398
	step [29/249], loss=76.0516
	step [30/249], loss=94.9786
	step [31/249], loss=87.5796
	step [32/249], loss=93.7972
	step [33/249], loss=93.4139
	step [34/249], loss=94.2188
	step [35/249], loss=104.6700
	step [36/249], loss=102.8006
	step [37/249], loss=102.6053
	step [38/249], loss=100.4278
	step [39/249], loss=107.6086
	step [40/249], loss=94.8077
	step [41/249], loss=85.0668
	step [42/249], loss=115.3135
	step [43/249], loss=88.5419
	step [44/249], loss=89.8477
	step [45/249], loss=90.0874
	step [46/249], loss=111.7820
	step [47/249], loss=97.1709
	step [48/249], loss=83.0732
	step [49/249], loss=114.9332
	step [50/249], loss=99.1423
	step [51/249], loss=97.3277
	step [52/249], loss=94.6505
	step [53/249], loss=96.0512
	step [54/249], loss=107.2243
	step [55/249], loss=106.5204
	step [56/249], loss=88.3146
	step [57/249], loss=95.3944
	step [58/249], loss=123.8670
	step [59/249], loss=97.8643
	step [60/249], loss=92.8534
	step [61/249], loss=93.6898
	step [62/249], loss=91.0856
	step [63/249], loss=86.7748
	step [64/249], loss=89.9251
	step [65/249], loss=95.0856
	step [66/249], loss=90.9396
	step [67/249], loss=117.1346
	step [68/249], loss=99.1553
	step [69/249], loss=100.7563
	step [70/249], loss=100.5534
	step [71/249], loss=110.4236
	step [72/249], loss=89.9025
	step [73/249], loss=108.1375
	step [74/249], loss=98.7316
	step [75/249], loss=110.1004
	step [76/249], loss=102.4236
	step [77/249], loss=88.5784
	step [78/249], loss=90.2955
	step [79/249], loss=99.0203
	step [80/249], loss=92.0695
	step [81/249], loss=104.1536
	step [82/249], loss=92.5139
	step [83/249], loss=104.0424
	step [84/249], loss=92.5443
	step [85/249], loss=94.9946
	step [86/249], loss=105.5308
	step [87/249], loss=97.3505
	step [88/249], loss=98.5735
	step [89/249], loss=108.3464
	step [90/249], loss=116.3905
	step [91/249], loss=98.6931
	step [92/249], loss=93.6417
	step [93/249], loss=88.8466
	step [94/249], loss=89.6413
	step [95/249], loss=89.8713
	step [96/249], loss=107.8141
	step [97/249], loss=103.4674
	step [98/249], loss=96.7668
	step [99/249], loss=107.9231
	step [100/249], loss=104.0029
	step [101/249], loss=108.9392
	step [102/249], loss=100.1553
	step [103/249], loss=77.7402
	step [104/249], loss=94.1645
	step [105/249], loss=98.2384
	step [106/249], loss=111.7417
	step [107/249], loss=94.1334
	step [108/249], loss=89.5605
	step [109/249], loss=90.5248
	step [110/249], loss=89.4113
	step [111/249], loss=111.2675
	step [112/249], loss=126.6029
	step [113/249], loss=78.5002
	step [114/249], loss=97.4567
	step [115/249], loss=101.3280
	step [116/249], loss=94.0079
	step [117/249], loss=93.9036
	step [118/249], loss=106.1726
	step [119/249], loss=95.3898
	step [120/249], loss=105.5942
	step [121/249], loss=79.9571
	step [122/249], loss=99.8722
	step [123/249], loss=77.4518
	step [124/249], loss=117.0484
	step [125/249], loss=91.0157
	step [126/249], loss=96.7242
	step [127/249], loss=96.3685
	step [128/249], loss=94.3065
	step [129/249], loss=98.1315
	step [130/249], loss=99.1376
	step [131/249], loss=105.5818
	step [132/249], loss=103.3884
	step [133/249], loss=126.7890
	step [134/249], loss=88.7721
	step [135/249], loss=82.8161
	step [136/249], loss=96.8599
	step [137/249], loss=107.2269
	step [138/249], loss=115.6612
	step [139/249], loss=90.3417
	step [140/249], loss=100.0637
	step [141/249], loss=90.9719
	step [142/249], loss=90.2153
	step [143/249], loss=98.3034
	step [144/249], loss=108.5145
	step [145/249], loss=95.1822
	step [146/249], loss=104.8083
	step [147/249], loss=83.2492
	step [148/249], loss=109.5788
	step [149/249], loss=82.4066
	step [150/249], loss=115.3896
	step [151/249], loss=83.0000
	step [152/249], loss=95.7438
	step [153/249], loss=105.5656
	step [154/249], loss=90.7101
	step [155/249], loss=119.1991
	step [156/249], loss=98.1841
	step [157/249], loss=98.4535
	step [158/249], loss=98.8382
	step [159/249], loss=98.2536
	step [160/249], loss=92.1363
	step [161/249], loss=96.9440
	step [162/249], loss=95.3611
	step [163/249], loss=94.2672
	step [164/249], loss=102.4145
	step [165/249], loss=107.4093
	step [166/249], loss=90.9090
	step [167/249], loss=90.5236
	step [168/249], loss=92.7427
	step [169/249], loss=94.1803
	step [170/249], loss=99.0518
	step [171/249], loss=89.8388
	step [172/249], loss=103.7684
	step [173/249], loss=83.5768
	step [174/249], loss=101.9696
	step [175/249], loss=107.1876
	step [176/249], loss=102.7650
	step [177/249], loss=98.5784
	step [178/249], loss=91.6509
	step [179/249], loss=86.6319
	step [180/249], loss=99.5984
	step [181/249], loss=93.7299
	step [182/249], loss=99.3686
	step [183/249], loss=110.0971
	step [184/249], loss=100.0062
	step [185/249], loss=94.5368
	step [186/249], loss=84.5238
	step [187/249], loss=79.6853
	step [188/249], loss=106.2626
	step [189/249], loss=100.2477
	step [190/249], loss=108.5308
	step [191/249], loss=92.5742
	step [192/249], loss=114.7899
	step [193/249], loss=109.4563
	step [194/249], loss=80.1425
	step [195/249], loss=85.5779
	step [196/249], loss=93.4282
	step [197/249], loss=89.0567
	step [198/249], loss=71.2173
	step [199/249], loss=99.3706
	step [200/249], loss=118.8718
	step [201/249], loss=103.3279
	step [202/249], loss=73.7921
	step [203/249], loss=96.5500
	step [204/249], loss=101.0082
	step [205/249], loss=97.4428
	step [206/249], loss=99.3282
	step [207/249], loss=83.3558
	step [208/249], loss=90.6555
	step [209/249], loss=90.8958
	step [210/249], loss=109.3611
	step [211/249], loss=104.8855
	step [212/249], loss=100.5451
	step [213/249], loss=96.6964
	step [214/249], loss=96.1256
	step [215/249], loss=83.4348
	step [216/249], loss=98.4149
	step [217/249], loss=113.8461
	step [218/249], loss=99.6878
	step [219/249], loss=100.4233
	step [220/249], loss=106.5552
	step [221/249], loss=98.2844
	step [222/249], loss=87.5574
	step [223/249], loss=102.7378
	step [224/249], loss=95.9573
	step [225/249], loss=86.8718
	step [226/249], loss=115.9202
	step [227/249], loss=105.8429
	step [228/249], loss=100.9953
	step [229/249], loss=96.9196
	step [230/249], loss=82.1203
	step [231/249], loss=89.9000
	step [232/249], loss=111.7093
	step [233/249], loss=78.4593
	step [234/249], loss=94.0061
	step [235/249], loss=114.4114
	step [236/249], loss=92.9127
	step [237/249], loss=99.1812
	step [238/249], loss=96.6825
	step [239/249], loss=101.7604
	step [240/249], loss=99.9896
	step [241/249], loss=114.0381
	step [242/249], loss=91.1016
	step [243/249], loss=95.8331
	step [244/249], loss=89.2473
	step [245/249], loss=97.5186
	step [246/249], loss=98.0294
	step [247/249], loss=103.9829
	step [248/249], loss=99.4504
	step [249/249], loss=12.4264
	Evaluating
	loss=0.0108, precision=0.3870, recall=0.8744, f1=0.5365
Training epoch 20
	step [1/249], loss=96.5553
	step [2/249], loss=102.4327
	step [3/249], loss=103.7758
	step [4/249], loss=102.9899
	step [5/249], loss=95.9022
	step [6/249], loss=93.9752
	step [7/249], loss=98.5001
	step [8/249], loss=116.5927
	step [9/249], loss=107.2128
	step [10/249], loss=90.1556
	step [11/249], loss=97.8983
	step [12/249], loss=110.8145
	step [13/249], loss=101.5554
	step [14/249], loss=107.1314
	step [15/249], loss=115.5737
	step [16/249], loss=88.9799
	step [17/249], loss=102.2337
	step [18/249], loss=95.3030
	step [19/249], loss=101.5455
	step [20/249], loss=105.2232
	step [21/249], loss=101.0535
	step [22/249], loss=94.9280
	step [23/249], loss=105.4248
	step [24/249], loss=104.1329
	step [25/249], loss=82.3602
	step [26/249], loss=87.9626
	step [27/249], loss=118.7532
	step [28/249], loss=100.7297
	step [29/249], loss=87.2657
	step [30/249], loss=95.6172
	step [31/249], loss=95.1329
	step [32/249], loss=83.6784
	step [33/249], loss=104.8493
	step [34/249], loss=79.4156
	step [35/249], loss=98.0080
	step [36/249], loss=113.1657
	step [37/249], loss=109.7492
	step [38/249], loss=114.3883
	step [39/249], loss=101.5712
	step [40/249], loss=97.6706
	step [41/249], loss=99.2139
	step [42/249], loss=93.1206
	step [43/249], loss=73.2475
	step [44/249], loss=96.5092
	step [45/249], loss=92.4680
	step [46/249], loss=90.1336
	step [47/249], loss=81.1099
	step [48/249], loss=93.2257
	step [49/249], loss=105.1425
	step [50/249], loss=87.4381
	step [51/249], loss=100.2496
	step [52/249], loss=100.4020
	step [53/249], loss=110.1673
	step [54/249], loss=104.0723
	step [55/249], loss=89.4732
	step [56/249], loss=96.8927
	step [57/249], loss=91.3981
	step [58/249], loss=86.8368
	step [59/249], loss=96.3948
	step [60/249], loss=99.5801
	step [61/249], loss=108.0149
	step [62/249], loss=88.6654
	step [63/249], loss=87.0007
	step [64/249], loss=98.1163
	step [65/249], loss=88.2974
	step [66/249], loss=86.9234
	step [67/249], loss=106.9424
	step [68/249], loss=86.4694
	step [69/249], loss=75.9130
	step [70/249], loss=111.7394
	step [71/249], loss=80.5679
	step [72/249], loss=100.2822
	step [73/249], loss=81.9259
	step [74/249], loss=92.0566
	step [75/249], loss=79.3292
	step [76/249], loss=100.1288
	step [77/249], loss=73.4292
	step [78/249], loss=88.2625
	step [79/249], loss=103.7404
	step [80/249], loss=106.1392
	step [81/249], loss=93.4083
	step [82/249], loss=84.8354
	step [83/249], loss=94.8457
	step [84/249], loss=88.4871
	step [85/249], loss=94.2493
	step [86/249], loss=98.7727
	step [87/249], loss=95.5474
	step [88/249], loss=106.3844
	step [89/249], loss=92.7219
	step [90/249], loss=81.2903
	step [91/249], loss=103.9380
	step [92/249], loss=88.0868
	step [93/249], loss=89.1513
	step [94/249], loss=104.3649
	step [95/249], loss=104.9026
	step [96/249], loss=118.3775
	step [97/249], loss=114.1699
	step [98/249], loss=85.5108
	step [99/249], loss=86.6006
	step [100/249], loss=87.4241
	step [101/249], loss=79.3624
	step [102/249], loss=89.5403
	step [103/249], loss=88.3852
	step [104/249], loss=75.6887
	step [105/249], loss=109.4332
	step [106/249], loss=115.4304
	step [107/249], loss=110.2312
	step [108/249], loss=95.0530
	step [109/249], loss=98.5872
	step [110/249], loss=92.1261
	step [111/249], loss=99.8399
	step [112/249], loss=96.0743
	step [113/249], loss=98.5129
	step [114/249], loss=116.8959
	step [115/249], loss=102.9330
	step [116/249], loss=89.7172
	step [117/249], loss=111.1240
	step [118/249], loss=101.8385
	step [119/249], loss=105.4678
	step [120/249], loss=107.2362
	step [121/249], loss=91.7413
	step [122/249], loss=118.8728
	step [123/249], loss=128.2992
	step [124/249], loss=77.1395
	step [125/249], loss=122.8407
	step [126/249], loss=111.2204
	step [127/249], loss=92.5085
	step [128/249], loss=111.7340
	step [129/249], loss=100.1960
	step [130/249], loss=93.9022
	step [131/249], loss=78.3299
	step [132/249], loss=102.1952
	step [133/249], loss=92.3916
	step [134/249], loss=109.6125
	step [135/249], loss=105.9967
	step [136/249], loss=106.6711
	step [137/249], loss=121.3451
	step [138/249], loss=110.3001
	step [139/249], loss=103.2107
	step [140/249], loss=92.4677
	step [141/249], loss=100.5030
	step [142/249], loss=99.0081
	step [143/249], loss=100.2934
	step [144/249], loss=97.3012
	step [145/249], loss=105.4360
	step [146/249], loss=91.4567
	step [147/249], loss=98.1308
	step [148/249], loss=107.2941
	step [149/249], loss=89.8507
	step [150/249], loss=90.0212
	step [151/249], loss=81.3150
	step [152/249], loss=93.5785
	step [153/249], loss=102.4055
	step [154/249], loss=92.9033
	step [155/249], loss=100.5693
	step [156/249], loss=100.9111
	step [157/249], loss=98.7252
	step [158/249], loss=120.3209
	step [159/249], loss=105.1681
	step [160/249], loss=99.8933
	step [161/249], loss=92.8533
	step [162/249], loss=102.1193
	step [163/249], loss=92.4920
	step [164/249], loss=88.2150
	step [165/249], loss=96.7768
	step [166/249], loss=94.1948
	step [167/249], loss=90.5194
	step [168/249], loss=86.4931
	step [169/249], loss=112.1220
	step [170/249], loss=98.6937
	step [171/249], loss=111.1378
	step [172/249], loss=97.4574
	step [173/249], loss=107.4591
	step [174/249], loss=105.8575
	step [175/249], loss=96.8216
	step [176/249], loss=90.7409
	step [177/249], loss=94.6687
	step [178/249], loss=98.1743
	step [179/249], loss=84.4557
	step [180/249], loss=101.0412
	step [181/249], loss=99.1377
	step [182/249], loss=84.6938
	step [183/249], loss=107.6811
	step [184/249], loss=110.8228
	step [185/249], loss=94.7578
	step [186/249], loss=78.1386
	step [187/249], loss=81.7814
	step [188/249], loss=94.8786
	step [189/249], loss=106.2090
	step [190/249], loss=88.5940
	step [191/249], loss=86.2570
	step [192/249], loss=95.3278
	step [193/249], loss=104.8471
	step [194/249], loss=101.4622
	step [195/249], loss=92.1001
	step [196/249], loss=74.0431
	step [197/249], loss=85.7262
	step [198/249], loss=95.0129
	step [199/249], loss=85.8608
	step [200/249], loss=92.8588
	step [201/249], loss=92.5150
	step [202/249], loss=113.4275
	step [203/249], loss=113.5012
	step [204/249], loss=104.6113
	step [205/249], loss=100.9310
	step [206/249], loss=97.5984
	step [207/249], loss=97.7846
	step [208/249], loss=92.4482
	step [209/249], loss=108.0002
	step [210/249], loss=109.5223
	step [211/249], loss=96.4296
	step [212/249], loss=89.4461
	step [213/249], loss=74.9074
	step [214/249], loss=90.4993
	step [215/249], loss=88.8996
	step [216/249], loss=98.5776
	step [217/249], loss=100.0894
	step [218/249], loss=92.6894
	step [219/249], loss=86.7590
	step [220/249], loss=84.3975
	step [221/249], loss=88.2749
	step [222/249], loss=88.7438
	step [223/249], loss=103.5058
	step [224/249], loss=86.5419
	step [225/249], loss=90.2956
	step [226/249], loss=91.7197
	step [227/249], loss=99.3121
	step [228/249], loss=111.5174
	step [229/249], loss=75.8686
	step [230/249], loss=97.3916
	step [231/249], loss=85.4799
	step [232/249], loss=79.2449
	step [233/249], loss=78.5299
	step [234/249], loss=97.0456
	step [235/249], loss=107.8163
	step [236/249], loss=111.1884
	step [237/249], loss=101.0835
	step [238/249], loss=96.8537
	step [239/249], loss=100.5113
	step [240/249], loss=100.2429
	step [241/249], loss=95.9904
	step [242/249], loss=99.3587
	step [243/249], loss=93.6663
	step [244/249], loss=108.1359
	step [245/249], loss=95.6617
	step [246/249], loss=94.1478
	step [247/249], loss=77.5332
	step [248/249], loss=103.6214
	step [249/249], loss=2.2444
	Evaluating
	loss=0.0102, precision=0.3616, recall=0.8730, f1=0.5114
Training epoch 21
	step [1/249], loss=108.5207
	step [2/249], loss=99.7286
	step [3/249], loss=107.2622
	step [4/249], loss=82.8448
	step [5/249], loss=113.1184
	step [6/249], loss=87.5431
	step [7/249], loss=94.1108
	step [8/249], loss=94.9101
	step [9/249], loss=87.0581
	step [10/249], loss=96.0511
	step [11/249], loss=109.1934
	step [12/249], loss=95.2989
	step [13/249], loss=87.1214
	step [14/249], loss=97.0415
	step [15/249], loss=87.6975
	step [16/249], loss=83.2727
	step [17/249], loss=96.6440
	step [18/249], loss=90.9719
	step [19/249], loss=102.7706
	step [20/249], loss=114.3786
	step [21/249], loss=101.9190
	step [22/249], loss=100.1987
	step [23/249], loss=129.9089
	step [24/249], loss=81.1695
	step [25/249], loss=106.0898
	step [26/249], loss=106.0313
	step [27/249], loss=83.1428
	step [28/249], loss=81.8158
	step [29/249], loss=106.4415
	step [30/249], loss=105.1891
	step [31/249], loss=93.2044
	step [32/249], loss=72.0426
	step [33/249], loss=89.4330
	step [34/249], loss=87.6955
	step [35/249], loss=94.9088
	step [36/249], loss=104.3054
	step [37/249], loss=100.5530
	step [38/249], loss=88.8317
	step [39/249], loss=105.4101
	step [40/249], loss=121.9488
	step [41/249], loss=92.4173
	step [42/249], loss=78.3645
	step [43/249], loss=85.4990
	step [44/249], loss=102.4206
	step [45/249], loss=98.3737
	step [46/249], loss=92.2762
	step [47/249], loss=91.5351
	step [48/249], loss=119.4771
	step [49/249], loss=86.8383
	step [50/249], loss=92.5092
	step [51/249], loss=87.4677
	step [52/249], loss=88.9730
	step [53/249], loss=128.3389
	step [54/249], loss=94.2479
	step [55/249], loss=91.7410
	step [56/249], loss=101.1906
	step [57/249], loss=81.8049
	step [58/249], loss=88.3987
	step [59/249], loss=98.3074
	step [60/249], loss=104.6413
	step [61/249], loss=98.0468
	step [62/249], loss=95.7528
	step [63/249], loss=99.3978
	step [64/249], loss=98.3759
	step [65/249], loss=85.6533
	step [66/249], loss=81.3500
	step [67/249], loss=98.4404
	step [68/249], loss=123.1918
	step [69/249], loss=88.6840
	step [70/249], loss=87.6046
	step [71/249], loss=108.4186
	step [72/249], loss=93.1115
	step [73/249], loss=94.4022
	step [74/249], loss=82.7502
	step [75/249], loss=87.0297
	step [76/249], loss=80.9030
	step [77/249], loss=116.4516
	step [78/249], loss=104.1186
	step [79/249], loss=80.4548
	step [80/249], loss=70.4323
	step [81/249], loss=98.9511
	step [82/249], loss=96.1770
	step [83/249], loss=95.8508
	step [84/249], loss=105.6363
	step [85/249], loss=86.9156
	step [86/249], loss=97.2361
	step [87/249], loss=99.0468
	step [88/249], loss=79.9526
	step [89/249], loss=106.8993
	step [90/249], loss=110.6211
	step [91/249], loss=99.5983
	step [92/249], loss=120.6430
	step [93/249], loss=74.0296
	step [94/249], loss=108.5363
	step [95/249], loss=95.1825
	step [96/249], loss=86.8649
	step [97/249], loss=100.2894
	step [98/249], loss=89.1561
	step [99/249], loss=87.2302
	step [100/249], loss=84.1500
	step [101/249], loss=86.7234
	step [102/249], loss=80.2770
	step [103/249], loss=101.0098
	step [104/249], loss=88.6032
	step [105/249], loss=85.8957
	step [106/249], loss=93.3274
	step [107/249], loss=109.9318
	step [108/249], loss=88.5263
	step [109/249], loss=116.3342
	step [110/249], loss=103.7285
	step [111/249], loss=96.2395
	step [112/249], loss=82.0852
	step [113/249], loss=86.1151
	step [114/249], loss=93.2432
	step [115/249], loss=97.7962
	step [116/249], loss=103.3023
	step [117/249], loss=91.7965
	step [118/249], loss=81.8792
	step [119/249], loss=82.0145
	step [120/249], loss=104.6500
	step [121/249], loss=96.9528
	step [122/249], loss=96.0844
	step [123/249], loss=101.0734
	step [124/249], loss=87.7105
	step [125/249], loss=97.2918
	step [126/249], loss=88.5224
	step [127/249], loss=94.4098
	step [128/249], loss=104.9907
	step [129/249], loss=111.1621
	step [130/249], loss=94.4547
	step [131/249], loss=99.8527
	step [132/249], loss=123.0150
	step [133/249], loss=88.0455
	step [134/249], loss=109.8650
	step [135/249], loss=89.8019
	step [136/249], loss=84.4312
	step [137/249], loss=94.5129
	step [138/249], loss=98.0192
	step [139/249], loss=83.0174
	step [140/249], loss=106.6151
	step [141/249], loss=88.5106
	step [142/249], loss=78.0794
	step [143/249], loss=105.7399
	step [144/249], loss=96.8495
	step [145/249], loss=88.9693
	step [146/249], loss=86.5113
	step [147/249], loss=90.1994
	step [148/249], loss=88.0493
	step [149/249], loss=90.9878
	step [150/249], loss=99.9514
	step [151/249], loss=90.7544
	step [152/249], loss=109.7318
	step [153/249], loss=75.1805
	step [154/249], loss=81.6865
	step [155/249], loss=104.8339
	step [156/249], loss=72.1236
	step [157/249], loss=93.5406
	step [158/249], loss=120.7190
	step [159/249], loss=92.5674
	step [160/249], loss=91.4800
	step [161/249], loss=98.3374
	step [162/249], loss=97.3875
	step [163/249], loss=101.6834
	step [164/249], loss=101.3240
	step [165/249], loss=78.9335
	step [166/249], loss=94.9494
	step [167/249], loss=120.6569
	step [168/249], loss=97.5701
	step [169/249], loss=89.5463
	step [170/249], loss=65.5005
	step [171/249], loss=103.2213
	step [172/249], loss=86.7821
	step [173/249], loss=110.9657
	step [174/249], loss=108.3432
	step [175/249], loss=96.8311
	step [176/249], loss=111.1833
	step [177/249], loss=101.0145
	step [178/249], loss=111.4339
	step [179/249], loss=88.2593
	step [180/249], loss=115.5836
	step [181/249], loss=99.4521
	step [182/249], loss=104.7958
	step [183/249], loss=95.7778
	step [184/249], loss=106.1599
	step [185/249], loss=89.8820
	step [186/249], loss=102.4749
	step [187/249], loss=115.4474
	step [188/249], loss=92.3475
	step [189/249], loss=92.9743
	step [190/249], loss=92.4260
	step [191/249], loss=109.3186
	step [192/249], loss=86.9717
	step [193/249], loss=92.1563
	step [194/249], loss=94.7878
	step [195/249], loss=91.9854
	step [196/249], loss=99.5333
	step [197/249], loss=90.4132
	step [198/249], loss=92.4063
	step [199/249], loss=89.0268
	step [200/249], loss=84.3002
	step [201/249], loss=80.6243
	step [202/249], loss=100.0702
	step [203/249], loss=89.6428
	step [204/249], loss=112.3997
	step [205/249], loss=96.8377
	step [206/249], loss=82.7592
	step [207/249], loss=106.0907
	step [208/249], loss=83.9734
	step [209/249], loss=99.3736
	step [210/249], loss=94.4273
	step [211/249], loss=84.0943
	step [212/249], loss=76.9196
	step [213/249], loss=98.8283
	step [214/249], loss=121.8991
	step [215/249], loss=88.5285
	step [216/249], loss=103.5371
	step [217/249], loss=84.5642
	step [218/249], loss=110.2583
	step [219/249], loss=114.4283
	step [220/249], loss=104.8852
	step [221/249], loss=110.9951
	step [222/249], loss=91.1587
	step [223/249], loss=92.8759
	step [224/249], loss=93.7710
	step [225/249], loss=92.8413
	step [226/249], loss=120.1450
	step [227/249], loss=100.3754
	step [228/249], loss=105.2763
	step [229/249], loss=82.1551
	step [230/249], loss=112.9917
	step [231/249], loss=87.3855
	step [232/249], loss=95.6614
	step [233/249], loss=84.9399
	step [234/249], loss=74.8804
	step [235/249], loss=107.9209
	step [236/249], loss=99.7394
	step [237/249], loss=90.3762
	step [238/249], loss=98.9755
	step [239/249], loss=102.0905
	step [240/249], loss=100.6453
	step [241/249], loss=99.4926
	step [242/249], loss=84.7866
	step [243/249], loss=97.8314
	step [244/249], loss=97.8422
	step [245/249], loss=90.3848
	step [246/249], loss=102.3902
	step [247/249], loss=82.4214
	step [248/249], loss=98.3968
	step [249/249], loss=2.4421
	Evaluating
	loss=0.0102, precision=0.3325, recall=0.9017, f1=0.4858
Training epoch 22
	step [1/249], loss=97.7363
	step [2/249], loss=75.6703
	step [3/249], loss=93.5761
	step [4/249], loss=99.8365
	step [5/249], loss=85.7840
	step [6/249], loss=100.1714
	step [7/249], loss=88.0196
	step [8/249], loss=103.8440
	step [9/249], loss=90.8723
	step [10/249], loss=104.5739
	step [11/249], loss=95.9698
	step [12/249], loss=91.8385
	step [13/249], loss=97.6240
	step [14/249], loss=91.2706
	step [15/249], loss=97.4053
	step [16/249], loss=76.9558
	step [17/249], loss=109.6287
	step [18/249], loss=97.5916
	step [19/249], loss=89.9253
	step [20/249], loss=93.7169
	step [21/249], loss=110.0290
	step [22/249], loss=99.0755
	step [23/249], loss=82.3797
	step [24/249], loss=110.7407
	step [25/249], loss=95.1744
	step [26/249], loss=102.2432
	step [27/249], loss=93.8900
	step [28/249], loss=96.1792
	step [29/249], loss=88.5882
	step [30/249], loss=98.4742
	step [31/249], loss=90.0294
	step [32/249], loss=109.4060
	step [33/249], loss=83.8589
	step [34/249], loss=97.5710
	step [35/249], loss=98.1495
	step [36/249], loss=106.8005
	step [37/249], loss=104.3551
	step [38/249], loss=81.6376
	step [39/249], loss=104.0551
	step [40/249], loss=94.3457
	step [41/249], loss=99.1814
	step [42/249], loss=106.4253
	step [43/249], loss=90.7647
	step [44/249], loss=87.0798
	step [45/249], loss=81.1162
	step [46/249], loss=84.0415
	step [47/249], loss=97.7056
	step [48/249], loss=84.8059
	step [49/249], loss=102.9992
	step [50/249], loss=97.2417
	step [51/249], loss=89.0520
	step [52/249], loss=93.4295
	step [53/249], loss=90.3252
	step [54/249], loss=90.7730
	step [55/249], loss=105.3499
	step [56/249], loss=108.9872
	step [57/249], loss=102.0438
	step [58/249], loss=95.4683
	step [59/249], loss=94.0420
	step [60/249], loss=107.7054
	step [61/249], loss=90.7746
	step [62/249], loss=92.5679
	step [63/249], loss=91.6501
	step [64/249], loss=86.7983
	step [65/249], loss=97.5465
	step [66/249], loss=92.6523
	step [67/249], loss=82.5866
	step [68/249], loss=99.4450
	step [69/249], loss=84.5611
	step [70/249], loss=87.7189
	step [71/249], loss=104.8984
	step [72/249], loss=93.9295
	step [73/249], loss=88.4004
	step [74/249], loss=88.8785
	step [75/249], loss=88.8842
	step [76/249], loss=95.5149
	step [77/249], loss=100.2827
	step [78/249], loss=103.2322
	step [79/249], loss=88.2563
	step [80/249], loss=79.1073
	step [81/249], loss=94.1998
	step [82/249], loss=96.6602
	step [83/249], loss=111.9911
	step [84/249], loss=86.8275
	step [85/249], loss=96.0578
	step [86/249], loss=82.6920
	step [87/249], loss=98.3016
	step [88/249], loss=90.0789
	step [89/249], loss=100.3477
	step [90/249], loss=99.5367
	step [91/249], loss=94.4246
	step [92/249], loss=97.5409
	step [93/249], loss=102.0026
	step [94/249], loss=90.7636
	step [95/249], loss=94.3432
	step [96/249], loss=93.5463
	step [97/249], loss=108.0300
	step [98/249], loss=87.1222
	step [99/249], loss=89.7822
	step [100/249], loss=97.7638
	step [101/249], loss=101.5419
	step [102/249], loss=98.2437
	step [103/249], loss=103.4976
	step [104/249], loss=85.2163
	step [105/249], loss=96.8109
	step [106/249], loss=102.5003
	step [107/249], loss=99.1067
	step [108/249], loss=92.5658
	step [109/249], loss=85.1426
	step [110/249], loss=91.9555
	step [111/249], loss=83.8257
	step [112/249], loss=81.0582
	step [113/249], loss=72.4253
	step [114/249], loss=111.4793
	step [115/249], loss=90.0268
	step [116/249], loss=82.9603
	step [117/249], loss=84.7673
	step [118/249], loss=87.7677
	step [119/249], loss=103.1317
	step [120/249], loss=92.4856
	step [121/249], loss=106.1232
	step [122/249], loss=78.5362
	step [123/249], loss=102.1120
	step [124/249], loss=103.7580
	step [125/249], loss=112.8103
	step [126/249], loss=100.9200
	step [127/249], loss=105.2220
	step [128/249], loss=85.2454
	step [129/249], loss=94.9114
	step [130/249], loss=89.3949
	step [131/249], loss=115.2906
	step [132/249], loss=94.0355
	step [133/249], loss=105.5213
	step [134/249], loss=111.5100
	step [135/249], loss=93.5951
	step [136/249], loss=80.5405
	step [137/249], loss=80.7998
	step [138/249], loss=95.1335
	step [139/249], loss=93.8000
	step [140/249], loss=82.6658
	step [141/249], loss=91.3474
	step [142/249], loss=102.0424
	step [143/249], loss=92.5430
	step [144/249], loss=100.0550
	step [145/249], loss=118.7587
	step [146/249], loss=84.1844
	step [147/249], loss=81.1649
	step [148/249], loss=105.9362
	step [149/249], loss=119.8993
	step [150/249], loss=100.0089
	step [151/249], loss=93.0314
	step [152/249], loss=99.8857
	step [153/249], loss=92.3587
	step [154/249], loss=83.8547
	step [155/249], loss=98.7177
	step [156/249], loss=98.5719
	step [157/249], loss=79.7206
	step [158/249], loss=87.1489
	step [159/249], loss=110.1551
	step [160/249], loss=115.7851
	step [161/249], loss=87.4379
	step [162/249], loss=98.1882
	step [163/249], loss=102.9548
	step [164/249], loss=113.2834
	step [165/249], loss=72.2833
	step [166/249], loss=83.5840
	step [167/249], loss=99.9090
	step [168/249], loss=93.8467
	step [169/249], loss=88.7310
	step [170/249], loss=86.7410
	step [171/249], loss=102.5884
	step [172/249], loss=93.7312
	step [173/249], loss=106.3188
	step [174/249], loss=95.5592
	step [175/249], loss=109.0869
	step [176/249], loss=99.5235
	step [177/249], loss=90.7336
	step [178/249], loss=109.0937
	step [179/249], loss=96.1373
	step [180/249], loss=105.8275
	step [181/249], loss=98.1031
	step [182/249], loss=101.8197
	step [183/249], loss=106.8503
	step [184/249], loss=93.7673
	step [185/249], loss=101.4090
	step [186/249], loss=91.3043
	step [187/249], loss=104.5841
	step [188/249], loss=87.2849
	step [189/249], loss=118.3180
	step [190/249], loss=102.7103
	step [191/249], loss=100.3520
	step [192/249], loss=84.6158
	step [193/249], loss=103.8577
	step [194/249], loss=116.6937
	step [195/249], loss=79.6938
	step [196/249], loss=104.6031
	step [197/249], loss=111.7759
	step [198/249], loss=88.9131
	step [199/249], loss=83.0459
	step [200/249], loss=85.9045
	step [201/249], loss=129.4849
	step [202/249], loss=78.2597
	step [203/249], loss=105.1158
	step [204/249], loss=82.4388
	step [205/249], loss=102.7866
	step [206/249], loss=85.8010
	step [207/249], loss=92.4261
	step [208/249], loss=93.1776
	step [209/249], loss=97.2947
	step [210/249], loss=88.8398
	step [211/249], loss=88.4865
	step [212/249], loss=108.3225
	step [213/249], loss=90.2903
	step [214/249], loss=83.9333
	step [215/249], loss=101.6498
	step [216/249], loss=95.4384
	step [217/249], loss=90.7879
	step [218/249], loss=90.2261
	step [219/249], loss=76.2225
	step [220/249], loss=90.0056
	step [221/249], loss=85.1692
	step [222/249], loss=85.0431
	step [223/249], loss=112.3859
	step [224/249], loss=94.0490
	step [225/249], loss=85.8244
	step [226/249], loss=84.6686
	step [227/249], loss=97.1748
	step [228/249], loss=83.6274
	step [229/249], loss=97.5465
	step [230/249], loss=83.3071
	step [231/249], loss=102.0294
	step [232/249], loss=88.2607
	step [233/249], loss=89.3020
	step [234/249], loss=97.0890
	step [235/249], loss=82.0122
	step [236/249], loss=111.5536
	step [237/249], loss=96.9966
	step [238/249], loss=75.5420
	step [239/249], loss=96.7523
	step [240/249], loss=79.9218
	step [241/249], loss=82.8739
	step [242/249], loss=93.4967
	step [243/249], loss=94.9028
	step [244/249], loss=82.2310
	step [245/249], loss=101.9501
	step [246/249], loss=103.8374
	step [247/249], loss=102.5390
	step [248/249], loss=89.7837
	step [249/249], loss=7.9081
	Evaluating
	loss=0.0110, precision=0.3184, recall=0.8955, f1=0.4698
Training epoch 23
	step [1/249], loss=135.4617
	step [2/249], loss=112.2724
	step [3/249], loss=110.1604
	step [4/249], loss=103.5454
	step [5/249], loss=99.0943
	step [6/249], loss=107.0033
	step [7/249], loss=103.2252
	step [8/249], loss=91.9280
	step [9/249], loss=107.1150
	step [10/249], loss=121.4890
	step [11/249], loss=97.7739
	step [12/249], loss=86.4027
	step [13/249], loss=118.0884
	step [14/249], loss=99.0490
	step [15/249], loss=81.6064
	step [16/249], loss=80.3993
	step [17/249], loss=104.9448
	step [18/249], loss=77.5265
	step [19/249], loss=119.5142
	step [20/249], loss=89.1736
	step [21/249], loss=66.4418
	step [22/249], loss=105.0655
	step [23/249], loss=104.7382
	step [24/249], loss=98.0060
	step [25/249], loss=113.0929
	step [26/249], loss=96.1272
	step [27/249], loss=73.0292
	step [28/249], loss=97.5644
	step [29/249], loss=96.1129
	step [30/249], loss=101.6887
	step [31/249], loss=88.0012
	step [32/249], loss=108.1423
	step [33/249], loss=85.7924
	step [34/249], loss=86.4593
	step [35/249], loss=82.7629
	step [36/249], loss=94.0767
	step [37/249], loss=66.6652
	step [38/249], loss=118.4418
	step [39/249], loss=103.3827
	step [40/249], loss=92.6134
	step [41/249], loss=85.9975
	step [42/249], loss=91.9298
	step [43/249], loss=106.5069
	step [44/249], loss=91.7863
	step [45/249], loss=99.8168
	step [46/249], loss=84.6552
	step [47/249], loss=94.3106
	step [48/249], loss=89.5552
	step [49/249], loss=81.6696
	step [50/249], loss=94.2559
	step [51/249], loss=107.4137
	step [52/249], loss=92.8735
	step [53/249], loss=107.0259
	step [54/249], loss=95.5481
	step [55/249], loss=87.5775
	step [56/249], loss=103.1389
	step [57/249], loss=91.6680
	step [58/249], loss=81.4120
	step [59/249], loss=105.2400
	step [60/249], loss=102.3602
	step [61/249], loss=75.2354
	step [62/249], loss=94.3766
	step [63/249], loss=98.4639
	step [64/249], loss=113.7520
	step [65/249], loss=115.8383
	step [66/249], loss=103.5740
	step [67/249], loss=90.7577
	step [68/249], loss=92.5793
	step [69/249], loss=83.1213
	step [70/249], loss=85.5794
	step [71/249], loss=97.2512
	step [72/249], loss=82.0417
	step [73/249], loss=89.0552
	step [74/249], loss=81.1790
	step [75/249], loss=112.1175
	step [76/249], loss=76.8579
	step [77/249], loss=97.7102
	step [78/249], loss=90.1652
	step [79/249], loss=82.8023
	step [80/249], loss=103.5030
	step [81/249], loss=107.0676
	step [82/249], loss=95.2841
	step [83/249], loss=74.7154
	step [84/249], loss=88.3150
	step [85/249], loss=106.3415
	step [86/249], loss=95.0155
	step [87/249], loss=83.5698
	step [88/249], loss=79.8747
	step [89/249], loss=85.0521
	step [90/249], loss=86.2818
	step [91/249], loss=96.3627
	step [92/249], loss=96.2137
	step [93/249], loss=75.0126
	step [94/249], loss=116.7545
	step [95/249], loss=83.4724
	step [96/249], loss=87.2846
	step [97/249], loss=111.2551
	step [98/249], loss=90.2703
	step [99/249], loss=97.7733
	step [100/249], loss=89.7859
	step [101/249], loss=76.9374
	step [102/249], loss=80.2823
	step [103/249], loss=105.2505
	step [104/249], loss=80.3708
	step [105/249], loss=91.1473
	step [106/249], loss=104.3693
	step [107/249], loss=81.9433
	step [108/249], loss=84.0253
	step [109/249], loss=92.8970
	step [110/249], loss=82.2948
	step [111/249], loss=90.2432
	step [112/249], loss=104.9710
	step [113/249], loss=78.8354
	step [114/249], loss=83.8115
	step [115/249], loss=76.1711
	step [116/249], loss=95.3981
	step [117/249], loss=79.7351
	step [118/249], loss=97.4402
	step [119/249], loss=105.7922
	step [120/249], loss=87.5631
	step [121/249], loss=101.2683
	step [122/249], loss=92.7059
	step [123/249], loss=105.4123
	step [124/249], loss=89.5437
	step [125/249], loss=91.4714
	step [126/249], loss=110.0544
	step [127/249], loss=103.2170
	step [128/249], loss=97.5263
	step [129/249], loss=102.2418
	step [130/249], loss=95.0622
	step [131/249], loss=112.0482
	step [132/249], loss=89.5280
	step [133/249], loss=75.7762
	step [134/249], loss=99.2292
	step [135/249], loss=86.5073
	step [136/249], loss=90.6352
	step [137/249], loss=83.6358
	step [138/249], loss=103.5444
	step [139/249], loss=98.5714
	step [140/249], loss=90.2052
	step [141/249], loss=84.4945
	step [142/249], loss=100.1270
	step [143/249], loss=89.1725
	step [144/249], loss=98.8500
	step [145/249], loss=94.7237
	step [146/249], loss=102.4940
	step [147/249], loss=70.8646
	step [148/249], loss=101.4802
	step [149/249], loss=78.1634
	step [150/249], loss=82.1868
	step [151/249], loss=88.4851
	step [152/249], loss=94.9550
	step [153/249], loss=90.5896
	step [154/249], loss=99.0799
	step [155/249], loss=75.2215
	step [156/249], loss=99.7771
	step [157/249], loss=69.4137
	step [158/249], loss=81.3425
	step [159/249], loss=104.6143
	step [160/249], loss=99.5696
	step [161/249], loss=90.3960
	step [162/249], loss=77.9882
	step [163/249], loss=81.5859
	step [164/249], loss=92.7224
	step [165/249], loss=87.1942
	step [166/249], loss=108.6076
	step [167/249], loss=81.7202
	step [168/249], loss=102.6977
	step [169/249], loss=94.1938
	step [170/249], loss=100.5974
	step [171/249], loss=81.3631
	step [172/249], loss=88.3271
	step [173/249], loss=76.6140
	step [174/249], loss=78.2916
	step [175/249], loss=107.2059
	step [176/249], loss=97.1357
	step [177/249], loss=76.1458
	step [178/249], loss=103.4230
	step [179/249], loss=76.5671
	step [180/249], loss=95.3381
	step [181/249], loss=93.9219
	step [182/249], loss=95.8677
	step [183/249], loss=82.7803
	step [184/249], loss=89.2264
	step [185/249], loss=98.7715
	step [186/249], loss=90.4012
	step [187/249], loss=106.3178
	step [188/249], loss=98.1181
	step [189/249], loss=108.4369
	step [190/249], loss=72.9607
	step [191/249], loss=89.5984
	step [192/249], loss=95.6767
	step [193/249], loss=72.0195
	step [194/249], loss=104.3335
	step [195/249], loss=113.7030
	step [196/249], loss=81.4151
	step [197/249], loss=109.8883
	step [198/249], loss=82.4705
	step [199/249], loss=109.5732
	step [200/249], loss=94.3888
	step [201/249], loss=84.7749
	step [202/249], loss=79.8464
	step [203/249], loss=104.7003
	step [204/249], loss=82.7696
	step [205/249], loss=89.4982
	step [206/249], loss=83.3438
	step [207/249], loss=99.4624
	step [208/249], loss=87.5520
	step [209/249], loss=96.4201
	step [210/249], loss=110.4146
	step [211/249], loss=105.3938
	step [212/249], loss=99.6659
	step [213/249], loss=98.3467
	step [214/249], loss=97.6991
	step [215/249], loss=95.7239
	step [216/249], loss=104.0893
	step [217/249], loss=88.3302
	step [218/249], loss=86.7202
	step [219/249], loss=96.0991
	step [220/249], loss=86.6583
	step [221/249], loss=93.6251
	step [222/249], loss=82.7206
	step [223/249], loss=106.1153
	step [224/249], loss=101.8595
	step [225/249], loss=111.6408
	step [226/249], loss=89.4052
	step [227/249], loss=86.6450
	step [228/249], loss=88.6747
	step [229/249], loss=96.5319
	step [230/249], loss=91.5263
	step [231/249], loss=103.2904
	step [232/249], loss=102.4779
	step [233/249], loss=76.7183
	step [234/249], loss=99.5236
	step [235/249], loss=104.3038
	step [236/249], loss=98.4365
	step [237/249], loss=92.8069
	step [238/249], loss=99.0175
	step [239/249], loss=81.4556
	step [240/249], loss=82.1338
	step [241/249], loss=120.9399
	step [242/249], loss=98.3446
	step [243/249], loss=105.7453
	step [244/249], loss=77.2709
	step [245/249], loss=90.5143
	step [246/249], loss=82.7901
	step [247/249], loss=96.2419
	step [248/249], loss=89.8859
	step [249/249], loss=3.1194
	Evaluating
	loss=0.0079, precision=0.4523, recall=0.8727, f1=0.5958
Training epoch 24
	step [1/249], loss=83.5905
	step [2/249], loss=86.4060
	step [3/249], loss=101.8462
	step [4/249], loss=87.2695
	step [5/249], loss=92.3156
	step [6/249], loss=99.0909
	step [7/249], loss=98.7529
	step [8/249], loss=95.1026
	step [9/249], loss=84.5083
	step [10/249], loss=103.4900
	step [11/249], loss=91.3849
	step [12/249], loss=90.0631
	step [13/249], loss=98.9203
	step [14/249], loss=110.5108
	step [15/249], loss=92.5864
	step [16/249], loss=88.4502
	step [17/249], loss=101.2044
	step [18/249], loss=102.1028
	step [19/249], loss=96.4677
	step [20/249], loss=100.1828
	step [21/249], loss=80.1613
	step [22/249], loss=76.9136
	step [23/249], loss=71.5811
	step [24/249], loss=77.3814
	step [25/249], loss=95.2555
	step [26/249], loss=87.1542
	step [27/249], loss=96.8685
	step [28/249], loss=102.4258
	step [29/249], loss=120.4346
	step [30/249], loss=103.3051
	step [31/249], loss=85.8913
	step [32/249], loss=91.9935
	step [33/249], loss=94.2392
	step [34/249], loss=94.5001
	step [35/249], loss=94.8760
	step [36/249], loss=101.1800
	step [37/249], loss=80.8679
	step [38/249], loss=91.4626
	step [39/249], loss=95.6921
	step [40/249], loss=90.9969
	step [41/249], loss=94.9983
	step [42/249], loss=93.2182
	step [43/249], loss=79.5642
	step [44/249], loss=94.7693
	step [45/249], loss=109.7084
	step [46/249], loss=94.2834
	step [47/249], loss=86.2530
	step [48/249], loss=83.0325
	step [49/249], loss=73.8579
	step [50/249], loss=84.1657
	step [51/249], loss=98.6635
	step [52/249], loss=121.6904
	step [53/249], loss=102.3682
	step [54/249], loss=97.6779
	step [55/249], loss=104.1887
	step [56/249], loss=89.1481
	step [57/249], loss=108.4113
	step [58/249], loss=87.6589
	step [59/249], loss=110.5120
	step [60/249], loss=87.6914
	step [61/249], loss=93.4308
	step [62/249], loss=88.3025
	step [63/249], loss=106.4587
	step [64/249], loss=117.1402
	step [65/249], loss=93.8686
	step [66/249], loss=90.9900
	step [67/249], loss=86.2801
	step [68/249], loss=103.2880
	step [69/249], loss=89.1679
	step [70/249], loss=92.0043
	step [71/249], loss=87.8901
	step [72/249], loss=92.2969
	step [73/249], loss=76.9401
	step [74/249], loss=72.3243
	step [75/249], loss=82.9157
	step [76/249], loss=103.1067
	step [77/249], loss=81.6270
	step [78/249], loss=75.4568
	step [79/249], loss=92.7557
	step [80/249], loss=91.1563
	step [81/249], loss=80.6103
	step [82/249], loss=110.9555
	step [83/249], loss=90.9954
	step [84/249], loss=124.7807
	step [85/249], loss=86.9742
	step [86/249], loss=81.5948
	step [87/249], loss=97.6731
	step [88/249], loss=114.5585
	step [89/249], loss=88.6001
	step [90/249], loss=71.0865
	step [91/249], loss=93.5279
	step [92/249], loss=111.4038
	step [93/249], loss=79.1121
	step [94/249], loss=106.3561
	step [95/249], loss=90.0545
	step [96/249], loss=96.2064
	step [97/249], loss=89.3459
	step [98/249], loss=95.6759
	step [99/249], loss=90.4274
	step [100/249], loss=89.8487
	step [101/249], loss=93.1022
	step [102/249], loss=75.2812
	step [103/249], loss=76.1402
	step [104/249], loss=87.0464
	step [105/249], loss=94.0107
	step [106/249], loss=114.0600
	step [107/249], loss=80.4147
	step [108/249], loss=76.8737
	step [109/249], loss=81.2841
	step [110/249], loss=90.2687
	step [111/249], loss=78.3044
	step [112/249], loss=100.5185
	step [113/249], loss=96.1332
	step [114/249], loss=91.1613
	step [115/249], loss=91.3832
	step [116/249], loss=104.9499
	step [117/249], loss=84.0033
	step [118/249], loss=85.1610
	step [119/249], loss=92.2741
	step [120/249], loss=108.9382
	step [121/249], loss=61.5898
	step [122/249], loss=84.8012
	step [123/249], loss=107.8970
	step [124/249], loss=78.0779
	step [125/249], loss=91.8338
	step [126/249], loss=94.4210
	step [127/249], loss=77.5748
	step [128/249], loss=98.8664
	step [129/249], loss=96.9582
	step [130/249], loss=71.9436
	step [131/249], loss=83.0023
	step [132/249], loss=99.8037
	step [133/249], loss=69.6251
	step [134/249], loss=100.0574
	step [135/249], loss=99.4108
	step [136/249], loss=85.0087
	step [137/249], loss=101.3737
	step [138/249], loss=84.3312
	step [139/249], loss=84.8363
	step [140/249], loss=86.9597
	step [141/249], loss=92.2648
	step [142/249], loss=104.0318
	step [143/249], loss=113.3828
	step [144/249], loss=91.8143
	step [145/249], loss=89.5272
	step [146/249], loss=71.1186
	step [147/249], loss=89.3342
	step [148/249], loss=98.6302
	step [149/249], loss=101.7896
	step [150/249], loss=103.4092
	step [151/249], loss=103.3143
	step [152/249], loss=87.1441
	step [153/249], loss=78.0202
	step [154/249], loss=88.9454
	step [155/249], loss=81.8652
	step [156/249], loss=123.4361
	step [157/249], loss=93.7123
	step [158/249], loss=101.3057
	step [159/249], loss=94.0228
	step [160/249], loss=104.7143
	step [161/249], loss=79.8760
	step [162/249], loss=89.1118
	step [163/249], loss=101.4989
	step [164/249], loss=110.6565
	step [165/249], loss=80.7831
	step [166/249], loss=85.9466
	step [167/249], loss=105.4019
	step [168/249], loss=101.2420
	step [169/249], loss=85.6226
	step [170/249], loss=92.1053
	step [171/249], loss=85.7186
	step [172/249], loss=99.7315
	step [173/249], loss=74.1200
	step [174/249], loss=88.3327
	step [175/249], loss=105.2197
	step [176/249], loss=106.7817
	step [177/249], loss=88.2199
	step [178/249], loss=102.6610
	step [179/249], loss=84.6431
	step [180/249], loss=106.6368
	step [181/249], loss=100.9867
	step [182/249], loss=110.0546
	step [183/249], loss=83.5123
	step [184/249], loss=78.9123
	step [185/249], loss=103.2843
	step [186/249], loss=106.3448
	step [187/249], loss=93.7406
	step [188/249], loss=88.3000
	step [189/249], loss=93.7925
	step [190/249], loss=109.4130
	step [191/249], loss=75.8210
	step [192/249], loss=99.2532
	step [193/249], loss=73.3254
	step [194/249], loss=86.1144
	step [195/249], loss=89.4149
	step [196/249], loss=102.0022
	step [197/249], loss=89.0822
	step [198/249], loss=104.2516
	step [199/249], loss=94.4804
	step [200/249], loss=90.0123
	step [201/249], loss=86.0663
	step [202/249], loss=102.8250
	step [203/249], loss=97.9732
	step [204/249], loss=82.7849
	step [205/249], loss=107.2637
	step [206/249], loss=89.7509
	step [207/249], loss=89.3580
	step [208/249], loss=104.4381
	step [209/249], loss=90.8896
	step [210/249], loss=82.2441
	step [211/249], loss=91.7687
	step [212/249], loss=86.8531
	step [213/249], loss=103.0610
	step [214/249], loss=71.9705
	step [215/249], loss=99.0083
	step [216/249], loss=79.3733
	step [217/249], loss=97.7847
	step [218/249], loss=86.9186
	step [219/249], loss=98.1930
	step [220/249], loss=86.7520
	step [221/249], loss=94.4622
	step [222/249], loss=95.8429
	step [223/249], loss=106.6316
	step [224/249], loss=85.4428
	step [225/249], loss=115.7404
	step [226/249], loss=98.9328
	step [227/249], loss=79.9770
	step [228/249], loss=97.9237
	step [229/249], loss=92.0896
	step [230/249], loss=89.8044
	step [231/249], loss=86.7180
	step [232/249], loss=95.3165
	step [233/249], loss=96.7330
	step [234/249], loss=74.7283
	step [235/249], loss=102.6529
	step [236/249], loss=93.9673
	step [237/249], loss=108.3613
	step [238/249], loss=72.1405
	step [239/249], loss=80.6400
	step [240/249], loss=94.1115
	step [241/249], loss=85.0489
	step [242/249], loss=86.0163
	step [243/249], loss=69.8343
	step [244/249], loss=114.5674
	step [245/249], loss=96.5800
	step [246/249], loss=122.1344
	step [247/249], loss=87.7102
	step [248/249], loss=100.3676
	step [249/249], loss=4.3700
	Evaluating
	loss=0.0096, precision=0.4149, recall=0.8888, f1=0.5658
Training epoch 25
	step [1/249], loss=100.2608
	step [2/249], loss=80.5649
	step [3/249], loss=103.1015
	step [4/249], loss=97.1926
	step [5/249], loss=85.4943
	step [6/249], loss=93.1445
	step [7/249], loss=79.1637
	step [8/249], loss=109.6893
	step [9/249], loss=90.7809
	step [10/249], loss=79.5166
	step [11/249], loss=77.7664
	step [12/249], loss=86.4255
	step [13/249], loss=100.4848
	step [14/249], loss=89.1635
	step [15/249], loss=89.4698
	step [16/249], loss=82.2992
	step [17/249], loss=73.0277
	step [18/249], loss=86.9147
	step [19/249], loss=102.0069
	step [20/249], loss=98.8811
	step [21/249], loss=85.6671
	step [22/249], loss=93.3612
	step [23/249], loss=107.3824
	step [24/249], loss=93.8406
	step [25/249], loss=91.0192
	step [26/249], loss=95.4398
	step [27/249], loss=117.8643
	step [28/249], loss=91.7770
	step [29/249], loss=98.5078
	step [30/249], loss=84.9708
	step [31/249], loss=85.8542
	step [32/249], loss=97.0717
	step [33/249], loss=57.9349
	step [34/249], loss=81.4510
	step [35/249], loss=94.2922
	step [36/249], loss=100.6175
	step [37/249], loss=81.1156
	step [38/249], loss=102.3589
	step [39/249], loss=89.6503
	step [40/249], loss=82.1129
	step [41/249], loss=81.5067
	step [42/249], loss=82.9176
	step [43/249], loss=86.0981
	step [44/249], loss=105.7533
	step [45/249], loss=98.8851
	step [46/249], loss=95.4889
	step [47/249], loss=86.8163
	step [48/249], loss=78.2796
	step [49/249], loss=74.7790
	step [50/249], loss=94.2606
	step [51/249], loss=97.2211
	step [52/249], loss=89.8900
	step [53/249], loss=91.1388
	step [54/249], loss=104.3647
	step [55/249], loss=83.5406
	step [56/249], loss=96.5702
	step [57/249], loss=92.7269
	step [58/249], loss=78.7851
	step [59/249], loss=87.0878
	step [60/249], loss=97.6321
	step [61/249], loss=94.7821
	step [62/249], loss=77.7490
	step [63/249], loss=95.7672
	step [64/249], loss=100.3409
	step [65/249], loss=84.2478
	step [66/249], loss=80.6565
	step [67/249], loss=95.1795
	step [68/249], loss=86.9071
	step [69/249], loss=86.9885
	step [70/249], loss=102.4638
	step [71/249], loss=82.5597
	step [72/249], loss=99.8362
	step [73/249], loss=108.1506
	step [74/249], loss=80.6267
	step [75/249], loss=93.5774
	step [76/249], loss=92.9476
	step [77/249], loss=100.4150
	step [78/249], loss=103.8851
	step [79/249], loss=93.0957
	step [80/249], loss=104.7177
	step [81/249], loss=84.9003
	step [82/249], loss=106.0023
	step [83/249], loss=106.0167
	step [84/249], loss=88.6946
	step [85/249], loss=89.2902
	step [86/249], loss=85.3288
	step [87/249], loss=99.1277
	step [88/249], loss=78.6124
	step [89/249], loss=108.8002
	step [90/249], loss=85.5867
	step [91/249], loss=70.7240
	step [92/249], loss=110.1367
	step [93/249], loss=88.2712
	step [94/249], loss=98.7508
	step [95/249], loss=90.5823
	step [96/249], loss=85.4091
	step [97/249], loss=110.3104
	step [98/249], loss=98.1225
	step [99/249], loss=90.8618
	step [100/249], loss=73.3164
	step [101/249], loss=89.0944
	step [102/249], loss=92.4573
	step [103/249], loss=95.7370
	step [104/249], loss=85.6097
	step [105/249], loss=90.6656
	step [106/249], loss=104.3160
	step [107/249], loss=71.4044
	step [108/249], loss=105.9360
	step [109/249], loss=92.5184
	step [110/249], loss=82.7142
	step [111/249], loss=90.9663
	step [112/249], loss=85.6290
	step [113/249], loss=89.8438
	step [114/249], loss=93.7264
	step [115/249], loss=91.7541
	step [116/249], loss=88.4991
	step [117/249], loss=90.7758
	step [118/249], loss=100.9539
	step [119/249], loss=94.4438
	step [120/249], loss=96.8482
	step [121/249], loss=91.7385
	step [122/249], loss=95.5728
	step [123/249], loss=80.2307
	step [124/249], loss=92.2888
	step [125/249], loss=102.8185
	step [126/249], loss=76.1635
	step [127/249], loss=92.2576
	step [128/249], loss=96.8617
	step [129/249], loss=97.1645
	step [130/249], loss=89.4358
	step [131/249], loss=104.8105
	step [132/249], loss=90.7218
	step [133/249], loss=98.1405
	step [134/249], loss=99.4060
	step [135/249], loss=94.1473
	step [136/249], loss=84.3097
	step [137/249], loss=98.8505
	step [138/249], loss=77.0515
	step [139/249], loss=91.2651
	step [140/249], loss=79.0526
	step [141/249], loss=92.9969
	step [142/249], loss=98.9595
	step [143/249], loss=78.5372
	step [144/249], loss=63.7143
	step [145/249], loss=88.5957
	step [146/249], loss=72.5190
	step [147/249], loss=95.6733
	step [148/249], loss=96.3245
	step [149/249], loss=84.5274
	step [150/249], loss=106.7860
	step [151/249], loss=106.5762
	step [152/249], loss=105.0923
	step [153/249], loss=97.4387
	step [154/249], loss=97.8776
	step [155/249], loss=98.7120
	step [156/249], loss=88.9387
	step [157/249], loss=107.7288
	step [158/249], loss=93.0283
	step [159/249], loss=98.9867
	step [160/249], loss=97.8058
	step [161/249], loss=83.1058
	step [162/249], loss=75.8971
	step [163/249], loss=80.2467
	step [164/249], loss=81.8611
	step [165/249], loss=95.6050
	step [166/249], loss=102.8650
	step [167/249], loss=108.8541
	step [168/249], loss=89.0973
	step [169/249], loss=97.8823
	step [170/249], loss=80.0501
	step [171/249], loss=113.4853
	step [172/249], loss=93.6750
	step [173/249], loss=79.8629
	step [174/249], loss=78.7183
	step [175/249], loss=82.0915
	step [176/249], loss=101.0598
	step [177/249], loss=84.5824
	step [178/249], loss=104.0072
	step [179/249], loss=84.8241
	step [180/249], loss=99.7912
	step [181/249], loss=94.4313
	step [182/249], loss=79.6894
	step [183/249], loss=92.0985
	step [184/249], loss=99.9707
	step [185/249], loss=107.5830
	step [186/249], loss=80.1681
	step [187/249], loss=97.1535
	step [188/249], loss=94.4592
	step [189/249], loss=80.7191
	step [190/249], loss=86.6342
	step [191/249], loss=107.1147
	step [192/249], loss=88.5163
	step [193/249], loss=102.0145
	step [194/249], loss=86.2277
	step [195/249], loss=97.1104
	step [196/249], loss=91.1899
	step [197/249], loss=97.2262
	step [198/249], loss=105.3816
	step [199/249], loss=104.4935
	step [200/249], loss=94.5926
	step [201/249], loss=92.2819
	step [202/249], loss=97.4549
	step [203/249], loss=77.1451
	step [204/249], loss=98.1285
	step [205/249], loss=81.6496
	step [206/249], loss=93.9370
	step [207/249], loss=86.0334
	step [208/249], loss=75.5320
	step [209/249], loss=100.6088
	step [210/249], loss=91.5286
	step [211/249], loss=92.0460
	step [212/249], loss=87.5458
	step [213/249], loss=84.0983
	step [214/249], loss=80.4614
	step [215/249], loss=94.2296
	step [216/249], loss=78.3511
	step [217/249], loss=88.8996
	step [218/249], loss=95.6467
	step [219/249], loss=87.4131
	step [220/249], loss=97.7483
	step [221/249], loss=97.6224
	step [222/249], loss=97.3768
	step [223/249], loss=90.1346
	step [224/249], loss=114.2650
	step [225/249], loss=87.8224
	step [226/249], loss=82.7162
	step [227/249], loss=88.3065
	step [228/249], loss=101.5886
	step [229/249], loss=107.2504
	step [230/249], loss=81.1553
	step [231/249], loss=96.9046
	step [232/249], loss=94.7092
	step [233/249], loss=81.9901
	step [234/249], loss=91.3447
	step [235/249], loss=97.9514
	step [236/249], loss=97.0912
	step [237/249], loss=79.6011
	step [238/249], loss=91.1566
	step [239/249], loss=108.8615
	step [240/249], loss=88.5326
	step [241/249], loss=97.0457
	step [242/249], loss=107.2577
	step [243/249], loss=75.3231
	step [244/249], loss=90.8999
	step [245/249], loss=95.6143
	step [246/249], loss=86.9811
	step [247/249], loss=103.5206
	step [248/249], loss=109.6062
	step [249/249], loss=4.3116
	Evaluating
	loss=0.0074, precision=0.4078, recall=0.9092, f1=0.5631
Training epoch 26
	step [1/249], loss=90.7100
	step [2/249], loss=81.6445
	step [3/249], loss=94.2161
	step [4/249], loss=82.3036
	step [5/249], loss=81.6283
	step [6/249], loss=120.9779
	step [7/249], loss=108.7541
	step [8/249], loss=87.9483
	step [9/249], loss=105.9358
	step [10/249], loss=91.2123
	step [11/249], loss=106.7703
	step [12/249], loss=95.3788
	step [13/249], loss=100.6493
	step [14/249], loss=100.6842
	step [15/249], loss=80.4870
	step [16/249], loss=75.4183
	step [17/249], loss=89.6853
	step [18/249], loss=95.3559
	step [19/249], loss=84.6984
	step [20/249], loss=84.0316
	step [21/249], loss=78.2294
	step [22/249], loss=82.3445
	step [23/249], loss=94.1176
	step [24/249], loss=104.2129
	step [25/249], loss=85.3901
	step [26/249], loss=71.9113
	step [27/249], loss=90.3964
	step [28/249], loss=107.6756
	step [29/249], loss=94.0680
	step [30/249], loss=116.0326
	step [31/249], loss=91.0046
	step [32/249], loss=95.4904
	step [33/249], loss=107.4924
	step [34/249], loss=68.2705
	step [35/249], loss=90.0238
	step [36/249], loss=104.0090
	step [37/249], loss=96.6009
	step [38/249], loss=92.9086
	step [39/249], loss=90.1923
	step [40/249], loss=94.9749
	step [41/249], loss=85.4501
	step [42/249], loss=83.3970
	step [43/249], loss=82.1073
	step [44/249], loss=86.5777
	step [45/249], loss=88.1596
	step [46/249], loss=86.1383
	step [47/249], loss=84.1817
	step [48/249], loss=88.6046
	step [49/249], loss=91.7477
	step [50/249], loss=92.0192
	step [51/249], loss=102.1949
	step [52/249], loss=98.9995
	step [53/249], loss=87.7493
	step [54/249], loss=103.5792
	step [55/249], loss=96.6540
	step [56/249], loss=101.5799
	step [57/249], loss=90.3769
	step [58/249], loss=101.0773
	step [59/249], loss=73.8217
	step [60/249], loss=101.8048
	step [61/249], loss=78.3637
	step [62/249], loss=83.0148
	step [63/249], loss=82.7256
	step [64/249], loss=99.4822
	step [65/249], loss=87.1103
	step [66/249], loss=92.5535
	step [67/249], loss=84.7489
	step [68/249], loss=82.7657
	step [69/249], loss=104.7803
	step [70/249], loss=81.0263
	step [71/249], loss=122.9924
	step [72/249], loss=90.7039
	step [73/249], loss=89.8909
	step [74/249], loss=82.1717
	step [75/249], loss=95.6066
	step [76/249], loss=92.1858
	step [77/249], loss=91.2465
	step [78/249], loss=81.5778
	step [79/249], loss=71.8464
	step [80/249], loss=100.8019
	step [81/249], loss=100.7423
	step [82/249], loss=106.0290
	step [83/249], loss=87.6448
	step [84/249], loss=91.0156
	step [85/249], loss=91.2104
	step [86/249], loss=89.3016
	step [87/249], loss=100.8244
	step [88/249], loss=85.3154
	step [89/249], loss=100.0507
	step [90/249], loss=108.3678
	step [91/249], loss=102.6035
	step [92/249], loss=96.1826
	step [93/249], loss=91.3768
	step [94/249], loss=77.9954
	step [95/249], loss=86.6759
	step [96/249], loss=97.1751
	step [97/249], loss=78.9903
	step [98/249], loss=91.8130
	step [99/249], loss=85.1383
	step [100/249], loss=79.9629
	step [101/249], loss=93.3910
	step [102/249], loss=93.4340
	step [103/249], loss=111.2701
	step [104/249], loss=88.3306
	step [105/249], loss=72.1014
	step [106/249], loss=90.0949
	step [107/249], loss=95.3690
	step [108/249], loss=85.6029
	step [109/249], loss=96.4415
	step [110/249], loss=102.1621
	step [111/249], loss=96.1512
	step [112/249], loss=94.3247
	step [113/249], loss=97.3240
	step [114/249], loss=90.9282
	step [115/249], loss=90.5740
	step [116/249], loss=71.3062
	step [117/249], loss=74.1435
	step [118/249], loss=107.1100
	step [119/249], loss=81.1101
	step [120/249], loss=87.1769
	step [121/249], loss=114.1540
	step [122/249], loss=92.8078
	step [123/249], loss=106.2259
	step [124/249], loss=94.8141
	step [125/249], loss=98.2671
	step [126/249], loss=83.0977
	step [127/249], loss=74.0052
	step [128/249], loss=79.5800
	step [129/249], loss=79.8107
	step [130/249], loss=65.4991
	step [131/249], loss=91.6364
	step [132/249], loss=102.1593
	step [133/249], loss=85.4020
	step [134/249], loss=100.5128
	step [135/249], loss=103.4877
	step [136/249], loss=80.8985
	step [137/249], loss=96.6588
	step [138/249], loss=77.5376
	step [139/249], loss=79.4799
	step [140/249], loss=80.6302
	step [141/249], loss=71.4813
	step [142/249], loss=95.4799
	step [143/249], loss=83.4741
	step [144/249], loss=82.6998
	step [145/249], loss=81.9701
	step [146/249], loss=96.5143
	step [147/249], loss=99.6552
	step [148/249], loss=102.2928
	step [149/249], loss=93.3507
	step [150/249], loss=95.4051
	step [151/249], loss=100.1520
	step [152/249], loss=92.7507
	step [153/249], loss=95.8406
	step [154/249], loss=84.0840
	step [155/249], loss=97.2065
	step [156/249], loss=92.0050
	step [157/249], loss=98.2486
	step [158/249], loss=87.4257
	step [159/249], loss=81.0452
	step [160/249], loss=87.5126
	step [161/249], loss=102.3218
	step [162/249], loss=83.5997
	step [163/249], loss=89.6088
	step [164/249], loss=109.2076
	step [165/249], loss=80.2374
	step [166/249], loss=116.8224
	step [167/249], loss=109.3991
	step [168/249], loss=83.8103
	step [169/249], loss=83.0534
	step [170/249], loss=91.8277
	step [171/249], loss=96.7342
	step [172/249], loss=94.7286
	step [173/249], loss=101.4054
	step [174/249], loss=98.1979
	step [175/249], loss=102.3072
	step [176/249], loss=89.6490
	step [177/249], loss=94.8247
	step [178/249], loss=70.0469
	step [179/249], loss=82.9183
	step [180/249], loss=83.1804
	step [181/249], loss=114.0432
	step [182/249], loss=82.6384
	step [183/249], loss=83.8315
	step [184/249], loss=105.5598
	step [185/249], loss=88.0476
	step [186/249], loss=103.2473
	step [187/249], loss=94.3098
	step [188/249], loss=94.3882
	step [189/249], loss=77.5548
	step [190/249], loss=95.6358
	step [191/249], loss=88.2665
	step [192/249], loss=77.4756
	step [193/249], loss=78.5404
	step [194/249], loss=89.7869
	step [195/249], loss=77.7190
	step [196/249], loss=85.9716
	step [197/249], loss=92.3448
	step [198/249], loss=75.8872
	step [199/249], loss=105.4958
	step [200/249], loss=92.6639
	step [201/249], loss=108.7533
	step [202/249], loss=94.1178
	step [203/249], loss=92.5475
	step [204/249], loss=91.2156
	step [205/249], loss=77.4458
	step [206/249], loss=91.6158
	step [207/249], loss=85.2378
	step [208/249], loss=98.2549
	step [209/249], loss=96.9573
	step [210/249], loss=87.9867
	step [211/249], loss=90.0493
	step [212/249], loss=86.8004
	step [213/249], loss=80.9739
	step [214/249], loss=92.2132
	step [215/249], loss=80.9129
	step [216/249], loss=92.9825
	step [217/249], loss=94.8363
	step [218/249], loss=74.9938
	step [219/249], loss=83.9571
	step [220/249], loss=79.4277
	step [221/249], loss=87.7893
	step [222/249], loss=86.6813
	step [223/249], loss=87.4503
	step [224/249], loss=89.3654
	step [225/249], loss=78.4547
	step [226/249], loss=107.3477
	step [227/249], loss=90.9333
	step [228/249], loss=85.5318
	step [229/249], loss=87.1885
	step [230/249], loss=82.1888
	step [231/249], loss=97.9899
	step [232/249], loss=93.0991
	step [233/249], loss=98.1732
	step [234/249], loss=83.1796
	step [235/249], loss=68.5266
	step [236/249], loss=95.7793
	step [237/249], loss=85.1556
	step [238/249], loss=89.3335
	step [239/249], loss=99.2534
	step [240/249], loss=94.8994
	step [241/249], loss=97.8696
	step [242/249], loss=102.6318
	step [243/249], loss=66.4356
	step [244/249], loss=103.9703
	step [245/249], loss=96.0383
	step [246/249], loss=83.3012
	step [247/249], loss=89.8445
	step [248/249], loss=81.5945
	step [249/249], loss=1.0710
	Evaluating
	loss=0.0077, precision=0.3603, recall=0.8866, f1=0.5124
Training epoch 27
	step [1/249], loss=96.5235
	step [2/249], loss=91.3398
	step [3/249], loss=102.0412
	step [4/249], loss=99.2441
	step [5/249], loss=86.1356
	step [6/249], loss=110.0208
	step [7/249], loss=91.7007
	step [8/249], loss=88.9701
	step [9/249], loss=91.4109
	step [10/249], loss=85.9003
	step [11/249], loss=96.3251
	step [12/249], loss=94.2450
	step [13/249], loss=83.4951
	step [14/249], loss=76.5623
	step [15/249], loss=69.8759
	step [16/249], loss=86.2587
	step [17/249], loss=81.0124
	step [18/249], loss=89.4977
	step [19/249], loss=95.3372
	step [20/249], loss=69.0242
	step [21/249], loss=82.1797
	step [22/249], loss=91.4393
	step [23/249], loss=89.5183
	step [24/249], loss=92.2313
	step [25/249], loss=91.0622
	step [26/249], loss=105.7969
	step [27/249], loss=89.5431
	step [28/249], loss=99.1997
	step [29/249], loss=75.7817
	step [30/249], loss=65.5731
	step [31/249], loss=100.7042
	step [32/249], loss=94.2745
	step [33/249], loss=68.2922
	step [34/249], loss=75.5462
	step [35/249], loss=94.8637
	step [36/249], loss=92.6585
	step [37/249], loss=85.9729
	step [38/249], loss=90.5041
	step [39/249], loss=78.4092
	step [40/249], loss=94.1106
	step [41/249], loss=88.3216
	step [42/249], loss=84.5162
	step [43/249], loss=71.2212
	step [44/249], loss=89.8819
	step [45/249], loss=81.1801
	step [46/249], loss=90.9432
	step [47/249], loss=88.0052
	step [48/249], loss=104.7383
	step [49/249], loss=104.3531
	step [50/249], loss=103.3285
	step [51/249], loss=81.5656
	step [52/249], loss=106.6741
	step [53/249], loss=96.6607
	step [54/249], loss=80.2076
	step [55/249], loss=100.8082
	step [56/249], loss=93.4172
	step [57/249], loss=70.4979
	step [58/249], loss=84.2131
	step [59/249], loss=75.7142
	step [60/249], loss=99.6375
	step [61/249], loss=83.1616
	step [62/249], loss=97.4494
	step [63/249], loss=92.0077
	step [64/249], loss=93.8982
	step [65/249], loss=76.7531
	step [66/249], loss=81.4933
	step [67/249], loss=83.3155
	step [68/249], loss=89.1707
	step [69/249], loss=69.0788
	step [70/249], loss=81.7617
	step [71/249], loss=97.0294
	step [72/249], loss=94.2081
	step [73/249], loss=89.9696
	step [74/249], loss=96.4240
	step [75/249], loss=83.9099
	step [76/249], loss=89.9083
	step [77/249], loss=107.5081
	step [78/249], loss=86.7539
	step [79/249], loss=86.6756
	step [80/249], loss=94.3823
	step [81/249], loss=85.4705
	step [82/249], loss=83.6247
	step [83/249], loss=95.5742
	step [84/249], loss=80.5147
	step [85/249], loss=84.2181
	step [86/249], loss=93.1210
	step [87/249], loss=86.8491
	step [88/249], loss=82.2675
	step [89/249], loss=109.6030
	step [90/249], loss=90.2426
	step [91/249], loss=85.2305
	step [92/249], loss=72.5837
	step [93/249], loss=98.3965
	step [94/249], loss=82.1209
	step [95/249], loss=91.2558
	step [96/249], loss=85.3853
	step [97/249], loss=94.4752
	step [98/249], loss=85.6100
	step [99/249], loss=102.5859
	step [100/249], loss=93.9977
	step [101/249], loss=96.8638
	step [102/249], loss=89.1911
	step [103/249], loss=76.5733
	step [104/249], loss=90.8985
	step [105/249], loss=78.7666
	step [106/249], loss=84.5386
	step [107/249], loss=93.6249
	step [108/249], loss=96.7729
	step [109/249], loss=81.3963
	step [110/249], loss=79.7960
	step [111/249], loss=73.6841
	step [112/249], loss=81.5956
	step [113/249], loss=91.5674
	step [114/249], loss=86.8320
	step [115/249], loss=75.8070
	step [116/249], loss=99.1787
	step [117/249], loss=109.3953
	step [118/249], loss=84.3538
	step [119/249], loss=67.8458
	step [120/249], loss=81.6212
	step [121/249], loss=89.1164
	step [122/249], loss=85.9741
	step [123/249], loss=81.6143
	step [124/249], loss=98.2767
	step [125/249], loss=90.6308
	step [126/249], loss=92.8280
	step [127/249], loss=107.7446
	step [128/249], loss=97.5578
	step [129/249], loss=98.9937
	step [130/249], loss=84.4597
	step [131/249], loss=94.7329
	step [132/249], loss=99.6148
	step [133/249], loss=98.7882
	step [134/249], loss=85.8190
	step [135/249], loss=111.2196
	step [136/249], loss=91.5268
	step [137/249], loss=79.9940
	step [138/249], loss=81.3327
	step [139/249], loss=97.4726
	step [140/249], loss=100.9856
	step [141/249], loss=88.2844
	step [142/249], loss=83.7692
	step [143/249], loss=103.2654
	step [144/249], loss=76.1747
	step [145/249], loss=103.0688
	step [146/249], loss=95.2764
	step [147/249], loss=102.4158
	step [148/249], loss=88.6057
	step [149/249], loss=89.9876
	step [150/249], loss=92.3754
	step [151/249], loss=89.4197
	step [152/249], loss=100.3225
	step [153/249], loss=90.7810
	step [154/249], loss=97.6499
	step [155/249], loss=96.2857
	step [156/249], loss=67.4819
	step [157/249], loss=84.8498
	step [158/249], loss=87.0268
	step [159/249], loss=90.6964
	step [160/249], loss=90.9571
	step [161/249], loss=91.3873
	step [162/249], loss=87.2383
	step [163/249], loss=95.6649
	step [164/249], loss=89.7596
	step [165/249], loss=87.4479
	step [166/249], loss=89.2886
	step [167/249], loss=100.5752
	step [168/249], loss=99.2224
	step [169/249], loss=87.1204
	step [170/249], loss=74.2650
	step [171/249], loss=86.6136
	step [172/249], loss=106.7251
	step [173/249], loss=85.2112
	step [174/249], loss=83.0065
	step [175/249], loss=94.1082
	step [176/249], loss=83.6148
	step [177/249], loss=86.5827
	step [178/249], loss=87.1769
	step [179/249], loss=102.4678
	step [180/249], loss=82.9824
	step [181/249], loss=100.1172
	step [182/249], loss=94.9494
	step [183/249], loss=87.4755
	step [184/249], loss=74.7808
	step [185/249], loss=80.7867
	step [186/249], loss=71.9146
	step [187/249], loss=100.0818
	step [188/249], loss=103.6423
	step [189/249], loss=100.3048
	step [190/249], loss=84.1059
	step [191/249], loss=96.8534
	step [192/249], loss=93.9184
	step [193/249], loss=81.0283
	step [194/249], loss=82.8923
	step [195/249], loss=92.1192
	step [196/249], loss=83.6000
	step [197/249], loss=85.3236
	step [198/249], loss=86.4173
	step [199/249], loss=83.1314
	step [200/249], loss=97.9052
	step [201/249], loss=90.2426
	step [202/249], loss=88.1130
	step [203/249], loss=101.8847
	step [204/249], loss=96.0407
	step [205/249], loss=84.7221
	step [206/249], loss=71.2148
	step [207/249], loss=96.0161
	step [208/249], loss=90.6227
	step [209/249], loss=98.2393
	step [210/249], loss=107.5918
	step [211/249], loss=81.8376
	step [212/249], loss=88.4020
	step [213/249], loss=81.1774
	step [214/249], loss=102.9080
	step [215/249], loss=84.4223
	step [216/249], loss=88.8823
	step [217/249], loss=94.5084
	step [218/249], loss=86.9410
	step [219/249], loss=93.2725
	step [220/249], loss=99.8115
	step [221/249], loss=106.6653
	step [222/249], loss=90.9438
	step [223/249], loss=78.9371
	step [224/249], loss=95.0242
	step [225/249], loss=97.4985
	step [226/249], loss=98.7450
	step [227/249], loss=70.1327
	step [228/249], loss=85.1821
	step [229/249], loss=78.9625
	step [230/249], loss=90.3113
	step [231/249], loss=89.1937
	step [232/249], loss=94.9546
	step [233/249], loss=113.9769
	step [234/249], loss=86.2126
	step [235/249], loss=107.6232
	step [236/249], loss=95.2069
	step [237/249], loss=89.0162
	step [238/249], loss=80.1692
	step [239/249], loss=78.5796
	step [240/249], loss=105.9479
	step [241/249], loss=123.8951
	step [242/249], loss=84.7983
	step [243/249], loss=80.1797
	step [244/249], loss=87.0558
	step [245/249], loss=103.7575
	step [246/249], loss=74.2259
	step [247/249], loss=87.1183
	step [248/249], loss=80.9108
	step [249/249], loss=1.6649
	Evaluating
	loss=0.0071, precision=0.4303, recall=0.8955, f1=0.5813
Training epoch 28
	step [1/249], loss=87.3314
	step [2/249], loss=110.1230
	step [3/249], loss=87.7338
	step [4/249], loss=90.3142
	step [5/249], loss=77.4734
	step [6/249], loss=85.3004
	step [7/249], loss=100.4915
	step [8/249], loss=84.8020
	step [9/249], loss=96.6352
	step [10/249], loss=83.6177
	step [11/249], loss=97.4434
	step [12/249], loss=70.2638
	step [13/249], loss=75.5011
	step [14/249], loss=79.3470
	step [15/249], loss=68.6725
	step [16/249], loss=98.5958
	step [17/249], loss=96.7342
	step [18/249], loss=81.8588
	step [19/249], loss=70.1035
	step [20/249], loss=88.6963
	step [21/249], loss=79.6135
	step [22/249], loss=98.0206
	step [23/249], loss=79.6782
	step [24/249], loss=93.1868
	step [25/249], loss=88.5358
	step [26/249], loss=91.1831
	step [27/249], loss=78.9056
	step [28/249], loss=91.6693
	step [29/249], loss=105.7231
	step [30/249], loss=91.8334
	step [31/249], loss=104.4563
	step [32/249], loss=99.3100
	step [33/249], loss=77.4483
	step [34/249], loss=93.4431
	step [35/249], loss=90.0515
	step [36/249], loss=87.3638
	step [37/249], loss=101.4713
	step [38/249], loss=86.7324
	step [39/249], loss=91.5408
	step [40/249], loss=76.6849
	step [41/249], loss=82.5567
	step [42/249], loss=88.8447
	step [43/249], loss=90.6748
	step [44/249], loss=84.2316
	step [45/249], loss=87.2838
	step [46/249], loss=99.0134
	step [47/249], loss=94.0222
	step [48/249], loss=88.2665
	step [49/249], loss=90.1161
	step [50/249], loss=90.6422
	step [51/249], loss=91.5288
	step [52/249], loss=89.8795
	step [53/249], loss=101.9375
	step [54/249], loss=84.0703
	step [55/249], loss=83.6494
	step [56/249], loss=75.8421
	step [57/249], loss=86.4666
	step [58/249], loss=93.5665
	step [59/249], loss=77.0036
	step [60/249], loss=91.8259
	step [61/249], loss=103.7027
	step [62/249], loss=83.4417
	step [63/249], loss=78.7564
	step [64/249], loss=83.9271
	step [65/249], loss=92.6891
	step [66/249], loss=91.4317
	step [67/249], loss=108.4541
	step [68/249], loss=102.1285
	step [69/249], loss=89.9604
	step [70/249], loss=109.3107
	step [71/249], loss=92.8637
	step [72/249], loss=88.7419
	step [73/249], loss=88.7129
	step [74/249], loss=87.8582
	step [75/249], loss=87.2909
	step [76/249], loss=88.2548
	step [77/249], loss=91.6156
	step [78/249], loss=82.6702
	step [79/249], loss=109.0743
	step [80/249], loss=74.9745
	step [81/249], loss=99.3859
	step [82/249], loss=84.6661
	step [83/249], loss=89.2440
	step [84/249], loss=107.7635
	step [85/249], loss=100.6937
	step [86/249], loss=82.8561
	step [87/249], loss=76.3592
	step [88/249], loss=75.9329
	step [89/249], loss=90.0975
	step [90/249], loss=83.0031
	step [91/249], loss=96.4137
	step [92/249], loss=92.8609
	step [93/249], loss=93.5443
	step [94/249], loss=77.7135
	step [95/249], loss=91.6034
	step [96/249], loss=76.8037
	step [97/249], loss=66.1088
	step [98/249], loss=87.6190
	step [99/249], loss=89.1064
	step [100/249], loss=98.2344
	step [101/249], loss=91.1783
	step [102/249], loss=90.7731
	step [103/249], loss=96.5107
	step [104/249], loss=88.4211
	step [105/249], loss=100.6432
	step [106/249], loss=92.5477
	step [107/249], loss=85.1610
	step [108/249], loss=81.7265
	step [109/249], loss=71.7426
	step [110/249], loss=106.1879
	step [111/249], loss=80.6183
	step [112/249], loss=92.5465
	step [113/249], loss=85.1202
	step [114/249], loss=93.9803
	step [115/249], loss=80.7150
	step [116/249], loss=85.2501
	step [117/249], loss=87.3409
	step [118/249], loss=89.3310
	step [119/249], loss=93.9032
	step [120/249], loss=84.0319
	step [121/249], loss=91.2285
	step [122/249], loss=94.4714
	step [123/249], loss=87.1575
	step [124/249], loss=108.0067
	step [125/249], loss=94.1221
	step [126/249], loss=76.7129
	step [127/249], loss=100.8297
	step [128/249], loss=93.7133
	step [129/249], loss=73.7423
	step [130/249], loss=68.9686
	step [131/249], loss=88.2837
	step [132/249], loss=80.5755
	step [133/249], loss=89.9839
	step [134/249], loss=75.7486
	step [135/249], loss=90.1837
	step [136/249], loss=80.6262
	step [137/249], loss=85.4169
	step [138/249], loss=81.8349
	step [139/249], loss=105.4593
	step [140/249], loss=91.7010
	step [141/249], loss=87.4624
	step [142/249], loss=78.6883
	step [143/249], loss=84.7392
	step [144/249], loss=87.9935
	step [145/249], loss=94.6476
	step [146/249], loss=89.7142
	step [147/249], loss=92.5546
	step [148/249], loss=74.9483
	step [149/249], loss=93.1613
	step [150/249], loss=69.3720
	step [151/249], loss=106.3277
	step [152/249], loss=86.1515
	step [153/249], loss=85.4563
	step [154/249], loss=98.9340
	step [155/249], loss=90.7485
	step [156/249], loss=98.9926
	step [157/249], loss=91.7872
	step [158/249], loss=87.8952
	step [159/249], loss=80.7854
	step [160/249], loss=74.6782
	step [161/249], loss=79.4141
	step [162/249], loss=95.2396
	step [163/249], loss=84.4249
	step [164/249], loss=110.2979
	step [165/249], loss=71.7041
	step [166/249], loss=84.3103
	step [167/249], loss=93.4993
	step [168/249], loss=96.2944
	step [169/249], loss=84.9040
	step [170/249], loss=88.7168
	step [171/249], loss=91.0319
	step [172/249], loss=83.3958
	step [173/249], loss=90.4739
	step [174/249], loss=93.4226
	step [175/249], loss=92.1358
	step [176/249], loss=102.3798
	step [177/249], loss=77.3747
	step [178/249], loss=76.7784
	step [179/249], loss=107.5907
	step [180/249], loss=84.0220
	step [181/249], loss=73.8852
	step [182/249], loss=70.7673
	step [183/249], loss=93.7939
	step [184/249], loss=79.2199
	step [185/249], loss=65.6257
	step [186/249], loss=74.4808
	step [187/249], loss=69.9257
	step [188/249], loss=94.0088
	step [189/249], loss=97.9757
	step [190/249], loss=93.1996
	step [191/249], loss=83.6931
	step [192/249], loss=95.1494
	step [193/249], loss=80.4680
	step [194/249], loss=101.0151
	step [195/249], loss=97.2755
	step [196/249], loss=89.4152
	step [197/249], loss=93.3245
	step [198/249], loss=76.8802
	step [199/249], loss=91.6102
	step [200/249], loss=96.3037
	step [201/249], loss=90.7994
	step [202/249], loss=87.5688
	step [203/249], loss=70.4624
	step [204/249], loss=93.0447
	step [205/249], loss=72.3995
	step [206/249], loss=76.7804
	step [207/249], loss=93.7243
	step [208/249], loss=84.3658
	step [209/249], loss=92.4644
	step [210/249], loss=94.8843
	step [211/249], loss=97.1599
	step [212/249], loss=100.5975
	step [213/249], loss=91.8472
	step [214/249], loss=86.0844
	step [215/249], loss=96.2240
	step [216/249], loss=95.3260
	step [217/249], loss=69.7349
	step [218/249], loss=99.7625
	step [219/249], loss=115.8056
	step [220/249], loss=87.3157
	step [221/249], loss=89.2344
	step [222/249], loss=95.0053
	step [223/249], loss=104.7447
	step [224/249], loss=102.7265
	step [225/249], loss=99.6845
	step [226/249], loss=87.7450
	step [227/249], loss=111.3852
	step [228/249], loss=103.9554
	step [229/249], loss=91.2880
	step [230/249], loss=105.7116
	step [231/249], loss=83.8912
	step [232/249], loss=85.4051
	step [233/249], loss=78.2575
	step [234/249], loss=86.7490
	step [235/249], loss=81.1084
	step [236/249], loss=82.6296
	step [237/249], loss=77.4232
	step [238/249], loss=89.4180
	step [239/249], loss=92.8612
	step [240/249], loss=80.3863
	step [241/249], loss=91.2829
	step [242/249], loss=68.2917
	step [243/249], loss=90.0607
	step [244/249], loss=81.8765
	step [245/249], loss=111.9774
	step [246/249], loss=104.9546
	step [247/249], loss=75.5902
	step [248/249], loss=79.0818
	step [249/249], loss=5.1736
	Evaluating
	loss=0.0092, precision=0.2883, recall=0.8639, f1=0.4324
Training epoch 29
	step [1/249], loss=85.4716
	step [2/249], loss=99.1136
	step [3/249], loss=95.1663
	step [4/249], loss=74.6716
	step [5/249], loss=78.5367
	step [6/249], loss=72.8285
	step [7/249], loss=90.3796
	step [8/249], loss=107.2304
	step [9/249], loss=83.2201
	step [10/249], loss=99.4501
	step [11/249], loss=84.2557
	step [12/249], loss=96.5373
	step [13/249], loss=94.2765
	step [14/249], loss=95.1040
	step [15/249], loss=79.7691
	step [16/249], loss=82.0094
	step [17/249], loss=94.7264
	step [18/249], loss=87.1221
	step [19/249], loss=86.4932
	step [20/249], loss=88.7943
	step [21/249], loss=97.7343
	step [22/249], loss=114.3024
	step [23/249], loss=76.9150
	step [24/249], loss=77.8199
	step [25/249], loss=91.4351
	step [26/249], loss=91.2910
	step [27/249], loss=89.9817
	step [28/249], loss=86.7319
	step [29/249], loss=78.4887
	step [30/249], loss=104.6607
	step [31/249], loss=84.4112
	step [32/249], loss=88.4759
	step [33/249], loss=74.0683
	step [34/249], loss=85.3464
	step [35/249], loss=98.4154
	step [36/249], loss=82.0845
	step [37/249], loss=85.7505
	step [38/249], loss=90.3379
	step [39/249], loss=82.7333
	step [40/249], loss=105.7784
	step [41/249], loss=71.0053
	step [42/249], loss=79.4218
	step [43/249], loss=87.5694
	step [44/249], loss=93.4019
	step [45/249], loss=85.4803
	step [46/249], loss=80.0883
	step [47/249], loss=75.5825
	step [48/249], loss=75.2237
	step [49/249], loss=91.2082
	step [50/249], loss=69.5922
	step [51/249], loss=98.8744
	step [52/249], loss=71.3117
	step [53/249], loss=80.7988
	step [54/249], loss=91.6524
	step [55/249], loss=83.1070
	step [56/249], loss=101.5889
	step [57/249], loss=78.2822
	step [58/249], loss=93.6255
	step [59/249], loss=90.8249
	step [60/249], loss=103.1749
	step [61/249], loss=85.7958
	step [62/249], loss=83.6089
	step [63/249], loss=83.8163
	step [64/249], loss=92.4873
	step [65/249], loss=101.5369
	step [66/249], loss=75.5527
	step [67/249], loss=83.3766
	step [68/249], loss=84.2879
	step [69/249], loss=87.0252
	step [70/249], loss=90.0541
	step [71/249], loss=82.6609
	step [72/249], loss=90.3368
	step [73/249], loss=95.5378
	step [74/249], loss=76.7631
	step [75/249], loss=105.6664
	step [76/249], loss=87.4836
	step [77/249], loss=88.1975
	step [78/249], loss=85.5577
	step [79/249], loss=88.0434
	step [80/249], loss=93.5983
	step [81/249], loss=83.7769
	step [82/249], loss=102.9256
	step [83/249], loss=78.4893
	step [84/249], loss=88.6597
	step [85/249], loss=85.7430
	step [86/249], loss=86.8509
	step [87/249], loss=106.9993
	step [88/249], loss=73.3125
	step [89/249], loss=71.0665
	step [90/249], loss=93.5578
	step [91/249], loss=82.0783
	step [92/249], loss=99.8875
	step [93/249], loss=109.5537
	step [94/249], loss=91.7612
	step [95/249], loss=103.6840
	step [96/249], loss=89.0858
	step [97/249], loss=82.9842
	step [98/249], loss=89.7080
	step [99/249], loss=92.6701
	step [100/249], loss=96.1785
	step [101/249], loss=93.2197
	step [102/249], loss=78.7146
	step [103/249], loss=71.5256
	step [104/249], loss=76.1252
	step [105/249], loss=81.1849
	step [106/249], loss=96.8348
	step [107/249], loss=93.1435
	step [108/249], loss=76.8896
	step [109/249], loss=89.9117
	step [110/249], loss=101.7675
	step [111/249], loss=62.7975
	step [112/249], loss=96.0604
	step [113/249], loss=91.3314
	step [114/249], loss=83.8734
	step [115/249], loss=111.8893
	step [116/249], loss=101.7901
	step [117/249], loss=81.0257
	step [118/249], loss=97.5598
	step [119/249], loss=89.8868
	step [120/249], loss=81.4646
	step [121/249], loss=74.2677
	step [122/249], loss=96.0403
	step [123/249], loss=78.5607
	step [124/249], loss=107.2735
	step [125/249], loss=105.4843
	step [126/249], loss=83.3068
	step [127/249], loss=91.5983
	step [128/249], loss=93.2460
	step [129/249], loss=68.7533
	step [130/249], loss=85.1229
	step [131/249], loss=81.9208
	step [132/249], loss=106.1230
	step [133/249], loss=94.8119
	step [134/249], loss=99.4225
	step [135/249], loss=89.3618
	step [136/249], loss=83.5926
	step [137/249], loss=99.0590
	step [138/249], loss=81.7444
	step [139/249], loss=75.8995
	step [140/249], loss=84.8128
	step [141/249], loss=93.1107
	step [142/249], loss=99.1905
	step [143/249], loss=73.6550
	step [144/249], loss=100.9150
	step [145/249], loss=84.9032
	step [146/249], loss=82.4233
	step [147/249], loss=75.8835
	step [148/249], loss=61.8519
	step [149/249], loss=78.2365
	step [150/249], loss=81.4878
	step [151/249], loss=81.6065
	step [152/249], loss=92.0070
	step [153/249], loss=96.5108
	step [154/249], loss=75.3988
	step [155/249], loss=64.6355
	step [156/249], loss=86.3276
	step [157/249], loss=97.4744
	step [158/249], loss=93.4975
	step [159/249], loss=84.1313
	step [160/249], loss=75.9205
	step [161/249], loss=81.2132
	step [162/249], loss=96.0376
	step [163/249], loss=92.3063
	step [164/249], loss=85.0246
	step [165/249], loss=104.7290
	step [166/249], loss=94.8959
	step [167/249], loss=92.7459
	step [168/249], loss=91.0853
	step [169/249], loss=85.1537
	step [170/249], loss=100.1146
	step [171/249], loss=79.8670
	step [172/249], loss=95.8156
	step [173/249], loss=88.3876
	step [174/249], loss=83.6067
	step [175/249], loss=102.8043
	step [176/249], loss=85.8818
	step [177/249], loss=81.7335
	step [178/249], loss=96.9992
	step [179/249], loss=93.5692
	step [180/249], loss=77.3258
	step [181/249], loss=74.0913
	step [182/249], loss=98.9997
	step [183/249], loss=100.4373
	step [184/249], loss=78.3818
	step [185/249], loss=82.4061
	step [186/249], loss=95.1434
	step [187/249], loss=82.1544
	step [188/249], loss=94.7998
	step [189/249], loss=88.9141
	step [190/249], loss=81.1381
	step [191/249], loss=99.8046
	step [192/249], loss=92.8979
	step [193/249], loss=85.2759
	step [194/249], loss=85.6581
	step [195/249], loss=76.9953
	step [196/249], loss=86.2057
	step [197/249], loss=88.1867
	step [198/249], loss=75.9394
	step [199/249], loss=75.1594
	step [200/249], loss=113.0709
	step [201/249], loss=94.6894
	step [202/249], loss=109.9719
	step [203/249], loss=84.0755
	step [204/249], loss=106.9335
	step [205/249], loss=93.2160
	step [206/249], loss=76.9537
	step [207/249], loss=96.4834
	step [208/249], loss=81.9500
	step [209/249], loss=80.9799
	step [210/249], loss=83.7439
	step [211/249], loss=88.2025
	step [212/249], loss=90.2981
	step [213/249], loss=105.4243
	step [214/249], loss=88.0382
	step [215/249], loss=97.9309
	step [216/249], loss=80.9319
	step [217/249], loss=87.0552
	step [218/249], loss=103.1362
	step [219/249], loss=72.8608
	step [220/249], loss=88.5835
	step [221/249], loss=77.9887
	step [222/249], loss=79.9701
	step [223/249], loss=90.6446
	step [224/249], loss=93.3779
	step [225/249], loss=92.0060
	step [226/249], loss=79.8840
	step [227/249], loss=93.7472
	step [228/249], loss=96.2529
	step [229/249], loss=97.8159
	step [230/249], loss=74.9747
	step [231/249], loss=109.0615
	step [232/249], loss=83.5533
	step [233/249], loss=99.6239
	step [234/249], loss=100.3487
	step [235/249], loss=85.9669
	step [236/249], loss=86.4034
	step [237/249], loss=78.7918
	step [238/249], loss=76.6173
	step [239/249], loss=71.6541
	step [240/249], loss=97.6121
	step [241/249], loss=82.5118
	step [242/249], loss=96.1194
	step [243/249], loss=96.7415
	step [244/249], loss=103.2923
	step [245/249], loss=73.3326
	step [246/249], loss=87.5693
	step [247/249], loss=70.0712
	step [248/249], loss=89.2753
	step [249/249], loss=4.3072
	Evaluating
	loss=0.0067, precision=0.4098, recall=0.8600, f1=0.5551
Training epoch 30
	step [1/249], loss=82.5246
	step [2/249], loss=102.1801
	step [3/249], loss=78.8261
	step [4/249], loss=106.3806
	step [5/249], loss=68.7569
	step [6/249], loss=94.4352
	step [7/249], loss=92.7305
	step [8/249], loss=74.4518
	step [9/249], loss=87.7090
	step [10/249], loss=83.3378
	step [11/249], loss=83.3284
	step [12/249], loss=94.3329
	step [13/249], loss=82.9089
	step [14/249], loss=83.2314
	step [15/249], loss=84.1519
	step [16/249], loss=79.3420
	step [17/249], loss=86.0564
	step [18/249], loss=70.2764
	step [19/249], loss=72.2505
	step [20/249], loss=89.5375
	step [21/249], loss=102.5908
	step [22/249], loss=95.0699
	step [23/249], loss=83.9204
	step [24/249], loss=84.8089
	step [25/249], loss=79.6184
	step [26/249], loss=61.9749
	step [27/249], loss=91.4190
	step [28/249], loss=88.5819
	step [29/249], loss=77.8678
	step [30/249], loss=87.1620
	step [31/249], loss=100.1888
	step [32/249], loss=97.5874
	step [33/249], loss=97.5127
	step [34/249], loss=93.1462
	step [35/249], loss=72.4788
	step [36/249], loss=76.9225
	step [37/249], loss=96.0493
	step [38/249], loss=84.2781
	step [39/249], loss=79.7084
	step [40/249], loss=64.3559
	step [41/249], loss=90.5512
	step [42/249], loss=78.5482
	step [43/249], loss=100.7840
	step [44/249], loss=90.2749
	step [45/249], loss=104.2126
	step [46/249], loss=82.4749
	step [47/249], loss=85.7835
	step [48/249], loss=81.7577
	step [49/249], loss=86.7129
	step [50/249], loss=87.9586
	step [51/249], loss=72.7489
	step [52/249], loss=102.5926
	step [53/249], loss=93.6471
	step [54/249], loss=68.2666
	step [55/249], loss=95.2893
	step [56/249], loss=86.3098
	step [57/249], loss=105.9642
	step [58/249], loss=84.0712
	step [59/249], loss=93.9467
	step [60/249], loss=80.9402
	step [61/249], loss=87.8612
	step [62/249], loss=94.4362
	step [63/249], loss=97.6388
	step [64/249], loss=77.3741
	step [65/249], loss=86.1604
	step [66/249], loss=85.0421
	step [67/249], loss=75.1166
	step [68/249], loss=99.8738
	step [69/249], loss=83.9463
	step [70/249], loss=90.7654
	step [71/249], loss=103.2253
	step [72/249], loss=75.7568
	step [73/249], loss=76.2877
	step [74/249], loss=79.9509
	step [75/249], loss=101.9295
	step [76/249], loss=89.4088
	step [77/249], loss=92.4154
	step [78/249], loss=78.0403
	step [79/249], loss=84.0052
	step [80/249], loss=71.0975
	step [81/249], loss=95.7223
	step [82/249], loss=71.1565
	step [83/249], loss=99.1191
	step [84/249], loss=86.3905
	step [85/249], loss=96.3508
	step [86/249], loss=84.8404
	step [87/249], loss=86.6201
	step [88/249], loss=102.3285
	step [89/249], loss=76.6602
	step [90/249], loss=89.7784
	step [91/249], loss=86.2835
	step [92/249], loss=84.9729
	step [93/249], loss=68.1767
	step [94/249], loss=86.1271
	step [95/249], loss=106.0448
	step [96/249], loss=77.4874
	step [97/249], loss=93.6021
	step [98/249], loss=87.8079
	step [99/249], loss=84.0451
	step [100/249], loss=83.2489
	step [101/249], loss=91.7679
	step [102/249], loss=77.9778
	step [103/249], loss=68.6967
	step [104/249], loss=88.2117
	step [105/249], loss=79.9268
	step [106/249], loss=63.3995
	step [107/249], loss=93.1831
	step [108/249], loss=69.9816
	step [109/249], loss=94.4897
	step [110/249], loss=98.8740
	step [111/249], loss=81.7731
	step [112/249], loss=94.3251
	step [113/249], loss=64.6965
	step [114/249], loss=89.9024
	step [115/249], loss=96.9155
	step [116/249], loss=75.8139
	step [117/249], loss=87.7381
	step [118/249], loss=82.7014
	step [119/249], loss=107.6671
	step [120/249], loss=86.2665
	step [121/249], loss=103.0825
	step [122/249], loss=97.8235
	step [123/249], loss=79.8034
	step [124/249], loss=74.2763
	step [125/249], loss=79.0058
	step [126/249], loss=83.3615
	step [127/249], loss=80.3222
	step [128/249], loss=90.0923
	step [129/249], loss=89.6071
	step [130/249], loss=87.7013
	step [131/249], loss=92.0399
	step [132/249], loss=102.3822
	step [133/249], loss=83.7348
	step [134/249], loss=93.1927
	step [135/249], loss=86.8171
	step [136/249], loss=97.3000
	step [137/249], loss=83.1892
	step [138/249], loss=100.1984
	step [139/249], loss=95.2934
	step [140/249], loss=79.9981
	step [141/249], loss=84.3239
	step [142/249], loss=99.7239
	step [143/249], loss=89.8534
	step [144/249], loss=86.0122
	step [145/249], loss=109.0352
	step [146/249], loss=96.1147
	step [147/249], loss=99.4373
	step [148/249], loss=91.0828
	step [149/249], loss=73.1576
	step [150/249], loss=86.9218
	step [151/249], loss=89.0422
	step [152/249], loss=90.7448
	step [153/249], loss=104.5477
	step [154/249], loss=76.9764
	step [155/249], loss=75.5203
	step [156/249], loss=88.3260
	step [157/249], loss=78.4248
	step [158/249], loss=94.6568
	step [159/249], loss=83.6114
	step [160/249], loss=84.5526
	step [161/249], loss=98.1858
	step [162/249], loss=79.2597
	step [163/249], loss=82.0777
	step [164/249], loss=93.7070
	step [165/249], loss=96.7890
	step [166/249], loss=94.5525
	step [167/249], loss=99.4125
	step [168/249], loss=69.6459
	step [169/249], loss=64.1314
	step [170/249], loss=92.0231
	step [171/249], loss=107.2148
	step [172/249], loss=84.1719
	step [173/249], loss=90.6061
	step [174/249], loss=85.9228
	step [175/249], loss=76.8159
	step [176/249], loss=97.0959
	step [177/249], loss=90.2588
	step [178/249], loss=77.3787
	step [179/249], loss=98.5471
	step [180/249], loss=111.2786
	step [181/249], loss=89.3406
	step [182/249], loss=87.4981
	step [183/249], loss=78.1957
	step [184/249], loss=83.5556
	step [185/249], loss=96.6802
	step [186/249], loss=79.0067
	step [187/249], loss=86.7202
	step [188/249], loss=81.9151
	step [189/249], loss=65.1215
	step [190/249], loss=88.0909
	step [191/249], loss=119.1092
	step [192/249], loss=79.9626
	step [193/249], loss=87.3942
	step [194/249], loss=91.5955
	step [195/249], loss=86.9389
	step [196/249], loss=92.1841
	step [197/249], loss=99.7124
	step [198/249], loss=94.6779
	step [199/249], loss=80.4493
	step [200/249], loss=82.3597
	step [201/249], loss=111.3430
	step [202/249], loss=99.5879
	step [203/249], loss=76.7818
	step [204/249], loss=91.2929
	step [205/249], loss=88.9356
	step [206/249], loss=107.3164
	step [207/249], loss=77.7725
	step [208/249], loss=92.5912
	step [209/249], loss=88.9354
	step [210/249], loss=83.1832
	step [211/249], loss=88.9871
	step [212/249], loss=95.5069
	step [213/249], loss=81.8529
	step [214/249], loss=88.2512
	step [215/249], loss=81.6786
	step [216/249], loss=80.0353
	step [217/249], loss=95.0358
	step [218/249], loss=81.1772
	step [219/249], loss=81.1153
	step [220/249], loss=73.2117
	step [221/249], loss=89.8443
	step [222/249], loss=81.0089
	step [223/249], loss=94.8019
	step [224/249], loss=78.6363
	step [225/249], loss=110.1628
	step [226/249], loss=83.5163
	step [227/249], loss=94.8668
	step [228/249], loss=92.0580
	step [229/249], loss=61.1856
	step [230/249], loss=88.3603
	step [231/249], loss=95.2010
	step [232/249], loss=91.8484
	step [233/249], loss=99.3850
	step [234/249], loss=76.6938
	step [235/249], loss=79.8921
	step [236/249], loss=86.2956
	step [237/249], loss=85.5594
	step [238/249], loss=85.6169
	step [239/249], loss=83.4027
	step [240/249], loss=87.5015
	step [241/249], loss=80.2859
	step [242/249], loss=72.3370
	step [243/249], loss=88.1830
	step [244/249], loss=90.1779
	step [245/249], loss=90.3801
	step [246/249], loss=77.2403
	step [247/249], loss=79.3504
	step [248/249], loss=88.7875
	step [249/249], loss=5.5316
	Evaluating
	loss=0.0056, precision=0.4810, recall=0.8810, f1=0.6222
Training finished
best_f1: 0.6870931951758062
directing: Z rim_enhanced: True test_id 3
removed wrong scan: weights_Z_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_278_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_276_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_298_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_210_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_262_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_254_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_58_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_293_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_291_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_206_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_252_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_265_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_295_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_221_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_292_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_294_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_297_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_214_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_296_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_225_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_300_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_243_xwqg-A00121_2019-05-15.npy
# all image files: 15579 # all weight files in weight_dir: 12484 # image files with weight 12424
removed wrong scan: weights_Z_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_278_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_276_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_298_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_210_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_262_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_254_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_58_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_293_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_291_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_206_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_252_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_265_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_295_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_221_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_292_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_294_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_297_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_214_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_296_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_225_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_300_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_243_xwqg-A00121_2019-05-15.npy
# all image files: 15579 # all weight files in weight_dir: 3060 # image files with weight 3060
train: /ibex/scratch/projects/c2052/air_tube_seg/training_samples_enhanced_one/Z 12424
Using 4 GPUs
Going to train epochs [1-30]
Training epoch 1
	step [1/195], loss=1077.2592
	step [2/195], loss=712.4337
	step [3/195], loss=471.6994
	step [4/195], loss=294.8649
	step [5/195], loss=272.6995
	step [6/195], loss=253.3291
	step [7/195], loss=252.8363
	step [8/195], loss=257.9337
	step [9/195], loss=248.6020
	step [10/195], loss=276.1667
	step [11/195], loss=234.7721
	step [12/195], loss=270.0114
	step [13/195], loss=239.1019
	step [14/195], loss=223.6147
	step [15/195], loss=254.8948
	step [16/195], loss=251.6005
	step [17/195], loss=239.9667
	step [18/195], loss=262.7611
	step [19/195], loss=245.5344
	step [20/195], loss=234.1690
	step [21/195], loss=247.0208
	step [22/195], loss=225.2424
	step [23/195], loss=220.0309
	step [24/195], loss=236.7446
	step [25/195], loss=244.1022
	step [26/195], loss=236.5045
	step [27/195], loss=210.9012
	step [28/195], loss=230.7164
	step [29/195], loss=220.7750
	step [30/195], loss=225.6089
	step [31/195], loss=228.3452
	step [32/195], loss=203.9819
	step [33/195], loss=217.8904
	step [34/195], loss=213.8367
	step [35/195], loss=195.1499
	step [36/195], loss=202.7987
	step [37/195], loss=207.2875
	step [38/195], loss=206.0882
	step [39/195], loss=207.2569
	step [40/195], loss=213.7668
	step [41/195], loss=198.4336
	step [42/195], loss=192.9900
	step [43/195], loss=203.1838
	step [44/195], loss=212.8479
	step [45/195], loss=224.7849
	step [46/195], loss=207.2112
	step [47/195], loss=205.1660
	step [48/195], loss=202.8255
	step [49/195], loss=207.5124
	step [50/195], loss=201.6802
	step [51/195], loss=204.1185
	step [52/195], loss=193.4162
	step [53/195], loss=207.4829
	step [54/195], loss=189.5128
	step [55/195], loss=196.5229
	step [56/195], loss=188.7034
	step [57/195], loss=183.4023
	step [58/195], loss=198.4743
	step [59/195], loss=209.7045
	step [60/195], loss=194.5069
	step [61/195], loss=179.7887
	step [62/195], loss=214.1636
	step [63/195], loss=200.2656
	step [64/195], loss=193.9996
	step [65/195], loss=175.3369
	step [66/195], loss=203.8265
	step [67/195], loss=201.1763
	step [68/195], loss=183.1947
	step [69/195], loss=199.1629
	step [70/195], loss=214.9992
	step [71/195], loss=189.0526
	step [72/195], loss=195.4677
	step [73/195], loss=194.4047
	step [74/195], loss=180.0985
	step [75/195], loss=186.1048
	step [76/195], loss=177.9770
	step [77/195], loss=190.4270
	step [78/195], loss=183.8064
	step [79/195], loss=183.4052
	step [80/195], loss=181.8248
	step [81/195], loss=192.6356
	step [82/195], loss=225.4435
	step [83/195], loss=182.1519
	step [84/195], loss=179.2081
	step [85/195], loss=184.1057
	step [86/195], loss=172.1552
	step [87/195], loss=160.4909
	step [88/195], loss=170.0568
	step [89/195], loss=188.5373
	step [90/195], loss=187.9573
	step [91/195], loss=166.3577
	step [92/195], loss=192.5070
	step [93/195], loss=167.0683
	step [94/195], loss=167.7407
	step [95/195], loss=201.7150
	step [96/195], loss=188.3233
	step [97/195], loss=181.1815
	step [98/195], loss=187.7769
	step [99/195], loss=172.5894
	step [100/195], loss=199.1035
	step [101/195], loss=157.5811
	step [102/195], loss=186.8179
	step [103/195], loss=168.9447
	step [104/195], loss=189.8789
	step [105/195], loss=175.2604
	step [106/195], loss=175.0318
	step [107/195], loss=206.5433
	step [108/195], loss=184.0686
	step [109/195], loss=151.2181
	step [110/195], loss=185.1668
	step [111/195], loss=201.0624
	step [112/195], loss=179.9571
	step [113/195], loss=169.6077
	step [114/195], loss=181.4577
	step [115/195], loss=162.8309
	step [116/195], loss=166.6867
	step [117/195], loss=170.4578
	step [118/195], loss=182.1854
	step [119/195], loss=175.3663
	step [120/195], loss=180.5366
	step [121/195], loss=157.4105
	step [122/195], loss=187.5025
	step [123/195], loss=182.1856
	step [124/195], loss=168.7650
	step [125/195], loss=158.6612
	step [126/195], loss=183.3718
	step [127/195], loss=170.7320
	step [128/195], loss=173.9697
	step [129/195], loss=153.5108
	step [130/195], loss=165.2876
	step [131/195], loss=173.5836
	step [132/195], loss=171.5550
	step [133/195], loss=148.1048
	step [134/195], loss=166.1983
	step [135/195], loss=181.8805
	step [136/195], loss=175.7728
	step [137/195], loss=153.5355
	step [138/195], loss=173.2830
	step [139/195], loss=156.0728
	step [140/195], loss=181.7660
	step [141/195], loss=172.0977
	step [142/195], loss=164.2384
	step [143/195], loss=158.0184
	step [144/195], loss=172.2043
	step [145/195], loss=154.4419
	step [146/195], loss=157.7514
	step [147/195], loss=193.0441
	step [148/195], loss=171.2037
	step [149/195], loss=180.5217
	step [150/195], loss=167.8387
	step [151/195], loss=166.1273
	step [152/195], loss=159.4688
	step [153/195], loss=168.7539
	step [154/195], loss=164.4230
	step [155/195], loss=160.2239
	step [156/195], loss=183.5060
	step [157/195], loss=165.7497
	step [158/195], loss=167.4672
	step [159/195], loss=184.1179
	step [160/195], loss=176.8510
	step [161/195], loss=154.5640
	step [162/195], loss=171.3062
	step [163/195], loss=162.7734
	step [164/195], loss=155.9192
	step [165/195], loss=165.7586
	step [166/195], loss=168.3450
	step [167/195], loss=161.6273
	step [168/195], loss=168.3953
	step [169/195], loss=141.2846
	step [170/195], loss=159.2720
	step [171/195], loss=182.3234
	step [172/195], loss=158.7583
	step [173/195], loss=174.0139
	step [174/195], loss=166.2141
	step [175/195], loss=149.4423
	step [176/195], loss=163.8283
	step [177/195], loss=153.8898
	step [178/195], loss=185.5131
	step [179/195], loss=155.0646
	step [180/195], loss=175.2531
	step [181/195], loss=146.0447
	step [182/195], loss=169.8485
	step [183/195], loss=141.9176
	step [184/195], loss=148.9056
	step [185/195], loss=145.0885
	step [186/195], loss=172.0554
	step [187/195], loss=154.9360
	step [188/195], loss=149.6386
	step [189/195], loss=157.5275
	step [190/195], loss=170.6940
	step [191/195], loss=168.2452
	step [192/195], loss=161.0033
	step [193/195], loss=176.0985
	step [194/195], loss=156.4577
	step [195/195], loss=14.8025
	Evaluating
	loss=0.2390, precision=0.3921, recall=0.9428, f1=0.5539
saving model as: 3_saved_model.pth
Training epoch 2
	step [1/195], loss=166.9984
	step [2/195], loss=157.1780
	step [3/195], loss=146.0894
	step [4/195], loss=178.2604
	step [5/195], loss=151.4033
	step [6/195], loss=135.0078
	step [7/195], loss=165.3346
	step [8/195], loss=169.1059
	step [9/195], loss=151.9804
	step [10/195], loss=155.6476
	step [11/195], loss=148.1045
	step [12/195], loss=184.1918
	step [13/195], loss=139.9396
	step [14/195], loss=167.0060
	step [15/195], loss=166.2486
	step [16/195], loss=169.7335
	step [17/195], loss=150.3360
	step [18/195], loss=158.4594
	step [19/195], loss=162.9012
	step [20/195], loss=142.6491
	step [21/195], loss=158.3548
	step [22/195], loss=137.7015
	step [23/195], loss=156.5851
	step [24/195], loss=134.6008
	step [25/195], loss=148.2240
	step [26/195], loss=167.8502
	step [27/195], loss=147.5215
	step [28/195], loss=146.8938
	step [29/195], loss=149.4843
	step [30/195], loss=166.2935
	step [31/195], loss=146.8997
	step [32/195], loss=173.6534
	step [33/195], loss=173.9939
	step [34/195], loss=159.1105
	step [35/195], loss=145.8432
	step [36/195], loss=171.3791
	step [37/195], loss=159.8320
	step [38/195], loss=178.8198
	step [39/195], loss=156.1358
	step [40/195], loss=159.0992
	step [41/195], loss=136.2949
	step [42/195], loss=157.4790
	step [43/195], loss=146.6519
	step [44/195], loss=168.1208
	step [45/195], loss=153.8143
	step [46/195], loss=165.8106
	step [47/195], loss=152.9346
	step [48/195], loss=136.0514
	step [49/195], loss=147.4738
	step [50/195], loss=134.0388
	step [51/195], loss=152.9163
	step [52/195], loss=147.9484
	step [53/195], loss=163.6744
	step [54/195], loss=161.5850
	step [55/195], loss=186.9107
	step [56/195], loss=184.7001
	step [57/195], loss=158.0015
	step [58/195], loss=155.1289
	step [59/195], loss=159.6597
	step [60/195], loss=176.9034
	step [61/195], loss=148.8488
	step [62/195], loss=158.4636
	step [63/195], loss=169.8119
	step [64/195], loss=172.2143
	step [65/195], loss=146.1531
	step [66/195], loss=163.0103
	step [67/195], loss=147.9088
	step [68/195], loss=166.4582
	step [69/195], loss=149.8756
	step [70/195], loss=157.7334
	step [71/195], loss=150.1476
	step [72/195], loss=139.0490
	step [73/195], loss=130.0821
	step [74/195], loss=156.4520
	step [75/195], loss=165.7239
	step [76/195], loss=152.7660
	step [77/195], loss=143.6959
	step [78/195], loss=168.0747
	step [79/195], loss=159.1656
	step [80/195], loss=144.2390
	step [81/195], loss=139.3479
	step [82/195], loss=156.5269
	step [83/195], loss=159.3884
	step [84/195], loss=148.2582
	step [85/195], loss=163.4841
	step [86/195], loss=163.3363
	step [87/195], loss=145.9830
	step [88/195], loss=156.6957
	step [89/195], loss=134.5535
	step [90/195], loss=145.9774
	step [91/195], loss=154.8766
	step [92/195], loss=143.8653
	step [93/195], loss=161.9323
	step [94/195], loss=139.9080
	step [95/195], loss=157.1295
	step [96/195], loss=140.3764
	step [97/195], loss=146.5292
	step [98/195], loss=139.7146
	step [99/195], loss=166.5461
	step [100/195], loss=149.6879
	step [101/195], loss=148.9864
	step [102/195], loss=165.6105
	step [103/195], loss=164.5749
	step [104/195], loss=128.7781
	step [105/195], loss=138.5745
	step [106/195], loss=138.9319
	step [107/195], loss=122.7949
	step [108/195], loss=134.4298
	step [109/195], loss=153.3313
	step [110/195], loss=155.6042
	step [111/195], loss=144.5915
	step [112/195], loss=136.6359
	step [113/195], loss=172.6440
	step [114/195], loss=138.7102
	step [115/195], loss=152.9394
	step [116/195], loss=145.4391
	step [117/195], loss=136.1340
	step [118/195], loss=155.1274
	step [119/195], loss=173.1926
	step [120/195], loss=142.7766
	step [121/195], loss=137.2146
	step [122/195], loss=149.7580
	step [123/195], loss=152.3820
	step [124/195], loss=152.3526
	step [125/195], loss=137.8822
	step [126/195], loss=153.5147
	step [127/195], loss=137.1495
	step [128/195], loss=134.6946
	step [129/195], loss=147.0030
	step [130/195], loss=152.8126
	step [131/195], loss=142.3707
	step [132/195], loss=167.6060
	step [133/195], loss=165.3139
	step [134/195], loss=154.2753
	step [135/195], loss=140.6981
	step [136/195], loss=145.9743
	step [137/195], loss=146.2979
	step [138/195], loss=141.1115
	step [139/195], loss=153.3406
	step [140/195], loss=151.9895
	step [141/195], loss=129.3866
	step [142/195], loss=157.7256
	step [143/195], loss=154.3875
	step [144/195], loss=150.2623
	step [145/195], loss=158.6246
	step [146/195], loss=133.5365
	step [147/195], loss=162.0495
	step [148/195], loss=155.3771
	step [149/195], loss=142.3983
	step [150/195], loss=149.9076
	step [151/195], loss=135.5846
	step [152/195], loss=150.7798
	step [153/195], loss=146.0567
	step [154/195], loss=140.0618
	step [155/195], loss=166.2321
	step [156/195], loss=124.6503
	step [157/195], loss=152.1154
	step [158/195], loss=139.4818
	step [159/195], loss=152.0489
	step [160/195], loss=138.0941
	step [161/195], loss=139.4183
	step [162/195], loss=156.5261
	step [163/195], loss=137.9014
	step [164/195], loss=136.2683
	step [165/195], loss=132.6992
	step [166/195], loss=159.2860
	step [167/195], loss=125.9706
	step [168/195], loss=155.3117
	step [169/195], loss=133.4957
	step [170/195], loss=132.0058
	step [171/195], loss=146.1010
	step [172/195], loss=134.8445
	step [173/195], loss=157.2535
	step [174/195], loss=155.0869
	step [175/195], loss=123.2282
	step [176/195], loss=155.9658
	step [177/195], loss=144.4862
	step [178/195], loss=145.5531
	step [179/195], loss=139.0683
	step [180/195], loss=144.3539
	step [181/195], loss=150.2450
	step [182/195], loss=140.8368
	step [183/195], loss=135.4355
	step [184/195], loss=144.6655
	step [185/195], loss=153.2784
	step [186/195], loss=137.0764
	step [187/195], loss=142.2225
	step [188/195], loss=135.7835
	step [189/195], loss=152.8799
	step [190/195], loss=143.6022
	step [191/195], loss=142.5036
	step [192/195], loss=135.3252
	step [193/195], loss=138.2966
	step [194/195], loss=136.9109
	step [195/195], loss=21.0826
	Evaluating
	loss=0.1731, precision=0.5106, recall=0.9219, f1=0.6572
saving model as: 3_saved_model.pth
Training epoch 3
	step [1/195], loss=134.9772
	step [2/195], loss=141.3493
	step [3/195], loss=139.2088
	step [4/195], loss=131.5278
	step [5/195], loss=136.0358
	step [6/195], loss=153.9334
	step [7/195], loss=122.3034
	step [8/195], loss=128.1794
	step [9/195], loss=147.6090
	step [10/195], loss=133.8418
	step [11/195], loss=135.4497
	step [12/195], loss=121.6582
	step [13/195], loss=145.8555
	step [14/195], loss=113.4900
	step [15/195], loss=148.3530
	step [16/195], loss=137.7164
	step [17/195], loss=137.7070
	step [18/195], loss=148.7063
	step [19/195], loss=137.2211
	step [20/195], loss=141.6935
	step [21/195], loss=143.2916
	step [22/195], loss=152.6045
	step [23/195], loss=134.2014
	step [24/195], loss=133.1744
	step [25/195], loss=150.0599
	step [26/195], loss=131.9800
	step [27/195], loss=140.5679
	step [28/195], loss=145.7602
	step [29/195], loss=134.8388
	step [30/195], loss=156.8027
	step [31/195], loss=131.1457
	step [32/195], loss=130.1783
	step [33/195], loss=152.8936
	step [34/195], loss=145.3182
	step [35/195], loss=132.3574
	step [36/195], loss=121.9451
	step [37/195], loss=141.9129
	step [38/195], loss=142.3672
	step [39/195], loss=144.2950
	step [40/195], loss=135.6285
	step [41/195], loss=135.9285
	step [42/195], loss=144.1587
	step [43/195], loss=131.8507
	step [44/195], loss=130.8293
	step [45/195], loss=153.1403
	step [46/195], loss=129.7543
	step [47/195], loss=143.5490
	step [48/195], loss=144.1939
	step [49/195], loss=146.8669
	step [50/195], loss=126.6509
	step [51/195], loss=138.1738
	step [52/195], loss=139.7803
	step [53/195], loss=155.7177
	step [54/195], loss=130.4942
	step [55/195], loss=117.7085
	step [56/195], loss=134.6458
	step [57/195], loss=135.3943
	step [58/195], loss=151.5912
	step [59/195], loss=154.6622
	step [60/195], loss=144.1288
	step [61/195], loss=134.3888
	step [62/195], loss=139.6767
	step [63/195], loss=130.1203
	step [64/195], loss=128.7574
	step [65/195], loss=156.8049
	step [66/195], loss=122.0156
	step [67/195], loss=134.8239
	step [68/195], loss=130.8640
	step [69/195], loss=132.1311
	step [70/195], loss=125.2753
	step [71/195], loss=136.7968
	step [72/195], loss=142.8634
	step [73/195], loss=160.6684
	step [74/195], loss=156.2717
	step [75/195], loss=145.5112
	step [76/195], loss=143.3981
	step [77/195], loss=118.5005
	step [78/195], loss=142.6324
	step [79/195], loss=146.8051
	step [80/195], loss=146.9664
	step [81/195], loss=146.8233
	step [82/195], loss=141.3217
	step [83/195], loss=123.1840
	step [84/195], loss=143.1735
	step [85/195], loss=139.5194
	step [86/195], loss=144.2078
	step [87/195], loss=136.8153
	step [88/195], loss=152.3007
	step [89/195], loss=143.5003
	step [90/195], loss=149.3672
	step [91/195], loss=141.6432
	step [92/195], loss=131.4729
	step [93/195], loss=155.7551
	step [94/195], loss=123.7607
	step [95/195], loss=144.6561
	step [96/195], loss=159.3220
	step [97/195], loss=152.7960
	step [98/195], loss=143.1155
	step [99/195], loss=141.3139
	step [100/195], loss=136.4670
	step [101/195], loss=148.2649
	step [102/195], loss=147.6347
	step [103/195], loss=139.3734
	step [104/195], loss=147.2809
	step [105/195], loss=153.4349
	step [106/195], loss=133.5061
	step [107/195], loss=124.1195
	step [108/195], loss=139.1454
	step [109/195], loss=130.1211
	step [110/195], loss=155.2069
	step [111/195], loss=154.5709
	step [112/195], loss=123.1424
	step [113/195], loss=139.7025
	step [114/195], loss=141.5739
	step [115/195], loss=148.1704
	step [116/195], loss=159.1063
	step [117/195], loss=138.4442
	step [118/195], loss=134.1804
	step [119/195], loss=147.6637
	step [120/195], loss=138.9656
	step [121/195], loss=152.1942
	step [122/195], loss=153.2819
	step [123/195], loss=112.3679
	step [124/195], loss=146.5131
	step [125/195], loss=135.5529
	step [126/195], loss=122.1657
	step [127/195], loss=129.7693
	step [128/195], loss=131.2752
	step [129/195], loss=141.2473
	step [130/195], loss=139.2744
	step [131/195], loss=138.4129
	step [132/195], loss=135.6370
	step [133/195], loss=133.1244
	step [134/195], loss=142.6105
	step [135/195], loss=130.3774
	step [136/195], loss=128.4415
	step [137/195], loss=115.3987
	step [138/195], loss=140.0385
	step [139/195], loss=138.9899
	step [140/195], loss=124.2656
	step [141/195], loss=127.9371
	step [142/195], loss=130.4452
	step [143/195], loss=137.9507
	step [144/195], loss=155.5283
	step [145/195], loss=134.0232
	step [146/195], loss=140.7494
	step [147/195], loss=135.7386
	step [148/195], loss=129.4265
	step [149/195], loss=156.3264
	step [150/195], loss=148.2687
	step [151/195], loss=133.0044
	step [152/195], loss=126.2796
	step [153/195], loss=140.0507
	step [154/195], loss=126.9177
	step [155/195], loss=157.3320
	step [156/195], loss=133.0674
	step [157/195], loss=138.1156
	step [158/195], loss=139.4495
	step [159/195], loss=141.2721
	step [160/195], loss=131.3973
	step [161/195], loss=134.8431
	step [162/195], loss=139.3770
	step [163/195], loss=141.9856
	step [164/195], loss=127.6991
	step [165/195], loss=132.7737
	step [166/195], loss=113.4069
	step [167/195], loss=140.2353
	step [168/195], loss=131.6765
	step [169/195], loss=133.9634
	step [170/195], loss=143.5247
	step [171/195], loss=137.8425
	step [172/195], loss=122.8837
	step [173/195], loss=128.7987
	step [174/195], loss=117.6214
	step [175/195], loss=145.2121
	step [176/195], loss=125.1945
	step [177/195], loss=136.3212
	step [178/195], loss=130.1519
	step [179/195], loss=127.7082
	step [180/195], loss=125.7734
	step [181/195], loss=141.6220
	step [182/195], loss=129.3141
	step [183/195], loss=124.2491
	step [184/195], loss=130.0392
	step [185/195], loss=106.7178
	step [186/195], loss=153.0506
	step [187/195], loss=130.3749
	step [188/195], loss=122.8832
	step [189/195], loss=134.7778
	step [190/195], loss=124.1318
	step [191/195], loss=131.3175
	step [192/195], loss=130.5788
	step [193/195], loss=144.3447
	step [194/195], loss=138.1766
	step [195/195], loss=18.1641
	Evaluating
	loss=0.1340, precision=0.4980, recall=0.9082, f1=0.6433
Training epoch 4
	step [1/195], loss=116.8589
	step [2/195], loss=118.9400
	step [3/195], loss=131.3436
	step [4/195], loss=145.8389
	step [5/195], loss=133.3013
	step [6/195], loss=125.7662
	step [7/195], loss=125.1192
	step [8/195], loss=136.3494
	step [9/195], loss=126.1341
	step [10/195], loss=133.7412
	step [11/195], loss=132.7197
	step [12/195], loss=153.6301
	step [13/195], loss=140.8465
	step [14/195], loss=134.5506
	step [15/195], loss=147.1081
	step [16/195], loss=108.4069
	step [17/195], loss=131.1327
	step [18/195], loss=145.1246
	step [19/195], loss=129.4341
	step [20/195], loss=116.4261
	step [21/195], loss=135.4039
	step [22/195], loss=132.9781
	step [23/195], loss=122.1630
	step [24/195], loss=135.1216
	step [25/195], loss=124.6986
	step [26/195], loss=127.8375
	step [27/195], loss=136.6213
	step [28/195], loss=118.0708
	step [29/195], loss=147.1081
	step [30/195], loss=121.7616
	step [31/195], loss=147.6026
	step [32/195], loss=131.5979
	step [33/195], loss=129.3732
	step [34/195], loss=147.1727
	step [35/195], loss=127.7584
	step [36/195], loss=139.7225
	step [37/195], loss=118.6315
	step [38/195], loss=131.4023
	step [39/195], loss=150.4661
	step [40/195], loss=124.2060
	step [41/195], loss=144.8672
	step [42/195], loss=130.5918
	step [43/195], loss=126.1494
	step [44/195], loss=116.9208
	step [45/195], loss=118.8934
	step [46/195], loss=142.3969
	step [47/195], loss=120.6190
	step [48/195], loss=135.9510
	step [49/195], loss=124.1786
	step [50/195], loss=122.1587
	step [51/195], loss=115.3759
	step [52/195], loss=133.0580
	step [53/195], loss=125.2169
	step [54/195], loss=125.4164
	step [55/195], loss=128.2405
	step [56/195], loss=122.0208
	step [57/195], loss=145.1510
	step [58/195], loss=155.8036
	step [59/195], loss=131.9498
	step [60/195], loss=123.3642
	step [61/195], loss=129.0960
	step [62/195], loss=148.3708
	step [63/195], loss=132.3083
	step [64/195], loss=110.2475
	step [65/195], loss=146.3075
	step [66/195], loss=143.9281
	step [67/195], loss=135.9215
	step [68/195], loss=120.6077
	step [69/195], loss=126.1013
	step [70/195], loss=123.0583
	step [71/195], loss=138.4995
	step [72/195], loss=128.2776
	step [73/195], loss=129.5063
	step [74/195], loss=110.8564
	step [75/195], loss=117.2328
	step [76/195], loss=123.9695
	step [77/195], loss=139.3458
	step [78/195], loss=127.2250
	step [79/195], loss=138.1576
	step [80/195], loss=129.0910
	step [81/195], loss=139.3506
	step [82/195], loss=124.9428
	step [83/195], loss=125.1803
	step [84/195], loss=113.8375
	step [85/195], loss=136.9410
	step [86/195], loss=124.9758
	step [87/195], loss=124.8488
	step [88/195], loss=129.7782
	step [89/195], loss=139.9745
	step [90/195], loss=124.0903
	step [91/195], loss=139.0223
	step [92/195], loss=138.1740
	step [93/195], loss=120.4712
	step [94/195], loss=124.2757
	step [95/195], loss=127.6727
	step [96/195], loss=125.6285
	step [97/195], loss=131.2970
	step [98/195], loss=125.1021
	step [99/195], loss=130.1680
	step [100/195], loss=116.2134
	step [101/195], loss=132.5175
	step [102/195], loss=119.3405
	step [103/195], loss=118.3147
	step [104/195], loss=134.8047
	step [105/195], loss=133.0014
	step [106/195], loss=141.8381
	step [107/195], loss=139.6566
	step [108/195], loss=133.6527
	step [109/195], loss=144.0595
	step [110/195], loss=128.2178
	step [111/195], loss=133.8195
	step [112/195], loss=125.2196
	step [113/195], loss=130.4938
	step [114/195], loss=121.5163
	step [115/195], loss=129.1751
	step [116/195], loss=131.8833
	step [117/195], loss=150.8372
	step [118/195], loss=132.0045
	step [119/195], loss=122.0203
	step [120/195], loss=121.3482
	step [121/195], loss=117.6787
	step [122/195], loss=105.2657
	step [123/195], loss=144.6802
	step [124/195], loss=130.0915
	step [125/195], loss=133.6900
	step [126/195], loss=138.7218
	step [127/195], loss=120.7555
	step [128/195], loss=134.9185
	step [129/195], loss=152.2667
	step [130/195], loss=120.0528
	step [131/195], loss=120.3602
	step [132/195], loss=128.4582
	step [133/195], loss=126.8781
	step [134/195], loss=142.4200
	step [135/195], loss=135.7230
	step [136/195], loss=136.7022
	step [137/195], loss=116.7830
	step [138/195], loss=135.1102
	step [139/195], loss=133.5890
	step [140/195], loss=122.1053
	step [141/195], loss=126.1622
	step [142/195], loss=130.9500
	step [143/195], loss=132.3311
	step [144/195], loss=137.1835
	step [145/195], loss=124.2243
	step [146/195], loss=142.1862
	step [147/195], loss=121.7393
	step [148/195], loss=128.9527
	step [149/195], loss=117.4527
	step [150/195], loss=111.9576
	step [151/195], loss=140.7705
	step [152/195], loss=138.6183
	step [153/195], loss=125.7385
	step [154/195], loss=129.5919
	step [155/195], loss=142.6396
	step [156/195], loss=116.2573
	step [157/195], loss=132.3288
	step [158/195], loss=110.3204
	step [159/195], loss=125.0954
	step [160/195], loss=127.8326
	step [161/195], loss=111.6351
	step [162/195], loss=123.0895
	step [163/195], loss=140.1429
	step [164/195], loss=122.3476
	step [165/195], loss=117.1939
	step [166/195], loss=118.7928
	step [167/195], loss=110.7856
	step [168/195], loss=133.4868
	step [169/195], loss=122.5999
	step [170/195], loss=134.8483
	step [171/195], loss=120.0596
	step [172/195], loss=129.6898
	step [173/195], loss=140.3031
	step [174/195], loss=128.2458
	step [175/195], loss=116.8913
	step [176/195], loss=125.2296
	step [177/195], loss=132.2814
	step [178/195], loss=134.3403
	step [179/195], loss=122.0666
	step [180/195], loss=112.8954
	step [181/195], loss=131.2376
	step [182/195], loss=127.7191
	step [183/195], loss=137.0071
	step [184/195], loss=120.0626
	step [185/195], loss=132.1430
	step [186/195], loss=134.4837
	step [187/195], loss=125.5569
	step [188/195], loss=132.4601
	step [189/195], loss=132.9295
	step [190/195], loss=112.2853
	step [191/195], loss=127.4321
	step [192/195], loss=124.7214
	step [193/195], loss=138.6351
	step [194/195], loss=126.7285
	step [195/195], loss=23.2598
	Evaluating
	loss=0.1045, precision=0.4545, recall=0.9243, f1=0.6094
Training epoch 5
	step [1/195], loss=129.3147
	step [2/195], loss=120.9979
	step [3/195], loss=107.8051
	step [4/195], loss=103.0160
	step [5/195], loss=125.0208
	step [6/195], loss=136.5079
	step [7/195], loss=115.6992
	step [8/195], loss=130.4297
	step [9/195], loss=119.1797
	step [10/195], loss=119.4819
	step [11/195], loss=138.9989
	step [12/195], loss=142.6938
	step [13/195], loss=132.2403
	step [14/195], loss=117.0441
	step [15/195], loss=126.7224
	step [16/195], loss=126.4324
	step [17/195], loss=134.5373
	step [18/195], loss=129.7660
	step [19/195], loss=118.0847
	step [20/195], loss=129.2129
	step [21/195], loss=126.4764
	step [22/195], loss=126.5395
	step [23/195], loss=121.8526
	step [24/195], loss=115.5032
	step [25/195], loss=135.0110
	step [26/195], loss=126.9056
	step [27/195], loss=126.4194
	step [28/195], loss=109.4268
	step [29/195], loss=124.5406
	step [30/195], loss=118.0510
	step [31/195], loss=135.0657
	step [32/195], loss=128.2076
	step [33/195], loss=107.2391
	step [34/195], loss=116.9675
	step [35/195], loss=124.6319
	step [36/195], loss=122.3092
	step [37/195], loss=109.4207
	step [38/195], loss=106.5426
	step [39/195], loss=119.2212
	step [40/195], loss=124.2998
	step [41/195], loss=134.0950
	step [42/195], loss=118.1046
	step [43/195], loss=120.0686
	step [44/195], loss=114.4077
	step [45/195], loss=113.4308
	step [46/195], loss=118.4053
	step [47/195], loss=108.6579
	step [48/195], loss=141.3048
	step [49/195], loss=123.7718
	step [50/195], loss=133.7398
	step [51/195], loss=132.6208
	step [52/195], loss=137.5101
	step [53/195], loss=107.1517
	step [54/195], loss=122.2264
	step [55/195], loss=110.1920
	step [56/195], loss=122.7477
	step [57/195], loss=141.2991
	step [58/195], loss=113.6400
	step [59/195], loss=127.0126
	step [60/195], loss=147.5020
	step [61/195], loss=133.0406
	step [62/195], loss=123.9151
	step [63/195], loss=149.9608
	step [64/195], loss=124.6381
	step [65/195], loss=119.4020
	step [66/195], loss=136.2020
	step [67/195], loss=121.9770
	step [68/195], loss=125.4279
	step [69/195], loss=114.7026
	step [70/195], loss=96.4724
	step [71/195], loss=135.9257
	step [72/195], loss=141.8080
	step [73/195], loss=127.1249
	step [74/195], loss=119.6444
	step [75/195], loss=123.1027
	step [76/195], loss=109.0122
	step [77/195], loss=111.7691
	step [78/195], loss=129.1825
	step [79/195], loss=134.2275
	step [80/195], loss=117.8578
	step [81/195], loss=125.7010
	step [82/195], loss=135.3449
	step [83/195], loss=125.3960
	step [84/195], loss=109.7684
	step [85/195], loss=123.1157
	step [86/195], loss=114.9346
	step [87/195], loss=123.9227
	step [88/195], loss=113.0636
	step [89/195], loss=138.2790
	step [90/195], loss=113.4123
	step [91/195], loss=139.3989
	step [92/195], loss=138.6995
	step [93/195], loss=126.9128
	step [94/195], loss=126.3240
	step [95/195], loss=118.8549
	step [96/195], loss=122.1562
	step [97/195], loss=135.3381
	step [98/195], loss=141.5585
	step [99/195], loss=138.5469
	step [100/195], loss=118.5882
	step [101/195], loss=118.2200
	step [102/195], loss=120.3222
	step [103/195], loss=121.3988
	step [104/195], loss=127.3690
	step [105/195], loss=131.6326
	step [106/195], loss=140.2015
	step [107/195], loss=129.0926
	step [108/195], loss=134.8546
	step [109/195], loss=113.7679
	step [110/195], loss=118.4931
	step [111/195], loss=125.2934
	step [112/195], loss=122.9041
	step [113/195], loss=125.7565
	step [114/195], loss=98.2886
	step [115/195], loss=125.5842
	step [116/195], loss=126.4701
	step [117/195], loss=93.6545
	step [118/195], loss=124.0974
	step [119/195], loss=127.2892
	step [120/195], loss=117.9496
	step [121/195], loss=111.0754
	step [122/195], loss=121.0759
	step [123/195], loss=109.3054
	step [124/195], loss=123.1397
	step [125/195], loss=136.3971
	step [126/195], loss=135.6176
	step [127/195], loss=124.6742
	step [128/195], loss=126.5167
	step [129/195], loss=113.5848
	step [130/195], loss=117.6840
	step [131/195], loss=122.6544
	step [132/195], loss=114.7155
	step [133/195], loss=130.8504
	step [134/195], loss=110.0109
	step [135/195], loss=115.0171
	step [136/195], loss=137.0341
	step [137/195], loss=122.4949
	step [138/195], loss=125.3680
	step [139/195], loss=110.8178
	step [140/195], loss=133.9395
	step [141/195], loss=117.7056
	step [142/195], loss=126.6617
	step [143/195], loss=115.9352
	step [144/195], loss=132.2046
	step [145/195], loss=131.1017
	step [146/195], loss=128.6441
	step [147/195], loss=127.8605
	step [148/195], loss=135.1917
	step [149/195], loss=126.6653
	step [150/195], loss=115.5978
	step [151/195], loss=114.6782
	step [152/195], loss=127.1020
	step [153/195], loss=109.5654
	step [154/195], loss=130.8599
	step [155/195], loss=132.5249
	step [156/195], loss=120.8041
	step [157/195], loss=120.7149
	step [158/195], loss=134.0080
	step [159/195], loss=114.0462
	step [160/195], loss=112.8192
	step [161/195], loss=115.2856
	step [162/195], loss=137.9953
	step [163/195], loss=105.6700
	step [164/195], loss=113.5604
	step [165/195], loss=138.8347
	step [166/195], loss=117.2036
	step [167/195], loss=104.6488
	step [168/195], loss=132.5841
	step [169/195], loss=118.2688
	step [170/195], loss=114.1782
	step [171/195], loss=123.4500
	step [172/195], loss=119.8226
	step [173/195], loss=119.4525
	step [174/195], loss=105.6913
	step [175/195], loss=126.0034
	step [176/195], loss=142.2873
	step [177/195], loss=106.0545
	step [178/195], loss=108.6268
	step [179/195], loss=130.6873
	step [180/195], loss=111.1628
	step [181/195], loss=130.3248
	step [182/195], loss=126.6287
	step [183/195], loss=120.5377
	step [184/195], loss=105.3056
	step [185/195], loss=116.1461
	step [186/195], loss=131.6400
	step [187/195], loss=111.7255
	step [188/195], loss=129.5666
	step [189/195], loss=128.9671
	step [190/195], loss=111.7978
	step [191/195], loss=128.5807
	step [192/195], loss=127.0976
	step [193/195], loss=124.7834
	step [194/195], loss=132.0593
	step [195/195], loss=10.6488
	Evaluating
	loss=0.0847, precision=0.4246, recall=0.9048, f1=0.5780
Training epoch 6
	step [1/195], loss=132.0678
	step [2/195], loss=111.7916
	step [3/195], loss=113.7100
	step [4/195], loss=108.4868
	step [5/195], loss=121.8875
	step [6/195], loss=109.7267
	step [7/195], loss=117.1943
	step [8/195], loss=115.5549
	step [9/195], loss=109.7986
	step [10/195], loss=137.0352
	step [11/195], loss=128.1077
	step [12/195], loss=118.2484
	step [13/195], loss=130.1794
	step [14/195], loss=131.5036
	step [15/195], loss=122.6814
	step [16/195], loss=142.5536
	step [17/195], loss=110.0707
	step [18/195], loss=137.2212
	step [19/195], loss=106.9649
	step [20/195], loss=148.5969
	step [21/195], loss=114.2242
	step [22/195], loss=123.1596
	step [23/195], loss=118.2587
	step [24/195], loss=135.5699
	step [25/195], loss=133.9737
	step [26/195], loss=116.9292
	step [27/195], loss=105.8916
	step [28/195], loss=98.8063
	step [29/195], loss=111.7771
	step [30/195], loss=117.5462
	step [31/195], loss=119.0711
	step [32/195], loss=108.4553
	step [33/195], loss=115.2618
	step [34/195], loss=103.2925
	step [35/195], loss=128.7392
	step [36/195], loss=136.8588
	step [37/195], loss=126.4862
	step [38/195], loss=121.4222
	step [39/195], loss=103.1342
	step [40/195], loss=123.5817
	step [41/195], loss=125.2723
	step [42/195], loss=138.1152
	step [43/195], loss=124.3658
	step [44/195], loss=118.1502
	step [45/195], loss=112.2150
	step [46/195], loss=126.3242
	step [47/195], loss=124.2978
	step [48/195], loss=87.9691
	step [49/195], loss=108.4646
	step [50/195], loss=135.3880
	step [51/195], loss=119.5043
	step [52/195], loss=122.8287
	step [53/195], loss=132.8846
	step [54/195], loss=141.9827
	step [55/195], loss=116.4478
	step [56/195], loss=125.2562
	step [57/195], loss=108.3656
	step [58/195], loss=114.4547
	step [59/195], loss=129.7516
	step [60/195], loss=124.5799
	step [61/195], loss=109.1866
	step [62/195], loss=129.5721
	step [63/195], loss=91.3206
	step [64/195], loss=100.3182
	step [65/195], loss=111.1190
	step [66/195], loss=136.2690
	step [67/195], loss=115.7470
	step [68/195], loss=121.1848
	step [69/195], loss=105.6220
	step [70/195], loss=102.5354
	step [71/195], loss=106.1740
	step [72/195], loss=121.3096
	step [73/195], loss=113.2312
	step [74/195], loss=126.5146
	step [75/195], loss=126.1086
	step [76/195], loss=136.8195
	step [77/195], loss=133.3186
	step [78/195], loss=134.3243
	step [79/195], loss=127.3705
	step [80/195], loss=118.0616
	step [81/195], loss=108.1160
	step [82/195], loss=131.5194
	step [83/195], loss=117.3843
	step [84/195], loss=110.2850
	step [85/195], loss=114.8627
	step [86/195], loss=106.7673
	step [87/195], loss=119.0264
	step [88/195], loss=118.9047
	step [89/195], loss=111.3164
	step [90/195], loss=129.4712
	step [91/195], loss=141.8318
	step [92/195], loss=118.2503
	step [93/195], loss=124.6005
	step [94/195], loss=127.3851
	step [95/195], loss=125.2667
	step [96/195], loss=123.7890
	step [97/195], loss=104.1558
	step [98/195], loss=106.9191
	step [99/195], loss=134.1905
	step [100/195], loss=111.9900
	step [101/195], loss=123.2494
	step [102/195], loss=124.3708
	step [103/195], loss=116.8299
	step [104/195], loss=124.5005
	step [105/195], loss=121.7357
	step [106/195], loss=138.6568
	step [107/195], loss=109.0851
	step [108/195], loss=110.8415
	step [109/195], loss=124.4121
	step [110/195], loss=115.3447
	step [111/195], loss=108.5137
	step [112/195], loss=112.9799
	step [113/195], loss=120.9507
	step [114/195], loss=109.8389
	step [115/195], loss=133.3303
	step [116/195], loss=114.7431
	step [117/195], loss=120.5885
	step [118/195], loss=144.4977
	step [119/195], loss=134.1592
	step [120/195], loss=127.0520
	step [121/195], loss=104.8445
	step [122/195], loss=107.3563
	step [123/195], loss=120.9745
	step [124/195], loss=111.3101
	step [125/195], loss=106.6917
	step [126/195], loss=130.0414
	step [127/195], loss=98.2103
	step [128/195], loss=116.7959
	step [129/195], loss=122.2522
	step [130/195], loss=128.0805
	step [131/195], loss=103.7847
	step [132/195], loss=119.7726
	step [133/195], loss=100.6795
	step [134/195], loss=119.7938
	step [135/195], loss=118.5586
	step [136/195], loss=127.7334
	step [137/195], loss=122.2639
	step [138/195], loss=101.0832
	step [139/195], loss=109.1819
	step [140/195], loss=116.1261
	step [141/195], loss=117.0696
	step [142/195], loss=117.8143
	step [143/195], loss=111.2124
	step [144/195], loss=111.7871
	step [145/195], loss=119.9952
	step [146/195], loss=116.9019
	step [147/195], loss=112.1662
	step [148/195], loss=118.2992
	step [149/195], loss=122.1694
	step [150/195], loss=106.6252
	step [151/195], loss=124.6054
	step [152/195], loss=126.8403
	step [153/195], loss=116.3326
	step [154/195], loss=128.2071
	step [155/195], loss=145.6571
	step [156/195], loss=103.8985
	step [157/195], loss=117.2890
	step [158/195], loss=113.9842
	step [159/195], loss=107.8444
	step [160/195], loss=104.7603
	step [161/195], loss=120.6125
	step [162/195], loss=111.4458
	step [163/195], loss=123.0178
	step [164/195], loss=107.3677
	step [165/195], loss=121.3862
	step [166/195], loss=114.4854
	step [167/195], loss=106.0488
	step [168/195], loss=142.5518
	step [169/195], loss=126.6156
	step [170/195], loss=128.1097
	step [171/195], loss=120.2314
	step [172/195], loss=114.0745
	step [173/195], loss=106.1344
	step [174/195], loss=107.6783
	step [175/195], loss=126.3611
	step [176/195], loss=115.4817
	step [177/195], loss=114.9331
	step [178/195], loss=110.2321
	step [179/195], loss=123.8564
	step [180/195], loss=136.5821
	step [181/195], loss=112.8059
	step [182/195], loss=109.5272
	step [183/195], loss=108.3206
	step [184/195], loss=118.9669
	step [185/195], loss=126.6990
	step [186/195], loss=105.5124
	step [187/195], loss=110.8565
	step [188/195], loss=117.2341
	step [189/195], loss=115.8893
	step [190/195], loss=129.6753
	step [191/195], loss=110.6974
	step [192/195], loss=107.7816
	step [193/195], loss=118.5863
	step [194/195], loss=134.7852
	step [195/195], loss=20.5885
	Evaluating
	loss=0.0647, precision=0.5356, recall=0.8968, f1=0.6707
saving model as: 3_saved_model.pth
Training epoch 7
	step [1/195], loss=118.9681
	step [2/195], loss=113.6842
	step [3/195], loss=114.4186
	step [4/195], loss=98.5419
	step [5/195], loss=122.2729
	step [6/195], loss=100.0630
	step [7/195], loss=102.9736
	step [8/195], loss=112.0634
	step [9/195], loss=120.0947
	step [10/195], loss=117.7944
	step [11/195], loss=119.7302
	step [12/195], loss=97.2867
	step [13/195], loss=119.8625
	step [14/195], loss=135.9534
	step [15/195], loss=110.2566
	step [16/195], loss=121.1259
	step [17/195], loss=118.2508
	step [18/195], loss=105.9471
	step [19/195], loss=126.5501
	step [20/195], loss=121.6636
	step [21/195], loss=116.7008
	step [22/195], loss=110.5652
	step [23/195], loss=124.1752
	step [24/195], loss=128.3123
	step [25/195], loss=131.9934
	step [26/195], loss=126.1644
	step [27/195], loss=130.7036
	step [28/195], loss=120.3307
	step [29/195], loss=100.5159
	step [30/195], loss=105.4289
	step [31/195], loss=112.7368
	step [32/195], loss=101.3551
	step [33/195], loss=120.9180
	step [34/195], loss=111.0658
	step [35/195], loss=132.8126
	step [36/195], loss=105.1444
	step [37/195], loss=123.1373
	step [38/195], loss=118.6972
	step [39/195], loss=108.6877
	step [40/195], loss=115.6930
	step [41/195], loss=116.3677
	step [42/195], loss=103.9818
	step [43/195], loss=121.0869
	step [44/195], loss=122.9995
	step [45/195], loss=115.0632
	step [46/195], loss=133.0190
	step [47/195], loss=113.2089
	step [48/195], loss=127.3132
	step [49/195], loss=108.0821
	step [50/195], loss=123.4453
	step [51/195], loss=101.0235
	step [52/195], loss=112.1873
	step [53/195], loss=122.3433
	step [54/195], loss=116.2939
	step [55/195], loss=106.6882
	step [56/195], loss=114.6385
	step [57/195], loss=109.0094
	step [58/195], loss=109.1013
	step [59/195], loss=99.1709
	step [60/195], loss=119.0058
	step [61/195], loss=117.5713
	step [62/195], loss=105.2188
	step [63/195], loss=109.4110
	step [64/195], loss=124.3420
	step [65/195], loss=110.1456
	step [66/195], loss=104.5083
	step [67/195], loss=120.6984
	step [68/195], loss=111.3691
	step [69/195], loss=109.1254
	step [70/195], loss=136.8254
	step [71/195], loss=119.6552
	step [72/195], loss=101.7693
	step [73/195], loss=119.3018
	step [74/195], loss=117.2053
	step [75/195], loss=111.5655
	step [76/195], loss=97.3012
	step [77/195], loss=116.5799
	step [78/195], loss=100.0817
	step [79/195], loss=136.3616
	step [80/195], loss=113.7878
	step [81/195], loss=124.5000
	step [82/195], loss=116.2424
	step [83/195], loss=123.6708
	step [84/195], loss=118.0021
	step [85/195], loss=97.4301
	step [86/195], loss=124.8694
	step [87/195], loss=133.3176
	step [88/195], loss=112.2519
	step [89/195], loss=132.7901
	step [90/195], loss=113.0287
	step [91/195], loss=124.2571
	step [92/195], loss=126.1927
	step [93/195], loss=113.1272
	step [94/195], loss=99.4451
	step [95/195], loss=110.8477
	step [96/195], loss=121.0056
	step [97/195], loss=104.7216
	step [98/195], loss=122.0779
	step [99/195], loss=122.2473
	step [100/195], loss=108.3503
	step [101/195], loss=106.2106
	step [102/195], loss=123.4735
	step [103/195], loss=104.1881
	step [104/195], loss=103.4619
	step [105/195], loss=130.2745
	step [106/195], loss=110.1077
	step [107/195], loss=114.6002
	step [108/195], loss=117.3842
	step [109/195], loss=114.7627
	step [110/195], loss=114.8955
	step [111/195], loss=107.7309
	step [112/195], loss=112.4509
	step [113/195], loss=114.7841
	step [114/195], loss=112.7182
	step [115/195], loss=107.3090
	step [116/195], loss=117.6931
	step [117/195], loss=111.4754
	step [118/195], loss=100.1670
	step [119/195], loss=128.4481
	step [120/195], loss=117.3525
	step [121/195], loss=100.5779
	step [122/195], loss=112.3109
	step [123/195], loss=123.3691
	step [124/195], loss=123.4015
	step [125/195], loss=122.4622
	step [126/195], loss=125.1071
	step [127/195], loss=116.3496
	step [128/195], loss=126.4925
	step [129/195], loss=109.0590
	step [130/195], loss=126.5984
	step [131/195], loss=117.1645
	step [132/195], loss=98.5144
	step [133/195], loss=113.0687
	step [134/195], loss=111.0634
	step [135/195], loss=142.9727
	step [136/195], loss=131.8361
	step [137/195], loss=108.0475
	step [138/195], loss=118.2398
	step [139/195], loss=109.4266
	step [140/195], loss=90.2564
	step [141/195], loss=130.9024
	step [142/195], loss=113.1387
	step [143/195], loss=115.3450
	step [144/195], loss=116.7850
	step [145/195], loss=118.0405
	step [146/195], loss=108.1980
	step [147/195], loss=114.9216
	step [148/195], loss=114.5932
	step [149/195], loss=113.9778
	step [150/195], loss=109.6795
	step [151/195], loss=109.8239
	step [152/195], loss=91.5413
	step [153/195], loss=126.8474
	step [154/195], loss=110.7213
	step [155/195], loss=131.9323
	step [156/195], loss=129.3370
	step [157/195], loss=99.5458
	step [158/195], loss=115.0794
	step [159/195], loss=109.2199
	step [160/195], loss=104.6776
	step [161/195], loss=126.3775
	step [162/195], loss=115.0134
	step [163/195], loss=108.1120
	step [164/195], loss=91.3253
	step [165/195], loss=107.4646
	step [166/195], loss=123.0484
	step [167/195], loss=110.9945
	step [168/195], loss=116.5826
	step [169/195], loss=117.2827
	step [170/195], loss=111.0534
	step [171/195], loss=122.9507
	step [172/195], loss=112.5247
	step [173/195], loss=117.5774
	step [174/195], loss=111.8505
	step [175/195], loss=126.0692
	step [176/195], loss=124.6704
	step [177/195], loss=112.2939
	step [178/195], loss=109.4982
	step [179/195], loss=126.8329
	step [180/195], loss=118.2756
	step [181/195], loss=120.7966
	step [182/195], loss=105.0455
	step [183/195], loss=113.3930
	step [184/195], loss=102.3047
	step [185/195], loss=92.6819
	step [186/195], loss=116.2440
	step [187/195], loss=110.9203
	step [188/195], loss=116.8244
	step [189/195], loss=101.3966
	step [190/195], loss=111.7048
	step [191/195], loss=118.9322
	step [192/195], loss=105.5481
	step [193/195], loss=108.4302
	step [194/195], loss=113.6652
	step [195/195], loss=11.6672
	Evaluating
	loss=0.0546, precision=0.4741, recall=0.8993, f1=0.6209
Training epoch 8
	step [1/195], loss=107.2683
	step [2/195], loss=118.7556
	step [3/195], loss=114.6462
	step [4/195], loss=102.8488
	step [5/195], loss=113.0446
	step [6/195], loss=95.0506
	step [7/195], loss=110.6616
	step [8/195], loss=105.3548
	step [9/195], loss=106.6249
	step [10/195], loss=114.8497
	step [11/195], loss=106.1146
	step [12/195], loss=124.9308
	step [13/195], loss=121.9013
	step [14/195], loss=116.0745
	step [15/195], loss=110.3717
	step [16/195], loss=103.4445
	step [17/195], loss=112.8121
	step [18/195], loss=122.3768
	step [19/195], loss=104.4771
	step [20/195], loss=124.9386
	step [21/195], loss=109.9551
	step [22/195], loss=114.6278
	step [23/195], loss=109.6915
	step [24/195], loss=98.3346
	step [25/195], loss=116.1663
	step [26/195], loss=97.5923
	step [27/195], loss=110.6439
	step [28/195], loss=123.7142
	step [29/195], loss=99.7399
	step [30/195], loss=119.9428
	step [31/195], loss=119.3804
	step [32/195], loss=122.3477
	step [33/195], loss=118.0872
	step [34/195], loss=120.1566
	step [35/195], loss=108.8901
	step [36/195], loss=106.1972
	step [37/195], loss=128.3520
	step [38/195], loss=107.7139
	step [39/195], loss=136.0445
	step [40/195], loss=120.3856
	step [41/195], loss=117.7945
	step [42/195], loss=127.6740
	step [43/195], loss=118.6611
	step [44/195], loss=113.9341
	step [45/195], loss=102.7321
	step [46/195], loss=103.7162
	step [47/195], loss=112.2945
	step [48/195], loss=100.1120
	step [49/195], loss=125.8694
	step [50/195], loss=130.5052
	step [51/195], loss=108.7372
	step [52/195], loss=121.8262
	step [53/195], loss=130.6462
	step [54/195], loss=124.9260
	step [55/195], loss=119.2706
	step [56/195], loss=107.3201
	step [57/195], loss=110.4042
	step [58/195], loss=97.7148
	step [59/195], loss=115.0909
	step [60/195], loss=127.1937
	step [61/195], loss=109.1026
	step [62/195], loss=126.8785
	step [63/195], loss=126.6580
	step [64/195], loss=115.8781
	step [65/195], loss=115.6667
	step [66/195], loss=114.9627
	step [67/195], loss=117.1889
	step [68/195], loss=132.7209
	step [69/195], loss=118.0450
	step [70/195], loss=133.8297
	step [71/195], loss=116.4123
	step [72/195], loss=120.2227
	step [73/195], loss=132.6603
	step [74/195], loss=115.8934
	step [75/195], loss=107.0090
	step [76/195], loss=116.9223
	step [77/195], loss=116.9245
	step [78/195], loss=120.2946
	step [79/195], loss=110.8864
	step [80/195], loss=91.1349
	step [81/195], loss=129.2009
	step [82/195], loss=129.6474
	step [83/195], loss=112.9052
	step [84/195], loss=104.3399
	step [85/195], loss=121.6508
	step [86/195], loss=110.6045
	step [87/195], loss=112.0510
	step [88/195], loss=92.2696
	step [89/195], loss=105.4027
	step [90/195], loss=100.0471
	step [91/195], loss=134.4556
	step [92/195], loss=123.5466
	step [93/195], loss=105.0093
	step [94/195], loss=110.6241
	step [95/195], loss=117.4217
	step [96/195], loss=102.7877
	step [97/195], loss=105.2811
	step [98/195], loss=112.4139
	step [99/195], loss=111.1602
	step [100/195], loss=114.8782
	step [101/195], loss=115.6899
	step [102/195], loss=111.2394
	step [103/195], loss=127.6120
	step [104/195], loss=118.3348
	step [105/195], loss=99.0275
	step [106/195], loss=99.5735
	step [107/195], loss=114.7915
	step [108/195], loss=108.5757
	step [109/195], loss=107.0435
	step [110/195], loss=120.7202
	step [111/195], loss=102.8841
	step [112/195], loss=115.0208
	step [113/195], loss=105.5613
	step [114/195], loss=107.3284
	step [115/195], loss=134.1951
	step [116/195], loss=118.9925
	step [117/195], loss=106.0493
	step [118/195], loss=113.5789
	step [119/195], loss=115.3016
	step [120/195], loss=104.3101
	step [121/195], loss=102.1937
	step [122/195], loss=114.4699
	step [123/195], loss=117.3894
	step [124/195], loss=130.6418
	step [125/195], loss=112.0916
	step [126/195], loss=133.9172
	step [127/195], loss=111.2807
	step [128/195], loss=111.4520
	step [129/195], loss=119.5305
	step [130/195], loss=113.7621
	step [131/195], loss=98.2441
	step [132/195], loss=111.4460
	step [133/195], loss=98.5559
	step [134/195], loss=118.6220
	step [135/195], loss=115.1862
	step [136/195], loss=112.7700
	step [137/195], loss=112.7732
	step [138/195], loss=107.7984
	step [139/195], loss=129.7500
	step [140/195], loss=116.3294
	step [141/195], loss=116.9858
	step [142/195], loss=121.3663
	step [143/195], loss=93.5110
	step [144/195], loss=105.7209
	step [145/195], loss=110.4099
	step [146/195], loss=98.9808
	step [147/195], loss=111.9922
	step [148/195], loss=104.0462
	step [149/195], loss=99.6256
	step [150/195], loss=127.7006
	step [151/195], loss=106.0146
	step [152/195], loss=103.2437
	step [153/195], loss=119.4385
	step [154/195], loss=99.0907
	step [155/195], loss=98.2510
	step [156/195], loss=119.8857
	step [157/195], loss=115.6447
	step [158/195], loss=113.8279
	step [159/195], loss=99.7094
	step [160/195], loss=111.2039
	step [161/195], loss=105.9659
	step [162/195], loss=90.8042
	step [163/195], loss=93.7918
	step [164/195], loss=111.6246
	step [165/195], loss=114.1268
	step [166/195], loss=123.2271
	step [167/195], loss=116.2578
	step [168/195], loss=100.6199
	step [169/195], loss=92.9267
	step [170/195], loss=102.8850
	step [171/195], loss=117.4459
	step [172/195], loss=105.2985
	step [173/195], loss=115.1665
	step [174/195], loss=99.3415
	step [175/195], loss=110.0712
	step [176/195], loss=106.6767
	step [177/195], loss=98.2943
	step [178/195], loss=102.2714
	step [179/195], loss=110.5669
	step [180/195], loss=105.7368
	step [181/195], loss=104.3178
	step [182/195], loss=111.4538
	step [183/195], loss=108.7407
	step [184/195], loss=116.5939
	step [185/195], loss=112.1615
	step [186/195], loss=105.9975
	step [187/195], loss=95.9371
	step [188/195], loss=131.9834
	step [189/195], loss=109.0646
	step [190/195], loss=99.7753
	step [191/195], loss=128.1349
	step [192/195], loss=108.3963
	step [193/195], loss=121.3666
	step [194/195], loss=101.0249
	step [195/195], loss=14.6144
	Evaluating
	loss=0.0441, precision=0.5254, recall=0.9129, f1=0.6669
Training epoch 9
	step [1/195], loss=117.4813
	step [2/195], loss=120.5658
	step [3/195], loss=99.6710
	step [4/195], loss=116.6061
	step [5/195], loss=108.8834
	step [6/195], loss=123.2916
	step [7/195], loss=103.9102
	step [8/195], loss=116.2145
	step [9/195], loss=107.0469
	step [10/195], loss=96.0131
	step [11/195], loss=113.1563
	step [12/195], loss=95.9018
	step [13/195], loss=120.0830
	step [14/195], loss=112.5776
	step [15/195], loss=112.7165
	step [16/195], loss=115.1773
	step [17/195], loss=95.8686
	step [18/195], loss=113.3834
	step [19/195], loss=109.4926
	step [20/195], loss=97.1911
	step [21/195], loss=104.1994
	step [22/195], loss=123.5158
	step [23/195], loss=120.6439
	step [24/195], loss=113.6570
	step [25/195], loss=116.4551
	step [26/195], loss=116.7628
	step [27/195], loss=108.7895
	step [28/195], loss=120.5228
	step [29/195], loss=102.9372
	step [30/195], loss=105.8925
	step [31/195], loss=107.3613
	step [32/195], loss=99.1554
	step [33/195], loss=113.1990
	step [34/195], loss=101.0744
	step [35/195], loss=108.9834
	step [36/195], loss=131.7659
	step [37/195], loss=108.4208
	step [38/195], loss=111.6081
	step [39/195], loss=118.6988
	step [40/195], loss=108.6156
	step [41/195], loss=109.7114
	step [42/195], loss=110.9466
	step [43/195], loss=112.8340
	step [44/195], loss=113.0224
	step [45/195], loss=108.2696
	step [46/195], loss=109.1569
	step [47/195], loss=111.4416
	step [48/195], loss=106.9322
	step [49/195], loss=101.3038
	step [50/195], loss=107.1311
	step [51/195], loss=121.3958
	step [52/195], loss=102.9696
	step [53/195], loss=114.3175
	step [54/195], loss=111.9408
	step [55/195], loss=83.1316
	step [56/195], loss=106.4169
	step [57/195], loss=108.4529
	step [58/195], loss=84.4445
	step [59/195], loss=102.2811
	step [60/195], loss=118.0859
	step [61/195], loss=109.4927
	step [62/195], loss=109.8462
	step [63/195], loss=100.0717
	step [64/195], loss=104.0833
	step [65/195], loss=103.3061
	step [66/195], loss=114.5655
	step [67/195], loss=98.7225
	step [68/195], loss=99.6552
	step [69/195], loss=125.8536
	step [70/195], loss=97.7872
	step [71/195], loss=103.6841
	step [72/195], loss=89.5389
	step [73/195], loss=107.2936
	step [74/195], loss=105.7787
	step [75/195], loss=102.3722
	step [76/195], loss=106.4830
	step [77/195], loss=119.3024
	step [78/195], loss=110.0601
	step [79/195], loss=108.4107
	step [80/195], loss=109.2636
	step [81/195], loss=127.9617
	step [82/195], loss=108.4481
	step [83/195], loss=104.8724
	step [84/195], loss=102.0900
	step [85/195], loss=112.8670
	step [86/195], loss=101.2567
	step [87/195], loss=124.6742
	step [88/195], loss=114.8673
	step [89/195], loss=108.8351
	step [90/195], loss=116.7261
	step [91/195], loss=97.0051
	step [92/195], loss=119.6155
	step [93/195], loss=111.7349
	step [94/195], loss=103.8853
	step [95/195], loss=118.7560
	step [96/195], loss=105.8006
	step [97/195], loss=112.3092
	step [98/195], loss=118.0410
	step [99/195], loss=109.9753
	step [100/195], loss=109.0754
	step [101/195], loss=133.9399
	step [102/195], loss=100.7049
	step [103/195], loss=102.0860
	step [104/195], loss=107.0174
	step [105/195], loss=123.5781
	step [106/195], loss=110.2414
	step [107/195], loss=105.9291
	step [108/195], loss=104.7079
	step [109/195], loss=115.3842
	step [110/195], loss=86.8563
	step [111/195], loss=110.7589
	step [112/195], loss=115.2239
	step [113/195], loss=110.6347
	step [114/195], loss=115.6756
	step [115/195], loss=104.4699
	step [116/195], loss=106.3619
	step [117/195], loss=101.7959
	step [118/195], loss=116.6389
	step [119/195], loss=115.8932
	step [120/195], loss=107.4586
	step [121/195], loss=114.3091
	step [122/195], loss=97.3463
	step [123/195], loss=125.0514
	step [124/195], loss=99.9717
	step [125/195], loss=111.4147
	step [126/195], loss=113.0405
	step [127/195], loss=107.7635
	step [128/195], loss=94.2794
	step [129/195], loss=115.3016
	step [130/195], loss=104.6958
	step [131/195], loss=115.2104
	step [132/195], loss=108.4490
	step [133/195], loss=108.1694
	step [134/195], loss=99.3204
	step [135/195], loss=109.2117
	step [136/195], loss=103.8878
	step [137/195], loss=120.7473
	step [138/195], loss=105.6599
	step [139/195], loss=104.6954
	step [140/195], loss=109.4268
	step [141/195], loss=124.1877
	step [142/195], loss=127.8152
	step [143/195], loss=110.5603
	step [144/195], loss=117.7597
	step [145/195], loss=109.6803
	step [146/195], loss=107.9203
	step [147/195], loss=116.5490
	step [148/195], loss=88.3623
	step [149/195], loss=111.7116
	step [150/195], loss=114.4007
	step [151/195], loss=121.5106
	step [152/195], loss=125.7689
	step [153/195], loss=108.7419
	step [154/195], loss=100.6109
	step [155/195], loss=115.4033
	step [156/195], loss=113.5201
	step [157/195], loss=109.1515
	step [158/195], loss=103.8358
	step [159/195], loss=117.0312
	step [160/195], loss=116.2485
	step [161/195], loss=120.5979
	step [162/195], loss=110.6427
	step [163/195], loss=111.5232
	step [164/195], loss=114.5282
	step [165/195], loss=107.8757
	step [166/195], loss=107.0796
	step [167/195], loss=98.5836
	step [168/195], loss=105.0557
	step [169/195], loss=123.8192
	step [170/195], loss=105.9060
	step [171/195], loss=121.7098
	step [172/195], loss=114.4825
	step [173/195], loss=119.1813
	step [174/195], loss=102.3712
	step [175/195], loss=102.6717
	step [176/195], loss=105.8988
	step [177/195], loss=104.9193
	step [178/195], loss=108.6063
	step [179/195], loss=107.6847
	step [180/195], loss=95.3287
	step [181/195], loss=102.6353
	step [182/195], loss=110.4070
	step [183/195], loss=117.9613
	step [184/195], loss=104.7088
	step [185/195], loss=103.9433
	step [186/195], loss=115.1237
	step [187/195], loss=92.0040
	step [188/195], loss=99.1043
	step [189/195], loss=109.2534
	step [190/195], loss=101.0546
	step [191/195], loss=107.4651
	step [192/195], loss=104.8045
	step [193/195], loss=116.1555
	step [194/195], loss=115.2391
	step [195/195], loss=8.2718
	Evaluating
	loss=0.0378, precision=0.4920, recall=0.9140, f1=0.6397
Training epoch 10
